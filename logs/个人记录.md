格式示例：

---

阶段(<阶段序号>)：<阶段时间>
    <阶段计划>

时间：\<年\>.\<月\>.\<日\>-\<年\>.\<月\>.\<日\>，阶段(<阶段序号>)

\[文献阅读\]
- <发表年份>-<会议/期刊>-<文献名称>
    <一句话总结>

\[书籍阅读\]
- <发表年份>-<书籍名称>: <阅读范围>
    CH<章节序号>-<章节名称>：<一句话总结/要点提炼>

\[文档阅读\]
- <文档名称>-<文档版本>: <阅读范围>
    CH<章节序号>-<章节名称>：<一句话总结/要点提炼>

\[下周计划\]
    <关于文献阅读>
    <关于书籍阅读>


---
# 阶段1
阶段(1)：2024.9-2025.6
    广泛阅读，在工程和科研方面打下理论基础
## 2024年
### 8月
时间：2024.8.22-2024.9.9

\[文献阅读\]
- 2023-NeurIPS-Are Emergent Abilities of Large Language Models a Mirage?
    研究者对于度量的选取造就了 LLM 具有涌现能力的“海市蜃楼” 

\[书籍阅读\]
- 2015-Mastering CMake: CH2-CH15、CH17
    CH2-Getting Started on Your Computer：CMake 构建流程纵览
    CH3-Writing CMakeLists Files：CMake 语言纵览
    CH4-CMake Cache：CMake 缓存机制介绍：`CMakeCache.txt`
    CH5-Key Concepts：CMake 概念介绍：源文件、目标文件、属性
    CH6-Policies：CMake 策略机制：为了兼容性
    CH7-Modules：CMake 模块：CMake 提供的 utility
    CH8-Installing Files：`Install()` 命令
    CH9-System Inspections：借助宏编写跨平台软件；`try_run/compile()` 命令
    CH10-Finding Packages：借助 CMake 分发的软件依赖包
    CH11-Custom Commands：为自定义目标添加自定义构建规则
    CH12-Packing with CPack：借助 CPack 调用本地打包工具
    CH13-Testing with CMake and CTest：`add_test()`
    CH14-CMake Tutorial：纵览：简单构建、添加库、添加属性、通过系统审查添加宏、添加测试、简单安装、添加共享库、生成器表达式（实际构建时才确定值的变量）、导出 CMake 软件包
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH1
    CH1-Intruduction：概率图模型：构建图模型表示概率系统中的独立性和依赖性关系，依据图模型进行概率推导
- 2023-A Survey of Large Language Models: CH0(Abstract)-CH5
    CH0-Abstract：全文结构分为：LLM 的预训练、微调、使用、评估四部分
    CH1-Introduction：语言模型的发展：SLM-NLM-PLM-LLM
    CH2-Overview：语言模型的 scaling 和涌现
    CH3-Resources of LLMs：公开可用的模型权重和训练数据集
    CH4-Pre-Training：数据收集、清洗；模型架构；训练技巧
    CH5-Adaptation of LLMs：指令微调；对齐微调；参数高效微调；存储高效微调

### 9月
时间：2024.9.9-2024.9.16

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH1
    CH1-What is Combinactorics： 组合数学关于：离散结构的存在、计数、分析和优化
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH2
    CH2-Foundation： 条件独立、条件概率密度函数、MAP 查询、图论
- 2023-A Survey of Large Language Models: CH6
    CH6-Utilization：提示词技巧：上下文学习（即（输入-输出）对）、思维链（即（输入-推理步骤-输出）三元组）、规划

时间：2024.9.16-2024.9.23

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH2
    CH2-Permutations and Combinations: 集合的排列/组合（组合数=排列数+除法原理），多重集合的排列/组合（多重集排列数=集合排列数+除法原理；多重集组合数=线性等式的解集大小），古典概型
- 2004-Convex Optimization: CH2-CH2.5
    CH2-CH2.5-Convex Sets: 凸组合、仿射组合；典型的凸集；保凸运算；凸集的支撑/分离超平面
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH3-CH3.3
    CH3-CH3.3-The Baysian Network Representation: 贝叶斯网络：以图的语义表示联合分布中成立的条件独立性；根据网络结构，将联合分布分解为多个条件概率分布的乘积

时间：2024.9.23-2024.9.30

\[文献阅读\]
- A Survey of Large Language Models: CH7-CH8
    CH7-Capacity and Evaluation: LLM 能力：1. 基本能力：语言生成（包括代码）、知识利用（例如知识密集的 QA）、复杂分析（例如数学推理）；2. 高级能力：对齐、和外部环境交互（例如为具身智能体生成动作规划）、工具利用（例如根据任务调用合适 API）；对一些 benchmark 的介绍

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH5
    CH5-Local Probabilistic Models: 紧凑的条件概率分布表示：利用针对上下文的独立性；假设原因之间影响独立，基于此假设的模型有：噪声或、BN2O（多观测的噪声或）、广义线性模型（“分数”线性于所有原因变量）、条件线性高斯（实际上将联合分布建模为了混合高斯分布）

### 10月
时间：2024.9.30-2024.10.7

\[文献阅读\]
- 2023-A Survey of Large Language Models: CH8-CH9
    CH8-Applicatoin: LLM 在多个任务中的应用
    CH9-Conclusion and future directions
- 2010-Importance Sampling A Review
    重要性采样主要的目标就是减少 Monte Carlo 采样估计的方差
    适应性参数化重要性采样：$q (x)$ 定义为多元正态或学生分布，优化和变异系数相关的度量以得到较优的分布参数
    序列重要性采样：链式分解 $p (x)$，链式构造 $q (x)$
    退火重要性采样：顺序地近似 $p (x)$，和 diffusion 很相似

\[书籍阅读\]
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH6-CH6.2
    CH6-Template-based Representations: 时序模型；Markov 假设+ 2-TBN = DBN（动态贝叶斯网络）；DNB 通常建模为状态-观测模型（状态和观测分离考虑，观测不会影响状态），两个例子：隐 Markov 模型，线性动态网络（所有的依赖都是线性 Gaussian）
- 2024-面向计算机科学的组合数学: CH1.7, CH2
    CH1.7-生成全排列: 中介数和排列之间的一一对应关系

时间：2024.10.7-2024.10.14

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: CH0-CH3.1
    CH0-CH3.1: Abstract, Background, Algorithm 1; 算法1本质上就是 attention 计算的 tiled 实现，其中主要是 softmax 因子反复的重缩放让算法1看起来不那么直观，但是缩放计算对于数值稳定性还是关键的。算法1中，对于每个 query ，它的 attention 结果是随着外层循环逐渐累加的，在每一次外层循环，其已经累积的部分 attention 结果中的 value 权重都会动态调整/更新，同时加上新的部分 attention 结果

\[书籍阅读\]
- A Tour of C++: CH5
- 面向计算机科学的组合数学: CH2.1-CH2.3
    CH2-鸽巢原理: 鸽巢原理仅解决存在性问题
- Probabilistic Graphical Models-Principles and Techniques: CH4-CH4.3.1
    CH4-CH4.3.1: Markov 网络的参数化：思路来源于统计物理学，是很直观的“统计”，使用因子函数衡量两个变量/粒子的交互/亲和性，使用规范化的因子乘积表示联合分布（Gibbs 分布），用以描述特定“配置”出现的概率。Markov 网络中的分离准则是可靠且若完备的（可靠：网络中，某独立性存在 --> 任意分解于网络的分布中，该独立性存在；弱完备：网络中，某独立性不存在 --> 某个分解于网络的分布中，该独立性不存在） 

\[文档阅读\]
- ultralytics v8.3.6 : Quickstart, Usage (Python usage, Callbacks, Configuration, Simple Utilities, Advanced Customization)
    对 YOLO 的 Python API 的简要介绍

时间：2024.10.14-2024.10.21

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: Sec3.1-Sec5
    Sec3.1-IO Analysis: FlashAttention 的 IO 复杂度为 $\Theta(N^2d^2M^{-1})$ ，标准 Attention 算法的则为 $\Theta (Nd + N^2)$，主要的差别就在于其中的 $M$ 和 $N^2$。标准 Attention 算法完全不使用 SRAM，所有访存都是 global memory 访问，其中对于 “权重矩阵” $P\in \mathbb R^{N\times N}$ 贡献了大量的 IO 复杂度 ($N^2$)。FlashAttention 利用了 SRAM，并且完全不存储 $P$ 到 DRAM，而是在算法整个过程保持一块 $P$ 在片上，因此减少了 IO 复杂度。
    Sec3.2-Block sparse Flash-Attention: 块稀疏拓展的差异就在于限制了“注意”的范围，故计算时可以跳过被 mask 的项，进而减少了访存和计算。
    Sec4-Experiments: FlashAttention 训练更快； FlashAttention 更内存高效（线性于序列长度），故允许更长的训练上下文。其原因在于 FlashAttention 不会一次直接计算整个 "权重矩阵" $P\in \mathbb R^{N\times N}$ ，而是在循环中逐行计算，实际上是以时间换空间， FLOP 提高了，但额外内存使用限制在了 $O (N)$ 而不是 $O (N^2)$ 。FLOP 提高带来的额外计算时间则被更少的 DRAM 访问抵消。
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec0-Sec2
    Sec0-Summary: 该文提出了 HC 定理的一种证明，进而强调了建模空间交互时，条件概率模型实际是优于联合概率模型的。
    Sec1-Sec2: 对于正分布，条件概率可以用于推导整个系统的联合概率，这依赖于 HC 定理。
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.3.1-CH4.4.2
    CH4.3.1-CH4.4.2: Markov 网络编码了三类独立性：成对独立性、局部独立性（Markov 毯）、全局独立性（d-seperation）。对于正分布，三者等价，对于非正分布（存在确定关系的分布）三者不等价，这是因为 Markov 网络的语义无法表示确定性关系。根据 HC 定理，正分布 $P$ 分解于 Markov network $\mathcal H$ 等价于 $P$ 满足 $\mathcal H$ 编码的三类独立性。
- 面向计算机科学的组合数学: CH3-CH3.3
    母函数：使用幂级数表示数列（数列由幂级数的系数构造）
\[文档阅读\]
- Pytorch 2.x: CH0
    CH0-General Introduction: `torch.compile` : TorchDynamo --> FX Graph in Torch IR --> AOTAutograd --> FX graph in Aten/Prims IR --> TorchInductor --> Triton code/OpenMP code...
- Triton: Vector Addition, Fused Softmax
    Triton 的编码思想基本和 CUDA 类似，Triton 最大的方便之处就在于把 CUDA 编程中最繁琐和困难的地址映射环节都封装在了 `tl.load` 中。

时间：2024.10.21-2024.10.28

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: SecA-SecC
    SecA-Related Work
    SecB-Algorithm Details: Memory-efficient forward/backward pass: 用 for 循环避免存储 $O(N^2)$ 的中间矩阵； FlashAttention backward pass: 反向算法反而相较于正向算法更加简洁，因为反向算法完全就是关于 tiled 矩阵乘，而不需要像正向算法频繁的 rescale
    SecC-Proofs
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec3
    Sec3-Markov Fields and the Harmmersly-Clifford Theorem: 证明思路：定义 ground state -> 定义 $Q$ 函数 -> 展开 $Q$ 函数 -> 证明 $Q$ 函数中的项 ($G$ 函数) 仅在相关的变量构成一个团时才非空
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.5
    CH4.5-Bayesian Networks and Markov Networks: 仅有弦图可以同时被两种结构表示且不丢失信息

### 11月
时间：2024.10.28-2024.11.5

\[文献阅读\]
- 2024-ICLR-FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning
    FlashAttention-2: 
    (1) 微调算法，减少非 matmul 运算: 移除了内层循环中对于 softmax weight 每次的一个 rescale 步骤，仅在内层循环结束后进行
    (2) 提高算法在 thread blocks 中的并行以提高 occupancy: 交换了内外层循环，使得外层迭代之间相互独立，故通过将 $\mathbf O$ 块分配给不同的 thread blocks 提高并行，本质就是并行化了外层循环
    (3) 在 warps 之间划分工作减少 shared memory 访问: 将 $\mathbf Q$ block 划分给 warps ，保持 $\mathbf {K, V}$ blocks 完整，其思想也是使得 warps 各自的负责的 $\mathbf O$ 块相互独立，故减少了最后累加归约所需要的 shared memory 读写
    在处理迭代式单 token decoding 时，FlashAttention-2 用 thread blocks 并行 laod KV cache 减少了 loading 时间

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.6.1
    CH4.6.1-Conditional Random Fields: CRF 建模条件概率分布，其图模型是部分有向图。其优势在于灵活性，CRF 允许我们用 Markov 网络的因子分解语义表示条件分布 (比条件贝叶斯网络需要显式指定 CPD 要灵活)，但相应的模型的解释性降低，学习到的参数没有特别明确的语义
- 面向计算机科学的组合数学: CH4-CH4.4.1
    令母函数中的系数为递推式的通项来将二者联系，进而将递推式写为关于母函数的方程，解方程得到母函数，就可以直接得到通项

\[文档阅读\]
- pytorch-tutorials-beginner: Learn the Basics
- pillow v11.0.0: Overview, Tutorial, Concepts
- Triton: Tutorials: Matrix Multiply
- argparse tutorial

时间：2024.11.5-2024.11.12

\[文献阅读\]
- 2023-SOSP-Efficient Memory Management for Large Language Model Serving with PagedAttention: Sec0-Sec4.5
    Sec0-Abstract
    Sec1-Introduction: 
        现存系统为 KV cache 预分配连续的显存，大小直接有请求的最大长度决定，该分配方式导致显存中存在大量内部和外部碎片，并且该分配方式不支持请求间共享 KV cache 的物理显存
        PagedAttention 以块为单位管理 KV cache，以块为粒度分配物理显存，进而减少了内部碎片并消除了外部碎片，同时以块为粒度支持了请求之间的 KV cache 共享
        PagedAttention 大幅提高了显存利用率，进而显著提高了请求的 batch size，进而提高了吞吐
    Sec2-Background:
        Prompt phase: 处理用户输入，生成 KV cache，使用矩阵-矩阵乘 (使用 masked self-attention 实现 causal attention)
        Auto regressive generation phase: 迭代式生成新 token，使用向量-矩阵乘
    Sec3-Memory Challenges in LLM Serving:
        LLM 服务受显存限制，尤其受 KV cache 所需要的空间限制
        朴素的连续显存分配策略导致 KV cache 存储空间的利用率很低

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH7, CH9.2-CH9.3
    CH7.1-Multivariate Gaussians: 
        高斯分布的两类参数化：标准、信息矩阵
        联合高斯的边际和条件密度也是高斯，同时条件高斯还满足线性高斯模型
        联合高斯中，协方差矩阵 $\Sigma$ 中的 0 项直接意味着统计上的独立性，信息矩阵 $J$ 中的 0 项意味着 Markov 条件独立性
    CH7.2-Gaussian Bayesian Networks:
        所有 CPDs 都是线性高斯模型的联合密度就是高斯
    CH7.3-Gaussian Markov Random Fields:
        任意高斯的信息表示可以直接和一个 pairwise Markov 网络关联，该网络带有二次的节点和边势能
        相反则不一定成立，要求 $J$ 正定，高斯分布才有定义，但存在一些充分条件，例如：对角主导、成对可规范化
    CH7.4-Summary:
        多元高斯可以被 Markov 和 Bayesian 网络表示
        高斯在表示上紧凑，计算上可解，对于复杂问题，也可以假设先验为高斯，或者将推理过程近似使得中间结果为高斯，以令计算过程可解
    CH9.2-'Variable Elimination: The Basic Ideas':
        变量消除的基本思想即动态规划：为了从 CPD $P (X\mid Y)$ 计算边际 $P (X)$，首先计算 $Y$ 的边际并存储，然后通过 $P (X) = \sum_y P (y) P (X\mid y)$ 计算，这比避免了为每个 $x\in Val (X)$ 重新计算 $Y$ 的边际
    CH9.3-Variable Elimination:
        将联合分布视作因子的乘积，为了计算一组变量上的边际，求和消去其他变量
        该过程可以总结为和-积变量消除算法，该算法的计算可以通过利用因子作用域有限的性质进行简化，即仅孤立出相关的因子进行求和
        要处理 evicdence，首先使用 evidence 简化所有因子 (留下和 evidence 相容的因子)，然后对简化的因子集合执行算法即可

时间：2024.11.11-2024.11.18

\[Paper\]
- 2023-SOSP-Efficient Memory Management for Large Language Model Serving with PagedAttention: Sec4.6-Sec10
    Sec4-Method: 
        Sec4.1-PagedAttention:
            PagedAttention 将序列的 KV cache 划分为块，每个块包含一定数量 tokens 对应的 KV cache
            KV 块在物理内存中可以不连续，在解码计算中，PagedAttention 按块读取所有需要的 KV 块执行计算
            KV cache 的块式管理使得我们在 vLLM 中可以使用灵活的内存管理方法
        Sec4.2-KV Cache Manager:
            vLLM 中，一个请求的 KV cache 会被划分为连续的逻辑块，逻辑块分别映射到不连续的物理块，KV 块从左到右被填充
            GPU worker 上的 block engine 分配连续的 DRAM，然后将连续的 DRAM 按照块划分
            每个 request 的逻辑块到物理块的映射信息由 block manager 维护在 block table 中
            块式的 KV cache 管理使得 vLLM 可以按照块的粒度动态增长用于存储 KV cache 的显存需求，而不是一次性预留大量可能不必要的位置
        Sec4.3-Decoding with PagedAttention and vLLM
            prefill 阶段中，vLLM 首先为 prompt 分配物理块，然后使用常规 self-attention 算法计算 prompt 的 KV cache，并生成第一个 token。在自回归解码阶段中，vLLM 使用 PagedAttention 计算每个 query token 的 KV cache，并逐个生成新 token。新的 KV cache 将被填充到块中的空 slot 中，若块填满，vLLM 创建新的逻辑块并分配相应的物理块
            全局上看，在每一次解码迭代中，vLLM 会选取一组候选序列进行批处理
            block size 更大使得 vLLM 可以并行处理更多位置的 KV cache，但内部碎片也会随之增大
        Sec4.4-Application to Other Decoding Scenarios
            Parallel sampling: 
            并行采样即 LLM 为单个请求生成多个序列，因此请求的 prompt 的 (大多数) KV cache 就可以共享，也就是这些序列的 prompt 部分的逻辑块会被映射到相同的物理块。共享数量由物理块的 reference count 计数。当新 token 需要被写入逻辑块时，如果其对应物理块 reference count 大于 1，vLLM 采用写时拷贝机制，将物理块内容拷贝到新的物理块，并相应减少原物理块的 reference count
            Beam search:
            beam search 在每个迭代从 $k\cdot |V|$ 个候选中保留 top-k 个序列
            beam candidates 共享初始 prompt 的 KV 块，如果它们来自相同的前缀，则会共享更多。共享模式会随着解码过程动态变化，一般的模式是在某个点分离，在之后的某个点收敛
            在之前的 LLM 服务系统中，对于分离的 beam candidates 的收敛行为需要拷贝大量的 KV cache，vLLM 中则可以直接共享对应的物理块，拷贝发生在分离块的写时拷贝过程，开销因此也限制在了一个块的大小
            Shared prefix:
            共享 system prompt 的 KV 块，其对应的物理块还可以预先缓存
            Mixed decoding methods:
            vLLM 支持同时用多个解码算法处理请求，因为 vLLM 使用逻辑块到物理块的映射层隐藏了具体的内存共享模式，故执行 kernel 仅需要接受物理块 ID 列表，不需要显式管理序列的内存共享模式
        Sec4.5-Scheduling and preemption:
            vLLM 采用先来先服务，对于被抢占序列则实行全部驱逐的策略
            恢复方法包括交换和重计算，注意重计算的延迟一般显著低于解码时的延迟，因为此时所有需要的 tokens 都已经知道
        Sec4.6-Distributed Execution:
            vLLM 使用 Megatron-LM 的并行策略，attention 算子在 attention head 维度划分
            对于每个 request，每个模型碎片处理的 token 序列是相同的，因此 KV cache 的物理块 ID 在 GPU worker 之间共享，但每个 GPU worker 上的物理块实际仅存储对应 attention head 那部分的 KV cache
            在执行的每一步中，调度器将每个序列 tokens 的 ID 和 block table 广播给 GPU worker，GPU worker 不需要在显存管理上同步，因为所有的显存管理信息在每次解码迭代的开始就被广播给了它们
    Sec5-Implementation:
        PagedAttention 使用三个 kernel 优化显存访问模式，包括
        fused reshape and block write kernel，将每个 transformer 层中新计算的 KV cache 划分为块，reshape 后根据 block table 的指定位置将其存储
        fused block read and attention kernel，根据 block table 读取 KV cache，计算 attention，每个 KV block 的读取由一个 warp 完成，保证 global memory 访问是合并的
        fused  block copy kernel，批处理多个块的写时拷贝操作
        PagedAttention 使用 `fork/append/free` 实现解码算法，`fork`  从现存序列创建新序列，`append` 将新 token 附加到现有序列， `free` 释放完成的序列
    Sec6-Evaluation:
        请求到达时间由不同请求率下的 Possion 分布生成
        度量系统在不同请求率下的规范化延迟，等于 mean(每个请求的端到端延迟除以其 token 数量)
        随着请求率上升，规范化延迟开始逐渐上升，超过处理能力后猛增，vLLM 可以显著提高该门槛，因为它节约了大量 KV cache 需要占用的显存，因此系统可以维持在较高的请求率下，故可以批处理更多请求
        在 compute-bound 的场景下 (序列较短，空间较充足)，vLLM 优势较小
        不同解码场景下的 block 共享也很高效，且随着 parallel samples 的数量或beam width 或 length of prefix 增长，效率也随之增长
    Sec7-Abalation Study:
        PagedAttention kernel 包含了访问 block table，执行额外分支和处理变长序列的额外开销，故 kernel latency 更高，memory-saving 仍然使得端到端表现更好
        block size 较小会导致难以高效并行读取和处理 KV cache，较大则内部碎片更大且共享机会更少，Block size = 16 是一个较好的 tradeoff
        关于恢复方法，重计算的开销和 block size 无关，因为它不涉及数据传输，交换方法则在 block size 更大时更高效
    Sec8-Discussion:
        块式的内存管理机制对于 KV cache 管理是高效的，原因在于 LLM 不能提前知道输出序列的长度，因此需要高效的动态内存分配，块式管理可以最小化这种情况下的内存碎片
    Sec9-Related Work:
        Orca 中的迭代级别调度和 vLLM 中的 PagedAttention 是互补的技术，Orca 通过调度和重叠请求提高请求处理的并行度，vLLM 提高了显存利用率，以容纳更大的工作集
        vLLM 通过减少内存碎片和进行内存共享提高了请求处理的并行度
    Sec10-Conclusion:
        PagedAttention 以块的粒度管理 KV cache，减少了内存碎片并进行了内存共享，vLLM 基于 PagedAttention, PagedAttention 算法展示了像虚拟内存和写时拷贝这样成熟的技术是如何可以被用于高效管理 LLM 服务中的 KVcache 和处理多种解码算法的

\[Book\]
- 面向计算机科学的组合数学: CH4.4.1-CH4.5.1
    直接根据递推式写下特征多项式 -> 解特征方程得到根 -> 根据根写下带有待定系数的通项 -> 根据数列初始值解出通项 -> 得到通项公式

\[Doc\]
- CUDA C++ Programming Guide v12.6: CH2
- docker/get-started: What is Docker, Docker Concepts