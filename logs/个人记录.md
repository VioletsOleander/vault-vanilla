# 2024 年
## 九月
### 第一周
时间：2024.9.2-2024.9.9

\[文献阅读\]
- 2023-NeurIPS-Are Emergent Abilities of Large Language Models a Mirage?
    研究者对于度量的选取造就了 LLM 具有涌现能力的“海市蜃楼” 

\[书籍阅读\]
- 2015-Mastering CMake: CH2-CH15、CH17
    CH2-Getting Started on Your Computer：CMake 构建流程纵览
    CH3-Writing CMakeLists Files：CMake 语言纵览
    CH4-CMake Cache：CMake 缓存机制介绍：`CMakeCache.txt`
    CH5-Key Concepts：CMake 概念介绍：源文件、目标文件、属性
    CH6-Policies：CMake 策略机制：为了兼容性
    CH7-Modules：CMake 模块：CMake 提供的 utility
    CH8-Installing Files：`Install()` 命令
    CH9-System Inspections：借助宏编写跨平台软件；`try_run/compile()` 命令
    CH10-Finding Packages：借助 CMake 分发的软件依赖包
    CH11-Custom Commands：为自定义目标添加自定义构建规则
    CH12-Packing with CPack：借助 CPack 调用本地打包工具
    CH13-Testing with CMake and CTest：`add_test()`
    CH14-CMake Tutorial：纵览：简单构建、添加库、添加属性、通过系统审查添加宏、添加测试、简单安装、添加共享库、生成器表达式（实际构建时才确定值的变量）、导出 CMake 软件包
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH1
    CH1-Intruduction：概率图模型：构建图模型表示概率系统中的独立性和依赖性关系，依据图模型进行概率推导
- 2023-A Survey of Large Language Models: CH0(Abstract)-CH5
    CH0-Abstract：全文结构分为：LLM 的预训练、微调、使用、评估四部分
    CH1-Introduction：语言模型的发展：SLM-NLM-PLM-LLM
    CH2-Overview：语言模型的 scaling 和涌现
    CH3-Resources of LLMs：公开可用的模型权重和训练数据集
    CH4-Pre-Training：数据收集、清洗；模型架构；训练技巧
    CH5-Adaptation of LLMs：指令微调；对齐微调；参数高效微调；存储高效微调

### 第二周
时间：2024.9.9-2024.9.16

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH1
    CH1-What is Combinactorics： 组合数学关于：离散结构的存在、计数、分析和优化
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH2
    CH2-Foundation： 条件独立、条件概率密度函数、MAP 查询、图论
- 2023-A Survey of Large Language Models: CH6
    CH6-Utilization：提示词技巧：上下文学习（即（输入-输出）对）、思维链（即（输入-推理步骤-输出）三元组）、规划

### 第三周
时间：2024.9.16-2024.9.23

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH2
    CH2-Permutations and Combinations: 集合的排列/组合（组合数=排列数+除法原理），多重集合的排列/组合（多重集排列数=集合排列数+除法原理；多重集组合数=线性等式的解集大小），古典概型
- 2004-Convex Optimization: CH2-CH2.5
    CH2-CH2.5-Convex Sets: 凸组合、仿射组合；典型的凸集；保凸运算；凸集的支撑/分离超平面
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH3-CH3.3
    Cjj3-CH3.3-The Bayesian Network Representation: 贝叶斯网络：以图的语义表示联合分布中成立的条件独立性；根据网络结构，将联合分布分解为多个条件概率分布的乘积

### 第四周
时间：2024.9.23-2024.9.30

\[文献阅读\]
- A Survey of Large Language Models: CH7-CH8
    CH7-Capacity and Evaluation: LLM 能力：1. 基本能力：语言生成（包括代码）、知识利用（例如知识密集的 QA）、复杂分析（例如数学推理）；2. 高级能力：对齐、和外部环境交互（例如为具身智能体生成动作规划）、工具利用（例如根据任务调用合适 API）；对一些 benchmark 的介绍

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH5
    CH5-Local Probabilistic Models: 紧凑的条件概率分布表示：利用针对上下文的独立性；假设原因之间影响独立，基于此假设的模型有：噪声或、BN2O（多观测的噪声或）、广义线性模型（“分数”线性于所有原因变量）、条件线性高斯（实际上将联合分布建模为了混合高斯分布）

## 十月
### 第一周
时间：2024.9.30-2024.10.7

\[文献阅读\]
- 2023-A Survey of Large Language Models: CH8-CH9
    CH8-Applicatoin: LLM 在多个任务中的应用
    CH9-Conclusion and future directions
- 2010-Importance Sampling A Review
    重要性采样主要的目标就是减少 Monte Carlo 采样估计的方差
    适应性参数化重要性采样：$q (x)$ 定义为多元正态或学生分布，优化和变异系数相关的度量以得到较优的分布参数
    序列重要性采样：链式分解 $p (x)$，链式构造 $q (x)$
    退火重要性采样：顺序地近似 $p (x)$，和 diffusion 很相似

\[书籍阅读\]
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH6-CH6.2
    CH6-Template-based Representations: 时序模型；Markov 假设+ 2-TBN = DBN（动态贝叶斯网络）；DNB 通常建模为状态-观测模型（状态和观测分离考虑，观测不会影响状态），两个例子：隐 Markov 模型，线性动态网络（所有的依赖都是线性 Gaussian）
- 2024-面向计算机科学的组合数学: CH1.7, CH2
    CH1.7-生成全排列: 中介数和排列之间的一一对应关系

### 第二周
时间：2024.10.7-2024.10.14

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: CH0-CH3.1
    CH0-CH3.1: Abstract, Background, Algorithm 1; 算法1本质上就是 attention 计算的 tiled 实现，其中主要是 softmax 因子反复的重缩放让算法1看起来不那么直观，但是缩放计算对于数值稳定性还是关键的。算法1中，对于每个 query ，它的 attention 结果是随着外层循环逐渐累加的，在每一次外层循环，其已经累积的部分 attention 结果中的 value 权重都会动态调整/更新，同时加上新的部分 attention 结果

\[书籍阅读\]
- A Tour of C++: CH5
- 面向计算机科学的组合数学: CH2.1-CH2.3
    CH2-鸽巢原理: 鸽巢原理仅解决存在性问题
- Probabilistic Graphical Models-Principles and Techniques: CH4-CH4.3.1
    CH4-CH4.3.1: Markov 网络的参数化：思路来源于统计物理学，是很直观的“统计”，使用因子函数衡量两个变量/粒子的交互/亲和性，使用规范化的因子乘积表示联合分布（Gibbs 分布），用以描述特定“配置”出现的概率。Markov 网络中的分离准则是可靠且若完备的（可靠：网络中，某独立性存在 --> 任意分解于网络的分布中，该独立性存在；弱完备：网络中，某独立性不存在 --> 某个分解于网络的分布中，该独立性不存在） 

\[文档阅读\]
- ultralytics v8.3.6 : Quickstart, Usage (Python usage, Callbacks, Configuration, Simple Utilities, Advanced Customization)
    对 YOLO 的 Python API 的简要介绍

### 第三周
时间：2024.10.14-2024.10.21

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: Sec3.1-Sec5
    Sec3.1-IO Analysis: FlashAttention 的 IO 复杂度为 $\Theta(N^2d^2M^{-1})$ ，标准 Attention 算法的则为 $\Theta (Nd + N^2)$，主要的差别就在于其中的 $M$ 和 $N^2$。标准 Attention 算法完全不使用 SRAM，所有访存都是 global memory 访问，其中对于 “权重矩阵” $P\in \mathbb R^{N\times N}$ 贡献了大量的 IO 复杂度 ($N^2$)。FlashAttention 利用了 SRAM，并且完全不存储 $P$ 到 DRAM，而是在算法整个过程保持一块 $P$ 在片上，因此减少了 IO 复杂度。
    Sec3.2-Block sparse Flash-Attention: 块稀疏拓展的差异就在于限制了“注意”的范围，故计算时可以跳过被 mask 的项，进而减少了访存和计算。
    Sec4-Experiments: FlashAttention 训练更快； FlashAttention 更内存高效（线性于序列长度），故允许更长的训练上下文。其原因在于 FlashAttention 不会一次直接计算整个 "权重矩阵" $P\in \mathbb R^{N\times N}$ ，而是在循环中逐行计算，实际上是以时间换空间， FLOP 提高了，但额外内存使用限制在了 $O (N)$ 而不是 $O (N^2)$ 。FLOP 提高带来的额外计算时间则被更少的 DRAM 访问抵消。
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec0-Sec2
    Sec0-Summary: 该文提出了 HC 定理的一种证明，进而强调了建模空间交互时，条件概率模型实际是优于联合概率模型的。
    Sec1-Sec2: 对于正分布，条件概率可以用于推导整个系统的联合概率，这依赖于 HC 定理。
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.3.1-CH4.4.2
    CH4.3.1-CH4.4.2: Markov 网络编码了三类独立性：成对独立性、局部独立性（Markov 毯）、全局独立性（d-seperation）。对于正分布，三者等价，对于非正分布（存在确定关系的分布）三者不等价，这是因为 Markov 网络的语义无法表示确定性关系。根据 HC 定理，正分布 $P$ 分解于 Markov network $\mathcal H$ 等价于 $P$ 满足 $\mathcal H$ 编码的三类独立性。
- 面向计算机科学的组合数学: CH3-CH3.3
    母函数：使用幂级数表示数列（数列由幂级数的系数构造）
\[文档阅读\]
- Pytorch 2.x: CH0
    CH0-General Introduction: `torch.compile` : TorchDynamo --> FX Graph in Torch IR --> AOTAutograd --> FX graph in Aten/Prims IR --> TorchInductor --> Triton code/OpenMP code...
- Triton: Vector Addition, Fused Softmax
    Triton 的编码思想基本和 CUDA 类似，Triton 最大的方便之处就在于把 CUDA 编程中最繁琐和困难的地址映射环节都封装在了 `tl.load` 中。

### 第四周
时间：2024.10.21-2024.10.28

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: SecA-SecC
    SecA-Related Work
    SecB-Algorithm Details: Memory-efficient forward/backward pass: 用 for 循环避免存储 $O(N^2)$ 的中间矩阵； FlashAttention backward pass: 反向算法反而相较于正向算法更加简洁，因为反向算法完全就是关于 tiled 矩阵乘，而不需要像正向算法频繁的 rescale
    SecC-Proofs
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec3
    Sec3-Markov Fields and the Harmmersly-Clifford Theorem: 证明思路：定义 ground state -> 定义 $Q$ 函数 -> 展开 $Q$ 函数 -> 证明 $Q$ 函数中的项 ($G$ 函数) 仅在相关的变量构成一个团时才非空
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.5
    CH4.5-Bayesian Networks and Markov Networks: 仅有弦图可以同时被两种结构表示且不丢失信息

## 十一月
### 第一周
时间：2024.10.28-2024.11.4

\[文献阅读\]
- 2024-ICLR-FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning
    FlashAttention-2: 
    (1) 微调算法，减少非 matmul 运算: 移除了内层循环中对于 softmax weight 每次的一个 rescale 步骤，仅在内层循环结束后进行
    (2) 提高算法在 thread blocks 中的并行以提高 occupancy: 交换了内外层循环，使得外层迭代之间相互独立，故通过将 $\mathbf O$ 块分配给不同的 thread blocks 提高并行，本质就是并行化了外层循环
    (3) 在 warps 之间划分工作减少 shared memory 访问: 将 $\mathbf Q$ block 划分给 warps ，保持 $\mathbf {K, V}$ blocks 完整，其思想也是使得 warps 各自的负责的 $\mathbf O$ 块相互独立，故减少了最后累加归约所需要的 shared memory 读写
    在处理迭代式单 token decoding 时，FlashAttention-2 用 thread blocks 并行 laod KV cache 减少了 loading 时间

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.6.1
    CH4.6.1-Conditional Random Fields: CRF 建模条件概率分布，其图模型是部分有向图。其优势在于灵活性，CRF 允许我们用 Markov 网络的因子分解语义表示条件分布 (比条件贝叶斯网络需要显式指定 CPD 要灵活)，但相应的模型的解释性降低，学习到的参数没有特别明确的语义
- 面向计算机科学的组合数学: CH4-CH4.4.1
    令母函数中的系数为递推式的通项来将二者联系，进而将递推式写为关于母函数的方程，解方程得到母函数，就可以直接得到通项

\[文档阅读\]
- pytorch-tutorials-beginner: Learn the Basics
- pillow v11.0.0: Overview, Tutorial, Concepts
- Triton: Tutorials: Matrix Multiply
- argparse tutorial

### 第二周
时间：2024.11.4-2024.11.11

\[文献阅读\]
- 2023-SOSP-Efficient Memory Management for Large Language Model Serving with PagedAttention: Sec0-Sec4.5
    Sec0-Abstract
    Sec1-Introduction: 
        现存系统为 KV cache 预分配连续的显存，大小直接有请求的最大长度决定，该分配方式导致显存中存在大量内部和外部碎片，并且该分配方式不支持请求间共享 KV cache 的物理显存
        PagedAttention 以块为单位管理 KV cache，以块为粒度分配物理显存，进而减少了内部碎片并消除了外部碎片，同时以块为粒度支持了请求之间的 KV cache 共享
        PagedAttention 大幅提高了显存利用率，进而显著提高了请求的 batch size，进而提高了吞吐
    Sec2-Background:
        Prompt phase: 处理用户输入，生成 KV cache，使用矩阵-矩阵乘 (使用 masked self-attention 实现 causal attention)
        Auto regressive generation phase: 迭代式生成新 token，使用向量-矩阵乘
    Sec3-Memory Challenges in LLM Serving:
        LLM 服务受显存限制，尤其受 KV cache 所需要的空间限制
        朴素的连续显存分配策略导致 KV cache 存储空间的利用率很低

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH7, CH9.2-CH9.3
    CH7.1-Multivariate Gaussians: 
        高斯分布的两类参数化：标准、信息矩阵
        联合高斯的边际和条件密度也是高斯，同时条件高斯还满足线性高斯模型
        联合高斯中，协方差矩阵 $\Sigma$ 中的 0 项直接意味着统计上的独立性，信息矩阵 $J$ 中的 0 项意味着 Markov 条件独立性
    CH7.2-Gaussian Bayesian Networks:
        所有 CPDs 都是线性高斯模型的联合密度就是高斯
    CH7.3-Gaussian Markov Random Fields:
        任意高斯的信息表示可以直接和一个 pairwise Markov 网络关联，该网络带有二次的节点和边势能
        相反则不一定成立，要求 $J$ 正定，高斯分布才有定义，但存在一些充分条件，例如：对角主导、成对可规范化
    CH7.4-Summary:
        多元高斯可以被 Markov 和 Bayesian 网络表示
        高斯在表示上紧凑，计算上可解，对于复杂问题，也可以假设先验为高斯，或者将推理过程近似使得中间结果为高斯，以令计算过程可解
    CH9.2-'Variable Elimination: The Basic Ideas':
        变量消除的基本思想即动态规划：为了从 CPD $P (X\mid Y)$ 计算边际 $P (X)$，首先计算 $Y$ 的边际并存储，然后通过 $P (X) = \sum_y P (y) P (X\mid y)$ 计算，这比避免了为每个 $x\in Val (X)$ 重新计算 $Y$ 的边际
    CH9.3-Variable Elimination:
        将联合分布视作因子的乘积，为了计算一组变量上的边际，求和消去其他变量
        该过程可以总结为和-积变量消除算法，该算法的计算可以通过利用因子作用域有限的性质进行简化，即仅孤立出相关的因子进行求和
        要处理 evicdence，首先使用 evidence 简化所有因子 (留下和 evidence 相容的因子)，然后对简化的因子集合执行算法即可

### 第三周
时间：2024.11.11-2024.11.18

\[文献阅读\]
- 2023-SOSP-Efficient Memory Management for Large Language Model Serving with PagedAttention: Sec4.6-Sec10
    Sec4-Method: 
        Sec4.1-PagedAttention:
            PagedAttention 将序列的 KV cache 划分为块，每个块包含一定数量 tokens 对应的 KV cache
            KV 块在物理内存中可以不连续，在解码计算中，PagedAttention 按块读取所有需要的 KV 块执行计算
            KV cache 的块式管理使得我们在 vLLM 中可以使用灵活的内存管理方法
        Sec4.2-KV Cache Manager:
            vLLM 中，一个请求的 KV cache 会被划分为连续的逻辑块，逻辑块分别映射到不连续的物理块，KV 块从左到右被填充
            GPU worker 上的 block engine 分配连续的 DRAM，然后将连续的 DRAM 按照块划分
            每个 request 的逻辑块到物理块的映射信息由 block manager 维护在 block table 中
            块式的 KV cache 管理使得 vLLM 可以按照块的粒度动态增长用于存储 KV cache 的显存需求，而不是一次性预留大量可能不必要的位置
        Sec4.3-Decoding with PagedAttention and vLLM
            prefill 阶段中，vLLM 首先为 prompt 分配物理块，然后使用常规 self-attention 算法计算 prompt 的 KV cache，并生成第一个 token。在自回归解码阶段中，vLLM 使用 PagedAttention 计算每个 query token 的 KV cache，并逐个生成新 token。新的 KV cache 将被填充到块中的空 slot 中，若块填满，vLLM 创建新的逻辑块并分配相应的物理块
            全局上看，在每一次解码迭代中，vLLM 会选取一组候选序列进行批处理
            block size 更大使得 vLLM 可以并行处理更多位置的 KV cache，但内部碎片也会随之增大
        Sec4.4-Application to Other Decoding Scenarios
            Parallel sampling: 
            并行采样即 LLM 为单个请求生成多个序列，因此请求的 prompt 的 (大多数) KV cache 就可以共享，也就是这些序列的 prompt 部分的逻辑块会被映射到相同的物理块。共享数量由物理块的 reference count 计数。当新 token 需要被写入逻辑块时，如果其对应物理块 reference count 大于 1，vLLM 采用写时拷贝机制，将物理块内容拷贝到新的物理块，并相应减少原物理块的 reference count
            Beam search:
            beam search 在每个迭代从 $k\cdot |V|$ 个候选中保留 top-k 个序列
            beam candidates 共享初始 prompt 的 KV 块，如果它们来自相同的前缀，则会共享更多。共享模式会随着解码过程动态变化，一般的模式是在某个点分离，在之后的某个点收敛
            在之前的 LLM 服务系统中，对于分离的 beam candidates 的收敛行为需要拷贝大量的 KV cache，vLLM 中则可以直接共享对应的物理块，拷贝发生在分离块的写时拷贝过程，开销因此也限制在了一个块的大小
            Shared prefix:
            共享 system prompt 的 KV 块，其对应的物理块还可以预先缓存
            Mixed decoding methods:
            vLLM 支持同时用多个解码算法处理请求，因为 vLLM 使用逻辑块到物理块的映射层隐藏了具体的内存共享模式，故执行 kernel 仅需要接受物理块 ID 列表，不需要显式管理序列的内存共享模式
        Sec4.5-Scheduling and preemption:
            vLLM 采用先来先服务，对于被抢占序列则实行全部驱逐的策略
            恢复方法包括交换和重计算，注意重计算的延迟一般显著低于解码时的延迟，因为此时所有需要的 tokens 都已经知道
        Sec4.6-Distributed Execution:
            vLLM 使用 Megatron-LM 的并行策略，attention 算子在 attention head 维度划分
            对于每个 request，每个模型碎片处理的 token 序列是相同的，因此 KV cache 的物理块 ID 在 GPU worker 之间共享，但每个 GPU worker 上的物理块实际仅存储对应 attention head 那部分的 KV cache
            在执行的每一步中，调度器将每个序列 tokens 的 ID 和 block table 广播给 GPU worker，GPU worker 不需要在显存管理上同步，因为所有的显存管理信息在每次解码迭代的开始就被广播给了它们
    Sec5-Implementation:
        PagedAttention 使用三个 kernel 优化显存访问模式，包括
        fused reshape and block write kernel，将每个 transformer 层中新计算的 KV cache 划分为块，reshape 后根据 block table 的指定位置将其存储
        fused block read and attention kernel，根据 block table 读取 KV cache，计算 attention，每个 KV block 的读取由一个 warp 完成，保证 global memory 访问是合并的
        fused  block copy kernel，批处理多个块的写时拷贝操作
        PagedAttention 使用 `fork/append/free` 实现解码算法，`fork`  从现存序列创建新序列，`append` 将新 token 附加到现有序列， `free` 释放完成的序列
    Sec6-Evaluation:
        请求到达时间由不同请求率下的 Possion 分布生成
        度量系统在不同请求率下的规范化延迟，等于 mean(每个请求的端到端延迟除以其 token 数量)
        随着请求率上升，规范化延迟开始逐渐上升，超过处理能力后猛增，vLLM 可以显著提高该门槛，因为它节约了大量 KV cache 需要占用的显存，因此系统可以维持在较高的请求率下，故可以批处理更多请求
        在 compute-bound 的场景下 (序列较短，空间较充足)，vLLM 优势较小
        不同解码场景下的 block 共享也很高效，且随着 parallel samples 的数量或beam width 或 length of prefix 增长，效率也随之增长
    Sec7-Abalation Study:
        PagedAttention kernel 包含了访问 block table，执行额外分支和处理变长序列的额外开销，故 kernel latency 更高，memory-saving 仍然使得端到端表现更好
        block size 较小会导致难以高效并行读取和处理 KV cache，较大则内部碎片更大且共享机会更少，Block size = 16 是一个较好的 tradeoff
        关于恢复方法，重计算的开销和 block size 无关，因为它不涉及数据传输，交换方法则在 block size 更大时更高效
    Sec8-Discussion:
        块式的内存管理机制对于 KV cache 管理是高效的，原因在于 LLM 不能提前知道输出序列的长度，因此需要高效的动态内存分配，块式管理可以最小化这种情况下的内存碎片
    Sec9-Related Work:
        Orca 中的迭代级别调度和 vLLM 中的 PagedAttention 是互补的技术，Orca 通过调度和重叠请求提高请求处理的并行度，vLLM 提高了显存利用率，以容纳更大的工作集
        vLLM 通过减少内存碎片和进行内存共享提高了请求处理的并行度
    Sec10-Conclusion:
        PagedAttention 以块的粒度管理 KV cache，减少了内存碎片并进行了内存共享，vLLM 基于 PagedAttention, PagedAttention 算法展示了像虚拟内存和写时拷贝这样成熟的技术是如何可以被用于高效管理 LLM 服务中的 KVcache 和处理多种解码算法的

\[书籍阅读\]
- 面向计算机科学的组合数学: CH4.4.1-CH4.5.1
    直接根据递推式写下特征多项式 -> 解特征方程得到根 -> 根据根写下带有待定系数的通项 -> 根据数列初始值解出通项 -> 得到通项公式

\[文档阅读\]
- CUDA C++ Programming Guide v12.6: CH2
- docker/get-started: What is Docker, Docker Concepts

### 第四周
时间：2024.11.18-2024.11.25

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH10.1-CH10.3, CH11.1-CH11.3.4
    CH10-Exect Inference: Clique Trees
        CH10.1-Variable Elimination and Clique Trees
            因子 $\psi_i$ 接受因子 $\psi_j$ 生成的消息 $\tau_j$，向其他因子发送消息 $\tau_i$
            簇图中，每个节点/簇和一组变量关联，两个簇在关联的变量存在交集时相连
            变量消除算法的执行过程定义了一个簇图：执行过程中用到的每个因子 $\psi_i$ 对应一个簇 $\pmb C_i$，如果执行过程中因子 $\psi_i$ 向因子 $\psi_j$ 发送了消息 $\tau_i$，则在簇图中连接 $\pmb C_i, \pmb C_j$
            变量消除算法定义的簇图实际是一颗树 (每个因子在算法中仅时使用一次，故在图中仅会有一个父节点)，该树还满足运行相交性质，故是一颗团树
            团树的运行相交性质暗示了独立性 (Theorem 10.2)
        CH10.2-Message Passing: Sum Product
            单个团树可以用于多次的变量消除执行，故考虑在团树中缓存部分计算结果 (以团为单位存储团上的边际分布)，可以使得多次的执行更加高效
            团树本身定义了可以在其中的因子执行的计算 (求和消去哪些变量)，并定义了这些计算的偏序结构，因此团树指导了变量消除的执行
            团树消息传递算法中，每个团的初始势能为其所有相关因子之积，每个团计算消息时，首先乘上所有传入消息，然后求和消去其作用域中除分离集以外的变量，该过程逐渐向上直到根团。根团最后得到其作用域上未规范化的边际 (等于未规范化的联合分布求和消去所有其他变量)，称该边际为信念 (Colloary 10.1)
            直观上， $\pmb C_i , \pmb C_j$ 之间的消息是 $\mathcal F_{\prec i(\rightarrow j)}$ 中所有因子的乘积在 $\pmb C_i$ 和 $\pmb C_j$ 之间边际化的结果 (Theorem 10.3)
            在团树消息传递算法的多次执行中，两个团之间发送的同向的消息完全一样，故本质上团树中的每条边仅关联两条消息 (一个方向一条)
            和积信念传播算法利用了该性质，算法包括 upward pass 和 downward pass，两次 pass 之后，每条边都得到自己相关的两条消息，因此每个团的信念可以被直接计算得到，故该算法在计算全部的团信念时很高效 (Colloary 10.2)，同时，该算法也校准了树种的全部因子
            和边关联的 sepset 上的信念就是和边关联的两条消息之积
            团树上的未规范化联合分布等于全部团的信念之积除去全部分离集的信念之积 (Proposition 10.3)，故团和分离集信念提供了一种重参数化，该性质被称为团树不变性
        CH10.3-Message Passing: Belief Update
            和-积-除算法中，消息传递过程是以信念更新的形式实现的，执行中每条边维护上次传递的消息，防止消息回传，即新消息要经过该边传递时，需要除去边上的上一条消息，避免旧消息的信息用于更新发送该消息的团
            该算法收敛时同样得到校准的树，因为收敛情况下，消息更新不再有效，即 $\sigma_{i\rightarrow j} = \mu_{i, j} = \sigma_{j\rightarrow i}$ 对于所有 $i, j$ 成立，意味着相邻团在 sepset 上的边际达成一致
            可以证明和积消息传递算法和信念更新算法 (和-积-除算法) 等价
            信念更新算法的执行过程中，团树不变性在每一次消息传递后也仍然保持 (Corollary 10.3)
            增量更新：将分布乘上一个新因子。在团树中的实现是将新因子乘入某个相关的团，然后执行一次 pass 以更新树中其他相关的团
            超过单个团范围的 query：在包含 query 的子树中构造包含 query 的边际，然后消元求得 query
            多 query：用动态规划思想计算每个 clique pair 的边际
    CH11-Inference as Optimization
        CH11.1-Introduction:
            近似方法的近似性来自于对目标分布 $P_\Phi$ 构建近似分布，近似分布形式更简单，一般会采用局部分解结构
            推理任务需要在近似分布上进行，故目标任务被转变为在一类近似分布 $\mathcal Q$ 上优化目标函数，找到最优近似分布 $Q$，因此目标任务属于约束优化问题
            近似方法主要分为三类：
            1. 在非团树结构中使用团树消息传递算法，例如环路信念传播算法。这类方法可以理解为优化能量泛函的近似形式
            2. 在团树结构中使用团树消息传递算法，其中消息是近似消息，该类算法称为期望传播算法，它在松弛的约束下最大化准确形式的能量泛函
            3. 推广平均场方法，使用准确形式的能量泛函
            准确推理问题也可以被重构为搜索问题，搜索接近 $P_\Phi$ 的校准分布
            近似分布和原分布之间的 KL 散度 $D(Q|| P_\Phi)$ 可以被重写为关于能量泛函的形式，最小化 KL 散度等价于最大化能量泛函 $F[\tilde P_\Phi, Q]$ (Theorem 11.2).
            对于任意的 $Q$，能量泛函都是划分函数 $Z$ 的对数的下界，即 $\ln Z \ge F[\tilde P_\Phi, Q]$，等号成立当且仅当 $D(Q||P_\Phi) = 0$
            本章讨论的近似推理方法都可以被视作优化能量泛函的策略，这些方法都属于变分方法
        CH11.2-Exact Inference as Optimization
            给定和一组信念 $Q$ 相关的簇树，我们可以将能量泛函 $F[\tilde P_\Phi, Q]$ 分解为关于分离集信念和簇信念的形式，之后便可以在信念集合的空间中定义约束优化问题 'CTree-Optimize' ，其目标是最大化分解形式的能量泛函，其约束保证了边际一致性 (即信念是校准的)
            使用拉格朗日乘子法可以最优解 $Q^*$ 中的信念应该服从的形式/方程(Theorem 11.3)
            进一步求解的方法可以为迭代求解，即迭代式将方程的 RHS 赋值给 LHS 直到收敛，在特定情况下 (例如簇树实际上是团树)，该方法的收敛性质有保证，因此在团树中，该方法等价于信念更新/和积消息传递算法
            簇图的运行相交性质的定义比簇树中的定义要松弛，但仍可以避免‘循环论证‘，但图中的回路仍然会导致一定程度的循环推理
            簇图中，边上的集合不一定是分离集，而是分离集的子集，或者说边上的集合不再保证是节点的作用域的交集
            虽然在簇图中进行优化最后得到的信念不保证是 $P_\Phi$ 的边际，但簇图仍维持和簇树一样的不变性质，因此簇图也可以视作对未规范化的原分布 $P_\Phi$ 的重参数化
            簇图中的信念实际上是分布 $P_{\mathcal T}$ 的边际，其中 $\mathcal T$ 是图中的一个子树，该性质称为树一致性
            关于构造簇图，对于成对 Markov 网络，我们可以为每个势能引入一个簇，在作用域相交的簇之间添加边，环状信念传播最初就基于该构造
            对于更复杂的网络，可以使用 Bethe 簇图构造方法
- 面向计算机科学的组合数学: CH5.1-CH5.2
\[文档阅读\]
- python/pep/PEP 8-Style Guide for Python Code

## 十二月
### 第一周
时间：2024.11.25-2024.12.2

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH11.5.1, CH12.1-CH12.3
    CH11-Inference as Optimization
        CH11.5-Structured Variational Inference
            CH11.5.1-The Mean Field Approximation
                平均场近似假设所有变量相互独立，故 $Q(\mathcal X)$ 可以被完全分解
                能量泛函的优化形式为迭代式优化 (坐标上升)；在每次迭代，我们仅优化 $Q_i(X_i)$ ，其他变量的边际固定；迭代式坐标上升算法保证收敛，因为能量泛函是有界的并且在坐标上升过程保证是不减的
                对 $Q_i(X_i)$ 的最优解的计算仅涉及到包含了变量 $X_i$ 的势能
    CH12-Partical-Based Approximation Inference
        本章讨论蒙特卡洛方法，即如何从目标分布或者其近似分布采样，以及如何根据这些样本为所要的期望构建估计器
        CH12.1-Forward Sampling
            前向采样利用了贝叶斯网络的分解定理，它根据 BN 的偏序对变量进行采样，因此，在父变量的采样值确定的情况下，每个变量的采样过程仅和其 CPD 有关
            要从后验中采样，一个简单的方法是拒绝采样，其拒绝采样得到的样本中和证据不一致的样本
            拒绝采样的问题在于如果证据本身出现的概率太低，许多样本会被拒绝，故采样的效率会过低
        CH12.2-Likelihood Weighting and Importance Sampling
            为了解决拒绝采样的低效问题，似然加权算法直接在采样过程中将观测变量的值设定为观察值，因此不需要再进行拒绝
            但是，直接将证据变量的值设定为观察值时，证据变量和其他变量的采样值之间的相关性会丢失；为了弥补这一点，似然采样算法为每个样本赋予一个权重，样本的权重即该样本中其他变量的采样值下证据出现的概率
            重要性采样从另一个提案分布中采样，并且相应地为样本加权，以保证我们计算的期望是想要的期望
            重要性采样在 $Q(\pmb X) \propto |f(\pmb X)|P(\pmb X)$ 时达到最低的方差
            规范化的重要性采样假定我们只能访问未规范化的分布 $\tilde P$，此时我们仍可以通过同时估计规范化参数 $Z$，来构建对某个相对于 $P$ 的期望的估计器
            规范化的重要性采样不是无偏的，其偏差和方差随着样本数量 $M$ 的倒数 $\frac 1 M$ 下降，也就是 $M$ 越大偏差和方差越低
            实践中，规范化重要性采样估计器的方差一般比未规范化的重要性采样估计器更低 (没有理论保证)，方差的降低往往比偏差更重要，因此即便在 $P$ 已知的情况下，也常常使用规范化的重要性采样估计器
            一组特定样本的有效样本大小依据方差来定义，即 $M_{\text{eff}}$ 个来自于 $P$ 的样本的方差等于来自于 $Q$ 的 $M$ 个样本
            在 BN 中，当重要性采样的提案分布 $Q$ 是由残缺化网络定义的时，重要性采样等价于似然加权算法
        CH12.3-Markov Chain Monte Carlo Methods
            在似然加权算法中，证据仅会影响证据变量的后代变量的采样过程，其非后代实际上还是从先验中采样，而不是后验，如果先验和后验之间的差异过大，权重将不足以弥补这一点
            MCMC 方法采用和加权方法完全不同的模式，MCMC 方法启发自对物理现象的观察，即一个粒子的状态演化过程是一个 Markov 链，随着其粒子的状态在 Markov 链上演化，粒子的状态分布会逐渐收敛到一个稳态分布
            MCMC 方法定义了一个 Markov 链，其稳态分布是所要的采样分布 $P$ (例如后验分布)，然后让从初始分布 (例如先验分布) 生成的样本随着 Markov 链对其赋值/状态进行演化；因为 Markov 链的状态分布会最终收敛到其稳态分布，故我们可以最终将该样本视作从所想要的分布中采样得到；在该过程中，样本的分布会逐渐靠近稳态分布 
            为了保证 Markov 链有唯一的稳态分布，其状态空间应该是各态遍历的 (即转移矩阵的所有项都应该为正)
            Gibbs 采样算法是 MCMC 方法的一种实现，它为每个变量构建一个分离的转移模型 (其转移模型就是 $P$ 中在给定所有其他变量的当前采样值的情况下当前变量的后验分布)，然后将各个变量的转移模型结合为 Markov 链的转移模型，可以证明该构造会让 Markov 链收敛到所想要的分布 $P$ 
            如果 Markov 链 $\mathcal T$ 相对于某个分布 $\pi$ 满足细致平衡方程，则 $\mathcal T$ 是可逆的，并且 $\pi$ 是它的稳态分布，如果 $\mathcal T$ 是规范的，则 $\pi$ 就是唯一的稳态分布
            MH 算法是构建稳态分布为 $P$ 的 Markov 链的通用方法，它使用提案分布 $Q$ 作为转移模型的一部分，并根据 $P$ 和 $Q$ 定义接受概率，提案分布和接受概率一起定义了 Markov 链的转移模型，可以证明该 Markov 链的稳态分布是 $P$ 
- 一份（不太）简短的 LaTex2e 介绍: CH1-CH2
    CH1-LaTeX 的基本概念
        LaTeX 命令分为两种：`\` + 一串字母；`\` + 单个非字母符号
        字母形式的命令忽略其后的空格字符
        LaTeX 的环境由 `\begin,\end` 命令包围
        LaTeX 用 `{}` 划分分组，限制命令的作用范围
        `\documentclass` 指定文档类，`\begin{document}` 开启文档环境，二者之间为导言区，用于用 `\usepackage` 使用宏包
        `\include, \input` 用于插入文件
    CH2-用 LaTeX 排版文字
        UTF-8 是对 Unicode 字符集的一种编码方式
        XeTeX 和 LuaTeX 完全支持 UTF-8，`fontspec` 宏包用于调节字体
        `ctex` 宏包和文档类 (`ctexart, ctexbook, ctexrep` ) 用于支持中文排版
        LaTeX 将空格和 Tab 视作空白字符，连续的空白视作一个空白，行首的空白会被忽略
        连续两个换行符生成一个空行，将文字分段 (等价于 `\par` )，连续空行视作一个空行
        LaTeX 会自动在合适位置断行断页，也可以手动用命令控制
\[文档阅读\]
- python/pep/PEP 257–Docstring Conventions

### 第二周
时间：2024.12.2-2024.12.9

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH17.1-CH17.3
    CH17-Parameter Estimation
        参数估计的两个主要方法是极大似然估计 (MLE) 和贝叶斯方法
        CH17.1-Maximum Likelihood Estimation
            假设空间包含了我们考虑的全部可能假设，目标函数用于度量假设空间中不同的假设和数据集的相关性有多好
            一个假设的 “优秀” 程度可以理解该假设可以将观测数据预测得有多好，如果在该假设下更有可能观测到给定的数据，则该假设就是一个好的假设
            似然函数是相对于参数 $\theta$ 和数据集 $\mathcal D$ 的函数，它度量了给定参数 $\theta$ 下数据集 $\mathcal D$ 出现的后验概率
            似然值更高的参数值更有可能生成观测到的数据，能够最大化似然的参数值就称为极大似然估计
            对数似然函数和似然函数单调性相同，因此最大化似然函数等价于最大化对数似然函数
            置信区间度量了我们关于我们的估计的信心程度
            充分统计量是关于数据的函数，它总结了数据中用于计算似然的相关信息，对于一个统计量，如果给定相同参数，两个数据集的该统计量相同意味着它们的似然相同，则该统计量就是充分统计量
            如果两个参数对于所有可能的数据集选择 $\mathcal D$ 的似然都相同，则称二者是不可区分的
        CH17.2-MLE for Bayesian Networks
            贝叶斯网络中，似然函数可以根据网络结构分解为各个 CPD 的局部似然函数的乘积，每个 CPD 的局部似然函数还可以进一步根据父变量的取值分解 (这里隐含的假设是各个 CPD 的参数值相互独立)
            该性质称为似然函数的可分解性，当各个 CPD 都有一个不相交的独立参数集合参数化时，该性质就成立
            如果似然函数的可分解性成立，则全局的 MLE 就等价于各个 CPD 的局部 MLE 的结合，全局的优化问题就分解为多个独立的子优化问题的结合
            MLE 的其中一个问题和数据划分现象有关，随着父变量集合的维度增长，数据集将会被划分为大量的小数据集，同时参数数量也指数增长，用于估计一个参数的样本数量将很少，这是限制从数据中学习 BN 的关键原因
        CH17.3-Bayesian Parameter Estimation
            和 MLE 相比，贝叶斯统计额外考虑了先验
            我们将关于参数 $\theta$ 的先验知识编码为一个概率分布，即先验分布，进而在观测到的数据和参数上构建联合分布，该联合分布可以由一个元网络表示
            我们可以将该联合分布分解为先验和似然的乘积，则参数的后验就和似然和先验的乘积成比例
            对下一个参数的预测可以被写为其条件概率和参数的后验概率的乘积在所有可能参数值 $\theta$ 上的积分
            对于均匀先验，预测 (贝叶斯估计) 的形式和 MLE 相似，差异在于贝叶斯估计为各个情况额外添加了一个“虚拟”的样本，贝叶斯估计和 MLE 随着样本数增长会收敛到同一值
            均匀先验下的贝叶斯估计称为拉普拉斯修正
            另一种常见先验是 Beta 分布，Beta 由两个超参数参数化，两个超参数对应于 “虚拟” 的样本数量
            如果先验是 Beta 分布，且似然函数是 Bernoulli 似然函数，则后验分布也将是 Beta 分布，其超参数由观测修正，故称 Beta 分布共轭于 Bernoulli 似然函数
            随着我们获得更多数据，先验的效果将逐渐减弱
            贝叶斯框架允许我们用先验分布表示先验知识，同时使用后验的尖峰程度区分少样本的情况和多样本的情况
            贝叶斯方法中，我们将参数视作随机变量，使用概率描述关于参数的不确定性，然后使用贝叶斯规则将观测纳入考虑，衡量观测如何影响我们关于参数的信念
            我们能否紧凑表示后验取决于先验的形式
            Dirichlet 先验和多项式模型共轭，即如果似然函数形式为多项式分布，先验为 Dirichlet 先验，则后验也是 Dirichlet 分布
            此时的估计也和 MLE 形式类似，差异在于为计数添加了超参数，故 Dirichlet 超参数也称为伪计数
            伪计数的和反映了关于先验的信心，称为等价样本大小
            此时的估计可以被视作先验值和 MLE 估计的加权均值，因此已知贝叶斯估计随着样本数量增大收敛到 MLE 估计，直观上看，大数据集使得先验的贡献可以忽略，但小数据集下估计会朝先验偏置
            先验的存在使得贝叶斯估计相较于 MLE 更稳定，这种平滑效应使得数据不足时的估计更健壮，如果先验知识不足，可以使用均匀先验，避免估计取极限值

\[文档阅读\]
- docker/get-started/Docker Concepts

### 第三周
时间：2024.12.9-2024.12.16

\[文献阅读\]
- 2020-TDPS-The Deep Learning Compiler A Comprehensive Survey: Sec1-Sec3
    Sec1-Introduction
        ONNX 定义了表示 DL 模型的统一格式
        DL 硬件可以分为三类：通用、专用、神经形态
        DL 编译器的目标是减轻手动在各个 DL 硬件上优化 DL 模型的负担，DL 编译器包括 TVM, Tensor Comprehension, nGraph, Glow, XLA
        DL 编译器的输入是 DL 框架中的模型描述，输出针对该模型和特定 DL 硬件架构的优化实现
        DL 编译器采用分层设计，包括前端、IR、后端，其中 IR 分多个层
    Sec2-Background
        Deep Learning Frameworks
        TensorFlow 采用原始算子的数据流图，同时用控制边拓展，TF 的前端是 Keras，由纯 Python 实现。
        PyTorch 将构建动态数据流图的算子嵌入 Python，Python 解释器执行控制流，Torch 的前端是 FastAI。
        ONNX 定义了可拓展的计算图模型。
        Deep Learning Hardware
        TPU 包括矩阵乘法单元、统一缓存、激活单元，矩阵乘法单元主要由脉冲阵列构成。TPU 可编程，且使用矩阵作为基本运算对象而不是向量或标量。
        Hardware-specific DL Code Generator
        FPGA 处于 CPUs/GPUs 和 ASIC 之间。HLS 编程模型提供编程 FPGA 的 C/C++ 接口，但 DL 模型通常用 DL 框架的语言描述而不是 C/C++，故将 DL 模型映射到 FPGA 仍然较复杂。
        目标为 FPGA 的代码生成器接受 DL 模型作为输入，输出 HLS 或 Verilog/VHDL。
        根据生成的 FPGA 加速设备的结构，代码生成器可以分类为针对处理器架构或针对流架构。
        处理器架构的 FPGA 加速设备包含多个处理单元，每个处理单元包含片上缓存和多个小处理引擎。目标是这类架构的 DL 代码生成器采用硬件模板来自动生成加速设备设计，模板参数包括处理单元的数量和每个处理单元中处理引擎的数量等。
        将 DL 模型映射到处理单元和处理引擎的调度参数包括 tile 大小和 batch 大小。
        以上介绍的参数一半通过设计空间探索确定。
        流处理架构的 FPGA 加速设备由多个不同的硬件块组成，通常 DL 模型的每一层对应其中一个块。可以以流水线方式利用所有的硬件块，处理流式输入数据。
    Sec3-Common Design Architectures of DL Compilers
        DL 模型会被 DL 编译器转化为多级 IR，其中前端针对高级 IR，后端针对低级 IR。高级 IR 用于实现独立于硬件的优化的转换，低级 IR 用于实现针对硬件的优化和转换。
        高级 IR 也称为图 IR，图 IR 表示了独立于硬件的计算和控制流，构建了算子和数据之间的控制流和依赖，同时为图级别优化提供了接口。
        低级 IR 更细粒度以反映硬件特性，低级 IR 允许使用编译器后端的第三方工具链进行优化。
        前端接受 DL 模型为输入，输出图 IR，对图 IR 的优化可以分类为：节点级别、块级别、数据流级别。优化过后的计算图会被传递给后端。
        后端接受图 IR 作为输入，输出低级 IR。后端可以直接将图 IR 转化为第三方工具链的 IR，例如 LLVM IR，进行通用目的的优化和生成；也可以使用自定义的编译 pass。常用的针对硬件的优化包括 hardware intrinsic mapping, memory allocation and fetching, memory latency hiding, parallelization, loop oriented optimization。
        现存的后端使用自动调度和自动调节来确定最优的参数设定。
        低级 IR 可以 JIT 编译也可以 AOT 编译。
\[Book\]
- Probabilistic Graphical Models-Principles and Techniques: CH17.4, CH19.1-CH19.2, CH20.1-CH20.3
    CH17-Parameter Estimation
        CH17.4-Bayesian Parameter Estimation in Bayesian Networks
            全局参数独立性：每个 CPD 各自的参数先验相互独立，此时完整参数的先验具有完全分解的形式。
            如果全局参数独立性成立，则完整的数据 d-separate 各个 CPD 的参数，这进而表明了完整参数的后验具有完全分解的形式。
            根据 Bayes rule，参数后验可以重写为似然函数乘上先验再除以数据集边际概率。似然函数可以分解为多个局部似然的乘积，先验也可以分解为多个局部先验的乘积 (全局参数独立性成立时)，因此后验也具有完全分解形式。后验的完全分解形式也可以直接通过 meta-Bayesian 网络中读出来。
            做预测时，我们需要在所有合法的参数值上积分，以计算后验概率。如果数据 IID，并且全局参数独立性成立，则后验的计算可以分解为多个局部积分的乘积，其中每个局部积分仅和一个 CPD 有关。
            因此，此时我们仅需要独立解决各个局部 Bayesian 估计问题，将各个局部解结合得到全局解。
            局部参数独立性意味着 CPD 的局部先验可以进一步根据父变量的赋值分解。
            如果 CPD 不是多项式 CPD，则不一定会存在共轭的先验或 Bayesian 积分的闭式解，当完整的 Bayesian 方法不可解时，我们可以考虑极大后验估计。如果数据量足够大，则后验分布应该在极大值上有锐利的风，此时 Bayesian 积分大致等价于极大后验估计。
            MAP 估计可以视作为似然函数提供了正则化的极大似然估计。正则化项的效果会随着样本数量增大而减少。
            实践中可以使用 MAP 估计，因为我们往往会选择一个定义良好的先验，使得 MAP 估计可解。
    CH19-Partially Observed Data
        CH19.1-Foundations
            To analyze the probabilistic model of the observed training set, we must consider not only the data-generation mechanism, but also the mechanism by which data are hidden. Every observation is derived by the combination of the two mechanisms.
            If the outcome variables and the observation variables are marginally independent, then we say the data missing model is missing completely at random. In this situation, the whole likelihood can be decomposed as the product of the likelihood of the outcome variables and the likelihood of the observations variables. We can maximize the likelihood of interest independently.
            Given the observed outcome variables, if the hidden outcome variables and the observation variables are conditionally independent, then we say the data missing model is missing at random. In this situation, we can also decompose the likelihood, and use only the observed variables to optimize the parameters of the outcome distribution.
            However, in general, the likelihood function of the observation is a sum of likelihood function of the observation with all possible hidden assignments, each of which defines a unimodal function. Thus the likelihood function with incomplete data is a multimodal function and takes the form of "a mixture of peaks".
            Thus the likelihood function is not decomposable again, and will be hard to optimize.
        CH19.2-Parameter Estimation
            Because the likelihood function is multimodal thus hard to optimize, when doing MLE, we have to maximize a highly nonlinear in a high dimensional space. There are two main classes of methods for performing this optimization: generic nonconvex optimization algorithm (gradient ascent), more specialized method for optimizing likelihood functions (EM).
            The gradient of the log-likelihood function with respect to a single CPD entry $P(x\mid \pmb u)$ is stated in Theorem 19.2. This theorem provides the form of the gradient of table-CPDs. For other CPDs, we can use the chain rule the derivatives to compute the gradient.
            To compute the gradient, we need to compute the joint distribution $P(X[m], \pmb U[m], \mid \pmb o[m], \pmb \theta)$ for each $m$, therefore we need to do inference for each data case using one clique tree calibration.
            After computing the gradient, there is a issue that all components of the gradient vector is nonnegative (since increasing each of the parameters will lead to higher likelihood). Thus, a step in the gradient direction will increase all parameters, leading to an illegal probability distribution.
            There are two common approaches to solve this issue. 
            The first one is to modify the gradient ascent procedure to respect these constraints. We project the gradient vector to the hyperplane that satisfies the linear constraints of the parameters, and ensure the gradient stepping do not step out of bounds so that the parameters will be nonnegative. 
            The second one is to reparametrize the problem, introduce new parameters $\lambda_{x\mid \pmb u}$ to define $P(x\mid \pmb u)$. Now the value of $\lambda$ is not under constraint. We can use standard gradient ascent procedure to update $\lambda$ s.
            Another way is to use Lagrange multipliers.
            The gradient ascent will only guarantee we converge to a local maximum. Therefore some methods like choosing random starting points, applying random perturbations to converge points can be used for help.
            EM algorithm is an alternative way to optimize a likelihood function. The intuition is to fill in the missing value, and then  use standard, complete data learning procedure. Such approaches are called data imputation methods in statistics.
            When learning with missing data, we are actually trying to solve two problems at once: learning the parameters, and hypothesizing values for unobserved variables in each data cases. Each of these task is fairly easy when we have the solution to the other.
            EM algorithm iteratively solve one of the two problems. We start from random initial point, and iteratively do the following two steps: 1. infer the expected sufficient statistics for the unobserved variables (E-step) 2. infer the parameter based on the complete data (M-step)
            This sequence of steps provably improve our parameters, which means the likelihood will be non-decreasing. Therefore, this algorithm is guaranteed to converge to a local maximum.
            Note that we use posterior probabilities to compute the expected sufficient, therefore we consider both the observed data (evidence) and the current parameter.
            In practice, EM generally converges to a local maximum of the likelihood function.
            Apply EM in clustering, we are viewing the data as coming from a mixture distribution and attempts to use the hidden variable to separate out the mixture into its components. If we use hard-assignment EM, we get k-means.
            Hard-assignment version tends to increase the contrast between different classes, since assignments have to choose between them. Soft-assignment can learn classes that are overlapping, since many instances contribute to two or more classes.
            Hard-assignment traverses the combinatorial space of assignments to the hidden variables $\mathcal H$. Soft-assignment traverses the continuous space of parameter assignments. The former makes discrete steps, and will converge faster. The latter can take paths that are infeasible to the former, and can shift two clusters' mean in a coordinated way, while the former can only "jump", since it cannot simultaneously reassign multiple instances and change the class means.
    CH20-Learning Undirected Models
        CH20.1-Overview
            The biggest difference between MN and BN is the partition function. This global factor couples all of the parameters  across the network, preventing us from decomposing the problem and estimating local groups of parameters separately.
            Therefore, even MLE estimation in the complete data case cannot be solved in a closed form (except for chordal MN, which is equivalent to BN).
            To learn MN, we generally resort to iterative methods, and each iteration step of it requires us to run inference in the network.
            Bayesian estimation for MN also has no closed-form solution. Thus the integration associated it must be performed using approximate inference (variational methods or MCMC).
            In this area, part of the work focuses on the formulation of alternative, more tractable objectives of this estimation problem. Another part of the work focuses on the approximate inference algorithms.
            Structure learning for MN also need approximation methods for similar reason. The advantage of MN's structure learning over BN's structure learning is the lack of acyclicity constraint. The acyclicity constraint will couple decisions regarding the family of different variables. 
        CH20.2-The Likelihood Function
            For MN which has equivalent structure to BN, we can use BN's CPDs to represent MN's potentials. The BN's MLE solution is exactly the MN's MLE solution.
            We use log-linear format to represent the Gibbs distribution. Thus the parameters to learn corresponds to the weight we put on each feature. In this setting, the sufficient statistics of the likelihood function are the sums of the feature values in the instances in $\mathcal D$.
            In this setting, the likelihood function can be described as a sum of two functions, the first one is linear in the parameters. 
            We can prove that $\ln Z(\pmb \theta)$ is convex with respect the $\pmb \theta$ (It has semi-positive Hessian). The first derivative of $\ln Z(\pmb \theta)$ with respect to $\theta_i$ is $E_{\pmb \theta}[f_i]$. The second derivative of $\ln Z(\pmb \theta)$ with respect to $\theta_i, \theta_j$ is $Cov_{\pmb \theta}[f_i; f_j]$.
            Therefore, $-\ln Z(\pmb \theta)$ is concave in $\pmb \theta$, the sum of a linear function and a concave function is concave. Thus the log-likelihood function is concave. Therefore the log-likelihood function has no local maximum. (only has multiple equivalent global maximum)
        CH20.3-Maximum (Conditional) Likelihood Parameter Estimation
            For a concave function, its maxima are precisely the points at which the gradient is zero. Thus we can precisely characterize the maximum likelihood parameters $\hat {\pmb \theta}$.
            At the maximal likelihood parameter $\hat {\pmb \theta}$, the expected value of each feature relative to $P_{\hat {\pmb \theta}}$ matches its empirical expectation in $\mathcal D$. In other words, we want the expected sufficient statistics in the learned distribution to match the empirical expectations. This type of equality constraint is called moment matching. Therefore, the MLE estimation is consistent: if the model if sufficiently expressive to capture the data-generating distribution, then, at the large sample limit, the optimum of the likelihood objective is the true model.
            Although the likelihood function is concave, there is no analytical form of its maximum. Thus we have to use iterative methods to search for the global optimum.
            The gradient can be computed according to (20.4), it is the difference between the feature's empirical count in the data and the expected count relative to the current parameterization $\pmb \theta$.
            To compute the expected count, we have to compute the different probabilities of the form $P_{\pmb \theta}(a, b)$, which needs us to do inference at each iteration. To reduce the computational cost, we may use approximate methods.
            In practice, standard gradient ascent converges slowly and is sensitive to the step size. Mush faster convergence can be obtained with second-order methods, which utilize the Hessian to provide the quadratic approximation to the function. The computation of Hessian is illustrated in (20.5), which may also need approximation.
            If we only need the model to do inference, we can train a discriminative model. In other words, we train a CRF that encodes a conditional distribution $P(\pmb Y\mid \pmb X)$.
            Now the objective is the conditional likelihood and its log. The objective can be proved to be concave. Each data instance $\pmb y[m]$ 's log-likelihood is the log-likelihood of the data case in the MN reduced to the context $\pmb x[m]$.
            In unconditional case, each gradient step requires only a single execution of inference. When training a CRF, we must execute inference for each data case (in the reduced, simpler MN). If the domain of $\pmb X$ is very large, the reduction will be more beneficial. Thus in this case training a discriminative network will be more economical.
            In the missing data case, the likelihood function will be multiple modal, thus losing its concavity.
            In this case, according to (20.9), the gradient of feature $f_i$ is the difference between two expectations - the feature expectation over the data and the hidden variables minus the feature expectation over all the variables.
            Applying EM in MN is similar to BN. The difference is in M-step, where MN needs run inference to get gradient.
            The trade-off between gradient method and EM method is more subtle in MN.

### 第四周
时间：2024.12.16-2024.12.23-2024.12.30

\[文献阅读\]
- 2020-TDPS-The Deep Learning Compiler A Comprehensive Survey: Sec4-Sec7
    Sec4-Key Components of DL Compilers
        Sec4.1-High-level IR
            High-level IR is also known as graph IR.
            4.1.1 Representation of Graph IR
            The representation of graph IR can be categorized into two classes: DAG-based IR, Let-binding-based IR.
            In DAG-based IR, the node represents atomic DL operator, the edge represents tensor.
            In DAG-based IR, the graph is acyclic without loops, therefore differs from the data dependency graph of generic compilers.
            There are already plenty of optimizations on DDG, like Common Subexpression Elimination, Dead Code Elimination. These algorithm can be combined with DL domain knowledge to optimize the DAG computation graph.
            DAG-based IR is simple, but may cause semantic ambiguity because of missing the definition of the computation scope.
            Let-binding offers let-expression to certain functions with restricted scope in high-level language to solve semantic ambiguity. 
            When using `let` keyword to define expression, a let node will be generated which points to the operator and the variable in the expression, instead of just building the computational relation between variables in DAG.
            In DAG-based compiler, to get the return value of certain expression, the corresponding node will the accessed and the related nodes will be searched. It is known as recursive descent technique.
            The let-binding based compiler will compute the results of all variables in a let expression, and builds a variable map. The compiler looks up this map to decide the value of the expression.
            TVM and Relay IR adopts both.
            The ways graph IR to represent tensor computation can be categorized into three classes: Function-based, Lambda expression, Einstein notation
            Glow, nGraph, XLA's IR (XLA's IR is HLO) use function-based representation to represent tensor computation. The function-based representation only provides encapsulated operators.
            Lambda expression uses variable binding and substitution to represent calculation. TVM uses tensor expression to represent tensor computation, in which the computational operator are defined by the output tensor's shape and the lambda expression of computing rules.
            Einstein notation is used to expression summation, in which the indexes for temporary variables do not need to be defined. The actual expression can be deduced from the Einstein notation. In Einstein notation, the operators should be associative and commutive, and thus the reduction operators can be executed by any order.
            4.1.2 Implementation of Graph IR
            Data representation
            Tensor can be represented by placeholder which only carries the shape information of the tensor. It helps separate the computation definition and actual computing. To support dynamic shape/dynamic model, placeholder should support unknown dimension size. Also, the bound inference and dimension checking should be relaxed, and extra mechanism is needed to guarantee memory validity.
            Data layout describes tensor's organization in memory, which is usually a mapping from logical indices to memory indices. The data layout includes the sequence of dimensions, tilling, padding, striding, etc.
            Bound inference is used to determine the bound of iterators when compiling DL models. The bound inference is often performed iteratively of recursively according to the computation graph and placeholders.
            Operators supported
            Operators supported by DL compilers will be the node in the computation graph.
            4.1.3 Discussion
            The data and operators designed in high-level IR are flexible and extensible enough to support diverse DL models. The high-level IRs are hardware-independent.
        Sec4.2-Low-level IR
            Low-level IR provides interface to tune the computation and memory access. The common implementation of low-level IR can be classified into 3 categories: Halide-based IR, polyhedral-based IR, other unique IR.
            Halide-based IR
            Halide's philosophy is to separate computation and schedule. Compilers adopting Halide try various possible schedules and choose the best one. TVM improved Halide-IR to independent symbolic IR.
            Polyhedral-based IR
            Different from Halide, the boundaries of memory bounds and loop nests can be polyhedrons with any shapes in the polyhedral model. The polyhedral-based IR makes it easy to apply polyhedral transformations (fusion, tiling, sinking, mapping).
            Other unique IR
            MLIR has a flexible type system and allows multiple abstraction levels. It induces dialects to represent multiple levels of abstraction. Each dialect consists of a set of defined immutable operations. Current dialects includes TensorFlow IR, XLA HLO IR, experimental polyhedral IR, LLVM IR, TensorFlow Lite.
            Most DL compiler's low-level IR will eventually lowered to LLVM IR to use LLVM's optimizer and code generator. LLVM also supports custom instruction set for specialized accelerator.
            DL compiler adopts two approaches to achieve hardware-dependent optimization: 1. perform target-specific loop transformation in the upper IR of LLVM 2. provide additional information about the hardware target for optimization passes.
        Sec4.3-Frontend optimizations
            Frontend optimizations are shared by different backends.
            The frontend optimizations are defined by passes. The passes traverse the graph's nodes and perform graph transformation. (rewrite the graph for optimization)
            Passes can be pre-defined or customized by developers. Most DL compliers can capture shape information in computation graph to do optimization.
            The frontend optimization can be classified into three categories: 1. node-level 2. block-level (local) 3. dataflow-level (global)
            Node-level optimizations
            The nodes of computation are coarse enough to enable optimizations inside a node.
            Node elimination: eliminate unnecessary nodes (e.g. operations lacking adequate inputs, zero-dim-tensor elimination)
            Node replacement: use lower-cost nodes
            Block-level optimizations
            Algebraic simplification: optimization in computation order, optimization in node combination, optimization of ReduceMean nodes
            Operator fusion: 
            Operator sinking: make similar operations closer in order to create opportunities for algebraic simplification 
            Dataflow-level optimizations
            CSE: use previously computed sub-expression's value to substitute other occurrences of that sub-expression in the graph
            DCE: a set of code is dead if it's computation result or side-effect are not used; DCE includes dead store elimination (remove never used tensor's storage operation)
            Static memory planning: done offline, aims to reuse memory as much as possible; in-place memory sharing: allocate only one copy for operation, for sharing between the input and output; standard memory sharing: reuse previous operations' memory without overlapping.
            Layout transformation: aims to find the best data layout for tensors in the computation and insert layout transformation node into the computation graph. The actual layout transformation is preformed by the backend. To find the best data layout, the hardware details are required, like cache line size, vectorization unit size, memory access pattern etc.
        Sec4.4-Backend optimizations
            Hardware-specific optimization
            includes: 
            1. hardware intrinsic mapping: transform a certain set of low-level IR instructions to highly optimized kernels in target hardware.
            2. memory allocation and fetching: 
            3. memory latency hiding: reorder the execution pipeline
            4. loop oriented optimizations: includes 1) loop fusion to fuse loops with the same boundaries 2) sliding windows 3) tiling: the tiling patter and size can be determined by auto-tuning 4) loop reordering (loop permutation): changes the order of iterations in a nested loop to increase spatial locality. It requires the loop is free of data-flow dependency between iterations 5) loop unrolling: usually applied in combination with loop split: first split the loop into two nested loops and unroll the inner loop
            5. Parallelization: utilizes accelerator's multi-thread and SIMD parallelism.
            Auto-tuning
            four key components:
            1. parameterization: the data parameter describes the data's specification; the target parameter describes hardware-specific characteristics (e.g.  shared memory and register size) and constraints
            2. cost model: 1) black-box model: only considers the final execution time 2) ML-based cost model: e.g. GBDT 3) pre-defined cost model
            3. searching technique: 
            4. acceleration: 1) parallelization 2) configuration reuse
            Optimized kernel libraries 
    Sec5-Taxonomy of DL Compilers
    Sec6-Evaluation
    Sec7-Conclusion and Future Directions

\[Book\]
- Probabilistic Graphical Models-Principles and Techniques: CH18.1-CH18.3
    18.1-Introduction
        In structure learning, we aims to recover $\mathcal G^*$ or its I-equivalence based on data. $\mathcal G^*$ is $P^*$ 's perfect map.
        The more edges our structure have, the more parameters we need to learn. Because of data fragmentation, the quality of estimated parameter will decrease if the number of samples is fixed. (Note that the standard deviation of MLE estimate if $1/\sqrt M$)
        Thus when doing density estimation from limited data, we prefer sparse structure even if the true structure $\mathcal G^*$ is more dense. Because we need to avoid overfitting.
        There are three methods for structure learning. 
        The first one is constraint-based structure learning, which tests the independence in data and find a network that best explains these independencies. This type of method is sensitive to failures in individual independencies test. If one of these tests return a wrong answer, the network construction will be misled.
        The second one is score-based structure learning. This method defines a hypothesis space of potential models and a score function that measures how well the model fits the observation. The task is to search the model that maximize the score in the hypothesis space. Score-based method consider the whole structure at once, thus is less sensitive to individual failures.
        The third method does not learn a single model but an ensemble of multiple possible structures.
    18.2-Constraint-Based Approaches
        Determining whether two variables are independent is often referred to as hypothesis testing.
    18.3-Structure Scores
        Score-based methods approach the problem of structure learning as an optimization problem.
        Intuitively, we need to find a model that would make the data as probable as possible. In this case, our model is pair $\langle \mathcal G,\pmb \theta_{\mathcal G}\rangle$. The likelihood score directly defines $\pmb \theta_{\mathcal G}$ to be its the MLE estimation $\hat {\pmb \theta}_{\mathcal G}$, and tries to find structure $\mathcal G$ that maximize $score_{L}(\mathcal G:\mathcal D) = \ell(\hat {\pmb \theta}_{\mathcal G}:\mathcal G)$.
        The likelihood score can decompose according to (18.4). We can observe that the likelihood measures the strength of the dependencies between variables and their parents. 
        For BN, the process of choosing a network structure is often subject to constraints. Some constraints are a consequence of the acyclicity requirement, others may be due to a preference for simpler structures.
        Because the property of mutual information, adding edge to a network will never decrease its likelihood score. Thus likelihood score will result in fully connected network in most cases. Therefore, the likelihood score can not avoid overfitting.
        The Bayesian method put a distribution on possible structures $\mathcal G$ and is proportional to the posterior $P(\mathcal G\mid \mathcal D)$. The Bayesian score is defined as $score_B(\mathcal G: \mathcal D) = \log P(\mathcal D\mid \mathcal G) + \log P(\mathcal G)$.
        The calculation of marginal likelihood $P(\mathcal D\mid \mathcal G)$ need us to integrate the whole parameter space $\Theta_{\mathcal G}$. Therefore, we are measuring the expected likelihood, averaged over different possible choices of $\pmb \theta_{\mathcal G}$ decreasing the sensitivity of the likelihood to the particular choice of parameters. 
        Another perspective to explain the Bayesian score is derived from the holdout testing methods. The Bayesian score can be viewed as a form of prequential analysis, where each instance is evaluated in incremental order, and contributes both to our evaluation of the model and to our final model score. The sequence order can be arbitrary. According to (18.8), the marginal likelihood can be approximately viewed as the estimation of the model' expected likelihood in the underlying distribution.
        If the parameter's priors are in conjugate case, the marginal likelihood of a single variable's form can be easily written. As a consequence, the marginal likelihood of the dataset can be further written simpler according to (18.9).
        The Bayesian score for BN cane be decomposed under the assumption of parameter independence. The the local independence is also satisfied, (18.9) can be applied to substitute the local terms of the factorized Bayesian score.
        If $M\to \infty$, the $\log P(\mathcal D\mid \mathcal G)$ can be represented as Theorem 18.1. We can observe that the Bayesian score tends to trade off the likelihood (fit the data) and the model complexity. Omitting the constant term, we get the BIC score.
        The log-likelihood term increase linear to $M$, and the model complexity term increase log to $M$, therefore the emphasis on data fitting will increase as $M$.
        BIC score and the Bayesian score are consistent, which means with adequate data, the score will select $\mathcal G^*$. or its I-equivalence.
        Consistency is an asymptotic property, and thus it does not imply much about the properties of networks learned with limited amounts of data.
        We call the prior satisfies parameter modularity if two structure's local structure are the same, their prior will be the same
        Under parameter modularity, Bayesian score will be decomposable, and thus the searching can be done locally and separately.
        The likelihood score is naturally decomposable.
- 面向计算机科学的组合数学: CH1.1-CH1.6, CH2.1-CH2.2, CH3.1-CH3.2, CH7
    CH1-排列组合
    CH2-鸽巢原理
    CH3-母函数
    CH7-Polya 计数理论

\[Doc\]
- mlir/Toy Tutorial: CH1-CH2
    CH1-Toy Language and AST
        unranked tensor parameter: the dimension is unknown, and will be specialized at call sites
    CH2-Emitting Basic MLIR
        There is no closed set of attributes, operations, types in MLIR.
        MLIR's extensibility is supported by dialects, which groups operations, attributes, types under the the same abstraction level.
        MLIR's core computation and abstraction unit are operations, which can be used to represent all core IR structures in LLVM like instructions, globals, modules.
        Operation's results and arguments are SSA values.
        Operation's name is prefixed with dialect's name.
        Operation can have zero or multiple attributes.
        Concepts to model an operation includes: name, SSA arguments, attributes, result values' types, source location, successor blocks, regions.
        Note that every operation has an associated mandatory source location. The debug info in MLIR is core requirement.
        All IR elements (refer to the concepts that model an operation) can be customized in MLIR.
        In C++, a dialect is implemented as a derived class of `mlir::Dialect`. It's attributes, operations, types are registered by an initializer method called by the constructor.
        Using tablegen to declaratively define a dialect is more simple.
        `MLIRContext` only loads builtin dialects by default. Customized dialect should be passed to template method `loadDialect` to be explicitly loaded.
        In C++, an operation is defined as a derived class of `mlir::Op` using CRTP. CRTP means that `mlir::Op` is a template class, whose template argument is the operation class (its derived class). By CRTP, `mlir::Op` can know its derived class in compilation, and thus can safely use `static_cast` to invoke derived class's method to achieve polymorphism in compilation.
        `mlir::Op` can take optional traits as template arguments to represent operation's properties and invariants.
        Operation class can also define its method to provide additional verification beyond the attached traits.
        Operation class should define static `build` method in order to be invoked by the `bulider` class to generate this operation's instance from a set of input values.
        Operation's `build` method should populate a `mlir::OperationState` with its possible discrete elements.
        After defining the operation class, we can invoke `addOperation` with its template argument being the operation class to register this operation into the dialect.
        `Operation` class is used to generally model all operations, and it does not describe the properties and types of a particular operation. Thus it is used as a generic API.
        Each specific operation is a derived class of `Op` .
        `Op` act as a smart pointer wrapper of `Operation*` . All the data of an operation is stored in the referenced `Operation` class. The `Op` class is an interface/wrapper to interact with `Op`, and thus is usually passed by-value.
        A `Operation*` can be `dyn_cast` to the corresponding `Op` . 
        Operations can also be defined by tablegen. 

# 2025 年
## 一月
### 第一周
Date: 2024.12.30-2025.1.6

\[Paper\]
- [[paper-notes/Latent Dirichlet Allocation-2003-JMLR|2003-JMLR-Latent Dirichlet Allocation]]: All
    Sec1-Introduction
        This paper focuses on modeling collections of discrete data, and aims to find efficient and statistically meaningful representation of the member in the collections.
        A basic method of processing documents in a corpora is tf-idf scheme, whose basic idea is using a word's tf-idf value to represent a word's importance to a document, and using a tf-idf vector to represent a document.
        Tf-idf scheme's description length is not small, and thus reveal little inter and intra document statistical structure.
        LSI do SVD to tf-idf matrix to capture a linear subspace in the tf-idf feature space, thus achieving dimension reduction.
        Another method of modeling data is simply using maximum likelihood of Bayesian method to fit a generative model for the data.
        pLSI model view each word in a document as sampled from a mixture model, where the mixture component is the a conditional multinominal distribution over vocabulary given a topic. Each document is represented by a probability distribution over the topics. The words in the document can be essentially viewed as first sample a topic from the topic distribution and sample the word from the multinominal distribution given the topic. By this way, the document's representation is significantly reduced.
        The problem of pLSI is the lack of modeling generative probabilistic model for the topic's distribution. Thus the parameter increases linearly with the size of the corpus and it is not clear to assign topic distribution for new document.
        pLSI assumes exchangeability of words in a document and documents in a corpus. According to Finetti's representation theorem, any collection of exchangeable random variables can have a joint mixture distribution. This lead us to not only consider a mixture distribution over words, but also consider a mixture distribution over documents. 
    Sec2-Notation and terminology
        We aims to find a probabilistic model that not only assign high probability to members of the corpus but also assign high probability to other similar documents.
    Sec3-Latent Dirichlet Allocation
        LDA's basic idea is to represent a document as a random mixture of latent topics, where each topic is characterized by a multinominal distribution over words. Note the difference between LDA's representation and pLSI's representation is that LDA is represented by a **random** mixture of topics, which indicates there is a distribution of the topic mixture. While in pLSI, the documents' topic mixture is not modeled as random, thus being deterministic.
        In LDA, the document is generated by: first sample $N$ (word counts) from a Poisson distribution, next sample $\theta$ (topic mixture proportion/document representation) from a Dirichlet distribution with parameter $\alpha$, next for each word, sample a topic from the multinominal defined by $\theta$ and sample a word from the multinominal defined by the sampled topic.
        LDA assumes the topic number is known and fixed, and the word's multinominal is fixed and is to be estimated from the corpus.
        In LDA, given $\alpha$, topic mixture $\theta$ is a $k$ -dimensional Dirichlet random variable and take values in a $(k-1)$ -simplex, with $\theta_i \ge 0, \sum_{i=1}^k \theta_i = 1$. Topic mixture $\theta$ defines a multinominal over $k$ possible topics.
        There are three levels to the LDA representation. $\alpha, \beta$ are corpus-level parameters, sampled once when generating the corpus. $\theta$ are document-level parameters, sampled once for each document. $z, w$  are word-level variables, sampled once for each word. In LDA, a document can associate multiple topics.
        In LDA, to obtain a document's marginal probability, we need to marginalize out the topic variables $z$ for each word and marginalize out the topic mixtures $\theta$ for the document.
    Sec4-Relationship with other latent variables
        In unigram model, each word are drawn from a single multinomial.
        In mixture of unigram model, each document associates a topic, and each topic defines a multinomial over the vocabulary. The multinominal can be viewed as a representation of the topic. The difference between mixture of unigram model and LDA is that LDA allow a document exhibit multiple topics, and defines a distribution over the topics.
        pLSI allow a document associate multiple topics, but each document's topic mixture is deterministic. The number of parameters grow linear with the corpus size.
        LDA treat the topic mixture as a random variable, making generalizing to new document easy and removing the linear dependency of the parameter number.
    Sec5-Inference and Parameter Estimation
        In inference, we need to compute the posterior $p(\theta, \mathbf z\mid \mathbf w, \alpha, \beta)$. We construct a variational model representing the approximate posterior and optimize the KL divergence between the variational approximate posterior and the true posterior to find the optimal approximate posterior.
        Note that the optimization for variational parameter is conducted for fixed $\mathbf w$. Therefore the variational parameter is conditioned on the document. The variational Dirichlet parameter $\gamma$ can be viewed as the representation of the document.
        In parameter estimation, we use variational EM procedure to maximize the log-likelihood. The E-step finds the optimal variational parameters, which is the same as the inference. The M-step finds the optimal model parameters $\alpha, \beta$ by MLE or by Bayesian estimation (smoothing). 
    Sec6-Example
        The prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document.
    Sec7-Applications and Empirical Results
        Perplexity monotonically decrease with the likelihood of test data and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. Lower perplexity indicates higher test set likelihood and thud indicates better generalization performance.
    Sec8-Discussion

\[Book\]
- [[book-notes/Probabilistic Graphical Models-Principles and Techniques|Probabilistic Graphical Models-Principles and Techniques]]: CH19.2.2.5, CH19.2.3, CH19.2.4
    CH19-Partially Observed Data
        CH19.2-Parameter Estimation
            CH19.2.2-Expectation Maximization
                CH19.2.2.5-Theoretical Foundations
                    Each iteration of the EM process can be viewed as maximizing an auxiliary function. Maximizing the auxiliary function will yield better log-likelihood.
                    For exponential family models, the expected log-likelihood is a linear function of the expected sufficient statistics. Thus to maximize the expected log-likelihood, we first derive the expected sufficient statistics, and then compute the parameters that maximize the expected log-likelihood. That's precisely the EM process.
                    In each EM iteration, we are actually optimizing a function of the parameter $\pmb \theta$ and the posterior choice $Q$. We define the energy functional associated with $P$ and $Q$ as $F[P, Q] = E_Q[\log \tilde P] + H_Q$. Then we can prove $\log Z = F[P, Q] + D(Q||P)$.
                    Let $P = P(\mathcal H\mid \mathcal D,\pmb \theta) = P(\mathcal H, \mathcal D\mid \pmb \theta)/P(\mathcal D\mid \pmb \theta)$. According to the previous conclusion, we can get $\ell(\pmb \theta : \mathcal D) = F[P, Q] + D(Q|| P)$. Therefore, $\ell(\pmb \theta: \mathcal D) = E_Q[\log P(\mathcal H, \mathcal D\mid \pmb \theta)] +H_Q+ D(Q|| P(\mathcal H\mid \mathcal D, \pmb \theta))$. That is, we get an equivalent form of the log-likelihood, which is written as a function of $Q$ and $\pmb \theta$.
                    Take a step further, we can get $\ell(\pmb \theta : \mathcal D) = E_Q[\ell(\pmb \theta :\langle \mathcal D, \mathcal H \rangle)] + H_Q +  D(Q||P(\mathcal H\mid \mathcal D, \pmb \theta))$.
                    Because entropy and KL divergence are non-negative, the expected log-likelihood is a lower bound of the actual log-likelihood. Also, the energy functional (expected log-likelihood + entropy term) is a tight lower bound of the actual log-likelihood.
                    EM procedure is actually optimize the energy functional in a coordinate ascent way. Given fixed parameter $\pmb \theta$, it first search the optimal $Q$ the minimize the KL divergence, and then, given fixed $Q$, it tries to find the optimal $\pmb \theta$ that maximize the expected log-likelihood.
                    Because the energy functional is a tight lower bound of the actual log-likelihood, improving the energy functional will guarantee to improve the log-likelihood. And the improvement is guaranteed to be as large as the energy functional's improvement.
                    For most learning problem, the log-likelihood is upper bounded, and because EM can monotonically improve the log-likelihood, EM is guaranteed to converge to a stationary point of the log-likelihood function.
            CH19.2.3-Comparisoin: Gradient Ascent versus EM
                EM and gradient ascent are both local, greedy in nature, and both guarantee to converge to a stationary point of the log-likelihood function.
            CH19.2.4-Approximate Inference
                When computing gradient approximately, the approximation error will dominate if the stationary point is close.
                There is no guarantee that approximate inference will find a local maxima, but in practice, approximation is unavoidable.
                In variational EM, the E-step is to find the optimal variational $Q$ for each instance. Each instance's optimal variational posterior is different. The algorithm is essentially performing coordinate-wise ascent alternating between optimization of $Q$ and $\pmb \theta$. Therefore it is not necessarily to take too many steps to find best variational $Q$ in one iteration.
                In variational EM, the energy functional is not necessarily a tight lower bound of the actual log-likelihood. It depends on the choice of the variational distribution family. Therefore, variational EM has no convergence guarantee and it is easy to get oscillations both within a E-step and over several steps.

### 第二周
Date: 2025.1.6-2025.1.13

\[Paper\]
- [[paper-notes/precipitation-nowcasting/Skilful Nowcasting of Extreme Precipitation with NowcastNet-2023-Nature|2023-Nature-Skilful Nowcasting of Extreme Precipitation with NowcastNet]]: All
    Sec0-Abstract
        Pure physics-based method can not capture convective dynamics. Data-driven learning can not obey physical laws like advective conservation. NowcastNet unify physical-evolution scheme and conditional learning methods, and optimize forecast error end-to-end.
    Sec1-Introduction
        Weather radar echoes provide cloud observations at sub-2km spatial resolution and up to 5-min temporal resolution.
        DARTS and pySTEPS are based on advection scheme. They predict the future motion fields and intensity residuals from radar observations and iteratively advect the past radar field according to the predicted motion field, with the addition of intensity residuals, to obtain the future fields. The advection scheme respects the physical conservation laws, but does not account for the convective influence. Also, existing advection method does not incorporate nonlinear evolution simulation and end-to-end forecast error optimization.
        Deep learning based methods do not account for the physical laws explicitly, and thus may produce unnatural motion and intensity, high location error and large cloud dissipation at increasing lead times.
        NowcastNet combines deep learning methods and physical principles. It integrates advective conservation into a learning based model, and thus can predict long-lived mesoscale advective pattern and short-lived convective detail.
    Sec2-NowcastNet
        Given past radar fields $\mathbf x_{-T_0:0}$, the generative model generate future fields $\widehat {\mathbf x}_{1:T}$ from random Gaussian vector $\mathbf z$ conditioned on the evolution network's prediction.
        The random Gaussian vector $\mathbf z$ introduce randomness to the generation to capture chaotic dynamics. Integration over the latent vector enables ensemble forecast.
        The evolution network's prediction aims to comply with the physical advection process and produce physically plausible prediction for advective features at 20km scale. The generative network's aims to generate fine-grained prediction that captures convective features at 1-2km scale. This scale disentanglement mitigates error propagation between scales in multiscale prediction network.
        The physical-conditioning mechanism is implemented by spatially adaptive normalization technique. In forward pass, in each layer of nowcast decoder, the mean and variance of the activations are replaced by spatially corresponding statistics computed from the evolution network predictions. Therefore the nowcast decoder combines mesoscale advective pattern governed by the physical laws and convective-scale details revealed by radar observations.
        NowcastNet is trained in an adversarial way. The temporal discriminator on the nowcast decoder takes the pyramid features in several time windows as input and output the possibility that the input is fake or real radar field. By deceiving the discriminator, NowcastNet can learn to generate convective details present in the radar observation but left out by the advection-based evolution network.
        To make the generated field spatially consistent with the read observation, the loss also includes pool regularization term, which enforce pooling-level consistency between ensemble prediction and real observation.
    Sec3-Evolution network
        Previous implementation of advection schemes' disadvantages include: 1. advection operation is not differentiable 2.  can not provide nonlinear modelling ability 3. auto regressive generation prevents direct optimization of forecast error and accumulates estimation error of initial states, motion field, intensity residual.
        The evolution network's evolution operator is differentiable and directly optimize the forecast error throughout the time horizon by back propagation.
        The evolution operator takes the motion field, intensity residual and the current field as input and output the field in the next time step by one step of advection. The evolution operator can finally produce predictions $\mathbf x_{1:T}$ for several time steps. The gradient can be passed through the evolution operator, to directly optimize the motion decoder, intensity decoder, and the evolution encoder.
        To avoid numerical instability from discontinuous interpolation, in the evolution operator, the gradient between each time step is detached.
        The motion decoder and intensity decoder simultaneously predict motion fields and intensity residuals at all future time steps. 
        The objective of evolution network is minimizing the forecast error throughout the time horizon. The accumulated error term in the loss involves the distance between the predict field $\mathbf x_{t}''$ and the real field $\mathbf x_t$. It also involves the distance between the advected field $\mathbf x_t'$ and the real field $\mathbf x_t$ to short cut the gradient. It can be viewed as a residual shortcut. Therefore the intensity $\mathbf s_t$ in learning will tend to fit the residual between the real field and the advected field.
        To ensure continuity and to ensure the large precipitation pattern's motion field being more smooth than the small ones. The loss also involves a motion regularization term, which penalize the motion field's gradient norm in a weighted way.
    Sec4-Evaluation settings
        An importance sampling strategy is applied to create datasets more representative of extreme-precipitation events.
    Sec5-Precipitation events
    Sec6-Meterologist evaluation
    Sec7-Quantitive evaluation
    Sec8-Methods
        According to the continuity equation, the temporal evolution of precipitation can be modelled as a composition of advection by motion fields and addition by intensity residuals. The evolution operator is constructed in this principle, and the motion field and intensity residuals are predicted by neural networks based on past radar observations.
        The evolution network is responsible for predicts future radar fields at 20-km scale. The backbone of evolution network is a two-way U-Net, where each convolution layer is spectral normalized and the skip connections concatenate the temporal dimension/the channel dimension.
        The evolution operator views motion field $\mathbf v_{1:T}$ as departure offset and views intensity residual $\mathbf s_{1:T}$ precipitation intensity growth or decay.
        As applying bilinear interpolation continuously will blur the field. The advected field $\mathbf x_t'$ is computed by nearest interpolation, but the gradient and loss is computed from the bilinear interpolated field $(\mathbf x_t')_{\mathrm {bili}}$.
        Gradient between two consecutive time steps in the operator is detached because successive bilinear interpolation will make end-to-end optimization unstable.
        To balance different rainfall levels, the distance calculation in the accumulation loss is weighted proportional to the rain rate.
        The generative network is responsible for generating the final predicted precipitation field at a 1-2km scale. The backbone of generative network is also a U-Net encoder decoder structure. The encoder is identical to the evolution network's encoder, and it takes the concatenation of $\mathbf x_{-T_0:0}$ and $\mathbf x_{1: T}''$ as input. The decoder has different structure, and it takes encoder's encoded representation added by the transformed latent random vector as input.
        The conditioning mechanism is implemented by applying the spatially adaptive normalization to each convolutional layer in the decoder. 
        The pooling-regularization term is calculated from the ensembled prediction.
    Sec9-Datasets
    Sec10-Evaluation
\[Book\]
- [[book-notes/深度强化学习|深度强化学习]]: CH1

### 第三周
Date: 2025.1.13-2025.1.20

\[Paper\]
- [[paper-notes/normalization/Semantic Image Synthesis with Spatially-Adaptive Normalization-2019-CVPR|2019-CVPR-Semantic Image Synthesis with Spatially-Adaptive Normalization]]: All
    Sec0-Abstract  
        Directly use semantic layout as input to the network is suboptimal, because the normalization layer tend to wash away semantic information.
        Spatially-Adaptive Normalization use semantic layout to modulate the activations in the normalization layer spatially-adaptively.
    Sec1-Introduction
        Semantic image synthesis focuses on converting semantic segmentation mask to a photorealistic image.
        In traditional architecture, the normalization layer will wash away the information in the semantic mask, preventing semantic information propagating through the network.
    Sec2-Related Work
        Conditional normalization layers requires external data and generally first normalize activations to zero mean and unit deviation, then modulate the normalized activations using the external semantic input.
        Generating image from semantic mask requires applying a spatially-varying transformation/modulation to the normalized activations.
    Sec3-Semantic Image Synthesis
        SPatially Adaptive (DE) normalization first apply batch normalization, and then modulate the normalized activations with learned scale and bias spatially-adaptively. That is, the modulation parameters depend on the input mask and vary with respect to the location $(x, y)$.
        With SPADE, there is no need to feed the mask to the first layer of the generator. The generator can take a random vector as input to support multi-modal synthesis. That is, for the same mask, different random vector can leads to different but semantically consistent images.
        In SPADE, the semantic mask is fed through spatially adaptive modulation, without normalization. Therefore the semantic information is better preserved.
        By replacing the input noise with the embedding vector of the style image computed by the image encoder, we can further control the style of the synthesized image.
    Sec4-Experiments
    Sec5-Conclusion
- [[paper-notes/precipitation-nowcasting/Skilful Precipitation Nowcasting Using Deep Generative Models of Radar-2021-Nature|2021-Nature-Skilful Precipitation Nowcasting Using Deep Generative Models of Radar]]: All
    Sec0-Abstract
        Operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture non-linear events like convective initiations.
        Deep learning methods directly predict future rain rates, free of physical constraints. Deep learning methods can predict low-intensity rainfall, while the lack of constraints lead to blurry prediction at longer lead time and heavier rain events.
    Sec1-Introduction
        Radar data is available every five minutes at 1km x 1km grid resolution.
        Advective methods rely on the advection equation, using optical flow and smoothness penalty to estimate motion field.
        Deep learning based models are directly trained on large corpora of radar observations and do not rely on in-built physical assumptions. Deep learning based model conduct optimization end-to-end and has fewer inductive biases, therefore greatly improve forecast quality at low precipitation levels.
    