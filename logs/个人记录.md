格式示例：

---

阶段(<阶段序号>)：<阶段时间>
    <阶段计划>

时间：\<年\>.\<月\>.\<日\>-\<年\>.\<月\>.\<日\>，阶段(<阶段序号>)

\[文献阅读\]
- <发表年份>-<会议/期刊>-<文献名称>
    <一句话总结>

\[书籍阅读\]
- <发表年份>-<书籍名称>: <阅读范围>
    CH<章节序号>-<章节名称>：<一句话总结/要点提炼>

\[文档阅读\]
- <文档名称>-<文档版本>: <阅读范围>
    CH<章节序号>-<章节名称>：<一句话总结/要点提炼>

\[下周计划\]
    <关于文献阅读>
    <关于书籍阅读>


---
# 阶段1
阶段(1)：2024.9-2024.12
    广泛阅读，在工程和科研方面打下理论基础
## 2024年
### 8月
时间：2024.8.22-2024.9.9

\[文献阅读\]
- 2023-NeurIPS-Are Emergent Abilities of Large Language Models a Mirage?
    研究者对于度量的选取造就了 LLM 具有涌现能力的“海市蜃楼” 

\[书籍阅读\]
- 2015-Mastering CMake: CH2-CH15、CH17
    CH2-Getting Started on Your Computer：CMake 构建流程纵览
    CH3-Writing CMakeLists Files：CMake 语言纵览
    CH4-CMake Cache：CMake 缓存机制介绍：`CMakeCache.txt`
    CH5-Key Concepts：CMake 概念介绍：源文件、目标文件、属性
    CH6-Policies：CMake 策略机制：为了兼容性
    CH7-Modules：CMake 模块：CMake 提供的 utility
    CH8-Installing Files：`Install()` 命令
    CH9-System Inspections：借助宏编写跨平台软件；`try_run/compile()` 命令
    CH10-Finding Packages：借助 CMake 分发的软件依赖包
    CH11-Custom Commands：为自定义目标添加自定义构建规则
    CH12-Packing with CPack：借助 CPack 调用本地打包工具
    CH13-Testing with CMake and CTest：`add_test()`
    CH14-CMake Tutorial：纵览：简单构建、添加库、添加属性、通过系统审查添加宏、添加测试、简单安装、添加共享库、生成器表达式（实际构建时才确定值的变量）、导出 CMake 软件包
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH1
    CH1-Intruduction：概率图模型：构建图模型表示概率系统中的独立性和依赖性关系，依据图模型进行概率推导
- 2023-A Survey of Large Language Models: CH0(Abstract)-CH5
    CH0-Abstract：全文结构分为：LLM 的预训练、微调、使用、评估四部分
    CH1-Introduction：语言模型的发展：SLM-NLM-PLM-LLM
    CH2-Overview：语言模型的 scaling 和涌现
    CH3-Resources of LLMs：公开可用的模型权重和训练数据集
    CH4-Pre-Training：数据收集、清洗；模型架构；训练技巧
    CH5-Adaptation of LLMs：指令微调；对齐微调；参数高效微调；存储高效微调

### 9月
时间：2024.9.9-2024.9.16

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH1
    CH1-What is Combinactorics： 组合数学关于：离散结构的存在、计数、分析和优化
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH2
    CH2-Foundation： 条件独立、条件概率密度函数、MAP 查询、图论
- 2023-A Survey of Large Language Models: CH6
    CH6-Utilization：提示词技巧：上下文学习（即（输入-输出）对）、思维链（即（输入-推理步骤-输出）三元组）、规划

时间：2024.9.16-2024.9.23

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH2
    CH2-Permutations and Combinations: 集合的排列/组合（组合数=排列数+除法原理），多重集合的排列/组合（多重集排列数=集合排列数+除法原理；多重集组合数=线性等式的解集大小），古典概型
- 2004-Convex Optimization: CH2-CH2.5
    CH2-CH2.5-Convex Sets: 凸组合、仿射组合；典型的凸集；保凸运算；凸集的支撑/分离超平面
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH3-CH3.3
    CH3-CH3.3-The Baysian Network Representation: 贝叶斯网络：以图的语义表示联合分布中成立的条件独立性；根据网络结构，将联合分布分解为多个条件概率分布的乘积

时间：2024.9.23-2024.9.30

\[文献阅读\]
- A Survey of Large Language Models: CH7-CH8
    CH7-Capacity and Evaluation: LLM 能力：1. 基本能力：语言生成（包括代码）、知识利用（例如知识密集的 QA）、复杂分析（例如数学推理）；2. 高级能力：对齐、和外部环境交互（例如为具身智能体生成动作规划）、工具利用（例如根据任务调用合适 API）；对一些 benchmark 的介绍

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH5
    CH5-Local Probabilistic Models: 紧凑的条件概率分布表示：利用针对上下文的独立性；假设原因之间影响独立，基于此假设的模型有：噪声或、BN2O（多观测的噪声或）、广义线性模型（“分数”线性于所有原因变量）、条件线性高斯（实际上将联合分布建模为了混合高斯分布）

### 10月
时间：2024.9.30-2024.10.7

\[文献阅读\]
- 2023-A Survey of Large Language Models: CH8-CH9
    CH8-Applicatoin: LLM 在多个任务中的应用
    CH9-Conclusion and future directions
- 2010-Importance Sampling A Review
    重要性采样主要的目标就是减少 Monte Carlo 采样估计的方差
    适应性参数化重要性采样：$q (x)$ 定义为多元正态或学生分布，优化和变异系数相关的度量以得到较优的分布参数
    序列重要性采样：链式分解 $p (x)$，链式构造 $q (x)$
    退火重要性采样：顺序地近似 $p (x)$，和 diffusion 很相似

\[书籍阅读\]
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH6-CH6.2
    CH6-Template-based Representations: 时序模型；Markov 假设+ 2-TBN = DBN（动态贝叶斯网络）；DNB 通常建模为状态-观测模型（状态和观测分离考虑，观测不会影响状态），两个例子：隐 Markov 模型，线性动态网络（所有的依赖都是线性 Gaussian）
- 2024-面向计算机科学的组合数学: CH1.7, CH2
    CH1.7-生成全排列: 中介数和排列之间的一一对应关系

时间：2024.10.7-2024.10.14

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: CH0-CH3.1
    CH0-CH3.1: Abstract, Background, Algorithm 1; 算法1本质上就是 attention 计算的 tiled 实现，其中主要是 softmax 因子反复的重缩放让算法1看起来不那么直观，但是缩放计算对于数值稳定性还是关键的。算法1中，对于每个 query ，它的 attention 结果是随着外层循环逐渐累加的，在每一次外层循环，其已经累积的部分 attention 结果中的 value 权重都会动态调整/更新，同时加上新的部分 attention 结果

\[书籍阅读\]
- A Tour of C++: CH5
- 面向计算机科学的组合数学: CH2.1-CH2.3
    CH2-鸽巢原理: 鸽巢原理仅解决存在性问题
- Probabilistic Graphical Models-Principles and Techniques: CH4-CH4.3.1
    CH4-CH4.3.1: Markov 网络的参数化：思路来源于统计物理学，是很直观的“统计”，使用因子函数衡量两个变量/粒子的交互/亲和性，使用规范化的因子乘积表示联合分布（Gibbs 分布），用以描述特定“配置”出现的概率。Markov 网络中的分离准则是可靠且若完备的（可靠：网络中，某独立性存在 --> 任意分解于网络的分布中，该独立性存在；弱完备：网络中，某独立性不存在 --> 某个分解于网络的分布中，该独立性不存在） 

\[文档阅读\]
- ultralytics v8.3.6 : Quickstart, Usage (Python usage, Callbacks, Configuration, Simple Utilities, Advanced Customization)
    对 YOLO 的 Python API 的简要介绍

时间：2024.10.14-2024.10.21

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: Sec3.1-Sec5
    Sec3.1-IO Analysis: FlashAttention 的 IO 复杂度为 $\Theta(N^2d^2M^{-1})$ ，标准 Attention 算法的则为 $\Theta (Nd + N^2)$，主要的差别就在于其中的 $M$ 和 $N^2$。标准 Attention 算法完全不使用 SRAM，所有访存都是 global memory 访问，其中对于 “权重矩阵” $P\in \mathbb R^{N\times N}$ 贡献了大量的 IO 复杂度 ($N^2$)。FlashAttention 利用了 SRAM，并且完全不存储 $P$ 到 DRAM，而是在算法整个过程保持一块 $P$ 在片上，因此减少了 IO 复杂度。
    Sec3.2-Block sparse Flash-Attention: 块稀疏拓展的差异就在于限制了“注意”的范围，故计算时可以跳过被 mask 的项，进而减少了访存和计算。
    Sec4-Experiments: FlashAttention 训练更快； FlashAttention 更内存高效（线性于序列长度），故允许更长的训练上下文。其原因在于 FlashAttention 不会一次直接计算整个 "权重矩阵" $P\in \mathbb R^{N\times N}$ ，而是在循环中逐行计算，实际上是以时间换空间， FLOP 提高了，但额外内存使用限制在了 $O (N)$ 而不是 $O (N^2)$ 。FLOP 提高带来的额外计算时间则被更少的 DRAM 访问抵消。
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec0-Sec2
    Sec0-Summary: 该文提出了 HC 定理的一种证明，进而强调了建模空间交互时，条件概率模型实际是优于联合概率模型的。
    Sec1-Sec2: 对于正分布，条件概率可以用于推导整个系统的联合概率，这依赖于 HC 定理。
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.3.1-CH4.4.2
    CH4.3.1-CH4.4.2: Markov 网络编码了三类独立性：成对独立性、局部独立性（Markov 毯）、全局独立性（d-seperation）。对于正分布，三者等价，对于非正分布（存在确定关系的分布）三者不等价，这是因为 Markov 网络的语义无法表示确定性关系。根据 HC 定理，正分布 $P$ 分解于 Markov network $\mathcal H$ 等价于 $P$ 满足 $\mathcal H$ 编码的三类独立性。
- 面向计算机科学的组合数学: CH3-CH3.3
    母函数：使用幂级数表示数列（数列由幂级数的系数构造）
\[文档阅读\]
- Pytorch 2.x: CH0
    CH0-General Introduction: `torch.compile` : TorchDynamo --> FX Graph in Torch IR --> AOTAutograd --> FX graph in Aten/Prims IR --> TorchInductor --> Triton code/OpenMP code...
- Triton: Vector Addition, Fused Softmax
    Triton 的编码思想基本和 CUDA 类似，Triton 最大的方便之处就在于把 CUDA 编程中最繁琐和困难的地址映射环节都封装在了 `tl.load` 中。

时间：2024.10.21-2024.10.28

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: SecA-SecC
    SecA-Related Work
    SecB-Algorithm Details: Memory-efficient forward/backward pass: 用 for 循环避免存储 $O(N^2)$ 的中间矩阵； FlashAttention backward pass: 反向算法反而相较于正向算法更加简洁，因为反向算法完全就是关于 tiled 矩阵乘，而不需要像正向算法频繁的 rescale
    SecC-Proofs
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec3
    Sec3-Markov Fields and the Harmmersly-Clifford Theorem: 证明思路：定义 ground state -> 定义 $Q$ 函数 -> 展开 $Q$ 函数 -> 证明 $Q$ 函数中的项 ($G$ 函数) 仅在相关的变量构成一个团时才非空
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.5
    CH4.5-Bayesian Networks and Markov Networks: 仅有弦图可以同时被两种结构表示且不丢失信息

