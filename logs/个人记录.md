# 2024 年
## 九月
### 第一周
时间：2024.9.2-2024.9.9

\[文献阅读\]
- 2023-NeurIPS-Are Emergent Abilities of Large Language Models a Mirage?
    研究者对于度量的选取造就了 LLM 具有涌现能力的“海市蜃楼” 

\[书籍阅读\]
- 2015-Mastering CMake: CH2-CH15、CH17
    CH2-Getting Started on Your Computer：CMake 构建流程纵览
    CH3-Writing CMakeLists Files：CMake 语言纵览
    CH4-CMake Cache：CMake 缓存机制介绍：`CMakeCache.txt`
    CH5-Key Concepts：CMake 概念介绍：源文件、目标文件、属性
    CH6-Policies：CMake 策略机制：为了兼容性
    CH7-Modules：CMake 模块：CMake 提供的 utility
    CH8-Installing Files：`Install()` 命令
    CH9-System Inspections：借助宏编写跨平台软件；`try_run/compile()` 命令
    CH10-Finding Packages：借助 CMake 分发的软件依赖包
    CH11-Custom Commands：为自定义目标添加自定义构建规则
    CH12-Packing with CPack：借助 CPack 调用本地打包工具
    CH13-Testing with CMake and CTest：`add_test()`
    CH14-CMake Tutorial：纵览：简单构建、添加库、添加属性、通过系统审查添加宏、添加测试、简单安装、添加共享库、生成器表达式（实际构建时才确定值的变量）、导出 CMake 软件包
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH1
    CH1-Intruduction：概率图模型：构建图模型表示概率系统中的独立性和依赖性关系，依据图模型进行概率推导
- 2023-A Survey of Large Language Models: CH0(Abstract)-CH5
    CH0-Abstract：全文结构分为：LLM 的预训练、微调、使用、评估四部分
    CH1-Introduction：语言模型的发展：SLM-NLM-PLM-LLM
    CH2-Overview：语言模型的 scaling 和涌现
    CH3-Resources of LLMs：公开可用的模型权重和训练数据集
    CH4-Pre-Training：数据收集、清洗；模型架构；训练技巧
    CH5-Adaptation of LLMs：指令微调；对齐微调；参数高效微调；存储高效微调

### 第二周
时间：2024.9.9-2024.9.16

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH1
    CH1-What is Combinactorics： 组合数学关于：离散结构的存在、计数、分析和优化
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH2
    CH2-Foundation： 条件独立、条件概率密度函数、MAP 查询、图论
- 2023-A Survey of Large Language Models: CH6
    CH6-Utilization：提示词技巧：上下文学习（即（输入-输出）对）、思维链（即（输入-推理步骤-输出）三元组）、规划

### 第三周
时间：2024.9.16-2024.9.23

\[书籍阅读\]
- 2009-Introductory Combinactorics: CH2
    CH2-Permutations and Combinations: 集合的排列/组合（组合数=排列数+除法原理），多重集合的排列/组合（多重集排列数=集合排列数+除法原理；多重集组合数=线性等式的解集大小），古典概型
- 2004-Convex Optimization: CH2-CH2.5
    CH2-CH2.5-Convex Sets: 凸组合、仿射组合；典型的凸集；保凸运算；凸集的支撑/分离超平面
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH3-CH3.3
    Cjj3-CH3.3-The Bayesian Network Representation: 贝叶斯网络：以图的语义表示联合分布中成立的条件独立性；根据网络结构，将联合分布分解为多个条件概率分布的乘积

### 第四周
时间：2024.9.23-2024.9.30

\[文献阅读\]
- A Survey of Large Language Models: CH7-CH8
    CH7-Capacity and Evaluation: LLM 能力：1. 基本能力：语言生成（包括代码）、知识利用（例如知识密集的 QA）、复杂分析（例如数学推理）；2. 高级能力：对齐、和外部环境交互（例如为具身智能体生成动作规划）、工具利用（例如根据任务调用合适 API）；对一些 benchmark 的介绍

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH5
    CH5-Local Probabilistic Models: 紧凑的条件概率分布表示：利用针对上下文的独立性；假设原因之间影响独立，基于此假设的模型有：噪声或、BN2O（多观测的噪声或）、广义线性模型（“分数”线性于所有原因变量）、条件线性高斯（实际上将联合分布建模为了混合高斯分布）

## 十月
### 第一周
时间：2024.9.30-2024.10.7

\[文献阅读\]
- 2023-A Survey of Large Language Models: CH8-CH9
    CH8-Applicatoin: LLM 在多个任务中的应用
    CH9-Conclusion and future directions
- 2010-Importance Sampling A Review
    重要性采样主要的目标就是减少 Monte Carlo 采样估计的方差
    适应性参数化重要性采样：$q (x)$ 定义为多元正态或学生分布，优化和变异系数相关的度量以得到较优的分布参数
    序列重要性采样：链式分解 $p (x)$，链式构造 $q (x)$
    退火重要性采样：顺序地近似 $p (x)$，和 diffusion 很相似

\[书籍阅读\]
- 2009-Probabilistic Graphical Models-Principles and Techniques: CH6-CH6.2
    CH6-Template-based Representations: 时序模型；Markov 假设+ 2-TBN = DBN（动态贝叶斯网络）；DNB 通常建模为状态-观测模型（状态和观测分离考虑，观测不会影响状态），两个例子：隐 Markov 模型，线性动态网络（所有的依赖都是线性 Gaussian）
- 2024-面向计算机科学的组合数学: CH1.7, CH2
    CH1.7-生成全排列: 中介数和排列之间的一一对应关系

### 第二周
时间：2024.10.7-2024.10.14

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: CH0-CH3.1
    CH0-CH3.1: Abstract, Background, Algorithm 1; 算法1本质上就是 attention 计算的 tiled 实现，其中主要是 softmax 因子反复的重缩放让算法1看起来不那么直观，但是缩放计算对于数值稳定性还是关键的。算法1中，对于每个 query ，它的 attention 结果是随着外层循环逐渐累加的，在每一次外层循环，其已经累积的部分 attention 结果中的 value 权重都会动态调整/更新，同时加上新的部分 attention 结果

\[书籍阅读\]
- A Tour of C++: CH5
- 面向计算机科学的组合数学: CH2.1-CH2.3
    CH2-鸽巢原理: 鸽巢原理仅解决存在性问题
- Probabilistic Graphical Models-Principles and Techniques: CH4-CH4.3.1
    CH4-CH4.3.1: Markov 网络的参数化：思路来源于统计物理学，是很直观的“统计”，使用因子函数衡量两个变量/粒子的交互/亲和性，使用规范化的因子乘积表示联合分布（Gibbs 分布），用以描述特定“配置”出现的概率。Markov 网络中的分离准则是可靠且若完备的（可靠：网络中，某独立性存在 --> 任意分解于网络的分布中，该独立性存在；弱完备：网络中，某独立性不存在 --> 某个分解于网络的分布中，该独立性不存在） 

\[文档阅读\]
- ultralytics v8.3.6 : Quickstart, Usage (Python usage, Callbacks, Configuration, Simple Utilities, Advanced Customization)
    对 YOLO 的 Python API 的简要介绍

### 第三周
时间：2024.10.14-2024.10.21

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: Sec3.1-Sec5
    Sec3.1-IO Analysis: FlashAttention 的 IO 复杂度为 $\Theta(N^2d^2M^{-1})$ ，标准 Attention 算法的则为 $\Theta (Nd + N^2)$，主要的差别就在于其中的 $M$ 和 $N^2$。标准 Attention 算法完全不使用 SRAM，所有访存都是 global memory 访问，其中对于 “权重矩阵” $P\in \mathbb R^{N\times N}$ 贡献了大量的 IO 复杂度 ($N^2$)。FlashAttention 利用了 SRAM，并且完全不存储 $P$ 到 DRAM，而是在算法整个过程保持一块 $P$ 在片上，因此减少了 IO 复杂度。
    Sec3.2-Block sparse Flash-Attention: 块稀疏拓展的差异就在于限制了“注意”的范围，故计算时可以跳过被 mask 的项，进而减少了访存和计算。
    Sec4-Experiments: FlashAttention 训练更快； FlashAttention 更内存高效（线性于序列长度），故允许更长的训练上下文。其原因在于 FlashAttention 不会一次直接计算整个 "权重矩阵" $P\in \mathbb R^{N\times N}$ ，而是在循环中逐行计算，实际上是以时间换空间， FLOP 提高了，但额外内存使用限制在了 $O (N)$ 而不是 $O (N^2)$ 。FLOP 提高带来的额外计算时间则被更少的 DRAM 访问抵消。
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec0-Sec2
    Sec0-Summary: 该文提出了 HC 定理的一种证明，进而强调了建模空间交互时，条件概率模型实际是优于联合概率模型的。
    Sec1-Sec2: 对于正分布，条件概率可以用于推导整个系统的联合概率，这依赖于 HC 定理。
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.3.1-CH4.4.2
    CH4.3.1-CH4.4.2: Markov 网络编码了三类独立性：成对独立性、局部独立性（Markov 毯）、全局独立性（d-seperation）。对于正分布，三者等价，对于非正分布（存在确定关系的分布）三者不等价，这是因为 Markov 网络的语义无法表示确定性关系。根据 HC 定理，正分布 $P$ 分解于 Markov network $\mathcal H$ 等价于 $P$ 满足 $\mathcal H$ 编码的三类独立性。
- 面向计算机科学的组合数学: CH3-CH3.3
    母函数：使用幂级数表示数列（数列由幂级数的系数构造）
\[文档阅读\]
- Pytorch 2.x: CH0
    CH0-General Introduction: `torch.compile` : TorchDynamo --> FX Graph in Torch IR --> AOTAutograd --> FX graph in Aten/Prims IR --> TorchInductor --> Triton code/OpenMP code...
- Triton: Vector Addition, Fused Softmax
    Triton 的编码思想基本和 CUDA 类似，Triton 最大的方便之处就在于把 CUDA 编程中最繁琐和困难的地址映射环节都封装在了 `tl.load` 中。

### 第四周
时间：2024.10.21-2024.10.28

\[文献阅读\]
- 2022-NeurIPS-FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness: SecA-SecC
    SecA-Related Work
    SecB-Algorithm Details: Memory-efficient forward/backward pass: 用 for 循环避免存储 $O(N^2)$ 的中间矩阵； FlashAttention backward pass: 反向算法反而相较于正向算法更加简洁，因为反向算法完全就是关于 tiled 矩阵乘，而不需要像正向算法频繁的 rescale
    SecC-Proofs
- 1974-Spatial Interaction and the Statistical Analysis of Lattice Systems: Sec3
    Sec3-Markov Fields and the Harmmersly-Clifford Theorem: 证明思路：定义 ground state -> 定义 $Q$ 函数 -> 展开 $Q$ 函数 -> 证明 $Q$ 函数中的项 ($G$ 函数) 仅在相关的变量构成一个团时才非空
\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.5
    CH4.5-Bayesian Networks and Markov Networks: 仅有弦图可以同时被两种结构表示且不丢失信息

## 十一月
### 第一周
时间：2024.10.28-2024.11.4

\[文献阅读\]
- 2024-ICLR-FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning
    FlashAttention-2: 
    (1) 微调算法，减少非 matmul 运算: 移除了内层循环中对于 softmax weight 每次的一个 rescale 步骤，仅在内层循环结束后进行
    (2) 提高算法在 thread blocks 中的并行以提高 occupancy: 交换了内外层循环，使得外层迭代之间相互独立，故通过将 $\mathbf O$ 块分配给不同的 thread blocks 提高并行，本质就是并行化了外层循环
    (3) 在 warps 之间划分工作减少 shared memory 访问: 将 $\mathbf Q$ block 划分给 warps ，保持 $\mathbf {K, V}$ blocks 完整，其思想也是使得 warps 各自的负责的 $\mathbf O$ 块相互独立，故减少了最后累加归约所需要的 shared memory 读写
    在处理迭代式单 token decoding 时，FlashAttention-2 用 thread blocks 并行 laod KV cache 减少了 loading 时间

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH4.6.1
    CH4.6.1-Conditional Random Fields: CRF 建模条件概率分布，其图模型是部分有向图。其优势在于灵活性，CRF 允许我们用 Markov 网络的因子分解语义表示条件分布 (比条件贝叶斯网络需要显式指定 CPD 要灵活)，但相应的模型的解释性降低，学习到的参数没有特别明确的语义
- 面向计算机科学的组合数学: CH4-CH4.4.1
    令母函数中的系数为递推式的通项来将二者联系，进而将递推式写为关于母函数的方程，解方程得到母函数，就可以直接得到通项

\[文档阅读\]
- pytorch-tutorials-beginner: Learn the Basics
- pillow v11.0.0: Overview, Tutorial, Concepts
- Triton: Tutorials: Matrix Multiply
- argparse tutorial

### 第二周
时间：2024.11.4-2024.11.11

\[文献阅读\]
- 2023-SOSP-Efficient Memory Management for Large Language Model Serving with PagedAttention: Sec0-Sec4.5
    Sec0-Abstract
    Sec1-Introduction: 
        现存系统为 KV cache 预分配连续的显存，大小直接有请求的最大长度决定，该分配方式导致显存中存在大量内部和外部碎片，并且该分配方式不支持请求间共享 KV cache 的物理显存
        PagedAttention 以块为单位管理 KV cache，以块为粒度分配物理显存，进而减少了内部碎片并消除了外部碎片，同时以块为粒度支持了请求之间的 KV cache 共享
        PagedAttention 大幅提高了显存利用率，进而显著提高了请求的 batch size，进而提高了吞吐
    Sec2-Background:
        Prompt phase: 处理用户输入，生成 KV cache，使用矩阵-矩阵乘 (使用 masked self-attention 实现 causal attention)
        Auto regressive generation phase: 迭代式生成新 token，使用向量-矩阵乘
    Sec3-Memory Challenges in LLM Serving:
        LLM 服务受显存限制，尤其受 KV cache 所需要的空间限制
        朴素的连续显存分配策略导致 KV cache 存储空间的利用率很低

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH7, CH9.2-CH9.3
    CH7.1-Multivariate Gaussians: 
        高斯分布的两类参数化：标准、信息矩阵
        联合高斯的边际和条件密度也是高斯，同时条件高斯还满足线性高斯模型
        联合高斯中，协方差矩阵 $\Sigma$ 中的 0 项直接意味着统计上的独立性，信息矩阵 $J$ 中的 0 项意味着 Markov 条件独立性
    CH7.2-Gaussian Bayesian Networks:
        所有 CPDs 都是线性高斯模型的联合密度就是高斯
    CH7.3-Gaussian Markov Random Fields:
        任意高斯的信息表示可以直接和一个 pairwise Markov 网络关联，该网络带有二次的节点和边势能
        相反则不一定成立，要求 $J$ 正定，高斯分布才有定义，但存在一些充分条件，例如：对角主导、成对可规范化
    CH7.4-Summary:
        多元高斯可以被 Markov 和 Bayesian 网络表示
        高斯在表示上紧凑，计算上可解，对于复杂问题，也可以假设先验为高斯，或者将推理过程近似使得中间结果为高斯，以令计算过程可解
    CH9.2-'Variable Elimination: The Basic Ideas':
        变量消除的基本思想即动态规划：为了从 CPD $P (X\mid Y)$ 计算边际 $P (X)$，首先计算 $Y$ 的边际并存储，然后通过 $P (X) = \sum_y P (y) P (X\mid y)$ 计算，这比避免了为每个 $x\in Val (X)$ 重新计算 $Y$ 的边际
    CH9.3-Variable Elimination:
        将联合分布视作因子的乘积，为了计算一组变量上的边际，求和消去其他变量
        该过程可以总结为和-积变量消除算法，该算法的计算可以通过利用因子作用域有限的性质进行简化，即仅孤立出相关的因子进行求和
        要处理 evicdence，首先使用 evidence 简化所有因子 (留下和 evidence 相容的因子)，然后对简化的因子集合执行算法即可

### 第三周
时间：2024.11.11-2024.11.18

\[文献阅读\]
- 2023-SOSP-Efficient Memory Management for Large Language Model Serving with PagedAttention: Sec4.6-Sec10
    Sec4-Method: 
        Sec4.1-PagedAttention:
            PagedAttention 将序列的 KV cache 划分为块，每个块包含一定数量 tokens 对应的 KV cache
            KV 块在物理内存中可以不连续，在解码计算中，PagedAttention 按块读取所有需要的 KV 块执行计算
            KV cache 的块式管理使得我们在 vLLM 中可以使用灵活的内存管理方法
        Sec4.2-KV Cache Manager:
            vLLM 中，一个请求的 KV cache 会被划分为连续的逻辑块，逻辑块分别映射到不连续的物理块，KV 块从左到右被填充
            GPU worker 上的 block engine 分配连续的 DRAM，然后将连续的 DRAM 按照块划分
            每个 request 的逻辑块到物理块的映射信息由 block manager 维护在 block table 中
            块式的 KV cache 管理使得 vLLM 可以按照块的粒度动态增长用于存储 KV cache 的显存需求，而不是一次性预留大量可能不必要的位置
        Sec4.3-Decoding with PagedAttention and vLLM
            prefill 阶段中，vLLM 首先为 prompt 分配物理块，然后使用常规 self-attention 算法计算 prompt 的 KV cache，并生成第一个 token。在自回归解码阶段中，vLLM 使用 PagedAttention 计算每个 query token 的 KV cache，并逐个生成新 token。新的 KV cache 将被填充到块中的空 slot 中，若块填满，vLLM 创建新的逻辑块并分配相应的物理块
            全局上看，在每一次解码迭代中，vLLM 会选取一组候选序列进行批处理
            block size 更大使得 vLLM 可以并行处理更多位置的 KV cache，但内部碎片也会随之增大
        Sec4.4-Application to Other Decoding Scenarios
            Parallel sampling: 
            并行采样即 LLM 为单个请求生成多个序列，因此请求的 prompt 的 (大多数) KV cache 就可以共享，也就是这些序列的 prompt 部分的逻辑块会被映射到相同的物理块。共享数量由物理块的 reference count 计数。当新 token 需要被写入逻辑块时，如果其对应物理块 reference count 大于 1，vLLM 采用写时拷贝机制，将物理块内容拷贝到新的物理块，并相应减少原物理块的 reference count
            Beam search:
            beam search 在每个迭代从 $k\cdot |V|$ 个候选中保留 top-k 个序列
            beam candidates 共享初始 prompt 的 KV 块，如果它们来自相同的前缀，则会共享更多。共享模式会随着解码过程动态变化，一般的模式是在某个点分离，在之后的某个点收敛
            在之前的 LLM 服务系统中，对于分离的 beam candidates 的收敛行为需要拷贝大量的 KV cache，vLLM 中则可以直接共享对应的物理块，拷贝发生在分离块的写时拷贝过程，开销因此也限制在了一个块的大小
            Shared prefix:
            共享 system prompt 的 KV 块，其对应的物理块还可以预先缓存
            Mixed decoding methods:
            vLLM 支持同时用多个解码算法处理请求，因为 vLLM 使用逻辑块到物理块的映射层隐藏了具体的内存共享模式，故执行 kernel 仅需要接受物理块 ID 列表，不需要显式管理序列的内存共享模式
        Sec4.5-Scheduling and preemption:
            vLLM 采用先来先服务，对于被抢占序列则实行全部驱逐的策略
            恢复方法包括交换和重计算，注意重计算的延迟一般显著低于解码时的延迟，因为此时所有需要的 tokens 都已经知道
        Sec4.6-Distributed Execution:
            vLLM 使用 Megatron-LM 的并行策略，attention 算子在 attention head 维度划分
            对于每个 request，每个模型碎片处理的 token 序列是相同的，因此 KV cache 的物理块 ID 在 GPU worker 之间共享，但每个 GPU worker 上的物理块实际仅存储对应 attention head 那部分的 KV cache
            在执行的每一步中，调度器将每个序列 tokens 的 ID 和 block table 广播给 GPU worker，GPU worker 不需要在显存管理上同步，因为所有的显存管理信息在每次解码迭代的开始就被广播给了它们
    Sec5-Implementation:
        PagedAttention 使用三个 kernel 优化显存访问模式，包括
        fused reshape and block write kernel，将每个 transformer 层中新计算的 KV cache 划分为块，reshape 后根据 block table 的指定位置将其存储
        fused block read and attention kernel，根据 block table 读取 KV cache，计算 attention，每个 KV block 的读取由一个 warp 完成，保证 global memory 访问是合并的
        fused  block copy kernel，批处理多个块的写时拷贝操作
        PagedAttention 使用 `fork/append/free` 实现解码算法，`fork`  从现存序列创建新序列，`append` 将新 token 附加到现有序列， `free` 释放完成的序列
    Sec6-Evaluation:
        请求到达时间由不同请求率下的 Possion 分布生成
        度量系统在不同请求率下的规范化延迟，等于 mean(每个请求的端到端延迟除以其 token 数量)
        随着请求率上升，规范化延迟开始逐渐上升，超过处理能力后猛增，vLLM 可以显著提高该门槛，因为它节约了大量 KV cache 需要占用的显存，因此系统可以维持在较高的请求率下，故可以批处理更多请求
        在 compute-bound 的场景下 (序列较短，空间较充足)，vLLM 优势较小
        不同解码场景下的 block 共享也很高效，且随着 parallel samples 的数量或beam width 或 length of prefix 增长，效率也随之增长
    Sec7-Abalation Study:
        PagedAttention kernel 包含了访问 block table，执行额外分支和处理变长序列的额外开销，故 kernel latency 更高，memory-saving 仍然使得端到端表现更好
        block size 较小会导致难以高效并行读取和处理 KV cache，较大则内部碎片更大且共享机会更少，Block size = 16 是一个较好的 tradeoff
        关于恢复方法，重计算的开销和 block size 无关，因为它不涉及数据传输，交换方法则在 block size 更大时更高效
    Sec8-Discussion:
        块式的内存管理机制对于 KV cache 管理是高效的，原因在于 LLM 不能提前知道输出序列的长度，因此需要高效的动态内存分配，块式管理可以最小化这种情况下的内存碎片
    Sec9-Related Work:
        Orca 中的迭代级别调度和 vLLM 中的 PagedAttention 是互补的技术，Orca 通过调度和重叠请求提高请求处理的并行度，vLLM 提高了显存利用率，以容纳更大的工作集
        vLLM 通过减少内存碎片和进行内存共享提高了请求处理的并行度
    Sec10-Conclusion:
        PagedAttention 以块的粒度管理 KV cache，减少了内存碎片并进行了内存共享，vLLM 基于 PagedAttention, PagedAttention 算法展示了像虚拟内存和写时拷贝这样成熟的技术是如何可以被用于高效管理 LLM 服务中的 KVcache 和处理多种解码算法的

\[书籍阅读\]
- 面向计算机科学的组合数学: CH4.4.1-CH4.5.1
    直接根据递推式写下特征多项式 -> 解特征方程得到根 -> 根据根写下带有待定系数的通项 -> 根据数列初始值解出通项 -> 得到通项公式

\[文档阅读\]
- CUDA C++ Programming Guide v12.6: CH2
- docker/get-started: What is Docker, Docker Concepts

### 第四周
时间：2024.11.18-2024.11.25

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH10.1-CH10.3, CH11.1-CH11.3.4
    CH10-Exect Inference: Clique Trees
        CH10.1-Variable Elimination and Clique Trees
            因子 $\psi_i$ 接受因子 $\psi_j$ 生成的消息 $\tau_j$，向其他因子发送消息 $\tau_i$
            簇图中，每个节点/簇和一组变量关联，两个簇在关联的变量存在交集时相连
            变量消除算法的执行过程定义了一个簇图：执行过程中用到的每个因子 $\psi_i$ 对应一个簇 $\pmb C_i$，如果执行过程中因子 $\psi_i$ 向因子 $\psi_j$ 发送了消息 $\tau_i$，则在簇图中连接 $\pmb C_i, \pmb C_j$
            变量消除算法定义的簇图实际是一颗树 (每个因子在算法中仅时使用一次，故在图中仅会有一个父节点)，该树还满足运行相交性质，故是一颗团树
            团树的运行相交性质暗示了独立性 (Theorem 10.2)
        CH10.2-Message Passing: Sum Product
            单个团树可以用于多次的变量消除执行，故考虑在团树中缓存部分计算结果 (以团为单位存储团上的边际分布)，可以使得多次的执行更加高效
            团树本身定义了可以在其中的因子执行的计算 (求和消去哪些变量)，并定义了这些计算的偏序结构，因此团树指导了变量消除的执行
            团树消息传递算法中，每个团的初始势能为其所有相关因子之积，每个团计算消息时，首先乘上所有传入消息，然后求和消去其作用域中除分离集以外的变量，该过程逐渐向上直到根团。根团最后得到其作用域上未规范化的边际 (等于未规范化的联合分布求和消去所有其他变量)，称该边际为信念 (Colloary 10.1)
            直观上， $\pmb C_i , \pmb C_j$ 之间的消息是 $\mathcal F_{\prec i(\rightarrow j)}$ 中所有因子的乘积在 $\pmb C_i$ 和 $\pmb C_j$ 之间边际化的结果 (Theorem 10.3)
            在团树消息传递算法的多次执行中，两个团之间发送的同向的消息完全一样，故本质上团树中的每条边仅关联两条消息 (一个方向一条)
            和积信念传播算法利用了该性质，算法包括 upward pass 和 downward pass，两次 pass 之后，每条边都得到自己相关的两条消息，因此每个团的信念可以被直接计算得到，故该算法在计算全部的团信念时很高效 (Colloary 10.2)，同时，该算法也校准了树种的全部因子
            和边关联的 sepset 上的信念就是和边关联的两条消息之积
            团树上的未规范化联合分布等于全部团的信念之积除去全部分离集的信念之积 (Proposition 10.3)，故团和分离集信念提供了一种重参数化，该性质被称为团树不变性
        CH10.3-Message Passing: Belief Update
            和-积-除算法中，消息传递过程是以信念更新的形式实现的，执行中每条边维护上次传递的消息，防止消息回传，即新消息要经过该边传递时，需要除去边上的上一条消息，避免旧消息的信息用于更新发送该消息的团
            该算法收敛时同样得到校准的树，因为收敛情况下，消息更新不再有效，即 $\sigma_{i\rightarrow j} = \mu_{i, j} = \sigma_{j\rightarrow i}$ 对于所有 $i, j$ 成立，意味着相邻团在 sepset 上的边际达成一致
            可以证明和积消息传递算法和信念更新算法 (和-积-除算法) 等价
            信念更新算法的执行过程中，团树不变性在每一次消息传递后也仍然保持 (Corollary 10.3)
            增量更新：将分布乘上一个新因子。在团树中的实现是将新因子乘入某个相关的团，然后执行一次 pass 以更新树中其他相关的团
            超过单个团范围的 query：在包含 query 的子树中构造包含 query 的边际，然后消元求得 query
            多 query：用动态规划思想计算每个 clique pair 的边际
    CH11-Inference as Optimization
        CH11.1-Introduction:
            近似方法的近似性来自于对目标分布 $P_\Phi$ 构建近似分布，近似分布形式更简单，一般会采用局部分解结构
            推理任务需要在近似分布上进行，故目标任务被转变为在一类近似分布 $\mathcal Q$ 上优化目标函数，找到最优近似分布 $Q$，因此目标任务属于约束优化问题
            近似方法主要分为三类：
            1. 在非团树结构中使用团树消息传递算法，例如环路信念传播算法。这类方法可以理解为优化能量泛函的近似形式
            2. 在团树结构中使用团树消息传递算法，其中消息是近似消息，该类算法称为期望传播算法，它在松弛的约束下最大化准确形式的能量泛函
            3. 推广平均场方法，使用准确形式的能量泛函
            准确推理问题也可以被重构为搜索问题，搜索接近 $P_\Phi$ 的校准分布
            近似分布和原分布之间的 KL 散度 $D(Q|| P_\Phi)$ 可以被重写为关于能量泛函的形式，最小化 KL 散度等价于最大化能量泛函 $F[\tilde P_\Phi, Q]$ (Theorem 11.2).
            对于任意的 $Q$，能量泛函都是划分函数 $Z$ 的对数的下界，即 $\ln Z \ge F[\tilde P_\Phi, Q]$，等号成立当且仅当 $D(Q||P_\Phi) = 0$
            本章讨论的近似推理方法都可以被视作优化能量泛函的策略，这些方法都属于变分方法
        CH11.2-Exact Inference as Optimization
            给定和一组信念 $Q$ 相关的簇树，我们可以将能量泛函 $F[\tilde P_\Phi, Q]$ 分解为关于分离集信念和簇信念的形式，之后便可以在信念集合的空间中定义约束优化问题 'CTree-Optimize' ，其目标是最大化分解形式的能量泛函，其约束保证了边际一致性 (即信念是校准的)
            使用拉格朗日乘子法可以最优解 $Q^*$ 中的信念应该服从的形式/方程(Theorem 11.3)
            进一步求解的方法可以为迭代求解，即迭代式将方程的 RHS 赋值给 LHS 直到收敛，在特定情况下 (例如簇树实际上是团树)，该方法的收敛性质有保证，因此在团树中，该方法等价于信念更新/和积消息传递算法
            簇图的运行相交性质的定义比簇树中的定义要松弛，但仍可以避免‘循环论证‘，但图中的回路仍然会导致一定程度的循环推理
            簇图中，边上的集合不一定是分离集，而是分离集的子集，或者说边上的集合不再保证是节点的作用域的交集
            虽然在簇图中进行优化最后得到的信念不保证是 $P_\Phi$ 的边际，但簇图仍维持和簇树一样的不变性质，因此簇图也可以视作对未规范化的原分布 $P_\Phi$ 的重参数化
            簇图中的信念实际上是分布 $P_{\mathcal T}$ 的边际，其中 $\mathcal T$ 是图中的一个子树，该性质称为树一致性
            关于构造簇图，对于成对 Markov 网络，我们可以为每个势能引入一个簇，在作用域相交的簇之间添加边，环状信念传播最初就基于该构造
            对于更复杂的网络，可以使用 Bethe 簇图构造方法
- 面向计算机科学的组合数学: CH5.1-CH5.2
\[文档阅读\]
- python/pep/PEP 8-Style Guide for Python Code

## 十二月
### 第一周
时间：2024.11.25-2024.12.2

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH11.5.1, CH12.1-CH12.3
    CH11-Inference as Optimization
        CH11.5-Structured Variational Inference
            CH11.5.1-The Mean Field Approximation
                平均场近似假设所有变量相互独立，故 $Q(\mathcal X)$ 可以被完全分解
                能量泛函的优化形式为迭代式优化 (坐标上升)；在每次迭代，我们仅优化 $Q_i(X_i)$ ，其他变量的边际固定；迭代式坐标上升算法保证收敛，因为能量泛函是有界的并且在坐标上升过程保证是不减的
                对 $Q_i(X_i)$ 的最优解的计算仅涉及到包含了变量 $X_i$ 的势能
    CH12-Partical-Based Approximation Inference
        本章讨论蒙特卡洛方法，即如何从目标分布或者其近似分布采样，以及如何根据这些样本为所要的期望构建估计器
        CH12.1-Forward Sampling
            前向采样利用了贝叶斯网络的分解定理，它根据 BN 的偏序对变量进行采样，因此，在父变量的采样值确定的情况下，每个变量的采样过程仅和其 CPD 有关
            要从后验中采样，一个简单的方法是拒绝采样，其拒绝采样得到的样本中和证据不一致的样本
            拒绝采样的问题在于如果证据本身出现的概率太低，许多样本会被拒绝，故采样的效率会过低
        CH12.2-Likelihood Weighting and Importance Sampling
            为了解决拒绝采样的低效问题，似然加权算法直接在采样过程中将观测变量的值设定为观察值，因此不需要再进行拒绝
            但是，直接将证据变量的值设定为观察值时，证据变量和其他变量的采样值之间的相关性会丢失；为了弥补这一点，似然采样算法为每个样本赋予一个权重，样本的权重即该样本中其他变量的采样值下证据出现的概率
            重要性采样从另一个提案分布中采样，并且相应地为样本加权，以保证我们计算的期望是想要的期望
            重要性采样在 $Q(\pmb X) \propto |f(\pmb X)|P(\pmb X)$ 时达到最低的方差
            规范化的重要性采样假定我们只能访问未规范化的分布 $\tilde P$，此时我们仍可以通过同时估计规范化参数 $Z$，来构建对某个相对于 $P$ 的期望的估计器
            规范化的重要性采样不是无偏的，其偏差和方差随着样本数量 $M$ 的倒数 $\frac 1 M$ 下降，也就是 $M$ 越大偏差和方差越低
            实践中，规范化重要性采样估计器的方差一般比未规范化的重要性采样估计器更低 (没有理论保证)，方差的降低往往比偏差更重要，因此即便在 $P$ 已知的情况下，也常常使用规范化的重要性采样估计器
            一组特定样本的有效样本大小依据方差来定义，即 $M_{\text{eff}}$ 个来自于 $P$ 的样本的方差等于来自于 $Q$ 的 $M$ 个样本
            在 BN 中，当重要性采样的提案分布 $Q$ 是由残缺化网络定义的时，重要性采样等价于似然加权算法
        CH12.3-Markov Chain Monte Carlo Methods
            在似然加权算法中，证据仅会影响证据变量的后代变量的采样过程，其非后代实际上还是从先验中采样，而不是后验，如果先验和后验之间的差异过大，权重将不足以弥补这一点
            MCMC 方法采用和加权方法完全不同的模式，MCMC 方法启发自对物理现象的观察，即一个粒子的状态演化过程是一个 Markov 链，随着其粒子的状态在 Markov 链上演化，粒子的状态分布会逐渐收敛到一个稳态分布
            MCMC 方法定义了一个 Markov 链，其稳态分布是所要的采样分布 $P$ (例如后验分布)，然后让从初始分布 (例如先验分布) 生成的样本随着 Markov 链对其赋值/状态进行演化；因为 Markov 链的状态分布会最终收敛到其稳态分布，故我们可以最终将该样本视作从所想要的分布中采样得到；在该过程中，样本的分布会逐渐靠近稳态分布 
            为了保证 Markov 链有唯一的稳态分布，其状态空间应该是各态遍历的 (即转移矩阵的所有项都应该为正)
            Gibbs 采样算法是 MCMC 方法的一种实现，它为每个变量构建一个分离的转移模型 (其转移模型就是 $P$ 中在给定所有其他变量的当前采样值的情况下当前变量的后验分布)，然后将各个变量的转移模型结合为 Markov 链的转移模型，可以证明该构造会让 Markov 链收敛到所想要的分布 $P$ 
            如果 Markov 链 $\mathcal T$ 相对于某个分布 $\pi$ 满足细致平衡方程，则 $\mathcal T$ 是可逆的，并且 $\pi$ 是它的稳态分布，如果 $\mathcal T$ 是规范的，则 $\pi$ 就是唯一的稳态分布
            MH 算法是构建稳态分布为 $P$ 的 Markov 链的通用方法，它使用提案分布 $Q$ 作为转移模型的一部分，并根据 $P$ 和 $Q$ 定义接受概率，提案分布和接受概率一起定义了 Markov 链的转移模型，可以证明该 Markov 链的稳态分布是 $P$ 
- 一份（不太）简短的 LaTex2e 介绍: CH1-CH2
    CH1-LaTeX 的基本概念
        LaTeX 命令分为两种：`\` + 一串字母；`\` + 单个非字母符号
        字母形式的命令忽略其后的空格字符
        LaTeX 的环境由 `\begin,\end` 命令包围
        LaTeX 用 `{}` 划分分组，限制命令的作用范围
        `\documentclass` 指定文档类，`\begin{document}` 开启文档环境，二者之间为导言区，用于用 `\usepackage` 使用宏包
        `\include, \input` 用于插入文件
    CH2-用 LaTeX 排版文字
        UTF-8 是对 Unicode 字符集的一种编码方式
        XeTeX 和 LuaTeX 完全支持 UTF-8，`fontspec` 宏包用于调节字体
        `ctex` 宏包和文档类 (`ctexart, ctexbook, ctexrep` ) 用于支持中文排版
        LaTeX 将空格和 Tab 视作空白字符，连续的空白视作一个空白，行首的空白会被忽略
        连续两个换行符生成一个空行，将文字分段 (等价于 `\par` )，连续空行视作一个空行
        LaTeX 会自动在合适位置断行断页，也可以手动用命令控制
\[文档阅读\]
- python/pep/PEP 257–Docstring Conventions

### 第二周
时间：2024.12.2-2024.12.9

\[书籍阅读\]
- Probabilistic Graphical Models-Principles and Techniques: CH17.1-CH17.3
    CH17-Parameter Estimation
        参数估计的两个主要方法是极大似然估计 (MLE) 和贝叶斯方法
        CH17.1-Maximum Likelihood Estimation
            假设空间包含了我们考虑的全部可能假设，目标函数用于度量假设空间中不同的假设和数据集的相关性有多好
            一个假设的 “优秀” 程度可以理解该假设可以将观测数据预测得有多好，如果在该假设下更有可能观测到给定的数据，则该假设就是一个好的假设
            似然函数是相对于参数 $\theta$ 和数据集 $\mathcal D$ 的函数，它度量了给定参数 $\theta$ 下数据集 $\mathcal D$ 出现的后验概率
            似然值更高的参数值更有可能生成观测到的数据，能够最大化似然的参数值就称为极大似然估计
            对数似然函数和似然函数单调性相同，因此最大化似然函数等价于最大化对数似然函数
            置信区间度量了我们关于我们的估计的信心程度
            充分统计量是关于数据的函数，它总结了数据中用于计算似然的相关信息，对于一个统计量，如果给定相同参数，两个数据集的该统计量相同意味着它们的似然相同，则该统计量就是充分统计量
            如果两个参数对于所有可能的数据集选择 $\mathcal D$ 的似然都相同，则称二者是不可区分的
        CH17.2-MLE for Bayesian Networks
            贝叶斯网络中，似然函数可以根据网络结构分解为各个 CPD 的局部似然函数的乘积，每个 CPD 的局部似然函数还可以进一步根据父变量的取值分解 (这里隐含的假设是各个 CPD 的参数值相互独立)
            该性质称为似然函数的可分解性，当各个 CPD 都有一个不相交的独立参数集合参数化时，该性质就成立
            如果似然函数的可分解性成立，则全局的 MLE 就等价于各个 CPD 的局部 MLE 的结合，全局的优化问题就分解为多个独立的子优化问题的结合
            MLE 的其中一个问题和数据划分现象有关，随着父变量集合的维度增长，数据集将会被划分为大量的小数据集，同时参数数量也指数增长，用于估计一个参数的样本数量将很少，这是限制从数据中学习 BN 的关键原因
        CH17.3-Bayesian Parameter Estimation
            和 MLE 相比，贝叶斯统计额外考虑了先验
            我们将关于参数 $\theta$ 的先验知识编码为一个概率分布，即先验分布，进而在观测到的数据和参数上构建联合分布，该联合分布可以由一个元网络表示
            我们可以将该联合分布分解为先验和似然的乘积，则参数的后验就和似然和先验的乘积成比例
            对下一个参数的预测可以被写为其条件概率和参数的后验概率的乘积在所有可能参数值 $\theta$ 上的积分
            对于均匀先验，预测 (贝叶斯估计) 的形式和 MLE 相似，差异在于贝叶斯估计为各个情况额外添加了一个“虚拟”的样本，贝叶斯估计和 MLE 随着样本数增长会收敛到同一值
            均匀先验下的贝叶斯估计称为拉普拉斯修正
            另一种常见先验是 Beta 分布，Beta 由两个超参数参数化，两个超参数对应于 “虚拟” 的样本数量
            如果先验是 Beta 分布，且似然函数是 Bernoulli 似然函数，则后验分布也将是 Beta 分布，其超参数由观测修正，故称 Beta 分布共轭于 Bernoulli 似然函数
            随着我们获得更多数据，先验的效果将逐渐减弱
            贝叶斯框架允许我们用先验分布表示先验知识，同时使用后验的尖峰程度区分少样本的情况和多样本的情况
            贝叶斯方法中，我们将参数视作随机变量，使用概率描述关于参数的不确定性，然后使用贝叶斯规则将观测纳入考虑，衡量观测如何影响我们关于参数的信念
            我们能否紧凑表示后验取决于先验的形式
            Dirichlet 先验和多项式模型共轭，即如果似然函数形式为多项式分布，先验为 Dirichlet 先验，则后验也是 Dirichlet 分布
            此时的估计也和 MLE 形式类似，差异在于为计数添加了超参数，故 Dirichlet 超参数也称为伪计数
            伪计数的和反映了关于先验的信心，称为等价样本大小
            此时的估计可以被视作先验值和 MLE 估计的加权均值，因此已知贝叶斯估计随着样本数量增大收敛到 MLE 估计，直观上看，大数据集使得先验的贡献可以忽略，但小数据集下估计会朝先验偏置
            先验的存在使得贝叶斯估计相较于 MLE 更稳定，这种平滑效应使得数据不足时的估计更健壮，如果先验知识不足，可以使用均匀先验，避免估计取极限值

\[文档阅读\]
- docker/get-started/Docker Concepts

### 第三周
时间：2024.12.9-2024.12.16

\[Paper\]
- 2020-TDPS-The Deep Learning Compiler A Comprehensive Survey: Sec1-Sec3
    Sec1-Introduction
        ONNX defines a unified format to represent DL models
        DL hardware can be divided into three categories: 1. general-purpose 2. dedicated 3. neuromorphic
        DL compiler aims to alleviate the burden of optimizing DL models on each DL hardware manually. DL compiler includes TVM, Tensor Comprehension, nGraph, Glow, XLA.
        DL compiler takes model description in DL frameworks as input and output the optimized code implementation of this model for DL hardware. The optimization is specific to model specification and hardware architecture.
        DL compiler also adopts layered design, including frontend, IR, backend, but the IR has multiple levels.
    Sec2-Background
        Deep Learning Frameworks
        TensorFlow employs a dataflow graph of primitive operators extended with restricted control edges.
        Keras is TensorFlow's frontend, written in pure Python.
        PyTorch embeds primitives for constructing dynamic dataflow graph in Python, where the control flow is executed by the Python interpreter.
        FastAI is PyTorch's frontend.
        ONNX defines a scalable computation graph model.
        Deep Learning Hardware
        TPU includes Matrix Multiplier Unit, Unified Buffer, and Activation Unit. MXU mainly consists of a systolic array. TPU is programmable but use matrix as primitive instead of vector or scalar.
        Hardware-specific DL Code Generator
        FPGA lies between CPUs/GPUs and ASIC. HLS programming model provides C/C++ programming interface to program FPGA. However, the DL model is usually described by the languages of DL framework instead of bare C/C++. Thus mapping DL models to FPGA remains a complicated work.
        The hardware-specific code generator targeting FPGA take DL model as input, output HLS or Verilog/VHDL. Based on the generated architecture of FPGA-based accelerators, the code generator can be classified as the processor architecture specific or the streaming architecture specific.
        The processor architecture FPGA accelerator comprises of several Processing Units, which are comprised of on-chip buffer and multiple smaller Processing Engines. The DL code generator targeting this architecture adopt hardware templates to generate the accelerator design automatically. The number of PUs and the number of PEs per PU are important template parameter. Tiling size and batch size are also essential scheduling parameters about mapping DL models to PUs and PEs. All these parameters are usually determined by design space exploration.
        The streaming architecture FPGA accelerator comprises of multiple different hardware blocks, and usually have one block for each layer of the input Dl model. All hardware block can be utilized in a pipeline manner with streaming input data.
    Sec3-Common Design Architectures of DL Compilers
        DL model will be translated into multi-level IRs by the DL compiler, where the high level IR resides on the frontend and the low-level IR resides on the backend. The high-level IR is associated with hardware-independent optimizations and transformations, and the low-level IR is associated with the hardware-specific optimizations and transformations.
        The high-level IR is also known as the graph IR, which represents hardware-independent computation and control flow. It establishes the control flow and the dependency between the operators and the data. It provides an interface for graph-level optimization.
        Low-level IR is fine-grained enough to reflect the hardware characteristics. Low-level IR should allow the usage of third-party tool-chains in compiler backends.
        The frontend takes DL model as input and output graph IR. The optimization on graph IR can be classified into: node-level, block-level, dataflow-level. The optimized computation graph will be passed to the backend.
        The backend takes graph IR as input and output low-level IR. The backend can directedly convert graph IR into third party toolchains' IR like LLVM IR for general purpose code generation and optimization. The backend can also use customized compilation pass to do better. The commonly-applied hardware-specific optimizations include hardware intrinsic mapping, memory allocation and fetching, memory latency hiding, parallelization, loop oriented optimization.
        Existing backend uses auto-scheduling and auto-tuning to determine the optimal parameter setting.
        Low-level IR can be compiler JIT or AOT.
\[Book\]
- Probabilistic Graphical Models-Principles and Techniques: CH17.4, CH19.1-CH19.2, CH20.1-CH20.3
    CH17-Parameter Estimation
        CH17.4-Bayesian Parameter Estimation in Bayesian Networks
            Global parameter independence: each CPDs' parameter's prior is independent from each other. Thus the prior of the whole parameter has a fully decomposed form.
            If global parameter independence holds, then complete data d-separates each CPDs' parameter, which in turn indicates that the posterior of the whole parameter had a fully decomposed form.
            According to Bayes rule, the parameter's posterior can be rewritten as the product of the likelihood function and the prior divided by the marginal probability of the data set. The likelihood function can be decomposed into the product of local likelihoods and the prior can be decomposed into the product of local priors (with global parameter independence holds). Therefore the posterior also has a fully decomposed form. The fully decomposed form can also be directly derived from the meta-Bayesian network.
            To do prediction, we need to integrate over all legal parameter values to calculate the posterior probability. If the data is IID, and the global parameter independence holds, then the calculation can be factorized into a product of local integration associated with each CPD.
            Therefore, what we need to do is solve the local Bayesian estimation problem independently and combine them into the global one.
            Local parameter independence indicates that the local prior can be further factorized according to the parent variables' assignment.
            If the CPDs are not multinominal CPDs, we may not have a conjugate prior or a closed-form integral for the Bayesian integral. When a full Bayesian solution is impractical, we may resort to maximum a posterior estimation. If we have a large amount of data, the posterior is often sharply peaked around its maximum, therefore in this case the Bayesian integral is roughly equivalent to the MAP estimation.
            MAP estimation can also be viewed as provide regularization over the likelihood function. The regularization term's effect will diminish with the increase of the number of samples.
            MAP estimation can be used in practice, because we will usually choose a well formed prior.
    CH19-Partially Observed Data
        CH19.1-Foundations
            To analyze the probabilistic model of the observed training set, we must consider not only the data-generation mechanism, but also the mechanism by which data are hidden. Every observation is derived by the combination of the two mechanisms.
            If the outcome variables and the observation variables are marginally independent, then we say the data missing model is missing completely at random. In this situation, the whole likelihood can be decomposed as the product of the likelihood of the outcome variables and the likelihood of the observations variables. We can maximize the likelihood of interest independently.
            Given the observed outcome variables, if the hidden outcome variables and the observation variables are conditionally independent, then we say the data missing model is missing at random. In this situation, we can also decompose the likelihood, and use only the observed variables to optimize the parameters of the outcome distribution.
            However, in general, the likelihood function of the observation is a sum of likelihood function of the observation with all possible hidden assignments, each of which defines a unimodal function. Thus the likelihood function with incomplete data is a multimodal function and takes the form of "a mixture of peaks".
            Thus the likelihood function is not decomposable again, and will be hard to optimize.
        CH19.2-Parameter Estimation
        CH20-Learning Undirected Models

### 第四周
时间：2024.12.16-2024.12.23-2024.12.30

\[Paper\]
- 2020-TDPS-The Deep Learning Compiler A Comprehensive Survey: Sec4-Sec7
    Sec4-Key Components of DL Compilers
        Sec4.1-High-level IR
            High-level IR is also known as graph IR.
            4.1.1 Representation of Graph IR
            The representation of graph IR can be categorized into two classes: DAG-based IR, Let-binding-based IR.
            In DAG-based IR, the node represents atomic DL operator, the edge represents tensor.
            In DAG-based IR, the graph is acyclic without loops, therefore differs from the data dependency graph of generic compilers.
            There are already plenty of optimizations on DDG, like Common Subexpression Elimination, Dead Code Elimination. These algorithm can be combined with DL domain knowledge to optimize the DAG computation graph.
            DAG-based IR is simple, but may cause semantic ambiguity because of missing the definition of the computation scope.
            Let-binding offers let-expression to certain functions with restricted scope in high-level language to solve semantic ambiguity. 
            When using `let` keyword to define expression, a let node will be generated which points to the operator and the variable in the expression, instead of just building the computational relation between variables in DAG.
            In DAG-based compiler, to get the return value of certain expression, the corresponding node will the accessed and the related nodes will be searched. It is known as recursive descent technique.
            The let-binding based compiler will compute the results of all variables in a let expression, and builds a variable map. The compiler looks up this map to decide the value of the expression.
            TVM and Relay IR adopts both.
            The ways graph IR to represent tensor computation can be categorized into three classes: Function-based, Lambda expression, Einstein notation
            Glow, nGraph, XLA's IR (XLA's IR is HLO) use function-based representation to represent tensor computation. The function-based representation only provides encapsulated operators.
            Lambda expression uses variable binding and substitution to represent calculation. TVM uses tensor expression to represent tensor computation, in which the computational operator are defined by the output tensor's shape and the lambda expression of computing rules.
            Einstein notation is used to expression summation, in which the indexes for temporary variables do not need to be defined. The actual expression can be deduced from the Einstein notation. In Einstein notation, the operators should be associative and commutive, and thus the reduction operators can be executed by any order.
            4.1.2 Implementation of Graph IR
            Data representation
            Tensor can be represented by placeholder which only carries the shape information of the tensor. It helps separate the computation definition and actual computing. To support dynamic shape/dynamic model, placeholder should support unknown dimension size. Also, the bound inference and dimension checking should be relaxed, and extra mechanism is needed to guarantee memory validity.
            Data layout describes tensor's organization in memory, which is usually a mapping from logical indices to memory indices. The data layout includes the sequence of dimensions, tilling, padding, striding, etc.
            Bound inference is used to determine the bound of iterators when compiling DL models. The bound inference is often performed iteratively of recursively according to the computation graph and placeholders.
            Operators supported
            Operators supported by DL compilers will be the node in the computation graph.
            4.1.3 Discussion
            The data and operators designed in high-level IR are flexible and extensible enough to support diverse DL models. The high-level IRs are hardware-independent.
        Sec4.2-Low-level IR
            Low-level IR provides interface to tune the computation and memory access. The common implementation of low-level IR can be classified into 3 categories: Halide-based IR, polyhedral-based IR, other unique IR.
            Halide-based IR
            Halide's philosophy is to separate computation and schedule. Compilers adopting Halide try various possible schedules and choose the best one. TVM improved Halide-IR to independent symbolic IR.
            Polyhedral-based IR
            Different from Halide, the boundaries of memory bounds and loop nests can be polyhedrons with any shapes in the polyhedral model. The polyhedral-based IR makes it easy to apply polyhedral transformations (fusion, tiling, sinking, mapping).
            Other unique IR
            MLIR has a flexible type system and allows multiple abstraction levels. It induces dialects to represent multiple levels of abstraction. Each dialect consists of a set of defined immutable operations. Current dialects includes TensorFlow IR, XLA HLO IR, experimental polyhedral IR, LLVM IR, TensorFlow Lite.
            Most DL compiler's low-level IR will eventually lowered to LLVM IR to use LLVM's optimizer and code generator. LLVM also supports custom instruction set for specialized accelerator.
            DL compiler adopts two approaches to achieve hardware-dependent optimization: 1. perform target-specific loop transformation in the upper IR of LLVM 2. provide additional information about the hardware target for optimization passes.
        Sec4.3-Frontend optimizations
            Frontend optimizations are shared by different backends.
            The frontend optimizations are defined by passes. The passes traverse the graph's nodes and perform graph transformation. (rewrite the graph for optimization)
            Passes can be pre-defined or customized by developers. Most DL compliers can capture shape information in computation graph to do optimization.
            The frontend optimization can be classified into three categories: 1. node-level 2. block-level (local) 3. dataflow-level (global)
            Node-level optimizations
            The nodes of computation are coarse enough to enable optimizations inside a node.
            Node elimination: eliminate unnecessary nodes (e.g. operations lacking adequate inputs, zero-dim-tensor elimination)
            Node replacement: use lower-cost nodes
            Block-level optimizations
            Algebraic simplification: optimization in computation order, optimization in node combination, optimization of ReduceMean nodes
            Operator fusion: 
            Operator sinking: make similar operations closer in order to create opportunities for algebraic simplification 
            Dataflow-level optimizations
            CSE: use previously computed sub-expression's value to substitute other occurrences of that sub-expression in the graph
            DCE: a set of code is dead if it's computation result or side-effect are not used; DCE includes dead store elimination (remove never used tensor's storage operation)
            Static memory planning: done offline, aims to reuse memory as much as possible; in-place memory sharing: allocate only one copy for operation, for sharing between the input and output; standard memory sharing: reuse previous operations' memory without overlapping.
            Layout transformation: aims to find the best data layout for tensors in the computation and insert layout transformation node into the computation graph. The actual layout transformation is preformed by the backend. To find the best data layout, the hardware details are required, like cache line size, vectorization unit size, memory access pattern etc.
        Sec4.4-Backend optimizations
            Hardware-specific optimization
            includes: 
            1. hardware intrinsic mapping: transform a certain set of low-level IR instructions to highly optimized kernels in target hardware.
            2. memory allocation and fetching: 
            3. memory latency hiding: reorder the execution pipeline
            4. loop oriented optimizations: includes 1) loop fusion to fuse loops with the same boundaries 2) sliding windows 3) tiling: the tiling patter and size can be determined by auto-tuning 4) loop reordering (loop permutation): changes the order of iterations in a nested loop to increase spatial locality. It requires the loop is free of data-flow dependency between iterations 5) loop unrolling: usually applied in combination with loop split: first split the loop into two nested loops and unroll the inner loop
            5. Parallelization: utilizes accelerator's multi-thread and SIMD parallelism.
            Auto-tuning
            four key components:
            1. parameterization: the data parameter describes the data's specification; the target parameter describes hardware-specific characteristics (e.g.  shared memory and register size) and constraints
            2. cost model: 1) black-box model: only considers the final execution time 2) ML-based cost model: e.g. GBDT 3) pre-defined cost model
            3. searching technique: 
            4. acceleration: 1) parallelization 2) configuration reuse
            Optimized kernel libraries 
    Sec5-Taxonomy of DL Compilers
    Sec6-Evaluation
    Sec7-Conclusion and Future Directions

\[Book\]
- Probabilistic Graphical Models-Principles and Techniques: CH18.1-CH18.3
    18.1-Introduction
        In structure learning, we aims to recover $\mathcal G^*$ or its I-equivalence based on data. $\mathcal G^*$ is $P^*$ 's perfect map.
        The more edges our structure have, the more parameters we need to learn. Because of data fragmentation, the quality of estimated parameter will decrease if the number of samples is fixed. (Note that the standard deviation of MLE estimate if $1/\sqrt M$)
        Thus when doing density estimation from limited data, we prefer sparse structure even if the true structure $\mathcal G^*$ is more dense. Because we need to avoid overfitting.
        There are three methods for structure learning. 
        The first one is constraint-based structure learning, which tests the independence in data and find a network that best explains these independencies. This type of method is sensitive to failures in individual independencies test. If one of these tests return a wrong answer, the network construction will be misled.
        The second one is score-based structure learning. This method defines a hypothesis space of potential models and a score function that measures how well the model fits the observation. The task is to search the model that maximize the score in the hypothesis space. Score-based method consider the whole structure at once, thus is less sensitive to individual failures.
        The third method does not learn a single model but an ensemble of multiple possible structures.
    18.2-Constraint-Based Approaches
        Determining whether two variables are independent is often referred to as hypothesis testing.
    18.3-Structure Scores
        Score-based methods approach the problem of structure learning as an optimization problem.
        Intuitively, we need to find a model that would make the data as probable as possible. In this case, our model is pair $\langle \mathcal G,\pmb \theta_{\mathcal G}\rangle$. The likelihood score directly defines $\pmb \theta_{\mathcal G}$ to be its the MLE estimation $\hat {\pmb \theta}_{\mathcal G}$, and tries to find structure $\mathcal G$ that maximize $score_{L}(\mathcal G:\mathcal D) = \ell(\hat {\pmb \theta}_{\mathcal G}:\mathcal G)$.
        The likelihood score can decompose according to (18.4). We can observe that the likelihood measures the strength of the dependencies between variables and their parents. 
        For BN, the process of choosing a network structure is often subject to constraints. Some constraints are a consequence of the acyclicity requirement, others may be due to a preference for simpler structures.
        Because the property of mutual information, adding edge to a network will never decrease its likelihood score. Thus likelihood score will result in fully connected network in most cases. Therefore, the likelihood score can not avoid overfitting.
        The Bayesian method put a distribution on possible structures $\mathcal G$ and is proportional to the posterior $P(\mathcal G\mid \mathcal D)$. The Bayesian score is defined as $score_B(\mathcal G: \mathcal D) = \log P(\mathcal D\mid \mathcal G) + \log P(\mathcal G)$.
        The calculation of marginal likelihood $P(\mathcal D\mid \mathcal G)$ need us to integrate the whole parameter space $\Theta_{\mathcal G}$. Therefore, we are measuring the expected likelihood, averaged over different possible choices of $\pmb \theta_{\mathcal G}$ decreasing the sensitivity of the likelihood to the particular choice of parameters. 
        Another perspective to explain the Bayesian score is derived from the holdout testing methods. The Bayesian score can be viewed as a form of prequential analysis, where each instance is evaluated in incremental order, and contributes both to our evaluation of the model and to our final model score. The sequence order can be arbitrary. According to (18.8), the marginal likelihood can be approximately viewed as the estimation of the model' expected likelihood in the underlying distribution.
        If the parameter's priors are in conjugate case, the marginal likelihood of a single variable's form can be easily written. As a consequence, the marginal likelihood of the dataset can be further written simpler according to (18.9).
        The Bayesian score for BN cane be decomposed under the assumption of parameter independence. The the local independence is also satisfied, (18.9) can be applied to substitute the local terms of the factorized Bayesian score.
        If $M\to \infty$, the $\log P(\mathcal D\mid \mathcal G)$ can be represented as Theorem 18.1. We can observe that the Bayesian score tends to trade off the likelihood (fit the data) and the model complexity. Omitting the constant term, we get the BIC score.
        The log-likelihood term increase linear to $M$, and the model complexity term increase log to $M$, therefore the emphasis on data fitting will increase as $M$.
        BIC score and the Bayesian score are consistent, which means with adequate data, the score will select $\mathcal G^*$. or its I-equivalence.
        Consistency is an asymptotic property, and thus it does not imply much about the properties of networks learned with limited amounts of data.
        We call the prior satisfies parameter modularity if two structure's local structure are the same, their prior will be the same
        Under parameter modularity, Bayesian score will be decomposable, and thus the searching can be done locally and separately.
        The likelihood score is naturally decomposable.
- 面向计算机科学的组合数学: CH1.1-CH1.6, CH2.1-CH2.2, CH3.1-CH3.2, CH7
    CH1-排列组合
    CH2-鸽巢原理
    CH3-母函数
    CH7-Polya 计数理论

\[Doc\]
- mlir/Toy Tutorial: CH1-CH2
    CH1-Toy Language and AST
        unranked tensor parameter: the dimension is unknown, and will be specialized at call sites
    CH2-Emitting Basic MLIR
        There is no closed set of attributes, operations, types in MLIR.
        MLIR's extensibility is supported by dialects, which groups operations, attributes, types under the the same abstraction level.
        MLIR's core computation and abstraction unit are operations, which can be used to represent all core IR structures in LLVM like instructions, globals, modules.
        Operation's results and arguments are SSA values.
        Operation's name is prefixed with dialect's name.
        Operation can have zero or multiple attributes.
        Concepts to model an operation includes: name, SSA arguments, attributes, result values' types, source location, successor blocks, regions.
        Note that every operation has an associated mandatory source location. The debug info in MLIR is core requirement.
        All IR elements (refer to the concepts that model an operation) can be customized in MLIR.
        In C++, a dialect is implemented as a derived class of `mlir::Dialect`. It's attributes, operations, types are registered by an initializer method called by the constructor.
        Using tablegen to declaratively define a dialect is more simple.
        `MLIRContext` only loads builtin dialects by default. Customized dialect should be passed to template method `loadDialect` to be explicitly loaded.
        In C++, an operation is defined as a derived class of `mlir::Op` using CRTP. CRTP means that `mlir::Op` is a template class, whose template argument is the operation class (its derived class). By CRTP, `mlir::Op` can know its derived class in compilation, and thus can safely use `static_cast` to invoke derived class's method to achieve polymorphism in compilation.
        `mlir::Op` can take optional traits as template arguments to represent operation's properties and invariants.
        Operation class can also define its method to provide additional verification beyond the attached traits.
        Operation class should define static `build` method in order to be invoked by the `bulider` class to generate this operation's instance from a set of input values.
        Operation's `build` method should populate a `mlir::OperationState` with its possible discrete elements.
        After defining the operation class, we can invoke `addOperation` with its template argument being the operation class to register this operation into the dialect.
        `Operation` class is used to generally model all operations, and it does not describe the properties and types of a particular operation. Thus it is used as a generic API.
        Each specific operation is a derived class of `Op` .
        `Op` act as a smart pointer wrapper of `Operation*` . All the data of an operation is stored in the referenced `Operation` class. The `Op` class is an interface/wrapper to interact with `Op`, and thus is usually passed by-value.
        A `Operation*` can be `dyn_cast` to the corresponding `Op` . 
        Operations can also be defined by tablegen. 


