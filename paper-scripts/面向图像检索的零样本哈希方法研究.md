# 1 文献综述
## 1.1 研究背景与意义
在大规模图像检索任务中，哈希方法在近几年受到了广泛的关注。哈希方法的目标是通过训练，提取出能最大限度保持图像之间关联性的二进制特征。二进制特征的优点在于存储经济和检索快速：二进制特征的每一位仅需要1个比特就可以存储，相较于至少需要32位表示的单精度浮点数，二进制特征显然可以极大地减少存储空间开销；二进制特征之间的距离可以用汉明距离表示，计算汉明距离只需要执行逐位执行异或运算再将各位结果相加，而异或运算在现代计算机上都以硬件实现，因此二进制特征可以极大地提高检索效率。二进制特征的这两点优势表明了哈希方法的重要性。
而由于对大规模图像数据集进行全部标注的成本过于昂贵，近年来，零样本图像检索任务开始受到了广泛的关注。传统的机器学习任务中模型虽然在训练中未见到测试样本，但训练样本和测试样本是来自同一域的，而在零样本任务中，测试样本和训练样本来源于不同的域，一般称训练域为可见类，测试域为不可见类，零样本任务的目标就在于让模型从可见类中学习到通用的知识，可以在迁移到不可见类时也保持很好的泛化能力。在这个背景下，零样本图像检索的目标就是在包含了可见类的训练集上学习通用的检索函数，使得检索函数在迁移到包含了不可见类的测试集上也有较高的检索精度。
综合上述两种任务，面向图像检索的零样本哈希方法就是目标于在包含了可见类的训练集上学习通用的哈希函数，使得哈希函数在迁移到包含了不可见类的测试集上也有较高的检索精度。
面向图像检索的零样本哈希方法可以在保持经济效率（节约人工数据标注开销）、空间效率（节约存储空间）和时间效率（提高检索速度）的情况下，达到客观的图像检索精度，满足用户需求，促进大规模图像检索方法的发展，因此具有重要的研究意义。
## 1.2 国内外研究现状
面向图像检索的哈希方法相关工作，最早可以追溯到谱哈希方法[1]，谱哈希方法首次提出了高质量的哈希码应该满足的三个约束：即不相关约束和平衡约束，以及显然的离散约束，其中不相关约束要求不同的哈希比特之间应该相互独立，平衡约束要求哈希比特的每一位都有百分之五十的概率是零或者是一。原文中，作者聚焦无监督条件下的哈希问题，利用欧几里得距离计算样本之间的亲和度，提出了约束下的最小化平均汉明距离的目标函数。在三个约束都严格保持的情况下，原问题等价于一个NP难的图划分问题[2]。因此作者进行了连续松弛，丢弃了离散约束，将原问题转化为典型的谱分解问题，直接求解图拉普拉斯矩阵的特征向量，再进行阈值化，得到各位的哈希比特。因为在求解过程中完全丢弃了离散约束，该方法会有较大的量化误差，导致在样本数较多的时候方法的效果劣化。
有监督核哈希[3]聚焦有监督条件下的哈希问题。作者认为先前工作的昂贵训练开销来源于它们使用了过于复杂的哈希目标函数，因此作者首次利用了内积和汉明距离一一相对的关系提出了最小化F范数形式的优化框架，通过优化内积，等价地优化哈希空间中样本间的汉明距离。整体的优化过程分为两步，第一步中，作者同样进行了连续松弛，丢弃了离散约束，通过求解特征值问题得到每一位哈希比特的松弛解，第二步以第一步的松弛解作为起始点，采用了Sigmoid平滑方法，在原来的松弛解的基础上进行梯度下降，以进一步优化。作者采用了贪心算法，在固定其他哈希比特的情况下，逐位计算每一位哈希比特的解。该方法中，Sigmoid平滑方法缓解了连续松弛带来的量化误差，较大地提升了哈希准确度，但逐位求解方法存在效率较低的问题，且量化误差无法完全被消除。
离散图哈希[4]聚焦无监督条件下的哈希问题，在[1]的基础上作出了改进，重点解决连续松弛导致的量化误差问题，提出了基于图的无监督哈希模型。作者首先通过[5]中的锚点图方法构建邻接图，以邻接图的图拉普拉斯矩阵同样构造了特征值问题。与[1]中保持平衡约束和不相关约束，丢弃离散约束的求解方法不同的是，作者保持了离散约束，将平衡约束和不相关约束松弛为损失函数中的正则项，因此将原问题转化为了带正则项的离散优化问题，正则项的系数作为超参数，控制对平衡约束和不相关约束的松弛程度，作者为该离散优化问题提出了交替优化方法，可以直接求解出离散的哈希码矩阵$B$。该工作通过保持离散约束，解决了量化误差问题，但平衡约束和不相关约束被迁移到了正则项中，因此哈希码可以保持平衡约束和不相关约束到哪种程度，还需要更细致的研究。
强约束离散哈希[6]在[4]的基础上作出了改进。[4]在正则项中引入了哈希码的连续近似矩阵作为辅助变量，在迭代优化的第二步中求解能保持平衡约束和不相关约束同时和离散哈希码矩阵最相似的连续近似矩阵。在[6]中，作者强化了连续近似矩阵在优化的第一步中的作用，借此强化了对离散哈希码矩阵不相关性和平衡性。同时，作者将哈希函数显式设定为线性变换（或核线性变换），将哈希函数的学习嵌入了离散哈希码的学习框架内，可以在共同的学习框架中同时学习到训练集的哈希码矩阵和哈希函数。
基于非对称率相似度矩阵的零样本哈希将研究问题迁移到零样本任务上，[7]在[6]的基础上做出改进。作者提出，零样本设定下，相较于增强类别内部的内聚性，哈希函数应该向增强不同类别之间的区分性偏置，因此对训练集的相似度矩阵进行了不对称放缩，强化了优化过程中模型对不同类样本的区别能力，缓解了模型将不可见类样本误识别为可见类样本的错误，以促进模型在零样本场景下的检索分辨能力。
原型对齐和域感知的零样本哈希[8]在[6]的基础上添加了原型学习项，同时使用了域感知策略，通过判断样本的哈希码和各个可见类的原型哈希码的距离是否超过特定的阈值，来判断样本是属于可见类还是不可见类，通过排除检索集中的可见类样本，强化模型的零样本表现。
# 2 问题定义
零样本图像检索问题中，将数据集定义下的样本空间$\mathcal S$划分为两个不相交的子空间$\mathcal S_{seen}$和$\mathcal S_{unseen}$（$\mathcal S_{seen} \cup \mathcal S_{unseen} = \mathcal S,\mathcal S_{seen}\cap \mathcal S_{unseen} = \emptyset$），分别称为可见类空间和不可见类空间。可见类空间包含了所有的可见类样本，不可见类空间包含了所有的不可见类样本。
本文聚焦有监督下的零样本图像检索问题，目标是利用包含了来自于$c$个可见类的一共$n$个样本的训练集$\mathcal D_{train} = \{(\symbfit x_i, y_i)\}_{i=1}^n$训练哈希函数$h:\mathbb R^d \mapsto \{\pm 1\}^r$，其中$r$是预设的哈希比特数。
训练集$\mathcal D_{train}$中：$\symbfit x_i$为训练集样本$i$的$d$维特征向量，$\symbfit x_i\in \mathbb R^d$；$y_i$为训练集样本$i$的标量标签，$y_i\in \{1,2,\cdots, c\}$

哈希函数要求能够在标签的指导下，将样本特征转化为利于检索的哈希码，也就是最大限度地在二进制空间$\{\pm 1\}^r$中保留样本在标签空间$\mathbb R$中的语义相似度信息，即相似的样本（相同标签的样本）的哈希码之间的汉明距离尽量小，不相似的样本（不同标签的样本）的哈希码之间的汉明距离尽量大。同时，哈希函数能够具有一定的零样本迁移能力，在零样本任务中，面对属于不可见类的样本$\symbfit x' \in \mathcal S_{unseen}$，也能够有效将样本特征转化为利于检索的哈希码。
# 3 方法介绍
## 3.1 符号定义
定义包含了所有训练集样本特征向量的矩阵$\mathbf X = [\symbfit x_1, \symbfit x_2,\cdots, \symbfit x_n]^T\in \mathbb R^{n\times d}$，定义训练集样本$i$的标签向量$\symbfit y_i \in \{0,1\}^c$为样本$i$的标签$y_i$对应的独热编码向量，即$\symbfit y_i$仅在第$y_i$项为$1$，其余项都为$0$，由此定义包含了所有训练集样本标签向量的矩阵$\mathbf Y = [\symbfit y_1,\symbfit y_2,\cdots,\symbfit y_n]^T\in \mathbb R^{n\times n}$。
定义训练集样本之间的相似度矩阵为$\mathbf A = 2\mathbf Y\mathbf Y^T - \mathbf 1\mathbf 1^T \in\mathbb R^{n\times n}$，可以知道$\mathbf A$的第$i$行第$j$列的项$a_{ij} = 1(i,j=1,2,\cdots,n)$当且仅当样本$i$的标签和样本$j$的标签相同，即$y_i = y_j$，否则$a_{ij}=-1$。
定义包含了所有训练集样本哈希码的矩阵$\mathbf B = [\mathbf b_1,\mathbf b_2,\cdots, \mathbf b_n]^T\in \{\pm 1\}^{n\times r}$，其中$\mathbf b_i\in \{\pm 1\}^r$是样本$i$的$r$位哈希码。
## 3.2 子集方法介绍
在原型对齐和域感知的零样本哈希[8]中，作者为了提高模型对于不见类样本的查询精度，设计了域感知策略，即通过比较检索集中每个样本的哈希码与每个可见类的哈希中心的距离是否小于特定的阈值，判断检索集中的样本是否属于可见类，在查询时，将被认为是可见类的检索集样本与查询样本的汉明距离设为最大。这个方法可以有效地防止属于不可见类的查询样本检索到属于可见类的检索集样本，提高了零样本检索精度。
我认为，这个方法实质上等价于对检索集取了一个子集，查询样本实质上是在这个子集中进行检索，这个子集应该尽量排除属于可见类的样本，保持属于不可见类的样本，以避免不可见类查询检索到不相关的可见类样本。
由于哈希模型学习到的哈希码的值空间被限制在了$\{\pm 1\}^r$中，且哈希中心的计算可能受到离群值的影响而不稳定，通过将样本的哈希码与哈希中心的距离与预设定的阈值比较的办法有时不能有效地对不可见类样本与可见类域作出划分。因此，我提出对该方法进行改进。
在实际任务中，样本特征$\symbfit x_i$往往是通过在大规模图像数据集上预训练的CNN模型进行提取，而执行的预训练任务一般都是简单的图像分类任务，因此，提取到的图像特征是自然适合做分类任务的。而当目标选取出检索集中的不可见类样本，尽量排除可见类样本时，我们可以之间将它建模为一个多分类任务，该任务共有$c+1$个类别，其中$c$个类别是可见类别，$1$个类别代表所有的不可见类都视为同一类，我们在训练集上，直接在图像特征上建模并训练简单的多分类模型，就可以有效地筛选出检索集中的不可见类样本。
在实际实现中，因为训练集中只有可见类样本，没有有标签的不可见类样本可以作为负例，所以我运用了集成学习的思想，训练了$c$个二元逻辑斯蒂分类模型，其中每一个模型都只选择$c$个可见类中的一类可见类样本作为正例，其余所有样本作为负例，在训练完成后，我得到$c$个二元分类模型。
在检索集上运用这$c$个二元分类模型，对于每个样本，如果它被模型认为是正例的概率小于预设的阈值$t(0<t<1)$，则认为它是负例样本，由此可以得到$c$个判断为负例的样本集合，对这$c$个集合取交集，得到集合$\mathcal R$，它包含了$c$个二元分类模型都判断为负例的样本，这些样本就是从检索集中筛选出的认为是属于不可见类的样本。集合$\mathcal R$就是实际检索中采用的检索子集。
## 3.3 优化目标定义
通用的有监督下学习哈希码的方式是优化以下目标：
$$\begin{align}
&\min_{\mathbf B} \|r\cdot\mathbf A-\mathbf B\mathbf B^T\|_F^2\\
&s.t.\mathbf B\in\{\pm 1\}^{n\times r}
\end{align}\tag{2.1}$$
根据谱哈希[1]，高质量的哈希码应该保持平衡约束和不相关约束，分别写为$\mathbf 1\mathbf B = \mathbf 0$以及$\mathbf B^T\mathbf B = n\cdot\mathbf I_{r\times r}$，其中$\mathbf I_{r\times r}$是形状为$r\times r$的单位矩阵，因此优化目标可以进一步写为：
$$\begin{align}
&\min_{\mathbf B}\|r\cdot \mathbf A- \mathbf B\mathbf B^T\|_F^2\\
&s.t.\mathbf B\in\{\pm 1\}^{n\times r}, \mathbf 1\mathbf B = \mathbf 0, \mathbf B^T\mathbf B = n\cdot \mathbf I_{r\times r}
\end{align}\tag{2.2}$$
该问题是NP难的，因此，为了让问题易于求解，根据离散图哈希[4]，将平衡约束和不相关约束松弛，引入惩罚项，将优化问题写为：
$$\begin{align}
&\min_{\mathbf B,\mathbf C} \|r\cdot \mathbf A - \mathbf B\mathbf B^T\|_F^2 + \rho\|\mathbf B - \mathbf C\|_F^2\\
& s.t. \mathbf B\in\{\pm 1\}^{n\times r},\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{2.3}$$
其中$\mathbf C \in \mathbb R^{n\times r}$是引入的辅助变量，可以将其视作哈希码矩阵$\mathbf B$的连续近似，通过将对哈希码矩阵的平衡约束和不相关约束迁移到它的连续近似矩阵$\mathbf C$上，并添加惩罚了$\mathbf B$和$\mathbf C$之间距离的惩罚项，实现了对平衡约束和不相关约束的松弛，使得原问题更易于求解，而超参数$\rho$控制了约束的松弛程度，当$\rho$非常大时，问题（2.3）等价于问题（2.2）

为了进一步强化$\mathbf B$和$\mathbf C$之间的联系，强约束离散哈希[6]将问题（2.3）中的第一项中的$\mathbf B\mathbf B^T$替换为了$\mathbf B\mathbf C^T$，得到：
$$\begin{align}
&\min_{\mathbf B,\mathbf C} \|r\cdot \mathbf A - \mathbf B\mathbf C^T\|_F^2 + \rho\|\mathbf B - \mathbf C\|_F^2\\
& s.t. \mathbf B\in\{\pm 1\}^{n\times r},\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{2.4}$$

此时我们显式规定哈希函数的形式为线性映射，记为：
$$h(\mathbf X) = \mathbf X\mathbf P$$
其中$\mathbf P \in \mathbb R^{d\times r}$
实际实现中，为了能捕捉更多的非线性性质，我们使用核化的线性映射，定义$\Phi(\mathbf X) = [\Phi(\symbfit x_1),\Phi(\symbfit x_2),\cdots, \Phi(\symbfit x_n)]^T$，其中$\Phi : \mathbb R^d \mapsto \mathbb R^{d'}$是非线性映射，$d'$为映射后的维度，则哈希函数的形式记为：
$$h(\mathbf X) = \Phi(\mathbf X)\mathbf P$$

将哈希函数的学习嵌入哈希码的学习框架中，同时参考基于非对称率相似度矩阵的零样本哈希[7]，为目标引入标签信息，得到：
$$\begin{align}
&\min_{\mathbf B,\mathbf C,\mathbf W,\mathbf P} \|r\cdot \mathbf A - \mathbf B\mathbf C^T\|_F^2 + \rho\|\mathbf B - \mathbf C\|_F^2 \\
&+\alpha\|\Phi(\mathbf X)\mathbf P - \mathbf B\|_F^2 + \beta\|\mathbf P\|_F^2\\
&+\gamma\|\mathbf Y\mathbf W-\mathbf B\|_F^2 + \mu\|\mathbf W\|_F^2
\\
& s.t. \mathbf B\in\{\pm 1\}^{n\times r},\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{2.5}$$
其中$\mathbf W \in \mathbb R^{c\times r}$，$\alpha, \beta,\gamma, \mu$均为超参数

在2.2.1节中，我通过多个分类器从检索集中选出了子集$\mathcal R$，$\mathcal R$包含了在阈值$t$下检索集中被分类为不可见类的样本。为了进一步强化可见类域和不可见类域之间的区分性，我认为可以再添加一项对于可见类样本和不可见类样本之间的距离进行惩罚的损失项，以扩大可见类域和不可见类域的距离，强化哈希函数对于可见类域和不可见类域的辨识性。
我从集合$\mathcal R$中随机抽取$m$个样本，作为不可见类样本的代表，记包含了这$m$个样本的特征的矩阵为$\mathbf X' \in \mathbb R^{m\times d}$，定义训练集和这个$m$个样本之间的相似度矩阵$\mathbf A' \in \mathbb R^{n\times m}$，$\mathbf A'$中的每一项都是常数$a'(0\ge a' \ge -1)$，$a'$定义了训练集样本和这$m$个样本之间的不相似程度，最后在目标中引入项$\|(\Phi(\mathbf X')\mathbf P)\mathbf B^T - r\cdot \mathbf A'\|_F^2$，得到：
$$\begin{align}
&\min_{\mathbf B,\mathbf C,\mathbf W,\mathbf P} \|r\cdot \mathbf A - \mathbf B\mathbf C^T\|_F^2 + \rho\|\mathbf B - \mathbf C\|_F^2 \\
&+\alpha\|\Phi(\mathbf X)\mathbf P - \mathbf B\|_F^2 + \beta\|\mathbf P\|_F^2\\
&+\gamma\|\mathbf Y\mathbf W-\mathbf B\|_F^2 + \mu\|\mathbf W\|_F^2\\
&+\eta\|(\Phi(\mathbf X')\mathbf P)\mathbf B^T - r\cdot \mathbf A'\|_F^2
\\
& s.t. \mathbf B\in\{\pm 1\}^{n\times r},\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{2.6}$$
其中$\eta$是超参数

因为我们在松弛平衡约束和不相关约束时，将该约束迁移到了$\mathbf B$的连续近似矩阵$\mathbf C$上，因此在优化过程中，新加入的项$\|(\Phi(\mathbf X')\mathbf P)\mathbf B^T - r\cdot \mathbf A'\|_F^2$会使得对$\mathbf P$的优化变得困难，为此，我将$\|(\Phi(\mathbf X')\mathbf P)\mathbf B^T - r\cdot \mathbf A'\|_F^2$中的$\mathbf B^T$也替换为它的连续近似$\mathbf C^T$，以简化优化过程，得到最终的优化目标：
$$\begin{align}
&\min_{\mathbf B,\mathbf C,\mathbf W,\mathbf P} \|r\cdot \mathbf A - \mathbf B\mathbf C^T\|_F^2 + \rho\|\mathbf B - \mathbf C\|_F^2 \\
&+\alpha\|\Phi(\mathbf X)\mathbf P - \mathbf B\|_F^2 + \beta\|\mathbf P\|_F^2\\
&+\gamma\|\mathbf Y\mathbf W-\mathbf B\|_F^2 + \mu\|\mathbf W\|_F^2\\
&+\eta\|(\Phi(\mathbf X')\mathbf P)\mathbf C^T - r\cdot \mathbf A'\|_F^2
\\
& s.t. \mathbf B\in\{\pm 1\}^{n\times r},\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{2.7}$$
# 4 优化过程
问题（2.5）是一个非凸的优化问题，我参考基于非对称率相似度矩阵的零样本哈希[7]、强约束离散哈希[6]、离散图哈希[1]，定义交替的优化过程：

更新$\mathbf P$：
固定$\mathbf B,\mathbf C,\mathbf W$，得到相对于$\mathbf P$的目标：
$$\min_{\mathbf P}\alpha \|\Phi(\mathbf X)\mathbf P - \mathbf B\|_F^2 + \beta\|\mathbf P\|_F^2 +\eta\|(\Phi(\mathbf X')\mathbf P)\mathbf C^T-r\cdot \mathbf A'\|_F^2\tag{3.1}$$
我们定义关于$\mathbf P$的函数：
$$f(\mathbf P) = \alpha \|\Phi(\mathbf X)\mathbf P - \mathbf B\|_F^2 + \beta\|\mathbf P\|_F^2 +\eta\|(\Phi(\mathbf X')\mathbf P)\mathbf C^T-r\cdot \mathbf A'\|_F^2$$
计算$f(\mathbf P)$相对于$\mathbf P$的导数，得到：
$$\begin{align}
\nabla_{\mathbf P} f(\mathbf P) &= 2( \alpha\Phi(\mathbf X)^T\Phi(\mathbf X) \mathbf P - \alpha \Phi(\mathbf X)^T\mathbf B +\\
&\beta \mathbf P + r\eta \Phi(\mathbf X')^T\Phi(\mathbf X')\mathbf P - r\eta \Phi(\mathbf X')^T\mathbf A'\mathbf C)
\end{align}$$
令目标（3.1）相对于$\mathbf P$的导数为$0$，得到：
$$ (\alpha\Phi(\mathbf X)^T\Phi(\mathbf X)  +
\beta \mathbf I_{r\times r}+ r\eta \Phi(\mathbf X')^T\Phi(\mathbf X'))\mathbf P=\alpha \Phi(\mathbf X)^T\mathbf B + r\eta \Phi(\mathbf X')^T\mathbf A'\mathbf C$$
解得：
$$\mathbf P=(\alpha\Phi(\mathbf X)^T\Phi(\mathbf X)+r\eta\Phi(\mathbf X')^T\Phi(\mathbf X') + \beta \mathbf I_{r\times r})^{-1}(\alpha\Phi(\mathbf X)^T\mathbf B + r\eta \Phi'(\mathbf X)^T\mathbf A'\mathbf C)$$

更新$\mathbf W$：
固定$\mathbf P,\mathbf B,\mathbf C$，得到相对于$\mathbf W$的目标：
$$\min_{\mathbf W}\gamma\|\mathbf Y\mathbf W-\mathbf B\|_F^2+\mu\|\mathbf W\|_F^2\tag{3.2}$$
我们定义关于$\mathbf W$的函数：
$$f(\mathbf W) = \gamma\|\mathbf Y\mathbf W-\mathbf B\|_F^2+\mu\|\mathbf W\|_F^2$$
计算$f(\mathbf W)$关于$\mathbf W$的导数，得到：
$$\nabla_{\mathbf W}  f(\mathbf W) = 2( \gamma\mathbf Y^T\mathbf Y\mathbf W -
\gamma\mathbf Y^T\mathbf B+\mu \mathbf W )
$$
令目标（3.2）相对于$\mathbf W$的导数为$0$，得到：
$$\gamma(\mathbf Y^T\mathbf Y + \frac {\mu}{\gamma} \mathbf I_{n\times n})\mathbf W=\gamma \mathbf Y^T\mathbf B$$
解得：
$$\mathbf W = (\mathbf Y^T\mathbf Y + \frac {\mu}{\gamma}\mathbf I_{n\times n})^{-1}\mathbf Y^T\mathbf B$$

更新$\mathbf B$：
固定$\mathbf P,\mathbf W,\mathbf C$，得到相对于$\mathbf B$的目标：
$$\begin{align}
&\min_{\mathbf B} \|r\cdot \mathbf A-\mathbf B\mathbf C^T\|_F^2+\rho\|\mathbf B-\mathbf C\|_F^2+\alpha \|\Phi(\mathbf X)\mathbf P-\mathbf B\|_F^2+\gamma\|\mathbf Y\mathbf W-\mathbf B\|_F^2\\
&s.t. \mathbf B\in\{\pm1\}^{n\times r}
\end{align}\tag{3.3}$$
对目标（3.3）进行化简，将其重写为：
$$\begin{align}
&\min_{\mathbf B}\text{const}-2 Tr[r\mathbf B^T\mathbf A\mathbf C + \rho\mathbf B\mathbf C +  \alpha\mathbf B\Phi(\mathbf X)\mathbf P + \gamma\mathbf B\mathbf Y\mathbf W)]\\
&s.t. \mathbf B\in\{\pm1\}^{n\times r}
\end{align}\tag{3.3}$$
因此目标（3.3）等价于以下最大化问题：
$$
\begin{align}
&\max_{\mathbf B}Tr[\mathbf B^T(r\mathbf A\mathbf C + \rho\mathbf C +  \alpha\Phi(\mathbf X)\mathbf P + \gamma\mathbf Y\mathbf W)]\\
&s.t. \mathbf B\in\{\pm 1\}^{n\times r}
\end{align}\tag{3.4}$$
问题（3.4）的最优解是：
$$\mathbf B = sgn(r\mathbf A\mathbf C + \rho\mathbf C +  \alpha\Phi(\mathbf X)\mathbf P + \gamma\mathbf Y\mathbf W)$$
在这一步的更新中，因为$sgn(\cdot)$中的各项$r\mathbf A\mathbf C, \rho\mathbf C ,\alpha\Phi(\mathbf X)\mathbf P,\gamma\mathbf Y\mathbf W$的数量级差异较大，为了平衡这些项的数量级，参考基于非对称率相似度矩阵的零样本哈希[7]，对每个项进行标准化，具体可以写为以下形式：
$$std(\mathbf M) = \frac {\mathbf M - mean(\mathbf M)}{\delta}$$
其中$\mathbf M$是和式中的某一项，$mean(\cdot)$函数计算一个矩阵中所有值的平均值，$\delta$表示该矩阵中所有值的标准差

更新$\mathbf C$：
固定$\mathbf B,\mathbf P,\mathbf W$，得到相对于$\mathbf C$的目标：
$$\begin{align}
&\min_{\mathbf C} \|r\cdot \mathbf A - \mathbf B\mathbf C^T\|_F^2 + \rho\|\mathbf B - \mathbf C\|_F^2 \\
&+\eta\|(\Phi(\mathbf X')\mathbf P)\mathbf C^T - r\cdot \mathbf A'\|_F^2
\\
& s.t. \mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{3.5}$$
将目标（3.5）进行化简，将其重写为：
$$\begin{align}
&\min_{\mathbf C}\text{const}- Tr[r\mathbf C^T\mathbf A\mathbf B + r\eta \mathbf C^T\mathbf A'\Phi(\mathbf X') \mathbf P+\rho\mathbf C^T \mathbf B)]\\
& s.t. \mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{3.5}$$
因此目标（3.5）等价于以下最大化问题：
$$\begin{align}
&\max_{\mathbf C}Tr[\mathbf C^T(r\mathbf A\mathbf B + r\eta \mathbf A'\Phi(\mathbf X') \mathbf P+\rho \mathbf B)]\\
&s.t.\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{3.6}$$
我们令$\mathbf Q = r\mathbf A\mathbf B + r\eta \mathbf A'\Phi(\mathbf X') \mathbf P+\rho \mathbf B$，则目标（3.6）可以写为：
$$\begin{align}
&\max_{\mathbf C}Tr[\mathbf C^T\mathbf Q]\\
&s.t.\mathbf 1\mathbf C = \mathbf 0,\mathbf C^T\mathbf C = n\cdot \mathbf I_{r\times r}
\end{align}\tag{3.7}$$
参考离散图哈希[4]，问题（3.7）的最优解写为：
$$\mathbf C^* = \sqrt n[\mathbf U,\bar {\mathbf U}][\mathbf V,\bar{\mathbf V}]^T$$
$\mathbf U,\mathbf V$来自于$\mathbf J\mathbf Q$的奇异值分解，即$\mathbf J\mathbf Q = \mathbf U\mathbf \Sigma \mathbf V^T$，其中$\mathbf J = \mathbf I_{r\times r} - \frac 1 n \mathbf 1\mathbf 1^T$，而$\bar{\mathbf U}$和$\bar {\mathbf V}$则通过施密特正交化得到
# 5 实验设定
## 5.1 数据集介绍
AWA2（Animals with Attributes 2）：AWA1[9]是一个中等规模的粗粒度数据集，包含了关于动物的一共30475张图片，分为50个类别，有85个属性，但该数据集存在类别不平衡的问题，且文献[10]发现在该数据集的标准划分中，检索集中部分类别和ImageNet[11]中的训练类别存在重叠。文献[10]认为在零样本图像检索任务中，在大规模数据集（常常是ImageNet[11]）上预训练图像特征提取模型的过程也属于检索模型的训练过程，因此应该保证用于预训练的数据集的训练类别和检索集中的不可见类别不相交。基于该想法，文献[10]提出了AWA2数据集，数据集包含了和AWA1相同的50个类别，一共有37322张图片，相较于AWA1，该数据集分布较为平衡，且数据集的建议划分方式保证了检索集中的不可见类别和ImageNet[11]中的训练类别不相交。
APY（Attribute Pascal and Yahoo）：APY是一个小规模的粗粒度数据集，包含了15339张图片，共32个类别，有64个属性
## 5.2 数据集划分和特征提取
我使用文献[10]提供的预提取的特征，即使用了在ImageNet上预训练的ResNet101[14]提取出的图像特征。关于数据集可见类和不可见类的划分，我也遵循文献[10]的推荐划分方式。具体地说，对于AWA2，可见类别有40个，不可见类别有10个；对于APY，可见类别有20个，不可见类别有12个，划分保证不可见类别和ImageNet的训练类别不重叠。
我遵循文献[7]和文献[8]的训练集检索集的样本划分方式。具体地说，对于AWA2，我从每个可见类中随机抽取100个样本，构成共含有4000个样本的训练集，其余可见类样本和所有的不可见类样本构成检索集；对于APY，我从每个可见类中随机抽取50个样本，构成共含有1000个样本的训练集，其余可见类样本和所有的不可见类样本构成检索集。
在模型超参数选择时，我使用留出法进行了训练集验证集划分，在选择好超参数后，我不划分验证集，用完整的训练集训练模型，并在测试集上评估。
## 5.3 实验细节设定
我参照文献[7]，对相似度矩阵进行了非对称率放缩，即将相似度矩阵中的$+1$放大至原来的$u$倍，以强化模型的零样本检索表现。
在哈希函数的定义中，我使用了大多数工作常用的高斯核函数，将锚点数量设定为1000个，锚点从训练集样本中随机选取。核函数的带宽设定为60，这使得经过核函数映射后的样本特征空间中的数值处在较合理的范围内，如果带宽设定得太小，映射后的样本特征空间中的数值大部分会趋近于0，会严重损害样本之间的区分性。
其余超参数的设定，我参考了文献[7]进行初始设定，在此基础上利用验证集对这些超参数进行了部分调节。
关于从检索子集中采样出用于扩大域间差异的样本的数量 ，因为子集选取方法只能粗略地尽量保证子集中的样本属于不可见类，而无法提供样本是属于具体的哪一个不可见类的信息。因此 较大时，很可能采样得到的 个样本各自属于不同的不可见类，为了防止域间差异扩大的学习减弱不可见类域的域内差异，我将 设置为 ，也就是每轮迭代仅采样一个样本进行域间差异扩大。
具体地说，在AWA2上，我设定$\alpha = 0.1$，$\gamma = 0.001$，$\rho = 1$，$\beta = 1$，$\mu = 1$，$\eta = 0.001$，$a'=0$，$u = 10$，$t=0.3$
在APY上，我设定$\alpha = 0.1$，$\gamma = 0.001$，$\rho = 1$，$\beta = 1$，$\mu = 1$，$\eta = 0.001$，$a'=-0.01$，$u = 5$，$t=0.3$
其中阈值$t$的设定遵循的原则是尽量比较小，以保证筛选出的子集中尽量包含多的不可见类样本，同时不必太小，以保证子集中的样本数量要多于5000个，方便进行方法之间的比较。
# 6 实验结果及分析
## 6.1 检索精度指标分析
检索方法的评估通常使用平均精度均值mAP（mean Average Precision）指标，我遵循通常的设定，使用前5000个查询结果的平均精度均值作为模型检索效果的评估，即mAP@5000，我将我的方法与DGH、SCDH、ASZH、PDZSH进行了比较，比较结果见表1
表1中，其他方法的结果主要摘自原文献，如果原文献没有该数据集上的结果，则结果来自于我自己对该方法的复现。

表1

| 方法    | AWA2：24bit | AWA2：48bit | AWA2：64bit | AWA2：128bit |
| ----- | ---------- | ---------- | ---------- | ----------- |
| DGH   | 0.1291     | 0.2200     | 0.2474     | 0.3070      |
| SCDH  | 0.2124     | 0.2142     | 0.2754     | 0.2988      |
| ASZH  | 0.2619     | 0.3787     | 0.4032     | 0.4158      |
| PDZSH | 0.3175     | 0.3655     | 0.4189     | 0.4619      |
| 我的方法  | 0.3456     | 0.4425     | 0.4597     | 0.4924      |

| 方法    | APY：24bit | APY：48bit | APY：64bit | APY：128bit |
| ----- | --------- | --------- | --------- | ---------- |
| DGH   | 0.1227    | 0.1836    | 0.2003    | 0.2531     |
| SCDH  | 0.1663    | 0.1968    | 0.2222    | 0.2402     |
| ASZH  | 0.1608    | 0.2054    | 0.2241    | 0.2598     |
| PDZSH | 0.2404    | 0.2736    | 0.3044    | 0.3184     |
| 我的方法  | 0.1837    | 0.2243    | 0.2423    | 0.2718     |

从表格1可以看出，在AWA2数据集上，我的方法的效果要好于所比较的所有方法，在APY数据集上，PDZSH方法的效果最好，我的方法的效果也相对较好，优于ASZH、SCDH等方法，但是要差于PDZSH方法。实验结果说明我的方法是相对有效的。
## 6.2 消融实验
为了进一步探究方法的有效性的来源，我在AWA2数据集和APY数据集上对我的方法进行了消融实验。具体地说，我分别在仅使用子集选取方法，禁用扩大可见类域和不可见类域的差异的方法的情况下，和仅使用扩大域间差异的方法，禁用子集选取方法的情况下，以及两种方法都不使用的情况下，额外进行了三次实验，探究两种方法的有效性。消融实验的超参数等细节遵循上文提到的设定，最终结果如表2所示。

表2

| 方法         | AWA2：24bit | AWA2：48bit | AWA2：64bit | AWA2：128bit |
| ---------- | ---------- | ---------- | ---------- | ----------- |
| 无子集选取和域间差异 | 0.2115     | 0.2802     | 0.2991     | 0.3335      |
| 仅域间差异      | 0.2119     | 0.2842     | 0.3000     | 0.3346      |
| 仅子集选取      | 0.3439     | 0.4334     | 0.4557     | 0.4976      |
| 二者皆有       | 0.3456     | 0.4425     | 0.4597     | 0.4924      |

| 方法         | APY：24bit | APY：48bit | APY：64bit | APY：128bit |
| ---------- | --------- | --------- | --------- | ---------- |
| 无子集选取和域间差异 | 0.1607    | 0.2054    | 0.2242    | 0.2598     |
| 仅域间差异      | 0.1607    | 0.2062    | 0.2233    | 0.2611     |
| 仅子集选取      | 0.1838    | 0.2236    | 0.2430    | 0.2712     |
| 二者皆有       | 0.1837    | 0.2243    | 0.2423    | 0.2718     |

从表二可以看出：子集选取方法可以显著提升检索精度，而域间差异方法相较之下效果则不够明显，在AWA2数据集上，域间差异方法可以在子集选取方法的基础上进一步提高检索精度。总体来说，子集选取方法是相对更加有效的提升检索精度的方法，域间差异方法的效果则相对不明显。
## 6.3 子集选取准确率分析
上一节消融实验证实了子集选取方法是相对更加有效的方法，本节通过实验探究子集选取方法的准确率。
保持其余超参数不变，改变阈值$t$，画出选取中的子集中不可见类样本所占的比例，结果如图1所示。

图1略

可以看到，无论是在AWA2数据集上和在APY数据集上，通过训练多个分类器进行子集选取的方法都有较高的准确率，因此也可以排除检索集内的大部分可见类样本，提高不可见类查询的检索精度。
## 6.4 部分超参数分析
### 6.4.1 $\eta$和$a'$
本节讨论和扩大域间差异的方法相关超参数是$\eta$和$a'$。
其中$\eta$决定了这一项和其他损失项之间的相对大小，$a'$决定了采样出的不可见类样本和训练集样本之间的相似度。
在本节中，我在实验中固定其他参数，固定哈希比特数为64位，调节$\eta$和$a'$，在AWA2数据集和APY数据集上探究这两个超参数对检索精度的影响。

在AWA2数据集上的实验结果如图1所示，在APY数据集上的结果如图2所示。

图1、2略

从图1、图2中可以看到，在$\eta$相对较小且$a'$较接近0的时候，检索效果较好，检索效果会随着$\eta$的增大和$a'$的减小而下降，这说明了扩大域间差异的方法还是具有一定的不稳定性，要控制好它和其余损失项的权衡，才能在不损害原来检索精度的情况下进一步提高检索精度，否则可能会损害原来的检索精度，起到反效果。
### 6.4.3 $m$
本节讨论和扩大域间差异方法相关的另一个超参数$m$。
超参数$m$是每一次迭代中从检索集子集中采样的用于进行扩大域间差异学习的样本个数，在5.3节实验细节设定中，我简要分析了将$m$设定为$1$的原因，本节用实验探究$m$变化对检索精度的影响。
在本节中，我在实验中固定其他参数，固定哈希比特数为64位，调节$m$，在AWA2数据集和APY数据集上探究该参数对mAP@5000的影响。

在AWA2数据集上的实验结果如图1所示，在APY数据集上的结果如图2所示。

图1、2略

从图中可以看到，随着$m$的逐渐增大，mAP@5000指标总体呈下降趋势，这说明了在$m$较大时，采样得到的$m$个样本很可能各自属于不同的不可见类，将这些样本在同一轮迭代中同时用于学习，会导致不可见类域内的差异被减弱，导致检索精度下降。因此在实际实验中，我采用$m=1$，即每一轮迭代仅采样一个样本用于域间差异扩大学习。
### 6.4.3 $u$
本节讨论和相似度矩阵的放缩相关的超参数$u$。
$u$是对相似度矩阵$\mathbf A$的不对称放缩系数，不对称放缩的想法来源于文献[7]，其中作者发现对相似度矩阵进行不对称放缩会强化哈希函数的迁移能力，具体地说，不对称放缩就是将$\mathbf A$中的$+1$放大为原来的$u$倍，即变为$+u$，而$\mathbf A$中的$-1$保持不变。本节用实验探究$u$变化对检索精度的影响。
在本节中，我在实验中固定其他参数，在哈希比特数为24位、48位、64位、128位的情况下，调节$u$，分别在AWA2数据集和APY数据集上探究该参数对mAP@5000的影响。

在AWA2数据集上的实验结果如图1所示，在APY数据集上的结果如图2所示。

图1、2略
### 6.4.3 核函数相关参数
核函数的使用使得哈希函数具有了学习数据集中的非线性模式的能力，我在实验中使用的是常用的高斯核函数，和高斯核函数相关的两个预设的参数是锚点数量和核函数带宽$\sigma$，本节通过实验对这两个参数对检索效果的影响进行研究。

在图1中，我固定锚点数量为1000，改变核函数带宽，在AWA2数据集上验证核函数带宽的变化对mAP@5000指标的影响。

在图2中，我固定核函数带宽为60，改变锚点数量，在AWA2数据集上验证锚点数量变化对mAP@5000指标的影响。
## 6.6 不同分类器的结果
## 6.7 收敛性分析

# 参考文献
[1] Weiss Y, Torralba A, Fergus R. Spectral hashing[J]. Advances in neural information processing systems, 2008, 21.
[2] Shi J, Malik J. Normalized cuts and image segmentation[J]. IEEE Transactions on pattern analysis and machine intelligence, 2000, 22(8): 888-905.
[3] Liu W, Wang J, Ji R, et al. Supervised hashing with kernels[C]//2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012: 2074-2081.
[4] Liu W, Mu C, Kumar S, et al. Discrete graph hashing[J]. Advances in neural information processing systems, 2014, 27.
[5] Liu W, He J, Chang S F. Large graph construction for scalable semi-supervised learning[C]//Proceedings of the 27th international conference on machine learning (ICML-10). 2010: 679-686.
[6] Chen Y, Tian Z, Zhang H, et al. Strongly constrained discrete hashing[J]. IEEE Transactions on Image Processing, 2020, 29: 3596-3611.
[7] Shi Y, Nie X, Liu X, et al. Zero-shot hashing via asymmetric ratio similarity matrix[J]. IEEE Transactions on Knowledge and Data Engineering, 2022, 35(5): 5426-5437.
[8] 董峰, 王永欣, 马玉玲, 王奎奎. 原型对齐和域感知的零样本哈希[J]. 计算机工程, [doi: 10.19678/j.issn.1000-3428.0067994](https://doi.org/10.19678/j.issn.1000-3428.0067994).
[9] Lampert C H, Nickisch H, Harmeling S. Attribute-based classification for zero-shot visual object categorization[J]. IEEE transactions on pattern analysis and machine intelligence, 2013, 36(3): 453-465.
[10] Xian Y, Lampert C H, Schiele B, et al. Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(9): 2251-2265.
[11] Deng J, Dong W, Socher R, et al. Imagenet: A large-scale hierarchical image database[C]//2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009: 248-255.
[12] Farhadi A, Endres I, Hoiem D, et al. Describing objects by their attributes[C]//2009 IEEE conference on computer vision and pattern recognition. IEEE, 2009: 1778-1785.
[13] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

