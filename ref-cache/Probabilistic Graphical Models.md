# 8 The Exponential Family 

# 8.1 Introduction 

In the previous chapters, we discussed several diferent representations of complex distributions. These included both representations of global structures (for example, Bayesian networks and Markov networks) and representations of local structures (for example, representations of CPDs and of potentials). In this chapter, we revisit these representations and view them from a diferent perspective. This view allows us to consider several basic questions and derive generic answers for these questions for a wide variety of representations. As we will see in later chapters, these solutions play a role in both inference and learning for the diferent representations we consider. 

We note, however, that this chapter is somewhat abstract and heavily mathematical. Although the ideas described in this chapter are of central importance to understanding the theoretical foundations of learning and inference, the algorithms themselves can be understood even without the material presented in this chapter. Thus, this chapter can be skipped by readers who are interested primarily in the algorithms themselves. 

# 8.2 Exponential Families 

parametric family 

Example 8.1 

Our discussion so far has focused on the representation of a single distribution (using, say, a Bayesian or Markov network). We now consider families of distributions . Intuitively, a family is a set of distributions that all share the same parametric form and difer only in choice of particular parameters (for example, the entries in table-CPDs). In general, once we choose the global structure and local structure of the network, we deﬁne a family of all distributions that can be attained by diferent parameters for this speciﬁc choice of CPDs. 

sufcient statistic function parameter space legal parameter natural parameter • A sufcient statistics function $\tau$ from assignments to $\mathcal{X}$ to $\mathcal{R}^{K}$ .

 • A parameter space that is a convex set $\Theta\subseteq\mathcal{R}^{M}$ of legal parameters .

 • $A$ natural parameter function t from $\mathcal{R}^{M}$ to $R^{K}$ .

 • An auxiliary measure $A$ over $\mathcal{X}$ . 

Each vector of parameters $\theta\in\Theta$ speciﬁes a distribution $P_{\theta}$ in the family as 

$$
P_{\theta}(\xi)=\frac{1}{Z(\theta)}A(\xi)\exp\left\{\langle\mathfrak{t}(\theta),\tau(\xi)\rangle\right\}
$$ 

where $\langle{\mathfrak{t}}(\theta),\tau(\xi)\rangle$ is the inner product of the vectors ${\mathfrak{t}}(\theta)$ and $\tau(\xi)$ , and 

$$
Z(\pmb\theta)=\sum_{\xi}A(\xi)\exp\left\{\langle\mathfrak t(\pmb\theta),\tau(\xi)\rangle\right\}
$$ 

partition function is the partition function of $\mathcal{P}$ , which must be ﬁnite. The parametric family $\mathcal{P}$ is deﬁned as: 

$$
{\mathcal{P}}=\{P_{\theta}:\theta\in\Theta\}.
$$ 

We see that an exponential family is a concise representation of a class of probability dis- tributions that share a similar functional form. A member of the family is determined by the parameter vector $\theta$ in the set of legal parameters. The sufcient statistic function $\tau$ summarizes the aspects of an instance that are relevant for assigning it a probability. The function t maps the parameters to space of the sufcient statistics. 

The measure $A$ assigns additional preferences among instances that do not depend on the parameters. However, in most of the examples we consider here $A$ is a constant, and we will mention it explicitly only when it is not a constant. 

Although this deﬁnition seems quite abstract, many distributions we already have encountered are exponential families. 

Example 8.2 Consider a simple Bernoulli distribution. In this case, the distribution over a binary outcome (such as a coin toss) is controlled by a single parameter $\theta$ that represents the probability of $x^{1}$ . To show that this distribution is in the exponential family, we can set 

$$
{\boldsymbol{\tau}}(X)=\langle{\pmb{I}}\{X=x^{1}\},{\pmb{I}}\{X=x^{0}\}\rangle,
$$ 

a numerical vector representation of the value of $X$ , and 

$$
{\mathfrak{t}}(\theta)=\langle\ln\theta,\ln(1-\theta)\rangle.
$$ 

It is easy to see that for $X=x^{1}$ , we have $\tau(X)=\langle1,0\rangle$ , and thus 

$$
\exp\left\{\langle\mathfrak{t}(\theta),\tau(X)\rangle\right\}=e^{1\cdot\ln\theta+0\cdot\ln(1-\theta)}=\theta.
$$ 

Similarly, for $X\,=\,x^{0}$ , we get that $\exp{\{\langle\mathfrak{t}(\theta),\tau(X)\rangle\}}\,=\,1\,-\,\theta$ . We conclude that, by setting $Z(\theta)=1$ , this representation is identical to the Bernoulli distribution. 

Example 8.3 Consider a Gaussian distribution over a single variable. Recall that 

$$
P(x)={\frac{1}{\sqrt{2\pi}\sigma}}\exp\left\{-{\frac{(x-\mu)^{2}}{2\sigma^{2}}}\right\}.
$$ 

Deﬁne 

$$
\begin{array}{r c l}{{\tau(x)}}&{{=}}&{{\langle x,x^{2}\rangle}}\\ {{\mathrm{t}(\mu,\sigma^{2})}}&{{=}}&{{\langle\displaystyle\frac{\mu}{\sigma^{2}},-\frac{1}{2\sigma^{2}}\rangle}}\\ {{Z(\mu,\sigma^{2})}}&{{=}}&{{\sqrt{2\pi}\sigma\exp\left\{\displaystyle\frac{\mu^{2}}{2\sigma^{2}}\right\}.}}\end{array}
$$ 

We can easily verify that 

$$
P(x)={\frac{1}{Z(\mu,\sigma^{2})}}\exp\left\{\langle\mathfrak{t}(\theta),\tau(X)\rangle\right\}.
$$ 

In fact, most of the parameterized distributions we encounter in probability textbooks can be represented as exponential families. This includes the Poisson distributions, exponential distributions, geometric distributions, Gamma distributions, and many others (see, for example, exercise 8.1). 

nonredundant parameter iz ation 

invertible exponential family 

We can often construct multiple exponential families that encode precisely the same class of distributions. There are, however, desiderata that we want from our representation of a class of distributions as an exponential family. First, we want the parameter space $\Theta$ to be “well-behaved,” in particular, to be a convex, open subset of $\mathcal{R}^{M}$ . Second, we want the parametric family to be nonredundant — to have each choice of parameters represent a unique distribution. More precisely, we want $\theta\:\neq\:\theta^{\prime}$ to im ly $P_{\theta}\;\neq\;P_{\theta^{\prime}}$ . It is easy check that a family is nonredundant if and only if the function t is invertible (over the set $\Theta$ ) . Such exponential families are called invertible . As we will discuss, these desiderata help us execute certain operations efectively, in particular, ﬁnding a distribution $Q$ in some exponential family that is a “good approximation” to some other distribution $P$ . 

# 8.2.1 Linear Exponential Families 

natural parameter A special class of exponential families is made up of families where the function $^\mathrm{t}$ is the identity function. This implies that the parameters are the same dimension $K$ as the representation of the data. Such parameters are also called the natural parameters for the given sufcient statistic function. The name reﬂects that these parameters do not need to be modiﬁed in the exponential form. When using natural parameters, equation (8.1) simpliﬁes to 

$$
P_{\theta}(\xi)=\frac{1}{Z(\theta)}\exp\left\{\langle\theta,\tau(\xi)\rangle\right\}.
$$ 

Clearly, for any given sufcient statistics function, we can reparameterize the exponential family using the natural parameters. However, as we discussed earlier, we want the space of parameters $\Theta$ to satisfy certain desiderata, which may not hold for the space of natural parameters. In fact, for the case of linear exponential families, we want to strengthen our desiderata, and require that any parameter vector in $\mathcal{R}^{K}$ deﬁnes a distribution in the family. Unfortunately, as stated, this desideratum is not always achievable. To understand why, recall that the deﬁnition of a legal parameter spa $\Theta$ requires that each parameter vector $\theta\,\in\,\Theta$ give rise to a legal (normalizable) distribution $P_{\theta}$ . These normalization requirements can impose constraints on the space of legal parameters. 

Example 8.4 Consider again the Gaussian distribution. Suppose we deﬁne a new parameter space using the deﬁnition of t . That is let $\begin{array}{r}{\eta=\mathfrak{t}(\mu,\sigma^{2})=\langle\frac{2\mu}{2\sigma^{2}},\overset{\cdot}{-}\frac{1}{2\sigma^{2}}\rangle}\end{array}$ − ⟩ be the natural parameters that corresponds to $\theta=\langle\mu,\sigma^{2}\rangle$ . Clearly, we can now write 

$$
P_{\eta}(x)\propto\exp\left\{\langle\eta,\tau(x)\rangle\right\}.
$$ 

However, not every choice of $\eta$ would lead to a legal distribution. For the distribution to be normalized, we need to be able to compute 

$$
\begin{array}{r c l}{Z(\pmb{\eta})}&{=}&{\displaystyle\int\exp\left\{\langle\pmb{\eta},\tau(x)\rangle\right\}d x}\\ &{=}&{\displaystyle\int\exp\left\{\eta_{1}x+\eta_{2}x^{2}\right\}d x.}\end{array}
$$ 

$\eta_{2}\geq0$ this integral is undeﬁned, since the function grows when $x$ $\infty$ and $-\infty$ . When $\eta_{2}<0$ , the integral has a ﬁnite value. Fortunately, if we consider ${\boldsymbol{\eta}}={\mathsf{t}}({\boldsymbol{\mu}},\sigma^{2})$ of equation (8.5), we see that the second component is always negative (since $\sigma^{2}\,>\,0.$ ). In fact, we can see that the image iginal parameter space, $\langle\mu,\sigma^{2}\rangle\in\mathcal{R}\times\mathcal{R}^{+}$ , through the function $\mathtt{t}(\mu,\sigma^{2})$ , is the space R × R . We can verify that, for every $\eta$ in that space, the normalization constant is well deﬁned. 

natural parameter space 

More generally, when we consider natural parameters for a sufcient statistics function $\tau$ , we deﬁne the set of allowable natural parameters, the natural parameter space , to be the set of natural parameters that can be normalized 

$$
\Theta=\left\{\pmb{\theta}\in\mathcal{R}^{K}:\int\exp\left\{\langle\pmb{\theta},\tau(\xi)\rangle\right\}d\xi<\infty\right\}.
$$ 

linear exponential family 

In the case of distributions over ﬁnite discrete spaces, all parameter choices lead to normalizable distributions, and so $\Theta\,=\,\mathcal{R}^{K}$ . In other examples, such as the Gaussian distribution, the natural parameter space can be more constrained. An exponential family over the natural parameter space, and for which the natural parameter space is open and convex, is called a linear exponential family . 

The use of linear exponential families signiﬁcantly simpliﬁes the deﬁnition of a family. To specify such a family, we need to deﬁne only the function $\tau$ ; all other parts of the deﬁnition are implicit based on this function. This gives us a tool to describe distributions in a concise manner. As we will see, linear exponential families have several additional attractive properties. 

Where do ﬁnd linear exponential families? The two examples we presented earlier were not phrased as linear exponential families. However, as we saw in example 8.4, we may be able to provide an alternative parameter iz ation of a nonlinear exponential family as a linear exponential family. This example may give rise to the impression that any family can be reparameterized in a trivial manner. However, there are more subtle situations. 

Example 8.5 Consider the Bernoulli distribution. Again, we might reparameterize $\theta$ by ${\sf t}(\theta)$ . However, the image of the function t of example 8.2 is the curve $\langle\ln\theta,\ln(1-\theta)\rangle$ . This curve is not a convex set, and it is clearly a subspace of the natural parameter space. 

Alternatively, we might con eter space $\mathcal{R}^{2}$ , corresponding to the sufcient statistic function $\tau(X)=\langle\pmb{I}\{X=x^{1}\},\pmb{I}\{X=x^{0}\}\rangle$ ⟨ { } { }⟩ of equation (8.2). This gives rise to the parametric form: 

$$
P_{\theta}(x)\propto\exp\left\{\langle\theta,\tau(x)\rangle\right\}=\exp\left\{\theta_{1}{\pmb I}\{X=x^{1}\}+\theta_{2}{\pmb I}\{X=x^{0}\}\right\}.
$$ 

Because the probability space is ﬁnite, this form does deﬁne a distribution for every choice of $\langle\theta_{1},\theta_{2}\rangle$ . However, it is not difcult to verify that this family is redundant: for every constant c , the parameters $\langle\theta_{1}+c,\theta_{2}+c\rangle$ deﬁne the same distribution as $\langle\theta_{1},\theta_{2}\rangle$ . 

Thus, a two-dimensional space is over parameterized for this distribution; conversely, the one- dimensional subspace deﬁned by the natural parameter function is not well behaved. The solution is to use an alternative representation of a one-dimensional space. Since we have a redundancy, we may as well clamp $\theta_{2}$ to be 0 . This results in the following representation of the Bernoulli distribution: 

$$
\begin{array}{l c l}{\boldsymbol{\tau}(\boldsymbol{x})}&{=}&{\boldsymbol{\mathit{1}}\{\boldsymbol{x}=\boldsymbol{x}^{1}\}}\\ {\boldsymbol{\mathrm{t}}(\boldsymbol{\theta})}&{=}&{\ln\displaystyle\frac{\boldsymbol{\theta}}{1-\boldsymbol{\theta}}.}\end{array}
$$ 

We see that 

$$
\begin{array}{c c c}{\displaystyle\exp\left\{\langle\mathfrak{t}(\theta),\tau(x^{1})\rangle\right\}}&{=}&{\displaystyle\frac{\theta}{1-\theta}}\\ {\displaystyle\exp\left\{\langle\mathfrak{t}(\theta),\tau(x^{0})\rangle\right\}}&{=}&{1.}\end{array}
$$ 

Thus, 

$$
Z(\theta)=1+\frac{\theta}{1-\theta}=\frac{1}{1-\theta}.
$$ 

Using these, we can verify that 

$$
P_{\theta}(x^{1})=(1-\theta){\frac{\theta}{1-\theta}}=\theta.
$$ 

We conclude that this exponential representation captures the Bernoulli distribution. Notice now that, in the new representation, the image of t is the whole real line $\mathcal{R}$ . Thus, we can deﬁne a linear exponential family with this sufcient statistic function. 

Example 8.6 Now, consider a multinomial variable $X$ with $k$ values $x^{1},\cdot\cdot\cdot,x^{k}$ . The situation here is similar to the one we had with the Bernoulli distribution. If we use the simplest exponential representation, we ﬁnd that the legal natural parameters are on a curv nifold of $\mathcal{R}^{k}$ . Thus, instead we deﬁne the sufcient statistic as a function from values of $x$ to R $\mathcal{R}^{k-1}$ : 

$$
\tau(x)=\langle\pmb{I}\!\!\{x=x^{2}\},\dots,\pmb{I}\!\!\{x=x^{k}\}\rangle.
$$ 

Using a similar argument as with the Bernoulli distribution, we see that if we deﬁne 

$$
{\mathfrak{t}}(\pmb{\theta})=\langle\ln\frac{\theta_{2}}{\theta_{1}},.\,.\,.\,,\ln\frac{\theta_{k}}{\theta_{1}}\rangle,
$$ 

then we reconstruct the original multinomial distribution. It is also easy to check that the image of t is $\mathcal{R}^{k-1}$ . Thus, by re parameter i zing, we get a linear exponential family. 

All these examples deﬁne linear exponential families. An immediate question is whether there exist families that are not linear. As we will see, there are such cases. However, the examples we present require additional machinery. 

# 8.3 Factored Exponential Families 

The two examples of exponential families so far were of univariate distributions. Clearly, we can extend the notion to multivariate distributions as well. In fact, we have already seen one such example. Recall that, in deﬁnition 4.15, we deﬁned log-linear models as distributions of the form: 

$$
P(X_{1},\ldots,X_{n})\propto\exp\left\{\sum_{i=1}^{k}\theta_{i}\cdot f_{i}(D_{i})\right\}
$$ 

where each feature $f_{i}$ is a function whose scope is $D_{i}$ . Such a distribution is clearly a linear exponential family where the sufcient statistics are the vector of features 

$$
\tau(\xi)=\langle f_{1}(d_{1}),.\,.\,.\,,f_{k}(d_{k})\rangle.
$$ 

As we have shown, by choosing the appropriate features, we can devise a log-linear model to represent a given discrete Markov network structure. This sufces to show that discrete Markov networks are linear exponential families. 

# 8.3.1 Product Distributions 

What about other distributions with product forms? Initially the issues seem deceptively easy. A product form of terms corresponds to a simple composition of exponential families 

Deﬁnition 8.2 exponential factor family 

Deﬁnition 8.3 family composition 

An (unnormalized) exponential factor family $\Phi$ is deﬁned by $\tau$ , t , $A$ , and $\Theta$ (as in the exponential family). A factor in this family is 

$$
\phi_{\pmb\theta}(\xi)=A(\xi)\exp\left\{\langle\mathtt{t}(\pmb\theta),\tau(\xi)\rangle\right\}.
$$ 

Let $\Phi_{1},\ldots,\Phi_{k}$ be exponential factor families, where each $\Phi_{i}$ is speciﬁed by $\tau_{i}$ , $\mathtt{t}_{i}$ , $A_{i}$ , and

 $\Theta_{i}$ $\Phi_{1},\ldots,\Phi_{k}$ e family $\Phi_{1}\times\Phi_{2}\times\cdot\cdot\cdot\times\Phi_{k}$ parameterized by $\theta\:=\:

$ $\pmb{\theta}_{1}\circ\pmb{\theta}_{2}\circ\cdot\cdot\cdot\circ\pmb{\theta}_{k}\in\Theta_{1}\times\Theta_{2}\times\cdot\cdot\cdot\times\Theta_{k}.$ , deﬁned as 

$$
P_{\pmb\theta}(\xi)\propto\prod_{i}\phi_{\pmb\theta_{i}}(\xi)=\left(\prod_{i}A_{i}(\xi)\right)\exp\left\{\sum_{i}\langle\mathfrak{t}_{i}(\pmb\theta_{i}),\tau_{i}(\xi)\rangle\right\}
$$ 

where $\phi_{\pmb{\theta}_{i}}$ is a factor in the i ’th factor family. 

It is clear from this deﬁnition that the composition of exponential factors is an exponential family with $\tau(\xi)=\tau_{1}(\xi)\circ\tau_{2}(\xi)\circ\cdot\cdot\cdot\circ\tau_{k}(\xi)$ and natural parameters $\mathtt{t}(\theta)=\mathtt{t}_{1}(\theta_{1})\circ\mathtt{t}_{2}(\theta_{2})\circ$ $\cdot\cdot\cdot\circ\,\mathtt{t}_{k}\big(\pmb{\theta}_{k}\big)$ . 

This simple observation sufces to show that if we have exponential representation for po- tentials in a Markov network (not necessarily simple potentials), then their product is also an exponential family. Moreover, it follows that the product of linear exponential factor families is a linear exponential family. 

# 8.3.2 Bayesian Networks 

Taking the same line of reasoning, we can also show that, if we have a set of CPDs from an exponential family, then their product is also in the exponential family. Thus, we can conclude that a Bayesian network with exponential CPDs deﬁnes an exponential family. To show this, we ﬁrst note that many of the CPDs we saw in previous chapters can be represented as exponential factors. 

# Example 8.7 

We start by examining a simple table-CPD $P(X\mid U)$ . Similar to the case of Bernoulli distribution, we can deﬁne the sufcient statistics to be indicators for diferent entries in $P(X\mid U)$ . Thus, we set 

$$
\tau_{P(X|U)}(\mathcal{X})=\langle\pmb{I}\{X=x,U=\pmb{u}\}:x\in V a l(X),\pmb{u}\in V a l(\pmb{U})\rangle.
$$ 

We set the natural parameters to be the corresponding parameters 

$$
\mathfrak{t}_{P(X\mid U)}(\pmb\theta)=\langle\ln P(x\mid\pmb u):x\in V a l(X),\pmb u\in V a l(\pmb U)\rangle.
$$ 

It is easy to verify that 

$$
P(x\mid\mathbf{u})=\exp\left\{\langle\mathsf{t}_{P(X\mid U)}(\pmb\theta),\tau_{P(X\mid U)}(x,\mathbf{u})\rangle\right\},
$$ 

since exactly one entry of $\tau_{P(X|U)}(x,u)$ is 1 and the rest are 0 . Note that this representation is not a linear exponential factor. 

Clearly, we can use the same representation to capture any CPD for discrete variables. In some cases, however, we can be more efcient. In tree-CPDs, for example, we can have a feature set for each leaf in tree, since all parent assignment that reach the leaf lead to the same parameter over the children. 

What happens with continuous CPDs? In this case, not every CPD can be represented by an exponential factor. However, some cases can. 

# Example 8.8 

$$
X=\beta_{0}+\beta_{1}u_{1}+\cdot\cdot\cdot+\beta_{k}u_{k}+\epsilon,
$$ 

where $\epsilon$ is a Gaussian random variable with mean 0 and variance $\sigma^{2}$ , representing the noise in the system. Stated diferently, the conditional density function of $X$ is 

$$
P(x\mid u)={\frac{1}{\sqrt{2\pi}\sigma}}\exp\left\{-{\frac{1}{2\sigma^{2}}}(x-(\beta_{0}+\beta_{1}u_{1}+\cdot\cdot\cdot+\beta_{k}u_{k}))^{2}\right\}.
$$ 

By expanding the squared term, we ﬁnd that the sufcient statistics are the ﬁrst and second moments of all the variables 

$$
\tau_{P(X|U)}(\mathcal{X})=\langle1,x,u_{1},\dots,u_{k},x^{2},x u_{1},\dots,x u_{k},u_{1}^{2},u_{1}u_{2},\dots,u_{k}^{2}\rangle,
$$ 

and the natural parameters are the coefcients of each of these terms. 

As the product of exponential factors is an exponential family, we conclude that a Bayesian network that is the product of CPDs that have exponential form deﬁnes an exponential family. 

However, there is one subtlety that arises in the case of Bayesian networks that does not arise for a general product form. When we deﬁned the product of a set of exponential factors in deﬁnition 8.3, we ignored the partition functions of the individual factors, allowing the partition function of the overall distribution to ensure global normalization. 

However, in both of our examples of exponential factors for CPDs, we were careful to construct a normalized conditional distribution. This allows us to use the chain rule to compose these factors into a joint distribution without the requirement of a partition function. This requirement turns out to be critical: We cannot construct a Bayesian network from a product of unnormalized exponential factors. 

Consider the network structure $A\rightarrow B$ , with binary variables. Now, suppose we want to represent the CPD $P(B\mid A)$ using a more concise representation than the one of example 8.7. As suggested by example 8.5, we might consider deﬁning 

$$
\tau(A,B)=\langle\pmb{I}\{A=a^{1}\},\pmb{I}\{B=b^{1},A=a^{1}\},\pmb{I}\{B=b^{1},A=a^{0}\}\rangle.
$$ 

That is, for each conditional distribution, we have an indicator only for one of the two relevant cases. The representation of example 8.5 suggests that we should deﬁne 

$$
{\mathfrak{t}}(\theta)=\left\langle\ln{\frac{\theta_{a^{1}}}{\theta_{a^{0}}}},\ln{\frac{\theta_{b^{1}|a^{1}}}{\theta_{b^{0}|a^{1}}}},\ln{\frac{\theta_{b^{1}|a^{0}}}{\theta_{b^{0}|a^{0}}}}\right\rangle.
$$ 

Does this construction give us the desired distribution? Under this construction, we would have 

$$
P_{\theta}(a^{1},b^{1})=\frac{1}{Z(\theta)}\frac{\theta_{a^{1}}\theta_{b^{1}|a^{1}}}{\theta_{a^{0}}\theta_{b^{0}|a^{1}}}.
$$ 

Thus, if this representation was faithful for the intended interpretation of the parameter values, we would have $\begin{array}{r}{Z(\pmb\theta)=\frac{1}{\theta_{a^{0}}\theta_{b^{0}\lfloor a^{1}}}}\end{array}$ . On the other hand, 

$$
P_{\theta}(a^{0},b^{0})=\frac{1}{Z(\theta)},
$$ 

which requires that $\begin{array}{r}{Z(\theta)=\frac{1}{\theta_{a^{0}}\theta_{b^{0}|a^{0}}}}\end{array}$ in order to be faithful to the desired distribution. Because these two constants are, in general, not equal, we conclude that this representation cannot be faithful to the original Bayesian network. 

The failure in this example is that the global normalization constant cannot play the role of a local normalization constant within each conditional distribution. This implies that to have an exponential representation of a Bayesian network, we need to ensure that each CPD is locally normalized. For every exponential CPD this is easy to do. We simply increase the dimension of $\tau$ by adding another dimension that has a constant value, say 1. Then the matching element of ${\mathfrak{t}}(\theta)$ can be the logarithm of the partition function. This is essentially what we did in example 8.8. 

We still might wonder whether a Bayesian network deﬁnes a linear exponential family. 

Consider the network structure $A\,\rightarrow\,C\,\leftarrow\,B$ , with binary variables. Assuming a representation that captures general CPDs, our sufcient statistics need to include features that distinguish between the following four assignments: 

$$
\begin{array}{l c l}{{\xi_{1}}}&{{=}}&{{\langle a^{1},b^{1},c^{1}\rangle}}\\ {{\xi_{2}}}&{{=}}&{{\langle a^{1},b^{0},c^{1}\rangle}}\\ {{\xi_{3}}}&{{=}}&{{\langle a^{0},b^{1},c^{1}\rangle}}\\ {{\xi_{4}}}&{{=}}&{{\langle a^{0},b^{0},c^{1}\rangle}}\end{array}
$$ 

More precisely, we need to be able to modify the CPD $P(C\mid A,B)$ to change the probability of one of these assignments without modifying the probability of the other three. This implies that $\tau(\xi_{1}),.\,.\,.\,,\tau(\xi_{4})$ must be linearly independent: otherwise, we could not change the probability of one assignment without changing the others. Because our model is a linear function of the sufcient statistics, we can choose any set of orthogonal basis vectors that we want; in particular, we can assume without loss of generality that the ﬁrst four coordinates of the sufcient statistics are $\tau_{i}(\xi)=I\!\!\{\xi=\xi_{i}\}$ { } , and that any additional coordinates of the sufcient statistics are not linearly dependent on these four. Moreover, since the model is over a ﬁnite set of events, any choice of parameters can be normalized. Thus, the space of natural parameters is $\mathcal{R}^{K}$ , where $K$ is dimension of the sufcient statistics vector. The linear family over such features is essentially a Markov network er the clique $\{A,B,C\}$ . Thus, the parameter iz ation of this family includes cases where $A$ and $B$ are not independent, violating the independence properties of the Bayesian network. 

# 

Thus, this simple Bayesian network cannot be represented by a linear family. More broadly, although a Bayesian network with suitable CPDs deﬁnes an exponential family, this family is not generally a linear one. In particular, any network that contains immoralities does not induce a linear exponential family. 

# 8.4 Entropy and Relative Entropy 

We now explore some of the consequences of representation of models in factored form and of their exponential family representation. These both suggest some implications of these representations and will be useful in developments in subsequent chapters. 

8.4.1 Entropy 

We start with the notion of entropy. Recall that the entropy of a distribution is a measure of the amount of “stochasticity” or “noise” in the distribution. A low entropy implies that most of the distribution mass is on a few instances, while a larger entropy suggests a more uniform distribution. Another interpretation we discussed in appendix A.1 is the number of bits needed, on average, to encode instances in the distribution. In various tasks we need to compute the entropy of given distributions. As we will see, we 

also encounter situations where we want to choose a distribution that maximizes the entropy subject to some constraints. A characterization of entropy will allow us to perform both tasks more efciently. 

# 8.4.1.1 Entropy of an Exponential Model 

We now consider the task of computing the entropy for distributions in an an exponential family deﬁned by $\tau$ and t . 

# Theorem 8.1 

Let $P_{\theta}$ be a distribution in an exponential family deﬁned by the functions $\tau$ and t . Then 

$$
\begin{array}{r}{\pmb{H}_{P_{\theta}}(\mathcal{X})=\ln Z(\pmb{\theta})-\langle\pmb{E}_{P_{\theta}}[\tau(\mathcal{X})],\mathsf{t}(\pmb{\theta})\rangle.}\end{array}
$$ 

While this formulation seems fairly abstract, it does provide some insight. The entropy decom- poses as a diference of two terms. The ﬁrst is the partition function $Z(\theta)$ . The second depends only on the ex cted value of the sufcient statistics $\tau(\mathcal{X})$ . Thus, instead of co idering each assignment to X , we need to know only the expectations of the statistics under $P_{\theta}$ θ . As we will see, this is a recurring theme in our discussion of exponential families. 

# Example 8.11 

We now apply this result to a Gaussian distribution $X\,\sim\,N(\mu,\sigma^{2})$ , as formulated in t expo- nential family in example 8.3. Plugging into equation (8.7) the deﬁnitions of $\tau$ , t , and Z from equation (8.4), equation (8.5), and equation (8.6), respectively, we get 

$$
{\begin{array}{l l l}{H_{P}(X)}&{=}&{{\cfrac{1}{2}}\ln2\pi\sigma^{2}+{\cfrac{\mu^{2}}{2\sigma^{2}}}-{\cfrac{2\mu}{2\sigma^{2}}}E_{P}[X]+{\cfrac{1}{2\sigma^{2}}}E_{P}[X^{2}]}\\ &{=}&{{\cfrac{1}{2}}\ln2\pi\sigma^{2}+{\cfrac{\mu^{2}}{2\sigma^{2}}}-{\cfrac{2\mu}{2\sigma^{2}}}\mu+{\cfrac{1}{2\sigma^{2}}}(\sigma^{2}+\mu^{2})}\\ &{=}&{{\cfrac{1}{2}}\ln2\pi e\sigma^{2}}\end{array}}
$$ 

We can apply the formulation of theorem 8.1 directly to write the entropy of a Markov network. 

Proposition 8.1 

$$
H_{P}(\mathcal{X})=\ln Z+\sum_{k}E_{P}[-\ln\phi_{k}(D_{k})].
$$ 

Example 8.12 Consider a simple Markov network with two potentials $\beta_{1}(A,B)$ and $\beta_{2}(B,C)$ , so that 

![](images/fe1dfcf4df8d173d96120220ad6634e19214b88c48779228bf9f0967ba72d87a.jpg) 

Simple calculations show that $Z=30$ , and the marginal distributions are Using proposition 8.1, we can calculate the entropy: 

$$
\begin{array}{r c l}{{H_{P}(A,B,C)}}&{{=}}&{{\ln Z+E_{P}[-\ln\beta_{1}(A,B)]+E_{P}[-\ln\beta_{2}(B,C)]}}\\ {{}}&{{=}}&{{\ln Z}}\\ {{}}&{{}}&{{-P(a^{0},b^{0})\ln\beta_{1}(a^{0},b^{0})-P(a^{0},b^{1})\ln\beta_{1}(a^{0},b^{1})}}\\ {{}}&{{}}&{{-P(a^{1},b^{0})\ln\beta_{1}(a^{1},b^{0})-P(a^{1},b^{1})\ln\beta_{1}(a^{1},b^{1})}}\\ {{}}&{{}}&{{-P(b^{0},c^{0})\ln\beta_{2}(b^{0},c^{0})-P(b^{0},c^{1})\ln\beta_{2}(b^{0},c^{1})}}\\ {{}}&{{}}&{{-P(b^{1},c^{0})\ln\beta_{2}(b^{1},c^{0})-P(b^{1},c^{1})\ln\beta_{2}(b^{1},c^{1})}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{3.4012}}\\ {{}}&{{}}&{{-0.47*0.69-0.05*0-0.23*0-0.25*1.60}}\\ {{}}&{{}}&{{-0.6*1.79-0.1*0-0.2*0-0.1*-0.69}}\\ {{}}&{{=}}&{{1.670.}}\end{array}
$$ 

In this example, the number of terms we evaluated is the same as what we would have considered using the original formulation of the entropy where we sum over all possible joint assignments. However, if we consider more complex networks, the number of joint assignments is exponentially large while the number of potentials is typically reasonable, and each one involves the joint assignments to only a few variables. 

Note, however, that to use the formulation of proposition 8.1 we need to perform a global computation to ﬁnd the value of the partition function $Z$ as well as the marginal distribution over the scope of each potential $D_{k}$ . As we will see in later chapters, in some network structures, these computations can be done efciently. 

Terms such as ${\cal E}_{P}[-\ln\beta_{k}(D_{k})]$ − resemble th ntropy of $D_{k}$ . However, since the marginal over $D_{k}$ is usually not identical to the potential $\beta_{k}$ , such terms are not entropy terms. In some sense we can think of $\ln{\cal Z}$ as a correction for this discrepancy. For example, if we multiply all the entries of $\beta_{k}$ by a constant $c_{i}$ , the corresponding term $E_{P}[-\ln\beta_{k}(D_{k})]$ − will decrease by $\ln{c}$ . However, at the same time $\ln{\cal Z}$ will increase by the same constant, since it is canceled out in the normalization. 

# 8.4.1.2 Entropy of Bayesian Networks 

We now consider the entropy of a Bayesian network. Although we can address this computation using our general result in theorem 8.1, it turns out that the formulation for Bayesian networks is simpler. Intuitively, as we saw, we can represent Bayesian networks as an exponential family where the partition function is 1 . This removes the global term from the entropy. 

Theorem 8.2 If $\begin{array}{r}{P(\mathcal{X})=\prod_{i}P(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})}\end{array}$ | is a distribution consistent with a Bayesian network $\mathcal{G}$ , then 

$$
H_{P}(\mathcal{X})=\sum_{i}H_{P}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})
$$ 

Proof 

$$
\begin{array}{c c l}{H_{P}(\mathcal{X})}&{=}&{\displaystyle E_{P}[-\ln P(\mathcal{X})]}\\ &{=}&{\displaystyle E_{P}\left[-\sum_{i}\ln P(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})\right]}\\ &{=}&{\displaystyle\sum_{i}E_{P}\left[-\ln P(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})\right]}\\ &{=}&{\displaystyle\sum_{i}H_{P}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}}),}\end{array}
$$ 

where the ﬁrst and last steps invoke the deﬁnitions of entropy and conditional entropy. 

We see that the entropy of a Bayesian network decomposes as a sum of conditional entropies of the individual conditional distributions. This representation suggests that the entropy of a Bayesian network can be directly “read of” from the CPDs. This impression is misleading. Recall that the conditional entropy term $H_{P}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})$ | can be written as a weighted average of simpler entropies of conditional distributions 

$$
H_{P}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})=\sum_{\mathrm{pa}_{i}^{\mathcal{G}}}P(\mathrm{pa}_{i}^{\mathcal{G}})H_{P}(X_{i}\mid\mathrm{pa}_{i}^{\mathcal{G}}).
$$ 

While each of the simpler entropy terms in the summation can be computed based on the CPD entries alone, the weighting term $P(\mathrm{pa}_{i}^{\mathcal{G}})$ is a marginal over $\mathrm{pa}_{i}^{\mathcal{G}}$ of the joint distribution, and depends on other CPDs upstream of $X_{i}$ . Thus, computing the entropy of the network requires that we answer probability queries over the network. 

However, based on local considerations alone, we can analyze the amount of entropy intro- duced by each CPD, and thereby provide bounds on the overall entropy: 

Proposition 8.2 If $\begin{array}{r}{{\overline{{P(\mathcal{X})=\prod_{i}{P(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})}}}}}\end{array}$ | is a distribution consistent with a Bayesian network $\mathcal{G}$ , then $\sum_{i}\operatorname*{min}_{\mathrm{pa}_{i}^{\mathcal{G}}}H_{P}(X_{i}\mid\mathrm{pa}_{i}^{\mathcal{G}})\leq H_{P}(X)\leq\sum_{i}\operatorname*{max}_{\mathrm{pa}_{i}^{\mathcal{G}}}H_{P}(X_{i}\mid\mathrm{pa}_{i}^{\mathcal{G}}).$ 

Thus, if all the CPDs in a Bayesian network are almost deterministic (low conditional entropy given each parent conﬁguration), then the overall entropy of the network is small. Conversely, if all the CPDs are highly stochastic (high conditional entropy) then the overall entropy of the network is high. 

# 8.4.2 Relative Entropy 

A related notion is the relative entropy between models. This measure of distance plays an important role in many of the developments of later chapters. 

If we consider the relative entropy between an arbitrary distribution $Q$ and a distribution $P_{\theta}$ within an exponential family, we see that the form of $P_{\theta}$ can be exploited to simplify the form of the relative entropy. 

Theorem 8.3 Consider a distribution $Q$ and a distribution $P_{\theta}$ in an exponential family deﬁned by $\tau$ and t . Then 

$$
\begin{array}{r}{D(Q\|P_{\theta})=-H_{Q}(\mathcal{X})-\langle\mathbf{E}_{Q}[\tau(\mathcal{X})],\mathbf{t}(\theta)\rangle+\ln Z(\theta).}\end{array}
$$ 

The proof is left as an exercise (exercise 8.2). 

We see that the quantities of interest are again the expected sufcient statistics and the partition function. Unlike the entropy, in this case we compute the expectation of the sufcient statistics according to $Q$ . 

If both distributions are in the same exponential family, then we can further simplify the form of the relative entropy. 

# Theorem 8.4 

Consider two distribution $P_{\theta_{1}}$ and $P_{\theta_{2}}$ within the same exponential family. Then 

$$
D(P_{\theta_{1}}\|P_{\theta_{2}})=\langle E_{P_{\theta_{1}}}[{\boldsymbol{\tau}}(\mathcal{X})],\mathbf{t}(\theta_{1})-\mathbf{t}(\theta_{2})\rangle-\ln\frac{Z(\theta_{1})}{Z(\theta_{2})}
$$ 

Proof Combine theorem 8.3 with theorem 8.1. 

When we consider Bayesian networks, we can use the fact that the partition function is constant to simplify the terms in both results. 

# Theorem 8.5 

If $P$ is a distribution consistent with a Bayesian network $\mathcal{G}$ , then 

$$
D(Q\|P)=-H_{Q}(\mathcal{X})-\sum_{i}\sum_{\mathrm{pa}_{i}^{\mathcal{G}}}Q(\mathrm{pa}_{i}^{\mathcal{G}})E_{Q(X_{i}\mid\mathrm{pa}_{i}^{\mathcal{G}})}\bigl[\ln P(X_{i}\mid\mathrm{pa}_{i}^{\mathcal{G}})\bigr];
$$ 

If $Q$ is also consistent with $\mathcal{G}$ , then 

$$
D(Q\|P)=\sum_{i}\sum_{\mathrm{\scriptsize{pa}}_{i}^{\mathcal{G}}}Q(\mathrm{\scriptsize{pa}}_{i}^{\mathcal{G}})D(Q(X_{i}\mid\mathrm{\scriptsize{pa}}_{i}^{\mathcal{G}})\|P(X_{i}\mid\mathrm{\scriptsize{pa}}_{i}^{\mathcal{G}})).
$$ 

The second result shows that, analogously to the form of the entropy of Bayesian networks, we can write the relative entropy between two distributions consistent with $\mathcal{G}$ as a weighted sum of the relative entropies between the conditional distributions. These conditional relative entropies can be evaluated directly using the CPDs of the two networks. The weighting of these relative entropies depends on the the joint distribution $Q$ . 

# 8.5 Projections 

projection As we discuss in appendix A.1.3, we can view the relative entropy as a notion of distance between two distributions. We can therefore use it as the basis for an important operation — the projection operation — which we will utilize extensively in subsequent chapters. Similar to the geometric concept of projecting a point onto a hyperplane, we consider the problem of ﬁnding the distribution, within a given exponential family, that is closest to a given distribution 

in terms of relative entropy. For example, we want to perform such a projection when we approximate a complex distribution with one with a simple structure. As we will see, this is a crucial strategy for approximate inference in networks where exact inference is infeasible. In such an approximation we would like to ﬁnd the best (that is, closest) approximation within a family in which we can perform inference. Moreover, the problem of learning a graphical model can also be posed as a projection problem of the empirical distribution observed in the data onto a desired family. 

Suppose we have a distribution $P$ and we want to approximate it with another distribution $Q$ in a class of distr utions $\mathcal{Q}$ (for example, an exponential family). For example, we might want to approximate P with a product of marginal distributions. Because the notion of relative entropy is not symmetric, we can use it to deﬁne two types of approximations. 

# Deﬁnition 8.4 

I-projection Let $P$ be a distribution and let $\mathcal{Q}$ be a convex set of distributions. • The I-projection (information projection) of $P$ onto $\mathcal{Q}$ is the distribution 

$$
Q^{I}=\arg\operatorname*{min}_{Q\in\mathcal{Q}}D(Q\|P).
$$ 

M-projection 

$$
Q^{M}=\arg\operatorname*{min}_{Q\in\mathcal{Q}}D(P\|Q).
$$ 

# 8.5.1 Comparison 

We can think of both $Q^{I}$ d $Q^{M}$ as the ion of $P$ into the set $\mathcal{Q}$ in the sense that it is the istribution closest to P . Moreover, if $P\in{\mathcal{Q}},$ ∈Q , then in both deﬁnitions the projection would be P . However, because the relative entropy is not symmetric, these two projections are, in general, diferent. To understand the diferences between these two projections, let us consider a few examples. 

# Example 8.13 

Suppose we have a non-Gaussian distribution $P$ over the reals. We can consider the M-projection and the I-projection on the family of Gaussian distributions. As a concrete example, consider the distribution $P$ of ﬁgure 8.1. As we can see, the two projections are diferent Gaussian distributions. (The M-projection was found using the analytic form that we will discuss, and the I-projection by gradient ascent in the $(\mu,\sigma^{2})$ space.) Although the means of the two projected distributions are relatively close, the M-projection has larger variance than the I-projection. 

We can better understand these diferences if we examine the objective function optimized by each projection. Recall that the M-projection $Q^{M}$ minimizes 

$$
D(P\|Q)=-H_{P}(X)+E_{P}[-\ln Q(X)].
$$ 

We see that, in general, we want $Q^{M}$ to have high density in regions that are probable according to $P$ , since a small $-\ln Q(X)$ in these regions will lead to a smaller second term. At the same time, there is a high penalty for assigning low density to regions where $P(X)$ is nonnegligible. 

![](images/07a0bbbc14098fff74d423aceee4e554221fe4e72baf1ec72c1857ede3469763.jpg) 
Figure 8.1 Example of M- and I-projections into the family of Gaussian distributions 

As a consequence, although the M-projection attempts to match the main mass of $P$ , its high variance is a compromise to ensure that it assigns reasonably high density to all regions that are in the support of $P$ . 

On the other hand, the I-projection minimizes 

$$
D(Q\|P)=-H_{Q}(X)+E_{Q}[-\ln P(X)].
$$ 

Thus, the ﬁrst term incurs a penalty for low entropy, which in the case of a Gaussian $Q$ translates to a penalty on small variance. The second term, $E_{Q}[-\ln P(X)]$ − , encodes a preference for assigning higher density to regions where $P(X)$ is large and very low density to regions where $P(X)$ is small. Without the ﬁrst term, we can minimize the second by putting all of the mass of $Q$ on the most probable point according to $P$ . The compromise between the two terms results in the distribution we see in ﬁgure 8.1. 

A similar phenomenon occurs in discrete distributions. 

Now consider the projection of a distribution $P(A,B)$ onto the family of factored distributions $Q(A,B)=Q(A)Q(B)$ . Suppose $P(A,B)$ is the following distribution: 

$$
\begin{array}{l c l}{P(a^{0},b^{0})}&{=}&{0.45}\\ {P(a^{0},b^{1})}&{=}&{0.05}\\ {P(a^{1},b^{0})}&{=}&{0.05}\\ {P(a^{1},b^{1})}&{=}&{0.45.}\end{array}
$$ 

That is, the distribution $P$ puts almost all of the mass on the event $A=B$ . This distribution is $a$ particularly difcult one to approximate using a factored distribution, since in $P$ the two variables $A$ and $B$ are highly correlated, a dependency that cannot be captured using a fully factored $Q$ . 

Again, it is instructive to compare the M-projection and the I-projection of this distribution (see ﬁgure 8.2). It follows from example A.7 (appendix A.5.3) that the M-projection of this distribution is 

![](images/18dfb51306e564a9e83d5118cca7a87bbcfc40a36fba84c9e35d39134650762f.jpg) 
Figure 8.2 Example of $\mathbf{M}\mathbf{-}$ and I-projections of a two variable discrete distribution where $P(a^{0}=$ $b^{0})=P(a^{1}=b^{1})=0.45$ and $\bar{P(a^{0}=b^{1})}=P(a^{0}=b^{1})=0.05$ onto factorized distribution. Each axis denotes the probability of an instance: $P(a^{1},b^{1}),\,P(a^{1},b^{0}),$ , and $P(a^{0},b^{1})$ . The wire surfaces mark the region of legal distributions. The solid surface shows the distributions where $A$ and independent of $B$ . The points show $P$ and its two projections. 

the uniform distribution: 

$$
\begin{array}{l c l}{{Q^{M}(a^{0},b^{0})}}&{{=}}&{{0.5*0.5=0.25}}\\ {{Q^{M}(a^{0},b^{1})}}&{{=}}&{{0.5*0.5=0.25}}\\ {{Q^{M}(a^{1},b^{0})}}&{{=}}&{{0.5*0.5=0.25}}\\ {{Q^{M}(a^{1},b^{1})}}&{{=}}&{{0.5*0.5=0.25.}}\end{array}
$$ 

In contrast, the I-projection focuses on one of the two “modes” of the distribution, either when both $A$ and $B$ are true or when both are false. Since the distribution is symmetric about these modes, there are two I-projections. One of them is 

$Q^{I}(a^{0},b^{0})$ = $0.25*0.25=0.0625$ $Q^{I}(a^{0},b^{1})$ = $0.25*0.75=0.1875$ $Q^{I}(a^{1},b^{0})$ = $0.75*0.25=0.1875$ $Q^{I}(a^{1},b^{1})$ = $0.75*0.75=0.5625$ . 

The second I-projection is symmetric around the opposite mode $a^{0},b^{0}$ . 

As in example 8.13, we can understand these diferences by considering the underlying math- ematics. The M-projection attempts to give all assignments reasonably high probability,  whereas the I-projection attempts to focus on high-probability assignments in $P$ while maintaining a reasonable entropy. In this case, this behavior results in a uniform distribution for the M-projection, whereas the I-projection places most of the probability mass on one of the two assignments where $P$ has high probability. 

# 8.5.2 M-Projections 

Can we say more about the form of these projections? We start by considering M-projections onto a simple family of distributions. 

Let $P$ be a distribution over $X_{1},\ldots,X_{n},$ and let $\mathcal{Q}$ be the family of distributions consistent with $\mathcal{G}_{\varnothing}$ , the empty graph. Then 

$$
Q^{M}=\arg\operatorname*{min}_{Q\mid=\mathcal{G}_{\varnothing}}D(P\|Q)
$$ 

is the distribution: 

$$
Q^{M}(X_{1},\ldots,X_{n})=P(X_{1})P(X_{2})\cdot\cdot\cdot P(X_{n}).
$$ 

Proof Consider a distribution $Q\vDash\mathcal{G}_{\emptyset}$ . Since $Q$ factorizes, we can rewrite $D(P\|Q)$ : 

$$
\begin{array}{l c l}{D(P\|Q)}&{=}&{E_{P}[\ln P(X_{1},\ldots,X_{n})-\ln Q(X_{1},\ldots,X_{n})]}\\ &{=}&{E_{P}[\ln P(X_{1},\ldots,X_{n})]-\displaystyle\sum_{i}E_{P}[\ln Q(X_{i})]}\\ &{=}&{E_{P}\Big[\ln\displaystyle\frac{P(X_{1},\ldots,X_{n})}{P(X_{1})\cdot\cdot\cdot P(X_{n})}\Big]+\displaystyle\sum_{i}E_{P}\Big[\ln\displaystyle\frac{P(X_{i})}{Q(X_{i})}\Big]}\\ &{=}&{D(P\|Q^{M})+\displaystyle\sum_{i}D(P(X_{i})\|Q(X_{i}))}\\ &{\geq}&{D(P\|Q^{M}).}\end{array}
$$ 

The last step relies on the nonnegativity of the relative entropy. We conclude that $D(P\|Q)\geq$ | | ≥ $D(P\|Q^{M})$ | | with equality only if $Q(X_{i})=P(X_{i})$ for all $i$ . That is, only when $Q=Q^{M}$ . 

Hence, the M-projection of $P$ onto factored distribution is simply the product of marginals of $P$ . 

This theorem is an instance of a much more general result. To understand the generalization, we observe that the family $\mathcal{Q}$ of fully factored distri ons is characterized by a vector of sufcient statistics that simply counts, for each variable $X_{i}$ , the number of occurrences of each of its values. The marginal distributions over the $X_{i}$ ’s are simply the expectations, relative to $P$ , of these sufcient statistics. We see that, by selecting $Q$ to match these expectations, we obtain the M-projection. 

As we now show, this is not an accident. The characterization of a distribution $P$ that is relevant to computing its M-p jection into $\mathcal{Q}$ is precisely the expectation, relative to $P$ , of the sufcient statistic function of . 

Theorem 8.6 Let $P$ be a distribution over $\mathcal{X}$ , and le $\mathcal{Q}$ be an exponential family deﬁned by the functions $\tau(\xi)$ and ${\mathfrak{t}}(\theta)$ . If there is a set of parameters θ such that $E_{Q_{\theta}}[\tau(\mathcal{X})]=E_{P}[\tau(\mathcal{X})]$ , then the M-projection of $P$ is $Q_{\theta}$ . 

Proof Suppose that $E_{P}[\tau(\mathcal{X})]=E_{Q_{\theta}}[\tau(\mathcal{X})]$ , and let $\theta^{\prime}$ be some set of parameters. Then, 

$$
\begin{array}{l c l}{\displaystyle D(P\|Q_{\theta^{\prime}})-D(P\|Q_{\theta})}&{=}&{\displaystyle-H_{P}(\mathcal{X})-\langle E_{P}[\tau(\mathcal{X})],\mathbf{t}(\theta^{\prime})\rangle+\ln Z(\theta^{\prime})}\\ &&{\displaystyle+H_{P}(\mathcal{X})+\langle E_{P}[\tau(\mathcal{X})],\mathbf{t}(\theta)\rangle-\ln Z(\theta)}\\ {\displaystyle}&{=}&{\displaystyle\langle E_{P}[\tau(\mathcal{X})],\mathbf{t}(\theta)-\mathbf{t}(\theta^{\prime})\rangle-\ln\frac{Z(\theta)}{Z(\theta^{\prime})}}\\ {\displaystyle}&{=}&{\displaystyle\langle E_{Q_{\theta}}[\tau(\mathcal{X})],\mathbf{t}(\theta)-\mathbf{t}(\theta^{\prime})\rangle-\ln\frac{Z(\theta)}{Z(\theta^{\prime})}}\\ {\displaystyle}&{=}&{D(Q_{\theta}\|Q_{\theta^{\prime}})\geq0.}\end{array}
$$ 

We conclude that the M-projection of $P$ is $Q_{\theta}$ . 

expected sufcient statistics 

This theorem suggests that we can consider both the distribution $P$ and the distributions in $\mathcal{Q}$ in terms of the expectations of $\tau(\mathcal{X})$ . Thus, instead of describing a distribution in the family by the set of parameters, we can describe it in terms of the expected sufcient statistics . To formalize this intuition, we need some additional notation. We deﬁne a mapping from legal parameters in $\Theta$ to vectors of sufcient statistics 

$$
\begin{array}{r}{e s s(\theta)=E_{Q_{\theta}}[\tau(\mathcal{X})].}\end{array}
$$ 

Theorem 8.6 shows that if $E_{P}[\tau(\mathcal{X})]$ X is in the image of ess , the the M-projection of $P$ is the distribution $Q_{\theta}$ that matches the expected sufcient statistics of P . In other words, 

$$
E_{Q^{M}}[\tau(\mathcal{X})]=E_{P}[\tau(\mathcal{X})].
$$ 

moment matching This result explains why M-projection is also referred to as moment matching . In many ex- ponential families the sufcient statistics are moments (mean, variance, and so forth) of the distribution. In such cases, the M-projection of $P$ is the distribution in the family that matches these moments in $P$ . 

We illustrate these concepts in ﬁgure 8.3. As we can see, the mapping $e s s(\pmb{\theta})$ directly relates parameters to expected sufcient statistics. By comparing the expected sufcient statistics of $P$ to these of distributions in $\mathcal{Q}$ , we can ﬁnd the M-projection. 

Moreover, using theorem 8.6, we obtain a general characterization of the M-projection function M-project $(s)$ , which maps a vector of expected sufcient statistics to a parameter vector: 

Corollary 8.1 

$$
\operatorname{M-projec}(s)=e s s^{-1}(s).
$$ 

That is, the parameters of the M-projection of $P$ are simply the inverse of the ess mapping, applied to the expected sufcient statistic vector of $P$ . This result allows us to describe the M-projection operation in terms of a speciﬁc function. This result assumes, of course, that $E_{P}[\tau]$ is in the image of ess and that ess is invertible. In many examples that we consider, the image of ess includes all possible vectors of expected sufcient statistics we might encounter. Moreover, if the parameter iz ation is nonredundant, then ess is invertible. 

![](images/3094ffea74ac85041c68b777c990b445795d43a2f708110ce26dd2e69f20c1e0.jpg) 
Figure 8.3 Illustration of the relations between parameters, distributions and expected sufcient statistics. Each parameter corresponds to a distribution, which in turn corresponds to a value of the expected statistics. The function ess maps parameters directly to expected statistics. If the expected statistics of $P$ and $Q_{\theta}$ match, then $Q_{\theta}$ is the M-projection of $P$ . 

Consider the exponential family of Gaussian distributions. Recall that the sufcient statistics func- tion for this family is $\tau(x)\,=\,\langle x,x^{2}\rangle$ . Given parameters $\theta\,=\,\langle\mu,\sigma^{2}\rangle$ , the expected value of $\tau$ is 

$$
e s s(\langle\mu,\sigma^{2}\rangle)={\cal E}_{Q_{\langle\mu,\sigma^{2}\rangle}}[\tau(X)]=\langle\;\mu,\sigma^{2}+\mu^{2}\rangle.
$$ 

It is not difcult to show that, for any distribution $P$ , $\pmb{E}_{P}[\tau(X)]$ must be in the image of this function (see exercise 8.4). Thus, for any choice of $P$ , we can apply theorem 8.6. Finally, we can easily invert this function: 

$$
\begin{array}{r}{\mathrm{M-product}(\langle s_{1},s_{2}\rangle)=e s s^{-1}(\langle s_{1},s_{2}\rangle)=\langle s_{1},s_{2}-s_{1}^{2}\rangle.}\end{array}
$$ 

Recall that $s_{1}=E_{P}[X]$ and $s_{2}=E_{P}\big[X^{2}\big]$   . Thus, the estimated parameters are the mean and variance of $X$ according to $P$ , as we would expect. 

This example shows that the “naive” choice of Gaussian distribution, obtained by matching the mean and variance of a variable $X$ , provides the best Gaussian approximation (in the M- projection sense) to a non-Gaussian distribution over $X$ . We have also provided a solution to the M-projection problem in the case of a factored product of multinomials, in proposition 8.3, which can be viewed as a special case of theorem 8.6. In a more general application of this result, we show in section 11.4.4 a general result on the form of the M-projection for a linear exponential family over discrete state space, including the class of Markov networks. 

The analysis for other families of distributions can be subtler. 

We now consider a more complex example of M-projection onto a chain network. Suppose we have a distribution $P$ over variables $X_{1},\dots,X_{n},$ and want to project it onto the family of distributions $Q$ of the distributions that are consistent with the network structure $X_{1}\to X_{2}\to\dots\to X_{n}$ . 

What are the sufcient statistics for this network? Based on our previous discussion, we see that each conditional distribution $Q(X_{i+1}\mid X_{i})$ requires a statistic of the form 

$$
\tau_{x_{i},x_{i+1}}(\xi)={\mathbf{I}\{X_{i}=x_{i},X_{i+1}=x_{i+1}\}}\ \ \forall\langle x_{i},x_{i+1}\rangle\in V a l(X_{i})\times V a l(X_{i+1}).
$$ 

These statistics are sufcient but are redundant. To see this, note that the “marginal statistics” must agree. That is, 

$$
\sum_{x_{i}}\tau_{x_{i},x_{i+1}}(\xi)=\sum_{x_{i+2}}\tau_{x_{i+1},x_{i+2}}(\xi)\ \forall x_{i+1}\in\mathit{V a l}(X_{i+1}).
$$ 

Although this representation is redundant, we can still apply the mechanisms discussed earlier and consider the function ess that maps parameters of such a network to the sufcient statistics. The expectation of an indicator function is the marginal probability of that event, so that 

$$
\begin{array}{r}{E_{Q_{\theta}}\bigl[\tau_{x_{i},x_{i+1}}(\mathcal{X})\bigr]=Q_{\theta}(x_{i},x_{i+1}).}\end{array}
$$ 

Thus, the function ess simply maps from $\theta$ to the pairwise marginals of consecutive variables in $Q_{\theta}$ . Because these are pairwise marginals of an actual distribution, it follows that these sufcient statistics satisfy the consistency constraints of equation (8.8). 

How do we invert this function? Given the statistics from $P$ , we want to ﬁnd a distribution $Q$ that matches them. We start building $Q$ along the structure of the chain. We choose $Q(X_{1})$ and $Q(X_{2}\mid X_{1})$ so that $Q(x_{1},x_{2})\,=\,E_{P}[\tau_{x_{1},x_{2}}(\mathcal{X})]\,=\,P(x_{1},x_{2})\,$ X ) . In fact, there is a unique choice that satisﬁes this equality, where $Q(X_{1},X_{2})\,=\,P(X_{1},X_{2})$ . This choice implies that the marginal distribution $Q(X_{2})$ matches the marginal distribution $P(X_{2})$ . Now, consider our choice of $Q(X_{3}\mid X_{2})$ . We need to ensure that 

$$
Q(x_{3},x_{2})=E_{P}[\tau_{x_{2},x_{3}}(\mathcal{X})]=P(x_{2},x_{3}).
$$ 

We note that, beca $Q(x_{3},x_{2})=Q(x_{3}\mid x_{2})Q(x_{2})=Q(x_{3}\mid x_{2})P(x_{2})$ e this equality by setting $Q(x_{3}\mid x_{2})=P(x_{3}\mid x_{2})$ | | . Moreover, this implies that $Q(x_{3})=P(x_{3})$ . We can continue this construction recursively to set 

$$
Q(x_{i+1}\mid x_{i})=P(x_{i+1}\mid x_{i}).
$$ 

Using the preceding argument, we can show that this choice will match the sufcient statistics of $P$ . This sufces to show that this $Q$ is the $M_{\sun}$ -projection of $P$ . 

Note that, although this choice of $Q$ coincides with $P$ on pairwise marginals of consecutive variables, it does not necessarily agree with $P$ on other marginals. As an extreme example, consider a distribution $P$ where $X_{1}$ and $X_{3}$ are identical and both are independent of $X_{2}$ . If we project this distribution onto a distribution $Q$ with th ture $X_{1}\rightarrow X_{2}\rightarrow X_{3}$ , then $P$ and $Q$ will not necessarily agree on the joint marginals of $X_{1},X_{3}$ . In Q this distribution will be 

$$
Q(x_{1},x_{3})=\sum_{x_{2}}Q(x_{1},x_{2})Q(x_{3}\mid x_{2}).
$$ 

# Since $Q(x_{1},x_{2})\,=\,P(x_{1},x_{2})\,=\,P(x_{1})P(x_{2})$ and $Q(x_{3}\mid x_{2})\,=\,P(x_{3}\mid x_{2})\,=\,P(x_{3})$ , we conclude that $Q(x_{1},x_{3})=P(x_{1})P(x_{3})$ , losing the equality between $X_{1}$ and $X_{3}$ in P . 

This analysis used a redundant parameter iz ation; exercise 8.6 shows how we can reparam- eterize a directed chain within the linear exponential family and thereby obtain an alternative perspective on the M-projection operation. 

So far, all of our examples have had the characteristic that the vector of expected sufcient statistics for a distribution $P$ is always in the image of ess ; thus, our task has only been to invert ess . Unfortunately, there are examples where not every vector of expected sufcient statistics can also be derived from a distribution in our exponential family. 

Example 8.17 Consider ily $\mathcal{Q}$ from example 8.10, of distributions parameterized using network structure A $A\rightarrow C\leftarrow B$ → ← , with binary variables $A,B,C$ . We can that e sufcient statistics for this distribution are indicators for all the joint assignments to $A,B$ , and C except one. That is, 

$$
\begin{array}{r l}{\tau(A,B,C)}&{=\langle\quad\pmb{I}\{A=a^{1},B=b^{1},C=c^{1}\},}\\ &{\quad\pmb{I}\{A=a^{0},B=b^{1},C=c^{1}\},}\\ &{\quad\pmb{I}\{A=a^{1},B=b^{0},C=c^{1}\},}\\ &{\quad\pmb{I}\{A=a^{1},B=b^{1},C=c^{0}\},}\\ &{\quad\pmb{I}\{A=a^{1},B=b^{0},C=c^{0}\},}\\ &{\quad\pmb{I}\{A=a^{0},B=b^{1},C=c^{0}\},}\\ &{\quad\pmb{I}\{A=a^{0},B=b^{0},C=c^{1}\}\rangle.}\end{array}
$$ 

If we look at the expected value of these statistics given some member of the family, we have that, since $A$ and $B$ are independent in $Q_{\theta}$ $,\,Q_{\theta}\bigl(a^{1},b^{1}\bigr)=Q_{\theta}\bigl(a^{1}\bigr)Q_{\theta}\bigl(b^{1}\bigr)$ . Thus, the expected statistics should satisfy 

$$
\begin{array}{r l}&{E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{1},C=c^{1}\}}}\right]+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{1},C=c^{0}\}}}\right]=}\\ &{\qquad\left(E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{1},C=c^{1}\}}}\right]+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{1},C=c^{0}\}}}\right]\right.}\\ &{\qquad\qquad+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{0},C=c^{1}\}}}\right]+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{0},C=c^{0}\}}}\right]\right)}\\ &{\qquad\left(E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{1},C=c^{1}\}}}\right]+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{1},B=b^{1},C=c^{0}\}}}\right]\right.}\\ &{\qquad\qquad\left.+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{0},B=b^{1},C=c^{1}\}}}\right]+E_{Q_{\theta}}\left[{\pmb{I}{\{A=a^{0},B=b^{1},C=c^{0}\}}}\right]\right)}\end{array}
$$ 

This constraint is not typically satisﬁed by the expected statistics from a general distribution $P$ we might consider projecting. Thus, in this case, there are expected statistics vectors that do not fall within the image of ess. 

In such cases, and in Bayesian networks in general, the projection procedure is more complex than inverting the ess function. Nevertheless, we can show that the projection operation still has an analytic solution. 

# Theorem 8.7 

t $P$ be a d bution over $X_{1},\dots,X_{n}.$ , and let $\mathcal{G}$ be a Bayesian network structure. Then the M-projection $Q^{M}$ is: 

$$
Q^{M}(X_{1},\ldots,X_{n})=\prod_{i}P(X_{i}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}}).
$$ 

Because the mapping ess for Bayesian networks is not invertible, the proof of this result (see exercise 8.5) does not build on theorem 8.6 but rather directly on theorem 8.5. This result turns out to be central to our derivation of Bayesian network learning in chapter 17. 

# 8.5.3 I-Projections 

What about I-projections? Recall that 

$$
\begin{array}{r}{D(Q\|P)=-H_{Q}(\mathcal{X})-E_{Q}[\ln P(\mathcal{X})].}\end{array}
$$ 

If $Q$ is in some exponential family, we can use the derivation of theorem 8.1 to simplify the entropy term. However, the exponential form of $Q$ does not provide insights into the second term. When dealing with the I-projection of a general distribution $P$ , we are left without further simpliﬁcations. However, if the distribution $P$ has some structure, we might be able to simplify $E_{Q}[\ln P(\mathcal{X})]$ X into simpler terms, although the projection problem is still a nontrivial one. We discuss this problem in much more detail in chapter 11. 

# 8.6 Summary 

In this chapter, we presented some of the basic technical concepts that underlie many of the techniques we explore in depth later in the book. We deﬁned the formalism of exponential families, which provides the fundamental basis for considering families of related distributions. We also deﬁned the subclass of linear exponential families, which are signiﬁcantly simpler and yet cover a large fraction of the distributions that arise in practice. 

We discussed how the types of distributions described so far in this book ﬁt into this frame- work, showing that Gaussians, linear Gaussians, and multinomials are all in the linear exponen- tial family. Any class of distributions representable by parameterizing a Markov network of some ﬁxed structure is also in the linear exponential family. By contrast, the class of distributions representable by a Bayesian network of some ﬁxed structure is in the exponential family, but is not in the linear exponential family when the network structure includes an immorality. 

We showed how we can use the formulation of an exponential family to facilitate computations such as the entropy of a distribution or the relative entropy between two distributions. The latter computation formed the basis for analyzing a basic operation on distributions: that of projecting general distributi $P$ into some exponential family $\mathcal{Q}_{i}$ , that is, ﬁnding the distribution within Q that is closest to P . Because the notion of relative entropy is not symmetric, this concept gave rise to two diferent deﬁnitions: I-projection, where we minimize $D(Q\|P)$ | | , and M-projection, where we minimize $D(P\|Q)$ | | . We analyzed the diferences between these two deﬁnitions and showed that solving the M-projection problem can be viewed in a particularly elegant way, constructing a distribution $Q$ that matches the expected sufcient statistics (or moments) of $P$ . 

As we discuss later in the book, both the I-projection and M-projection turn out to play an important role in graphical models. The M-projection is the formal foundation for addressing the learning problem: there, our goal is to ﬁnd a distribution in a particular class (for example, a Bayesian network or Markov network of a given structure) that is closest (in the M-projection sense) to the empirical distribution observed in a data set from which we wish to learn (see equation (16.4)). The I-projection operation is used when we wish to take a given graphical model $P$ and answer probability queries; when $P$ is too complex to allow queries to be answered efciently, one strategy is to construct a simpler distribution $Q$ , which is a good approximation to $P$ (in the I-projection sense). 

# 8.7 Relevant Literature 

The concept of exponential families plays a central role in formal statistic theory. Much of the theory is covered by classic textbooks such as Barndorf-Nielsen (1978). See also Lauritzen (1996). Geiger and Meek (1998) discuss the representation of graphical models as exponential families and show that a Bayesian network usually does not deﬁne a linear exponential family. 

The notion of I-projections was introduced by Csiszàr (1975), who developed the “information geometry” of such projections and their connection to diferent estimation procedures. In his terminology, M-projections are called “reverse I-projections.” The notion of M-projection is closely related to parameter learning, which we revisit in chapter 17 and chapter 20. 

# 8.8 Exercises 

# Exercise $8.1\star$ 

Poisson distribution A variable $X$ with $V a l(X)=0,1,2,.\,.\,.\,.$ is Poisson-distributed with parameter $\theta>0$ if 

$$
P(X=k)={\frac{1}{k!}}\exp{-\theta\theta^{k}}.
$$ 

This distribution has the property that $\pmb{{\cal E}}_{P}[X]=\theta$ . 

a. Show how to represent the Poisson distribution as a linear exponential family. (Note that unlike most of our running examples, you need to use the auxiliary measure $A$ in the deﬁnition.) b. Use results developed in this chapter to ﬁnd the entropy of a Poisson distribution and the relative entropy between two Poisson distributions. c. What is the function ess associated with this family? Is it invertible? 

# Exercise 8.2 

Prove theorem 8.3. 

# Exercise 8.3 

In this exercise, we will provide a characterization of when two distributions $P_{1}$ and $P_{2}$ will have the same M-projection. 

a. $P_{1}$ an $P_{2}$ be $\mathcal{X}$ and let $\mathcal{Q}$ be an exponentia amily ﬁned the functions $\tau(\xi)$ and ${\mathfrak{t}}(\theta)$ . If I $E_{P_{1}}[\tau(\mathcal{X})]=E_{P_{2}}[\tau(\mathcal{X})]$ X X , then the M-projection of P $P_{1}$ and P $P_{2}$ 2 onto Q is identical. b. Now, show that if the function $e s s(\pmb{\theta})$ is invertible, then we can prove the converse, showing that the M-projection of $P_{1}$ and $P_{2}$ is identical only if $\pmb{E}_{P_{1}}[\tau(\mathcal{X})]=\pmb{E}_{P_{2}}[\hat{\tau}(\mathcal{X})]$ X X . Conclude that this is the case for linear exponential families. 

# Exercise 8.4 

Consider the function ess for Gaussian variables as described in example 8.15. 

a. What is the image of ess ? 

b. Consider terms of the form $\pmb{E}_{P}[\tau(X)]$ for the Gaussian sufcient statistics from that example. Show that for any distribution $P$ , the expected sufcient statistics is in the image of ess . 

# Exercise ${\bf8.5\star}$ 

Prove theorem 8.7. (Hint: Use theorem 8.5.) 

# Exercise ${\bf8.6\star}$ 

Let $X_{1},\dotsc,X_{n}$ a family $\mathcal{Q}$ of chain distributions of the form $Q(X_{1},\dotsc,X_{\mathfrak{n}})=Q(X_{1})Q(X_{2}\mid X_{1})\dot{\cdot}\cdot\cdot Q(X_{\mathfrak{n}}\mid\breve{X}_{\mathfrak{n}-1})$ | · · · | − ) . We now show how to reformulate this family as a linear exponential family. 

a. Show that the following vector of statistics is su￿cient and nonredundant for distributions in the family: 

$$
\tau(X_{1},\dots,X_{\mathsf{n}})=\left(\begin{array}{l}{\mathbf{\mathit{I}}\{X_{1}=x_{1}^{\uparrow}\},}\\ {\dots}\\ {\mathbf{\mathit{I}}\{X_{\mathsf{n}}=x_{\mathsf{n}}^{\uparrow}\},}\\ {\mathbf{\mathit{I}}\{X_{1}=x_{1}^{\uparrow},X_{2}=x_{2}^{\uparrow}\},}\\ {\dots}\\ {\mathbf{\mathit{I}}\{X_{\mathsf{n-1}}=x_{\mathsf{n-1}}^{\uparrow},X_{\mathsf{n}}=x_{\mathsf{n}}^{\uparrow}\}}\end{array}\right).
$$ 

b. econstruct the distributions $Q(X_{1})$ and $Q\big(X_{\mathbf{i}+1}\mid X_{\mathbf{i}}\big)$ from the the expec ion $E_{\mathsf{Q}}[\tau(X_{1},\dot{.}\,.\,,X_{\mathsf{n}})]$ . This shows that given the expected su￿cient statistics you can reconstruct Q . c. Suppose you know $Q$ . Show how to reparameterize it as a linear exponential model 

$$
Q(X_{1},\ldots,X_{\mathsf{n}})={\frac{1}{Z}}\exp\left\{\sum_{\mathbf{i}}\theta_{\mathbf{i}}I\{X_{\mathbf{i}}=x_{\mathbf{i}}^{1}\}+\sum_{\mathbf{i}}\theta_{\mathbf{i},\mathbf{i}+1}I\{X_{\mathbf{i}}=x_{\mathbf{i}}^{1},X_{\mathbf{i}+1}=x_{\mathbf{i}+1}^{1}\}\right\}{\mathrm{~and~}}x_{\mathbf{i}}.
$$ 

Note that, because the statistics are su￿cient, we know that there are some parameters for which we get equality; the question is to determine their values. Speciﬁcally, show that if we choose: 

$$
\theta_{\mathbf{i}}=\mathsf{I n}\,\frac{Q(x_{1}^{0},.\,.\,,x_{\mathbf{i}-1}^{0},x_{\mathbf{i}}^{1},x_{\mathbf{i}+1}^{0},.\,.\,.\,,x_{\mathsf{n}}^{0})}{Q(x_{1}^{0},.\,.\,.\,,x_{\mathsf{n}}^{0})}
$$ 

and 

$$
\theta_{\mathbf{i},\mathbf{i}+1}=\mathsf{I n}\,\frac{Q(x_{1}^{0},\ldots,x_{\mathbf{i}-1}^{0},x_{\mathbf{i}}^{1},x_{\mathbf{i}+1}^{1}x_{\mathbf{i}+2}^{0},\ldots,x_{\mathsf{n}}^{0})}{Q(x_{1}^{0},\ldots,x_{\mathsf{n}}^{0})}-\theta_{\mathbf{i}}-\theta_{\mathbf{i}+1}
$$ 

then we get equality in equation (8.9) for all assignments to $X_{1},\dotsc,X_{n}$ . 

# Part II 

# Inference 

# 9 

# Exact Inference: Variable Elimination 

In this chapter, we discuss the problem of performing inference in graphical models. We show that the structure of the network, both the conditional independence assertions it makes and the associated factorization of the joint distribution, is critical to our ability to perform inference efectively, allowing tractable inference even in complex networks. 

conditional probability query 

Our focus in this chapter is on the most common query type: the conditional probability query , $P(Y\mid E=e)$ (see section 2.1.5). We have already seen several examples of conditional probability queries in chapter 3 and chapter 4; as we saw, such queries allow for many useful reasoning patterns, including explanation, prediction, intercausal reasoning, and many more. By the deﬁnition of conditional probability, we know that 

$$
P(Y\mid E=e)=\frac{P(Y,e)}{P(e)}.
$$ 

Each of the instantiations of the numerator is a probability expression $P(\pmb{y},e)$ , which can be computed by summing out all entries in the joint that correspond to assignments consistent with $\mathbfit{\Delta}_{\mathcal{Y},\mathbf{\Delta}e}$ . More precisely, let $W=\mathcal{X}-Y-E$ be the random variables that are neither query nor evidence. Then 

$$
P(\pmb{y},e)=\sum_{\pmb{w}}P(\pmb{y},e,\pmb{w}).
$$ 

Because $Y,E,W$ are all of the network variables, each term $P(\pmb{y},\pmb{e},\pmb{w})$ in the summation is simply an entry in the joint distribution. 

The probability $P(e)$ can also be computed directly by summing out the joint. However, it can also be computed as 

$$
P(e)=\sum_{y}P(y,e),
$$ 

renormalization which allows us to reuse our computation for equation (9.2). If we compute both equation (9.2) and equation (9.3), we can then divide each $P(\pmb{y},e)$ by $P(e)$ , to get the desired conditional probability $P(\pmb{y}\mid\pmb{e})$ Note that this process corresponds to taking the vector of marginal probabilities $P(\pmb{y}^{1},\pmb{e}),\dots,P(\pmb{y}^{k},\pmb{e})$ (where $k\,=\,|\mathit{V a l}(Y)|)$ and renormalizing the entries to sum to 1 . 

# 9.1 Analysis of Complexity 

In principle, a graphical model can be used to answer all of the query types described earlier. We simply generate the joint distribution and exhaustively sum out the joint (in the case of a conditional probability query), search for the most likely entry (in the case of a MAP query), or both (in the case of a marginal MAP query). However, this approach to the inference problem is not very satisfactory, since it returns us to the exponential blowup of the joint distribution that the graphical model representation was precisely designed to avoid. 

Unfortunately, we now show that exponential blowup of the inference task is (almost  certainly) unavoidable in the worst case: The problem of inference in graphical models is $\mathcal{N P}$ -hard, and therefore ly requires exponential time in the worst ca except in the unlikely event that P $\mathcal{P}=\mathcal{N P}$ NP ). Even worse, approximate inference is also NP -hard. Importantly, however, the story does not end with this negative result. In general, we care not about the worst case, but about the cases that we encounter in practice. As we show in the remainder of this part of the book, many real-world applications can be tackled very efectively using exact or approximate inference algorithms for graphical models. 

In our theoretical analysis, we focus our discussion on Bayesian networks. Because any Bayesian network can be encoded as a Markov network with no increase in its representation size, a hardness proof for inference in Bayesian networks immediately implies hardness of inference in Markov networks. 

# 9.1.1 Analysis of Exact Inference 

To address the question of the complexity of BN inference, we need to address the question of how we encode a Bayesian network. Without going into too much detail, we can assume that the encoding speciﬁes the DAG structure and the CPDs. For the following results, we assume the worst-case representation of a CPD as a full table of size $|V a l(\{X_{i}\}\cup\mathrm{Pa}_{X_{i}})|$ . 

As we discuss in appendix A.3.4, most analyses of complexity are stated in terms of decision problems. We therefore begin with a formulation of the inference problem as a decision prob- lem, and then discuss the numerical version. One natural decision version of the conditional probability task is the problem $B N–P r–D P,$ , deﬁned as follows: 

Given a $\mathcal{B}$ over $\mathcal{X}$ , a variable $X\in{\mathcal{X}}$ , and a value $x\in V a l(X)$ , decide whether $P_{\mathcal{B}}(X=x)>0$ . 

# Theorem 9.1 

Proof It is straightforward to prove that $B N–P r–D P$ is in $\mathcal{N P}$ : In the guessing phase, we full assignment $\xi$ to the network variables. In the veriﬁcation phase, we check whether X $X=x$ in $\xi;$ , and whether $P(\xi)\,>\,0$ . One of these guesses succeeds if and only if $P(X\,=\,x)\,>\,0$ . Computing $P(\xi)$ for a full assignment of the network variables requires only that we multiply the relevant entries in the factors, as per the chain rule for Bayesian networks, and hence can be done in linear time. 

To prove $\mathcal{N P}$ -hardness, we need to show that, if we can answer instances in BN-Pr-DP , we can use that as a subroutine to answer questions in a class of problems that is known to be $\mathcal{N P}$ -hard. We will use a reduction from the 3-SAT problem deﬁned in deﬁnition A.8. 

![](images/b75ddde8f0c0af7656fa1e96b577b56a7d34cae1b516a18d67fb0541decb04cc.jpg) 
Figure 9.1 An outline of the network structure used in the reduction of 3-SAT to Bayesian network inference. 

To show the reduction, we show the following: Given any 3-SAT formula $\phi$ , we can create a Bayesian network $\mathcal{B}_{\phi}$ with some distinguished variable $X$ , such that $\phi$ is satisﬁable if and only if $P_{\mathcal{B}_{\phi}}(X=x^{1})>0$ . Thus, if we can solve the Bayesian network inference problem in polynomial time, we can also solve the 3-SAT problem in polynomial time. To enable this conclusion, our BN $\mathcal{B}_{\phi}$ has to be constructible in time that is polynomial in the length of the formula $\phi$ . 

Consider a 3-SAT instance $\phi$ over the propositional variables $q_{1},\ldots,q_{n}$ . Figure 9.1 illustrates the structure of the network constr ted in this reduction. Our Bayesian network $\mathcal{B}_{\phi}$ has a node $Q_{k}$ for each propositional variable $q_{k}$ ; these variables are roots, with $P(q_{k}^{1})=0.5$ . It also has a no $C_{i}$ for each cl e $C_{i}$ . There is an edge from $Q_{k}$ to $C_{i}$ if $q_{k}$ or $\neg q_{k}$ is one of the literals in $C_{i}$ . The CPD for $C_{i}$ is deterministic, and chosen such that it exactly duplicates the behavior of the clause. Note that, because $C_{i}$ contains at most three variables, the CPD has at most eight distributions, and at most sixteen entries. 

We want to introduce a variable $X$ that has the value 1 if and only if all the $C_{i}$ ’s have the value 1 . We can achieve this requirement by having $C_{1},\ldots,C_{m}$ be parents of $X$ . This construction, however, has the property that $P(X\mid C_{1},.\,.\,,C_{m})$ is exponentially large when written as a table. To avoid this difculty, we introduce intermediate “AND” gates $A_{1},\dots,A_{m-2}$ , so that $A_{1}$ is the “AND” of $C_{1}$ and $C_{2}$ , $A_{2}$ is the “AND” of $A_{1}$ and $C_{3}$ , and so on. The last variable $X$ is the “AND” of $A_{m-2}$ and $C_{m}$ . This construction achieves the desired efect: $X$ has value 1 if and only if all the clauses are satisﬁed. Furthermore, in this construction, all variables have at most three (binary-valued) parents, so that the size of $\mathcal{B}_{\phi}$ is polynomial in the size of $\phi$ . It follows that $P_{\mathcal{B}_{\phi}}(x^{1}\mid q_{1},.\,.\,.\,,q_{n})=1$ if and only if $q_{1},\ldots,q_{n}$ is a satisfying assignment for $\phi$ . Because the prior probability of each possible assignment is $1/2^{n}$ , we get that the overall probability $P_{\mathcal{B}_{\phi}}(x^{1})$ is the number of satisfying assignments to $\phi$ , divided by $2^{n}$ . We can therefore test whether $\phi$ has a satisfying assignment simply by checking whether $P(x^{1})>0$ . 

This analysis shows that the decision problem associated with Bayesian network inference is $\mathcal{N P}$ -complete. However, the problem is originally a numerical problem. Precisely the same construction allows us to provide an analysis for the original problem formulation. We deﬁne the problem $B N!P r$ as follows: 

Given: a Bayesian network $\mathcal{B}$ over $\mathcal{X}$ , a variable $X\,\in\,{\mathcal{X}}$ , and a value $x\;\in\;V a l(X)$ , compute $P_{\mathcal{B}}(X=x)$ . 

Our task here is to compute the total probability of network instantiations that are consistent with $X=x$ . Or, in other words, to do a weighted count of instantiations, with the weight being the probability. An appropriate complexity class for counting problems is $\#\mathcal{P}$ : Whereas $\mathcal{N P}$ represents problems of deciding “are there any solutions that satisfy certain requirements,” $\#\mathcal{P}$ P represents problems that ask “how many solutions are there that satisfy certain requirements.” It is not surprising that we can relate the complexity of the BN inference problem to the counting class $\#\mathcal{P}$ : 

The problem BN-Pr is $\#\mathcal{P}$ -complete. We leave the proof as an exercise (exercise 9.1). 

# 9.1.2 Analysis of Approximate Inference 

Upon noting the hardness of exact inference, a natural question is whether we can circumvent the difculties by compromising, to some extent, on the accuracies of our answers. Indeed, in many applications we can tolerate some imprecision in the ﬁnal probabilities: it is often unlikely that a change in probability from 0 . 87 to 0 . 92 will change our course of action. Thus, we now explore the computational complexity of approximate inference. 

To analyze the approximate inference task formally, we must ﬁrst deﬁne a metric for evaluating the quality of our approximation. We can consider two perspectives on this issue, depending on how we choose to deﬁne our query. Consider ﬁrst our previous formulation of the conditional probabilit query task, wh e our goal is to compute the probability $P(Y\mid e)$ for some set of variables Y $Y$ and evidence e . The result of this type of query is a probability distribution over $Y$ . Given an approximate answer to this query, we can evaluate its quality using any of the distance metrics we deﬁne for probability distributions in appendix A.1.3.3. 

There is, however, another way of looking at this task, one that is somewhat simpler and will be very useful for analyzing its complexity. Consider a speciﬁc query $P(\pmb{y}\mid\pmb{e})$ , where we are focusing on one particular assignment $_{_y}$ . The approximate answer to this query is a number $\rho$ , whose accuracy we wish to evaluate relative to the correct probability. One way of evaluating the accuracy of an estimate is as simple as the diference between the approximate answer and the right one. 

$$
|P(\pmb{y}\mid e)-\rho|\leq\epsilon.
$$ 

This deﬁnition, although plausible, is somewhat weak. Consider, for example, a situation in which we are trying to compute the probability of a really rare disease, one whose true probability is, say, 0 . 00001 . In this case, an absolute error of 0 . 0001 is unacceptable, even though such an error may be an excellent approximation for an event whose probability is 0 . 3 . A stronger deﬁnition of accuracy takes into consideration the value of the probability that we are trying to estimate: 

Deﬁnition 9.2 relative error 

An estimate $\rho$ has relative error $\epsilon$ for $P(\pmb{y}\mid\pmb{e})$ if: 

$$
\frac{\rho}{1+\epsilon}\le P(\pmb{y}\mid e)\le\rho(1+\epsilon).
$$ 

Note that, unlike absolute error, relative error makes sense even for $\epsilon>1$ . For example, $\epsilon=4$ means that $P(\pmb{y}\mid e)$ is at least 20 percent of $\rho$ and at most 600 percent of $\rho$ . For probabilities, where low values are often very important, relative error appears much more relevant than absolute error. 

With these deﬁnitions, we can turn to answering the question of whether approximate in- ference is actually an easier problem. A priori, it seems as if the extra slack provided by the approximation might help. Unfortunately, this hope turns out to be unfounded. As we now show, approximate inference in Bayesian networks is also $\mathcal{N P}$ -hard. 

This result is straightforward for the case of relative error. 

Theorem 9.3 The following problem is -hard: 

Given a Bayesian network $\mathcal{B}$ ove $\mathcal{X}$ , a variable $X\in{\mathcal{X}}$ , and a value $x\in V a l(X)$ , ﬁnd a number $\rho$ that has relative error ϵ for $P_{\mathcal{B}}(X=x)$ . 

Proof The proof is obvious based on the original $\mathcal{N P}$ k inference (theorem 9.1). There, we proved that it is NP -hard to decide whethe $P_{\mathcal{B}}(x^{1})>0$ . Now, assume that we have an algorithm that returns an estimate $\rho$ to the same $P_{\mathcal{B}}(x^{1})$ , which B is guaranteed to have relative error $\epsilon$ for some $\epsilon>0$ . Then $\rho>0$ if and only if $P_{\mathcal{B}}(x^{1})>0$ . Thus, achieving this relative error is as $\mathcal{N P}$ -hard as the original problem. 

We can generalize this result to make $\epsilon(n)$ a function that grows with the input size $n$ . Thus, for example, we can deﬁne $\epsilon(n)=2^{2^{n}}$ and the theorem still holds. Thus, in a sense, this result is not so interesting as a statement about hardness of approximation. Rather, it tells us that relative error is too strong a notion of approximation to use in this context. 

What about absolute error? As we will see in section 12.1.2, the problem of just approximating $P(X\,=\,x)$ up to some ﬁxed absolute error $\epsilon$ has a randomized polynomial time algorithm. Therefore, the problem cannot be $\mathcal{N P}$ -hard unless $\mathcal{N P}=\mathcal{R P}$ . T sult is an improvement on the exact case, where even the task of computing $P(X=x)$ is -hard. 

Unfortunately, the good news is very limited in scope, in that it disappears once we introduce e. Speciﬁcally, it is $\mathcal{N P}$ -hard to ﬁnd an absolute approximation to $P(x\mid e)$ for any $\epsilon<1/2$ . 

Theorem 9.4 

Given a Ba netw $\mathcal{B}$ over $\mathcal{X}$ e $X\,\in\,{\mathcal{X}}$ , a value $x\,\in\,V a l(X)$ , and a observation $E=e$ for $E\subset{\mathcal{X}}$ ⊂X and $e\in V a l(E)$ ∈ , ﬁnd a number $\rho$ that has absolute error ϵ for $P_{\mathcal{B}}(X=x\mid e)$ . 

Proof The proof uses the same construction that we used before. Consider a formula $\phi$ , and nsider the analogous BN $\mathcal{B}$ , as described in theorem 9.1. Recall that our BN had a variable $Q_{i}$ for each propositional variable $q_{i}$ in our Boolean formula, a bunch of other intermediate variables, and then a variable $X$ whose value, given any assignment of values $q_{1}^{1},q_{1}^{0}$ to the $Q_{i}$ ’s, was the associated truth value of the formula. We now show that, given such an approximation algorithm, we can cide ether the formula is satis ble. We begin by computing $P(Q_{1}\mid x^{1})$ . We pick the value $v_{1}$ for $Q_{1}$ that is most likely given $x^{1}$ , and we instantiate it to this value. That , we generate a network $\mathcal{B}_{2}$ that does not contain $Q_{1}$ , and that represents the distribution $\mathcal{B}$ B conditioned on $Q_{1}\,=\,v_{1}$ . We repeat this process for $Q_{2},\ldots,Q_{n}$ . This results in some assignment $v_{1},\dots,v_{n}$ to the $Q_{i}$ ’s. We now prove that this is a satisfying assignment if and only if the original formula $\phi$ was satisﬁable. 

We begin with the easy case. If $\phi$ is not satisﬁable, then $v_{1},\dots,v_{n}$ can hardly be a satisfying assignment for it. Now, assume that $\phi$ is satisﬁable. We show that it also has a satisfying assignment with $Q_{1}\,=\,v_{1}$ . If $\phi$ is satisﬁable with both $Q_{1}\,=\,q_{1}^{1}$ and $Q_{1}\,=\,q_{1}^{0}$ , then this is obvious. Assume, however, that $\phi$ is satisﬁable, but not when $Q_{1}\,=\,v$ . Then necessarily, we will have that $P(Q_{1}=v\mid x^{1})$ is 0, and the probability of the complementary event i 1. If we have an approximation $\rho$ whose error is guaranteed to be $<1/2$ , then choosing the v that maximizes this probability is guaranteed to pick the $v$ whose probability is 1. Thus, in either case the formula has a satisfying assignment where $Q_{1}=v$ . 

We can continue in this fashion, proving by induction on $k$ that $\phi$ has a satisfying assignment with $Q_{1}=v_{1},.\,.\,.\,,Q_{k}=v_{k}$ . In the case where $\phi$ is satisﬁable, this process will terminate with a satisfying assignment. In the case where $\phi$ is not, it clearly will not terminate with a satisfying assignment. We can determine which is the case simply by checking whether the resulting assignment satisﬁes $\phi$ . This gives us a polynomial time process for deciding satisﬁability. 

Because $\epsilon=1/2$ corresponds to random guessing, this result is quite discouraging. It tells us that, in the case where we have evidence, approximate inference is no easier than exact inference, in the worst case. 

# 9.2 Variable Elimination: The Basic Ideas 

We begin our discussion of inference by discussing the principles underlying exact inference in graphical models. As we show, the same graphical structure that allows a compact represen- tation of complex distributions also help support inference. In particular, we can use dynamic programming techniques (as discussed in appendix A.3.3) to perform inference even for certain large and complex networks in a very reasonable time. We now provide the intuition underlying these algorithms, an intuition that is presented more formally in the remainder of this chapter. 

We begin by considering the inference task in a very simple network $A\,\rightarrow\,B\,\rightarrow\,C\,\rightarrow$ $D$ . We ﬁrst provide a phased computation, which uses results from the previous phase for the computation in the next phase. We then reformulate this process in terms of a global computation on the joint distribution. 

Assume that our ﬁrst goal is to compute the probability $P(B)$ , that is, the distribution over values $b$ of $B$ . Basic probabilistic reasoning (with no assumptions) tells us that 

$$
P(B)=\sum_{a}P(a)P(B\mid a).
$$ 

Fortunately, we have all the required numbers in our Bayesian network representation: each number $P(a)$ is in the CPD for $A$ , and each number $P(b\mid a)$ is in the CPD for $B$ . Note that if $A$ has $k$ values and $B$ has $m$ values, the number of basic arithmetic operations required is $O(k\times m)$ : to compute $P(b)$ , we m st multiply $P(b\mid a)$ $P(a)$ for each of the $k$ values of $A$ , and then add them u that is, $k$ multiplications and k $k-1$ − additions; this process must be repeated for each of the m values b . 

Now, assume we want to compute $P(C)$ . Using the same analysis, we have that 

$$
P(C)=\sum_{b}P(b)P(C\mid b).
$$ 

Again, the co itional probabilities $P(c\mid b)$ are known: they constitute the CPD for $C$ . The probability of B is not speciﬁed as part of the network parameters, but equation (9.4) shows us how it can be computed. Thus, we can compute $P(C)$ . We can continue the process in an analogous way, in order to compute $P(D)$ . 

Note that the structure of the network, and its efect on the parameter iz ation of the CPDs, is critical for our ability to perform this computation as described. Speciﬁcally, assume that $A$ had been a parent of $C$ . In this case, the CPD for $C$ would have included $A$ , and our computation of $P(B)$ would not have sufced for equation (9.5). 

Also note that this algorithm does not compute single values, but rather sets of values at a time. In particular equation (9.4) computes an entire distribution over all of the possible values of $B$ . All of these are then used in equation (9.5) to compute $P(C)$ . This property turns out to be critical for the performance of the general algorithm. 

Let us analyze the complexity of this process on a general chain. Assume that we have a chain with $n$ variables $X_{1}\,\rightarrow\,.\,.\,\rightarrow\,X_{n},$ each e in $k$ values. As described, the algorithm would compute $P(X_{i+1})$ from $P(X_{i})$ , for $i=1,\dots,n-1$ − . Each such step would consist of the following computation: 

$$
P(X_{i+1})=\sum_{x_{i}}{P(X_{i+1}\mid x_{i})P(x_{i})},
$$ 

where $P(X_{i})$ is computed in the previous step. The cost of each such step is $O(k^{2})$ : The distributi er $X_{i}$ has $k$ va s, and the CPD $P(X_{i+1}\mid X_{i})$ s $k^{2}$ values; we need to multiply $P(x_{i})$ , for value x , with each CPD entry $P(x_{i+1}\mid x_{i})$ $\,\!\,k^{2}$ multiplications), and then, for each value $x_{i+1}$ , sum up the co entries ( $(k\times(k-1)$ × − additions). We need to perform this process for every variable $X_{2},\ldots,X_{n}$ ; hence, the total cost is $O(n k^{2})$ . 

By comparison, consider the process of generating the entire joint and summing it out, which requires that we generate $k^{n}$ probabilities for the diferent events $x_{1},\dots,x_{n}$ . Hence, we have at least one example where, despite the exponential size of the joint distribution, we can do inference in linear time. 

Using this process, we have managed to do inference over the joint distribution without ever generating it explicitly. What is the basic insight that allows us to avoid the exhaustive enumeration? Let us reexamine this process in terms of the joint $P(A,B,C,D)$ . By the chain rule for Bayesian networks, the joint decomposes as 

$$
P(A)P(B\mid A)P(C\mid B)P(D\mid C)
$$ 

To compute $P(D)$ , we need to sum together all of the entries where $D=d^{1}$ , and to (separately) sum together all of the entries where $D\ =\ d^{2}$ . The exact computation that needs to be 

![](images/52a6137cf3ae5ad55699e70c707c9901c25b5aaae89da9334aabcbd8897f2c1a.jpg) 
Figure 9.2 Computing $P(D)$ by summing over the joint distribution for a chain $A\to B\to C\to$ $D$ ; all of the variables are binary valued. 

performed, for binary-valued variables $A,B,C,D$ , is shown in ﬁgure 9.2. 

Examining this summation, we see that it has a lot of structure. For example, the third and fourth terms in the ﬁrst two entries are both $P(c^{1}\mid b^{1})P(d^{1}\mid c^{1})$ . We can therefore modify the computation to ﬁrst compute 

$$
P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2})
$$ 

and only then multiply by the common term. The same structure is repeated throughout the table. If we perform the same transformation, we get a new expression, as shown in ﬁgure 9.3. 

We now observe that certain terms are repeated several times in this expression. Speciﬁcally, $P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2})$ and $P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2})$ are each repeated four times. Thus, it seems clear that we can gain signiﬁcant computational savings by computing them once and then storing them. There are two such expressions, one for each value of $B$ . Thus, we e a function $\tau_{1}\ :\ \,V a l(B)\mapsto I\!\!R,$ , where $\tau_{1}(b^{1})$ is the ﬁrst of these two expressions, and $\tau_{1}(b^{2})$ is the second. Note that $\tau_{1}(B)$ corresponds exactly to $P(B)$ . 

The resulting expression, assuming $\tau_{1}(B)$ has been computed, is shown in ﬁgure 9.4. Examin- ing this new expression, we see that we once again can reverse the order of a sum and a product, resulting in the expression of ﬁgure 9.5. And, once again, we notice some shared expressions, that are better computed once and used multiple times. We deﬁne $\tau_{2}~:~V a l(C)\mapsto I\!\!R.$ . 

$$
\begin{array}{l l l}{{\tau_{2}(c^{1})}}&{{=}}&{{\tau_{1}(b^{1})P(c^{1}\mid b^{1})+\tau_{1}(b^{2})P(c^{1}\mid b^{2})}}\\ {{\tau_{2}(c^{2})}}&{{=}}&{{\tau_{1}(b^{1})P(c^{2}\mid b^{1})+\tau_{1}(b^{2})P(c^{2}\mid b^{2})}}\end{array}
$$ 

1. When $D$ is binary-valued, we can get away with doing only the ﬁrst of these computations. However, this trick does not carry over to the case of variables with more than two values or to the case where we have evidence. Therefore, our example will show the computation in its generality. 

$$
\begin{array}{c c}{{}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{1}\mid b^{1})~~~P(d^{1}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{1}\mid b^{2})~~~P(d^{1}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{2}\mid b^{1})~~~P(d^{1}\mid c^{2})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{2}\mid b^{2})~~~P(d^{1}\mid c^{2})}}\\ {{}}&{{}}&{{}}\\ {{}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{1}\mid b^{1})~~~P(d^{2}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{1}\mid b^{2})~~~P(d^{2}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{2}\mid b^{1})~~~P(d^{2}\mid c^{2})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{2}\mid b^{2})~~~P(d^{2}\mid c^{2})}}\end{array}
$$ 

Figure 9.3 The ﬁrst transformation on the sum of ﬁgure 9.2 

$$
\begin{array}{c c c}{\tau_{1}(b^{1})}&{P(c^{1}\mid b^{1})}&{P(d^{1}\mid c^{1})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{1}\mid b^{2})}&{P(d^{1}\mid c^{1})}\\ {+}&{\tau_{1}(b^{1})}&{P(c^{2}\mid b^{1})}&{P(d^{1}\mid c^{2})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{2}\mid b^{2})}&{P(d^{1}\mid c^{2})}\\ {}&{}&{}&{}\\ {\tau_{1}(b^{1})}&{P(c^{1}\mid b^{1})}&{P(d^{2}\mid c^{1})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{1}\mid b^{2})}&{P(d^{2}\mid c^{1})}\\ {+}&{\tau_{1}(b^{1})}&{P(c^{2}\mid b^{1})}&{P(d^{2}\mid c^{2})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{2}\mid b^{2})}&{P(d^{2}\mid c^{2})}\end{array}
$$ 

$$
{\begin{array}{l l l}{}&{(\tau_{1}(b^{1})P(c^{1}\mid b^{1})+\tau_{1}(b^{2})P(c^{1}\mid b^{2}))}&{P(d^{1}\mid c^{1})}\\ {+}&{(\tau_{1}(b^{1})P(c^{2}\mid b^{1})+\tau_{1}(b^{2})P(c^{2}\mid b^{2}))}&{P(d^{1}\mid c^{2})}\\ {}&{}&{}\\ {+}&{(\tau_{1}(b^{1})P(c^{1}\mid b^{1})+\tau_{1}(b^{2})P(c^{1}\mid b^{2}))}&{P(d^{2}\mid c^{1})}\\ {+}&{(\tau_{1}(b^{1})P(c^{2}\mid b^{1})+\tau_{1}(b^{2})P(c^{2}\mid b^{2}))}&{P(d^{2}\mid c^{2})}\end{array}}
$$ 

Figure 9.5 The third transformation on the sum of ﬁgure 9.2 

$$
{\begin{array}{r l}{\tau_{2}(c^{1})}&{P(d^{1}\mid c^{1})}\\ {+}&{\tau_{2}(c^{2})}&{P(d^{1}\mid c^{2})}\\ {\,}&{}\\ {\tau_{2}(c^{1})}&{P(d^{2}\mid c^{1})}\\ {+}&{\tau_{2}(c^{2})}&{P(d^{2}\mid c^{2})}\end{array}}
$$ 

Figure 9.6 The fourth transformation on the sum of ﬁgure 9.2 

The ﬁnal expression is shown in ﬁgure 9.6. 

Summarizing, we begin by computing $\tau_{1}(B)$ , which requires four multiplications and two additions. Using it, we can compute $\tau_{2}(C)$ , which also requires four multiplications and two additions. Finally, we can compute $P(D)$ , again, at the same cost. The total number of operations is therefore 18. By comparison, generating the joint distribution requires $16\cdot3=48$ multiplications (three for each of the 16 entries in the joint), and 14 additions (7 for each of $P(d^{1})$ and $P(d^{2}))$ . 

Written somewhat more compactly, the transformation we have performed takes the following steps: We want to compute 

$$
P(D)=\sum_{C}\sum_{B}\sum_{A}P(A)P(B\mid A)P(C\mid B)P(D\mid C).
$$ 

We push in the ﬁrst summation, resulting in 

$$
\sum_{C}P(D\mid C)\sum_{B}P(C\mid B)\sum_{A}P(A)P(B\mid A).
$$ 

We compute the product $\psi_{1}(A,B)=P(A)P(B\mid A)$ a d then sum out $A$ to obtain the func- $\begin{array}{r}{\tau_{1}(B)=\sum_{A}\psi_{1}(A,B)}\end{array}$ . Speciﬁcally, for each value b , we compute $\begin{array}{r l r}{\tau_{1}(b)=\sum_{A}\psi_{1}(A,b)=}\end{array}$ $\textstyle\sum_{A}P(A)P(b\mid A)$ . We then continue by computing: 

$$
\begin{array}{r c l}{{\psi_{2}(B,C)}}&{{=}}&{{\tau_{1}(B)P(C\mid B)}}\\ {{\tau_{2}(C)}}&{{=}}&{{\displaystyle\sum_{B}\psi_{2}(B,C).}}\end{array}
$$ 

This computation results in a new vector $\tau_{2}(C)$ , which we then proceed to use in the ﬁnal phase of computing $P(D)$ . 

dynamic programming 

# 

This procedure is performing dynamic programming (see appendix A.3.3); doing this sum- mation the naive way w uld h us compute every $\begin{array}{r}{P(b)=\sum_{A}P(A)P(b\mid A)}\end{array}$ | many times, once for every value of C and D . In general, in a chain of length $n$ , this internal summation would be computed exponentially many times. Dynamic programming “inverts” the order of computation — performing it inside out instead of outside in. Speciﬁcally, we perform the innermost summation ﬁrst, computing once and for all the values in $\tau_{1}(B)$ ; that allows us to compute $\tau_{2}(C)$ once and for all, and so on. 

To summarize, the two ideas that help us address the exponential blowup of the joint distribution are: 

• Because of the structure of the Bayesian network, some subexpressions in the joint depend only on a small number of variables.

 • By computing these expressions once and caching the results, we can avoid generating them exponentially many times. 

# 9.3 Variable Elimination 

factor To formalize the algorithm demonstrated in the previous section, we need to introduce some basic concepts. In chapter 4, we introduced the notion of a factor $\phi$ over a scope $S c o p e[\phi]=X$ , which is a function $\phi:V a l(X)\mapsto I\!\!R$ . The main steps in the algorithm described here can be viewed as a manipulation of factors. Importantly, by using the factor-based view, we can deﬁne the algorithm in a general form that applies equally to Bayesian networks and Markov networks. 

![](images/7910f7d1d1658bb966789c46e52c859b7c766f3e0c7732e16b190a3c9bd7e344.jpg) 
Figure 9.7 Example of factor marginalization: summing out $B$ . 

# 9.3.1 Basic Elimination 

# 9.3.1.1 Factor Marginalization 

The key operation that we are performing when computing the probability of some subset of variables is that of marginalizing out variables from a distribution. That is, we have a distribution over a of variables $\mathcal{X}$ , and we want to compute the marginal of that distribution over some subset X . We can view this computation as an operation on a factor: 

Deﬁnition 9.3 factor marginalization Let $X$ be a set of v iab s, and $Y\notin X$ a variable. Let $\phi(X,Y)$ e a factor. We deﬁne the factor marginalization of Y $Y$ in φ , denoted $\textstyle\sum_{Y}\phi$ , to be a factor $\psi$ over X such that: 

$$
\psi(X)=\sum_{Y}\phi(X,Y).
$$ 

This operation is also called summing out of $Y$ in $\psi$ . 

The key point in this deﬁnition is that we only sum up entries in the table where the values of $X$ match up. Figure 9.7 illustrates this process. 

The process of marginalizing a joint distribution $P(X,Y)$ onto $X$ in a Bayesian network is simply summing out the variables $Y$ in the factor corresponding to $P$ . If we sum out all variables, we get a factor consisting of a single number whose value is 1 . If we sum out all of the variables in the unnormalized distribution $\tilde{P}_{\Phi}$ deﬁned by the product of factors in a Markov network, we get the partition function. 

A key observation used in performing inference in graphical models is that the operations of factor product and summation behave precisely as do product and summation over numbers. , both operations are commutative, s $\phi_{1}\cdot\phi_{2}\,=\,\phi_{2}\,\cdot\,\phi_{1}$ and $\begin{array}{r l}{\sum_{\boldsymbol{X}}\sum_{\boldsymbol{Y}}\phi\;=}\end{array}$ P $\textstyle\sum_{Y}\sum_{X}\phi$ . Products are also associative, so that $\left(\phi_{1}\cdot\phi_{2}\right)\cdot\phi_{3}=\phi_{1}\cdot\left(\phi_{2}\cdot\phi_{3}\right)$ · · · · ) . Most importantly, 

![](images/cc1c1a63031b2a68f051ab83d7b0ebe34448d256f2e8f04485bd4d5665ca7276.jpg) 

we have a simple rule allowing us to exchange summation and product: If $X\not\in S c o p e[\phi_{1}]$ , then 

$$
\sum_{X}(\phi_{1}\cdot\phi_{2})=\phi_{1}\cdot\sum_{X}\phi_{2}.
$$ 

# 9.3.1.2 The Variable Elimination Algorithm 

The key to both of our examples in the last section is the application of equation (9.6). Speciﬁ- cally, in our chain example of section 9.2, we can write: 

$$
P(A,B,C,D)=\phi_{A}\cdot\phi_{B}\cdot\phi_{C}\cdot\phi_{D}.
$$ 

On the other hand, the marginal distribution over $D$ is 

$$
P(D)=\sum_{C}\sum_{B}\sum_{A}P(A,B,C,D).
$$ 

Applying equation (9.6), we can now conclude: 

$$
\begin{array}{r c l}{{P(D)}}&{{=}}&{{\displaystyle\sum_{C}\displaystyle\sum_{B}\displaystyle\sum_{A}\phi_{A}\cdot\phi_{B}\cdot\phi_{C}\cdot\phi_{D}}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\displaystyle\sum_{B}\phi_{C}\cdot\phi_{D}\cdot\left(\displaystyle\sum_{A}\phi_{A}\cdot\phi_{B}\right)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\phi_{D}\cdot\left(\displaystyle\sum_{B}\phi_{C}\cdot\left(\displaystyle\sum_{A}\phi_{A}\cdot\phi_{B}\right)\right),}}\end{array}
$$ 

where the diferent transformations are justiﬁed by the limited scope of the CPD factors; for example, the second equality is justiﬁed by the fact that the scope of $\phi_{C}$ and $\phi_{D}$ does not contain $A$ . In general, any marginal probability computation involves taking the product of all the CPDs, and doing a summation on all the variables except the query variables. We can do these steps in any order we want, as long as we only do a summation on a variable $X$ after multiplying in all of the factors that involve $X$ . 

In general, we can view the task at hand as that of computing the value of an expression of the form: 

$$
\sum_{Z}\prod_{\phi\in\Phi}\phi.
$$ 

sum-product 

variable elimination 

We call this task the sum-product inference task. The key insight that allows the efective computation of this expression is the fact that the scope of the factors is limited, allowing us to “push in” some of the summations, performing them over the product of only a subset of factors. One simple instantiation of this algorithm is a procedure called sum-product variable elimination (VE), shown in algorithm 9.1. The basic idea in the algorithm is that we sum out variables one at a time. When we sum out any variable, we multiply all the factors that mention that variable, generating a product factor. Now, we sum out the variable from this combined factor, generating a new factor that we enter into our set of factors to be dealt with. 

Based on equation (9.6), the following result follows easily: 

Theorem 9.5 Let $X$ e set of variables, and let $\Phi$ be a set o hat for each $\phi\in\Phi$ , $S c o p e[\phi]\subseteq X$ . Let Y $Y\subset X$ ⊂ be a set of query variables, and let $Z=X\mathrm{~-~}Y$ − . Then for any ordering ≺ over Z , Sum-Product $\textstyle\mathcal{\mathrm{NE}}(\Phi,Z,\prec)$ returns a factor $\phi^{*}(Y)$ such that 

$$
\phi^{*}(Y)=\sum_{Z}\prod_{\phi\in\Phi}\phi.
$$ 

We can apply this algorithm to the task of computing the probability distribution $P_{\mathcal{B}}(Y)$ for a Bayesian network $\mathcal{B}$ . We simply instantiate $\Phi$ to consist of all of the CPDs: 

$$
\Phi=\{\phi_{X_{i}}\}_{i=1}^{n}
$$ 

$\phi_{X_{i}}\;=\;P(X_{i}\;\mid\;\mathrm{Pa}_{X_{i}})$ . We then apply the variable elimination algorithm to the set $\left\{Z_{1},.\,.\,.\,,Z_{m}\right\}=\mathcal{X}-Y$ (that is, we eliminate all the nonquery variables). 

We can also apply precisely the same algorithm to the task of computing conditional prob- abilities in a Markov network. We simply initialize the factors to be the clique potentials and 

![](images/6792aca268a0c20d90c2d5d08c61c86cf022b9cdd03881750691da39b9a34698.jpg) 
Figure 9.8 The Extended-Student Bayesian network 

run the elimination algorithm. As for Bayesian networks, we then apply the variable elimination algorithm the set $Z=\mathcal{X}-Y$ . T procedure returns an unnormalized factor over the query variables Y . The distribution over $Y$ can be obtained by normalizing the factor; the partition function is simply the normalizing constant. 

Let us demonstrate the procedure on a nontrivial example. Consider the network demonstrated in ﬁgure 9.8, which is an extension of our Student network. The chain rule for this network asserts that 

$$
\begin{array}{r c l}{{P(C,D,I,G,S,L,J,H)}}&{{=}}&{{P(C)P(D\mid C)P(I)P(G\mid I,D)P(S\mid I)}}\\ {{}}&{{}}&{{P(L\mid G)P(J\mid L,S)P(H\mid G,J)}}\\ {{}}&{{=}}&{{\phi_{C}(C)\phi_{D}(D,C)\phi_{I}(I)\phi_{G}(G,I,D)\phi_{S}(S,I)}}\\ {{}}&{{}}&{{\phi_{L}(L,G)\phi_{J}(J,L,S)\phi_{H}(H,G,J).}}\end{array}
$$ 

We will now apply the $V E$ algorithm to compute $P(J)$ . We will use the elimination ordering: $C,D,I,H,G,S,L$ : 

1. Eliminating $C$ : We compute the factors 

$$
\begin{array}{r c l}{\psi_{1}(C,D)}&{=}&{\phi_{C}(C)\cdot\phi_{D}(D,C)}\\ {\tau_{1}(D)}&{=}&{\displaystyle\sum_{C}\psi_{1}.}\end{array}
$$ 

2. Eliminating $D$ : Note that we have already eliminated one of the original factors that involve $D$ — $\phi_{D}(D,C)=P(D\mid C)$ . On the other hand, we introduced the factor $\tau_{1}(D)$ that involves $D$ . Hence, we now compute: 

$$
\begin{array}{r c l}{{\psi_{2}(G,I,D)}}&{{=}}&{{\phi_{G}(G,I,D)\cdot\tau_{1}(D)}}\\ {{\tau_{2}(G,I)}}&{{=}}&{{\displaystyle\sum_{D}\psi_{2}(G,I,D).}}\end{array}
$$ 

3. Eliminating $I$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{3}(G,I,S)}}&{{=}}&{{\phi_{I}(I)\cdot\phi_{S}(S,I)\cdot\tau_{2}(G,I)}}\\ {{\tau_{3}(G,S)}}&{{=}}&{{\displaystyle\sum_{I}\psi_{3}(G,I,S).}}\end{array}
$$ 

4. Eliminating $H$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{4}(G,J,H)}}&{{=}}&{{\phi_{H}(H,G,J)}}\\ {{\tau_{4}(G,J)}}&{{=}}&{{\displaystyle\sum_{H}\psi_{4}(G,J,H).}}\end{array}
$$ 

Note that $\tau_{4}\equiv1$ (all of its entries are exac : we are simply computing $\textstyle\sum_{H}P(H\mid G,J)$ | , which is a probability distribution for every $G,J$ , and hence sums to 1. A naive execution of this algorithm will end up generating this factor, which has no value. Generating it has no impact on the ﬁnal answer, but it does complicate the algorithm. In particular, the existence of this factor complicates our computation in the next step. 

5. Eliminating $G$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{5}(G,J,L,S)}}&{{=}}&{{\tau_{4}(G,J)\cdot\tau_{3}(G,S)\cdot\phi_{L}(L,G)}}\\ {{\tau_{5}(J,L,S)}}&{{=}}&{{\displaystyle\sum_{G}\psi_{5}(G,J,L,S).}}\end{array}
$$ 

Note that, without the factor $\tau_{4}(G,J)$ , the results of this step would not have involved $J$ .

 6. Eliminating $S$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{6}(J,L,S)}}&{{=}}&{{\tau_{5}(J,L,S)\cdot\phi_{J}(J,L,S)}}\\ {{\tau_{6}(J,L)}}&{{=}}&{{\displaystyle\sum_{S}\psi_{6}(J,L,S).}}\end{array}
$$ 

7. Eliminating $L$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{7}(J,L)}}&{{=}}&{{\tau_{6}(J,L)}}\\ {{\tau_{7}(J)}}&{{=}}&{{\displaystyle\sum_{L}\psi_{7}(J,L).}}\end{array}
$$ 

We summarize these steps in table 9.1. 

Note that we can use any elimination ordering. For example, consider eliminating variables in the order $G,\,I,\,S,\,L,\,H,\,C,\,D$ . We would then get the behavior of table 9.2. The result, as before, is precisely $P(J)$ . However, note that this elimination ordering introduces factors with much larger scope. We return to this point later on. 

Table 9.1 A run of variable elimination for the query $P(J)$ 
![](images/2c1c21ab7dfc4373ffe9279eab5d5e65efd1f2db25ea799b991a723190af1dbf.jpg) 

Table 9.2 A diferent run of variable elimination for the query $P(J)$ 
![](images/508834ed8348cfb3fc2a217215d55ec8f80288833133b13d73c94e63c4902e90.jpg) 

# 9.3.1.3 Semantics of Factors 

It is interesting to consider the semantics of the intermediate factors generated as part of this computation. In many of the examples we have given, they correspond to marginal or conditional probabilities in the network. However, although these factors often correspond to such probabilities, this is not always the case. Consider, for example, the network of ﬁgure $9.9\mathrm{a}$ . The result of eliminating the variable $X$ is a factor 

$$
\tau(A,B,C)=\sum_{X}P(X)\cdot P(A\mid X)\cdot P(C\mid B,X).
$$ 

This factor does not correspond to any probability or conditional probability in this network. To understand why, consider the various options for the meaning of this factor. Clearly, it cannot be a conditional distribution where $B$ is on the left hand side of the conditioning bar (for example, $P(A,B,C))$ , as $P(B\mid A)$ has not yet been multiplied in. The mo candidate is $P(A,C\mid B)$ . However, this conjecture is also false. The obability $P(A\mid B)$ | relies he ily on the properties of the CPD $P(B\mid A)$ for example, if B is determi stically equal to A , $P(A\mid B)$ has a very diferent form than if B depends only very weakly on A . Since the CPD $P(B\mid A)$ was not taken into consideration when computing $\tau(A,B,C)$ , it cannot represent the conditional probability $P(A,C\mid B)$ . In general, we can verify that this factor 

![](images/290124f499eb4a94cf1d8d6d480e2e6161cb54b9d17a29b72e9a6affa0a50999.jpg) 
Figure 9.9 Understanding intermediate factors in variable elimination as conditional probabilities: (a) A Bayesian network where elimination does not lead to factors that have an interpretation as conditional probabilities. (b) A diferent Bayesian network where the resulting factor does correspond to a conditional probability. 

does not correspond to any conditional probability expression in this network. 

It is interesting to note, however, that the resulting factor does, in fact, correspond to a conditional probability $P(A,C\mid B)$ , but in a diferent network : the one shown in ﬁgure 9.9b, where all CPDs except for B are the same. In fact, this phenomenon is a general one (see exercise 9.2). 

# 9.3.2 Dealing with Evidence 

It remains only to consider how we would introduce evidence. For example, assume we observe the value $i^{1}$ (the student is intelligent) and $h^{0}$ (the student is unhappy). Our goal is to compute $P(J\mid i^{1},h^{0})$ . First, we reduce this problem to computing the unnormalized distribution $P(J,i^{1},h^{0})$ . From this intermediate result, we can compute the conditional probability as in equation (9.1), by renormalizing by the probability of the evidence $P(i^{1},h^{0})$ . 

factor reduction 

How do we compute $P(J,i^{1},h^{0})$ ? The key observation is proposition 4.7, which shows us how to view, as a Gibbs distribution, an unnormalized measure derived from introducing evidence into a Bayesian network. Thus, we can view this computation as summing out all of the entries in the reduced factor : $P[i^{1}h^{0}]$ whose scope is $\{C,D,G,L,S,J\}$ . This factor is no longer normalized, but it is still a valid factor. 

Based on this observation, we can now apply precisely the same sum-product variable elim- ination algorithm to the task of computing $P(\pmb{Y},\pmb{e})$ . We simply apply the algorithm to the set of factors in the network, reduced by $E=e$ , and eliminate the variables in $\mathcal{X}-Y-E$ . The returned factor $\phi^{*}(Y)$ is precisely $P(\pmb{Y},\pmb{e})$ . To obtain $P(Y\mid e)$ we simply renormalize $\phi^{*}(Y)$ by multiplying it by $\textstyle{\frac{1}{\alpha}}$ to obtain a legal distribution, where $\alpha$ is the sum over the entries in our unnormalized distribution, which represents the probability of the evidence. To summarize, the algorithm for computing conditional probabilities in a Bayesian or Markov network is shown in algorithm 9.2. 

We demonstrate this process on the example of computing $P(J,i^{1},h^{0})$ . We use the same 

![](images/92d14ca92532f8aa3e03f262653607230847c2cdddc7402a825367d5387c2108.jpg) 

elimination ordering that we used in table 9.1. The results are shown in table 9.3; the step num- bers correspond to the steps in table 9.1. It is interesting to note the diferences between the two runs of the algorithm. First, we notice that steps (3) and (4) disappear in the computation with evidence, since $I$ and $H$ do not need to be eliminated. More interestingly, by not eliminating $I$ , we avoid the step that correlates $G$ and $S$ . In this execution, $G$ and $S$ never appear together in the same factor; they are both eliminated, and only their end results are combined. Intuitively, $G$ and $S$ are conditionally independent given $I$ ; hence, observing $I$ renders them independent, so that we do not have to consider their joint distribution explicitly. Finally, we notice that $\phi_{I}[I=i^{1}]\,=\,P(i^{1})$ is a factor over an empty scope, which is simply a number. It can be multiplied into any factor at any point in the computation. We chose arbitrarily to incorporate it into step $(2^{\prime})$ . Note that if our goal is to compute a conditional probability given the evidence, and not the probability of the evidence itself, we can avoid multiplying in this factor entirely, since its efect will disappear in the renormalization step at the end. 

is deﬁned over the following set of variables: 

• For each factor $\phi_{c}\in\Phi$ with scope $X_{c},$ , we have a variable $\theta_{\pmb{x}_{c}}$ for every $\pmb{x}_{c}\in V a l(\pmb{X}_{c})$ . • For each variable $X_{i}$ and every value $x_{i}\in V a l(X_{i})$ , we have a binary-valued variable $\lambda_{x_{i}}$ 

In other words, the polynomial has one argument for each of the network parameters and for each possible assignment to a network variable. The polynomial $f_{\Phi}$ is now deﬁned as follows: 

$$
f_{\Phi}(\pmb\theta,\pmb\lambda)=\sum_{x_{1},...,x_{n}}\left(\prod_{\phi_{c}\in\Phi}\theta_{\pmb x_{c}}\cdot\prod_{i=1}^{n}\lambda_{x_{i}}\right).
$$ 

Evaluating the network polynomial is equivalent to the inference task. In particular, let $Y=y$ be an assignment to some subset of network variables; deﬁne an assignment $\lambda^{y}$ as follows: 

With this deﬁnition, we can now show (exercise 9.4a) that: 

$$
f_{\Phi}(\pmb\theta,\lambda^{y})=\tilde{P}_{\Phi}(Y=y\mid\pmb\theta).
$$ 

The derivatives of the network polynomial are also of signiﬁcant interest. We can show (exer- cise 9.4b) that 

$$
\frac{\partial f_{\Phi}(\pmb\theta,\pmb\lambda^{\pmb y})}{\partial\lambda_{x_{i}}}=\tilde{P}_{\Phi}(x_{i},\pmb y_{-i}\mid\pmb\theta),
$$ 

where $\pmb{y}_{-i}$ is the assignment in $_{_y}$ to all variables other than $X_{i}$ . We can also show that 

$$
{\frac{\partial f_{\Phi}(\pmb\theta,\lambda^{y})}{\partial\theta_{x_{c}}}}={\frac{{\tilde{P}}_{\Phi}(\pmb y,\pmb x_{c}\mid\pmb\theta)}{\theta_{x_{c}}}}~;
$$ 

sensitivity analysis 

this fact is proved in lemma 19.1. These derivatives can be used for various purposes, including retracting or modifying evidence in the network (exercise 9.4c), and sensitivity analysis — comput- ing the efect of changes in a network parameter on the answer to a particular probabilistic query (exercise 9.5). 

Of course, as deﬁned, the representation of the network polynomial is exponentially large in the number of variables in the network. However, we can use the algebraic operations performed in a run of variable elimination to deﬁne a network polynomial that has precisely the same complexity as the VE run. More interesting, we can also use the same structure to compute efciently all of the derivatives of the network polynomial, relative both to the $\lambda_{i}$ and the $\theta_{\pmb{x}_{c}}$ (see exercise 9.6). 

# 9.4 Complexity and Graph Structure: Variable Elimination 

From the examples we have seen, it is clear that the VE algorithm can be computationally much more efcient than a full enumeration of the joint. In this section, we analyze the complexity of the algorithm, and understand the source of the computational gains. 

We also note that, aside from the asymptotic analysis, a careful implementation of this algorithm can have signiﬁcant ramiﬁcations on performance; see box 10.A. 

# 9.4.1 Simple Analysis 

Let us begin with a simple analysis of the basic computational operations taken by algorithm 9.1. Assume we have $n$ random variables, and $m$ initial factors; in a Bayesian network, we have $m=n$ ; in a Markov network, we may have more factors than variables. For simplicity, assume we run the algorithm until all variables are eliminated. 

The algorithm consists of a set of elimination steps, where, in each step, the algorithm picks a variable $X_{i}$ , then multiplies all factors involving that variable. The result is a single large factor $\psi_{i}$ . The variable then gets summed out of $\psi_{i}$ , resulting in a new factor $\tau_{i}$ whose scope is the scope of $\psi_{i}$ minus $X_{i}$ . Thus, the work revolves around these factors that get created and processed. Let $N_{i}$ be the number of entries in the factor $\psi_{i}$ , and let $N_{\mathrm{max}}=\operatorname*{max}_{i}N_{i}$ . 

We begin by counting the number of multiplication steps. Here, we note that the total number of factors ever entered into the set of factors $\Phi$ is $m+n$ : the $m$ initial factors, plus the $n$ factors $\tau_{i}$ . Each of these factors $\phi$ is multiplied exactly once: when it is multiplied in line 3 of Sum-Product-Eliminate-Var to produce a large factor $\psi_{i}$ , it is also extracted from $\Phi$ . The cost of multiplying $\phi$ to produce $\psi_{i}$ is at most $N_{i}$ , since each entry of $\phi$ is multiplied into $\psi_{i}$ the total number of multiplication steps is at most $(n+m)N_{i}\leq$ $(n+m)N_{\mathrm{max}}\,=\,O(m N_{\mathrm{max}})$ . To analyze the number of addition steps, we note that the marginalization operation in line 4 touches each entry in $\psi_{i}$ exactly once. Thus, the cost of this operation is exactly $N_{i}$ ; we execute this operation once for each factor $\psi_{i}$ , so that the total number of additions is at most $n N_{\mathrm{max}}$ . Overall, the total amount of work required is $O(m N_{\mathrm{max}})$ . 

The source of the inevitable exponential blowup is the potentially exponential size of the factors $\psi_{i}$ . If each variable has no more than $v$ values, and a factor $\psi_{i}$ has a scope that contains $k_{i}$ variables, then $N_{i}\leq v^{k_{i}}$ . Thus, we see that the computational cost of the VE algorithm is dominated by the sizes of the intermediate factors generated, with an exponential growth in the number of variables in a factor. 

# 9.4.2 Graph-Theoretic Analysis 

Although the size of the factors created during the algorithm is clearly the dominant quantity in the complexity of the algorithm, it is not clear how it relates to the properties of our problem instance. In our case, the only aspect of the problem instance that afects the complexity of the algorithm is the structure of the underlying graph that induced the set of factors on which the algorithm was run. In this section, we reformulate our complexity analysis in terms of this graph structure. 

# 9.4.2.1 Factors and Undirected Graphs 

We begin with the observation that the algorithm does not care whether the graph that generated the factors is directed, undirected, or partly directed. The algorithm’s input is a set of factors $\Phi$ , and the only relevant aspect to the computation is the scope of the factors. Thus, it is easiest to view the algorithm as operating on an undirected graph $\mathcal{H}$ . 

More precisely, we can deﬁne the notion of an undirected graph associated with a set of factors: 

Let $\Phi$ be a set of factors. We deﬁne 

$$
S c o p e[\Phi]=\cup_{\phi\in\Phi}S c o p e[\phi]
$$ 

to be the set of all variables appearing in any of the factors in $\Phi$ . We deﬁne ${\mathcal{H}}_{\Phi}$ to be the undirected graph whose nodes correspond to the variables in Scope [Φ] and where we have an edge $X_{i}{-}X_{j}\in{\mathcal{H}}_{\Phi}$ if and only if there exists a factor $\phi\in\Phi$ such that $X_{i},X_{j}\in S c o p e[\phi]$ . 

In words, t irected graph ${\mathcal{H}}_{\Phi}$ introduces a fully connected subgraph over t scope of each factor $\phi\in\Phi$ ∈ , and hence is the minimal I-map for the distribution induced by Φ . We can now show that: 

Let $P$ be a distribution deﬁned by multiplying the factors in $\Phi$ and normalizing to deﬁne $^a$ distribution. Letting $X=S c o p e[\Phi]$ , 

$$
P(X)=\frac{1}{Z}\prod_{\phi\in\Phi}\phi,
$$ 

here $\begin{array}{r}{Z=\sum_{X}\prod_{\phi\in\Phi}\phi}\end{array}$ Q . Then ${\mathcal{H}}_{\Phi}$ is the minimal Markov network $I\cdot$ map for $P$ , and the factors ∈ $\Phi$ are a parameter iz ation of this network that deﬁnes the distribution P . 

The proof is left as an exercise (exercise 9.7). 

Note that, for a set of factors $\Phi$ deﬁned by a Bayesian netwo $\mathcal{G}$ , in the case without evidence, the undirected graph ${\mathcal{H}}_{\Phi}$ is precisely the moralized graph of G . In this case, the product of the factors is a normalized distribution, so the partition function of the resulting Markov network is simply 1 . Figure 4.6a shows the initial graph for our Student example. 

More interesting is the Markov network induced by a set of factors $\Phi[e]$ deﬁned by the reduction of the factors in a Bayesian network to some context $E=e$ . In this case, recall that the variables in $E$ are removed from the factors, so $X=S c o p e[\Phi_{e}]=\mathcal{X}-E$ . Furthermore, as we discussed, the unnormalized product of the factors is $P(X,e)$ , and the partition function of the resulting Markov network is precisely $P(e)$ . Figure 4.6b shows the initial graph for our Student example with evidence $G\;=\;g$ , and ﬁgure 4.6c shows the case with evidence $G=g,S=s$ . 

# 9.4.2.2 Elimination as Graph Transformation 

Now, consider the efect of a variable elimination step on the set of factors maintained by the algorithm and on the associated Markov network. When a variable $X$ is eliminated, several operations take place. First, we create a single factor $\psi$ that contains $X$ and all of the variables $Y$ with which it appears in factors. Then, we eliminate $X$ from $\psi$ , replacing it with a new factor $\tau$ that contains all of the variables $Y$ but does not contain $X$ . Let $\Phi_{X}$ be the resulting set of factors. 

ﬁll edge 

How does the graph ${\mathcal{H}}_{\Phi_{X}}$ from $\mathcal{H}_{\Phi}\mathfrak{H}$ The step of constructing $\psi$ generates edges between all of the variables Y $Y\in Y$ ∈ . Some of them were present in ${\mathcal{H}}_{\Phi}$ , whereas others are introduced due to the elimination step; edges that are introduced by an elimination step are called ﬁll edges . The step of eliminating $X$ from $\psi$ to construct $\tau$ has the efect of removing $X$ and all of its incident edges from the graph. 

![](images/31cdfc0084379ddb6de8422d0d10272e1cbef6d6f4f830d88863477cfed31aa5.jpg) 
Figure 9.10 Variable elimination as graph transformation in the Student example, using the elimi- nation order of table 9.1: (a) after eliminating $C$ ; (b) after eliminating $D$ ; (c) after eliminating $I$ . 

Consider again our Student network, in the case without evidence. As we said, ﬁgure 4.6a shows the original Markov network. Figure 9.10a shows the result of eliminating the variable $C$ . Note that there are no ﬁll edges introduced in this step. 

After an elimination step, the subsequent elimination steps use the new set of factors. In other words, they can be seen as operations over the new graph. Figure 9.10b and c show the graphs resulting from eliminating ﬁrst $D$ and then $I$ . Note that the step of eliminating $I$ results in a (new) ﬁll edge $G{-}S$ , induced by the factor $G,I,S$ . 

The computational steps of the algorithm are reﬂected in this series of graphs. Every factor that appears in one of the steps in the algorithm is reﬂected in the graph as a clique. In fact, we can summarize the computational cost using a single graph structure. 

# 9.4.2.3 The Induced Graph 

We deﬁne an undirected graph that is the union of all of the graphs resulting from the diferent steps of the variable elimination algorithm. 

Deﬁnition 9.5 induced graph Let $\Phi$ of factors over ${\mathcal X}\,=\,\{X_{1},.\,.\,.\,,X_{n}\}.$ , and $\prec\,b e$ an elimin on order for so subset $X\subseteq\mathcal{X}$ ⊆X . The induced graph $\mathcal{T}_{\Phi,\prec}$ is an undirected graph over X , where $X_{i}$ and $X_{j}$ are connected by an edge if they both appear in some intermediate factor $\psi$ generated by the VE algorithm using $\prec$ as an elimination ordering. For a Bayesian network graph $\mathcal{G}$ , we use $\mathcal{T}_{\mathcal{G},\prec}$ to denote the in ced graph for the factors $\Phi$ corresponding to the CPDs in $\mathcal{G}$ ; similarly, for a Markov network H we use $\mathcal{T}_{\mathcal{H},\prec}$ to denote the induced graph for the factors Φ corresponding to the potentials in H . The indu aph $\mathcal{T}_{\mathcal{G},\prec}$ for our Student example is show in ﬁgure 9.11a. We can see that the ﬁll edge $G{-}S$ , introduced in step (3) when we eliminated I , is the only ﬁll edge introduced. As we discussed, each factor $\psi$ used in the computation corresponds to a complete subgraph of the graph $\mathcal{T}_{\mathcal{G},\prec}$ and is therefore a clique in the graph. The connection between cliques in $\mathcal{T}_{\mathcal{G},\prec}$ and factors $\psi$ is, in fact, much tighter: 

![](images/d556a8c24eb9ce9fbc4270a465fb5393f5392fca8f908544832931ba7d517f40.jpg) 
(c) 

1. The scope of every factor generated during the variable elimination process is a clique in $\mathcal{T}_{\Phi,\prec}$ .

 2. Every maximal clique in $\mathcal{L}_{\Phi,-}$ is the scope of some intermediate factor in the computation. 

Proof We begin with the ﬁrst statement. Consider a factor $\psi(Y_{1},.\,.\,.\,,Y_{k})$ generated during the VE process. By the deﬁnition of the induced graph, there must be an edge between each $Y_{i}$ and $Y_{j}$ . Hence $Y_{1},\ldots,Y_{k}$ form a clique. 

To prove the second stateme consider some maximal clique $Y=\{Y_{1},.\,.\,.\,,Y_{k}\}$ . Assume, without loss of generality, that $Y_{1}$ is the ﬁrst of the varia s in Y in the ordering $\prec_{!}$ , and is therefore the ﬁrst among this set to be eliminated. Since Y $Y$ is a clique, there is an edge from $Y_{1}$ to each other $Y_{i}$ . Note that, once $Y_{1}$ is eliminated, it can appear in no more factors, so there can be no new edges added to it. Hence, the edges involving $Y_{1}$ were added prior to this point in the computation. The existence of an edge between $Y_{1}$ and $Y_{i}$ therefore implies that, at this point, there is a factor containing both $Y_{1}$ and $Y_{i}$ . When $Y_{1}$ is eliminated, all these factors must be multiplied. Therefore, the product step results in a factor $\psi$ that contains all of $Y_{1},Y_{2},.\,.\,.\,,Y_{k}$ . Note that this factor can contain no other variables; if it did, these variables would also have an edge to all of $Y_{1},\ldots,Y_{k}$ , so that $Y_{1},\ldots,Y_{k}$ would not constitute a maximal connected subgraph. 

Let us verify that the second property holds for our example. Figure 9.11b shows the maximal cliques in $\mathcal{T}_{\mathcal{G},\prec}$ : 

$$
\begin{array}{l c l}{{{\cal C}_{1}}}&{{=}}&{{\{{\cal C},{\cal D}\}}}\\ {{{\cal C}_{2}}}&{{=}}&{{\{{\cal D},{\cal I},{\cal G}\}}}\\ {{{\cal C}_{3}}}&{{=}}&{{\{{\cal I},{\cal G},{\cal S}\}}}\\ {{{\cal C}_{4}}}&{{=}}&{{\{{\cal G},{\cal J},{\cal L},{\cal S}\}}}\\ {{{\cal C}_{5}}}&{{=}}&{{\{{\cal G},{\cal H},{\cal J}\}.}}\end{array}
$$ 

Both these properties hold for this set of cliques. For example, $C_{3}$ corresponds to the factor $\psi$ generated in step (5). 

Thus, there is a direct correspondence between the maximal factors generated by our algorithm and maximal cliques in the induced graph. Importantly, the induced graph and the size of the maximal cliques within it depend strongly on the elimination ordering. Consider, for example, our other elimination ordering for the Student network. In this case, we can verify that our induced graph has a maximal clique over $G,I,D,L,J,H$ , a second over $S,I,D,L,J,H_{z}$ , and a third over $C,D,J$ ; indeed, the graph is missing only the edge between $S$ and $G$ , and some edges involving $C$ . In this case, the largest clique contains six variables, as opposed to four in our original ordering. Therefore, the cost of computation here is substantially more expensive. 

Deﬁnition 9.6 induced width 

tree-width 

We deﬁne the width of an induced graph to be the number of nodes in the largest clique in the graph minus 1. We deﬁne the induced width $w_{\mathcal{K},\prec}$ of an ordering $\prec$ tiv o a graph $\mathcal{K}$ (direct or undirected) to be the width of the ph $\mathcal{T}_{\mathcal{K},\prec}$ induced by applying VE to K using the ordering ≺ . We deﬁne the tree-width of a graph K to be its minimal induced width $\begin{array}{r}{w_{K}^{*}=\operatorname*{min}_{\prec}w(\mathcal{Z}_{K,\prec})}\end{array}$ I . ≺ K ≺ 

The minimal induced width of the graph $\mathcal{K}$ provides us a bound on the est performance we can hope for by applying VE to a probabilistic model that factorizes over K . 

# 9.4.3 Finding Elimination Orderings $\star$ 

How can we compute the minimal induced width of the graph, and the elimination ordering achieving that width? Unfortunately, there is no easy way to answer this question. 

Theorem 9.7 

It follows directly that ﬁnding the optimal elimination ordering is also $\mathcal{N P}$ -hard. Thus, we cannot easily tell by looking at a graph how computationally expensive inference on it will be. Note that this $\mathcal{N P}$ -completeness result is distinct from the $\mathcal{N P}$ -hardness of inference itself. That is, even if some oracle gives us the best elimination ordering, the induced width might still be large, and the inference task using that ordering can still require exponential time. 

However, as usual, $\mathcal{N P}$ -hardness is not the end of the story. There are several techniques that one can use to ﬁnd good elimination orderings. The ﬁrst uses an important graph-theoretic property of induced graphs, and the second uses heuristic ideas. 

# 9.4.3.1 Chordal Graphs 

chordal graph Recall from deﬁnition 2.24 that an undirected graph is chordal if it contains no cycle of length greater than three that has no “shortcut,” that is, every minimal loop in the graph is of length three. As we now show, somewhat surprisingly, the class of induced graphs is equivalent to the class of chordal graphs. We then show that this property can be used to provide one heuristic for constructing an elimination ordering. 

Theorem 9.8 Every induced graph is chordal. 

Proof Assume by contradiction that we have such a cycle $X_{1}{\mathrm{-}}X_{2}{\mathrm{-}}\ldots{\mathrm{-}}X_{k}{\mathrm{-}}X_{1}$ for $k>3$ , and assume without loss of generality that $X_{1}$ is the ﬁrst variable to be eliminated. As in the proof of theorem 9.6, no edge incident on $X_{1}$ is added after $X_{1}$ is eliminated; hence, both edges $X_{1}{-}X_{2}$ and $X_{1}{-}X_{k}$ must exist at this point. Therefore, the edge $X_{2}{-}X_{k}$ will be added at the same time, contradicting our assumption. 

verify that the graph re 9.11a is chordal. For example, the loop $H\rightarrow$ $G\rightarrow L\rightarrow J\rightarrow H$ is cut by the chord $G\rightarrow J$ . 

The converse of this theorem states that any chordal graph $\mathcal{H}$ is an induced graph for ome orderi One way of showing that is to show that there is an elimination ordering for H for which itself is the induced graph. 

# Theorem 9.9 

Any chordal graph $\mathcal{H}$ admits an elimination ordering that does not introduce any ﬁll edges into the graph. 

Proof We p ove this result by induction on the number of nodes in the tr . Le $\mathcal{H}$ be ordal graph with n nodes. As we showed in theorem 4.12, there is a clique tree T $\mathcal{T}$ for H . Let $C_{k}$ be a clique in the tree that is a leaf, that is, it has only a single other clique as a neighbor. Let $X_{i}$ be me variabl at is in $C_{k}$ but not in its n bor. Let ${\mathcal{H}}^{\prime}$ be the graph obtained by eliminating $X_{i}$ . Because $X_{i}$ belon nly to the clique $C_{k}$ , its neighbors are precisely $C_{k}-\{X_{i}\}$ . Because all of them are also in $C_{k}$ , they are connected to each other. Hence, eliminating $X_{i}$ introduces no ﬁll edges. Because ${\mathcal{H}}^{\prime}$ is also chordal, we can now apply the inductive hypothesis, proving the result. 

![](images/4735fc0cfa22bcaea45345a50f77cdc946bb8e6dd36596cd750aaac5fa42261c.jpg) 

# Example 9.2 

maximum cardinality 

Example 9.3 We can illustrate this construction on the graph of ﬁgure 9.11a. The maximal cliques in the induced graph are shown in $b,$ , and a clique tree for this graph is shown in c. One can easily verify that each sepset separates the two sides of the tree; for example, the sepset $\{G,S\}$ separates $C,I,D$ (on the left) from $L,J,H$ (on the right). The elimination ordering $C,D,I,H,G,S,L,J$ , an extension of the elimination in table 9.1 that generated this induced graph, is one ordering that might arise from the construction of theorem 9.9. For example, it ﬁrst eliminates $C,D$ , which are both in $^a$ leaf clique; it then eliminates $I$ , which is in a clique that is now a leaf, following the elimination of $C,D$ . Indeed, it is not hard to see that this ordering introduces no ﬁll edges. By contrast, the ordering in table 9.2 is not consistent with this construction, since it begins by eliminating the variables $G,I,S$ , none of which are in a leaf clique. Indeed, this elimination ordering introduces additional ﬁll edges, for example, the edge $H\rightarrow D$ . 

An alternative method for constructing an elimination ordering that introduces no ﬁll edges in a chordal graph is the Max-Cardinality algorithm, shown in algorithm 9.3. This method does not use the clique tree as its starting point, but rather operates directly on the graph. When applied to a chordal graph, it constructs an elimination ordering that eliminates cliques one at a time, starting from the leaves of the clique tree; and it does so without ever considering the clique tree structure explicitly. 

Consider applying Max-Cardinality to the chordal graph of ﬁgure 9.11. Assume that the ﬁrst node selected is $S$ . The second node selected must be one of $S$ ’s neighbors, say $J$ . The node that has the largest number of marked neighbors are now $G$ and $L$ , which are chosen subsequently. Now, the unmarked nodes that have the largest number of marked neighbors (two) are $H$ and $I$ . Assume we select $I$ . Then the next nodes selected are $D$ and $H$ , in any order. The last node to be selected is $C$ . One possible resulting ordering in which nodes are marked is thus $S,J,G,L,I,H,D,C$ . Importantly, the actual elimination ordering proceeds in reverse. Thus, we ﬁrst eliminate $C,D,$ , then $H$ , and so on. We can now see that this ordering always eliminates a variable from a clique that is a leaf clique at the time. For example, we ﬁrst eliminate $C,D$ from a leaf clique, then $H$ , then $G$ from the clique $\{G,I,D\}$ , which is now (following the elimination of $C,D)$ a leaf. 

As in this example, Max-Cardinality always produces an elimination ordering that is consistent with the construction of theorem 9.9. As a consequence, it follows that Max-Cardinality , when applied to a chordal graph, introduces no ﬁll edges. 

Theorem 9.10 Let H be a chordal graph. Let π be the ranking obtained by running Max-C dinality on $\mathcal{H}$ . Then Sum-Product-VE (algorithm 9.1), eliminating variables in order of increasing π , does not introduce any ﬁll edges. 

The proof is left as an exercise (exercise 9.8). 

The maximum cardinality search algorithm can also be used to construct an elimination ordering for a nonchordal graph. However, it turns out that the orderings produced by this method are generally not as good as those produced by various other algorithms, such as those described in what follows. 

triangulation 

polytree 

To summarize, we have shown that, if we construct a chordal graph that contains the graph ${\mathcal{H}}_{\Phi}$ corresponding to our s of factors $\Phi$ , we can use it as the basis for inference using $\Phi$ . The process of turning a graph H into a chordal graph is also called triangulation , since it ensures that the largest unbroken cycle in the graph is a triangle. Thus, we can reformulate our goal of ﬁnding an elimination ordering as that of triangulating a graph $\mathcal{H}$ so that the largest clique in the resulting graph is as small as possible. Of course, this insight only reformulates the problem: Inevitably, the problem of ﬁnding such a minimal triangulation is also $\mathcal{N P}$ -hard. Nevertheless, there are several graph-theoretic algorithms that address this precise problem and ofer diferent levels of performance guarantee; we discuss this task further in section 10.4.2. 

Box 9.B — Concept: Polytrees. One particularly simple class of chordal graphs is the class of Bayesian networks whose graph $\mathcal{G}$ is $a$ polytree . Recall from deﬁnition 2.22 that a polytree is a graph where there is at most one trail between every pair of nodes. 

Polytrees received a lot of attention in the early days of Bayesian networks, because the ﬁrst widely known inference algorithm for any type of Bayesian network was Pearl’s message passing algorithm for polytrees. This algorithm, a special case of the message passing algorithms described in subsequent chapters of this book, is particularly compelling in the case of polytree networks, since it consists of nodes passing messages directly to other nodes along edges in the graph. Moreover, the cost of this computation is linear in the size of the network (where the size of the network is measured as the total sizes of the CPDs in the network, not the number of nodes; see exercise 9.9). From the perspective of the results presented in this section, this simplicity is not surprising: In a polytree, any maximal clique is a family of some variable in the network, and the clique tree structure roughly follows the network topology. (We simply throw out families that do not correspond to a maximal clique, because they are subsumed by another clique.) 

Somewhat ironically, the compelling nature of the polytree algorithm gave rise to a long-standing misconception that there was a sharp tractability boundary between polytrees and other networks, in that inference was tractable only in polytrees and NP-hard in other networks. As we discuss in this chapter, this is not the case; rather, there is a continuum of complexity deﬁned by the size of the largest clique in the induced graph. 

# 9.4.3.2 Minimum Fill/Size/Weight Search 

An alternative approach for ﬁnding elimination orderings is based on a very straightforward intuition. Our goal is to construct an ordering that induces a “small” graph. While we cannot 

![](images/1f6e5c40cec9273d7551ce9744ddbb6d532cd17389d1373b9205426f560524a5.jpg) 

ﬁnd an ordering that achieves the global minimum, we can eliminate variables one at a time in a greedy way, so that each step tends to lead to a small blowup in size. 

The general algorithm is shown in algorithm 9.4. At each point, the algorithm evaluates each of the remaining variables in the network based on its heuristic cost function. Some common cost criteria that have been used for evaluating variables are: 

• Min-neighbors: The cost of a vertex is the number of neighbors it has in the current graph.

 • Min-weight: The cost of a vertex is the product of weights — domain cardinality — of its neighbors.

 • Min-ﬁll: - The cost of a vertex is the number of edges that need to be added to the graph due to its elimination.

 • Weighted-min-ﬁll: The cost of a vertex is the sum of weights of the edges that need to be added to the graph due to its elimination, where a weight of an edge is the product of weights of its constituent vertices. 

Intuitively, min-n ghbors and min-weight count the size or weight of the largest clique in $\mathcal{H}$ after eliminating X . Min-ﬁll and weighted-min-ﬁll count the number or weight of edges that would be introduced into $\mathcal{H}$ by eliminating $X$ . It can be shown (exercise 9.10) that none of these criteria is universally better than the others. 

This type of greedy search can be done either deterministic ally (as shown in algorithm 9.4), or stochastically. In the stochastic variant, at each step we select some number of low-scoring vertices, and then choose among them using their score (where lower-scoring vertices are selected with higher probability). In the stochastic variants, we run multiple iterations of the algorithm, and then select the ordering that leads to the most efcient elimination — the one where the sum of the sizes of the factors produced is smallest. 

Empirical results show that these heuristic algorithms perform surprisingly well in practice. Generally, Min-Fill and Weighted-Min-Fill tend to work better on more problems. Not surpris- ingly, Weighted-Min-Fill usually has the most signiﬁcant gains when there is some signiﬁcant variability in the sizes of the domains of the variables in the network. Box 9.C presents a case study comparing these algorithms on a suite of standard benchmark networks. 

Box 9.C — Case Study: Variable Elimination Orderings. Fishelson and Geiger (2003) performed a comprehensive case study of diferent heuristics for computing an elimination ordering, testing them on eight standard Bayesian network benchmarks, ranging from 24 nodes to more than 1,000. For each network, they compared both to the best elimination ordering known previously, obtained by an expensive process of simulated annealing search, and to the network obtained by a state- of-the-art Bayesian network package. They compared to stochastic versions of the four heuristics described in the text, running each of them for 1 minute or 10 minutes, and selecting the best network obtained in the diferent random runs. Maximum cardinality search was not used, since it is known to perform quite poorly in practice. 

The results, shown in ﬁgure 9.C.1, suggest several conclusions. First, we see that running the stochastic algorithms for longer improves the quality of the answer obtained, although usually not by a huge amount. We also see that diferent heuristics can result in orderings whose computational cost can vary in almost an order of magnitude. Overall, Min-Fill and Weighted-Min-Fill achieve the best performance, but they are not universally better. The best answer obtained by the greedy algorithms is generally very good; it is often signiﬁcantly better than the answer obtained by a deterministic state-of-the-art scheme, and it is usually quite close to the best-known ordering, even when the latter is obtained using much more expensive techniques. Because the computational cost of the heuristic ordering-selection algorithms is usually negligible relative to the running time of the inference itself, we conclude that for large networks it is worthwhile to run several heuristic algorithms in order to ﬁnd the best ordering obtained by any of them. 

# 9.5 Conditioning $\star$ 

# 

An alternative approach to inference is based on the idea of conditioning . The conditioning algorithm is based on the fact (illustrated in section 9.3.2), that observing the value of certain variables can simplify the variable elimination process. When a variable is not observed, we can use a case analysis to enumerate its possible values, perform the simpliﬁed VE computation, and then aggregate the results for the diferent values. As we will discuss, in terms of number of operations, the conditioning algorithm ofers no beneﬁt over the variable elimination al- gorithm. However, it ofers a continuum of time-space trade-ofs, which can be extremely important in cases where the factors created by variable elimination are too big to ﬁt in main memory. 

# 9.5.1 The Conditioning Algorithm 

The conditioning algorithm is easiest to explain in the context of a Markov network. Let $\Phi$ be a set of factors over $X$ and $P_{\Phi}$ be the associated distribution. We assume that any observations were already assimilated into $\Phi$ , so that our goal is to compute $P_{\Phi}(\pmb{Y})$ for some set of query variables $Y$ . For example, if we want to do inference in the Student network given the evidence $G=g$ , we would reduce the factors reduced to this context, giving rise to the network structure shown in ﬁgure $4.6\mathrm{b}$ . 

![](images/74a76c49f09b8e4965ade5a5c617045220d9a710e0e6ff76d697ba728455459e.jpg) 
Figure 9.C.1 — Comparison of algorithms for selecting variable elimination ordering. 

Computational cost of variable elimination inference in a range of benchmark networks, obtained by various algorithms for selecting an elimination ordering. The cost is measured as the size of the factors generated during the process of variable elimination. For each network, we see the cost of the best-known ordering, the ordering obtained by Hugin (a state-of-the-art Bayesian network package), and the ordering obtained by stochastic greedy search using four diferent search heuristics — Min-Neighbors, Min-Weight, Min-Fill, and Weighted-Min-Fill — run for 1 minute and for 10 minutes. 

![](images/b77a7a5be7063f947b9056ce4539581a1c9feb0289bdbc5864425b2c9b9fc106.jpg) 

The conditioning algorithm is based on the following simple derivation. Let $U\subseteq X$ be any set of variables. Then we have that: 

$$
\tilde{P}_{\Phi}(Y)=\sum_{\mathbf{\substack{u}\in\,}V a l(U)}\tilde{P}_{\Phi}(Y,\mathbf{\acute{u}}).
$$ 

The key observation is that each term $\tilde{P}_{\Phi}(Y,{\pmb u})$ can be computed by marginalizing out the variable in $X\mathrm{~-~}U\mathrm{~-~}Y$ in the unnormalized measure $\tilde{P}_{\Phi}[{\pmb u}]$ obtained by reducing $\mathbf{\bar{\mathit{P}}}_{\Phi}$ to the context u . As we have already discussed, the reduced measure is simply the measure deﬁned by reducing each of the factors to the context $\mathbfit{u}$ . The reduction process generally produces a simpler structure, with a reduced inference cost. 

We can use this formula to compute $P_{\Phi}(\pmb{Y})$ as follows: We construct a network $\mathcal{H}_{\Phi}[\pmb{u}]$ for each assignment $\mathbfit{u}$ ; these networks have identical structures, but diferent parameters. We run sum-product inference in each of them, to obtain a factor over the desired query set $Y$ . We then simply add up these factors to obtain $\tilde{P}_{\Phi}(Y)$ . We can also derive $P_{\Phi}(\pmb{Y})$ by renormalizing this factor to obtain a distribution. As usual, the normalizing constant is the partition function for $P_{\Phi}$ . However, applying equation (9.11) to the case of $Y=\emptyset$ , we conclude that 

$$
Z_{\Phi}=\sum_{\pmb{u}}Z_{\Phi[\pmb{u}]}.
$$ 

Thus, we can derive the overall partition function from the partition functions for the diferent subnetworks $\mathcal{H}_{\Phi[\pmb{u}]}$ . The ﬁnal algorithm is shown in algorithm 9.5. (We note tha Cond-Prob-VE was called without evidence, since we assumed for simplicity that our factors Φ have already been reduced with the evidence.) 

Assume that we want to compute $P(J)$ in the Student network with evidence $G=g^{1}$ , so that our initial graph would be the one shown in ﬁgure $4.6b$ . We can now perform inference by enumerating all of the assignments s to the variable $S$ . For each such assignment, we run inference on a graph structured as in ﬁgure 4.6c, with the factors reduced to the assignment $g^{1},s$ . In each such network we compute a factor over $J$ , and add them all up. Note that the reduced network contains two disconnected components, and so we might be tempted to run inference only on the component that contains $J$ . However, that procedure would not produce a correct answer: The value we get by summing out the variables in the second component multiplies our ﬁnal factor. Although this is $^a$ constant multiple for each value of $s$ , these values are generally diferent for the diferent values of $S$ . Because the factors are added before the ﬁnal renormalization, this constant inﬂuences the weight of one factor in the summation relative to the other. Thus, if we ignore this constant component, the answers we get from the $s^{1}$ computation and the $s^{0}$ computation would be weighted incorrectly. 

cutset conditioning 

Historically, owing to the initial popularity of the polytree algorithm, the conditioning ap- proach was mostly used in the case where the transformed network is a polytree. In this case, the algorithm is called cutset conditioning . 

# 9.5.2 Conditioning and Variable Elimination 

At ﬁrst glance, it might appear as if this process saves us considerable computational cost over the variable elimination algorithm. After all, we have reduced the computation to one that performs variable elimination in a much simpler network. The cost arises, of course, from the fact that, when we condition on $U$ , we need to perform variable elimination on the conditioned network multiple times, once for each assignment $u\in V a l(U)$ . e cost of this computation is $O(|V a l(\pmb{U})|)$ , which is exponential in the number of variables in U . Thus, we have not avoided the exponential blowup associated with the probabilistic inference process. In this section, we provide a formal complexity analysis of the conditioning algorithm, and compare it to the complexity of elimination. This analysis also reveals various interesting improvements to the basic conditioning algorithm, which can dramatically improve its performance in certain cases. 

To understand the operation of the conditioning algorithm, we return to the basic description of the probabilistic inference task. Consider our query $J$ in the Extended Student network. We know that: 

$$
p(J)=\sum_{C}\sum_{D}\sum_{I}\sum_{S}\sum_{G}\sum_{L}\sum_{H}P(C,D,I,S,G,L,H,J).
$$ 

Reordering this expression slightly, we have that: 

$$
p(J)=\sum_{g}\left[\sum_{C}\sum_{D}\sum_{I}\sum_{S}\sum_{L}\sum_{H}P(C,D,I,S,g,L,H,J)\right].
$$ 

The expression inside the parentheses is precisely the result of computing the probability of $J$ in the network $\mathcal{H}_{\Phi_{G=g}}$ , where $\Phi$ is the set of CPD factors in $\mathcal{B}$ . 

In other words, the conditioning algorithm is simply executing parts of the basic summation deﬁning the inference task by case analysis, enumerating the possible values of the conditioning 

![](images/1f965b24e2c88bafa776e8efd8693a4ca915d4e4dd83a37f5e25fceb4f5ca10f.jpg) 

variables. By contrast, variable elimination performs the same summation from the inside out, using dynamic programming to reuse computation. 

Indeed, if we simply did conditioning on all of the variables, the result would be an explicit summation of the entire joint distribution. In conditioning, however, we perform the condi- tioning step only on some of the variables, and use standard variable elimination — dynamic programming — to perform the rest of the summation, avoiding exponential blowup (at least over that part). 

In general, it follows that both algorithms are performing the same set of basic operations (sums and products). However, where the variable elimination algorithm uses the caching of dynamic programming to save redundant computation throughout the summation, conditioning uses a full enumeration of cases for some of the variables, and dynamic programming only at the end. 

From this argument, it follows that conditioning always performs no fewer steps than variable elimination. To understand why, consider the network of example 9.4 and assume that we are trying to compute $P(J)$ . The conditioned network ${\mathcal{H}}_{\Phi_{G=g}}$ has a set of factors most of which are identical to those in the original network. The exceptions are the reduced factors: $\phi_{L}[G=g](L)$ and $\phi_{H}[G=g](H,J)$ . For each of the three values $g$ of $G$ , we are performing variable elimination over these factors, eliminating all variables except for $G$ and $J$ . 

We can imagine “lumping” these three computations into one, by augmenting the scope of each factor with the variable $G$ . More precisely, we deﬁne a set of augmented factors $\phi^{+}$ as follows: The scope of the factor $\phi_{G}$ already contains $G$ , so $\phi_{G}^{+}(G,D,I)=\phi_{G}(G,D,I)$ . For the factor $\phi_{L}^{+}$ , we simply combine the three factors $\phi_{L,g}(L)$ , so that $\phi_{L}^{+}(L,g)=\phi_{L}[G=g](L)$ for all $g$ . Not surprisingly, the resulting factor $\phi_{L}^{+}(L,G)$ is simply our original CPD factor $\phi_{L}(L,G)$ . We deﬁne $\phi_{H}^{+}$ in the same way. The remaining factors are unrelated to $G$ . For each other variable $X$ over scope $Y$ , we simply deﬁne $\phi_{X}^{+}(Y,G)=\phi_{X}(Y)$ ; that is, the value of the factor does not depend on the value of $G$ . 

We can easily verify that, if we run variable elimination over he se of factors $\mathcal{F}_{X}^{+}$ for $X\,\in\,\{C,D,I,G,S,L,J,H\}$ , eliminating all variables except for J and G , we are perform- ing precisely the same computation as the three iterations of variable elimination for the three diferent conditioned networks $\mathcal{H}_{\Phi_{G=g}}$ : Factor entries involving diferent values $g$ of $G$ never in- 

![](images/accdf1e5efff758ee985d89c56f95ff4adca95ef4a0f4296d57d15b36c96605d.jpg) 

teract, and the computation performed for the entries where $G=g$ is precisely the computation performed in the network $\mathcal{H}_{\Phi_{G=g}}$ . 

Speciﬁcally, assume we are using the ordering $C,D,I,H,S,L$ to perform the elimination within each conditioned network $\mathcal{H}_{\Phi_{G=g}}$ . The steps of the computation are shown in table 9.4. Step (7) corresponds to the product of all of the remaining factors, which is the last step in variable elimination. The ﬁnal step in the conditioning algorithm, where we add together the results of the three computations, is precisely the same as eliminating $G$ from the resulting factor $\tau_{7}(G,J)$ . 

It is instructive to compare this execution to the one obtained by running variable elimination on the original set of factors, with the elimination ordering $C,D,I,H,S,L,G$ ; that is, we follow the ordering used within the conditioned networks for the variables other than $G,J$ , and then eliminate $G$ at the very end. In this process, shown in table 9.5, some of the factors involve $G$ , but others do not. In particular, step (1) in the elimination algorithm involves only $C,D$ , whereas in the conditioning algorithm, we are performing precisely the same computation over $C,D$ three times: once for each value $g$ of $G$ . In general, we can show: 

Let $\Phi$ be a set of factors, and $Y$ be a query. Let $U$ be a set of conditioning variables, and $Z=\mathcal{X}-Y-U$ . Let $\prec\,b e$ the elimination ordering over $Z$ used he variable elimination algorithm over e network $\mathcal{H}_{\Phi_{u}}$ in the onditioning algorithm. Let ≺ $\prec^{+}$ rdering that is with ≺ over the variables in Z , and where, for each variable $U\in U$ ∈ , we have that $Z\prec^{+}U$ ≺ . Then the number of operations performed by the conditi ng is no less than the number of operations performed by variable elimination with the ordering ≺ $\prec^{+}$ . 

We omit the proof of this theorem, which follows precisely the lines of our example. 

Thus, conditioning always requires no fewer computations than variable elimination with some particular ordering (which may or may not be a good one). In our example, the wasted computation from conditioning is negligible. In other cases, however, as we will discuss, we can end up with a large amount of redundant computation. In fact, in some cases, conditioning can be signiﬁcantly worse: 

![](images/fe534ffbae8d7f5e5015b2156bf131461f0a77a282c24967e6af874a2e613657.jpg) 
Figure 9.12 Networks where conditioning performs unnecessary computation 

to cut the single loop in the network. In this case, we would perform the entire elimination of the chain $A_{1}\to.\,.\,.\to A_{k-1}$ multiple times — once for every value of $A_{k}$ . 

Consider the network shown in ﬁgure 9.12b and assume that we wish to use cutset conditioning, where we cut every loop in the network. The most efcient way of doing so is to condition on every other $A_{i}$ variable, for example, $A_{2},A_{4},.\cdot\cdot,A_{k}$ (assuming for simplicity that $k$ is even). The cost of the conditioning algorithm in this case is exponential in $k$ , whereas the induced width of the network is 2 , and the cost of variable elimination is linear in $k$ . 

Given this discussion, one might wonder why anyone bothers with the conditioning algorithm. There are two main reasons. First, variable elimination gains its computational savings from caching factors computed as intermediate results. In complex networks, these factors can grow very large. In cases where memory is scarce, it might not be possible to keep these factors in memory, and the variable elimination computation becomes infeasible (or very costly due to constant thrashing to disk). On the other hand, conditioning does not require signiﬁcant amounts of memory: We run inference separately for each assignment $\mathbfit{u}$ to $U$ and simply accumulate the results. Overall, the computation requires space that is linear only in the size of the network. Thus, we can view the trade-of of conditioning versus variable elimination as a time-space trade-of. Conditioning saves space by not storing intermediate results in memory, but then it may cost additional time by having to repeat the computation to generate them. 

The second reason for using conditioning is that it forms the basis for a useful approximate inference algorithm. In particular, in certain cases, we can get a reasonable approximate solution by enumerating only some of the possible assignment $u\in V a l(U)$ . We return to this approach in section 12.5 

# 9.5.3 Graph-Theoretic Analysis 

As in the case of variable elimination, it helps to reformulate the complexity analysis of the conditioning algorithm in graph-theoretic terms. Assume that we choose to condition on a set $U$ , and perform variable elimination on the remaining variables. We can view each of these steps in terms of its efect on the graph structure. 

Let us begin with the step of conditioning the network on some variable $U$ . Once again, it is easiest to view this process in terms of its efect on an undirected graph. As we discussed, this step efectively introduces $U$ into every factor parameterizing the current graph. In graph- theoretic terms, we have introduced $U$ into every clique in the graph, or, more simply, introduced an edge between $U$ and every other node currently in the graph. 

When we ﬁnish the conditioning process, we perform elimination on the remaining variables. We have already analyzed the efect on the graph of eliminating a variable $X$ : When we eliminate $X$ , we add edges between all of the current neighbors of $X$ in the graph. We then remove $X$ from the graph. 

We can now deﬁne an induced graph for the conditioning algorithm. Unlike the graph for variable elimination, this graph has two types of ﬁll edges: those induced by conditioning steps, and those induced by the elimination steps for the remaining variables. 

Deﬁnition 9.7 conditioning induced graph 

# Example 9.7 

$\Phi$ be a set of factors over $\mathcal{X}=\{X_{1},\cdot\cdot\cdot,X_{n}\}$ $U\subset\mathcal X$ a set of conditioning variables, and $\prec\,b e$ ≺ be an elimination ering for some subset $X\subseteq\mathcal{X}-U$ ⊆X − . The induced graph $\mathcal{L}_{\Phi,-\langle,U|}$ is an undirected graph over X with the following edges: • $a$ conditioning edge between every variable $U\in U$ and every other variable $X\in{\mathcal{X}}$ ; • $a$ factor ed e between every pair of variables $X_{i},X_{j}\in X$ that both appear in some interme- diate factor ψ generated by the VE algorithm using ≺ as an elimination ordering. 

Consider the Student example of ﬁgure 9.8, where our query is $P(J)$ . Assume that (for some reason) we condition on the variable $L$ and perform elimination on the remaining variables using the ordering $C,D,I,H,G,S$ . The graph induced by this conditioning set and this elimination ordering is shown in ﬁgure 9.13, with the conditioning edges shown as dashed lines and the factor edges shown, as usual, by complete lines. The step of conditioning on $L$ causes the introduction of the edges between $L$ and all the other variables. The set of factors we have after the conditioning step immediately leads to the introduction of all the factor edges except for the edge $G{-}S_{i}$ ; this latter edge results from the elimination of $I$ . 

We can now use this graph to analyze the complexity of the conditioning algorithm. 

# Theorem 9.12 

![](images/fa7b2af60cfcabcf61bd5fd5e5bb2d44acccc45d95be4199c674e423ef6604a1.jpg) 

Figure 9.13 Induced graph for the Student example using both conditioning and elimination: we condition on $L$ and eliminate the remaining variables using the ordering $C,D,I,H,G,S$ . 

The proof is left as an exercise (exercise 9.12). 

This theorem provides another perspective on the trade-of between conditioning and elimi- nation in terms of their time complexity. Consider, as we did earlier, an algorithm that simply defers the elimination of the conditioning variables $U$ until the end. Consider the efect on the graph of the earlier steps of the elimination algorithm (those preceding the elimination of $U$ ). As variables are eliminated, certain edges might be added between the variables in $U$ and other variables (in particular, we add an dge between $X$ and $U\in U$ whenever they are both neigh- bors of some eliminated variable Y $Y$ ). However, conditioning adds edges between the variables $U$ and all other variables $X$ . Thus, conditioning always results in a graph that contains at least as many edges as the induced graph from elimination using this ordering. 

However, we can also use the same graph to precisely estimate the time-space trade-of provided by the conditioning algorithm. 

Consider an application of the co tioning algorithm to a set of factors $\Phi$ , where $U\subset\mathcal{X}$ is the ning variables, and ≺ is the elimination ordering used for the liminated variables $X\subseteq\mathcal{X}-U$ ⊆X − . The space complexity of the algorithm is $O(n\cdot v^{m_{f}})$ , where v is a bound on the domain size of any variable, and $m_{f}$ is the size of the largest clique in the graph using only factor edges. 

The proof is left as an exercise (exercise 9.13). 

By comparison, the asymptotic space complexity of variable elimination is the same as its time complexity: exponential in the size of the largest clique containing both types of edges. Thus, we see precisely that conditioning allows us to perform the computation using less space, at the cost (usually) of additional running time. 

# 9.5.4 Improved Conditioning 

As we discussed, in terms of the total operations performed, conditioning cannot be better than variable elimination. As we now show, conditioning, naively applied, can be signiﬁcantly worse. 

However, the insights gained from these examples can be used to improve the conditioning algorithm, reducing its cost signiﬁcantly in many cases. 

# 9.5.4.1 Alternating Conditioning and Elimination 

As we discussed, the main problem associated with conditioning is the fact that all computations are repeated for all values of the conditioning variables, even in cases where the diferent computations are, in fact, identical. This phenomenon arose in the network of example 9.5. 

It seems clear, in this example, that we uld prefer to eliminate the chain $A_{1}\to.\,.\,.\to A_{k-1}$ once and for all, before conditioning on $A_{k}$ . Having eliminated the chain, we would then end up with a much simpler network, involving factors only over $A_{k}$ , $B$ , $C$ , and $D$ , to which we can then apply conditioning. 

The perspective described in section 9.5.3 provides the foundation for implementing this idea. As we discussed, variable elimination works from the inside out, summing out variables in the innermost summation ﬁrst and caching the results. On the other hand, conditioning works from the outside in, performing the entire internal summation (using elimination) for each value of the conditioning variables, and only then summing the results. However, there is nothing that forces us to split our computation on the outermost summations before considering the inner ones. Speciﬁcally, we can eliminate one or more variables on the inside of the summation before conditioning on any variable on the outside. 

# Example 9.8 

Consider again the network of ﬁgure $9.l2a,$ , and assume that our goal is to compute $P(D)$ . We might formulate the expression as: 

$$
\sum_{A_{k}}\sum_{B}\sum_{C}\sum_{A_{1}}\cdot\cdot\cdot\sum_{A_{k-1}}P(A_{1},\cdot\cdot\cdot,A_{k},B,C,D).
$$ 

We can ﬁrst perform the internal summations on $A_{k-1},\ldots,A_{1}$ , resulting in a set of factors over the scope $A_{k},B,C,D$ . We can now condition this network (that is, the Markov network induced by the resulting set of factors) on $A_{k}$ , resulting in a set of simpliﬁed networks over $B,C,D$ (one for each value of $A_{k.}$ ). In each such network, we use variable elimination on $B$ and $C$ to compute $^a$ factor over $D$ , and aggregate the factors from the diferent networks, as in standard conditioning. 

In this example, we ﬁrst perform some elimination, then condition, and then elimination on the remaining network. Clearly, we can generalize this idea to deﬁne an algorithm that alternates the operations of elimination and conditioning arbitrarily. (See exercise 9.14.) 

# 9.5.4.2 Network Decomposition 

A second class of examples where we can signiﬁcantly improve the performance of condition- ing arises in networks where conditioning on some subset of variables splits the graph into independent pieces. 

# Example 9.9 

Consider the network of example 9.6, and assume that $k=16$ , and that we begin by conditioning on $A_{2}$ . After this step, the network is decomposed into two independent pieces. The standard conditioning algorithm would continue by conditioning further, say on $A_{3}$ . However, there is really no need to condition the top part of the network — the one associated with the variables $A_{1},B_{1},C_{1}$ on the variable $A_{3}$ : none of the factors mention $A_{3}$ , and we would be repeating exactly the same computation for each of its values. 

Clearly, having partitioned the network into two completely independent pieces, we can now perform the computation on each of them separately, and then combine the results. In particular, the conditioning variables used on one part would not be used at all to condition the other. More precisely, we can deﬁne an algorithm that checks, after each conditioning step, whether the resulting set of factors has been disconnected or not. If it has, it simply partitions them into two or more disjoint sets and calls the algorithm recursively on each subset. 

# 9.6 Inference with Structured CPDs $\star$ 

We have seen that BN inference exploits the network structure, in particular the conditional independence and the locality of inﬂuence. But when we discussed representation, we also allowed for the representation of ﬁner-grained structure within the CPDs. It turns out that a carefully designed inference algorithm can also exploit certain types of local CPD structure. We focus on two types of structure where this issue has been particularly well studied — independence of causal inﬂuence, and asymmetric dependencies — using each of them to illustrate a diferent type of method for exploiting local structure in variable elimination. We defer the discussion of inference in networks involving continuous variables to chapter 14. 

# 9.6.1 Independence of Causal Inﬂuence 

The earliest and simplest instance of exploiting local structure was for CPDs that exhibit inde- pendence of causal inﬂuence, such as noisy-or. 

# 9.6.1.1 Noisy-Or Decompositions 

Consider a simple network consisting of a binary variable $Y$ and its four binary parents $X_{1},X_{2},X_{3},X_{4}$ , where the CPD of $Y$ is a noisy-or. Our goal is to compute the probability of $Y$ . The operations required to execute this process, assuming we use an optimal ordering, is: 

• 4 multiplications for $P(X_{1})\cdot P(X_{2})

$ • 8 multiplications for $P(X_{1},X_{2})\cdot P(X_{3})

$ • 16 multiplications for $P(X_{1},X_{2},X_{3})\cdot P(X_{4})

$ • 32 multiplications for P $P(X_{1},X_{2},X_{3},X_{4})\cdot P(Y\mid X_{1},X_{2},X_{3},X_{4})$ 

The total is 60 multiplications, plus another 30 additions to sum out $X_{1},\dots,X_{4}$ , in order to reduce the resulting factor $P(X_{1},X_{2},X_{3},X_{4},Y)$ , of size 32, into the factor $P(Y)$ of size 2. 

However, we can exploit the structure of the CPD to substantially reduce the amount of computation. As we discussed in section 5.4.1, a noisy-or variable can be decomposed into a de- terministic OR of independent noise variables, resulting in the subnetwork shown in ﬁgure $9.14\mathrm{a}$ . This transformation, by itself, is not very helpful. The factor $P(Y\mid Z_{1},Z_{2},Z_{3},Z_{4})$ is still of size 32 if we represent it as a full factor, so we achieve no gains. 

The key idea is that the deterministic OR variable can be decomposed into various cascades of deterministic OR variables, each with a very small indegree. Figure $9.14\mathrm{b}$ shows a simple 

![](images/9803ea1ef758e8cb0e8659467d11eb294a68518011cc517ca2308a6ec2f2191f.jpg) 
Figure 9.14 Diferent decompositions for a noisy-or CPD: (a) The standard decomposition of a noisy- or. (b) A tree decomposition of the deterministic-or. (c) A tree-based decomposition of the noisy-or. (d) A chain-based decomposition of the noisy-or. 

decomposition of the deterministic OR as a tree. We can simplify this construction by eliminating the intermediate variables $Z_{i}$ , integrating the “noise” for each $X_{i}$ into the appropriate $O_{i}$ . In particular, $O_{1}$ would be the noisy-or of $X_{1}$ and $X_{2}$ , with the original noise parameters and a leak parameter of 0 . The resulting construction is shown in ﬁgure $9.14\mathrm{c}$ . 

We can now revisit the inference task in this apparently more complex network. An optimal ordering for variable elimination is $X_{1},X_{2},X_{3},X_{4},O_{1},O_{2}$ . The cost of performing elimination of $X_{1},X_{2}$ is: 

• 8 multiplications for $\psi_{1}(X_{1},X_{2},O_{1})=P(X_{1})\cdot P(O_{1}\mid X_{1},X_{2})

$ • 4 additions to sum o $X_{1}$ $\begin{array}{r}{\tau_{1}(X_{2},O_{1})=\sum_{X_{1}}\psi_{1}(X_{1},X_{2},O_{1})}\end{array}

$ • 4 multiplications for $\psi_{2}(X_{2},O_{1})=\tau_{1}(X_{2},O_{1})\cdot P(X_{2})$ ·

 • 2 additions for $\begin{array}{r}{\tau_{2}(O_{1})=\sum_{X_{2}}\psi_{2}(X_{2},O_{1})}\end{array}$ 

The cost for eliminating $X_{3},X_{4}$ is identical, as is the cost for subsequently eliminating $O_{1},O_{2}$ . Thus, the total number of operations is $3\cdot(8+4)=36$ multiplications and $3\cdot(4+2)=18$ additions. 

A diferent decomposition of the OR variable is as a simple cascade, where each $Z_{i}$ is consec- utively OR’ed with the previous intermediate result. This decomposition leads to the construction of ﬁgure $9.14\mathrm{d}$ . For this construction, an optimal elimination ordering is $X_{1},O_{1},X_{2},O_{2},X_{3},O_{3},X_{4}$ A simple analysis shows that it takes 4 multiplications and 2 additions to eliminate each of $X_{1},\dots,X_{4}$ , and 8 multiplications and 4 additions to eliminate each of $O_{1},O_{2},O_{3}$ . The total cost is $4\cdot4+3\cdot8=40$ multiplications and $4\cdot2+3\cdot4=20$ additions. 

# 9.6.1.2 The General Decomposition 

Clearly, the construction used in the preceding example is a general one that can be applied to more complex networks and other types of CPDs that have independence of causal inﬂuence. We take a variable whose CPD has independence of causal inﬂuence, and generate its decomposition into a set of independent noise models and a deterministic function, as in ﬁgure 5.13. 

We then cascade the computation of the deterministic function into a set of smaller steps. Given our assumption about the symmetry and associativity of the deterministic function in the deﬁnition of symmetric ICI (deﬁnition 5.13), any decomposition of the deterministic function results in the same answer. Speciﬁcally, consider a variable $Y$ with parents $X_{1},\ldots,X_{k}$ , whose deﬁnition 5.13. We can decompose $Y$ by introducing $k-1$ intermediate variables $O_{1},.\ldots,O_{k-1}$ , such that: 

• the variable $Z$ , and each of the $O_{i}$ ’s, has exactly two parents in $Z_{1},.\,.\,.\,,Z_{k},O_{1},.\,.\,.\,,O_{i-1}$ ;

 • the CPD of $Z$ and of $O_{i}$ is the deterministic $\diamondsuit$ of its two parents;

 • each $Z_{l}$ and each $O_{i}$ is a parent of at most one variable in $O_{1},.\,.\,.\,,O_{k-1},Z$ . 

These conditions ensure that $Z=Z_{1}\!\diamond Z_{2}\!\diamond...\diamond Z_{k}$ , but that this function is computed gradually, where the node corresponding to each intermediate result has an indegree of 2. 

We note that we can save some extraneous nodes, as in our example, by aggregating the noisy dependence of $Z_{i}$ on $X_{i}$ into the CPD where $Z_{i}$ is used. 

After executing this decomposition for every ICI variable in the network, we can simply apply variable elimination to the decomposed network with the smaller factors. As we saw, the complexity of the inference can go down substantially if we have smaller CPDs and thereby smaller factors. 

We note that the sizes of the intermediate factors depend not only on the number of variables in their scope, but also on the domains of these variables. For the case of noisy-or variables (as well as noisy-max, noisy-and, and so on), the domain size of these variables is ﬁxed and fairly small. However, in other cases, the domain might be quite large. In particular, in the case of generalized linear models, the domain of the intermediate variable $Z$ generally grows linearly with the number of parents. 

Example 9.10 Consider a variable $Y$ with $\mathrm{Pa}_{Y}=\{X_{1},.\,.\,.\,,X_{k}\}$ , where each $X_{i}$ is binary. Assu at Y ’s CPD is a generalized linear model, whose parameters are $w_{0}=0$ and $w_{i}=w$ for all $i>1$ . Then the domain of the intermediate variable $Z$ is $\{0,1,\ldots,k\}$ . In this case, th decomposition provides $^a$ trade-of: The size of the original CPD for $P(Y\mid X_{1},.\,.\,,X_{k})$ grows as $2^{k}$ ; the size of the factors in the decomposed network grow roughly as k $k^{3}$ . In diferent situations, one approach might be better than the other. 

Thus, the decomposition of symmetric ICI variables might not always be beneﬁcial. 

# 9.6.1.3 Global Structure 

Our decomposition of the function $f$ that deﬁnes the variable $Z$ can be done in many ways, all of which are equivalent in terms of their ﬁnal result. However, they are not equivalent from the perspective of computational cost. Even in our simple example, we saw that one decomposition can result in fewer operations than the other. The situation is signiﬁcantly more complicated when we take into consideration other dependencies in the network. 

Example 9.11 Consider the network of ﬁgure $9.14c,$ and assume that $X_{1}$ and $X_{2}$ have a joint parent $A$ . In this case, we eliminate $A$ ﬁrst, and end up with a factor over $X_{1},X_{2}$ . Aside from the $4+8\,=$ 12 multiplications and 4 additions required to compute this factor $\tau_{0}(X_{1},X_{2})$ , it now takes 8 multiplications to com te $\psi_{1}(X_{1},X_{2},O_{1})\,=\,\tau_{0}(X_{1},X_{2})\,\cdot\,P(O_{1}\mid X_{1},X_{2})\nonumber$ , and $4+2\,=\,6$ additions to sum out $X_{1}$ and $X_{2}$ in $\psi_{1}$ . The rest of the computation remains unchanged. Thus, the total number of operations required to eliminate all of $X_{1},\dots,X_{4}$ (after the elimination of $A$ ) is $8+12=20$ multiplications and $6+6=12$ additions. 

Conversely, assume that $X_{1}$ and $X_{3}$ have the joint parent $A$ . In this case, it still requires 12 multiplications and 4 additions to compute a factor $\tau_{0}(X_{1},X_{3})$ , but the remaining operations become signiﬁcantly more complex. In particular, it takes: 

• 8 multiplications for $\psi_{1}(X_{1},X_{2},X_{3})=\tau_{0}(X_{1},X_{3})\cdot{\cal P}(X_{2})$ • 16 multiplications for $\cdot\,\psi_{2}(X_{1},X_{2},X_{3},O_{1})=\psi_{1}(X_{1},X_{2},X_{3})\cdot P(O_{1}\mid X_{1},X_{2})$ ) • 8 additions fo $\begin{array}{r}{\tau_{2}(X_{3},O_{1})=\sum_{X_{1},X_{2}}\psi_{2}(X_{1},X_{2},X_{3},O_{1})}\end{array}$ 

The same number of operations is required to eliminate $X_{3}$ and $X_{4}$ . (Once these steps are completed, we can eliminate $O_{1},O_{2}$ as usual.) Thus, the total number of operations required to eliminate all of $X_{1},\dots,X_{4}$ (after the elimination of $A$ ) is $2\cdot(8+16)=48$ multiplications and $2\cdot8=16$ additions, considerably more than our previous case. 

Clearly, in the second network structure, had we done the decomposition of the noisy-or variable so as to make $X_{1}$ and $X_{3}$ parents of $O_{1}$ (and $X_{2},X_{4}$ parents of $O_{2}.$ ), we would get the same cost as we did in the ﬁrst case. However, in order to do that, we need to take into consideration the global structure of the network, and even the order in which other variables are eliminated, at the same time that we are determining how to decompose a particular variable with symmetric ICI. In particular, we should determine the structure of the decomposition at the same time that we are considering the elimination ordering for the network as a whole. 

# 9.6.1.4 Heterogeneous Factorization 

An alternative approach that achieves this goal uses a diferent factorization for a network — one that factorizes the joint distribution for the network into CPDs, as well as the CPDs of symmetric ICI variables into smaller components. This factorization is heterogeneous , in that some factors must be combined by product, whereas others need to be combined using the type of operation that corresponds to the symmetric ICI function in the corresponding CPD. One can then deﬁne a heterogeneous variable elimination algorithm that combines factors, using whichever operation is appropriate, and that eliminates variables. Using this construction, we can determine a global ordering for the operations that determines the order in which both local 

![](images/c2e513144c284e547fc260b8541b58ba035cec3923106790dc98c3f11d7fa58d.jpg) 
Figure 9.15 A Bayesian network with rule-based structure: (a) the network structure; (b) the CPD for the variable $D$ . 

factors and global factors are combined. Thus, in efect, the algorithm determines the order in which the components of an ICI CPD are “recombined” in a way that takes into consideration the structure of the factors created in a variable elimination algorithm. 

# 9.6.2Context-Speciﬁc Independence 

A second important type of local CPD structure is the context-speciﬁc independence, typically encoded in a CPD as trees or rules. As in the case of ICI, there are two main ways of exploiting this type of structure in the context of a variable elimination algorithm. One approach (exercise 9.15) uses a decomposition of the CPD, which is performed as a preprocessing step on the network structure; standard variable elimination can then be performed on the modiﬁed network. The second approach, which we now describe, modiﬁes the variable elimination algorithm itself to conduct its basic operations on structured factors. We can also exploit this structure within the context of a conditioning algorithm. 

# 9.6.2.1 Rule-Based Variable Elimination 

An alternative approach is to introduce the structure directly into the factors used in the variable elimination algorithm, allowing it to take advantage of the ﬁner-grained structure. It turns out that this approach is easier to understand and implement for CPDs and factors represented as rules, and hence we present the algorithm in this context. 

As speciﬁed in section 5.3.1.2, a rule-based CPD is described as a set of mutually exclusive and exhaustive rules, where each rule $\rho$ has the form $\langle c;p\rangle$ . As we already discussed, a tree-CPD and a tabular CPD can each be converted into a set of rules in the obvious way. 

# Example 9.12 

set of rules: 

$$
\left\{\begin{array}{c c}{{\rho_{1}}}&{{\langle b^{0},d^{0};1-q_{1}\rangle}}\\ {{\rho_{2}}}&{{\langle b^{0},d^{1};q_{1}\rangle}}\\ {{}}&{{}}\\ {{\rho_{3}}}&{{\langle a^{0},b^{1},d^{0};1-q_{2}\rangle}}\\ {{\rho_{4}}}&{{\langle a^{0},b^{1},d^{1};q_{2}\rangle}}\\ {{\rho_{5}}}&{{\langle a^{1},b^{1},d^{0};1-q_{3}\rangle}}\\ {{\rho_{6}}}&{{\langle a^{1},b^{1},d^{0};q_{3}\rangle}}\end{array}\right\}
$$ 

Assume that the CPD $P(E\mid A,B,C,D)$ is also associated with a set of rules. Our discussion will focus on rules involving the variable D , so we show only that part of the rule set: 

$$
\left\{\begin{array}{c c}{\rho_{7}}&{\langle a^{0},d^{0},e^{0};1-p_{1}\rangle}\\ {\rho_{8}}&{\langle a^{0},d^{0},e^{1};p_{1}\rangle}\\ {\rho_{9}}&{\langle a^{0},d^{1},e^{0};1-p_{2}\rangle}\\ {\rho_{10}}&{\langle a^{0},d^{1},e^{1};p_{2}\rangle}\\ {\rho_{11}}&{\langle a^{1},b^{0},c^{1},d^{0},e^{0};1-p_{4}\rangle}\\ {\rho_{12}}&{\langle a^{1},b^{0},c^{1},d^{0},e^{1};p_{4}\rangle}\\ {\rho_{13}}&{\langle a^{1},b^{0},c^{1},d^{1},e^{0};1-p_{5}\rangle}\\ {\rho_{14}}&{\langle a^{1},b^{0},c^{1},d^{1},e^{1};p_{5}\rangle}\end{array}\right\}
$$ 

Using this type of process, the entire distribution can be factorized into a multiset of rules ${\mathcal{R}},$ which is the union of all of the rules associated with he CPDs of the diferent v ables in the network. Then, the probability of any instantiation ξ to the network variables X can be computed as 

$$
P(\xi)=\prod_{\langle c;p\rangle\in\mathcal{R},\xi\sim c}p,
$$ 

where we recall that $\xi\sim c$ holds if the assignments $\xi$ and $^c$ are compatible, in that they assign the same values to those variables that are assigned values in both. 

Thus, as for the tabular CPDs, the distribution is deﬁned in terms of a product of smaller components. In this case, however, we have broken up the tables into their component rows. This deﬁnition immediately suggests that we can use similar ideas to those used in the table- based variable elimination algorithm. In particular, we can multiply rules with each other and sum out a variable by adding up rules that give diferent values to the variables but are the same otherwise. 

In general, we deﬁne the following two key operations: 

Deﬁnition 9.8 rule product 

Deﬁnition 9.9 rule sum 

This deﬁnition is signiﬁcantly more restricted than the product of tabular factors, since it requires that the two rules have precisely the same context. We return to this issue in a moment. 

After this operation, $Y$ is summed out in the context $^c$ . 

Both of these operations can only be applied in very restricted settings, that is, to sets of rules that satisfy certain stringent conditions. In order to make our set of rules amenable to the application of these operations, we might need to reﬁne some of our rules. We therefore deﬁne the following ﬁnal operation: 

Deﬁnition 9.10 rule split 

Let $\rho=\langle c;p\rangle$ be a rule, and let $Y$ be a variable. We deﬁne the rule split $S p l i t(\rho\angle Y)$ as follows: If $Y\in S c o p e[c].$ , then $S p l i t(\rho\angle Y)=\{\rho\}.$ ; otherwise, 

$$
S p l i t(\rho\angle Y)=\{\langle c,Y=y;p\rangle\ :\ y\in V a l(Y)\}.
$$ 

In general, the pu ose of rule splitti is to make the context of one rule $\rho=\langle c;p\rangle$ compatible with the context c $c^{\prime}$ of another rule $\rho^{\prime}$ . Naively, we might take all the variables in $S c o p e[c^{\prime}]\mathrm{~-~}$ Scope [ c ] and split $\rho$ recursively on each one of them. However, this process creates unnecessarily many rules. 

Example 9.13 Consider $\rho_{2}$ and $\rho_{14}$ in example 9.12, and assume we want to multiply them together. To do so, we need to split $\rho_{2}$ in order to produce a rule with an identical context. If we naively split $\rho_{2}$ on all three variables $A,C,E$ that appear in $\rho_{14}$ and not in $\rho_{2}$ , the result would be eight rules of the form: $\langle a,b^{0},c,d^{1},e;q_{1}\rangle$ , one for each combination of values $a,c,e$ . However, the only rule we really need in order to perform the rule produ operation is $\langle a^{1},b^{0},c^{1},d^{1},e^{1};q_{1}\rangle$ . 

Intuitively, having split $\rho_{2}$ on the variable A , it is wasteful to continue splitting the rule whose context is $a^{0}$ , since this rule (and any derived from it) will not participate in the desired rule product operation with $\rho_{14}$ . Thus, a more parsimonious split of $\rho_{14}$ that still generates this last rule is: 

$$
\left\{\begin{array}{l}{\langle a^{0},b^{0},d^{1};q_{1}\rangle}\\ {\langle a^{1},b^{0},c^{0},d^{1};q_{1}\rangle}\\ {\langle a^{1},b^{0},c^{1},d^{1},e^{0};q_{1}\rangle}\\ {\langle a^{1},b^{0},c^{1},d^{1},e^{1};q_{1}\rangle}\end{array}\right\}
$$ 

This new rule set is still a mutually exclusive and exhaustive partition of the space originally covered by $\rho_{2}$ , but contains only four rules rather than eight. 

In general, we can construct these more parsimonious splits using the recursive procedure shown in algorithm 9.6. This procedure gives precisely the desired result shown in the example. Rule splitting gives us the tool to take a set of rules and reﬁne them, allowing us to apply either the rule-product operation or the rule-sum operation. The elimination algorithm is shown in algorithm 9.7. Note that the ﬁgure only shows the procedure for eliminating a single variable $Y$ . The outer loop, which iteratively eliminates nonquery variables one at a time, is precisely the same as the Sum-Product-VE procedure in algorithm 9.1, except that it takes as input a set of rule factors rather than table factors. 

To understand the operation of the algorithm more concretely, consider the following example: 

# Example 9.14 

![](images/a37f7ee6a7bcd7687cdc05ac7e6cc56453d9c13f161dc249f38c374d1ef6bce4.jpg) 

The rules $\rho_{3}$ on the one hand, and $\rho_{7},\rho_{8}$ on the other, have compatible contexts, so we can choose to combine them. We begin by splitting $\rho_{3}$ and $\rho_{7}$ on each other’s context, which results in: 

$$
\left\{\begin{array}{l l}{\rho_{15}}&{\left\langle a^{0},b^{1},d^{0},e^{0};1-q_{2}\right\rangle}\\ {\rho_{16}}&{\left\langle a^{0},b^{1},d^{0},e^{1};1-q_{2}\right\rangle}\\ {}&{}\\ {\rho_{17}}&{\left\langle a^{0},b^{0},d^{0},e^{0};1-p_{1}\right\rangle}\\ {\rho_{18}}&{\left\langle a^{0},b^{1},d^{0},e^{0};1-p_{1}\right\rangle}\end{array}\right\}
$$ 

The contexts of $\rho_{15}$ and $\rho18$ match, so we can now apply rule product, replacing the pair by: 

$$
\left\{\begin{array}{c c}{{\rho_{19}}}&{{\langle a^{0},b^{1},d^{0},e^{0};(1-q_{2})(1-p_{1})\rangle}}\end{array}\right\}
$$ 

We can now split $\rho_{8}$ using the context of $\rho_{16}$ and multiply the matching rules together, obtaining 

$$
\left\{\begin{array}{c c}{{\rho_{20}}}&{{\langle a^{0},b^{0},d^{0},e^{1};p_{1}\rangle}}\\ {{\rho_{21}}}&{{\langle a^{0},b^{1},d^{0},e^{1};(1-q_{2})p_{1}\rangle}}\end{array}\right\}.
$$ 

The resulting rule set contains $\rho_{17},\rho_{19},\rho_{20},\rho_{21}$ in place of $\rho_{3},\rho_{7},\rho_{8}$ . 

We can apply a similar process to $\rho_{4}$ and $\rho_{9},\rho_{10}$ , which leads to their substitution by the rule set: 

$$
\left\{\begin{array}{c c}{\rho_{22}}&{\langle a^{0},b^{0},d^{1},e^{0};1-p_{2}\rangle}\\ {\rho_{23}}&{\langle a^{0},b^{1},d^{1},e^{0};q_{2}(1-p_{2})\rangle}\\ {\rho_{24}}&{\langle a^{0},b^{0},d^{1},e^{1};p_{2}\rangle}\\ {\rho_{25}}&{\langle a^{0},b^{1},d^{1},e^{1};q_{2}p_{2}\rangle}\end{array}\right\}.
$$ 

We can now eliminate $D$ in the context $a^{0},b^{1},e^{1}$ . only rules in $\mathcal{R}^{+}$ compatible with this context are $\rho_{21}$ and $\rho_{25}$ . We extract them fro $\mathcal{R}^{+}$ R and sum them; the resu ng rule $\langle a^{0},b^{1},e^{1};(1-q_{2})p_{1}+q_{2}p_{2}\rangle$ , is then inserted into R $\mathcal{R}^{-}$ . We can similarly eliminate D in the context $a^{0},b^{1},e^{0}$ . 

The process continues, with rules being split and multiplied. When $D$ has been eliminated in a set of mutually exclusive and exhaustive contexts, then we have exhausted all rules involving $D$ ; at this point, $\mathcal{R}^{+}$ is empty, and the process of eliminating $D$ terminates. 

![](images/4a2092097f97884ecb799b4eaa218ebaf41f0ebd052bdfe32c9bf039565165e2.jpg) 

A diferent way of understanding the algorithm is to consider its application to rule sets that originate from standard table-CPDs. It is not difcult to verify that the algorithm performs exactly the same set of operations as standard variable elimination. For example, the standard operation of factor product is simply the application of rule splitting on all of the rules that constitute the two tables, followed by a sequence of rule product operations on the resulting rule pairs. (See exercise 9.16.) 

To prove that the algorithm computes the correct result, we need to show that each operation performed in the context of the algorithm maintains a ce n correctness invariant. Let $\mathcal{R}$ be the current set of rules maintained by the algorithm, and W be the variables that have not yet been eliminated. Each operation must maintain the following condition: 

![](images/7b9b0b9c031c41e81c5756fcb425611eb807f37aab18593150e9f5b3a18b4ebd.jpg) 
Figure 9.16 Conditioning a Bayesian network whose CPDs have CSI: (a) conditioning on $a^{0}$ ; (b) conditioning on $a^{1}$ . 

The probability of a context $^c$ such that $S c o p e[c]\subseteq W$ can be obtained by multiplying all rules $\langle c^{\prime};p\rangle\in{\mathcal{R}}$ whose context is compatible with $^c$ . 

It is not difcult to show that the invariant holds initially, and that each step in the algorithm maintains it. Thus, the algorithm as a whole is correct. 

# 9.6.2.2 Conditioning 

We can also use other techniques for exploiting CSI in inference. In particular, we can generalize the notion of conditioning to this setting n an interesting way. Consider a network $\mathcal{B}$ , and assume that we condition it on a variable U . So far, we have assumed that the structure of the diferent conditioned networks, for the diferent values $u$ of $U$ , is the same. When the CPDs are tables, with no extra structure, this assumption generally holds. However, when the CPDs have CSI, we might be able to utilize the additional structure to simplify the conditioned networks considerably. 

Consider the network shown in ﬁgure 9.15, as described in example 9.12. Assume we condition this network on the variable $A$ . If we condition on $a^{0}$ , we see that the reduced CPD for $E$ no longer depends on $C$ . Thus, the conditioned Markov network for this set of factors is the one shown in ﬁgure 9.16a. By contrast, when we condition on $a^{1}$ , the reduced factors do not “lose” any variables aside from $A$ , and we obtain the conditioned Markov network shown in ﬁgure 9.16b. Note that the network in ﬁgure 9.16a is so simple that there is no point performing any further conditioning on it. Thus, we can continue the conditioning process for only one of the two branches of the computation — the one corresponding to $a^{1}$ . 

In general, we can extend the conditioning algorithm of section 9.5 to account for CSI in the CPDs or in the factors of a Markov network. Consider a single conditioning step on a variable $U$ . As we enumerate the diferent possible values $u$ of $U$ , we generate a possibly diferent conditioned network for each one. Depending on the structure of this network, we select which step to take next in the context of this particular network. In diferent networks, we might choose a diferent variable to use for the next conditioning step, or we might decide to stop the conditioning process for some networks altogether. 

We have presented two approaches to variable elimination in the case of local structure in the CPDs: preprocessing followed by standard variable elimination, and specialized variable elimination algorithms that use a factorization of the structured CPD. These approaches ofer diferent trade-ofs. On the one hand, the specialized variable elimination approach reveals more of the structure of the CPDs to the inference algorithm, allowing the algorithm more ﬂexibility in exploiting this structure. Thus, this approach can achieve lower computational cost than any ﬁxed decomposition scheme (see box 9.D). By comparison, the preprocessing approach embeds some of the structure within deterministic CPDs, a structure that most variable elimination algorithms do not fully exploit. 

On the other hand, specialized variable elimination schemes such as those for rules require the use of special-purpose variable elimination algorithms rather than of-the-shelf packages. Furthermore, the data structures for tables are signiﬁcantly more efcient than those for other types of factors such as rules. Although this diference seems to be an implementation issue, it turns out to be quite signiﬁcant in practice. One can somewhat address this limitation by the use of more sophisticated algorithms that exploit efcient table-based operations whenever possible (see exercise 9.18). 

Although the trade-ofs between these two approaches is not always clear, it is generally the case that, in networks with signiﬁcant amounts of local structure, it is valuable to design an inference scheme that exploits this structure for increased computational efciency. 

Box 9.D — Case Study: Inference with Local Structure. A natural question is the extent to which local structure can actually help speed up inference. 

In one experimental comparison by Zhang and Poole (1996), four algorithms were applied to frag- ments of the CPCS network (see box 5.D): standard variable elimination (with table representation of factors), the two decompositions illustrated in ﬁgure 9.14 for the case of noisy-or, and a special- purpose elimination algorithm that uses a heterogeneous factorization. The results show that in a network such as CPCS, which uses predominantly noisy-or and noisy-max CPDs, signiﬁcant gains in performance can be obtained. They results also showed that the two decomposition schemes (tree-based and chain-based) are largely equivalent in their performance, and the heterogeneous factorization outperforms both of them, due to its greater ﬂexibility in dynamically determining the elimination ordering during the course of the algorithm. 

For rule-based variable elimination, no large networks with extensive rule-based structure had been constructed. So, Poole and Zhang (2003) used a standard benchmark network, with 32 variables and 11,018 entries. Entries that were within 0 . 05 of each other were collaped, to construct a more compact rule-based representation, with a total of 5,834 distinct entries. As expected, there are a large number of cases where the use of rule-based inference provided signiﬁcant savings. However, there were also many cases where contextual independence does not provide signiﬁcant help, in which case the increased overhead of the rule-based inference dominates, and standard VE performs better. 

At a high level, the main conclusion is that table-based approaches are amenable to numerous optimizations, such as those described in box 10.A, which can improve the performance by an order of magnitude or even more. Such optimizations are harder to deﬁne for more complex data structures. Thus, it is only useful to consider algorithms that exploit local structure either when it is extensively present in the model, or when it has speciﬁc structure that can, itself, be exploited using specialized algorithms. 

# 9.7 Summary and Discussion 

In this chapter, we described the basic algorithms for exact inference in graphical models. As we saw, probability queries essentially require that we sum out an exponentially large joint distribution. The fundamental idea that allows us to avoid the exponential blowup in this task is the use of dynamic programming, where we perform the summation of the joint distribution from the inside out rather than from the outside in, and cache the intermediate results, thereby avoiding repeated computation. 

We presented an algorithm based on this insight, called variable elimination. The algorithm works using two fundamental operations over factors — multiplying factors and summing out variables in factors. We analyzed the computational complexity of this algorithm using the structural properties of the graph, showing that the key computational metric was the induced width of the graph. 

We also presented another algorithm, called conditioning, which performs some of the sum- mation operations from the outside in rather than from the inside out, and then uses variable elimination for the rest of the computation. Although the conditioning algorithm is never less expensive than variable elimination in terms of running time, it requires less storage space and hence provides a time-space trade-of for variable elimination. 

We showed that both variable elimination and conditioning can take advantage of local structure within the CPDs. Speciﬁcally, we presented methods for making use of CPDs with independence of causal inﬂuence, and of CPDs with context-speciﬁc independence. In both cases, techniques tend to fall into two categories: In one class of methods, we modify the network structure, adding auxiliary variables that reveal some of the structure inside the CPD and break up large factors. In the other, we modify the variable elimination algorithm directly to use structured factors rather than tables. 

Although exact inference is tractable for surprisingly many real-world graphical models, it is still limited by its worst-case exponential performance. There are many models that are simply too complex for exact inference. As one example, consider the $n\times n$ grid-structured pairwise Markov networks of box 4.A. It is not difcult to show that the minimal tree-width of this network is $n$ . Because these networks are often used to model pixels in an image, where $n\,=\,1,000$ is quite common, it is clear that exact inference is intractable for such networks. Another example is the family of networks that we obtain from the template model of example 6.11. Here, the moralized network, given the evidence, is a fully connected bipartite graph; if we have $n$ variables on one side and $m$ on the other, the minimal tree-width is $\operatorname*{min}(n,m)$ , which can be very large for many practical models. Although this example is obviously a toy domain, examples of similar structure arise often in practice. In later chapters, we will see many other examples where exact inference fails to scale up. Therefore, in chapter 11 and chapter 12 we discuss approximate inference methods that trade of the accuracy of the results for the ability to scale up to much larger models. 

One class of networks that poses great challenges to inference is the class of networks induced by template-based representations. These languages allow us to specify (or learn) very small, compact models, yet use them to construct arbitrarily large, and often densely connected, networks. Chapter 15 discusses some of the techniques that have been used to deal with dynamic Bayesian networks. 

Our focus in this chapter has been on inference in networks involving only discrete variables. The introduction of continuous variables into the network also adds a signiﬁcant challenge. Although the ideas that we described here are instrumental in constructing algorithms for this richer class of models, many additional ideas are required. We discuss the problems and the solutions in chapter 14. 

# 9.8 Relevant Literature 

The ﬁrst formal analysis of the computational complexity of probabilistic inference in Bayesian networks is due to Cooper (1990). 

peeling forward-backward algorithm 

nonserial dynamic programming 

Variants of the variable elimination algorithm were invented independently in multiple com- munities. One early variant is the peeling algorithm of Cannings et al. (1976, 1978), formulated for the analysis of genetic pedigrees. Another early variant is the forward-backward algorithm , which performs inference in hidden Markov models (Rabiner and Juang 1986). An even earlier variant of this algorithm was proposed as early as 1880, in the context of continuous models (Thiele 1880). Interestingly, the ﬁrst variable elimination algorithm for fully general models was invented as early as 1972 by Bertelé and Brioschi (1972), under the name nonserial dynamic programming . However, they did not present the algorithm in the setting of probabilistic inference in graph- structured models, and therefore it was many years before the connection to their work was recognized. Other early work with similar ideas but a very diferent application was done in the database community (Beeri et al. 1983). 

The general problem of probabilistic inference in graphical models was ﬁrst tackled by Kim and Pearl (1983), who proposed a local message passing algorithm in polytree-structured Bayesian networks. These ideas motivated the development of a wide variety of more general algorithms. One such trajectory includes the clique tree methods that we discuss at length in the next chapter (see also section 10.6). A second includes a specrum of other methods (for example, Shachter 1988; Shachter et al. 1990), culminating in the variable elimination algorithm, as presented here, ﬁrst described by Zhang and Poole (1994) and subsequently by Dechter (1999). Huang and Darwiche (1996) provide some useful tips on an efcient implementation of algorithms of this type. 

Dechter (1999) presents interesting connections between these algorithms and constraint- satisfaction algorithms, connections that have led to fruitful work in both communities. Other generalizations of the algorithm to settings other than pure probabilistic inference were described by Shenoy and Shafer (1990); Shafer and Shenoy (1990) and by Dawid (1992). The construction of the network polynomial was proposed by Darwiche (2003). 

The complexity analysis of the variable elimination algorithm is described by Bertelé and Brioschi (1972); Dechter (1999). The analysis is based on core concepts in graph theory that have been the subject of extensive theoretical analysis; see Golumbic (1980); Tarjan and Yannakakis (1984); Arnborg (1985) for an introduction to some of the key concepts and algorithms. 

Much work has been done on the problem of ﬁnding low-tree-width triangulations or (equiv- alently) elimination orderings. One of the earliest algorithms is the maximum cardinality search of Tarjan and Yannakakis (1984). Arnborg, Corneil, and Proskurowski (1987) show that the prob- lem of ﬁnding the minimal tree-width elimination ordering is $\mathcal{N P}$ -hard. Shoikhet and Geiger (1997) describe a relatively efcient algorithm for ﬁnding this optimal elimination ordering — one whose cost is approximately the same as the cost of inference with the resulting ordering. Becker and Geiger (2001) present an algorithm that ﬁnds a close-to-optimal ordering. Neverthe- less, most implementations use one of the standard heuristics. A good survey of these heuristic methods is presented by Kjærulf (1990), who also provides an extensive empirical comparison. Fishelson and Geiger (2003) suggest the use of stochastic search as a heuristic and provide another set of comprehensive experimental comparisons, focusing on the problem of genetic linkage analysis. Bodlaender, Koster, van den Eijkhof, and van der Gaag (2001) provide a series of simple preprocessing steps that can greatly reduce the cost of triangulation. 

The ﬁrst incarnation of the conditioning algorithm was presented by Pearl (1986a), in the context of cutset conditioning, where the conditioning variables cut all loops in the network, forming a polytree. Becker and Geiger (1994); Becker, Bar-Yehuda, and Geiger (1999) present a va- riety of algorithms for ﬁnding a small loop cutset. The general algorithm, under the name global conditioning , was presented by Shachter et al. (1994). They also demonstrated the equivalence of conditioning and variable elimination (or rather, the clique tree algorithm) in terms of the under- lying computations, and pointed out the time-space trade-ofs between these two approaches. These time-space trade-ofs were then placed in a comprehensive computational framework in the recursive conditioning method of Darwiche (2001b); Allen and Darwiche (2003a,b). Cutset algorithms have made a signiﬁcant impact on the application of genetic linkage analysis Schäfer (1996); Becker et al. (1998), which is particularly well suited to this type of method. 

The two noisy-or decomposition methods were described by Olesen, Kjærulf, Jensen, Falck, Andreassen, and Andersen (1989) and Heckerman and Breese (1996). An alternative approach that utilizes a heterogeneous factorization was described by Zhang and Poole (1996); this approach is more ﬂexible, but requires the use of a special-purpose inference algorithm. For the case of CPDs with context-speciﬁc independence, the decomposition approach was proposed by Boutilier, Friedman, Goldszmidt, and Koller (1996). The rule-based variable elimination algorithm was proposed by Poole and Zhang (2003). The trade-ofs here are similar to the case of the noisy-or methods. 

# 9.9 Exercises 

Exercise $\mathbf{9.1}\star$ 

Prove theorem 9.2. 

Exercise ${\bf9.2\star}$ 

Consider a factor produced as a product of some of the CPDs in a Bayesian network $\mathcal{B}$ : 

$$
\tau(W)=\prod_{i=1}^{k}P(Y_{i}\mid\mathrm{Pa}_{Y_{i}})
$$ 

where $W=\cup_{i=1}^{k}\big(\{Y_{i}\}\cup\mathrm{Pa}_{Y_{i}}\big)$ { } ∪ . 

a. Show that $\tau$ is a conditional probability in some network. More precisely, construct another Bayesian network $\mathcal{B}^{\prime}$ and a disjoint partition $W=Y\cup Z$ such that $\tau(\pmb{W})=\dot{P_{\mathcal{B}^{\prime}}}(Y\mid Z)$ . 

b. Conclude that all of the intermediate factors produced by the variable elimination algorithm are also conditional probabilities in some network. 

# Exercise 9.3 

Consider a modiﬁed variable elimination algorithm that is allowed to multiply all of the entries in a single factor by some arbitrary constant. (For example, it may choose to renormalize a factor to sum to 1.) If we run this algorithm on the factors resulting from a Bayesian network with evidence, which types of queries can we still obtain the right answer to, and which not? 

# Exercise $9.4\star$ 

This exercise shows basic properties of the network polynomial and its derivatives: 

a. Prove equation (9.8). b. Prove equation (9.9). 

evidence retraction c. Let $\pmb{Y}\,=\,\pmb{y}$ e assignment. For $Y_{i}\,\in\,Y$ , we now consider hat happens if we retract e observation $Y_{i}\,=\,y_{i}$ . More precisely, let ${\pmb{y}}_{-i}$ be the assignment in y to all variables other than Y $Y_{i}$ . − Show that 

$$
\begin{array}{r c l}{{P(\pmb{y}_{-i},Y_{i}=y_{i}^{\prime}\mid\theta)}}&{{=}}&{{\displaystyle\frac{\partial f_{\Phi}(\pmb{\theta},\pmb{\lambda}^{y})}{\lambda_{y_{i}^{\prime}}}}}\\ {{P(\pmb{y}_{-i}\mid\theta)}}&{{=}}&{{\displaystyle\sum_{y_{i}^{\prime}}\frac{\partial f_{\Phi}(\pmb{\theta},\pmb{\lambda}^{y})}{\lambda_{y_{i}^{\prime}}}.}}\end{array}
$$ 

# Exercise $9.5\star$ 

sensitivity analysis 

In this exercise, you will show how you can use the gradient of the probability of a Bayesian network to perform sensitivity analysis , that is, to compute the efect on a probability query of changing the parameters in a sing PD $P(X\mid U)$ . More precisely, let $\theta$ be one set of parameters for a $\mathcal{G}$ , wh we have that $\theta_{x\mid u}$ | is the parameter associated with the conditional probability entry $P(X\mid U)$ | . Let $\theta^{\prime}$ be another parameter assignment that is the same except that we replace the parameters $\theta_{x\mid u}$ with $\theta_{x\mid u}^{\prime}=\theta_{x\mid u}+\Delta_{x\mid u}$ . 

For an assignment $e$ (which may or may not involve variables in $X,U$ , compute the change $P(e:$ $\pmb{\theta})-P(e:\mathbf{\check{\theta}}^{\prime})$ in terms of $\Delta_{x\mid u}$ , and the network derivatives. 

# Exercise ${\bf9.6\star}$ 

Consider some run of variable elimination over the factors $\Phi$ , where all variables are eliminated. This run generates some set of intermediate factors $\tau_{i}(W_{i})$ . We can deﬁne a set of intermediate (arithmetic, not random) variables $v_{i k}$ corresponding to the diferent entries $\tau_{i}(w_{i}^{k})$ . 

a. Show how, for each variable $v_{i j}$ , we can write down an algebraic expression that deﬁnes $v_{i j}$ in terms of: the parameters $\lambda_{x_{i}}$ ; the parameters $\theta_{x_{c}}$ ; and variables $v_{j l}$ for $j<i$ . b. Use your answer to the previous part to deﬁne an alternative representation whose complexity is linear in the total size of the intermediate factors in the VE run. c. Show how the same representation can be used to compute all of the derivatives of the network polynomial; the complexity of your algorithm should be linear in the compact representation of the network polynomial that you derived in the previous part. (Hint: Consider the partial derivatives of the network polynomial relative to each $v_{i j}$ , and use the chain rule for derivatives.) 

# Exercise 9.7 

Prove proposition 9.1. 

# Exercise ${\bf9.8\star}$ 

Prove theorem 9.10, by showing that any ordering produced by the maximum cardinality search algorithm eliminates cliques one by one, starting from the leaves of the clique tree. 

# Exercise 9.9 

a. Show that variable elimination on polytrees can be performed in linear time, assuming that the local probability models are represented as full tables. Speciﬁcally, for any polytree, describe an elimination ordering, and show that the complexity of variable elimination with your ordering is linear in the size of the network. Note that the linear time bound here is in terms of the size of the CPTs in the network, so that the cost of the algorithm grows exponentially with the number of parents of a node. b. Extend your result from (1) to apply to cases where the CPDs satisfy independence of causal inﬂuence. Note that, in this case, the network representation is linear in the number of variables in the network, and the algorithm should be linear in that number. c. Now extend your result from (1) to apply to cases where the CPDs are tree-structured. In this case, the network representation is the sum of the sizes of the trees in the individual CPDs, and the algorithm should be linear in that number. 

# Exercise ${\bf9.10\star}$ 

Consider the four criteria described in connection with Greedy-Ordering of algorithm 9.4: Min-Neighbors, Min-Weight, Min-Fill, and Weighted-Min-Fill. Show that none of these criteria dominate the others; that is, for any pair, there is always a graph where the ordering produced by one of them is better than that produced by the other. As our measure of performance, use the computational cost of full variable elimination (that is, for computing the partition function). For each counterexample, deﬁne the structure of the graph and the cardinality of the variables, and show the ordering produced by each member of the pair. 

# Exercise $\mathbf{9.11}\star$ 

Let $\mathcal{H}$ be an undirected graph, and $\prec$ an Pro that $X{-}Y$ ll edg for all induce i $i\stackrel{.}{=}1,\ldots,k$ nd only if there is a path . $X{\mathrm{-}}Z_{1}{\mathrm{-}}\ldots Z_{k}{\mathrm{-}}Y$ in H such that $Z_{i}\prec X$ ≺ and $Z_{i}\prec Y$ ≺ 

# Exercise $\mathbf{9.12\star}$ 

Prove theorem 9.12. 

# Exercise ${\bf9.13\star}$ 

Prove theorem 9.13. 

# Exercise $9.14\star$ 

The standard conditioning algorithm ﬁrst conditions the network on the conditioning variables $U$ , splitting the computation into a set of computations, one for every instantiation $\mathbfit{u}$ to $U$ ; it then performs variable elimination on the remaining network. As we discussed in section 9.5.4.1, we can generalize conditioning so that it alternates conditioning steps and elimination in an arbitrary way. In this question, you will formulate such an algorithm and provide a graph-theoretic analysis of its complexity. 

et $\Phi$ be a set of factors over $\mathcal{X}$ , and let $_{X}$ be a set of non riab e a summ cedure $\sigma$ that each to be a X $X\,^{-}\!\in\,X$ ∈ e of operations, each of w appears in the sequence σ precisely once. The semantics of this procedure is that, h is either $\bar{e l i m}(X)$ or $c o n d(X)$ for some $X\in X$ ∈ , such going from left to right, we perform the operation described on the variables in sequence. For example, the summation procedure of example 9.5 would be written as: 

$$
e l i m(A_{k-1}),e l i m(A_{k-2}),.\,.\,.\,.\,e l i m(A_{1}),c o n d(A_{k}),e l i m(C),e l i m(B).
$$ 

a. Deﬁne an algorithm that takes a summation sequence as input and performs the operations in the order stated. Provide precise pseudo-code for the algorithm. b. Deﬁne the notion of an induced graph for this algorithm, and deﬁne the time and space complexity of the algorithm in terms of the induced graph. 

# Exercise $\mathbf{9.15\star}$ 

In section 9.6.1.1, we described an approach to decomposing noisy-or CPDs, aimed at reducing the cost of variable elimination. In this exercise, we derive a construction for CPD-trees in a similar spirit. 

a. Consider a variable $Y$ that has a binary-valued parent $A$ and four additional parents $X_{1},\dots,X_{4}$ . Assume that the CPD of $Y$ is structured as a tree whose ﬁrst split is $A$ , and where $Y$ depends only on $X_{1},X_{2}$ in the $A=a^{1}$ branch, and only on $X_{3},X_{4}$ in the $\ensuremath{\boldsymbol{A}}^{\textup{\scriptsize{\bar{\ }}}}=\ensuremath{\boldsymbol{a}}^{0}$ branch. Deﬁne two new variables, $Y_{a^{1}}$ and $Y_{a^{0}}$ , which represent the value that $Y$ would take if $A$ were to have the value $a^{1}$ , and the value that $Y$ would take if $A$ were to have the value $a^{0}$ . Deﬁne a new model for $Y$ that is deﬁned in terms of these new variables. Your model should precisely specify the CPDs for $Y_{a^{1}}$ , $Y_{a^{0}}$ , and $Y$ in terms of $Y\mathbf{\dot{s}}$ original CPD. 

b. Deﬁne a general procedure that recursively decomposes a tree-CPD using the same principles. 

# Exercise 9.16 

In this exercise, we show that rule-based variable elimination performs exactly the same operations as table-based variable elimination, when applied to rules generated from table-CPDs. Consider two table fac $\phi(X),\phi^{\prime}(Y)$ . Let $\mathcal{R}$ be the set of constituent rules for $\phi(X)$ and $\mathcal{R}^{\prime}$ the set of constituent rules for $\phi(Y)$ . 

a. Show that the operation of multiplying $\boldsymbol{\phi}\cdot\boldsymbol{\phi}^{\prime}$ can be implemented as a series of rule splits on $\mathcal{R}\cup\mathcal{R}^{\prime}$ , followed by a series of rule products. b. ow that the operation of summing out $Y\in X$ in $\phi$ can be implemented as a series of rule sums in . 

# Exercise $9.17\star$ 

Prove that each step in the algorithm of algorithm 9.7 maintains the program-correctness invariant de- scribed in the text: Let $\mathcal{R}$ be the current set of rules maintained by the algorithm, and $W$ be the variables that have not yet been eliminated. The invariant is that: 

ility of a context $^c$ such that $S c o p e[c]\subseteq W$ can be obtained by multiplying all rules $\langle\pmb{c}^{\prime};\hat{p}\rangle\in\mathcal{R}$ whose context is compatible with c . 

# Exercise ${\bf9.18\star\star}$ 

Consider an alternative factorization of a Bayesian network where each factor is a hybrid between a rule and a table, called a confactor . Like a rule, a confactor associated with a context $^{c;}$ however, rather than a single number, each confactor contains not a single number, but a standard table-based factor. For example, the CPD of ﬁgure 5.4a would have a confactor, associated with the middle branch, whose context is $a^{1},{\bar{s}}^{0}$ , and whose associated table is 

$$
\begin{array}{l l}{{l^{0},j^{0}}}&{{0.9}}\\ {{l^{0},j^{1}}}&{{0.1}}\\ {{l^{1},j^{0}}}&{{0.4}}\\ {{l^{1},j^{1}}}&{{0.6}}\end{array}
$$ 

Extend the rule splitting algorithm of algorithm 9.6 and the rule-based variable elimination algorithm of algorithm 9.7 to operate on confactors rather than rules. Your algorithm should use the efcient table-based data structures and operations when possible, resorting to the explicit partition of tables into rules only when absolutely necessary. 

# Exercise ${\bf9.19\star\star}$ 

We have shown that the sum-product variable elimination algorithm is sound, in that it returns the same answer as ﬁrst multiplying all the factors, and then summing out the nonquery variables. Exercise 13.3 asks for a similar argument for max-product. One can prove similar results for other pairs of operations, such as max-sum. Rather than prove the same result for each pair of operations we encounter, we now provide a generalized variable elimination algorithm from which these special cases, as well as others, follow directly. This general algorithm is based on the following result, which is stated in terms of a pair of abstract operators: generalized combination of two factors, denoted $\phi_{1}\otimes\phi_{2}$ N φ ; and generalized marginaliz on of a factor $\phi$ over a subset $W$ , denoted $\Lambda_{W}(\phi)$ . We deﬁne our generalized variable elimination algorithm in direct analogy to the sum-product a rithm of algorithm 9.1, replacing factor product with N and summation for variable elimination with Λ . 

We now show that if these two operators satisfy certain conditions, the variable elimination algorithm for these two operations is sound: 

Commutativity of combination: For any factors $\phi_{1},\phi_{2}$ : 

$$
\phi_{1}\bigotimes\phi_{2}=\phi_{2}\bigotimes\phi_{1}.
$$ 

Associativity of combination: For any factors $\phi_{1},\phi_{2},\phi_{3}$ : 

$$
\phi_{1}\bigotimes(\phi_{2}\bigotimes\phi_{3})=(\phi_{1}\bigotimes\phi_{2})\bigotimes\phi_{3}.
$$ 

Consonance of marginalization: If $\phi$ is a factor of scope $W$ , and $Y,Z$ are disjoint subsets of $W$ , then: 

$$
\Lambda_{Y}(\Lambda_{Z}(\phi))=\Lambda_{(Y\cup Z)}(\phi).
$$ 

Marginalization over combination: If $\phi_{1}$ is a factor of scope $W$ and $Y\cap W=\emptyset$ , then: 

$$
\Lambda_{Y}(\phi_{1}\bigotimes\phi_{2})=\phi_{1}\bigotimes\Lambda_{Y}(\phi_{2}).
$$ 

Show that if $\otimes$ and $\Lambda$ satisfy the preceding axioms, t n we obtain a theorem analogous to th rem 9.5. That is, the algorithm, when applied to a set of factors Φ and a set of variables to be eliminated Z , returns a factor 

$$
\phi^{*}(Y)=\Lambda z(\bigotimes_{\phi\in\Phi}\phi).
$$ 

# Exercise ${\bf9.20\star\star}$ 

You are taking the ﬁnal exam for a course on computational complexity theory. Being somewhat too theoretical, your professor has insidiously sneaked in some unsolvable problems and has told you that exactly $K$ of the $N$ problems have a solution. Out of generosity, the professor has also given you a probability distribution over the solvability of the $N$ problems. 

To f malize the scenario, let $\mathcal{X}=\{X_{1},.\,.\,.\,,X_{N}\}$ variables correspo ing to the N questions in the exam where $V a l(X_{i})\,=\,\bar{\mathrm{\left\{0(unsolveeable),1(solveeable)\right\}}}$ { } . Fur ermore, let B be a Bayesian network parameterizing a probability d strib ion over X (that is, problem i may be easily used to solve problem $j$ so that the probabilities that i and $j$ are solvable are not independent in general). 

a. We begin by describing a method for computing the probability of a question being solvable. That is we want to compute $\breve{P}(X_{i}=1,\mathrm{PSbubble}(\hat{\mathcal{X}})=\breve{K})$ ) where 

$$
\operatorname{PSIM}({\mathcal{X}})=\sum_{i}\mathbf{1}\{X_{i}=1\}
$$ 

is the number of solvable problems assigned by the professor. 

To this end, we deﬁne an extended factor $\phi$ as a “regular” factor $\psi$ and an index so that it deﬁnes a ction $\phi(X,L):V a l(X)\times\{0,\dot{.}\,.\,,N\}\mapsto I\!\!R$ where $X=S c o p e[\phi]$ . A projection of such a factor $[\phi]_{l}$ l is a regular factor $\psi:V a l(X)\mapsto I\!\!R,$ 7→ , such that $\psi(X)=\phi(X,{\bar{l}})$ . 

Provide a deﬁnition of factor combination and factor marginalization for these extended factors such that 

$$
P(X_{i},\mathrm{PSimize}(\mathcal{X})=K)=\left[\sum_{\mathcal{X}-\{X_{i}\}}\prod_{\phi\in\Phi}\phi\right]_{K},
$$ 

where each $\phi\in\Phi$ is an extended factor corresponding to some CPD of the Bayesian network, deﬁned as follows: 

$$
\begin{array}{r}{\phi_{X_{i}}\big(\{X_{i}\}\cup\mathbf{Pa}_{X_{i}},k\big)=\left\{\begin{array}{l l}{P(X_{i}\mid\mathrm{Pa}_{X_{i}})}&{\mathrm{if~}X_{i}=k}\\ {0}&{\mathrm{otherwise}}\end{array}\right.}\end{array}
$$ 

b. Show that your operations satisfy the condition of exercise 9.19 so that you can compute equation (9.16) use the generalized variable elimination algorithm. 

c. Realistically, you will have time to work on exactly $M$ problems $1\leq M\leq N)$ . Obviously, your goal is to maximize the expected number of solvable problems that you attempt. (Luckily for you, every solvable problem that you attempt you will solve correctly, and you neither gain nor lose credit for working on an unsolvable problem.) Let $\mathbf{Y}$ be a subset of $\mathcal{X}$ indicating exactly $M$ problems you choose to work on, and let 

$$
\operatorname{Correct}(X,Y)=\sum_{X_{i}\in Y}X_{i}
$$ 

be the number of solvable problems that you attempt. The expected number of problems you solve is 

$$
\pmb{\mathscr{E}}_{P_{\mathcal{B}}}[\mathrm{Correct}(\mathcal{X},Y)\mid\mathrm{PSbubble}(\mathcal{X})=K].
$$ 

Using your generalized variable elimination algorithm, provide an efcient algorithm for computing this expectation. 

d. Your goal is to ﬁnd $\mathbf{Y}$ that optimizes equation (9.17). Provide a simple example showing that: 

$$
\arg\operatorname*{max}_{Y:\vert Y\vert=M}E_{P_{B}}[\mathrm{Correct}(\mathcal{X},Y)]\neq\arg\operatorname*{max}_{Y:\vert Y\vert=M}E_{P_{B}}[\mathrm{Correct}(\mathcal{X},Y)\ \vert\ \mathrm{possible}(\mathcal{X})=K
$$ 

e. Give an efcient algorithm for ﬁnding 

$$
\arg\operatorname*{max}_{Y:\lvert Y\rvert=M}E_{P_{\mathcal{B}}}\bigl[\mathrm{Correct}(\mathcal{X},Y)\ \vert\ \mathrm{PSbubble}(\mathcal{X})=K\bigr].
$$ 

(Hint: Use linearity of expectations.) 

# 10 Exact Inference: Clique Trees 

In the previous chapter, we showed how we can exploit the structure of a graphical model to perform exact inference efectively. The fundamental insight in this process is that the factorization of the distribution allows us to perform local operations on the factors deﬁning the distribution, rather than simply generate the entire joint distribution. We implemented this insight in the context of the variable elimination algorithm, which sums out variables one at a time, multiplying the factors necessary for that operation. 

In this chapter, we present an alternative implementation of the same insight. As in the case of variable elimination, the algorithm uses manipulation of factors as its basic computational step. However, the algorithm uses a more global data structure for scheduling these operations, with surprising computational beneﬁts. 

Throughout this chapter, we will assume that we are dealing with a set of factors $\Phi$ over a set of variables $\mathcal{X}$ , where each factor $\phi_{i}$ has a scope $X_{i}$ . This set of factors deﬁnes a (usually) unnormalized measure 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\prod_{\phi_{i}\in\Phi}\phi_{i}(X_{i}).
$$ 

For a Bayesian network without evidence, the factors are simply the CPDs, and the measure $\tilde{P}_{\Phi}$ is a normalized distrib ion. rk $\mathcal{B}$ with evidence $E=e$ , the factors are the CPDs restricted to e , and $\tilde{P}_{\Phi}(\mathcal{X})\overset{\cdot}{=}P_{\mathcal{B}}(\mathcal{X},e)$ X X . Fo Gibbs distribution (with or without B evidence), the factors are the (restricted) potentials, and $\tilde{P}_{\Phi}$ is the unnormalized Gibbs measure. 

It is important to note that all of the operations that one can perform on a normalized distri- bution can also be performed on an unnormalized measure. In particular, we can marginalize $\tilde{P}_{\Phi}$ on a subset of the variables by summing out the others. We can also consider a conditional measure, ${\tilde{P}}_{\Phi}(X\mid Y)={\tilde{P}}_{\Phi}(X,{\dot{Y}})/{\tilde{P}}_{\Phi}(Y)$ | (which, in fact, is the same as $P_{\Phi}(X\mid Y))$ ). 

# 10.1 Variable Elimination and Clique Trees 

Recall that the basic operation of the variable elimination algorithm is the manipulation of factors. Each step in the computation creates a factor $\psi_{i}$ by multiplying existing factors. A variable is then eliminated in $\psi_{i}$ to generate a new factor $\tau_{i}$ , which is then used to create another factor. In this section, we present another view of this computation. We consider a factor $\psi_{i}$ to be a computational data structure, which takes “messages” $\tau_{j}$ generated by other factors $\psi_{j}$ , and generates a message $\tau_{i}$ that is used by another factor $\psi_{l}$ . 

![](images/4b991a122494f6b8563e766af53c9629c730c3921434241fb8c764c75572646d.jpg) 
Figure 10.1 Cluster tree for the VE execution in table 9.1 

# 10.1.1 Cluster Graphs 

We begin by deﬁning a cluster graph — a data structure that provides a graphical ﬂowchart of the factor-manipulation process. Each node in the cluster graph is a cluster , which is associated with a subset of variables; the graph contains undirected edges that connect clusters whose scopes have some non-empty intersection. We note that this deﬁnition is more general than the data structures we use in this chapter, but this generality will be important in the next chapter, where we signiﬁcantly extend the algorithms of this chapter. 

Deﬁnition 10.1 cluster graph family preservation sepset 

# Example 10.1 

A cluster graph $\mathcal{U}$ for a ctors $\Phi$ over $\mathcal{X}$ is an undirected graph, each of whose no $i$ associated with a subset $C_{i}\subseteq\mathcal X$ ⊆X cluster gra ust be fam — each factor $\phi\in\Phi$ ∈ must be associate ith a cluster $C_{i:}$ , denoted $\alpha(\phi)$ , such that Scope $S c o p e[\phi]\subseteq C_{i}$ ⊆ . Each edge between a pair of clusters $C_{i}$ and $C_{j}$ is associated with $a$ sepset $S_{i,j}\subseteq C_{i}\cap C_{j}$ . 

An execution of variable elimination deﬁnes a cluster graph: We have a cluster for each factor $\psi_{i}$ used in the computation, which is associated with the set of variables $C_{i}=S c o p e[\psi_{i}]$ . We draw an edge between two clusters $C_{i}$ and $C_{j}$ if the message $\tau_{i}$ , produced by eliminating a variable in $\psi_{i}$ , is used in the computation of $\tau_{j}$ . 

Consider the elimination process of table 9.1. In this case, we have seven factors $\psi_{1},.\,.\,.\,,\psi_{7}$ , whose scope is shown in the table. The message $\tau_{1}(D)$ , generated from $\psi_{1}(C,D)$ , participates in the computation of $\psi_{2}$ . Thus, we would have an edge from $C_{1}$ to $C_{2}$ . Similarly, the message $\tau_{3}(G,S)$ is generated from $\psi_{3}$ and used in the computation of $\psi_{5}$ . Hence, we introduce an edge between $C_{3}$ and $C_{5}$ . The entire graph is shown in ﬁgure 10.1. The edges in the graph are annotated with directions, indicating the ﬂow of messages between clusters in the execution of the variable elimination algorithm. Each of the factors in the initial set of factors $\Phi$ is also associated with $^a$ clust $C_{i}$ . For example, the cluster $\phi_{D}(D,C)$ (corresponding to the CPD $P(D\mid C),$ ) is associated with $C_{1}$ , and the cluster $\phi_{H}(H,G,J)$ (corresponding to the CPD $P(H\mid G,J))$ is associated with $C_{4}$ . 

# 10.1.2 Clique Trees 

The cluster graph associated with an execution of variable elimination is guaranteed to have certain properties that turn out to be very important. 

First, recall that the variable elimination algorithm uses each intermediate factor $\tau_{i}$ at most once: when $\phi_{i}$ is used in Sum-Product-Eliminate-Var to create $\psi_{j}$ , it is removed from the set of factors $\Phi$ , and thus cannot be used again. Hence, the cluster graph induced by an execution of variable elimination is necessarily a tree. 

upstream clique downstream clique 

Deﬁnition 10.2 running intersection property 

We note that although a cluster graph is deﬁned to be an undirected graph, an execution of variable elimination does deﬁne a direction for the edges, as induced by the ﬂow of messages between the clusters. The directed graph induced by the messages is a directed tree, with all the messages ﬂowing toward a single cluster where the ﬁnal result is computed. This cluster is called the root of the directed tree. Using standard conventions in computer science, we assume that the root of the tree is “up,” so that messages sent toward the root are sent upward. If $C_{i}$ is on the path from $C_{j}$ to the root we say that $C_{i}$ is upstream from $C_{j}$ , and $C_{j}$ is downstream from $C_{i}$ . We note that, for reasons that will become clear later on, the directions of the edges and the root are not part of the deﬁnition of a cluster graph. The cluster tree deﬁned by variable elimination satisﬁes an important structural constraint: 

Let $\mathcal{T}$ be a cluster ee over a set of factors $\Phi$ . We denote by $\nu_{\tau}$ the vertices of $\mathcal{T}$ and b $\mathcal{E}_{\mathcal{T}}$ its edge that T $\mathcal{T}$ has the runn g intersection property $i f,$ whenever there is a v iable X s that $X\in C_{i}$ ∈ and $X\in C_{j}$ , then X is also in every cluster in the (unique) path in T $\mathcal{T}$ between $C_{i}$ and $C_{j}$ . 

Note that the running intersection property implies that $S_{i,j}=C_{i}\cap C_{j}$ . 

# Example 10.2 

We can easily check that the running intersection property holds for the cluster tree of ﬁgure 10.1. For example, $G$ is present in $C_{2}$ and in $C_{4}$ , so it is also present in the cliques on the path between them: $C_{3}$ and $C_{5}$ . 

Intuitively, the running intersection property must hold for cluster trees induced by variable elimination because a variable appears in every factor from the moment it is introduced (by multiplying in a factor that mentions it) until it is summed out. We now prove that this property holds in general. 

# Theorem 10.1 

t $\mathcal{T}$ be a cluster tree induced by a variable elimination algorithm over some set of factors $\Phi$ . Then $\mathcal{T}$ T satisﬁes the running intersection property. Proof Let $C$ and $C^{\prime}$ be two clusters that contain $X$ . Let $C_{X}$ be the cluster where $X$ is eliminated. (If $X$ is a query variable, we assume that it is eliminated in the last cluster.) We will prove that $X$ must be present in every cluster on the path between $C$ and $C_{X}$ , and analogously for $C^{\prime}$ , thereby proving the result. First, we observe that the computation at $C_{X}$ must take place later in the algorithm’s execu- tion than the computation at $C$ : When $X$ is eliminated in $C_{X}$ , all of the factors involving $X$ are multiplied into $C_{X}$ ; the result of the summation does not have $X$ in its domain. Hence, after this elimination, $\Phi$ no longer has any factors containing $X$ , so no factor generated afterward will contain $X$ in its domain. By assumption, $X$ is in the domain of the factor in $C$ . We also know that $X$ is not eliminated in $C$ . Therefore, the message computed in $C$ must have $X$ in its domain. By deﬁnition, the from $C$ . Hence, it will also have $X$ in its scope. The same argument applies to show that all cliques upstream from $C$ will have $X$ in their scope, until $X$ is eliminated, which happens only in $C_{X}$ . Thus, $X$ must appear in all cliques between $C$ and $C_{X}$ , as required. 

A very similar proof can be used to show the following result: 

Proposition 10.1 

$\mathcal{T}$ be a cluster tree induced by $^a$ variable elimination algorithm over some set of factors $\Phi$ . Let $C_{i}$ and $C_{j}$ be two neighboring clusters, such that $C_{i}$ passes the message $\tau_{i}$ to $C_{j}$ . Then the scope of the message $\tau_{i}$ is precisely $C_{i}\cap C_{j}$ . 

The proof is left as an exercise (exercise 10.1). 

It turns out that a cluster tree that satisﬁes the running intersection property is an extremely useful data structure for exact inference in graphical models. We therefore deﬁne: 

Deﬁnition 10.3 clique tree clique 

Let $\Phi$ be a set of factors over $\mathcal{X}$ . A cluster tree ver $\Phi$ that satisﬁ the running intersection property is called a clique tree (sometimes also called a junction tree or a join tree ). In the case of a clique tree, the clusters are also called cliques . 

Note that we have already deﬁned one notion of a clique tree in deﬁnition 4.17. This double deﬁnition is not an overload of terminology, because the two deﬁnitions are actually equivalent: It follows from the results of this chapter that $\mathcal{T}$ is a clique tree for $\Phi$ (in sense of deﬁni- tion 10.3) if and only if it is a clique tree for a chordal graph containing H ${\mathcal{H}}_{\Phi}$ (in the sense of deﬁnition 4.17), and these properties are true if and only if the clique-tree data structure admits variable elimination by passing messages over the tree. 

We ﬁrst show that the running intersection property implies the independence statement, which i the heart of our ﬁrst deﬁnition of clique trees. Let $\mathcal{T}$ be a cluster tree over $\Phi$ , and let H ${\mathcal{H}}_{\Phi}$ be the undirected graph associated with this set of factors. For any sepset $\boldsymbol{S}_{i,j}$ , let $W_{<(i,j)}$ be the set of all variables in the scope of clusters in the $C_{i}$ side of the tree, and $W_{<(j,i)}$ be the set of all variables in the scope of clusters in the $C_{j}$ side of the tree. 

# Theorem 10.2 

$\mathcal{T}$ satisﬁes the running intersection property if and only if, for every sepset $\boldsymbol{S}_{i,j}$ , we have that $W_{<(i,j)}$ and $W_{<(j,i)}$ are separated in ${\mathcal{H}}_{\Phi}$ given $\boldsymbol{S}_{i,j}$ . 

The proof is left as an exercise (exercise 10.2). 

To conclude the proof of the equivalence of the two deﬁnitions, it remains only to show that the running intersection operty for a ee $\mathcal{T}$ implies that each node in $\mathcal{T}$ corresponds to a ique in a chordal graph H containing H , and that each maximal clique in H is represented in $\mathcal{T}$ T . This result follows from our ability to use any clique tree satisfying the running intersection property to perform inference, as shown in this chapter. 

# 10.2 Message Passing: Sum Product 

In the previous section, we started out with an execution of the variable elimination algorithm, and showed that it induces a clique tree. In this section, we go in the opposite direction. We assume that we are given a clique tree as a starting point, and we will show how this data structure can be used to perform variable elimination. As we will see, the clique tree is a very 

![](images/b274097050017a684068fd52e3686b84c51a9a6df59ec71179948cc5d309ac73.jpg) 
Figure 10.2 Simpliﬁed clique tree $\mathcal{T}$ for the Extended Student network 

useful and versatile data structure. For one, the same clique tree can be used as the basis for many diferent executions of variable elimination. More importantly, the clique tree provides a data structure for caching computations, allowing multiple executions of variable elimination to be performed much more efciently than simply performing each one separately. 

Consider some set of factors $\Phi$ over $\mathcal{X}$ , nd assume that we are given a clique tree $\mathcal{T}$ over $\Phi$ , as deﬁned in deﬁnition 4.17. In particular, T is guaranteed to satisfy the family preservation and running intersection properties. As we now show, we can use the clique tree in several diferent ways to perform exact inference in graphical models. 

# 10.2.1 Variable Elimination in a Clique Tree 

One way of using a clique tree is simply as guidance for the operations of variable elimination. The factors $\psi$ are computed in the cliques, and messages are sent along the edges. Each clique takes the incoming messages (factors), multiplies them, sums out one or more variables, and sends an outgoing message to another clique. As we will see, the clique-tree data structure dictates the operations that are performed on factors in the clique tree and a partial order over these operations. In particular, if clique $C^{\prime}$ requires a message from $C$ , then $C^{\prime}$ must wait with its computation until $C$ performs its computation and sends the appropriate message to $C^{\prime}$ . We begin with an example and then describe the general algorithm. 

# 10.2.1.1 An Example 

Figure 10.2 shows one possible clique tree $\mathcal{T}$ for the Student ne rk. N that it is diferent from the clique tree of ﬁgure 10.1, in that nonmaximal cliques ( $(C_{6}$ and $C_{7}$ ) are absent. Nev- ertheless, it is straightforward to verify that $\mathcal{T}$ satisﬁes both the family preservation and the running intersection property. The ﬁgure also speciﬁes the assignment $\alpha$ of the initial factors (CPDs) to cliques. Note that, in some cases (for example, the CPD $P(I))$ ), we have more than one possible clique into which the factor can legally be assigned; as we will see, the algorithm applies for any legal choice. 

Our ﬁrst step is to generate a set of initial potentials associated with the diferent cliques. The initial potential $\psi_{i}(C_{i})$ is computed by multiplying the initial factors assigned to the clique $C_{i}$ . For example, $\psi_{5}(J,L,G,S)=\phi_{L}(L,G)\cdot\phi_{J}(J,L,S)$ . 

Now, assume that our task is to compute the probability $P(J)$ . We want to do the variable elimination process so that $J$ is not eliminated. Thus, we select as our root clique some clique that contains $J$ , for example, $C_{5}$ . We then execute the following steps: 

![](images/4ec5d977d3c3b62509abf3d27085e34ffa165b4943575f0242c7432aa7bdd29e.jpg) 

Figure 10.3 Two diﬀerent message propagations with diﬀerent root cliques in the Student clique tree: (a) $C_{5}$ is the root; (b) $C_{3}$ is the root. 

1. In $C_{1}$ : We eliminate $C$ for g $\textstyle\sum_{C}\psi_{1}(C,D)$  . The resulting factor has scope $D$ . We send it as a message $\delta_{1\rightarrow2}(D)$ to $C_{2}$ . →

 2. In $C_{2}$ : W ne $\beta_{2}(G,I,D)=\,\delta_{1\rightarrow2}(D)\cdot\psi_{2}(G,I,D)$ . We the liminate $D$ to get a factor over $G,I$ . The resulting factor is $\delta_{2\to3}(G,I)$ , which is sent to $C_{3}$ .

 3. In $C_{3}$ : We deﬁne $\beta_{3}(G,S,I)=\delta_{2\rightarrow3}(G,I)\cdot\psi_{3}(G,S,I)$ and eliminate $I$ to get a factor over $G,S_{i}$ , which is $\delta_{3\rightarrow5}(G,S)$ .

 4. In $C_{4}$ min $H$ by performing $\textstyle\sum_{H}\psi_{4}(H,G,J)$  and send out the resulting factor as $\delta_{4\to5}(G,J)$ to $C_{5}$ . →

 5. In $C_{5}$ : We deﬁne $\beta_{5}(G,J,S,L)=\delta_{3\rightarrow5}(G,S)\cdot\delta_{4\rightarrow5}(G,J)\cdot\psi_{5}(G,J,S,L).$ 

The factor $\beta_{5}$ is a factor over $G,J,S,L$ that encodes the joint distribution $P(G,J,L,S)$ : all the CPDs have been multiplied in, and all the other variables have been eliminated. If we now want to obtain $P(J)$ , we simply sum out $G,\,L,$ , and $S$ . 

We note that the operations in the elimination process could also have been done in another order. The only constraint is that a clique get all of its incoming messages from its downstream neighbors before it sends its outgoing message toward its upstream neighbor. We say that a clique is ready when it has received all of its incoming messages. Thus, for example, $C_{4}$ is ready at the very start of the algorithm, and the computation associated with it can be performed at any point in the execution. However, $C_{2}$ is ready only after it receives its message from $C_{1}$ . Thus, $C_{1},C_{4},C_{2},C_{3},C_{5}$ is a legal execution ordering for a tree rooted at $C_{5}$ , whereas $C_{2},C_{1},C_{4},C_{3},C_{5}$ is not. Overall, the set of messages transmitted throughout the execution of the algorithm is shown in ﬁgure $10.3\mathrm{a}$ . 

As we mentioned, the choice of root clique is not fully determined. To derive $P(J)$ , we could have chosen $C_{4}$ as the root. Let us see how the algorithm would have changed in that case: 

1. In $C_{1}$ : The computation and message are unchanged.

 2. In $C_{2}$ : The computation and message are unchanged.

 3. In $C_{3}$ : The computation and message are unchanged.

 4. In $C_{5}$ : We deﬁne $\beta_{5}(G,J,S,L)=\delta_{3\rightarrow5}(G,S)\cdot\psi_{5}(G,J,S,L)$ and eliminate $S$ and $L$ . We send out the resulting factor as $\delta_{5\to4}(G,J)$ to $C_{4}$ .

 5. In $C_{4}$ : We deﬁne β $\beta_{4}(H,G,J)=\delta_{5\rightarrow4}(G,S)\cdot\psi_{4}(H,G,J).$ 

We can now extract $P(J)$ by eliminating $H$ and $G$ from $\beta_{4}(H,G,J)$ . 

In a similar way, we can apply exactly the same process to computing the distribution over any other variable. For example, if we want to compute the probability $P(G)$ , we could choose any of the cliques where it appears. If we use $C_{3}$ , for example, the computation in $C_{1}$ and $C_{2}$ is identical. The computation in $C_{4}$ is the same as in the ﬁrst of our two executions: a message is computed and s nt to $C_{5}$ . In $C_{5}$ , we compute $\beta_{5}(G,J,S,L)=\delta_{4\rightarrow5}(G,J)\cdot\psi_{5}(G,J,S,L)$ , and we eliminate J and L to produce a message $\delta_{5\to3}(G,S)$ , which can then be sent to $C_{3}$ and used in the operation: 

$$
\beta_{3}(G,S,I)=\delta_{2\rightarrow3}(G,I)\cdot\delta_{5\rightarrow3}(G,S)\cdot\psi_{3}(G,S,I).
$$ 

Overall, the set of messages transmitted throughout this execution of the algorithm is shown in ﬁgure 10.3b. 

# 10.2.1.2 Clique-Tree Message Passing 

message passing 

initial potential 

We can now specify a general variable elimination algorithm that can be implemented via message passing in a clique tree. Let $\mathcal{T}$ be a clique tree with the cliques $C_{1},\ldots,C_{k}$ . We begin by multiplying the factors assigned to each clique, resulting in our initial potentials. We then use the clique-tree data structure to pass messages between neighboring cliques, sending all messages toward the root clique. We describe the algorithm in abstract terms; box 10.A provides some important tips for efcient implementation. 

Recall that each factor $\phi\in\Phi$ is assigned to some clique $\alpha(\phi)$ . We deﬁne the initial potential of $C_{j}$ to be: 

$$
\psi_{j}(C_{j})=\prod_{\phi\;\;:\;\alpha(\phi)=j}\phi.
$$ 

Because each factor is assigned to exactly one clique, we have that 

$$
\prod_{\phi}\phi=\prod_{j}\psi_{j}.
$$ 

![](images/fde9fa86557954bcb0ebefbe68336d4b2526124a3ef589f553c4c11b259245e2.jpg) 
Figure 10.4 An abstract clique tree that is not chain-structured 

Let $C_{r}$ be the selected root clique. We now perform sum-product variable elimination over the cliques, starting from the leaves of the clique tree and moving inward. More precisely, for each clique $C_{i}$ , we deﬁne $\mathrm{Nb}_{i}$ to be the set of indexes of cliques that are neighbors of $C_{i}$ . Let $p_{r}(i)$ be the upstream neighbor of $i$ (the one on the path to the root clique $r$ ). Each clique $C_{i}$ , except for the root, performs a message passing computation and sends a message to its upstream neighbor $C_{p_{r}(i)}$ . 

sum-product message passing 

The message from $C_{i}$ to another clique $C_{j}$ is computed using the following sum-product message passing computation: 

$$
\delta_{i\rightarrow j}=\sum_{C_{i}-S_{i,j}}\psi_{i}\cdot\prod_{k\in(\mathrm{Nb}_{i}-\{j\})}\delta_{k\rightarrow i}.
$$ 

In words, the clique $C_{i}$ multiplies all incoming messages from its other neighbors with its initial clique potential, resulting in a factor $\psi$ whose scope is the clique. It then sums out all variables except those in the sepset between $C_{i}$ and $C_{j}$ , and sends the resulting factor as a message to $C_{j}$ . 

beliefs 

This message passing process proceeds up the tree, culminating at the root clique. When the root clique has received all messages, it multiplies them with its own initial potential. The result is a factor called the beliefs , denoted $\beta_{r}(C_{r})$ . It represents, as we show, 

$$
\tilde{P}_{\Phi}(C_{r})=\sum_{\chi-C_{r}}\prod_{\phi}\phi.
$$ 

The complete algorithm is shown in algorithm 10.1. 

Example 10.3 Consider the abstract clique tree of ﬁgure 10.4, and assume that we have selected $C_{6}$ as our root clique. The numbering of the cliques denotes one possible ordering of the operations, with $C_{1}$ being the ﬁrst to compute its message. However, multiple other orderings are legitimate, for example, $2,5,1,3,4,6$ ; in general, any ordering that respects the ordering constraints $\{(2~\prec~3),(3~\prec$ $4),(1\prec4),(4\prec6),(5\prec6)\}$ is a legal ordering for the message passing process. 

We can use this algorithm to compute the marginal probability of any set of query nodes $Y$ which is fully contained in some clique. We select one such clique $C_{r}$ to be the root, and perform the clique-tree message passing toward that root. We then extract $\tilde{P}_{\Phi}(Y)$ from the ﬁnal potential at $C_{r}$ by summing out the other variables $C_{r}-Y$ . 

![](images/b12b8615437b813fc88af990b2460ab128f7d4027b6df818b6be9de956293af6.jpg) 

# 10.2.1.3 Correctness 

We now prove that this algorithm, when applied to a clique tree that satisﬁes the family preser- vation and running intersection property, computes the desired expressions over the messages and the cliques. 

In our algorithm, a variable $X$ is eliminated only when a message is sent from $C_{i}$ to a neighboring $C_{j}$ such that $X\in C_{i}$ and $X\notin C_{j}$ . We ﬁrst prove the following result: 

# Proposition 10.2 

Assume that $X$ is eliminated when a message is sent from $C_{i}$ to $C_{j}$ . Then $X$ does not appear anywhere in the tree on the $C_{j}$ side of the edge $(i{-}j)$ . 

Proof The proof is a simple consequence of the running intersection property. Assume by contradiction that $X$ appears in some other clique $C_{k}$ that is on the $C_{j}$ side of the tree. Then $C_{j}$ is on the path from $C_{i}$ to $C_{k}$ . But we know that $X$ appears in both $C_{i}$ and $C_{k}$ but not in $C_{j}$ , violating the running intersection property. 

Based on this result, we can provide a semantic interpretation for the messages used in the clique tree. Let $(i{-}j)$ be some edge in the clique tree. We use $\mathcal{F}_{\prec(i\rightarrow j)}$ to denote the set of factors in the cliques on the $C_{i}$ -side of the edge and $\mathcal{V}_{\prec(i\rightarrow j)}$ to denote the set of variables that appear on the $C_{i}$ -side but are not in the sepset. For example, in the clique tree of ﬁgure 10.2, we have that $\mathcal{F}_{\prec(3\rightarrow5)}\,=\,\{P(C),P(D\mid C),P(G\mid I,D),P(I),P(S\mid I)\}$ and $\mathcal{V}_{\prec(3\rightarrow5)}=\{C,D,I\}$ . Intuitively, the message passed between the cliques $C_{i}$ and $C_{j}$ is the product of all the factors in $\mathcal{F}_{\prec(i\rightarrow j)}$ , marginalized over the variables in the sepset (that is, summing out all the others). 

Theorem 10.3 Let $\delta_{i\to j}$ be a message from $C_{i}$ to $C_{j}$ . Then: 

$$
\delta_{i\rightarrow j}\big(\pmb{S}_{i,j}\big)=\sum_{\mathcal{V}_{\prec(i\rightarrow j)}}\prod_{\phi\in\mathcal{F}_{\prec(i\rightarrow j)}}\phi.
$$ 

Proof The proof proceeds by induction on the length of the path from the leaves. For the base case, the clique $C_{i}$ is a leaf in the tree. In this case, the result follows from a simple examination of the operations executed at the clique. 

Now, consider a clique $C_{i}$ that is not a leaf, and consider the expression 

$$
\sum_{\mathcal{V}_{\prec(i\rightarrow j)}}\prod_{\phi\in\mathcal{F}_{\prec(i\rightarrow j)}}\phi.
$$ 

Let $\displaystyle i_{1},.\cdot\cdot\cdot,i_{m}$ be the neighboring cliques of $C_{i}$ other than $C_{j}$ . It follows immediately from proposition 10.2 that $\mathcal{V}_{\prec(i\rightarrow j)}$ is the disjoint union of $\mathcal{V}_{\prec(i_{k}\rightarrow i)}$ for $k\,=\,1,\hdots,m$ and the variables $Y_{i}$ el nated at $C_{i}$ itself. Similarly, $\mathcal{F}_{\prec(i\rightarrow j)}$ is the disjoint union of the $\mathscr{F}_{\prec(i_{k}\rightarrow i)}$ and the factors F from which $\psi_{i}$ was computed. Thus equation (10.3) is equal to 

$$
\sum_{Y_{i}}\sum_{\mathcal{V}_{\prec(i_{1}\rightarrow i)}}\cdots\sum_{\mathcal{V}_{\prec(i_{m}\rightarrow i)}}\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{1}\rightarrow i)}}\phi\right)\cdot\cdot\cdot\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{m}\rightarrow i)}}\phi\right)\cdot\left(\prod_{\phi\in\mathcal{F}_{i}}\phi\right).
$$ 

As we just showed, for each $k$ , none of the variables in $\mathcal{V}_{\prec(i_{k}\rightarrow i)}$ appear in any of the other factors. Thus, we can use equation (9.6) and push in the summation over $\mathcal{V}_{\prec(i_{k}\rightarrow i)}$ in equa- tion (10.4), and obtain: 

$$
\sum_{Y_{i}}\left(\prod_{\phi\in\mathcal{F}_{i}}\phi\right)\cdot\sum_{\mathcal{V}_{\prec(i_{1}\to i)}}\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{1}\to i)}}\phi\right)\cdot\cdot\cdot\sum_{\mathcal{V}_{\prec(i_{m}\to i)}}\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{m}\to i)}}\phi\right).
$$ 

Using the inductive hypothesis and the deﬁnition of $\psi_{i}$ , this expression is equal to 

$$
\sum_{Y_{i}}\psi_{i}\cdot\delta_{i_{1}\rightarrow i}\cdot\cdot\cdot\cdot\delta_{i_{m}\rightarrow i},
$$ 

which is precisely the operation used to compute the message $\delta_{i\to j}$ . 

This theorem is closely related to theorem 10.2, which tells us that a sepset divides the graph into conditionally independent pieces. It is this conditional independence property that allows the message over the sepset to summarize completely the information in one side of the clique tree that is necessary for the computation in the other. 

Based on this analysis, we can show that: 

Let $C_{r}$ be the root clique in a clique tree, and assume that $\beta_{r}$ is computed as in the algorithm of algorithm 10.1. Then 

$$
\beta_{r}(C_{r})=\sum_{\mathcal{X}-C_{r}}\tilde{P}_{\Phi}(\mathcal{X}).
$$ 

As we discussed earlier, this algorithm applies both to Bayesian network and Markov network inference. For a Bayesian network $\mathcal{B}$ , if $\Phi$ consists of the C Ds in $\mathcal{B}$ , reduced with some evidence $e$ , then $\beta_{r}(\boldsymbol{C}_{r})\,=\,P_{\mathcal{B}}(\boldsymbol{C}_{r},e)$ . For a $\mathcal{H}$ , if Φ consists of the compatibility functions deﬁning the network, then $\beta_{r}(C_{r})\,=\,\tilde{P}_{\Phi}(C_{r})$ . In both cases, we can obtain the probability over the variables in $C_{r}$ as usual, by normalizing the resulting factor to sum to 1. In the Markov network, we can also obtain the value of the partition function simply by summing up all of the entries in the potential of the root clique $\beta_{r}(C_{r})$ . 

# 10.2.2 Clique Tree Calibration 

We have shown that we can use the same clique tree to compute the probability of any variable in $\mathcal{X}$ . In many applications, we often wish to estimate the probability of a large number of variables. For example, in a medical-diagnosis setting, we generally want the probability of several possible diseases. Furthermore, as we will see, when learning Bayesian networks from partially observed data, we always want the probability distributions over each of the unobserved variables in the domain (and their parents). 

Therefore, let us consider the task of computing the posterior distribution over every random variable in the network. The most naive approach is to do inference separately for each variable. Letting $c$ be the cost of a single execution of clique tree inference, the total cost of this algorithm is nc . An approach that is slightly less naive is to run the algorithm once for every clique, making it the root. The total cost of this variant is $K c,$ where $K$ is the number of cliques. However, it turns out that we can do substantially better than either of these approaches. 

Let us revisit our clique tree of ﬁgure 10.2 and consider the three diferent executions of the clique tree algorithm that we described: one where $C_{5}$ is the root, one where $C_{4}$ is the root, and one where $C_{3}$ is the root. As we pointed out, the messages sent from $C_{1}$ to $C_{2}$ and from $C_{2}$ to $C_{3}$ are the same in all three executions. The message sent from $C_{4}$ to $C_{5}$ is the same in both of the executions where it appears. In the second of the three executions, there simply is no message from $C_{4}$ to $C_{5}$ — the message goes the other way, from $C_{5}$ to $C_{4}$ . 

More generally, consider two neighboring cliques $C_{i}$ and $C_{j}$ in some clique tree. It follows from theorem 10.3 that the value of the message sent from $C_{i}$ to $C_{j}$ does not depend on speciﬁc choice of root clique: As long as the root clique is on the $C_{j}$ -side, exactly the same message is sent from $C_{i}$ to $C_{j}$ . The same argument applies if the root is on the $C_{i}$ -side. Thus, in all executions of the clique tree algorithm, whenever a message is sent between two cliques in the same direction, it is necessarily the same. Thus, for any given clique tree, each edge has two messages associated with it: one for each direction of the edge. If we have a total of $c$ cliques, there are $c-1$ edges in the tree; therefore, we have $2(c-1)$ messages to compute. 

![](images/d5b2bf3867ba2ed23f96a5932eee9bfb114f4d73ab4ef004e6bc4f5704f20aea.jpg) 
Figure 10.5 Two steps in a downward pass in the Student network 

We can compute both messages for each edge by the following simple asynchronous algorithm. Recall that a clique can transmit a message upstream toward the root when it has all of the messages from its downstream neighbors. We can generalize this concept as follows: 

Deﬁnition 10.4 ready clique 

dynamic programming sum-product belief propagation 

upward pass downward pass Let $\tau$ be a clique tree. We say that $C_{i}$ is ready to transmit to a neighbor $C_{j}$ when $C_{i}$ has messages from all of its neighbors except from $C_{j}$ . 

When $C_{i}$ is ready to transmit to $C_{j}$ , it can compute the message $\delta_{i\to j}(S_{i,j})$ by multiplying its initial potential with all of its incoming messages except the one from $C_{j}$ , and then eliminate the variables in $C_{i}-S_{i,j}$ . In eﬀect, this algorithm uses yet another layer of dynamic programming to avoid recomputing the same message multiple times. 

Algorithm 10.2 shows the full procedure, often called sum-product belief propagation . As written, the algorithm is deﬁned asynchronously , with each clique sending a message as soon as it is ready. One might wonder why this process is guaranteed to terminate, that is, why there is always a clique that is ready to transmit to some other clique. In fact, the message passing process performed by the algorithm is equivalent to a much more systematic process that consists of an upward pass and a downward pass . In the upward pass, we ﬁrst pick a root and send all messages toward the root. When this process is complete, the root has all messages. Therefore, it can now send the appropriate message to all of its children. This 

![](images/0be5eda88199a2c3282c4ce196d0bc6daab2cc9df3cfb764f3ae05ebf36f6597.jpg) 

message scheduling 

# Example 10.4 

beliefs 

Corollary 10.2 algorithm continues until the leaves of the tree are reached, at which point no more messages need to be sent. This second phase is called the downward pass. The asynchronous algorithm is equivalent to this systematic algorithm, except that the root is simply the ﬁrst clique that happens to obtain messages from all of its neighbors. In an actual implementation, we might want to schedule this process more explicitly. (At the very least, the algorithm would check in line 2 that a message is not computed more than once.) 

Figure 10.3a shows the upward pass of the clique tree algorithm when $C_{5}$ is the root. Figure $\it{10.5a}$ shows a possible ﬁrst step in a downward pass, where $C_{5}$ sends a message to its child $C_{3}$ , based on the message from $C_{4}$ and its initial potential. As soon as a child of the root receives a message, it has all of the information it needs to send a message to its own children. Figure $l0.5b$ shows $C_{3}$ sending the downward message to $C_{2}$ . 

At the end of this process, we compute the beliefs for all cliques in the tree by multiplying the initial potential with each of the incoming messages. The key is to note that the messages used in the computation of $\beta_{i}$ are precisely the same messages that would have been used in a standard upward pass of the algorithm with $C_{i}$ as the root. Thus, we conclude: 

Assume that, for each clique $i$ , $\beta_{i}$ is computed as in the algorithm of algorithm 10.2. Then 

$$
\beta_{i}(C_{i})=\sum_{\mathcal{X}-C_{i}}\tilde{P}_{\Phi}(\mathcal{X}).
$$ 

Note that it is important that $C_{i}$ compute the message to a neighboring clique $C_{j}$ based on its initial potential $\psi_{i}$ and not its modiﬁed potential $\beta_{i}$ . The latter already integrates information from $j$ . If the message were computed based on this latter potential, we would be double-counting the factors assigned to $C_{j}$ (multiplying them twice into the joint). 

When this process concludes, each clique contains the marginal (unnormalized) probability over the variables in its scope. As we discussed, we can compute the marginal probability over a particular variable $X$ by selecting a clique whose scope contains $X$ , and eliminating the redundant variables in the clique. A key point is that the result of this process does not depend on the clique we selected. That is, if $X$ appears in two cliques, they must agree on its marginal. 

Deﬁnition 10.5 calibrated 

beliefs 

Two adjacent cliques $C_{i}$ and $C_{j}$ are said to be calibrated if 

$$
\sum_{C_{i}-S_{i,j}}\beta_{i}(C_{i})=\sum_{C_{j}-S_{i,j}}\beta_{j}(C_{j}).
$$ 

A clique tree $\mathcal{T}$ is calibrated if all pairs of adjacent cliques are calibrated. For a calibrated clique tree, we use the term clique beliefs for $\beta_{i}(C_{i})$ and sepset beliefs for 

$$
\mu_{i,j}(S_{i,j})=\sum_{C_{i}-S_{i,j}}\beta_{i}(C_{i})=\sum_{C_{j}-S_{i,j}}\beta_{j}(C_{j}).
$$ 

The main advantage of the clique tree algorithm is that it computes the posterior probability of all variables in a graphical model using only twice the computation of the upward pass in the same tree. Letting $c$ once again be the execution cost of message passing in a clique tree to one root, the cost of this algorithm is $2c$ . By comparison, recall that the cost of doing a separate computation for each variable is $n c$ and a separate computation for each root clique is $K c,$ where $K$ is the number of cliques. In most cases, the savings are considerable, making the clique tree algorithm the algorithm of choice in situations where we want to compute the posterior of multiple query variables. 

stride 

Box 10.A — Skill: Efcient Implementation of Factor Manipulation Algorithms. While sim- ple conceptually, the implementation of algorithms involving manipulation of factors can be sur- prisingly subtle. In particular, diferent design decisions can lead to orders-of-magnitude diferences in performance, as well as diferences in the accuracy of the results. We now discuss some of the key design decisions in these algorithms. We note that the methods we describe here are equally applicable to the algorithms in many of the other chapters in the book, including the variable elimination algorithm of chapter 9, the exact and approximate sum-product message passing algorithms of chapters 10 and 11, and many of the MAP algorithms of chapter 13. 

The ﬁrst key decision is the representation of our basic data structure: a factor, or a multidimen- sional table, with an entry for each possible assignment to the variables. One standard technique for storing multidimensional tables is to ﬂatten them into a single array in computer memory. For each variable, we also store its cardinality, and its stride , or step size in the factor. For example, given a factor $\phi(A,B,C)$ over variables $A$ , $B$ , and $C$ , with cardinalities 2, 2, and 3, respectively, we can represent the factor in memory by the array 

$$
p h i[0.\ldots11]=\left\{\phi(a^{1},b^{1},c^{1}),\phi(a^{2},b^{1},c^{1}),\phi(a^{1},b^{2},c^{1}),.\ldots,\phi(a^{2},b^{2},c^{3})\right\}.
$$ 

Here the stride for variable $A$ is $l,$ for $B$ is 2 and for $C$ is 4. If we add a fourth variable, $D$ , its stride would be $l2,$ , since we would need to step over twelve entries before reaching the next assignment to $D$ . Notice how, using each variable’s stride, we can easily go from a variable assignment to $^a$ corresponding index into the factor array 

$$
i n d e x=\sum_{i}a s s i g n m e n t[i]\cdot p h i\,.s t r i d e[i]
$$ 

![](images/f0df1ecdbfdb01b9d923c4211973de73f59df91ff73948446bae7520eeafb5ed.jpg) 

and vice versa 

$$
a s s i g n m e n t[i]=\lfloor i n d e x/p h i\,.s t r i d e[i]\rfloor\mod{c a r d[i]}
$$ 

With this factor representation, we can now design a library of operations: product , marginal- ization , maximization , reduction , and so forth. Since many inference algorithms involve multiple iterations over a series of factor operations, it is important that these be high-performance. One of the key design decisions is indexing the appropriate entries in each factor for the operations that we wish to perform. (In fact, when one uses a naive implementation of index computations, one often discovers that 90 percent of the running time is spent on that task.) 

factor product 

Algorithm 10.A.1 provides an example for the product between two arbitrary factors. Here we deﬁne phi.stride $\mathbf{\nabla}![X]\ =\ 0$ if $X\ \notin\ S c o p e[\phi]$ . The inner loop (over l ) advances to the next assignment to the variables in $\psi$ and calculates indexes into each other factor array on the ﬂy. It can be understood by considering the equation for computing index shown earlier. Similar on- the-ﬂy index calculations can be applied for other factor operations. We leave these as an exercise (exercise 10.3). 

For iterative algorithms or multiple queries, where the same operation (on diferent data) is performed a large number of times, it may be beneﬁcial to cache these index mappings for later use. Note, however, that the index mappings require the same amount of storage as the factors themselves, that is, are exponential in the number of variables. Thus, this design choice ofers a direct trade-of between memory usage and speed, especially in view of the fact that the index computations require approximately the same amount of work as the factor operation itself. Since performance of main memory is orders of magnitudes faster than secondary storage (disk), when memory limitations are an issue, it is better not to cache index mappings for large problems. One exception is template models, where savings can be made by reusing the same indexes for diferent instantiations of the factor templates. 

An additional trick in reducing the computational burden is to preallocate and reuse memory for factor storage. Allocating memory is a relatively expensive procedure, and one does not want to waste time on this task inside a tight loop. To illustrate this point, we consider the example of variable elimination for computing $\psi(A,D)$ as 

$$
\psi(A,D)=\sum_{B,C}\phi_{1}(A,B)\phi_{2}(B,C)\phi_{3}(C,D)=\sum_{B}\phi_{1}(A,B)\sum_{C}\phi_{2}(B,C)\phi_{3}(C,D).
$$ 

Here we need to compute three intermediate factors: $\tau_{1}(B,C,D)=\phi_{2}(B,C)\phi_{3}(C,D);\tau_{2}(B,D)=$ $\textstyle\sum_{C}\tau_{1}(B,C,D)$ ; and $\tau_{3}(A,B,D)=\phi_{1}(A,B)\tau_{2}(B,D)$ . Notice that, once $\tau_{2}(B,D)$ has been calculated, we no longer need the values in $\tau_{1}(B,C,D)$ . By initially allocating memory large enough to hold the larger of $\tau_{1}(B,C,D)$ and $\tau_{3}(A,B,D)$ , we can use the same memory for both. Because every operation in a variable elimination or message passing algorithm requires the computation of one or more intermediate factors, some of which are much larger than the desired end product, the savings in both time (preallocation) and memory (reusage) can be signiﬁcant. 

log-space factor marginalization 

We now turn our attention to numerical considerations. Operations such as factor product involve multiplying many small numbers together, which can lead to underﬂow problems due to ﬁnite precision arithmetic. The problem can be alleviated somewhat by renormalizing the factor after each operation (so that the maximum entry in the factor is one); this operation leaves the results to most queries unchanged (see exercise 9.3). However, if each entry in the factor is computed as the product of many terms, underﬂow can still occur. An alternative solution is to perform the computation in log-space , replacing multiplications with additions; this transformation allows for greater machine precision to be utilized. Note that marginalization , which requires that we sum entries, cannot be performed in log-space; it requires exponentiating each entry in the factor, performing the marginalization, and taking the log of the result. Since moving from log-space to probability-space incurs a signiﬁcant decrease in dynamic range, factors should be normalized before applying this transform. One standard trick is to shift every entry by the maximum entry 

$$
\begin{array}{r}{p h i[i]\leftarrow\exp\left\{\mathit{l o g P h i}[i]-c\right\},}\end{array}
$$ 

where $c=\operatorname*{max}_{i}\mathcal{I}o g P h i[i]$ ; this transformation ensures that the resulting factor has a maximum entry of one and prevents overﬂow. 

We note that there are some caveats to operating in log-space. First, one may incur a performance hit: Floating point multiplication is no slower than ﬂoating point addition, but the transformation to and from log-space, as required for marginalization, can take a signiﬁcant proportion of the total processing time. This caveat does not apply to algorithms such as max-product, where maximization can be performed in log-space; indeed, these algorithms are almost always implemented as max- sum. Moreover, log-space operations require care in handling nonpositive factors (that is, factors with some zero entries). 

Finally, at a higher level, as with any software implementation, there is always a trade-of between speed, memory consumption, and reusability of the code. For example, software specialized for the case of pairwise potentials over a grid will almost certainly outperform code written for general graphs with arbitrary potentials. However, the small performance hit in using well designed general purpose code often outweighs the development efort required to reimplement algorithms for each specialized application. However, as always, it is also important not to try to optimize code too early. It is more beneﬁcial to write and proﬁle the code, on real examples, to determine what operations are causing bottlenecks. This allows the development efort to be targeted to areas that can yield the most gain. 

# 10.2.3 A Calibrated Clique Tree as a Distribution 

A calibrated clique tree is more than simply a data structure that stores the results of probabilistic inference for all of the cliques in the tree. As we now show, it can also be viewed as an alternative representation of the measure $\tilde{P}_{\Phi}$ . 

At calibration, we have that: 

$$
\beta_{i}=\psi_{i}\cdot\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}.
$$ 

We also have that: 

$$
\begin{array}{r c l}{\mu_{i,j}(S_{i,j})}&{=}&{\displaystyle\sum_{C_{i}\sim S_{i,j}}\beta_{i}(C_{i})}\\ &{=}&{\displaystyle\sum_{C_{i}\sim S_{i,j}}\psi_{i}\cdot\prod_{k\in\mathbb{N}_{b_{i}}}\delta_{k\to i}}\\ &{=}&{\displaystyle\sum_{C_{i}\sim S_{i,j}}\psi_{i}\cdot\delta_{j\to i}\prod_{k\in(\mathbb{N}_{b_{i}}-\{j\})}\delta_{k\to i}}\\ &{=}&{\displaystyle\delta_{j\to i}\sum_{C_{i}\sim S_{i,j}}\psi_{i}\cdot\prod_{k\in(\mathbb{N}_{b_{i}}-\{j\})}\delta_{k\to i}}\\ &{=}&{\displaystyle\delta_{j\to i}\delta_{i\to j},}\end{array}
$$ 

where the fourth equality holds because no variable in the scope of $\delta_{j\rightarrow i}$ is involved in the summation. 

We can now show the following important result: 

# Proposition 10.3 

At convergence of the clique tree calibration algorithm, we have that: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\frac{\prod_{i\in\mathcal{V}_{\mathcal{T}}}\beta_{i}(C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(S_{i,j})}.
$$ 

Proof Using equation (10.8), the numerator in the right-hand side of equation (10.10) can be rewritten as: 

$$
\prod_{i\in\mathcal{V}_{T}}\psi_{i}(C_{i})\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}.
$$ 

![](images/6528c8dab49338e85d387fb280df5ece0d279abfc3c451aeee100a63761eb0e0.jpg) 

Figure 10.6 The clique and sepset beliefs for the Misconception example. Using equation (10.9), the denominator can be rewritten as: 

$$
\prod_{(i-j)\in\mathcal{E}\tau}\delta_{i\to j}\delta_{j\to i}.
$$ 

Each message $\delta_{i\to j}$ appears exactly once in the numerator and exactly once in the denominator, so that all messages cancel. The remaining expression is simply: 

$$
\prod_{i\in\mathcal{V}_{T}}\psi_{i}(C_{i})=\tilde{P}_{\Phi}.
$$ 

reparameteriza- tion clique tree invariant 

Example 10.5 

Thus, via equation (10.10), the clique and sepsets beliefs provide a re parameter iz ation of the unnormalized measure. This property is called the clique tree invariant , for reasons which will become clear later on in this chapter. 

Another intuition for this result can be obtained from the following example: 

Consider a clique tree obtained from Markov network $\scriptstyle A-B-C-D,$ , with an appropriate set of factors $\Phi$ . Our clique tree in this case would have three cliques $C_{1}\,=\,\{A,B\}$ , $C_{2}\,=\,\{B,C\}$ , and $C_{3}=\{C,D\}$ . When the clique tree is calibrated, we have that $\beta_{1}(A,B)=\tilde{P}_{\Phi}(A,B)$ and $\beta_{2}(B,C)\,=\,\tilde{P}_{\Phi}(B,C)$ . From the conditional independence properties of this distribution, we have that 

$$
{\tilde{P}}_{\Phi}(A,B,C)={\tilde{P}}_{\Phi}(A,B){\tilde{P}}_{\Phi}(C\mid B),
$$ 

and 

$$
\tilde{P}_{\Phi}(C\mid B)=\frac{\beta_{2}(B,C)}{\tilde{P}_{\Phi}(B)}.
$$ 

As $\beta_{2}(B,C)=\tilde{P}_{\Phi}(B,C)$ , we can obtain $\tilde{P}_{\Phi}(B)$ by marginalizing $\beta_{2}(B,C)$ . Thus, we can write: 

$$
\begin{array}{r c l}{{\tilde{P}_{\Phi}(A,B,C)}}&{{=}}&{{\beta_{1}(A,B)\displaystyle\frac{\beta_{2}(B,C)}{\sum_{C}\beta_{2}(B,C)}}}\\ {{}}&{{=}}&{{\displaystyle\frac{\beta_{1}(A,B)\beta_{2}(B,C)}{\sum_{C}\beta_{2}(B,C)}.}}\end{array}
$$ 

In fact, when the two cliques are calibrated, they must agree on the marginal of $B$ . Thus, the expression in the denominator can equivalently be replaced by $\textstyle\sum_{A}\beta_{1}(A,B)$ . 

Based on this analysis, we now formally deﬁne the distribution represented by a clique tree: 

Deﬁnition 10.6 clique tree measure 

We deﬁne the measure induced by a calibrated tree $\mathcal{T}$ to be: 

$$
Q_{\mathcal{T}}=\frac{\prod_{i\in\nu_{\mathcal{T}}}\beta_{i}(C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(S_{i,j})},
$$ 

where 

$$
\mu_{i,j}=\sum_{\boldsymbol{C}_{i}-\boldsymbol{S}_{i,j}}\beta_{i}(\boldsymbol{C}_{i})=\sum_{\boldsymbol{C}_{j}-\boldsymbol{S}_{i,j}}\beta_{j}(\boldsymbol{C}_{j}).
$$ 

# Example 10.6 

Consider, for example, the Markov network of example 3.8, whose joint distribution is shown in ﬁgure 4.2. One clique tree for this network consists of the two cliques $\{A,B,D\}$ and $\{B,C,D\}$ , with the sepset $\{B,D\}$ . The ﬁnal potentials and sepset for this example are shown in ﬁgure 10.6. It is straightforward to conﬁrm that the clique tree is indeed calibrated. One can also verify that this clique tree provides a re parameter iz ation of the original distribution. For example, consider the entry $\tilde{P}_{\Phi}(a^{1},b^{0},c^{1},d^{0})=100$ . According to equation (10.10), the clique tree measure is: 

$$
\frac{\beta_{1}(a^{1},b^{0},d^{0})\beta_{2}(b^{0},c^{1},d^{0})}{\mu_{1,2}(b^{0},d^{0})}=\frac{200\cdot300,100}{600,200}=100,
$$ 

as required. 

Our analysis so far shows that for a set of calibrated potentials derived from clique tree inference, we have two properties: the clique tree measure is $\tilde{P}_{\Phi}$ and the ﬁnal beliefs are the marginals of $\tilde{P}_{\Phi}$ . As we now show, these two properties coincide for any calibrated clique tree. 

# Theorem 10.4 

$\mathcal{T}$ ique tree over $\Phi$ , and $\beta_{i}(C_{i})$ be a set tials for $\mathcal{T}$ . Then, $\tilde{P}_{\Phi}(\mathcal{X})\propto Q_{\mathcal{T}}$ X ∝ if and only if, for each i $i\in\mathcal{V}_{T}$ ∈V , we have that $\dot{\beta}_{i}(\mathbf{\cal{C}}_{i})\propto\tilde{P_{\Phi}}(\mathbf{\cal{C}}_{i})$ ∝ . T T 

Proof Let $r$ e any clique in $\mathcal{T}$ , which we choose to be th oot. Deﬁne e descendant cliques of a clique $C_{i}$ to be the cliques that are downstream from $C_{i}$ relative to $C_{r}$ ; the nondescendant cliques are then the remaining cliques (other than $C_{i.}$ ). Let $X$ be the variables in the scope of the nondescendant cliques. It follows immediately from theorem 10.2 that 

$$
{\tilde{P}}_{\Phi}\models(C_{i}\ \bot\ X\ |\ S_{i,p_{r}(i)}).
$$ 

From this, we obtain, using the standard chain-rule argument, that: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\tilde{P}_{\Phi}(C_{r})\cdot\prod_{i\neq r}\tilde{P}_{\Phi}(C_{i}\mid S_{i,p_{r}(i)}).
$$ 

We can rewrite equation (10.11) as a similar product, using the same root: 

$$
Q_{\mathcal{T}}(\mathcal{X})=\beta_{r}(C_{r})\cdot\prod_{i\neq r}\beta_{i}(C_{i}\mid S_{i,p_{r}(i)}).
$$ 

The “if” direction now follows from direct substitution of $\beta_{i}$ for each $\tilde{P}_{\Phi}(C_{i})$ . 

To prove the “only if” direction, we note that each of the terms $\beta_{i}(C_{i}\ \mid\ S_{i,p_{r}(i)})$ is a conditional distribution; hence, if we marginalize out the variables not in $C_{r}$ in the distribution $Q_{\mathcal{T}}$ , each of these conditional distributions marginalizes to 1 , and so we are left with $Q_{\mathcal{T}}(C_{r})=$ $\beta_{r}(C_{r})$ . It now follows that if $\tilde{P}_{\Phi}\propto Q_{\mathcal{T}}$ ∝ T , then $\tilde{P}_{\Phi}(C_{r})\propto Q_{\mathcal{T}}(C_{r})=\beta_{r}(C_{r})$ ∝ T . Because this argument applies to any choice of root clique, we have proved that this equality holds for every clique. 

 Thus, we can view the clique tree as an alternative representation of the joint measure, one that directly reveals the clique marginals. As we will see, this view turns out to be very useful, both in the next section and in chapter 11. 

# 10.3 Message Passing: Belief Update 

The previous section showed one approach to message passing in clique trees, based on the same ideas of variable elimination that we discussed in chapter 9. In this section, we present a related approach, but one that is based on very diferent intuitions. We begin by describing an alternative message passing scheme that is diferent from but mathematically equivalent to that of the previous section. We then show how this new approach can be viewed as operations on the re parameter iz ation of the distribution in terms of the clique and sepset beliefs $\{\beta_{i}(C_{i})\}_{i\in\mathcal{V}_{\mathcal{T}}}$ and $\{\mu_{i,j}(S_{i,j})\}_{(i-j)\in\mathcal{E}_{T}}$ . Each message passing step will change this representation while leaving it a re parameter iz ation of $\tilde{P}_{\Phi}$ . 

# 10.3.1 Message Passing with Division 

Consider again the message passing process used in CTree-SP-Calibrate (algorithm 10.2). There, two messages are passed along each link $(i{-}j)$ . Assume, without loss of generality, that the ﬁrst message is passed from $C_{j}$ to $C_{i}$ . A return message from $C_{i}$ to $C_{j}$ is passed when $C_{i}$ has received messages from all of its other neighbors. 

At this point, $C_{i}$ has all of the necessary information to compute its ﬁnal potential. It multiplies the initial potential with the incoming messages from all of its neighbors: 

$$
\beta_{i}=\psi_{i}\cdot\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}.
$$ 

As we discussed, this ﬁnal potential is not used in computing the message to $C_{j}$ : this potential already incorporates the information (message) passed from $C_{j}$ ; if we used it when computing the message to $C_{j}$ , this information would be double-counted. Thus, the message from $C_{i}$ to $C_{j}$ is computed in a way that omits the information obtained from $C_{j}$ : we multiply the initial potential with all of the messages except for the message from $C_{i}$ , and then marginalize over the sepset (equation (10.2)). 

A diferent approach to computing the same expression is to multiply in all of the messages, and then divide the resulting factor by $\delta_{j\rightarrow i}$ . To make this notion precise, we must deﬁne a factor-division operation: 

![](images/cf8d0452733c4c14c7234afdb6ea9658d362b670e024bf13b43576f345696de6.jpg) 
Figure 10.7 An example of factor division 

Deﬁnition 10.7 factor division 

Let $X$ and $Y$ be disjoint sets of variables, and let $\phi_{1}(X,Y)$ and $\phi_{2}(Y)$ be two factors. We deﬁne the division $\frac{\phi_{1}}{\phi_{2}}$ to be a factor $\psi$ of scope $X,Y$ deﬁned as follows: 

$$
\psi(X,Y)=\frac{\phi_{1}(X,Y)}{\phi_{2}(Y)},
$$ 

where we deﬁne $0/0=0$ . 

Note that, as in the case of other factor operations, factor division is done component by component. Figure 10.7 shows an example. Also note that the operation is not well deﬁned if the denominator is zero and the numerator is not. 

We now see that we can compute the expression of equation (10.2) by computing the beliefs as in equation (10.12), and then dividing by the remaining message: 

$$
\delta_{i\rightarrow j}=\frac{\sum_{C_{i}-S_{i,j}}\beta_{i}}{\delta_{j\rightarrow i}}.
$$ 

Example 10.7 Let us return to the simple clique tree in example 10.5, and assume that $C_{2}$ serves as the (de facto) root, so that we ﬁrst pass messages from $C_{1}$ to $C_{2}$ and from $C_{3}$ to $C_{2}$ . The message $\delta_{1\rightarrow2}$ $\textstyle\sum_{A}\psi_{1}(A,B)$ . Using the variable elimination message ( CTree-SP-Calibrate ), we pass a return message $\begin{array}{r}{\delta_{2\to1}(B)\,=\,\sum_{C}\psi_{2}(B,C)\delta_{3\to2}(C)}\end{array}$ . Alternatively, we can compute → $\beta_{2}(B,C)=\delta_{1\rightarrow2}(B)\cdot\delta_{3\rightarrow2}(C)\cdot\psi_{2}(B,C)$ → · → · , and then send a message 

$$
{\frac{\sum_{C}\beta_{2}(B,C)}{\delta_{1\rightarrow2}(B)}}=\sum_{C}{\frac{\beta_{2}(B,C)}{\delta_{1\rightarrow2}(B)}}=\sum_{C}\psi_{2}(B,C)\cdot\delta_{3\rightarrow2}(C).
$$ 

Thus, the two approaches are equivalent. 

sum-product- divide 

Based on this insight, we can deﬁne the sum-product-divide message passing scheme, where each clique $C_{i}$ maintains its fully updated current beliefs $\beta_{i}$ , which are deﬁned as in equa- tion (10.8). Each sepset also maintains its beliefs $\mu_{i,j}$ deﬁned as the product of the messages in both directions, as in equation (10.9). We now show that the entire message passing process can be executed in an equivalent way in terms of the clique and sepset beliefs, without having to remember the initial potentials $\psi_{i}$ or to compute explicitly the messages $\delta_{i\to j}$ . 

The message passing process follows the lines of example 10.7. Each clique $C_{i}$ initializes $\beta_{i}$ as $\psi_{i}$ and then updates it by multiplying with message updates received from its neighbors. Each sepset $\boldsymbol{S}_{i,j}$ maintains $\mu_{i,j}$ as the previous message passed along the edge $(i{-}j)$ , regardless of the direction. This message is used to ensure that we do not double-count: Whenever a new message is passed along the edge, it is divided by the old message, eliminating the previous message from the update to the clique. Somewhat surprisingly, as we will show, the message passing operation is correct regardless of the clique that sent the last message on the edge. Intuitively, once the message is passed, its information is incorporated into both cliques; thus, each needs to divide by it when passing a message to the other. We can view this algorithm as maintaining a set of belief over the cliques in the tree. The message passing operation takes the beliefs of one clique and uses them to update the beliefs of a neighbor. Thus, we call this algorithm belief-update message passing; it is also known as the Lauritzen-Spiegelhalter algorithm . 

Continuing with example 10.7, assume that $C_{2}$ initially passes an uninformed message to $C_{3}$ : $\begin{array}{r}{\sigma_{2\rightarrow3}=\sum_{B}\psi_{2}(B,C)}\end{array}$ . This message multiplies the beliefs about $C_{3}$ , so that, at this point: 

$$
\beta_{3}(C,D)=\psi_{3}(C,D)\sum_{B}\psi_{2}(B,C).
$$ 

This message is also stored in the sepset as $\mu_{2,3}$ . Now, assume that $C_{3}$ sends a message to $C_{2}$ : $\begin{array}{r}{\sigma_{3\rightarrow2}(C)=\sum_{D}\beta_{3}(C,D)}\end{array}$ . This message is divided by $\mu_{2,3}$ , so the actual update for $C_{2}$ is: 

$$
\begin{array}{c c l}{\displaystyle\frac{\sigma_{3\to2}(C)}{\mu_{2,3}(C)}}&{=}&{\displaystyle\frac{\sum_{D}\beta_{3}(C,D)}{\mu_{2,3}(C)}}\\ &{=}&{\displaystyle\frac{\sum_{D}\psi_{3}(C,D)\mu_{2,3}(C)}{\mu_{2,3}(C)}}\\ &{=}&{\displaystyle\sum_{D}\psi_{3}(C,D).}\end{array}
$$ 

This expression is precisely the update that $C_{2}$ would have received from $C_{3}$ in the case where $C_{2}$ does not ﬁrst send an uninformed message. At this point, the message stored in the sepset is 

$$
\sum_{D}\beta_{3}(C,D)=\sum_{D}\left(\psi_{3}(C,D)\cdot\sum_{B}\psi_{2}(B,C)\right).
$$ 

Assu t at the next step $C_{2}$ receives a message from $C_{1}$ , containing $\textstyle\sum_{A}\psi_{1}(A,B)$ . The sepset $S_{\mathrm{1,2}}$ contains a message that is identically 1 , so that this message is transmitted as is. $A t$ this point, $C_{2}$ has received informed messages from both sides and is therefore informed. Indeed, we have shown that: 

$$
\beta_{2}(B,C)=\psi_{2}(B,C)\cdot\sum_{A}\psi_{1}(A,B)\cdot\sum_{D}\psi_{3}(C,D).
$$ 

as required. 

![](images/0081832ae1f43d277197bf858637e5b838ad5c1afac458b355a385c7d321d875.jpg) 

The precise algorithm is shown in algorithm 10.3. Note that, as written, the message passing algorithm is underspeciﬁed: in line 3, we can select any pair of cliques $C_{i}$ and $C_{j}$ between which we will pass a message. Interestingly, we can make this choice arbitrarily, without damaging the correctness of the algorithm. For example, if $C_{i}$ (for some reason) passes the same message to $C_{j}$ a second time, the process of dividing out by the stored message reduces the message actually passed to 1 , so that it has no inﬂuence. Furthermore, if $C_{i}$ passes a message to $C_{j}$ based on partial information (that is, without taking into consideration all of its incoming messages), and then resends a more updated message later on, the efect is identical to simply sending the updated message once. Moreover, at convergence, regardless of the message passing steps used, we necessarily have a calibrated clique tree. This property follows from the fact that, in order for all message updates to have no efect, we need to have 

$$
\sum_{C_{i}-S_{i,j}}\beta_{i}=\mu_{i,j}=\sum_{C_{j}-S_{i,j}}\beta_{j}.
$$ 

Thus, at convergence, each pair of neighboring cliques $i,j$ must agree on the variables in sepset, and the message $\mu_{i,j}$ is precisely the sepset marginal. These properties also follow from the equivalence between belief-update message passing and sum-product message passing, which we show next. 

# 10.3.2 Equivalence of Sum-Product and Belief Update Messages 

So far, although we used sum-product message propagation to motivate the deﬁnition of the belief update steps, we have not shown a direct connection between them. We now show a simple and elegant equivalence between the two types of message passing operations. From this result, it immediately follows that belief-update message passing is guaranteed to converge to the correct marginals. 

Our proof is based on equation (10.8) and equation (10.9), which provide a mapping between the sum-product and belief-update representations. We consider corresponding runs of the two algorithms in which an identical sequence of message passing steps is executed. We show that these two properties hold as an invariant between the data structures maintained by the two algorithms. The invariant holds initially, and it is maintained throughout the corresponding runs. 

Theorem 10.5 Consider a set of sum-product initial potentials $\{\psi_{i}\ \ :\ \ i\,\in\,\mathcal{V}_{\mathcal{T}}\}$ V T } and me $\{\delta_{i\to j},\delta_{j\to i}\ \ :$ $(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}\}$ , and a set of belief-update beliefs { $\{\beta_{i}\,:\,i\in\mathcal{V}_{\mathcal{T}}\}$ ∈V T } and messages { $\{\mu_{i,j}\,:\,(i{-}j)\in\mathcal{E}_{T}\}$ , for which equation (10.8) and equation (10.9) hold. For any pair of neighboring cliques $C_{i},C_{j}$ , let $\{\delta_{i\to j}^{\prime},\delta_{j\to i}^{\prime}\ :\ (i\!-\!j)\in\mathcal{E}_{T}\}$ → ssages following an application of SP-Message $(i,j)$ , and { $\{\beta_{i}^{\prime}\ :\ C_{i}\in\mathcal{T}\}$ ∈T } $\{\mu_{i,j}^{\prime}\ :\ (i{-}j)\in\mathcal{E}_{T}\}$ ∈E T } , be the set of belief-update beliefs following an application of BU-Message $(i,j)$ . Then equation (10.8) and equation (10.9) also hold for the new beliefs $\delta_{i\to j}^{\prime},\,\beta_{i}^{\prime},\,\mu_{i,j}^{\prime}$ . 

The proof uses simple algebraic manipulation, and it is left as an exercise (exercise 10.4). This equivalence implies another result that will prove important in subsequent developments: 

# Corollary 10.3 

Proof The proof of proposition 10.3 relied only on equation (10.8) and equation (10.9). Because these two equalities hold in every step of the belief-update message passing algorithm, we have that the clique tree invariant also holds continuously. 

This equivalence also allows us to deﬁne a message schedule that guarantees convergence to the correct clique marginals in two passes: We simply follow the same upward-downward-pass schedule used in CTree-SP-Calibrate , using any (arbitrarily chosen) root clique $C_{r}$ . 

# 10.3.3 Answering Queries 

As we have seen, a calibrated clique tree contains the answer to multiple queries at once: the posterior probability of any set of variables that are present together in a single clique. A particular type of query that turns out to be important in this setting is the computation of the posterior for families of variables in a probabilistic network: a node and its parents in the context of Bayesian networks, or a clique in a Markov network. The family preservation property for cluster graphs (and hence for clique trees) implies that a family must be a subset of some cluster in the cluster graph. 

In addition to these queries, which we get immediately as a by-product of calibration, we can also use a clique tree for other queries. We describe the algorithm for these queries in terms of a calibrated clique tree that satisﬁes the clique tree invariant. Due to the equivalence of sum-product and belief-update message passing, we can obtain such a clique tree using either method. 

# 10.3.3.1 Incremental Updates 

incremental update 

Consider a situation where, at some point in time, we have a certain set of observations, which we use to condition our distribution and reach conclusions. At some later time, we obtain additional evidence, and want to update our conclusions accordingly. This type of situation, where we want to perform incremental update is very common in a wide variety of settings. For example, in a medical setting, we often perform diagnosis on the basis of limited evidence; the initial diagnosis helps us decide which tests to perform, and the results need to be incorporated into our diagnosis. 

The most naive approach to dealing with this task is simply to condition the initial factors (for example, the CPDs) on all of the evidence, and then redo the calibration process from the beginning, starting from these factors. A somewhat more efcient approach is based on the view of the clique tree as representing the distribution $\tilde{P}_{\Phi}$ . 

Assume that our initial distribution $\tilde{P}_{\Phi}$ (prior to the new information) is represented via a set of factors $\Phi$ , as in equation (10.1). Given some evidence $Z=z$ , we can obtain $\tilde{P}_{\Phi}(\mathcal{X},Z=z)$ X by zeroing out the entries in the unnormalized distribution that are inconsistent with the evidence $Z=z$ . We can accomplish this efect by multiplying $\tilde{P}_{\Phi}$ with an additional factor which is the indicator function $I\{Z=z\}$ { } . More precisely, assume that our current distribution over $\mathcal{X}$ is deﬁned by a set of factors Φ , so that 

$$
{\tilde{P}}_{\Phi}({\mathcal{X}})=\prod_{\phi\in\Phi}\phi.
$$ 

Then, 

$$
\tilde{P}_{\Phi}(\mathcal{X},Z=z)=\mathbf{I}\{Z=z\}\cdot\prod_{\phi\in\Phi}\phi.
$$ 

Let $\tilde{P}_{\Phi}^{\prime}(\mathcal{X})=\tilde{P}_{\Phi}(\mathcal{X},Z=z)$ . 

Now, assume that we have a clique tree (calibrated or not) that represents this distribution using the clique tree invariant. That is: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=Q_{\mathcal{T}}=\frac{\prod_{i\in\nu_{\mathcal{T}}}\beta_{i}(C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(S_{i,j})}.
$$ 

We can represent the distribution $\tilde{P}_{\Phi}^{\prime}(\mathcal{X})$ X as 

$$
\tilde{P}_{\Phi}^{\prime}(\mathcal{X})=\mathbfcal{I}\{Z=z\}\cdot\frac{\prod_{i\in\mathcal{V}_{T}}\beta_{i}(C_{i})}{\prod_{(i-j)\in\mathcal{E}_{T}}\mu_{i,j}(S_{i,j})}.
$$ 

Thus, we obtain a representation of $\tilde{P}_{\Phi}^{\prime}$ in the clique tree simply by multiplying in the new factor $I\{Z=z\}$ into some clique $C_{i}$ containing the variable $Z$ . 

If the clique tree is calibrated before this new factor is introduced, then the clique $C_{i}$ has already assimilated all of the other information in the graph. Thus, the clique $C_{i}$ itself is now fully informed, and no additional message passing is required in order to obtain $\tilde{P}_{\Phi}^{\prime}(C_{i})$ . Other cliques, however, still need to be updated with the new information. To obtain $\check{P}_{\Phi}^{\prime}(C_{j})$ for another clique $C_{j}$ , we need only transmit messages from $C_{i}$ to $C_{j}$ , via the intervening cliques on the path between them. (See exercise 10.10.) As a consequence, the entire tree can be recalibrated to account for the new evidence using a single pass. Note that retracting evidence is not as simple: Once we multiply parts of the distribution by zero, these parts are lost, and they cannot be recovered. Thus, if we want to reserve the ability to retract evidence, we must store the beliefs prior to the conditioning step (see exercise 10.12). 

Interestingly, the same incremental-update approach applies to other forms of updating the distribution. In particular, we can multiply the distribution with a factor that is not an indicator function for some variable, an operation that is useful in various applications. The same analysis holds unchanged. 

# 10.3.3.2 Queries Outside a Clique 

Consider a query $P(Y\mid e)$ where the variables $Y$ are not present together in a single cliq One naive approach is to construct a clique tree where we force one of the cliques to contain $Y$ (see exercise 10.13). However, this approach forces us to tailor our clique tree to diferent queries, negating many of its advantages. An alternative approach is to perform variable elimination over a calibrated clique tree. 

Consider the simple clique tree of example 10.7, and assume that we have calibrated the clique tree, so that the beliefs represent the joint distribution as in equation (10.10). Assume that we now want to compute the probability $\tilde{P}_{\Phi}(B,D)$ . If the entire clique tree is calibrated, so is any (connected) subtree $\mathcal{T}^{\prime}$ . Letting $\mathcal{T}^{\prime}$ consist of the two cliques $C_{2}$ and $C_{3}$ , it follows from theorem 10.4 that: 

$$
\tilde{P}_{\Phi}(B,C,D)=Q_{\cal T}{}^{\prime}.
$$ 

By the clique tree invariant (equation (10.10)), we have that: 

$$
\begin{array}{c c l}{{\tilde{P}_{\Phi}(B,D)}}&{{=}}&{{\displaystyle\sum_{C}\tilde{P}_{\Phi}(B,C,D)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\frac{\beta_{2}(B,C)\beta_{3}(C,D)}{\mu_{2,3}(C)}}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\tilde{P}_{\Phi}(B\mid C)\tilde{P}_{\Phi}(C,D),}}\end{array}
$$ 

where the last equality follows from calibration. Each of these probability expressions corresponds to a set of clique beliefs divided by a message. We can now perform variable elimination, using these factors in the usual way. 

![](images/b576e9372df91d3c243bb80825b3ff5c22154069a7357f49b64d3a7ac55c48c9.jpg) 

More generally, we can compute the joint probability $\tilde{P}_{\Phi}(Y)$ for an arbitrary subset $Y$ by using the beliefs in a calibrated clique tree to deﬁne factors corresponding to conditional probabilities in $\tilde{P}_{\Phi}$ , and then performing variable elimination over the resulting set of factors. The precise algorithm is shown in algorithm 10.4. The savings over simple variable elimination arise because we do not have to perform inference over the entire clique tree, but only over a portion of the tree that contains the variables $Y$ that constitute our query. In cases where we have a very large clique tree, the savings can be signiﬁcant. 

# 10.3.3.3 Multiple Queries 

Now, assume that we want to compute the probabilities of an entire set of queries where the variables are not together in a clique. For example, we might wish to compute $\tilde{P}_{\Phi}(X,Y)$ for every pair of variables $X,Y\,\in\,{\mathcal{X}}\,-\,E$ . Clearly, the approach of constructing a clique tree to ensure that our query variables are present in a single clique breaks down in this case: If every pair of variables is present in some clique, there must be some clique that contains all of the variables (see exercise 10.14). 

A somewhat less naive approach is simply to run the variable elimination algorithm of algo- rithm 10.4 $\textstyle{\binom{n}{2}}$  times, once for each pair of variables $X,Y$ . However, because pairs of variables, on average, are fairly far from each other in the clique tree, this approach requires fairly sub- stantial running time (see exercise 10.15). An even better approach can be obtained by using dynamic programming . 

r a calibrated clique tree $\mathcal{T}$ over $\Phi$ , and assume we want to compute the probability $\tilde{P}_{\Phi}(X,Y)$ for every pair of variables $X,Y$ . We execute this process by gradually constructing a table for each $C_{i},C_{j}$ that contains $\tilde{P}_{\Phi}(C_{i},C_{j})$ . We construct the table for $i,j$ in order of the distance between $C_{i}$ and $C_{j}$ in the tree. 

The base case is when $i,j$ are neighboring cliques. In this case, we simply extract $\tilde{P}_{\Phi}(C_{i})$ from its clique beliefs, and compute 

$$
\tilde{P}_{\Phi}(C_{j}\mid C_{i})=\frac{\beta_{j}(C_{j})}{\mu_{i,j}(C_{i}\cap C_{j})}.
$$ 

From these, we can compute $\tilde{P}_{\Phi}(C_{i},C_{j})$ . 

Now, consider a pair of cliques $C_{i},C_{j}$ that are not neighbors, and let $C_{l}$ be the neighbor of $C_{j}$ that is one step closer in the clique tree to $C_{i}$ . By construction, we have already computed $\tilde{P}_{\Phi}(C_{i},C_{l})$ and $\tilde{P}_{\Phi}(C_{l},C_{j})$ . The key now, is to observe that 

$$
{\tilde{P}}_{\Phi}\models(C_{i}\ \bot\ C_{j}\ |\ C_{l}).
$$ 

Thus, we can compute 

$$
\tilde{P}_{\Phi}(C_{i},C_{j})=\sum_{C_{l}-C_{j}}\tilde{P}_{\Phi}(C_{i},C_{l})\tilde{P}_{\Phi}(C_{j}\mid C_{l}),
$$ 

where $\tilde{P}_{\Phi}(C_{j}\mid C_{l})$ can be easily computed from the marginal $\tilde{P}_{\Phi}(C_{j},C_{l})$ . 

The cost of this computation is signiﬁcantly lower than that of running variable elimination in the clique tree $\textstyle{\binom{n}{2}}$ times (see exercise 10.15). 

# 10.4 Constructing a Clique Tree 

So far, we have assumed that a clique tree is given to us. How do we construct a clique tree for a set of factors, or, equivalently, for its underlying undirected graph $\mathcal{H}_{\Phi}\mathrm{'}$ ? There are two basic approaches, the ﬁrst based on variable elimination and the second on direct graph manipulation. 

# 10.4.1 Clique Trees from Variable Elimination 

The ﬁrst approach is based on variable elimination. As we discussed in section 10.1.1, the execution of a variable elimination algorithm can be associated with a cluster graph: A cluster $C_{i}$ corresponds to the factor $\psi_{i}$ generated during the execution of the algorithm, and an undirected edge connects $C_{i}$ and $C_{j}$ when $\tau_{i}$ is used (directly) in the computation of $\psi_{j}$ (or vice versa). As we showed in section 10.1.1, this cluster graph is a tree, and it satisﬁes the running intersection property; hence, it is a clique tree. 

As we showed in theorem 9.6, each factor in an execution of variable elimination with the ordering $\prec$ is a subset of a clique in the induced graph $\mathcal{L}_{\Phi,-}$ . Furthermore, every maximal clique is a factor in the computation. Based on this result, we can conclude that, in the clique tree $\mathcal{T}$ induced by variable elimination using the ordering $\prec_{!}$ , ach clique is also a clique in the induced graph $\mathcal{T}_{\Phi,-}$ , and each clique in $\mathcal{T}_{\Phi,\prec}$ is a clique in T $\mathcal{T}$ . This equivalence is the reason for the use of term clique in this context. 

In the context of clique tree inference, it is standard to reduce the tree to contain only clusters that are (maximal) cliques in $\mathcal{T}_{\Phi,-}$ . Sp ﬁcally, we eliminate from the tree a cluster $C_{j}$ which is a strict subset of some other cluster $C_{i}$ : 

![](images/45e4a37e2e3d52934743b6ffa97eddf23871e4cfb6c27670f2dc98265e6e5c06.jpg) 
Figure 10.8 A modiﬁed Student BN with an unambitious student 

![](images/da811aef1b5487d9c286d2bdb061fd79bcec50ce26a9e680721407f62db3e3dc.jpg) 
Figure 10.9 A clique tree for the modiﬁed Student BN of ﬁgure 10.8 

Let $\tau$ be a clique tree for a set of factors $\Phi$ . Then there exists a clique tree $\mathcal{T}^{\prime}$ such that: 

• each clique in $\mathcal{T}^{\prime}$ is also a clique in $\tau$ ; • there is no pair of cliques $C_{i},C_{j}$ in $\mathcal{T}^{\prime}$ such that $C_{j}\subset C_{i}$ . 

Proof The proof is constructive, eliminating redundant cliques one by one. We begin with $T^{\prime}=T$ . $C_{j},C_{i}$ be a pair of cliques in $\mathcal{T}^{\prime}$ such that $C_{j}\subset C_{i}$ . By t unnin tersection property, $C_{j}$ is a subset of all cliques on the path between $C_{j}$ and $C_{i}$ . Let $C_{l}$ be some neighbor clique of $C_{j}$ s that $C_{j}\subseteq C_{l}$ . We simply remo $C_{j}$ from the tree, and connect all of the neighbors of $C_{j}$ , except for $C_{l}$ itself, directly to $C_{l}$ l . The proof that the resulting structure is a valid clique tree is not diﬃcult, and it is left as an exercise (exercise 10.16). 

As each of the clique elimination steps reduces the number of cliques in the tree, the process must terminate. When it does, we have a valid clique tree that does not contain a pair of cliques where one subsumes the other. 

The reduction used in this theorem is precisely the one we used to transform the tree in ﬁgure 10.1 to the one in ﬁgure 10.2. Consider also the following slightly more complex example (one that does not result in a clique tree that has the form of a chain): 

Example 10.10 Assume that our student plans to be a beach bum upon graduation, so his happiness does not depend on getting a job. On the other hand, his happiness still depends on his grade. The network is shown in ﬁgure 10.8. Variable elimination with the ordering $J,L,S,H,C,D,I,G,$ , followed by a pruning of the nonmaximal clusters from the tree (as in theorem 10.6), we obtain the clique tree shown in ﬁgure 10.9. 

# 10.4.2 Clique Trees from Chordal Graphs 

Theorem 10.6 shows that there exists a clique tree for $\Phi$ whose cliques are precisely the maximal cliques in $\mathcal{T}_{\Phi,-}$ . This observation leads us to an alternative approach to constructing a clique tree. As we discussed in section 9.4.3.1, the induced graph $\mathcal{L}_{\prec,\Phi}$ is necessarily a chordal graph. In fact, the converse also holds: any chordal graph can be used as the basis for inference. To see that, recall that theorem 4.12 states that any chordal graph is associated with a clique tree. The algorithms presented in this chapter show that any clique tree can be used for inference. Thus, for any set of factors $\Phi$ , we can construct a clique tree for inference over $\Phi$ by constructing a chordal graph $\mathcal{H}^{*}$ that contains the edges in ${\mathcal{H}}_{\Phi}$ , ﬁnding the maximal cliques in it, and connecting them appropriately. We now discuss each of these steps. 

triangulation 

The process of constructing a chordal graph that subsumes an existing graph $\mathcal{H}$ is called triangulation . Not surprisingly, ﬁnding a minimum triangulation, that is, one where the largest clique in the resulting chordal graph has minimum size, is $\mathcal{N P}$ -hard. There are exact algorithms for ﬁnding an optimal triangulation of a graph, which are exponential in the size of the largest clique in the graph. In practice, these algorithms are too expensive, and one typically resorts to heuristic algorithms. Other triangulation methods provide a guarantee that the size of the largest clique is within a certain constant factor of optimal. These algorithms are less expensive, but they are still typically too costly in most applications. In practice, the most common approach to triangulation in graphical models uses the node-elimination techniques that we discussed in section 9.4.3.2. 

Given a chordal graph $\mathcal{H}$ , we now must ﬁnd the imal cliques in the graph. In general, ﬁnding the maximal cliques in a graph is also an NP -hard problem. However, for chordal graphs the problem is quite easy. There are several methods. One of the simplest is to run maximum cardinality search on the resulting chordal graph and collect the maximal cliques generated in the process. By theorem 9.10, this process introduces no ﬁll edges. It follows from theorem 9.6 that this process therefore generates all of the maximal cliques in $\mathcal{H}$ . Another method is to begin with a family, each member of which is guaranteed to be a clique, and then use a greedy algorithm that adds nodes to the clique until it no longer induces a fully connected subgraph. This algorithm can be performed efciently, because the number of maximal cliques in a chordal graph is at most $n$ . 

maximum spanning tree 

Finally, we must determine the edges in the clique tree. Again, one approach for achieving this task is maximum cardinality search, which also dictates which clique transmits information to which other clique. A somewhat more efcient approach that achieves the same efect is via a maximum spanning tree algorithm. More speciﬁcally, we build an undirected graph whose nodes are the maximal cliques in $\mathcal{H}$ , and where every pair of nodes $C_{i},C_{j}$ is connected by an edge whose weight is $|C_{i}\cap C_{j}|$ . We then use a standard algorithm to ﬁnd a tree in this graph whose weight — that is, the sum of the weights of the edges in the graph — is maximal. 

![](images/fdf5da8d6ccb2e9383d234fdc58ac50482dbebfd31e67d62920a5ee170b90372.jpg) 
Figure 10.10 Example of a clique-tree construction algorithm: (a) Undirected factor graph (moralized graph for the network of ﬁgure 10.8). (b) One possible triangulation for this graph. (c) Cluster graph with edge weights where clusters are the cliques. 

One such algorithm is shown in algorithm A.2. We can show that this approach results in the same structure as the one constructed by maximal cardinality search, and therefore it satisﬁes the running intersection property (see exercise 10.17). 

To summarize, we can construct a clique tree as follows: 

1. Given a set of factors, construct the undirected graph ${\mathcal{H}}_{\Phi}$ .

 2. Triangulate ${\mathcal{H}}_{\Phi}$ to construct a chordal graph $\mathcal{H}^{*}$ .

 3. Find cliques in ${\mathcal{H}}^{*}$ , and make each one a node in a cluster graph.

 4. Run the maximum spanning tree algorithm on the cluster graph to construct a tree. ﬁgure 10.10b, is obtained by introducing the ﬁll edge $G{-}S$ . This triangulation is a minimal one, as is the one that introduces the edge $L,I$ . There are also, of course, other nonminimal triangulations. The graph of the maximal cliques for the chordal graph of ﬁgure 10.10b, with the associated weights on the edges, is shown in ﬁgure 10.10c. It is easy to verify that the maximum weight spanning tree is the clique tree shown in ﬁgure 10.9. 

Once a clique tree is constructed, it can be used for inference using either of the two message passing algorithms described earlier. 

# 10.5 Summary 

In this chapter, we have described a somewhat diferent perspective on the basic task of exact inference. This approach uses a preconstructed clique tree as a data structure for exact inference. Messages are passed between the cliques in the clique tree, with the end result that the cliques are calibrated — all cliques agree on the same marginal beliefs of any variable they share. We showed two diferent approaches to message passing in clique trees. The ﬁrst uses the same operations as variable elimination, using dynamic programming to cache messages in order to avoid repeated computation. The second uses belief propagation messages, which propagate marginal beliefs between cliques in an attempt to make them agree with each other. Both approaches allow calibration of the entire clique tree within two passes over the tree. 

It is instructive to compare the standard variable elimination algorithm of chapter 9 and the algorithm obtained by variable elimination in a clique tree. In principle, they are equivalent, in that they both use the same basic operations of multiplying factors and summing out variables. Furthermore, the cliques in the clique tree are basically the factors in variable elimination. Thus, we can use any variable elimination algorithm to ﬁnd a clique tree, and any clique tree to deﬁne an elimination ordering. It follows that the two approaches have basically the same computational complexity. 

In practice, however, the two algorithms ofer diferent trade-ofs. On one hand, clique trees have several advantages. Most importantly, through the use of dynamic programming, the  clique tree provides answers to multiple cliques using a single computation. Additional layers of dynamic programming allow the same data structure to answer an even broader range of queries, and to dynamically introduce and retract evidence. Moreover, the clique tree approach executes a nontrivial number of the required operations in advance, including the construction of the basic data structures, the choice of elimination ordering (which is almost determined), and the product of the CPDs assigned to a single clique. 

On the other hand, clique trees, as typically implemented, also have disadvantages. First, clique trees are more expensive in terms of space. In a clique tree, we keep all intermediate factors, whereas in variable elimination we can throw them out. If there are $c$ cliques, the cost of the clique tree algorithm can be as much as $2c$ times as expensive. More importantly, in a clique tree, the structure of the computation is ﬁxed and predetermined. We therefore have less ﬂexibility to take advantage of computational efciencies that arise because of speciﬁc features of the evidence and query. For example, in the Student network with evidence $i^{1}$ , the variable elimination algorithm could avoid introducing a dependence between $G$ and $S$ , resulting in substantially smaller factors. In the clique tree algorithm, the clique structure is usually predetermined, precluding these online optimizations. The diference in cost can be quite dramatic in situations where there is a lot of evidence. This type of situation-speciﬁc simpliﬁcation occurs even more often in networks that exhibit context-speciﬁc independence. Finally, in standard implementaitons, the cliques in a clique tree are typically the maximal cliques in a triangulated graph. Furthermore, the operations performed in the clique tree computation are typically implemented in a fairly standard way, where the incoming messages are multiplied with the clique beliefs, and the outgoing message is generated. This approach is not always optimal (see exercise 10.7). 

We can modify each of these algorithms to have some of the advantages of the other. For example, we can choose to deﬁne a clique tree online, after the evidence is obtained. In this case, the clique tree structure can take advantage of simpliﬁcations resulting from the evidence. However, we lose the advantage of precomputing the clique tree ofine. As another example, we can store intermediate results in a variable elimination execution, and then do a downward pass to obtain the marginal posteriors of all variables. Here, we gain the advantage of reusing computation, at the cost of additional space. In general, we can view these two algorithms as two examples in a space of variable elimination algorithms. There are many other variants that make somewhat diferent trade-ofs, but, fundamentally, they are performing essentially equivalent computations. 

# 10.6 Relevant Literature 

The content of this chapter is closely related to that of chapter 9; hence, many of the citations in section 9.8 are highly relevant to the discussion in this chapter, and the reader is encouraged to explore those as well. 

Hugin 

Following the lines of the polytree algorithm, Pearl (1988) proposed a simple approach that clustered nodes so as to produce a polytree; this approach, however, produced very inefcient trees. The sum-product message passing algorithm in clique trees was developed by Shenoy and Shafer (1990); Shafer and Shenoy (1990), who described it in a much broader form that applies to many factored models other than probabilistic graphical models. The sum-product-divide approach was developed in parallel, in a series of papers by Lauritzen and Spiegelhalter (1988) and Jensen, Olesen, and Andersen (1990). This line of work also generated the perspective of message passing operations as performing a re parameter iz ation of the original distribution, an intuition that has been very inﬂuential in some of the work of chapter 11. The sum-product- divide algorithm formed the basis for the Hugin Bayesian network system, described by Andersen, Olesen, Jensen, and Jensen (1989), leading to the common use of the name “ Hugin algorithm” for this method. 

Some simple modiﬁcations to the clique tree algorithm can greatly improve its efciency. For example, Kjaerulf (1997) describes a method for improving the in-clique computations by using a nested clique tree data structure. Jensen (1995) provides a method for efciently doing incremental update and retraction of evidence in a clique tree. Park and Darwiche (2004b) provide a derivation of the clique tree algorithm in terms of gradients of the network polynomial (see box 9.A). This approach can also be used as the basis for incremental update and evidence retraction. 

Clique trees and their variants have been extended in many ways, and used for many tasks other than simple probabilistic inference. A complete survey is outside the scope of this book, but some of these applications and generalizations are mentioned in later chapters. 

# 10.7 Exercises 

Exercise 10.1 Prove proposition 10.1. 

Exercise 10.2 Prove theorem 10.2. 

# Exercise 10.3 

factor marginalization Show how to perform efcient index computations for factor marginalization along the lines discussed in box 10.A. 

# Exercise 10.4 

Prove theorem 10.5. 

# Exercise $10.5\star$ 

Let $\mathcal{T}$ be a clique tree an $C_{r}$ be a root. Let $C_{j}$ be a clique in the tree and $C_{i}$ its upward neighb Let $\beta_{j}$ be the potential at $C_{j}$ after the upward pass of CTree-SP-Upward (algorithm 10.1). Show that $\beta_{j}$ represents the correct conditional probability $\tilde{P}_{\Phi}(\bar{\mathbf{\calC}}_{j}\mid\mathbf{\calS}_{i,j})$ | . In other words, letting $X=C_{j}-S_{i,j}$ and $\bar{\boldsymbol{S}}=\boldsymbol{S}_{i,j}$ , we have that: 

$$
{\frac{\beta_{j}(X,S)}{\beta_{j}(S)}}={\tilde{P}}_{\Phi}(X\mid S).
$$ 

# Exercise $10.6\star$ 

Assume that w have constructed a lique tree $\mathcal{T}$ for a given Bayesian network graph $\mathcal{G}$ , and that each of the cliques in T contains at m t k nodes. Now, the user decides to add a single edge to the Bayesian network, resulting in a network G ${\mathcal{G}}^{\prime}$ . (The edge can be added between any pair of nodes in the network, so long as it maintains acyclicity.) What is the tightest bound you can provide on the maximum clique size in a clique tree $\mathcal{T}^{\prime}$ for $\mathcal{G}^{\prime}?$ Justify your response by plaining how to construct such a clique tree. (Note: you can construct, using only the fact that You do not need to provide the optimal cli T $\bar{\mathcal T}$ is a clique tree for e tree T $\mathcal{T}^{\prime}$ . The que G .) on asks for the tightest clique tree that 

# Exercise 10.7 

The algorithm for performing variable elimination in a clique tree (algorithm 10.1 and algorithm 10.2) speciﬁes a particular approach for sending a variable elimination message: First (in a preprocessing step), the initial clique potentials are generated by multiplying all the factors assigned to a clique. Then, in the message passing operation ( SP-Message ), the initial potential is multiplied by all of the incoming messages, and the variables not on the sepset are summed out. 

Is this the most computationally efcient procedure for executing the message passing step? Either explain why or provide a counterexample. 

# Exercise $10.8\star$ 

network polynomial Consider again the network polynomial construction of box 9.A and the algorithm of exercise 9.6 for efciently computing all the polynomial’s derivatives. Show that this algorithm provides an alternative derivation of the up/down clique-tree calibration procedure for sum-product clique trees. In other words, ﬁnd a correspondence between the computation of the partial derivatives in exercise 9.6 and the message passing operations in the clique tree algorithm. 

# Exercise ${\bf10.9\star\star}$ 

rule-based clique tree 

incremental update 

Use your answer to exercise 10.8 to come up with a rule-based clique tree algorithm, based on the rule- based variable elimination procedure of section 9.6.2.1. Your algorithm should compute in a single upward- downward pass all of the marginal probabilities for all variables in the network; its complexity should be twice the cost of the simple rule-based variable elimination of section 9.6.2.1. 

# Exercise $10.10\star$ 

Let $\mathcal{T}$ be a calibrated clique tree representing the unnormalized distribution $\begin{array}{r}{\tilde{P}_{\Phi}=\prod_{\phi\in\Phi}\phi}\end{array}$ . Let $\phi^{\prime}$ be a ∈ new factor, and let $\tilde{P}_{\Phi}^{\prime}=\tilde{P}_{\Phi}\cdot\phi^{\prime}$ · . Let $C_{i}$ me clique such that $S c o p e[\phi]\subseteq C_{i}$ ⊆ Show at we can perform an incremental update to obtain $\tilde{P}_{\Phi}^{\prime}(C_{j})$ for any clique $C_{j}$ by multiplying $\phi^{\prime}$ into $\beta_{i}$ and then propagating messages from $C_{i}$ to $C_{j}$ along the path between them. 

# Exercise $10.11\star$ 

Consider the problem of eliminating extraneous variables from a clique tree. More precisely, given a calibra e tree $\mathcal{T}$ over $\mathcal{X}$ , we want t enerate a (calibrated) clique tree $\mathcal{T}^{\prime}$ whose scope is some subset $Y\subset{\mathcal{X}}$ ⊂X . Clearly, we want to make T $\mathcal{T}^{\breve{\prime}}$ as small as possible (in terms of the size of the resulting cliques). However, we do not want to construct and calibrate a clique tree from scratch; rather, we want to reuse our previous computation. 

a. Suppose $\mathcal{T}$ consists tw $C_{1}$ and $C_{2}$ over variables $\{A,B,C\}$ and $\{C,D\}$ , respectively. What is the resulting T $\mathcal{T}^{\prime}$ if $Y=\{B,C\}$ { } ? b. For the clique tree $\mathcal{T}$ deﬁned in part 1, what is $\mathcal{T}^{\prime}$ if $\boldsymbol{Y}=\{B,D\}^{\mathrm{\scriptscriptstyleT}}$ ? c. Now consider an arbitrary que tree $\mathcal{T}$ over $\mathcal{X}$ an n arb ary $Y\subseteq\mathcal{X}$ . Provide an algorithm to transform a calibrated tree T into a calibrated tree T $\mathcal{T}^{\prime}$ over Y . Your algorithm should not resort to manipulating the underlying network or factors; all operations should be performed directly on the clique tree. 

$\mathcal{T}$ 

# Exercise $10.12\star\star$ 

Let $\mathcal{T}$ be a clique tree over $\mathcal{X}$ , deﬁned via a set of initial factors $\Phi$ . Let $\pmb{Y}\,=\,\pmb{y}$ be some observed assignment to a subset of the variables. Consider a setting where we might be unsure about a particular observation $Y_{i}=y_{i}^{j}$ for $Y_{i}\in Y$ , and that we want ct on the unnormalized probability of the other possible values $y_{i}^{k}$ . More precisely, let $Y_{-i}=Y-\{Y_{i}\}$ −{ } and $\pmb{y}_{-i}$ be the assignment in $_{_y}$ − to $Y_{-i}$ . We want to compute $\tilde{P}_{\Phi}(y_{i}^{k},Y_{-i}\,=\,\pmb{y}_{-i})$ ) for every $Y_{i}$ and every $y_{i}^{k}$ . Describe a variant of − sum-product message passing (algorithm 10.2) that can perform this task without requiring more messages than the standard two-pass calibration. (Hint: Rather than reducing the factors prior to message passing, consider reducing factors during the message passing process.) 

# Exercise 10.13 

Provide a simple method for constructing a clique tree such that a given set of variables $\mathbf{Y}$ is guaranteed to be to together in some clique. Your algorithm should use a standard clique-tree construction algorithm as a black box. 

# Exercise 10.14 

Assume that we have a c ue tre $\mathcal{T}$ over $\mathcal{X}$ s h that, for every pair of nodes $X,Y\in{\mathcal{X}}$ , there ists a clique that contains both X and Y . Prove that must contain a single clique that contains all of . 

# Exercise $10.15\star$ 

Consider the task of using a calibrated clique tree $\mathcal{T}$ over $\Phi$ to compute all of the pairwise marginals of vari- ables, ${\tilde{P}}_{\Phi}(X,Y)$ for all $X,Y$ . Assume that our probabilistic network consists of a chain $X_{1}{-}X_{2}{\mathrm{-}}\ldots{\mathrm{-}}X_{n}$ , and that our ue tr m $1\!-.\ldots\!-\!n-1$ where $S c o p e[C_{i}]=\{X_{i},X_{i+1}\}$ . Also assume that each variable $X_{i}^{-}$ has $|\mathit{V a l}(X_{i})|=d$ . 

a. What is the total cost (number of multiplication steps and number of addition steps) of doing variable elimination over this chain-structured clique tree, as described in the algorithm of algorithm 10.4, for all $\textstyle{\binom{n}{2}}$  variable pairs? b. What is the total cost of the algorithm described in section 10.3.3.3? Provide a precise expression, not merely an asymptotic upper bound. c. Since we are computing marginals for all variable pairs, we may store any computations done for the previous pairs and use them to save time for the remaining pairs. Construct a dynamic programming algorithm for this speciﬁc chain structure that reduces the complexity of this task by a factor of $d^{2}$ over the algorithm described in section 10.3.3.3? Explain why this approach idea would not work for a general clique tree. 

# Exercise 10.16 

Complete the proof of theorem 10.6 by showing that, if we have a valid clique tree, then a clique elimination step, as described, results in a valid clique tree. In particular, show that it is a tree, that it satisﬁes the family preservation property, and that it satisﬁes the running intersection property. 

# Exercise $10.17\star$ 

Show that the clique tree constructed using the maximum-weight-spanning tree procedure of section 10.4.2 satisﬁes the running intersection property. (Hint: Show that this tree structure can also be constructed using the maximum-cardinality algorithm.) 

# 11 Inference as Optimization 

# 11.1 Introduction 

In the previous chapters we examined exact inference. We have seen that for many networks we can perform exact inference efciently. As we have seen, the computational and space complexity of the clique tree is exponential in the tree-width of the network. This means that the exact algorithms we examined become infeasible for networks with a large tree-width. In many real-life applications, we encounter such networks. This motivates examination of approximate inference methods that are applicable to networks where exact inference is intractable. 

In this chapter we consider a class of approximate inference methods, where the ap- proximation arises from constructing an approximation to the target distribution $P_{\Phi}$ . This approximation takes a simpler form that allows for inference. In general, the sim- pler approximating form exploits a local factorization structure that is similar in nature to the structure exploited by graphical models. 

The speciﬁc algorithms we consider difer in many details, and yet they share some common conceptual principles. We now review these principles to provide a common framework for the remaining presentation. In each method, we deﬁne a target class $\mathcal{Q}$ of “easy” dis utions $Q$ and then search for an instance within that class that is the “best” approximation to $P_{\Phi}$ . Queries can then be answered using inference on $Q$ rather than on $P_{\Phi}$ . All of the methods we describe optimize (roughly) the same target function for measuring the similarity between $Q$ and $P_{\Phi}$ . 

constrained optimization 

This approach reformulates the inference task as one of optimizing an objective function over the class $\mathcal{Q}$ . This problem falls into the category of constrained optimization . Such problems can be solved using a variety of diferent methods. Thus, the formulation of inference from this perspective opens the door to the application of a range of techniques developed in the optimization literature. Currently, the technique most often used in the setting of graphical models is one based on the use of Lagrange multipliers , which we review in appendix A.5.3. This method produce a set of equations that characterize the optima of the objective. In our setting, this characterization takes the form of a set of ﬁxed-point equations that deﬁne each variable in terms of others. A particularly compelling and elegant result is that the ﬁxed-point equations derived from the constrained energy optimization, for any of the methods we describe, can be viewed as passing messages over a graph object. Indeed, as we will show, even the standard sum-product algorithm for clique trees (algorithm 10.2) can be rederived from this perspective. Moreover, many other message passing algorithms follow from the same derivation. Methods in this class fall into three main categories. The ﬁrst category includes methods that use clique-tree message passing schemes on structures other than trees. This class of methods, which includes the famous loopy belief propagation algorithm, can be understood as optimizing approximate versions of the energy functional. The second category includes methods that use message propagation on clique trees with approximate messages. This class of methods, often known as the expectation propagation algorithm, maximize the exact energy functional, but with relaxed consistency constraints on the representation $Q$ . Finally, in the third category there are methods that generalize the mean ﬁeld method originating in statistical physics. These methods e the exact energy functional, but they restrict attention to a class $\mathcal{Q}$ consisting of distributions $Q$ that have a particular simple factorization. This factorization is chosen to be simple enough to ensure that we can perform inference with $Q$ . 

More broadly, each of these algorithms can be described from two perspectives: as a proce- dural description of a message passing algorithm, or as an optimization problem consisting of an objective and a constraint space. Historically, the message passing algorithm generally origi- nated ﬁrst, sometimes long before the optimization interpretation was understood. However, the optimization perspective provides a much deeper understanding of these methods, and it shows that message passing is only one way of performing the optimization; it also helps point the way toward useful generalizations. In the ensuing discussion, we usually begin the presentation of each class of methods by describing a simple variant of the algorithm, providing a concrete manifestation to ground the concepts. We then present the optimization perspective on the algorithm, allowing a deeper understanding of the algorithm. Finally, we discuss generalizations of the simple algorithm, often ones that are derived directly from the optimization perspective. 

# 11.1.1 Exact Inference Revisited $\star$ 

Before considering approximate inference methods, we start by casting exact inference as an optimization problem. The concepts we introduce here will serve in the discussion of the following approximate inference methods. 

Assume we have a factorized distribution of the form 

$$
P_{\Phi}(\mathcal{X})=\frac{1}{Z}\prod_{\phi\in\Phi}\phi(U_{\phi}),
$$ 

where the factors $\phi$ in $\Phi$ comprise the distribution, and the variables $U_{\phi}\,=\,S c o p e[\phi]\,\subseteq\,\mathcal{X}$ are the scope of each factor. For example, the factors might be CPDs in a Bayesian network, generally restricted by an evidence set, or they might be potentials in a Markov network. We are interested in answering queries about the distribution $P_{\Phi}$ . These include queries about marginal probabilities of variables and queries about the partition function $Z$ . As we discussed, if $P_{\Phi}$ is a Bayesian network with instantiated evidence on some variables, then the partition function $Z$ is the probability of the evidence. 

Recall that the end product of belief propagation is a calibrated cluster tree. Also recall that a calibrated set of beliefs for the cluster tree represents a distribution. In exact inference we ﬁnd a set of calibrated beliefs that represent $P_{\Phi}(\mathcal X)$ . That is, we ﬁnd beliefs that match the distribution represented by given set of initial potentials. Thus, we can view exact inference as searching over the set of d ibutions $\mathcal{Q}$ that are representable by the cluster tree to ﬁnd a distribution $Q^{*}$ that matches $P_{\Phi}$ . 

Intuitively, we can rephrase this question as searching for a calibrated distribution that is as close as possible to $P_{\Phi}$ . There are many possible ways of measuring the distance between two distributions, such as the Euclidean distance $\mathrm{(L_{2})}$ , or the $\mathrm{L_{1}}$ distance and the related variational distance (see appendix A.1.3.3). As we will see, our main challenge, however, is our aim to avoid performing inference with the distribution $P_{\Phi}$ ; in particular, we cannot efectively compute marginal distributions in $P_{\Phi}$ . Hence, we need methods that allow us to optimize the distance between $Q$ and $P_{\Phi}$ without answering hard queries about $P_{\Phi}$ . A priori, this requirement may seem impossible to satisfy. However, it turns out that there exists a distance measure — the relative entropy (or KL-divergence) — that allows us to exploit the structure of $P_{\Phi}$ without performing reasoning with it. 

Recall that the relative entropy between $P_{1}$ and $P_{2}$ is deﬁned as 1 

$$
D(P_{1}\|P_{2})=E_{P_{1}}\biggl[\ln\frac{P_{1}(\mathcal{X})}{P_{2}(\mathcal{X})}\biggr].
$$ 

Also recall that the relative entropy is always nonnegative, and equal to 0 if and only if $P_{1}=P_{2}$ . Thus, we can use it as a distance measure, and choose to ﬁnd an approximation $Q$ to $P_{\Phi}$ that minimizes the relative entropy. 

M-projection I-projection 

However, as we discussed, the relative entropy is not symmetric $-\;D(P_{1}\|P_{2})\neq D(P_{2}\|P_{1})$ | | ̸ | | . In section 8.5, we discussed the use of relative entropy for projecting a distribution into a restricted class; this projection can aim to minimize either $D(P_{\Phi}\|Q)$ | | , via the $M\cdot$ -projection , or $D(Q\|P_{\Phi})$ | | , via the I-projection . A priori, it might appear that the M-projection is more appropriate, since one of the main information-theoretic justiﬁcations for the relative entropy $D(P_{\Phi}\|Q)$ | | is the numb of bits lost when coding a true message distribution $P_{\Phi}$ using an (approximate) estimate Q . However, as the discussion of section 8.5.2 shows, computing the M-projection $Q\,-\,\arg\operatorname*{min}_{Q}D(P_{\Phi}\|Q)\,-$ | | — requires that we compute marginals of $P_{\Phi}$ and is therefore equivalent to running inference in $P_{\Phi}$ . Somewhat surprisingly, as we show in the subsequent discussion, this does not apply to I-projection: we can exploit the structure of $P_{\Phi}$ to optimize arg min Q $D(Q\|P_{\Phi})$ efciently, without running inference in $P_{\Phi}$ 

To summarize this discussion, we want to search for a distribution Q that minimizes $D(Q\|P_{\Phi})$ | | . To deﬁne and analyze this optimization problem formally, we also eed spec- ify he objects we optimize over. Suppose we are given a clique tree structure T $\mathcal{T}$ for $P_{\Phi}$ ; that is, T $\mathcal{T}$ satisﬁes the running intersection property and the family preservation property. Moreover, suppose we are given a set of beliefs 

$$
Q=\{\beta_{i}:i\in\mathcal{V}_{\mathcal{T}}\}\cup\{\mu_{i,j}:(i\mathrm{-}j)\in\mathcal{E}_{\mathcal{T}},\}
$$ 

where $C_{i}$ d notes clusters in $\mathcal{T},\,\beta_{i}$ denotes beliefs over $C_{i}$ , and $\mu_{i,j}$ denotes beliefs over $\boldsymbol{S}_{i,j}$ of edges in . 

As in deﬁnition 10.6, the set of beliefs in $\mathcal{T}$ deﬁnes a distribution $Q$ by the formula 

$$
Q(\mathcal X)=\frac{\prod_{i\in\mathcal V_{T}}\beta_{i}}{\prod_{(i-j)\in\mathcal E_{T}}\mu_{i,j}}.
$$ 

calibration marginal consistency (See section 10.2.3.) Due to the calibration requirement, the set of beliefs $Q$ satisﬁes the marginal consistency constraints if, for each $(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}$ , the beliefs on $\boldsymbol{S}_{i,j}$ are the mar nal of $\beta_{i}$ (and $\beta_{j})$ . Recall that theorem 10.4 shows that if $Q$ is a set of calibrated beliefs for T $\mathcal{T}$ and $Q$ is the distribution deﬁned by equation (11.2), then 

$$
\begin{array}{r c l}{{\beta_{i}[{\pmb c}_{i}]}}&{{=}}&{{Q({\pmb c}_{i})}}\\ {{\mu_{i,j}[{\pmb s}_{i,j}]}}&{{=}}&{{Q({\pmb s}_{i,j}).}}\end{array}
$$ 

Thus, the beliefs correspond to marginals of the distribution $Q$ deﬁned by equation (11.2). 

Thus, we are now searching over a set of distributions $Q$ that are representable by a set of beliefs $Q$ over the cliques and sepsets in a particular clique tree structure $\mathcal{T}$ . Note that when deciding on the representation of $Q$ we are actually making two decisions: We are deciding both on the space of distributions that we are considering (all distributions for which $\mathcal{T}$ is an I-map), and on the representation of these distributions (as a set of calibrated clique beliefs). Both of these decisions are signiﬁcant components in the speciﬁcation of our optimization problem. 

With these deﬁnitions in hand, we can now view exact inference as maximizing $-D(Q\|P_{\Phi})$ | | over the space of calibrated sets $Q$ . 

$$
\begin{array}{r l}&{Q=\{\beta_{i}:i\in\mathcal{V}_{\mathcal{T}}\}\cup\{\mu_{i,j}:(i\mathrm{-}j)\in\mathcal{E}_{\mathcal{T}}\}}\\ &{-D(Q\|P_{\Phi})}\end{array}
$$ 

$$
\begin{array}{r c l}{\mu_{i,j}[\pmb{s}_{i,j}]}&{=}&{\displaystyle\sum_{\pmb{C}_{i}-\pmb{S}_{i,j}}\beta_{i}(\pmb{c}_{i})\quad\forall(i\!-\!j)\in\mathcal{E}_{T},\forall\pmb{s}_{i,j}\in V a l(S_{i,j})}\\ {\displaystyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})}&{=}&{1\qquad\forall i\in\mathcal{V}_{T}.}\end{array}
$$ 

In solving this optimization problem, we conceptually examine diferent conﬁgurations of beliefs that satisfy the marginal consistency constraints, and we select the conﬁguration that maximizes the objective. Such an exhaustive examination, of course, is impossible to perform in practice. However, there are efective solutions to this problem that ﬁnd the maximum point. We have already seen that, if $\mathcal{T}$ is a proper cluster tree for the set of original potentials $\Phi$ , we know that there is a set $Q$ that induces, via equation (11.2), a distribution $Q=P_{\Phi}$ . Because this solution achieves a relative entropy of 0 , which is the highest value possible, it is the unique global optimum of this optimization. 

This optimum can be found using the exact inference algorithms we developed in chapter 10. 

# 11.1.2 The Energy Functional 

The preceding discussion suggests a strategy for constructing approximations of $P_{\Phi}$ . Instead of searching over the space of all calibrated cluster trees, we can search over a space of “simpler” distributions. In this search we will not ﬁnd a distribution equivalent to $P_{\Phi}$ , yet we might ﬁnd one that is reasonably close to $P_{\Phi}$ . Moreover, as part of the design of the target set of distributions, we can ensure that these distributions are ones in which we can perform inference efciently. 

One problem that we will face is that the target of the optimization $D(Q\|P_{\Phi})$ | | is unwieldy for direct optimization. The relative entropy term contains an explicit summation over all possible instantiations of $\mathcal{X}$ , an operation that is infeasible in practice. However, since we know the form of $\ln{P_{\Phi}}(\xi)$ from equation (11.1), we can exploit its structure to rewrite the relative entropy in a simpler form, as shown in the following theorem. 

$$
F[\tilde{P}_{\Phi},Q]=E_{Q}\Big[\ln\tilde{P}(\mathcal{X})\Big]+H_{Q}(\mathcal{X})=\sum_{\phi\in\Phi}E_{Q}[\ln\phi]+H_{Q}(\mathcal{X}).
$$ 

Proof 

$$
D(Q\|P_{\Phi})=E_{Q}[\ln Q(\mathcal{X})]-E_{Q}[\ln P_{\Phi}(\mathcal{X})].
$$ 

Using the product form of $P_{\Phi}$ , we have that 

$$
\ln P_{\Phi}(\mathcal{X})=\sum_{\phi\in\Phi}\ln\phi(\pmb{U}_{\phi})-\ln Z.
$$ 

Moreover, recall that $H_{Q}(\mathcal{X})=-E_{Q}[\ln Q(\mathcal{X})]$ X − X . Plugging these into equation (11.4), we get 

$$
\begin{array}{r c l}{{\displaystyle D(Q\|P_{\Phi})}}&{{=}}&{{\displaystyle-H_{Q}({\mathcal X})-E_{Q}\left[\sum_{\phi\in\Phi}\ln\phi({\pmb U}_{\phi})\right]+E_{Q}[\ln Z]}}\\ {{}}&{{=}}&{{\displaystyle-F[\tilde{P}_{\Phi},Q]+\ln Z.}}\end{array}
$$ 

Importantly, the term $\ln{\cal Z}$ does not depend on $Q$ . Hence, minimizing the relative entropy $D(Q\|P_{\Phi})$ | | is equivalent to maximizing the energy functional $F[\tilde{P}_{\Phi},Q]$ . 

free energy 

energy term 

entropy term 

This latter term relates to concepts from statistical physics, and it is the negative of what is referred to in that ﬁeld as the (Helmholtz) free energy . While explaining the physics-based motivation for this term is out of the scope of this book, we continue to use the standard terminology of energy functional. 

The energy functional contains two terms. The ﬁrst, called the energy term , involves expecta- tions of the logarithms of factors in $\Phi$ . Here, each factor in $\Phi$ appears as a separate term. Thus, if the factors that comprise $\Phi$ are small, each expectation deals with relatively few variables. The difculties in dealing with these expectations depends on the properties of the distribution $Q$ . Assuming that inference is “easy” in $Q$ , we should be able to evaluate such expectations relatively easily. The second term, called the entropy term , is the entropy of $Q$ . Again, the choice of $Q$ determines whether we can evaluate this term. However, we will see that, for the choices we make, this term will also be tractable. 

# 11.1.3 Optimizing the Energy Functional 

In the remainder of this chapter, we pose the problem of ﬁnding a good approximation  $Q$ as one of maximizing the energy functional, or, equivalently, minimizing the relative entropy. Importantly, the energy functional involves expectations in $Q$ . As we show, by choosing approximations $Q$ that allow for efcient inference, we can both evaluate the energy functional and optimize it efectively. 

Moreover, since $D(Q\|P_{\Phi})\ge0$ | | ≥ , we have that 

$$
\ln Z\geq F[\tilde{P}_{\Phi},Q].
$$ 

lower bound 

variational method 

That is, the energy functional is a lower bound on the logarithm of the partition function $Z$ , for any choice of $Q$ . Why is this fact signiﬁcant? Recall that, in directed models, the partition function $Z$ is the probability of the evidence. Computing the partition function is often the hardest part of inference. And so, this theorem shows that if we have a good approximation (that is, $D(Q\|P_{\Phi})$ | | is small), then we can get a good lower-bound approximation to $Z$ . The fact that this approximation is a lower bound will play an important role in later chapters on learning. 

In this chapter, we explore inference methods that can be viewed as strategies for optimizing the energy functional. These kinds of methods are often referred to as variational methods . The name refers to a general strategy in which we want to solve a problem by introducing new variational parameters that increase the degrees of freedom over which we optimize. Each choice of these parameters gives an approximate answer. We then attempt to optimize the variational parameters to get the best approximation. In our case, the task is to answer queries about $P_{\Phi}$ , and the variational parameters describe the distribution $Q$ . In the methods we consider, we vary these parameters to try to ﬁnd a good approximation to the target query. 

# 11.2 Exact Inference as Optimization 

Before considering approximate inference methods, we illustrate the the use of a variational approach to rederive an exact inference procedure. The concepts we introduce here will serve in discussion of the following approximate inference methods. 

As we have already seen, the optimization problem CTree-Optimize-KL has a unique solution. We start by reformulating the optimization problem in terms of the energy functional. As we have seen, maximizing the energy functional is equivalent to minimizing the relative entropy between $Q$ and $P_{\Phi}$ . 

Once we restrict attention to calibrated cluster trees, we can further simplify the objective function. More precisely, we can rewrite the energy functional in a factored form as a sum of terms each of which depends directly only on one of the beliefs in $Q$ . This form reveals the structure in the distribution, and it is therefore a much better starting point for further analysis. As we will see, this form is also the basis for our approximations in subsequent sections. 

$$
\tilde{F}[\tilde{P}_{\Phi},Q]=\sum_{i\in\mathcal{V}_{T}}E_{C_{i}\sim\beta_{i}}[\ln\psi_{i}]+\sum_{i\in\mathcal{V}_{T}}H_{\beta_{i}}(C_{i})-\sum_{(i-j)\in\mathcal{E}_{T}}H_{\mu_{i,j}}(S_{i,j}),
$$ 

where $\psi_{i}$ is the initial potential assigned to $C_{i}$ : 

$$
\psi_{i}=\prod_{\phi,\alpha(\phi)=i}\phi,
$$ 

and $E_{C_{i}\sim\beta_{i}}[\cdot]$ · denotes expectation on the value $C_{i}$ given the beliefs $\beta_{i}$ 

Before we prove that the energy functional is equivalent to its factored variant, let us ﬁrst study its components. The ﬁrst term is a sum of terms of the form ${\cal E}_{C_{i}\sim\beta_{i}}[\ln\psi_{i}]$ . Recall that $\psi_{i}$ ∼ is a factor (not necessarily a distribution) over the scope $C_{i}$ , that is, a function from $V a l(C_{i})$ to $I\!R^{+}$ . Its logarithm is therefore a function from $V a l(C_{i})$ to $I\!\!R$ . The beliefs $\beta_{i}$ are a distribution over $V a l(C_{i})$ . We can therefore compute the expectation $\textstyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})\ln\psi_{i}$ . The last two terms are entropies of the beliefs associated with the clusters and sepsets in the tree. The important beneﬁt of this reformulation is that all the terms are local , in the sense that they refer to a speciﬁc belief factor. As we will see, this will make our tasks much simpler. 

If $Q$ is a set of calibrated beliefs for $\mathcal{T}$ , and $Q$ is deﬁned by equation (11.2), then 

$$
\tilde{F}[\tilde{P}_{\Phi},Q]=F[\tilde{P}_{\Phi},Q].
$$ 

Proof Note that $\begin{array}{r}{\ln\psi_{i}=\sum_{\phi,\alpha(\phi)=i}\ln\phi}\end{array}$ . Moreover, since $\beta_{i}(c_{i})=Q(c_{i})$ , we conclude that 

$$
\sum_{i}E_{C_{i}\sim\beta_{i}}[\ln\psi_{i}]=\sum_{\phi}E_{C_{i}\sim Q}[\ln\phi].
$$ 

It remains to show that 

$$
H_{Q}(\mathcal{X})=\sum_{i\in\mathcal{V}_{T}}H_{\beta_{i}}(C_{i})-\sum_{(i-j)\in\mathcal{E}_{T}}H_{\mu_{i,j}}(S_{i,j}).
$$ 

This equality follows directly from equation (11.2) and theorem 10.4. 

Using this form of the energy, we can now deﬁne the optimization problem. We ﬁrst need to deﬁne the space over which we are optimizing. If $Q$ is factorized according to $\mathcal{T}$ , we can represent it by a set of calibrated beliefs. Marginal consistency is a constraint on the beliefs that requires neighboring beliefs to agree on the marginal distribution on their joint subset. It is equivalent to requiring that the beliefs be calibrated. Thus, we pose the following constrained optimization procedure: 

$$
\begin{array}{l l}{{}}&{{Q=\{\beta_{i}:i\in\mathcal{V}_{\mathcal{T}}\}\cup\{\mu_{i,j}:(i\mathrm{-}j)\in\mathcal{E}_{\mathcal{T}}\}}}\\ {{}}&{{\tilde{F}[\tilde{P}_{\Phi},Q]}}\end{array}
$$ 

$$
\begin{array}{r c l}{\mu_{i,j}[\pmb{s}_{i,j}]}&{=}&{\displaystyle\sum_{\pmb{c}_{i}-\pmb{S}_{i,j}}\beta_{i}(\pmb{c}_{i})}\\ &&{\quad\quad\quad\quad\quad\forall(i\!-\!j)\in\mathcal{E}_{\mathcal{T}},\forall\pmb{s}_{i,j}\in V a l(\pmb{S}_{i,j})}\\ {\displaystyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})}&{=}&{1\qquad\quad\forall i\in\mathcal{V}_{\mathcal{T}}}\\ {\beta_{i}(\pmb{c}_{i})}&{\geq}&{0\qquad\quad\forall i\in\mathcal{V}_{\mathcal{T}},\pmb{c}_{i}\in V a l(\pmb{C}_{i}).}\end{array}
$$ 

The constraints equation (11.7), equation (11.8), and equation (11.9) ensure that the beliefs in $Q$ are calibrated and represent legal distributions (exercise 11.2). 

# 11.2.1 Fixed-Point Characterization 

We can now prove that the stationary points of this constrained optimization function — the points at which the gradient is orthogonal to all the constraints — can be characterized by a set of ﬁxed-point equations . As we show, these equations turn out to be the update equations in the sum-product belief-propagation procedure ( CTree-SP-calibrate in algorithm 10.2). Thus, if we turn these equations into an iterative algorithm, as we will describe, we obtain precisely the belief propagation algorithm in clique trees. We note that for this derivation and other similar ones later in the chapter, we restrict attention to models where all of the potentials are strictly positive (contain no zero entries). Although the results generally hold also for the case of deterministic potentials (zero entries), the proofs are considerably more complex and are outside the scope of this book. 

Recall that a stationary point of a function is either a local maximum, a local minimum, or a saddle point. In the optimization problem CTree-Optimize , there is a single global maximum (see theorem 11.1). Although we do not show it here, one can show that it is also the only stationary point (see exercise 11.3), and thus once we ﬁnd a stationary point, we know that we have found the maximum. 

We want to characterize this stationary point by a set of equations that must hold when the choice of beliefs in $Q$ is at the stationary point. Recall that our aim is to maximize the function $\tilde{F}[\tilde{P}_{\Phi},Q]$ under the consistency constraints. The method of Lagrange multipliers , reviewed in appendix A.5.3, provides us with tools for dealing with constrained optimization. Because the characterization of the stationary point is of central importance to later developments, we examine how to construct such a characterization using the method of Lagrange multipliers. 

When using the method of Lagrange multipliers, we start by deﬁning a Lagrangian with a Lagrange multiplier for each of the constraints on the function we want to optimize. In our case, we have the constraints in equation (11.7) and equation (11.8). We note that, in principle, we also need to introduce a Lagrange multiplier for the inequality constraint that ensures that all beliefs are nonnegative. However, as we will see, the assumption that factors are strictly positive implies that the beliefs we construct in the solution to the optimization problem will be nonnegative, and thus we do not need to enforce these constraints actively. We therefore obtain the following Lagrangian: 

$$
\begin{array}{r c l}{\mathcal{I}}&{=}&{\tilde{F}[\tilde{{P}}_{\Phi},Q]}\\ &&{-\displaystyle\sum_{i\in\mathcal{V}_{T}}\lambda_{i}\left(\displaystyle\sum_{c_{i}}\beta_{i}(c_{i})-1\right)}\\ &&{-\displaystyle\sum_{i}\displaystyle\sum_{j\in\mathrm{Nb}_{i}}\displaystyle\sum_{s_{i,j}}\lambda_{j\to i}[s_{i,j}]\left(\displaystyle\sum_{c_{i}\sim s_{i,j}}\beta_{i}(c_{i})-\mu_{i,j}[s_{i,j}]\right),}\end{array}
$$ 

where $\mathrm{Nb}_{i}$ is the neighbors of $C_{i}$ in the clique tree. We introduce Lagrange multipliers $\lambda_{i}$ for each beliefs factor $\beta_{i}$ to ensure that it sums to 1 . We also introduce, for each pair of neighboring cliques $i$ and $j$ and assignment to their sepset $\pmb{s}_{i,j}$ , a Lagrange multiplier $\lambda_{j\rightarrow i}[\pmb{s}_{i,j}]$ to ensure that the marginal distribution of $\pmb{s}_{i,j}$ in $\beta_{j}$ is consistent with its value in the sepset beliefs $\mu_{i,j}$ . (Note that we also introduce another Lagrange multiplier for the direction $i\rightarrow j$ .) 

Remember that $\mathcal{I}$ is a function of the clique beliefs $\{\beta_{i}\}$ , the sepset beliefs $\left\{\mu_{i,j}\right\}$ , and the Lagrange multipliers. To ﬁnd the maximum of the Lagrangian, we take its partial derivatives with respect to $\beta_{i}(\mathbf{\alpha}_{i}),\,\mu_{i,j}[\mathbf{\boldsymbol{s}}_{i,j}]$ , and the Lagrange multipliers. These last derivatives reconstruct the original constraints. The ﬁrst two types of derivatives require some work. Diferentiating the Lagrangian (see exercise 11.1), we get that 

$$
\begin{array}{r c l}{\displaystyle\frac{\partial}{\partial\beta_{i}(\pmb{c}_{i})}\mathcal{I}}&{=}&{\ln\psi_{i}[\pmb{c}_{i}]-\ln\beta_{i}(\pmb{c}_{i})-1-\lambda_{i}-\displaystyle\sum_{j\in\mathrm{Nb}_{i}}\lambda_{j\rightarrow i}[\pmb{s}_{i,j}]}\\ {\displaystyle\frac{\partial}{\partial\mu_{i,j}[\pmb{s}_{i,j}]}\mathcal{I}}&{=}&{\ln\mu_{i,j}[\pmb{s}_{i,j}]+1+\lambda_{i\rightarrow j}[\pmb{s}_{i,j}]+\lambda_{j\rightarrow i}[\pmb{s}_{i,j}].}\end{array}
$$ 

At the stationary point, these derivatives are zero. Equating each derivative to 0 , rearranging terms, and exponentiating, we get 

$$
\begin{array}{r c l}{{\beta_{i}(c_{i})}}&{{=}}&{{\displaystyle\exp\left\{-1-\lambda_{i}\right\}\psi_{i}[c_{i}]\prod_{j\in\ensuremath{\mathrm{Nb}}_{i}}\exp\left\{-\lambda_{j\to i}[\pmb{s}_{i,j}]\right\}}}\\ {{\ }}&{{\ }}&{{\ }}\\ {{\mu_{i,j}[\pmb{s}_{i,j}]}}&{{=}}&{{\displaystyle\exp\left\{-1\right\}\exp\left\{-\lambda_{i\to j}[\pmb{s}_{i,j}]\right\}\exp\left\{-\lambda_{j\to i}[\pmb{s}_{i,j}]\right\}.}}\end{array}
$$ 

These equations describe beliefs as functions of terms of the form $\exp\left\{-\lambda_{i\rightarrow j}[\pmb{s}_{i,j}]\right\}$ . In fact, $\mu_{i,j}$ is a product of two such terms (and a constant). This suggests that these terms play the role of a message $\delta_{i\to j}$ . To make this more explicit, we deﬁne 

$$
\delta_{i\rightarrow j}[\mathscr{s}_{i,j}]=\exp\left\{-\lambda_{i\rightarrow j}[\mathscr{s}_{i,j}]-\frac{1}{2}\right\}.
$$ 

(We add the term $-\frac{1}{2}$ to deal with the additional $\exp\left\{-1\right\}$ term, but since this is a multiplicative constant, it is not that crucial.) We can now rewrite the resulting system of equations as 

$$
\begin{array}{r c l}{{\beta_{i}(c_{i})}}&{{=}}&{{\displaystyle\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}\psi_{i}(c_{i})\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\to i}[{\pmb s}_{i,j}]}}\\ {{}}&{{}}&{{}}\\ {{\mu_{i,j}[{\pmb s}_{i,j}]}}&{{=}}&{{\delta_{i\to j}[{\pmb s}_{i,j}]\delta_{j\to i}[{\pmb s}_{i,j}].}}\end{array}
$$ 

Combining these equations with equation (11.7), we now rewrite the message $\delta_{i\to j}$ as a function of other messages: 

$$
\begin{array}{c c l}{\delta_{i\to j}[\pmb{s}_{i,j}]}&{=}&{\displaystyle\frac{\mu_{i,j}[\pmb{s}_{i,j}]}{\delta_{j\to i}[\pmb{s}_{i,j}]}}\\ &{=}&{\displaystyle\frac{\sum_{\pmb{c}_{i}\sim\pmb{s}_{i,j}}\beta_{i}(\pmb{c}_{i})}{\delta_{j\to i}[\pmb{s}_{i,j}]}}\\ &{=}&{\displaystyle\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}\sum_{\pmb{c}_{i}\sim\pmb{s}_{i,j}}\psi_{i}(\pmb{c}_{i})\prod_{\pmb{k}\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\to i}[\pmb{s}_{i,k}].}\end{array}
$$ 

Note that the term $\begin{array}{r l}{\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}}\end{array}$  − 	 is a constant (since it does not depend on $c_{i}$ ), and when we combine these equations with equation (11.8), we can solve for $\lambda_{i}$ to ensure that this constant normalizes the clique beliefs $\beta_{i}$ . We note that if the original factors deﬁne a distribution that sums to 1 , then the solution for $\lambda_{i}$ that satisﬁes equation (11.8) will be one where $\begin{array}{r}{\lambda_{i}=\frac{1}{2}(|\mathrm{Nb}_{i}|-1)}\end{array}$ | | − , that is, the normalizing constant is 1 . 

A set of beliefs $Q$ is a stationary point of CTree-Optimize if and only if there exists a set of factors $\{\delta_{i\rightarrow j}[S_{i,j}]:(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}\}$ such that 

$$
\delta_{i\rightarrow j}\propto\sum_{C_{i}-S_{i,j}}\psi_{i}\left(\prod_{k\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\rightarrow i}\right)
$$ 

and moreover, we have that 

$$
\begin{array}{r c l}{\beta_{i}}&{\propto}&{\psi_{i}\left(\displaystyle\prod_{j\in\ensuremath{\mathrm{Nb}}_{i}}\delta_{j\to i}\right)}\\ {\mu_{i,j}}&{=}&{\delta_{j\to i}\cdot\delta_{i\to j}.}\end{array}
$$ 

ﬁxed-point equations 

This theorem characterizes the solution of the optimization problem in terms of ﬁxed- point equations that must hold when we ﬁnd a maximal $Q$ . These ﬁxed-point equations deﬁne the relationships that must hold between the diferent parameters involved in the optimization problem. Most importantly, equation (11.10) deﬁnes each message in terms of other messages, allowing an easy iterative approach to solving the ﬁxed point equations. These same themes appear in all the approaches we will discuss later in this chapter. 

# 11.2.2 Inference as Optimization 

The ﬁxed-point characterization of theorem 11.3 focuses on the relationships that hold at the maximum point (or points). However, they also hint at a way of achieving these relationships. Intuitively, a change in $Q$ that reduces the diferences between the left-hand and right-hand side of these equations will get us closer to a maximum point. The most direct way of reducing such discrepancies is to apply the equations as assignments and iteratively apply equations to the current values of the right-hand side to deﬁne a new value for the left-hand side. 

More precisely, we initialize all of the $\delta_{i\to j}$ ’s to 1 and then iteratively apply equation (11.10), computing the left-hand side $\delta_{i\to j}$ of each equality in terms of the right-hand side (essentially converting each equality sign to an assignment). Clearly, a single iteration of this process does not usually sufce to make the equalities hold; however, under certain conditions (which hold in a clique tree), we can guarantee that this process converges to a solution satisfying all of the equations in equation (11.10); the other equations are now easy to satisfy. 

Each assignment step deﬁned by a ﬁxed-point equation corresponds to a message passing step, where an outgoing message $\delta_{i\to j}$ is deﬁned in terms of incoming messages $\delta_{k\to i}$ . The fact that the process requires multiple assignments to converge corresponds to the fact that inference requires multiple message passing steps. In this speciﬁc example, a particular order of applying the ﬁxed-point equation reconstructs the sum-product message passing algorithm in cluster trees shown in algorithm 10.2. As we will see, however, when we consider other variants of the optimization problem, the associated ﬁxed-point equations result in new algorithms. 

![](images/fc354cd0d0674bec5465f41f60e4d1e568974a9b94750e127a89a514e3f95067.jpg) 
Figure 11.1 An example of a cluster graph. (a) A simple network. (b) A clique tree for the network in (a). (c) A cluster graph for the same network. 

# 11.3 Propagation-Based Approximation 

In this section, we consider approximation methods that use exactly the same message propa- gation as in exact inference. However, these propagation schemes use a general-purpose cluster graph, as in deﬁnition 10.1, rather than a clique tree. Since the constraints deﬁning a clique tree were crucial in ensuring exact inference, the message-propagation schemes that use cluster graphs will generally not provide the correct answers. 

We begin by deﬁning the general message passing algorithm in a cluster graph. We then show that it can be derived, using the same process as in the previous section, from a set of ﬁxed-point equations induced by the stationary points of an approximate energy functional. 

# 11.3.1 A Simple Example 

Consider the simple Markov network of ﬁgure 11.1a. Recall that, to perform exact inference within this network, we must ﬁrst reduce it to a tree, such as the tree of ﬁgure 11.1b. Inference in this simple tree involves passing messages over the sepset, which consists of the variables $\{B,D\}$ . 

Now suppose that, instead, we perform inference as follows. We set up four clusters, which correspond to the four initial potentials: $C_{1}=\{A,B\}$ , $C_{2}=\{B,C\}$ , $C_{3}=\{C,D\}$ , $C_{4}=$ $\{A,D\}$ . We connect these clusters to each other as shown in the cluster graph of ﬁgure 11.1c. Note that this cluster graph contains loops (undirected cycles), and is therefore not a tree; such graphs are often called loopy . Nevertheless, we can apply the belief-update propagation algorithm CTree-BU-calibrate (algorithm 10.3). Although in our discussion of that algorithm we assumed that the input is a tree, there is nothing in the algorithm itself that relies on that fact. In each step of the algorithm we propagate a message between neighboring clusters. Thus, it is perfectly applicable to a general cluster graph that may not necessarily be a tree. 

The clusters in this cluster graph are smaller than those in the clique tree of ﬁgure 11.1b; therefore, the message passing steps are less expensive. But what is the result of this procedure? Suppose we propagate messages in the following order $\mu_{1,2},\,\mu_{2,3},\,\mu_{3,4}$ , and then $\mu_{4,1}$ . In the ﬁrst message, t $\{A,B\}$ cluster passes information to the $\{B,C\}$ cluster through a marginal distribution on B . This information is then propagated to next cluster, and so on. However, in the ﬁnal message $\mu_{4,1}$ , this information reaches the original cluster, but this time as observation 

![](images/1c0f0cea6084c17af89803a0e75977d20da56e3eb7778df0cdb36309255024a7.jpg) 
Figure 11.2 An example run of loopy belief propagation in the simple network of ﬁgure 11.1a. In this run, all potentials prefer consensus assignments over nonconsensus ones. In each iteration, we perform message passing for all the edges in the cluster graph of ﬁgure 11.1b. 

about the values of $A$ . As an example, suppose all clusters favor consensus joint assignments; that is, $\beta_{1}(a^{0},b^{0})$ and $\beta_{1}(a^{1},b^{1})$ are much larger than $\beta_{1}(a^{1},b^{0})$ and $\beta_{1}(a^{0},b^{1})$ , and similarly for the other beliefs. Thus, if the message $\mu_{1,2}$ strengthens the belief that $B\,=\,b^{1}$ , then the message $\mu_{2,3}$ will increase the belief in $C=c^{1}$ and so on. Once we get around the loop, the message $\mu_{4,1}$ will strengthen the support in $A=a^{1}$ . This message will be incorporated into the cluster as though it were independent evidence that did not depend on the initial propagation. Now, if we continue to apply the same sequence of propagations again, we will keep increasing the beliefs in the assignment of $A=a^{1}$ . This behavior is illustrated in ﬁgure 11.2. As we can see, in later iterations the procedure overestimates the marginal probability of $A$ . However, the efect of the “feedback” decays until the iterations converge. 

This simple experiment already suggests several important issues we need to consider: 

• In the case of cluster trees, we described a sequence of message propagations that calibrate the tree in two passes. Once the tree is calibrated, additional message propagations do not change any of the beliefs. Thus, we can say that the propagation process has converged . When we consider our example, it seems clear that the process may not converge in two passes, since information from one pass will circulate and afect the next round. Indeed, it is far from clear that the propagation of beliefs necessarily converges at all.

 • In the case of cluster trees, we saw that, in a calibrated tree, each cluster of beliefs is the joint marginal of the cluster variables. As our example suggests, for cluster graph propagation, the beliefs on $A$ are not necessarily the marginal probability in $P_{\Phi}$ . Thus, the question is the relationship between the calibrated cluster graph and the actual probability distribution. 

Before we address these questions, we present the algorithm in more general terms. 

![](images/3eb21a347b4b2a6c6f0853a7992254d93bebda355a44b3d88bd55c50850e6b4c.jpg) 
Figure 11.3 Two examples of generalized cluster graph for an MRF with potentials over $\{A,B,C\}$ , $\{B,C,D\}$ , $\{B,D,F\}$ , $\{B,E\}$ and $\{D,E\}$ . 

Box 11.A — Case Study: Turbocodes and loopy belief propagation. The idea of propagating messages in loopy graphs was ﬁrst proposed in the early days of the ﬁeld, in parallel with the introduction of the ﬁrst exact inference algorithms. As we discussed in box 9.B, one of the ﬁrst inference algorithms was Pearl’s message passing for singly connected Bayesian networks (polytrees). In his 1988 book, Pearl says: 

When loops are present, the network is no longer singly connected and local propagation schemes will invariably run into trouble . . . If we ignore the existence of loops and permit the nodes to continue communicating with each other as if the network were singly connected, messages may circulate indeﬁnitely around the loops and the process may not converge to a stable equilibrium . . . Such oscillations do not normally occur in probabilistic networks . . . which tend to bring all messages to some stable equilibrium as time goes on. However, this asymptotic equilibrium is not coherent, in the sense that it does not represent the posterior probabilities of all nodes of the networks. 

loopy belief propagation 

message decoding 

As a consequence of these problems, the idea of loopy belief propagation was largely abandoned for many years. 

Surprisingly, the revival of loopy belief propagation is due to a seemingly unrelated advance in coding theory. The area of coding addresses the problem of sending messages over a noisy channel, and recovering it from the garbled result. Formally, the coding task can be deﬁned as follows. We wish to send a $k$ -bit message $u_{1},\ldots,u_{k}$ . We code the message using a number of bits $x_{1},.\ldots,x_{n},$ , which are then sent over the noisy channel, resulting in a set of (possibly corrupted) outputs $y_{1},\dotsc,y_{n},$ which can be either discrete or continuous. Diferent channels introduce noise in diferent ways: In a simple Gaussian noise model, each bit sent is corrupted independently by the addition of some Gaussian noise; another simple model ﬂips each bit independently with some probability; more complex channel models, where noise is added in a correlated way to consecutive bits, are also used. The message decoding task is to recover an estimate $\hat{u_{1}},\dots,\hat{u_{k}}$ from $y_{1},\dotsc,y_{n}$ . The bit error rate is the probability that a bit is ultimately decoded incorrectly. This error rate depends on the code and decoding algorithm used and on the amount of noise in the channel. The rate of a code is $k/n$ — the ratio between the number of bits in the message and the number of bits used to transmit it. 

For example, a very simple repetition code takes each bit and transmits it three times, then decodes the bit by majority voting on the three (noisy) copies received. If the channel corrupts each bit with probability $p$ , the bit error rate of this algorithm is $p^{3}+3p^{2}$ , which, for reasonable values 

![](images/06d3b621bd608115cfc7a94054d65839dc4d8d93453af9cc02a3bd59ade27116.jpg) 
Figure 11.A.1 — Two examples of codes (a) A $k\,=\,4,n\,=\,7$ parity check code, where every four message bits are sent along with three bits that encode parity checks. (b) A $k=4,n=8$ turbocode. Here, the $X^{a}$ bits $X_{1},X_{3},X_{5},X_{7}$ are simply the original bits $U_{1},U_{2},U_{3},U_{4}$ and are omitted for clarity of the diagram; the $X^{b}$ bits use a shift register — a state bit that changes with each bit of the message, where the $i$ th state bit depends on the $(i-1)\mathrm{{st}}$ state bit and on the i th message bit. The code uses two shift registers, one applied to the original message bits and one to a set of permuted message bits (using some predetermined permutations). The sent bits contain both the original message bits and some number of the state bits. 

of p , is much lower than $p$ . The rate of this code is $1/3;$ , because for every message bit, three bits are transmitted. In general, we can get better bit error rates by increasing the redundancy of the code, so we want to compare the bit error rate of diferent codes that have the same rate. Repetition codes are some of the least efcient codes designed. Figure 11.A.1a shows a simple rate 4/7 parity check code, where every four message bits are sent along with three bits that encode parity checks (exclusive ORs) of diferent subsets of the four bits. 

In 1948, Claude Shannon provided a theoretical analysis of the coding problem (Shannon 1948). For a given rate, Shannon provided an upper bound on the maximum noise level that can be tolerated while still achieving a certain bit error rate, no matter which code is used. Shannon also showed that there exist channel codes that achieve this limit, but his proof was nonconstructive — he did not present practical encoders and decoders that achieve this limit. 

Since Shannon’s landmark result, multiple codes were suggested. However, despite a gradual improvement in the quality of the code (bit-error rate for a given noise level), none of the codes even came close to the Shannon limit. The big breakthrough came in the early 1990s, when Berrou et al. (1993) came up with a new scheme that they called a turbocode , which, empirically, came much closer to achieving the Shannon limit than any other code proposed up to that point. However, their decoding algorithm had no theoretical justiﬁcation, and, while it seemed to work well in real examples, could be made to diverge or converge to the wrong answer. The second big breakthrough was the subsequent realization that turbocodes were simply performing belief propagation on $a$ Bayesian network representing the probability model for the code and the channel noise. 

To understand this, we ﬁrst observe that message decoding can easily be reformulated as a probabilistic inference task: We have a prior over the message bits $U=\left<U_{1},.\,.\,.\,,U_{k}\right>$ , a (usually deterministic) function that deﬁnes how a message is converted into a sequence of transmitted bits $X_{1},\dots,X_{n},$ and another (stochastic) model that deﬁnes how the channel randomly corrupts the $X_{i}\,{\stackrel{\prime}{s}}$ to produce $Y_{i}$ ’s. The decoding task can then be viewed as ﬁnding the most likely joint assignment to $U$ given the observed message bits ${\pmb y}\,=\,\langle y_{1},.\,.\,.\,,y_{n}\rangle$ , or (alternatively) as ﬁnding the posterior $P(U_{i}\mid\pmb{y})$ for each bit $U_{i}$ . The ﬁrst task is a MAP inference task, and the second task one of computing posterior probabilities. Unfortunately, the probability distribution is of high dimension, and the network structure of the associated graphical model is quite densely connected and with many loops. 

The turbocode approach, as ﬁrst proposed, comprised both a particular coding scheme, and the use of a message passing algorithm to decode it. The coding scheme transmits two sets of bits: one set comprises the original message bits $X^{a}\;=\;\left\langle X_{1}^{a},.\,.\,.\,,X_{k}^{a}\right\rangle\;=\;{\pmb u}$ ⟩ , and the second some set $X^{b}\,=\,\langle X_{1}^{b},.\,.\,.\,,X_{k}^{b}\rangle$ ⟩ of transformed bits (like the parity c its, but more complicated). The received bits then can also be partitioned into the noisy $\boldsymbol{y}^{a},\boldsymbol{y}^{b}$ . Importantly, the code is designed so that the message can be decoded (albeit with errors) using either $\boldsymbol{y}^{a}$ or $\boldsymbol{y}^{b}$ . The turbocoding algorithm then works as follows: It uses the model of $X^{a}$ (trivial in this case) and of the channel noise to compute a posterior probability over $U$ given $\boldsymbol{y}^{a}$ . It then uses that posterior $\pi_{a}(U_{1}),.\,.\,.\,,\pi_{a}(U_{k})$ as a prior over $U$ and computes a new posterior over $U$ , using the model for $X^{b}$ and the channel, and $\boldsymbol{y}^{b}$ as the evidence, to compute a new posterior $\pi_{b}(U_{1}),.\,.\,.\,,\pi_{b}(U_{k})$ . The “new information,” which is $\pi_{b}(U_{i})/\pi_{a}(U_{i})$ , is then transmitted back to the ﬁrst decoder, and the process repeats until a stopping criterion is reached. In efect, the turbocoding idea was to use two weak coding schemes, but to “turbocharge” them using a feedback loop. Each decoder is used to decode one subset of received bits, generating a more informed distribution over the message bits to be subsequently updated by the other. The speciﬁc method proposed used particular coding scheme for the $X^{b}$ bits, illustrated in ﬁgure 11.A.1b. 

This process looked a lot like black magic, and in the beginning, many people did not even believe that the algorithm worked. However, when the empirical success of these properties was demonstrated conclusively, an attempt was made to understand its theoretical properties. McEliece et al. (1998) subsequently showed that the speciﬁc message passing procedure proposed by Berrou et al. is precisely an application of belief propagation (with a particular message passing schedule) to the Bayesian network representing the turbocode (as in ﬁgure 11.A.1b). 

This revelation had a tremendous impact on both the coding theory community and the graphical models community. For the former, loopy belief propagation provides a general-purpose algorithm for decoding a large family of codes. By separating the algorithmic question of decoding from the question of the code design, it allowed the development of many new coding schemes with improved properties. These codes have come much, much closer to the Shannon limit than any previous codes, and they have revolutionized both the theory and the practice of coding. For the graphical models community, it was the astounding success of loopy belief propagation for this application that led to the resurgence of interest in these approaches, and subsequently to much of the work described in this chapter. 

# 11.3.2 Cluster-Graph Belief Propagation 

cluster graph 

Deﬁnition 11.2 running intersection property 

The basis for our message passing algorithm is the cluster graph of deﬁnition 10.1, ﬁrst deﬁned in section 10.1.1. In that section, we required that cluster graphs be trees and that they respect the running intersection property. Those requirements led us to the deﬁnition of a clique tree. Here, we remove the ﬁrst of these two assumptions, allowing inference to be performed on a loopy cluster graph. However, we still wish to require a variant of the running intersection property that is generalized to this case: for any two clusters containing $X$ , there is precisely one path between them over which information about $X$ can be propagated. 

We say that $\mathcal{U}$ satisﬁes the running intersection property if, w ever there is a variable $X$ such that $X\in C_{i}$ and $X\in C_{j},$ , then there is a single path between $C_{i}$ and $C_{j}$ for which $X\in S_{e}$ for all edges $e$ in the path. 

This generalized running intersection property implies that all edges associated with $X$ form a tree that spans all the clusters that contain $X$ . Thus, intuitively, there is only a single path by which information that is directly about $X$ can ﬂow in the graph. Both parts of this assumption are signiﬁcant. The fact that some path must exist forces information about $X$ to ﬂow between all clusters that contain it, so that, in a calibrated cluster graph, all clusters must agree about the marginal distribution of $X$ . The fact that there is at most one path prevents information about $X$ from cycling endlessly in a loop, making our beliefs more extreme due to “cyclic arguments.” 

Importantly, however, since the graph is not necessarily a tree, the same pair of clusters might also be connected by other paths. For example, in the cluster graph of ﬁgure 11.3a, we see that the edges labeled with $B$ form a subtree that spans all the clusters that contain $B$ . However, there are loops in the graph. For example, there are two paths fro ${C_{3}}=\left\{{B,D,F}\right\}$ to $C_{2}=\{B,C,D\}$ . The ﬁrst, through $C_{4}$ propagates information about B , and the second, through $C_{5}$ , propagates information about D . Thus, we can still get circular reasoning, albeit less directly than we would in a graph that did not satisfy the running intersection property; we return to this point in section 11.3.8. Note that while in the case of trees the deﬁnition of running intersection implied that $S_{i,j}=C_{i}\cap C_{j}$ , in a gra this e lity is no longer enf ed by the running intersection property. For example, cliques $C_{1}$ and $C_{2}$ in ﬁgure 11.3a have B in common, but $S_{1,2}=\{C\}$ . 

In clique trees, inference is performed by calibrating beliefs. In a cluster graph, we can also associate cluster $C_{i}$ with beliefs $\beta_{i}$ . We now say that a cluster graph is calibrated if for each 

calibrated cluster graph 

edge $(i{-}j)$ , connecting the clusters $C_{i}$ and $C_{j}$ , we have that 

$$
\sum_{C_{i}-S_{i,j}}\beta_{i}=\sum_{C_{j}-S_{i,j}}\beta_{j};
$$ 

that is, the two clusters agree on the marginal of variables in $\boldsymbol{S}_{i,j}$ . Note that this deﬁni- tion is weaker than cluster tree calibration, since the clusters do not necessarily agree on the joint marginal of all the variables they have in common, but only on those variables in the sepset. However, if a calibrated cluster graph satisﬁes the running intersection property, then the marginal of a variable $X$ is identical in all the clusters that contain it. 

![](images/c3aa0d78d193fe48327359bcfe4fdc7869c3feebeeb6f0621ad06ef5157e706e.jpg) 

How do we calibrate a cluster graph? Because calibration is a local property that relates adjoining clusters, we want to try to ensure that each cluster is sharing information with its 

![](images/3fcbd6bc327cd9c59c34503791e60bcc23c3b299a683d57b341448d6adc5fb4c.jpg) 
Figure 11.4 An example of a $4\times4$ two-dimensional grid network 

neighbors. From the perspective of a single cluster $C_{i}$ , there is not much diference between a cluster graph and a cluster tree. The cluster is related to each neighboring cluster through an edge that conveys information on variables in the sepset. Thus, we can transmit information by simply having one cluster pass a message to the other. 

However, a priori, it is not clear how we can execute a message passing algorithm over a loopy clustergraph. In particular, the sum-product calibration of algorithm 10.2 sends a message only when the sending clique is ready to transmit, that is, when all other incoming messages have been received. In the loopy cluster graph, initially, there is no cluster that has received any incoming messages. Thus, no cluster is ready to transmit, and the algorithm is deadlocked. However, in section 10.3, we showed that the two algorithms are actually equivalent; that is, any sequence of sum-product propagation steps can be emulated by the same sequence of belief- update propagation steps and leads to the same beliefs. In this transformation, we have that $\mu_{i,j}=\delta_{i\to j}\delta_{j\to i}$ . Thus, we can construct a “deadlock-free” variant of the sum-product message passing algorithm simply by initializing all messages $\delta_{i\to j}=\mathbf{1}$ . This initialization of the sum- product algorithm is equivalent to the standard initialization of the belief update algorithm, in which $\mu_{i,j}=\mathbf{1}$ . Importantly, in this variant of the sum-product algorithm, each cluster begins with all of the incoming messages initialized, and therefore it can send any of the outgoing messages at any time, without waiting for any other cluster. 

cluster-graph belief propagation 

Algorithm 11.1 shows the sum-product message passing algorithm for cluster graphs; other than the fact that the algorithm is applied to graphs rather than trees, the algorithm is identical to CTree-SP-Calibrate . In much the same manner, we can adapt CTree-BU-Calibrate to deﬁne a procedure CGraph-BU-Calibrate that operates over cluster graphs using belief-update message passing steps. Both of these algorithms are instances of a general class of algorithms called cluster-graph belief propagation , which passes messages over cluster graphs. 

Before we continue, we note that cluster-graph belief propagation can be signiﬁcantly cheaper than performing exact inference. A canonical example of a class of networks that is compactly representable yet hard for inference is the class of grid-structured Markov networks (such as the ones used in image analysis; see box 4.B). In these networks, each variable $A_{i,j}$ corresponds to a point on a two-dimensional grid. Each edge in this network corresponds to a potential between adjacent points on the grid, with $A_{i,j}$ connected to the four nodes $A_{i-1,j}$ , $A_{i+1,j}$ , 

![](images/31a1d5ee9883947a021f5f620f572c41705f211c961d5176391b5db16be4b122.jpg) 
Figure 11.5 An example of generalized cluster graph for a $3\times3$ grid network 

$A_{i,j-1}$ , $A_{i,j+1}$ (except for nodes $A_{i,j}$ on the boundary of the grid); see ﬁgure 11.4. Such a network has only pairwise potentials, and hence it is very compactly represented. Yet, exact inference requires separating sets, which are as large as cutsets in the grid. Hence, in an $n\times n$ grid, exact computation is exponential in $n$ . 

However, we can easily create a generalized cluster graph for grid networks that directly corresponds to the factors in the network. In this cluster graph, each cluster represents beliefs over two neighboring grid variables, and each cluster has a small number of adjoining edges that connect it to other clusters that share one of the two variables. See ﬁgure 11.5 for an example for a small $3\times3$ grid. (Note that there are several ways of constructing such a cluster graph; this ﬁgure represents one reasonable choice.) A round of propagations in the generalized cluster graph is linear in the size of the grid (quadratic in $n$ ). 

# 11.3.3 Properties of Cluster-Graph Belief Propagation 

What can we say about the properties and guarantees provided by cluster-graph belief propa- gation? We now consider some of the ramiﬁcations of the “mechanical” operation of message passing in the graph. Later, when we discuss cluster-graph belief propagation as an optimization procedure, we will revisit this question from a diferent perspective. 

# 11.3.3.1 Re parameter iz ation 

reparameteriza- tion 

cluster graph invariant 

Recall that in section 10.2.3 we showed that belief propagation maintains an invariant property. This allowed us to show that the convergence point represents a re parameter iz ation of the original distribution. We can directly extend this property to cluster graphs, resulting in a cluster graph invariant . 

Let $\mathcal{U}$ be a generalized cluster graph over a set of factors $\Phi$ . Consider the set of beliefs $\{\beta_{i}\}$ and sepsets $\{\mu_{i,j}\}$ at any iteration of CGraph-BU-Calibrate ; then 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\frac{\prod_{i\in\mathcal{V}_{\mathcal{U}}}\beta_{i}[C_{i}]}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{U}}}\mu_{i,j}[S_{i,j}]}.
$$ 

where $\begin{array}{r}{\tilde{P}_{\Phi}(\mathcal{X})=\prod_{\phi\in\Phi}\phi}\end{array}$ is the unnormalized distribution deﬁned by $\Phi$ . ∈ Proof Recall that $\begin{array}{r}{\beta_{i}=\psi_{i}\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\rightarrow i}}\end{array}$ Q and that $\mu_{i,j}=\delta_{j\to i}\delta_{i\to j}$ . We now have ∈ $\begin{array}{r c l}{\displaystyle\frac{\prod_{i\in\mathcal{V}_{\mathcal{U}}}\beta_{i}[C_{i}]}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{U}}}\mu_{i,j}[S_{i,j}]}}&{=}&{\displaystyle\frac{\prod_{i\in\mathcal{V}_{\mathcal{U}}}\psi_{i}[C_{i}]\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\to i}[S_{i,j}]}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{U}}}\delta_{j\to i}[S_{i,j}]\delta_{i\to j}[S_{i,j}]}}\\ &{=}&{\displaystyle\prod_{i\in\mathcal{V}_{\mathcal{U}}}\psi_{i}[C_{i}]}\\ &{=}&{\displaystyle\prod_{\phi\in\Phi}\phi(\boldsymbol{U}_{\phi})=\tilde{P}_{\Phi}(\mathcal{X}).}\end{array}$ 

Note that the second step is based on the fact that each message $\delta_{i\to j}$ appears exactly once in the numerator and the denominator and thus can be canceled. 

# 

This property shows that cluster-graph belief propagation preserves all of the informa- tion about the original distribution. In particular, it does not “dilute” the original factors by performing propagation along loops. Hence, we can view the process as trying to represent the original factors anew in a more useful form. 

11.3.3.2 Tree Consistency 

Recall that theorem 10.4 implies that, in a calibrated cluster tree, the belief over a cluster is the marginal of the distribution. Thus, in a calibrated cluster tree, we can “read of” the marginals of $P_{\Phi}$ locally from clusters that contain them. More precisely, by normalizing the beliefs factor $\beta_{i}$ (so that it sums to 1), we get the marginal distribution over $C_{i}$ . An obvious question is whether a corresponding property holds for cluster-graph belief propagation. Suppose we manage to calibrate a generalized cluster graph and normalize the resulting beliefs; do we have an interpretation for the beliefs in each cluster? 

As we saw in our simple example (ﬁgure 11.2), the beliefs we compute by BU-message are not necessarily marginals of $P_{\Phi}$ , but rather an approximation. Can we say anything about the quality of this approximation? To characterize the beliefs we get at the end of the process, we can use the cluster tree invariant property applied to subtrees of a cluster graph. 

Consider a subtree $\mathcal{T}$ of $\mathcal{U}$ ; that is, a subset of clusters and edges that together form a tree that satisﬁes the running intersection property. For example, consider the cluster graph of ﬁgure 11.1c. If we remove one of the clusters and its incident edges, we are left with a proper cluster tree. Note that the running intersection property is not necessarily as easy to achieve in general, since removing some edges from the cluster graph may result in a graph that violates the running intersection property relative to a variable, necessitating the removal of additional edges, and so on. Once we select a tree $\mathcal{T}$ , we can think of it as deﬁning a distribution 

$$
P_{\mathcal T}(\mathcal X)=\frac{\prod_{i\in\mathcal\nu_{\mathcal T}}\beta_{i}(C_{i})}{\prod_{(i-j)\in\mathcal E_{\mathcal T}}\mu_{i,j}[S_{i,j}]}.
$$ 

If the cluster graph is calibrated, then by deﬁnition so is $\mathcal{T}$ . And so, because $\mathcal{T}$ is a tree that satisﬁes the running intersection property, we can apply theorem 10.4, and we conclude that 

$$
\beta_{i}(C_{i})=P_{\mathcal{T}}(C_{i}).
$$ 

tree consistency That is, the beliefs over $C_{i}$ in the tree are the marginal of $P_{\mathcal{T}}$ , a property called tree consistency . 

As a concrete example, consider the cluster graph of ﬁgure 11.1c. Removing the cluster $C_{4}=\{A,D\}$ , we are left with a proper cluster tree $\mathcal{T}$ . The preceding argument implies that once we have calibrated the cluster graph, we have $\beta_{1}(A,B)=P\tau(A,B)$ . This result suggests that $\beta_{1}(A,B)\,\neq\,P_{\Phi}(A,B)$ ; to show this formally, contrast equation (11.11) with theorem 11.4. We see that the tree distribution involves some of the terms that deﬁne the joint distribution. Thus, we can conclude that 

$$
P_{\mathcal{T}}(A,B,C,D)=P_{\Phi}(A,B,C,D)\frac{\mu_{3,4}[D]\mu_{1,4}[A]}{\beta_{4}(A,D)}.
$$ 

We see that unless $\beta_{4}(A,D)=\mu_{3,4}[D]\mu_{1,4}[A]$ , $P_{\mathcal{T}}$ will be diferent from $P_{\Phi}$ . This conclusion suggests that, in this example, the beliefs $\beta_{1}(A,B)$ in the calibrated cluster graph are not the marginal $P_{\Phi}(A,B)$ . 

Clearly, we can apply the same type of reasoning using other subtrees of $\mathcal{U}$ . And so we reach the surprising conclusion that equation (11.11) must hold with respect to every cluster tree embedded in $\mathcal{U}$ . In our example, we n see that by removing a single cluster, we can construct three diferent trees that contain $C_{1}$ . The same beliefs $\beta_{1}(A,B)$ are the marginal of the three distributions deﬁned by each of these trees. While these three distributions agree on the joint marginal of $A$ and $B$ , they can difer on the joint marginal distributions of other pairs of variables. 

cluster graph residual 

Moreover, these subtrees allow us to get insight about the quality of the marginal distributions we read from the calibrated cluster graph. Consider our example again: we can use the residual term $\frac{\mu_{3,4}[D]\mu_{1,4}[A]}{\beta_{4}(A,D)}$ to analyze the error in the marginal distribution. In this simple example, this analysis is fairly straightforward (see exercise 11.4). 

In other cases, the analysis can be more complex. For example, suppose we want to ﬁnd a subtree in the cluster graph for a grid (e.g., ﬁgure 11.5). To construct a tree, we must remove a nontrivial number of clusters. More precisely, because each cluster corresponds to an edge in the grid, a cl ee corresponds to a subtree of the grid. For an $n\times n$ grid, such a tree will have at most $n^{2}-1$ − edges of the $2n(n-1)$ edges in the grid. Thus, each cluster tree contains about half of the clusters in the original cluster graph. In such a situation the residual term is more complex, and we cannot necessarily evaluate it. 

# 11.3.4 Analyzing Convergence $\star$ 

A key question regarding the belief propagation algorithm is whether and when it converges. Indeed, there are many networks for which belief propagation does not converge; see box 11.C. 

Although we cannot hope for convergence in all cases, it is important to understand when this algorithm does converge. We know that if the cluster graph is a tree then the algorithm will converge. Can we ﬁnd other classes of cluster graphs for which we can prove convergence? 

synchronous BP 

BP operator 

One method of analyzing convergence is based on the following important perspective on belief propagation. This analysis is easier to perform on a variant of BP called synchronous $B P$ that performs all of the message updates simultaneously. Consider the update step that takes all of the messages $\delta^{t}$ at a particular iteration $t$ and produces a new set of messages $\delta^{t+1}$ for the next step. Letting $\Delta$ be the space of all possible messages in the cluster graph, we can view the belief-propagation update operator as a function $G_{B P}\ :\ \Delta\mapsto\Delta$ 7→ . Consider the standard sum-product message update: 

$$
\delta_{i\rightarrow j}^{\prime}\propto\sum_{C_{i}-S_{i,j}}\psi_{i}\cdot\prod_{k\in(\mathrm{Nb}_{i}-\{j\})}\delta_{k\rightarrow i},
$$ 

where we normalize each message to sum to 1; this renormalization step is essential to avoid a degenerate convergence to the 0 message. We can now deﬁne the $B P$ operator as the function that simultaneously takes one set of messages and computes a new one: 

$$
G_{B P}(\{\delta_{i\to j}\})=\{\delta_{i\to j}^{\prime}\}.
$$ 

The question of convergence of the algorithm now reduces to one of asking whether repeated applications of the operator $G_{B P}$ are guaranteed to converge. 

One interesting, albeit strong, condition that guarantees convergence is the contraction prop- erty : 

Deﬁnition 11.3 contraction 

For a number $\alpha\in[0,1)$ operator $G$ ric space $(\Delta,D(;))$ is an $\alpha$ - contraction relative to the distance function I $D(;)$ if, for any δ $\delta,\delta^{\prime}\in\Delta$ , we have that: 

$$
D(G(\delta);G(\delta^{\prime}))\leq\alpha D(\delta;\delta^{\prime}).
$$ 

In other words, an operator is a contraction if its application to two points in the space is guaranteed to decrease the distance between them by at least some constant factor $\alpha<1$ . 

A basic result in analysis shows that, under fairly weak conditions, if an operator $G$ is a contraction, we have that repeated applications of $G$ are guaranteed to converge to a unique ﬁxed point: 

Proposition 11.2 ﬁxed-point 

Let $G$ be an $\alpha$ -contraction of a complete metric space $(\Delta,D(;))$ . Then there is a unique ﬁxed-point $\delta^{*}$ for which $G(\delta^{*})=\delta^{*}$ . Moreover, for any $\delta$ , we have that 

$$
\operatorname*{lim}_{n\to\infty}G^{n}(\delta)=\delta^{*}.
$$ 

The proof is left as an exercise (exercise 11.5). 

Indeed, the contraction rate $\alpha$ can be used to provide bounds on the rate of convergence of the algorithm to its unique ﬁxed point: To reach a point that is guaranteed to be within $\epsilon$ of $\delta^{*}$ , it sufces to apply $G$ the following number of times: 

$$
\log_{\alpha}{\frac{\epsilon}{\mathrm{diameter}(\Delta)}},
$$ 

where diameter $\begin{array}{r}{\cdot(\Delta)=\operatorname*{max}_{\delta,\delta^{\prime}\in\Delta}D(\delta;\delta^{\prime})}\end{array}$ . 

Applying this analysis to the operator $G$ induced by the belief-propagation message update is far from trivial. This operator is complex and nonlinear, because it involves both multiplying messages and a renormalization step. A review of these analyses is outside the scope of this book. At a high level, these results show that if the factors in the network are fairly “smooth,” one can guarantee that the synchronous BP operator is a contraction and hence converges to a unique ﬁxed point. We describe one of the simplest of these results, in order to give a ﬂavor for this type of analysis. 

This analysis applies to synchronous loopy belief propagation over a pairwise Markov network with two-valued random variables $X_{i}\,\in\,\{-1,+1\}\quad$ . Speciﬁcally, we assume that the network model is parameterized as follows: 

$$
P(x_{1},.\,.\,,x_{n})=\frac{1}{Z}\exp\left(\sum_{(i,j)}\epsilon_{i,j}(x_{i},x_{j})+\sum_{i}\epsilon_{i}(x_{i}),\right),
$$ 

where we assume for simplicity of notation that $\epsilon_{i,j}=0$ when $X_{i}$ and $X_{j}$ are not neighbors in the network. 

hyperbolic tangent 

We begin by introducing some notation. The hyperbolic tangent function is deﬁned as: 

$$
\operatorname{tanh}(w)={\frac{e^{w}-e^{-w}}{e^{w}+e^{-w}}}={\frac{e^{2w}-1}{e^{2w}+1}}.
$$ 

The hyperbolic tangent has a shape very similar to the sigmoid function of ﬁgure 5.11a. The following condition can be shown to sufce for $G_{B P}$ to be a contraction, and hence for the convergence of belief propagation to a unique ﬁxed point: 

$$
\operatorname*{max}_{i}\operatorname*{max}_{j\in\mathrm{Nb}_{i}}\sum_{k\in\mathrm{Nb}_{i}-\{j\}}\operatorname{tanh}{|\epsilon_{k,i}|}<1.
$$ 

Intuitively, this expression measures the total extent to which $i$ ’s neighbors other than $j$ can inﬂuence the message from $i$ to $j$ . The larger the magnitude of the parameters in the network, the larger this sum. 

The analysis of the more general case is signiﬁcantly more complex but shares the same intuitions. At a very high level, if we can place strong bounds on the skew of the parameters in a factor: 

$$
\operatorname*{max}_{\pmb{x},\pmb{x}^{\prime}}\phi(\pmb{x})/\phi(\pmb{x}^{\prime}),
$$ 

we can guarantee convergence of belief propagation. Intuitively, the lower the skew of the factors in our network, the more each message update “smoothes out” diferences between entries in the messages, and therefore also makes diferent messages more similar to each other. 

While the conditions that underlie these theorems are usually too stringent to hold in practice, this analysis does provide useful insight. First, it suggests that networks with potentials that are closer to deterministic are more likely to have problems with convergence, an observation that certainly holds in practice. Second, although global contraction throughout the space is a very strong assumption, a contraction property in a region of the space may be plausible, guaranteeing convergence of the algorithm if it winds up (or is initialized) in this region. These results and their ramiﬁcations are only now being explored. 

# 11.3.5 Constructing Cluster Graphs 

So far, we have taken the cluster graph to be given. However, the choice of cluster graph is generally far from obvious, and it can make a signiﬁcant diference to the algorithm. Recall that, even in exact inference, more than one clique tree can be used to perform inference for a given distribution. However, while these diferent trees can vary in their computational cost, they all give rise to the same answers. In the case of cluster graph approximations, diferent 

graphs can lead to very diferent answers. Thus, when selecting a cluster graph, we have to consider trade-ofs between cost and accuracy, since cluster graphs that allow fast propagation might result in a poor approximation. 

It is important to keep in mind that the structure of the cluster graph determines the prop- agation steps the algorithm can perform, and thus dictate what type of information is passed during the propagations. These choices directly inﬂuence the quality of the results. 

Example 11.1 Consider, for examp the cluster graphs $\mathcal{U}_{1}$ $\mathcal{U}_{2}$ of ﬁgure 11.3a and ﬁgure 11.3b. Both a fairl imilar, yet in U $\mathcal{U}_{2}$ the ed between $C_{1}$ and $C_{2}$ involves the mar al distribution over B and C . On the other hand, in U $\mathcal{U}_{1}$ , we propagate the margin only er C . Intuitively, we expect inference in $\mathcal{U}_{2}$ to better capture the dependencies between $B$ and $C$ . For , assu that the potential of $C_{1}$ intro es strong cor ations between B d C (say $B\,=\,C.$ ). In U $\mathcal{U}_{2}$ , this correlation is conveyed $C_{2}$ directly. In U $\mathcal{U}_{1}$ , t marginal on C is conveyed on the edge (1 – 2) , while the marginal on B is conveyed through $C_{4}$ . In this case, the strong dependency between the two variables is lost. In particular, if the marginal on $C$ is difuse (close to uniform), then the message $C_{1}$ sends to $C_{4}$ will also have a uniform distribution on $B$ , and from $C_{2}\mathit{\dot{s}}$ perspective the messages on $B$ and $C$ will appear as two independent variables. 

On the other hand, if we introduce many messages between clusters or increase the scope of these messages, we run the risk of constructing a tree that violates the running intersection property. And so, we have to worry about methods that ensure that the resulting structure is a proper cluster graph. We now consider several approaches for constructing cluster graphs. 

# 11.3.5.1 Pairwise Markov Networks 

pairwise Markov networks 

We start with the class of pairwise Markov networks . In these networks, we have a univariate potential $\phi_{i}[X_{i}]$ over each variable $X_{i}$ , and in addition a pairwise potential $\phi_{(i,j)}[X_{i},X_{j}]$ over some pairs of variables. These pairwise potentials correspond to edges in the Markov network. Many problems are naturally formulated as pairwise Markov networks, including the grid networks we discussed earlier and Boltzmann distributions (see box 4.C). Indeed, if we are willing to transform our variables, any distribution can be reformulated as a pairwise Markov network (see exercise 11.10). 

One straightforward transformation of such a network into a cluster graph is as follows: For each potential, we introduce a corresponding cluster, and put edges between the clusters that have overlapping scope. In other words, there is an edge between the cluster $C_{(i,j)}$ that corresponds to the edge $X_{i}{-}X_{j}$ and the clusters $C_{i}$ and $C_{j}$ that correspond to the univariate factors over $X_{i}$ and $X_{j}$ . Figure 11.6 illustrates this construction in the case of a 3 by 3 grid network. 

Because there is a direct correspondence between the clusters in the cluster graphs and vari- 

![](images/c4c7e85612aadc64c8f5198213afd7e69fd2108f5ceb76abd662e1c526ba450d.jpg) 
Figure 11.6 A generalized cluster graph for the $3\times3$ grid when viewed as pairwise MRF 

loopy belief propagation ables or edges in the original Markov network, it is often convenient to think of the propagation steps as operations on the original network. Moreover, since each pairwise cluster has only two neighbors, we consider two propagation steps along the path $C_{i}{-}C_{(i,j)}{-}C_{j}$ as propagating information between $X_{i}$ and $X_{j}$ . (See exercise 11.9.) Indeed, early versions of cluster-graph be- lief propagation were stated in these terms. This algorithm is known as loopy belief propagation , since it uses propagation steps used by algorithms for Markov trees, except that it was applied to networks with loops. 

# 11.3.5.2 Bethe Cluster Graph 

A natural question is how we can extend this idea to networks that are more complex than pairwise Markov networks. Once we have larger potentials, they may overlap in ways that result in complex interactions among them. 

Bethe cluster graph 

One simple construction, called the Bethe cluster graph , uses a bipartite graph. The ﬁrst layer consists of “large” clusters, with one cluster for each factor $\phi$ in $\Phi$ , whose scope is $S c o p e[\phi]$ . These clusters ensure that we satisfy the family-preservation property. The second layer consists of “small” univariate clusters, one for each random variable. Finally, we place an edge between each univariate cluster $X$ on the second layer and each cluster in the ﬁrst layer that includes $X$ ; the scope of this edge is $X$ itself. For a concrete example, see ﬁgure 11.7a. 

We can easily verify that this cluster graph is a proper one. First, by construction, it satisﬁes the family preservation property. Second, the edges that mention a variable $X$ form a star- shaped subgraph with edges from the univariate cluster for $X$ to all the large clusters that contain $X$ . It is also easy to check that, if we apply this procedure to a pairwise Markov network, it results in the “natural” cluster graph for the pairwise network that we discussed. The construction of this cluster graph is simple and can easily be automated. 

![](images/197ec18c6fc469fe59d99b56211b01a5b5e2150ac9153e4429b11b86547343d1.jpg) 
Figure 11.7 Examples of generalized cluster graphs for network with potentials over $\{A,B,C\}$ , $\{B,C,D\},\,\{B,D,F\},\,\{B,E\}$ and $\{D,E\}$ . For visual clarity, sepsets have been omitted — the sepset between any pair of clusters is the intersection of their scopes. (a) Bethe factorization. (b) Capturing interactions between $\{A,B,C\}$ and $\{B,C,D\}$ . 

# 11.3.5.3 Beyond Marginal Probabilities 

The main limitation of using the Bethe cluster graph is that information between diferent clusters in the top level is passed through univariate marginal distributions. Thus, interactions between variables are lost during propagations. Consider the example of ﬁgure 11.7a. Suppose that $C_{1}$ creates a strong dependency between $B$ and $C$ . These two variables are shared with $C_{2}$ . However, the messages between two clusters are mediated through the univariate factors. And thus, interactions introduced by one cluster are not directly propagated to the other. 

One possible solution is to merge some of the large clusters. For example, if we want to capture the interactions between $C_{1}$ and $C_{2}$ in ﬁgure 11.7a, we can replace both of them by a cluster with the score $A,B,C,D$ . This new cluster will allow us to capture the interactions between the factors involved in these two clusters. This modiﬁcation, however, comes at a price, since the cost of manipulating a cluster grows exponentially with this scope. Moreover, this approach seems excessive in this case, since we can summarize these interactions simply using a distribution over $B$ and $C$ . This intuition suggests the construction of ﬁgure 11.7b. Note that this cluster graph is equivalent to ﬁgure 11.3b; see exercise 11.6. 

Can we generalize this construction? A reasonable goal might be to capture all pairwise interactions. We can try to use a construction similar to the Bethe approximation, but in- troducing an intermediate level that includes pairwise clusters. In the same manner as we introduced $C_{12}$ in ﬁgure 11.7b, we can introduce other pairs that are shared by more than two clusters. As a concrete example, consider the factors $C_{1}=\{A,B,C\}$ , $C_{2}=\{B,C,D\}$ , and ${C_{3}}=\left\{{A,C,D}\right\}$ . The relevant pairwise factors that capture interactions among these clusters 

![](images/09fb3e6f1a2e4d90831bd9350700f30bd986321559c2deaa663322045b826d9c.jpg) 
Figure 11.8 Ex generalized cluster graph for network with potentials over $\{A,B,C\}$ , $\{B,C,D\},$ , and { $\{A,C,D\}$ } . For visual clarity, sepsets have been omitted — the sepset between any pair of clusters is the intersection of their scopes. (a) A Bethe-like factorization with pairwise marginals that leads to an illegal cluster graph. (b) One possible way to make this graph legal. 

are $\{B,C\}=C_{1}\cap C_{2}$ , $\{C,D\}=C_{2}\cap C_{3}$ , and $\{A,C\}=C_{1}\cap C_{3}$ . The resulting cluster graph appears in ﬁgure 11.8a. Unfortunately, a quick check shows that this cluster graph does not satisfy the running intersection property — all the edges in this graph are labeled by $C$ , and together they form a loop. As a result, information concerning $C$ can propagate indeﬁnitely around the loop, “overcounting” the efect of $C$ in the result. 

How do we avoid this problem? In this speciﬁc example, we can consider a weaker approx- imation by removing $C$ from one of the intersection sets. For example, if we remove $C$ from $C_{5}$ , we get the cluster graph of ﬁgure 11.8b. This cluster graph satisﬁes the running intersection property. An alternative approach tries to “compensate” somehow for the violation of the run- ning intersection property using a more complex message passing algorithm; see section 11.3.7.3. 

belief propagation nonconvergence Box 11.B — Skill: Making loopy belief propagation work in practice. One of the main prob- lems with loopy belief propagation is nonconvergence . This problem is particularly serious when we build systems that use inference as a subroutine within other tasks, for example, as the inner loop of a learning algorithm (see, for example, section 20.5.1). Several approaches have been used for addressing this nonconvergence issue. Some are fairly simple heuristics. Others are more so- phisticated, and typically are based on the characterization of cluster-graph belief propagation as optimizing the approximate free-energy functional. 

A ﬁrst observation is that, often, nonconvergence is a local problem. In many practical cases, most of the beliefs in the network do converge, and only a small portion of the network remains problematic. In such cases, it is often quite reasonable simply to stop the algorithm at some point (for example, when some predetermined amount of time has elapsed) and use the beliefs at that point, or a running average of the beliefs over some time window. This heuristic is particularly reasonable when we are not interested in individual beliefs, but rather in some aggregate over the entire network, for example, in a learning setting. 

A second observation is that nonconvergence is often due to oscillations in the beliefs (see sec- tion 11.3.1). This observation suggests that we dampen the oscillations by reducing the diference between two subsequent updates. Consider the belief-propagation update rule in SP-Message $(i,j)$ : 

$$
\delta_{i\rightarrow j}\leftarrow\sum_{C_{i}-S_{i,j}}\psi_{i}\prod_{k\neq j}\delta_{k\rightarrow i}.
$$ 

damping We can replace this line by $^a$ damped (or smoothed ) version that averages the update $\delta_{i\to j}$ with the previous message between the two cliques: 

$$
\delta_{i\to j}\leftarrow\lambda\left(\sum_{C_{i}-S_{i,j}}\psi_{i}\prod_{k\neq j}\delta_{k\to i}\right)+(1-\lambda)\delta_{i\to j}^{\mathrm{old}},
$$ 

stable convergence point 

message scheduling 

asynchronous BP tree reparameter- ization 

residual belief propagation 

where $\lambda$ is the damping weight and $\delta_{i\to j}^{\mathrm{old}}$ is the previous value of the message. When $\lambda\,=\,1$ , → this update is equivalent to standard belief propagation. For $0\,<\,\lambda\,<\,1$ , the update is partial and although it shifts $\beta_{j}$ toward agreement with $\beta_{i}$ , it leaves some momentum for the old value of the belief, a dampening efect that in turn reduces the ﬂuctuations in the beliefs. It turns out that this damped update rule is “equivalent” to the original update rule, in that a set of beliefs is a convergence point of the damped update if and only if it is a convergence point of standard updates (see exercise 11.13). Moreover, one can show that, if run from a point close enough to a stable convergence point of the algorithm, with a sufciently small $\lambda$ , this damped update rule is guaranteed to converge. Of course, this guarantee is not very useful in practice, but there are indeed many cases where the damped update rule is convergent, whereas the original update rule oscillates indeﬁnitely. 

A broader-spectrum heuristic, which plays an important role not only in ensuring convergence but also in speeding it up considerably, is intelligent message scheduling . It is tempting to implement BP message passing as a synchronous algorithm, where all messages are updated at once. It turns out that, in most cases, this schedule is far from optimal, both in terms of reaching convergence, and in the number of messages required for convergence. The latter problem is easy to understand: In a cluster graph with m edges, and diameter $d$ , synchronous message passing requires $m(d-1)$ messages to pass information from one side of the graph to the other. By contrast, asynchronous message passing, appropriately scheduled, can pass information between two clusters at opposite ends of the graph using $d-1$ messages. Moreover, the fact that, in synchronous message passing, each cluster uses messages from its neighbors that are based on their previous beliefs appears to increase the chances of oscillatory behavior and nonconvergence in general. 

In practice, an asynchronous message passing schedule works signiﬁcantly better than the synchronous approach. Moreover, even greater improvements can be obtained by scheduling messages in a guided way. One approach, called tree re parameter iz ation (TRP) , selects a set of trees, each of which spans a large number of the clusters, and whose union covers all of the edges in the network. The TRP algorithm then iteratively selects a tree and does an upward-downward calibration of the tree, keeping all other messages ﬁxed. Of course, calibrating this tree has the efect of “uncalibrating” other trees, and so this process repeats. This approach has the advantage of passing information more globally within the graph. It therefore converges more often, and more quickly, than other asynchronous schedules, particularly if the trees are selected using a careful design that accounts for the properties of the problem. 

An even more ﬂexible approach attempts to detect dynamically in which parts of the network messages would be most useful. Speciﬁcally, as we observed, often some parts of the network converge fairly quickly, whereas others require more messages. We can schedule messages in a way that accounts for their potential usefulness; for example, we can pass a message between clusters where the beliefs disagree most strongly on the sepset. This approach, called residual belief propagation is convenient, since it is fully general and does not require a deep understanding of the properties of the network. It also works well across a range of diferent real-world networks. 

An alternative general-purpose approach to avoiding nonconvergence is to directly optimize the energy functional. Here, several methods have been proposed. The simplest is to use standard optimization methods such as gradient ascent to optimize $\tilde{F}[\tilde{P}_{\Phi},Q]$ (see appendix A.5.2 and exercise 11.12). Other methods are more specialized to the form of the energy functional, and they often turn out to be more efcient (see section 11.7). Although these methods do improve convergence, they are somewhat complex to implement, and have not (at this time) been used extensively in practice. 

It turns out that many of the parameter settings encountered during a learning algorithm are problematic, and cause cluster-graph belief propagation to diverge. Intuitively, in many real-world problems, “appropriate” parameters encode strong constraints that tend to drive the algorithm toward well-behaved regions of the space. However, the parameters encountered during an iterative learning procedure have no such properties, and often allow the algorithm to end up in difcult regions. One approach is to train some parameters of the model separately, using a simpler network. We then use these parameters as our starting point in the general learning procedure. The use of “reasonable” parameters in the model can stabilize BP, allowing it to converge within the context of the general learning algorithm. 

local maxima 

A ﬁnal problem with cluster-graph belief propagation is the fact that the energy functional objective is multimodal, and so there are many local maxima to which a cluster-graph belief propagation algorithm might converge (if it converges). One can, of course, apply any of the standard approaches for addressing optimization of multimodal functions, such as initializing the algorithm heuristically, or using multiple restarts with diferent initializations. In the setting of $B P,$ initialization must be done with care, so as not to lose the connection to the correct underlying distribution $P_{\Phi}$ , as reﬂected by the invariant of theorem 11.4. In sum-product belief propagation, we can simply initialize the messages to something other than 1 . In belief update propagation, care must be taken to initialize messages and beliefs in a coordinate way, to preserve $P_{\Phi}$ . 

Box 11.C — Case Study: BP in practice. To convey the behavior of belief propagation in practice, we demonstrate its performance on an $11\times11$ (121 binary variables) Ising grid (see box 4.C). The potentials of the network were randomly sampled as follows: Each univariate potential was sampled uniformly in the interval $[0,1]$ ; for each pair of variables $X_{i},Z_{j},\,w_{i,j}$ is sampled uniformly in the range $[-C,C]$ (recall that in an Ising model, we deﬁne the negative log potential $\epsilon_{i,j}(x_{i},x_{j})=

$ $-w_{i,j}x_{i}x_{j}).$ . This sampling process creates an energy function where some potentials are attractive

 $(w_{i,j}\,>\,0)$ and some are repulsive $(w_{i,j}\,<\,0)$ , resulting in a nontrivial inference problem. The magnitude of $C$ (11 in this example) controls the magnitude of the energy forces and higher values correspond, on average, to more challenging inference problems. 

Figure 11.C.1 illustrates the convergence behavior on this problem. Panel (a) shows the percentage of messages converged as a function of time for three variants of the belief propagation algorithm: synchronous BP with damping (dashed line), where only a small fraction of the messages ever converge; asynchronous BP with damping (smoothing) that converges (solid line); asynchronous $B P$ with no damping (dash-dot line) that does not fully converge. The beneﬁt of using asynchronous propagation over synchronous updating is obvious. At ﬁrst, it appears as if smoothing messages is not beneﬁcial. This is because some percentage of messages can converge quickly when updates are 

![](images/d3b0e195cf78230d96c39d65c022d8abd53b16a3e1db87d19c25248f89f69c08.jpg) 
Figure 11.C.1 — Example of behavior of BP in practice on an ${\bf11}\times{\bf11}$ Ising grid. (a) Percentage of messages converged as a function of time for three diferent BP variants. (b) A marginal where both variants converge rapidly. (c–e) Marginals where the synchronous BP marginals oscillate around the asynchronous BP marginals. (f) A marginal where both variants are inaccurate. 

not slowed down by smoothing. However, the overall beneﬁt of damping is evident, and without it the algorithm never converges. 

The remaining panels illustrate the progression of the marginal beliefs over the course of the algorithm. (b) shows a marginal where both the synchronous and asynchronous updates converge quite rapidly and are close to the true marginal (thin solid black). Such behavior is atypical, and it comprises only around 10 percent of the marginals in this example. In the vast majority of the cases (almost 80 percent in this example), the synchronous beliefs oscillate around the asynchronous ones ((c)–(e)). In many cases, such as the ones shown in (e), the entropy of the synchronous beliefs is quite signiﬁcant. For about 10 percent of the marginals (for example (f)), both the asynchronous and synchronous marginals are inaccurate. In these cases, using more informed message schedules can signiﬁcantly improve the algorithms performance. 

These qualitative diferences between the BP variants are quite consistent across many random and real-life models. Typically, the more complex the inference problem, the larger the gaps in performance. For very complex real-life networks involving tens of thousands of variables and multiple cycles, even asynchronous BP is not very useful and more elaborate propagation methods or convergent alternatives must be adopted. 

# 11.3.6 Variational Analysis 

So far, our discussion of cluster-graph belief propagation has been procedural, motivated purely by similarity to message passing algorithms for cluster trees. Is there any formal justiﬁcation for this approach? Is there a sense in which we can view this algorithm as providing an approxima- tion to the exact inference task? In this section, we show that cluster-graph belief propagation can be justiﬁed using the energy functional formulation of section 11.1. Speciﬁcally, the mes- sages passed by cluster-graph belief propagation can be derived from ﬁxed-point equations for the stationary points of an approximate version of the energy functional of equation (11.3). As we will see, this formulation provides signiﬁcant insight into the generalized belief propagation algorithm. It allows us to understand better the convergence properties of cluster-graph belief propagation and to characterize its convergence points. It also suggests generalizations of the algorithm that have better convergence properties, or that optimize a better approximation to the energy functional. 

Our construction will be similar to the one in section 11.2 for exact inference. However, there are important diferences that underlie the fact that this algorithm is only an approximate inference algorithm. 

factored energy functional 

marginal polytope 

First, the exact energy functional $F[\tilde{P}_{\Phi},Q]$ has terms involving the entropy of an entire joint distribution; thus, it cannot be tractably optimized. However, the factored energy functional $\left[\tilde{F}[\tilde{P}_{\Phi},Q]\right.$ is deﬁned in terms of entropies of clusters and sepsets, each of which can be computed efciently based purely on local information at the clusters. Importantly, however, unlike for clique trees, $\tilde{F}[\tilde{P_{\Phi}},Q]$ is no longer simply a reformulation of the energy functional, but rather an approximation of it. 

However, even the factored energy functional cannot be optimized over the space of all marginals $Q$ that correspond to some actual distribution $P_{\Phi}$ . More precisely, consider some cluster graph $\mathcal{U}$ ; for a distribution $P$ e deﬁne $Q_{P}=\{P(C_{i})\}_{i\in\mathcal{V}_{\mathcal{U}}}\cup\{P(S_{i,j})\}_{(i-j)\in\mathcal{E}_{\mathcal{U}}}$ . We now deﬁne the marginal polytope of U to be 

$$
M a r g[\mathcal{U}]=\{Q_{P}:P\mathrm{~is~a~distribution~over~}\mathcal{X}\}
$$ 

That is, the marginal polytope is the set of all cluster (and sepset) beliefs that can be obtained from marginalizing an actual distribution $P$ . It is called the marginal polytope because it is the set of marginals obtained from the polytope of all pro bility distributions over $\mathcal{X}$ . Unfortunately, not every set of beliefs that correspond to clusters in U is in the marginal polytope; that is, there are calibrated cluster graph beliefs that do not represent the marginals of any single coherent joint distribution over $\mathcal{X}$ (see exercise 11.2). However, the marginal polytope is a complex object with exponentially many facets. (In fact, the problem of determining whether a set of beliefs is in the marginal polytope can be shown to be $\mathcal{N P}$ -hard.) Thus, optimizing a function over the local consistency polytope 

marginal polytope is a computationally difcult task that is generally as hard as exact inference over the cluster graph. To circumvent these problems, we perform our optimization over the local consistency polytope : 

$$
\begin{array}{r l}&{\mathcal{U}]=}\\ &{\left\{\begin{array}{l l l}{\phantom{-}\{\beta_{i}:i\in\mathcal{V}_{\mathcal{U}}\}\cup}&{\left|\begin{array}{l l l}{\mu_{i,j}[\pmb{s}_{i,j}]}&{=}&{\sum_{\pmb{C}_{i}-\pmb{S}_{i,j}}\beta_{i}(\pmb{c}_{i})}&{\forall(i\!-\!j)\in\mathcal{E}_{\mathcal{U}},\forall\pmb{s}_{i,j}\in\mathcal{U}}\\ {\{\mu_{i,j}:(i\!-\!j)\in\mathcal{E}_{\mathcal{U}}\}}&{\left|\begin{array}{l l l}{1}&{=}&{\sum_{\pmb{C}_{i}}\beta_{i}(\pmb{c}_{i})}&{\forall i\in\mathcal{V}_{\mathcal{U}}}\\ {\beta_{i}(\pmb{c}_{i})}&{\geq}&{0}&{\forall i\in\mathcal{V}_{\mathcal{U}},\pmb{c}_{i}\in V a l(\pmb{C}_{i})}\end{array}\right|\left.\right.}\end{array}
$$ 

pseudo-marginals 

We can think of the local consistency polytope as deﬁning a set of pseudo-marginal distri- butions , each one over the variables in one cluster. The constraints imply that these pseudo- marginals must be calibrated and therefore locally consistent with each other. However, they are not necessarily marginals of a single underlying joint distribution. 

Overall, we can write down an optimization problem as follows: 

CGraph-Optimize : Find Q maximizing F ˜ [ P ˜ Φ , Q ] subject to $Q\in L o c a l[\mathcal{U}]$ 

# 

Thus, our optimization problem contains two approximations: We are using an approx- imation, rather than an exact, energy functional; and we are optimizing it over the space of pseudo-marginals, which is a relaxation (a superspace) of the space of all coherent probability distributions that factorize over the cluster graph. 

In section 11.1, we noted that the energy functional is a lower bound on the log-partition function; thus, by maximizing it, we get better approximations of $P_{\Phi}$ . Unfortunately, the factored energy functional, which is only an approximation to the true energy functional, is not necessarily also a lower bound. Nonetheless, it is still a reasonable strategy to maximize the approximate energy functional, since it may lead to a good approximation of the log-partition function. 

ﬁxed-point equations 

Theorem 11.5 

This maximization problem directly generalizes CTree-Optimize to the case of cluster graphs. Not surprisingly, we can derive a similar analogue to theorem 11.3, where we characterize the stationary points of this optimization problem as solutions to a set of ﬁxed-point equations . 

$$
\delta_{i\rightarrow j}\propto\sum_{C_{i}-S_{i,j}}\psi_{i}\cdot\prod_{k\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\rightarrow i}.
$$ 

and moreover, we have that 

$$
\begin{array}{r c l}{\beta_{i}}&{\propto}&{\psi_{i}\cdot\displaystyle\prod_{j\in\ensuremath{\mathrm{Nb}}_{i}}\delta_{j\to i}}\\ {\mu_{i,j}}&{=}&{\delta_{j\to i}\cdot\delta_{i\to j}.}\end{array}
$$ 

The proof is identical to the proof of theorem 11.3. 

This theorem shows that we can characterize convergence points of the energy function in terms of the original potentials and messages between clusters. We can, once again, deﬁne a procedural variant, in which we initialize $\delta_{i\to j}$ , and then iteratively use equation (11.18) to redeﬁne each $\delta_{i\to j}$ in terms of the current values of other $\delta_{k\to i}$ . This process is identical (up to a renormalization step) to the update formula we use in CTree-SP-calibrate (algorithm 10.2). Indeed, we deﬁned CGraph-SP-Calibrate , a cluster graph version of CTree-SP-Calibrate , the mes- sage passing steps are simply executing this iterative process using the ﬁxed-point equation. Theorem 11.5 shows that convergence points of this procedure are related to stationary points of $\tilde{F}[\tilde{P}_{\Phi},Q]$ . 

Corollary 11.1 $Q$ is the ce point of applying CGraph-SP-Calibrate $(\Phi,{\mathcal{U}})$ if and only if $Q$ is a stationary point of $\tilde{F}[\tilde{P}_{\Phi},\bar{Q}]$ . 

Due to the equivalence between sum-product and belief update messages, it follows that convergence points of CGraph-BU-Calibrate are also convergence points of CGraph-SP-Calibrate . 

Corollary 11.2 At convergence of CGraph-BU-Calibrate , the set of beliefs is a stationary point of $\tilde{F}[\tilde{P}_{\Phi},Q]$ 

It is tempting to interpret this result as stating that the convergence points of belief propa- gation are maxima of the factored energy functional. However, there are several gaps between the theorem and this idealized interpretation, which it is important to understand. First, we note that maxima of a function are not necessarily ﬁxed points. In this case, we can verify that $\tilde{F}[\tilde{P}_{\Phi},Q]$ is bounded from above, and thus must have a maximum. However, if the maximum is a boundary point (where some of the probabilities in $Q$ are 0), it may not be a ﬁxed point. Fortunately, this situation is rare in practice, and it can be guaranteed not to arise under fairly benign assumptions. 

stable convergence point 

Second, we note that maxima are not the only ﬁxed points of the belief propagation algorithm; minima and saddle points are also ﬁxed points. Intuitively, however, such solutions are not likely to be stable, in the sense that slight perturbations to the messages will drive the process away from them. Indeed, it is possible to show (although this result is outside the scope of this book) that stable convergence points of belief propagation are always local maxima of the function. 

The most important limitation of this result, however, is that it does not show that we can reach these maxima by applying belief propagation steps. There is no guarantee that the message passing steps of cluster-graph belief propagation necessarily improve the energy functional: a message passing step may increase or decrease the energy functional. Indeed, as we showed, there are examples where the belief propagation procedure oscillates indeﬁnitely and fails to converge. Even more surprisingly, this problem is not simply a matter of the algorithm being unable to “ﬁnd” the maximum. One can show examples where the global maximum is not a stable convergence point of belief propagation. That is, while it is, in principle, a ﬁxed point of the algorithm, it will never be reached in practice, since even a slight perturbation will give rise to oscillatory behavior. 

Nevertheless, this result is of signiﬁcant importance in several ways. First, it provides us with a declarative semantics for cluster-graph belief propagation in terms of optimization of a target functional. The success of the belief propagation algorithm, when it converges, leads us to hope that the development of new, possibly more convergent, methods to solve the optimization problem may give rise to good solutions. Second, the declarative view deﬁnes the problem in terms of an objective — the factored energy functional — and a set of constraints — the set of locally consistent pseudo-marginals. Both of these are approximations to the ones used in the optimization problem for exact inference. When we view the task from this perspective, some potential directions for improvements become obvious: We can perhaps achieve a better approximation by making our objective a better approximation to the true energy functional, or by tightening our constraints so as to make the constraint space closer to the exact marginal polytope. We will describe some of the extensions based on these ideas; others are mentioned in section 11.7. 

# 11.3.7 Other Entropy Approximations $\star$ 

The variational analysis of the previous section provides us with a framework for understanding the properties of this type of approximation, and for providing signiﬁcant generalizations. 

# 11.3.7.1 Motivation 

To understand this general framework, consider ﬁrst the form of the factored energy functional when our cluster graph $\mathcal{U}$ has the form of the Bethe approximation. Recall that in the Bethe approximation graph there are two layers: one consisting of clusters that correspond to factors in $\Phi$ , and the other consisting of univariate clusters. When the cluster graph is calibrated, these univariate clusters have the same distribution as the sepsets between them and the factors in the ﬁrst layer. As such, we can combine together the entropy terms for all the sepsets labeled by $X$ and the associated univariate cluster and rewrite the energy functional, as follows: 

Proposition 11.3 If $Q=\{\beta_{\phi}:\phi\in\Phi\}\cup\{\beta_{i}(X_{i})\}$ is a calibrated set of beliefs for a Bethe cluster graph $\mathcal{U}$ with clusters $\left\{C_{\phi}\ :\ \phi\in\Phi\right\}\cup\left\{X_{i}\ :\ X_{i}\in\mathcal{X}\right\}$ , then 

$$
\tilde{F}[\tilde{P}_{\Phi},Q]=\sum_{\phi\in\Phi}\pmb{E}_{S c o p e[\phi]\sim\beta_{\phi}}[\ln\phi]+\sum_{\phi\in\Phi}\pmb{H}_{\beta_{\phi}}(C_{\phi})-\sum_{i}(d_{i}-1)\pmb{H}_{\beta_{i}}(X_{i}),
$$ 

where $d_{i}=|\{\phi:X_{i}\in S c o p e[\phi]\}|$ is the number of factors that contain $X_{i}$ . 

Bethe free energy Note that equation (11.19) is equivalent to the factored energy functional only when $Q$ is cali- brated. However, because we are interested only in such cases, we can freely alternate between the two forms for the purpose of ﬁnding ﬁxed points of the factored energy functional. Equa- tion (11.19) is the negation of a term known as the Bethe free energy in statistical mechanics. The Bethe cluster graph we discussed earlier is a construction that is designed to match the Bethe free energy functional. 

Why is this reformulation useful? Recall that, in our discussion of generalized cluster graphs, we required the running intersection property. This property has two important implications. First is that the set of clusters that contain some variable $X$ are connected; hence, the marginal over $X$ will be the same in all of these clusters at the calibration point. Second is that there is no cycle of clusters and sepsets all of which contain $X$ . We motivated this assumption by noting that it prevents us from allowing information about $X$ to cycle endlessly through a loop. This new formulation provides a more formal justiﬁcation. As we can see, if the variable $X_{i}$ appears in $d_{i}$ clusters in the cluster graph, then it appears in an entropy term with a positive sign exactly $d_{i}$ times. Owing to the running intersection property, the number of sepsets that contain $X_{i}$ is $d_{i}-1$ (the number of edges in a tr $k$ vertices is $k-1)$ , so that $X_{i}$ appe in an entropy term with a egative sign exactly $d_{i}-1$ − ti this case, the entropy of $X_{i}$ appears with positive sign $d_{i}$ times, and with negative sign $d_{i}-1$ − times, so overall it is counted exactly once. 

counting number weighted approximate entropy 

Example 11.2 Bethe cluster graph 

This reformulation suggests a much more general class of entropy approximations. We can construct deﬁne a set of regions $\mathbf{R}$ , each with its own scope $C_{r}$ and its own counting number $\kappa_{r}$ . We can now deﬁne the following weighted approximate entropy : 

$$
\tilde{H}_{Q}^{\kappa}(\mathcal{X})=\sum_{r}\kappa_{r}H_{\beta_{r}}(C_{r}).
$$ 

The simple Bethe cluster graph of section 11.3.5.2 ﬁts easily into this new framework. The construc- tion has two levels of regions: a set of “large” $\mathbf{R}^{+}$ , where ea $r\in\mathbf{R}^{+}$ contains multiple variables, and singleton regio con the i vidu bles $X_{i}\,\in\,{\mathcal{X}}$ ∈X . oth types of regions have counting numb $\kappa_{r}$ for r $r\in\mathbf{R}^{+}$ ∈ and $\kappa_{i}$ r $X_{i}\in\mathcal{X}$ ∈X . All fa $\Phi$ re assi only to large reg $\psi_{i}=1$ for all $i$ . We use $\mathrm{Nb}_{r}$ to denote the set { $\{X_{i}\in C_{r}\}$ ∈ } , and $\mathrm{Nb}_{i}$ to denote the set { $\{r\ :\ X_{i}\in C_{r}\}$ } . 

To capture exactly the Bethe free energy, we set each large region to have a counting number of 1 , and each singleton region corresponding to $X_{i}$ to have a counting number of $1-d_{i}$ where $d_{i}$ is the number of large regions that contain $X_{i}$ in their scope. We see that in this construction the region graph energy functional is identical to the Bethe free energy of equation (11.19). 

However, this framework also allows us to capture much richer constructions. 

Example 11.3 Consider again the example of ﬁgure 11.8a. As we discussed in section 11.3.5.3, this cluster graph has the beneﬁt of maintaining the pairwise correlations between all pairs of variables when passing messages between clusters. Unfortunately, it is not a legal cluster graph, since it does not satisfy the running intersection property. We can obtain another perspective on the problem with this cluster graph by examining the energy functional associated with it: 

$$
\begin{array}{r c l}{{\tilde{F}[\tilde{P}_{\Phi},Q]}}&{{=}}&{{{\pmb E}_{\beta_{1}}[\ln\phi_{1}({\cal A},{\cal B},{\cal C})]+{\pmb E}_{\beta_{2}}[\ln\phi_{2}({\cal B},{\cal C},{\cal D})]+{\pmb E}_{\beta_{3}}[\ln\phi_{3}({\cal A},{\cal C},{\cal D})}}\\ {{}}&{{}}&{{+{\pmb H}_{\beta_{1}}({\cal A},{\cal B},{\cal C})+{\pmb H}_{\beta_{2}}({\cal B},{\cal C},{\cal D})+{\pmb H}_{\beta_{3}}({\cal A},{\cal C},{\cal D})}}\\ {{}}&{{}}&{{-{\pmb H}_{\beta_{4}}({\cal B},{\cal C})-{\pmb H}_{\beta_{5}}({\cal A},{\cal C})-{\pmb H}_{\beta_{6}}({\cal C},{\cal D}).}}\end{array}
$$ 

As we can see, the variable $C$ appears in three clusters and three sepsets. As a consequence, the counting number of $C$ in the energy functional is 0 . This means that we are undercounting the entropy of $C$ in the approximation. Indeed, as we discussed, this cluster graph does not satisfy the running intersection property. Thus, we considered modifying the graph by removing $C$ from one of the sepsets. However, if we consider this problem from the perspective of the energy functional, we can deal with the problem by adding another factor $\beta_{7}$ that has $C$ as its scope. If we add $H_{\beta_{7}}(C)$ to the energy functional we solve the undercounting problem. This results in a modiﬁed energy functional 

$$
\begin{array}{r c l}{{\tilde{P}_{\Phi},Q]}}&{{=}}&{{{\cal E}_{\beta_{1}}[\ln\phi_{1}(A,B,C)]+{\cal E}_{\beta_{2}}[\ln\phi_{2}(B,C,D)]+{\cal E}_{\beta_{3}}[\ln\phi_{3}(A,C,D)]}}\\ {{}}&{{}}&{{+{\cal H}_{\beta_{1}}(A,B,C)+{\cal H}_{\beta_{2}}(B,C,D)+{\cal H}_{\beta_{3}}(A,C,D)+{\cal H}_{\beta_{7}}(C)}}\\ {{}}&{{}}&{{-{\cal H}_{\beta_{4}}(B,C)-{\cal H}_{\beta_{5}}(A,C)-{\cal H}_{\beta_{6}}(C,D).}}\end{array}
$$ 

This is simply an instance of our weighted entropy approximation, with seven regions: the three triplets, the three pairs, and the singleton $C$ . 

This perspective provides a clean and simple framework for proposing generalizations to the class of approximations deﬁned by the cluster graph framework. Of course, to formulate our optimization problem fully, we need to deﬁne the constraints and construct algorithms that solve the resulting optimization problems. We now address these issues in the context of two diferent classes of weighted entropy approximations. 

# 11.3.7.2 Convex Approximations 

One of the biggest problems with the objective used in standard loopy BP is that it gives rise to a nonconvex optimization problem. In fact, the objective often has multiple local optima. These properties make the optimization hard and the answers nonrobust. However, a diferent choice of counting numbers can lead to a concave optimization objective, and hence to a convex optimization problem. Such problems are much easier to solve using a range of algorithms, and the solutions ofer a satisfying guarantee of optimality. We ﬁrst deﬁne the class of convex BP objectives and then describe one solution algorithm. 

We focus our discussion on energy functionals whose structure uses the two-layer Bethe cluster graph structure of example 11.2, but where the counting numbers are diferent. To preserve the desired semantics of the counting numbers, we require: 

$$
\kappa_{i}=1-\sum_{r\in\mathrm{Nb}_{i}}\kappa_{r},
$$ 

ensuring that the total counting number of terms involving the entropy of $X_{i}$ is precisely 1 . When we deﬁne $\kappa_{r}\,=\,1$ for all $r\,\in\,\mathbf{R}$ , this constraint implies the counting numbers in the Bethe free energy. 

We now introduce the following condition on the counting numbers: 

Deﬁnition 11.4 convex counting numbers 

We say that a vector of counting numbers $\kappa_{r}$ is convex if there exist nonnegative numbers $\nu_{r},\,\nu_{i}$ , and $\nu_{r,i}$ such that: 

$$
\begin{array}{r l r}{\kappa_{r}}&{=}&{\nu_{r}+\sum_{i\,:\,X_{i}\in C_{r}}\nu_{r,i}\quad{\it f o r\;a l l\;r}}\\ {\kappa_{i}}&{=}&{\nu_{i}-\sum_{r\,:\,X_{i}\in C_{r}}\nu_{r,i}\quad{\it f o r\;a l l\;i}}\end{array}
$$ 

Assuming that we have a set of convex counting numbers, we can rewrite the weighted approximate entropy of equation (11.20) as: 

$$
\begin{array}{l}{{\displaystyle\sum_{r}\kappa_{r}{\pmb H}_{\beta_{r}}({\pmb C}_{r})+\sum_{i}\kappa_{i}{\pmb H}_{\beta_{i}}({\pmb X}_{i})=}}\\ {{\displaystyle\sum_{r}\nu_{r}{\pmb H}_{\beta_{r}}({\pmb C}_{r})+\sum_{r,X_{i}\in{\pmb C}_{r}}\nu_{r,i}\big({\pmb H}_{\beta_{r}}({\pmb C}_{r})-{\pmb H}_{\beta_{i}}({\pmb X}_{i})\big)+\sum_{i}\nu_{i}{\pmb H}_{\beta_{i}}({\pmb X}_{i}).}}\end{array}
$$ 

Importantly, when the beliefs satisfy the marginal-consistency constraints, the terms in the second summation can be rewritten as conditional entropies: 

$$
H_{\beta_{r}}(C_{r})-H_{\beta_{i}}(X_{i})=H_{\beta_{r}}(C_{r}\mid X_{i}).
$$ 

Plugging this result back into equation (11.23), we obtain an objective that is a summation of terms each of which is either an entropy or a conditional entropy, all with positive coefcients. Because both entropies and conditional entropies are convex, we obtain the following result: 

# Proposition 11.4 

concave over constraints convex entropy The function in equation (11.23) is a concave function for any set of beliefs $Q$ that satisﬁes the marginal consistency constraints. 

This type of objective function is called concave over the constraints , since it is not generally concave, but it is concave over the subspace that satisﬁes the constraints of our optimization problem. An entropy as in equation (11.23) that uses convex counting numbers is called a convex entropy . 

Assuming that the potentials are all strictly positive, we can now conclude that the optimiza- tion problem CGraph-Optimize with convex counting numbers is a convex optimization problem that has a unique global optimum. 

Convex optimization problems can, in principle, be solved by a range of diferent algorithms, all of which guarantee convergence to the unique global optimum. However, the basic optimiza- tion problem can easily get intractably large. Recall that to formulate our optimization space, we need to introduce an optimization variable for each assignment of values to each cluster in our cluster graph, and a constraint for each assignment of values to each sepset in the graph. 

Example 11.4 Consider a grid-structured network corresponding to a modestly sized $500\times500$ image, where each pixel can take 100 values. The structure of the graph is a pairwise network, with approximately $2\times250,000$ clusters (pairwise edges), e $100\times100=10,000$ values. The total number of variables is therefore 500 $\vert,000\times10,000=5\times10^{9}$ × × , an unmanageable number for most optimizers. 

Fortunately, due to the convexity of this problem, we have that strong duality holds (see appendix A.5.4), and therefore we can ﬁnd a solution to this problem by solving its dual. The message passing algorithms that we derive from the Lagrange multipliers are one method that we can use for solving the dual. (For example, exercise 11.17 provides one message passing algorithm for a Bethe cluster graph with general counting numbers.) However, the message passing algorithms are not directly optimizing the objective. Rather, they characterize the optimum using a set of ﬁxed-point equations, and attempt to converge to the optimum by iterating through these equations. This process is generally not guaranteed to achieve the optimum, even when the problem is convex. Again, we can consider using other optimization algorithms over the dual problem. However, a message passing approach has some important advantages, such as modularity and efciency. 

Fortunately, a careful reformulation of the message passing scheme can be shown to guar- antee convergence to the global optimum. This reformulation is diferent for synchronous and asynchronous message passing. We present the asynchronous version, which is simpler and also likely to be more efcient in practice. 

The algorithm, shown in algorithm 11.2, uses the following quantities in its computations: 

$$
\hat{\nu}_{i}=\nu_{i}+\sum_{r\in\ensuremath{\mathrm{Nb}}_{i}}\nu_{r};\qquad\qquad\qquad\qquad\hat{\nu}_{i,r}=\nu_{r}+\nu_{i,r}.
$$ 

In each message passing iteration, it traverses the variables $X_{i}$ in a round-robin fashion; for each $X_{i}$ , it computes two sets of messages: incoming messages $\delta_{r\rightarrow i}(X_{i})$ from regions to variables, 

Procedure Convex-BP-Msg ( $\psi_{r}(C_{r})$ // set of initial potentials $\sigma_{i\rightarrow r}(C_{r})$ // Current node-to-region messages ) 1 for $i=1,\dots,n$ 2 $//$ Compute incoming messages from neighboring regions to X i 3 for $r\in\mathrm{Nb}_{i}$ 4 $\begin{array}{r}{\delta_{\boldsymbol{r}\rightarrow\boldsymbol{i}}(X_{i})\gets\;\sum_{\boldsymbol{C}_{\boldsymbol{r}}-X_{i}}\Big(\psi_{\boldsymbol{r}}(\boldsymbol{C}_{\boldsymbol{r}})\prod_{\boldsymbol{j}\in\mathrm{Nb}_{\boldsymbol{r}}-\{\boldsymbol{i}\}}\sigma_{\boldsymbol{j}\rightarrow\boldsymbol{r}}(\boldsymbol{C}_{\boldsymbol{r}})\Big)^{\frac{1}{\hat{\nu}_{i,\boldsymbol{r}}}}}\end{array}$ 5 $//$ Compute beliefs for $X_{i}$ , renormalizing to avoid numerical underﬂows 6 $\begin{array}{r}{\beta_{i}(\overbrace{X_{i}}^{\mathrm{Yukawa}\cdots\cdots}\overbrace{\frac{1}{Z_{X_{i}}}}^{\cdots}\prod_{r\in\mathrm{Nb}_{i}}(\delta_{r\rightarrow i}(X_{i}))^{\hat{\nu}_{i,r}/\hat{\nu}_{i}}}\end{array}$ Q 7 // Compute outgoing messages from $X_{i}$ to neighboring re- gions 8 for $\bar{r}\in\mathrm{Nb}_{i}$ 10 $\begin{array}{r l}{\dot{\mathrm{\boldmath{~\sigma~}}}_{i\rightarrow r}({\cal C}_{r})\gets}&{\left(\psi_{r}({\cal C}_{r})\prod_{j\in\mathrm{Nb}_{r}-\{i\}}\sigma_{j\rightarrow r}({\cal C}_{r})\right)^{-\frac{\nu_{i,r}}{\tilde{\nu}_{i,r}}}\left(\frac{\beta_{i}(X_{i})}{\delta_{r\rightarrow i}(X_{i})}\right)^{\nu_{r}}}\\ {{\mathrm{\bfreturn}~}\{\sigma_{i\rightarrow r}({\cal C}_{r})\}_{i,r\in\mathrm{Nb}_{i}}}\end{array}$ factor graph 

and outgoing messages $\sigma_{i\rightarrow r}(C_{r})$ from variables to regions (essentially passing messages over the factor graph ). The overall process is initialized (in the ﬁrst message passing iteration) by setting $\sigma_{i\to r}\,=\,1$ . This algorithm is guaranteed to converge to the global maximum of our convex energy functional. 

This derivation applies to any set of convex counting numbers, leaving open the question of which counting numbers are likely to be give the best approximation. Although there is currently no theoretical analysis answering this question, intuitively, we might argue that we want the counting numbers for diferent regions to be as close as possible to uniform. This intuition is also supported by the fact that the Bethe approximation, which sets all $\kappa_{r}=1$ , obtains very high- quality approximations when it converges. Thus, we can try to select nonnegative coefcients $\nu_{i},~\nu_{r}$ , and $\nu_{i,r}$ for which $\kappa_{r}$ and $\kappa_{i}$ , deﬁned via equation (11.22), satisfy equation (11.21) and minimize 

$$
\sum_{r\in\mathbf{R}^{+}}(\kappa_{r}-1)^{2}.
$$ 

TRW 

Other choices are also possible. For example, the tree-reweighted belief propagation (TRW) algorithm computes convex counting numbers for a pairwise Markov network using the following process: We ﬁrst deﬁne a probability distribution $\rho$ over trees $\mathcal{T}$ in the network, such that each edge in the pairwise network is present in at least one tree. This distribution deﬁnes a set of weights: 

$$
\begin{array}{r l}{\kappa_{i}}&{{}=-\sum_{\mathcal T\ni X_{i}}\rho(\mathcal T)}\\ {\kappa_{i,j}}&{{}=\sum_{\mathcal T\ni(X_{i},X_{j})}\rho(\mathcal T)}\end{array}
$$ 

This computation results in a set of convex counting numbers (see exercise 11.18). Preliminary results suggest that the TRW counting numbers and the ones derived from optimizing equa- tion (11.25) appear to achieve similar performance in practice. 

However, the comparison to standard (Bethe-approximation) BP is less clear. When standard BP converges, it generally tends to produce better results than the convex counterparts, and almost universally it converges much faster. Conversely, when standard BP does not converge, the convex algorithms have an advantage; but, as we discuss in box 11.B, there are many tricks we can use to improve the convergence of BP, so it is not clear how often nonconvergence is a problem. One setting where a convergent algorithm can have important beneﬁts is in those settings (chapter 19 and chapter 20) where we generally learn the model using iterative, hill-climbing methods that use inference in the inner loop for tasks such as gradient computations. There, the use of a nonconvergent algorithm for computing the gradient can severely destabilize the learning algorithm. In other settings, however, the decision of whether to use standard or convex BP is one of approximate optimization of a pretty good (although still approximate) objective, versus exact optimization of an objective that is generally not as good. The right decision in this trade-of is not clear, and needs to be made speciﬁcally for the target application. 

# 11.3.7.3 Region Graph Approximations 

As illustrated in example 11.3, a very diferent motivation for using an objective based on diferent counting numbers is to improve the quality of the approximation by better capturing interactions between variables. As we showed in this example, we can use the notion of a weighted entropy approximation to deﬁne a (hopefully) better approximation to the entropy. Of course, to specify the optimization problem fully, we also need to specify the constraints. In this example, it is fairly straightforward to do so: we want $\beta_{7}(C)$ to be consistent with the marginal probability of $C$ in one of the other beliefs that mention $C$ . Now, we have an optimization problem that seems to solve the problem we set out to solve: It can compute beliefs on each of the original factors while maintaining consistency at the level of each pairwise marginal shared among these factors. 

However, the new optimization problem we deﬁned is not one that corresponds to a cluster graph. To see this, notice that $\beta_{7}$ appears in the role of a cluster. But, if it is a cluster, it would have to be connected to one of the other factors by a sepset with scope $C$ , which would require an additional term in the energy functional associated with this cluster graph. Thus, it is not immediately clear how we would go about optimizing the new modiﬁed functional. 

We now discuss a general framework that deﬁnes the form of the optimization objective and the constraints for constructions that capture higher-level interactions between the variables. We also describe a message passing algorithm that can be used to ﬁnd ﬁxed points of this optimization problem. 

Region Graphs The basic structure we consider is similar to a cluster graph, but unlike cluster graphs we no longer distinguish two types of vertices (clusters and sepsets). Rather, we can have a more deeply nested hierarchy of regions, related by containment. 

![](images/fb0601e9d08519a16cac90eb6dec042d0cefc3bf915a3ecb949c0b62fedbcac5.jpg) 
Figure 11.9 An example of simple region graph 

is associated with a distinct set of random variables $C_{r}$ . Whenever there is an arc from a region $r_{i}$ to a region $r_{j}$ we require that $S c o p e[r_{i}]\supset S c o p e[r_{j}]$ . Regions that have no incoming edges are called top regions . 

counting numbers 

Each region is associated with a counting number $\kappa_{r}\in\mathbb{R}$ . Note that the counting number may be negative, positive, or zero. 

Because containment is transitive, we have that if there is a directed path from $r$ to $r^{\prime}$ , then $S c o p e[r^{\prime}]\subset S c o p e[r]$ . Thus, a region graph is acyclic. 

family preservation 

To deﬁne the energy term in the free energy, we must assign our original factors in $\Phi$ to regions in the region graph. Here, because diferent regions are counted to diferent extents, it is useful to allow a fa assigned t e region. Thus, for each $\phi\in\Phi$ we have a set of regions $\alpha(\phi)\subset\mathbf{R}$ ⊂ such that $S c o p e[\phi]\subseteq C_{r}$ ⊆ . This property is analogous to the family-preservation property for cluster graphs. Throughout this book, we assume without loss of generality that any $r\in\alpha(\phi)$ is a top region. 

We are now ready to deﬁne the energy functional associated with a region graph: 

$$
\tilde{F}[\tilde{P}_{\Phi},Q]=\sum_{r}\kappa_{r}E_{C_{r}\sim\beta_{r}}[\ln\psi_{r}]+\tilde{H}_{Q}^{k}(\mathcal{X}),
$$ 

where $\psi_{r}$ is deﬁned as: 

$$
\psi_{r}=\prod_{\phi\in\Phi\ :\ r\in\alpha(\phi)}\phi.
$$ 

As with cluster graphs, a region graph deﬁnes a set of beliefs, one per region. We use the notation $\beta_{r}(C_{r})$ to denote the belief associated with the region $r$ over the set of variables $C_{r}=S c o p e[r]$ . 

Figure 11.9 demonstrates the region graph construction for the approximation of example 11.3. This example contains three regions that correspond to the initial factors in the distribution we want to approximate. The lower set of regions are the pairwise intersections between the three factors. The lowest region is associated with the variable $C$ . 

Whereas the counting numbers specify the energy functional, the graph structure speciﬁes the constraints over the beliefs that we wish to associate with the regions. In particular, we 

![](images/ae20deef87ea84848612e9f5dff21a811205e459b20095b6200782a3a23c5885.jpg) 
Figure 11.10 The region graph corresponding to the Bethe cluster graph of ﬁgure 11.7a 

want the beliefs to satisfy the calibration constraints that are implied by the edges in the region graph structure. 

Deﬁnition 11.6 region graph calibration 

Given a region graph, a set of region beliefs is calibrated if whenever $r\to r^{\prime}$ appears in the region graph then 

$$
\sum_{C_{r}-C_{r^{\prime}}}\beta_{r}(C_{r})=\beta_{r^{\prime}}(C_{r}^{\prime})
$$ 

The region graph structure provides a very general framework for specifying energy-functional optimization problems. The set of regions and their counting numbers tell us which components appear in the energy functional, and with what weight. The arcs in the region graph tell us which consistency constraints we wish to enforce. We can choose which consistency constraints we want to enforce by adding or removing regions or edges between them; we note that this can be done without afecting the energy functional by simply giving the regions introduced a counting number of 0 . The more edges we have, the more constraints we require regarding the calibration of diferent beliefs. 

The region graph framework is very general, and it can encode a broad spectrum of opti- mization objectives and constraints. However, not all such formulations are equally reasonable as approximations to our true objective, which is the exact energy functional of equation (11.3). The following requirement captures some of the essential properties that make a region graph construction suitable for this purpose. 

Deﬁnition 11.7 

e $X_{i}$ , let $\overline{{\mathbf{R}_{i}\;=\;}}\left\{r\;:\;X_{i}\;\in\;S c o p e[r]\right\}$ ; for each factor $\phi\,\in\,\Phi$ , let $\mathbf{R}_{\phi}\,=\,\{r\,:$ $S c o p e[\phi]\subseteq C_{r}\}$ ⊆ } . We now deﬁne the following conditions for a region graph: • variable connectedness : for every variable $X_{i}$ , the set $\mathbf{R}_{i}$ forms a single connected component; • factor connectedness : for every factor $\phi.$ , the set $\mathbf{R}_{\phi}$ forms a single connected component; • factor preservation : $\sum_{r\in\alpha(\phi)}\kappa_{r}=1.$ • running intersection : for every variable $X_{i}$ , 

The connectedness requirement for variables ensures that the beliefs about any individual vari- able $X_{i}$ in any calibrated region graph will be consistent for all beliefs that contain $X_{i}$ . The counting number condition for variables ensures that we are not not overcounting or under- counting the entropy of $X_{i}$ . Together, these conditions extend the running intersection property, ensuring that the subgraph containing $X_{i}$ is connected and that $X_{i}$ is “counted” once in total. We note that, like the running intersection property, this requirement does not address the ex- tent to which we count the contribution associated with the interactions between pairs or larger subsets of variables. 

The factor-preservation condition for factors ensures that, when we sum up the energy terms of the diferent regions, each factor is counted exactly once in total. As we will see, this ensures that a calibrated region graph will still encode our original distribution $P_{\Phi}$ . Finally, the connectedness condition for factors ensures that we cannot double-count contributions of our initial factors: that is, a factor cannot “ﬂow” around a loop. 

An examination conﬁrms that all parts of the region graph condition hold both for the Bethe region graph and for the region graph of ﬁgure 11.9. 

How do we construct a valid region graph? One approach is based on the following simple recursive construction for the counting numbers. We ﬁrst deﬁne, for a region $r$ , 

$$
\mathbf{Up}(r)=\{r^{\prime}\ :\ (r^{\prime}\to r)\in\mathcal{E}_{\mathcal{R}}\}
$$ 

to be the set of regions that are directly upwards of $r$ ; similarly, we deﬁne 

$$
\mathbf{Dewn}(r)=\{r^{\prime}\ :\ (r\to r^{\prime})\in\mathcal{E}_{\mathcal{R}}\}.
$$ 

We also deﬁne the upward closure of $r$ to be the set $\mathbf{Up}^{*}(r)$ of all the regions from which there is a directed path to $r$ , and the downward closure $\mathbf{D o w n}^{*}(r)$ to be all the regions that can be reached by a directed path from $r$ ; ﬁnally, we deﬁne $\mathbf{Dofwr}^{+}(r)=\{r\}\cup\mathbf{Dofwr}^{*}(r)$ . 

We can now deﬁne the counting numbers recursively, using the following formula: 

$$
\kappa_{r}=1-\sum_{r^{\prime}\in\mathbf{U}\mathbf{p}^{*}(r)}\kappa_{r^{\prime}}.
$$ 

This condition ensures that the sum of the counting numbers of $r$ and all of the regions above it will be 1 . Intuitively, we can think of the counting number of the region $r$ as correcting for overcounting or undercounting of the weight of the scope of $r$ by regions above it. Now, assume that our region graph is structured so that, for each variable $X_{i}$ , there is a unique region $r_{i}$ such that every other region whose scope contains $X_{i}$ is an ancestor of $r_{i}$ . Then, we are guaranteed that both the connectedness and the counting number condition for $X_{i}$ hold. We can similarly require that for any factor $\phi$ , there is a unique region $r_{\phi}$ such that any other region whose scope contains $S c o p e[\phi]$ is an ancestor of $r_{\phi}$ . This construction guarantees the requirements for factor connectedness and counting numbers. 

It is easy to see that the Bethe region graph of example 11.2 satisﬁes both of these conditions. Moreover, this process of guaranteeing that a unique minimal region exists for each $X_{i}$ is essentially what we did in example 11.3 to produce a valid region graph. 

These conditions provide us with a simple strategy for constructing a saturated region graph . 

![](images/e95f218fcee7baf3d52011fd69ef631b1803af5e9a6536fc3c397d785b08a264.jpg) 

We start with initial set of regions. Often, these regions will be the initial factors in $P_{\Phi}$ , although we can decide to work with bigger regions that capture some more global interactions. We then extend this set of regions into a valid region graph, where our goal is to represent appropriately any subset of variables that is shared by some of the regions. We therefore expand the set of regions to be closed under intersections. We connect these regions so that the upward closure of each region contains all of its supersets. The full procedure is shown in algorithm 11.3. Unlike the Bethe approximation, this region graph maintains the consistency of higher-order marginals. The example of ﬁgure 11.9 is an example of running this procedure on the original set of regions $\{A,B,C\}$ , $\{B,C,D\}$ , and $\{A,C,D\}$ . As our previous discussion suggests, this procedure guarantees a region graph that satisﬁes the region graph condition. 

Belief Propagation in Region Graphs Given a region graph, we are faced with the task of optimizing the free energy associated with its structure: 

RegionGraph-Optimize : 

$$
\begin{array}{l}{Q=\{\beta_{r}:i\in\mathcal{V}_{\mathcal{R}}\}}\\ {\tilde{F}[\tilde{P}_{\Phi},Q]}\end{array}
$$ 

$$
\begin{array}{r l r}{\displaystyle\sum_{\boldsymbol{C}_{r^{\prime}}-\boldsymbol{C}_{r}}\beta_{r^{\prime}}(\boldsymbol{c}_{r^{\prime}})}&{=}&{\beta_{r}(\boldsymbol{c}_{r})\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(11.30)}\\ &{}&{\quad\quad\quad\quad\quad\quad\quad\quad\forall r\in\mathcal{V}_{\mathcal{R}},\forall r^{\prime}\in\mathbf{U}\mathbf{p}(r),\forall\boldsymbol{c}_{r}\in V a l(\boldsymbol{C}_{r})}\\ {\displaystyle\sum_{\boldsymbol{C}_{r}}\beta_{r}(\boldsymbol{c}_{r})}&{=}&{1\quad\quad\quad\quad\forall r\in\mathcal{V}_{\mathcal{R}}\quad\quad\quad\quad\quad\quad\quad(11.31)}\\ {\displaystyle\beta_{r}(\boldsymbol{c}_{r})}&{\geq}&{0\quad\quad\quad\quad\forall r\in\mathcal{V}_{\mathcal{R}},\boldsymbol{c}_{r}\in V a l(\boldsymbol{C}_{r}).\quad(11.32)}\end{array}
$$ 

Our strategy for devising algorithms for solving this optimization problem is similar to the approach we took in section 11.3.6. Using the method of Lagrange multipliers, we characterize the stationary points of the target function (given the constraints) as a set of ﬁxed-point equations. We then ﬁnd an iterative algorithm that attempts to reach such a stationary point. 

We ﬁrst characterize the ﬁxed point via the Lagrange multipliers. As before, we form a Lagrangian by introducing terms for each of the constraints: from equation (11.30), we obtain a Lagrange multiplier $\lambda_{r,r^{\prime}}(c_{r})$ for every pair $r^{\prime}\,\in\,\mathbf{Up}(r)$ a d every $c_{r}\ \in\ V a l(C_{r})$ rom equation (11.31), we obtain a Lagrange multiplier $\lambda_{r}$ for every r and every $c_{r}\,\in\,V a l(C_{r})$ ∈ ; as before, we assume that we are dealing with interior ﬁxed points only, and so do not have to worry about the inequality constraint. We diferentiate the Lagrangian relative to each of the region beliefs $\beta_{r}(c_{r})$ , and obtain the following set of ﬁxed-point equations : 

$$
\kappa_{r}\ln\beta_{r}(\pmb{c}_{r})=\lambda_{r}+\kappa_{r}\ln\psi_{r}(\pmb{c}_{r})-\sum_{r^{\prime}\in\mathbf{Lip}(r)}\lambda_{r,r^{\prime}}(\pmb{c}_{r})+\sum_{r^{\prime}\in\mathbf{Lip}(r)}\lambda_{r^{\prime},r}(\pmb{c}_{r^{\prime}})-\kappa_{r}\ln\psi_{r}(\pmb{c}_{r})
$$ 

For regions for which $\kappa_{r}\neq0$ , we can rewrite this equation to conclude that: 

$$
\beta_{r}(\pmb{c}_{r})=\frac{1}{Z_{r}}\psi_{r}(\pmb{c}_{r})\left(\frac{\prod_{r^{\prime}\in\mathbf{D o w n}(r)}\delta_{r\rightarrow r^{\prime}}(\pmb{c}_{r^{\prime}})}{\prod_{r^{\prime}\in\mathbf{U p}(r)}\delta_{r^{\prime}\rightarrow r}(\pmb{c}_{r})}\right)^{1/\kappa_{r}}.
$$ 

From this equality, one can conclude the following result: 

Theorem 11.6 Assume that our region graph satisﬁes the family preservation property. Then, at ﬁxed points of the RegionGraph-Optimize optimization problem, we have that: 

$$
P_{\Phi}(\mathcal{X})\propto\prod_{r}(\beta_{r})^{\kappa_{r}}.
$$ 

The proof is derived from a simple cancellation of messages in the diferent terms (see exer- cise 11.16). 

This result tells us that we can reparameterize the initial distribution $P_{\Phi}$ in terms of the ﬁnal beliefs obtained as ﬁxed points of the region graph optimization problem. It tells us that we can represent the distribution in terms of a calibrated set of beliefs for the individual regions. This 

![](images/283353daf637649d98c3dc51f10bfe5acbbd3eb50d0277dead3a5afa03f1c7ae.jpg) 
Figure 11.11 The messages participating in diferent region graph computations. Participating mes- sages are marked with thicker arrows. (a) The computation of the beliefs $\beta_{r}(C_{r})$ ; (b) The set $N(r_{u},r_{d})$ participating in the computation of the message $\delta_{r_{u}\rightarrow r_{d}}$ ; (c) The set $D(r_{u},r_{d})$ participating in the com- putation of the message $\delta_{r_{u}\rightarrow r_{d}}$ . 

result is a very powerful one, because it holds for any set of counting numbers that satisﬁes the family preservation property — a very large class. Of course, this result only shows that any ﬁxed point is a re parameter iz ation of the distribution, but not that such a re parameter iz ation exists. However, under our assumption that all of the initial factors in $\Phi$ are strictly positive, one can show that such a ﬁxed point, and hence a corresponding re parameter iz ation, necessarily exists. 

message passing algorithm 

As we can see, unlike the case of cluster graphs, the ﬁxed-point equations for region graphs are more involved, and do not lead directly to an elegant message passing algorithm . Indeed, the derivation of the update rules from the Lagrange multipliers often involves multiple steps of algebraic manipulation. (Although such derivations are possible in restricted cases; see exercise 11.17 and exercise 11.19.) In the remainder of this section, we present, without derivation, one set of update rules that can be derived from equation (11.34), in the speciﬁc case where the counting numbers are as presented in equation (11.29). 

The basic idea is similar to the message update used in cluster graphs. There, each sepset carried messages sent by one of the clusters through the sepset to the neighboring cluster. We can think of that as a message from the cluster to the sepset. The analog of this in region graph is a message from a region t ion below it. Thus, for each pair $r_{1}\,\rightarrow\,r_{2}$ in the region graph we will have a message $\delta_{r_{1}\to r_{2}}$ whose scope is Scope [ r ] . All messages are associated with → 2 “downward” edges of the form $r_{1}\rightarrow r_{2}$ , but they are used to deﬁne the beliefs and messages of regions that contain them. 

The deﬁnitions of the messages and the beliefs are somewhat involved. We begin by deﬁning the beliefs of a region as a function of these messages, which is somewhat simpler: 

$$
\begin{array}{r l r}{\beta_{r}(\mathbf{\cal{C}}_{r})}&{=}&{\psi_{r}(\mathbf{\cal{C}}_{r})\left(\displaystyle\prod_{r_{u}\in{\bf U p}(r)}\delta_{r_{u}\to r}(\mathbf{\cal{C}}_{r})\right)}\\ &{}&{\left(\displaystyle\prod_{r_{d}\in{\bf D o w n}^{*}(r)}\prod_{r_{u}\in{\bf U p}(r_{d})-{\bf D o w n}^{+}(r)}\delta_{r_{u}\to r_{d}}(\mathbf{\cal{C}}_{r_{d}})\right).}\end{array}
$$ 

In other words, the belief of a region is the product of three groups of terms. The ﬁrst two are very natural: the initial beliefs $\psi_{r}$ ( 1 for all regions except the top ones) and the messages from its upward regions. The last group contain all messages sent to regions below the region $r$ from regions other than the region $r$ itself and regions below it. In other words, these are messages from “external” sources to regions below the region $r$ ; see ﬁgure 11.11a. Thus, the beliefs of the region are not inﬂuenced by messages it sends down, but only by messages sent to it or its subsets from other regions. 

Again, it is instructive to compare this deﬁnition to our deﬁnition of beliefs in cluster graphs. In cluster graphs, the belief over a sepset is the product of messages from the neighboring clusters. These messages correspond in our case to messages from upward regions. The belief over a cluster $C$ is the product of its initial potential and messages sent from neighboring clusters to the sepsets adjacent to $C$ . The sepsets correspond to the regions in $\mathbf{D o w n}^{+}(C)$ ; the message sent by another clique $C^{\prime}$ to this sepset corresponds to messages sent by an “external” source to a region in $\mathbf{D o w n}^{+}(C)$ . 

We now move to deﬁning the computation of a message from $r_{u}$ to $r_{d}$ , also illustrated in ﬁgure 11.11b,c: 

$$
\delta_{r_{u}\rightarrow r_{d}}(C_{r_{d}})=\frac{\sum_{C_{r_{u}}-C_{r_{d}}}\psi_{r_{u}}(C_{r_{u}})\prod_{r_{1}\rightarrow r_{2}\in N(r_{u},r_{d})}\delta_{r_{1}\rightarrow r_{2}}(C_{r_{2}})}{\prod_{r_{1}\rightarrow r_{2}\in D(r_{u},r_{d})}\delta_{r_{1}\rightarrow r_{2}}(C_{r_{2}}).}
$$ 

The numerator involves the initial factor assigned to the region, and a product of messages associated with the set of edges 

$$
N(r_{u},r_{d})=\{(r_{1}\to r_{2})\in{\mathscr{E}}_{\mathcal{R}}:r_{1}\notin\mathbf{D o w n}^{+}(r_{u}),r_{2}\in\mathbf{D o w n}^{+}(r_{u})-\mathbf{D o w n}^{+}(r_{u})\}
$$ 

This set contains edges from sources “external” to $r_{u}$ that are outside the scope of inﬂuence of $r_{d};$ that is, they either enter $r_{u}$ directly, or enter regions below $r_{u}$ that are not below $r_{d}$ . The denominator involves a product of the messages in the set: 

$$
D(r_{u},r_{d})=\{(r_{1}\to r_{2})\in{\mathscr{E}}_{\mathcal{R}}:r_{1}\in\mathbf{D o w n}^{+}(r_{u})-\mathbf{D o w n}^{+}(r_{d}),r_{2}\in\mathbf{D o w n}^{+}(r_{2})\}
$$ 

This set counts information that would be passed from $r_{u}$ to regions below $r_{d}$ indirectly — not through $r_{d}$ . We want to divide by these messages, since otherwise the same information would be incorporated multiple times into the beliefs and the messages. 

Applying equation (11.36) to this example, we get a set of equation representing the potentials as function of the initial factors and the messages: 

$$
\begin{array}{l c l}{{\beta_{1}}}&{{=}}&{{\psi_{1}\delta_{2\rightarrow4}\delta_{3\rightarrow5}\delta_{6\rightarrow7}}}\\ {{\beta_{2}}}&{{=}}&{{\psi_{2}\delta_{1\rightarrow4}\delta_{3\rightarrow6}\delta_{5\rightarrow7}}}\\ {{\beta_{3}}}&{{=}}&{{\psi_{3}\delta_{1\rightarrow5}\delta_{2\rightarrow6}\delta_{4\rightarrow7}}}\\ {{\beta_{4}}}&{{=}}&{{\delta_{1\rightarrow4}\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}}}\\ {{\beta_{5}}}&{{=}}&{{\delta_{1\rightarrow5}\delta_{3\rightarrow5}\delta_{4\rightarrow7}\delta_{6\rightarrow7}}}\\ {{\beta_{6}}}&{{=}}&{{\delta_{2\rightarrow6}\delta_{3\rightarrow6}\delta_{4\rightarrow7}\delta_{5\rightarrow7}}}\\ {{\beta_{7}}}&{{=}}&{{\delta_{4\rightarrow7}\delta_{5\rightarrow7}\delta_{6\rightarrow7}.}}\end{array}
$$ 

Applying equation (11.37), we can construct the messages. For example, 

$$
\delta_{4\rightarrow7}=\sum_{B}\delta_{1\rightarrow4}\delta_{2\rightarrow4}.
$$ 

One easy way to derive this message directly is to use the marginal consistency constraint: 

$$
\beta_{7}=\sum_{B}\beta_{4}.
$$ 

Plugging in the expanded form of the two beliefs, we get 

$$
\delta_{4\rightarrow7}\delta_{5\rightarrow7}\delta_{6\rightarrow7}=\sum_{B}\delta_{1\rightarrow4}\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}.
$$ 

If we now isolate $\delta_{4\rightarrow7}\ w e$ get 

$$
\delta_{4\rightarrow7}=\frac{\sum_{b}\delta_{1\rightarrow4}\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}}{\delta_{5\rightarrow7}\delta_{6\rightarrow7}}.
$$ 

After we cancel out the common terms $\delta_{5\rightarrow7}$ and $\delta_{6\rightarrow7}$ , we get the desired form. 

This message is essentially identical to the message in a cluster graph where we marginalize the other incoming messages in the cluster to send a message to a particular sepset. Here region 4 behaves as a cluster and region 7 as a sepset. The other messages incoming to region 7 have a similar form. 

Messages into the middle layer regions have more complex form. For example 

$$
\delta_{1\rightarrow4}=\frac{\sum_{A}\psi_{1}\delta_{3\rightarrow5}}{\delta_{5\rightarrow7}}.
$$ 

Again, we can use the marginal consistency constraint 

$$
\beta_{4}=\sum_{A}\beta_{1}
$$ 

to reconstruct the message. Plugging in the expanded form of the two beliefs, and isolating $\delta_{1\rightarrow4}$ we get 

$$
\delta_{1\rightarrow4}=\frac{\sum_{A}\psi_{1}\delta_{2\rightarrow4}\delta_{3\rightarrow5}\delta_{6\rightarrow7}}{\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}}.
$$ 

After we cancel out $\delta_{2\rightarrow4}$ and $\delta_{6\rightarrow7}$ , we get the desired form. 

These deﬁnitions set up a message passing algorithm similar to CGraph-SP-Calibrate , except that we use the messages as formulated in equation (11.37). As with belief propagation on cluster graphs, we can prove that convergence points of such propagations are stationary points of the RegionGraph-Optimize optimization problem. 

A set of beliefs $Q$ is a stationary point of RegionGraph-Optimize for region graph $\mathcal{R}$ if and only if for every edge $(i\!-\!j)\in\mathcal{E}_{\mathcal{R}}$ there are auxiliary factors $\delta_{u\rightarrow d}(C_{d})$ that satisfy equation (11.36) and equation (11.37). 

This result is a direct generalization of theorem 11.5, and is proved in a similar way. We leave the detail as an exercise (see exercise 11.14). Much of the discussion following theorem 11.5 applies here. In particular, we do not have guarantees that iterations of message passing will converge. However, if they do, we have reached a stationary point of the energy functional. In practice, the experience is that when we consider moving from the Bethe approximation to “richer” region graphs that contain intermediate regions with larger subsets, problems of nonconverging runs are less common. For example, a region graph construction for grids is much more convergent than the corresponding cluster graph (see exercise 11.15). However, except for special cases (for example, region graphs that correspond to cluster trees), we do not know how to characterize region graphs where belief propagation converges. 

# 11.3.8 Discussion 

Cluster-graph belief propagation methods such as the ones we have described in this chapter pro- vide a general-purpose mechanism for approximate inference in graphical models. In principle, they apply to any network, including networks with high tree-width, for which exact inference is intractable. They have been applied successfully to a large number of dramatically diferent applications, including (among many others) message decoding in communication over a noisy channel (see box 11.A), predicting protein structure (see box 20.B), and image segmentation (see box 4.B). 

However, it is important to keep in mind that cluster-graph belief propagation is not  a global panacea to the problem of inference in graphical models. The algorithm may not converge, and when it does converge, there may be multiple diferent convergence points. Although there are currently no conditions characterizing precisely when cluster-graph belief propagation converges, several factors seem to play a role. 

The ﬁrst is the topology of the network: A network containing a large number of short loops is more likely to be nonconvergent. Although this notion has been elusive to characterize in practice, it has been shown that cluster-graph belief propagation is guaranteed to converge on networks with a single loop. 

An even more signiﬁcant factor is the extent to which the factors parameterizing the network are skewed, or close to deterministic. Intuitively, deterministic factors can cause difculties in several ways. First, they often induce strong correlations between variables, which cluster-graph belief propagation (depending on the approximation chosen) can lose. This error can have an efect not only for the correlated variables, but also for marginals of variables that interact with both. Second, close-to-deterministic factors allow information to be propagated reliably through long paths in the network. Recall that part of our motivation for the running intersection property was to prevent information about some variable to be propagated inﬁnitely through a loop. While the running intersection property prevents such loops from occurring structurally, deterministic potentials allow us to recreate them using an appropriate choice of parameters. For example, if $A$ is deterministic ally equal to $B$ , then we can have a cycle of clusters where $A$ appears in some of the clusters and $B$ in others. Although this cluster graph may satisfy the running intersection property relative to $A$ , efectively there is a cycle in which the same variable appears in all clusters. Finally, as we discussed in section 11.3.4, factors that are less skewed provide smoothing of the messages, reducing oscillations; indeed, one can even prove that, if the skew of the factors in the network is sufciently bounded, it can give rise to a contraction property that guarantees convergence. 

In summary, the key factor relating to convergence of belief propagation appears to be the extent to which the network contains strong inﬂuences that “pull” a variable in diferent directions. Owing to its local nature, the algorithm is incapable of reconciling these diferent constraints, and it can therefore oscillate as diferent messages arrive that pull it in one direction or another. 

A second problem relates to the quality of the results obtained. Despite the appeal and im- portance of the energy-based analysis, it does not (except in a few rare cases — see section 11.7) provide any guarantees on the accuracy of the marginals obtained by cluster-graph belief prop- agation. This is in contrast to the sampling-based methods of chapter 12, where we are at least assured that, if we run the algorithm for long enough, we will obtain accurate estimates of the posteriors. (Of course, the key question of “how long is long enough” does not usually have an answer, so it is not clear how important this distinction is in practice.) Empirical results show that, in the settings where cluster-graph belief propagation convergence is more likely (not too many tight loops, no highly skewed factors), one also often obtains reasonable answers. 

Importantly, these answers are often good but overconﬁdent: The value $x\ \in\ V a l(X)$ to which cluster-graph belief propagation gives the highest probability is often the value for which $P_{\Phi}(X=x)$ is indeed the highest, but the probability assigned to $x$ by the approximation is often too high. This phenomenon arises (partly) from the fact the cluster-graph belief propagation ignores correlations between messages and can therefore count the same piece of evidence multiple times as it arrives along diferent paths, leading to overly strong conclusions. In other cases, however, the answers obtained by cluster-graph belief propagation are simply wrong (see section 11.3.1); unfortunately, there is currently no way of determining when the answers returned by a run of cluster-graph belief propagation are reasonable approximations to the true marginals. 

The intuitions described previously do help us, however, to design approximations that are more likely to produce good answers. In general, we cannot construct a cluster graph that pre- serves all of the higher-order interactions among the factors. Hence, we need to decide which factors to include in the cluster graph and how to relate them. As the preceding discussion suggests, we do better if we construct approximations that incorporate tight loops and maintain the strongest factors within clusters as much as possible. While these intuitions provide reason- able rules of thumb on how to construct approximations, it is not obvious how to capture them within a general-purpose automated cluster-graph construction procedure. 

![](images/bd427b567b9970d4db45cb5b263b33b4a651f3924f3fdcd9db6ecfbc7705c043.jpg) 
Figure 11.12 A cluster for a $4\times4$ grid network. The structure within each cluster represents the arcs whose factors are assigned to that cluster. 

# 11.4 Propagation with Approximate Messages $\star$ 

The cluster-graph belief propagation methods achieved approximation by “relaxing” the require- ment of having a cluster tree. Instead, we used a cluster graph and constructed an approximation by a set of pseudo-marginals. This approximation avoided the need to construct large clusters, which incur an exponential cost in terms of memory and running time. In this section, we consider an approach in which the simpliﬁcation occurs within a given cluster structure; rather than simplify this structure, we perform approximate propagation steps within it. This allows us to keep the correct clusters and gain efciency by using more compact representations and operations on these clusters (at the cost of introducing approxima- tions). Importantly, this approach is orthogonal to the methods we described in the previous section, in that approximate message passing can occur both within a clique tree or a cluster graph approximation. For ease of presentation, we focus on the case of clique trees in this section, but the ideas easily carry through to the more general setting. 

The basic concept behind the methods described in this section is the use of approximate messages in clique tree (or cluster graph) propagation. Instead of representing messages in the clique trees as factors, we use more compact representations of approximate factors. There are many diferent schemes that we can use for approximating messages. To ground the discussion, we begin in section 11.4.1 by describing one important instantiation of this general framework, which is very natural in our setting — message approximation using a factored form (for example, as a product of marginals). In section 11.4.2 and section 11.4.3 we then discuss algorithms that perform message passing using approximate messages. In section 11.4.4 we describe a general algorithm, called expectation propagation , that applies to any approximation in the exponential family. Finally, in section 11.4.5, we show that the expectation propagation algorithm for the exponential family can be derived from a variational analysis, similar to the ones we discussed in the previous section. 

![](images/0aad29acd7c044957485d7ef03b7e699255eb75c20e14444852134641e59eba4.jpg) 
Figure 11.13 Efect of diferent message factorizations on the structure of the belief factor $\tilde{\beta}_{2}$ in the example of ﬁgure 11.12. 

# 11.4.1 Factorized Messages 

We begin by considering a concrete example where message approximation may be useful. Consider again the $4\times4$ grid network of ﬁgure 11.4. We can construct a cluster tree for this network as shown in ﬁgure 11.12. (Note that this cluster tree is not the optimal one for this network.) In our discussion of inference until now, we ignored the inner structure of clusters and treated the cluster as being associated with a single factor. Now, however, we keep track of the structure of the original factors associated with the cluster. We will see shortly why keeping this structure will help us. In ﬁgure 11.12 this structure is portrayed by “subnetworks” within each cluster. 

Calibration in this cluster tree involves sending messages that are joint measures over four variables. An intuitive idea is to simplify the messages by using a factored representation. For example, consider the message $\delta_{1\rightarrow2}$ , and suppose that we approximate it by a factored form 

$$
\tilde{\delta}_{1\rightarrow2}[A_{1,1},A_{2,1},A_{3,1},A_{4,1}]=\tilde{\delta}_{1\rightarrow2}[A_{1,1}]\tilde{\delta}_{1\rightarrow2}[A_{2,1}]\tilde{\delta}_{1\rightarrow2}[A_{3,1}]\tilde{\delta}_{1\rightarrow2}[A_{4,1}].
$$ 

What can we gain from such an approximation? Clearly, this form provides a more concise representation of the message. Instead of representing the message with values for the sixteen possible joint assignments, we send a message with two values for each variables (leading to a total of eight “parameters”). If we consider bigger grids, then the saving will be more substantial. 

Does this saving gain us efciency? Naively, the answer is no. Recall that the main compu- tational cost of exact inference is generating the message in a cluster. This requires multiplying incoming messages with the original factor of the cluster and marginalization. In our example, $C_{2}$ involves eight variables, and so this operation should involve operations over the $2^{8}=\bar{2}56$ joint values of these variables. Thus, one might argue that the saving of eight parameters in representing the message does not deal with the core computational problem we have at hand and leads to negligible improvement. 

It turns out, however, that if we consider the internal structure of the cluster we can exploit the factored form of the message. Consider computing the beliefs over $C_{2}$ given messages from $C_{1}$ and $C_{3}$ . In exact computation, we multiply the potential $\psi_{2}$ with $\delta_{1\rightarrow2}$ and $\delta_{3\rightarrow2}$ and normalize to get the beliefs about $C_{2}$ . However, if we approximate both messages by a product of univariate terms, we notice that the product of the messages with the factors in $C_{2}$ forms a network structure that we can exploit. In our example, this network is a tree-structured network shown in ﬁgure 11.13a. We can easily calibrate this network and answer queries about the beliefs over $C_{2}$ without enumerating all joint assignments to variables in this cluster. 

We can get similar savings even if we use a richer approximation that can better capture dependencies among variables in the message. Suppose we approximate $\delta_{1\rightarrow2}$ by a factored form that corresponds to the chain structure $A_{1,1}{-}A_{2,1}{-}A_{3,1}{-}A_{4,1}$ . This approximation makes conditional independence assumptions about the variables, but it captures some of the dependencies among them. In this example, this representation actually captures the message $\delta_{1\rightarrow2}$ . However, we can check that $\delta_{3\rightarrow2}$ does not satisfy the conditional independence of $A_{1,2}$ and $A_{3,2}$ given $A_{2,2}$ . Thus, in this case, a chain representation is an approximation. If we multiply these two approximations with the cluster $C_{2}$ , we get a set of factors that has the structure shown in ﬁgure 11.13b. Although not a tree, this graph has a tree-width of 2, regardless of the grid size. Thus, once again, we can use exact inference methods on the resulting product of factors. 

factor set 

factor set product 

factor set marginalization 

We can exploit this intuition by maintaining both the initial potentials and the messages as factor sets . For the initial potential, these factors are the parameter iz ation of the original network; for messages, these factors are introduced by the approximation. A factor set $\vec{\phi}=\{\phi_{1},.\.\,.\,,\phi_{k}\}$ } provides a compact representation for the higher-dimensional factor $\phi_{1}\cdot\phi_{2}\cdot\cdot\cdot\cdot\phi_{k}$ . 

Recall that belief propagation involves two main operations: product and marginalization. The product of factor sets is easy. Suppose we have the factor sets $\vec{\phi}_{1}$ and $\vec{\phi}_{2}$ . The factor set $\vec{\phi}_{1}\cdot\vec{\phi}_{2}$ · is simply the factor set that contains the union of the two factor sets (we assume that the components of the factor sets are distinct). 

The difcult operation is marginalization . Suppose we have a factor set $\vec{\phi}=\left\{\phi_{1},.\,.\,.\,,\phi_{k}\right\}$ { } , d we consider the marginalization $\textstyle\sum_{X}{\vec{\phi}}$ . This operation couples all the factors that contain $X$ . In general, for a well-constructed clique tree, the message that results from marginalizing a clique will not satisfy any conditional independence statements and therefore cannot be factorized (see exercise 11.21). 

Returning to our example of ﬁgure 11.12, one of our inference steps is to compute: 

$$
\delta_{2\rightarrow3}=\sum_{A_{1,1},A_{2,1},A_{3,1},A_{4,1}}\psi_{2}\cdot\delta_{1\rightarrow2}.
$$ 

To achieve the efcient inference steps we discussed earlier, we want to approximate $\delta_{2\rightarrow3}$ by a product of simpler terms. This is an instance of a problem we encountered in section $8.5\,-$ approximating a given distribution by another distribution from a given family of distributions. In our case, the approximating family is the set of distributions that take a particular product form. As we discussed in section 8.5, there are several ways of projecting a distribution $P$ into some constrained class of distributions $\mathcal{Q}$ . Inde , throughout our discussion this chapter so far, we have been searching for a distribution Q which is the I-projection of $P_{\Phi}$ — the one 

![](images/09e4ad4f7f0867d3993e69f5df1365de0816b9b6b7b4de4dc3c4a130e2ec3bc6.jpg) 
Figure 11.14 Example of propagation in cluster tree with factorized messages 

that minimizes $D(Q\|P_{\Phi})$ | | . However, as we discussed in section 8.5, we cannot compute the I-projection of a distribution in closed form. Conversely, the M-projection for many classes $\mathcal{Q}$ of factored distributions has an easy closed form expression. For example, the M-projection of $P$ onto the class of factored distributions is simply the product of marginals of $P$ (see proposition 8.3). We can compute these marginals by computing the marginals of the individual variables in the message. We note that the message is often not normalized, and is therefore not a distribution; however, we can normalize the message and treat it as a distribution without changing the ﬁnal posterior obtained by the inference algorithm (see exercise 9.3). 

Given the simplicity of M-projections, one might wonder why we used I-projections in our discussion so far. There are two main reasons. First, the theory of energy functionals allows us to use I-projections to lower-bound the partition function. Second, and more importantly, computing marginal probabilities for our distribution $P_{\Phi}$ is generally intractable. Thus, although the M-projection has a simple form, it is infeasible to apply when we have an intractable network structure. In our current setting, the situation is diferent. Here, our projection task is to approximate the outgoing message from a cluster $C$ . As discussed earlier, we assume that the product of the beliefs and the approximated messages for $C$ is a factor set with tractable structure. This assumption allows us to use exact inference within the cluster $C$ to compute the marginal probabilities needed for M-projections. 

# 11.4.2 Approximate Message Computation 

We now discuss in greater detail the task of computing factorized messages. To begin, consider the projection of $\delta_{2\rightarrow3}$ into a fully factorized representation. According to proposition 8.3, to build the M-projection of $\delta_{2\rightarrow3}$ onto a factored distribution, we need to compute the marginals of $A_{1,2},\,A_{2,2},\,A_{3,2}$ , and $A_{4,2}$ in $\delta_{2\rightarrow3}$ . Since $\delta_{2\rightarrow3}$ is deﬁned to be the marginal of the factor set $\psi_{2}\cdot\delta_{1\rightarrow2}$ , we need to compute terms such as $P_{\psi_{2}\cdot\delta_{1\rightarrow2}}(A_{1,2})$ , where we use $P_{\psi_{2}\cdot\delta_{1\rightarrow2}}$ to denote the measure represented by the factor set. At an abstract level, this operation involves computing $\delta_{2\rightarrow3}$ and then projecting it onto the simpler representation. However, if we examine the structure of this factor set (see ﬁgure 11.14), we see that this computation can be done using standard exact inference on the factor set. In this particular case, the factor set is a tree network, and so inference is cheap. Similarly, we can compute the marginals over the variables in $\delta_{2\rightarrow3}$ . Thus, we can use the properties of M-projections and exact inference to compute the resulting projected message without ever explicitly enumerating the joint over the four variables in $\delta_{2\rightarrow3}$ . In an $n\times n$ grid, for example, we can perform these operations in time that is linear in $n$ , whereas the explicit computation would have constructed a factor of exponential size. As we discussed, the more complex approximation of ﬁgure 11.13b also has a bounded tree-width of 2, and therefore also allows linear time inference. 

![](images/c74a3debe3147fb364d691b26e92425dddbf5a3baedb44c52473d496040c163b.jpg) 

The overall structure of the algorithm is shown in algorithm 11.4. At a high level, we can view exact inference in the factor set as a “black box” and not concern ourselves with the exact implementation. However, it is also useful to consider how this type of projected message may be computed efciently. Most often, the inference data structure $\mathcal{U}$ is a cluster graph or a clique tree, into which the initial factors $\vec{\phi}$ can easily be incorporated, and from which the target factors, of the appropriate scopes, can be easily extracted. To allow for that, we typically design the cluster graph to be family-preserving with respect to both sets of factors. Under that assumption, we can extract a factor $\psi_{j}$ over the scope $Y_{j}$ by identifying a cluster $C_{j}$ in $\mathcal{U}$ whose scope contains $Y_{j}$ , and marginalizing it over $Y_{j}$ . As an alternative approach, we can construct an unconstrained clique tree, and use the out-of-clique inference algorithm of section 10.3.3.2 to extract from the graph the joint marginals of subsets of variables that are not together in a clique. (We note that out-of-clique inference is more challenging in the context of cluster graphs, since the path used to relate the clusters containing the query variables can afect the outcome of the computation; see exercise 11.22.) 

If our representation of a message is simply a product of marginals over disjoint subsets of variables, this algorithm sufces to produce the output message: We produce a factor over each (disjoint) scope $Y_{j};$ the product of these factors is the M-projection of the distribution. But for richer approximations, the required operation is somewhat more complex. 

Example 11.6 Consider again our grid example of ﬁgure 11.13b. Here, the clique needs to take in messages that involve factors over pairs of variables $A_{i,1},A_{i+1,1}$ and produce messages over pairs of variables $A_{i,2},A_{i+1,2}$ . We can accomplish this goal by constructing, within this clique, a nested clique tree that has at least one clique containing each of these pairs. For this purpose, we can use any clique tree based on triangulating the structure inside the clique in ﬁgure 11.13b. For example, we can have a tree with the cliques $\{A_{1,1},A_{1,2},A_{2,2}\}$ , $\{A_{1,1},A_{2,1},A_{2,2}\}$ , $\{A_{2,1},A_{2,2},A_{3,2}\}$ , $\{A_{2,1},A_{3,1},A_{3,2}\}$ , $\{A_{3,1},A_{3,2},A_{4,2}\}$ , $\{A_{3,1},A_{4,1},A_{4,2}\}$ . 

Our goal now is to produce a set of factors that is the M-projection of the true message onto the chain. Assume that we extract from the clique tree the pairwise marginals $P(A_{1,2},A_{2,2})$ , $P(A_{2,2},A_{3,2})$ , and $P(A_{3,2},A_{4,2})$ . However, we cannot directly encode the distribution using these factors as a factor set, since this approach would double-count the probability of the singletons $A_{2,2}$ and $A_{3,2}$ , each of which appears in two factors. To produce the true $M$ -projected distribution, we need to divide by the double-counted marginals $A_{2,2}$ and $A_{3,2}$ . 

We can achieve this correction easily by computing these node marginals and adding to our factor set representation two factors 

$$
\frac{1}{\tilde{\delta}_{2\rightarrow3}(A_{2,2})},\frac{1}{\tilde{\delta}_{2\rightarrow3}(A_{3,2})}
$$ 

that compensate for the double-counting. This factor set represents the M-projection of the distribu- tion, and it can be used in subsequent message passing steps. Equivalently, we are representing the distribution as a calibrated clique tree over $A_{1,2},A_{2,2},A_{3,2},A_{4,2}$ , where the factors derived from the pairwise marginals are the clique beliefs, the factors derived from inverted node marginals are the sepset messages, and the overall distribution is encoded as in equation (10.11). 

We can easily generalize this approach to more complex message representations. Most generally, assume that we choose to encode our message between cluster $i$ and cluster $j$ using a representation as in equation (11.35); more precisely, we have some set of factors $\{\beta_{r}(X_{r})$ : $X_{r}\in\mathbf{R}_{i,j}\}$ , each raised to some power: 

$$
\tilde{\delta}_{i\rightarrow j}=\prod_{r\in\mathbf{R}_{i,j}}(\beta_{r}(\pmb{X}_{r}))^{\kappa_{r}}.
$$ 

We can compute this approximation in our procedure using the Factored-Project algorithm, using the set $\{X_{r}\in{\bf R}_{i,j}\}$ as $y$ . The output of this procedure is a set of calibrated marginals over these scopes, and can therefore be plugged into equation (11.38) to produce a message in the appropriate factored form. The same factors, each raised to its appropriate power, can be used as the factorized message passed to cluster $j$ . 

Importantly, we note that this approach does not always provide an exact M-projection of the true message. This property is guaranteed only when the set of regions forms a clique tree, allowing the M-projection to be calculated in closed form using equation (11.38); in other cases, we obtain only an approximation to the M-projection. In practice, we often choose some simple clique tree, as in example 11.6, to encode our distribution, allowing the M-projection to be performed easily and exactly. However, in some cases, a clique tree approximation, to stay tractable, is forced to make too many independence assumptions, leading to a poor approximation of the message. Hence, the approach of an approximation M-projection into a richer class of distributions may provide a useful alternative in some cases. 

In summary, we have shown how we can perform an approximate message passing step with structured messages. The algorithm maintains messages and beliefs in factored form. Factors are entered into a nested clique tree or cluster graph, and message propagation is used to compute the factors describing the output message. In the fully factored case, this representation is simply a set of factors, and the multiplication operation used in message passing is simply a union of factor sets. In the more complex setting, we may need to postprocess the set of marginals — exponentiating them by appropriate counting numbers — to eliminate double-counting. The resulting set of factors is the compact representation of the outgoing message. 

# 11.4.3 Inference with Approximate Messages 

Equipped with these operations on factor sets, we can consider how to use these tools within cluster tree propagation. Our algorithm will maintain the beliefs at each cluster $i$ using a factor set $\vec{\phi}_{i}$ . Initially, we are given a cluster tree $\mathcal{T}$ with an assignment of the original factors to clusters. We have also determined the factorized form for each sepset, in terms of a network structure $\mathcal{G}_{i,j}$ that describes the desired factorization for $\tilde{\delta}_{i\to j}$ and $\tilde{\tilde{\delta}}_{j\rightarrow i}$ . 

There are two main strategies for applying the ideas described in the previous section to deﬁne an approximate message passing algorithms in clique trees. One is based on a sum- product message passing scheme, and the other on belief update messages. As we will see, although these two strategies are equivalent in the exact inference setting, they lead to fairly diferent algorithms when we consider approximations. 

For notational simplicity, we introduce the notation M-project- $\mathrm{distr}_{i,j}$ to be the combined operation of marginalizing variables that do not appear in $\boldsymbol{S}_{i,j}$ and performing the M-projection onto the set of distributions that can be factorized according to $\mathcal{G}_{i,j}$ . 

# 11.4.3.1 Sum-Product Propagation 

Consider the application of sum-product propagation ( CTree-SP-Calibrate , algorithm 10.2), to our grid example. In this case, we need to perform the following operations: 

$$
\begin{array}{l c l}{{\delta_{1\rightarrow2}}}&{{=}}&{{\psi_{1}}}\\ {{\delta_{2\rightarrow3}}}&{{=}}&{{\displaystyle\sum_{A_{1,1},\ldots,A_{4,1}}\psi_{2}\cdot\delta_{1\rightarrow2}}}\\ {{}}&{{}}&{{}}\\ {{\delta_{3\rightarrow4}}}&{{=}}&{{\displaystyle\sum_{A_{1,2},\ldots,A_{4,2}}\psi_{3}\cdot\delta_{2\rightarrow3}}}\\ {{}}&{{}}&{{}}\\ {{\delta_{4\rightarrow3}}}&{{=}}&{{\displaystyle\sum_{A_{1,4},\ldots,A_{4,4}}\psi_{4}}}\\ {{\delta_{3\rightarrow2}}}&{{=}}&{{\displaystyle\sum_{A_{1,3},\ldots,A_{4,3}}\psi_{3}\cdot\delta_{4\rightarrow3}}}\\ {{\delta_{2\rightarrow1}}}&{{=}}&{{\displaystyle\sum_{A_{1,2},\ldots,A_{4,2}}\psi_{2}\cdot\delta_{3\rightarrow2}}}\end{array}
$$ 

A straightforward application of approximation is to replace each of the messages by the M-projected version: 

$$
\begin{array}{r c l}{{\tilde{\delta}_{1\to2}}}&{{=}}&{{\mathrm{M-projecheck{\mathrm{dist}}_{1,2}(\psi_{1})}}}\\ {{\tilde{\delta}_{2\to3}}}&{{=}}&{{\mathrm{M-projecheck{\mathrm{dist}}_{2,3}(\psi_{2}\cdot\tilde{\delta}_{1\to2})}}}\\ {{\tilde{\delta}_{3\to4}}}&{{=}}&{{\mathrm{M-projecheck{\mathrm{dist}}_{3,4}(\psi_{3}\cdot\tilde{\delta}_{2\to3})}}}\\ {{\tilde{\delta}_{4\to3}}}&{{=}}&{{\mathrm{M-projecheck{\mathrm{dist}}_{3,4}(\psi_{4})}}}\\ {{\tilde{\delta}_{3\to2}}}&{{=}}&{{\mathrm{M-projecheck{\mathrm{dist}}_{2,3}(\psi_{3}\cdot\tilde{\delta}_{4\to3})}}}\\ {{\tilde{\delta}_{2\to1}}}&{{=}}&{{\mathrm{M-projecheck{\mathrm{dist}}_{1,2}(\psi_{2}\cdot\tilde{\delta}_{3\to2}).}}}\end{array}
$$ 

Each of these projection operations can be performed using the procedure described in the previous section. 

sum-product expectation propagation 

More generally, our sum-product expectation propagation procedure is identical to sum-product propagation of section 10.2, except that we modify the basic propagation procedure SP-Message so that, rather than simply marginalizing the product of factors, it computes their M-projection. Otherwise, the general structure of the propagation procedure is maintained exactly as before. Each cluster collects the messages from its neighbors and when possible sends outgoing mes- sages. As for the original sum-product message passing, this process converges after a single upward-and-downward pass over the clique tree structure. Thus, unlike most of the other approximations we discuss in this chapter, this procedure terminates in a ﬁxed number of steps. 

Note that, after performing the propagation, the ﬁnal beliefs over the individual clusters in the tree are not an explicit representation of a joint distribution over the variables in the cluster. In this case, the ﬁnal beliefs over a cluster $C$ are represented in a factorized form, as a set of beliefs. In our running example, the computed beliefs over $C_{2}$ have the form 

$$
\tilde{\beta}_{2}=\psi_{2}\cdot\tilde{\delta}_{1\rightarrow2}\tilde{\delta}_{3\rightarrow2},
$$ 

and the structure shown in ﬁgure 11.13a. Because this structure allows tractable inference, we can answer queries about the posterior distribution of these variables using standard inference methods, such as variable elimination or clique tree inference. 

These computational beneﬁts come at a price. The exact beliefs over $C_{2}$ are not decompos- able (see exercise 11.20). And thus, although forcing a particular independence structure on the marginal distribution has computational advantages, it does lose information. With this caveat in mind, we can still question whether the algorithm ﬁnds the best possible approximation within the set of constraints we are considering. 

Example 11.7 

![](images/ebc922ae08ca94974b355c8adf7c79f4206f49a5c50d729f6bcda4f30e0c803e.jpg) 

![](images/eb2474498a67c6430fede3e595eb4ab70cccf9bb52cf77d1ed7d1943f086e9cb.jpg) 

Figure 11.15 Markov network used to demonstrate approximate message passing. (a) A simple Markov network with four pairwise potentials. (b) A clique tree for this network. 

If we perform exact inference in this network, we ﬁnd the following marginal posteriors: 

$P(a^{0},b^{0})$ = 0 . 274 $P(c^{0},d^{0})$ = 0 . 102 $P(a^{0},b^{1})$ = 0 . 002 $P(c^{0},d^{1})$ = 0 . 018 $P(a^{1},b^{0})$ = 0 . 041 $P(c^{1},d^{0})$ = 0 . 368 $P(a^{1},b^{1})$ = 0 . 682 $P(c^{1},d^{1})$ = 0 . 512 . 

We see that the preference for $C=c^{1}$ is reﬂected in this distribution (with the marginal distribution $P(c^{1})\,=\,0.88\$ . In addition, the strong coupling between $A$ and $B$ is propagated through the network, which results in making $D=d^{1}$ more probable when $C=c^{1}$ . 

What happens when we perform inference using the cluster tree of ﬁgure 11.15b and use approx- imate messages that are products of marginals? It is easy to see that, because $\phi_{1}$ is symmetric, we get that $\tilde{\delta}_{1\rightarrow2}[a^{1}]\,=\,0.5$ , and $\Ddot{\delta}_{1\rightarrow2}[b^{1}]\,=\,0.5$ . We can compare the exact message and the → → approximate one 

$\delta_{1\rightarrow2}(a^{0},b^{0})$ = 0 . 495

 $\delta_{1\rightarrow2}(a^{0},b^{1})$ = 0 . 005

 $\delta_{1\rightarrow2}(a^{1},b^{0})$ = 0 . 005

 $\delta_{1\rightarrow2}(a^{1},b^{1})$ = 0 . 495 

$$
\begin{array}{r c l}{{\tilde{\delta}_{1\to2}(a^{0},b^{0})}}&{{=}}&{{0.5*0.5=0.25}}\\ {{\tilde{\delta}_{1\to2}(a^{0},b^{1})}}&{{=}}&{{0.5*0.5=0.25}}\\ {{\tilde{\delta}_{1\to2}(a^{1},b^{0})}}&{{=}}&{{0.5*0.5=0.25}}\\ {{\tilde{\delta}_{1\to2}(a^{1},b^{1})}}&{{=}}&{{0.5*0.5=0.25.}}\end{array}
$$ 

Thus, the approximate message is one where each joint assignment to $A$ and $B$ is equiprobable. This approximation loses the coupling that $\phi_{1}$ introduces between $A$ and $B$ , and therefore it is $^a$ poor approximation to the exact message. 

Next, we multiply this approximate message into the clique $C_{2}$ . The initial factor here is $\psi_{2}=$ $\phi_{2}\cdot\phi_{3}\cdot\phi_{4}$ , and after multiplying it with $\tilde{\delta}_{1\rightarrow2}$ we get the beliefs 

$$
\begin{array}{r c l}{{\tilde{\beta}_{2}(c^{0},d^{0})}}&{{=}}&{{0.021}}\\ {{\tilde{\beta}_{2}(c^{0},d^{1})}}&{{=}}&{{0.042}}\\ {{\tilde{\beta}_{2}(c^{1},d^{0})}}&{{=}}&{{0.833}}\\ {{\tilde{\beta}_{2}(c^{1},d^{1})}}&{{=}}&{{0.104.}}\end{array}
$$ 

Note that this factor is essentially a normalization of $\phi_{4}$ , since the message $\tilde{\delta}_{1\rightarrow2}$ puts a uniform → distribution of $A$ and $B$ , and since $\phi_{2}$ and $\phi_{3}$ are symmetric. Because the information about the coupling between $A$ and $B$ is not propagated into this cluster, we lose the consequent coupling between $C$ and $D$ , and the resulting approximation to $P(C,D)$ is also quite poor. 

By contrast, if we now compute $\tilde{\delta}_{2\to1}$ , we get that 

$$
\begin{array}{r c l}{{\tilde{\delta}_{2\to1}[a^{1}]}}&{{=}}&{{0.904}}\\ {{\tilde{\delta}_{2\to1}[b^{1}]}}&{{=}}&{{0.173.}}\end{array}
$$ 

This message ascribes high probability to $a^{1}$ and a low one to $b^{1}$ . This is quite diferent from the original coupling introduced by $\phi_{1}$ . Thus, when we combine the two to get the approximated posterior over $A$ and $B$ , we get the following beliefs factor: 

$$
\begin{array}{r l r}{\tilde{\beta}_{1}(a^{0},b^{0})}&{=}&{0.326}\\ {\tilde{\beta}_{1}(a^{0},b^{1})}&{=}&{0.001}\\ {\tilde{\beta}_{1}(a^{1},b^{0})}&{=}&{0.031}\\ {\tilde{\beta}_{1}(a^{1},b^{1})}&{=}&{0.642.}\end{array}
$$ 

This approximation is fairly close to the exact marginal over $A$ and $B$ . 

The problem in this example is that the approximation of $\tilde{\delta}_{1\rightarrow2}$ is done blindly, not taking → into account its efect on approximations on computations in downstream clusters. Thus, the message did not place sufcient emphasis on obtaining a correct approximation for the case $A=a^{1}$ , which roughly corresponds to the “important” (high-probability) case $C=c^{1}$ . 

# 11.4.3.2 Belief Update Propagation 

Example 11.7 shows a case where factorizing messages leads to a big error in the approximated beliefs. Let us examine this example in somewhat more detail. Given the update of $\tilde{\delta}_{2\to1}$ , the → approximation of $P(A,B)$ is more or less on target. Can we use this information to improve our approximation of $P(C,D)$ ? To do this, we would need to revise $\tilde{\delta}_{1\rightarrow2}$ . The posterior over → $A$ and $B$ informs us that most of the mass of the probability distribution is on $a^{1},b^{1}$ . With this information, we might want to change $\tilde{\delta}_{1\rightarrow2}$ to reﬂect preferences for $a^{1}$ and $b^{1}$ . 

A priori, it appears that this idea is inherently problematic; after all, a key constraint for exact inference is to avoid feedback from $\delta_{2\to1}$ to $\delta_{1\rightarrow2}$ , so as not to double-count evidence. For this reason, we took care, in the sum-product message-passing algorithm, not to multiply in the message $\delta_{2\rightarrow1}$ when passing messages from $C_{1}$ to $C_{2}$ . 

However, recall that in section 10.3, we presented the sum-product-divide update rule and showed its equivalence to the sum-product rule. Brieﬂy recapping, we can take the sum-product update rule: 

$$
\delta_{i\rightarrow j}=\sum_{C_{i}-S_{i,j}}\psi_{i}\left(\prod_{k\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\rightarrow i},\right)
$$ 

and multiply and divide by $\delta_{j\rightarrow i}$ , resulting in the rule: 

$$
\delta_{i\rightarrow j}\leftarrow\frac{\sum_{C_{i}-S_{i,j}}\beta_{i}}{\delta_{j\rightarrow i}}.
$$ 

These two rules are therefore equivalent in the exact case . However, when we consider approxi- mate inference, the situation is more complex. 

belief-update expectation propagation 

Consider now performing belief-update expectation propagation message passing. Assume, as before, that the algorithm is maintaining a set of approximate beliefs $\mathbf{\bar{\beta}}_{i}$ . We now have two possible stages in which to do the project. The ﬁrst is: 

$$
\tilde{\delta}_{i\to j}\gets\mathrm{M\cdotprojecheck{-d i s t r}}_{i,j}\left(\frac{\tilde{\beta}_{i}}{\tilde{\delta}_{j\to i}}\right).
$$ 

This version is identical to the approximate sum-product we discussed before: The beliefs factor $\beta_{i}$ accounts for all of the incoming messages; when we divide by the message $\widetilde\delta_{j\rightarrow i}$ before → projecting, we are projecting the product of all the other incoming message, which is precisely the sum-product message. 

Alternatively, we can do the projection before we divide by $\tilde{\delta}_{j\rightarrow i}$ . In this second approach, → we ﬁrst project $\tilde{\beta}_{i}$ , and then divide by $\tilde{\delta}_{j\to i}$ : 

$$
\begin{array}{r c l}{{\tilde{\sigma}_{i\to j}}}&{{\leftarrow}}&{{\mathrm{M-product-distr}_{i,j}(\tilde{\beta}_{i})}}\\ {{\tilde{\delta}_{i\to j}}}&{{\leftarrow}}&{{\frac{\tilde{\sigma}_{i\to j}}{\tilde{\delta}_{j\to i}}.}}\end{array}
$$ 

In this update, we ﬁrst collect all messages into $C_{i}$ ; we then compute the beliefs about $C_{i}$ and project this to the required form of the message. As in the exact belief update algorithm, this term accounts for information sent from $C_{j}$ . Note that both $\tilde{\sigma}_{i\rightarrow j}$ and $\overleftarrow{\tilde{\delta}}_{j\rightarrow i}$ have the same → → factorization, and hence so does their quotient $\tilde{\delta}_{i\rightarrow j}$ . 

This message, which is subsequently used to update $C_{j}$ , is very diferent from the sum- product update. This is because the incoming message was used in determining the approxima- tion. The approximation process is invariably a trade-of, in that a better approximation of some regions of the probability space results in a worse approximation in others. In the belief-update form of message passing, we can take into account the current ap- proximation of the message from the target clique when deciding on our approximation, potentially focusing more of our attention on “more relevant” parts of the space. 

To integrate this update rule into the standard belief-update message passing algorithm, we simply replace BU-Message of algorithm 10.3 with EP-Message , shown in algorithm 11.5. We note that the data structures in this procedure are slightly diferent from those in the original algorithm. First, we maintain the cluster beliefs implicitly, as a factor set, and consider the product of these factors only as part of the M-projection operation. Second, we do not only 

![](images/a5492e6768615ffb07535f61297e2be41db613986f0670f30797a643be6e7015.jpg) 

expectation propagation 

# Example 11.8 

keep the previous message sent over the edge, but rather keep the messages $\tilde{\delta}_{i\rightarrow j}$ sent in → both directions; this more reﬁned bookkeeping is necessary for dividing by the correct message following the approximation. This algorithm is called expectation propagation (EP) for reasons that are explained later. 

To understand the behavior of this algorithm, consider the application of belief-update message propagation to example 11.7. Suppose we initialize all messages to 1 and use updates of the form equation (11.39). We start by propagating a message $\tilde{\sigma}_{1\rightarrow2}\,\,\bar{=}\,\,\mathrm{M-project-distr}_{1,2}(\tilde{\beta}_{1})$ from $C_{1}$ to $C_{2}$ . Because $\tilde{\delta}_{1\rightarrow2}$ at this stage is 1, the resulting update is exactly the one we discussed in → example 11.7. If we now perform propagation from $C_{2}$ to $C_{1}$ , we get the message $\tilde{\delta}_{2\to1}$ derived in → example 11.7, multiplied by a constant (since $\tilde{\delta}_{1\rightarrow2}$ is uniform). At this point, as we discussed, the → clique beliefs $\beta_{1}$ are a fairly reasonable approximation to the posterior. 

Using the revised update rule, we now project ${\tilde{\beta}}_{1}$ , and then divide by $\tilde{\delta}_{2\to1}$ : 

$$
\tilde{\delta}_{1\to2}\gets\frac{\mathrm{M-project-distr}_{1,2}(\tilde{\beta}_{1})}{\tilde{\delta}_{2\to1}}.
$$ 

This quotient, which is then subsequently used to update $C_{2}$ , is very diferent from the previous update $\tilde{\delta}_{2\to1}$ . Speciﬁcally, The marginal ${\tilde{\beta}}_{1}$ at this stage puts a posterior of $0.642+0.031=0.673$ → on $a^{1}$ , and 0 . 642 on $b^{1}$ . To avoid double-counting the contribution of $\tilde{\delta}_{2\to1}$ , we need to divide this → marginal by this message. After we normalize messages, we obtain: 

$$
\begin{array}{r l}{\tilde{\delta}_{1\to2}[a^{1}]}&{\leftarrow\quad\frac{0.674}{0.673}\qquad=\frac{0.744}{4.15}=0.179}\\ &{\overbrace{\delta}_{1\to2}[a^{0}]}&{\leftarrow\quad\frac{0.327}{0.673}\qquad=\frac{3.406}{4.15}=0.821}\\ &{\overbrace{\delta}_{1\to2}[b^{1}]}&{\leftarrow\quad\frac{0.642}{0.673}\qquad=\frac{3.710}{0.827}}\\ &{\overbrace{\delta}_{1\to2}[b^{0}]}&{\leftarrow\quad\frac{0.642}{0.673}\qquad=\frac{3.710}{0.827}=0.895}\\ &{\overbrace{\delta}_{1\to2}[b^{0}]}&{\leftarrow\quad\frac{0.358}{0.642}\qquad=\frac{0.433}{0.385}=0.\frac{433}{4.144}=0.105.}\end{array}
$$ 

Recall that this message can be viewed as $a$ “correction term” for the sepset marginals, relative to the message $\tilde{\delta}_{2\to1}$ . Thus, its efect is to reduce the support for $a^{1}$ (which was very high in ${\tilde{\beta}}_{2}$ ), and → at the same time to increase the support for $b^{1}$ . 

Propagating this message to $C_{2}$ and updating the clique beliefs ${\tilde{\beta}}_{2}$ , we get a normalized factor of: 

$$
\begin{array}{r c l}{{\tilde{\beta}_{2}(c^{0},d^{0})}}&{{\leftarrow}}&{{0.031}}\\ {{\tilde{\beta}_{2}(c^{0},d^{1})}}&{{\leftarrow}}&{{0.397}}\\ {{\tilde{\beta}_{2}(c^{1},d^{0})}}&{{\leftarrow}}&{{0.317}}\\ {{\tilde{\beta}_{2}(c^{1},d^{1})}}&{{\leftarrow}}&{{0.254.}}\end{array}
$$ 

These beliefs are closer to the exact marginals in that they distribute some of the mass that was previously assigned to $c^{1},d^{0}$ to two other cases. However, they are still quite far from the exact marginal. 

This example demonstrates several issues. First, there is signiﬁcant diference between the two update rules. After incorporating $\tilde{\delta}_{2\to1}$ , the message from $C_{1}$ to $C_{2}$ is readjusted to account for → the new information, leading to a diferent approximation and hence a diferent update $\tilde{\delta}_{1\rightarrow2}$ . → Second, unlike sum-product propagation, belief update propagation does not generally converge within two rounds, even in a clique tree. In fact, an immediate question is whether these iterations converge at all. And if they do, is there anything we can say about the convergence points? As we show in section 11.4.5, the answers to these questions are very similar to the answers we got in the case of cluster-graph belief propagation. 

# 11.4.4 Expectation Propagation 

So far, our discussion of approximate message passing has focused on a particular type of ap- proximation: approximating a complex joint distribution as a product of small factors. However, the same ideas are applicable to a broad range of approximations. We now consider this process from a more general perspective, which will allow us to use a wider range of structured approx- imation in messages. Moreover, as we will see, this generalized form simpliﬁes the variational analysis of this approach. 

exponential family 

The framework we use is based on the idea of the exponential family , as presented in chapter 8. As we discussed there, the exponential families are a general class of distributions, that contains many of the distributions of interest. Recall that a family of distributions $\mathcal{Q}$ is in the exponential family if it can be deﬁned by two functions: the sufcient statistic function $\tau(x)$ , and the natural parameter function ${\mathfrak{t}}(\theta)$ , so that any distribution $Q$ in the family can be written as 

$$
Q({\pmb x})=\frac{1}{Z({\pmb\theta})}\exp\left\{\langle\tau({\pmb x}),{\tt t}({\pmb\theta})\rangle\right\},
$$ 

where $\theta$ is a set of parameters that specify the particular member of the family. 

To simplify the discussion we will focus on linear exponential families, where ${\sf t}(\theta)=\theta$ . Recall that linear exponential families include Markov networks (and consequently chordal Bayesian networks). 

exponential family messages As we now show, the approximate message passing approach described earlier applies when 

Algorithm 11.6 The message passing step in the expectation propagation algorithm. The algorithm performs approximate message propagation by projecting expected sufcient statistics. Procedure M-Project-Distr ( Q , // target exponential family for projection φ // Factor set ) 1 $X\leftarrow\mathit{S c o p e}[\vec{\phi}]$ // Variables in factor set 2 $\bar{\tau}\leftarrow\mathbf{\Sigma}_{\Phi}E_{{\pmb x}\sim\prod_{\phi\in\vec{\phi}}\left[\tau_{\mathcal{Q}_{i,j}}({\pmb x})\right]}$ Q   ∈ 3 // Compute expectation of sufcient statistics relative to dis- tribution deﬁned by product of factors 4 $\theta\gets\mathrm{\smile-project}(\bar{\tau})$ 5 return ( θ ) 

M-projection we choose to approximate messages by distributions from (linear) exponential families. Specif- ically, assume that we restrict each sepset $\boldsymbol{S}_{i,j}$ to be represented within an exponential family $\mathcal{Q}_{i,j}$ deﬁned by a sufcient statistics function $\tau_{i,j}$ . When performing message passing from $C_{i}$ to $C_{j}$ , we compute the marginal of $\tilde{\beta}_{i}$ , usually represented as a factor set $\vec{\phi}_{i}$ , and project it into $\mathcal{Q}_{i,j}$ using the $M_{\sun}$ -projection operator M-project- $\mathrm{distr}_{i,j}$ . This computation is often done using inference procedure that takes into account the structure of $\tilde{\beta}_{i}$ as a factor set. 

It turns out that the entire message passing operation can be formulated cleanly within this framework. If we are using an exponential family to represent our messages, then both the approximate clique marginal $\tilde{\sigma}_{i\rightarrow j}$ and the previous message $\tilde{\delta}_{j\to i}$ can be represented in the → → exponential form. Thus, if we ignore normalization factors, we have: 

$$
\begin{array}{r c l}{{\tilde{\sigma}_{i\to j}}}&{{\propto}}&{{\exp\left\{\langle\theta_{\tilde{\sigma}_{i\to j}},\tau_{i,j}({\pmb s}_{i,j})\rangle\right\}}}\\ {{\tilde{\delta}_{j\to i}}}&{{\propto}}&{{\exp\left\{\langle\theta_{\tilde{\delta}_{j\to i}},\tau_{i,j}({\pmb s}_{i,j})\rangle\right\}}}\\ {{\tilde{\delta}_{i\to j}}}&{{=}}&{{\displaystyle\frac{\tilde{\sigma}_{i\to j}}{\tilde{\delta}_{j\to i}}\propto\exp\left\{\langle(\theta_{\tilde{\sigma}_{i\to j}}-\theta_{\tilde{\delta}_{j\to i}}),\tau_{i,j}({\pmb s}_{i,j})\rangle\right\},}}\end{array}
$$ 

where $\theta_{\tilde{\sigma}_{i\rightarrow j}}$ and $\theta_{\tilde{\delta}_{i\to j}}$ are the parameters of the messages $\tilde{\sigma}_{i\rightarrow j}$ and $\tilde{\delta}_{i\to j}$ respectively. 

Since these messages are in an exponential family, it sufces to represent each of them by the parameters that describe them. We can then view propagation steps as updating these parameters. Speciﬁcally, we can rewrite the update step in line 4 of EP-Message as 

$$
\theta_{\tilde{\delta}_{i\to j}}\leftarrow(\theta_{\tilde{\sigma}_{i\to j}}-\theta_{\tilde{\delta}_{j\to i}}).
$$ 

Note that, in the case of exact inference in discrete networks, the original update and the one using the exponential family representation are essentially identical, since the exponential family representation of factors is of the same size as the factor. Indeed, the standard update is often performed in a logarithmic representation (for reasons of numerical stability; see box 10.A), which gives rise precisely to the exponential family update. 

The ﬁnal issue we must address is the construction of the exponential-family representation of $\tilde{\sigma}_{i\rightarrow j}$ in line 1 of the algorithm. Recall that this process involves the M-projection of $\tilde{\beta}_{i}$ expectation propagation 

EP message passing 

onto $\mathcal{Q}_{i,j}$ . As we discussed in section 8.5, the M-projection of a distribution $P$ within an exponential family $\mathcal{Q}$ is the distri tion $Q\in{\mathcal{Q}},$ which deﬁnes the same expectation over the sufcient statistics as deﬁned by P . In that section, we described a two-phase procedure for computing this approximating distribution: We compute the expected sufcient statistics induced by $P$ , and then ﬁnd the parameters for a distribution $Q\in{\mathcal{Q}}$ that induces the same expected sufcient statistics. We can apply this approach to deﬁne a general procedure for performing the operation M-project- $\cdot\mathrm{distr}_{i,j}\bigl(\vec{\phi}_{i}\bigr)$ for a general exponential family. We ﬁrst compute the expected expectation of $\tau_{i,j}$ according to $\tilde{\beta}_{i}$ . We then ﬁnd the distribution within $\mathcal{Q}_{i,j}$ that gives rise to the same expected sufcient statistics. This step is accomplished by the application of the function M-project that takes a vector of sufcient statistics and returns a parameter vector in the exponential family that induces precisely the expected sufcient statistics $\bar{\tau}_{i,j}$ . This function is shown in algorithm 11.6. The use of the expectation step in computing the messages is the basis for the name expectation propagation , which describes the general procedure for any member of the exponential family. 

As we have already discussed, dividing by the previous message corresponds to subtraction of the parameters. Thus, overall, we obtain the following $E P$ message passing step: 

$$
\theta_{\tilde{\delta}_{i\rightarrow j}}\leftarrow\mathrm{M-project}_{i,j}(E_{S_{i,j}\sim\tilde{\beta}_{i}}[\tau_{i,j}(S_{i,j})])-\theta_{\tilde{\delta}_{j\rightarrow i}}.
$$ 

How expensive are the two key steps in the expectation-propagation message passing proce- dure? The ﬁrst step is computing the expectation $\pmb{{\cal E}}_{S_{i,j}\sim\tilde{\beta}_{i}}[\tau_{i,j}]$ . In the case of discrete networks, ∼ this step might be as expensive as the number of possible values of $C_{i}$ . However, as we saw in the previous section, in many cases we can use the structure of the factors that make up $\tilde{\beta}_{i}$ to perform this expectation much more efciently. The second step is computing M-project on these factors. For some exponential families, this step is trivial, and can be done using a plug-in formula. In other families, this is a complex problem by itself. We will return to these issues in much greater detail in later chapters (particularly chapter 19 and chapter 20). Usually, when we design an approximation algorithm, we choose an exponential family for which this second step is easy. 

The factored distributions we discussed earlier are perhaps the simplest example of an expo- nential family that we can use in this algorithm. However, other representations also fall into this class. 

Example 11.9 Consider using a chain network to approximate each message, as in ﬁgure 11.15b. In example 8.16 and exercise 8.6 we showed that this class of distributions is a linear family and constructed the function M-project for it. Following the derivation, suppose the variables in the sepset are $X_{1},\ldots,X_{k}$ and we want to represent messages using the network structure $X_{1}\rightarrow X_{2}\rightarrow.~.~.\rightarrow$ $X_{k}$ . In example 8.16, we showed that the expected sufcient statistics are summarized in the vector of indicators comprising: $I\!\!\{x_{i}^{j}\}$ { } for $i=1,\dots,k$ and $x_{i}^{j}\,\in\,V a l(X_{i})$ ∈ ; and $I\!\!\left\{x_{i}^{j},x_{i+1}^{\ell}\right\}$ } for $i\,=\,1,.\,.\,.\,,k\,-\,1$ , $x_{i}^{j}\ \in\ V a l(X_{i})$ ∈ , $x_{i}^{\ell}\;\in\;V a l(X_{i+1})$ ∈ we have the expected value of these statistics, we can reconstruct the distribution $Q(X_{i+1}\mid X_{i})$ | as described in exercise 8.6. Given these subprocedures, the remaining propagation steps have been described earlier. 

If we consider a chain approximation to grid network, we can use the chains as in ﬁgure 11.13b. In this case, the main cost of a propagation step is the projection. The messages incoming to $^a$ cluster consists of univariate beliefs and pairwise beliefs along the column. When combined with the clique factors, these result in a ladder-like network (shown in ﬁgure 11.13b). As we discussed, we can a build a (nested) clique tree for this network that involves clusters of at most three variables. Thus, we can perform inference efciently on this nested clique tree to compute the expectations on pairwise beliefs in the outgoing column, and then use those expectations to reconstruct parameters. 

The view of the expectation propagation algorithm in these terms allows us to understand the scope of this approach. We can apply this approach to any class of distributions in the linear exponential family for which the computation of expected sufcient statistics and the M- projection operation can be implemented efectively. Note that not every class of distributions we have discussed obeys these two restrictions. In particular, Markov networks are in the linear exponential family, but they do not have an efective M-projection procedure. Thus, a general Markov network structure is not often used to represent messages in the expectation propagation algorithm. Conversely, Bayesian networks are not in the linear exponential family (see section 8.3.2). One might argue that Bayesian networks should be usable, since the M- projection operation can be implemented analytically (see theorem 8.7), allowing the expectation propagation algorithm to be applied efectively. 

This argument, however, brings up one important caveat regarding the expectation propaga- tion update rule. In the rule, we subtract two sets of parameters: the parameters associated with the message from $j$ to $i$ are subtracted from the parameters obtained from M-projection operation for $C_{i}$ . For the classes of distributions that we considered earlier, the space of legal parameters $\Theta$ was the entire real space ${I\!\!R}^{K}$ ; this space is closed under subtraction, guarantee- ing that the result of this update rule is a valid distribution in our space. This property does not hold for every exponential family; in particular, it is not the case for Bayesian networks with immoralities. Thus, we may end up in situations where the resulting parameters do not actually deﬁne a legal distribution in our space. We return to this issue when we discuss the applica- tion of expectation propagation to Gaussians in section 14.3.3, where it can give rise to severe problems. Thus, the most commonly used class of distributions in the expectation propagation algorithm is the class of low tree-width chordal graphs. Because these graphs are both Bayesian networks and Markov networks, they are both in the linear exponential family and admit an efective M-projection operation. The example of the chain distribution we used earlier falls into this category. In more general cases, these distributions are represented as clique trees, allowing them to be represented using a smaller set of factors: clique beliefs and sepset messages. 

# 11.4.5 Variational Analysis 

To deﬁne a variational principle for expectation propagation, we take an approach similar to the one we discussed in the context of cluster-graph belief propagation. Again, we consider an approximation $Q$ that consists of a set of pseudo marginals. In fact, we use the same energy functional $\tilde{F}[\tilde{P}_{\Phi},Q]$ . 

The main diference from the case of cluster-graph belief propagation is that, in the current approximation, the cluster tree is not calibrated. As we project messages into an approximate form, we no longer ensure that beliefs of neighboring clusters agree on the joint distribution of the variables they share. Instead, we maintain a weaker property that depends on the nature of the approximation. For example, in the case where we use messages that are product of marginal distributions, intuition suggests that neighboring clusters will eventually agree on the marginal distributions of the variables in the sepset. 

To gain better insight into the constraints we need, we start with reasoning about properties of convergence points of the algorithm. Suppose we iterate expectation-propagation belief update propagations until convergence. Now, consider two neighboring clusters $i$ and $j$ . Since the algorithm has converged, it follows that further updates do not change the cluster beliefs. Thus, the assignment of the expectation-propagation update rule of equation (11.41) becomes an equality for all clusters. We can then reason that 

$$
\mathrm{M-project}_{i,j}\big(E_{S_{i,j}\sim\tilde{\beta}_{i}}[\tau_{i,j}]\big)=\theta_{\tilde{\delta}_{i\rightarrow j}}+\theta_{\tilde{\delta}_{j\rightarrow i}}.
$$ 

A similar argument holds for the M-projection of the $j$ cluster beliefs, $\mathrm{M-protect}_{i,j}\big(E_{S_{i,j}\sim\tilde{\beta}_{j}}[\tau_{i,j}]\big)$ . It follows that the projection of the two beliefs onto $\mathcal{Q}_{i,j}$ result in the same distribution. 

expectation consistency constraint 

This discussion suggests that we can pose the problem as optimizing the same objective as CTree-Optimize , except that we now replace the constraint equation (11.7) with an expectation consistency constraint : 

$$
E_{S_{i,j}\sim\mu_{i,j}}[\tau_{i,j}]=E_{S_{i,j}\sim\beta_{j}}[\tau_{i,j}].
$$ 

In general, $\tau_{i,j}$ is a vector, and so this equation deﬁnes a vector of constraints. 

We now can deﬁne the optimization problem. It is identical to the optimization problems we already encountered, except that we relax the marginal consistency constraints. 

$$
\begin{array}{r c l l}{\displaystyle E_{S_{i,j}\sim\mu_{i,j}}[\tau_{i,j}]}&{=}&{\displaystyle E_{S_{i,j}\sim\beta_{j}}[\tau_{i,j}]}&{\forall(i-j)\in\mathcal{E}_{\mathcal{T}}}\\ {\displaystyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})}&{=}&{1}&{\forall i\in\mathcal{V}_{\mathcal{T}}}\\ {\displaystyle\sum_{\pmb{s}_{i,j}}\mu_{i,j}[\pmb{s}_{i,j}]}&{=}&{1}&{\forall(i-j)\in\mathcal{E}_{\mathcal{T}}}\\ {\displaystyle\beta_{i}(\pmb{c}_{i})}&{\geq}&{0}&{\forall i\in\mathcal{V}_{\mathcal{T}},\pmb{c}_{i}\in V a l(\pmb{C}_{i}).}\end{array}
$$ 

Like CGraph-Optimize , this optimization problem is an approximation to the problem of optimizing the energy functional in two ways. First, the space over which we are optimizing is the set of pseudo-marginals $Q$ . Because our marginalization constraint only requires that two neighboring clusters agree on their expectations, they will generally not agree on the full marginals. Thus, in general, a solution to this problem will generally not correspond to the marginals of an actual distribution $Q$ , even in the context of a clique tree. Second, because we can deﬁne the true energy function $F[\tilde{P}_{\Phi},Q]$ only for coherent joint distributions, we must resort here to optimizing its factored form $\tilde{F}[\tilde{P}_{\Phi},Q]$ . Although the two forms are equivalent for distributions over clique trees, they are not equivalent in this setting (since the exact energy functional is not even deﬁned outside the space of coherent distributions $Q.$ ). Thus, as in the case of the CGBP algorithms in the previous section, we are approximating both the objective and the optimization space. 

ﬁxed-point equations 

Theorem 11.8 

The generalization of theorem 11.3 for this relaxed optimization problem follows using more or less the same proof structure, where we characterize the stationary points of the constrained objective using a set of ﬁxed-point equations that deﬁne a message passing algorithm. 

Let $Q$ be a set of beliefs ch that $\mu_{i,j}$ is in the exponential family $\mathcal{Q}_{i,j}$ for al $(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}$ . Let M-project- $\mathrm{distr}_{i,j}$ be the M-projection opera e family $\mathcal{Q}_{i,j}$ . Then $Q$ is a stationary point of EP-Optimize if and only if for every edge $(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}$ ∈E there are auxiliary beliefs $\delta_{i\to j}(S_{i,j})$ and T $\delta_{j\rightarrow i}(S_{i,j})$ so that 

$$
\begin{array}{r c l}{\delta_{{i\to j}}}&{=}&{\displaystyle\frac{\mathrm{M-projecheck{\Gamma}}_{i,j}(\beta_{i})}{\delta_{j\to i}}}\\ {\beta_{i}}&{\propto}&{\displaystyle\psi_{i}\cdot\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\to i}}\\ {\mu_{i,j}}&{\propto}&{\displaystyle\delta_{j\to i}\cdot\delta_{i\to j}.}\end{array}
$$ 

Proof As in previous proofs of this kind, we deﬁne a Lagrangian that consists of the (ap- proximate) energy functional $\tilde{F}[\tilde{P}_{\Phi},Q]$ as well as Lagrange multiplier terms for each of the constraints. In our case, we have a vector of Lagrange multipliers for each of the constraints in equation (11.43). 

$$
\begin{array}{r c l}{\displaystyle\mathcal{I}}&{=}&{\displaystyle\sum_{i\in\mathcal{V}_{T}}\pmb{E}_{C_{i}\sim\beta_{i}}[\ln\psi_{i}]+\displaystyle\sum_{i\in\mathcal{V}_{T}}\pmb{H}_{\beta_{i}}(C_{i})-\displaystyle\sum_{(i-j)\in\mathcal{E}_{T}}\pmb{H}_{\mu_{i,j}}(\pmb{S}_{i,j})}\\ &&{\displaystyle-\sum_{i}\sum_{j\in\mathsf{N}_{b}}\vec{\lambda}_{j}\!\cdot\!\big(\pmb{E}_{S_{i,j}\sim\mu_{i,j}}[\tau_{i,j}]-\pmb{E}_{S_{i,j}\sim\beta_{j}}[\tau_{i,j}]\big)}\\ &&{\displaystyle-\sum_{i\in\mathcal{V}_{T}}\lambda_{i}\left(\sum_{c_{i}}\beta_{i}(\pmb{c}_{i})-1\right)-\displaystyle\sum_{(i-j)\in\mathcal{E}_{T}}\lambda_{i,j}\left(\sum_{s_{i,j}}\mu_{i,j}[\pmb{s}_{i,j}]-1\right).}\end{array}
$$ 

Taking partial derivatives of $J$ with respect to $\beta_{i}(\mathbf{\boldsymbol{c}}_{i})$ and $\mu_{i,j}[\pmb{s}_{i,j}]$ and equating these deriva- tives to zero, we get the following equalities that must hold at a stationary point: 

$$
\begin{array}{r c l}{\beta_{i}(\pmb{c}_{i})}&{\propto}&{\psi_{i}(\pmb{c}_{i})\displaystyle\prod_{j\in\mathrm{Nb}_{i}}\exp\left\{\vec{\lambda}_{j\rightarrow i}\cdot\tau_{i,j}(\pmb{s}_{i,j})\right\}}\\ {\mu_{i,j}[\pmb{s}_{i,j}]}&{\propto}&{\exp\left\{\vphantom{\frac{1}{\beta_{i}}}\left(\vec{\lambda}_{j\rightarrow i}+\vec{\lambda}_{i\rightarrow j}\right)\cdot\tau_{i,j}(\pmb{s}_{i,j})\right\}.}\end{array}
$$ 

Note that $(\vec{\lambda}_{j\rightarrow i}\,+\,\vec{\lambda}_{i\rightarrow j})$ serves as the natural parameters of $\mu_{i,j}$ in its exponential form → → representation. 

Moreover, the constraint of equation (11.43) implies that 

$$
E_{S_{i,j}\sim\mu_{i,j}}[\tau_{i,j}]=E_{S_{i,j}\sim\beta_{j}}[\tau_{i,j}].
$$ 

Thus, using theorem 8.6, we conclude that $\mu_{i,j}=\mathrm{M-project-distr}_{i,j}(\beta_{i})$ . By deﬁning 

$$
\begin{array}{r}{\delta_{i\to j}[s_{i,j}]\propto\exp\left\{\vec{\lambda}_{i\to j}\cdot\tau_{i,}(s_{i,j})\right\},}\end{array}
$$ 

we can verify that the statement of the theorem is satisﬁed. 

Theorem 11.8 shows that, if we perform EP belief update propagation until convergence, then we reach a stationary point of EP-Optimize . Thus, this result provides an optimization semantics for expectation-propagation message passing. 

Our discussion of expectation propagation and the proof were posed in the context of linear exponential families. The same ideas can be extended to nonlinear families but require additional subtleties that we do not discuss. 

# 11.4.6 Discussion 

In this section, we presented an alternative approach for inference in large graphical models. Rather than modifying the global structure of the inference object, we modify the structure of the messages and how they are computed. Although we focused on the application of this approximation to clique trees, it is equally applicable in the context of cluster graphs. The message passing algorithms (both the sum-product algorithm and the belief update algorithm) can be used for passing messages between clusters in general graphs. Moreover, the variational analysis of section 11.4.5 also applies, essentially without change, to cluster graphs, using the same derivation as in section 11.3. 

Note that the expectation propagation algorithm sufers from the same caveats we discussed in the previous section: Iterations of EP message propagation are not guaranteed to in-  duce monotonic improvements to the objective function, and the algorithm does not always converge. Moreover, even when the algorithm does converge, the clusters are only approximately calibrated: their marginals agree on the expectations of the sufcient statistics (say the individual marginals), but not on other aspects of the distribution (say marginals over pairs of variables). As a consequence, if we want to answer a query using the network, it may make a diference from which cluster we extract the answer. 

We presented expectation propagation in the context of its application to factored messages. In the simplest case, of fully factored messages, the messages are simply cluster marginals over individual variables. The similarity between this variant of expectation propagation and belief propagation is quite striking; indeed, one can simulate expectation propagation with fully factored messages using cluster-graph belief propagation with a particular factor graph structure (see exercise 11.23). The more general case of messages that are not fully factored (for example, ﬁgure 11.13b) is more complex, and they cannot be mapped directly to belief propagation in cluster graphs. However, a mapping does exist between expectation propagation in discrete networks with factorized messages and cluster-graph belief propagation with region graphs. 

More important, however, is the fact that expectation propagation provides a general approach for dealing with distributions in the exponential family. It therefore provides message passing algorithms for a broad class of models. For example, we will see an application of expectation propagation to hybrid (continuous/discrete) graphical models in section 14.3.3. Its broad appli- cability makes expectation propagation an important component in the approximate inference toolbox. 

# 11.5 Structured Variational Approximations 

In the previous two sections, we examined approximations based on belief propagation. As we saw, both methods can be viewed as optimizing an approximate energy functional over the structured variational class of pseudo-marginals. These pseudo-marginals generally do not correspond to a globally coherent joint distribution $Q$ . The structured variational approach aims to optimize the energy functional over a family $\mathcal{Q}$ of coherent distributions $Q$ . This family is chosen to be computationally tractable, and hence it is generally not sufciently expressive to capture all of the information in $P_{\Phi}$ . 

More precisely, we aim to address the following maximization problem: 

$\begin{array}{l l}{{\mathrm{Find}}}&{{Q\in{\mathcal Q}}}\\ {{\mathrm{maximize}}}&{{F[\tilde{P}_{\Phi},Q]}}\end{array}$ 

where $\mathcal{Q}$ is a given family of distributions. In these methods, we are using the exact energy functional $F[\tilde{P}_{\Phi},\bar{Q}]$ , which satisﬁes theorem 11.2. Thus, maximizing the energy functional corresponds directly to obtaining a better approximation to $P_{\Phi}$ (in terms of $D(Q\|P_{\Phi}))$ ). 

structured variational 

The main parameter in this maximization problem is the choice of family Q . This choice induces a trade-of. On one hand, families that are “simpler,” that is, that can be described by a Bayesian network or a Markov network with small tree-width, allow more efcient inference. As we will see, simpler families also allow us to solve the maximization problem efciently. On the other hand, if the fa y $\mathcal{Q}$ is too restrictive, then it cannot r resent distributions that are good approximations of $P_{\Phi}$ , giving rise to a poor approximation Q . In either case, this family is generally chosen to have enough structure that allows inference to be tractable, giving rise to the name structured variational approximation. 

As we will see, the methods of this type difer from generalized belief propagation in sev- eral ways. They are guaranteed to lower-bound the log-partition function, and they also are guaranteed to converge. 

# 11.5.1 The Mean Field Approximation 

mean ﬁeld The ﬁrst approach we consider is called the mean ﬁeld approximation. As we will see, in many respects, it resembles the algorithm obtained using the Bethe approximation to the energy functional. In particular, the resulting algorithm performs message passing where the messages are distributions over single variables. As we will see, however, the form of the updates is somewhat diferent. 

11.5.1.1 The Mean Field Energy 

Unlike our presentation in earlier sections, we begin our discussion with the energy functional, and we derive the algorithm directly from analyzing it. The mean ﬁeld algorithm ﬁnds the distribution $Q$ , which is closest to $P_{\Phi}$ in terms of $D(Q\|P_{\Phi})$ | | within the class of distributions representable as a product of independent marginals: 

$$
Q({\mathcal{X}})=\prod_{i}Q(X_{i}).
$$ 

On the one hand, the approximation of $P_{\Phi}$ as a fully factored distribution is likely to lose a lot of information in the distribution. On the other hand, this approximation is computationally attractive, since we can easily evaluate any query on $Q$ by a product over terms that involve the variables in the scope of the query. Moreover, to represent $Q$ , we need only to describe the marginal probabilities of each of the variables. 

energy functional 

As in previous sections, the mean ﬁeld algorithm is derived by considering ﬁxed points of the energy functional . We thus begin by considering the form of the energy functional in equation (11.3) when $Q$ has the form of a product distribution as in equation (11.48). We can then characterize its ﬁxed points and thereby derive an iterative algorithm to ﬁnd such ﬁxed points. 

The functional contains two terms. The ﬁrst is a sum of terms of the form $E_{U_{\phi}\sim Q}[\ln\phi]$ , where we need to evaluate 

$$
\begin{array}{r c l}{{E_{U_{\phi}\sim Q}[\ln\phi]}}&{{=}}&{{\displaystyle\sum_{\pmb{u}_{\phi}}Q(\pmb{u}_{\phi})\ln\phi(\pmb{u}_{\phi})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{\pmb{u}_{\phi}}\left(\prod_{X_{i}\in U_{\phi}}Q(x_{i})\right)\ln\phi(\pmb{u}_{\phi}).}}\end{array}
$$ 

As shown, we can use the form of $Q$ to compute $Q(\pmb{u}_{\phi})$ as a product of marginals, allowing the evaluation of this term to be performed in time linear in the number of values of $U_{\phi}$ . Because this cost is linear in the description size of the factors of $P_{\Phi}$ , we cannot expect to do much better. 

As we saw in section 8.4.1, the term $H_{Q}(\mathcal X)$ also decomposes in this case. 

Corollary 11.3 

$$
H_{Q}({\mathcal{X}})=\sum_{i}H_{Q}(X_{i}).
$$ 

Thus, the energy functional for a fully factored distribution $Q$ can be rewritten simply as a sum of expectations, each one over a small set of variables. Importantly, the complexity of this expression depends on the size of the factors in $P_{\Phi}$ , and not on the topology of the network. Thus, the energy functional in this case can be represented and manipulated efectively, even in networks that would require exponential time for exact inference. 

Example 11.10 Continuing our running example, consider the form of the mean ﬁeld energy for a $4\times4$ grid network. Based on our discussion, we see that it has the form 

$$
{\begin{array}{r c l}{F[{\tilde{P}}_{\Phi},Q]}&{=}&{\displaystyle\sum_{i\in\{1,2,3\},j\in\{1,2,3,4\}}E_{Q}[\ln\phi(A_{i,j},A_{i+1,j})]}\\ &&{+\displaystyle\sum_{i\in\{1,2,3,4\},j\in\{1,2,3\}}E_{Q}[\ln\phi(A_{i,j},A_{i,j+1})]}\\ &&{+\displaystyle\sum_{i\in\{1,2,3,4\},j\in\{1,2,3,4\}}H_{Q}(A_{i,j}).}\end{array}}
$$ 

We see that the energy functional involves only expectations over single variables and pairs of neighboring variable expression has the same general form for an $n\times n$ grid. Thus, although the tree-width of an $n\times n$ × grid is exponential in $n$ , the energy functional can be represented and computed in cost $O(n^{2})$ ; that is, in a time linear in the number of variables. 

# 11.5.1.2 Maximizing the Energy Functional: Fixed-point Characterization 

The next step is to consider the task of optimizing the energy function: ﬁnding the distribution $Q$ for which this energy functional is maximized: 

$$
\begin{array}{r}{\{Q(X_{i})\}}\\ {F[\tilde{P}_{\Phi},Q]}\end{array}
$$ 

$$
\begin{array}{r c l}{{Q(\mathcal{X})}}&{{=}}&{{\displaystyle\prod_{i}Q(X_{i})}}\\ {{}}&{{}}&{{}}\\ {{\sum_{x_{i}}Q(x_{i})}}&{{=}}&{{1\quad\forall i.}}\end{array}
$$ 

To simplify notation, from now on we use $X_{-i}$ to denote $\mathcal{X}-\{X_{i}\}$ 

Note that, unlike the cluster-graph belief propagation algorithms of section 11.3 and the ex- pectation propagation algorithm of section 11.4, here we are not approximating the objective. We are approximating only the optimization space by selecting a space of distributions $\mathcal{Q}$ that generally does not contain our original distribution $P_{\Phi}$ . 

As with the previous optimization problems in this chapter, we use the method of Lagrange multipliers to derive a characterization of the stationary points of $F[\tilde{P}_{\Phi},Q]$ . However, the structure of $Q$ allows us to consider the optimal value of each component (that is, marginal distribution) given the rest. (This iterative optimization procedure was not feasible in cluster trees and graphs due to constraints that relate diferent beliefs.) 

We now provide a set of ﬁxed-point equations that characterize the stationary points of the mean ﬁeld optimization problem: 

Theorem 11.9 

conditional expectation 

The distribution $Q(X_{i})$ is a local maximum of Mean-Field given $\{Q(X_{j})\}_{j\neq i}$ if and only if 

$$
Q(x_{i})=\frac{1}{Z_{i}}\exp\left\{\sum_{\phi\in\Phi}E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]\right\},
$$ 

where $Z_{i}$ is a local normalizing constant and I $E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]$ is the conditional expectation given X∼ the value $x_{i}$ 

$$
E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]=\sum_{\pmb{u}_{\phi}}Q(\pmb{u}_{\phi}\mid x_{i})\ln\phi(\pmb{u}_{\phi}).
$$ 

Proof The proof of this theorem relies on proving the ﬁxed-point characterization of the indi- vidual marginal $Q(X_{i})$ in terms of the other components, $Q(X_{1}),\dots,Q(X_{i-1}),\,Q(X_{i+1}),\dots,$ , $Q(X_{n})$ , as speciﬁed in equation (11.52). 

We ﬁrst consider the restriction of our objective $F[\tilde{P}_{\Phi},Q]$ to those terms that involve $Q(X_{i})$ : 

$$
F_{i}[Q]=\sum_{\phi\in\Phi}{\pmb E}_{{\pmb U}_{\phi}\sim Q}[\ln\phi]+{\pmb H}_{Q}(X_{i}).
$$ 

To optimize $Q(X_{i})$ , we deﬁne the Lagrangian that consists of all terms in $F[\tilde{P}_{\Phi},Q]$ that involve $Q(X_{i})$ 

$$
L_{i}[Q]=\sum_{\phi\in\Phi}{\pmb E}_{{\pmb U}_{\phi}\sim Q}[\ln\phi]+{\pmb H}_{Q}(X_{i})+\lambda\left(\sum_{x_{i}}{Q}(x_{i})-1\right).
$$ 

The Lagrange multiplier $\lambda$ corresponds to the constraint that $Q(X_{i})$ is a distribution. We now take derivatives with respect to $Q(x_{i})$ . The following result plays an important role in the remainder of the derivation: 

Lemma 11.1 If $\begin{array}{r}{Q(\mathcal{X})=\prod_{i}Q(X_{i})}\end{array}$ then, for any function $f$ with scope $U$ , 

$$
{\frac{\partial}{\partial Q(x_{i})}}E_{U\sim Q}[f(U)]=E_{U\sim Q}[f(U)\mid x_{i}].
$$ 

The proof of this lemma is left as an exercise (see exercise 11.24). Using this lemma, and standard derivatives of entropies, we see that 

$$
{\frac{\partial}{\partial Q(x_{i})}}L_{i}=\sum_{\phi\in\Phi}{\pmb E}_{{\pmb X}\sim Q}[\ln\phi\mid x_{i}]-\ln Q(x_{i})-1+{\lambda}.
$$ 

Setting the derivative to 0 , and rearranging terms, we get that 

$$
\ln Q(x_{i})=\lambda-1+\sum_{\phi\in\Phi}E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}].
$$ 

We take exponents of both sides and renormalize; because $\lambda$ is constant relative to $x_{i}$ , it drops out in the renormalization, so that we obtain the formula in equation (11.52). 

This derivation, by itself, shows only that the solution of equation (11.52) is a stationary point of equation (11.53). To prove that it is a maximum, we note that equation (11.53) is a sum of two terms: $\begin{array}{r}{\sum_{\phi\in\Phi_{\circ}}E_{U_{\phi}\sim Q}[\ln\phi]}\end{array}$ is linear in $Q(X_{i})$ , given all the other components $Q(X_{j});H_{Q}(X_{i})$ ∈ is a concave function in $Q(X_{i})$ . As a whole, given the other components of $Q$ , the function $F_{i}$ is concave in $Q(X_{i})$ , and therefore has a unique global optimum, which is easily veriﬁed to be equation (11.52) rather than any of the extremal points. 

From this it follows that: 

Corollary 11.4 The distribution $Q$ is a stationary point of Mean-Field if and only if, for each $X_{i}.$ , equation (11.52) holds. 

In contrast to theorem 11.9, this result only provides a characterization of stationary points of the objective, and not necessarily of its optima. The stationary points include local maxima, local minima, and saddle points. The reason for the diference is that, although each “coordinate” $Q(X_{i})$ is guaranteed to be locally maximal given the others, the direction that locally improves the objective may require a coordinated change in several components. We return to this point in section 11.5.1.3. 

We now move to interpreting this characterization. The key term in equation (11.52) is the argument in the expectation. We can prove the following property. 

$$
Q(x_{i})={\frac{1}{Z_{i}}}\exp\left\{E_{X_{-i}\sim Q}[\ln P_{\Phi}(x_{i}\mid X_{-i})]\right\}
$$ 

where $Z_{i}$ is a normalizing constant. 

Proof Recall that $\begin{array}{r}{\tilde{P}_{\Phi}\;=\;\prod_{\phi\in\Phi}\phi}\end{array}$ Q is the unnormalized measure deﬁned by $\Phi$ . Due to the ∈ linearity of expectation: 

$$
\sum_{\phi\in\Phi}{\pmb E}_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]={\pmb E}_{\mathcal{X}\sim Q}\Bigl[\ln\tilde{P}_{\Phi}(X_{i},{\pmb X}_{-i})\mid x_{i}\Bigr].
$$ 

Because $Q$ is a product of marginals, we can rewrite $Q(X_{-i}\mid x_{i})=Q(X_{-i})$ , and get that: 

$$
{\cal E}_{\mathcal{X}\sim Q}\Bigl[\ln\tilde{P}_{\Phi}(X_{i},{\pmb X}_{-i})\mid x_{i}\Bigr]={\pmb E}_{{\pmb X}_{-i}\sim Q}\Bigl[\ln\tilde{P}_{\Phi}(x_{i},{\pmb X}_{-i})\Bigr].
$$ 

Using properties of conditional distributions, it follows that: 

$$
{\tilde{P}}_{\Phi}(x_{i},X_{-i})=Z P_{\Phi}(x_{i},X_{-i})=Z P_{\Phi}(X_{-i})P_{\Phi}(x_{i}\mid X_{-i}).
$$ 

We conclude that 

$$
\sum_{\phi\in\Phi}{\pmb E}_{X\sim Q}[\ln\phi\mid x_{i}]={\pmb E}_{X_{-i}\sim Q}[\ln P_{\Phi}(x_{i}\mid X_{-i})]+{\pmb E}_{X_{-i}\sim Q}[\ln P_{\Phi}(X_{-i})Z].
$$ 

Plugging this equality into the update equation equation (11.52), we get that 

$$
{\cal Q}(x_{i})=\frac{1}{Z_{i}}\exp\left\{E_{X_{-i}\sim{\cal Q}}[\ln{\cal P}_{\Phi}(x_{i}\mid X_{-i})]\right\}\exp\left\{E_{X_{-i}\sim{\cal Q}}[\ln{\cal P}_{\Phi}(X_{-i})Z]\right\}.
$$ 

The term $\ln P_{\Phi}(X_{-i})Z$ does not depend on the value of $x_{i}$ . Recall that when we multiply a belief by a constant factor, it does not change the distribution $Q$ ; in fact, as we renormalize the distribution at the end to sum to 1, this constant will be “absorbed” into the normalizing function, to achieve normalization. Thus, we can simply ignore this term, thereby achieving the desired conclusion. We note that this type of algebraic manipulation will prove useful multiple times throughout this section. 

This corollary shows that $Q(x_{i})$ is the geometric average of the conditional probability of $x_{i}$ given all other variables in the domain. The average is based on the probability that $Q$ assigns to all possible assignments to the variables in the domain. In this sense, the mean ﬁeld approximation requires that the marginal of $X_{i}$ be “consistent” with the marginals of other variables. 

Note that, in $P_{\Phi}$ , we can also represent the marginal of $X_{i}$ as an average: 

$$
P_{\Phi}(x_{i})=\sum_{x_{-i}}P_{\Phi}(x_{-i})P_{\Phi}(x_{i}\mid x_{-i})=E_{X_{-i}\sim P_{\Phi}}[P_{\Phi}(x_{i}\mid X_{-i})].
$$ 

This average is an arithmetic average, whereas the one used in the mean ﬁeld approximation is a geometric average. In general, the latter tends to lead to marginals that are more sharply peaked than the original marginals in $P_{\Phi}$ . More signiﬁcant, however, is the fact that the expectations in equation (11.55) are taken relative to $P_{\Phi}$ , whereas the ones in equation (11.54) are taken relative to the approximation $Q$ . Thus, this similarity does not imply as a consequence that our approximation in $Q$ to the marginals in $P_{\Phi}$ is a good one. 

# 11.5.1.3 Maximizing the Energy Functional: The Mean Field Algorithm 

How do we convert the ﬁxed-point equation of equation (11.52) into an update algorithm? We start by observing that if $X_{i}\,\notin\,S c o p e[\phi]$ then $E_{U_{\phi}\sim Q}[\ln\phi\mid x_{i}]=E_{U_{\phi}\sim Q}[\ln\phi]$ . Thus, expec- ∼ ∼ tation terms on such factors are independent of the value of $X_{i}$ . Consequently, we can absorb them into the normalization constant $Z_{i}$ and get the following simpliﬁcation. 

In the mean ﬁeld approximation, $Q(X_{i})$ is locally optimal only if 

$$
Q(x_{i})=\frac{1}{Z_{i}}\exp\left\{\sum_{\phi:X_{i}\in S c o p e[\phi]}E_{(U_{\phi}-\{X_{i}\})\sim Q}[\ln\phi(U_{\phi},x_{i})]\right\}.
$$ 

where $Z_{i}$ is a normalizing constant. 

This representation shows that $Q(X_{i})$ has to be consistent with the expectation of the potentials in which it appears. In our grid network example, this characterization implies that $Q(A_{i,j})$ is a product of four terms measuring its interaction with each of its four neighbors: 

$$
Q(a_{i,j})=\frac{1}{Z_{i,j}}\exp\left\{\begin{array}{l l}{\sum_{a_{i-1,j}}Q(a_{i-1,j})\ln(\phi(a_{i-1,j},a_{i,j}))+}\\ {\sum_{a_{i,j-1}}Q(a_{i,j-1})\ln(\phi(a_{i,j-1},a_{i,j}))+}\\ {\sum_{a_{i+1,j}}Q(a_{i+1,j})\ln(\phi(a_{i,j},a_{i+1,j}))+}\\ {\sum_{a_{i,j+1}}Q(a_{i,j+1})\ln(\phi(a_{i,j},a_{i,j+1}))}\end{array}\right\}.
$$ 

Each term is a (geometric) average of one of the potentials involving $A_{i,j}$ . For example, the ﬁnal term in the exponent represents a geometric average of the potential between $A_{i,j}$ and $A_{i,j+1}$ , averaged using the distribution $Q(A_{i,j+1})$ . 

The characterization of corollary 11.6 provides tools for developing an algorithm to maximize $F[\tilde{P}_{\Phi},Q]$ ] . For example, examining equation (11.57), we see that we can easily evaluate the term within the exponential by considering each of $A_{i,j}$ ’s neighbors and computing the interaction between the values that neighbor can take and possible values of $A_{i,j}$ . Moreover, in this example, we see that $Q(A_{i,j})$ does not appear on the right-hand side of the update rule. Thus, we can choose $Q(A_{i,j})$ , which satisﬁes the required equality by assigning it to the term denoted by the right-hand side of the equation. 

This last observation is true in general. All the terms on the right-hand side of equation (11.56) involve expectations of variables other than $X_{i}$ , and do not depend on the choice of $Q(X_{i})$ . We can achieve equality simply by evaluating the exponential terms for each value $x_{i}$ , normalizing the results to sum to 1 , and then assigning them to $Q(X_{i})$ . As a consequence, we reach the optimal value of $Q(X_{i})$ in one easy step. 

This last statement must be interpreted with some care. The resulting value for $Q(X_{i})$ is its optimal value given the choice of all other marginals. Thus, this step optimizes our function relative only to a single coordinate in the space — the marginal of $Q(X_{i})$ . To optimize the function in its entirety, we need to optimize relative to all of the coordinates. We can embed this step in an iterated coordinate ascent algorithm, which repeatedly optimizes a single marginal at a time, given ﬁxed choices to all of the others. The resulting algorithm is shown in algorithm 11.7. Importantly, a single optimization of $Q(X_{i})$ does not usually sufce: a subsequent modiﬁcation 

![](images/adf8c766ddbebdd22bea637b0335cc94ad46aa9a3c38e2cb0517fbe74d06459b.jpg) 

to another marginal $Q(X_{j})$ may result in a diferent optimal parameter iz ation for $Q(X_{i})$ . Thus, the algorithm repeats these steps until convergence. Note that, in practice, we do not test for equality in line 9, but rather for equality up to some ﬁxed small-error tolerance. 

A key property of the coordinate ascent procedure is that each step leads to an increase in the energy functional. Thus, each iteration of Mean-Field results in a better approximation $Q$ to the target density $P_{\Phi}$ , guaranteeing convergence. 

The Mean-Field iterations are guaranteed to converge. Moreover, the distribution $Q^{*}$ returned by Mean-Field is a stationary point of $F[\tilde{P}_{\Phi},Q]$ , subject to the constraint that $\begin{array}{r}{Q(\mathcal{X})=\prod_{i}Q(X_{i})}\end{array}$ is a distribution. 

Proof We showed earlier that each iteration of Mean-Field is monotonically nondecreasing in $F[\tilde{P}_{\Phi},Q]$ ] . Because the energy functional is bounded, the sequence of distributions represented by successive iterations of Mean-Field must converge. At the convergence point the ﬁxed-point equations of theorem 11.9 hold for all the variables in the domain. As a consequence, the convergence point is a stationary point of the energy functional. 

As we discussed, the distribution $Q^{*}$ returned by Mean-Field is not necessarily a local op- timum of the algorithm. However, local minima and saddle points are not stable convergence points of the algorithm, in the sense that a small perturbation of $Q$ followed by optimization will lead to a better convergence point. Because the algorithm is unlikely to accidentally land precisely on the unstable point and get stuck there, in practice, the convergence points of the algorithm are local maxima. 

In general, however, the result of the mean ﬁeld approximation is a local maximum, and not necessarily a global one. 

![](images/932d21f997db344905c19f31cee915bc3d782ba8ff917f0d50914adaed7f4ac5.jpg) 
Figure 11.16 An example of a multimodal mean ﬁeld energy functional landscape. In this network, $P(a,b)=0.5-\epsilon$ if $a\neq b$ and $\epsilon$ if $a=b$ . The axes correspond to the mean ﬁeld marginal for $Q(a^{1})$ and $Q(b^{1})$ and the contours show equi-values of the energy functional for diferent choices of these variational parameters. As we can see, there are two modes to the energy functional, one roughly corresponds to $a^{1},b^{0}$ and the other one to $a^{0},b^{1}$ . In addition, there is a saddle point at (0 . 5 , 0 . 5) . 

Consider a distribution $P_{\Phi}$ that is an approximate XOR (exclusive or) of two variables $A$ and $B$ , that ${P_{\Phi}}(a,b)=0.5-\epsilon$ if $a\neq b$ and ${\cal P}_{\Phi}(a,b)=\epsilon$ if $a=b$ . Clearly, we cannot approxima $P_{\Phi}$ by a product of marginals, since such a product cannot capture the relationship between A and $B$ . It turns out that if $\epsilon$ is sufciently small, say 0 . 01 , then the energy potential surface has two local maxima that correspond to the two cases where $a\neq b$ . See ﬁgure 11.16. (For sufciently large ϵ , such as 0 . 1 , the mean ﬁeld approximation has a single maximum point at the uniform distribution.) 

We can use standard strategies, such as multiple random restarts, to try to avoid getting stuck in local maxima. However, these do not overcome the basic shortcoming of the mean ﬁeld approximation, which is apparent in this example. The approximation cannot describe complex posteriors, such as the XOR posterior we discussed. And thus, we cannot expect it to give satisfactory approximations in these situations. To provide better approximations, we must use a richer class of distributions $\mathcal{Q}_{i}$ , which has greater expressive power. 

# 11.5.2 Structured Approximations 

The mean ﬁeld algorithm provides an easy approximation method. However, it is limited by forcing $Q$ to be a very simple distribution. As we just saw, the fact that all variables are independent of each other in $Q$ can lead to very poor approximations. Intuitively, if we 

![](images/d3b5b87112aefd50dbc3da22925dcb78aeb992542a3c74fe411fb8ac8bdd82bf.jpg) 
Figure 11.17 Two structures for variational approximation of a $4\times4$ grid network 

use a distribution $Q$ that can capture some of the dependencies in $P_{\Phi}$ , we can get a better approximation. Thus, we would like to explore the spectrum of approximations between the mean ﬁeld approximation and exact inference. 

A natural approach to get richer approximations that capture some of the dependencies in $P_{\Phi}$ is to use network structures of diferent complexity. By adding and removing edges from the network we can control the cost of inference in the approximating distribution and how well it captures dependencies in the target distribution. We can achieve this type of ﬂexibility by using either Bayesian networks or Markov networks. Both types of networks lead to similar approximations, and so we focus on the undirected case, parameterized as Gibbs distributions (so that we are not restricted to factors over maximal cliques). Exercise 11.34 develops similar ideas using a Bayesian network approximation. 

# 11.5.2.1 Fixed-Point Characterization 

We now consider the form of the variational approximation when we are given a general form of $Q$ as a Gibbs parametric family. Formally, we assume we are given a set of potential scopes $\{C_{j}\subseteq\mathcal{X}:j=1,.\,.\,.\,,J\}$ . We can then choose an approximation $Q$ that has the form: 

$$
Q(\mathcal X)=\frac{1}{Z_{Q}}\prod_{j=1}^{J}\psi_{j},
$$ 

where $\psi_{j}$ is a factor with $S c o p e[\psi_{j}]=C_{j}$ . 

Example 11.12 Consider again the grid network example. There are many possible approximating network struc- tures we can choose that allow for efcient inference. As a concrete example, we might choose potential scopes $\{A_{1,1},A_{1,2}\}$ , $\{A_{1,2},A_{1,3}\},.\,.\,.\,,\{A_{2,1},A_{2,2}\}$ , $\{A_{2,2},A_{2,3}\},.\,.\,.$ . That is, we pre- serve the dependencies between variables in the same row, but ignore the ones that relate difer- ent columns. Alternatively, we can consider an approximation that preserves dependencies along columns and ignores the dependencies between rows. As we can see in ﬁgure 11.17, in both cases, 

the structure we use is a collection of independent chain structures. Exact inference with such structures is linear, and so the cost of inference is not much worse than in the mean ﬁeld approx- imation. Clearly, we can also consider many other structures for the approximating distributions. These might introduce additional dependencies and can have higher cost in terms of inference. We will return to the question of what structure to use. 

Assume that we decide on the form of the potentials for the approxi ating family $\mathcal{Q}$ . As before, we consider the form of the energy functional for a distribution Q in this family. We then characterize the stationary points of the functional, and we use those to derive an iterative optimization algorithm. 

As before, evaluating the terms that involve $E_{U_{\phi}\sim Q}[\ln\phi]$ requires performing expectations ∼ with respect to the variables in Scope [ φ ] . Unlike the case of mean ﬁeld approximation, the complexity of computing this expectation depends on the structure of the approximating distri- bution. However, we assume that we can solve this problem by exact inference (in the network corresponding to $Q.$ ), using the methods we discussed in previous chapters. 

As discussed in section 8.4.1, the entropy term in the energy functional also reduces to computing a similar set of expectation terms: 

Proposition 11.5 

$$
{H_{Q}}(\mathcal{X})=-\sum_{j=1}^{J}{E_{{C_{j}}\sim Q}}[\ln{{\psi_{j}}({C_{j}})}]+\ln{Z_{Q}}.
$$ 

Overall, we obtain the following form for the energy functional, for distributions $Q$ in the family $\mathcal{Q}$ : 

$$
{\cal F}[\tilde{P}_{\Phi},Q]=\sum_{k=1}^{K}{\pmb E}_{Q}[\ln\phi_{k}]-\sum_{j=1}^{J}{\pmb E}_{Q}[\ln\psi_{j}]+\ln Z_{Q}.
$$ 

As before, the hard question is how to optimize the potential to get the best approximation. We solve this problem using the same general strategy we discussed in the context of the mean ﬁeld approximation. First, we derive the ﬁxed-point equations that hold when the approximation is a local maximum (or, more precisely, a stationary point) of the energy functional. We then use these ﬁxed-point equations to help derive an optimization algorithm. 

ﬁxed-point equations 

# Theorem 11.11 

We derive the ﬁxed-point equations by taking derivatives of $F[\tilde{P}_{\Phi},Q]$ with respect to param- eters of the distribution $Q$ . In our case, the parameters will be an entry $\psi_{j}(c_{j})$ in each of the factors that deﬁne the distribution. We then set those equations to zero, obtaining the following result: 

If $\begin{array}{r}{Q(\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}}\end{array}$ Q , then the potential $\psi_{j}$ is a stationary point of the energy functional if and only if 

$$
\psi_{j}(c_{j})\propto\exp\left\{E_{Q}\Big[\ln\tilde{P}_{\Phi}\ |\ c_{j}\Big]-\sum_{k\neq j}E_{Q}[\ln\psi_{k}\ |\ c_{j}]-F[\tilde{P}_{\Phi},Q]\right\}.
$$ 

The proof is straightforward algebraic manipulation and is left as an exercise (exercise 11.26). 

This theorem establishes a characterization of the ﬁxed point as the diference between the expected value of logarithm of the original potentials and the expected value of the logarithm of the approximating potentials. The last term in equation (11.60) is the energy functional $\bar{F}[\tilde{P}_{\Phi},Q]$ , which is independent of the assignment $c_{j}$ ; thus, as we discussed in the proof of corollary 11.5, we can absorb this term into the normalization constant of the distribution and ignore it. 

If $\begin{array}{r}{Q(\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}}\end{array}$ Q , then the potential $\psi_{j}$ is a stationary point of the energy functional if and only if: 

$$
\psi_{j}(\pmb{c}_{j})\propto\exp\left\{E_{Q}\left[\ln\tilde{P}_{\Phi}\mid\pmb{c}_{j}\right]-\sum_{k\neq j}\pmb{E}_{Q}[\ln\psi_{k}\mid\pmb{c}_{j}]\right\}.
$$ 

As we show in section 11.5.2.3 and section 11.5.2.4, we can often exploit additional structure in $Q$ to reduce further the complexity of the ﬁxed-point equations, and hence of the resulting update steps. The following discussion, which describes the procedure of applying the ﬁxed- point equations to ﬁnd a stationary point of the energy functional, is orthogonal to these simpliﬁcations. 

# 11.5.2.2 Optimization 

Given a set of ﬁxed-point equations as in equation (11.61), our task is to ﬁnd a distribution $Q$ that satisﬁes them. As in section 11.5.1, our strategy is based on the key observation that the factor $\psi_{j}$ does not afect the right-hand side of the ﬁxed-point equations deﬁning its value: The ﬁrst expectation, I $\pmb{{\cal E}}_{Q}\left[\ln\tilde{P}_{\Phi}\mid\pmb{c}_{j}\right]$ i , is conditioned on $c_{j}$ and therefore does not depend on the parameter iz ation of $\psi_{j}$ . The same observation holds for the second expectation, $E_{Q}[\ln\psi_{k}\mid c_{j}]$ | , for any $k\neq j$ . (Importantly, there is no such term for $k\,=\,j$ in the right-hand side.) Thus, we can use the same general approach as in Mean-Field : We can optimize each potential $\psi_{j}$ , given values for the other potentials , by simply selecting $\psi_{j}$ to satisfy the ﬁxed-point equation. As for the case of mean ﬁeld, this step is guaranteed to increase (or not decrease) the value of the objective; thus, the overall process is guaranteed to converge to a stationary point of the objective. 

This last step requires that we perform inference in the approximating distribution $Q$ to compute the requisite expectations. Although this step was also present (implicitly) in the mean ﬁeld approximation, there the structure of the approximating distribution was trivial, and so the inference step involved only individual marginals. Here, we need to collect the expectation of several factors, and each of these requires that we compute expectations given diferent assignments to the factor of interest. (See exercise 11.27 for a discussion of how these expectations can be computed efciently.) For a general distribution $Q$ , even one with tractable structure, running inference in the corresponding network $\mathcal{H}_{Q}$ can be costly, and we may want to reduce the number of calls to the inference subroutine. 

This observation leads to a question of how best to perform updates for several factors in $Q$ . We can consider two strategies. The sequential update strategy is similar to our strategy in the mean ﬁeld algorithm: We choose a factor $\psi_{j}$ , apply the ﬁxed-point equation to that factor by running inference in $\mathcal{H}_{Q}$ , update the distribution, and then repeat this process with another factor until convergence. The problematic aspect of this approach is that we need to perform inference after each update step. For example, if we are using cluster tree inference in the network $\mathcal{H}_{Q}$ , the network parameter iz ation changes after each update step, so we need to recalibrate the clique tree every time. Some of these steps can be made more efcient by selecting an appropriate order of updates and using dynamic programming (see exercise 11.27), but the process can still be quite expensive. 

An alternative approach is the parallel update strategy, where we compute the right-hand side of our ﬁxed-point equations (for example, equation (11.61)) simultaneously for each of the factors in $Q$ . If we are using a cluster tree for inference, this process involves multiple queries from the same calibrated cluster tree. Thus, we can perform a single calibration step and use the resulting tree to reestimate all of our potentials. However, the diferent queries required all have diferent evidence; hence, it is not easy to obtain signiﬁcant computational savings, and the algorithms needed are fairly tricky. Nevertheless, this approach might be less costly than recalibrating the clique tree $J$ times. 

On the other hand, the guarantees provided by these two update steps are diferent. For the sequential update strategy, we can prove that each update step is monotonic in the energy functional: each step maximizes the value of one potential given the values of all the others, and therefore is guaranteed not to decrease (and generally to increase) the energy functional. This monotonic improvement implies that iterations of sequential updates necessarily converge, generally to a local maximum. The issue of convergence is more complicated in the parallel update strategy. Because we update all the potentials at once, we have no guarantees that any ﬁxed-point equation holds after the update; a value that was optimal for $\psi_{j}$ with respect to the values of all other factors before the parallel update step is not necessarily optimal given their new values. As such, it is conceivable that parallel updates will not converge (for example, oscillate between two sets of values for the potentials). Such oscillations can generally be avoided using damped update steps, similar to these we discussed in the case of cluster- graph belief propagation (see box 11.B), but this modiﬁed procedure still does not guarantee convergence. 

At this point, there is no generally accepted procedure for scheduling updates in variational methods, and diferent approaches are likely to be best for diferent applications. 

# 11.5.2.3 Simplifying the Update Equations 

Equation (11.61) provides a general characterization of the ﬁxed points of the energy functional, for any approximating class of distributions $\mathcal{Q}$ obeying a particular factorization, as in equ tion (11.58). In many cases, we can exploit additional structure of the approximating class Q and of the distribution $P_{\Phi}$ to simplify signiﬁcantly the form of these ﬁxed-point equations and thereby make the update step more efcient. 

The simpliﬁcations we describe take two forms. The ﬁrst utilizes marginal independencies in $\mathcal{Q}$ to simplify the right-hand side of the ﬁxed-point equation, eq tion (11.61), elimi ing irrelevant terms. The second xploits interactions between the form of Q and the form of $P_{\Phi}$ to simplify the factorization of Q , without loss in expressive power. Both simpliﬁcations allow the ﬁxed-point updates to be performed more efciently. We motivate each of the simpliﬁcations using an example, and then we present the general result. 

Example 11.13 Once again, consider the $4\!\times\!4$ grid network. Assume that we approximate it by a “row” network that has the structure shown in ﬁgure 11.17a. This approximating network consists of four independent chains. Now we can apply the general form of the ﬁxed-point equation equation (11.61) for a speciﬁc entry in our approximation, say: 

$$
\begin{array}{r}{{}_{1}(a_{1,1},a_{1,2})\propto\exp\left\{\begin{array}{l}{\pmb{E}_{Q}\left[\ln\tilde{P}_{\Phi}\mid a_{1,1},a_{1,2}\right]}\\ {-\sum_{\scriptstyle\begin{array}{c}{i=1,\dots,4}\\ {j=1,\dots,3}\\ {(i,j)\neq(1,1)}\end{array}}\pmb{E}_{Q}\left[\ln\psi_{(i,j)}(A_{i,j},A_{i,j+1})\mid a_{1,1},a_{1,2}\right]}\\ {}\end{array}\right\}}\end{array}
$$ 

As in the proof of corollary 11.5, the expectation of $\ \ln{\tilde{P}_{\Phi}}$ is the sum of expectations of the logarithm of each of the potentials in $\Phi$ . Some of these terms, however, do not depend on the choice of value of $A_{1,1},A_{1,2}$ we are evaluating. For example, because $A_{2,1}$ and $A_{2,2}$ are independent of $A_{1,1},A_{1,2}$ in $Q$ , we conclude that 

$$
E_{\{A_{2,1},A_{2,2}\}\sim Q}[\ln\phi(A_{2,1},A_{2,2})\mid a_{1,1},a_{1,2}]=E_{\{A_{2,1},A_{2,2}\}\sim Q}[\ln\phi(A_{2,1},A_{2,2})].
$$ 

Thus, this term will contribute the same value to each of the entries of $\psi(A_{1,1},A_{1,2})$ , and can therefore be absorbed into the corresponding normalizing term. We can continue in this manner and remove all terms that are not dependent on the context of the factor we are in- terested in. Overall, we can remove any term $E_{Q}[\ln\phi(A_{i,j},A_{i,j+1})\mid a_{1,1},a_{1,2}]$ | and any term $E_{Q}[\ln\phi(A_{i,j},A_{i+1,j})\mid a_{1,1},a_{1,2}]$ | except those where i $i=1$ ilarly, we can remove any term $E_{Q}\left[\ln\psi_{(i,j)}{\left(A_{i,j},A_{i,j+1}\right)}\mid a_{1,1},a_{1,2}\right]$   except those where i $i\,=\,1$ . These simpliﬁcations result in the following update rule: 

$$
\begin{array}{r l}&{\psi_{1,1}(a_{1,1},a_{1,2})\propto}\\ &{\qquad\exp\left\{\begin{array}{l}{\sum_{j=1,\dots,3}\pmb{E}_{\{A_{1,j},A_{1,j+1}\}\sim Q}\left[\ln\phi_{(1,j)}(A_{1,j},A_{1,j+1})\mid a_{1,1},a_{1,2}\right]}\\ {+\sum_{j=1,\dots,4}\pmb{E}_{\{A_{1,j},A_{2,j}\}\sim Q}\left[\ln\phi_{(1,j)}(A_{1,j},A_{2,j})\mid a_{1,1},a_{1,2}\right]}\\ {-\sum_{j=2,3}\pmb{E}_{\{A_{1,j},A_{1,j+1}\}\sim Q}\left[\ln\psi_{(1,j)}(A_{1,j},A_{1,j+1})\mid a_{1,1},a_{1,2}\right]}\end{array}\right\}.}\end{array}
$$ 

We can generalize this analysis to arbitrary sets of factors: 

Theorem 11.12 

$$
\psi_{j}(\pmb{c}_{j})\propto\exp\left\{\sum_{\phi\in A_{j}}\pmb{E}_{\mathcal{X}\sim Q}[\ln\phi\ |\ \pmb{c}_{j}]-\sum_{\psi_{k}\in B_{j}}\pmb{E}_{\mathcal{X}\sim Q}[\ln\psi_{k}\ |\ \pmb{c}_{j}]\right\},
$$ 

where 

$$
A_{j}=\{\phi\in\Phi:Q\neq(U_{\phi}\perp C_{j})\}
$$ 

and 

$$
B_{j}=\{\psi_{k}:Q\neq(C_{k}\perp C_{j})\}-\{C_{j}\}.
$$ 

![](images/c009de3902c6b9500417e012007efbdcb821fa3e5bc46817aff034321c928574.jpg) 
Figure 11.18 A diamond network and three possible approximating structures 

Stated in words, this result shows that the parameter iz ation of a factor $\psi_{j}(C_{j})$ depends only on factors in $P_{\Phi}$ and in $Q$ whose scopes are not independent of $C_{j}$ in $\mathcal{Q}$ . This result, applied to example 11.13, provides us precisely with the simpliﬁcation shown: only factors whose scopes intersect with the ﬁrst row are relevant to $\psi_{1,1}(A_{1,1},A_{1,2})$ . Thus, we can use independence properties of the approximating family $\mathcal{Q}$ to simplify the right-hand side of equation (11.61) by removing irrelevant terms. 

# 11.5.2.4 Simplifying the Family $\mathcal{Q}$ 

It turns out that a similar analysis allows us to simplify the form of the approximating family $\mathcal{Q}$ without loss in the quality of the approximation. We start by considering a simple example. 

Example 11.14 Consider again the four-variable pairwise Markov network of ﬁgure 11.18a, which is parameterized by the pairwise factors: 

$$
{\cal P}_{\Phi}(A,B,C,D)\propto\phi_{A B}(A,B)\cdot\phi_{B C}(B,C)\cdot\phi_{C D}(C,D)\cdot\phi_{A D}(A,D).
$$ 

Consider applying the variational approximation with the distribution 

$$
Q(A,B,C,D)=\frac{1}{Z_{Q}}\psi_{1}(A,B)\cdot\psi_{2}(C,D)
$$ 

that has the structure shown in ﬁgure 11.18b. Using equation (11.62), we conclude that the ﬁxed-point characterization of $\psi_{1}$ is 

$$
\psi_{1}(a,b)\propto\exp{\{E_{Q}[\ln\phi_{A B}(A,B)\mid a,b]+E_{Q}[\ln\phi_{B C}(B,C)\mid a,b]+E_{Q}[\ln\phi_{A D}(A,B)\mid a,b]\}}
$$ 

Can we further simplify this equation? Consider the ﬁrst term. Clearly, ${\cal E}_{Q}[\ln\phi_{A B}(A,B)\mid a,b]=$ | $\ln\phi_{A B}(a,b)$ . What about the second term, $E_{Q}[\ln\phi_{B C}(B,C)\mid a,b].$ | ? To compute this expectation, we need to compute $Q(B,C\mid a,b)$ . According to the structure of $Q$ , we can see that $Q(B,C\mid a,b)={\left\{\begin{array}{l l}{Q(C)}&{I\!f\,B=b}\\ {0}&{o t h e r w i s e.}\end{array}\right.}$ 

Thus, we conclude that 

$$
\pmb{{\cal E}}_{A,B,C\sim Q}[\ln\phi_{B C}(B,C)\mid a,b]=\pmb{{\cal E}}_{C\sim Q}[\ln\phi_{B C}(b,C)].
$$ 

We can simplify the third term in exactly the same way, concluding that: 

$$
\psi_{1}(a,b)\propto\exp\left\{\ln\phi_{A B}(a,b)+E_{C\sim Q}[\ln\phi_{B C}(b,C)]+E_{D\sim Q}[\ln\phi_{A D}(a,D)]\right\}.
$$ 

Setting $\psi_{1}^{\prime}(a)=\exp\{E_{D\sim Q}[\ln\phi_{A D}(a,D)]\}$ { } and $\psi_{1}^{\prime\prime}(b)=\exp\{E_{C\sim Q}[\ln\phi_{B C}(b,C)]\}$ { } , we con- ∼ ∼ clude that the optimal $\psi_{1}$ factorizes as a product of three factors: 

$$
\psi_{1}(A,B)=\phi_{A B}(A,B)\cdot\psi_{1}^{\prime}(A)\cdot\psi_{1}^{\prime\prime}(B).
$$ 

Have we gained anything from this decomposition? First, we see that $Q$ preserves the original pairwise interaction term $\phi(A,B)$ from $P_{\Phi}$ . Moreover, the efect of the interactions between these variables and the rest of the network ( $\mathcal{C}$ and $D$ in this example) is summarized by $^a$ univariate factor for each of the variables. Thus, $Q$ does not change the interaction between $A$ and $B$ . 

Applying the same set of arguments to $\psi_{2}$ , we conclude that we can rewrite $Q$ as 

$$
Q^{\prime}(A,B,C,D)=\frac{1}{Z_{Q}}\phi_{A B}(A,B)\cdot\phi_{C D}(C,D)\cdot\psi_{1}^{\prime}(A)\cdot\psi_{1}^{\prime\prime}(B)\cdot\psi_{2}^{\prime}(C)\cdot\psi_{2}^{\prime\prime}(D)
$$ 

The preceding discussion shows that the best approximation to $P_{\Phi}$ within $\mathcal{Q}$ can be rewritten in the form of $Q^{\prime}$ . Thus, there is no point in using the more complicated form of the approximating family of equation (11.63); we may as well use the form of $Q^{\prime}$ in equation (11.64). Note that the form of $Q^{\prime}$ involves a product of a subset of the original factors, which we keep intact without change, and $a$ set of new factors, which we need to optimize. In this example, instead of estimating two pairwise potentials, we estimate four univariate potentials, which utilize a smaller number of parameters. 

Moreover, the update equations for $Q^{\prime}$ are simpler. Consider, for example, applying equa- tion (11.62) for $\psi_{1}^{\prime}$ : 

$$
\begin{array}{r l}&{)\propto\pmb{E}_{B\sim Q^{\prime}}[\ln\phi_{A B}(a,B)\mid a]+\pmb{E}_{D\sim Q^{\prime}}[\ln\phi_{A D}(a,D)\mid a]+\pmb{E}_{B,C\sim Q^{\prime}}[\ln\phi_{B C}(B,B)\mid a]}\\ &{\quad-\pmb{E}_{B\sim Q^{\prime}}[\ln\phi_{A B}(a,B)\mid a]-\pmb{E}_{B\sim Q^{\prime}}[\ln\psi_{1}^{\prime\prime}(B)\mid a]}\\ &{\quad=\pmb{E}_{D\sim Q^{\prime}}[\ln\phi_{A D}(a,D)\mid a]+\pmb{E}_{B,C\sim Q^{\prime}}[\ln\phi_{B C}(B,C)\mid a]-\pmb{E}_{B\sim Q^{\prime}}[\ln\psi_{1}^{\prime\prime}(B)\mid a].}\end{array}
$$ 

The terms involving $E_{Q^{\prime}}[\ln\phi_{A B}\mid a]$ | ] appear twice, once as a factor in $P_{\Phi}$ and once as a factor in $Q^{\prime}$ . These two terms cancel out, and we are left with the simpler update equation. Although this equation does not explicitly mention $\phi_{A B}$ , this factor participates in the computation of $Q^{\prime}(B\mid a)$ that implicitly appears in $E_{B\sim Q^{\prime}}[\ln\psi_{1}^{\prime\prime}(B)\mid a]$ | . 

Note that this result is somewhat counter intuitive, since it shows that the interactions between $A$ and $B$ are captured by the original potential in $P_{\Phi}$ . Intuitively, we would expect the chain of inﬂuence $\scriptstyle A-D-C-B$ to introduce additional interactions between $A$ and $B$ that should be represented in $Q$ . This is not the only counter intuitive result. 

# Example 11.15 

Consider another approximating family for the same network, using the network structure shown in ﬁgure 11.18c. In this approximation, we have two pairwise factors, $\psi_{1}(A,C)$ , and $\psi_{2}(B,D)$ . Applying the same set of arguments as before, we can show that the update equation can be written as 

$$
\begin{array}{r c l}{\ln\psi_{1}(a,c)}&{\propto}&{{\cal E}_{B\sim Q}[\ln\phi_{A B}(a,B)]+{\cal E}_{D\sim Q}[\ln\phi_{A D}(a,D)]}\\ &&{+{\cal E}_{B\sim Q}[\ln\phi_{B C}(B,c)]+{\cal E}_{D\sim Q}[\ln\phi_{C D}(c,D)].}\end{array}
$$ 

Thus, we can factorize $\psi_{1}$ into two factors, one with a scope of $A$ and the other with $C$ 

$$
\psi_{1}(A,C)=\psi_{1}^{\prime}(A)\cdot\psi_{1}^{\prime\prime}(C).
$$ 

In other words, the approximation in this case is equivalent to the mean ﬁeld approximation. This result shows that, in some cases, we can remove spurious dependencies in the approximating distribution. However, this result is surprising, since it holds regardless of the actual values of the potentials in $P_{\Phi}$ . And so, we can imagine a network where there are very strong interactions between $A$ and $C$ and between $B$ and $D$ in $P_{\Phi}$ , and yet the variational approximation with a network structure of ﬁgure 11.18c will not capture these dependencies. This is a consequence of using $I^{,}$ -projections. Had we used an $M\cdot$ -projection that minimizes $D(P_{\Phi}\|Q)$ | | , then we would have represented the dependencies between $A$ and $C$ ; see exercise 11.30. 

These two examples suggest that we can use the ﬁxed-point characterization to reﬁne an initial approximating network by factorizing its factors into a product of, possibly smaller, factors and potentials from $P_{\Phi}$ . We now consider the general theory of such factorizations and then discuss its implications. 

We start with a simple deﬁnition and a proposition that form the basis of the simpliﬁcations we consider. 

Deﬁnition 11.8 interface 

# Example 11.16 

Let $\mathcal{H}$ be a Markov network structure and let $X,Y\subseteq\mathcal{X}$ . We deﬁne the $Y$ - interface of $X$ , denoted Interfac $z e_{\mathcal{H}}(X;Y)$ , to be the minimal subset of X such that $\mathrm{sep}_{\mathcal{H}}(X;Y\mid$ Interface $_{\mathcal{U}}(X;Y)_{,}$ ) . That is, the $Y$ -interface of $X$ is the subset of $X$ that sufces to separate it from $Y$ . 

The $\{A,D\}$ -in rface of $\{A,B\}$ in $\mathcal{H}_{P_{\Phi}}$ of ﬁgure 11.18 is $\{A,B\}$ , since neither $A$ is parated from $\{A,D\}$ given B , no is B is separated from $\{A,D\}$ given A . In $\mathcal{H}_{\mathcal{Q}_{1}}$ , we have that B is separated from $\{A,D\}$ given A , so that Interfa $\iota c e_{{\mathcal{H}_{\mathcal{Q}_{1}}}}(\{A,B\};\{A,D\})$ is $\{A\}$ . The same holds in ${\mathcal{H}}_{{\mathcal{Q}}_{3}}$ . In $\mathcal{H}_{\mathcal{Q}_{2}}$ , we have that, again, neither $A$ nor $B$ sufces to separate the other from $\{A,D\}$ , and hence, Interfa $\iota c e_{{\mathcal{H}_{\mathcal{Q}_{2}}}}(\{A,B\};\{A,D\})=\{A,B\}$ . 

The deﬁnition of interface can be used to reduce the scope of the conditional expectations in the ﬁxed-point equations: 

# Proposition 11.6 

$$
E_{U_{\phi}\sim Q}[\phi\mid c_{j}]=E_{U_{\phi}\sim Q}[\phi\mid c_{j}\langle I n t e r f a c e_{\mathcal{H}}(C_{j};U_{\phi})\rangle].
$$ 

The proof follows immediately from the deﬁnition of conditional independence. 

This proposition provides a principled approach for reformulating terms on the right-hand side of the ﬁxed-point equation. 

We can use this simpliﬁcation result to deﬁne a two-phase strategy for designing approximation. First, we deﬁne a “rough” outline for approximation by deﬁning $Q$ over factors with a fairly large scope. We use this outline to obtain a set of update equations, as implied by equation (11.62) on $Q$ . We then derive a ﬁner-grained representation by factorizing each of these factors using proposition 11.6. This process results in a ﬁner- grained approximation that is provably equivalent to the one with which we started. 

Theorem 11.13 (Factorization) Let $\mathcal{Q}$ be an approximating family deﬁned in terms of factors $\{\psi_{j}(C_{k})\}$ , which Markov network structure $\mathcal{H}_{\mathcal{Q}}$ . Let $Q\in{\mathcal{Q}}$ be a stationary point of the energy functional $F[\tilde{P}_{\Phi},Q]$ subject to the given factorization. Then, factors in $Q$ are factorized as 

$$
\psi_{j}(C_{j})=\prod_{\phi\in\Phi_{j}}\phi\prod_{D_{l}\in\mathcal{D}_{j}}\psi_{j,l}(D_{l}),
$$ 

where 

$$
\Phi_{j}=\{\phi\in\Phi:S c o p e[\phi]\subseteq C_{j}\}
$$ 

and 

$$
D_{j}=\{I n t e r f a c e_{\mathcal{H}_{\emptyset}}(C_{j};X):X\in\{S c o p e[\phi]:\phi\in\Phi-\Phi_{j}\}\cup\{S c o p e[\psi_{k}]:k\neq j\},
$$ 

This theorem states that $\psi_{j}$ can be written as the product of two sets of factors. The ﬁrst set contains factors in the original distribution $P_{\Phi}$ whose scope is a subset of the scope of $\psi_{j}$ . The factors in the second set are the interfaces of $\psi_{j}$ with other factors that appear in the update equation. These include factors in $P_{\Phi}$ that are partially “covered” by the scope of $\psi_{k}$ , and other factors in $Q$ . The set $\mathcal{D}_{k}$ deﬁnes the set of interfaces between $\psi_{k}$ and these factors. 

To gain a better understanding of this theorem, let us consider various approximations in two concrete examples. The ﬁrst example serves to demonstrate the ease with which this theorem allows us to determine the form of the factorization of $Q$ . 

Let us return to example 11.14. In example 11.16, we have already shown the interfaces of $\{A,B\}$ with $\{A,D\}$ in $\mathcal{H}_{1}$ . This analysis, togeth rem 11.13, direct mply the reduced factor- example 11.14 In particular, for $\psi_{1}(\{A,B\})$ { } , we have that $\Phi_{1}$ contains only the factor $\phi(\{A,B\})$ { } in $P_{\Phi}$ , which therefore constitutes the ﬁrst term in the factorization of equation (11.65). The second set of terms in the equation corresponds to the interfaces of $\{A,B\}$ with other factors in bo $\mathcal{H}_{P_{\Phi}}$ and in $\mathcal{H}_{\mathcal{Q}_{1}}$ . We get two such interfaces: one with scope $\{A\}$ from the factor $\phi\big(\{A,D\}\big)$ in $P_{\Phi}$ , and one with scope $\{B\}$ f e factor $\phi(\{B,C\})$ . 

Assume that we add the edge $A{-}C$ , as in ﬁgure 11.18d. Now, Interfa $c e_{\mathcal{H}_{\mathcal{Q}_{3}}}(\{A,B\};\{B,C\})$ is the entire set $\{A,B\}$ , since $B$ no longer separates $C$ from $A$ . Thus, in this case, the second set of terms in the factorization of $\psi$ also contains a new pairwise interaction factor $\psi_{1,\{A,B\}}$ . As $^a$ consequence, the pairwise interaction of $A,B$ is no longer the same in $Q$ and in $P_{\Phi}$ . This result is somewhat counterintuit n the simpler ork $\mathcal{H}_{\mathcal{Q}_{1}}$ , h contained no factors allowing any interaction between the $A,B$ pair and the $C,D$ pair, the $A,B$ interaction was the same in $P_{\Phi}$ and in $Q$ . But if we enrich our approximation (presumably allowing a better ﬁt via the introduction of the $A,C$ factor), the pairwise interaction term does change. 

Finally, $\mathcal{H}_{\mathcal{Q}_{2}}$ does not contain an $\{A,B\}$ factor. Here, $\Phi_{j}\,=\,\emptyset$ for both factors in $\mathcal{H}_{\mathcal{Q}_{2}}$ , and each $\mathcal{D}_{j}$ consists solely of singleton scopes; for example, Interfa $c e_{\mathcal{H}_{\mathcal{Q}_{2}}}(\{A,C\};\{A,D\})=\{A\}$ . 

Our second example serves to illustrate the two-phase strategy described earlier, where we ﬁrst select a “rough” approximation containing a few large factors and then use the theorem to reﬁne them. 

Consider again our running example of the $4\times4$ grid. Suppose we select an approximation where each factor consists of the variables in a single row in the grid. Thus, for example, $C_{1}=$ $\{A_{1,1},.\,.\,.\,,A_{1,4}\}$ . Note that this approximation is not the one shown in ﬁgure 11.17a, since the structure in our approximation here is a full clique over each row. We now apply theorem 11.13. What is the factorization of $C_{1}$ ? First, we search for factors in $\Phi_{1}$ . We see that the factors $\phi(A_{1,1},A_{1,2})$ , $\phi(A_{1,2},A_{1,3})$ , and $\phi(A_{1,3},A_{1,4})$ have a scope that is a subset of $C_{1}$ . Next, we consider the interfaces between $C_{1}$ and other factors in $P_{\Phi}$ and $Q$ . For example, the interface with $\phi(A_{1,1},A_{2,1})$ is $\{A_{1,1}\}$ . Similarly, $\{A_{1,2}\},\,\{A_{1,3}\}$ , and $\{A_{1,4}\}$ are interfaces with other factors in $P_{\Phi}$ . It is easy to convince ourselves that these are the only non-empty interfaces in $\mathcal{L}_{1}$ . Thus, by applying theorem 11.13, we get the following factorization: 

$$
\begin{array}{r c l}{\psi_{1}(A_{1,1},\dots,A_{1,4})}&{=}&{\phi(A_{1,1},A_{1,2})\cdot\phi(A_{1,2},A_{1,3})\cdot\phi(A_{1,3},A_{1,4})}\\ &&{\psi_{1,1}(A_{1,1})\cdot\psi_{1,2}(A_{1,2})\cdot\psi_{1,3}(A_{1,3})\cdot\psi_{1,4}(A_{1,4}).}\end{array}
$$ 

We conclude that, once we decide that the approximation should decouple the rows in the group, we might as well work with an approximation where we keep all original potentials along each row and introduce univariate potentials only to capture interactions along columns. Additional potentials, such as a potential between $A_{1,1}$ and $A_{1,3}$ , would not improve the approximation. Thus, while we started with an approximation containing full cliques on each of the rows, we ended up with an approximation whose structure is that of ﬁgure 11.17a, and where we have only the original factors and new factors over single variables. 

We can work directly with this new factorized form of $Q$ , ignoring our original factorization entirely. More precisely, we deﬁne $Q^{\prime}$ to be 

$$
\begin{array}{r c l}{{Q^{\prime}(\mathcal{X})}}&{{=}}&{{\phi(A_{1,1},A_{1,2})\cdot\phi(A_{1,2},A_{1,3})\cdot\phi(A_{1,3},A_{1,4})}}\\ {{}}&{{}}&{{\cdot\cdot\cdot}}\\ {{}}&{{}}&{{\phi(A_{4,1},A_{4,2})\cdot\phi(A_{4,2},A_{4,3})\cdot\phi(A_{4,3},A_{4,4})}}\\ {{}}&{{}}&{{\psi_{1,1}(A_{1,1})\cdot\,\cdot\psi_{4,4}(A_{4,4}).}}\end{array}
$$ 

In this new form, we ﬁx the value of all the pairwise potentials, and so we have to deﬁne an update rule only for the new singleton potentials. For example, consider the ﬁxed-point equation for $\psi_{1,1}(A_{1,1})$ . Applying theorem 11.12 we get that 

$$
\begin{array}{r l}&{\ln\psi_{1,1}(a_{1,1})\propto}\\ &{\qquad+E_{Q^{\prime}}[\ln\phi(A_{1,1},A_{2,1})\mid a_{1,1}]+E_{Q^{\prime}}[\ln\phi(A_{1,2},A_{2,2})\mid a_{1,1}]}\\ &{\qquad+E_{Q^{\prime}}[\ln\phi(A_{1,3},A_{2,3})\mid a_{1,1}]+E_{Q^{\prime}}[\ln\phi(A_{1,4},A_{2,4})\mid a_{1,1}]}\\ &{\qquad-E_{Q^{\prime}}[\ln\psi_{1,2}(A_{1,2})\mid a_{1,1}]-E_{Q^{\prime}}[\ln\psi_{1,3}(A_{1,3})\mid a_{1,1}]-E_{Q^{\prime}}[\ln\psi_{1,4}(A_{1,4})\mid a_{1,1}]}\end{array}
$$ 

where we have exploited the fact that the terms involving factors such as $\phi(A_{1,1},A_{1,2})$ appear in both $P_{\Phi}$ and $Q$ , and so cancel out of the equation. Note that to compute terms such as $E_{Q^{\prime}}[\ln\phi(A_{1,2},A_{2,2})\mid a_{1,1}]$ | we need to evaluate $Q^{\prime}(A_{1,2},A_{2,2}\,\,\mid\,\,a_{1,1})\;=\;Q^{\prime}(A_{1,2}\,\,\mid\,\,a_{1,1})$ · $Q^{\prime}(A_{2,2})$ (where we used the independencies in $Q^{\prime}$ to simplify the joint marginal). Note that $Q^{\prime}(A_{2,2})$ does not change when we update factors in the ﬁrst row, such as $\psi_{1,1}(A_{1,1})$ . Thus, we can cache the computation of this marginal when updating the factors $\psi_{1,1}(A_{1,1}),.\,.\,.\,,\psi_{1,4}(A_{1,4})$ . When performing inference in a large model this can result in dramatic efect. 

cluster mean ﬁeld 

This example is a special case of an approximation approach called cluster mean ﬁeld . In this case, our initial approximation has the form 

$$
Q(\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}(C_{j}),
$$ 

where the scopes $C_{1},\ldots,C_{K}$ are partition of $\mathcal{X}$ . That is, each pair of factors have disjoint scopes, and each variable in X appears in one factor. This approximation resembles the mean ﬁeld approximation, except that it is clusters, rather than individual variables, that are marginally independent. We can now apply theorem 11.13 to reﬁne the approximation. Because the factors are all disjoint, there are no chains of inﬂuence, and so the interfaces take a particularly simple form: 

Proposition 11.7 Let $\begin{array}{r}{Q(\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}(C_{j})}\end{array}$ be a cluster mean ﬁeld approximation to a set of factors $P_{\Phi}$ , and let $\psi_{j}$ be a factor of Q . Then, the set $\mathcal{D}_{j}$ of theorem 11.13 can be written as 

$$
\begin{array}{r}{\mathcal{D}_{j}=\{C_{j}\cap S c o p e[\phi]:\phi\in\Phi-\Phi_{j}\}-\{\emptyset\}.}\end{array}
$$ 

The proof follows directly from the independence properties in $Q$ , and is left as an exercise (exercise 11.31). 

In words, this result states that the interfaces of a cluster are simply the places where the cluster scope intersects potentials in $\Phi$ that are not fully contained in the cluster. In our grid example, when we choose the clusters to be the individual columns, the interfaces are the intersections with the row potentials, which are precisely the singleton variables that we discussed in example 11.18. 

We conclude this discussion with a slightly more elaborate example, demonstrating again the strength of this result: 

Example 11.19 Consider again our $4\times4$ grid, and the “comb” approximation whose structure is shown in ﬁg- ure 11.19a. In this structure, we have a fully connected clique over each of the columns, and a “backbone” connecting the columns to each other. Consider again the factorization of the potential over $C_{1}=\{A_{1,1},.\,.\,.\,,A_{4,1}\}$ . As in the previous example, the ﬁrst term in the new factorization contains the pairwise factors $\phi(A_{1,1},A_{2,1})$ , $\phi(A_{2,1},A_{3,1})$ , and $\phi(A_{3,1},A_{4,1})$ . The second set of terms contains the interfaces with other factors in $P_{\Phi}$ and $Q$ . Due to the structure of the approxi- mation, the $Q$ interfaces introduce only singleton potentials. The factors in $P_{\Phi}$ , however, are more interesting. Consider, for example, the factor $\phi(A_{4,1},A_{4,2})$ . The interface of $C_{1}$ with $\{A_{4,1},A_{4,2}\}$ is $A_{1,1},A_{4,1}$ — the variable $A_{4,1}$ separates $C_{1}$ from itself, and the variable $A_{1,1}$ from $A_{4,2}$ . Now, consider $a$ factor $\phi(A_{2,3},A_{3,3}).$ ; in this case, the interface is simply $A_{1,1}$ , which separates the ﬁrst column from both of these variables. Continuing this argument, it follows that all other factors in 

![](images/f4756da2eccc31dfdd59eca4ae25d79391c4298258641d82ecb805b0d89948af.jpg) 
Figure 11.19 Simpliﬁcation of approximating structure in cluster mean ﬁeld. (a) Example of an approximating structure we can use in a variational approximation of a $4\!\times\!4$ grid network. (b) Simpliﬁcation of that network using theorem 11.13. 

$P_{\Phi}$ induce an interface containing a variable at the head of the column and (possibly) another variable in the column. Thus, we can eliminate any (new) pairwise interaction terms between any other pair of variables. For $a$ general $n\times n$ grid, this reduces the overall number of (new) pairwise potentials from $\textstyle n\cdot{\binom{n}{2}}$ to $n\times(n-1)$ . 

# 11.5.2.5 Selecting the Approximation 

In general, both the quality and the computational complexity of the variational approximation depend on the structure of $P_{\Phi}$ and the structure of the approximating family $\mathcal{Q}$ . There are several guiding intuitions. First, we want to be able to perform efcient inference in the approximating network. In example 11.18, the approximating structure was a chain of variables, where we can perform inference in linear time (as a function of the number of variables in the chain). In general, we often select our network so that the resulting factorization leads to a tractable network (that is, one of low tree-width). 

It is important to note, however, that the structure of the original distribution is not the only aspect in determining the complexity of inference in $Q$ . We also need to take into account factors that correspond to the interfaces of the cluster. In our grid example, these interfaces involved a single variable at time, and so they did not add to the network complexity. However, in more complex networks, these factors can have a signiﬁcant efect. 

Another consideration besides computational complexity is the quality of our approximation. Intuitively, we should design $\mathcal{Q}$ so as to preserve the strong dependencies in $P_{\Phi}$ . By preserving such dependencies we maintain the main efects in the distribution we want to apply. 

These intuitions provide some guidelines in choosing the approximating distribution. How- ever, these choices are far from an exact science at this stage. The theory we described here allows to automate two parts of the process: deﬁning the form of the approximation given some initial rough set of (disjoint or overlapping) clusters; and deﬁning the ﬁxed-point iterations to 

![](images/1857bbd9c72bde62828a805e9b24b10d3d76857a6ef6aeada9f5c6af25a8dd45.jpg) 
Figure 11.20 Illustration of the variationa bound $-\ln(x)\geq-\lambda x+\ln(\lambda)+1$ . For any $x$ , the bound holds for all $\lambda$ , and is tight for some value of λ . 

optimize such an approximation. The current tools do not provide for an automated way for determining what are reasonable sets of clusters to achieve a desired degree of approximation. 

# 11.5.3 Local Variational Methods $\star$ 

variational method lower bound 

variational lower bound 

Lemma 11.2 

The general method that we used throughout this chapter is an instance of a general class of methods known as variational methods . In this class of methods, we take a complex objective function $f_{\mathrm{obj}}(\pmb{x})$ , and lower or upper bound it using a parameterized family of functions ${\pmb g}({\pmb x},{\pmb\lambda})$ . Focusing, for concreteness, on the case of a lower bound , this family has the property that $f_{\mathrm{obj}}(\pmb{x})\,\geq\,\pmb{g}(\pmb{x},\pmb{\lambda})$ for any value of $\lambda$ , and that, for any $_{_{x}}$ , the bound is tight for some value of λ (a diferent one for every $_{_{x}}$ ). 

As an example, we can show the variational lower bound : 

For any choice of $\lambda$ and $x$ 

$$
-\ln(x)\geq-\lambda x+\ln(\lambda)+1,
$$ 

and, for any $x$ , this bound is tight for some value of $\lambda$ . 

Proof Consider the tangent of $\ln(x)$ at the point $x_{0}$ 

$$
f_{\mathrm{obj}}(x:x_{0})=\ln(x_{0})+(x-x_{0})\frac{1}{x_{0}}=\frac{x}{x_{0}}+\ln(x_{0})-1.
$$ 

Since $\ln(x)$ is a concave function, it is upper bounded by each of its tangents. And so, $-\ln(x)\geq-f_{\mathrm{obj}}(x:x_{0})$ for any choice of $x$ and $x_{0}$ . Setting $x_{0}=\lambda^{-1}$ leads to the desired result. 

convex duality 

variational parameter 

This result is illustrated in ﬁgure 11.20. It is a special case of a general result in the ﬁeld of convex duality , which guarantees the existence of such bounds for a broad class of functions. 

This l wer bound allows to approximate a nonlinear function $-\ln(x)$ with a term that s linear in x . This simpliﬁcation comes at the price of introducing a new variational parameter λ , whose value is undetermined. If we optimize $\lambda$ exactly for each value of $x$ , we obtain a tight lower bound, but a bound is obtained for any value of $\lambda$ . 

The techniques we have used in this chapter so far also fall into this category. Equation (11.5) shows that the energy functional is a lower bound on the log-partition function for any distribu- tion $Q$ . Thus, we can take $f_{\mathrm{obj}}$ to be the partition function, $_{_{x}}$ to correspond to the parameters of the true distribution $P_{\Phi}$ , and $\lambda$ to correspond to the parameters of the approximating dis- tribution $Q$ . Although the lower bound is tight when $Q$ precisely represents $P_{\Phi}$ , for reasons of efciency, we generally optimize $Q$ in a restricted space that provides a bound, but not a tight one, on the log-partition function. 

variational variable elimination 

This general approach of introducing auxiliary variational parameters that help in simplifying a complex objective function appears in many other domains. While it is beyond our scope to introduce a general theory of variational methods, we now brieﬂy describe one other application of variational methods that is relevant to probabilistic inference and does not fall directly within the scope of optimizing the energy functional. This application arises in the context of exact inference using an algorithm such as variable elimination. Here, we use variational bounds to avoid creating large factors that can lead to exponential complexity in the algorithm, giving rise to an approximate variational variable elimination algorithm. Such simpliﬁcations can be achieved in several ways; we describe two. 

# 11.5.3.1 Variational Bounds 

Consider, for example, the diamond network of ﬁgure 11.18a. Assume that we run variable elimination to sum out the variable $B$ , which we assume for convenience is binary-valued. The elimination of $B$ introduces a new factor: 

$$
\phi_{B}(A,C)=\sum_{b}\phi_{1}(A,b)\phi_{2}(b,C)
$$ 

Coupling $A$ and $C$ in a single factor may be expensive, for example, if $A$ and $C$ have many values. In more complex networks, this type of coupling can induce complexity if an elimination step couples a larger set of variables, or if the local coupling leads to additional cost later in the computation, when we eliminate $A$ or $C$ . 

As we now show, we can use a variational bound to avoid this coupling. Consider the following bound: 

Proposition 11.8 

$$
\ln(1+e^{x})\geq\lambda x+H(\lambda),
$$ 

$$
1+e^{x}\geq e^{\lambda x+H(\lambda)}.
$$ 

Why is this useful? Using some algebraic manipulation, we can bound each of the entries in our newly generated factor: 

$$
\begin{array}{r c l}{{\phi_{B}(a,c)}}&{{=}}&{{\phi_{1}(b^{0},a)\phi_{2}(b^{0},c)+\phi_{1}(b^{1},a)\phi_{2}(b^{1},c)}}\\ {{}}&{{=}}&{{\phi_{1}(b^{0},a)\phi_{2}(b^{0},c)\left[1+\exp\left\{\ln\frac{\phi_{1}(b^{1},a)\phi_{2}(b^{1},c)}{\phi_{1}(b^{0},a)\phi_{2}(b^{0},c)}\right\}\right]}}\\ {{}}&{{\geq}}&{{\phi_{1}(b^{0},a)\phi_{2}(b^{0},c)\exp\left\{\lambda_{a,c}\ln\frac{\phi_{1}(b^{1},a)\phi_{2}(b^{1},c)}{\phi_{1}(b^{0},a)\phi_{2}(b^{0},c)}+H(\lambda_{a,c})\right\}}}\\ {{}}&{{=}}&{{\left(\phi_{1}(b^{0},a)^{1-\lambda_{a,c}}\phi_{1}(b^{1},a)^{\lambda_{a,c}}\right)\cdot}}\\ {{}}&{{}}&{{\left(\phi_{2}(b^{0},c)^{1-\lambda_{a,c}}\phi_{2}(b^{1},c)^{\lambda_{a,c}}\right)\cdot e^{H(\lambda_{a,c})}.}}\end{array}
$$ 

Thus, we can replace a factor that couples $A$ and $C$ by a product of three terms: an expression involving only factors of $A$ , an expression involving only factors of $C$ , and the ﬁnal factor $e^{H(\lambda_{a,c})}$ . However, all three terms also involve the variational parameter $\lambda_{a,c}$ and therefore also depend on both $A$ and $C$ . At this point, it is unclear what we gain from the transformation. 

However, we can choose the same $\lambda$ for all joint assignments to $A,C$ . In doing so, we replace four variational parameters by a single parameter $\lambda$ . This operation relaxes the bound, which is no longer tight. On the other hand, it also decouples $A$ and $C$ , leading to a product of terms none of which depends on both variables: 

$$
\phi_{1}(b^{0},a)^{1-\lambda}\phi_{1}(b^{1},a)^{\lambda}\bigr)\cdot\bigl(\phi_{2}(b^{0},c)^{1-\lambda}\phi_{2}(b^{1},c)^{\lambda}\bigr)\cdot e^{\pmb{H}(\lambda)}=\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{2}(c,\lambda)e^{\pmb{H}(\lambda)}
$$ 

Thus, if we use this approximation, we have efectively eliminated $B$ without coupling $A$ and $C$ . As we saw in chapter 9, this type of simpliﬁcation can circumvent the need for coupling yet more variables in later stages in variable elimination, potentially leading to signiﬁcant savings. 

It is interesting to observe how $\lambda$ decouples the two factors. Each factor is replaced by a geometric average over the values of $B$ . The variational parameter speciﬁes the weight we assign to each of the two cases. Note that the original bound in equation (11.66) is tight; thus, if we pick the “right” variational parameter $\lambda_{a,c}$ for each assignment $a,c,$ we reproduce the correct factor $\phi_{B}(A,C)$ . However, these variational parameters are generally diferent for each assignment $a,c,$ and hence, a single variational parameter cannot optimize all of the terms. Our choice of $\lambda$ efectively determines the quality of our approximation for each of the terms $\phi_{B}(a,c)$ . Thus, the overall quality of our approximation for a particular choice of $\lambda$ depends on the importance of these diferent terms in the variable elimination computation as a whole. 

Other variational approximations exploit speciﬁc parametric forms of CPDs in the network. For example, consider networks with sigmoid CPDs (see section 5.4.2). Recall that a logistic CPD $P(X\mid U)$ has the parametric form: 

$$
P(x^{1}\mid\mathbf{u})=\mathrm{sigmoid}(\sum_{i}w_{i}u_{i}+w_{0}),
$$ 

where $\begin{array}{r}{\mathrm{sigmoid}(x)=\frac{1}{1+e^{-x}}}\end{array}$ . The observation of $X$ couples the parents $U$ . Can we decouple these parents using an approximation? Using proposition 11.8 we can ﬁnd an upper bound: 

$$
\ln\operatorname{sigmoid}(\sum_{i}w_{i}u_{i}+w_{0})\leq\lambda\left(\sum_{i}w_{i}u_{i}+w_{0}\right)-H(\lambda).
$$ 

Similarly to our earlier example, such an approximation allows us to replace a factor over several variables by a product of smaller factors. In this case, all the parents of $X$ are decoupled by the approximate form. 

# 11.5.3.2 Variational Variable Elimination 

How do we use this approximation in the course of inference? Note that equation (11.67) provides a lower bound to $\phi_{B}(a,c)$ for every value of $a,c$ . Assume that, in the course of running variable elimination, rather than generating $\phi_{B}(A,C)$ , we introduce the expression in equation (11.67) and continue the variable elimination process with these decoupled factors. From a graph- theoretic perspective, the result of this approximation when applied to a variable $B$ is analogous to the efect of conditioning on $B$ , as described in section 9.5.3: it removes from the graph $B$ and all its adjacent edges. However, unlike conditioning, we do not enumerate and perform inference for all values of $B$ (of course, at the cost of obtaining an approximate result). 

variational variable elimination 

More generally, in an execution of variable elimination, there may be some set of elimination steps that create large factors that couple many variables. This variational approximation can allow us to avoid this coupling. Like conditioning (see section 9.5.4.1), we can perform such an approximation step not only at the very beginning, but also in a way that is interleaved with variable elimination, allowing us to reuse computation. This class of algorithms is called variational variable elimination . 

What is the result of this approximation? Each of the entries in the approximated factor is replaced with a lower bound; thus, each entry in every subsequent factor produced by the algorithm is also a lower bound to the original entry. If we proceed to eliminate all variables, either exactly or using additional variational approximation steps for other intermediate factors, the outcome of this process is a lower bound to the partition function. If we do not eliminate all of the variables, the result is an approximate factor in which every entry is a lower bound to the original. Of course, once we renormalize the factor to produce a distribution, we can make no guarantees about the direction of the approximation for any given entry. Nevertheless, the resulting factor might be a reasonable approximation to the original. 

The quality of our approximation depends on the choice of variational parameters introduced during the course of the variable elimination algorithm. How do we select them? One approach is simply to select the variational parameter at each step to optimize the quality of our ap- proximation at that step. However, this high-level goal is not fully deﬁned. For example, we can choose $\lambda$ so as to make $\tilde{\phi}_{1}(a^{1},\lambda)\tilde{\phi}_{2}(b^{1},\lambda)e^{H(\lambda)}$ as close as possible to $\phi_{B}(a^{1},c^{1})$ ; or, we can focus on $a^{1},c^{0}$ . The decision of where to focus our “approximation efort” depends on the impact of these components of the factor on the ﬁnal outcome of the computation. Thus, a more correct approach is to identify the actual expression that we are trying to estimate — for example, the partition function — and to try to maximize our bound to that expression. In our simple example, we can write down the partition function as a function of the variational parameter $\lambda$ introduced when eliminating $B$ : 

$$
\tilde{Z}(\lambda)=e^{H(\lambda)}\sum_{a}\sum_{c}\phi_{3}(a,d)\phi_{4}(c,d)\sum_{d}\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{2}(c,\lambda).
$$ 

This expression is a function of $\lambda$ ; we can then try to identify the best bound by maximizing $\operatorname*{max}_{\lambda}\hat{\tilde{Z}}(\lambda)$ , say, using gradient ascent or another numerical optimization method. 

However, as we discussed, in most cases we would use several approximate elimination steps within the variable elimination algorithm. In our example, the elimination of $D$ also couples $A$ and $C$ , a situation we may wish to avoid. Thus, we could apply the same type of variational bound to the internal summation: 

$$
\phi_{D}(A,C)=\sum_{d}\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{2}(c,\lambda),
$$ 

giving rise to the bound $\tilde{\phi}_{3}(a,\lambda^{\prime})\tilde{\phi}_{4}(c,\lambda^{\prime})e^{H(\lambda^{\prime})}$ . The resulting approximate partition function now has the form 

$$
\tilde{Z}(\lambda,\lambda^{\prime})=e^{H(\lambda)}e^{H(\lambda^{\prime})}\sum_{a}\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{3}(a,\lambda^{\prime})\sum_{c}\tilde{\phi}_{2}(c,\lambda)\tilde{\phi}_{4}(c,\lambda^{\prime}).
$$ 

We can now maximize $\mathrm{max}_{\lambda,\lambda^{\prime}}\,\tilde{Z}(\lambda,\lambda^{\prime})$ ; the higher the value we ﬁnd, the better our approxi- mation to the true partition function. 

In general, our approximate partition function will be a function $\tilde{Z}(\lambda)$ , where $\lambda$ is the vector of all variational parameters $\lambda$ produced during the diferent approximation steps in the algorithm. One approach is to reformulate the variable elimination algorithm so that it produces factors that are not purely numerical, but rather symbolic expressions in the variables $\lambda$ ; see exercise 11.32. Given the multivariate function $\tilde{Z}(\lambda)$ , we can optimize it numerically to produce the best possible bound. A similar principle applies when we use the variational bound for sigmoid CPDs; see exercise 11.36 for an example. While we only sketch the basic idea here, this approach can be used as the basis for an algorithm that interleaves variational approximation steps with exact elimination steps to form a variational variable elimination algorithm. 

# 11.6 Summary and Discussion 

In this chapter, we have described a general class of methods for performing approximate inference in a distribution $P_{\Phi}$ deﬁned by a graphical model. These methods all attempt to construct a representation $Q$ within some approximation class $\mathcal{Q}$ that best approximates $P_{\Phi}$ . The key issue that must be tackled in this task is the construction of such an approximation without performing inference on the (intractable) $P_{\Phi}$ . The methods that we described all take a similar approach of using the I-projection framework, whereby we minimize the KL- divergence $D(Q\|P_{\Phi})$ | | ; these methods all reformulate this problem as one of maximizing the energy functional. 

The diferent methods that we described all follow a similar template. We optimize the free energy, or an approximation thereof, over a class of representations $\mathcal{Q}$ . We provided an optimization-based view for four diferent methods, each of which makes a particular choice regarding the objective and the constraints. We now recap these choices and their repercussions. 

Clique tree calibration optimizes the factored energy functional, which is exact for clique trees. The optimization is performed over the space of calibrated clique potentials. For clique trees, any set of calibrated clique potentials must arise from a real distribution, and so this space is precisely the marginal polytope. As a consequence, both our objective and our constraint space are exact, so our solution represents the exact posterior. 

Cluster-graph (loopy) belief propagation optimizes the factored energy functional, which is approximate for loopy graphs. The optimization is performed over the space of locally consistent pseudo-marginals, which is a relaxation of the marginal polytope. Thus, both the objective and the constraint space are approximate. 

Expectation propagation over clique trees optimizes the factored energy functional, which is the exact objective in this case. However, the constraints deﬁne a space of pseudo-marginals that are not even entirely consistent — only their moments are required to match. Thus, the constraint space here is a relaxation of our constraints, even when the structure is a tree. Expectation propagation over cluster graphs adds, on top of these, all of the approximations induced by belief propagation. 

Finally, structured variational methods optimize the exact factored energy functional, for a class of distributions that is (generally) less expressive than necessary to encode $P_{\Phi}$ . As a consequence, the constraint space is actually a tightening of our original constraint space. Because the objective function is exact, the result of this optimization provides a lower bound on the value of the exact optimization problem. As we will see, such bounds can play an important role in the context of learning. 

In the approaches we discussed, the optimization method was based on the method of Lagrange multipliers. This derivation gave rise to a series of ﬁxed-point equations for the solution, where one variable is deﬁned in terms of the others. As we showed, an iterative solution for these ﬁxed-point equations gives rise to a suite of message passing algorithms that generalize the clique-tree message passing algorithms of chapter 10. 

This general framework opens the door to the development of many other approaches. Each of the methods that we described here involves a design choice in three dimensions: the objective function that we aim to optimize, the space of (pseudo-)distributions over which we perform our optimization, and the algorithm that we choose to use in order to perform the optimization. Although these three decisions afect each other, these dimensions are sufciently independent that we can improve each one separately. Some recent work takes exactly this approach. For example, we have already seen some work that focuses on better approximations to the energy functional; other work (see section 11.7) focuses on identifying constraints over the space of pseudo-marginals that make it a tighter relaxation to the (exact) marginal polytope; yet other work aims to ﬁnd better (for example, more convergent) algorithms for a general class of optimization problems. 

Many of these improvements aimed to address a fundamental problem arising in these al- gorithms: the possible lack of convergence to a stable solution. The issue of convergence is one on which some progress has been made. Some recently developed methods have better convergence properties in practice, and others are even guaranteed to converge. There are also theoretical analyses that can help determine when the algorithms converge. 

A second key question, and one on which relatively little progress has been made, is the quality of the approximation. There is very little work that provides guarantees on the error made in the answers (for example, individual marginals) by any of these methods. As a consequence, there is almost no work that provides help in choosing a low-error approximation for a particular problem. Thus, the problem of applying these methods in practice is still a combination of manual tuning on one hand, and luck on the other. The development of automated methods that can help guide the selection of an approximating class for a particular problem is an important direction for future work. 

# 11.7 Relevant Literature 

Variational approximation methods are applicable in a wide variety of settings, including quan- tum mechanics (Sakurai 1985), statistical mechanics (Parisi 1988), neural networks (Elfadel 1995), and statistics (Rustagi 1976). The literature on the use of these approximation methods for inference problems in graphical models is heavily inﬂuenced by ideas in statistical mechanics, although the connections are beyond the scope of this book. 

The rules for belief propagation were developed in Pearl’s (1986b; 1988) analysis of inference in singly connected networks. In his book, Pearl noted that these propagation rules lead to wrong answers in multiconnected networks. Interest in “loopy” inference on pairwise networks was raised in several publications (Frey 1998; MacKay and Neal 1996; Weiss 1996) that reported good empirical results when running Pearl’s algorithm on networks with loops. The most impressive success was in the error-correcting code scheme known as turbocodes (Berrou et al. 1993; McEliece et al. 1995). These decoding algorithms were shown to be equivalent to belief propagation in a network with loops (Kschischang and Frey 1998; McEliece et al. 1998; Frey and MacKay 1997). Researchers using these ad hoc algorithms reported that although they did not compute the correct posteriors, they often did compute the correct MAP assignment. 

These empirical successes led to examination of both the reasons for the success of such methods and evaluated their performance in other domains. Weiss (2000) showed that when a network has a single loop, an analytic relationship exists between the belief computed and the true marginals, and it used that relationship to characterize network topologies for which the MAP solution is provably correct. Weiss and Freeman (2001a) then showed that if loopy belief propagation converges in a linear Gaussian network, then it computes correct posterior means; see section 14.7 for more references on results for the Gaussian case. These initial analytic results were supplemented by promising empirical results (Murphy et al. 1999; Horn and McEliece 1997; Weiss 2001). 

Several alternative formulations of loopy belief propagation appeared in the literature. These include factor graphs (Kschischang et al. 2001b), tree-based re parameter iz ation (Wainwright et al. 2003a), and algebraic formulations (Aji and McEliece 2000). In parallel, several authors (Yedidia et al. 2000; Dechter et al. 2002) proposed extending the idea of belief propagation to more generalized cluster graphs (or region graphs). Welling (2004) examined methods for automatically choosing the regions for this approximation. 

A major advance in our understanding of this general class of approximation algorithms came with the analysis of Yedidia et al. (2000, 2005), showing that these methods can be posed as attempting to maximize an approximate energy functional. This result connected the algo- rithmic developments in the ﬁeld with literature on free-energy approximations developed in statistical mechanics (Bethe 1935; Kikuchi 1951). This insight is the basis for our discussion of the Bethe energy functional in section 11.3.6. This result also provided a connection between belief propagation algorithms and other approximation procedures, such as structured varia- tional methods. In addition, it led to the development of new types of approximate inference procedures. These include direct optimization of the Bethe energy functional (Welling and Teh 2001) using gradient-based methods, as well as provably convergent “double loop” variants of belief propagation (Yuille 2002; Heskes et al. 2003). Another class of algorithms combined exact inference on subtrees of the cluster graph within cluster-graph belief propagation (Wainwright et al. 2001, 2002a). This combination leads to faster convergence and introduces new directions for characterizing the errors of the approximation. Wainwright and Jordan (2003) review the connections between belief propagation and optimization of the energy functional. 

An active area of research is improving and analyzing the convergence properties of gener- alized belief propagation algorithms. In practice, the techniques most important for improving convergence are damping (Murphy et al. 1999; Heskes 2002) and message scheduling algorithms, such as the tree-based re parameter iz ation algorithm of (Wainwright et al. 2003a) or the residual belief propagation algorithm of Elidan et al. (2006). On the theoretical side, key directions include the identiﬁcation of a set of sufcient conditions for the existence of unique local maxima of the Bethe energy functional (Pakzad and Anantharam 2002; Heskes 2002), as well as conditions that ensure a unique convergence point of belief propagation (Tatikonda and Jordan 2002; Ihler et al. 2003; Heskes 2004; Ihler et al. 2005; Mooij and Kappen 2007). Our discussion in section 11.3.4 is based on that of Mooij and Kappen (2007). A related trajectory attempts to estimate the error of the approximate marginal probabilities; see, for example, Leisink and Kappen (2003); Ihler (2007). 

A diferent generalizatiohn of belief-propagation algorithms develops variants of the energy functional that have desired properties. For example, if the energy functional is convex, then it is guaranteed to have a single maximum. An initial convexiﬁed free-energy functional was introduced in Wainwright et al. (2002b). This functional has the additional property of providing an upper bound on the partition function. Recently, there has been signiﬁcant work that provides a more detailed characterization of convex energy functionals (Wainwright and Jordan 2003; Heskes 2006; Wainwright and Jordan 2004; Sontag and Jaakkola 2007). Although the convexity of energy functional implies a unique maximum, it does not generally guarantee that a message passing algorithm will converge. Recent alternative algorithms provide guaranteed convergence for such energy functionals (Heskes 2006; Globerson and Jaakkola 2007a; Hazan and Shashua 2008). 

A recent extension is the combination of belief propagation with particle-based methods. The basic idea is to use particle sampling to perform the basic belief propagation steps (Sudderth et al. 2003; Isard 2003). This combination allows us to use cluster-graph belief propagation on networks with continuous non-Gaussian variables, which appear in applications in vision and signal processing; see also section 14.7. 

The idea of factored messages ﬁrst appeared in several contexts (Dechter and Rish 1997; Dechter 1997; Boyen and Koller 1998b; Murphy and Weiss 2001). Similar ideas that involve projection of messages onto “simple” representations (such as Gaussians) are common in the control and tracking literature (Bar-Shalom 1992). The generalization of these ideas in the form of expectation propagation was introduced by Minka (2001b). The connection between expectation propagation and the Bethe energy functional was explored by (Minka 2001b; Heskes and Zoeter 2002; Heskes et al. 2005). The connection between expectation propagation and generalized belief propagation in the case of discrete variables is explored by Welling et al. (2005). 

One of the early applications of variational methods to probabilistic methods was the use of mean ﬁeld approximation for Boltzmann machines (Peterson and Anderson 1987), an approach then widely adopted in computer vision (see, for example, Li (2001)). This methodology was introduced into the ﬁeld of directed graphical models by Saul et al. (1996), following ideas that appeared in the context of Helmholtz machines (Hinton et al. 1995). 

The mean ﬁeld approximation cannot capture strong dependencies in the posterior distribu- tion. Saul and Jordan (1996) suggested to circumvent this problem by using structured varia- tional methods. These ideas were extended for diferent forms of approximating distributions and target networks. Ghahramani and Jordan (1997) used independent hidden Markov chains to approximate factorial HMMs, a speciﬁc form of a dynamic Bayesian network. Barber and Wiegerinck (1998) use a Boltzmann machine approximation. Wiegerinck (2000) uses Markov net- works and cluster trees as approximate distribution. Xing et al. (2003) describe cluster mean ﬁeld and suggest efcient implementations. Geiger et al. (2006) further extend Wiegerinck’s proce- dure and introduce efcient propagation schemes to maximize parameters in the approximating distribution. 

The idea of using a mixture of mean ﬁeld approximation was developed by Jaakkola and Jordan (1998). These ideas were extended to general networks with auxiliary variables by El-Hay and Friedman (2001). 

Jaakkola and Jordan (1996b,a) introduced local variational approximations to compute both upper bounds and lower bounds of the log-likelihood function (that is, of $\log Z)$ ). They demon- strated these methods by a large scale study of inference in the QMR-DT network (Jaakkola and Jordan 1999), where they show that variational methods are more efective than particle based methods. Additional extension on these methods appeared in $\mathrm{Mg}$ and Jordan (2000). 

Tutorials discussing the use of structured and local methods appear in Jordan et al. (1998); Jaakkola (2001). 

# 11.8 Exercises 

# Exercise 11.1 

entropy Show that the derivative of the entropy is: 

$$
\frac{\partial}{\partial Q({\pmb x})}H_{Q}({\pmb X})=-\ln Q({\pmb x})-1.
$$ 

# Exercise ${\bf11.2\star}$ 

local consistency polytope 

marginal polytope 

Consider the set of locally consistent distributions, as in the local consistency polytope of equation (11.16). 

a. For a cluster graph $\mathcal{U}$ that is a clique tree $\mathcal{T}$ (satisfying the running intersection property), show that the set of distributions satisfying these constraints is precisely the marginal polytope — the set of legal distributions $Q$ that can be represented over $\mathcal{T}$ . b. Show that, for a cluster graph that is not a tree, there are parameter iz at ions that satisfy these constraints but are not marginals of any legal probability distribution. 

# Exercise ${\bf1.3\star}$ 

In the text, we showed that CTree-Optimize has a unique global optimum. Show that it also has a unique ﬁxed point. 

# Exercise $11.4\star$ 

Consider the network of ﬁgure 11.1c. We have shown that 

$$
P_{\mathcal{T}}(A,B,C,D)\propto P_{\Phi}(A,B,C,D)\frac{\mu_{3,4}[D]\mu_{1,4}[A]}{\beta_{4}(A,D)},
$$ 

cluster graph residual 

where $\mathcal{T}$ is the cluster tree we get if we remove $C_{4}\,=\,\{A,D\}$ . Show how to use this result on the residual to bound the error in the estimation of marginals in this cluster graph. 

# Exercise $11.5\star$ 

Prove proposition 11.2. 

# Exercise 11.6 

Compare the cluster graphs in ﬁgure 11.3 and ﬁgure 11.7. 

a. Show that $\mathcal{U}_{2}$ and $\mathcal{U}_{3}$ are equivalent in the following sense: Any set of calibrated potentials of one can be transformed to a calibrated set of potentials of the other. b. Show similar equivalence between $\mathcal{U}_{1}$ and $\mathcal{U}_{3}$ . 

# Exercise 11.7 

image segmentation As we discussed in box 4.B, Markov networks are commonly used for image-processing tasks. We now consider the application of Markov networks to foreground-background image segmentation . Here, we do not have a predetermined set of classes, each with its own model. Rather, for each pixel, we have a binary-valued variable $X_{i}$ , where $x_{i}^{1}$ means that $X_{i}$ is a foreground pixel, and $x_{i}^{0}$ a background pixel. In this case, we have no node potentials (say that features of an individual pixel cannot help distinguish foreground from background). The network structure is a grid, where for each pair of adjacent pixels $i,j$ , we have an edge potential $\phi_{i,j}\bigl(X_{i},X_{j}\bigr)=\alpha_{i,j}$ if $X_{i}=X_{j}$ and 1 otherwise. A large value of $\alpha_{i,j}$ makes it more likely that $X_{i}=X_{j}$ (that is, pixels $i$ and $j$ are assigned to the same segment), and a small value makes it less likely. 

Using the standard Bethe cluster graph, compute the ﬁrst message sent by loopy belief propagation from any variable to any one of its neighbors. What is wrong with this approach? 

# Exercise 11.8 

Let $X$ be a node with par ts $U\,=\,\{U_{1},.\,.\,.\,,U_{k}\}$ , where $P(X\mid U)$ is a tree-CPD. Assume that we have a cluster consisting of X and U . In this question, you will show how to exploit the structure of the tree-CPD to perform message passing more efciently. 

a. Consider a step where our cluster gets incoming messages about $U_{1},\ldots,U_{k}$ , and sends a message about $X$ . Show how this step can be executed in time linear in the size of the tree-CPD of $P(X\mid U)$ . b. Now, consider the step where our clique gets incoming messages about $U_{1},\dots,U_{k-1}$ and $X$ , and send a message about $U_{k}$ . Show how this step can also be executed in time linear in the size of the tree-CPD. 

# Exercise 11.9 

Recall that a pairwise Markov network consists of univariate potential $\phi_{i}\big[X_{i}\big]$ over each variable $X_{i}$ , and in addition a pairwise potential $\phi_{(i,j)}[X_{i},X_{j}]$ over some pairs of variables. In section 11.3.5.1 we showed a simple transformation of such a network to a cluster graph where we introduce a cluster for each potential. Write the update equations for cluster-graph belief propagation for such a network. Show that we can formulate inference as the direct propagation of messages between variables by implicitly combining messages through pairwise potentials. 

# Exercise $1.10\star$ 

Suppose we are given a set of factors $\Phi=\{\phi_{1},.\,.\,.\,,\phi_{K}\}$ over $X=\{X_{1},.\,.\,.\,,X_{n}\}$ . O these fa rs into a pairwise Markov netw troducing new auxiliary variables $Y=\{Y_{1},.\,.\,.\,,Y_{k}\}$ { } so that $Y_{j}$ denotes a joint assignment to $S c o p e[\phi_{j}]$ . Show how to construct a set of factors Φ $\Phi^{\prime}$ that is a pairwise Markov network over $\mathbf{\bar{X}}\cup\mathbf{Y}$ such that $\tilde{P}_{\Phi^{\prime}}({\pmb x})=P_{\Phi}({\pmb x})$ for each assignment to $_{X}$ . 

# Exercise $11.11\star$ 

BN cluster graph In this question, we study how we can deﬁne a cluster graph for a Bayesian network 

a. onsider the following two schemes for converting a Bayesian network structure $\mathcal{G}$ to a cluster graph $\mathcal{U}$ U . For each of these two schemes, either show (by proving the necessary properties) that it produces a valid cluster graph for a general Bayesian network, or disprove this result by showing a counterexample. • eme 1: For ea no $X_{i}$ in $\mathcal{G}$ , deﬁne a cluster $C_{i}$ over $X_{i}$ ’s family. Connect $C_{i}$ and $C_{j}$ if $X_{j}$ is a parent of $X_{i}$ i in , and deﬁne the sepset to be the intersection of the clusters. 

• eme 2: For e no $X_{i}$ in $\mathcal{G}$ , deﬁne a cluster $C_{i}$ ov $X_{i}$ ’s family. Connect $C_{i}$ and $C_{j}$ if $X_{j}$ is a parent of $X_{i}$ in G , and deﬁne the sepset to be the { $\{X_{j}\}$ } . 

b. Construct an alternative scheme to the ones proposed earlier that uses a minimum spanning tree algorithm to transform any Bayesian network into a valid cluster graph. 

# Exercise $11.12\star$ 

Suppose we want to use a gradient method to directly maximize $\tilde{F}[\tilde{P}_{\Phi},Q]$ with respect to entries in $Q$ . For simplicity, assume that we are dealing with a pairwise network for both $P_{\Phi}$ and $\bar{Q}$ , and so the entries in $Q$ are all univariate and pairwise potentials. 

a. Derive $\frac{\partial\tilde{F}[\tilde{P}_{\Phi},Q]}{\partial Q(x_{i})}$ and $\frac{\partial\tilde{F}[\tilde{P}_{\Phi},Q]}{\partial Q(x_{i},y_{i})}$ for the two types of potentials we have. 

b. Recall that we cannot simply choose $Q$ to maximize $\tilde{F}[\tilde{P}_{\Phi},Q]$ . We need to ensure that every potential in $Q$ is nonnegative and sums to 1 . In addition, we need to maintain the marginal consistency between each pairwise potential and the associated univariate potential marginals in $Q$ . A standard solution is to write $Q$ as a function of meta parameters $\eta,$ , so that the transformation from $\eta$ to $Q$ ensures the consistency. Suggest such a re parameter iz ation, and derive the gradient of $\tilde{F}[\tilde{P}_{\Phi},Q]$ with respect to this re parameter iz ation. (Hint: use the chain law of partial derivatives.) 

# Exercise $11.13\star$ 

Prove that if the damped updates of equation (11.14) converge, then they converge to a stationary point of $\tilde{F}[\tilde{P}_{\Phi},Q]$ . 

# Exercise $11.14\star$ 

In this exercise you will prove theorem 11.7. 

a. Start by deﬁning the Lagrangian 

$$
\begin{array}{r c l}{\mathcal{I}}&{=}&{\tilde{F}[\tilde{P}_{\Phi},Q]}\\ &&{-\displaystyle\sum_{i\in\mathcal{V}_{\mathcal{R}}}\lambda_{r}\left(\displaystyle\sum_{\pmb{c}_{r}}\beta_{r}(\pmb{c}_{r})-1\right)}\\ &&{-\displaystyle\sum_{(r^{\prime}\to r)\in\mathcal{E}_{\mathcal{R}}}\displaystyle\sum_{\pmb{c}_{r}}\lambda_{r^{\prime}\to r}[\pmb{c}_{r}]\left(\displaystyle\sum_{\pmb{c}_{r^{\prime}}\sim\pmb{c}_{r}}\beta_{r^{\prime}}(\pmb{c}_{r^{\prime}})-\beta_{r}(\pmb{c}_{r})\right),}\end{array}
$$ 

and show that any maximum point satisﬁes 

$$
\frac{\partial}{\partial\beta_{r}(\pmb{c}_{r})}\mathcal{I}=\kappa_{r}\ln\psi_{r}(\pmb{c}_{r})-\kappa_{r}\ln\beta_{r}(\pmb{c}_{r})-\kappa_{r}-\lambda_{r}-\sum_{r^{\prime}\in\mathbf{U}\mathbf{p}(r)}\lambda_{r^{\prime}\to r}[\pmb{c}_{r}]+\sum_{r^{\prime}\in\mathbf{D o w n}(r)}\psi_{r^{\prime}}(\pmb{c}_{r})-\psi_{r}(\pmb{c}_{r}),
$$ 

where $\kappa_{r}$ is the counting number of the region, as deﬁned in equation (11.20). 

b. Deﬁne $\delta_{r_{u}\rightarrow r_{d}}$ in terms of the Lagrange multipliers and show that your solution satisﬁes equation (11.37) and equation (11.36). 

# Exercise 11.15 

Consider n $n\times n$ grid network. ximation for this network that would have a region r for each small square $A_{i,j},A_{i,j+1},A_{i+1,j},A_{i+1,j+1}$ . Describe the structure of the graph and the computation of the messages and beliefs. 

# Exercise 11.16 

Prove theorem 11.6. 

# Exercise $11.17\star$ 

In this exercise, we will derive a message passing algorithm, called the child-parent algorithm , for Bethe region graphs. Although the messages in this algorithm are somewhat convoluted, they have the advantage of corresponding directly to the Lagrange multipliers for the region graph energy functional. Moreover, although somewhat opaque, the message passing algorithm is a very slight variation on the scheme used in standard belief propagation: the standard messages are simply raised to a power. 

Consider the Bethe cluster graph of example 11.2, where we assume that all counting numbers are nonzero. Let $\rho_{i}\,=\,{1}/{\kappa_{i}}$ and $\rho_{r}\,=\,1/\kappa_{r}$ . Starting from equation (11.34), derive the correctness of the following update rules for the messages and the beliefs. For all $r\in\mathbf{R}^{+}$ : 

$$
\beta_{r}(\boldsymbol{C}_{r})\gets\left(\tilde{\psi}_{r}(\boldsymbol{C}_{r})\prod_{X_{i}\in\boldsymbol{C}_{r}}\delta_{i\to r}(X_{i})\right)^{\rho_{r}}.
$$ 

$$
\delta_{i\rightarrow r}(X_{i})\leftarrow\left[\left(\prod_{r^{\prime}\rightarrow r}\delta_{i\rightarrow r^{\prime}}(X_{i})\right)^{\rho_{i}}\left(\sum_{C_{r}=X_{i}}\psi_{r}(C_{r})\left(\prod_{X_{j}\in C_{r},j\neq i}\delta_{j\rightarrow r}\right)^{\rho_{r}}\right)\right]^{-\frac{1}{\rho_{i}+\rho_{r}}}.
$$ 

# Exercise ${\bf118\star}$ 

Show that the counting numbers deﬁned by equation (11.26) are convex. (Hint: Show ﬁrst the convexity of counting numbers obtained from this analysis for a tree-structured MRF.) 

# Exercise ${\bf119\star}$ 

In this exercise, we will derive another message passing algorithm, called the two-way algorithm , for ﬁnding ﬁxed points of region-based energy functionals; this algorithm allows for more general region graphs than in exercise 11.17. It uses two messages along each link ${\dot{r}}\to r^{\prime}$ : one from $r$ to $r^{\prime}$ and one from $\mathit{\Pi}_{r^{\prime}}$ to $r$ . 

Consider a region-based free energy as in eq ation (11.27). For any region $r$ , let $p_{r}\,=\,|\mathbf{U}\mathbf{p}(r)|$ be the number of regions that are directly upward of r . Assume that for any top region (so that $p_{r}=0$ ), we have that $\kappa_{r}=1$ . We now deﬁne $q_{r}\doteq(\dot{1}-\kappa_{r})/p_{r e g i o n}$ , taking $q_{r}=1$ $p_{r}=0$ (so that $\kappa_{r}=1$ , as per our assumption). Assume furthermore that $q_{r}\neq2$ , and deﬁne $\beta_{r}=1/(2-q_{r})$ − . 

The following equalities deﬁne the messages and the potentials in this algorithm: 

$$
\begin{array}{l l l l}{\psi_{r}(C_{r})}&{=}&{\displaystyle(\psi_{r}(C_{r}))^{\kappa_{r}}\prod_{r^{\prime\prime}\in{\bf U}_{P}(r)\to\{r^{\prime}\}}\tilde{\delta}_{r^{\prime\prime}\to r}^{d o m}(C_{r})\prod_{r^{\prime\prime}\in\mathrm{Damv}(r)}\tilde{\delta}_{r^{\prime\prime}\to r}^{u p}(C_{r^{\prime\prime}})}\\ {\tilde{\delta}_{r^{\prime\prime}\to r^{\prime}}^{u p}}&{=}&{\displaystyle\tilde{\psi}_{r}(C_{r})\prod_{r^{\prime\prime}\in{\bf U}_{P}(r)\to\{r^{\prime}\}}\tilde{\delta}_{r^{\prime\prime}\to r}^{d o m}(C_{r})\prod_{r^{\prime\prime}\in\mathrm{Damv}(r)}\tilde{\delta}_{r^{\prime\prime}\to r}^{u p}(C_{r^{\prime\prime}})}\\ &{\displaystyle(\psi_{r}(C_{r}))^{\kappa_{r}}}&\\ {\tilde{\delta}_{r^{\prime\prime}\to r^{\prime}}^{d o m}}&{=}&{\displaystyle\sum_{c_{r^{\prime\prime}}\to c_{r^{\prime\prime}}}\tilde{\psi}_{r}(C_{r})\prod_{r^{\prime\prime}\in{\bf U}_{P}(r)}\tilde{\delta}_{r^{\prime\prime}\to r}^{u p}(C_{r})\prod_{r^{\prime\prime}\in\mathrm{Damv}(r)\to\{r^{\prime\prime}\}}\tilde{\delta}_{r^{\prime\prime}\to r}^{u p}(C_{r^{\prime\prime}})}\\ &{\overset{C o p}{\to}^{r_{s^{\prime\prime}}}}&{=}&{\displaystyle(\tilde{\delta}_{r_{s^{\prime\prime}}\to r^{\prime}}^{u p}(C_{r}))^{\beta_{r}}\tilde{(\delta}_{r^{\prime\prime}\to r^{\prime\prime}}^{u p}(C_{r}))^{\beta_{r}-1}}\\ {\tilde{\delta}_{r^{\prime\prime}\to r^{\prime}}^{u p}}&{=}&{\displaystyle(\tilde{\delta}_{r_{s^{\prime\prime}}\to r^{\prime}}^{u p}(C_{r}))^{\beta_{s^{\prime}}-1}\tilde{(\delta}_{r^{\prime\prime}\to r^{\prime}}^{u p}(C_{r}))^{\beta_{r}}}\\ {\beta_{r}(C_{r})}&{=}&{\displaystyle\tilde{\psi}_{r}(C_{r})\prod_{r^{\prime\prime}\in{\bf U}_{P}(r)}\delta_{r^{\
$$ 

Note that the messages $\tilde{\delta}_{r\rightarrow r^{\prime}}^{u p}$ (or $\widetilde{\delta}_{r\rightarrow r^{\prime}}^{d o w n}$ ) are as we would expect: the message sent from $r$ to $r^{\prime}$ is simply the product of $r$ ’s initial potential with all of its incoming messages except the one from → → $r^{\prime}$ (and similarly for the message sent from $\hat{r^{\prime}}$ to $r$ ). However, as we discussed in our derivation of the region graph algorithm, this computation will double-count the information that arrived at $r$ from $r^{\prime}$ via an indirect path. The ﬁnal computation of the messages $\delta_{r\rightarrow r^{\prime}}^{u p}$ and $\delta_{r\rightarrow r^{\prime}}^{d o w n}$ is intended to correct for that double-counting. 

In this exercise, you will show that the ﬁxed points of equation (11.33) are precisely the same as the ﬁxed points of the update equations equation (11.71)–equation (11.76). 

a. We begin by deﬁning the messages in terms of the beliefs and the Lagrange multipliers: 

$$
\begin{array}{r c l}{\tilde{\delta}_{r\rightarrow r^{\prime}}^{u p}({\pmb c}_{r})}&{=}&{\exp\{\lambda_{r,r^{\prime}}({\pmb c}_{r})\}}\\ {\tilde{\delta}_{r\rightarrow r^{\prime}}^{d o w n}({\pmb c}_{r^{\prime}})}&{=}&{\beta_{r^{\prime}}({\pmb c}_{r^{\prime}})^{q_{r^{\prime}}}\exp\{\lambda_{r,r^{\prime}}({\pmb c}_{r^{\prime}})\}.}\end{array}
$$ 

Show that these messages satisfy the ﬁxed-point equations in equation (11.33). b. Show that 

$$
\left(\beta_{r}(\pmb{c}_{r})\right)^{\kappa_{r}}\propto\left(\beta_{r}(\pmb{c}_{r})\right)^{\kappa_{r}-1}\tilde{\psi}_{r}(\pmb{c}_{r})\prod_{r^{\prime}\in\mathbf{U}\mathbf{p}(r)}\delta_{r^{\prime\prime}\rightarrow r}^{d o w n}(\pmb{C}_{r})\prod_{r^{\prime\prime}\in\mathbf{D}\mathbf{w}\mathbf{n}(r)}\delta_{r^{\prime\prime}\rightarrow r}^{u p}(\pmb{C}_{r^{\prime\prime}}).
$$ 

Conclude that equation (11.76) holds. 

c. Show that 

$$
\tilde{\delta}_{r\rightarrow r^{\prime}}^{u p}\delta_{r^{\prime}\rightarrow r}^{d o w n}=\beta_{r}=\delta_{r\rightarrow r^{\prime}}^{u p}\tilde{\delta}_{r^{\prime}\rightarrow r}^{d o w n},
$$ 

and that 

$$
\delta_{r\rightarrow r^{\prime}}^{u p}\delta_{r^{\prime}\rightarrow r}^{d o w n}=(\beta_{r})^{q_{r}}.
$$ 

Show that the only solution to these equations is given by equation (11.73) and equation (11.74). 

d. Show by direct derivation that theorem 11.6 holds for the potentials deﬁned by equation (11.71)–11.76. (Hint: consider separately those regions that have no parents, and recall our previous assumptions.) 

# Exercise 11.20 

Consider the marginal probability over $C_{2}=\{A_{1,2},A_{2,2},A_{3,2},A_{4,2}\}$ in the $4\times4$ grid network. Show that, if we assume general potentials, this marginal probability does not satisfy any conditional indepen- dencies. 

# Exercise ${\bf11.21\star}$ 

nsider a clique tree $\mathcal{T}$ that is cons ed from a run of variable elimination over a Gibbs distribution $P_{\Phi}^{\prime}$ , as described in section 10.1.2. Let $\boldsymbol{S}_{i,j}$ be a sepset in the clique tree. Show that, for general potentials, $P_{\Phi}^{\prime}$ does not satisfy any conditional independencies relative to the marginal distribution over $\bar{S_{i,j}}$ . (Hint: consider the actual run of variable elimination that led to the construction of this sepset, and the efect on the graph of eliminating these variables.) 

# Exercise ${\bf l l.22\star}$ 

In section 10.3.3.2 we described an algorithm that uses a calibrated clique tree to compute a joint marginal over a set of variables that is not a subset of any clique in the tree. We can consider using the same procedure in a calibrated cluste graph $\mathcal{U}$ . For example, consider a pair of variables $X,Y$ that are not found together in any cluster in . 

a. Deﬁne an algorithm that, given clus $\boldsymbol{C}_{1}\ni\boldsymbol{X}$ and $C_{2}\ni Y$ and a path between them in the cluster graph computes a joint distribution $P(X,Y)$ . b. A cluster graph can contain more than one path between $C_{1}$ and $C_{2}$ . Provide an example showing that the answer returned by this algorithm can depend on the choice of path used. 

# Exercise ${\bf11.23\star}$ 

Suppose we have a cluster $\mathcal{T}$ , and we consider two approximation schemes for inference in this cluster tree. 

a. Using expectation propagation with fully factored approximation to the messages (as in ﬁgure 11.13). 

b. Using graph app ach on the cluster graph $\mathcal{U}$ co ructed in the following man Ea cluster $C_{i}\in\mathcal{T}$ ∈T appears in U . In additio each variable $X_{k}$ that a ars in me sepset $\boldsymbol{S}_{i,j}$ in U we introduce a new cluster with scope { $\{X_{k}\}$ } and connect it to both $\bar{\boldsymbol{C}}_{i}$ and $C_{j}$ . (This construction is similar to the Bethe cluster graph, except that we use the clusters in T as the “big” clusters in the construction.) 

Show that both approximations are equivalent in the sense that their respective energy functionals (and the constraints they satisfy) coincide. 

# Exercise 11.24 

Prove lemma 11.1. 

# Exercise $\mathbf{1.25\star}$ 

In this exercise, you will provide a simpler proof for a special case of theorem 11.9 and corollary 11.6. t each $X_{i}\,\in\,{\mathcal{X}}$ is a binary-valued random variable, parameterized with a single parameter $q_{i}=Q(x_{i}^{1})$ . 

a. By considering the derivative of $F[\tilde{P}_{\Phi},Q]$ and using lemma 11.1, prove theorem 11.9 without using Lagrange multipliers. b. Now, prove corollary 11.6. 

# Exercise $\mathbf{1.26\star}$ 

In this exercise, we will prove theorem 11.11. The proof relies on the following proposition, which charac- terizes the derivatives of an expectation relative to a Gibbs distribution. 

# Proposition 11.9 

$$
\frac{\partial}{\partial\psi_{j}(c_{j})}E_{Q}[f(U)]=\frac{Q(c_{j})}{\psi_{j}(c_{j})}\left(E_{Q}[f(U)\mid c_{j}]-E_{Q}[f(U)]\right)+E_{Q}\biggl[\frac{\partial}{\partial\psi_{j}(c_{j})}f(U)\biggr].
$$ 

a. Prove proposition 11.9. b. Apply this proposition to prove theorem 11.11. 

# Exercise $\mathbf{1.27\star}$ 

Consider the structured variational approximation of equation (11.61). As we discussed, to execute this update we need to collect the expectation of several factors, and each of these requires that we compute expectations given diferent assignments to the factor of interest. Assuming we use a clique tree to perform our inference, show how we can use a dynamic programming algorithm to reuse computation so as to evaluate these updates more efciently. 

# Exercise 11.28 

factorial HMM Consider the factorial HMM model shown in ﬁgure $6.3\mathrm{a}$ . Find a variational update rule for the structured variational approximation that factorizes as a set of clusters corresponding to the individual chains, that is, a cluster $\{\bar{X_{i}^{(0)}},\ldots,\bar{X_{i}^{(T)}}\}$ } for each $i$ . Make sure that you simplify your clusters as much as possible, as in section 11.5.2.3 and section 11.5.2.4. 

# Exercise ${\bf11.29\star}$ 

Now, consider the DBN whose structure is a set of variables $X_{i}$ that evolve over time, where, at each time point, the chains are correlated by a tree structure that is the same for all time slices. Let $\mathrm{Pa}_{i}$ be the parents of $X_{i}$ in the tree. Such structures (or extensions along similar lines) arise in several applications, such as those involving evolution of DNA or protein sequences; here, the chains encode spatial correlations over the sequence, and the tree the evolutionary process of each letter (DNA base pair or protein amino acid), whether across species, as in the phylogenetic HMM of box 6.A, or within a family tree (as in box 3.B). 

Consider an unrolled network over time $0,\cdot\cdot\cdot,T$ , where the initial $X_{i}^{(0)}$ are all independent. Find update rules for the following cluster variational approximations. In both cases, make sure that you simplify your clusters as much as possible, as in section 11.5.2.3 and section 11.5.2.4. 

a. Find an update rule for the cluster variational approximation that has a cluster for each chain; that is, a cluster $\{X_{i}^{(0)},\ldots,X_{i}^{(T)}\}$ } for each $i$ . b. Find an update rule for the cluster variational approximation that has a cluster for each time slice; that is, a cluster $\{X_{1}^{(t)},\allowbreak\cdot\cdot,X_{n}^{(t)}\}$ } for each $t$ . 

# Exercise 11.30 

Consider a distribution $P_{\Phi}$ that consists of the pairwise Markov network of ﬁgure 11.18a, and consider approximating it with distribution $Q$ that is represented by a pairwise Markov network of ﬁgure 11.18c. Derive the potentials $\psi_{1}(A,C)$ and $\psi_{1}(B,D)$ that maximize $\bar{D(P_{\Phi}\|Q)}$ | | . If $A$ and $C$ are not independent in $P_{\Phi}$ , will they be independent in the $Q$ with these potentials? 

# Exercise $\mathbf{1.31\star}$ 

Prove proposition 11.7. 

# Exercise $11.32\!\star\!\star$ 

Describe an algorithm that performs variational variable elimination with optimization of the variational parameters. Your algorithm should take as input a set of factors $\Phi$ , an elimination ordering $X_{1},\dots,X_{n}$ , and a subset $X_{i_{1}},.\.\cdot\cdot,X_{i_{k}}$ of steps at which variational approximations should be performed. Your algorithm should use the technique of section 11.5.3.2 to avoid coupling the factors in the elimination step of each $X_{i_{j}}$ , introducing a single variational parameter $\lambda_{j}$ for each such step. The result of the variable elimination procedure should be a symbolic expression in the $\lambda_{j}$ ’s. Explain precisely how to construct this symbolic expression and how to optimize it to select the optimal set of variational parameters $\lambda_{1},\ldots,\lambda_{k}$ . 

# Exercise ${\bf11.33\star}$ 

Show that if $\begin{array}{r}{Q=\prod_{i}Q(X_{i}\mid U_{i})}\end{array}$ Q | , then $\frac{\partial}{\partial Q(x_{i}\mid\mathbf{\nabla}\mathbf{u}_{i})}E_{Q}[f(\mathbf{Y})]=\frac{1}{Q(x_{i}\mid\mathbf{\nabla}\mathbf{u}_{i})}E_{Q}[f(\mathbf{Y})\mid x_{i},\mathbf{\nabla}\mathbf{u}_{i}].$ 

# Exercise $11.34\!\star\!\star$ 

Develop a variational approximation using Bayesian networks. Assume that $Q$ is represented by a Bayesian network of a given structure $\mathcal{G}$ . Derive the ﬁxed-point characterization of parameters that maximize the energy functional (that is, the analog of theorem 11.11) for this type of approximation. (Hint: use theorem 8.2 and exercise 11.33 in your derivation.) 

# Exercise $\mathbf{1.35\star}$ 

Prove proposition 11.8. 

# Exercise $\mathbf{1.36\star}$ 

In this exercise we consider inference in two-layered Bayesian networks with logistic CPDs. In these networks, all variables are binary. The variables ${\dot{X}}_{1},\dots,{\dot{X}}_{n}$ in the top layer are all independent of each other. The variables $Y_{1},\dots,Y_{m}$ in the bottom layer depend on the variables in the top layer using a logistic CPD: 

$$
P(y_{j}^{1}\mid x_{1},\ldots,x_{n})=\operatorname{sigmoid}(\sum_{i}w_{i,j}x_{i}+w_{0,j}).
$$ 

Suppose we observe some of the variables on the bottom layer, and we want to estimate the probability of the evidence. 

a. Show that when computing the probability of evidence, we can remove variables $Y_{i}$ that are not observed. 

b. Given evidence $_{_{_{_{_{_{_{_{_{_}}}}}}}}}$ , use the bound of equation (11.68) to write a variational upper bound of $P(\pmb{y})$ . c. Develop a mean ﬁeld approximation to lower-bound the probability of the same evidence. 

# Exercise $\mathbf{11.37\star\star}$ 

a. Show that the following function $f_{\mathrm{obj}}$ is convex: 

$$
f_{\mathrm{obj}}(a_{1},\dots,a_{n})=\ln\sum_{i}e^{a_{i}}.
$$ 

b. Prove the following bound 

$$
f_{\mathrm{obj}}(a_{1},\dots,a_{n})\geq\sum_{i}\lambda_{i}a_{i}-\sum_{i}\lambda_{i}\ln\lambda_{i},
$$ 

for any choice of $\left\{\lambda_{i}\right\}$ so that $\textstyle\sum_{i}\lambda_{i}=1$ P and $\lambda_{i}\geq0$ for all $i$ . (Hint: use the convexity of $f_{\mathrm{obj}}(\)$ .) c. Write 

$$
\ln{\cal Z}=f_{\mathrm{obj}}(\{\ln\tilde{P}_{\Phi}(\xi)\}),
$$ 

and use the preceding bound to write a lower bound for $\ln{\cal Z}$ . Use this analysis to provide an alternative derivation of the mean ﬁeld approximation. 

# Exercise $11.38\star\star$ 

mixture distribution We now consider a very diferent class of representations that we can use in a structure variational approximation. Here, our distribution $Q$ is a mixture distribution . More precisely, in addition to the variables $\mathcal{X}$ in $P_{\Phi}$ , we introduce a new variable $T$ . We can now deﬁne an approximating family: 

$$
Q(\mathcal{X})=\sum_{t}Q(t,\mathcal{X})=\sum_{t}Q(t)Q(\mathcal{X}\mid t),
$$ 

which is a mixture of diferent approximations $Q(\mathcal{X}\mid t)$ . As one simple instantiation, we might have: 

$$
Q(\mathcal{X},T)=Q(T)\prod_{i}Q(X_{i}\mid T),
$$ 

where each mixture component, $Q(\mathcal{X}\mid t)$ , is a mean ﬁeld distribution. 

a. Assuming that $Q$ is structured as in equation (11.82), prove that 

$$
F[\tilde{P}_{\Phi},Q]=\sum_{t}Q(t)F[\tilde{P}_{\Phi},Q(\mathcal{X}\mid t)]+I_{Q}(T;\mathcal{X}),
$$ 

where $I_{Q}(T;\mathcal{X})$ X is the mutual information between $T$ and $\mathcal{X}$ . 

This result quantiﬁes the gains that we can obtain by using a mixture distribution. The ﬁrst term is the weighted average of the energy functional of the mixture components; this term is bounded by the best of the components in isolation. The improvement over using speciﬁc components is captured by the second term, which measures the extent to which $T$ inﬂuences the distribution over $\mathcal{X}$ . If the components represent identical distributions, then this term is zero, and, as expected, the value of the energy functional is identical to the one obtained using the component representation. By contrast, if the components are diferent, then $T$ is informative on $\mathcal{X}$ , and the lower bound provided by $Q$ can be better than that of each of the components. We note that the “best scenario” for improvement is when the component terms $F[\tilde{P}_{\Phi},Q(\mathcal{X}\mid t)]$ X | are similar, and yet $I_{Q}(T;\mathcal{X})$ X is large. In other words, the best approximation is obtained when each of the components provides a good approximation, but by using a diferent distribution. 

We now use this formulation of the energy functional to produce an update rule for this variational approximation. This derivation uses a local variational approximation. 

b. Let $\{\lambda(\xi,t):\xi\in V a l(\mathcal{X}),t\in V a l(T)\}$ be a family of constants. Use lemma 11.2 to show that: 

$$
\pmb{I}_{Q}(T;\mathcal{X})\geq\pmb{H}_{Q}(T)-\sum_{\xi,t}\lambda(\xi,t)Q(t)+\pmb{E}_{Q}[\ln\lambda(\mathcal{X},T)]+1.
$$ 

Show also that this bound is tight if we choose a diferent value of $\lambda(\xi,t)$ for each possible combination of $\xi,t$ . 

c. Suppose that we use a factorized form for the variational parameters: 

$$
\lambda(\xi,t)=\lambda(t)\prod_{i}\lambda(x_{i}\mid t).
$$ 

Show that 

$$
\begin{array}{r c l}{{F[\tilde{P}_{\Phi},Q]}}&{{\ge}}&{{\displaystyle\sum_{t}Q(t)F[\tilde{P}_{\Phi},Q(\mathcal{X}\mid t)]+H_{Q}(T)-E_{Q}\Big[\tilde{\lambda}(T)\Big]\;+}}\\ {{}}&{{}}&{{}}\\ {{}}&{{}}&{{E_{Q}[\lambda(T)]+\displaystyle\sum_{i}E_{Q}[\lambda(X_{i}\mid T)],}}\end{array}
$$ 

d. Now, assume concretely that our approximation $Q$ has the form of equation (11.83), and that all of the $X_{i}$ variables are binary-valued. Use your lower bound in equation (11.85) as an approximation to the free energy, and provide an update rule for all of the parameters in $Q$ : $P(t)$ and ${\bar{P}}(x_{i}^{1}\mid t)$ | . e. Analyze the computational complexity of applying these ﬁxed-point equations. 

# 12 

particle 

# Particle-Based Approximate Inference 

In the previous chapter, we discussed one class of approximate inference methods. The tech- niques used in that chapter gave rise to algorithms that were very similar in ﬂavor to the factor-manipulation methods underlying exact inference. In this chapter, we discuss a very dif- ferent class of methods, ones that can be roughly classiﬁed as particle-based methods . In these methods, we approximate the joint distribution as a set of instantiations to all or some of the variables in the network. These instantiations, often called particles , are designed to provide a good representation of the overall probability distribution. 

Particle-based methods can be roughly characterized along two axes. On one axis, approaches vary on the process by which particles are generated. There is a wide variety of possible processes. At one extreme, we can generate particles using some deterministic process. At another, we can sample particles from some distribution. Within each category, there are many possible variations. 

full particle 

collapsed particle 

On the other axis, techniques use diferent notions of particles. Most simply, we can consider full particles — complete assignments to all of the network variables $\mathcal{X}$ . The disadvantage of this approach is that each particle covers only a very small part of the space. A more efective notion is that of a collapsed particle . A collapsed particle speciﬁes an assignment $\mathbfit{w}$ only to some subset of the variables $W$ , associating with it the conditional distribution $P(\mathcal{X}\mid\mathbf{\mu}w)$ or some “summary” of it. 

The general framework for most of the discussion in this chapter is as follows. Consider some distr ution $P(\mathcal X)$ , and assume we want to estimate the probability of some event $Y=y$ relative to P , for some $Y\subseteq\mathcal{X}$ and $\pmb{y}\in V a l(\pmb{Y})$ More generally, we might want to estimate the expectation of some function $f(\mathcal{X})$ relative to P ; this task is a generalization, since we can choose $f(\xi)=I\!\!\left\{\xi\langle Y\rangle=y\right\}$ { ⟨ ⟩ } , where we recall that $\xi\langle Y\rangle$ is e assignment in $\xi$ to the variables $Y$ . We approximate this expectation by generating a set of M particles, estimating the value of the function or its expectation relative to each of the generated particles, and then aggregating the results. 

For most of this chapter, we focus on methods that generate particles using random sampling: In section 12.1, we consider the simplest possible method, which simply generates samples from the original network. In section 12.2, we present a signiﬁcantly improved method that generates samples from a distribution that is closer to the posterior distribution. In section 12.3, we discuss a method based on Markov chains that deﬁnes a sampling process that, as it converges, generates samples from distributions arbitrarily close to the posterior. In section 12.5, we consider a very diferent type of method, one that generates particles deterministic ally by 

![](images/9ca2802c58955b8937bb0ea227c654b9f21027d4ef2ce4770f418cf0c0cfce29.jpg) 
Figure 12.1 The Student network $\mathcal{B}^{s t u d e n t}$ revisited 

searching for high-probability instantiations in the joint distribution. Finally, in section 12.4, we extend these methods to the case of collapsed particles. We note that, unlike our discussion of exact inference, some of the methods presented in this chapter — forward sampling and likelihood weighting — apply (at least in their simple form) only to Bayesian networks, and not to Markov networks or chain graphs. 

# 12.1 Forward Sampling 

forward sampling The simplest approach to the generation of particles is forward sampling . Here, we generate random samples $\xi[1],\cdot\cdot\cdot,\xi[M]$ from the distribution $P(\mathcal X)$ . We ﬁrst show how we can easily generate particles from $P_{\mathcal{B}}(\mathcal{X})$ by sampling from a Bayesian network. We then analyze the number of particles needed in order to get a good approximation of the expectation of some target function $f$ . We ﬁnally discuss the diﬃculties in generating samples from the posterior $P_{\mathcal{B}}(\mathcal{X}\mid e)$ . We note that, in undirected models, even generating a sample from the prior distribution is a diﬃcult task. 

# 12.1.1 Sampling from a Bayesian Network 

Sampling from a Bayesian network is a very simple process. 

# Example 12.1 

Consider the Student network, shown again in ﬁgure 12.1. We begin by sampling $D$ with the appropriate (unconditional) distribution; that is, we ﬁguratively toss a coin that lands heads $(d^{1}$ ) 40 percent of the time and tails $(d^{0})$ ) the remaining 60 percent. Let us assume that the coin landed 

![](images/0c90a57c93edab2fe60a13601cd3e7a7d56a33a40d404d642ef6c0b528f505d7.jpg) 

heads, so that we pick the value $d^{1}$ for $D$ . Similarly, we sample $I$ from its distribution; say that the result is $i^{0}$ . en those, we know the r ht distribution from which to sample $G$ : $P(G\mid i^{0},d^{1})$ , as deﬁned by $G\acute{s}$ ’s CPD; we therefore pick G to be $g^{1}$ with probability 0 . 05 , $g^{2}$ with probability 0 . 25 , and $g^{3}$ with probability 0 . 7 . The process continues similarly for $S$ and $L$ . 

As shown in algorithm 12.1, we sample the nodes in some order consistent with the partial order of the BN, so that by the time we sample a node we have values for all of its parents. We can then sample from the distribution deﬁned by the CPD and by the chosen values for the node’s parents. Note that the algorithm requires that we have the ability to sample from the distributions underlying our CPD. Such sampling is straightforward in the discrete case (see box 12.A), but subtler when dealing with continuous measures (see section 14.5.1). 

Box 12.A — Skill: Sampling from a Discrete Distribution. How do we generate a sample from a distribution? For a uniform distribution, we can use any pseudo-random number generator on our machine. Other distributions require more thought, and much work has been devoted in statistics to the problem of sampling from a variety of parametric distributions. Most obviously, consider a multinomial distribution $P(X)$ for $V a l(X)\,=\,\{x^{1},\cdot\,\cdot\,,x^{k}\}$ , which is deﬁned by parameters $\theta_{1},\ldots,\theta_{k}$ . This process can be done quite simply as follows: We generate a sample $s$ uniformly from the interval $[0,1]$ . We then partition the interval into $k$ subintervals: $[0,\theta_{1}),[\theta_{1},\theta_{1}+\theta_{2}),.\;.\;.,$ ; at is, the i th interval is $\textstyle[\sum_{j=1}^{i-1}\theta_{j},\sum_{j=1}^{i}\theta_{j})$ . If $s$ is in the i th interval, then the sampled value is $x^{i}$ . We can determine the interval for s using binary search in time $O(\log k)$ . 

This approach gives us a general-purpose solution for generating samples from the CPD of any discrete-valued variable: given a parent assignment $\mathbfit{u}$ , we can always generate the full conditional distribution $P(X\mid{\pmb u})$ and sample from it. (Of course, more efcient methods may exist if $X$ has $^a$ large value space or a CPD that requires an expensive computation.) As we discuss in section 14.5.1, the problem of sampling from continuous CPDs is considerably more complex. 

Using basic convergence bounds (see appendix A.2), we know that from a set of particles $\mathcal{D}=\{\xi[1],\dots,\xi[M]\}$ generated via this sampling process, we can estimate the expectation of any function $f$ as: 

$$
\hat{\pmb{{\cal E}}}_{\mathcal{D}}(f)=\frac{1}{M}\sum_{m=1}^{M}f(\xi[m]).
$$ 

In the case where our task is to compute $P(\pmb{y})$ , this estimate is simply the fraction of particles where we have seen the event $_{_y}$ : 

$$
\hat{P}_{\mathcal{D}}(\pmb{y})=\frac{1}{M}\sum_{m=1}^{M}\pmb{I}\{\pmb{y}[m]=\pmb{y}\},
$$ 

where we use $\pmb{y}[m]$ to denote $\xi[m]\langle Y\rangle\mathrm{~-~}$ the assignment to $Y$ in the particle $\xi[m]$ . For example, our estimate for the probability of an event such as $i^{1},l^{0}$ (a smart student getting a bad letter) is the fraction of particles in which $I$ got the value $i^{1}$ and $L$ the value $l^{0}$ . Note that the same set of particles can be used to estimate the probabilities of multiple events. 

This sampling process requires one sampling operation for each random variable in the net- work. For each variable $X$ , we need to index into the CPD using the current partial instantiation of the parents of $X$ . Using an appropriate data structure, this indexing can be accomplishing in time $O(|\mathrm{Pa}_{X}|)$ . The actual sampling process, we discussed, requires time $O(\log|V a l(X)|)$ (assuming appropriate preprocessing). Letting M be the total number of particles generated, $n=|{\mathcal{X}}|$ , $p=\mathrm{max}_{i}\left|\mathrm{Pa}_{X_{i}}\right|$ , and $d=\operatorname*{max}_{i}{\left|V a l(X_{i})\right|}$ , the overall cost is $O(M n p\log d)$ . 

# 12.1.2 Analysis of Error 

Of course, the quality of the estimate obtained depends heavily on the number of particles generated. We now analyze the question of the number of particles required to obtain certain performance guarantees. We focus on the analysis for the case where our goal is to estimate $P(\pmb{y})$ . 

The techniques of appendix A.2 provide us with the necessary tools for this analysis. Consider the quality of our estimate for a particular event $Y\,=\,y$ . We deﬁne a new random variable over the probability space of $P$ , using the indicator function $I\{Y=y\}$ { } . This is a Bernoulli random variable, and hence our $M$ particles in $\mathcal{D}$ deﬁne $M$ independent Bernoulli trials, each with success probability $P(\pmb{y})$ . 

Hoefding bound 

We can now apply the Hoefding bound (theorem A.3) to show that this estimate is close to the truth with high probability: 

$$
\begin{array}{r}{P_{\mathcal{D}}\big(\hat{P}_{\mathcal{D}}(\pmb{y})\not\in[P(\pmb{y})-\epsilon,P(\pmb{y})+\epsilon]\big)\leq2e^{-2M\epsilon^{2}}.}\end{array}
$$ 

This analysis provides us with an estimate of how many samples are required to achieve an estimate whose error is bounded by $\epsilon$ , with probability at least $1-\delta$ . Setting 

$$
2e^{-2M\epsilon^{2}}\le\delta
$$ 

sample size estimator 

and doing simple algebraic manipulations, we get that the required sample size to get an estimator with $(\epsilon,\delta)$ reliability is: 

$$
M\geq\frac{\ln(2/\delta)}{2\epsilon^{2}}.
$$ 

Chernof bound relative error 

We can similarly apply the Chernof bound (theorem A.4) to conclude that $\hat{P}_{\mathcal{D}}(y)$ is also D within a relative error $\epsilon$ of the true value $P(\pmb{y})$ , with high probability. Speciﬁcally, we have that: 

$$
\begin{array}{r}{P_{\mathcal{D}}\big(\hat{P}_{\mathcal{D}}(\pmb{y})\notin P(\pmb{y})(1\pm\epsilon)\big)\le2e^{-M P(\pmb{y})\epsilon^{2}/3}.}\end{array}
$$ 

Note that in this analysis, unlike the one based on Hoefding’s bound, the error probability (the chance of getting an estimate that is more than $\epsilon$ away from the true value) depends on the actual target value $P(\pmb{y})$ . This dependence is not surprising for a relative error bound. Assume that we generate $M$ samples, but we generate none where $\pmb{y}[m]=\pmb{y}$ . Our estimate $\hat{P}_{\mathcal{D}}(y)$ is D simply 0 . However, if $P(\pmb{y})$ is very small, it is fairly likely that we simply have not generated any samples where this event holds. In this case, our estimate of 0 is not going to be within any relative error of $P(\pmb{y})$ . Thus, for very small values of $P(\pmb{y})$ , we need many more samples in order to guarantee that our estimate is close with high probability. 

Examining equation (12.4), we can see that, for a given $\epsilon$ , the number of samples needed to guarantee a certain error probability $\delta$ is: 

$$
M\geq3\frac{\ln(2/\delta)}{P(\pmb{y})\epsilon^{2}}.
$$ 

Thus, the number of required samples grows inversely with the probability $P(\pmb{y})$ . 

In summary, to guarantee an absolute error of $\epsilon$ with probability at least $1-\delta$ , we need a number of samples that grows logarithmically in $1/\delta$ and quadratically in $1/\epsilon$ . To guarantee a relative error of $\epsilon$ with error probability at most $\delta$ , we need a number of samples that grows similarly in $\delta$ and $\epsilon$ , but that also grows linearly with $1/P(\pmb{y})$ . A signiﬁcant problem with using this latter bound is that we do not know $P(\pmb{y})$ . (If we did, we would not have to estimate it.) Thus, we cannot determine how many samples we need in order to ensure a good estimate. 

# 12.1.3 Conditional Probability Queries 

So far, we have focused on the problem of estimating marginal probabilities, that is, probabilities of events $Y=y$ relative to the original joint distribution. In general, however, we are interested in conditional probabilities of the form $P(\pmb{y}\mid\pmb{E}\,=\,e)$ . Unfortunately, it turns out that this estimation task is signiﬁcantly harder. 

rejection sampling 

One approach to this task is simply to generate samples from the posterior pr ability $P(\mathcal X\mid$ $e$ ) . We can do so by a process called rejection sampling : We generate samples x from $P(X)$ , as in section 12.1.1. We then reject any sample that is not compatible with $e$ . The resulting samples are sampled from the posterior $P(X\mid e)$ . The results of the analysis in section 12.1.2 now apply unchanged. 

The problem, of course, is that the number of unrejected particles can be quite small. In general, the expected number of particles that are not rejected from an original sample set of size $M$ is $M P(e)$ . For example, if $P(e)=0.001$ , then even for $M\,=\,10,000$ samples, the expected number of unrejected particles is 10 . Conversely, to obtain at least $M^{*}$ unrejected particles, we need to generate on average $M=M^{*}/P(e)$ samples from $P(X)$ . 

Unfortunately, in many applications, low-probability evidence is the rule rather than the exception. For example, in medical diagnosis, any set of symptoms typically has low probability. In general, as the number of observe variables $k=|E|$ grows, the probability of the evidence usually decreases exponentially with k . 

An alternative approach to the problem is to use a separate estimator for $P(e)$ and for $P(\pmb{y},e)$ and then compute the ratio. We can show that if we have estimators of low relative error for both of these quantities, then their ratio will also have a low relative error (exercise 12.2). Unfortunately, this approach only moves the problem from one place to the other. As we said, the number of samples required to achieve a low relative error also grows linearly with 

 $1/P(e)$ . The number of samples required to get low absolute error does not grow with $P(e)$ . However, it is not hard to verify (exercise 12.2) that a bound on the absolute error for $P(e)$ does not sufce to get any type of bound (relative or absolute) for the ratio $P(\pmb{y},\pmb{e})/P(\pmb{e})$ . 

# 12.2 Likelihood Weighting and Importance Sampling 

The rejection sampling process seems very wasteful in the way it handles evidence. We generate multiple samples that are inconsistent with our evidence and that are ultimately rejected without contributing to our estimator. In this section, we consider an approach that makes our samples more relevant to our evidence. 

# 12.2.1 Likelihood Weighting: Intuition 

Consider the network in example 12.1, and assume that our evidence is $d^{1},s^{1}$ . Our forward sampling process might begin by generating a value of $d^{0}$ for $D$ . No matter how the sampling process proceeds, this sample will always be rejected as being incompatible with the evidence. 

It seems much more sensible to simply force the samples to take on the appropriate values at observed nodes. That is, when we come to sampling a node $X_{i}$ whose value has been observed, we simply set it to its observed value. 

In general, however, this simple approach can generate incorrect results: 

Consider the network of example 12.1, and assume that our evidence is $s^{1}-a$ student who received a high SAT score. Using the naive process, we sample $D$ and $I$ from their prior distribution, set $S\;=\;s^{1}$ , and then sample $G$ and $L$ appropriately. All of our samples will have $S\;=\;s^{1}$ , as desired. However, the expected number of samples that have $I=i^{1}-a n$ intelligent student — is 30 percent, the same as in the prior distribution. Thus, this approach fails to conclude that the posterior probability of $i^{1}$ is higher when we observe $s^{1}$ . 

The problem with this approach is that it fails to account for the fact that, in the standard forward sampling process, the node $S$ is more likely to take the value $s^{1}$ when its parent $I$ has the value $i^{1}$ than when $I$ has the value $i^{0}$ . In particular, consider an imaginary process where we run rejection sampling many times; samples where we generated the value ${\mathit{\bar{I}}}\,=\,i^{1}$ would have generated $S=s^{1}$ in 80 percent of the samples, whereas samples where we generated the value $I=i^{0}$ would have generated $S=s^{1}$ in only 5 percent of the samples. To simulate this long-run behavior within a single sample, we should conclude that a sample where we have 

![](images/c7ea3ec62927a4c4eb689f8eb48b8a23cce6afd781aabc42a49d65437ad7873a.jpg) 

$I=i^{1}$ and force $S=s^{1}$ should be worth 80 percent of a sample, whereas one where we have $I=i^{0}$ and force $S=s^{1}$ should only be worth 5 percent of a sample. 

When we have multiple observations and we want our sampling process to set all of them to their observed values, we need to consider the probability that each of the observation nodes, had it been sampled using the standard forward sampling process, would have resulted in the observed values. The sampling events for each node in forward sampling are independent, and hence the weight for each sample should be the product of the weights induced by each evidence node separately. 

# Example 12.3 

likelihood weighting 

weighted particle 

estimator 

Consider the same network, where our evidence set now consists of $l^{0},s^{1}$ . Assume that we sample $D\,=\,d^{1}$ , $I\,=\,i^{0}$ , set $S\;=\;s^{1}$ , sample $G\,=\,g^{2}$ , and set $L\,=\,l^{0}$ . The probability that, given $I\,=\,i^{0}$ , forward sampling would have generated $S\;=\;s^{1}$ is 0 . 05 . The probability that, given $G\,=\,g^{2}$ , forward sampling would have generated $L\,=\,l^{0}$ is 0 . 4 . If we consider the standard forward sampling process, each of these events is the result of an independent coin toss. Hence, the probability that both would have occurred is simply the product of their probabilities. Thus, the weight required for this sample to compensate for the setting of the evidence is $0.05\cdot0.4=0.02.\$ 

Generalizing this intuition results in an algorithm called likelihood weighting (LW) , shown in algorithm 12.2. The name indicates that the weights of diferent samples are derived from the likelihood of the evidence accumulated throughout the sampling process. 

This process generates a weighted particle . We can now estimate a conditional proba- bility $P(\pmb{y}\mid\pmb{e})$ by using LW-Sample $M$ times to generate a set $\mathcal{D}$ of weighted particles $\langle\xi[1],w[1]\rangle,.\,.\,.\,,\langle\xi[M],w[M]\rangle$ . We then estimate: 

$$
\hat{P}_{\mathcal{D}}(\pmb{y}\mid e)=\frac{\sum_{m=1}^{M}w[m]\pmb{I}\{\pmb{y}[m]=\pmb{y}\}}{\sum_{m=1}^{M}w[m]}.
$$ 

This estimator is an obvious generalization of the one we used for unweighted particles in equation (12.2). There, each particle had weight 1 ; hence, the terms in the numerator were unweighted, and the denominator, which is the sum of all the particle weights, was simply $M$ . It is also important to note that, as in forward sampling, the same set of samples can be used to estimate the probability of any event $_{_y}$ . 

Aside from some intuition, we have provided no formal justiﬁcation for the correctness of LW as yet. It turns out that LW is a special case of a very general approach called importance sampling , which also provides us the basis for an analysis. We begin by providing a general description and analysis of importance sampling, and then reformulate LW as a special case of this framework. 

# 12.2.2 Importance Sampling 

importance sampling target distribution 

proposal distribution 

support 

Let $X$ be a variable (or set of variables) that takes on values in some space $V a l(X)$ . Importance sampling is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$ , typically called the target distribution . As we discussed, we can estimate this expectation by generating samples ${\pmb x}[1],\cdot\cdot\cdot,{\pmb x}[M]$ from $P$ , and then estimating 

$$
E_{P}[f]\approx\frac{1}{M}\sum_{m=1}^{M}f(\pmb{x}[m]).
$$ 

In some cases, however, we might prefer to generate samples from a diferent distribution $Q$ , known as the proposal distribution or sampling distribution . There are several reasons why we might wish to sample from a diferent distribution. Most importantly for our purposes, it might be impossible or computationally very expensive to generate samples from $P$ . For example, $P$ might be a posterior distribution for a Bayesian network, or even a prior distribution for a Markov network. 

In this section, we discuss how we might obtain estimates of an expectation relative to $P$ by generating samples from a diferent distribution $Q$ . In general, the proposal distribution $Q$ can be arbitrary; we require only that $Q({\pmb x})\,>\,0$ whenever $P(\pmb{x})\;>\;0$ , so that $Q$ does not “ignore” any states that have nonzero probability relative to $P$ . (More formally, the support of a distribution $P$ is the set of points $_{_{x}}$ for which $P({\pmb x})\,>\,0$ ; we require that the support of $Q$ contain the support of $P$ .) However, as we will see, the computational performance of this approach does depend strongly on the extent to which $Q$ is similar to $P$ . 

# 12.2.2.1 Unnormalized Importance Sampling 

If we generate samples from $Q$ instead of $P$ , we cannot simply average the $f$ -value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that 

$$
\pmb{{\cal E}}_{P(X)}[f(X)]=\pmb{{\cal E}}_{Q(X)}\bigg[f(X)\frac{P(X)}{Q(X)}\bigg].
$$ 

This equality follows directly: 1 

$$
\begin{array}{r c l}{{E_{Q(X)}\biggl[f(X)\frac{P(X)}{Q(X)}\biggr]}}&{{=}}&{{\displaystyle\sum_{x}Q(x)f(x)\frac{P(x)}{Q(x)}}}\\ {{}}&{{=}}&{{\displaystyle\sum_{x}f(x)P(x)}}\\ {{}}&{{=}}&{{E_{P(X)}[f(X)].}}\end{array}
$$ 

Based on this observation, we can use the standard estimator for expectations relative to $Q$ . We generate a set of samples ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ from $Q$ , and then estimate: 

$$
\hat{\pmb{{\cal E}}}_{\cal D}(f)=\frac{1}{M}\sum_{m=1}^{M}f({\pmb x}[m])\frac{{\cal P}({\pmb x}[m])}{{\cal Q}({\pmb x}[m])}.
$$ 

unnormalized importance sampling estimator 

unbiased estimator 

# Proposition 12.1 

We call this estimator the unnormalized importance sampling estimator; this method is also often called unweighted importance sampling (this terminology is confusing, inasmuch as the particles here are also associated with weights). The factor $P(\pmb{x}[m])/Q(\pmb{x}[m])$ can be viewed as a correction weight to the term $f(\ensuremath{\boldsymbol{x}}[m])$ , which we would have used had $Q$ been our target distribution. We use $w(x)$ to denote $P(\pmb{x})/Q(\pmb{x})$ . 

Our analysis immediately implies that this estimator is unbiased , that is, its mean for any data set is precisely the desired value: 

For data sets $\mathcal{D}$ sampled from $Q$ , we have that: 

$$
\pmb{E}_{\mathcal{D}}\Big[\hat{\pmb{E}}_{\mathcal{D}}(f)\Big]=\pmb{E}_{Q(\pmb{X})}[f(\pmb{X})w(\pmb{X})]=\pmb{E}_{P(\pmb{X})}[f(\pmb{X})].
$$ 

We can also estimate the distribution of this estimator around its mean. Letting $\epsilon_{\mathcal{D}}~=$ $\hat{\pmb{{\cal E}}}_{\mathcal{D}}(f)-\pmb{{\cal E}}_{\cal P}[f(\pmb{x})]$ − , we have that, since $M\rightarrow\infty$ : 

$$
E_{\mathcal{D}}[\epsilon_{\mathcal{D}}]\sim\mathcal{N}\left(0;\sigma_{Q}^{2}/M\right),
$$ 

where 

$$
\begin{array}{r c l}{\sigma_{Q}^{2}}&{=}&{{\cal E}_{Q(X)}\big[(f(X)w(X))^{2}\big]-{\cal E}_{Q(X)}[(f(X)w(X))]^{2}}\\ &{=}&{{\cal E}_{Q(X)}\big[(f(X)w(X))^{2}\big]-({\cal E}_{P(X)}[f(X)])^{2}.}\end{array}
$$ 

estimator variance 

As we discussed in appendix A.2, the variance of this type of estimator — an average of $M$ independent random samples from a distribution — decreases linearly with the number of samples. This point is important, since it allows us to provide a bound on the number of samples required to obtain a reliable estimate. 

To understand the constant term in this expression, consider the (uninteresting) case where the function $f$ is the constant function $f(\xi)\equiv1$ . In this case, equation (12.9) simpliﬁes to: 

$$
\begin{array}{r c l}{E_{Q(X)}\left[w(X)^{2}\right]-E_{P(X)}[1]}&{=}&{E_{Q(X)}\Biggl[\left(\frac{P(X)}{Q(X)}\right)^{2}\Biggr]-\left(E_{Q(X)}\left[\frac{P(X)}{Q(X)}\right]\right)^{2},}\end{array}
$$ 

which is simply the variance of the weighting function $P(x)/Q(x)$ . Thus, the more diferent $Q$ is from $P$ , the higher the variance of this estimator. When $f$ is an indicator function over part of the space, we obtain an identical expression restricted to the relevant subspace. In general, one can show that the lowest variance is achieved when 

$$
Q(X)\propto|f(X)|P(X);
$$ 

thus, for example, if $f$ is an indicator function over part of the space, we want our sampling distribution to be $P$ conditioned on the subspace. 

Note that we should avoid cases where our sampling probability $Q(X)\ll P(X)f(X)$ in any part of the space, since these cases can lead to very large or even inﬁnite variance. Thus, care must be taken when using very skewed sampling distributions, to ensure that probabilities in $Q$ are close to zero only when $P(X)f(X)$ is also very small. 

# 12.2.2.2 Normalized Importance Sampling 

One problem with the preceding discussion is that it assumes that $P$ is known. A frequent situation, and one of the most common reasons why we must resort to sampling from a diferent distribution $Q$ , is that $P$ is known only up to a normalizing constant $Z$ . Speciﬁcally, what we have access to is a function ${\tilde{P}}(X)$ such that $\tilde{P}$ is not a normalized distribution, but ${\tilde{P}}(X)=Z P(X)$ . For example, in a Bayesi work $\mathcal{B}$ , we might have (for $X=\mathcal{X}$ ) $P(\mathcal X)$ be our posterior distribution $P_{\mathcal{B}}(\mathcal{X}\mid e)$ , and $\tilde{P}(\mathcal X)$ X b the unnormalized distribution $P_{\mathcal{B}}(\mathcal{X},e)$ . In a Markov network, $P(\mathcal X)$ might be $P_{\mathcal H}(\mathcal X)$ , and $\tilde{P}$ might be the unnormalized distribution obtained by multiplying together the clique potentials, but without normalizing by the partition function. 

In this context, we cannot deﬁne the weights relative to $P$ , so we deﬁne: 

$$
w(X)=\frac{\tilde{P}(X)}{Q(X)}.
$$ 

Unfortunately, with this deﬁnition of weights, the analysis justifying the use of equation (12.8) breaks down. However, we can use a slightly diferent estimator based on similar intuitions. As before, the weight $w(X)$ is a random variable. Its expected value is simply $Z$ : 

$$
E_{Q(X)}[w(X)]=\sum_{x}Q(x)\frac{\tilde{P}(x)}{Q(x)}=\sum_{x}\tilde{P}(x)=Z.
$$ 

This quantity is the normalizing constant of the distribution $\tilde{P}$ , which is itself often of consid- erable interest, as we will see in our discussion of learning algorithms. 

We can now rewrite equation (12.7): 

$$
\begin{array}{r c l}{E_{P(X)}[f(X)]}&{=}&{\displaystyle\sum_{x}P(x)f(x)}\\ &{=}&{\displaystyle\sum_{x}Q(x)f(x)\frac{P(x)}{Q(x)}}\\ &{=}&{\displaystyle\frac{1}{Z}\sum_{x}Q(x)f(x)\frac{\tilde{P}(x)}{Q(x)}}\\ &{=}&{\displaystyle\frac{1}{Z}E_{Q(X)}[f(X)w(X)]}\\ &{=}&{\displaystyle\frac{E_{Q(X)}[f(X)w(X)]}{E_{Q(X)}[w(X)]}.}\end{array}
$$ 

We can use an empirical estimator for both the numerator and denominator. Given $M$ samples ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ from $Q$ , we can estimate: 

$$
\hat{\pmb{{\cal E}}}_{\mathscr{D}}(f)=\frac{\sum_{m=1}^{M}f({\pmb x}[m])w({\pmb x}[m])}{\sum_{m=1}^{M}w({\pmb x}[m])}.
$$ 

normalized importance sampling estimator 

We call this estimator the normalized importance sampling estimator ; it is also known as the weighted importance sampling estimator. 

The normalized estimator involves a quotient, and it is therefore much more difcult to analyze theoretically. However, unlike the unnormalized estimator of equation (12.8), the normalized estimator is not unbiased. This bias is particularly immediate in the case $M=1$ . Here, the estimator reduces to: 

$$
\frac{f(\pmb{x}[1])w(\pmb{x}[1])}{w(\pmb{x}[1])}=f(\pmb{x}[1]).
$$ 

Because $\pmb{x}[1]$ is sampled from $Q$ , the mean of the estimator in this case is $E_{Q(X)}[f(X)]$ rather than the desired $\pmb{E}_{P(\pmb{X})}[f(\pmb{X})]$ . Conversely, when $M$ goes to inﬁnity, we have that each of the numerators and denominators converges to the expected value, and our analysis of the expectation applies. In general, for ﬁnite $M$ , the estimator is biased, and the bias goes down as $1/M$ . 

One can show that the variance of the importance sampling estimator with $M$ data instances is approximately: 

$$
{\texttt V a r}_{P}\Big[\hat{\pmb{E}}_{D}(f(\pmb{X}))\Big]\approx\frac{1}{M}{\texttt V a r}_{P}[f(\pmb{X})](1+{\texttt V a r}_{Q}[w(\pmb{X})]),
$$ 

which also goes down as $1/M$ . Theoretically, this variance and the variance of the unnormalized estimator (equation (12.8)) are incomparable, and each of them can be larger than the other. Indeed, it is possible to construct examples where each of them performs better than the other. In practice, however, the variance of the normalized estimator is typically lower than that of the unnormalized estimator. This reduction in variance often outweighs the bias term, so that the normalized estimator is often used in place of the unnormalized estimator, even in cases where $P$ is known and we can sample from it efectively. 

Note that equation (12.14) can be used to provide a rough estimate on the quality of a set of samples generated using normalized importance sampling. Assume that we were to estimate

 $\pmb{{\cal E}}_{P}[f]$ using a standard sampling method, where we generate $M$ IID samples from $P(X)$ .

 (Obviously, this is generally intractable, but it provides a useful benchmark for comparison.) This approach would result in a variance ${\mathbb{W}}a r_{P}[f(X)]/M$ . The ratio between these two variances is: 

$$
\frac{1}{1+\mathbb{W}a r_{Q}[w(\pmb{x})]}.
$$ 

Thus, we would expect $M$ weighted samples generated by importance sampling to be “equiv- alent” to $M/(1+\mathbb{W}a r_{Q}[w({\pmb x})])$ samples generated by IID sampling from $P$ . We can use this observation to deﬁne a rule of thumb for the efective sample size of a particular set $\mathcal{D}$ of $M$ samples resulting from a particular run of importance sampling: 

$$
\begin{array}{r c l}{{M_{\mathrm{eff}}}}&{{=}}&{{\displaystyle\frac{M}{1+V a r[\mathcal{D}]}}}\\ {{V a r[\mathcal{D}]}}&{{=}}&{{\displaystyle\sum_{m=1}^{M}w(\pmb{x}[m])^{2}-(\sum_{m=1}^{M}w(\pmb{x}[m]))^{2}.}}\end{array}
$$ 

This estimate can tell us whether we should continue generating additional samples. 

# 12.2.3 Importance Sampling for Bayesian Networks 

With this theoretical foundation, we can now describe the application of importance sampling to Bayesian networks. We begin by providing the proposal distribution most commonly used for Bayesian networks. This distribution $Q$ uses the network structure and its CPDs to focus the sampling process on a particular part of the joint distribution — the one consistent with a particular event $Z=z$ . We show several ways in which this construction can be applied to the Bayesian network inference task, dealing with various types of probability queries. Finally, we brieﬂy discuss several other proposal distributions, which are somewhat more complicated to implement but may perform better in practice. 

# 12.2.3.1 The Mutilated Network Proposal Distribution 

Assume that we are interested in a particular event $Z=z$ , either because we wish to estimate its probability, or because we have observed it as evidence. We wish to focus our sampling process on the parts of the joint that are consistent with this event. In this section, we deﬁne an importance sampling process that achieves this goal. 

To gain some intuition, consider the network of ﬁgure 12.1 and assume that we are interested in a particular event concerning a student’s grade: $G\,=\,g^{2}$ . We wish to bias our sampling toward parts of the space where this event holds. It is easy to take this event into consideration when sampling $L$ : we imply sample $L$ m $P(L\mid g^{2})$ . However, it is considerably more difcult to account for G ’s inﬂuence on $D,\,I$ , and S without doing inference in the network. 

Our goal is to deﬁne a simple proposal distribution that allows for the efcient generation of particles. We therefore avoid the problem of accounting for the efect of the event on nondescendants; we deﬁne a proposal distribution that “sets” the value of a $Z\in Z$ to take the 

![](images/93fa8a737f4704b4b7c04ca6405014fbd6dd826910628183f5dfc8c757f6cdcd.jpg) 
Figure 12.2 The mutilated network $\mathcal{B}_{I=i^{1},G=g^{2}}^{s t u d e n t}$ used for likelihood weighting 

prespeciﬁed value in a way that inﬂuences the sampling process for its descendants, but not for the other nodes in the network. The proposal distribution is most easily described in terms of a Bayesian network: 

Deﬁnition 12.1 mutilated network 

Let $\mathcal{B}$ be a network, and $\mathcal{Z}_{1}=z_{1},.\,.\,.\,,\mathcal{Z}_{k}=z_{k}$ , abbreviated $Z=z$ , an instantiation of variables. We deﬁne the mutilated network $\mathcal{B}_{Z=z}$ as follows: 

• ode $Z_{i}\in Z$ has no parents in $\mathcal{B}_{Z=z}$ f $Z_{i}$ in $\mathcal{B}_{Z=z}$ gives probability 1 to $Z_{i}=z_{i}$ and probability 0 to all other values $z_{i}^{\prime}\in V a l(Z_{i})$ ∈ . • The parents and CPDs of all other nodes $X\not\in Z$ are unchanged. 

For example, the network $\mathcal{B}_{I=i^{1},G=g^{2}}^{s t u d e n t}$ is shown in ﬁgure 12.2. As we can see, the node $G$ is decoupled from its parents, eliminating its dependence on them (the node $I$ has no parents in the original network, so its parent set remains empty). Furthermore, both $I$ and $G$ have CPDs that are deterministic, ascribing probability 1 to their (respective) observed values. 

Importance sampling with this proposal distribution is precisely equivalent to the LW algo- rithm shown in algorithm 12.2, with $\Tilde{P(\mathcal{X})}=P_{\mathcal{B}}(\mathcal{X},z)$ X X and the proposal distribution $Q$ induced B by the mutilated network $\mathcal{B}_{Z=z}$ . More formally, we can show the following proposition: 

Proposition 12.2 

$$
w(\xi)=\frac{P_{\mathcal{B}}(\xi)}{P_{\mathcal{B}_{Z=z}}(\xi)}.
$$ 

The proof is not difcult and is left as an exercise (exercise 12.4). It is important to note, however, that the algorithm does not require the explicit construction of the mutilated network. It simply traverses the original network, using the process shown in algorithm 12.2. 

As we now show, this proposal distribution can be used for estimating a variety of Bayesian network queries. 

# 12.2.3.2 Unconditional Probability of an Event $\star$ 

We begin by considering the simple problem of computing the unconditional probability of an event $Z=z$ . Although we can clearly use forward sampling for estimating this probability, we can also use unnormalized importance sampling, where the target distribution $P$ is simply our prior distribution $P_{\mathcal{B}}(\mathcal{X})$ , and the proposal distribution $Q$ is the one deﬁned by the mutilated network $\mathcal{B}_{Z=z}$ . Our al is to estimate the expectation of a function $f$ , which is the indicator function of the query z : $f(\xi)=I\!\!\left\{\xi\langle Z\rangle=z\right\}$ { ⟨ ⟩ } . 

The unnormalized importance-sampling estimator for this case is simply: 

$$
\begin{array}{r c l}{{\hat{P}_{\mathcal D}(z)}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m=1}^{M}I\{\xi[m]\langle Z\rangle=z\}w(\xi[m])}}\\ {{}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m=1}^{M}w[m],}}\end{array}
$$ 

where the equality follows because, by deﬁnition of $Q$ , our sampling process generates samples $\xi[m]$ only where $_z$ holds. 

When trying to bound the relative error of an estimator, a key quantity is the variance of the estimator relative to its mean . In the Chernof bound, when we are estimating the probability $p$ of a very low-probability event, the variance of the estimator, which is $p(1-p)$ , is very high relative to the mean $p$ . Importance sampling removes some of the variance associated with this sampling process, and it can therefore achieve better performance in certain cases. 

In this case, the samples are derived from our proposal distribution $Q$ , and the value of the function whose expectation we are computing is simply the weight. Thus, we need to bound the variance of the function $w(\mathcal{X})$ under our distribution $Q$ . Let us consider the sampling process in the algorithm. As we go through the variables in the network, we encounter the observed variables $Z_{1},.\,.\,.\,,Z_{k}$ . At each point, we multiply our current weight $w$ by some conditional probability number $P_{\mathcal{B}}(Z_{i}=z_{i}\mid\mathrm{Pa}_{Z_{i}})$ . 

One situation where we can bound the variance arises in a restricted class of networks, one where the entries in the CPD of the variables $Z_{i}$ are bounded away from the extremes of 0 and 1 . More precisely, we assume that there is some pair of numbers $\ell>0$ and $u<1$ such that: for each variable $Z\in Z$ , $z\,\in\,V a l(Z)$ , and $u\in{\it V a l}(\mathrm{Pa}_{Z})$ , we have that $P_{\mathcal{B}}(Z=z\mid\mathrm{Pa}_{Z}=$ $\pmb{u})\in[\ell,u]$ . Next, we assume that $|Z|=k$ for some small k . This assumption is not a trivial one; while queries often involve only a small number of variables, we often have a fairly large number of observations that we wish to incorporate. 

Under these assumptions, the weight $w$ generated through the LW process is necessarily in the interval $\ell^{k}$ and $u^{k}$ . We can now redeﬁne our weights by dividing each $w[m]$ by $u^{k}$ : 

$$
w^{\prime}[m]=w[m]/u^{k}.
$$ 

Each weight $w^{\prime}[m]$ is now a real-valued random variable in the range $[(\ell/u)^{k},1]$ . For a data set $\mathcal{D}$ of weights $w[1],\ldots,w[M]$ , we can now deﬁne: 

$$
\hat{p}_{\mathcal{D}}^{\prime}=\frac{1}{M}\sum_{m=1}^{M}w^{\prime}[m].
$$ 

The key point is that the mean of this random variable, which is $P_{\mathcal{B}}(z)/u^{k}$ , is therefore also in the range $[(\ell/u)^{k},1]$ , and its variance is, at worst, the variance of a Bernoulli random variable with the same mean. Thus, we now have a random variable whose variance is not that small relative to its mean. 

A simple generalization of Chernof’s bound (theorem A.4) to the case of real-valued variables can now be used to show that: 

$$
\begin{array}{r c l}{{P_{\mathcal{D}}(\hat{P}_{\mathcal{D}}(z)\not\in P_{\mathcal{B}}(z)(1\pm\epsilon))}}&{{=}}&{{P_{\mathcal{D}}(\hat{p}_{\mathcal{D}}^{\prime}\not\in\displaystyle\frac{1}{u^{k}}P_{\mathcal{B}}(z)(1\pm\epsilon))}}\\ {{}}&{{\le}}&{{2e^{-M\frac{1}{u^{k}}P_{\mathcal{B}}(z)\epsilon^{2}/3}.}}\end{array}
$$ 

sample size We can use this equation, as in the case of Bernoulli random variables, to derive a sufcient condition for the sample size that can guarantee that the estimator $\hat{P}_{\mathcal{D}}(z)$ of equation (12.16) D has error at most $\epsilon$ with probability at least $1-\delta$ : 

$$
M\geq\frac{3\ln(2/\delta)u^{k}}{P_{\mathcal{B}}(z)\epsilon^{2}}.
$$ 

Since $\mathcal{P}_{\mathcal{B}}(z)\geq\ell^{k}$ , a (stronger) sufcient condition is that: 

$$
M\geq\frac{3\ln(2/\delta)}{\epsilon^{2}}\left(\frac{u}{\ell}\right)^{k}.
$$ 

Chernof bound 

It is instructive to compare this bound to the one we obtain from the Chernof bound in equation (12.5). The bound in equation (12.18) makes a weaker assumption about the probability of the event $_z$ . Equation (12.5) requires that $P_{\mathcal{B}}(z)$ not be too low. By contrast, equation (12.17) assumes only that this probability is in a bounded range $\ell^{k},u^{k}$ ; the actual probability of the event $_z$ can still be very low — we have no guarantee on the actual magnitude of $\ell$ . Thus, for example, if our event $_z$ corresponds to a rare medical condition — one that has low probability given any instantiation of its parents — the estimator of equation (12.16) would give us a relative error bound, whereas standard sampling would not. 

We can use this bound to determine in advance the number of samples required for a certain desired accuracy. A disadvantage of this approach is that it does not take into consideration the speciﬁc samples we happened to generate during our sampling process. Intuitively, not all samples contribute equally to the quality of the estimate. A sample whose weight is high is more compatible with the evidence $e$ , and it arguably provides us with more information. Conversely, a low-weight sample is not as informative, and a data set that contains a large number of low-weight samples might not be representative and might lead to a poor estimate. A somewhat more sophisticated approach is to preselect not the number of particles, but a predeﬁned total weight. We then stop sampling when the total weight of the generated particles reaches our predeﬁned lower bound. 

![](images/498b93717f0e0e3c7e9759fc9196ea2b5e8e555476c719fa4984cf5f2bc5b6f8.jpg) 

data-dependent likelihood weighting 

Theorem 12.1 

expected sample size 

Theorem 12.2 

For this algorithm, we can provide a similar theoretical analysis with certain guarantees for this data-dependent likelihood weighting approach. Algorithm 12.3 shows an algorithm that uses a data-dependent stopping rule to terminate the sampling process when enough weight has been accumulated. We can show that: 

Data-Dependent-LW returns an estimate $\hat{p}$ for $P_{\mathcal{B}}(Z=z)$ which, with probability at least $1-\delta$ , has a relative error of ϵ . 

We can also place an upper bound on the expected sample size used by the algorithm: The expected number of samples used by Data-Dependent-LW is 

$\frac{u^{k}}{P_{\mathcal{B}}(z)}\gamma\leq\left(\frac{u}{\ell}\right)^{k}\gamma,$ where $\begin{array}{r}{\gamma=\frac{4(1+\epsilon)}{\epsilon^{2}}\ln\frac{2}{\delta}}\end{array}$ . 

The intuition behind this result is straightforward. The algorithm terminates when $W\geq\gamma u^{k}$ . The expected contribution of each sample is $E_{Q(\mathcal{X})}[w(\xi)]\,=\,P_{\mathcal{B}}(z)$ . Thus, the total number X B of samples required to achieve a total weight of $W\geq\gamma u^{k}$ is $M\geq\gamma u^{k}/P_{\mathcal{B}}(z)$ . Although this bound on the expected number of samples is no better than our bound in equation (12.17), the data-dependent bound allows us to stop early in cases where we were lucky in our random choice of samples, and to continue sampling in cases where we were unlucky. 

# 12.2.3.3 Ratio Likelihood Weighting 

ratio likelihood weighting 

We now move to the problem of computing a conditional probability $P(\pmb{y}\mid\pmb{e})$ for a speciﬁc event $_{_y}$ . One obvious approach is ratio likelihood weighting : we compute the conditional probability as $P(\pmb{y},e)/P(e)$ , and use unnormalized importance sampling (equation (12.16)) for both the numerator and denominator. 

We can therefore estimat nditional probability $P(\pmb{y}\mid\pmb{e})$ in two phases: We use the algorithm of algorithm 12.2 $12.2\;\;M$ times with the argument $Y~=~y,E~=~e$ , to gener- ate on et $\mathcal{D}$ of weighted samples $(\xi[1],w[1]),.\,.\,.\,,(\xi[M],w[M])$ . We use the same algo- rithm $M^{\prime}$ times with the argument $E=e$ , to generate another set ${\mathcal{D}}^{\prime}$ of weighted samples $(\xi^{\prime}[1],w^{\prime}[1]),.\,.\,.\,,(\xi^{\prime}[M^{\prime}],w^{\prime}[M^{\prime}])$ . We can then estimate: 

$$
\hat{P}_{\mathcal{D}}(\pmb{y}\mid e)=\frac{\hat{P}_{\mathcal{D}}(\pmb{y},e)}{\hat{P}_{\mathcal{D}^{\prime}}(e)}=\frac{1/M\sum_{m=1}^{M}w[m]}{1/M^{\prime}\sum_{m=1}^{M^{\prime}}w^{\prime}[m]}.
$$ 

In ratio LW, the numerator and denominator are both using unnormalized importance sam- pling, which admits a rigorous theoretical analysis. Thus, we can now provide bounds on the number of samples $M$ required to obtain a good estimate for both $P(\pmb{y},e)$ and $P(e)$ . 

# 12.2.3.4 Normalized Likelihood Weighting 

Ratio LW allows us to estimate the probability of a single query $P(\pmb{y}\mid\pmb{e})$ . In many cases, however, we are inter ted in estimating an entire joint distribution $P(Y\mid e)$ for some variable or subset of variables Y . We can answer such a query by running ratio LW for each $\pmb{y}\in V a l(\pmb{Y})$ , but this approach is typically too computationally expensive to be practical. 

normalized likelihood weighting 

An alternative approach is to use normalized likelihood weighting , which is based on the normalized importance sampling estimator of equation (12.13). In this application, our target distribution is $P(\mathcal{X})=\operatorname{P_{\mathcal{B}}}(\mathcal{X}\mid e)$ . As we mentioned, we do not have access to $P$ directly; rather, we can evaluate $\Tilde{P}(\mathcal{X})=P_{\mathcal{B}}(\mathcal{X},e)$ X B X , which is the probability of a full assignment and can be easily computed via the chain rule. In this case, we are trying to estimate the expectation of a function $f$ which is the indicator function of the query $_{_y}$ : $f(\xi)=I\!\!\left\{\xi\langle Y\rangle=y\right\}$ { ⟨ ⟩ } . Applying the normalized importance sampling estimator of equation (12.13) to this setting, we obtain precisely the estimator of equation (12.6). 

The quality of the importance sampling estimator depends largely on how close the proposal distribution $Q$ is to the target distribution $P$ . We can gain intuition for this question by considering two extreme cases. If all of the evidence in our network is at the roots, the proposal distribution is precisely the posterior, and there is no need to compensate; indeed, no evidence is encountered along the way, and all samples will have the same weight $P(e)$ . On the other side of the spectrum, if all of the evidence is at the leaves, our proposal distribution $Q(\mathcal X)$ is the prior distribution $P_{\mathcal{B}}(\mathcal{X})$ , leaving the correction purely to the weights. In this situation, LW will work reasonably only if the prior is similar to the posterior. Otherwise, most of our samples will be irrelevant, a fact that will be reﬂected by their low weight. For example, consider a medical-diagnosis setting, and assume that our evidence is a very unusual combination of symptoms generated by only one very rare disease. Most samples will not involve this disease and will give only very low probability to this combination of symptoms. Indeed, the combinations sampled are likely to be irrelevant and are not useful at all for understanding what disease the patient has. We return to this issue in section 12.2.4. 

To understand the relationship between the prior and the posterior, note that the prior is a weighted average of the posteriors, weighted over diferent instantiations of the evidence: 

$$
P(\mathcal{X})=\sum_{e}P(e)P(\mathcal{X}\mid e).
$$ 

If the evidence is very likely, then it is a major component in this summation, and it is probably not too far from the prior. For example, in the network $\mathcal{B}^{s t u d e n t}$ , the event $S=s^{1}$ is fairly likely, and the posterior distribution $P_{\mathcal{B}^{s t u d e n t}}(\mathcal{X}\mid s^{1})$ is fairly similar to the prior. However, for unlikely evidence, the weight of $P(\mathcal{X}\mid e)$ is negligible, and there is nothing constraining the posterior to be similar to the prior. Indeed, our distribution $P_{\mathcal{B}^{s t u d e n t}}(\mathcal{X}\mid l^{0})$ is very diferent from the prior. 

Unfortunately, there is currently no formal analysis for the number of particles required to achieve a certain quality of estimate using normalized importance sampling. In many cases, we simply preselect a number of particles that seems large enough, and we generate that number. Alternatively, we can use a heuristic approach that uses the total weight of the particles generated so far as guidance as to the extent to which they are representative. Thus, for example, we might decide to generate samples until a certain minimum bound on the total weight has been reached, as in Data-Dependent-LW . We note, however, that this approach is entirely heuristic in this case (as in all cases where we do not have bounds $[\ell,u]$ on our CPDs). Furthermore, there are cases where the evidence is simply unlikely in all conﬁgurations, and therefore all samples will have low weights. 

# 12.2.3.5 Conditional Probabilities: Comparison 

We have seen two variants of likelihood weighting: normalized LW and ratio LW. Ratio LW has two related advantages. The normalized LW process samples an assignment of the variables $Y$ (those not in $E$ ), whereas ratio LW simply sets the values of these variables. The additional sam- pling step for $Y$ introduces additional variance into the overall process, leading to a reduction in the robustness of the estimate. Thus, in many cases, the variance of this estimator is lower than that of equation (12.6), leading to more robust estimates. 

A second advantage of ratio LW is that it is much easier to analyze, and therefore it is associated with stronger guarantees regarding the number of samples required to get a good estimate. However, these bounds are useful only under very strong conditions: a small number of evidence variables, and a bound on the skew of the CPD entries in the network. 

On the other hand, a signiﬁcant disadvantage of ratio LW is the fact that each query $_{_y}$ requires that we generate a new set of samples for the event $\mathbfit{\Delta}_{\mathcal{Y},\mathbf{\Delta}e}$ . It is often the case that we want to evaluate the probability of multiple queries relative to the same set of evidence. The normalized LW approach allows these multiple computations to be executed relative to the same set of samples, whereas ratio LW requires a separate sample set for each query $_{_y}$ . This cost is particularly problematic when we are interested in computing the joint distribution over a subset of variables. Probably due to this last point, normalized LW is used more often in practice. 

# 12.2.4 Importance Sampling Revisited 

The likelihood weighting algorithm uses, as its proposal distribution, the very simple distribution obtained from mutilating the network by eliminating edges incoming to observed variables. However, this proposal distribution can be far from optimal. For example, if the CPDs associated with these evidence variables are skewed, the importance weights are likely to be quite large, resulting in estimators with high variance. Indeed, somewhat surprisingly, even in very simple cases, the obvious proposal distribution may not be optimal. For example, if $X$ is not a root node in the network, the optimal proposal distribution for computing $P(X=x)$ may not be the distribution $P$ , even without evidence! (See exercise 12.5.) 

The importance sampling framework is very general, however, and several other proposal distributions have been utilized. For example, backward importance sampling generates samples for parents of evidence variables using the likelihood of their children. Most simply, if $X$ is a variable whose child $Y$ is observed to be $Y=y$ , we might generate some samples for $X$ from renormalized distribution $Q(X)\propto P(Y=y\mid X)$ . We can continue this process, sampling $X$ ’s parents from the likelihood of $X$ ’s sampled value. We can also propose more complex schemes that sample the value of a variable given a combination of sampled or observed values for some of its parents and/or children. One can also consider hybrid approaches that use some global approximate inference algorithm (such as those in chapter 11) to construct a proposal distribution, which is then used as the basis for sampling. As long as the importance weights are computed correctly, we are guaranteed that this process is correct. (See exercise 12.7.) This process can lead to signiﬁcant improvements in theory, and it does lead to improvements in some cases in practice. 

# 12.3 Markov Chain Monte Carlo Methods 

One of the limitations of likelihood weighting is that an evidence node afects the sampling only for nodes that are its descendants. The efect on nodes that are nondescendants is accounted for only by the weights. As we discussed, in cases where much of the evidence is at the leaves of the network, we are essentially sampling from the prior distribution, which is often very far from the desired posterior. We now present an alternative sampling approach that generates a sequence of samples. This sequence is constructed so that, although the ﬁrst sample may be generated from the prior, successive samples are generated from distributions that provably get closer and closer to the desired posterior. We note that, unlike forward sampling methods (including likelihood weighting), Markov chain methods apply equally well to directed and to undirected models. Indeed, the algorithm is easier to present in the context of a distribution $P_{\Phi}$ deﬁned in terms of a general set of factors $\Phi$ . 

# 12.3.1 Gibbs Sampling Algorithm 

Gibbs sampling One idea for addressing the problem with forward sampling approaches is to try to “ﬁx” the sample we generated by resampling some of the variables we generated early in the process. Perhaps the simplest method for doing this is presented in algorithm 12.4. This method, called Gibbs sampling , starts out by generating a sample of the unobserved variables from some initial distribution; for example, we may use the mutilated network to generate a sample using forward sampling. Starting from that sample, we then iterate over each of the unobserved variables, sampling a new value for each variable given our current sample for all other variables. This process allows information to “ﬂow” across the network as we sample each variable. 

To apply this algorithm to a network with evidence, we ﬁrst reduce all of the factors by the observations $e$ , so that the distribution $P_{\Phi}$ used in the algorithm corresponds to $P(X\mid e)$ . 

![](images/dd3adf76b15bac1f140cad1dafdf2491e8d9c6b1f79be5ea5d4f8674692b39e2.jpg) 

Example 12.4 Let us revisit example 12.3, recalling that we have the observations $s^{1},l^{0}$ . In this case, our algo- rithm will generate samples over the variables $D,I,G$ . The set of reduced factors $\Phi$ is therefore: $P(I),P(D),P(G\mid I,D),P(s^{1}\mid I),P(l^{0}\mid G)$ . Our algorithm begins by generating one sam- ple, say by forward sampling. Assume that this sample is $d^{(0)}\,=\,d^{1},i^{(0)}\,=\,i^{0},g^{(0)}\,=\,g^{2}$ . In the ﬁrst iteration, it would now resample all of the unobserved variables, one at a time, in some predetermined order, say $G,I,D$ . Thus, we ﬁrst sample $g^{(1)}$ from the distribution $P_{\Phi}(G\mid d^{1},i^{0})$ . 

Note that because we are computing the distribution over a single variable given all the others, this computation can be performed very efciently: 

$$
\begin{array}{l l l}{{P_{\Phi}(G\mid d^{1},i^{0})}}&{{=}}&{{\displaystyle\frac{P(i^{0})P(d^{1})P(G\mid i^{0},d^{1})P(l^{0}\mid G)P(s^{1}\mid i^{0})}{\sum_{g}P(i^{0})P(d^{1})P(g\mid i^{0},d^{1})P(l^{0}\mid g)P(s^{1}\mid i^{0})}}}\\ {{}}&{{=}}&{{\displaystyle\frac{P(G\mid i^{0},d^{1})P(l^{0}\mid G)}{\sum_{g}P(g\mid i^{0},d^{1})P(l^{0}\mid g)}.}}\end{array}
$$ 

Thus, we can compute the distribution simply by multiplying all factors that contain $G$ , with all other variables instantiated, and renormalizing to obtain a distribution over $G$ . 

Having sampled $g^{(1)}\,=\,g^{3}$ , w ntinue to resampling $i^{(1)}$ from he distribution $P_{\Phi}(I\mid$ $d^{1},g^{3})$ , obtaining, for example, i $i^{(1)}\,=\,i^{1}$ ; note that the distribution for I is conditioned on the newly sampled value $g^{(1)}$ . Finally, we sample $d^{(1)}$ from $P_{\Phi}(D\mid g^{3},i^{1})$ , obtaining $d^{1}$ . The result of the ﬁrst iteration of sampling is, then, the sample $(i^{1},d^{1},g^{3})$ . The process now repeats. 

Note that, unlike forward sampling, the sampling process for $G$ takes into consideration the downstream evidence at its child $L$ . Thus, its sampling distribution is arguably closer to the posterior. Of course, it is not the true posterior, since it still conditions on the originally sampled values for $I,D$ , which were sampled from the prior distribution. However, we now resample $I$ and $D$ from a distribution that conditions on the new value of $G$ , so one can imagine that their sampling distribution may also be closer to the posterior. Thus, perhaps the next sample of $G$ , 

![](images/5adb2737d927876e6023122408bf91349cd8b79c0d5e39a968adfcab0853ab73.jpg) 
Figure 12.3 The Grasshopper Markov chain 

which uses these new values for $I,D$ (and conditions on the evidence $l^{0}$ ), will be sampled from a distribution even closer to the posterior. Indeed, this intuition is correct. One can show that, as we repeat this sampling process, the distribution from which we generate each sample gets closer and closer to the posterior $P_{\Phi}(X)=P(X\mid e)$ . 

Markov chain Monte Carlo 

In the subsequent sections, we formalize this intuitive argument using a framework called Markov chain Monte Carlo (MCMC) . This framework provides a general approach for generating samples from the posterior distribution, in cases where we cannot efciently sample from the posterior directly. In MCMC, we construct an iterative process that gradually samples from distributions that are closer and closer to the posterior. A key question is, of course, how many iterations we should perform before we can collect a sample as being (almost) generated from the posterior. In the following discussion, we provide the formal foundations for MCMC algorithms, and we try to address this and other important questions. We also present several valuable generalizations. 

# 12.3.2 Markov Chains 

# 12.3.2.1 Basic Deﬁnition 

At a high level, a Markov chain is deﬁned in terms of a graph of states over which the sampling algorithm takes a random walk. In the case of graphical models, this graph is not the original graph, but rather a graph whose nodes are the possible assignments to our variables $X$ . 

Deﬁnition 12.2 Markov chain transition model 

homogeneous Markov chain 

Example 12.5 

A Markov chain is deﬁned via a state space $V a l(X)$ and a model that deﬁnes, for every state $\textbf{\em x}\in{\mathrm{\em V a l}}(X)$ a next-state dis ion over $V a l(X)$ . More precisely, the transition del $\mathcal{T}$ speciﬁes for each pair of state ${\boldsymbol{x}},{\boldsymbol{x}}^{\prime}$ the probability $\tau(\pmb{x}\ \rightarrow\ \pmb{x}^{\prime})$ of going from $_{_{x}}$ to x $\scriptstyle{\boldsymbol{x}}^{\prime}$ . This transition probability applies whenever the chain is in state $_{_{x}}$ . We note that, in this deﬁnition and in the subsequent discussion, we restrict attention to homogeneous , where the system dynamics do not change over time. We illustrate this concept with a simple example. 

remains in position $+4$ . Thus, for example, $\mathcal{T}(+4\rightarrow+4)=0.75$ . We can visualize the state space as a graph, with probability-weighted directed edges corresponding to transitions between diferent states. The graph for our example is shown in ﬁgure 12.3. 

We can imagine a random sampling process, that deﬁnes a random sequence of states ${\pmb x}^{(0)},{\pmb x}^{(1)},{\pmb x}^{(2)},\breve{~}.\cdot\cdot$ . Because the transition model is random, the state of the process at step $t$ can be viewed as a random variable $X^{(t)}$ . We assume that the initial state $X^{{\bar{(0)}}}$ is distributed according to some initial state distribution $P^{(0)}(X^{(0)})$ . We can now deﬁne distributions over the subsequent states $P^{(1)}(X^{(1)}),P^{(2)}(X^{(2)}),.\,.\,.$ using the chain dynamics: 

$$
P^{(t+1)}(\pmb{X}^{(t+1)}=\pmb{x}^{\prime})=\sum_{\pmb{x}\in V a l(\pmb{X})}P^{(t)}(\pmb{X}^{(t)}=\pmb{x})\pmb{\mathcal{T}}(\pmb{x}\rightarrow\pmb{x}^{\prime}).
$$ 

Intuitively, the probability of being at state $\scriptstyle{\boldsymbol{x}}^{\prime}$ at time $t+1$ is the sum over all possible states $_{_{x}}$ that the chain could have been in at time $t$ of the probability being in state $_{_{x}}$ times the probability that the chain took a transition from $_{_{x}}$ to $\scriptstyle{\boldsymbol{x}}^{\prime}$ . 

# 12.3.2.2 Asymptotic Behavior 

For our purposes, the most important aspect of a Markov chain is its long-term behavior. 

# Example 12.6 

Because the grasshopper’s motion is random, we can consider its location at time $t$ to be a random variable, which we denote $X^{(t)}$ . Consider the distribution over $X^{(t)}$ . Initially, the grasshopper is at 0 , so that $P(X^{(0)}\,=\,0)\,=\,1$ . At time 1 , we have that $X^{(1)}$ is 0 with probability 0 . 5 , $+1$ $-1$ h probabil 0 . 25 . At time 2 , we have that $X^{(2)}$ is 0 with pr bility

 $0.5^{2}+2\cdot0.25^{2}=0.375$ · $+1$ $-1$ ch with probability $2(0.5\cdot0.25)=0.25$ , and +2 and

 $-2$ − each with probability $0.25^{2}\,=\,0.0625$ . As the proces nues, the probability gets spread out over more and more of the states. For example, at time t $t=10$ , the probabilities of the diferent states range from 0 . 1762 for the value 0 , and 0 . 0518 for the values $\pm4$ . At $t=50$ , the distribution is almost uniform, with a range of 0 . 1107 – 0 . 1116 . 

Thus, one approach for sampling from the uniform distribution over the set $-4,\cdot\cdot\cdot,+4$ is to start of at 0 and then randomly choose the next state from the transition model for this chain. After some number of such steps $t$ , our state $X^{(t)}$ would be sampled from a distribution that is very close to uniform over this space. We note that this approach is not a very good one for sampling from a uniform distribution; indeed, the expected time required for such a chain even to reach the boundaries of the interval $[-K,K]$ is $K^{2}$ steps. However, this general approach applies much more broadly, including in cases where our “long-term” distribution is not one from which we can easily sample. 

MCMC sampling 

Markov chain Monte carlo (MCMC) sampling is a process that mirrors the dynamics of the Markov chain; the process of generating an MCMC trajectory is shown in algorithm 12.5. The sample $\mathbfit{x}^{(t)}$ is drawn from the distribution $P^{(t)}$ . We are interested in the limit of this process, that is, whether $P^{(t)}$ converges, and if so, to what limit. 

![](images/1892b71641e5ed3d58edd2b340c444b88724612eee3906ae59df4605838a09f6.jpg) 

![](images/58659338645f1a761413f7c694d5fda029e5024f4e90722c57f71ea4919d61fb.jpg) 
Figure 12.4 A simple Markov chain 

# 12.3.2.3 Stationary Distributions 

Intuitively, as the process converges, we would expect $P^{(t+1)}$ to be close to $P^{(t)}$ . Using equation (12.20), we obtain: 

$$
P^{(t)}(\pmb{x}^{\prime})\approx P^{(t+1)}(\pmb{x}^{\prime})=\sum_{\pmb{x}\in V a l(\pmb{X})}P^{(t)}(\pmb{x})\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime}).
$$ 

At convergence, we would expect the resulting distribution $\pi(X)$ to be an equilibrium relative to the transition model; that is, the probability of being in a state is the same as the probability of transitioning into it from a randomly sampled predecessor. Formally: 

# Deﬁnition 12.3 

stationary distribution A distribution $\pi(X)$ is $a$ stationary distribution for a Markov chain $\mathcal{T}$ if it satisﬁes: 

$$
\pi(X=x^{\prime})=\sum_{x\in V a l(X)}\pi(X=x)\mathcal{T}(x\to x^{\prime}).
$$ 

A stationary distribution is also called an invariant distribution . 

As we have already discussed, the uniform distribution is a stationary distribution for the Markov chain of example 12.5. To take a slightly diferent example: 

# Example 12.7 

Figure 12.4 shows an example of a diferent simple Markov chain where the transition probabilities are less uniform. By deﬁnition, the stationary distribution $\pi$ must satisfy the following three equations: 

$$
\begin{array}{l c l}{{\pi(x^{1})}}&{{=}}&{{0.25\pi(x^{1})+0.5\pi(x^{3})}}\\ {{\pi(x^{2})}}&{{=}}&{{0.7\pi(x^{2})+0.5\pi(x^{3})}}\\ {{\pi(x^{3})}}&{{=}}&{{0.75\pi(x^{1})+0.3\pi(x^{2}),}}\end{array}
$$ 

as well as the one asserting that it is a legal distribution: 

$$
\pi(x^{1})+\pi(x^{2})+\pi(x^{3})=1.
$$ 

$I t$ is straightforward to verify that this system has a unique solution: $\pi(x^{1})=0.2$ , $\pi(x^{2})=0.5$ , $\pi(x^{3})=0.3$ . For example, the ﬁrst equation asserts that 

$$
0.2=0.25\cdot0.2+0.5\cdot0.3,
$$ 

which clearly holds. 

In general, there is no guarantee that our MCMC sampling process converges to a stationary distribution. 

# Example 12.8 

periodic Markov chain 

reducible Markovchain 

ergodic Markov chain 

Deﬁnition 12.4 regular Markov chain 

Consider th rkov chain ov $x^{1}$ and $x^{2}$ , suc that $\tau(x^{1}\rightarrow x^{2})=1$ and $\mathcal{T}(x^{2}\rightarrow$ $x^{1})=1$ . If P $P^{(0)}$ is such that $P^{(0)}(x^{1})=1$ , then the step t distribution $P^{(t)}$ has $P^{(t)}(x^{1})=1$ if $t$ is even, and $P^{(t)}(x^{2})=1$ if $t$ is odd. Thus, there is no convergence to a stationary distribution. 

Markov chains such as this, which exhibit a ﬁxed cyclic behavior, are called periodic Markov chains . 

There is also no guarantee that the stationary distribution is unique: In some chains, the stationary distribution reached depends on our starting distribution $\bar{P}^{(0)}$ . Situations like this occur when the chain has several distinct regions that are not reachable from each other. Chains such as this are called reducible Markov chains . 

We wish to restrict attention to Markov chains that have a unique stationary distribution, which is reached from any starting distribution $P^{(0)}$ . There are various conditions that sufce to guarantee this property. The condition most commonly used is a fairly technical one: that the chain be ergodic . In the context of Markov chains where the state space $V a l(X)$ is ﬁnite, the following condition is equivalent to this requirement: 

In our Markov chain of example 12.5, the probability of getting from any state to any state in exactly 9 steps is greater than 0. Thus, this Markov chain is regular. Similarly, in the Markov chain of example 12.7, we can get from any state to any state in exactly two steps. The following result can be shown to hold: 

# Theorem 12.3 

Ensuring regularity is usually straightforward. Two simple conditions that together guarantee regularity in ﬁnite-state Markov chains are as follows. First, it is possible to get from any state to any state using a positive probability path in the state graph. Second, for each state $_{_{x}}$ , there is a positive probability of transitioning from $_{_{x}}$ to $_{_{x}}$ in one step (a self-loop). These two conditions together are sufcient but not necessary to guarantee regularity (see exercise 12.12). However, they often hold in the chains used in practice. 

# 12.3.2.4 Multiple Transition Models 

In the case of graphical models, our state space has a factorized structure — each state is an assignment to several variables. When deﬁning a transition model over this state space, we can consider a fully general case, where a transition can go from any state to any state. However, it is often convenient to decompose the transition model, considering transitions that update only a single component of the state vector at a time, that is, only a value for a single variable. 

# Example 12.9 

kernel 

multi-kernel Markov chain Consider an extension to our Grasshopper chain, where the grasshopper lives, not on a line, but in a two-dimensional plane. In this case, the state of the system is deﬁned via a pair of random variables $X,Y$ . Although we could deﬁne a joint transition model over both dimensions simultaneously, it might be easier to have separate transition models for the $X$ and $Y$ coordinate. 

In this case, as in several other settings, we often deﬁne a set of transition models, each with its own dynamics. Each such transition model $\mathcal{T}_{i}$ is called a kernel . In certain cases, the diferent kernels are necessary, because no single kernel on its own sufces to ensure regularity. This is the case in example 12.9. In other cases, having multiple kernels simply makes the state space more “connected” and therefore speeds the convergence to a stationary distribution. 

There are several ways of constructing a single Markov chain from multiple kernels . One com- mon approach is simply to select randomly between them at each step, using any distribution. Thus, for example, at each step, we might select one of $\tau_{1},\dots,\tau_{k}$ , each with probability $1/k$ . Alternatively, we can simply cycle over the diferent kernels, taking each one in turn. Clearly, this approach does not deﬁne a homogeneous chain, since the kernel used in step $i$ is diferent from the one used in step $i+1$ . However, we can simply view the process as deﬁning a single chain $\mathcal{T}$ , each of whose steps is an aggregate step, consisting of ﬁrst taking $\mathcal{T}_{1}$ , then $\tau_{2},\,.\,.\,,$ , through $\mathcal{T}_{k}$ . 

In the case of graphical models, one approach is to deﬁne a multikernel chain, where we have a el $\mathcal{T}_{i}$ for eac ariable $X_{i}\in X$ $X_{-i}=\mathcal{X}-\{X_{i}\}$ , and let $\mathbf{\Delta}x_{i}$ denote an tion to $X_{i}$ . The model T $\mathcal{T}_{i}$ takes a state ( $({\boldsymbol{x}}_{-i},{\boldsymbol{x}}_{i})$ and transitions to a state of the form ( $(\pmb{x}_{-i},x_{i}^{\prime})$ . As − − we discussed, we can combine the diferent kernels into a single global model in various ways. 

Regardless of the structure of the diferent kernels, we can prove that a distribution is a stationary distribution for the multiple kernel chain by proving that it is a stationary dis- tribution (satisﬁes equation (12.21)) for each of individual kernels $\mathcal{T}_{i}$ . Note that each kernel by itself is generally not ergodic; but as long as each kernel satisﬁes certain conditions (speciﬁed in deﬁnition 12.5) that imply that it has the desired stationary distribution, we can combine them to produce a coherent chain, which may be ergodic as a whole. This 

# ability to add new types of transitions to our chain is an important asset in dealing with the issue of local maxima, as we will discuss. 

# 12.3.3 Gibbs Sampling Revisited 

The theory of Markov chains provides a general framework for generating samples from a target distribution $\pi$ . In this section, we discuss the application of this framework to the sampling tasks encountered in probabilistic graphical models. In this case, we typically wish to generate samples from the posterior n $P(X\mid E=e)$ , where $X=\mathcal{X}-E$ . Thus, we wish to deﬁne a chain for which $P(X\mid e)$ | is e st y distribution. Thus, we deﬁne the states of the Markov chain to be instantiations x to X − $\mathcal{X}-E$ . In order to deﬁne a Markov chain, we need to deﬁne a process that transitions from one state to the other, converging to a stationary distribution $\pi(X)$ , which terior distribution $P(X\mid e)$ . As in our earlier example, we assume that P $P(X\mid e)\;=\;P_{\Phi}$ | for some set of factor $\Phi$ that are deﬁned by reducing the original factors in our graphical model by the evidence e . This reduction allows us to simplify notation and to discuss the methods in a way that applies both to directed and undirected graphical models. 

Gibbs chain 

Gibbs stationary distribution 

Markov blanket 

Gibbs sampling is based on one yet efective Markov chain for factored state spaces, which is particularly efcient for gr hical models. We deﬁne the kernel $\mathcal{T}_{i}$ as follows. In tively, we simply “forget” the value of $X_{i}$ in the current state and sample a new value for $X_{i}$ from its posterior given the rest of the current state. More precisely, let $({\boldsymbol{x}}_{-i},{\boldsymbol{x}}_{i})$ be a state in the chain. We deﬁne: 

$$
\begin{array}{r}{\mathcal{T}_{i}((\pmb{x}_{-i},x_{i})\rightarrow(\pmb{x}_{-i},x_{i}^{\prime}))=P(x_{i}^{\prime}\mid\pmb{x}_{-i}).}\end{array}
$$ 

Note that the transition probability does not depend on the current value $x_{i}$ of $X_{i}$ , but only on the remaining state ${\pmb x}_{-i}$ . It is not difcult to show that the posterior distribution $P_{\Phi}(X)=$ $P(\mathcal{X}\mid e)$ is a stationary distribution of this process. (See exercise 12.13.) 

The sampling algorithm for a single trajectory of the Gibbs chain was shown earlier in this section, in algorithm 12.4. Recall that the Gibbs chain is deﬁned via a set of kernels; we use the multistep approach to combine them. Thus, the diferent local kernels are taken consecutively; having changed the value for a variable $X_{1}$ , the value for $X_{2}$ is sampled based on the new value. Note that a step in the aggregate chain occurs only once we have executed every local transition once. 

Gibbs sampling is particularly easy to implement in the many graphical models where we can compute the transition probability $P(X_{i}\mid\pmb{x}_{-i})$ (in line 5 of the algorithm) very efciently. In particular, as we now show, this distribution can be done based only on the Markov blanket of $X_{i}$ . We show this analysis for a Markov network; the application to Bayesian networks is straightforward. Recalling deﬁnition 4.4, we have that: 

$$
\begin{array}{r c l}{P_{\Phi}(X)}&{=}&{\displaystyle\frac{1}{Z}\prod_{j}\phi_{j}(D_{j})}\\ &{=}&{\displaystyle\frac{1}{Z}\prod_{j\ :\ X_{i}\in D_{j}}\phi_{j}(D_{j})\prod_{j\ :\ X_{i}\not\in D_{j}}\phi_{j}(D_{j}).}\end{array}
$$ 

Let $\mathbf{\Delta}x_{j,-i}$ denote the assignment in ${\pmb x}_{-i}$ to $D_{j}-\{X_{i}\}$ , noting that when $X_{i}\notin D_{j}$ , $\mathbf{\Delta}x_{j,-i}$ is a full assignment to $D_{j}$ . We can now derive: 

$$
\begin{array}{r c l}{P(x_{i}^{\prime}\mid x_{-i})}&{=}&{\frac{P(x_{i}^{\prime},x_{-i})}{\sum_{x_{i}^{\prime\prime}}P(x_{i}^{\prime},x_{-i})}}\\ &{=}&{\frac{\frac{1}{Z}\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime},x_{j-i})\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime},x_{j-i})}{\frac{1}{Z}\sum_{x_{i}^{\prime\prime}}\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime\prime},x_{j-i})\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime\prime},x_{j-i})}}\\ &{=}&{\frac{\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime},x_{j-i})\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{j-i})}{\sum_{x_{i}^{\prime\prime}}\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime\prime},x_{j-i})\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{j-i})}}\\ &{=}&{\frac{\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime},x_{j-i})}{\sum_{x_{i}^{\prime\prime}}\prod_{D_{j}\ni X_{i}}\phi_{j}(x_{i}^{\prime\prime},x_{j-i})}.}\end{array}
$$ 

This last expression uses only the factors involving $X_{i}$ , and depends only on the instantiation in ${\pmb x}_{-i}$ of $X_{i}$ ’s Markov blanket. In the case of Bayesian networks, this expression reduces to a formula involving only the CPDs of $X_{i}$ and its children, and its value, again, depends only on the assignment in ${\pmb x}_{-i}$ to the Markov blanket of $X_{i}$ . 

Consider again the Student network of ﬁgure 12.1, with the evidence $s^{1},l^{0}$ . The kernel for the $G$ en a state $(i,d,g,s^{1},l^{0})$ , we deﬁne ${\mathcal{T}}((i,g,d,s^{1},l^{0})\ \rightarrow$ $(i,g^{\prime},d,s^{1},l^{0}))\,=\,P(g^{\prime}\mid i,d,s^{1},l^{0})$ | . Th value can be computed locally, using only the CPDs that involve G , that is, the CPDs of G and L : 

$$
P(g^{\prime}\mid i,d,s^{1},l^{0})=\frac{P(g^{\prime}\mid i,d)P(l^{0}\mid g^{\prime})}{\sum_{g^{\prime\prime}}P(g^{\prime\prime}\mid i,d)P(l^{0}\mid g^{\prime\prime})}.
$$ 

Similarly, the kernel for the variable $I$ is deﬁned to be ${\mathcal{T}}((i,g,d,s^{1},l^{0})\,\rightarrow\,(i^{\prime},g,d,s^{1},l^{0}))\,=$ $P(i^{\prime}\mid g,d,s^{1},l^{0})$ , which simpliﬁes as follows: 

$$
P(i^{\prime}\mid g,d,s^{1},l^{0})=\frac{P(i^{\prime})P(g\mid i^{\prime},d)P(s^{1}\mid i^{\prime})}{\sum_{i^{\prime\prime}}P(i^{\prime\prime})P(g\mid i^{\prime\prime},d)P(s^{1}\mid i^{\prime\prime})}.
$$ 

block Gibbs sampling 

As presented, the algorithm is deﬁned via a sequence of local kernels, where each samples a single variable conditioned on all the rest. The reason for this approach is computational. As we showed, we can easily compute the transition model for a single variable given the rest. However, there are cases where we can simultaneously sample several variables efciently. Speciﬁcally, assume we can partition the variables $X$ into several disjoint blocks of variables $X_{1},\dots,X_{k}$ , such that we can efciently sample $\mathbf{\Delta}x_{i}$ from $P_{\Phi}(X_{i}\mid\mathbf{\mathcal{x}}_{1},.\,.\,,\mathbf{\mathcal{x}}_{i-1},\mathbf{\mathcal{x}}_{i+1},.\,.\,.\,,\mathbf{\mathcal{x}}_{k})$ . In this case, we can modify our Gibbs sampling algorithm to iteratively sample blocks of variables, rather than individual variables, thereby taking much “longer-range” transitions in the state space in a single sampling step. Here, like in Gibbs sampling, we deﬁne the algorithm to be producing a new sample only once all blocks have been resampled. This algorithm is called block Gibbs . Note that standard Gibbs sampling is a special case of block Gibbs sampling, with the blocks corresponding to individual variables. 

![](images/b05c089544fc883b925f38ca6be1f2f3b1f21684a9e36fd5428ee5b845079227.jpg) 
Figure 12.5 A Bayesian network with four students, two courses, and ﬁve grades 

with a variable representing its difculty. We also have a set of grades for students in classes (not necessarily $^a$ grade for each student in every class). Using an abbreviated notation, we have $^a$ set of variables $I_{1},\ldots,I_{n}$ for the students (where each $I_{j}=I(s_{j}))$ ), $D=\{D_{1},.\,.\,.\,,D_{\ell}\}$ for the courses, and $G=\{G_{j,k}\}$ for th s, wh h variable $G_{j,k}$ has the parents $I_{j}$ and $D_{k}$ . See ﬁgure 12.5 for an example with n $n=4$ and ℓ $\ell=2$ . Let us assume that we observe the grades, so that we have evidence $G=g$ . An examination of active paths shows that the diferent variables $I_{j}$ are conditionally independent given an assignment $^d$ to $_D$ . Thus, given $D=d,G=g$ , we can efciently sample all of the $\boldsymbol{I}$ variables as a block by sampling each $I_{j}$ independently of the others. Similarly, we can sample all of the $_D$ variables as a block given an assignment $I=i,G=g$ . Thus, we can alternate steps where in one we sample $\pmb{i}[m]$ given $_{g}$ and $d[m]$ , and in the other we sample $d[m+1]$ given $_{g}$ and $\pmb{i}[m]$ . 

In this example, we can easily apply block Gibbs because the variables in each block are marginally independent given the variables outside the block. This independence property allows us to compute efciently the conditional distribution $P_{\Phi}(X_{i}\mid\mathbf{\mathcal{x}}_{1},.\,.\,,\mathbf{\mathcal{x}}_{i-1},\mathbf{\mathcal{x}}_{i+1},.\,.\,.\,,\mathbf{\mathcal{x}}_{k})$ , and to sample from it. Importantly, however, full independence is not essential: we need only have the property that the block-conditional distribution can be efciently manipulated. For example, in a grid-structured network, we can easily deﬁne our blocks to consist of separate rows or of separate columns. In this case, the structure of each block is a simple chain-structured network; we can easily compute the conditional distribution of one row given all the others, and sample from it (see exercise 12.3). 

We note that the Gibbs chain is not necessarily regular, and might not converge to a unique stationary distribution. 

Consider a sim e net rk that consists of a single $\nu$ -structur $X\rightarrow Z\leftarrow Y$ , where the variables are all binary, X and $Y$ are both uniformly distributed, and Z is the deterministic exclusive or of $X$ and $Y$ (t is, $Z=z^{1}$ if $X\neq Y.$ ). Consider applying Gibbs sampling to thi with the evidence z $z^{1}$ . The true posterior assigns probability $1/2$ to each of the two states $x^{1},y^{0},z^{1}$ and $x^{0},y^{1},z^{1}$ . Assu that we start the ﬁrst of these two states. In is case, $P(X\mid y^{0},z^{1})$ assig probability 1 to x $x^{1}$ , so that the X transition leaves the value of X unchanged. Similarly, the Y transition leaves the value of $Y$ unchanged. Therefore, the chain will simply stay at the initial state forever, and it will never sample from the other state. The analogous phenomenon occurs for the other starting state. This chain is an example of a reducible Markov chain. 

However, this chain is guaranteed to be regular whenever the distribution is positive, so that every value of $X_{i}$ has positive probability given an assignment ${\pmb x}_{-i}$ to the remaining variables. 

# Theorem 12.4 

mixing Let H be a Markov network such that all of the clique potentials are strictly positive. Then the Gibbs-sampling Markov chain is regular. 

The proof is not difcult, and is left as an exercise (exercise 12.20). Positivity is, however, not necessary; there are many examples of nonpositive distributions where the Gibbs chain is regular. Importantly, however, even chains that are regular may require a long time to mix , that is, get close to the stationary distribution. In this case, instances generated from early in the sampling process will not be representative of the desired stationary distribution. 

# 12.3.4 A Broader Class of Markov Chains $\star$ 

As we discussed, the use of MCMC methods relies on the construction of a Markov chain that has the desired properties: regularity, and the target stationary distribution. In the previous section, we described the Gibbs chain, a simple Markov chain that is guaranteed to have these properties under certain assumptions. However, Gibbs sampling is applicable only in certain circumstances; in particular, we must be able to sample from the distribution $P(X_{i}\mid\mathbf{\sigma}\mathbf{x}_{-i})$ . Although this sampling step is easy for discrete graphical models, in continuous models, the conditional distribution may not be one that has a parametric form that allows sampling, so that Gibbs is not applicable. 

Even more important, the Gibbs chain uses only very local moves over the state space: moves that change one variable at a time. In models where variables are tightly cor- related, such moves often lead from states whose probability is high to states whose probability is very low. In this case, the high-probability states will form strong basins of attraction, and the chain will be very unlikely to move away from such a state; that is, the chain will mix very slowly. In this case, we often want to consider chains that allow a broader range of moves, including much larger steps in the space. The framework we develop in this section allows us to construct a broad family of chains in a way that guarantees the desired stationary distribution. 

12.3.4.1 Detailed Balance 

Before we address the question of how to construct a Markov chain with a particular stationary distribution, we address the question of how to verify easily that our Markov chain has the desired stationary distribution. Fortunately, we can deﬁne a test that is local and easy to check, and that sufces to characterize the stationary distribution. As we will see, this test also provides us with a simple method for constructing an appropriate chain. 

# Deﬁnition 12.5 

reversible Markov chain 

A ﬁnite-state Markov chain $\mathcal{T}$ is reversible if there exists a unique distribution $\pi$ such that, for all $x,x^{\prime}\in V a l(X)$ : 

$$
\pi(x){\mathcal{T}}(x\to x^{\prime})=\pi(x^{\prime}){\mathcal{T}}(x^{\prime}\to x).
$$ 

detailed balance This equation is called the detailed balance . 

The product $\pi({\pmb x})\mathcal{T}({\pmb x}\,\rightarrow\,{\pmb x}^{\prime})$ represents a process where we pick a starting state at random according to $\pi$ , and then take a random transition from the chosen state according to the transition model. The detailed balance equation asserts that, using this process, the probability of a transition from $_{_{x}}$ to $\scriptstyle{\boldsymbol{x}}^{\prime}$ is the same as the probability of a transition for $\scriptstyle{\boldsymbol{x}}^{\prime}$ to $_{_{x}}$ . 

Reversibility implies that $\pi$ is a stationary distri ion of $\mathcal{T}$ , but not necessarily that the chain will converge to $\pi$ (see example 12.8). However, if T $\mathcal{T}$ is regular, then convergence is guaranteed, and the reversibility condition provides a simple characterization of its stationary distribution: 

If $\mathcal{T}$ is regular and it sa ﬁes the detailed balance equation relative to $\pi$ , then $\pi$ is the unique stationary distribution of T . 

The proof is left as an exercise (exercise 12.14). 

# Example 12.13 

We can test this proposition on the Markov chain of ﬁgure 12.4. Our detailed balance equation for the two states $x^{1}$ and $x^{3}$ asserts that 

$$
\pi(x^{1}){\mathcal{T}}(x^{1}\rightarrow x^{3})=\pi(x^{3}){\mathcal{T}}(x^{3}\rightarrow x^{1}).
$$ 

Testing this equation for the stationary distribution $\pi$ described in example 12.7, we have: 

$$
0.2\cdot0.75=0.3\cdot0.5=0.15.
$$ 

The detailed balance equation can also be applied to multiple kernels. If each kernel $\mathcal{T}_{i}$ satisﬁes the detailed balance equation relative to some stationary distribution $\pi$ , then so does the mi ure transition model $\mathcal{T}$ (see exercise 12.16). The application to the multistep transition model $\mathcal{T}$ is also possible, but requires some care (see exercise 12.17). 

# 12.3.4.2 Metropolis-Hastings Algorithm 

Metropolis- Hastings algorithm 

proposal distribution The reversibility condition gives us a condition for verifying that our Markov chain has the desired stationary distribution. However, it does not provide us with a constructive approach for producing such a Markov chain. The Metropolis-Hastings algorithm is a general construction that allows us to build a reversible Markov chain with a particular stationary distribution. 

Unlike the Gibbs chain, the algorithm does not assume that we can generate next-state samples from a particular target distribution. Rather, it uses the idea of a proposal distribution that we have already seen in the case of importance sampling. 

As for importance sampling, the proposal distribution in the Metropolis-Hastings algorithm is intended to deal with cases where we cannot sample directly from a desired distribution. In the case of a Markov chain, the target distribution is our next-state sampling distribution at a given state. We would like to deal with cases where we cannot sample directly from this target. Therefore, we sample from a diferent distribution — the proposal distribution — and then correct for the resulting error. However, unlike importance sampling, we do not want to keep track of importance weights, which are going to decay exponentially with the number of transitions, leading to a whole slew of problems. Therefore, we instead randomly choose whether to accept the proposed transition, with a probability that corrects for the discrepancy between the proposal distribution and the target. 

More precise proposal distribution $\mathcal{T}^{Q}$ deﬁnes a transition model over our state space: For each state x , T $\mathcal{T}^{Q}$ deﬁnes a distribution over possible successor states in $V a l(X)$ , from acceptance probability which we select randomly a candidate next state $\scriptstyle{\boldsymbol{x}}^{\prime}$ . We can either accept the proposal and transition to $\scriptstyle{\boldsymbol{x}}^{\prime}$ , or reject it and stay at $_{_{x}}$ . Thus, for each pair of states ${\boldsymbol{x}},{\boldsymbol{x}}^{\prime}$ we have an acceptance probability $\mathcal{A}(\pmb{x}\rightarrow\pmb{x}^{\prime})$ . The actual transition model of the Markov chain is then: 

$$
\begin{array}{r c l}{{\mathcal{T}(x\to x^{\prime})}}&{{=}}&{{\mathcal{T}^{Q}(x\to x^{\prime})\mathcal{A}(x\to x^{\prime})\qquad x\neq x^{\prime}}}\\ {{\mathcal{T}(x\to x)}}&{{=}}&{{\mathcal{T}^{Q}(x\to x)+\sum_{x^{\prime}\neq x}\mathcal{T}^{Q}(x\to x^{\prime})(1-\mathcal{A}(x\to x^{\prime})).}}\end{array}
$$ 

By using a proposal distribution, we allow the Metropolis-Hastings algorithm to be applied even in cases where we cannot directly sample from the desired next-state distribution; for example, where the distribution in equation (12.22) is too complex to represent. The choice of proposal distribution can be arbitrary, so long as it induces a regular chain. One simple choice in discrete factored state spaces is to use a multiple transition model, where $\mathcal{T}_{i}^{Q}$ is a uniform distribution over the values of the variable $X_{i}$ . 

Given a proposal distribution, we can use the detailed balance equation to select the accep- tance probabilities so as to obtain the desired stationary distribution. For this Markov chain, the detailed balance equations assert that, for all $\mathbf{\nabla}x\neq\mathbf{\nabla}x^{\prime}$ , 

$$
\pi(x)\mathcal{T}^{Q}(x\to x^{\prime})\mathcal{A}(x\to x^{\prime})=\pi(x^{\prime})\mathcal{T}^{Q}(x^{\prime}\to x)\mathcal{A}(x^{\prime}\to x).
$$ 

We can verify that the following acceptance probabilities satisfy these equations: 

$$
\mathcal{A}(x\rightarrow x^{\prime})=\operatorname*{min}\left[1,\frac{\pi(x^{\prime})\mathcal{T}^{Q}(x^{\prime}\rightarrow x)}{\pi(x)\mathcal{T}^{Q}(x\rightarrow x^{\prime})}\right],
$$ 

and hence that the chain has the desired stationary distribution: 

Theorem 12.5 Let $\mathcal{T}^{Q}$ be any proposal distribution, and consider the Markov chain deﬁned by equation (12.25) and equation (12.26). If this Markov chain is regular, then it has the stationary distribution $\pi$ . 

The proof is not difcult, and is left as an exercise (exercise 12.15). Let us see how this construction process works. 

Example 12.14 Assume that our proposal distribution $\mathcal{T}^{Q}$ is given by the chain of ﬁgure 12.4, but that we want to sample from a stationary distribution π $\pi^{\prime}$ where: $\pi^{\prime}(x^{1})=0.6$ , $\pi^{\prime}(x^{2})=0.3,$ , and $\pi^{\prime}(x^{3})=0.1$ . To deﬁne the chain, we need to compute the acceptance probabilities. Applying equation (12.26), we obtain, for example, that: 

$$
\begin{array}{l l l}{{A(x^{1}\to x^{3})}}&{{=}}&{{\displaystyle\operatorname*{min}\left[1,\frac{\pi^{\prime}(x^{3})\mathcal{T}^{Q}(x^{3}\to x^{1})}{\pi^{\prime}(x^{1})\mathcal{T}^{Q}(x^{1}\to x^{3})}\right]=\operatorname*{min}\left[1,\frac{0.1\cdot0.5}{0.6\cdot0.75}\right]=0.11}}\\ {{A(x^{3}\to x^{1})}}&{{=}}&{{\displaystyle\operatorname*{min}\left[1,\frac{\pi^{\prime}(x^{1})\mathcal{T}^{Q}(x^{1}\to x^{3})}{\pi^{\prime}(x^{3})\mathcal{T}^{Q}(x^{3}\to x^{1})}\right]=\operatorname*{min}\left[1,\frac{0.6\cdot0.75}{0.1\cdot0.5}\right]=1.}}\end{array}
$$ 

We can now easily verify that the stationary distribution of the chain resulting from equation (12.25) and these acceptance probabilities gives the desired stationary distribution $\pi^{\prime}$ . 

The Metropolis-Hastings algorithm has a particularly natural implementation in the context of graphical models. Each local transition model $\mathcal{T}_{i}$ is deﬁned via an associated proposal distribution $\mathcal{T}_{i}^{Q_{i}}$ . The acceptance probability for this chain has the form 

$$
\begin{array}{r c l}{{{\mathcal A}(x_{-i},x_{i}\to x_{-i},x_{i}^{\prime})}}&{{=}}&{{\mathrm{min}\left[1,\frac{\pi\left(x_{-i},\,x_{i}^{\prime}\right){\mathcal T}_{i}^{Q_{i}}\left(x_{-i},\,x_{i}^{\prime}\to x_{-i},\,x_{i}\right)}{\pi\left(x_{-i},\,x_{i}\right){\mathcal T}_{i}^{Q_{i}}\left(x_{-i},\,x_{i}\to x_{-i},\,x_{i}^{\prime}\right)}\right]}}\\ {{}}&{{=}}&{{\mathrm{min}\left[1,\frac{P_{\Phi}\left(x_{i}^{\prime},\,x_{-i}\right)}{P_{\Phi}\left(x_{i},\,x_{-i}\right)}\frac{{\mathcal T}_{i}^{Q_{i}}\left(x_{-i},\,x_{i}^{\prime}\to x_{-i},\,x_{i}\right)}{{\mathcal T}_{i}^{Q_{i}}\left(x_{-i},\,x_{i}\to x_{-i},\,x_{i}^{\prime}\right)}\right].}}\end{array}
$$ 

The proposal distributions are usually fairly simple, so it is easy to compute their ratios. In the case of graphical models, the ﬁrst ratio can also be computed easily: 

$$
\begin{array}{r c l}{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime},{\pmb x}_{-i})}{P_{\Phi}(x_{i},{\pmb x}_{-i})}}&{=}&{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid{\pmb x}_{-i})P_{\Phi}({\pmb x}_{-i})}{P_{\Phi}(x_{i}\mid{\pmb x}_{-i})P_{\Phi}({\pmb x}_{-i})}}\\ &{=}&{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid{\pmb x}_{-i})}{P_{\Phi}(x_{i}\mid{\pmb x}_{-i})}.}\end{array}
$$ 

As for Gibbs sampling, we can use the observation that each variable $X_{i}$ is conditionally independent of the remaining variables in the network given its Markov blanket. Letting $U_{i}$ denote ${\mathrm{MB}}_{\mathcal{K}}(X_{i})$ , and $\pmb{u}_{i}=(\pmb{x}_{-i})\langle\pmb{U}_{i}\rangle$ , we have that: 

$$
\begin{array}{r c l}{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid\mathbf{\boldsymbol{x}}_{-i})}{P_{\Phi}(x_{i}\mid\mathbf{\boldsymbol{x}}_{-i})}}&{=}&{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid\mathbf{\boldsymbol{u}}_{i})}{P_{\Phi}(x_{i}\mid\mathbf{\boldsymbol{u}}_{i})}.}\end{array}
$$ 

This expression can be computed locally and efciently, based only on the local parameter iz ation of $X_{i}$ and its Markov blanket (exercise 12.18). 

The similarity to the derivation of Gibbs sampling is not accidental. Indeed, it is not difcult to show that Gibbs sampling is simply a special case of Metropolis-Hastings, one with a particular choice of proposal distribution (exercise 12.19). 

The Metropolis-Hastings construction allows us to produce a Markov chain for an arbitrary stationary distribution. Importantly, however, we point out that the key theorem still requires that the constructed chain be regular. This property does not follow directly from the construction. In particular, the exclusive-or network of example 12.12 induces a nonregular Markov chain for any Metropolis-Hastings construction that uses a local proposal distribution — one that proposes changes to only a single variable at a time. In order to obtain a regular chain for this example, we would need a proposal distribution that allows simultaneous changes to both $X$ and $Y$ at a single step. 

# 12.3.5 Using a Markov Chain 

So far, we have discussed methods for deﬁning Markov chains that induce the desired stationary distribution. Assume that we have constructed a chain that has a unique stationary distribution $\pi$ , which is the one from which we wish to sample. How do we use this chain to answer queries? A naive answer is straightforward. We run the chain using the algorithm of algorithm 12.5 until it converges to the stationary distribution (or close to it). We then collect a sample from $\pi$ . We repeat this process once for each particle we want to collect. The result is a data set $\mathcal{D}$ consisting of independent particles, each of which is sampled (approximately) from the stationary distribution $\pi$ . The analysis of section 12.1 is applicable to this setting, so we can provide tight bounds on the number of samples required to get estimators of a certain quality. Unfortunately, matters are not so straightforward, as we now discuss. 

# 12.3.5.1 Mixing Time 

burn-in time 

Deﬁnition 12.6 A critical gap in this description of the MCMC algorithm is a speciﬁcation of the burn-in time $T-$ the number of steps we take until we collect a sample from the chain. Clearly, we want to wait until the state distribution is reasonably close to $\pi$ . More precisely, we want to ﬁnd a $T$ that guarantees that, regardless of our starting distribution $P^{(0)},\,\bar{P^{(T)}}$ is within some small

 $\epsilon$ of $\pi$ . In this context, we usually use variational distance (see section A.1.3.3) as our notion of

 “within $\epsilon$ .” 

Let $\mathcal{T}$ be a Markov chain. Let $T_{\epsilon}$ be the minimal $T$ such that, for any starting distribution $P^{(0)}$ , we have that: 

$$
D_{\mathrm{var}}(P^{(T)};\pi)\leq\epsilon.
$$ 

mixing time Then $T_{\epsilon}$ is called the $\epsilon$ - mixing time of $\mathcal{T}$ . 

In certain cases, the mixing time can be extremely long. This situation arises in chains where the state space has several distinct regions each of which is well connected, but where transitions between regions are low probability. In particular, we can estimate the extent to which the chain allows mixing using the following quantity: 

Deﬁnition 12.7 conductance 

t $\mathcal{T}$ be a Markov chain transition model and $\pi$ its stationary distribution. The conductance of $\mathcal{T}$ is deﬁned as follows: 

$$
\begin{array}{c c}{\displaystyle\operatorname*{min}}&{\displaystyle\frac{P(\mathcal{S}\cap\mathcal{S}^{c})}{\pi(\mathcal{S})},}\\ {0<\pi(\mathcal{S})\le1/2}\end{array}
$$ 

where $\pi(S)$ is the probability assigned by the stationary distribution to the set of states $s$ , $S^{c}=$ $V a l(X)-S,$ , and 

$$
P(\mathcal{S}\sim\mathcal{S}^{c})=\sum_{\pmb{x}\in\mathcal{S},\pmb{x}^{\prime}\in\mathcal{S}^{c}}\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime}).
$$ 

Intuitively, $P(S\curvearrowright S^{c})$ is the total “bandwidth” for transitioning f m $s$ to its comp ment. In cases where the conductance is low, there is some set of states S where, once in S , it is very difcult to transition out of it. Figure 12.6 visualizes this type of situation, where the only tran on between $S=\{x^{1},x^{2},x^{3}\}$ and its complement is the dashed transition between $x^{2}$ and $x^{4}$ , which has a very low probability. In cases such as this, if we start in a state within S , the chain is likely to stay in S and to take a very long time before exploring other regions of the state space. Indeed, it is possible to provide both upper and lower bounds on the mixing rate of a Markov chain in terms of its conductance. 

In the context of Markov chains corresponding to graphical models, chains with low conduc- tance are most common in networks that have deterministic or highly skewed parameter iz ation. 

![](images/74ee3d8db9c7e7535bb5501146d327ccc3b923c4ad8612f698e54620d45962d2.jpg) 
Figure 12.6 Visualization of a Markov chain with low conductance 

In fact, as we saw in example 12.12, networks with deterministic CPDs might even lead to reducible chains, where diferent regions are entirely disconnected. However, even when the dis- tribution is positive, we might still have regions that are connected only by very low-probability transitions. (See exercise 12.21.) 

There are methods for providing tight bounds on the $\epsilon$ -mixing time of a given Markov chain. These methods are based on an analysis of the transition matrix between the states in the Markov chain. Unfortunately, in the case of graphical models, an exhaustive enumeration of the exponentially many states is precisely what we wish to avoid. (If this enumeration were feasible, we would not have to resort to approximate inference techniques in the ﬁrst place.) Alternatively, there is a suite of indirect techniques that allow us to provide bounds on the mixing time for some general class of chains. However, the application of these methods to each new class of chains requires a separate and usually quite sophisticated mathematical analysis. As of yet, there is no such analysis for the chains that are useful in the setting of graphical models. A more common approach is to use a variety of heuristics to try to evaluate the extent to which a sample trajectory has “mixed.” See box 12.B for some further discussion. 

# 12.3.5.2 Collecting Samples 

The burn-in time for a large Markov chain is often quite large. Thus, the naive algorithm described above has to execute a large number of sampling steps for every usable sample. However, a key observation is that, if $\bar{\mathbf{\mathit{x}}^{(t)}}$ is sampled from $\pi$ , then $\cdot_{\pmb{x}}(t{+}1)$ is also sampled from $\pi$ . Thus, once we have run the chain long enough that we are sampling from the stationary distribution (or a distribution close to it), we can continue generating samples from the same trajectory and obtain a large number of samples from the stationary distribution. 

More formally, assume that we use $\mathbfit{\bar{x}}^{(0)},\hat{\mathbfit{\alpha}},\mathbfit{x}^{(T)}$ as our burn-in phase, and then collect $M$ mples ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ from the stationary dis simp might collect $M$ consecutive samples, so that ${\pmb x}[m]\,=\,{\pmb x}^{(T+m)}$ , for $m\,=\,1,\ldots,M$ . If $\pmb{x}^{(T+1)}$ is sampled from $\pi$ , then so are all of the samples in $\mathcal{D}$ . Thus, if our chain has mixed by the time we collect our ﬁrst sample, then for any function $f$ , 

$$
\hat{\boldsymbol E}_{\mathcal{D}}(\boldsymbol f)=\frac{1}{M}\sum_{m=1}^{M}\boldsymbol f(\boldsymbol x[m],\boldsymbol e)
$$ 

estimator is an unbiased estimator for $E_{\pi(X)}[f(X,e)]$ . 

How good is this estimator? As we discussed in appendix A.2.1, the quality of an unbiased estimator is measured by its variance: the lower the variance, the higher the probability that the estimator is close to its mean. In theorem A.2, we showed an analysis of the variance of an estimator obtained from $M$ independent samples. Unfortunately, we cannot apply that analysis in this setting. The key problem, of course, is that consecutive samples from the same trajectory are correlated. Thus, we cannot expect the same performance as we would from $M$ independent samples from $\pi$ . More formally, the variance of the estimator is signiﬁcantly higher than that of an estimator generated by $M$ independent samples from $\pi$ , as discussed before. 

Example 12.15 

central limit theorem 

# Theorem 12.6 

Consider the Gibbs chain for the deterministic exclusive-or network of example 12.12, and assume we compute, for a given run of the chain, the fraction of states in which $x^{1}$ holds in the last 100 states traversed by the chain. A chain started in the state $x^{1},y^{0}$ would have that $100/100$ of the states have $x^{1}$ , whereas a chain started in the state $x^{0},y^{1}$ would have that $0/100$ of the states have $x^{1}$ . Thus, the variance of the estimator is very high in this case. 

One can formalize this intuition by the following generalization of the central limit theorem that applies to samples collected from a Markov chain: 

Let T be a rkov chain a ${\overline{{X}}}[1],\ldots,X[M]$ a set of samples collected from $\mathcal{T}$ at its stationary distribution P . Then, since $M\longrightarrow\infty$ : 

$$
\left(\hat{E}_{\mathcal{D}}(f)-E_{X\sim P}[f(X)]\right)\longrightarrow\mathcal{N}\left(0;\sigma_{f}^{2}\right)
$$ 

where 

$$
\sigma_{f}^{2}={\mathbb{V}\!a r_{X\sim\mathcal{T}}}[f(X)]+2\sum_{\ell=1}^{\infty}C o\nu_{\mathcal{T}}[f(X[m]);f(X[m+\ell])]<\infty.
$$ 

autocovariance The terms in the summation are called autocovariance terms, since they measure the covariance between samples from the chain, taken at diferent lags. The stronger the correlations between diferent samples, the larger the autocovariance terms, the higher the variance of our estimator. This result is consistent with the behavior we discussed in example 12.12. 

We want to use theorem 12.6 in order to assess the quality of our estimator. In order to do so, we need to estimate the quantity $\sigma_{f}^{2}$ . We can estimate the variance from our empirical data using the standard estimator: 

$$
\pmb{V a r}_{\pmb{X}\sim\mathcal{T}}[f(\pmb{X})]\approx\frac{1}{M-1}\left[\sum_{m=1}^{M}\left(f(\pmb{X})-\hat{\pmb{E}}_{\mathcal{D}}(f)\right)^{2}\right].
$$ 

To estimate the autocovariance terms from the empirical data, we compute: 

$$
\mathbb{C}o v_{\mathcal{T}}[f(X[m]);f(X[m+\ell])]\approx\frac{1}{M-\ell}\sum_{m=1}^{M-\ell}(f(X[m]-\hat{\mathbf{E}}_{\mathcal{D}}(f))(f(X[m+\ell]-\hat{\mathbf{E}}_{\mathcal{D}}(f))).
$$ 

At ﬁrst glance, theorem 12.6 suggests that the variance of the estimate could be reduced if the chain is allowed a sufcient number of iterations between sample collections. Thus, having collected a particle ${\pmb x}^{(T)}$ , we can let the chain run for a while, and collect a second particle $\pmb{x}^{(T+d)}$ for some appropriate choice of $d$ . For $d$ large enough, ${\pmb x}^{(T)}$ and $\pmb{x}^{(T+d)}$ are only slightly correlated, reducing the correlation in the preceding theorem. 

However, this approach is suboptimal for various reasons. First, the time $d$ required for “forgetting” the correlation is clearly related to the mixing time of the chain. Thus, chains that are slow to mix initially also require larger $d$ in order to produce close-to-independent particles. Nevertheless, the samples do come from the correct distribution for any value of $d$ , and hence it is often better to compromise and use a shorter $d$ than it is to use a shorter burn-in time $T$ . This method thus allows us to collect a larger number of usable particles with fewer transitions of the Markov chain. Indeed, although the samples between ${\pmb x}^{(\bar{T})}$ and $\pmb{x}^{(T+d)}$ are not independent samples, there is no reason to discard them. That is, one can show that using all of the samples $\mathbf{\boldsymbol{x}}^{(T)},\mathbf{\boldsymbol{x}}^{(T+1)},\ldots,\mathbf{\boldsymbol{x}}^{(T+d)}$ produces a provably better estimator than using just the two samples ${\pmb x}^{(T)}$ and $\pmb{x}^{(T+d)}$ : our variance is always no higher if we use all of the samples we generated rather than a subset. Thus, the strategy of picking only a subset of the samples is useful primarily in settings where there is a signiﬁcant cost associated with using each sample (for example, the evaluation of $f$ is costly), so that we might want to reduce the overall number of particles used. 

Box 12.B — Skill: MCMC in Practice. A key question when using a Markov chain is evaluating the time required for the chain to “mix” — that is, approach the stationary distribution. As we discussed, no general-purpose theoretical analysis exists for the mixing time of graphical models. However, we can still hope to estimate the extent to which a sample trajectory has “forgotten” its origin. Recall that, as we discussed, the most common problem with mixing arises when the state space consists of several regions that are connected only by low-probability transitions. If we start the chain in a state in one of these regions, it is likely to spend some amount of time in that same region before transitioning to another region. Intuitively, the states sampled in the initial phase are clearly not from the stationary distribution, since they are strongly correlated with our initial state, which is arbitrary. However, later in the trajectory, we might reach a state where the current state is as likely to have originated in any initial state. In this case, we might consider the chain to have mixed. 

Diagnosing convergence of a Markov chain Monte Carlo method is a notoriously hard problem. The chain may appear to have converged simply by spending a large number of iterations in a particular mode due to low conductance between modes. However, there are approaches that can tell us if a chain has not converged. 

One technique is based directly on theorem 12.6. In particular, we can compute the ratio ρ ℓ of the estimated autocovariance in equation (12.28) to the estimated variance in equation (12.27). This ratio is known as the autocorrelation of lag ℓ ; it provides a normalized estimate of the extent to which the chain has mixed in $\ell$ steps. In practice, the autocorrelation should drop of exponentially with the length of the lag, and one way to diagnose a poorly mixing chain is to observe high autocorrelation at distant lags. Note, however, that the number of samples available for computing autocorrelation decreases with lag, leading to large variance in the autocorrelation estimates at large lags. 

A diferent technique uses the observation that multiple chains sampling the same distribution should, upon convergence, all yield similar estimates. In addition, estimates based on a complete set of samples collected from all of the chains should have variance comparable to variance in each of the chains. More formally, assume that $K$ separate chains are each run for $T+M$ steps starting from a diverse set of starting points. After discarding the ﬁrst $T$ samples from each chain, let $X_{k}[m]$ denote $a$ sample from chain $k$ after iteration $T+m$ . We can now compute the $B$ (between-chains) and $W$ (within-chain) variances: 

$$
\begin{array}{r c l}{\bar{f}_{k}}&{=}&{\displaystyle\frac{1}{M}\sum_{m=1}^{M}f(X_{k}[m])}\\ {\bar{f}}&{=}&{\displaystyle\frac{1}{K}\sum_{k=1}^{K}\bar{f}_{k}}\\ {B}&{=}&{\displaystyle\frac{M}{K-1}\sum_{k=1}^{K}(\bar{f}_{k}-\bar{f})^{2}}\\ {W}&{=}&{\displaystyle\frac{1}{K}\frac{1}{M-1}\sum_{k=1}^{K}\sum_{m=1}^{M}\left(f(X_{k}[m])-\bar{f}_{k}\right)^{2}.}\end{array}
$$ 

The expression $\begin{array}{r}{V=\frac{M-1}{M}W+\frac{1}{M}B}\end{array}$ can now be shown to overestimate the variance of our estimate of $f$ based on the collected samples. In the limit of $M\longrightarrow\infty$ →∞ , both $W$ and $V$ converge to the true variance of the estimate. One measure of disagreement between chains is given by $\begin{array}{r}{\hat{R}=\sqrt{\frac{V}{W}}}\end{array}$ q . If the chains have not all converged to the stationary distribution, this estimate will be high. If this value is close to 1, either the chains have all converged to the true distribution, or the starting points were not sufciently dispersed and all of the chains have converged to the same mode or a set of modes. We can use this strategy with multiple diferent functions $f$ in order to increase our conﬁdence that our chain has mixed. We can, for example, use indicator functions of various events, as well as more complex functions of multiple variables. 

Overall, although the strategy of using only a single chain produces more viable particles using lower computational cost, there are still signiﬁcant advantages to the multichain approach. First, by starting out in very diferent regions of the space, we are more likely to explore a more representative subset of states. Second, the use of multiple chains allows us to evaluate the extent to which our chains are mixing. Thus, to summarize, a good strategy for using a Markov chain in practice is a hybrid approach, where we run a small number of chains in parallel for a reasonably long time, using their behavior to evaluate mixing. After the burn-in phase, we then use the existence of multiple chains to estimate convergence. If mixing appears to have occurred, we can use each of our chains to generate multiple particles, remembering that the particles generated in this fashion are not independent. 

# 12.3.5.3 Discussion 

MCMC methods have many advantages over other methods. Unlike the global approximate inference methods of the previous chapter, they can, at least in principle, get arbitrarily close to the true posterior. Unlike forward sampling methods, these methods do not degrade when the probability of the evidence is low, or when the posterior is very diferent from the prior. Furthermore, unlike forward sampling, MCMC methods apply to undirected models as well as to directed models. As such, they are an important component in the suite of approximate inference techniques. 

However, MCMC methods are not generally an out-of-the-box solution for dealing with in- ference in complex models. First, the application of MCMC methods leaves many options that need to be speciﬁed: the proposal distribution, the number of chains to run, the metrics for evaluating mixing, techniques for determining the delay between samples that would allow them to be considered independent, and more. Unfortunately, at this point, there is little theoretical analysis that can help answer these questions for the chains that are of interest to us. Thus, the application of Markov chains is more of an art than a science, and it often requires signiﬁcant experimentation and hand-tuning of parameters. 

Second, MCMC methods are only viable if the chain we are using mixes reasonably quickly. Unfortunately, many of the chains derived from real-world graphical models frequently have multimodal posterior distributions, with slow mixing between the modes. For such chains, the straightforward MCMC methods described in this chapter are unlikely to work. In such cases, diagnostics such as the ones described in box 12.B can be used to determine that the chain is not mixing, and better methods must then be applied. The key to improving the convergence of a Markov chain is to introduce transitions that take larger steps in the space, allowing the chain to move more rapidly between modes, and thereby to better explore the space. The best strategy is often to analyze the properties of the posterior landscape of interest, and to construct moves that are tailored for this speciﬁc space. (See, for example, exercise 12.23.) Fortunately, the ability to mix diferent reversible kernels within a single chain (as discussed in section 12.3.4) allows us to introduce a variety of long-range moves while still maintaining the same target posterior. 

simulated annealing 

temperature parameter 

In addition to the use of long-range steps that are speciﬁcally designed for particular (classes of) chains, there are also some general-purpose methods that try to achieve that goal. The block Gibbs approach (section 12.3.3) is an instance of this general class of methods. Another strategy uses the same ideas in simulated annealing to improve convergence of local search to a better optimum. Here, we can deﬁne an intermediate distribution parameterized by a temperature parameter $T;\,T$ : 

$$
\tilde{P}_{T}(X)\propto\exp\{-\frac{1}{T}\log\tilde{P}(X)\}.
$$ 

This distribution is similar to our original target distribution $\tilde{P}$ . At a low temperature of $T=1$ , this equation yields the original target distribution. But as the temperature increases, modes become broader and merge, reducing the multimodality of the distribution and increasing its mixing rate. We can now deﬁne various methods that use a combination of related chains running at diferent temperatures. At a high level, the higher-temperature chain can be viewed as proposing a step, which we can accept or reject using the acceptance probability of our true target distribution. (See section 12.7 for references to some of these more advanced methods.) In efect, these approaches use the higher-temperature chains to deﬁne a set of larger steps in the space, thereby providing a general-purpose method for achieving more rapid movement between multiple modes. However, this generality comes at the computational cost of running parallel 

![](images/f63b7910a75024eda568b4d85c52fa131cf6d8a470e0d03b4112f44b6eb23ebc.jpg) 
Figure 12.C.1 — Example of bugs model speciﬁcation (a) A simple hybrid Bayesian network. (b) A bugs deﬁnition of a probabilistic model over this network. 

chains; thus, if we can understand our speciﬁc posterior well enough to construct specialized operators that move between modes, that often provides a more efective solution. 

Box 12.C — Case Study: The bugs System. One of the main advantages of MCMC methods is their broad applicability to a very general class of networks. Not only do they apply (at least in principle) to any discrete network, regardless of its complexity, they also generalize fairly simply to continuous variables (see section 14.5.3). One very useful system that exploits this generality is the bugs system, developed by Thomas et al. (1992). This system provides a general-purpose language for representing a broad range of probabilistic models and uses MCMC to run inference over these models. 

BUGS system 

The bugs system provides a programming-language-based representation of a probabilistic model. The model deﬁnes a joint distribution over a set of random variables. Variables can be deﬁned as functions of each other; these functions can be deterministic functions, or stochastic functions utiliz- ing a rich set of predeﬁned distributions. For example, consider the simple Bayesian network shown in ﬁgure 12.C.1a, where $A,B,C$ are discrete and $X,Y$ are continuous. One possible probabilistic model can be written in bugs using the commands shown in ﬁgure 12.C.1b. This model deﬁnes: A to be a binary-valued variable, with $P(a^{1})\,=\,0.3$ ; $B$ is a 3-valued variable that depends on A , whose CPT is deﬁned in the trix $P$ ; $X$ is a Gaussian random variable with mea $-1$ a precision (inverse variance) $0.25;Y$ is a conditional Gaussian whose mean depends on X and B and whose precision also depends on $X$ ; and $C$ is a logistic function of $4X+2$ . Even in this very simple example, we can see that the bugs language provides a rich language for encoding diferent families of functional and stochastic dependencies between variables. 

Given a probabilistic model deﬁned in this way, the bugs system can instantiate evidence for some of the variables (for example, by reading their values from a ﬁle) and then perform inference over the model by running various MCMC algorithms. The system analyzes the parametric form specifying the distribution of the diferent variables, and it selects an appropriate sampling algorithm to use. The user speciﬁes the number of sampling iterations to perform, and which variables are to be monitored — their values are to be stored during the MCMC iterations. We can then compute such values as the mean and standard deviation of these monitored variables. The system also provides various methods to help detect convergence of the MCMC runs (see also box 12.B). 

Overall, the bugs tool provides a general-purpose and highly ﬂexible framework for specifying and reasoning with probabilistic models. Its ability to provide such a high level of expression power rests on the generality of MCMC as an inference method, and its applicability to a very broad range of distributions (broader than any other inference method currently available). 

# 12.4 Collapsed Particles 

So far, we have restricted our attention to methods that use as their particles only instantiations $\xi$ to all the network variables. Clearly, covering an exponentially large state space with a small number of instantiations is difcult, and it often takes a large number of full particles to obtain reasonable estimates. One approach for improving the performance of particle-based methods is to use as particles partial assignments to some subset of the network variables, combined with some type of closed-form representation of a distribution over the rest. 

More precisely, assume that we p tion $\mathcal{X}$ into two subsets: $X_{p}$ — the variables whose assignment deﬁnes the particle, and $X_{d}$ — the variables over which we will maintain a closed- form distribution. Then collapsed particles c stantiation $\pmb{x}_{p}\in V a l(\pmb{X}_{p})$ , coupled with some representation of the distribution $P(X_{d}\mid\pmb{x}_{p},\pmb{e})$ | . The particle is “collapsed” because some of the variables are not assigned but rather summarized using a distribution. Collapsed particles are also known as Rao-Blackwellized particles . 

Assume that we want to estimate an expectation of some function $f(\xi)$ relative to our posterior distribution $P(X_{p},X_{d}\mid e)$ . We now have: 

$$
\begin{array}{c c l}{{E_{P(\xi|e)}[f(\xi)]}}&{{=}}&{{\displaystyle\sum_{x_{p},x_{d}}P(x_{p},x_{d}\mid e)f(x_{p},x_{d},e)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{x_{p}}P(x_{p}\mid e)\sum_{x_{d}}P(x_{d}\mid x_{p},e)f(x_{p},x_{d},e)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{x_{p}}P(x_{p}\mid e)\left(E_{P(X_{d}\mid x_{p},e)}[f(x_{p},X_{d},e)]\right).}}\end{array}
$$ 

We can use the samples $\pmb{x}_{p}[m]$ to approximate any expectation relative to the distribution $P(X_{p}\mid e)$ , using the techniques described above. In particular, we can approximate the expectation of the expression in parentheses — an expression that is itself an expectation. 

In the case of collapsed particles, we assume that the internal expectation can be computed (or approximated) efciently. As we will discuss, we can explicitly represent the distribution $P(X_{d}\mid\pmb{x}_{p},\pmb{e})$ as a graphical model, using constructions such as the reduced M ov net of section 4.2.3. We thus have a hybrid approach where we generate samples $\pmb{x}_{p}$ from $X_{p}$ 

and perform exact inference on $X_{d}$ given $\pmb{x}_{p}$ . Thus, this approach deﬁnes a spectrum: When $\boldsymbol{X}_{p}=\boldsymbol{X}$ , collapsed particles are simply full particles, and we are simply applying the methods deﬁned earlier in this chapter; when $\boldsymbol{X}_{p}\:=\:\emptyset$ , we have a single particle whose associated distribution is our original network, so that we are back in the regime of exact inference. We note that, in some cases, the network might not be sufciently simple for exact inference. However, it might be amenable to some other form of approximation — for example, one of the methods we discuss in chapter 11. Intuitively, the fewer variables we sample (keep in $\centering X_{p}.$ ), 

 the larger the part of the probability mass that we cover using each collapsed particle $\pmb{x}_{p}[m]$ . From an alternative perspective, we are performing an exact computation for the expectation relative to $X_{d},$ thereby eliminating any contribution it makes to the bias or the variance of the estimator. Thus, if $\left|{X}_{p}\right|$ is fairly small, we can obtain much better estimates for the distribution using signiﬁcantly fewer particles. 

In this section, we describe extensions of the approaches discussed earlier in this chapter to the case of collapsed particles. 

# 12.4.1 Collapsed Likelihood Weighting $\star$ 

We begin by describing a collapsed extension to likelihood weighting. We ﬁrst describe the algorithm generally, from the perspective of normalized importance sampling. We then consider a speciﬁc application that is a direct extension to the full-particle version of likelihood weighting. 

12.4.1.1 Collapsed Importance Sampling 

Recall that, in importance sampling, we generate our samples from an alternative proposal distribution $Q$ , and we compensate for the discrepancy by associating with each particle a weight $w[m]$ . In the case of collapsed particles, we are generating speciﬁc particles only for the variables in $X_{p}$ , so that $Q$ would be a distribution over $\pmb{x}_{p}$ . We thus generate a data set 

$$
\mathcal{D}=\{(\pmb{x}_{p}[m],w[m],P(\pmb{X}_{d}\mid\pmb{x}_{p}[m],e))\}_{m=1}^{M},
$$ 

where each $\pmb{x}_{p}[m]$ is sampled from $Q$ . Here we will discuss both the choice of proposal distribution $Q$ and the computation of the weights $w[m]$ . 

Our goal is to estimate the expectation of equation (12.29). In efect, we are estimat- ing the expectation of a new function $g$ , which represents the internal expectation: $g\ =$ $E_{P(X_{d}|x_{p},e)}[f(x_{p},X_{d},e)]$ . Using normalized importance sampling, we estimate this expec- tation as: 

$$
\hat{\boldsymbol E}_{\mathcal{D}}(\boldsymbol f)=\frac{\sum_{m=1}^{M}w[m]\left(\boldsymbol E_{P(\boldsymbol X_{d}|\boldsymbol x_{p}[m],e)}[\boldsymbol f(\boldsymbol x_{p}[m],\boldsymbol X_{d},e)]\right)}{\sum_{m=1}^{M}w[m]}.
$$ 

Example 12.16 Consider the Extended Student network, repeated in ﬁgure $\it{12.7a}$ , with the evidence $d^{1},h^{0}$ . Assume that we choose to partition the variables as follows: $\boldsymbol{\cal X}_{p}=\{D,G\}$ , and $\boldsymbol{X}_{d}=\{C,I,S,L,J,H\}$ . In this case, each particle deﬁnes an assignment $(d,g)$ to the variables $D,G$ . Assuming that our algorithm follows the template of full-particle likelihood weighting, we would ensure that our proposal distribution $Q$ ascribes only positive probability to particles $(d^{1},g)$ that are compatible with our evidence $d^{1}$ . Each such particle is also associa ith a distribution $P(C,I,S,L,H\mid$ $g,d^{1},h^{0})$ . The reduced Markov network shown in ﬁgure 12.7b represents this distribution. 

![](images/8bccb5d844e5d3b143a18eaa9d047b4972ac59aa32be95f244fa7647361a22c8.jpg) 
Figure 12.7 Networks illustrating collapsed importance sampling: (a) The Extended-Student Bayesian network $\boldsymbol{\mathcal{B}}^{s t u d e n t}$ ; (b) The network $\mathcal{B}_{G=g,D=d}^{s t u d e n t}$ reduced by $G=g,D=d$ . 

e that our query is $P(j^{1}\mid d^{1},h^{0})$ , so that our function $f$ is the indicator function $I\{\xi\langle J\rangle=j^{1}\}$ . For each particle, we evaluate 

$$
{\cal E}_{P(C,I,S,L,J,H|g,d^{1},h^{0})}\big[{\cal I}\{\xi\langle J\rangle=j^{1}\}\big]=P(j^{1}\mid g,d^{1},h^{0}).
$$ 

We then compute an average of these probabilities, weighted by the importance weights (which we will discuss). The computation of the probabilities $P(J\mid g,d^{1},h^{0})$ can be done using inference in the reduced network, which is simpler than inference in the original network. (Although, in this very simple network, the savings are not signiﬁcant.) 

Note that the extent to which the reduced network allows more efective inference than the original one depends on our choice of variables $X_{p}$ . For example, if (for some reason) we choose ${\cal{X}}_{p}\,=\,\{H\}$ , the resulting conditioned network is no simpler than the original, and the use of particle-based methods has no computational beneﬁts over the use of exact inference. 

# 12.4.1.2 Formal Description 

To specify the algorithm formally, we must deﬁne the proposal distribution $Q$ and the associated importance weights. We begin by partitioning our evidence set $E$ into two subsets: $\begin{array}{r l}{\boldsymbol{E_{p}}}&{{}\!\!\!\!=}\end{array}$ $\pmb{E}\cap\pmb{X}_{p}$ , and $E_{d}=E\cap X_{d}$ , with $e_{p}$ and $e_{d}$ deﬁned accordingly. This partition determines how we handle each of the observed variables: evidence in $E_{p}$ will be treated as in likelihood weighting, modifying our sampling process and the importance weights; evidence in $E_{d}$ will be accounted for as part of the exact inference process. 

Now, consider an arbitrary proposal distribution $Q$ . We can go through an analysis similar to the one that allowed us to derive equation (12.12): 

$$
\begin{array}{r c l}{{E_{P(\xi|e)}[f(\xi)]}}&{{=}}&{{\displaystyle\sum_{\boldsymbol{x}_{p},\boldsymbol{x}_{d}}P(\boldsymbol{x}_{p},\boldsymbol{x}_{d}\mid e)f(\boldsymbol{x}_{p},\boldsymbol{x}_{d},e)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{\boldsymbol{x}_{p}}Q(\boldsymbol{x}_{p})\frac{P(\boldsymbol{x}_{p}\mid e)}{Q(\boldsymbol{x}_{p})}\sum_{\boldsymbol{x}_{d}}P(\boldsymbol{x}_{d}\mid\boldsymbol{x}_{p},e)f(\boldsymbol{x}_{p},\boldsymbol{x}_{d},e).}}\end{array}
$$ 

We can now reformulate the term $P(x_{p}\mid e)$ as: 

$$
\begin{array}{l c l}{P(\pmb{x}_{p}\mid e)}&{=}&{\displaystyle\frac{P(\pmb{x}_{p},e)}{P(e)}}\\ &{=}&{\displaystyle\frac{P(\pmb{x}_{p},e_{p},e_{d})}{P(e)}}\\ &{=}&{\displaystyle\frac{1}{P(e)}P(\pmb{x}_{p},e_{p})P(e_{d}\mid\pmb{x}_{p},e_{p}).}\end{array}
$$ 

Plugging this result back into our derivation, we obtain that: 

$$
\begin{array}{c l}{{}}&{{E_{P(\xi|e)}[f(\xi)]=\displaystyle\frac{1}{P(e)}\sum_{x_{p}}Q(x_{p})\frac{P(x_{p},e_{p})}{Q(x_{p})}P(e_{d}\mid x_{p},e_{p})\sum_{x_{d}}P(x_{d}\mid x_{p},e)f(x_{d}\mid x_{d})}}\\ {{=}}&{{\displaystyle\frac{1}{P(e)}E_{Q(X_{p})}\biggl[\frac{P(x_{p},e_{p})}{Q(x_{p})}P(e_{d}\mid x_{p},e_{p})E_{P(x_{d}\mid x_{p},e)}[f(x_{p},x_{d},e)]\biggr].}}\end{array}
$$ 

This analysis suggests that the appropriate importance weights should be deﬁned as: 

$$
w(\pmb{x}_{p})=\frac{P(\pmb{x}_{p},\pmb{e}_{p})}{Q(\pmb{x}_{p})}P(\pmb{e}_{d}\mid\pmb{x}_{p},\pmb{e}_{p}).
$$ 

Indeed, if we compute the mean of our importance weights, as in equation (12.11), we obtain the following formula for the normalized importance sampling estimator: 

$$
\begin{array}{c c l}{{E_{Q(X_{p})}[w(X_{p})]}}&{{=}}&{{\displaystyle\sum_{x_{p}}Q(x_{p})\frac{P(x_{p},e_{p})}{Q(x_{p})}P(e_{d}\mid x_{p},e_{p})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{x_{p}}P(x_{p},e_{p})P(e_{d}\mid x_{p},e_{p})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{x_{p}}P(e_{d},x_{p},e_{p})=P(e_{d},e_{p}).}}\end{array}
$$ 

Thus, if we select our importance weights as in equation (12.32), we have that: 

$$
E_{P(\xi|e)}[f(\xi)]=\frac{E_{Q(X_{p})}\big[w(X_{p})E_{P(x_{d}|x_{p},e)}[f(x_{p},x_{d},e)]\big]}{E_{Q(X_{p})}[w(X_{p})]},
$$ 

as desired. 

# 12.4.1.3 Proposal Distribution 

Our preceding analysis does not place any restrictions on the proposal distribution; we can choose any proposal distribution $Q$ that seems appropriate (as long as it dominates $P$ ). However, it is important to remember our two main desiderata for a proposal distribution: easy generation of samples from $Q$ and similarity between $Q$ and our target distribution $P(X_{p}\mid e)$ . The proposal distribution we used for the full particle case attempted to address both of these desiderata, at least to some extent. In this section, we describe a generalization of that proposal distribution for the case of collapsed particles. 

Our goal is to generate particles from a distribution $Q(\boldsymbol{\cal X}_{p})$ . Following the template for the full particle case, we would sample each unobserved variable $X\in X_{p}$ from its C . The reason we can execute this process is that we were careful to sample the parents of $X_{i}$ before that, so that the distribution from which we should sample $X$ is uniquely deﬁned. In the collapsed case, however, $\mathrm{Pa}_{X}$ might not be within the set $X_{p}$ , in which case we would not have values for them when sampling $X$ . For example, returning to the setting of example 12.16, it is not clear how we would deﬁne a sampling distribution for the variable $G$ . 

Most simply, we can select as our subset $X_{p}$ an upwardly closed subset of nodes in the network. In our example, we might select $X_{p}$ to consist of all of the variables $C,D,I,G$ . This variant of the algorithm is very close to full-particle likelihood weighting. Here, we are back in a situation where we can order the variables in such a way that each unobserved variable can be sampled using its original CPD, using the previously sampled assignment to its parents. Each observed variable $X_{i}$ is “sampled,” as in standard likelihood weighting, from a mutilated network that ascribes probability 1 to its observed value $e_{p}\langle X_{i}\rangle$ . The computation of the importance weights for this case is straightforward: We ﬁrst compute the part of the importance weight corresponding to $P(\pmb{x}_{p},\pmb{e}_{p})/Q(\pmb{x}_{p})$ , using precisely the same incremental computation as in standard likelihood weighting. We then compute $P(e_{d}\mid\mathbf{\sigma}x_{p},e_{p})$ in the network conditioned on $\mathbf{\Delta}x_{p},e_{p}$ , and we multiply the importance weight by this additional factor. 

Example 12.17 Continuing example 12.16, assume we choose $\boldsymbol{X}_{p}$ to be the upwardly closed set $C,D,I,G$ . We would sample $C,I,$ and $G$ from their CPD; $D$ would be sampled from a CPD that has no parents, and ascribes probability 1 to $d^{1}$ . The importance weight for a particle $(c,d^{1},i,g)$ is then computed as $P(d^{1}\mid c)\cdot P(h^{0}\mid c,d^{1},i,g)$ . Note that this last term requires inference in the network, speciﬁcally, the marginalization of $L,J,S$ . 

More generally, we can deﬁne a fully general proposal distribution $Q$ by specifying a topo- logical ordering $X_{1},\ldots,X_{k}$ over $\boldsymbol{X}_{p}$ , and a proposal distribution deﬁned in terms of a Bayesian network $\mathcal{G}_{Q}$ over $X_{p}$ that speciﬁes, for each variable $X_{i}$ , $i\,=\,1,\ldots,k$ , a parent set $\mathrm{Pa}_{X_{i}}^{\mathcal{G}_{Q}}\subseteq\{X_{1},\dots,X_{i-1}\}$ } , and a CPD $Q(X_{i}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}_{Q}})$ . This approach allows us to represent − an arbitrary proposal distribution. 

We can now consider the computation of each of the three terms in the deﬁnition of the importance weights in equation (12.32). The term $Q(x_{p})$ can be computed very simply via the chain rule for the proposal network $\mathcal{G}_{Q}$ $P(e_{d}\mid\pmb{x}_{p},\pmb{e}_{p})$ can be computed using inference in the conditioned network B $\mathcal{B}_{E_{p}=e_{p},X_{p}={\pmb x}_{p}}$ — we compute the probability of the query $P(e_{d}\mid e_{p},\pmb{x}_{p})$ in this network. As we stated, the whole approach of collapsed particles is based on the premise that (exact or approximate) inference in this conditioned network is feasible. Similarly, we can compute $P(\pmb{x}_{p},\pmb{e}_{p})$ in the same network. 

# 12.4.2 Collapsed MCMC 

The collapsed MCMC algorithm is also based on equation (12.29); however, as in section 12.3, we simplify our notation by deﬁning $\Phi$ to be the set of factors reduced by the evidence $e$ , so that $P_{\Phi}(X)=P(X\mid e)$ (for $X=\mathcal{X}-E)$ . As in collapsed likelihood weighting, we approximate the outer expectation by generating particles that are instantiations of $X_{p}$ . Here, we generate the particles $\mathbf{\Delta}x_{p}[m]$ using a Markov chain process; at the limit of the chain, these particles will be sampled from the marginal posterior $P_{\Phi}(X_{p})$ . For each such particle $\pmb{x}_{p}$ , we maintain some representation of the distrib ion $P_{\Phi}(X_{d}\mid\pmb{x}_{p})$ , and perform (exact or approximate) inference to compute the expectation of $f$ relative to this distribution. For simplicity, we focus our discussion on Gibbs sampling; the extension to a Metropolis-Hastings algorithm with a general proposal distribution is straightforward. 

We deﬁne the collapsed Gibbs sampling algorithm via a Markov chain whose states are instantiations to $X_{p}$ . As we discuss, to provide an unbiased estimator for the expectation over $P_{\Phi}(X_{p})$ in equation (12.29), we want the stationary distribution of this Markov chain to be $P_{\Phi}(X_{p})$ . We thus modify our Gibbs sampling algorithm as follows: As in standard ing, we deﬁne rnel for each variable $X_{i}\in X_{p}$ . Let ${\pmb x}_{-i}$ be an assignment to $X_{p}-\{X_{i}\}$ } . The kernel for $X_{i}$ is deﬁned as follows: 

$$
{\mathcal{T}}_{i}((x_{-i},x_{i})\to(\pmb{x}_{-i},x_{i}^{\prime}))=P_{\Phi}(x_{i}^{\prime}\mid\pmb{x}_{-i}).
$$ 

This equation is very similar to equation (12.22). The only diference is that the event ${\pmb x}_{-i}$ on which we condition the distribution over $X_{i}$ is not a full assignment to all the other variables in the network, but rather only to the remaining variables in $X_{p}$ . Thus, the efcient computation of equation (12.23), where we simply compute the distribution over $X_{i}$ given its Markov blanket, may not apply. However, we can compute this probability using inference in the network $\mathcal{H}_{\Phi[\pmb{x}_{p}]}$ — the Markov network reduced over the assignment $\pmb{x}_{p}$ — a model that we assume to be tractable. Indeed, the sampling approach can be well integrated with a clique tree over the variables $X_{p}$ to allow the sampling process to be executed efciently (see exercise 12.27). 

Having deﬁned the chain, we can use it in any of the ways described in section 12.3.5 to collect a data set $\mathcal{D}$ of particles $\pmb{x}_{p}^{(m)}$ , each of which is associated with a distribution over $X_{d}$ . Using these particles, we can estimate: 

$$
\hat{\boldsymbol E}_{\mathcal{D}}(f)=\frac{1}{M}\sum_{m=1}^{M}\left(\boldsymbol E_{P_{\Phi[\boldsymbol x_{p}[m]]}(\boldsymbol X_{d})}[f(\boldsymbol x_{p},\boldsymbol x_{d},e)]\right).
$$ 

Example 12.18 Consider the Markov network deﬁned by the Bayesian network of example 12.11, reduced over the evidence $G=g$ . This Markov network is a bipartite graph, where we have two sets of variables $I,D$ such that the only edges in the network are of the form $I_{j},D_{k}$ . Let $\phi_{j,k}(I_{j},D_{k})$ be the factor associated with each such edge. (For simplicity of notation, we assume that there is a factor for every such edge; the factors corresponding to grades that were not in the model will be vacuous.) We can now apply collapsed Gibbs sampling, selecting to sample the variables $_D$ and maintain $^a$ distribution over $\boldsymbol{I}$ ; this choice will (in most cases) be better, since there are usually more students than courses, and it is generally better to sample in a lower-dimensional space. Thus, our Markov 

chain needs to be able to sample $D_{k}$ from $P_{\Phi}(D_{k}\mid d_{-k})$ : 

$$
P_{\Phi}(d_{k}\mid d_{-k})=\frac{P_{\Phi}(d_{1},d_{2},.\,.\,.\,,d_{m})}{\sum_{D_{k}}P_{\Phi}(D_{k},d_{-k})}.
$$ 

The expression in the numerator, and each term in the sum in the denominator, is the probability of a full assignment to $_D$ . This type of expression can be computed as the partition function of the reduced Markov network that we obtain by setting $D=d$ : 

$$
P_{\Phi[\pmb d]}(I_{1},.\,.\,.\,,I_{n})=\frac{1}{Z(\pmb d)}\prod_{j,k}\phi_{j,k}(I_{j},d_{k}).
$$ 

Each of the factors in the product is a singleton over an individual $I_{j}$ . Thus, we can compute the partition function or marginals of this distribution in linear time. In particular, we can easily compute the Gibbs transition probability. Using the same analysis, we can easily compute an expression for $P_{\Phi[d]}(I)$ in closed form, as a product of individual factors over the $I_{j}$ ’s. Thus, we can also easily compute expectations of any function over these variables, such as the expected value of the number of smart students who got a grade of a $C$ in an easy class (see exercise 12.28). 

correspondence problem 

data association 

identity resolution image registration 

Box 12.D — Concept: Correspondence and Data Association. One simple but important prob- lem that arises in many settings is the correspondence problem : mapping between one set of objects ${\mathcal U}=\{u_{1},.\,.\,.\,,u_{k}\}$ and another $\mathcal{V}=\{v_{1},.\,.\,.\,,v_{m}\}$ . This problem arises across multiple erse application domains. Per ps the most familiar are physical sensing applications, where U are sensor measurements and V objects that can generate such measurements; we want to know which object generated which measurement. For example, the objects may be airplanes, and the sensor measurements blips on a radar screen; or the objects may be obstacles in a robot’s environments, and the sensor measurements readings from a laser range ﬁnder. (See box 15.A.) In this incarnation, this task is known as data association . However, there are also many other applications, of which we list only a few examples: 

• Matching citations in text to the entities to which they refer (see, for example, box 6.D); this problem has been called identity resolution and record matching . • Image registration , or matching features in an image representing one view of an object to features in an image representing a diferent view; this problem arises in applications such as stereo reconstruction or structure from motion . • Matching words in a sentence in one language to words in the same sentence, translated to a diferent language; this problem is called word alignment . • Matching genes in a DNA sequence of one organism to orthologous genes in the DNA sequence of another organism. 

This problem has been tackled using a range of models and a variety of inference methods. In this case study, we describe some of these approaches and the design decisions they made. 

correspondence variable 

mutual exclusion constraints 

Probabilistic Correspondence To formulate the correspondence problem as a probabilistic model, we can introduce a set of correspondence variables that indicate the correspondence be- tween one set of objects and the other. One approach is to have binary-valued variables $C_{i j}$ , such that $C_{i j}=$ true when $u_{i}$ matches $v_{j}$ , and false otherwise. While this approach is simple, it places no constraints on the number of matches for each $i$ or for each $j$ . Typically, we want to restrict our model so that, at least on one side, the match is unique. For example, in the radar tracking application, we typically want to assume that each measurement was derived from only a single object. In order to accommodate that in this model, we would need to add (hard) mutual exclusion constraints ( mutex ) that $C_{i j}=t r u e$ implies that $C_{i j^{\prime}}=$ false for all $j^{\prime}\neq j$ . The resulting model is very densely connected, and it can be challenging for inference algorithms to deal with. 

A more parsimonious representation uses nonbinary variables $C_{i}$ such that $V a l(C_{i})=\{1,.\,.\,.\,,m\}$ here, $C_{i}=j$ indicates that $u_{i}$ is matched to $v_{j}$ . The mutex constraints for the matches to $u_{i}$ are then forced by the fact that each variable $C_{i}$ takes on only $a$ single value. Of course, we might also have mutex constraints in the other direction, where we want to assume that each $v_{j}$ matches only a single $u_{i}$ . We will return to this setting. 

The probabilistic model and the evidence generally combine to produce a set of afnities $w_{i j}$ that specify how likely $u_{i}$ is to match to $v_{j}$ . For convenience, we assume that these afnities are represented in log-space. These afnities can be derived from a generative probability — how likely is $v_{j}$ to have generated $u_{i}$ , or from an undirected potential that measures the quality of the match. (See, for example, box 6.D for an application where both approaches have been utilized.) For example, in the robot obstacle matching task, $\exp(w_{i j})$ may evaluate how likely it is that an obstacle $v_{j}$ ,at location $L_{j}$ , generated a measurement $u_{i}$ at sensed location $S_{i}$ . In the citation matching problem, we may have a model that tells us how likely it is that an individual with the name “John X. Smyth” generated a citation $^ Ḋ J Ḍ$ Smith.” The afnities $w_{i j}$ deﬁne $^a$ node potential over the variables $C_{i}$ : $\phi_{i}(C_{i}=j)=\exp(w_{i j})$ . In addition, there may well be other components to the model, as we will discuss. 

The general inference task is then to compute the distribution over the correspondence variables or (as a fallback) to ﬁnd the most likely assignment. We now discuss the challenges posed by this task in diferent variants of the correspondence problem, and some of the solutions. 

The simplest case (which rarely arises in practice) is the one just described: We have only $^a$ set of node potentials $\phi_{i}(C_{i}\,=\,j)$ that specify the afnity of each $u_{i}$ to each $v_{j}$ . In this case, we have only a set of unrelated node potentials over the variables $C_{i}$ , making the model one comprising independent random variables. We can then easily ﬁnd the most likely assignment $c_{i}^{*}=\arg\operatorname*{max}_{j}\phi_{i}(C_{i}=j)$ , or even compute the full posterior $P(C_{i})\propto\phi_{i}(C_{i})$ . 

The inference task becomes much more challenging when we extend the model in a way that induces correlations over the correspondence variables. We now describe three settings where such complications arise, noting that these are largely orthogonal, so that more than one of these com- plicating factors may arise in any given application. 

Two-Sided Mutex Constraints The ﬁrst complication arises if we want to impose mutex con- straints not just on the correspondence of the $u_{i}$ ’s to the $v_{j}\,\overset{\prime}{\boldsymbol{s}},$ but also in the reverse direction; that is, we want each $v_{j}$ to be assigned to exactly one $u_{i}$ . (We note that, unless $k=m$ , a perfect match is not possible; however, we can circumvent this detail by adding “dummy” objects that are equally good matches to anything.) This requirement induces a model where all of the $C_{i}$ ’s are connected to each other with potentials that impose mutex constraints, making it clearly intractable for $^a$ variable elimination algorithm. Nevertheless, several techniques have been efectively applied to this problem. 

bipartite matching MAP assignment 

First, we note that we can view the correspondence problem, in this case, as a bipartite graph, where the $u_{i}$ ’s are one set of vertices, the $v_{j}$ ’s are a second set, and an edge of weight $w_{i j}$ connects $u_{i}$ to each $v_{j}$ . An assignment satisfying the mutex constraints is $^a$ bipartite matching in this graph — a subset of edges such that each node has exactly one adjacent edge in the matching. Finding the single highest-probability assignment — the MAP assignment — is equivalent to one of ﬁnding the bipartite matching whose weight (sum of the edge weights in the matching) is largest. This problem can be solved very efciently using combinatorial optimization algorithms: If we can match any $u_{i}$ to any $v_{j}$ , the total running time is $O(k^{3})$ ; if we have only $\ell$ possible edges, the running time is $O(k^{2}\log(k)+k\ell)$ . 

Of course, in many cases, a single assignment is not an adequate solution for our problem, since there may be many distinct solutions that have high probability. For computing posteriors in this setting, two main approaches have been used. 

The ﬁrst is an MCMC sampler over the space of possible matchings. Here, Gibbs sampling is not a viable approach, since any change to the value of a single variable would give rise to an assignment that violates the mutex constraints. A Metropolis-Hastings chain is a good solution, but for good performance, the proposal distribution must be carefully chosen. Most obvious is to pick two variables such that $C_{i_{1}}=j_{1}$ and $C_{i_{2}}=j_{2}$ , and propose a move that ﬂips them, setting $C_{i_{1}}=j_{2}$ and $C_{i_{2}}=j_{1}$ . While this approach is a legal chain, it can be slow to mix: in most local maxima, a single ﬂip of this type will often produce a very poor solution. Thus, some work has been done on constructing proposal distributions with larger steps that ﬂip multiple variables at $a$ time while still maintaining a legal matching. 

The second solution uses loopy belief propagation over the Markov network with the $C_{i}$ variables. As we mentioned, however, the mutex constraints give rise to a fully connected network, which is $a$ very challenging setup for belief propagation. Here, however, we can adopt a simple heuristic: We begin without including the mutex constraints and run inference. We then examine the resulting posterior and identify those mutex constraints that are violated with high probability (those where $P(C_{i_{1}}\,=\,C_{i_{2}})$ ) is high). We then add those violated constraints and repeat the inference. In practice, this process usually converges long before all $k^{2}$ mutex constraints are added. 

Unobserved Attributes The second type of complication arises when the afnities $w_{i j}$ depend on properties $A_{j}$ of the objects that are unobserved, and need to be inferred. For example, in the citation matching problem, we generally do not know the correct name of an author or a paper; given the name of an author $v_{j}$ , we can determine the afnity $w_{i j}$ of any citation $u_{i}$ . Conversely, given a set of citations $u_{i}$ that match $v_{j}$ , we can infer the most likely name to have generated the observed names in these citations. The same situation arises in many other applications. For example, in the airplane tracking application, the plane’s location at a given point in time is generally unknown, although we may have a prior on this location based on observations at previous time points. Given the location of airplane $v_{j}$ , we can determine how likely it was to have generate the blip $u_{i}$ . Conversely, once we assign blip $u_{i}$ to $v_{j}$ , we can update our posterior on $v_{j}\,\acute{s}$ location. 

More formally, here we have a set of observed features $B_{i}$ for each object $u_{i}$ , and a set of hidden attributes $A_{j}$ for each $v_{j}$ . We have a prior $P(A_{j})$ , and $^a$ set of factors $\phi_{i}(A_{j},B_{i},C_{i})$ that are vacuous (uniform) if $C_{i}\ne j$ . We want to compute the posterior over $A_{j}$ . For this problem, 

![](images/29f10e8fc003f2b49121e915cab41c5b15063f20ef89082f51f305d13cfa39d2.jpg) 
Figure 12.D.1 — Results of a correspondence algorithm for 3D human body scans The model ex- plicitly captures correlations between diferent correspondence variables. 

one obvious extension of the MCMC approach we described is to use a collapsed MCMC approach, where we sample the correspondence variables but maintain a closed-form distribution over $A_{j}$ ; see exercise 12.29. When the features $A_{k}$ are such that maintaining a closed-form posterior is challenging (for example, they are continuous and do not have a simple parametric form), we often adopt an approach where we pick a single assignment to each $A_{j}$ ; this solution can be implemented in the framework of an EM algorithm (see box 19.D for one example). 

correlated correspondence Directly Correlated Correspondences A ﬁnal complication arises when we wish to model direct correlations between the values of diferent correspondence variables. This type of correlation would arise naturally, for example, in an application where we match visual features (small patches) in two images of the same real-world object (say a person). Here, we not only want the local appearance of a feature to be reasonably similar in the two images; we also want the relative geometry of the features to be consistent. For example, if a patch containing the eye is next to a patch containing the nose in one image, they should also be close in the other. These (soft) constraints can be represented as potentials on pairs of variables $C_{i_{1}},C_{i_{2}}$ (or even on larger subsets). Similar situations arise in sequence alignment, where we often prefer to match adjacent sequence elements (whether words or genes) on one side to adjacent sequence elements on the other. 

Here, we can often exploit the fact that the spatial structure we are trying to encode is local, so that the resulting Markov network over the $C_{i}$ variables is often reasonably structured. As a consequence, techniques such as loopy belief propagation, if carefully designed, can be surprisingly efective in this setting. 

Figure 12.D.1 demonstrates the results of one model along these lines. The model, due to Anguelov et al. (2004), aims to ﬁnd a correspondence between a set of (automatically selected) landmarks on diferent three-dimensional scans of human bodies. Here, $\phi_{i}(C_{i}=j)$ represents the extent to which the local appearance around the i th landmark in one scan is similar to the local appearance of the j th landmark on the other. This task is very challenging for two reasons. First, the local appearance of corresponding patches on two scans can be very diferent, so that the node potentials are only somewhat informative. Second, the two scans exhibit signiﬁcant deformation, both of shape and of pose, so that standard parametric models of deformation do not work. In this task, modeling correlations between the diferent correspondence variables allows us to capture constraints regarding the preservation of the object geometry, speciﬁcally, the fact that distances between corresponding points should be roughly preserved. This feature was essential for obtaining reasonable correspondences. Most of the constraints regarding preservation of distances relate to pairs of nearby points, so that the resulting Markov network was not too densely connected, allowing a judicious application of loopy belief propagation to work well. 

# 12.5 Deterministic Search Methods $\star$ 

So far, we have focused on particles generated by random sampling. Random sampling tries to explore the state space of the distribution “uniformly,” generating each state proportionately to its probability. A sampling-based approach can, however, be problematic when we have a highly skewed distribution, where only a small number of states have nonnegligible probability. In this case, sampling methods will tend to sample the same small set of states repeatedly, wasting computational resources to no gain. An alternative approach, designed for settings such as this, is to use a deterministic method that explicitly searches for high-probability states. 

Intuitively, in these search methods, we deterministic ally generate some set of distinct assign- ments $\mathcal{D}=\{\xi[1],\dots,\xi[M]\}$ . We then approximate the joint distribution $P$ by considering only these instantiations, ignoring the rest. Intuitively, if our particles account for a large proportion of the probability mass, we have a reasonable approximation to the joint. 

In the Student network of ﬁgure 12.1, the most likely ten instantiations (of 48), in decreasing order, are: 

![](images/6fe3effbc2e2617ccc89dbb9d066345aceb25d3131620c15fe0060fbbdeb7c13.jpg) 

Together, they account for about 82.6 percent of the probability mass. To account for 95 percent of the probability mass, we would need twenty-two instantiations, and to account for 99 percent of the mass, we would need thirty-three instantiations. 

Intuitively, the quality of our particle-based approximation to the joint distribution improves with the amount of probability mass accounted for in our particles. Thus, our goal is to enu- merate instantiations that have high probability in some unnormalized measure $\tilde{P}$ . Speciﬁcally, in the case of Bayesian networks, we want to enumerate instantiations that are likely given $e$ . 

We can formalize this goal as one of ﬁnding the highest-probability instantiations in $\tilde{P}$ , until we reach some upper bound $K$ on the number of particles. (One might be tempted to use, as a stopping criterion, a lower bound on the total mass accumulated in the enumerated particles; however, because we generally do not know the normalizing constant, the absolute mass accumulated has no real interpretation.) Clearly, this problem encompasses within it the task of ﬁnding the single most likely instantiation, also known as the maximum a posteriori (MAP) assignment . The problem of ﬁnding the MAP assignment is the focus of chapter 13. Not surprisingly, many of the successful methods for enumerating high-probability assignments are extensions of methods for ﬁnding the MAP assignment. Thus, we largely defer discussion of methods for ﬁnding high-probability particles to that chapter. (See, for example, exercise 15.10.) In this section, we focus primarily on the question of using a set of deterministic ally selected particles to approximate the answers to conditional probability queries. 

Consider any event $Z=z$ . Approximating the joint using the set of high-probability assign- ments $\mathcal{D}$ , we have that one natural estimate for $\tilde{P}(z)$ is: 

$$
\sum_{m=1}^{M}{I\!\!\!\!/}\{z[m]=z\}\tilde{P}(\xi[m]),
$$ 

where $z[m]=\xi[m]\langle{\cal Z}\rangle=z$ . It is important to note a key diference between this estimate and the one of equation (12.2). There, we merely counted particles, whereas in this case, the particles are weighted by their probability. Intuitively, the diference is due to the fact that, in sampling methods, particles are generated proportionately to their probability. Thus, higher- probability instantiations are generated more often. If we then also weighted the sampled particles by their probability, we would be “double-counting” the probability. Alternatively, we can view this formula as an instance of the importance sampling estimator, equation (12.8). In this case, each particle $\xi[m]$ is generated from a (diferent) deterministic proposal distribution $Q$ , that ascribes it probability $1.^{4}$ Hence, we must weight the particle by $\tilde{P(\xi[m])}/Q(\xi[m])=\tilde{P}(\xi[m])$ . 

lower bound 

Here, however, we have no guarantees about the bias of the estimator. Depending on our search procedure, we might end up with estimates that are arbitrarily bad. At one extreme, for example, we might have a search procedure that avoids (for as many particles as possible) any instantiation $\xi$ where $\xi\langle Z\rangle=z$ . Clearly, our estimate will then be biased in favor of low values. A more correct approach is to use our particles to provide both an upper and lower bound to the unnormalized probability of any event $_z$ . Some of our particles $\xi[m]$ have $z[m]=z$ . The total probability mass of these particles is a lower bound on the probability mass of all instantiations $\xi$ where $Z=z$ . Similarly, the total probability mass of particles that have $z[m]\neq z$ is a lower bound on the complementary mass, and hence provides an upper bound on the probability mass of the assignments where $Z=z$ : 

$$
\sum_{m=1}^{M}\pmb{I}\{z[m]=z\}\tilde{P}(\xi[m])\leq\tilde{P}(\pmb{Z}=z)\leq\left(1-\sum_{m=1}^{M}\pmb{I}\{z[m]\neq z\}\tilde{P}(\xi[m])\right).
$$ 

Equivalently, we can deﬁne $\begin{array}{r}{\rho=1-\sum_{m=1}^{M}\tilde{P}(\xi[m])}\end{array}$ to be the probability mass not accounted for by our particles. The bound can then be rewritten as: 

$$
\sum_{m=1}^{M}\pmb{1}\{\pmb{z}[m]=\pmb{z}\}\tilde{P}(\xi[m])\leq\tilde{P}(\pmb{Z}=\pmb{z})\leq\rho+\sum_{m=1}^{M}\pmb{1}\{\pmb{z}[m]=\pmb{z}\}\tilde{P}(\xi[m]).
$$ 

This reformulation reﬂects the fact that the unaccounted probability mass can be associated either with the event $Z=z$ or with its complement. If all of the unaccounted probability mass is associated with $_z$ , we get the upper bound, and if all of it is associated with the complement, we get the lower bound. 

Consider the Student network of ﬁgure 12.1, and the event $l^{1}\mathrm{~-~}a$ strong letter. Within the ten particles of example 12.19, the particles compatible with this event are: $\xi$ [3] , $\xi$ [4] , $\xi$ [5] , $\xi$ [7] , $\xi$ [8] , and $\xi$ [9] . Together, they account for 0 . 433 of the probability mass in the joint. The complementary event accounts for 0 . 393 of the mass. The total unaccounted mass is $1-0.826\,=\,0.174$ . From these numbers, we obtain the following bounds: 

$$
0.433\leq P(l^{1})\leq1-0.393=0.433+0.174=0.607.
$$ 

The true probability of this event is 0 . 502 . 

Now, consider the event $i^{0},l^{1}\ -\ a$ weak student getting a strong letter. In this case, we have three instantiations compatible with this assignment: $\xi$ [4] , $\xi$ [5] , and $\xi$ [8] . Together, they account for 0 . 244 of the probability mass. The remaining assignments, which are incompatible with the event, account for 0 . 582 of the mass. Altogether, we obtain the following bounds for the probability of this event: 

$$
0.244\leq P(i^{0},l^{1})\leq(1-0.582)=0.418.
$$ 

The true probability of this event is 0 . 272 . 

When evaluating these results, it is important to note that they are based on the use of only ten particles. The results from any sampling-based method that uses only ten particles would probably be signiﬁcantly worse. 

We can see that the true probability can lie anywhere within the interval speciﬁed by equa- tion (12.35), and there is no particular justiﬁcation for choosing any particular point within the range (for example, the point speciﬁed by equation (12.34)). If our search happens to ﬁrst ﬁnd instantiations that are compatible with $_z$ , the lower bound is likely to be a better estimate; if it ﬁrst ﬁnds instantiations that are incompatible with $_z$ , the upper bound is likely to be closer. 

The fact that we obtain both lower and upper bounds on the mass in the unnormalized measure also allows us to provide bounds on the value of probability (or conditional) probability queries. Assume we are trying to compute $P(\pmb{y}\mid e)=\bar{P}(\pmb{y},e)\dot{/}\bar{P}(e)$ . We can now obtain upper and lower bounds for both the numerator and denominator, using equation (12.35). A lower bound for the numerator and an upper bound for the denominator provide a lower bound for the ratio. Similarly, an upper bound for the numerator and a lower bound for the denominator provide an upper bound for the ratio. Analogously, we can obtain bounds on the marginal probability $P(\pmb{y})$ by normalizing $\tilde{P}(\pmb{y})$ by $\tilde{P}(\tilde{t r u e})$ . 

More precisely, assume that we have bounds: 

$$
\begin{array}{r l r}{\ell_{{\pmb y},e}}&{\leq}&{P({\pmb y},e)\quad\leq\quad u_{{\pmb y},e}}\\ {\ell_{e}}&{\leq}&{P(e)\quad\leq\quad u_{e}.}\end{array}
$$ 

Then we can bound: 

$$
\begin{array}{r l r}{\frac{\ell_{y,e}}{u_{e}}}&{\le}&{P(\pmb{y}\mid e)\quad\le\quad\frac{u_{\pmb{y},e}}{\ell_{e}}.}\end{array}
$$ 

Example 12.21 Assume we want to compute $P(i^{0}\mid l^{1})$ — the probability that a student with a strong letter is not intelligent — using our ten samples from example 12.19. We have already shown the bounds for both the numerator and the denominator. We obtain: 

$$
\begin{array}{r l r l r}{0.265/0.607}&{\leq}&{P(i^{0}\mid l^{1})}&{\leq}&{0.439/0.433}\\ {0.437}&{\leq}&{P(i^{0}\mid l^{1})}&{\leq}&{1.014.}\end{array}
$$ 

The true probability of this event is 0 . 542 , which is fairly far away from any of the bounds. 

Interestingly, the upper bound in this case is greater than 1 . Although clearly valid, this con- clusion is not particularly interesting. In general, the deterministic approximations are of value only if we can cover a very large fraction of our probability mass with a small number of particles. While this constraint might seem very restrictive (as it often is), there are nevertheless applications where the probabilities are very skewed, and this assumption is a very good one; we return to this point in section 12.6. 

A very similar discussion applies to the extension of deterministic search methods to collapsed particles. In this case, we approximate the outer expectation in equation (12.29) using a set $\mathcal{D}$ of particles $\pmb{x}_{p}[1],\pmb{x}.\pmb{x}.,\pmb{x}_{p}[M]$ , which are selected using some deterministic search procedure. As usual, each particle is associated with a distributi $P(X_{d}\mid\mathbf{\mathcal{X}}_{p}[m],e)$ . 

Consider the task of computing the probability $P(z)$ . As in the case of full particles, each of the generated particles $\pmb{x}_{p}[m]$ accounts for a certain part of the probability mass. However, in this case, we cannot simply test whether the event $_z$ is compatible with the particle, since the particle might not specify a full assignment to the variables $Z$ . Rather, we have to compute the probability of $_z$ relative to the distribution over all of the variables deﬁned by the particle. (See exercise 12.26.) 

Thus, in particular, the lower bound on $P(z)$ deﬁned by our particles is: 

$$
\begin{array}{r c l}{{}}&{{}}&{{\displaystyle\sum_{m=1}^{M}P(\pmb{x}_{p}[m])\left(E_{P(\pmb{X}_{d}\mid\pmb{x}_{p}[m])}[I\{\pmb{x}_{p},\pmb{X}_{d}\langle\pmb{Z}\rangle=\pmb{z}\}]\right)}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{\displaystyle\sum_{m=1}^{M}P(\pmb{x}_{p}[m])P(\pmb{z}\mid\pmb{x}_{p}[m]).}}\end{array}
$$ 

The lower bound assumes that none of the unaccounted probability mass is compatible with $_z$ . Similarly, the upper bound assumes that all of this unaccounted mass is compatible with $_z$ , leading to: 

$$
\left(\sum_{m=1}^{M}P(\boldsymbol{z},\mathbf{x}_{p}[m])\right)+\left(1-\sum_{m=1}^{M}P(\mathbf{x}_{p}[m])\right).
$$ 

Once again, we can compute both the probability $P(z\mid\mathbf{\sigma}x_{p})$ and the weight $P(x_{p})$ by using inference in the conditioned network. 

incremental conditioning 

bounded conditioning 

This method is simply an incremental version of the conditioning algorithm of section 9.5, using $\pmb{x}_{p}$ as a conditioning set. However, rather than enumerating all possible assignments to the conditioning set, we enumerate only a subset of them, attempting to cover most of the probability mass. This algorithm is also called bounded conditioning . 

# 12.6 Summary 

This chapter presents a series of methods that attempt to approximate a joint distribution using a set of particles. We discussed three main classes of techniques for generating particles. 

Importance sampling, and speciﬁcally likelihood weighting, generates particles by random sampling from a distribution. Because we cannot, in general, sample from the posterior dis- tribution, we generate particles from a diferent distribution, called the proposal distribution, and then adjust their weights so as to get an unbiased estimator. The proposal distribution is a mutilated Bayesian network, and the samples are generated using forward sampling. Owing to the use of forward sampling, likelihood weighting applies only to directed graphical mod- els. However, if we somehow choose a proposal distribution, the more general framework of importance sampling can also be applied to undirected graphical models (see exercise 12.9). 

Markov chain Monte Carlo techniques attempt to generate samples from the posterior distri- bution. We deﬁne a Markov chain, or a stochastic sampling process, whose stationary distri- bution (the asymptotic result of the sampling process) is the correct posterior distribution. The Metropolis-Hastings algorithm is a general scheme for specifying a Markov chain that induces a particular posterior distribution. We showed how we can use the Metropolis-Hastings algorithm for graphical models. We also showed a particular instantiation, called Gibbs sampling, that is speciﬁcally designed for graphical models. 

Finally, search methods take a diferent approach, where particles are generated determin- istically, trying to focus on instantiations that have high probability. Unlike random sampling methods, deterministic search methods do not provide an unbiased estimator for the target query. However, they do provide sound upper and lower bounds. When the probability distribu- tion is highly difuse, so that many particles are necessary to cover most of the probability mass, these bounds will be very loose, and generally without value. However, when a small number of instantiations account for a large fraction of the probability mass, deterministic search tech- niques can obtain very accurate results with a very small number of particles, often much more accurate results than sampling-based methods with a comparable number of particles. There are several applications that have this property. For example, when performing fault diagnosis (see, for example, box 5.A), where faults are very rare, it can be very efcient to enumerate all hypotheses where the system has up to $K$ faults. Because multiple faults are highly unlikely, 

even a small value of $K$ (2 or 3) will likely sufce to cover most of the probability mass — even the mass consistent with our evidence. Another example is speech recognition (box 6.B), where only very few trajectories through the HMM are likely. In both of these applications, deterministic search methods have been successfully applied. 

From a high level, it appears that sampling methods are the ultimate general-purpose inference algorithm. They are the only method that can be applied to arbitrary probabilistic models and that is guaranteed to achieve the correct results at the large sample limit. Indeed, when faced with a complex probabilistic model that involves continuous variables or a nonparametric model, there are often very few other choices available to us. While optimization-based methods, such as those of chapter 11, can sometimes be applied, the application often requires a nontrivial derivation, speciﬁc to the problem at hand. Moreover, these methods provide no accuracy guarantee. Conversely, it seems that sampling-based methods can be applied easily, of-the- shelf, to virtually any model. 

This impression, however, is somewhat misleading. While it is true that sampling methods  provide asymptotic guarantees, their performance for reasonable sample sizes is very difcult to predict. In practice, a naive application of sampling methods to a complex probabilistic model often fails dismally, in that the estimates obtained from any rea- sonable number of samples are highly inaccurate. Thus, the success of these methods depends heavily on the properties of the distribution, and on a careful design of our sampling algorithm. Moreover, there is little theoretic basis for this design, so that the process of getting sampling methods to work is largely a matter of intuition and intensive experimentation. 

Nevertheless, the methods described in this chapter do provide an important component in our arsenal of methods for inference in complex models. Moreover, they are often used very successfully in combination with exact or approximate global inference methods. Standard com- binations include the use of global inference for providing more informed proposal distributions, and for manipulating collapsed particles. Such combinations are highly successful in practice, and they often lead to much better results than any of the two types of inference methods in isolation. 

Having described these basic methods, we showed how they can be extended to the case of collapsed particles, which consist of an assignment to a subset of network variables, associ- ated with a closed-form distribution over the remaining ones. The answer to a query is then a (possibly weighted) sum over the particles, of the answer to the query within each associated dis- tribution. This approach approximates part of the inference task via particles, while performing exact inference on a subnetwork, which may be simpler than the original network. 

# 12.7 Relevant Literature 

The $\mathcal{N P}$ -hardness of approximate probabilistic inference in Bayesian networks was shown by Dagum and Luby (1993). 

There is a vast literature on the use of Monte Carlo methods for estimating integrals in general, and the expectation of functions in particular. See Robert and Casella (2005) for one good review. Geweke (1989) proves some of the basic results regarding the accuracy of the importance sampling estimates. Probabilistic logic sampling was ﬁrst proposed by Henrion (1986). The improvement to likeli- 

hood weighting was proposed independently by Fung and Chang (1989) and by Shachter and Peot (1989). Cano et al. (2006) and Dagum and Luby (1997) proposed a variant of likelihood weighting based on unnormalized importance sampling, which separately estimates $P(\pmb{y},e)$ and $P(e)$ , as described in section 12.2.3.2. Dagum and Luby also proposed the use of a data-dependent stopping rule for the case where the CPD entries are bounded away from 0 or 1 . For this case, they provide guaranteed bounds on the expected number of samples required to achieve a certain error rate, as discussed in section 12.2.3.2. Pradhan and Dagum (1996) provide some empirical validation of this algorithm, applied to a large medical diagnosis network. 

Various heuristic approaches for improving the proposal distribution have been proposed. Fung and del Favero (1994) proposed the backward sampling algorithm, that allows for generating samples from evidence nodes in the direction that is opposite to the topological order of nodes in the network, combining their likelihood function with the CPDs of some previously sampled variables. Other variants use some alternative form of approximate inference over the network to produce an approximation $Q(X)$ to $P(X\mid E=e)$ , and then use $Q$ as a proposal distribution for importance sampling. For example, de Freitas et al. (2001) use variational methods (described in chapter 11) in this way. 

Shachter and Peot (1989) proposed an adaptive approach, called self-importance sampling , which adapts the proposal distribution to the samples obtained, attempting to increase the probability of sampling in higher-probability regions of the posterior distribution $P(X\mid e)$ . This approach was subsequently improved by Cheng and Druzdzel (2000) and by Ortiz and Kaelbling (2000), who proposed an adaptive importance sampling method that uses the variance of the estimator in a more principled way. 

Shwe and Cooper (1991) applied importance sampling to the QMR-DT network for medical diagnosis (Shwe et al. 1991). Their variant of the algorithm combined self-importance sampling and an improved proposal distribution called Markov blanket scoring. This proposal distribution was designed to be computed efciently in the context of BN2O networks. 

Sampling methods based on Markov chains were ﬁrst proposed for models arising in statistical physics. In particular, the Metropolis-Hastings algorithm was ﬁrst proposed by Metropolis et al. (1953). Geman and Geman (1984) applied Gibbs sampling to image restoration in a paper that was very inﬂuential in popularizing this method within computer vision, and subsequently in related communities. Ackley, Hinton, and Sejnowski (1985) propose the use of Gibbs sampling within Boltzmann machines, for both inference and parameter estimation. Pearl (1987) introduced Gibbs sampling for Bayesian networks. York (1992) continues this work, speciﬁcally addressing the problem of networks where some states have low (or even zero) probability. 

Over the years, extensive work has been done on the topic of MCMC methods, addressing a broad range of topics including: theoretical analyses, application to new classes of models, improved algorithms that are faster or have better convergence properties, speciﬁc applications, and many more. Works reviewing some of these developments include Neal (1993); Smith and Roberts (1993); Gilks et al. (1996); Gamerman and Lopes (2006). Neal (1993), in particular, provides both an excellent tutorial, guidelines for practitioners, and a comprehensive annotated bibliography for relevant papers (up to 1993). Tierney (1994) discusses the conditions under which we can use multiple kernels within a single chain. Nummelin (1984, 2002) shows the central limit theorem for samples from a Markov chain. MacEachern and Berliner (1994) show that subsampling the samples derived from a Markov chain is suboptimal. Gelman and Rubin (1992) provide a speciﬁc strategy for applying MCMC: they specify the number of chains, the burn-in time, and the intervals at which samples should be selected. Their strategy is applicable mostly for problems for which a good initial distribution is available, but provides insights more broadly. 

annealed importance sampling 

Algorithms that improve convergence are particularly relevant for the high-dimensional, mul- timodal distributions that often arise in the setting of graphical models. Some methods for addressing this issue use larger, nonlocal steps in the search space, which are helpful in break- ing out of local optima; for example, for pairwise MRFs where all variables have a uniform set of values, Swendsen and Wang (1987) and Barbu and Zhu (2005) propose moves that simulta- neously ﬂip an entire subgraph from one value to another. Higdon (1998) discusses the general idea of introducing auxiliary variables as a mechanism for taking larger steps in the space. The temperature-based methods draw on the idea of simulated annealing (Kirkpatrick et al. 1983). These methods include simulated tempering (Marinari and Parisi 1992; Geyer and Thompson 1995) in which the state of the model is augmented with a temperature variable for purposes of sampling; parallel tempering (Swendsen and Wang 1986; Geyer 1991) runs multiple chains at diferent temperatures at the same time and allows chains to exchange datapoints; tempered transitions (Neal 1996) proposes a new sample by moving up and down the temperature sched- ules; and annealed importance sampling Neal (2001) uses a similar approach in combination with an importance sampling reweighting scheme. 

The bugs system is an invaluable tool for the application of MCMC methods, inasmuch as it encompasses a large class of models and implements a number of fairly advanced techniques for improving the mixing rate. The system itself, and some of the ideas in it, are described by Thomas et al. (1992) and Gilks et al. (1994). 

The task of ﬁnding high-probability assignments in a probabilistic model is very closely related to the problem of ﬁnding a MAP assignment. We refer the reader to section 13.9 for many of the relevant references on that topic. 

Techniques based on deterministic search were popular in the early days of the Bayesian network community, often motivated by connections with search algorithms and constraint- satisfaction algorithms in AI. As a few examples, Henrion (1991) proposes a search-based algo- rithm aimed speciﬁcally at BN2O networks, whereas Poole (1993b, 1989) describes an algorithm aimed at more general network structures. Horvitz et al. (1989) propose an algorithm that com- bines conditioning with search, to obtain some of the beneﬁts of exact inference in a search- based algorithm. More recently, Lerner et al. (2000) use a collapsed search-based approach for the problem of fault diagnosis, where probabilities are also highly skewed. 

The use of collapsed particles, and speciﬁcally the Rao-Blackwell theorem, for sampling-based estimation was proposed by Gelfand and Smith (1990) and further developed by Liu et al. (1994) and Robert and Casella (1996). This idea has since been used in many applications of sampling techniques to graphical models, where sampling some of the variables can often allow us to perform tractable inference over the others. The idea of sampling some of the variables in a Bayesian network and performing exact inference over the others was explored by Bidyuk and Dechter (2007). 

partition function 

We have focused our presentation here on the problem of generating samples from a distri- bution so as to estimate the posterior. However, another task of signiﬁcant practical importance is that of computing the partition function of an unnormalized measure. As we will see, this task is of particular importance in learning, since the normalizing constant is often used as a measure of the overall quality of a learned model. Computation of a partition function by directly sampling the distribution leads to estimates of high variance, since such estimates are usually dominated by a small number of samples with high unnormalized probabilities. In order to avoid this problem, a ratio of partition functions is computed; see exercise 12.8. An equivalent problem of free energy diference, or partition function ratio, has been tackled in computa- tional physics and computation chemistry communities with a range of methods, including free energy perturbation, thermodynamic integration, bridge sampling, umbrella sampling, and Jarzynski equation; these methods are in essence importance-sampling algorithms. See Gelman and Meng (1998); Jarzynski (1997); Neal (2001) for some examples. 

Extensive work has been done on the correspondence problem in its various incarnations: data association, record matching, identity resolution, and more. See, for example, Bar-Shalom and Fortmann (1988); Bar-Shalom (1992) for a review of some of the key ideas. Pasula et al. (1999) ﬁrst proposed the use of MCMC methods to sample the space of possible associations in target tracking, based on the analysis of Jerrum and Sinclair (1997) for sampling over matchings. Dellaert et al. (2003) also use MCMC for the data-association problem in a structure-from- motion task in computer vision; they propose a more sophisticated proposal distribution that allows more rapid mixing. Anguelov et al. (2004) suggest the use of belief propagation for solving the correspondence problem, in settings where the correspondences are correlated with each other. Their results form part of box 12.D. Some work has also been done on ﬁnding the MAP assignment to a data-association problem using variants of belief propagation (Chen et al. 2003; Duchi et al. 2006) or other methods (Lacoste-Julien et al. 2006). 

# 12.8 Exercises 

# Exercise $12.1\star$ 

Consider the sequence of variables $T_{n}$ deﬁned in appendix A.2. Given $\epsilon$ and $\delta$ , we deﬁne $m(\epsilon,\delta)\,=$ $\begin{array}{r}{\operatorname*{min}_{n}P(|T_{n}-\hat{p}|\,\geq\,\epsilon)\,\leq\,\delta}\end{array}$ to be s allest numbe of trials we need to ensure tha robability of deviating ϵ standard deviations from p is less than δ . Although we cannot compute $m(\epsilon,{\hat{\delta}})$ exactly, we can upper-bound it using the bounds we discuss earlier. 

a. Use Chebyshev’s bound to give an upper bound on $m(\epsilon,\delta)$ . (You can use the fact that $\begin{array}{r}{\mathbb{W}a r[T_{n}]\leq\frac{1}{4n}}\end{array}$ ≤ .) b. Use the Chernof bounds discussed above to give an upper bound on $m(\epsilon,\delta)$ . c. What is the diference between these two bounds? How does each of these depend on $\epsilon$ and $\delta\P$ What are the implications if we want design a test to estimate $p?$ ? 

# Exercise 12.2 

Consider the problem of providing a reliable estimate for a ratio $p/q$ , where we have estimators $\hat{p}$ for $p$ and $\hat{q}$ for $q$ . 

a. Assume that $\hat{p}$ has $\epsilon$ relative error to $p$ and $\hat{q}$ has $\epsilon$ relative error to $q$ . More precisely, $\hat{p}\,\in\,[(1\mathrm{~-~}$ − $\epsilon)p$ $\cdot,(1+\epsilon)p]$ , and $\hat{q}\in[(1-\epsilon)q,(1+\epsilon)q]$ ∈ − . Provide a relative error bound on $\hat{p}/\bar{q}$ relative to p/q . b. Now, assume that we have only an absolute error bound for $\hat{p}$ and $\hat{q}$ : $\hat{p}\in\left[p\!-\!\epsilon,p\!+\!\epsilon\right]$ ∈ − ] , $\hat{q}\in[q-\epsilon,q+\epsilon]$ ∈ − . Show by example that $\hat{p}/\hat{q}$ can be an arbitrarily bad estimate for p/q . c. What type of guarantee (if any) can you provide if $\hat{p}$ has low absolute error but $\hat{q}$ has low relative error? 

# Exercise 12.3 

Assume we have a calibrated clique tree for a distribution $P_{\Phi}$ . Show how we can use the clique tree to generate samples that are sampled exactly from $P_{\Phi}$ . 

# Exercise 12.4 

Prove proposition 12.2. 

# Exercise $12.5\star$ 

Let $\mathcal{B}$ be a Bayesian network, and $X$ a variable th ot of the network. Show that $P_{\mathcal{B}}$ may not be the optimal proposal distribution for estimating $P_{\mathcal{B}}(X=x)$ . 

# Exercise 12.6 

edge reversal In exercise 3.12, we deﬁned the edge reversal transformation, which reverses the directionality of an edge $X\rightarrow Y$ . How would you apply such a transformation in the context of likelihood weighting, and what would be the beneﬁts? 

# Exercise $12.7\star$ 

Let $\mathcal{B}$ be a network, $E=e$ an o ervation, and $X_{1},\ldots,X_{k}$ be some ordering (not pological) of the unobserved varia B . onsider a sampling algor $X_{1},\dots,X_{k}$ in order, randomly selects a value $x_{i}[m]$ for $X_{i}$ using some distribution $Q(X_{i}\mid x_{1}[m],.\,.\,.\,,x_{i-1}[m],e)$ | ) . − 

a. Write the formula for the importance weights in normalized importance sampling using this sampling process. b. Using your answer in part 1, deﬁne an improved likelihood weighting algorithm that samples variables in the network in topological order, but, for a variable $X_{i}$ with parents $U_{i}$ , samples $X_{i}$ using a distribution that uses both $\mathbf{\nabla}x_{-i}[m]$ and the evidence in $X_{i}$ ’s Markov blanket. c. Does your approach from part 2 generate samples from the posterior distribution $P(X_{1},.\,.\,.\,,X_{k}\mid e)?$ Explain. 

# Exercise ${\bf12.8\star}$ 

Consider the normalized importance sampling algorithm, but now assume that, in equation (12.10), an unnormalized measure ${\tilde{Q}}(X)$ is used in place of $Q(X)$ . Show that the average of the weights converges to $\frac{\sum_{\pmb{X}}\tilde{P}(\pmb{x})}{\sum_{\pmb{X}}\tilde{Q}(\pmb{X})}$ , which is the ratio of the normalizing constants of the two distributions. 

# Exercise 12.9 

In this question, we consider the application of importance sampling to Markov networks. 

a. Explain intuitively why we cannot simply apply likelihood weighting to Markov networks. b. Show how likelihood weighting can be applied to chordal Markov networks. Is this approach interesting? Explain. c. Provide a technique by which the more general framework of importance sampling can be applied to Markov networks. Be sure to deﬁne both a reasonable proposal distribution and an algorithmic technique for computing the weights. 

# Exercise $12.10\star$ 

Consider the Grasshopper example of ﬁgure 12.3 

a. Assume that the probability space is the full set of (positive and negative) integers; the transition model is now the same for all $i$ (and not diferent for $i=\pm4)$ ). Assuming that the grasshopper starts from 0 , use the central limit theorem (theorem A.2) to bound the probability that the grasshopper reaches an integer larger than $\left\lceil\sqrt{2T}\right\rceil$ m after $T$ steps. b. Returning to the setting where the grasshopper moves ove the integers in the range $\pm B$ , try con- structing a chain that reaches every state in time linear in B (Hint: Your chain will be nonreversible, and it will require the addition of another variable to the state description.) 

# Exercise 12.11 

Show an example of a Markov chain where the limiting distribution reached via repeated applications of equation (12.20) depends on the initial distribution $P^{(0)}$ . 

# Exercise $12.12\times$ 

Consider the following two conditions on a Markov chain $\mathcal{T}$ : 

a. It is possible to get from any state to any state using a positive probability path in the state graph. b. For each state $x$ , there is a positive probability of transitioning directly from $x$ to $x$ (a self-loop). 

a. Show that, for a ﬁnite-state Markov chain, these two conditions together imply that $\mathcal{T}$ is regular. b. Show that regularity of the Markov chain implies condition 1. c. Show an example of a regular Markov chain that does not satisfy the condition 2. d. Now let us weaken condition 2, requiring only that there exists a state $x$ with a positive probability of transitioning directly from $x$ to $x$ . Show that this weaker condition and condition 1 together still sufce to ensure regularity. 

# Exercise 12.13 

Show directly from equation (12.21) (without using the detailed balance equation) that the posterior distri- bution $P(\mathcal{X}\mid e)$ is a stationary distribution of the Gibbs chain (equation (12.22)). 

# Exercise 12.14 

Show that any distribution $\pi$ that satisﬁes the detailed balance equation, equation (12.24), must be a stationary distribution of $\mathcal{T}$ . 

# Exercise 12.15 

Prove theorem 12.5. 

# Exercise 12.16 

Let $V a l(X)$ be a set of states, and let $\mathcal{T}_{1},\ldots,\mathcal{T}_{k}$ be a se of ker of which satisﬁes the detailed balance equation relative to some stationary distribution π . Let $p_{1},\ldots,p_{k}$ be any distribution over the dexes $1,\ldots,k$ . P ve that the mixture Markov chain $\mathcal{T}$ , which at each st takes a step sampled from $\mathcal{T}_{i}$ T with probability p i , also satisﬁes the detailed balance equation relative to π . 

# Exercise $12.17\star$ 

Let $\tau_{1},\dots,\tau_{k}$ b as in exercise 12 6. Consider the aggr te Markov chain $\mathcal{T}$ , where each step consists of a sequence of k steps, with step i being sampled from T . 

a. Provide an example dem strating that $\mathcal{T}$ may not satisfy the detailed balance equation relative to $\pi$ . b. Show that, nevertheless, T $\mathcal{T}$ has the stationary distribution $\pi$ . c. Provide a multistep kernel using $\mathcal{T}_{1},\ldots,\mathcal{T}_{k}$ that satisﬁes the detailed balance equation relative to $\pi$ . 

# Exercise 12.18 

a. $X$ e in a Markov network $\mathcal{H}$ , and let $_{_y}$ be an assignment of values to $X$ ’s Markov blanket $\pmb{Y}=\mathrm{MB}_{\mathcal{H}}(\boldsymbol{X})$ . Provide an efcient algorithm for computing H $\frac{P(x^{\prime}\mid\pmb{y})}{P(x\mid\pmb{y})}$ for $x,x^{\prime}\;\in\;V a l(X)$ . Your algorithm should involve only local parameters — potentials for cliques involving X . b. Show how this algorithm applies to Bayesian networks. 

# Exercise 12.19 

Show that Gibbs sampling is a special case of the Metropolis-Hastings algorithm. More precisely, pro- vide a particular proposal distribution $Q_{i}$ for each local transition $\boldsymbol{\mathcal{T}}^{\bar{Q}_{i}}$ that induce precisely the same distribution over the transitions taken as the associated Gibbs transition distribution . 

# Exercise ${\bf12.20\star}$ 

Prove theorem 12.4. 

# Exercise 12.21 

Consider the network of example 12.12, but now assume that we make $Z$ a slightly noisy exclusive or of its parents. That is, with some small probability $q,\;Z$ is chosen uniformly at random regardless of the values of $X$ and $Y$ , and with probability $1-q$ , $Z$ is the exclusive or of $X$ and $Y$ . Analyze the transitions between states in the Gibbs chain for this network, with the evidence z $z^{1}$ , and particularly the expected time required to transition between diferent regions. 

# Exercise 12.22 

Consider the same situation as in importance sampling, where we have an unnormalized measure ${\tilde{P}}(X)$ from which it is hard to sample, and a proposal distribution $Q$ which is (hopefully) close to the normalized distribution $P\propto{\tilde{P}}$ , from which we can draw independent samples. Consider a Markov chain where we deﬁne 

$$
\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime})=Q(\pmb{x}^{\prime})\operatorname*{min}\left[1,\frac{w(\pmb{x}^{\prime})}{w(\pmb{x})}\right]
$$ 

for $\pmb{x}^{\prime}\neq\pmb{x}$ , where $w(x)$ is as d ﬁne n equation (12.10); we deﬁne $\begin{array}{r}{\mathcal{T}(\pmb{x}\rightarrow\pmb{x})=1-\sum_{\pmb{x}^{\prime}\neq\pmb{x}}\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime})}\end{array}$ P . Intuitively, the transition from x to $\mathbf{\Phi}_{\mathbf{X}}^{\prime}$ selects an independent sample $\scriptstyle{\boldsymbol{x}}^{\prime}$ from Q and then moves toward it, depending on whether its importance weight is better than that of our current point $_{_{x}}$ . 

a. Show that $\mathcal{T}$ deﬁnes a legal transition model b. Show that P is the stationary distribution of . 

# Exercise $12.23\!\star\!\star$ 

Swendson-Wang algorithm labeling MRF 

The Swendson-Wang algorithm is a Metropolis-Hastings algorithm for labeling MRFs , such as those used in image segmentation (box 4.B), where all variables take values in the same set of labels $l\,=\,1,.\,.\,.\,,L$ . Unlike the standard application of Metropolis-Hastings, this variant takes large steps in the space, which simultaneously ﬂip the values of multiple variables together. 

The algorithm pro eds as follows. Let $\mathcal{X}$ be the variables in the network, and $\mathcal{E}$ the set of edges in the pairwise MRF. Let ξ be our current state in the sampling process. We begin by partitioning the variables $\mathcal{X}$ into $L$ disjoint subsets, based on their value in $\xi$ : ${\check{X_{l}}}\overset{\cdot}{=}\{X_{i}\ :\ x_{i}\overset{\cdot}{=}l\}$ We now randomly select a subset of the ed in the graph to roduce a new set E $\mathcal{E}^{\prime}$ ′ : each edge $(i,j)\in\mathcal{E}$ ∈E is selected indepen tly, with probability $q_{i,j}$ . We now use E $\hat{\mathcal{E}}^{\prime}$ to partition the graph into connected ponents. Each set $\boldsymbol{X}_{l}$ is partitioned into K connected components, where a connected component $\boldsymbol{X}_{l k}$ is a maximal subset of nodes $X_{l k}\subseteq X_{l}$ such that each $\bar{X_{i}},X_{j}\in X_{l k}$ is connected by a path within $\mathcal{E}^{\prime}$ . The result is a set of connected components: 

$$
{\mathcal C}=\{{\pmb X}_{l k}\ :\ l=1,.\,.\,.\,,L;k=1,.\,.\,.\,,K_{l}\}.
$$ 

We now select a nnected component $X_{l k}\in\mathcal{C}$ uniformly at random. Let $q(\pmb{Y}\mid\xi)$ be the probability with which a set Y is selected using this procedure. 

We now propose to assign $\mathbf{\Delta}X_{l k}$ a new label $l^{\prime}$ with probability $q(l^{\prime}\;\;\mid\xi,X_{l k})$ ; note that this probability can depend in arbitrary ways on the current state. This proposed move, if accepted, deﬁnes a new state $\xi^{\prime}$ , where $X_{i}^{\prime}=l^{\prime}$ for any $X_{i}\in X_{l k}$ , and $X_{i}^{\prime}=X_{i}$ otherwise. Note that this proposed move ﬂips a large number of variables at the same time, and thus it takes much larger steps in the space than a local Gibbs or Metropolis-Hastings sampler for this MRF. 

In this exercise, we show how to use this proposal distribution within the Metropolis-Hastings algorithm. 

a. Let $\xi$ and $\xi^{\prime}$ be a pair of states such that: 

• $\mathbf{Y}$ forms a conn ted component in $\mathcal{E}$ ; • the variables in Y all take the value l in $\xi$ ; • the variables in $\mathbf{Y}$ all take the value $l^{\prime}$ in $\xi^{\prime}$ ; • all other variables in $\mathcal{X}-Y$ take the same value in $\xi$ and $\xi^{\prime}$ . 

Show that 

$$
{\frac{q(Y\mid\xi)}{q(Y\mid\xi^{\prime})}}={\frac{\prod_{(i,j)\in{\mathcal{E}}(Y,X_{l}-Y)}(1-q_{i,j})}{\prod_{(i,j)\in{\mathcal{E}}(Y,X_{l^{\prime}}^{\prime}-Y)}(1-q_{i,j})}}
$$ 

where: $\boldsymbol{X}_{l}$ is the set of vertices with label $l$ in $\xi$ , $X_{l^{\prime}}^{\prime}$ the set of vertices with label $l^{\prime}$ in $\xi^{\prime}$ ; and where $\mathcal{E}(Y,Z)$ (between two disjoint sets $Y,Z)$ is the set of edges connecting nodes in $\mathbf{Y}$ to nodes in $Z$ . b. Use this fact to obtain an acceptance probability for the proposed move that respects the detailed- balance equation. 

# Exercise $12.24\!\star\!\star$ 

Let us return to the setting of exercise 9.20. 

a. Suppose now that you want to sample from $P(\mathcal{X}\mid\mathrm{PSimle}(\mathcal{X})=K)$ ) using a Gibbs sampling strategy. Why is this a bad idea? 

b. Can you devise a Metropolis-Hastings MCMC strategy that generates samples from the correct posterior? Describe the proposal distribution and the acceptance probabilities, and prove that your scheme does sample from the correct posterior. Explain intuitively why your chain is likely to work better than the Gibbs chain. You may assume that $1<K<N$ . 

# Exercise $12.25\!\star\!\star$ 

annealed importance sampling 

In this exercise we develop the annealed importance sampling procedure. Assume that we want to generate samples from $p(\pmb{x})\ \propto\ f(\pmb{x})$ from which sampling is hard. Assume also that we have a distribution $q(\pmb{x})\ \propto\ g(\pmb{x})$ ∝ from which sampling is easy. In principle, we can use $q$ as a proposal distribution for p , and apply importance sampling. However, if $q$ and $p$ are very diferent, the results are likely to be quite poor. We now construct a sequence of Markov chains that will allow us to incrementally produce a lower-variance importance-sampling estimator. 

e technique is as follows. We deﬁne a sequence of distributions, $p_{0},\ldots,p_{k}$ , where $p_{i}({\pmb x})\propto f_{i}({\pmb x})$ , and $f_{i}$ is deﬁned as: 

$$
f_{i}({\pmb x})=p({\pmb x})^{\beta_{i}}q({\pmb x})^{1-\beta_{i}},
$$ 

where $1=\beta_{0}>\beta_{1}>.$ . Note that $p_{\mathrm{0}}=p$ and $p_{k}=q$ . We assume that we can generate samples from $p_{k}$ , and that, for each $p_{i}$ , $i=1,\ldots,k-1$ , we have a Markov chain $\mathcal{T}_{i}$ hose stationary distribution is p . To generate a weighted sample $\mathbfit{x},w$ relative to our target distribution p , we follow the following algorithm: 

$$
\begin{array}{l l l}{{\pmb x}_{k}}&{{\sim}}&{{p}_{k}(\pmb X)}\\ {{\pmb x}_{i}}&{{\sim}}&{{\mathcal T}_{i}({\pmb x}_{i+1}\rightarrow\pmb X)\ \ \ i=(k-1),.\,.\,.\,,1.}\end{array}
$$ 

Finally, we deﬁne our sample to be ${\pmb x}={\pmb x}_{1}$ , with weight 

$$
w=\prod_{i=1}^{k}{\frac{f_{i-1}({\pmb x}_{i})}{f_{i}({\pmb x}_{i})}}.
$$ 

To prove that these importance weights are correct, we deﬁne both a target distribution and a proposal distribution over the larger state space $({\pmb x}_{1},.\cdot\cdot,{\pmb x}_{k})$ . We then show that the importance weights deﬁned in equation (12.38) are correct relative to these distributions over the larger space. 

a. Let 

$$
\mathcal{T}_{i}^{-1}(\pmb{x}\rightarrow\pmb{x}^{\prime})=\mathcal{T}_{i}(\pmb{x}^{\prime}\rightarrow\pmb{x})\frac{f_{i}(\pmb{x}^{\prime})}{f_{i}(\pmb{x})}
$$ 

deﬁne the reversal of the transition model deﬁned by $\mathcal{T}_{i}$ . Show that ${\mathcal{T}}_{i}^{-1}(X\,\rightarrow\,X^{\prime})$ → is a valid transition model. 

b. Deﬁne 

$$
f^{*}(\pmb{x}_{1},.\,.\,.\,,\pmb{x}_{k})=f_{0}(\pmb{x}_{1})\prod_{i=1}^{k-1}\mathcal{T}_{i}^{-1}(\pmb{x}_{i}\to\pmb{x}_{i+1}),
$$ 

$p^{*}({\pmb x}_{1},.\,.\,.\,,{\pmb x}_{k})\ \propto\ f^{*}({\pmb x}_{1},.\,.\,.\,,{\pmb x}_{k})$ . Use your answer from above to conclude that $p^{*}(\mathbf{x}_{1})=\bar{p(\mathbf{x}_{1})}$ . 

c. Let $g^{*}$ be the function encoding the joint distribution from which $\pmb{x}_{1},\dots,\pmb{x}_{k}$ are sampled in the annealed importance sampling procedure equation (12.37). Show that the weight in equation (12.38) can be obtained as 

$$
{\frac{f^{*}({\pmb x}_{1},.\,.\,.\,,{\pmb x}_{k})}{g^{*}({\pmb x}_{1},.\,.\,.\,.\,,{\pmb x}_{k})}}.
$$ 

One can show, under certain assumptions, that the variance of the weights obtained by this procedure grows linearly in the dimension $n$ of the number of variables $_{X}$ , whereas the variance in a traditional importance sampling procedure grows exponentially in $n$ . 

# Exercise 12.26 

This exercise explores one heuristic approach for deterministic search in a Bayesian network. It is an intermediate method between full-particle search and collapsed-particle search: It uses partial instantiations as particles but does not perform inference on the resulting conditional distribution. 

Assume that our goal is to provide upper and lower bounds on the probability of some event $_{_y}$ in a Bayesian network $\mathcal{B}$ over $\mathcal{X}$ . Le $X_{1},\dots,X_{n}$ be some topological ordering of $\mathcal{X}$ . We enume s that are partial a to X , where each partial assi antiates some subset $X_{1},\bar{\cdot}\cdot\cdot\,,X_{k}$ ; note that the set ${X_{1}^{-},\ldots,X_{k}}$ is not an arbitrary subset of $\textstyle\bar{X}_{1},\dotsc,\bar{X}_{n}$ , but rather the ﬁrst k variables in the ordering. Diferent partial assignments may instantiate diferent preﬁxes of the variables. We organize these partial assignments in a tree, where each node is labeled with some partial assignment $\left(x_{1},.\,.\,.\,,x_{k}\right)$ . The children of a node labeled $\left(x_{1},.\,.\,.\,,x_{k}\right)$ are $\left(x_{1},.\,.\,.\,,x_{k},x_{k+1}\right)$ , for each $x_{k+1}\in V a l(X_{k+1})$ n iteratively grow the tree by choosing some leaf in the tree, corresponding to an assignment ( $\left(x_{1},.\,.\,.\,,x_{k}\right)$ , and expanding the tree to include its children $\left(x_{1},.\,.\,.\,,x_{j},x_{k+1}\right)$ for all possible values $x_{k+1}$ . 

Consider a particular tree, wit et of leaves ${\cal L}\,=\,\{\ell[1],.\,.\,.\,,\ell[{\cal M}]\}$ , where each leaf $\ell[m]\,\in\,L$ is associated with the assignment $\pmb{x}[m]$ to some subset of variables $X[m]$ ] . 

a. Each leaf $\ell[m]$ in the tree deﬁnes a particle. Specify the assignment and probability associated with this particle, and describe how we would compute its probability efciently. b. Show how to use your probability estimates from part 1 (a) to provide both a lower and an upper bound for $P(\pmb{y})$ . c. Based on your answer from part 1, provide a simple heuristic for choosing the next leaf to expand in the partial search tree. 

# Exercise $12.27\!\star\!\star$ 

Consider the application of collapsed Gibbs sampling, where we use a clique tree to manipulate the conditional distribution $\tilde{P}(\boldsymbol{X}_{d}\mid\boldsymbol{X}_{p})$ | . Develop an algorithm in which, after an initial calibration step, all of the variables $X_{i}\,\in\,X_{p}$ ∈ in can be resampled using a single pass over the clique tree. (Hint: Use the algorithm developed in exercise 10.12.) 

# Exercise 12.28 

Consider the setting of example 12.18, where we assume that all grades are observed but none of the $I_{j}$ or $D_{k}$ variables are observed. Show how you would use the set of collapsed samples generated in this example to compute the expected value of the number of smart students $\bar{(i}^{1})$ who got a grade of a $\mathrm{~C~}(g^{3})$ in an easy class $\dot{(}d^{0})$ . 

# Exercise ${\bf12.29\star}$ 

Consider the oblem described in box .D: W ave two sets of objects ${\mathcal{U}}=\{u_{1},.\,.\,.\,,u_{k}\}$ and another V $\mathcal{V}=\{v_{1},.\,.\,.\,,\hat{v_{m}}\}$ { } , and we wish to m U ’s to V $\mathcal{V}_{\mathsf{S}}$ We have a set of ob features $B_{i}$ for each object u i , and a set of hidden attributes $A_{j}$ for each $v_{j}$ . We have a prior $P(A_{j})$ , and a set of factors $\dot{\phi_{i}}(A_{j},B_{i},C_{i})$ such that $\phi_{i}(a_{j},b_{i},C_{i})=\breve{1}$ for all $\mathbf{\delta}_{a_{j}},\mathbf{\delta}_{b_{i}}$ if $C_{i}\neq j$ . The model contains no other potentials. 

We wish to compute the posterior over $A_{j}$ using collapsed Gibbs sampling, where we sample the $C_{i}$ ’s but maintain a closed-form posterior over the $A_{j}$ ’s. Provide a sampling scheme for this task, showing clearly both the sampling distribution for the $C_{i}$ variables and the computation of the closed form over the $\bar{\mathbf{\delta}_{A_{i}}}$ variables given the assignment to the $C_{i}$ ’s. 

# 13 MAP Inference 

# 13.1 Overview 

So far, we have dealt solely with conditional probability queries. However, MAP queries, which we deﬁned in section 2.1.5, are also very useful in a variety of applications. As a reminder, a MAP query aims to ﬁnd the most likely assignment to all of the (non-evidence) variables. A marginal MAP query aims to ﬁnd the most likely assignment to a subset of the variables, marginalizing out over the rest. 

MAP queries are often used as a way of “ﬁlling in” unknown information. For example, we might be trying to diagnose a complex device, and we want to ﬁnd a single consistent hypothesis about failures in diferent components that explains the observed behavior. Another example arises when we are trying to decode messages transmitted over a noisy channel. In such cases, the receiver observes a sequence of bits received over the channel, and then it attempts to ﬁnd the most likely assignment of input bits that could have generated this observation (taking into account the code used and a model of the channel noise). This type of query is much better viewed as a MAP query than as a standard probability query, because we are not interested in the most likely values for the individual bits sent, but rather in the message whose overall probability is highest. A similar phenomenon arises in speech recognition, where we are trying to decode the most likely utterance given the (noisy) acoustic signal; here also we are not interested in the most likely value of individual phonemes uttered. 

# 13.1.1 Computational Complexity 

As for the case of conditional probability queries, it is instructive to analyze the computational complexity of the problem. There are many possible ways of formulating the MAP problem as a decision problem. One that is convenient for our purposes is the problem BN-MAP-DP , deﬁned as follows: 

Given a Bayesian etwork $\mathcal{B}$ over $\mathcal{X}$ and a number $\tau$ , decide whether there exists an assignment $_{_{x}}$ to X such that $P(\pmb{x})>\tau$ . 

It turns out that a very similar construction to theorem 9.1 can be used to show that the BN-MAP-DP problem is also $\mathcal{N P}$ -complete. 

The proof is left as an exercise (exercise 13.1). We can also deﬁne an analogous decision problem BN-margMAP-DP for marginal MAP: 

Given a Bayesian network $\mathcal{B}$ ove $\mathcal{X}$ , a number $\tau$ , and a subset $Y\subset{\mathcal{X}}$ , decide whether there exists an assignment $_{_y}$ to Y such that $P(\pmb{y})>\tau$ . 

Because marginal MAP is a generalization of MAP, we immediately conclude the following: 

# Corollary 13.1 

The decision problem BN-margMAP-DP is NP -hard. 

However, for the case of marginal MAP, we cannot conclude that BN-margMAP-DP is in $\mathcal{N P}$ . Intuitively, as we said, the marginal MAP problem involves elements of both maximization and summation, a combination that is signiﬁcantly harder than either subtask in isolation. In fact, it is possible to show that BN-margMAP-DP is complete for a much harder complexity class: 

# Theorem 13.2 

The decision problem BN-margMAP-DP is complete for $\mathcal{N P}^{\mathcal{P}\mathcal{P}}$ Deﬁning the complexity class $\mathcal{N P}^{\mathcal{P}\mathcal{P}}$ is outside the scope of this book (see section 9.8), but it is generally considered very hard, since it is known to contain the entire polynomial hierarchy, of which $\mathcal{N P}$ is only the ﬁrst level. 

While the “harder” complexity class of the marginal MAP problem indicates that it is more difcult, the implications of this formulation may be somewhat abstract. A more concrete ramiﬁcation is the following result, which states that the marginal MAP problem is $\mathcal{N P}$ -hard even for polytree networks: 

# Theorem 13.3 

polytree The following decision problem is $\mathcal{N P}$ -hard: 

Given a polytree Bayesian network $\mathcal{B}$ o $\mathcal{X}$ , a subset $Y\subset{\mathcal{X}}$ , and a number $\tau$ , decide whether there exists an assignment $_{_y}$ to Y such that $P(\pmb{y})>\tau$ . 

We defer the justiﬁcation for this result to section 13.2.3. 

# 13.1.2 Overview of Solution Methods 

As for conditional probability queries, when addressing MAP queries, it is useful to reformulate the joint distribution somewhat more abstractly, as a product of factors. Consider a distribution $P_{\Phi}(\mathcal X)$ deﬁned via a set of factors $\Phi$ and an unnormalized density $\tilde{P}_{\Phi}$ . We need to compute: 

$$
\xi^{m a p}=\arg\operatorname*{max}_{\xi}P_{\Phi}(\xi)=\arg\operatorname*{max}_{\xi}\frac{1}{Z}\tilde{P}_{\Phi}(\xi)=\arg\operatorname*{max}_{\xi}\tilde{P}_{\Phi}(\xi).
$$ 

In particular, if $P_{\Phi}(\mathcal{X})=P(\mathcal{X}\mid e)$ , then we aim to maximize $P(\mathcal{X},e)$ . 

The MAP task goes hand in hand with ﬁnding the value of the unnormalized probability of the most likely assignment: $\operatorname*{max}_{\xi}\tilde{P}_{\Phi}(\xi)$ . We note that, given an assignment $\xi$ , we can easily compute its unnormalized probability simply by multiplying all of the factors in $\Phi$ , evaluated at $\xi$ . However, we cannot retrieve the actual probability of $\xi$ without computing the partition function, a problem that requires that we also solve the sum-product task. 

max-product Because $\mathbf{\tilde{P}}_{\Phi}$ is a product of factors, tasks that involve maximizing $\tilde{P}_{\Phi}$ are often called max- 

max-sum 

energy minimization product inference tasks. Note that we often convert the max-product problem into log-space and maximize $\log{\tilde{P}_{\Phi}}$ . This logarithm is a sum of factors that correspond to negative energies (see section 4.4.1.2), and hence this version of the problem is often called the max-sum problem. It is also common to negate the factors and minimize the sum of the energies for the diferent potentials; this version is generally called an energy minimization problem. The transformation into log-space has several signiﬁcant advantages. First, it avoids the numerical issues associated with multiplying many small numbers together. More importantly, it transforms the problem into a linear one; as we will see, this transformation allows certain valuable tools to be brought to bear. For consistency with the rest of the book, we mostly use the max-product variant of the problem in the remainder of this chapter. However, all of our discussion carries over with minimal changes to the analogous max-sum (or min-sum) problem: we simply take the logarithm of all factors, and replace factor product steps with factor additions. 

Many diferent algorithms, both exact and approximate, have been proposed for addressing the MAP problem. Most obviously, the goal of the MAP task is ﬁnd an assignment to a set of variables whose score (unnormalized probability) is maximal. Thus, it is an instance of an optimization problem (see appendix A.4.1), a class of problems for which many general-purpose solutions have been developed. These methods include heuristic hill-climbing methods (see appendix A.4.2), as well as more specialized optimization methods. Some of these solutions have also been usefully applied to the MAP problem. 

There are also many algorithms that are speciﬁcally targeted at the max-product (or min- sum) task, and exploit some of its special structure, most notably the connection to the graph representation. A large subset of algorithms operate by ﬁrst computing a set of factors that are max-marginals . Max-marginals are a general notion that can be deﬁned for any function: 

max-marginal 

$$
M a x M a r g_{f}(\pmb{y})=\operatorname*{max}_{\xi\langle\pmb{Y}\rangle=\pmb{y}}f(\xi),
$$ 

For example, the max-marginal $M a x M a r g_{\tilde{P}_{\Phi}}(Y)$ is a factor that determines a value for each assignment $_{_y}$ to $Y$ ; this value is the unnormalized probability of the most likely joint assignment consistent with $_{_y}$ . 

decoding max-marginals unambiguous 

A large class of MAP algorithms proceed by ﬁrst computing an exact or approximate set of max-marginals for all of the variables in $\mathcal{X}$ , and then attempting to extract an exact or approx- imate MAP assignment from these max-marginals. The ﬁrst phase generally uses techniques such as variable elimination or message passing in clique trees or cluster graphs, algorithms similar to those we applied in the context of sum-product inference. Now, assume we have a set of (exact or approximate) max-marginals $\{M a x M a r g_{f}(X_{i})\}_{X_{i}\in\mathcal{X}}$ . A key question is how we use those max-marginals to construct an overall assignment. As we show, the computation of (approximate) max-marginals allows us to solve a global optimization problem as a set of local optimization problems for individual variables. This task, known as decoding , is to construct a joint assignment that locally optimizes each of the beliefs. If we can construct such an assignment, we will see that we can provide guarantees on its (strong local or even global) optimality. One such setting is when the max-marginals are unambiguous : For 

each variable $X_{i}$ , there is a unique $\boldsymbol{x}_{i}^{*}$ that maximizes: 

$$
x_{i}^{*}=\arg\operatorname*{max}_{x_{i}\in V a l(X_{i})}M a x M a r g_{f}(x_{i}).
$$ 

When the max-marginals are unambiguous, identifying the locally optimizing assignment is easy. When they are ambiguous, the solution is nontrivial even for exact max-marginals, and can require an expensive computational procedure in its own right. 

The marginal MAP problem appears deceptively similar to the MAP task. Here, we aim to ﬁnd the assignment whos ) marginal probability is maximal. Here, we partition $\mathcal{X}$ into two disjoint subsets, X $\mathcal{X}=Y\cup W$ ∪ , and aim to compute: 

$$
{\pmb y}^{m-m a p}=\arg\operatorname*{max}_{\pmb y}P_{\Phi}(\pmb y)=\arg\operatorname*{max}_{\pmb y}\sum_{\pmb W}\tilde{P}_{\Phi}(\pmb y,\pmb W).
$$ 

Thus, the marginal MAP problem involves both multiplication and summation, a com-  bination that makes the task much more difcult, both theoretically and in practice. In particular, exact inference methods such as variable elimination can be intractable, even in simple networks. And many of the approximate methods that have been developed for MAP queries do not extend easily to marginal MAP. So far, the only efective approximation technique for the marginal MAP task uses a heuristic search over the assignments $_{_y}$ , while employing some (exact or approximate) sum-product inference over $W$ in the inner loop. 

# 13.2 Variable Elimination for (Marginal) MAP 

We begin our discussion with the most basic inference algorithm: variable elimination. We ﬁrst present the simpler case of pure MAP queries, which turns out to be quite straightforward. We then discuss the issues that arise in marginal MAP queries. 

# 13.2.1 Max-Product Variable Elimination 

To gain some intuition for the MAP problem, let us begin with a very simple example. 

Consider the Bayesian network $A\ \rightarrow\ B$ . Assume we have no evidence, so that our goal is to compute: 

$$
\begin{array}{r c l}{\displaystyle\operatorname*{max}_{a,b}P(a,b)}&{=}&{\displaystyle\operatorname*{max}_{a,b}P(a)P(b\mid a)}\\ &{=}&{\displaystyle\operatorname*{max}_{a}\operatorname*{max}_{b}P(a)P(b\mid a).}\end{array}
$$ 

Consider any particular value $a$ of $A$ , and let us consider possible completions of that assignment. Among all possible completions, we want to pick one that maximizes the probability: 

$$
\operatorname*{max}_{b}P(a)P(b\mid a)=P(a)\operatorname*{max}_{b}P(b\mid a).
$$ 

Thus, a necessary condition for our assignment $a,b$ to have the maximum probability is that $B$ must be chosen so as to maximize $P(b\mid a)$ . Note that this ondition is not su cient: we must also choose the value of $A$ appropriately; but for any choice of A , we must choose B as described. 

![](images/e7125313c18bd099e32f508670c52dd34f201512d64c074467372333823d3896.jpg) 
Figure 13.1 Example of the max-marginalization factor operation for variable $B$ 

Let $\phi(a)$ denote the internal expression max b $P(b\mid a)$ . For example, consider the following assignment of parameters: 

$$
{\begin{array}{r l}{{\frac{a^{0}\quad a^{1}}{0.4\quad0.6}}}&{{}\qquad{\frac{A\parallel\quad b^{0}\quad\quad b^{1}}{a^{0}\parallel}}\qquad0.1\qquad0.9}\\ {0.55}&{{}0.45.}\end{array}}
$$ 

In this case, we have that $\phi(a^{1})=\operatorname*{max}_{b}P(b\mid a^{1})=0.55$ and $\phi(a^{0})=\operatorname*{max}_{b}P(b\mid a^{0})=0.9$ . To compute the max-marginal over A , we now compute: 

$$
\operatorname*{max}_{a}P(a)\phi(a)=\operatorname*{max}\left[0.4\cdot0.9,0.6\cdot0.55\right]=0.36.
$$ 

As in the case of sum-product queries, we can reinterpret the computation in this example in terms of factors. We deﬁne a new operation on factors, as follows: 

Deﬁnition 13.2 factor maximization 

Let $X$ be a set o ariables, and $Y\notin X$ a v able. Let $\phi(X,Y)$ be a factor. We deﬁne the factor maximization of $Y$ in $\phi$ to be factor $\psi$ over X such that: 

$$
\psi(X)=\operatorname*{max}_{Y}\phi(X,Y).
$$ 

The operation over the factor $P(B\mid A)$ in example 13.1 is performing $\phi(A)=\operatorname*{max}_{B}P(B\mid A)$ . Figure 13.1 presents a somewhat larger example. 

The key observation is that, like equation (9.6), we can sometimes exchange the order of maximization and product operations: If $X\not\in S c o p e[\phi_{1}]$ , then 

$$
\operatorname*{max}_{X}(\phi_{1}\cdot\phi_{2})=\phi_{1}\cdot\operatorname*{max}_{X}\phi_{2}.
$$ 

In other words, we can “push in” a maximization operation over factors that do not involve the variable being maximized. A similar property holds for exchanging a maximization with a factor 

![](images/f1efa60be02ac5149205fd8c039f477376960c6782925fbab94e434e8a9adb72.jpg) 

summation operation: If $X\not\in S c o p e[\phi_{1}]$ , then 

$$
\operatorname*{max}_{X}(\phi_{1}+\phi_{2})=\phi_{1}+\operatorname*{max}_{X}\phi_{2}.
$$ 

max-product variable elimination 

This insight leads directly to a max-product variable elimination algorithm, which is directly analogous to the algorithm in algorithm 9.1. The diference is that in line 4, we replace the expr ion $\textstyle\sum_{Z}\psi$ with the expression $\operatorname*{max}_{Z}\,\psi$ . The algorithm is shown in algorithm 13.1. The same template also covers max-sum, if we replace product of factors with addition of factors. 

If $X_{i}$ is the ﬁnal variable in this elimination process, we have maximized all variables other than $X_{i}$ , so that the resulting factor $\phi_{X_{i}}$ is the max-marginal over $X_{i}$ . 

Example 13.2 Consider again our very simple Student network, shown in ﬁgure 3.4. Our goal is to compute the most likely instantiation to the entire network, without evidence. We will use the elimination ordering $S,I,D,L,G$ . Note that, unlike the case of sum-product queries, we have no query variables, so that all variables are eliminated. The computation generates the factors shown in table 13.1. For example, the ﬁrst step would compute $\textstyle\tau_{1}(I)\,=\,\operatorname*{max}_{s}\phi_{S}(I,s)$ . Speciﬁcally, we would get $\tau_{1}(i^{0})\,=\,0.95$ and $\tau_{1}(i^{1})\,=\,0.8$ . Note, by contrast, that the same factor computed with summation instead of maximization would give $\tau_{1}(I)\equiv1$ , as iscussed. The ﬁnal factor, $\tau_{5}(\emptyset)$ ∅ , is simply $^a$ number, whose value is $\operatorname*{max}_{S,I,D,L,G}P(S,I,D,L,G).$ 

For this network, we can verify that the value is 0 . 184 . 

The factors generated by max-product variable elimination have an identical structure to those generated by the sum-product algorithm using the same ordering. Thus, our entire analysis  of the computational complexity of variable elimination, which we performed for sum- product in section 9.4, applies unchanged. In particular, we can use the same algorithms for ﬁnding elimination orderings, and the complexity of the execution is precisely the same induced width as in the sum-product case. We can also use similar ideas to exploit structure in the CPDs; see, for example, exercise 13.2. 

# 13.2.2 Finding the Most Probable Assignment 

decoding We now tackle the original MAP problem: decoding , or ﬁnding the most likely assignment itself. 

Algorithm 13.1 Variable elimination algorithm for MAP. The algorithm can be used both in its max-product form, as shown, or in its max-sum form, replacing factor product with factor 

Procedure Max-Product-VE ( $\Phi$ , // Set of factors over $_{X}$ $\prec$ // Ordering on $_{X}$ ) 1 Let $X_{1},\ldots,X_{k}$ be an ordering of $X$ such that 2 $X_{i}\prec X_{j}$ $i<j$ 3 for $i=1,\dots,k$ 4 $(\Phi,\phi_{X_{i}})\leftarrow\mathrm{~Max}.$ -Product-Eliminate $\cdot\mathrm{Var}(\Phi,X_{i})$ 5 $x^{*}\gets$ ← back- ${\mathsf{A P}}\big(\big\{\phi_{X_{i}}\ :\ i=1,.\.\.\,,k\big\}\big)$ ) 6 return $\boldsymbol{x}^{*},\boldsymbol{\Phi}$ // Φ contains the probability of the MAP Procedure Max-Product-Eliminate-Var ( $\Phi$ , // Set of factors $Z$ // Variable to be eliminated ) 1 $\begin{array}{r l}&{\Phi^{\prime}\leftarrow\ \{\phi\in\Phi\ :\ Z\in S c o p e[\phi]\}}\\ &{\Phi^{\prime\prime}\leftarrow\ \Phi-\Phi^{\prime}}\\ &{\psi\leftarrow\ \prod_{\phi\in\Phi^{\prime}}\phi}\\ &{\tau\leftarrow\ \operatorname*{max}_{Z}\psi}\\ &{\mathrm{{\bfreturn}\ (\Phi^{\prime\prime}\cup\{\tau\},\psi)}}\end{array}$ 2 3 4 5 Procedure Traceback-MAP ( $\left\{\phi_{X_{i}}\ :\ i=1,.\,.\,.\,,k\right\}$ ) 1 for $i=k,\dots,1$ 2 $\mathbf{\delta}\mathbf{\mathfrak{u}}_{i}\gets\mathbf{\delta}(x_{i+1}^{*},.\,.\,.\,,x_{k}^{*})\langle S c o p e[\phi_{X_{i}}]-\{X_{i}\}\rangle$ 3 $//$ The maximizing assignment to the variables eliminated after 4 $\begin{array}{r l}{}&{{}\qquad X_{i}}\\ {\quad}&{{}x_{i}^{*}\leftarrow\arg\operatorname*{max}_{x_{i}}\phi_{X_{i}}(x_{i},{\mathbf{u}}_{i})}\end{array}$ 5 // $\boldsymbol{x}_{i}^{*}$ is chosen so as to maximize the corresponding entry in the factor, relative to the previous choices $\mathbf{\mathbf{\mathit{u}}}_{i}$ 6 return $\boldsymbol{x}^{*}$ 

As we have discussed, the result of the computation is a max-marginal $M a x M a r g_{\Tilde{P}_{\Phi}}(X_{i})$ over the ﬁnal uneliminated variable, $X_{i}$ . We can now choose the maximizing value $\boldsymbol{x}_{i}^{*}$ for $X_{i}$ . Importantly, from the deﬁnition of max-marginals, we are guaranteed that there exists some assignment $\xi^{*}$ consistent with $\boldsymbol{x}_{i}^{*}$ . But how do we construct such an assignment? We return once again to our simple example: 

$\operatorname*{max}_{b}P(a,b)$ . This computation tells us, for each value of $a$ , which value of $b$ we must choose to complete the assignment in a way that maximizes the probability. In our example, the maximizing value of $B$ for $a^{1}$ is $b^{0}$ , and the maximizing value of $B$ for $a^{0}$ is $b^{1}$ . However, we cannot actually select the value of $B$ at this point, since we do not yet know the correct (maximizing) value of $A$ . We therefore proceed with the computation of example 13.1, and compute both the max-marginal over $A$ , $\textstyle\operatorname*{max}_{a}P(a)\phi(a)$ , and the value a that maximizes this expression. In this case, $P(a^{1})\phi(a^{1})\,=\,0.6\cdot0.55\,=\,0.33,$ , and $P(a^{0})\phi(a^{0})\,=\,0.4\cdot0.9\,=\,0.36$ . The maximizing value $a^{*}$ of A is therefore a $a^{0}$ . The key insight is that, given this value of A , we can now go back and select the corresponding value of $B$ — the one that maximizes $\phi(a^{*})$ . Thus, we obtain that our maximizing assignment is $a^{0},b^{1}$ , as expected. 

traceback 

The key intuition in this computation is that, as we eliminate variables, we cannot determine their maximizing value. However, we can determine a “conditional” maximizing value — their maximizing value given the values of the variables that have not yet been eliminated. When we pick the value of the ﬁnal variable, we can then go back and pick the values of the other variables accordingly. For the last variable eliminated, say $X$ , the factor for the value $x$ contains the probability of the most likely assignment that contains $X\,=\,x$ . Thus, we correctly select the most likely assignment to $X$ , and therefore to all the other variables. This process is called traceback of the solution. 

The algorithm implementing this intuition is shown in algorithm 13.1. Note that the operation in line 2 of Traceback-MAP is well deﬁned, since all of the variables remaining in $S c o p e[\phi_{X_{i}}]$ were eliminated after $X_{i}$ , and hence must be within the set $\{X_{i+1},.\.\cdot,X_{k}\}$ . We can show that the algorithm returns the MAP: 

Theorem 13.4 The algorithm of algorithm 13.1 returns 

$$
\pmb{x}^{*}=\arg\operatorname*{max}_{\pmb{x}}\prod_{\phi\in\Phi}\phi,
$$ 

and $\Phi$ , which contains a single factor of empty scope whose value is: 

$$
\operatorname*{max}_{\pmb{x}}\prod_{\phi\in\Phi}\phi.
$$ 

The proof follows in a straightforward way from the preceding intuitions, and we leave it as an exercise (exercise 13.3). 

We note that the traceback procedure is not an expensive one, since it simply involves a linear traversal over the factors deﬁned by variable elimination. In each case, when we select a value $\boldsymbol{x}_{i}^{*}$ for a variable $X_{i}$ in line 2, we are guaranteed that $\boldsymbol{x}_{i}^{*}$ is, indeed, a part of a jointly coherent MAP assignment. Thus, we will never need to backtrack and revisit this decision, trying a diferent value for $X_{i}$ . 

Example 13.4 Returning to example 13.2, we now consider the traceback phase. We begin by computing $g^{\ast}\,=$ arg $\operatorname*{max}_{g}\psi_{5}(g)$ . It is important to remember that $g^{*}$ is not the value that maximizes $P(G)$ . It is the value of $G$ that participates in the most likely complete assignment to all the network variables $\mathcal{X}=\{S,I,D,L,G\}$ . Given $g^{*}$ , we can now compute $l^{*}=\arg\operatorname*{max}$ l $\psi_{4}(g^{*},l)$ . The value $l^{*}$ is 

the value of $L$ in the most likely complete assignment to $\mathcal{X}$ . We use the same procedure for the remaining variables. Thus, 

$$
\begin{array}{l c l}{{d^{*}}}&{{=}}&{{\arg\displaystyle\operatorname*{max}_{d}\psi_{3}(g^{*},d)}}\\ {{i^{*}}}&{{=}}&{{\arg\displaystyle\operatorname*{max}_{i}\psi_{2}(g^{*},i,d^{*})}}\\ {{s^{*}}}&{{=}}&{{\arg\displaystyle\operatorname*{max}_{s}\psi_{1}(i^{*},s).}}\end{array}
$$ 

It is straightforward (albeit somewhat tedious) to verify that the most likely assignment is $d^{1}$ , $i^{0}$ , $g^{3}$ , $s^{0},\,l^{0}$ , and its probability is (approximately) the value 0 . 184 that we obtained in the ﬁrst part of the computation. 

The additional step of computing the actual assignment does not add signiﬁcant time com- plexity to the basic max-product task, since it simply does a second pass over the same set of factors computed in the max-product pass. With an appropriate choice of data structures, this cost can be linear in the number $n$ of variables in the network. The cost in terms of space is a little greater, inasmuch as the MAP pass requires that we store the intermediate results in the max-product computation. However, the total cost is at most a factor of $n$ greater than the cost of the computation without this additional storage. 

The algorithm of algorithm 13.1 ﬁnds the one assignment of highest probability. This assign- ment gives us the single most likely explanation of the situation. In many cases, however, we want to consider more than one possible explanation. Thus, a common task is to ﬁnd the set of the $K$ most likely assignments. This computation can also be performed using the output of a run of variable elimination, but the algorithm is signiﬁcantly more intricate. (See exercise 13.5 for one simpler case.) An alternative approach is to use one of the search-based algorithms that we discuss in section 13.7. 

# 13.2.3 Variable Elimination for Marginal MAP $\star$ 

max-sum-product We now turn our attention to the application of variable elimination algorithms to the marginal MAP problem. Recall that our marginal MAP problem can be written as arg $\begin{array}{r}{\operatorname*{max}_{y}\,\sum_{W}\tilde{P}_{\Phi}(\Breve{\pmb{y}},\pmb{W})}\end{array}$ P , where $\pmb{y}\cup\pmb{W}=\mathcal{X}$ , so that $\tilde{P}_{\Phi}(\pmb{y},\pmb{W})$ is a product of factors in some set $\Phi$ . Thus, our com- putation has the following max-sum-product form: 

$$
\operatorname*{max}_{Y}\sum_{W}\prod_{\phi\in\Phi}\phi.
$$ 

This form immediately suggests a variable elimination algorithm, along the lines of similar algorithms for sum-product and max-product. This algorithm simply puts together the ideas we used for probability queries on one hand and MAP queries on the other. Speciﬁcally, the summations and maximizations outside the product can be viewed as operations on factors. Thus, to compute the value of this expression, we simply have to eliminate the variables $W$ by summing them out, and the variables in $Y$ by maximizing them out. When eliminating a variable $X$ , whether by summation or by maximization, we simply multiply all the factors whose scope involves $X$ , and then eliminate $X$ to produce the resulting factor. Our ability to perform this step is justiﬁed by the exchangeability of factor summation/maximization and factor product (equation (9.6) and equation (13.6)). 

Example 13.5 Consider again the network of ﬁgure 3.4, and assume that we wish to ﬁnd the probability of the most likely instantiation of SAT result and letter quality: 

$$
\operatorname*{max}_{S,L}\sum_{G,I,D}P(I,D,G,S,L).
$$ 

We can perform this computation by eliminating the variables one at a time, as appropriate. Speciﬁcally, we perform the following operations: 

$$
\begin{array}{r l}{\psi_{1}(G,d,D)}&{=\ \ \phi_{0}(D)\cdot\phi_{G}(G,d,D)}\\ {\eta_{1}(d,D)}&{=\ \ \sum_{\sigma}\psi_{1}(L,G,D)}\\ &{=\ \ \gamma_{1}(d,G,D)}\\ {\psi_{2}(S,d,I)}&{=\ \ \phi_{I}(I)\cdot\phi_{S}(S,I)\cdot\eta_{I}(I,G)}\\ {\eta_{2}(S,d,I)}&{=\ \ \gamma_{2}(S,G,I)}\\ {\psi_{3}(S,L,G)}&{=\ \ \gamma_{3}(S,G,D)\cdot\phi_{L}(G,G)}\\ {\eta_{3}(S,L)}&{=\ \ \sum_{\sigma}\psi_{3}(S,G,L)}\\ {\psi_{4}(S,L)}&{=\ \eta_{5}(S,L)}\\ {\tau_{4}(L)}&{=\ \arg\psi_{4}(S,L)}\\ {\psi_{5}(L)}&{=\ \tau_{4}(L)}\\ {\tau_{6}(I)}&{=\ \arg\psi_{5}(L).}\end{array}
$$ 

Note that the ﬁrst three factors $\tau_{1},\tau_{2},\tau_{3}$ are generated via the operation of summing out, whereas the last two are generated via the operation of maxing out. 

This process computes the unnormalized probability of the marginal MAP assignment. We can ﬁnd the most likely values to the max-variables exactly as we did in the case of MAP: We simply keep track of the factors associated with them, and then we work our way backward to compute the most likely assignment; see exercise 13.4. 

Example 13.6 

The similarity between this algorithm and the previous variable elimination algorithms we described may naturally lead one to conclude that the computational complexity is also similar. Unfortunately, that is not the case: this process is computationally much more expensive than the corresponding variable elimination process for pure sum-product or pure max-product. The difculty stems from the fact that we are not free to choose an arbitrary elimination ordering. When summing out variables, we can utilize the fact that the operations of summing out diferent variables commute. Thus, when performing summing-out operations for sum-product variable 

![](images/ad2a95ad368f08138afbae400b3a2ce2dc7a57ec0326104010c37adccb26a5da.jpg) 
Figure 13.2 A network where a marginal MAP query requires exponential time 

elimination, we could sum out the variables in any order. Similarly, we could use the same freedom in the case of max-product elimination. Unfortunately, the max and sum operations do not commute (exercise 13.19). Thus, in order to maintain the correct semantics of marginal MAP queries, as speciﬁed in equation (13.4), we must perform all the variable summations before we can perform any of the variable maximizations. 

constrained elimination ordering 

Example 13.7 

As we saw in example 9.1, diferent elimination orderings can induce very diferent widths. When we constrain the set of legal elimination orderings, we have a smaller range of possibilities, and even the best elimination ordering consistent with the constraint might have signiﬁcantly larger width than a good unconstrained ordering. 

Consider the network shown in ﬁgure 13.2, and assume that we wish to compute 

$$
y^{m\cdot m a p}=\arg\operatorname*{max}_{Y_{1},\ldots,Y_{n}}\sum_{X_{1},\ldots,X_{n}}P(Y_{1},\ldots,Y_{n},X_{1},\ldots,X_{n}).
$$ 

As we discussed, we must ﬁrst sum out $X_{1},\dots,X_{n}.$ , and only then deal with the maximization over the $Y_{i}$ ’s. Unfortunately, the factor generated after summing out all of the $X_{i}\,{\stackrel{\circ}{s}}$ contains all of their neighbors, that is, all of the $Y_{i}$ ’s. This factor is exponential in $n$ . By contrast, the minimal induced width of this network is 2 , so that any probability query (assuming a small number of query variables) or MAP query can be performed on this network in linear time. 

traceback 

# 

As we can see, even on very simple polytree networks, elimination algorithms can require exponential time to solve a marginal MAP query. One might hope that this blowup is a consequence of the algorithm we use, and that perhaps a more clever algorithm would avoid this problem. Unfortunately, theorem 13.3 shows that this difculty is unavoidable, and unless $\mathcal{P}=\mathcal{N P}$ , some exact marginal MAP computation require exponential time, even in very simple networks. Importantly, however, we must keep in mind that this result does not afect every marginal MAP query. Depending on the structure of the network and the choice of maximization variables, the additional cost induced by the constrained elimination ordering may or may not be prohibitive. 

Putting aside the issue of computational cost, once we have executed a run of variable elimi- nation for the marginal MAP problem, the task of ﬁnding the actual marginal MAP assignment can be addressed using a traceback procedure that is directly analogous to Traceback-MAP of algorithm 13.1; we leave the details as an exercise (exercise 13.4). 

![](images/2490c66bfc2dbcabce9d7d1ce96499b28772f0fac9c3c86c15178e0005740c1c.jpg) 

# 13.3 Max-Product in Clique Trees 

We now extend the ideas used in the MAP variable elimination algorithm to the case of clique trees. As for the case of sum-product, the beneﬁt of the clique tree algorithm is that it uses dynamic programming to compute an entire set of marginals simultaneously. For sum-product, we used clique trees to compute the sum-marginals over each of the cliques in our tree. Here, we compute a set of max-marginals over each of those cliques. 

At this point, one might ask why we want to compute an entire set of max-marginals si- multaneously. After all, if our only task is to compute a single MAP assignment, the variable elimination algorithm provides us with a method for doing so. There are two reasons for considering this extension. 

First, a set of max-marginals can be a useful indicator for how conﬁdent we are in particular components of the MAP assignment. Assume, for example, that our variables are binary-valued, and that the max-marginal for $X_{1}$ has MaxMarg $\lceil(x_{1}^{1})=3$ and $M a x M a r g(x_{1}^{0})=2.95$ , whereas the max-marginal for $X_{2}$ has $M a x M a r g(x_{2}^{1})=3$ and MaxMarg $\cdot(x_{2}^{0})=1$ . In this case, we know that there is an alternative joint assignment whose probability is very close to the optimum, in which $X_{1}$ takes a diferent value; by contrast, the best alternative assignment in which $X_{2}$ takes a diferent value has a much lower probability. Note that, without knowing the partition function, we cannot determine the actual magnitude of these diferences in terms of probability. But we can determine the relative diference between the change in $X_{1}$ and the change in $X_{2}$ . 

pseudo-max- marginal 

Second, in many cases, an exact solution to the MAP problem via a variable elimination procedure is intractable. In this case, we can use message passing procedures in cluster graphs, similar to the clique tree procedure, to compute approximate max-marginals. These pseudo- max-marginals can be used for selecting an assignment; while this assignment is not generally the MAP assignment, we can nevertheless provide some guarantees in certain cases. As before, our task has two parts: computing the max-marginals and decoding them to extract a MAP assignment. We describe each of those steps in turn. 

# 13.3.1 Computing Max-Marginals 

In the same way that we used dynamic programming to modify the sum-product variable elimination algorithm to the case of clique trees, we can also modify the max-product algorithm to deﬁne a max-product belief propagation algorithm in clique trees. The resulting algorithm executes precisely the same initialization and overall message scheduling as in the sum-product max-product message passing 

max-marginal 

Proposition 13.1 belief propagation algorithm of algorithm 10.2; the only diference is the use of max-product rather than sum-product message passing, as shown in algorithm 13.2; as for variable elimination, the procedure has both a max-product and a max-sum variant. 

As for sum-product message passing, the algorithm will converge after a single upward and downward pass. After those steps, the resulting clique tree $\mathcal{T}$ will contain the appropriate max-marginal in every clique. 

Consider a run of the max-product clique tree algorithm, where we initialize with a set of factors $\Phi$ . Let $\beta_{i}$ be a set of beliefs arising from an upward and downward pass of this algorithm. Then for each clique $C_{i}$ and each assignment $c_{i}$ to $C_{i}$ , we have that 

$$
\beta_{i}(\pmb{c}_{i})=M a x M a r g_{\Tilde{P}_{\Phi}}(\pmb{c}_{i}).
$$ 

That is, the clique belief contains, for each assignment $c_{i}$ to the clique variables, the (unnormal- ized) measure $\overset{\cdot}{P}_{\Phi}(\xi)$ of the most likely assignment $\xi$ consistent with $c_{i}$ . The proof is exactly the same as the proof of theorem 10.3 and corollary 10.2 for sum-product clique trees, and so we do not repeat the proof. Note that, because the max-product message passing process does not compute the partition function, we cannot derive from these max-marginals the ac- tual probability of any assignment; however, because the partition function is a constant, we can still compare the values associated with diferent assignments, and therefore compute the assignment $\xi$ that maximizes $\tilde{P}_{\Phi}(\xi)$ . 

Because max-product message passing over a clique tree produces max-marginals in every clique, and because max-marginals must agree, it follows that any two adjacent cliques must agree on their sepset: 

$$
\operatorname*{max}_{C_{i}-S_{i,j}}\beta_{i}=\operatorname*{max}_{C_{j}-S_{i,j}}\beta_{j}=\mu_{i,j}(S_{i,j}).
$$ 

max-calibrated 

Corollary 13.2 

Example 13.8 

max-product belief update 

In this case, the clusters are said to be max-calibrated . We say that a clique tree is max-calibrated if all pairs of adjacent cliques are max-calibrated. 

The beliefs in a clique tree resulting from an upward and downward pass of the max-product clique tree algorithm are max-calibrated. 

We can also deﬁne a max-product belief update message passing algorithm that is entirely analogous to the belief update variant of sum-product message passing. In particular, in line 1 of algorithm 10.3, we simply replace the summation with the maximization operation: 

$$
\sigma_{i\rightarrow j}\leftarrow\operatorname*{max}_{C_{i}-S_{i,j}}\beta_{i}.
$$ 

The remainder of the algorithm remains completely unchanged. As in the sum-product case, the max-product belief propagation algorithm and the max-product belief update algorithm 

![](images/1de27552a22cee80667ca058d126819521354193c5bf01045bdeec2f0a6cc768.jpg) 

Figure 13.3 The max-marginals for the Misconception example. Listed are the beliefs for the two cliques and the sepset. 

are exactly equivalent. Thus, we can show that the analogue to equation (10.9) holds also for max-product: 

$$
\mu_{i,j}\big(\pmb{S}_{i,j}\big)=\delta_{j\rightarrow i}\big(\pmb{S}_{i,j}\big)\cdot\delta_{i\rightarrow j}\big(\pmb{S}_{i,j}\big).
$$ 

In particular, this equivalence holds at convergence, so that a clique’s max-marginal over a sepset can be computed from the max-product messages. 

# 13.3.2 Message Passing as Re parameter iz ation 

reparameteriza- tion clique tree measure 

Somewhat surprisingly, as for the sum-product case, we can view the max-product message passing steps as re parameter i zing the original distribution, in a way that leaves the distribution invariant. More precisely, we view a set of beliefs $\beta_{i}$ and sepset messages $\mu_{i,j}$ in a max-product clique tree as deﬁning a measure using equation (10.11), precisely as for sum-product trees: 

$$
Q_{\mathcal{T}}=\frac{\prod_{i\in\nu_{\mathcal{T}}}\beta_{i}(C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(S_{i,j})}.
$$ 

When we begin a run of max-product belief propagation, the initial potentials are simply the initial potentials in $\Phi$ , and the messages are all 1 , so that $Q_{\mathcal{T}}$ is precisely $\tilde{P}_{\Phi}$ . Examining the proof of corollary 10.3, we can see that it does not depend on the deﬁnition of the messages in terms of summing out the beliefs, but only on the way in which the messages are then used to update the receiving beliefs. Therefore, the proof of the theorem holds unchanged for max-product message passing, proving the following result: 

# Theorem 13.5 

Example 13.9 Let $\{\beta_{i}\}$ and $\{\mu_{i,j}\}$ be the max-calibrated set of beliefs obtained from executing max-product mes- sage passing, and let $Q_{T}$ be the distribution induced by these beliefs. Then $Q_{\mathcal{T}}$ is a representation of the distribution $\tilde{P}_{\Phi}$ that also satisﬁes the max-product calibration constraints of equation (13.10). 

Continuing with example 13.8, it is straightforward to conﬁrm that the original measure $\tilde{P}_{\Phi}$ can be reconstructed directly from the max-marginals and the sepset message. For example, consider the entry $\tilde{P}_{\Phi}(a^{1},b^{0},c^{1},d^{0})=100$ . According to equation (10.10), the clique tree measure is: 

$$
\frac{\beta_{1}(a^{1},b^{0},d^{0})\beta_{2}(b^{0},c^{1},d^{0})}{\mu_{1,2}(b^{0},d^{0})}=\frac{100\cdot300,000}{300,000}=100,
$$ 

as required. The equivalence for other entries can be veriﬁed similarly. 

Comparing this computation to example 10.6, we see that the sum-product clique tree and the max-product clique tree both induce re parameter iz at ions of the original measure $\tilde{P}_{\Phi}$ , but these two re parameter iz at ions are diferent, since they must satisfy diferent constraints. 

# 13.3.3 Decoding Max-Marginals 

Given the max-marginals, can we ﬁnd the actual MAP assignment? In the case of variable elimination, we had the max-marginal only for a single variable $X_{i}$ (the last to be eliminated). Therefore, although we could identify the assignment for $X_{i}$ in the MAP assignment, we had to perform a traceback procedure to compute the assignments to the other variables. Now the situation appears diferent: we have max-marginals for all of the variables in the network. Can we use this property to simplify this process? 

One obvious solution is to use the max-marginal for each variable $X_{i}$ to compute its own optimal assignment, and thereby compose a full joint assignment to all variables. However, this simplistic approach may not always work. 

# Example 13.10 

Consider a simple XOR-like distribution $P(X_{1},X_{2})$ that gives probability 0 . 1 to the assignments ere $X_{1}=X_{2}$ and 0 . 4 to the assignment here $X_{1}\neq X_{2}$ . In this case, for each assignment to $X_{1}$ , there is a corresponding assignment to $X_{2}$ whose probability is 0 . 4 . Thus, the max-marginal of $X_{1}$ is the symmetric factor (0 . 4 , 0 . 4) , and similarly for $X_{2}$ . Indeed, we can choose either of the two values for $X_{1}$ and complete it to a MAP assignment, and similarly for $X_{2}$ . However, if we choose the values for $X_{1}$ and $X_{2}$ in an inconsistent way, we may get an assignment whose probability is much lower. Thus, our joint assignment cannot be chosen by separately optimizing the individual max-marginals. 

Recall that we deﬁned a set of node beliefs to be unambiguous if each belief has a unique maximal value. This condition prevents symmetric cases like the one in the preceding example. Indeed, it is not difcult to show the following result: 

Proposition 13.3 The following two conditions are equivalent: • The set of node beliefs $\{M a x M a r g_{\tilde{P}_{\Phi}}(X_{i})\ :\ X_{i}\in\mathcal{X}\}$ ∈X} is unambiguous, with $x_{i}^{*}=\arg\operatorname*{max}_{x_{i}}M a x M a r g_{\tilde{P}_{\Phi}}(X_{i})$ 

the unique optimizing value for $X_{i}$ ; 

• $\tilde{P}_{\Phi}$ has a unique MAP assignment $(x_{1}^{*},\cdot\cdot\cdot,x_{n}^{*})$ . 

See exercise 13.8. For generic probability measures, the assumption of unambiguity is not overly stringent, since we can always break ties by introducing a slight random perturbation into all of the factors, making all of the elements in the joint distribution have slightly diferent probabilities. However, if the distribution has special structure — deterministic relationships or shared parameters — that we want to preserve, this type of ambiguity may be unavoidable. 

Thus, if there are no ties in any of the calibrated node beliefs, we can ﬁnd the unique MAP assignment by locally optimizing the assignment to each variable separately. If there are ties in the node beliefs, our task can be reformulated as follows: 

Deﬁnition 13.3 local optimality 

decoding 

traceback 

# Theorem 13.6 

Let $\beta_{i}(C_{i})$ be a belief in a max-calibrated clique tree. We say that an assignment $\xi^{*}$ has the local optimality property $i f,$ for each clique $C_{i}$ in the tree, we have that 

$$
\xi^{*}\langle C_{i}\rangle\in\arg\operatorname*{max}_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i}),
$$ 

that is, the assignment to $C_{i}$ in $\xi^{*}$ optimizes the $C_{i}$ belief. The task of ﬁnding a locally optimal assignment $\xi^{*}$ given a max-calibrated set of beliefs is called the decoding task. 

Solving the decoding task in the ambiguous case can be done using a traceback procedure as in algorithm 13.1. However, local optimality provides us with a simple, local test for verifying whether a given assignment is the MAP assignment: 

Let $\beta_{i}(C_{i})$ a set of max-calibrated beliefs in a clique tree $\mathcal{T}$ , with $\mu_{i,j}$ the associated sep beliefs. Let $Q_{\mathcal{T}}$ be the clique tree measure deﬁned as in equation (13.12). Then an assignment $\xi^{*}$ T satisﬁes the local optimality prope relative to the beliefs $\{\beta_{i}(C_{i})\}_{i\in\mathcal{V}_{\mathcal{T}}}$ if and only if it is the global MAP assignment relative to $Q_{\mathcal{T}}$ . 

Proof The proof of the “if” direction follows directly from our previous results. We have that $Q_{\mathcal{T}}$ is max-calibrated, and hence is a ﬁxed point of the max-product algorithm. (In other words, if we run max-product inference on the distribution deﬁned by $Q_{\mathcal{T}}$ , we would get precisely the beliefs $\beta_{i}(C_{i})$ .) Thus, these beliefs are max-marginals of $Q_{\mathcal{T}}$ . If $\xi^{*}$ is the MAP assignment to $Q_{T;}$ , it must maximize each one of its max-marginals, proving the desired result. 

The proof of the only if direction requires the following lemma, which plays an even more signiﬁcant role in later analyses. 

# Lemma 13.1 

Let $\phi$ be a actor over scope $Y$ and $\psi$ be a factor over scope $Z\subset Y$ such that $\psi$ is a max-marginal of $\phi$ over Z ; that is, for any z : 

$$
\psi(z)=\operatorname*{max}_{\pmb{y}\sim z}\phi(\pmb{y}).
$$ 

Let $\pmb{y}^{*}=\arg\operatorname*{max}_{\pmb{y}}\phi(\pmb{y})$ . Then $y^{*}$ is also an optimal assignment for the factor $\phi/\psi$ , where, as usual, we take $\psi({\pmb y}^{*})=\psi({\pmb y}^{*}\langle{\pmb Z}\rangle)$ . 

Proof Recall that, due to the properties of max-marginalization, each entry $\psi(z)$ arises from some entry $\phi(\pmb{y})$ such that $\pmb{y}\sim\pmb{z}$ . Because $\boldsymbol{y}^{*}$ achieves the optimal value in $\phi$ , and $\psi$ is the max-marginal of $\phi$ , we have that $z^{*}$ achieves the optimal value in $\psi$ . Hence, $\phi({\pmb y}^{*})=\psi(z^{*})$ , so that $\begin{array}{r}{\left(\frac{\phi}{\psi}\right)(\pmb{y}^{*})=1}\end{array}$  . Now, consider any other assignment $_{_y}$ and the assignment $z=y\langle Z\rangle$ . Either the value of $_z$ is obtained from $_{_y}$ , or it is obtained from some other $y^{\prime}$ whose value is larger. In the ﬁrst case, we have that $\phi(\pmb{y})=\psi(\pmb{z})$ , so that $\begin{array}{r}{\left(\frac{\phi}{\psi}\right)(\pmb{y})=1}\end{array}$  . In the second case, we have that $\phi(\pmb{y})<\psi(\pmb{z})$ and $\begin{array}{r}{\left(\frac{\phi}{\psi}\right)(\pmb{y})<1}\end{array}$  . In either case, 

$$
\left(\frac{\phi}{\psi}\right)(\pmb{y})\leq\left(\frac{\phi}{\psi}\right)(\pmb{y}^{*}),
$$ 

as required. 

To prove the only-if direction, we ﬁrst rewrite the clique tree distribution of equation (13.12) in a dire ed way. We select a root clique $C_{r}$ ; for each clique $\textit{i}\neq\textit{r}$ , let $\pi(i)$ be the parent clique of i in this rooted tree. We then assign each sepset $S_{i,\pi(i)}$ to the child clique i . Note that, because each clique has at most one parent, each clique is assigned at most one sepset. Thus, we obtain the following rewrite of equation (13.12): 

$$
\beta_{r}(C_{r})\prod_{i\neq r}\frac{\beta_{i}(C_{i})}{\mu_{i,\pi(i)}(S_{i,\pi(i)})}.
$$ 

Now, let $\xi^{*}$ be an assignment that satisﬁes the local optimality property. By assumption, it optimizes every one of the beliefs. Thus, the conditions of lemma 13.1 hold for each of the ratios in this product, and for the ﬁrst term involving the root clique. Thus, $\xi^{*}$ also optimizes each one of the terms in this product, and therefore it optimizes the product as a whole. It must therefore be the MAP assignment. 

As we will see, these concepts and related results have important implications in some of our later derivations. 

# 13.4 Max-Product Belief Propagation in Loopy Cluster Graphs 

In section 11.3 we applied the sum-product message passing using the clique tree algorithm to a loopy cluster graph, obtaining an approximate inference algorithm. In the same way, we can generalize max-product message passing to the case of cluster graphs. The algorithms that we present in this section are directly analogous to their sum-product counterparts in section 11.3. However, as we discuss, the guarantees that we can provide are much stronger in this case. 

# 13.4.1 Standard Max-Product Message Passing 

As for the case of clique trees, the algorithm divides into two phases: computing the beliefs using message passing and using those beliefs to identify a single joint assignment. 

# 13.4.1.1 Message Passing Algorithm 

The message passing algorithm is straightforward: it is precisely the same as the algorithm of algorithm 11.1, except that we use the procedure of algorithm 13.2 in place of the SP-Message procedure. As for sum-product, there are no guarantees that this algorithm will converge. Indeed, in practice, it tends to converge somewhat less often than the sum-product algorithm, perhaps because the averaging efect of the summation operation tends to smooth out messages, and reduce oscillations. Many of the same ideas that we discussed in box 11.B can be used to improve convergence in this algorithm as well. 

pseudo-max- marginal 

Corollary 13.3 

At convergence, the result will be a set of calibrated clusters: As for sum-product, if the clusters are not calibrated, convergence has not been achieved, and the algorithm will continue iterating. However, the resulting beliefs will not generally be the exact max-marginals; these resulting beliefs are often called pseudo-max-marginals . 

As we saw in section 11.3.3.1 for sum-product, the distribution invariance property that holds for clique trees is a consequence only of the message passing procedure, and does not depend on the assumption that the cluster graph is a tree. The same argument holds here; thus, proposition 13.2 can be used to show that max-product message passing in a cluster graph is also simply re parameter i zing the distribution: 

In an execution of max-product message passing (whether belief propagation or belief update) in a cluster graph, the invariant equation (10.10) holds initially, and after every message passing step. 

# 13.4.1.2 Decoding the Pseudo-Max-Marginals 

Given a set of pseudo-max-marginals, we now have to solve the decoding problem in order to identify a joint assignment. In general, we cannot expect this assignment to be the exact MAP, but we can hope for some reasonable approximation. But how do we identify such an assignment? It turns out that our ability to do so depends strongly on whether there exists some assignment that satisﬁes the local optimality property of deﬁnition 13.3 for the max-calibrated beliefs in the cluster graph. Unlike in the case of clique trees, such a joint assignment does not necessarily exist: 

# Example 13.11 

Consider a cluster graph with the three clusters $\{A,B\},\{B,C\},\{A,C\}$ and the beliefs 

![](images/039093780c09247bbb848f718964fbd19f07409ec2dd9129d5269f87b4576172.jpg) 

These beliefs are max-calibrated, in that all messages are $(2,2)$ . However, there is no joint assign- ment that maximizes all of the cluster beliefs simultaneously. For example, if we select $a^{0},b^{1}$ , we maximize the value in the $A,B$ belief. We can now select $c^{0}$ to maximize the value in the $B,C$ belief. However, we now have a nonmaximizing assignment $a^{0},c^{0}$ in the $A,C$ belief. No matter which assignment of values we select in this example, we do not obtain a single joint assignment that maximizes all three beliefs. 

frustrated loop Loops such as this are often called frustrated . 

In other cases, a locally optimal joint assignment does exist. In particular, when all the  node beliefs are all unambiguous, it is not difcult to show that all of the cluster beliefs also have a unique maximizing assignment, and that these local cluster-maximizing as- signments are necessarily consistent with each other (exercise 13.9). However, there are also other cases where the node beliefs are ambiguous, and yet a locally optimal joint assignment exists: 

Consider a cluster graph of the same structure as in example 13.11, but with the beliefs: 

![](images/67675983d5bfa068f1499725f2b9240f6538fe2d5615e0d50d2d6fce1d056b73.jpg) 

In this case, the beliefs are ambiguous, yet a locally optimal joint assignment exists (both $a^{1},b^{1},c^{1}$ and $a^{0},b^{0},c^{0}$ are locally optimal). 

In general, the decoding problem in a loopy cluster graph is not a trivial task. Recall that, in clique trees, we could simply choose any of the maximizing assignments for the beliefs at a clique, and be assured that it could be extended into a joint MAP assignment. Here, as illustrated by example 13.11, we may make a choice for one cluster that cannot be extended into a consistent joint assignment. In that example, of course, there is no assignment that works. However, it is not difcult to construct examples where one choice of locally optimal assignments would give rise to a consistent joint assignment, whereas another would not (exercise 13.10). 

constraint satisfaction problem 

How do we ﬁnd a locally optimal joint assignment, if one exists? Recall from the deﬁnition that an assignment is locally optimal if and only if it selects one of the optimizing assignments in every single cluster. Thus, we can essentially label the assignments in each cluster as either “legal” if they optimize the belief or “illegal” if they do not. We now must search for an assignment to $\mathcal{X}$ that results in a legal value for each cluster. This problem is precisely a constraint satisfaction problem (CSP) , where the constraints are derived from the local optimality condition. More precisely, a constraint satisfaction problem can be deﬁned in terms of a Markov network (or factor graph) where all of the entries in the beliefs are either 0 or 1. The CSP problem is now one of ﬁnding an assignment whose (unnormalized) measure is 1, if one exists, and otherwise reporting failure. In other words, the CSP problem is simply that of ﬁnding the MAP assignment in this model with $\{0,1\}$ -valued beliefs. The ﬁeld of CSP algorithms is a large one, and a detailed survey is outside the scope of the book; see section 13.9 for some background reading. We note, however, that the CSP problem is itself $\mathcal{N P}$ -hard, and therefore we have no guarantees that a locally optimal assignment, even if one exists, can be found efciently. 

Thus, given a max-product calibrated cluster graph, we can convert it to a discrete-valued CSP by simply taking the belief in each cluster, changing each assignment that locally optimizes the belief to 1 and all other assignments to 0 . We then run some CSP solution method. If the outcome is an assignment that achieves 1 in every belief, this assignment is guaranteed to be a locally optimal assignment. Otherwise, there is no locally optimal assignment. In this case, we must resort to the use of alternative solution methods. One heuristic in this latter situation is to use information obtained from the max-product propagation to construct a partial assignment. For example, assume that a variable $X_{i}$ is unambiguous in the calibrated cluster graph, so that the only value that locally optimizes its node marginal is $x_{i}$ . In this case, we may 

![](images/9650f84aa79b95f9fcd7de348715484b575925c3edbedb64b9867203fb50e171.jpg) 
Figure 13.4 Two induced subgraphs derived from ﬁgure 11.3a. (a) Graph over $\{B,C\}$ ; (b) Graph over $\{C,E\}$ . 

decide to restrict attention only to assignments where $X_{i}=x_{i}$ . In many real-world problems, a large fraction of the variables in the network are unambiguous in the calibrated max-product cluster graph. Thus, this heuristic can greatly simplify the model, potentially even allowing exact methods (such as clique tree inference) to be used for the resulting restricted model. We note, however, that the resulting assignment would not necessarily satisfy the local optimality condition, and all of the guarantees we will present hold only under that assumption. 

# 13.4.1.3 Strong Local Maximum 

What type of guarantee can we provide for a decoded assignment from the pseudo-max- marginals produced by the max-product belief propagation algorithm? It is certainly not the case that this assignment is the MAP assignment; nor is it even the case that we can guarantee that the probability of this assignment is “close” in any sense to that of the true MAP assignment. 

strong local maximum 

Deﬁnition 13.4 induced subgraph 

Example 13.13 

However, if we can construct a locally optimal assignment $\xi^{*}$ relative to the beliefs produced by max-product BP, we can prove that $\xi^{*}$ is a strong local maximum , in the following sense: For certain subsets of variables $Y\subset{\mathcal{X}}$ , there is assignment $\xi^{\prime}$ t t is higher-scoring than $\xi^{*}$ and difers from it only in the assignment to Y . These subsets $Y$ are those that induce any disjoint union of subgraphs each of which contains at most a single loop (including trees, which contain no loops). 

Let $\mathcal{U}$ be a cluster graph over $\mathcal{X}$ , and $Y\subset\mathcal{X}$ be some s of variables. We deﬁne the in uced subgraph $\mathcal{U}_{Y}$ to be the subgraph of clusters and sepsets in U that contain some variable in Y . 

This deﬁnition is most easily understood in the context of a pairwise Markov network, where the cluster graph is simply the set of edges in the MRF and the sepsets are the individual variables. In this case, the induced subgraph for a set $Y$ is simply the set of nodes corresponding to $Y$ and any edges that contain them. In a more general cluster graph, the result is somewhat more complex: 

# Theorem 13.7 

Let be a x-product calibrated cluster graph f $\tilde{P}_{\Phi}$ , and let $\xi^{*}$ be a locally optimal assignment for U . Let Z be any set of variables for which U $\mathcal{U}_{Z}$ is a collection of disjoint subgraphs each of which contains at most a single loop. Then for any assignment $\xi^{\prime}$ which is the same as $\xi^{*}$ except for the assignment to the variables in $Z$ , we have that 

$$
\begin{array}{r}{\tilde{P}_{\Phi}\big(\xi^{\prime}\big)\leq\tilde{P}_{\Phi}\big(\xi^{*}\big).}\end{array}
$$ 

Proof We prove the theorem under the assumption that $\mathcal{U}_{Z}$ is a single tree, leaving the rest of the proof as an exercise (exercise 13.12). Owing to the recalibration property, we can rewrite the joint probability $\tilde{P}_{\Phi}$ as in equation (13.12). We can partition the terms in this expression into two groups: those that involve variab s in $Z$ and those that do not. Let $Y=\mathcal{X}-Z$ and $y^{*}$ be the locally optimal assignment to Y . We now consider the unnormalized measure obtained over $Z$ when we restrict the distribution to the event $\pmb{Y}=\pmb{y}^{*}$ (as in deﬁnition 4.5). Since we set $\pmb{Y}=\pmb{y}^{*}$ , the terms corresponding to beliefs that do not involve $Z$ are constant, and hence they do not afect the comparison between $\xi^{\prime}$ and $\xi^{*}$ . 

We can now deﬁne $\tilde{P}_{y^{*}}^{\prime}(Z)$ to be the measure obtained by restricting equation (13.12) only to the terms in the beliefs (at both clusters and sepsets) that involve variables in $Z$ . It follows that an assignment $_z$ optimizes $\tilde{P}_{\Phi}(z,y^{*})$ if and only if it optimizes $\tilde{P}_{y^{\ast}}^{\prime}$ . This measure precisely corresponds to a clique tree w se structure is $\mathcal{U}_{Z}$ hose beliefs are the beliefs in our original calibrated cluster graph U , but restricted to $\pmb{Y}=\pmb{y}^{*}$ . Let $\mathcal{T}_{\pmb{y}^{*}}$ represent this clique tree and its associated beliefs. 

Because $\mathcal{U}$ is max-product calibrated, so is its ubgraph $\mathcal{T}_{y^{\ast}}$ . Moreover, if an assign- ment $(\boldsymbol{y}^{*},z^{*})$ is optimal for some belief $\beta_{i}$ , then z $z^{*}$ is also optimal for the restricted belief $\beta_{i}[\pmb{Y}=\pmb{y}^{*}]$ . We therefore have a max-product calibrated clique tree $\mathcal{T}_{\pmb{y}^{*}}$ and $z^{*}$ is a loc optimal assignment for it. Because this is a clique tree, local optimality implies MAP, and so $z^{*}$ must be a MAP assignment in this clique tree. As a consequence, there is no assignment $z^{\prime}$ that has a higher probability in $\tilde{P}_{y^{\ast}}^{\prime}$ , proving the desired result. 

To illustrate the power of this theorem, consider the following example: 

# Example 13.14 

Consider the $4\times4$ grid network in ﬁgure 11.4, and assume that we use the pairwise cluster graph construction of ﬁgure 11.6 (shown there for a $3\times3$ grid). This result implies that the MAP solution found by max-product belief propagation has higher probability than any assignment obtained by changing the assignment to any of the following subsets of variables $Y$ : 

• a set of variables in any single row, such as $\begin{array}{r}{\pmb{Y}=\{A_{1,1},A_{1,2},A_{1,3},A_{1,4}\};}\end{array}$ ; • a set of variables in any single column; • a “comb” structure such as the variables in row 1 , column 2 and column 4 ; • a single loop, such as $Y=\{A_{1,1},A_{1,2},A_{2,2},A_{2,1}\}$ ; • a collection of disconnected subsets of the preceding form, for example: the union of the variables in rows 1 and 3 ; or the loop above union with the L-structure consisting of the variables in row 4 and the variables in column 4 . 

This result is a powerful one, inasmuch as it shows that the solution obtained from max-product belief propagation is robust against large perturbations. Thus, although 

# one can construct examples where max-product belief propagation obtains the wrong solutions, these solutions are strong local maxima, and therefore they often have high probability. 

# 13.4.2 Max-Product BP with Counting Numbers $\star$ 

The preceding algorithm performs max-product message passing that is analogous to the sum- product message passing with the Bethe free-energy approximation. We can also construct analogues of the various generalizations of sum-product message passing, as deﬁned in sec- tion 11.3.7. We can derive max-product variants based both on the region-graph methods, which allow us to introduce larger clusters, and based on the notion of alternative counting numbers. From an algorithmic perspective, the transformation of sum-product to max-product algorithms is straightforward: we simply replace summation with maximization. The key question is the extent to which we can provide a formal justiﬁcation for these methods. 

Recall that, in our discussion of sum-product algorithms, we derived the belief propagation algorithms in two diferent ways. The ﬁrst was simply by taking the message passing algorithm on clique trees and running it on loopy cluster graphs, ignoring the presence of loops. The second derivation was obtained by a variational analysis, where the algorithm arose naturally as the ﬁxed points of an approximate energy functional. This view was compelling both because it suggested some theoretical justiﬁcation for the algorithm and, even more important, because it immediately gave rise to a variety of generalizations, obtained from diferent approximations to the energy functional, diferent methods for optimizing the objective, and more. 

For the case of max-product, our discussion so far follows the ﬁrst approach, viewing the message passing algorithm as a simple generalization of the max-product clique tree algorithm. Given the similarity between the sum-product and max-product algorithms presented so far, one may assume that we can analogously provide a variational justiﬁcation for max-product, for example, as optimizing the same energy functional, but with max-calibration rather than sum-calibration constraints on adjacent clusters. For example, in a variational derivation of the max-product clique tree algorithm, we would replace the sum-calibration constraint of equa- tion (11.7) with the analogous max-calibration constraint of equation (13.10). Although plausible, this analogy turns out to be incorrect. The key problem is that, whereas the sum-marginalization constraint of equation (11.7) is a simple linear equality, the constraint of equation (13.10) is not. Indeed, the max function involved in the constraint is not even smoothly diferentiable, so that the framework of Lagrange multipliers cannot be applied. 

However, as we now show, we can provide an optimization-based derivation and more formal justiﬁcation for max-product BP with convex counting numbers. For these variants, we can even show conditions under which these algorithms are guaranteed to produce the correct MAP assignment. We begin this section by describing the basic algorithm, and proving the key optimality result: that any locally optimal assignment for convex max-product BP is guaranteed to be the MAP assignment. Then, in section 13.5, we provide an alternative view of this approach in terms of its relationship to two other classes of algorithms. This perspective will shed additional insight on the properties of this algorithm and on the cases in which it provides a useful guarantee. 

![](images/ce8d5ecb42f304f79d7907e693a6840f92564130b762016ba1d4bdc19c9a61f5.jpg) 

# 13.4.2.1 Max-Product with Counting Numbers 

We begin with a reminder of the notion of belief propagation with counting numbers. For concreteness, we also provide the max-product variant of a message passing algorithm for this case, although (as we mentioned) the max-product variant can be obtained from the sum- product algorithm using a simple syntactic substitution. 

counting numbers 

Bethe cluster graphs 

In section 11.3.7, we deﬁned a set of sum-product message passing algorithms; these algorithms were deﬁned in terms of a set of counting numbers that specify the extent to which entropy terms for diferent subsets of variables are counted in the entropy approximation used in the energy functional. For a given set of counting numbers, one can derive a message passing algorithm by using the ﬁxed point equations obtained by diferentiating the Lagrangian for the energy functional, with its sum-product calibration constraints. The standard belief propagation algorithm is obtained from the Bethe energy approximation; other sets of counting numbers give rise to other message passing algorithms. 

As we discussed, one can take these sum-product message passing algorithms (for example, those in exercise 11.17 and exercise 11.19) and convert them to produce a max-product variant by simply replacing each summation operation as maximization. For concreteness, in algorithm 13.3, we repeat the algorithm of exercise 11.17, instantiated to the max-product setting. Recall that this algorithm applies only to Bethe cluster graphs , that is, graphs that have two levels of regions: “large” regions $r$ containing multiple variables with counting numbers $\kappa_{r}$ , and singleton regions containing individual variables $X_{i}$ with counting numbers $\kappa_{i}$ ; all factors in $\Phi$ are assigned only to large regions, so that $\psi_{i}=1$ for all $i$ . 

reparameteriza- tion 

A critical observation is that, like the sum-product algorithms, and like the max-product clique tree algorithm (see theorem 13.5), these new message passing algorithms are a re parameter iz ation of the original distribution. In other words, their ﬁxed points are a diferent representation of the same distribution, in terms of a set of max-calibrated beliefs. This property, which is stated for sum-product in theorem 11.6, asserts that, at ﬁxed points of the message passing algorithm, we have that: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\prod_{r}(\beta_{r})^{\kappa_{r}}.
$$ 

The proof of this equality (see exercise 11.16 and exercise 11.19) is a consequence only of the way in which we deﬁne region beliefs in terms of the messages. Therefore, the re parameter iz ation property applies equally to ﬁxed points of the max-product algorithms. It is this property that will be critical in our derivation. 

# 13.4.2.2 Optimality Guarantee 

As in the case of standard max-product belief propagation algorithms, given a set of max-product calibrated beliefs that reparameterize the distribution, we now search for an assignment that is locally optimal for this set of beliefs. However, as we now show, under certain assumptions, such an assignment is guaranteed to be the MAP assignment. 

convex counting numbers 

Although more general variants of this theorem exist, we focus on the case of a Bethe- structured region graph, as described. Here, we also assume that our large regions in $\mathbf{R}$ have counting number 1 . We assume also that factors in the network are assigned only to large regions, so that $\psi_{i}=1$ for all $i$ . Finally, in a property that is critical to the upcoming derivation, we assume that the counting numbers $\kappa_{r}$ are convex, as deﬁned in deﬁnition 11.4. Recall that a vector of counting numbers $\kappa_{r}$ is convex if there exist nonnegative numbers $\nu_{r},\;\nu_{i}$ , and $\nu_{r,i}$ such that: 

$$
\begin{array}{r l r}{\kappa_{r}}&{=}&{\nu_{r}+\sum_{i\,:\,X_{i}\in C_{r}}\nu_{r,i}\quad\mathrm{for~all}\ r}\\ {\kappa_{i}}&{=}&{\nu_{i}-\sum_{r\,:\,X_{i}\in C_{r}}\nu_{r,i}\quad\mathrm{for~all}\ i.}\end{array}
$$ 

This is the same assumption used to guarantee that the region-graph energy functional in equation (11.27) is a concave function. Although here we have no energy functional, the purpose of this assumption is similar: As we will see, it allows us to redistribute the terms in the re parameter iz ation of the probability distribution, so as to guarantee that all terms have a positive coefcient. 

From these assumptions, we can now prove the following theorem: 

Theorem 13.8 Let $P_{\Phi}$ be a distribution, and consider a Bethe-structured region graph with large regions and singleton regions, where the counting numbers are convex. Assume that we have a set of max- calibrated beliefs $\beta_{r}(C_{r})$ and $\beta_{i}(X_{i})$ such that equation (13.16) holds. If there exists an assignment $\xi^{*}$ that is locally optimal relative to each of the beliefs $\beta_{r}$ , then $\xi^{*}$ is the optimal MAP assignment. 

Proof Applying equation (13.16) to our Bethe-structured graph, we have that: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\prod_{r\in\mathbf{R}}\beta_{r}\prod_{i}\beta_{i}^{\kappa_{i}}.
$$ 

Owing to the convexity of the counting numbers, we can rewrite the right-hand side as: 

$$
\prod_{r}(\beta_{r})^{\nu_{r}}\prod_{i}(\beta_{i})^{\nu_{i}}\prod_{i,r\ :\ X_{i}\in C_{r}}\left(\frac{\beta_{r}}{\beta_{i}}\right)^{\nu_{r,i}}.
$$ 

Owing to the nonnegativity of the coefcients $\nu$ , we have that: 

$$
\begin{array}{c c l}{\displaystyle\operatorname*{max}_{\xi}\tilde{P}_{\Phi}(\xi)}&{=}&{\displaystyle\operatorname*{max}_{\xi}\prod_{r}(\beta_{r}(c_{r}))^{\nu_{r}}\prod_{i}(\beta_{i}(x_{i}))^{\nu_{i}}\prod_{i,r\,:\,X_{i}\in C_{r}}\left(\frac{\beta_{r}}{\beta_{i}}(c_{r})\right)^{\nu_{r,i}}}\\ &{\le}&{\displaystyle\prod_{r}(\operatorname*{max}_{c_{r}}\beta_{r}(c_{r}))^{\nu_{r}}\prod_{i}(\operatorname*{max}_{x_{i}}\beta_{i}(x_{i}))^{\nu_{i}}\prod_{i,r\,:\,X_{i}\in C_{r}}\left(\operatorname*{max}_{c_{r}}\frac{\beta_{r}}{\beta_{i}}(c_{r})\right)^{\nu_{r,i}}.}\end{array}
$$ 

We have now reduced this expression to a product of terms, each raised to the power of a positive exponent. Some of these terms are factors in the max-product calibrated network, and others are ratios of factors and their max-product marginal over an individual variable. The proof now is exactly the same as the proof of theorem 13.6. Let $\xi^{*}$ be an assignment that satisﬁes the local optimality property. By assumption, it optimizes every one of the region beliefs. Because the ratios involve a factor and its max-marginal, the conditions of lemma 13.1 hold for each of the ratios in this expression. Thus, $\xi^{*}$ optimizes each one of the terms in this product, and therefore it optimizes the product as a whole. It therefore optimizes $\tilde{P}_{\Phi}(\xi)$ , and must therefore be the MAP assignment. 

We can also derive the following useful corollary, which allows us, in certain cases, to char- acterize parts of the MAP solution even if the local optimality property does not hold: 

Under the setting of theorem 13.8, if a variable $X_{i}$ takes a particular value $\boldsymbol{x}_{i}^{*}$ in all locally optimal assignments $\xi^{*}$ then $x_{i}^{m a p}\,=\,x_{i}^{*}$ in the MAP assignment. More generally, if there is some set $S_{i}$ such that, in any locally optimal assignment $\xi^{*}$ we have that $x_{i}^{*}\in S_{i}$ ∈ , then $x_{i}^{m a p}\in S_{i}$ ∈ . 

At ﬁrst glance, the application of this result seems deceptively easy. After all, in order to be locally optimal, an assignment must assign to $X_{i}$ one of the values that maximizes its individual node marginal. Thus, it appears that we can easily extract, for each $X_{i}$ , some set $S_{i}$ (perhaps an overestimate) to which corollary 13.4 applies. Unfortunately, when we use this procedure, we cannot guarantee that $x_{i}^{m a p}$ is actually in the set $S_{i}$ . The corollary applies only if there exists a locally optimal assignment to the entire set of beliefs. If no such assignment exists, the set of locally maximizing values in $X_{i}$ ’s node belief may have no relation to the true MAP assignment. 

# 13.4.3 Discussion 

In this section, we have shown that max-product message passing algorithms, if they converge, provide a max-calibrated re parameter iz ation of the distribution $\tilde{P}_{\Phi}$ . This repa- rameterization essentially converts the global optimization problem of ﬁnding a single joint MAP assignment to a local optimization problem: ﬁnding a set of locally optimal assignments to the individual cliques that are consistent with each other. Importantly, we can show that this locally consistent assignment, if it exists, satisﬁes strong opti- mality properties: In the case of the standard (Bethe-approximation) re parameter iz ation, 

# the joint assignment satisﬁes strong local optimality; in the case of re parameter iz at ions based on convex counting numbers, it is actually guaranteed to be the MAP assignment. 

Although these guarantees are very satisfying, their usefulness relies on several important questions that we have not yet addressed. The ﬁrst two relate to the max-product calibrated re parameter iz ation of the distribution: does one exist, and can we ﬁnd it? First, for a given set of counting numbers, does there always exist a max-calibrated re parameter iz ation of $\tilde{P}_{\Phi}$ in terms of these counting numbers? Somewhat surprisingly, as we show in section 13.5.3, the answer to that question is yes for convex counting numbers; it turns out to hold also for the Bethe counting numbers, although we do not show this result. Second, we must ask whether we can always ﬁnd such a re parameter iz ation. We know that if the max-product message passing algorithm converges, it must converge to such a ﬁxed point. But unfortunately, there is no guarantee that the algorithm will converge. In practice, standard max-product message passing often does not converge. For certain speciﬁc choices of convex counting numbers (see, for example, box 13.A), one can design algorithms that are guaranteed to be convergent. 

However, even if we ﬁnd an appropriate re parameter iz ation, we are still left with the problem of extracting a joint assignment that satisﬁes the local optimality property. Indeed, such as assignment may not even exist. In section 13.5.3.3, we present a necessary condition for the existence of such an assignment. 

It is currently not known how the choice of counting numbers afects either of these two issues: our ability to ﬁnd efectively a max-product calibrated re parameter iz ation, and our ability to use the result to ﬁnd a locally consistent joint assignment. Empirically, preliminary results suggest that nonconvex counting numbers (such as those obtained from the Bethe approxima- tion) converge less often than the the convex variants, but converge more quickly when they do converge. The diferent convex variants converge at diferent rates, but tend to converge to ﬁxed points that have a similar set of ambiguities in the beliefs. Moreover, in cases where convex max-product BP converges whereas standard max-product does not, the resulting beliefs often contain many ambiguities (beliefs with equal values), making it difcult to determine whether the local optimality property holds, and to identify such an assignment if it exists. 

tree-reweighted belief propagation LP relaxation 

Box 13.A — Concept: Tree-Reweighted Belief Propagation. One algorithm that is worthy of special mention, both because of historical precedence and because of its popularity, is the tree- reweighted belief propagation algorithm (often known as TRW ). This algorithm was the ﬁrst mes- sage passing algorithm to use convex counting numbers; it was also the context in which message passing algorithms were ﬁrst shown to be related to the linear program relaxation of the MAP optimization problem that we discuss in section 13.5. This algorithm, developed in the context of a pairwise Markov network, utilizes the same approach as in the TRW variant of sum-product message passing: It deﬁnes a probability distribution over trees $\mathcal{T}$ in the network, so that each edge in the pairwise network appears in at least one tree, and it then deﬁnes the counting numbers to be the edge and negative node appearance probabilities, as deﬁned in equation (11.26). Note that, unlike the algorithms of section 13.4.2.1, here the factors do not have a counting number of 1 , so that the algorithms we presented there require some modiﬁcation. Brieﬂy, the max-product TRW algorithm uses the following update rule: 

$$
\delta_{i\rightarrow j}=\operatorname*{max}_{x_{i}}\left[\left(\psi_{i}(x_{i})\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}(x_{i})\right)^{\frac{\kappa_{i,j}}{\kappa_{i}}}\frac{1}{\delta_{j\rightarrow i}(x_{i})}\psi_{i,j}(x_{i},x_{j})\right].
$$ 

One particular variant of the TRW algorithm, called TRW-S, provides some particularly satisfying guarantees. Assume that we order the nodes in the network in some ﬁxed ordering $X_{1},\ldots,X_{n},$ , and consider a set of trees each of which is a subsequence of this ordering, that is, of the form $X_{i_{1}},\dots,X_{i_{k}}$ for $i_{1}<\,.\,.\,i_{k}$ . We now pass messages in the network by repeating two phases, where in one phase we pass messages from $X_{1}$ towards $X_{n}$ , and in the other from $X_{n}$ towards $X_{1}$ . With this message passing scheme, it is possible to guarantee that the algorithm continuously increases the dual objective, and hence it is convergent. 

# 13.5 MAP as a Linear Optimization Problem $\star$ 

A very important and useful insight on the MAP problem is derived from viewing it directly as an optimization problem. This perspective allows us to draw upon the vast literature on optimization algorithms and apply some of these ideas to the speciﬁc case of MAP inference. Somewhat surprisingly, some of the algorithms that we describe elsewhere in this chapter turn out to be related to the optimization perspective; the insights obtained from understanding the connections can provide the basis for a theoretical analysis of these methods, and they can suggest improvements. 

For the purposes of this section, we assume that the distribution speciﬁed in the MRF is positive, so that all of the entries in all of the factors are positive. This assumption turns out to be critical for some of the derivations in this section, and facilitates many others. 

# 13.5.1 The Integer Program Formulation 

integer linear program 

max-sum 

The basic MAP problem can be viewed as an integer linear program — an optimization problem (see appendix A.4.1) over a set of integer valued variables, where both the objective and the constraints are linear. To deﬁne a linear optimization problem, we must ﬁrst turn all of our products into summations. This transformation gives rise to the following max-sum form: 

$$
\arg\operatorname*{max}_{\xi}\log\tilde{P}_{\Phi}(\xi)=\arg\operatorname*{max}_{\xi}\sum_{r\in\mathbf{R}}\log(\phi_{r}(\pmb{c}_{r})),
$$ 

where $\Phi=\{\phi_{r}\ :\ r\in\mathbf{R}\}$ , and $C_{r}$ is the scope of $\phi_{r}$ . 

For $r\in\mathbf{R}$ , we deﬁne $n_{r}=|V a l(C_{r})|$ . For an nment $\xi$ , if $\xi\langle C_{r}\rangle=c_{r}^{j}$ , the factor $\log(\phi_{r})$ makes a contribution to the objective of $\log(\phi_{r}(c_{r}^{j}))$ , a quantity that we denote as $\eta_{r}^{j}$ . 

optimization variables 

We introduce optimization variables $q(\pmb{x}_{r}^{j})$ , where $r\in\mathbf{R}$ enumerates the di nt factors, and $j=1,\dots,n_{r}$ enumerates the diferent possible assignments to the variables $C_{r}$ that comprise the factor $C_{r}$ . These variables take binary values, so that $q(\pmb{x}_{r}^{j})=1$ if and only if $C_{r}=c_{r}^{j}$ , and 0 otherwise. It is important to distinguish the optimization variables from the random variables in our original graphical model; here we have an optimization variable $q(\pmb{x}_{r}^{j})$ for each joint assignment $c_{r}^{j}$ to the model variables $C_{r}$ . 

Let $q$ denote a vector of the optimization variables $\{q(\pmb{x}_{r}^{j})\ :\ r\in\mathbf{R};\ j=1,.\,.\,.\,,n_{r}\}$ ∈ , and $\eta$ denote a vector of the coefcient $\eta_{r}^{j}$ sorted in the same order. Both of these are vectors of dimension $\begin{array}{r}{N=\sum_{k=1}^{K}n_{r}}\end{array}$ . With this interpretation, the MAP objective can be rewritten as: 

$$
\mathbf{maximize}_{q}\sum_{r\in\mathbf{R}}\sum_{j=1}^{n_{r}}\eta_{r}^{j}q(\pmb{x}_{r}^{j}),
$$ 

or, in shorthand, maximize ${}_{q}\eta^{T}q$ . 

Example 13.15 Assume that we have a pairwise MRF shaped like a triangle $\scriptstyle A-B-C-A.$ , so that we have three factors over pairs of connected random variables: $\phi_{1}(A,B),\phi_{2}(B,C),\phi_{3}(A,C)$ . Assume that $A,B$ are binary-valued, whereas $C$ takes three values. Here, we would have the optimization variables $q(\pmb{x}_{1}^{1}),.\,.\,,q(\pmb{x}_{1}^{4}),q(\pmb{x}_{2}^{1}),.\,.\,.\,,q(\pmb{x}_{2}^{6}),q(\pmb{x}_{3}^{1}),.\,.\,.\,,q(\pmb{x}_{3}^{6})$ . We assume that the values of the variables are enumerated le xico graphically, so that $q(x_{3}^{4})$ , for example, corresponds to $a^{2},c^{1}$ . 

We can view our MAP inference problem as optimizing this linear objective over the space of assignments to $\pmb q\,\in\,\{0,1\}^{N}$ that correspond to legal assignments to $\mathcal{X}$ . What constraints on $q$ do we need to impose in order to guarantee that it corresponds to some assignment to $\mathcal{X}?$ Most obviously, we need to ensure that, in each factor, only a single assignment is selected. Thus, in our example, we cannot have both $q(\pmb{x}_{1}^{1})=1$ and $q(\pmb{x}_{1}^{2})=1$ . Slightly subtler are the cross-factor consistency constraints: If two factors share a variable, we need to ensure that the assignment to this variable according to $q$ is consistent in those two factors. In our example, for instance, if we have that $q(\pmb{x}_{1}^{1})=1$ , so that $B\,=\,b^{1}$ , we would need to have $q(\pmb{x}_{2}^{1})=1$ , $q(\pmb{x}_{2}^{2})=1$ , or $q(\pmb{x}_{2}^{3})=1$ . 

There are several ways of encoding these consistency constraints. First, we require that we restrict attention to integer solutions: 

$$
q(\pmb{x}_{r}^{j})\in\{0,1\}\qquad\qquad\qquad\mathrm{For~all~}r\in\mathbf{R};\,j\in\{1,.\,.\,,n_{r}\}.
$$ 

We can now utilize two linear equalities to enforce the consistency of these integer solutions. The ﬁrst constraint enforces the mutual exclusivity within a factor: 

$$
\sum_{j=1}^{n_{r}}q(\pmb{x}_{r}^{j})=1
$$ 

$$
{\mathrm{For~all~}}r\in\mathbf{R}.
$$ 

The second constraint implies that factors in our MRF agree on the variables in the intersection of their scopes: 

$$
\sum_{j\ :\ c_{r}^{j}\sim s_{r,r^{\prime}}}q(\pmb{x}_{r}^{j})=\sum_{l\ :\ c_{r^{\prime}}^{l}\sim s_{r,r^{\prime}}}q(\pmb{x}_{r^{\prime}}^{l})
$$ 

Note that this constraint is vacuous for pairs of clusters whose intersection is empty, since there are no assignments $s_{r,r^{\prime}}\in V a l(C_{r}\cap C_{r^{\prime}})$ . 

# Example 13.16 

Returning to example 13 tual constraints for $\phi_{1}$ would assert that $\textstyle\sum_{j=1}^{4}q(\pmb{x}_{1}^{j})=$ 1 . Altogether, we would have three such constraints — one for each factor. The consistency con- straints associated with $\phi_{1}(A,B)$ and $\phi_{2}(B,C)$ assert that: 

$$
\begin{array}{l c l}{{q(\pmb{x}_{1}^{1})+q(\pmb{x}_{1}^{3})}}&{{=}}&{{q(\pmb{x}_{2}^{1})+q(\pmb{x}_{2}^{2})+q(\pmb{x}_{2}^{3})}}\\ {{q(\pmb{x}_{1}^{2})+q(\pmb{x}_{1}^{4})}}&{{=}}&{{q(\pmb{x}_{2}^{4})+q(\pmb{x}_{2}^{5})+q(\pmb{x}_{2}^{6}),}}\end{array}
$$ 

where the ﬁrst constraint ensures consistency when $B=b^{1}$ and the second when $B=b^{2}$ . Overall, we would have three such constraints for $\phi_{2}(B,C),\phi_{3}(A,C)$ , corresponding to the three values of $C$ , and two constraints for $\phi_{1}(A,B),\phi_{3}(A,C)$ , corresponding to the two values of $A$ . 

Together, these constraints imply that there is a one-to-one mapping between possible assignments to the $q(\pmb{x}_{r}^{j})$ optimization variables and legal assignments to $A,B,C$ . 

In general, equation (13.20), equation (13.21), and equation (13.22) together imply that the assignment $q(\pmb{x}_{r}^{j})$ ’s correspond to a single legal assignment: 

# Proposition 13.4 

Any assignment to the optimization variables $q$ that satisﬁes equation (13.20), equation (13.21), and equation (13.22) corresponds to a single legal assignment to $X_{1},\ldots,X_{n}$ . 

The proof is left as an exercise (see exercise 13.13). 

Thus, we have now reformulated the MAP task as an integer linear program, where we optimize the linear objective of equation (13.19) subject to the constraints equation (13.20), equation (13.21), and equation (13.22). We note that the problem of solving integer linear programs is itself $\mathcal{N P}$ - hard, so that (not surprisingly) we have not avoided the basic hardness of the MAP problem. However, there are several techniques that have been developed for this class of problems, which can be usefully applied to integer programs arising from MAP problems. One of the most useful is described in the next section. 

# 13.5.2 Linear Programming Relaxation 

LP relaxation 

linear program 

linear program One of the methods most often used for tackling integer linear programs is the method of linear program relaxation . In this approach, we turn a discrete, combinatorial optimization problem into a continuous problem. This problem is a linear program (LP), which can be solved in polynomial time, and for which a range of very efcient algorithms exist. One can then use the solutions to this LP to obtain approximate solutions to the MAP problem. To perform this relaxation, we substitute the constraint equation (13.20) with a relaxed constraint : 

$$
q(\pmb{x}_{r}^{j})\geq0\quad\mathrm{For~all~}r\in\mathbf{R},\ j\in\{1,\ldots,n_{r}\}.
$$ 

This constraint and equation (13.21) together imply that each $q(\pmb{x}_{j}^{r})\,\in\,[0,1]$ ; thus, we have relaxed the combinatorial constraint into a continuous one. This relaxation gives rise to the following linear program (LP): 

![](images/c35f44ebe5f16184cbc243a95831b614c6d14d72b5e351b68de6a0dd719f0790.jpg) 

This linear program is a relaxation of our original integer program, since every assignment to $q$ that satisﬁes the constraints of the integer problem also satisﬁes the constraints of the linear program, but not the other way around. Thus, the optimal value of the objective of the relaxed version will be no less than the value of the (same) objective in the exact version, and it can be greater when the optimal value is achieved at an assignment to $q$ that does not correspond to a legal assignment $\xi$ . 

pseudo-marginals local consistency polytope 

A closer examination shows that the space of assignments to $q$ that satisﬁes the constraints of MAP-LP corresponds exactly to the locally consistent pseudo-marginals for our cluster graph $\mathcal{U}$ , which comprise the local consistency polytope Local [ U ] , deﬁned in equation (11.16). To see this equivalence, we note that equation (13.23) and equation (13.21) imply that any assignment to $q$ deﬁnes a set of locally normalized distributions over the clusters in the cluster graph — nonnegative factors that sum to 1; by equation (13.22), these factors must be sum-calibrated. Thus, there is a one-to-one mapping between consistent pseudo-marginals and possible solutions to the LP. 

We can use this observation to answer the following important question: Given a non-integer solution to the relaxed LP, how can we derive a concrete assignment? One obvious approach is a greedy assignment process, which assigns values to the variables $X_{i}$ one at a time. For each variable, and for each possible assignment $x_{i}$ , it considers the set of reduced pseudo-marginals that would result by setting $X_{i}=x_{i}$ . We can now compute the energy term (or, equivalently, the LP objective) for each such assignment, and select the value $x_{i}$ that gives the maximum value. We then permanently reduce each of the pseudo-marginals with the assignment $X_{i}=x_{i}$ , and continue. We note that, at the point when we assign $X_{i}$ , some of the variables have already been assigned, whereas others are still undetermined. At the end of the process, all of the variables have been assigned a speciﬁc value, and we have a single joint assignment. 

marginal polytope 

To understand the result obtained by this algorithm, recall that Local [ U ] is a supe et of the marginal polytope Marg [ U ] — the space of legal distributions that factorize over U (see equation (11.15)). Because our objective in equation (13.19) is linear, it has the same optimum over the marginal polytope as over the original space of $\{0,1\}$ solutions: The value of the objective at a point corresponding to a distribution $P(\mathcal X)$ is the expectation of its value at the assignments $\xi$ that receive positive probability in P ; therefore, one cannot achieve a higher value of the objective with respect to $P$ than with respect to the highest-value assignment $\xi$ . Thus, if we could perform our optimization over the continuous space Marg [ U ] , we would ﬁnd the optimal solution to our MAP objective. However, as we have already discussed, the marginal 

polytope is a complex object, which can be speciﬁed only using exponentially many constraints. Thus, we cannot feasibly perform this optimization. 

By contrast, the optimization problem obtained by this relaxed version has a linear objective with linear constraints, and both involve a number of terms which is linear in the size of the cluster graph. Thus, this linear program admits a range of efcient solutions, including ones with polynomial time guarantees. We can thus apply of-the-shelf methods for solving such problems. Of course, the result is often fractional, in which case it is clearly not an optimal solution to the MAP problem. 

The LP formulation has advantages and disadvantages. By formulating our problem as  a linear program, we obtain a very ﬂexible framework for solving it; in particular, we can easily incorporate additional constraints into the LP, which reduce the space of possible assignments to $q.$ , eliminating some solutions that do not correspond to actual distributions over $\mathcal{X}$ . (See section 13.9 for some references.) On the other hand, as we discussed in example 11.4, the optimization problems deﬁned over this space of constraints are very large, making standard optimization methods very expensive. Of course, the LP has special structure: For example, when viewed as a matrix, the equality constraints in this LP all have a particular block structure that corresponds to the structure of adjacent clusters; moreover, when the MRF is not densely connected, the constraint matrix is also sparse. However, standard LP solvers may not be ideally suited for exploiting this special structure. Thus, empirical evidence suggests that the more specialized solution methods for the MAP problems are often more efective than using of-the-shelf LP solvers. As we now discuss, the convex message passing algorithms described in section 13.4.2 can be viewed as specialized solution methods to the dual of this LP. More recent work explicitly aims to solve this dual using general-purpose optimization techniques that do take advantage of the structure; see section 13.9 for some references. 

# 13.5.3 Low-Temperature Limits 

In this section, we show how we can use a limit process to understand the connection between the relaxed MAP-LP and both sum-product and max-product algorithms with convex counting numbers. As we show, this connection provides signiﬁcant new insight on all three algorithms. 

# 13.5.3.1 LP as Sum-Product Limit 

More precisely, recall that the energy functional is deﬁned as: 

$$
{\cal F}[P_{\Phi},Q]=\sum_{\phi_{r}\in\Phi}{\cal E}_{C_{r}\sim Q}[\log\phi_{r}(C_{r})]+\tilde{\cal H}_{Q}(\mathcal{X}),
$$ 

where $\tilde{H}_{Q}(\mathcal{X})$ X is some (exact or approximate) version of the entropy of $Q$ . Consider the ﬁrst term in this expression, also called the energy term. Let $q(\pmb{x}_{r}^{j})$ denote the cluster marginal $\beta_{r}(c_{r}^{j})$ . Then we can rewrite the energy term as: 

$$
\sum_{r\in\mathbf{R}}\sum_{j=1}^{n_{r}}q(\pmb{x}_{r}^{j})\log\bigl(\phi_{r}(\pmb{c}_{r}^{j}),\
$$ 

which is precisely the objective in our LP relaxation of the MAP problem. Thus, the energy functional is simply a sum of two terms: the LP relaxation objective, and an entropy term. In temperature- weighted energy function temperature 

the energy functional, both of these terms receive equal weight. Now, however, consider an alternative objective, called the temperature-weighted energy function . This objective is deﬁned in terms of a temperature parameter $T>0$ : 

$$
\tilde{F}^{(T)}[P_{\Phi},Q]=\sum_{\phi_{r}\in\Phi}E_{C_{r}\sim Q}[\log\phi_{r}(C_{r})]+T\tilde{H}_{Q}(\mathcal{X}).
$$ 

As usual in our derivations, we consider the task of maximizing this objective subject to the sum-marginalization constraints, that is, that $Q\in L o c a l[\mathcal{U}]$ . 

The temperature-weighted energy functional reweights the importance of the two terms in the objective. Since $T\longrightarrow0$ → , we will place a greater emphasis on the l ergy term (the hich is precisely the objective of the relaxed LP. Thus, since $T\longrightarrow0$ − → , the objective

 $\tilde{F}^{(T)}[P_{\Phi},Q]$ tends to the LP objective. Can we then infer that the ﬁxed points of the objective

 (say, those obtained from a message passing algorithm) are necessarily optima of the LP? The answer to this question is positive for concave versions of the entropy, and negative otherwise. 

In particular, assume that $\tilde{H}_{Q}(\mathcal X)$ X is a weighted entropy $\tilde{H}_{Q}^{\kappa}(\dot{\chi})$ , such that $\kappa$ is a convex set of counting numbers, as in equation (11.20). From the assumption on convexity of the counting numbers and the positivity of the distribution, it follows that the function $\tilde{F}^{(T)}[P_{\Phi},Q]$ is strongly convex in the distribution $Q$ . The space Local [ U ] is a convex space. Thus, there is a unique global minimum $Q^{*(T)}$ for every $T$ , and that optimum changes continuously in $T$ . Standard results now imply that the limit of $Q^{*(T)}$ is optimal for the limiting problem, which is precisely the LP. 

On the other hand, this result does not hold for nonconvex entropies, such as the one obtained by the Bethe approximation, where the objective can have several distinct optima. In this case, there are examples where a sequence of optima obtained for diferent values of $T$ converges to a point that is not a solution to the LP. Thus, for the remainder of this section, we assume that $\tilde{H}_{Q}(\mathcal X)$ X is derived from a convex set of counting numbers. 

# 13.5.3.2 Max-Product as Sum-Product Limit 

What do we gain from this perspective? It does not appear practical to use this characterization as a constructive solution method. For one thing, we do not want to solve multiple optimization problems, for diferent values of $T$ . For another, the optimization problem becomes close to degenerate as $T$ grows small, making the problem hard to solve. However, if we consider the dual problem to each of the optimization problems of this sequence, we can analytically characterize the limit of these duals. Surprisingly, this limit turns out to be a ﬁxed point of the max-product belief propagation algorithm. 

We ﬁrst note that the temperature-weighted energy functional is virtually identical in its form to the original functional. Indeed, we can formalize this intuition if we divide the objective through by $T$ ; since $T>0$ , this step does not change the optima. The resulting objective has the form: 

$$
\begin{array}{r c l}{\displaystyle\frac{1}{T}\sum_{\phi_{r}\in\Phi}\pmb{E}_{C_{r}\sim Q}[\log\phi_{r}(\pmb{C}_{r})]+\pmb{H}_{Q}(\mathcal{X})}&{=}&{\displaystyle\sum_{\phi_{r}\in\Phi}\pmb{E}_{C_{r}\sim Q}\left[\frac{1}{T}(\log\phi_{r}(\pmb{C}_{r}))\right]+\tilde{\pmb{H}}_{Q}(\mathcal{X})}\\ &{=}&{\displaystyle\sum_{\phi_{r}\in\Phi}\pmb{E}_{C_{r}\sim Q}\left[\log\phi_{r}^{1/T}\right]+\tilde{\pmb{H}}_{Q}(\mathcal{X}).}\end{array}
$$ 

This objective has precisely the same form as the standard approximate energy functional, but for a diferent set of factors: the original factors, raised to the power of $1/T$ . This set of factors deﬁnes a new unnormalized density: 

$$
\tilde{P}_{\Phi}^{(T)}(\mathcal{X})=(\tilde{P}_{\Phi}(\mathcal{X}))^{1/T}.
$$ 

Because our entropy is concave, and using our assumption that the distribution is positive, the approximate free energy $\tilde{F}[P_{\Phi},Q]$ is strictly convex and hence has a unique global minimum $Q^{(T)}$ for each temperature $T$ . We can now consider the Lagrangian dual of this new objective, and characterize this unique optimum via its dual parameter iz ation $Q^{(T)}$ . In particular, as we have previously shown, $Q^{(T)}$ is a re parameter iz ation of the distribution $\tilde{P}_{\Phi}^{(T)}(\bar{\mathcal X})$ X : 

$$
\tilde{P}_{\Phi}^{(T)}=\prod_{r\in{\bf R}}\beta_{r}^{(T)}\prod_{i}(\beta_{i}^{(T)})^{\kappa_{i}}=\prod_{r\in{\bf R}^{+}}(\beta_{r}^{(T)})^{\kappa_{i}},
$$ 

where, for simplicity of notation, we deﬁne $\mathbf{R}^{+}=\mathbf{R}\cup\mathcal{X}$ and $\kappa_{r}=1$ $r\in\mathbf{R}$ . 

Our goal is now to understand what hap $Q^{(T)}$ as we take $T\longrightarrow0$ − → . We ﬁrst reformulate these beliefs by deﬁning, for every region r $r\in\mathbf{R}^{+}$ ∈ : 

$$
\begin{array}{r c l}{\bar{\beta}_{r}^{(T)}}&{=}&{\displaystyle\operatorname*{max}_{{\pmb x}_{r}^{\prime}}\beta_{r}^{(T)}({\pmb x}_{r}^{\prime})}\\ {\tilde{\beta}_{r}^{(T)}({\pmb c}_{r})}&{=}&{\displaystyle\left(\frac{\beta_{r}^{(T)}({\pmb c}_{r})}{\bar{\beta}_{r}^{(T)}}\right)^{T}.}\end{array}
$$ 

The entries in the new beliefs $\tilde{\boldsymbol{\beta}}^{(T)}\,=\,\{\tilde{\beta}_{r}^{(T)}(\boldsymbol{C}_{r})\}$ { } take values between 0 and 1 , with the maximal entry in each belief always having the value 1 . 

We now deﬁne the limiting value of these beliefs: 

$$
{\tilde{\beta}}_{r}^{(0)}({\pmb{c}}_{r})=\operatorname*{lim}_{T\longrightarrow0}{\tilde{\beta}}_{r}^{(T)}({\pmb{c}}_{r}).
$$ 

Because the optimum changes continuously in $T$ , and because the beliefs take values in a convex space (all are in the range $[0,1])$ ), the limit is well deﬁned. Our goal is to show that the limit beliefs $\tilde{\boldsymbol{\beta}}^{(0)}$ are a ﬁxed point of the max-product belief propagation algorithm for the model $\tilde{P}_{\Phi}$ . We show this result in two parts. We ﬁrst show that the limit is max-calibrated, and then that it provides a re parameter iz ation of our distribution $\tilde{P}_{\Phi}$ . 

The limiting beliefs $\tilde{\boldsymbol{\beta}}^{(0)}$ are max-calibrated. 

Proof We wish to show that for any region $r$ , any $X_{i}\in C_{r}$ , and any $x_{i}\in V a l(X_{i})$ , we have: 

$$
\operatorname*{max}_{\pmb{c}_{r}\sim x_{i}}\tilde{\beta}_{r}^{(0)}(\pmb{c}_{r})=\tilde{\beta}_{i}^{(0)}(x_{i}).
$$ 

Consider the left-hand side of this equality. 

$$
{\begin{array}{r l}{\operatorname*{max}_{i\in[0,\pi]}|\tilde{\mathcal{H}}^{n}(e_{i})}&{=\;\;\operatorname*{max}_{i\in[0,\pi]}\left\{\operatorname*{min}_{j\in[\pi]}\tilde{\mathcal{H}}^{n}(e_{j})\right\}}\\ {\mathrm{(i)}}&{=\;\;\operatorname*{min}_{j\in[0,\pi]}\left(\sum_{k=1}^{\pi}(\tilde{\mathcal{H}}^{n}(e_{k}))^{(1)/2}\right)^{T}}\\ &{=\;\;\operatorname*{min}_{j\in[0,\pi]}\left(\sum_{k=1}^{\pi}\left(\frac{\tilde{\mathcal{H}}^{n}(e_{k})}{\tilde{\mathcal{H}}^{n}(e_{k})}\right)\right)^{T}}\\ &{=\;\;\operatorname*{min}_{j\in[0,\pi]}\left(\sum_{k=1}^{\pi}\left(\sum_{k=1}^{\pi}\tilde{\mathcal{H}}^{n}(e_{k})\right)\right)^{T}}\\ {\mathrm{(i i)}}&{=\;\;\operatorname*{min}_{j\in[0,\pi]}\left({\frac{1}{\tilde{\mathcal{H}}^{n}(e_{j})}}\beta^{\mathbb{T}}(e_{k})\right)^{T}}\\ {\mathrm{(ii)}}&{=\;\;{\frac{1}{\operatorname*{min}_{j\in[0,\pi]}}}\left({\frac{\beta\mathbb{T}}{\beta^{n}(e_{j})}}\frac{\beta^{n}(e_{j})}{\mu_{j}^{n}}\right)^{T}}\\ {\mathrm{(v)}}&{=\;\;{\frac{1}{\operatorname*{min}_{j\in[0,\pi]}}}\left({\frac{\beta\mathbb{T}}{\beta^{n}(e_{j})}}(\tilde{\mathcal{H}}^{n}(e_{k}))^{(1)/2}\right)^{T}}\end{array}}
$$ 

as required. In this derivation, the step marked (i) is a general relationship between max- imization and summation; see lemma 13.2. The step marked (ii) is a consequence of the sum-marginalization property of the region beliefs $\beta_{r}^{(T)}(C_{r})$ relative to the individual node belief. The step marked (iii) is simply multiplying and dividing by the same expression. The step marked (iv) is derived directly by substituting the deﬁnition of $\tilde{\beta}_{i}^{(T)}(x_{i})$ . The step marked (v) is a consequence of the fact that, because of sum-marginalization, $\bar{\beta}_{i}^{(T)}/\bar{\beta}_{r}^{(T)}$ (for $X_{i}\in C_{r}$ ) ed in the range $[1,|\,V a l(C_{r}-\{X_{i}\})|]$ for any $T\,>\,0$ , and therefore its limit, since $T\longrightarrow0$ is 1 . 

It remains to prove the following lemma: 

$$
\operatorname*{max}_{i}\operatorname*{lim}_{T\longrightarrow0}a_{i}(T)=\operatorname*{lim}_{T\longrightarrow0}\left(\sum_{i}(a_{i}(T))^{1/T}\right)^{T}
$$ 

Proof Because the functions are continuous, we have that, for some $T_{0}$ , there exists some $j$ such that, for any $T\,<\,T_{0}$ , $a_{j}(T)\,\geq\,a_{i}(T)$ for all $i\neq j$ ; assume, for simplicity, that this $j$ is unique. (The proof of the more general case is similar.) Let $\begin{array}{r}{a_{j}^{*}\,=\,\operatorname*{lim}_{T\longrightarrow0}a_{j}(T)}\end{array}$ . The left-hand side of equation (13.31) is then clearly $a_{j}^{*}$ . The expression on the right-hand side can be rewritten: 

$$
\operatorname*{lim}_{T\longrightarrow0}a_{j}(T)\left(\sum_{i}\left(\frac{a_{i}(T)}{a_{j}(T)}\right)^{1/T}\right)^{T}\ =a_{j}^{*}\left[\operatorname*{lim}_{T\longrightarrow0}\left(\sum_{i}\left(\frac{a_{i}(T)}{a_{j}(T)}\right)^{T}\right)^{1/T}\right]\ =a_{j}^{*}
$$ 

The ﬁrst equality follows from the fact that the $a_{j}(T)$ sequence is convergent. The second follows from the fact that, because $a_{j}(T)\,>\,a_{i}(T)$ for all $i\neq j$ and all $T\,<\,T_{0}$ , the ratio $a_{i}(T)/a_{j}(T)$ is bounded in $[0,1]$ , with $a_{j}(T)/a_{j}(T)=1$ ; therefore the limit is simply 1 . 

The proof of this lemma concludes the proof of the theorem. We now wish to show the second important fact: 

Theorem 13.9 The limit $\tilde{\boldsymbol{\beta}}^{(0)}$ is $a$ proportional re parameter iz ation of $\tilde{P}_{\Phi}$ , that is: 

$$
\tilde{P}_{\Phi}(\mathcal{X})\propto\prod_{r\in\mathbf{R}}(\tilde{\beta}_{r}^{(0)}(\pmb{c}_{r}))^{\kappa_{r}}.
$$ 

Proof Due to equation (13.26), we have that 

$$
\tilde{P}_{\Phi}^{(T)}(\mathcal{X})=\prod_{r\in\mathbf{R}}(\beta_{r}^{(T)}(c_{r}))^{\kappa_{r}}.
$$ 

We can raise each side to the power $T$ , and obtain that: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\left(\prod_{r\in\mathbf{R}}(\beta_{r}^{(T)}(\pmb{c}_{r}))^{\kappa_{r}}\right)^{T}.
$$ 

We can divide each side by 

$$
\left(\prod_{r\in\mathbf{R}^{+}}(\bar{\beta}_{r}^{(T)})^{\kappa_{r}}\right)^{T},
$$ 

to obtain the equality 

$$
\frac{\tilde{P}_{\Phi}(\mathcal{X})}{\left(\prod_{r\in\mathbf{R}^{+}}(\bar{\beta}_{r}^{(T)})^{\kappa_{r}}\right)^{T}}=\prod_{r}(\tilde{\beta}_{r}^{(T)}(\pmb{c}_{r}))^{\kappa_{r}}.
$$ 

This equality holds for every value of $T>0$ . Moreover, as we argued, the right-hand side is bounded in $[0,1]$ , and hence so is the left-hand side. As a consequence, we have an equality of two bounded continuous functions of $T$ , so that they must also be equal at the limit $T\longrightarrow0$ → . It follows that the limiting beliefs $\tilde{\boldsymbol{\beta}}^{(0)}$ are proportional to a re parameter iz ation of $\tilde{P}_{\Phi}$ . 

Overall, the analysis in this section reveals interesting connections between three separate al- gorithms: the linear program relaxation of the MAP problem, the low-temperature limit of sum-product belief propagation with convex counting numbers, and the max-product reparam- eterization with (the same) convex counting numbers. These connections hold for any set of convex counting numbers and any (positive) distribution $\tilde{P}_{\Phi}$ . 

Speciﬁcally, we have characterized the solution to the relaxed LP as the limit of a sequence of optimization problems, each deﬁned by a temperature-weighted convex energy functional. Each of these optimization problems can be solved using an algorithm such as convex sum- product BP, which (assuming convergence) produces optimal beliefs for that problem. We have also shown that the beliefs obtained in this sequence can be reformulated to converge to a new set of beliefs that are max-product calibrated. These beliefs are ﬁxed points of the convex max-product BP algorithm. Thus, we can hope to use max-product BP to ﬁnd these limiting beliefs. 

Our earlier results show that the ﬁxed points of convex max-product BP, if they admit a locally optimal assignment, are guaranteed to produce the MAP assignment. We can now make use of the results in this section to shed new light on this analysis. 

Theorem 13.10 Assume that we have a set of max-calibrated beliefs $\beta_{r}(C_{r})$ and $\beta_{i}(X_{i})$ such that equation (13.16) holds. Assume furthermore that $\xi^{*}$ is a locally consistent joint assignment relative to these beliefs. Then the MAP-LP relaxation is tight. 

Proof We ﬁrst observe that 

$$
\begin{array}{r l r}&{}&{\underset{\xi}{\operatorname*{max}}\log\tilde{P}_{\Phi}(\xi)=\underset{Q\in M a r g\lfloor\mathcal{U}\rfloor}{\operatorname*{max}}E_{\xi\sim Q}\Big[\log\tilde{P}_{\Phi}(\xi)\Big]}\\ &{}&{\quad\quad\quad\quad\le\underset{Q\in L o c a l\lfloor\mathcal{U}\rfloor}{\operatorname*{max}}E_{\xi\sim Q}\Big[\log\tilde{P}_{\Phi}(\xi)\Big],}\end{array}
$$ 

which is equal to the value of MAP-LP . Note that we are abusing notation in the expectation used in the last expression, $Q\ \in\ L o c a l[\mathcal{U}]$ is not a distribution but a set of pseudo- marginals. However, because $\log{\tilde{P}_{\Phi}(\xi)}$ factors according to the structure of the clusters in the pseudo-marginals, we can use a set of pseudo-marginals to compute the expectation. 

Next, we note that for any set of functions $f_{r}(C_{r})$ whose scopes align with the clusters $C_{r}$ , we have that: 

$$
\begin{array}{r l r}{\underset{Q\in L o c a l[\mathcal{U}]}{\operatorname*{max}}E_{C_{r}\sim Q}\Bigg[\underset{r}{\sum}f_{r}(C_{r})\Bigg]=\underset{Q\in L o c a l[\mathcal{U}]}{\operatorname*{max}}\underset{r}{\sum}E_{C_{r}\sim Q}[f_{r}(C_{r})]}&\\ &{\leq\underset{r}{\sum}\underset{C_{r}}{\operatorname*{max}}f_{r}(C_{r}),}&\end{array}
$$ 

because an expectation is smaller than the max. 

We can now apply this derivation to the reformulation of $\tilde{P}_{\Phi}$ that we get from the reparame- terization: 

$$
\begin{array}{r}{\underset{I[\mathcal{U}]}{\mathrm{x}}\,\pmb{E}_{Q}\Big[\log\tilde{P}_{\Phi}(\xi)\Big]=\underset{Q\in L o c a l\mathcal{U}}{\operatorname*{max}}\pmb{E}_{Q}\bigg[\begin{array}{l}{\sum_{r}\nu_{r}\log(\beta_{r}(c_{r}))+\sum_{i}\nu_{i}\log\beta_{i}(x_{i})}\\ {+\sum_{i,r\,:\,X_{i}\in C_{r}}\nu_{r,i}\left(\log\beta_{r}(c_{r})-\log\beta_{i}(x_{i})\right)}\end{array}\bigg].}\end{array}
$$ 

From the preceding derivation, it follows that: 

$$
\begin{array}{r l r}{\lefteqn{\le\sum_{r}\operatorname*{max}_{\boldsymbol{c}_{r}}\nu_{r}\log(\beta_{r}(\boldsymbol{c}_{r}))+\sum_{i}\operatorname*{max}_{x_{i}}\nu_{i}\log\beta_{i}(x_{i})}}\\ &{}&{+\sum_{i,r\,:\,X_{i}\in\boldsymbol{C}_{r}}\operatorname*{max}_{\boldsymbol{c}_{r};x_{i}=\boldsymbol{c}_{r}\langle X_{i}\rangle}\nu_{r,i}\left(\log\beta_{r}(\boldsymbol{c}_{r})-\log\beta_{i}(x_{i})\right).}\end{array}
$$ 

And from the positivity of the counting numbers, we get 

$$
\begin{array}{r l r}{\lefteqn{=\sum_{r}\nu_{r}\operatorname*{max}_{\boldsymbol{c}_{r}}\log(\beta_{r}(\boldsymbol{c}_{r}))+\sum_{i}\nu_{i}\operatorname*{max}_{x_{i}}\log\beta_{i}(\boldsymbol{x}_{i})}}\\ &{}&{+\sum_{i,r\,:\,X_{i}\in C_{r}}\nu_{r,i}\operatorname*{max}_{\boldsymbol{c}_{r};x_{i}=\boldsymbol{c}_{r}\langle X_{i}\rangle}\left(\log\beta_{r}(\boldsymbol{c}_{r})-\log\beta_{i}(\boldsymbol{x}_{i})\right).}\end{array}
$$ 

Now, due to lemma 13.1 (reformulated for log-factors), we have that $\xi^{*}$ optimizes each of the maximization expressions, so that we conclude: 

$$
\begin{array}{r l}{\lefteqn{=\sum_{r}\nu_{r}\log(\beta_{r}(\pmb{c}_{r}^{*}))+\sum_{i}\nu_{i}\log\beta_{i}(x_{i}^{*})}}\\ &{+\sum_{i,r\ :\ X_{i}\in\pmb{C}_{r}}\nu_{r,i}\left(\log\beta_{r}(\pmb{c}_{r}^{*})-\log\beta_{i}(x_{i}^{*})\right)}\\ &{=\log\Tilde{P}_{\Phi}(\xi^{*}).}\end{array}
$$ 

Putting this conclusion together with equation (13.32), we obtain: 

$$
\begin{array}{r l}&{\underset{\xi}{\operatorname*{max}}\log\tilde{P}_{\Phi}(\xi)\leq\underset{Q\in L o c a l\mathcal{U}]}{\operatorname*{max}}E_{\xi\sim Q}\Bigl[\log\tilde{P}_{\Phi}(\xi)\Bigr]}\\ &{\qquad\qquad\qquad\leq\log\tilde{P}_{\Phi}(\xi^{*}).}\end{array}
$$ 

Because the right-hand side is clearly $\leq$ the left-hand side, the entire inequality holds as an equality, proving that 

$$
\operatorname*{max}_{\xi}\log\tilde{P}_{\Phi}(\xi)=\operatorname*{max}_{Q\in L o c a l[\mathcal{U}]}E_{\xi\sim Q}\Bigl[\log\tilde{P}_{\Phi}(\xi)\Bigr],
$$ 

that is, the value of the integer program optimization is the same as that of the relaxed LP. 

This last fact has important repercussions. In particular, it shows that convex max-product BP can be decoded only if the LP is tight; otherwise, there is no locally optimal joint assignment, and no decoding is possible. It follows that convex max-product BP provides provably useful results only in cases where MAP-LP itself provides the optimal answer to the MAP problem. We note that a similar conclusion does not hold for nonconvex variants such as those based on the standard Bethe counting numbers; in particular, standard max-product BP is not an upper bound to MAP-LP , and therefore it can return solutions in the interior of the polytope of MAP-LP . As a consequence, it may be decodable 

even when the LP is not tight; in that case, the returned joint assignment may be the MAP, or it may be a suboptimal assignment. 

This result leaves several intriguing open questions. First, we note that this result shows a connection between the results of max-product and the LP only when the LP is tight. It is an open question whether we can show a general connection between the max-product beliefs and the dual of the original LP. A second question is whether we can construct better techniques that directly solve the LP or its dual; indeed, recent work (see section 13.9) explores a range of other techniques for this task. A third question is whether this technique provides a useful heuristic: Even if the re parameter iz ation we derive does not have a locally consistent joint assignment, we can still use it to construct an assignment using various heuristic methods, such as selecting for each variable $X_{i}$ the assignment $x_{i}^{*}=\arg\operatorname*{max}_{x_{i}}\beta_{i}(x_{i})$ . While there are no guarantees about this solution, it may still work well in practice. 

# 13.6 Using Graph Cuts for MAP 

In this section, we discuss the important class of metric and semi-metric MRFs, which we deﬁned in box 4.D. This class has received considerable attention, largely owing to its importance in computer- vision applications. We show how this class of networks, although possibly very densely connected, can admit an optimal or close-to-optimal solution, by virtue of structure in the potentials. 

# 13.6.1 Inference Using Graph Cuts 

The basic graph construction is deﬁned for pairwise MRFs consisting solely of binary-valued variables $(\mathcal{V}=\{0,1\})$ ). Although this case has restricted applicability, it forms the basis for the general case. As we now show, the MAP problem for a certain class of binary-valued MRFs  can be solved optimally using a very simple and efcient graph-cut algorithm. Perhaps the most surprising aspect of this reduction is that this algorithm is guaranteed to return the optimal solution in polynomial time, regardless of the structural complexity of the underlying graph. This result stands in contrast to most of the other results presented in this book, where polynomial-time solutions were obtainable only for graphs of low tree width. Equally noteworthy is the fact that a similar result does not hold for sum-product computations over this class of graphs; thus, we have an example of a class of networks where sum-product inference and MAP inference have very diferent computational properties. 

graph cut 

We ﬁrst deﬁne the min-cut problem for a graph, and then show how the MAP problem can be reduced to it. The min- ut pro lem is deﬁned by a set of vertices $\mathcal{Z}$ , plus two distinguished nodes generally known as s and t . We have a set of directed edges E over ${\mathcal{Z}}\cup\{s,t\}$ , where each edge $\left(z_{1},z_{2}\right)\,\in\,\mathcal{E}$ is ed with a ative cost $c o s t(z_{1},z_{2})$ . A graph cut is a disjoint partition of Z into Z $\mathcal{Z}_{s}\cup\mathcal{Z}_{t}$ ∪Z such that $s\in\mathcal{Z}_{s}$ ∈Z and $t\in\mathcal{Z}_{t}$ . The cost of the cut is: 

$$
c o s t(\mathcal{Z}_{s},\mathcal{Z}_{t})=\sum_{z_{1}\in\mathcal{Z}_{s},z_{2}\in\mathcal{Z}_{t}}c o s t(z_{1},z_{2}).
$$ 

In wor the cost is the total sum of the edges ross from the $\mathcal{Z}_{s}$ side of the partition to the Z side. The minimal cut is the partition Z $\mathcal{Z}_{s},\mathcal{Z}_{t}$ Z that achieves the minimal cost. While presenting a min-cut algorithm is outside the scope of this book, such algorithms are standard, have polynomial-time complexity, and are very fast in practice. 

How do we reduce the MAP problem to one of computing cuts on a graph? Intuitively, we need to design our graph so that a cut corresponds to an assignment to $\mathcal{X}$ , and its cost to the value of the assignment. The construction follows straightforwardly from this intuition. Our vertices (other than $s,t)$ represent the variables in our MRF. We use the $s$ side of the cut to represent the label 0 , and th $t$ side to represent the label 1 . Thus, we map a cut $\mathcal{C}=\left(\mathcal{Z}_{s},\mathcal{Z}_{t}\right)$ to the following assignment $\xi^{\mathcal{C}}$ : 

$$
x_{i}^{\mathcal{C}}=0\quad\mathrm{~if~and~only~if~}\quad z_{i}\in\mathcal{Z}_{s}.
$$ 

We begin by demonstrating the construction on the simple case of the generalized Ising model of equation (4.6). Note that energy functions are invariant to additive changes in all of the components, since these just serve to move all entries in $E(x_{1},\dots,x_{n})$ by some additive factor, leaving their relative order invariant. Thus, we can assume, without loss of generality, that all components of the energy function are nonnegative. Moreover, we can assume that, for every node $i$ , either $\epsilon_{i}(1)=0$ or $\epsilon_{i}(0)=0$ . We now construct the graph as follows: 

• If $\epsilon_{i}(1)=0$ , we introduce an edge $z_{i}\rightarrow t$ , with cost $\epsilon_{i}(0)$ .

 • If $\epsilon_{i}(0)=0$ , we introduce an edge $s\rightarrow z_{i}$ , with cost $\epsilon_{i}(1)$ .

 • For each pair of variables $X_{i},X_{j}$ that are connected by an edge in the MRF, we introduce both an edge $(z_{i},z_{j})$ and the edge $(z_{j},z_{i})$ , both with cost $\lambda_{i,j}\geq0$ . 

Now, cons er the cost of a cut $\left(\mathcal{Z}_{s},\mathcal{Z}_{t}\right)$ . If $z_{i}~\in~\mathcal{Z}_{s}$ , then $X_{i}$ is assigned a value of 0 . In this case, z and t are on opposite sides of the cut, and so we will get a contribution of $\epsilon_{i}(0)$ to the cost of the cut; this contribution is precisely the $X_{i}$ node energy of the assignment $X_{i}=0$ , as we would want. alogous argument applies when $z_{i}\in\mathcal{Z}_{t}$ . We now onsider the edge potential. The edge $(z_{i},z_{j})$ only makes a contribution to the cut if we place $z_{i}$ and $z_{j}$ on opposite sides of the cut; in this case, the contribution is $\lambda_{i,j}$ . Conversely, the pair $X_{i},X_{j}$ makes a contribution of $\lambda_{i,j}$ to the energy function if $X_{i}\neq X_{j}$ , and otherwise it contributes 0 . Thus, the contribution of the edge to the cut is precisely the same as the contribution of the node pair to the energy function. Overall, we have shown that the cost of the cut is precisely the same as the energy of the corresponding assignment. Thus, the min-cut algorithm is guaranteed to ﬁnd the assignment to $\mathcal{X}$ that minimizes the energy function, that is, $\xi^{m a p}$ . 

Consider a simple example where we have four variables $X_{1},X_{2},X_{3},X_{4}$ connected in a loop with the edges $X_{1}{-}X_{2}$ , $X_{2}{-}X_{3}$ , $X_{3}{-}X_{4}$ , $X_{1}{-}X_{4}$ . Assume we have the following energies, where we list only components that are nonzero: 

$$
\begin{array}{l l l l}{{\epsilon_{1}(0)=7}}&{{\epsilon_{2}(1)=2}}&{{\epsilon_{3}(1)=1}}&{{\epsilon_{4}(1)=6}}\\ {{\lambda_{1,2}=6}}&{{\lambda_{2,3}=6}}&{{\lambda_{3,4}=2}}&{{\lambda_{1,4}=1.}}\end{array}
$$ 

The graph construction and the minimum cut for this example are shown in ﬁgure 13.5. 

Going by the node potentials alone, the optimal assignment is $X_{1}=1$ , $X_{2}=0$ , $X_{3}=0$ , $X_{4}=$ 0 . However, we also have interaction potentials that encourage agreement between neighboring nodes. In particular, there are fairly strong potentials that induce $X_{1}=X_{2}$ and $X_{2}=X_{3}$ . Thus, the node-optimal assignment achieves a penalty of 7 from the contributions of $\lambda_{1,2}$ and $\lambda_{1,4}$ . 

![](images/da07f053ac4c9b485873855b0f3e1a4cf3005a624ac81a82df6dbca07773e44f.jpg) 
Figure 13.5 Example graph construction for applying min-cut to the binary MAP problem, based on example 13.17. Numbers on the edges represent their weight. The cut is represented by the set of nodes in $\mathcal{Z}_{t}$ . Dashed edges are ones that participate in the cut; note that only one of the two directions of a bidirected edge contributes to the weight of the cut, which is 6 in this example. 

Conversely, the assignment where $X_{2}$ and $X_{3}$ agree with $X_{1}$ gets a penalty of only 6 from the $X_{2}$ and $X_{3}$ node contributions and from the weaker edge potentials $\lambda_{3,4}$ and $\lambda_{1,4}$ . Thus, the overall MAP assignment has $X_{1}=1$ , $X_{2}=1$ , $X_{3}=1$ , $X_{4}=0$ . 

As we mentioned, the MAP problem in such graphs reduces to a minimum cut problem regardless of the network connectivity. Thus, this approach allows us to ﬁnd MAP solution for a class of MRFs for which probability computations are intractable. 

We can easily extend this construction beyond the generalized Ising model: 

Deﬁnition 13.5 

submodular energy function 

$$
\epsilon_{i,j}(1,1)+\epsilon_{i,j}(0,0)\leq\epsilon_{i,j}(1,0)+\epsilon_{i,j}(0,1).
$$ 

The graph construction for submodular energies, which is shown in detail in algorithm 13.4, is a little more elaborate. It ﬁrst normalizes each edge potential by subtracting $\epsilon_{i,j}(0,0)$ from all entries; this operation subtracts a constant amount from the energies of all assignments, corresponding to a constant multiple in probability space, which only changes the (in this case irrelevant) partition function. It then moves as much mass as possible to the individual node potentials for $i$ and $j$ . These steps leave a single pairwise term that deﬁnes an energy only for the assignment $v_{i}=0,v_{j}=1$ : 

$$
\epsilon_{i,j}^{\prime}(0,1)=\epsilon_{i,j}(1,0)+\epsilon_{i,j}(0,1)-\epsilon_{i,j}(0,0)-\epsilon_{i,j}(1,1).
$$ 

Algorithm 13.4 Graph-cut algorithm for MAP in pairwise binary MRFs with submodular potentials 

![](images/2dcb5017869cdbc3db0fe57f8e57f2ad8c4eb3af2750736346c0000dd4cd2236.jpg) 

Because of submodularity, this term satisﬁes $\epsilon_{i,j}^{\prime}(0,1)\geq0$ . The algorithm executes this trans- formation for every pairwise potential $i,j$ . The resulting energy function can easily be converted into a graph using essentially the same construction that we used earlier; the only slight dif- ference is that for our new energy function $\epsilon_{i,j}^{\prime}(v_{i},v_{j})$ we need to introduce only the edge $(z_{i},z_{j})$ , with cost $\epsilon_{i,j}^{\prime}(0,1)$ ; we do not introduce the opposite edge $(z_{j},z_{i})$ . We now use the same mapping between s-t cuts in the graph and assignment to the variables $X_{1},\dots,X_{n}$ . lt to verify that the cost of an s-t cut $\mathcal{C}$ in the resulting graph is precisely $E(\xi^{\mathcal{C}})+\mathrm{Const}$ (see exercise 13.14). Thus, ﬁnding the minimum cut in this graph directly gives us the cost-minimizing assignment $\xi^{m a p}$ . 

Note that for pairwise submodular energy, there is an LP relaxation of the MAP integer optimization, which is tight. Thus, this result provides another example where having a tight LP relaxation allows us to ﬁnd the optimal MAP assignment. 

# 13.6.2 Nonbinary Variables 

In the case of nonbinary variables, we can no longer use a graph construction to solve the MRF optimally. Indeed, the problem of optimizing the energy function, even if it is submodular, is $\mathcal{N P}$ -hard in this case. Here, a very useful technique is to take greedy hill-climbing steps, but where each step involves a globally optimal solution to a simpliﬁed problem. Two types of steps have been utilized extensively: alpha-expansion and alpha-beta swap . As we will show, under appropriate conditions on the energy function, both the alpha-expansion step and the alpha-beta-swap steps can be performed optimally by applying the min-cut procedure to an appropriately constructed MRF. Thus, the search procedure can take a global step in the space. 

alpha-expansion 

restricted energy function 

The alpha-expansion considers a particular value $v$ ; the step simultaneously considers all of the variables $X_{i}$ in the MRF, and allows each of them to take one of two values: it can keep its current value $x_{i}$ , or change its value to $v$ . Thus, the step expands the set of variables that take the label $v$ ; the label $v$ is often denoted $\alpha$ in the literature; hence the name alpha-expansion. 

The alpha-expansion algorithm is shown in algorithm 13.5. It consists of repeated applications of alpha-expansion steps, for diferent labels $v$ . Each alpha-expansion step is deﬁned relative to our current assignment $_{_{x}}$ and a target label $v$ . Our goal is to select, for each variable $X_{i}$ whose current label $x_{i}$ is other than $v$ , whether in the new assignment $\scriptstyle{\boldsymbol{x}}^{\prime}$ its new label will remain $x_{i}$ or move to $v$ . We do so using a new MRF that has binary variables $T_{i}$ for each variable $X_{i}$ ; we then deﬁne a new assignment $\scriptstyle{\boldsymbol{x}}^{\prime}$ so that $x_{i}^{\prime}=x_{i}$ if $T_{i}=t_{i}^{0}$ , and $x_{i}^{\prime}=v$ if $T_{i}=t_{i}^{1}$ . We deﬁne a new restricted energy function $E^{\prime}$ using the following set of potentials: 

It is straightforward to see that for any assignment $t$ , $E^{\prime}(t)=E(x^{\prime})$ . Thus, ﬁnding the optimal $t$ corresponds to ﬁnding the optimal $\scriptstyle{\boldsymbol{x}}^{\prime}$ in the restricted space of $v$ -expansions of $_{_{x}}$ . 

In order to optimize $t$ using graph cuts, the new energy $E^{\prime}$ needs to be submodular, as in equation (13.33). Plugging in the deﬁnition of the new potentials, we get the following constraint: 

$$
\epsilon_{i,j}(x_{i},x_{j})+\epsilon_{i,j}(v,v)\leq\epsilon_{i,j}(x_{i},v)+\epsilon_{i,j}(v,x_{j}).
$$ 

Now, if we have an MRF deﬁned by some distance function $\mu$ , then $\epsilon_{i,j}(v,v)=0$ by reﬂexivity, and the remaining inequality is a direct consequence of the triangle inequality. Thus, we can apply the alpha-expansion procedure to any metric MRF. 

alpha-beta swap 

The second type of step is the alpha-beta swap . Here, we consider two labels: $v_{1}$ and $v_{2}$ . The step allows each variable whose current label is $v_{1}$ to keep its value or change it to $v_{2}$ , and conversely for variables currently labeled $v_{2}$ . Like the alpha-expansion step, the alpha-beta swap over a given assignment $_{_{x}}$ can be deﬁned easily by constructing a new energy function, over which min-cut can be performed. The details are left as an exercise (exercise 13.15). We note that the alpha-beta-swap operation requires only that the energy function be a semimetric (that is, the triangle inequality is not required). 

These two steps allow us to use the min-cut procedure as a subroutine in solving the MAP problem in metric or semimetric MRFs with nonbinary variables. 

![](images/849df35fa382a6cb9adafe348fbf5a6d7d5ec3509d5fce01895bb3472678787c.jpg) 

Box 13.B — Case Study: Energy Minimization in Computer Vision. Over the past few years, MRFs have become a standard tool for addressing a range of low-level vision tasks, some of which we reviewed in box 4.B. As we discussed, the pairwise potentials in these models are often aimed at penalizing discrepancies between the values of adjacent pixels, and hence they often naturally satisfy the submodularity assumption that are necessary for the application of graph cut methods. Also very popular is the TRW-S variant of the convex belief propagation algorithms, described in box 13.A. Standard belief propagation has also been used in multiple applications. 

stereo reconstruction 

Vision problems pose some signiﬁcant challenges. Although the grid structures associated with images are not dense, they are very large, and they contain many tight loops, which can pose difculties for convergence of the message passing algorithm. Moreover, in some tasks, such as stereo reconstruction , the value space of the variables is a discretization of a continuous space, and therefore many values are required to get a reasonable approximation. As a consequence, the representation of the pairwise potentials can get very large, leading to memory problems. 

A number of fairly comprehensive empirical studies have been done comparing the various meth- ods on a suite of computer-vision benchmark problems. By and large, it seems that for the grid- structured networks that we described, graph-cut methods with the alpha-expansion step and TRW- S are fairly comparable, with the graph-cut methods dominating in running time; both signiﬁcantly 

![](images/45851dba3e29bcd34027d2777346efaf7c238b71a647aab51f335e0996ec0485.jpg) 
Figure 13.B.1 — MAP inference for stereo reconstruction The top row contains a pair of stereo im- ages for a problem known as Teddy and the target output (darker pixels denote a larger $z$ value); the images are taken from Scharstein and Szeliski (2003). The bottom row shows the best energy obtained as a function of time by several diferent MAP algorithms:max-product BP, the TRW variant of convex BP, min-cut with alpha-expansion, and min-cut with alpha-beta swap. The left image is for Teddy , and the right is for a diferent stereo problem called Tsukuba . 

outperform the other methods. Figure 13.B.1 shows some sample results on stereo-reconstruction problems; here, the energies are close to submodular, allowing the application of a range of diferent methods. 

The fact that convex BP is solving the dual problem to the relaxed LP allows it to provide a lower bound on the energy of the true MAP assignment. Moreover, as we discussed, it can sometimes provide optimality guarantees on the inferred solution. Thus, it is sometimes possible to compare the results of these methods to the true global optimum of the energy function. Somewhat surprisingly, it appears that both methods come very close to achieving optimal energies on a large fraction of these benchmark problems, suggesting that the problem of energy minimization for these MRFs is essentially solved. 

In contrast to this optimistic viewpoint is the observation that the energy minimizing conﬁgura- tion is often signiﬁcantly worse than the “target” assignment (for example, the true depth disparity in a stereo reconstruction problem). In other words, the ground truth often has a worse energy (lower probability) than the assignment that optimizes the energy function. This ﬁnding suggests that a key problem is that of designing better energy functions, which better capture the structure of our target assignments. This topic has been the focus of much recent work. In many cases, the resulting energies involve nonlocal interactions between the pixels, and are therefore signiﬁcantly more complex. Some evidence suggests that as the graph becomes more dense and less local, belief propagation methods start to degrade. Conversely, as the potentials become less submodular, the graph-cut methods become less applicable. Thus, the design of new energy-minimization methods that are applicable to these richer energy functions is a topic of signiﬁcant current interest. 

# 13.7 Local Search Algorithms $\star$ 

A ﬁnal class of methods that have been applied to MAP and marginal MAP queries are methods that search over the space of assignments. The task of searching for a high-weight (or low-cost) assignment of values to a set of variables is a central one in many applications, and it has received attention in a number of communities. Methods for addressing this task come in many ﬂavors. 

systematic search 

branch-and- bound 

local search 

search space 

beam search 

marginal MAP 

Some of those methods are systematic : They search the space so as to ensure that assign- ments that are not considered are not optimal, and thereby guarantee an optimal solution. Such methods generally search over the space of partial assignments, starting with the empty assign- ment, and assigning variables one at a time. One such method, known as branch-and-bound , is described in appendix A.4.3. 

Other methods are nonsystematic, and they come without performance guarantees. Here, many of the methods search over the space of full assignments, usually by making local changes to the assignment so as to improve its score. These local search methods generally provide no guarantees of optimality. Appendix A.4.2 describes some of the techniques that are most commonly applied in practice. 

The application of search techniques to the MAP problem is a fairly straightforward process: The search space is deﬁned by the possible assignments $\xi$ to $\mathcal{X}$ , and $\operatorname{log}\tilde{P}(\xi)$ is the score; we omit details. Although generally less powerful than the methods we described earlier, these methods do have some advantages. For example, the beam search method of appendix A.4.2 provides a useful alternative in cases where the complete model is too large to ﬁt into memory; see exercise 15.10. We also note that branch-and-bound does provide a simple method for ﬁnding the $K$ most likely assignment; see exercise 13.18. This algorithm requires at least as much computation time as the clique tree–based algorithm, but signiﬁcantly less space. 

These methods have much greater applicability in the context of marginal MAP problem, where most other methods are not (currently) applicable. Here, we search over the space of assignments $_{_y}$ to the max-variables $Y$ . Here, we conduct the search so that we can ﬁx some or all of the max-variables to have a concrete assignment. As we show, this allows us to remove the constraint on the variable elimination ordering, allowing an unrestricted ordering to be used. 

Here, we search over the space of assignments $_{_y}$ for those that maximize 

$$
\mathrm{score}({\pmb y})=\sum_{W}\tilde{P}_{\Phi}({\pmb y},W).
$$ 

search operator 

tabu search 

Several search procedures are appropriate in this setting. In one approach, we use some local search algorithm, as in appendix A.4.2. As usual in local search, the algorithm begins with some complete assignment $\pmb{y}_{0}$ to $Y$ . We then consider applying diferent search operators to $_{y;}$ ; for each such operator $O$ , we produce a new partial assignment $\pmb{y}^{\prime}\,=\,o(\pmb{y})$ as a successor to the current state, which is evaluated by computing $\operatorname{score}(y^{\prime})$ . Importantly, since we now have a complete assignment to the max-variables ${\pmb y}^{\prime}\,=\,o({\pmb y})$ , the resulting score is simply a sum-product expression, and it can be computed by standard sum-product elimination of the variables $W$ , with no constraints on the variable ordering. The tree-width in these cases is usually much smaller than in the constrained case; for example, in the network of ﬁgure 13.2, the network for a ﬁxed assignment $y^{\prime}$ is simply a chain, and the computation of the score can therefore be done in time linear in $n$ . 

While we can consider a variety of search operators, the most straightforward are operators of the form $d o(Y_{i}=y_{i}^{j})$ , which set a variable $Y_{i}\in Y$ to the value $y_{i}^{j}$ . We can now apply any greedy local-search algorithm, such as those described in appendix A.4.2. Empirical evidence suggests that greedy hill climbing with tabu search performs very well on this task, especially if initialized intelligently. In particular, one simple yet good heuristic is to calibrate the clique tree with no assignment to the max-variables; we then compute, for each $Y_{i}$ its unnormalized probability $\tilde{P}_{\Phi}(\bar{Y_{i}})$ (which can be extracted from any clique containing $Y_{i}$ ), and initialize $y_{i}=$ arg $\operatorname*{max}_{Y_{i}}\tilde{P}_{\Phi}(Y_{i})$ . 

While simple in principle, a naive implementation of this algorithm can be quite costly. Let $k=|Y|$ e for simplicity that $|V a l(Y_{i})|=d$ for all $Y_{i}\in Y$ . Each step of the search requires $k\times(d-1)$ × − evaluations of score , each of which involves a run of probabilistic inference over the network. Even for simple networks, this cost can often be prohibitive. 

dynamic programming 

Fortunately, we can greatly improve the computational performance of this algorithm using the same type of dynamic programming tricks that we used in other parts of this book. Most important is the observation that we can compute the score of all of the operators in our search using a single run of clique tree propagation, in the clique tree corresponding to an unconstrained elimination ordering. Let $\mathcal{T}$ an unconstrained clique tree over $\mathcal{X}=Y\cup W$ , itialized with the original potentials of $\tilde{P}_{\Phi}$ . Let $_{_y}$ be our current assignment to Y . For any $Y_{i}$ , let $Y_{-i}=Y-\{Y_{i}\}$ and $\pmb{y}_{-i}$ be the assignment in $_{_y}$ to $Y_{-i}$ . We can use the algorithm developed in exercise 10.12 to compute $\tilde{P}_{\Phi}(Y_{i},\pmb{y}_{-i})$ for every $Y_{i}\in Y$ . Recall that − this algorithm requires only a single clique tree calibration that computes all of the messages; with those messages, each clique that contains a variable $Y_{i}$ can locally compute $\tilde{P}_{\Phi}(Y_{i},\bar{\pmb{y}_{-i}})$ − in time that is linear in the size of the clique. This idea reduces the cost of each step by a factor of $O(k d)$ , an enormous saving. For example, in the network of ﬁgure 13.2, we can use a clique tree whose cliques are of the form $X_{i},Y_{i+1},X_{i+1}$ , with sepsets $X_{i}$ between cliques. Here, the maximum clique size is 3, and the computation requires time linear in $k$ . 

We can also use search methods other than local hill climbing. One alternative is to utilize a systematic search procedure that is guaranteed to ﬁnd the exact solution. Particularly well suited to this task is the branch-and-bound search described in appendix A.4.3. Recall that branch-and- bound systematically explores partial assignments to the variables $Y$ ; it only discards a partial assignment $\scriptstyle y^{\prime}$ if it already has a complete solution $_{_y}$ that is provably better than the best possible solution that one can obtain by extending $\pmb{y}^{\prime}$ to a complete assignment. This pruning relies on having a way of estimating the upper bound on a partial assignment $\pmb{y}^{\prime}$ . In our setting, such an upper bound can be obtained by using variable elimination, ignoring the constraint on the ordering whereby all summations occur before all maximizations. An algorithm based on these ideas is developed further in exercise 13.20. 

# 13.8 Summary 

In this chapter, we have considered the problem of ﬁnding the MAP assignment and described a number of methods for addressing it. The MAP problem has a broad range of applications, in computer vision, computational biology, speech recognition, and more. Although the use of MAP inference loses us the ability to measure our conﬁdence (or uncertainty) in our conclusions, there are good reasons nevertheless for using a single MAP assignment rather than using the marginal probabilities of the diferent variables. One is the preference for obtaining a single coherent joint assignment, whereas a set of individual marginals may not make sense as a whole. The second is that there are inference methods that are applicable to the MAP problem and not to the task of computing probabilities, so that the former may be tractable even when the latter is not. 

The methods we discussed fall into several major categories. The variable elimination method is very similar to the approaches we discussed in chapter 9, where we replace summation with maximization. The only slight extension is the traceback procedure, which allows us to identify the MAP assignment once the variable elimination process is complete. 

Although one can view the max-product clique tree algorithm as a dynamic programming extension of variable elimination, it is more illuminating to view it as a method for reparame- terizing the distribution to produce a max-calibrated set of beliefs. With this re parameter iz ation, we can convert the global optimization problem — ﬁnding a coherent joint assignment — to a local optimization problem — ﬁnding a set of local assignments each of which optimizes its (calibrated) belief. Importantly, the same view also characterizes the cluster-graph-based belief propagation algorithms. The properties of max-calibrated beliefs allow us to prove strong (local or global) optimality properties for the results of these diferent message passing algorithms. In particular, for message passing with convex counting numbers we can sometimes construct an assignment that is the true MAP. 

A seemingly very diferent class of methods is based on considering the integer program that directly encodes our optimization problem, and then constructing a relaxation as a linear program. Somewhat surprisingly, there is a deep connection between the convex max-product BP algorithm and the linear program relaxation. In particular, the solution to the dual problem of this LP is a ﬁxed point of any convex max-product BP algorithm; thus, these algorithms can be viewed as a computational method for solving this dual problem. The use of these message passing methods ofers a trade-of: they are space-efcient and easy to implement, but they may not converge to the optimum of the dual problem. 

Importantly, the ﬁxed point of a convex BP algorithm can be used to provide a MAP assign- ment only if the MAP LP is a tight relaxation of the integer MAP optimization problem. Thus, it appears that the LP relaxation is the fundamental construct in the application and analysis of the convex BP algorithms. This conclusion motivates two recent lines of work in MAP inference: One line attempts to construct tighter relaxations to the MAP optimization problem; importantly, since the same relaxation is used for both the free energy optimization in section 11.3.6 and for the MAP relaxations, progress made on improved relaxations for one task is directly useful for the other. The second line of work attempts to solve the LP or its dual using techniques other than message passing. While the problems are convex and hence can in principle be solved directly using standard techniques, the size of the problems makes the cost of this sim- ple approach prohibitive in many practical applications. However, the rich and well-developed theory of convex optimization provides a wealth of potential tools, and some are already being adapted to take advantage of the structure of the MAP problem. It is likely that eventually these algorithms will replace convex BP as the method of choice for solving the dual. See section 13.9 for some references along those lines. 

A diferent class of algorithms is based on reducing the MAP problem in pairwise, binary MRFs to one of ﬁnding the minimum cut in a graph. Although seemingly restrictive, this procedure forms a basic building block for solving a much broader class of MRFs. These methods provide an efective solution method only for MRFs where the potentials satisfy (or almost satisfy) the submodularity property. Conversely, their complexity depends fairly little on the complexity of the graph (the number of edges); as such, they allow certain MRFs to be solved efciently that are not tractable to any other method. Empirically, for energies that are close to submodular, 

passing. We note that in this case, also, there is an interesting connection to the linear programming view: The case that admits an optimal solution using minimum cut (pairwise, binary MRFs whose potentials are submodular) are also ones where there is a tight LP relaxation to the MAP problem. Thus, one can view the minimum-cut algorithm as a specialized method for exploiting special structure in the LP for solving it more efciently. 

In contrast to the huge volume of work on the MAP problem, relatively little work has been done on the marginal MAP problem. This lack is, in some sense, not surprising: the intrinsic difculty of the problem is daunting and eliminates any hope of a general-purpose solution. Nevertheless, it would be interesting to see whether some of the recent algorithmic techniques developed for the MAP problem could be extended to apply to the marginal MAP case, leading to new solutions to the marginal MAP problem for at least a subset of MRFs. 

# 13.9 Relevant Literature 

We begin by reminding the reader, before tackling the literature, that there is a conﬂict of terminologies here: In some papers, the MAP problem is called MPE, whereas the marginal MAP problem is called simply MAP. 

The problem of ﬁnding the MAP assignment in a probabilistic model was ﬁrst addressed by Viterbi (1967), in the context of hidden Markov models; this algorithm came to be called the Viterbi algorithm . A generalization to other singly connected Bayesian networks was ﬁrst proposed by Pearl (1988). The clique tree algorithm for this problem was described by Lauritzen and Spiegelhalter (1988). Shimony (1994) showed that the MAP problem is $\mathcal{N P}$ -hard in general networks. 

The problem of ﬁnding a MAP assignment to an MRF is equivalent (up to a negative-logarithm energy minimization 

iterated conditional modes 

transformation) to the task of minimizing an energy function that is deﬁned as a sum of terms, each involving a small number of variables. There is a considerably body of literature on the energy minimization problem, in both continuous and discrete space. Extensive work on energy minimization in MRFs has been done in the computer-vision community, where the locality of the spatial structure naturally deﬁnes a highly structured, often pairwise, MRF. 

Early work on the energy minimization task focused on hill-climbing techniques, such as simple coordinate ascent (known under the name iterated conditional modes (Besag 1986)) or simulated annealing (Barnard 1989). Many other search methods for the MAP problem have been proposed, including systematic approaches such as branch-and-bound (Santos 1991; Marinescu et al. 2003). 

The interest in max-product belief propagation on a loopy graph ﬁrst arose in the context of turbo-decoding. The ﬁrst general-purpose theoretical analysis for this approach was provided by Weiss and Freeman (2001b), who showed optimality properties of an assignment derived from an unambiguous set of beliefs reached at convergence of max-product BP. In particular, they showed that the assignment is the global optimum for networks involving only a single loop, and a strong local optimum (robust to changes in the assignments for a disjoint collection of single loops and trees) in general. 

Wainwright, Jaakkola, and Willsky (2004) ﬁrst proposed the view of message passing as repa- rameterizing the distribution so as to get the local beliefs to correspond to max-marginals. In subsequent work, Wainwright, Jaakkola, and Willsky (2005) developed the ﬁrst convexiﬁed message passing algorithm for the MAP problem. The algorithm, known as TRW, used an ap- proximation of the energy function based on a convex combination of trees. This paper was the ﬁrst to show lemma 13.1. It also showed that if a ﬁxed point of the TRW algorithm satisﬁed a stronger property than local optimality, it provided the MAP assignment. However, the TRW algorithm did not monotonically improve its objective, and indeed the algorithm was generally not convergent. Kolmogorov (2006) deﬁned TRW-S, a variant of TRW that passes message asyn- chronously, in a particular order. TRW-S is guaranteed to increase the objective monotonically, and hence is convergent. However, TRW-S is not guaranteed to converge to the global optimum of the dual objective, since it can get stuck in local optima. 

The connections between max-product BP, the lower-temperature limit of sum-product BP, and the linear programming relaxation were studied by Weiss, Yanover, and Meltzer (2007). They also showed results on the optimality of partial assignments extracted from unambiguous beliefs derived from convex BP ﬁxed points, extending earlier results of Kolmogorov and Wainwright (2005) for TRW-S. 

Max ﬂow techniques to solve submodular binary problems were originally developed by Boros, Hammer and collaborators (Hammer 1965; Boros and Hammer 2002). These techniques were popularized in the vision-MRF community by Greig, Porteous, and Seheult (1989), who were the ﬁrst to apply these techniques to images. Ishikawa (2003) extended this work to the nonbinary case, but assuming that the interaction between variables is convex. Boykov, Veksler, and Zabih (2001) were the ﬁrst to propose the alpha-expansion and alpha-beta swap steps, which allow the application of graph-cut methods to nonbinary problems; they also prove certain guarantees regarding the energy of the assignment found by these global steps, relative to the energy of the optimal MAP assignment. Kolmogorov and Zabih (2004) generalized and analyzed the graph constructions used in these methods, using techniques similar to those described by Boros and Hammer (2002). Recent work extends the scope of the MRFs to which these techniques can be applied, by introducing preprocessing steps that modify factors that do not satisfy the submodularity assumptions. For example, Rother et al. (2005) consider a method that truncates the potentials that do not conform to submodularity, as part of the iterative alpha-expansion algorithm, and they show that this approach, although not making optimal alpha-expansion steps, is still guaranteed to improve the objective at each iteration. We note that, for the case of metric potentials, belief propagation algorithms such as TRW also do well (see box 13.B); moreover, Felzenszwalb and Huttenlocher (2006) show how the computational cost of each message passing step can be reduced from $O(K^{2})$ to $O(K)$ , where $K$ is the total number of labels, reducing the cost of these algorithms in this setting. 

Szeliski et al. (2008) perform an in-depth empirical comparison of the performance of diferent methods on an ensemble of computer vision benchmark problems. Other empirical comparisons include Meltzer et al. (2005); Kolmogorov and Rother (2006); Yanover et al. (2006). 

The LP relaxation for MRFs was ﬁrst proposed by Schlesinger (1976), and then subsequently rediscovered independently by several researchers. Of these, the most relevant to our presen- tation is the work of Wainwright, Jaakkola, and Willsky (2005), who also established the ﬁrst connection between the LP dual and message passing algorithms, and proposed the TRW algo- rithm. Various extensions were subsequently proposed by various authors, based on diferent relaxations that require more complex convex optimization algorithms (Muramatsu and Suzuki 2003; Kumar et al. 2006; Ravikumar and Laferty 2006). Surprisingly, Kumar et al. (2007) subse- quently showed that the simple LP relaxation was tighter (that is, better) relaxation than all of those more sophisticated methods. 

A spate of recent works (Komodakis et al. 2007; Schlesinger and Giginyak 2007a,b; Sontag and Jaakkola 2007; Globerson and Jaakkola 2007b; Werner 2007; Sontag et al. 2008) make much deeper use of the linear programming relaxation of the MAP problem and of its dual. Globerson and Jaakkola (2007b); Komodakis et al. (2007) both demonstrate a message passing algorithm derived from this dual. The algorithm of Komodakis et al. is based on a dual decomposition algorithm, and is therefore guaranteed to converge to the optimum of the dual objective. Solving the LP relaxation or its dual does not generally give rise to the optimal MAP assignment. The work of Sontag and Jaakkola (2007); Sontag et al. (2008) shows how we can use the LP formulation to gradually add local constraints that hold for any set of pseudo-marginals deﬁned by a real distribution. These constraints make the optimization space a tighter relaxation of the marginal polytope and thereby lead to improved approximations. Sontag et al. present empirical results that show that a small number of constraints often sufce to deﬁne the optimal MAP assignment. 

Komodakis and colleagues 2005; 2007 also make use of LP duality in the context of graph cut methods, where it corresponds to the well-known duality between min-cut and max-ﬂow. They use this approach to derive primal-dual methods that speed up and extend the alpha-expansion method in several ways. 

Santos (1991, 1994) studied the question of ﬁnding the $M$ most likely assignments. He pre- sented an exact algorithm that uses the linear programming relaxation of the integer program, augmented with a branch-and-bound search that uses the LP as the bound. Nilsson (1998) pro- vides an alternative algorithm that uses propagation in clique trees. Yanover and Weiss (2003) subsequently generalized this algorithm for the case of loopy BP. 

Park and Darwiche extensively studied the marginal MAP problem, providing complexity re- sults (Park 2002; Park and Darwiche 2001), local search algorithms (Park and Darwiche 2004a) 

(including an efcient clique tree implementation), and a systematic branch-and-bound algo- rithm (Park and Darwiche 2003) based on the bound obtained by exchanging summation and maximization. 

The study of constraint satisfaction problems, and related problems such as Boolean satisﬁ- ability (see appendix A.3.4) is the focus of a thriving research community, and much progress has been made. One recent overview can be found in the textbook of Dechter (2003). There has been a growing interest recently in relating CSP methods to belief propagation techniques, most notably the survey propagation (for example, (Maneva et al. 2007)). 

# 13.10 Exercises 

# Exercise $13.1\star\star$ 

Prove theorem 13.1. 

# Exercise ${\bf13.2\star}$ 

Provide a structured variable elimination algorithm that solves the MAP task for networks with rule-based CPDs. 

a. Modify the algorithm Rule-Sum-Product-Eliminate-Var in algorithm 9.7 to deal with the max-product task. b. Show how we can perform the backward phase that constructs the most likely assignment to $\mathcal{X}$ . Make sure you describe which information needs to be stored in the forward phase so as to enable the backward phase. 

# Exercise 13.3 

Prove theorem 13.4. 

# Exercise 13.4 

Show how to adapt Traceback-MAP of algorithm 13.1 to ﬁnd the marginal MAP assignment, given the factors computed by a run of variable elimination for marginal MAP. 

# Exercise $13.5\star$ 

Consider the task of ﬁnding the second-most-likely assignment in a graphical model. Assume that we have produced a max-calibrated clique tree. 

a. Assume that the probabilistic model is unambiguous. Show how we can ﬁnd the second-best assign- ment using a single pass over the clique tree. b. Now answer the same question in the case where the probabilistic model is ambiguous. Your method should use only the precomputed max-marginals. 

# Exercise $13.6\star$ 

Now, consider the task of ﬁnding the third-most-likely assignment in a graphical model. Finding the third-most-probable assignment is more complicated, since it cannot be computed from max-marginals alone. 

a. We deﬁne the notion of constrained max-marginal : a max-marginal in a distribution that has some variable $X_{k}$ cons ined to take on only certain values. For $D_{k}\check{\subset}\,V a l(X_{k})$ , we deﬁne the constrained max-marginal of $X_{i}$ to be: $M a x M a r g_{\tilde{P}_{X_{k}\,\in\,D_{k}}}(X_{i}=x_{i})=\operatorname*{max}_{\{x:X_{i}=x_{i},X_{k}\in D_{k}\}}\tilde{P}({\pmb x}).$ 

Explain how to compute the preceding constrained max-marginals for all $i$ and $x_{i}$ using max-product message passing. 

b. Find the third-most-probable assignment by using two sets of constrained max-marginals. 

Exercise 13.7 Prove proposition 13.1. 

# Exercise 13.8 

Prove proposition 13.3. 

# Exercise 13.9 

Assume that max-product belief propagation converges to a set of calibrated beliefs $\beta_{i}(C_{i})$ . Assume that each belief is unambiguous, so that it has a unique maximizing assignment $c_{i}^{*}$ . Prove that all of these locally optimizing assignments are consistent with each other, in that if $X_{k}=x_{k}^{*}$ in one assignment $c_{i}^{*}$ , then $X_{k}=x_{k}^{*}$ in every other assignment $c_{j}^{*}$ for which $X_{k}\in C_{j}$ . 

# Exercise 13.10 

Construct an example of a max-product calibrated cluster graph in which (at least) some beliefs have two locally optimal assignments, such that one local assignment can be extended into a globally consistent joint assignment (across all beliefs), and the other cannot. 

# Exercise $13.11\star$ 

Consider a cluste h $\mathcal{U}$ hat contains only a si le loop, and assume that we have a of max-prod calibrated beliefs { $\tilde{\left\{\beta_{i}\right\}}$ } for U and an assignment $\xi^{\bar{\ast}}$ hat is locally optimal relative to { $\{\beta_{i}\}$ } . Prove that $\xi^{*}$ is the MAP assignment relative to the distribution P $P_{\mathcal{U}}$ . (Hint: Use lemma 13.1 and a proof similar to that of U theorem 13.6.) 

# Exercise 13.12 

Using exercise 13.11, complete the proof of theorem 13.6 First prove e result for sets $Z$ for which $\mathcal{U}_{Z}$ contains only a single loop. Then prove the result for any Z for which U $\mathcal{U}_{Z}$ is a combination of disconnected trees and loops. 

# Exercise 13.13 

Prove proposition 13.4. 

# Exercise 13.14 

Show that the algorithm in algorithm 13.4 returns the correct MAP assignment. First show that for any cut $\mathcal{C}=\mathcal{Z}_{s},\mathcal{Z}_{t}$ , we have that 

$$
c o s t(\mathcal{C})=E(\xi^{\mathcal{C}})+\mathrm{Const.}
$$ 

Conclude the desired result. 

# Exercise $13.15\star$ 

Show how the optimal alpha-beta swap step can be found by running min-cut on an appropriately constructed graph. More precisely: 

a. Deﬁne a set of binary variables $t_{1},\dots,t_{n}$ , such that the value of the $t_{i}$ ’s deﬁnes an alpha-beta-swap transformation on the $x_{i}$ ’s. b. Deﬁne an energy function $E^{\prime}$ over the $_T$ variables such that $E^{\prime}(t)=E(\pmb{x}^{\prime})$ . c. Show that the energy function $E^{\prime}$ is submodular if the original energy function $E$ is a semimetric. 

# Exercise $13.16\star$ 

truncation As we discussed, many energy functions are not submodular. We now describe a method that allows min-cut methods to be applied to energy functions where most of the terms are submodular, but some small subset is not submodular. This method is based on the truncation of the nonsubmodular potentials, so as to make them submodular. 

Algorithm 13.6 Efcient min-sum message passing for untruncated 1-norm energies Procedure Msg-Truncated-1-Norm ( $c$ // Parameters deﬁning the pairwise factor $h_{i}(x_{i})$ // Single-variable term in equation (13.36) ) 1 for $x_{j}=1,\cdot\cdot\cdot,K-1$ 2 $r(x_{j})\gets\ \operatorname*{min}[h_{i}(x_{j}),r(x_{j}-1)+c]$ 3 for $x_{j}=K-2,.\ldots,0$ − 4 $r(x_{j})\gets\ \operatorname*{min}[r(x_{j}),r(x_{j}+1)+c]$ 5 return ( r ) 

a. Let $E$ be an energy function over binary-valued variables that contains some number of pairwise terms $\epsilon_{\underline{{{i}}},\underline{{{j}}}}(v_{i},v_{j})$ that do not satisfy equation (13.33). Assume that we replace each such pairwise term $\epsilon_{i,j}$ with a term $\epsilon_{i,j}^{\prime}$ that satisﬁes this inequality, by decreasing $\epsilon_{i,j}(0,\bar{0})$ , by increasing $\epsilon_{i,j}(1,0)$ or $\epsilon_{i,j}(0,1)$ , or both. The node energies remain unchanged. Let $E^{\prime}$ be the resulting energy. Show that if $\xi^{*}$ optimizes $E^{\prime}$ , then $E(\xi^{*})\leq E(\mathbf{0})$ b. Describe how, in the multilabel case, this procedure can be used within the alpha-expansion algorithm to ﬁnd a local optimum of the energy function. 

# Exercise $13.17\star$ 

Consider the task of passing a message over an edge $X_{i}{-}X_{j}$ in a metric MRF; our goal is to make the message passing step more efcient by exploiting the metric structure. As usual in metric MRFs, we consider the problem in terms of energies; thus, our message computation takes the form: 

$$
\delta_{i\rightarrow j}(x_{j})=\operatorname*{min}_{x_{i}}(\epsilon_{i,j}(x_{i},x_{j})+h_{i}(x_{i})),
$$ 

where $\begin{array}{r}{h_{i}(x_{i})=\epsilon_{i}(x_{i})+\sum_{k\neq j}\delta_{i\rightarrow j}(x_{k})}\end{array}$ . In general, this computation requires $O(K^{2})$ steps. However, we now consider two special cases where this computation can be done in $O(K)$ steps. 

a. Assume that $\epsilon_{i,j}(x_{i},x_{j})$ is an Ising energy function, as in equation (4.6). Show how the message can be computed in $O(K)$ steps. b. Now assume that both $X_{i},X_{j}$ take on value $\{0,.\,.\,.\,,K\,-\,1\}$ Assume that $\epsilon_{i,j}(x_{i},x_{j})$ is a nontruncated 1-norm, as in equation (4.7) with $p=1$ and $\mathrm{dist}_{\mathrm{max}}=\infty$ ∞ . Show that the algorithm in algorithm 13.6 computes the correct message in $O(K)$ steps. c. Extend the algorithm of algorithm 13.6 to the case of a truncated 1-norm (where $\mathrm{{dist}_{\mathrm{{max}}}<\infty)}$ ). 

# Exercise $13.18\star$ 

Consider the use of the branch-and-bound algorithm of appendix A.4.3 for ﬁnding the top $K$ highest- probability assignments in an (unnormalized) distribution $\tilde{P}_{\Phi}$ deﬁned by a set of factors $\Phi$ . 

a. Consider a partial assignment $_{_y}$ to some set of variables $\mathbf{Y}$ . Provide both an upper and a lower bound to $\log\tilde{P}_{\Phi}(\pmb{y})$ . b. Describe how to use your bounds in the context of a branch-and-bound algorithm to ﬁnd the MAP assignment for $\tilde{P}_{\Phi}$ Φ . Can you use both the lower and upper bounds in your search? c. Extend your algorithm to ﬁnd the $K$ highest probability joint assignments in $\tilde{P}_{\Phi}$ Φ . Hint: Your algorithm should ﬁnd the assignments in order of decreasing probability, starting with the MAP. Be sure to reuse as much of your previous computations as possible as you continue the search for the next assignment. 

# Exercise 13.19 

Show that, for any function $f$ , 

$$
\operatorname*{max}_{x}\sum_{y}f(x,y)\leq\sum_{y}\operatorname*{max}_{x}f(x,y),
$$ 

and provide necessary and sufcient conditions for when equation (13.37) holds as equality. 

# Exercise ${\bf13.20\star}$ 

a. Use equation (13.37) to provide an efcient algorithm for computing an upper bound 

$$
\operatorname{bound}({\pmb y}_{1...i})=\operatorname*{max}_{y_{i+1},...,y_{n}}\operatorname{score}({\pmb y}_{1...i},y_{i+1},.\,.\,.\,,y_{n}),
$$ 

where $\operatorname{score}(y)$ is deﬁned as in equation (13.35). Your computation of the bound should take no more than a run of variable elimination in an unconstrained elimination ordering over all of the network variables. 

b. Use this bound to construct a branch-and-bound algorithm for the marginal-MAP problem. 

# Exercise ${\bf13.21\star}$ 

In this question, we consider the application of conditioning to a marginal MAP query: 

$$
{\underset{Y}{\operatorname{arg\,max}}}\sum_{Z}\prod_{\phi\in\Phi}\phi.
$$ 

Let $U$ be a set of conditioning variables. 

a. Consider ﬁrst the case of a simple MAP query, so that $z=\emptyset$ and $Y=\mathcal{X}$ . Show how you would 

adapt Conditioning in algorithm 9.5 to deal with the max-product rather than the sum-product task. b. Now, consider a max-sum-product task. When is $U$ a legal set of conditioning variables for this query? Justify your response. (Hint: Recall that the order of the operations we perform must respect the ordering constraint discussed in section 2.1.5, and that the elimination operations work from the outside in, and the conditioning operations from the inside out.) c. Now, assuming that $U$ is a legal set of conditioning variables, specify a conditioning algorithm that computes the value of the corresponding max-sum-product query, as in equation (13.8). d. Extend your max-sum-product algorithm to compute the actual maximizing assignment to $\mathbf{Y}$ , as in the MAP query. Your algorithm should work for any legal conditioning set $U$ . 

# 14 Inference in Hybrid Networks 

In our discussion of inference so far, we have focused on the case of discrete probabilistic models. However, many interesting domains also contain continuous variables such as temperature, location, or distance. In this chapter, we address the task of inference in graphical models that involve such variables. 

For this chapter, let $\mathcal{X}=\Gamma\cup\Delta$ , where $\Gamma$ denotes the continuous variables and $\Delta$ the discrete variables. In cases where we wish to distinguish discrete and continuous variables, we use the convention that discrete variables are named with letters near the beginning of the alphabet $(A,B,C),$ , whereas continuous ones are named with letters near the end $(X,Y,Z)$ . 

# 14.1 Introduction 

# 14.1.1 Challenges 

At an abstract level, the introduction of continuous variables in a graphical model is not difcult. As we saw in section 5.5, we can use a range of diferent representations for the CPDs or factors in our network. We now have a set of factors, over which we can perform the same operations that we utilize for inference in the discrete case: We can multiply factors, which in this case corresponds to multiplying the multidimensional continuous functions representing the factors; and we can marginalize out variables in a factor, which in this case is done using integration rather than summation. It is not difcult to show that, with these operations in hand, the sum- product inference algorithms that we used in the discrete case can be applied without change, and are guaranteed to lead to correct answers. 

Unfortunately, a little more thought reveals that the correct implementation of these basic operations poses a range of challenges, whose solution is far from obvious. 

The ﬁrst challenge involves the representation of factors involving continuous variables. Unlike discrete variables, there is no universal representation of a factor over continuous variables, and so we must usually select a parametric family for each CPD or initial factor in our network. Even if we pick the same parametric family for each of our initial factor in the network, it may not be the case that multiplying factors or marginalizing a factor leaves it within the parametric family. If not, then it is not even clear how we would represent the intermediate results in our inference process. The situation becomes even more complex when factors in the original network call for the use of diferent parametric families. In this case, it is generally unlikely that we can ﬁnd a single parametric family that can correctly encode all of the intermediate factors in our network. In fact, in some cases — most notably networks involving both discrete and continuous variables — one can show that the intermediate factors cannot be represented using any ﬁxed number of parameters; in fact, the representation size of those factors grows exponentially with the size of the network. 

A second challenge involves the marginalization step, which now requires integration rather than summation. Integration introduces a new set of subtleties. First, not all functions are integrable: in some cases, the integral may be inﬁnite or even ill deﬁned. Second, even functions where the integral is well deﬁned may not have a closed-form integral, requiring the use of a numerical integration method, which is usually approximate. 

# 14.1.2 Discretization 

An alternative approach to inference in hybrid models is to circumvent the entire problem of dealing with continuous factors: We simply convert all continuous variable to discrete ones by discretizing their domain into some ﬁnite set of intervals. Once all variables have been discretized, the result is a standard discrete probabilistic model, which we can handle using the standard inference techniques described in the preceding chapters. 

How do we convert a hybrid CPDs into a table? Assume that we have a variable $Y$ with a continuous parent $X$ . Let $A$ be the discrete variable that replaces $X$ and $B$ the discrete variable that replaces $Y$ . Let $a\in\mathit{V a l}(A)$ cor spond to the interval $[x^{1},x^{2}]$ for $X$ , and $b\,\in\,V a l(B)$ correspond to the interval $[y^{1},y^{2}]$ for Y . 

In principle, one approach for discretization is to deﬁne 

$$
P(b\mid a)=\int_{x^{1}}^{x^{2}}p(Y\in[y^{1},y^{2}]\mid X=x)p(X=x\mid X\in[x^{1},x^{2}])d x.
$$ 

This integral averages out the conditional probability that $Y$ is in the interval $[y^{1},y^{2}]$ given $X$ , aggregating over the diferent values of $x$ in the relevant interval for $X$ . The distribution used in this formulation is the prior probability $p(X)$ , which has the efect of weighting the average more toward more likely values of $X$ . While plausible, this computation is expensive, since it requires that we perform inference in the model. Moreover, even if we perform our estimation relative to the prior $p(X)$ , we have no guarantees of a good approximation, since our posterior over $X$ may be quite diferent. 

Therefore, for simplicity, we often use simpler approximations. In one, we simply select some particular valu ${{x}^{*}}\in[{{x}^{1}},{{x}^{2}}]$ , and estimate $P(b\mid a)$ as the total probability mass of the interval $[y^{1},y^{2}]$ given $x^{*}$ : : 

$$
P(Y\in[y^{1},y^{2}]\mid x^{*})=\int_{y^{1}}^{y^{2}}p(y\mid x^{*})d y.
$$ 

For some density functions $p$ , we can compute this interval in closed form. In others, we might have to resort to numerical integration methods. Alternatively, we can average the values over the interval $[x^{1},x^{2}]$ using a predeﬁned distribution, such as the uniform distribution over the interval. 

Although discretization is used very often in practice, as we discussed in section 5.5, it has several signiﬁcant limitations. The discretization is only an approximation of the true probability distribution. In order to get answers that do not lose most of the information, our discretization scheme must have a ﬁne resolution where the posterior probability mass lies. Unfortunately, before we actually perform the inference, we do not know the posterior distribution. Thus, we must often resort to a discretization that is fairly ﬁne-grained over the entire space, leading to a very large domain for the resulting discrete variable. 

This problem is particularly serious when we need to approximate distributions over more than a handful of discretized variables. As in any table-based CPD, the size of the resulting factor is exponential in the number of variables. When this number is large and the discretization is anything but trivial, the size of the factor can be huge. For example, if we need to represent a distribution over $d$ continuous variables, each of which is discretized into $m$ values, the total number of parameters required is $O(m^{d})$ . By contrast, if the distribution is a $d$ -dimensional Gaussian, the number of parameters required is only $O(d^{2})$ . Thus, not only does the discretiza- tion process introduce approximations into our joint probability distribution, but we also often end up converting a polynomial parameter iz ation into an exponential one. 

Overall, discretization provides a trade-of between accuracy of the approximation and cost of computation. In certain cases, acceptable accuracy can be obtained at reasonable cost. However, in many practical applications, the computational cost required to obtain the accuracy needed for the task is prohibitive, making discretization a nonviable option. 

# 14.1.3 Overview 

Thus, we see that inference in continuous and hybrid models, although similar in principle to discrete inference, brings forth a new set of challenges. In this chapter, we discuss some of these issues and show how, in certain settings, these challenges can be addressed. 

As for inference in discrete networks, the bulk of the work on inference in continuous and hybrid networks falls largely into two categories: Approaches that are based on message passing methods, and approaches that use one of the particle-based methods discussed in chapter 12. However, unlike the discrete case, even the message passing algorithms are rarely exact. 

The message passing inference methods have largely revolved around the use of the Gaussian distribution. The easiest case is when the distribution is, in fact, a multivariate Gaussian. In this case, many of the challenges described before disappear. In particular, the intermediate factors in a Gaussian network can be described compactly using a simple parametric representation called the canonical form . This representation is closed under the basic operations using in inference: factor product, factor division, factor reduction, and marginalization. Thus, we can deﬁne a set of simple data structures that allow the inference process to be performed. Moreover, the integration operation required by marginalization is always well deﬁned, and it is guaranteed to produce a ﬁnite integral under certain conditions; when it is well deﬁned, it has a simple analytical solution. 

As a consequence, a fairly straightforward modiﬁcation of the discrete sum-product algo- rithms (whether variable elimination or clique tree) gives rise to an exact inference algorithm for Gaussian networks. A similar extension results in a Gaussian version of the loopy belief propagation algorithm; here, however, the conditions on integrability impose certain constraints about the form of the distribution. Importantly, under these conditions, loopy belief propagation for Gaussian distributions is guaranteed to return the correct means for the variables in the network, although it can underestimate the variances, leading to overconﬁdent estimates. 

There are two main extensions to the purely Gaussian case: non-Gaussian continuous densi- ties, and hybrid models that involve both discrete and continuous variables. The most common method for dealing with these extensions is the same: we approximate intermediate factors in the computation as Gaussians; in efect, these algorithms are an instance of the expectation propagation algorithm discussed in section 11.4.4. As we discuss, Gaussians provide a good basis for the basic operations in these algorithms, including the key operation of approximate marginalization. Interestingly, there is one class of inference tasks where this general algorithm is guaranteed to produce exact answers to a certain subset of queries. This is the class of CLG net- works in which we use a particular form of clique tree for the inference. Unfortunately, although of conceptual interest, this “exact” variant is rarely useful except in fairly small problems. 

An alternative approach is to use an approximation method that makes no parametric as- sumptions about the distribution. Speciﬁcally, we can approximate the distribution as a set of particles, as described in chapter 12. As we will see, particle-based methods often provide the easiest approach to inference in a hybrid network. They make almost no assumptions about the form of the CPDs, and can approximate an arbitrarily complex posterior. Their primary disadvantage is, as usual, the fact that a very large number of particles might be required for a good approximation. 

# 14.2 Variable Elimination in Gaussian Networks 

The ﬁrst class of networks we consider is the class of Gaussian networks, as described in chapter 7: These are networks where all of the variables are continuous, and all of the local factors encode linear dependencies. In the case of Bayesian networks, the CPDs take the form of linear Gaussians (as in deﬁnition 5.15). In the case of Markov networks, they can take the form of general log-quadratic form, as in equation (7.7). 

As we showed in chapter 7, both of these representations are simply alternative parameter- izations of a joint multivariate Gaussian distributions. This observation immediately suggests one approach to performing exact inference in this class of networks: We simply convert the LG network into the equivalent multivariate Gaussian, and perform the necessary operations — marginalization and conditioning — on that representation. Speciﬁcally, as we discussed, if we have a Gaussian distribution $p(X,Y)$ represented as a mean vector and a covariance matrix, we can extract the marginal distribution $p(Y)$ simply by restricting attention to the elements of the mean and the covariance matrix that correspond to the variables in $Y$ . The operation of conditioning a Gaussian on evidence $Y\,=\,y$ is also easy: we simply instantiate the variables $Y$ to their observed values $_{_y}$ in the joint density function, and renormalize the resulting unnormalized measure over $X$ to obtain a new Gaussian density. 

This approach simply generates the joint distribution over the entire set of variables in the network, and then manipulates it directly. However, unlike the case of discrete distributions, the representation size of the joint density in the Gaussian case is quadratic, rather than exponential, in the number of variables. Thus, these operations are often feasible in a Gaussian network in cases that would not be feasible in a comparable discrete network. 

Still, even quadratic cost might not be feasible in many cases, for example, when the network is over thousands of variables. Furthermore, this approach does not exploit any of the structure represented in the network distribution. An alternative approach to inference is to adapt the message passing algorithms, such as variable elimination (or clique trees) for exact inference, or belief propagation for approximation inference, to the linear Gaussian case. We now describe this approach. We begin with describing the basic representation used for these message passing schemes, and then present these two classes of algorithms. 

# 14.2.1 Canonical Forms 

As we discussed, the key diference between inference in the continuous and the discrete case is that the factors can no longer be represented as tables. Naively, we might think that we can represent factors as Gaussians, but this is not the case. The reason is that linear Gaussian CPDs are generally not Gaussians, but are rather a conditional distribution. Thus, we need to ﬁnd a more general representation for factors, that accommodates both Gaussian distributions and linear Gaussian models, as well as any combination of these models that might arise during the course of inference. 

# 14.2.1.1 The Canonical Form Representation 

The simplest representation used in this setting is the canonical form , which represents the intermediate result as a log-quadratic form $\exp\bigl(Q({\pmb x})\bigr)$ where $Q$ is some quadratic function. In the inference setting, it is useful to make the components of this representation more explicit: 

Deﬁnition 14.1 canonical form A canonical form $\mathcal{C}\left(X;K,h,g\right)$ (or $\mathcal{C}\left(K,h,g\right)$ if we omit the explicit reference to $X$ ) is deﬁned as: 

$$
\mathcal{C}\left({X};{K},{h},{g}\right)=\exp\left(-\frac{1}{2}{X}^{T}{K}{X}+{h}^{T}{X}+{g}\right).
$$ 

We can represent every Gaussian as a canonical form. Rewriting equation (7.1), we obtain: 

$$
\begin{array}{r l r}{\lefteqn{\frac{1}{(2\pi)^{n/2}|{\Sigma}|^{1/2}}\exp\left(-\frac{1}{2}({\pmb x}-{\pmb\mu})^{T}{\Sigma}^{-1}({\pmb x}-{\pmb\mu})\right)}}\\ &{=}&{\exp\left(-\frac{1}{2}{\pmb x}^{T}{\Sigma}^{-1}{\pmb x}+{\pmb\mu}^{T}{\Sigma}^{-1}{\pmb x}-\frac{1}{2}{\pmb\mu}^{T}{\Sigma}^{-1}{\pmb\mu}-\log\left((2\pi)^{n/2}|{\Sigma}|^{1/2}\right)\right).}\end{array}
$$ 

Thus, $\mathcal{N}\left(\pmb{\mu};\Sigma\right)=\mathcal{C}\left(K,h,g\right)$ where: 

$$
\begin{array}{r c l}{K}&{=}&{\Sigma^{-1}}\\ {\pmb{h}}&{=}&{\Sigma^{-1}\pmb{\mu}}\\ {g}&{=}&{-\displaystyle\frac{1}{2}\pmb{\mu}^{T}\Sigma^{-1}\pmb{\mu}-\log\left((2\pi)^{n/2}|\Sigma|^{1/2}\right).}\end{array}
$$ 

However, canonical forms are more general than Gaussians: If $K$ is not invertible, the canonical form is well deﬁned, but it is not the inverse of a legal covariance matrix. In particular, we can easily represent linear Gaussian CPDs as canonical forms (exercise 14.1). 

# 14.2.1.2 Operations on Canonical Forms 

canonical form product 

It is possible to perform various operations on canonical forms. The product of two canonical form factors over the same scope $X$ is simply: 

$$
\mathcal{C}\left(K_{1},\pmb{h}_{1},g_{1}\right)\cdot\mathcal{C}\left(K_{2},\pmb{h}_{2},g_{2}\right)=\mathcal{C}\left(K_{1}+K_{2},\pmb{h}_{1}+\pmb{h}_{2},g_{1}+g_{2}\right).
$$ 

When we have two canonical factors over diferent scopes $X$ and $Y$ , we simply extend the scope of both to make their scopes match and then perform the operation of equation (14.2). The extension of the scope is performed by simply adding zero entries to both the $K$ matrices and the $h$ vectors. 

Example 14.1 Consider the following two canonical forms: 

$$
\begin{array}{r l r}{\phi_{1}(X,Y)}&{=}&{\mathcal{C}\left(X,Y;\left[\begin{array}{c c}{1}&{-1}\\ {-1}&{1}\end{array}\right]\!,\left(\begin{array}{c}{1}\\ {-1}\end{array}\right)\!,-3\right)}\\ {\phi_{2}(Y,Z)}&{=}&{\mathcal{C}\left(Y,Z;\left[\begin{array}{c c}{3}&{-2}\\ {-2}&{4}\end{array}\right]\!,\left(\begin{array}{c}{5}\\ {-1}\end{array}\right)\!,1\right).}\end{array}
$$ 

We can extend the scope of both of these by simply introducing zeros into the canonical form. For example, we can reformulate: 

$$
\phi_{1}(X,Y,Z)=\mathcal{C}\left(X,Y,Z;\left[\begin{array}{c c c}{1}&{-1}&{0}\\ {-1}&{1}&{0}\\ {0}&{0}&{0}\end{array}\right],\left(\begin{array}{c}{1}\\ {-1}\\ {0}\end{array}\right),-3\right),
$$ 

and similarly for $\phi_{2}(X,Y,Z)$ . The two canonical forms now have the same scope, and can be multiplied using equation (14.2) to produce: 

$$
\mathcal{C}\left(X,Y,Z;\left[\begin{array}{c c c}{{1}}&{{-1}}&{{0}}\\ {{-1}}&{{4}}&{{-2}}\\ {{0}}&{{-2}}&{{4}}\end{array}\right],\left(\begin{array}{c}{{1}}\\ {{4}}\\ {{-1}}\end{array}\right),-2\right).
$$ 

canonical form division 

The division of canonical forms (which is required for message passing in the belief propaga- tion algorithm) is deﬁned analogously: 

$$
\frac{\mathcal{C}\left(K_{1},h_{1},g_{1}\right)}{\mathcal{C}\left(K_{2},h_{2},g_{2}\right)}=\mathcal{C}\left(K_{1}-K_{2},h_{1}-h_{2},g_{1}-g_{2}\right).
$$ 

vacuous canonical form 

canonical form marginalization Note that the vacuous canonical form , which is the analogue of the “all $1^{\circ}$ factor in the discrete case, is deﬁned as $K=0$ , $h=0$ , $g=0$ . Multiplying or dividing by this factor has no efect. 

Less obviously, we can marginalize a canonical form onto a subset of its variables. Let $\mathcal{C}\left(X,Y;K,h,g\right)$ be some canonical form over $\{X,Y\}$ where 

$$
K=\left[\begin{array}{c c}{K_{X X}}&{K_{X Y}}\\ {K_{Y X}}&{K_{Y Y}}\end{array}\right]\quad;\quad h=\left(\begin{array}{c}{h_{X}}\\ {h_{Y}}\end{array}\right).
$$ 

The marginalization of this function onto the variables $X$ is, as usual, the integral over the variables $Y$ : 

$$
\int\mathcal{C}\left(X,Y;K,h,g\right)d Y.
$$ 

As we discussed, we have to guarantee that all of the integrals resulting from marginalization operations are well deﬁned. In the case of canonical forms, the integral is ﬁnite if and only if $K_{Y Y}$ is positive deﬁnite, or equivalently, that it is the inverse of a legal covariance matrix. In this case, the result of the integration operation is a canonical form $\mathcal{C}\left(X;K^{\prime},h^{\prime},g^{\prime}\right)$ given by: 

$$
\begin{array}{r l r}{K^{\prime}}&{=}&{K_{X X}-K_{X Y}K_{Y Y}^{-1}K_{Y X}}\\ {h^{\prime}}&{=}&{h_{X}-K_{X Y}K_{Y Y}^{-1}h_{Y}}\\ {g^{\prime}}&{=}&{g+\frac{1}{2}\left(\log\vert2\pi K_{Y Y}^{-1}\vert+h_{Y}^{T}K_{Y Y}^{-1}h_{Y}\right).}\end{array}
$$ 

canonical form reduction 

Finally, it is possible to reduce a canonical form to a context representing evidence. Assume that the canonical form $\mathcal{C}\left(X,Y;K,h,g\right)$ is given by equation (14.4). Then setting $Y\,=\,y$ results in the canonical form $\mathcal{C}\left(X;K^{\prime},h^{\prime},g^{\prime}\right)$ given by: 

$$
\begin{array}{r c l}{{K^{\prime}}}&{{=}}&{{K_{X X}}}\\ {{h^{\prime}}}&{{=}}&{{h_{X}-K_{X Y}y}}\\ {{g^{\prime}}}&{{=}}&{{g+h_{Y}^{T}y-\displaystyle\frac{1}{2}y^{T}K_{Y Y}y.}}\end{array}
$$ 

See exercise 14.3. 

Importantly, all of the factor operations can be done in time that is polynomial in the scope of the factor. In particular, the product or division of factors requires quadratic time; factor marginalization, which requires matrix inversion, can be done naively in cubic time, and more efciently using advanced methods. 

# 14.2.2 Sum-Product Algorithms 

sum-product The operations described earlier are the basic building blocks for all of our sum-product exact inference algorithms: variable elimination and both types of clique tree algorithms. Thus, we can adapt these algorithms to apply to linear Gaussian networks, using canonical forms as our representation of factors. For example, in the Sum-Product-VE algorithm of algorithm 9.1, we simply implement the factor product operation as in equation (14.2), and replace the summa- tion operation in Sum-Product-Eliminate-Var with an integration operation, implemented as in equation (14.5). 

Care must be taken regarding the treatment of evidence. In discrete factors, when instantiating a variable $Z\;=\;z$ , we could leave the variable $Z$ in the factors involving it, simply zeroing the entries that are not consistent with $Z~=~z$ . In the case of continuous variables, our representation of factors does not allow that option: when we instantiate $Z=z$ , the variable $Z$ is no longer part of the canonical form. Thus, it is necessary to reduce all the factors participating in the inference process to a scope that no longer contains $Z$ . This reduction step is already part of the variable elimination algorithm of algorithm 9.2. It is straightforward to ensure that the clique tree algorithms of chapter 10 similarly reduce all clique and sepset potentials with the evidence prior to any message passing steps. 

well-deﬁned marginalization 

A more important problem that we must consider is that the marginalization operation may not be well deﬁned for an arbitrary canonical form. In order to show the correctness of an inference algorithm, we must show that it executes a marginalization step only on canonical forms for which this operation is well deﬁned . We prove this result in the context of the sum- product clique tree algorithm; the proof for the other cases follows in a straightforward way, due to the equivalence between the upward pass of the diferent message passing algorithms. 

Whenever SP-Message is called, within the CTree-SP-Upward algorithm (algorithm 10.1) the marginal- ization operation is well-deﬁned. 

Proof Consider a call SP-Message $(i,j)$ , and let $\psi(C_{i})$ be the factor constructed in the clique prior to sending the message. Let $\mathcal{C}\left(C_{i};K,h,g\right)$ be the canonical form associated with $\psi(C_{i})$ . Let $\beta_{i}(C_{i})=\mathcal{C}\left(C_{i};K^{\prime},h^{\prime},g^{\prime}\right)$    be the ﬁnal clique potential that would be obtained at $C_{i}$ i in the case where $C_{i}$ is the root of the clique tree computation. The only diference between these two potentials is that the latter also incorporates the message $\delta_{j\rightarrow i}$ from $C_{j}$ . 

Let $Y=C_{j}-S_{i,j}$ be the variables that are marginali d when the message is computed. By the running intersection property, none of the variables Y $Y$ appear in the scope of the sepset $\boldsymbol{S}_{i,j}$ . Thus, the message $\delta_{j\rightarrow i}$ does not mention any of the variables $Y$ . We can verify, by examining equation (14.2), that multiplying a canonical form by a factor that does not mention $Y$ does not change the entries in the matrix $K$ that are associated with the variables in $Y$ . It follows that $K_{Y Y}=K_{Y Y}^{\prime}$ , that is, the submatrices for $Y$ in $K$ and $K^{\prime}$ are the same. Because the ﬁnal clique potential $\beta_{i}(C_{i})$ is its (unnormalized) marginal posterior, it is a normalizable Gaussian distribution, and hence the matrix $K^{\prime}$ is positive deﬁnite. As a consequence, the submatrix $K_{Y Y}^{\prime}$ is also positive deﬁnite. It follows that $K_{Y Y}$ is positive deﬁnite, and therefore the marginalization operation is well deﬁned. 

It follows that we can adapt any of our exact inference algorithms to the case of linear Gaussian networks. The algorithms are essentially unchanged; only the representation of factors and the implementation of the basic factor operations are diferent. In particular, since all factor operations can be done in polynomial time, inference in linear Gaussian networks is linear in the number of cliques, and at most cubic in the size of the largest clique. By comparison, recall that the representation of table factors is, by itself, exponential in the scope, leading to the exponential complexity of inference in discrete networks. 

It is interesting to compare the clique tree inference algorithm to the naive approach of simply generating the joint Gaussian distribution and marginalizing it. The exact inference algorithm requires multiple steps, each of which involves matrix product and inversion. By comparison, the joint distribution can be computed, as discussed in theorem 7.3, by a set of vector-matrix products, and the marginalization of a joint Gaussian over any subset of variables is trivial (as in lemma 7.1). Thus, in cases where the Gaussian has sufciently low dimension, it may be less computationally intensive to use the naive approach for inference. Conversely, in cases where the distribution has high dimension and the network has reasonably low tree-width, the message passing algorithms can ofer considerable savings. 

# 14.2.3 Gaussian Belief Propagation 

Gaussian belief propagation 

The Gaussian belief propagation algorithm utilizes the information form, or canonical form, of the Gaussian distribution. As we discussed, a Gaussian network is encoded using a set of local quadratic potentials, as in equation (14.1). Reducing a canonical-form factor on evidence also results in a canonical-form factor (as in equation (14.6)), and so we can focus attention on a representation that consists of a product of canonical-form factors. This product results in an overall quadratic form: 

$$
p(X_{1},\ldots,X_{n})\propto\exp\left(-{\frac{1}{2}}X^{T}J X+h^{T}X\right).
$$ 

The measure $p$ is normalizable and deﬁnes a legal Gaussian distribution if and only if $J$ is positive deﬁnite. Note that we can obtain $J$ by adding together the individual matrices $K$ deﬁned by the various potentials parameterizing the network. 

In order to apply the belief propagation algorithm, we must deﬁne a cluster graph and assign the components of this parameter iz ation to the diferent clusters in the graph. As in any belief propagation algorithm, we need the cluster graph to respect the family preservation property. In our setting, the only terms in the quadratic form involve single variables — the $h_{i}X_{i}$ terms — and pairs of variables $X_{i},X_{j}$ for which $J_{i j}\neq0$ . Thus, the minimal cluster g at satisﬁes the family preservation requirement would contain a cluster for each edge $X_{i},X_{j}$ (pairs for which $J_{i j}\neq0$ ). We choose to use a Be ctured cluster graph that has a cluster for each variable $X_{i}$ and a cluster for each edge $X_{i},X_{j}$ . While it is certainly possible to deﬁne a belief propagation algorithm on a cluster graph with larger cliques, the standard application runs belief propagation directly on this pairwise network. 

We note that the parameter iz ation of the cluster graph is not uniquely deﬁned. In particular, a term of the form $J_{i i}X_{i}^{2}$ can be partitioned in inﬁnitely many ways among the node’s own cluster and among the edges that contain $X_{i}$ . Each of these partitions deﬁnes a diferent set of potentials in the cluster graph, and hence will induce a diferent execution of belief propagation. We describe the algorithm in terms of the simplest partition, where each diagonal term $J_{i i}$ is assigned to the corresponding $X_{i}$ cluster, and the of-diagonal terms $J_{i j}$ are assigned to the $X_{i},X_{j}$ cluster. 

With this decision, the belief propagation algorithm for Gaussian networks is simply derived from the standard message passing operations, implemented with the canonical-form operations for the factor product and marginalization steps. For concreteness, we now provide the precise message passing steps. The message from $X_{i}$ to $X_{j}$ has the form 

$$
\delta_{i\rightarrow j}(x_{j})=\exp\left(-\frac{1}{2}J_{i\rightarrow j}x_{j}^{2}+h_{i\rightarrow j}x_{j}\right).
$$ 

We compute the coefcients in this expression via a two-stage process. The ﬁrst step corresponds to the message sent from the $X_{i}$ cluster to the $X_{i},X_{j}$ edge; in this step, $X_{i}$ aggregates all of the information from its own local potential and the messages sent from its other incident edges: 

$$
\begin{array}{r l r}{\hat{J}_{i\setminus j}}&{=}&{J_{i i}+\sum_{k\in\mathrm{Nb}_{i}-\{j\}}J_{k\to i}}\\ {\hat{h}_{i\setminus j}}&{=}&{h_{i}+\sum_{k\in\mathrm{Nb}_{i}-\{j\}}h_{k\to i}.}\end{array}
$$ 

In the second step, the $X_{i},X_{j}$ edge takes the message received from $X_{i}$ and sends the ap- propriate message to $X_{j}$ . The form of the message can be computed (with some algebraic manipulation) from the formulas for the conditional mean and conditional variance, that we used in theorem 7.4, giving rise to the following update equations: 

$$
\begin{array}{r c l}{J_{i\to j}}&{=}&{-J_{j i}\hat{J}_{i\backslash j}^{-1}J_{j i}}\\ {h_{i\to j}}&{=}&{-J_{j i}\hat{J}_{i\backslash j}^{-1}\hat{h}_{i\backslash j}.}\end{array}
$$ 

These messages can be scheduled in various ways, either synchronously or asynchronously (see box 11.B). 

If and when the message passing process has converged, we can compute the $X_{i}$ -entries of the information form by combining the messages in the usual way: 

$$
\begin{array}{l l l}{\hat{J}_{i}}&{=}&{J_{i i}+\displaystyle\sum_{k\in\ensuremath{\mathrm{Nb}}_{i}}J_{k\to i}}\\ {\hat{h}_{i}}&{=}&{\displaystyle h_{i}+\sum_{k\in\ensuremath{\mathrm{Nb}}_{i}}h_{k\to i}.}\end{array}
$$ 

From this information-form representation, $X_{i}$ ’s approximate mean $\hat{\mu}_{i}$ and covariance $\hat{\sigma}_{i}^{2}$ can be reconstructed as usual: 

$$
\begin{array}{c c c}{{\hat{\mu}_{i}}}&{{=}}&{{(\hat{J}_{i})^{-1}\hat{h}_{i}}}\\ {{\hat{\sigma}_{i}^{2}}}&{{=}}&{{(\hat{J}_{i})^{-1}}}\end{array}
$$ 

One can now show the following result, whose proof we omit: 

Let $\hat{\mu}_{i},\hat{\sigma}_{i}^{2}$ be a set of ﬁxed points of the message passing process deﬁned in equation (14.8), equa- tion (14.9). Then $\hat{\mu}_{i}$ is the correct posterior mean for the variable $X_{i}$ in the distribution $p$ . 

Thus, if the BP message passing process converges, the resulting beliefs encode the correct mean of the joint distribution. The estimated variances $\hat{\sigma}_{i}^{2}$ are generally not correct; rather, they are an underestimate of the true variances, so that the resulting posteriors are “overconﬁdent.” 

This correctness result is predicated on convergence. In general, this message passing process may or may not converge. Moreover, their convergence may depend on the order in which messages are sent. However, unlike the discrete case, one can provide a very detailed char- acterization of the convergence properties of this process, as well as sufcient conditions for convergence (see section 14.7 for some references). In particular, one can show that the pairwise normalizability condition, as in deﬁnition 7.3, sufces to guarantee the convergence of the belief propagation algorithm for any order of messages. Recall that this condition guarantees that each edge can be associated with a potential that is a normalized Gaussian distribution. As a consequence, when the Gaussian parameterizing the edge is multiplied with Gaussians encoding the incoming message, the result is also a well-normalized Gaussian. 

We note that pairwise normalizability is sufcient, but not necessary, for the convergence of belief propagation. 

# Example 14.2 

Consider the Gaussian MRF shown in ﬁgure 14.1. This model deﬁnes a frustrated loop, since three of the edges in the loop are driving $X_{1},X_{2}$ toward a positive correlation, but the edge between them is driving in the opposite direction. The larger the value of $r$ , the worse the frustration. This model is diagonally dominant for any value of $r<1/3$ . It is pairwise normalizable for any $r<0.39030$ ; however, it deﬁnes a valid Gaussian distribution for values of $r$ up to 0 . 5 . 

In practice, the Gaussian belief propagation algorithm often converges and provides an excel- lent alternative for reasoning in Gaussian distributions that are too large for exact techniques. 

![](images/a9b83af17904dff70e126bc5c84403f0bbf26d3a5f2edf8c1b97446b1fe80d37.jpg) 
Figure 14.1 A Gaussian MRF used to illustrate convergence properties of Gaussian belief propaga- tion. In this model, $J_{i i}=1$ , and $J_{i j}=r$ for all edges $(i,j)$ , except for $J_{12}=-r$ . 

![](images/25cc62b2f21aac86cf9ea95479c05002ccc70b0aa06f40e0030576b360e97fd1.jpg) 
Figure 14.2 Simple CLG network used to demonstrate hardness of inference. $D_{1},\dots,D_{n}$ are discrete, and $X_{1},\ldots,X_{n}$ are continuous. 

# 14.3 Hybrid Networks 

So far, we have dealt with models that involve only continuous variables. We now begin our discussion of hybrid networks — those that include both continuous and discrete variables. We focus the bulk of our discussion on conditional linear Gaussian (CLG) networks (deﬁnition 5.16), where there are no discrete variables with continuous parents, and where all the local probability models of continuous variables are conditional linear Gaussian CPDs. In the next section, we discuss inference for non-Gaussian dependencies, which will allow us to deal with non-CLG dependencies. 

Even for this restricted class of networks, we can show that inference is very challenging. Indeed, we can show that inference in this class of networks is $\mathcal{N P}$ -hard, even when the network structure is a polytree. We then show how the expectation propagation approach described in the previous section can be applied in this setting. Somewhat surprisingly, we show that this approach provides “exact” results in certain cases, albeit mostly ones of theoretical interest. 

# 14.3.1 The Difculties 

As we discussed earlier, at an abstract level, variable elimination algorithms are all the same: They perform operations over factors to produce new factors. In the case of discrete models, factors can be represented as tables, and these operations can be performed efectively as table operations. In the case of Gaussian networks, the factors can be represented as canonical forms. As we now show, in the hybrid case, the representation of the intermediate factors can grow arbitrarily complex. 

Consider the simple network shown in ﬁgure 14.2, where we assume that each $D_{i}$ is a discrete binary variable, $X_{1}$ is a conditional Gaussian, and each $X_{i}$ for $\textit{i}>1$ is a conditional linear 

![](images/359335343171b207b16d87250f972e42c40988aec6136421440ed26dd7417628.jpg) 
Figure 14.3 Joint marginal distribution $p(X_{1},X_{2})$ for a network as in ﬁgure 14.2 

Gaussian (CLG) (see deﬁnition 5.15): 

$$
p(X_{i}\mid X_{i-1},D_{i})=\mathcal{N}\left(X_{i}\mid\alpha_{i,d_{i}}x_{i-1}+\beta_{i,d_{i}};\sigma_{i,d_{i}}^{2}\right),
$$ 

where for simplicity we take $\alpha_{1,d_{1}}=0$ , so the same formula applies for all $i$ . Assume that our goal is to compute $P(X_{n})$ . To do so, we marginalize the joint distribution: 

$$
p(X_{n})=\sum_{D_{1},\dots,D_{n}}\int p(D_{1},\dots,D_{n},X_{1},\dots,X_{n})d X_{1}\dots d X_{n-1}.
$$ 

Using the chain rule, the joint distribution is deﬁned as: 

$$
p(D_{1},\ldots,D_{n},X_{1},\ldots,X_{n})=\prod_{i=1}^{n}P(D_{i})p(X_{1}\mid D_{1})\prod_{i=2}^{n}p(X_{i}\mid D_{i},X_{i-1}).
$$ 

We can reorder the sums and integrals and push each of them in over factors that do not involve the variable to be marginalized. Thus, for example, we have that: 

$$
p(X_{2})=\sum_{D_{2}}P(D_{2})\int p(X_{2}\mid X_{1},D_{2})\sum_{D_{1}}p(X_{1}\mid D_{1})P(D_{1})d X_{1}.
$$ 

Using the same variable elimination approach that we used in the discrete case, we ﬁrst generate a factor over $X_{1}$ $P(D_{1})p(X_{1}\mid D_{1})$ and s $D_{1}$ . This factor is then ltiplied with $p(X_{2}\mid X_{1},D_{2})$ | to generate a factor over $X_{1},X_{2},D_{2}$ . We can now e e $X_{1}$ by integrating the function that corresponds to this factor, to generate a factor over $X_{2},D_{2}$ . We can now sum out $D_{2}$ to get $p(X_{2})$ . The process of computing $p(X_{n})$ is analogous. 

Now, consider the marginal distribution $p(X_{i})$ for $i=1,\dots,n$ . For $i=1$ , this distribution is a mixture of two Gaussians, one corresponding to the value $D_{1}=d_{1}^{1}$ and the other to $D_{1}=d_{1}^{0}$ . For $i=2$ , let us ﬁrst consider the distribution $p(X_{1},X_{2})$ . This distribution is a mixture of four Gaussians, for the four diferent instantiations of $D_{1},D_{2}$ . For example, assume that we have: 

$$
\begin{array}{r c l}{p(X_{1}\mid d_{1}^{0})}&{=}&{\mathcal{N}\left(0;0.7^{2}\right)}\\ {p(X_{1}\mid d_{1}^{1})}&{=}&{\mathcal{N}\left(1.5;0.6^{2}\right)}\\ {p(X_{2}\mid X_{1},d_{2}^{0})}&{=}&{\mathcal{N}\left(-1.5X_{1};0.6^{2}\right)}\\ {p(X_{2}\mid X_{1},d_{2}^{1})}&{=}&{\mathcal{N}\left(1.5;0.7^{2}\right).}\end{array}
$$ 

The joint marginal distribution $p(X_{1},X_{2})$ is shown in ﬁgure 14.3. Note that the mixture contains two components where $X_{1}$ and $X_{2}$ are independent; these components correspond to the instantiations where $D_{2}\,=\,d_{2}^{1}$ , in which we have $\alpha_{2,1}\,=\,0$ . As shown in lemma 7.1, the marginal distribution of a Gaussian is also a Gaussian, and the same applies to a mixture. Hence the marginal distribution $p(X_{2})$ is also a mixture of four Gaussians. We can easily extend this argument, showing that $p(X_{i})$ is a mixture of $2^{i}$ Gaussians. In general, even representing the correct marginal distribution in a hybrid network can require space that is exponential in the size of network. 

Indeed, this type of example can be used as the basis for proving a result about the hardness of inference in models of this type. Clearly, as CLG networks subsume standard discrete networks, exact inference ch networks is necessarily $\mathcal{N P}$ -hard. More surprising, however, is the fact that this task is NP -hard even in very simple network structures such as polytrees. In fact, the problem of computing the probability of a single discrete variable, or even approximating this probability with any absolute error strictly less than $1/2$ , is $\mathcal{N P}$ -hard. 

To deﬁne the problem precisely, assume we are working with ﬁnite precision continuous variables. We deﬁne the following decision problem CLG-DP : 

Input: A CLG Bayesian network $\mathcal{B}$ over $\Delta\cup\Gamma$ , evidence $E=e$ , and a discrete variable $A\in\Delta$ . 

Output: “Yes” if $P_{\mathcal{B}}(A=a^{1}\mid E=e)>0.5$ . 

Theorem 14.2 The problem CLG-DP is $\mathcal{N P}$ -hard even if $\mathcal{B}$ is a polytree. 

The fact that exact inference in polytree CLGs is $\mathcal{N P}$ -hard may not be very surprising by itself. After all, the distribution of a continuous variable in a CLG distribution, even in a simple polytree, can be a mixture of exponentially many Gaussians. Therefore, it might be expected that tasks that require that we reason directly with such a distribution are hard. Somewhat more surprisingly, this phenomenon arises even in networks where the prior distribution of every continuous variable is a mixture of at most two Gaussians. 

The problem CLG-DP is NP -hard even if $\mathcal{B}$ is a polytree where all of the discrete variables are binary-valued, and where every continuous variable has at most one discrete ancestor. 

Intuitively, this proof relies on the use of activated v-structures to introduce, in the posterior, dependencies where a continuous variable can have exponentially many modes. 

Overall, these results show that even the easiest approximate inference task — inference over a binary-valued variable that achieves absolute error less than $0.5\:-$ is intractable in CLG networks. This fact implies that one should not expect to ﬁnd a polynomial-time approximate inference algorithm with a useful error bound without further restrictions on the structure or the parameters of the CLGs. 

# 14.3.2 Factor Operations for Hybrid Gaussian Networks 

Despite the discouraging results in the previous section, one can try to produce useful algorithms for hybrid networks in order to construct an approximate inference algorithm that has good performance, at least in practice. We now present the basic factor operations required for message passing or variable elimination in hybrid networks. In subsequent sections, we describe two algorithms that use these operations for inference. 

# 14.3.2.1 Canonical Tables 

As we discussed, the key decision in adapting an exact inference algorithm to a class of hybrid networks is the representation of the factors involved in the process. In section 14.2, when doing inference for linear Gaussian networks, we used canonical forms to represent factors. This representation is rich enough to capture both Gaussians and linear Gaussian CPDs, as well as all of the intermediate expressions that arise during the course of inference. In the case of CLG networks, we must contend with discrete variables as well as continuous ones. In particular, a CLG CPD has a linear Gaussian model for each instantiation of the discrete parents. 

Extending on the canonical form, we can represent this CPD as a table, with one entry for each instantiation of the discrete variables, each associated with a canonical form over the continuous ones: 

Deﬁnition 14.2 canonical table $A$ canonical table $\phi$ over $D,X$ for $D\ \subseteq\ \Delta$ and $X\subseteq\Gamma$ is a table with an en for each

 $\pmb{d}\in\mathit{V a l}(\pmb{D})$ , where each entry contai a canonical form $\mathcal{C}\left(X;K_{d},h_{d},g_{d}\right)$ over X . We use

 $\phi(d)$ to denote the canonical form over X associated with the instantiation d . 

The canonical table representation subsumes both the canonical form and the table factors used in the con t of discrete net or the former, $D=\emptyset$ , s e hav nly a single canonical form over X . For the latter, $X=\emptyset$ o that the param $K_{d}$ and $h_{d}$ are vacuous, and we remain with a canonical form $\exp(g_{d})$ for each entry $\phi(d)$ . Clearly, a standard table factor $\phi(D)$ can be reformulated in this way by simply taking $g_{d}=\ln(\phi(d))$ . Therefore, we can represent any of the original CPDs or Gaussian potentials in a CLG network as a canonical table. 

Now, consider the operations on factors used by the various exact inference algorithms. Let us ﬁrst consider the operations of factor product and factor division. As for the table representation of discrete factors, these operations are performed between corresponding table entries, in the usual way. The product or division operations for the individual entries are performed over the associated canonical forms, as speciﬁed in equation (14.2) and equation (14.3). 

Example 14.3 Assume we have two factors $\phi_{1}(A,B,X,Y)$ and $\phi_{2}(B,C,Y,Z)$ , which we want to multiply in order to produce $\tau(A,B,C,X,Y,Z)$ . The resulting factor $\tau$ has an entry for each instantiation of the discrete variables $A,B,C$ . The entry for a particular instantiation $a,b,c$ is a canonical form, which is derived as the product of the two canonical forms: the one associated with $a,b$ in $\phi_{1}$ and the one associated with $b,c$ in $\phi_{2}$ . The product operation for two canonical forms of diferent scopes is illustrated in example 14.1. 

Similarly, reducin a canonical table with evidence is straightforwar Let $\{d,x\}$ be a set of observations (where d is discrete and $_{_{x}}$ is continuous). We instantiate d in a canonical table by 

![](images/1e62d61ea27eb7f2be3ea98bd09bd2e4c25ca134278c2153b2458dc406054d88.jpg) 
Figure 14.4 Summing and collapsing a Gaussian mixture. (a) Two Gaussian measures and the measure resulting from summing them. (b) The measure resulting from collapsing the same two measures into a single Gaussian. 

setting the entries which are not consistent with $^d$ to zero. We instantiate $_{_{x}}$ by instantiating every canonical form with this evidence, as in equation (14.6). 

Finally, consider the marginalization operation. Here, we have two very diferent cases: inte- grating out a continuous variable and summing out a discrete one. The operation of continuous marginalization (integration of of a continuous variable) is straightforward: we simply apply the operation of equation (14.5) to each of the canonical forms in our table. More precisely, assume at our canonica table consists of a set of canonic forms $\mathcal{C}\left(X,Y;K_{d},h_{d},g_{d}\right)$ , indexed by $^d$ . Now, for each d separately, we can integrate out Y $Y$ in the appropriate canonical form, as in equation (14.5). This results in a new canonical table $\mathcal{C}\left(X;K_{d}^{\prime},h_{d}^{\prime},g_{d}^{\prime}\right)$    , indexed by $^d$ . Clearly, this has the desired efect: Before the operation, we have a mixture, where each mixture is a function over a set of variables $X,Y$ ; after the operation, we have a mixture with the same set of components, but now each component only represents the function over the variables $X$ . The only important restriction, as we noted in the derivation of equation (14.5), that each of the matrices $K_{d,Y Y}$ be positive deﬁnite, so that the integral is well deﬁned. 

# 14.3.2.2 Weak Marginalization 

The task of discrete marginalization, however, is signiﬁcantly more complex. To understand the difculty, consider the following example: 

Assume that we have a canonical form $\phi(A,X)$ , for a binary-valued variable A and a continuous variable $X$ . Furthermore, assume that the two canonical forms in the table (associated with $a^{0}$ and $a^{1}$ ) are both weighted Gaussians: $\begin{array}{c c l}{{\phi(a^{0})}}&{{=}}&{{0.4\times\mathcal{N}\left(X\mid0;1\right)}}\\ {{\phi(a^{1})}}&{{=}}&{{0.6\times\mathcal{N}\left(X\mid3;4\right).}}\end{array}$ 

Figure 14.4a shows the two canonical forms, as well as the marginal distribution over $X$ . Clearly, this distribution is not a Gaussian; in fact, it cannot be represented at all as a canonical form. 

We see that the family of canonical tables is not closed under discrete marginalization: this operation takes a canonical table and produces something that is not representable in this form. 

We now have two alternatives. The ﬁrst is to enrich the family that we use for our represen- tation of factors. Speciﬁcally, we would have to use a table where, for each instantiations of the discrete variables, we have a mixture of canonical forms. In this case, the discrete marginaliza- tion operation is trivial: In example 14.4, we would have a single entry that contains the marginal distribution shown in ﬁgure 14.4a. While this family is closed under discrete marginalization, we have only resurrected our original problem: As discrete variables are “eliminated,” they simply induce more components in the mixture of canonical forms; the end result of this process is simply the original exponentially large mixture that we were trying to avoid. 

mixture collapsing M-projection 

# Proposition 14.2 

The second alternative is to approximate the result of the discrete marginalization operation. In our example, when marginalizing $D$ , we can approximate the resulting mixture of Gaussians by collapsing it into a single Gaussian, as shown in ﬁgure 14.4b. An appropriate approximation to use in this setting is the $M\cdot$ -projection operation introduced in deﬁnition 8.4. Here, we select the Gaussian distribution $\hat{p}$ that minimizes $D(p\|\hat{p})$ | | . In example 8.15 we provided a precise characterization of this operation: 

Let $p$ be an arbitrary distribution over $X_{1},\ldots,X_{k}$ . Let $\mu$ be the mean vector of $p.$ , and $\Sigma$ be the matrix of covariances in $p$ : 

$$
\begin{array}{r c l}{\mu_{i}}&{=}&{{\cal E}_{p}[X_{i}]}\\ {\Sigma_{i,j}}&{=}&{{\pmb C}o v_{p}[X_{i};X_{j}].}\end{array}
$$ 

Then the Gaussian distribution $\hat{p}=\mathcal{N}\left(\mu;\Sigma\right)$ N is the one that minimizes $D(p\|\hat{p})$ | | among all Gaus- sian distributions. 

Using this result, we have: 

Proposition 14.3 Let $p$ be the density function of a mixture of $k$ Gaussians $\begin{array}{r}{\{\langle w_{i},\mathcal{N}(\pmb{\mu}_{i};\Sigma_{i})\rangle\}_{i=1}^{k}\,f o r\sum_{i=1}^{k}w_{i}=1.}\end{array}$ P Let $q=\mathcal{N}\left(\boldsymbol{\mu};\boldsymbol{\Sigma}\right)$ be a Gaussian distribution deﬁned as: 

$$
\begin{array}{r c l}{{\pmb{\mu}}}&{{=}}&{{\displaystyle\sum_{i=1}^{k}w_{i}\pmb{\mu}_{i}}}\\ {{\Sigma}}&{{=}}&{{\displaystyle\sum_{i=1}^{k}w_{i}\Sigma_{i}+\sum_{i=1}^{k}w_{i}(\pmb{\mu}_{i}-\pmb{\mu})(\pmb{\mu}_{i}-\pmb{\mu})^{T}.}}\end{array}
$$ 

Then $q$ has the same ﬁrst two moments (means and covariances) as $p_{z}$ , and is therefore the Gaussian distribution that minimizes $D(p\|q)$ | | among all Gaussian distributions. 

The proof is left as an exercise (exercise 14.2). 

Note that the covariance matrix, as deﬁned by the collapsing operation, has two terms: one term is the weighted average of the covariance matrices of the mixture components; the second corresponds to the distances between the means of the mixture components — the larger these distances, the larger the “space” between the mixture components, and thus the larger the variances in the new covariance matrix. 

Example 14.5 Consider again the discrete marginalization problem in example 14.4. Using proposition 14.3, we have that the mean and variance of the optimal Gaussian approximation to the mixture are: 

$$
\begin{array}{r c l}{\mu}&{=}&{0.4\cdot0+0.6\cdot3=1.8}\\ {\sigma^{2}}&{=}&{(0.4\cdot1+0.6\cdot4)+(0.4\cdot(1.8-0)^{2}+0.6\cdot(3-1.8)^{2}=4.96.}\end{array}
$$ 

The resulting Gaussian approximation is shown in ﬁgure 14.4b. 

Clearly, when approximating a mixture of Gaussians by one Gaussian, the quality of the approximation depends on how close the mixture density is to a single multivariate Gaussian. When the Gaussians are very diferent, the approximation can be quite bad. Using these tools, we can deﬁne the discrete marginalization operation. 

# Deﬁnition 14.3 

weak marginalization Assume we have a canonical table deﬁne $\{A,B,X\}$ , where $A,B\subseteq\Delta$ and $X\subseteq\Gamma$ . Its weak marginal is a canonical table over $A,X$ , deﬁned as follows: For every value $a\in\mathit{V a l}(A)$ , we select the table entries consistent with $^{a}$ and sum them together to obtain a single table entry. The summation operation uses the collapsing operation of proposition 14.3. 

One problem with this deﬁnition is that the collapsing operation was deﬁned in proposition 14.3 only for a mixture of Gaussians and not for a mixture of general canonical forms. Indeed, the operation of combining canonical forms is well deﬁned if and only if the canonical forms have ﬁnite ﬁrst two moments, which is the case only if they can be represented as Gaussians. This restriction places a constraint on our inference algorithm: we can marginalize a discrete variable only when the associated canonical forms represent Gaussians. We will return to this point. 

# 14.3.3 EP for CLG Networks 

The previous section described the basic data structure that we can use to encode factors in a hybrid Gaussian network, and the basic factor operations needed to manipulate them. Most important was the deﬁnition of the weak marginalization operation, which approximates a mixture of Gaussians as a single Gaussian, using the concept of M-projection. 

Gaussian EP 

With our deﬁnition of weak marginalization and other operations on canonical tables, we can now deﬁne a message passing algorithm based on the framework of the expectation propagation described in section 11.4. As a reminder, to perform a message passing step in the EP algorithm, a cluster multiplies all incoming messages, and then performs an approximate marginalization on the resulting product factor. This last step, which can be viewed abstractly as a two-step process — exact marginalization followed by M-projection — is generally performed in a single approximate marginalization step. For example, in section 11.4.2, the approximate marginals were computed by calibrating a cluster graph (or clique tree) and then extracting from it a set of required marginals. 

To apply EP in our setting, we need only to deﬁne the implementation of the M-projection operation M-Project-Distr , as needed in line 1 of algorithm 11.5. This operation can be performed using the weak marginalization operation described in section 14.3.2.2, as shown in detail in algorithm 14.1. The marginalization step uses two types of operation. The continuous variables 

![](images/2640fd1103121c546c89391cf9a1ef5c0a03642251e855e5f0561f4bddf49777.jpg) 

are eliminated using the marginalization operation of equation (14.5) over each entry in the canonical table separately. The discrete variables are then summed out. For this last step, there are two cases. If the factor $\tau$ contains only discrete variables, then we use standard discrete marginalization. If not, then we use the weak marginalization operation of deﬁnition 14.3. 

In principle, this application of the EP framework is fairly straightforward. There are, however, two important subtleties that arise in this setting. 

# 14.3.3.1 Ordering Constraints 

First, as we discussed, for weak marginalization to be well deﬁned, the canonical form being marginalized needs to be a Gaussian distribution, and not merely a canonical form. In some cases, this requirement is satisﬁed simply because of the form of the potentials in the original network factorization. For example, in the Gaussian case, recall that our message passing process is well deﬁned if our distribution is pairwise normalizable. In the conditional Gaussian case, we can guarantee normalizability if for each cluster over scope $X$ , the initial factor in that cluster is a canonical table such that each canonical form entry $\mathcal{C}\left(X;K_{d},h_{d},g_{d}\right)$ is normalizable. Because normalizability is closed under factor product (because the sum of two PSD matrices is also PSD) and (both weak and strong) marginalization, this requirement guarantees us that all factors produced by a sum-product algorithm will be normalizable. However, this requirement is not always easy to satisfy: 

![](images/b90bc9d7777ad7e4f46f8a2834a2198c4002cab5621e4fef260eb1773f38ab84.jpg) 
Figure 14.5 Example of unnormalizable potentials in a CLG clique tree. (a) A simple CLG (b) A clique tree for it. 

In this network, with an appropriate message passing order, one can guarantee that canonical tables are normalizable at the appropriate time point. In particular, we can ﬁrst pass a message from $C_{1}$ to $C_{2}$ ; this message is a Gaussian obtained by weak marginalization of $p(A,X)$ onto $X$ . The resulting potential at $C_{2}$ is now a product of a legal Gaussian density over $X$ (derived from the incoming message) multiplied by $P(B)$ and the conditional linear Gaussian $p(Y\mid X,B)$ . The resulting distribution is a standard mixture of Gaussians, where each component in the mixture is normalizable. Thus, weak marginalization onto $Y$ can be performed, allowing the message passing process to continue. 

This example illustrates that we can sometimes ﬁnd a legal message passing order even in cases where the initial potentials are not normalizable. However, such a message passing order may not always exist. 

Example 14.7 

order-constrained message passing 

Consider the network in ﬁgure 14.6a. After moralization, the graph, shown in ﬁgure 14.6b, is already triangulated. If we now extract the maximum cliques and build the clique tree, we get the tree shown in ﬁgure 14.6c. Unfortunately, at this point, neither of the two leaves in this clique tree can send a message. For ample, the clique $\{B,X,Y\}$ contains the CPDs for $P(B)$ and $P(Y\mid B,X)$ , but not the CPD for X . Hence, the canonical forms over $\{X,Y\}$ represent linear Gaussian CPDs and not Gaussians. It follows that we cannot marginalize out B , and thus this clique cannot send a message. For similar reasons, the clique $\{A,C,Y,Z\}$ cannot send a message. Note, however, that a diferent clique tree can admit message passing. In particular, both the trees in (d) and (e) admit message passing using weak marginalization. 

As we can see, not every cluster graph allows message passing based on weak marginalization to take place. More formally, we say that a variable $X_{i}$ is order constrained at cluster $C_{k}$ if we require a normalizable probability distribution over $X_{i}$ at $C_{k}$ in order to send messages. In a cluster graph for a CLG Bayesian network, if $C_{k}$ requires weak marginalization, then any continuous variable $X_{i}$ in $C_{k}$ is order constrained. 

If $X_{i}$ is order constrained in $C_{k}$ , then we need to ensure that $X_{i}$ has a well-deﬁned distri- bution. In order for that to hold, $C_{k}$ must have obtained a valid probability distribution over $X_{i}$ ’s parents, whether from a factor within $C_{k}$ or from a message sent by another cluster. Now, consider a continuous parent $X_{j}$ of $X_{i}$ , and let $C_{l}$ be the cluster from which $C_{k}$ obtains the 

![](images/2a5d17b1c9a416768e3af678e700149dc20ee2a78f70e4d570787369095bde16.jpg) 
Figure 14.6 A simple CLG and possible clique trees with diferent correctness properties. (a) A simple CLG. (b) The moralized (and triangulated) graph for the CLG. (c) Clique tree derived the moralized graph in (b). (d) A clique tree with a strong root. (e) A clique tree without a strong root that allows message passing using weak marginalization. 

distribution over $X_{j}$ . (In the ﬁrst case, we have $k=l.$ .) In order for $X_{j}$ to have a well-deﬁned distribution in $C_{l}$ , we must have that $X_{j}$ is order constrained in $C_{l}$ . This process continues until the roots of the networks are reached. 

Gaussian normalizability 

# 

In the context of Markov networks, the situation is somewhat less complex. The use of a global partition function allows us to specify models where individual factors are all normalizable, ensuring a legal measure. In particular, extending deﬁnition 7.3, we can require that all of the entries in all of the canonical tables parameterizing the network be normalizable . In this case, the factors in all clusters in the network can be normalized to produce valid distributions, avoiding any constraints on message passing. Of course, the factor normalizability constraint is only a sufcient condition, in that there are models that do not satisfy this constraint and yet allow weak marginalization to take place, if messages are carefully ordered. 

As this discussion shows, the constraint on normalizability of the Gaussian in the context of weak marginalization can impose signiﬁcant constraints on the structure of the cluster graph and/or the order of the message passing. 

# 14.3.3.2 Ill-Deﬁned Messages 

belief-update The second subtlety arises when we apply the belief-update message passing algorithm rather than sum product. Recall that the belief update algorithm has important advantages, in that it gradually tunes the approximation to the more relevant regions in the probability space. 

the result of weak marginalization own in ﬁgure 14.4b. Now, assume that $p(Y\mid x,b^{1})=$ ${\mathcal{N}}\left(Y\mid x;1\right)$ , and that we observe b $b\,=\,b^{1}$ and $Y\,=\,-2$ . The evidence $Y\,=\,-2$ is much more consistent with the left-hand (mean 0 ) component in the mixture, which is the one derived from $p(X\mid a^{0})$ . Given this evidence, the exact posterior over $X$ would give much higher probability to the a $a^{0}$ component in the mixture. However, our Gaussian approximation was the M-projection of a mixture where this component had its prior weight of 0 . 4 . To obtain a better approximation, we would want to construct a new M-projection in $C_{1}$ that gives more weight to the ${\mathcal{N}}\left(X\mid0;1\right)$ component in the mixture when collapsing the two Gaussians. 

This issue is precisely the motivation that we used in section 11.4.3.2 for the belief update algo- rithm. However, the use of division in the Gaussian EP algorithm can create unexpected complications, in that the messages passed can represent unnormalizable densities, with negative variance terms; these can lead, in turn, to unnormalizable densities in the clusters, and thereby to nonsensical results. 

Example 14.9 Consider again the CLG in ﬁgure 14.5a, but where we now assume that the CPDs of the discrete variables are uniform and the CPDs of the continuous nodes are deﬁned as follows: 

$$
\begin{array}{r l r}{p(X\mid A)}&{=}&{\left\{\begin{array}{l l}{{\mathcal N}\left(0;2\right)\quad A=a^{0}}\\ {{\mathcal N}\left(0;6\right)\quad A=a^{1}}\end{array}\right.}\\ {p(Y\mid B,X)}&{=}&{\left\{\begin{array}{l l}{{\mathcal N}\left(X;0.01\right)\quad B=b^{0}}\\ {{\mathcal N}\left(-X;0.01\right)\quad B=b^{1}.}\end{array}\right.}\end{array}
$$ 

Consider the execution of message passing on the clique tree of ﬁgure 14.5b, with the evidence $Y=4$ . To make the presentation easier to follow, we present some of the intermediate results in moments form (means and covariances) rather than canonical form. 

The message passing algorithm ﬁrst sends a mess e from the clique $\{A,X\}$ to the cliq $\{B,X,Y\}$ . Since the cliques share only the variable X , we collapse the prior distribution of X using proposition 14.3 and get the message $\delta_{1\rightarrow2}\,=\mathcal{N}\left(X\mid0;4\right)$ . This message is stored in the sepset at $\mu_{1,2}$ . We also multiply the potential of $\{B,X,Y\}$ by this message, getting a mixture of two Gaussians with equal weights: 

$$
\begin{array}{r c l}{{\beta_{2}(b^{0})}}&{{=}}&{{\mathcal{N}\,\bigg(X,Y\mid\left(\begin{array}{c}{{0}}\\ {{0}}\end{array}\right);\left[\begin{array}{c c}{{4}}&{{4}}\\ {{4}}&{{4.01}}\end{array}\right]\bigg)}}\\ {{\beta_{2}(b^{1})}}&{{=}}&{{\mathcal{N}\,\bigg(X,Y\mid\left(\begin{array}{c}{{0}}\\ {{0}}\end{array}\right);\left[\begin{array}{c c}{{4}}&{{-4}}\\ {{-4}}&{{4.01}}\end{array}\right]\bigg)\,.}}\end{array}
$$ 

After instantiating the evidence $Y=4$ we get a new mixture of two Gaussians: 

$$
\begin{array}{c c l}{{\beta_{2}(b^{0})}}&{{=}}&{{{\mathcal N}\left(X\mid{3.99;0.00998}\right)}}\\ {{\beta_{2}(b^{1})}}&{{=}}&{{{\mathcal N}\left(X\mid-{3.99;0.00998}\right).}}\end{array}
$$ 

Note hat, in the original mixt $Y$ has the same marginal — $\mathcal{N}\left(Y\mid0;4.01\right)$ — for both $b^{0}$ and b $b^{1}$ . Therefore, the evidence $Y=4$ has the same likelihood in both cases, so that the posterior weights of the two cases are still the same as each other. 

We now need to send a message back to the clique $\{A,X\}$ . To do so, we collapse the two Gaussians, resulting in a mess e $\delta_{2\rightarrow1}\,=\,\mathcal{N}\left(X\mid0;15.93\right)$ . Note that, in this example, the evidence causes the variance of X to increase. 

To incorporate the message $\delta_{2\to1}$ into the clique $\{A,X\}$ , we divide it by the mess $\mu_{1,2}$ and multiply each entry in $\beta_{1}(A,X)$ by the resulting quotient $\frac{\delta_{2\to1}}{\mu_{1,2}}$ . In particular, for $A\,=\,a^{1}$ , we perform the operation: 

$$
\frac{\mathcal{N}\left(0;6\right)\cdot\mathcal{N}\left(0;15.93\right)}{\mathcal{N}\left(0;4\right)}.
$$ 

This operation can carried out using the canonical form $\mathcal{C}\left(K,h,g\right)$ . Con peration over the coefcient K , which represents the inverse of the covariance matrix: K $K\,=\,\Sigma^{-1}$ . In our case, the Gaussians are all one-dimensional, so $\textstyle K={\frac{1}{\sigma^{2}}}$ . As in equation (14.2) and equation (14.3), the product and division operation reduces to addition and subtraction of the coefcient $K$ . Thus, the $K$ for the resulting potential is: 

$$
\frac{1}{6}+\frac{1}{15.93}-\frac{1}{4}=-0.0206<0!
$$ 

However, $K<0$ does not represent a legal Gaussian: it corresponds to $\textstyle\sigma^{2}={\frac{1}{-0.0206}}$ , which is not − a legal variance. 

In practice, this type of situation does occur, but not often. Therefore, despite this complica- tion, the belief-update variant of the EP algorithm is often used in practice. 

# 14.3.4 An “Exact” CLG Algorithm $\star$ 

Lauritzen’s algorithm 

There is one case where we can guarantee that the belief update algorithm is not only well deﬁned but even returns “exact” answers. Of course, truly exact answers are not generally possible in a CLG network. Recall that a CLG distribution is a mixture of possibly exponentially many Gaussian hypotheses. The marginal distribution over a single variable can similarly be an exponentially large mixture (as in ﬁgure 14.2). Thus, for a query regarding the marginal distribution of a single continuous variable, even representing the answer might be intractable. Lauritzen’s algorithm provides a compromise between correctness and computational efciency. It is exact for queries involving discrete variables, and it provides the exact ﬁrst and second moments — means and (co)variances — for the continuous variables. More precisely, for a query $p(D,X\mid e)$ for $D\subseteq\Delta$ and $X\subseteq\Gamma$ , L ritzen’s algorithm returns an answer $\hat{p}$ such that ${\hat{p}}(D)=p(D\mid e)$ | is correct, and for each d , ${\hat{p}}(X\mid d)$ | has the correct ﬁrst and second moments. For many applications, queries of this type are sufcient. 

The algorithm is a modiﬁcation of the clique tree algorithm for discrete networks. It uses precisely the same type of message passing that we described, but over a highly restricted clique tree data structure. As we will show, these restrictions can (and do) cause signiﬁcant blowup in the size of the clique tree (much larger than the induced width of the network) and hence do not violate the $\mathcal{N P}$ -hardness of inference in these graphs. Indeed, these restrictions restrict the algorithm’s usefulness to a fairly narrow class of problems, and they render it primarily of theoretical interest. 

# 14.3.4.1 Strongly Rooted Trees 

The ordering constraint we employ on the clique tree, as described in section 14.3.3.1, guarantees that we can calibrate the clique tree without requiring weak marginalization on the upward pass. 

That is, the clique tree has a particular clique that is a strong root ; when we pass messages from the leaves of the clique tree toward this root, no weak marginalization is required. 

This property has several implications. First, weak marginalization is the only operation that requires that the clique potential be normalizable. In the upward pass (from the leaves toward the root), no such marginalization is required, and so we need not worry about this constraint. Intuitively, in the downward pass, each clique has already obtained all of the necessary information to deﬁne a probability distribution over its scope. This distribution allows weak marginalization to be performed in a well-deﬁned way. Second, because weak marginalization is the only operation that involves approximation, this requirement guarantees that the message passing in the upward pass is exact. As we will discuss, this property sufces to guarantee that the weak marginals in the downward pass are all legal. Indeed, as we will show, this property also allows us to guarantee that these marginals all possess the correct ﬁrst and second moments (means and covariances). 

When does our clique tree structure guarantee that no weak marginalization has to take place? Consider two cliques $C_{1}$ and $C_{2}$ , such that $C_{2}$ is the upward neighbor of $C_{1}$ . There are two cases. Either no marginalization of any discrete variable takes place between $C_{1}$ and $C_{2}$ , or it does. In the ﬁrst case, only continuous variables are eliminated, so that $C_{2}-C_{1}\subseteq\Gamma$ . In the second case, in order to avoid weak marginalization, we must avoid collapsing any Gaussians. Thus, we must have that the message from $C_{2}$ to $C_{1}$ contains no continuous variables, so that $C_{1}\cap C_{2}\subseteq\Delta$ . Overall, we have: 

A clique $C_{r}$ in a clique tree $\mathcal{T}$ is $^a$ strong root if for every clique $C_{1}$ and its upward neighbor $C_{2}$ , we have that $C_{2}-C_{1}\subseteq\Gamma$ or that $C_{1}\cap C_{2}\subseteq\Delta$ . A clique tree is called strongly rooted if it has a strong root. 

ﬁgure 14.6d shows a strongly rooted clique tree for the network of ﬁgure 14.6a. Here, the strong root is the clique $\{A,B,C,Y\}$ . For both of the two nonroot cliques, we have that no discrete variables are marginalized between them and their upward neighbor $\{A,B,C,Y\}$ , so that the ﬁrst condition of the deﬁnition holds. 

If we now apply the message passing procedure of algorithm 14.1, we note that the weak marginalization can only occur in the downward pass. In the downward pass, $C_{i}$ has received messages from all of its neighbors, and therefore $\beta_{i}(C_{i})$ represents a probability distribution over $C_{i}$ . Hence, $\tau(A,B,X)$ is a mixture of Gaussians over $X$ , so that the weak marginalization operation is well deﬁned. 

# 14.3.4.2 Strong Roots and Correctness 

So far, we have shown that the strongly rooted requirement ensures that the operations in the clique tree are well deﬁned, speciﬁcally, that no collapsing is performed unless the canonical forms represent Gaussians. However, as we have already shown, this condition is not necessary for the message passing to be well deﬁned; for example, the clique tree of ﬁgure 14.6e is not strongly rooted, but allows weak marginalization. However, as we now show, there are other reasons for using strongly rooted trees for inference in hybrid networks. Speciﬁcally, the presence of a strong root ensures not only that message passing is well deﬁned, but also that message passing leads to exact results, in the sense that we described. 

Let $\mathcal{T}$ be a clique tree and $C_{r}$ be a strong root in $\mathcal{T}$ . After instantiating the evidence and run ng CTree-BU-Calibrate using EP-Message with CLG-M-Project-Distr for message passing, the tree T $\mathcal{T}$ calibrated and every potential contains the correct (weak) marginal. In particular, every clique C contains the correct probability distribution e discrete variables $C\cap\Delta$ and the correct mean and covariances of the continuous variables $C\cap\Gamma$ ∩ . 

Proof Consider the steps in the algorithm CTree-BU-Calibrate . The clique initialization step is exact: each CPD is multiplied into a clique that contains all of its variables, and the product operation for canonical tables is exact. Similarly, evidence instantiation is also exact. It remains only to show that the message passing phase is exact. 

The upward pass is simple — as we discussed, all of the marginalization operations involved are strong margin aliz at ions, and therefore all the operations are exact. Thus, the upward pass is equivalent to running the variable elimination algorithm for the variables in the strong root, and its correctness follows from the correctness of the variable elimination algorithm. The result of the upward pass is the correct (strong) marginal in the strong root $C_{r}$ . 

The downward pass involves weak marginalization, and therefore, it will generally not result in the correct distribution. We wish to show that the resulting distributions in the cliques are the correct weak marginals. The proof is by induction on the distance of the clique from the strong root $C_{r}$ . The base case is the root clique itself, where we have already shown that we have exactly the correct marginal. Now, assume now that we have two cliques $C_{i}$ and $C_{j}$ such that $C_{i}$ is the upward neighbor of $C_{j}$ . By the inductive hypothesis, after $C_{i}$ receives the message from its upward neighbor, it has the correct weak marginals. We need to show that, after $C_{j}$ receives the message from $C_{i}$ , it also has the correct weak marginal. 

Let $\beta_{j}$ and $\beta_{j}^{\prime}$ denote the potential of $C_{j}$ before and after $C_{i}$ sends the downward message to $C_{j}$ . Let $\mu_{i,j}$ denote the sepset message before the downward message, and $\delta_{i\to j}$ denote the actual message sent. Note that $\delta_{i\to j}$ is the (weak) marginal of the clique potential $\beta_{i}$ , and is therefore the correct weak marginal, by the inductive hypothesis. 

We ﬁrst show that, after the message is sent, $C_{j}$ agrees with $C_{i}$ on the marginal distribution of the sepset $\boldsymbol{S}_{i,j}$ . 

$$
\sum_{C_{j}-S_{i,j}}\beta_{j}^{\prime}=\sum_{C_{j}-S_{i,j}}\beta_{j}\cdot\frac{\delta_{i\rightarrow j}}{\mu_{i,j}}\quad=\frac{\delta_{i\rightarrow j}}{\mu_{i,j}}\cdot\sum_{C_{j}-S_{i,j}}\beta_{j}\quad=\frac{\delta_{i\rightarrow j}}{\mu_{i,j}}\cdot\mu_{i,j}=\delta_{i\rightarrow j},
$$ 

where the marginalization $\sum_{C_{j}-S_{i,j}}$ also denotes integration, when appropriate. This deriva- tion is correct because this ginalization $\sum_{C_{j}-S_{i,j}}$ is an exact operation: By the strong root − property, all margin aliz at ions toward the strong root (that is, from $j$ to $i_{.}$ ) are strong marginaliza- tions. Thus, the marginal of $C_{j}$ after the message is the same as the (weak) marginal of $C_{i}$ , and the two cliques are calibrated. Because the (weak) marginal of $C_{i}$ is correct, so is the marginal of $C_{j}$ . Note that this property does not hold in a tree that is not strongly rooted. In general, in such a clique tree, $\begin{array}{r}{\mu_{i,j}\neq\sum_{C_{j}-S_{i,j}}\beta_{j}}\end{array}$ . 

It remains to show that $\beta_{j}^{\prime}$ is the correct weak marginal for all the variables in $C_{j}$ . As shown in exercise 10.5, the premessage potential $\beta_{j}$ already encodes the correct posterior conditional distribution $P(C_{j}\mid S_{i,j})$ . (We use $P$ to denote the posterior, after conditioning on the evidence.) In other words, letting $X=C_{j}-S_{i,j}$ and $\boldsymbol{S}=\boldsymbol{S}_{i,j}$ , we have that: 

$$
{\frac{\beta_{j}(X,S)}{\beta_{j}(S)}}=P(X\mid S).
$$ 

Now, since the last message along the edge $C_{i}{-}C_{j}$ was sent from $C_{j}$ to $C_{i}$ , we have that 

$$
{\frac{\beta_{j}(X,S)}{\beta_{j}(S)}}={\frac{\beta_{j}(X,S)}{\mu_{i,j}(S)}}.
$$ 

We therefore have that the potential of $C_{j}$ after the message from $C_{i}$ is: 

$$
\beta_{j}^{\prime}={\frac{\beta_{j}\delta_{i\rightarrow j}}{\mu_{i,j}}}=\delta_{i\rightarrow j}P(X\mid S).
$$ 

Thus, every entry in $\beta_{j}^{\prime}$ is a Gaussian, which is derived as a product of two terms: one is a Gaussian over $S$ that has the same ﬁrst and second moments as $P$ , and the second is the correct $P(X\mid S)$ . It follows easily that e resulting product $\beta_{j}(X,S)$ is a Gaussian that has the same ﬁrst and second moments as P (see exercise 14.5). This concludes the proof of the result. 

Note that, if the tree is not strongly rooted, the proof breaks down in two places: the upward pass is not exact, and equation (14.12) does not hold. Both of these arise from the fact that

 $\begin{array}{r}{\sum_{C_{j}-S_{i,j}}\beta_{j}}\end{array}$ is the exact marginal, whereas, if the tree is not strongly rooted, $\mu_{i,j}$ is computed

 (in some cliques) using weak marginalization. Thus, the two are not, in general, equal. As a consequence, although the weak marginal of $\beta_{j}$ is $\mu_{i,j}$ , the second equality fails in the derivation: the weak marginal of a product $\beta_{j}\cdot\frac{\delta_{i\rightarrow j}}{\mu_{i,j}}$ is not generally equal to the product of $\frac{\delta_{i\to j}}{\mu_{i,j}}$ and the weak marginal of $\beta_{j}$ . Thus, the strong root property is essential for the strong correctness properties of this algorithm. 

# 14.3.4.3 Strong Triangulation and Complexity 

constrained elimination ordering 

Deﬁnition 14.5 We see that strongly rooted trees are necessary for the correct execution of the clique tree algorithm in CLG networks. Thus, we next consider the task of constructing a strongly rooted tree for a given network. As we discussed, one of our main motivations for the strong root requirement was the ability to perform the upward pass of the clique tree algorithm without weak marginalization. Intuitively, the requirement that discrete variables not be eliminated before their continuous neighbors implies a constraint on the elimination ordering within the clique tree. Unfortunately, constrained elimination orderings can give rise to clique trees that are much larger — exponentially larger — than the optimal, unconstrained clique tree for the same network. We can analyze the implications of the strong triangulation constraint in terms of the network structure. 

Let $\mathcal{G}$ be a hy A continuous connected compon in $\mathcal{G}$ set of variables $X\subseteq\Gamma$ hat: if $X_{1},X_{2}\in X$ ∈ then there exists a path between $X_{1}$ and $X_{2}$ in the moralized graph $\mathcal{M}[\mathcal{G}]$ M G such that all the nodes on the path are in Γ . A continuous connected component is maximal if it is not a proper subset of any larger continuous connected component. The discrete neighbors of a continuous connected component $X$ are all the discrete variables that are adjacent to some node $X\in X$ in the moralized graph $\mathcal{M}[\mathcal{G}]$ . 

For example, in the network ﬁgure 14.6a, all of the continuous variables $\{X,Y,Z\}$ are in a single continuous connected component, and all the discrete variables are its neighbors. 

We can understand the implications of the strong triangulation requirement in terms of continuous connected components: 

Let $\mathcal{G}$ be a hybrid network, and le $\mathcal{T}$ b a stron rooted clique tree for $\mathcal{G}$ Then for any maximal continuous connected component X G , with D its d eighbors, T $\mathcal{T}$ includes a clique that contains (at least) all of the nodes in D and some node $X\in X$ . 

The proof is left as an exercise (exercise 14.6). 

In the CLG of ﬁgure 14.6a, all of the continuous variables are in one connected component, and all of the discrete variables are its neighbors. Thus, a strongly rooted tree must have a clique that contains all of the discrete variables and one of the continuous ones. Indeed, the strongly rooted clique tree of ﬁgure $14.6\mathrm{d}$ contains such a clique — the clique $\{A,B,C,Y\}$ . 

This analysis allows us to examine a CLG network, and immediately conclude lower bounds on the computational complexity of clique tree inference in that network. For example, the polytree CLG in ﬁgure 14.2 has a continuous connected component containing all of the continuous variables $\{X_{1},.\,.\,.\,,X_{n}\}$ , which has all of the discrete variables as neighbors. Thus, any strongly rooted clique tree for this network necessarily has an exponentially large clique, which is as we would expect, given that this network is the basis for our $\mathcal{N P}$ -hardness theorem. 

Because many CLG networks have large continuous connected components that are adjacent to many discrete variables, a strongly rooted clique tree is often far too large to be useful. However, the algorithm presented in this section is of conceptual interest, since it clearly illustrates when the EP message passing can lead to inaccurate or even nonsensical answers. 

# 14.4 Nonlinear Dependencies 

In the previous sections, we dealt with a very narrow class of continuous networks: those where all of the CPDs are parameterized as linear Gaussians. Unfortunately, this class of networks is inadequate as a model for many practical applications, even those involving only continuous variables. For example, as we discussed, in modeling the car’s position as a function of its previous position and its velocity (as in example 5.20), rather than assume that the variance in the car’s position is constant, it might be more reasonable to assume that the variance of the car is larger if the velocity is large. This dependence is nonlinear, and it cannot be accommodated within the framework of linear Gaussians. In this section, we relax the assumption of linearity and present one approach for dealing with continuous networks that include nonlinear dependencies. We note that the techniques in this section can be combined with the ones described in section 14.3 to allow our algorithms to extend to networks that allow discrete variables to depend on continuous parents. 

Once again, the standard solution in this setting can be viewed as an instance of the general expectation propagation framework described in section 11.4.4, and used before in the context of the CLG models. Since we cannot tractably represent and manipulate the exact factors in this setting, we need to use an approximation, by which intermediate factors in the computation are approximated using a compact parametric family. Once again, we choose the family of Gaussian measures as our representation. 

At a high level, the algorithm proceeds as follows. As in expectation propagation (EP), each cluster $C_{i}$ maintains its potentials in a nonexplicit form, as a factor set $\vec{\phi}_{i}$ ; some of these factors are from the initial factor set $\Phi$ , and others from the incoming messages into $C_{i}$ . Importantly, due to our use of a Gaussian representation for the EP factors, the messages are all Gaussian measures. 

linearization 

To pass a message from $C_{i}$ to $C_{j}$ , $C_{i}$ approximates the product of the factors in $\vec{\phi}_{i}$ as a Gaussian distribution, a process called linearization , for reasons we will explain. The resulting Gaussian distribution can then be marginalized onto $\boldsymbol{S}_{i,j}$ to produce the approximate cluster marginal $\tilde{\sigma}_{i\rightarrow j}$ . Essentially, the combination of the linearization step and the marginalization of → the resulting Gaussian over the sepset give rise to the weak marginalization operation that we described. 

The basic computational step in this algorithm is the linearization operation. We provide several options for performing this operation, and discuss their trade-ofs. We then describe in greater detail how this operation can be used within the EP algorithm, and the constraints that it imposes on its detailed implementation. 

# 14.4.1 Linearization 

We ﬁrst consider the basic computational task of approximating a distribution $p(X_{1},\cdot\cdot\,,X_{d})$ as a Gaussian distribution ${\hat{p}}(X)$ . For the purposes of this section, we assume that our distribution is deﬁned in terms of a Gaussian distribution $p_{0}\bigl(Z_{1},.\,.\,,Z_{l}\bigr)\,=\,\mathcal{N}\,(Z\mid\mu;\Sigma)$ and a set of deterministic functions $X_{i}=f_{i}(Z_{i})$ . Intuitively, the auxiliary Z variables encompass all of the stochasticity in the distribution, and the functions $f_{i}$ serve to convert these auxiliary variables into the variables of interest. For a vector of functions $\vec{f}=\mathbf{\Phi}(f_{1},.\,.\,.\,,f_{d})$ and a Gaussian distribution $p_{0}$ , we can now deﬁne $p(\boldsymbol{X},\boldsymbol{Z})$ to be the distribution that has $p(Z)=p_{0}(Z)$ and $X_{i}\,=\,f_{i}(Z)$ with probability 1. (Note that this distribution has a point mass at the discrete points where $X={\vec{f}}(Z)$ .) We use $p(X)=p_{0}(Z)\bigoplus[X={\vec{f}}(Z)]$ to refer to the marginal of this distribution over X . 

linearization 

Our goal is to compute a Gaussian distribution ${\hat{p}}(X)$ that approximates this marginal distri- bution $p(X)$ . We call the procedure of determining $\hat{p}$ from $p_{0}$ and $\bar{f}$ a linearization of $\bar{f}$ . We now describe the two main approaches that have been proposed for the linearization operation. 

# 14.4.1.1 Taylor Series Linearization 

As we know, if $p_{0}(Z)$ is a Gaussian distribution and $X\,=\,f(Z)$ is a linear function, then $p(X)=p(f(Z))$ is also a Gaussian distribution. Thus, one very simple and commonly used approach is to approximate $f$ as a linear function $\hat{f}$ , and then deﬁne $\hat{p}$ in terms of $\hat{f}$ . 

Taylor series 

Extended Kalman ﬁlter 

The most standard linear approximation for $f(Z)$ is the Taylor series expansion around the mean of $p_{\mathrm{0}}(Z)$ : 

$$
\hat{f}(Z)=f(\mu)+\left.\nabla f\right|_{\mu}Z.
$$ 

The Taylor series is used as the basis for the famous extended Kalman ﬁlter (see section 15.4.1.2). Although the Taylor series expansion provides us with the optimal linear approximation to $f$ , the Gaussian ${\hat{p}}(X)\,=\,p_{0}(Z)\bigoplus{\hat{f}}(Z)$ L may not be the optimal Gaussian approximation to $p(X)=p_{0}(Z)\bigoplus f(Z)$ 

Example 14.11 Consider the function $X=Z^{2}$ , and as me that $p(Z)=\mathcal{N}\left(Z\mid0;1\right)$ . The mean of $X$ is simply ${\pmb E}_{p}[X]={\pmb E}_{p}\big[Z^{2}\big]=1$ . The variance of X is 

$$
\pmb{W a r}_{p}[X]=\pmb{E}_{p}\big[X^{2}\big]-\pmb{E}_{p}[X]^{2}=\pmb{E}_{p}\big[Z^{4}\big]-\pmb{E}_{p}\big[Z^{2}\big]^{2}=3-1^{2}=2.
$$ 

On the other hand, the ﬁrst-order Taylor series approximation of $f$ at the mean value $Z=0$ is: 

$$
\hat{f}(Z)=0^{2}+(2Z)_{U=0}Z\equiv0.
$$ 

Thus, ${\hat{p}}(X)$ will simply be a delta function where all the mass is located at $X=0$ , a very poor approximation to $p$ . 

This example illustrates a limitation of this simple approach. In general, the quality of the Taylor series approximation depends on how well $\hat{f}$ approximates $f$ in the neighborhood of the mean of $Z$ , where the size of the neighborhood is determined by the variance of $p_{\mathrm{0}}(Z)$ . The approximation is good only if the linear term in the Taylor expansion of $f$ dominates in this neighborhood, and the higher-order terms are small. In many practical situations, this is not the case, for example, when $f$ changes very rapidly relative to the variance of $p_{\mathrm{0}}(Z)$ . In this case, using the simple Taylor series approach can lead to a very poor approximation. 

14.4.1.2 M-Projection Using Numerical Integration 

M-projection The Taylor series approach uses what may be considered an indirect approach to approximating $p\cdot$ we ﬁrst simplify the nonlinear function $f$ and only then compute the resulting distribution. Alternatively, we can directly approximate $p$ using a Gaussian distribution $\hat{p}_{i}$ , by using the $M\cdot$ - projection operation introduced in deﬁnition 8.4. Here, we select the Gaussian distribution $\hat{p}$ that minimizes $D(p\|\hat{p})$ . 

In proposition 14.2 we provided a precise characterization of this operation. In particular, we showed that we can obtain the M-projection of $p$ by evaluating the following set of integrals, corresponding to the moments of $p$ : 

$$
\begin{array}{r c l}{{E_{p}[X_{i}]}}&{{=}}&{{\displaystyle\int_{-\infty}^{\infty}f_{i}(z)p_{0}(z)d z}}\\ {{E_{p}[X_{i}X_{j}]}}&{{=}}&{{\displaystyle\int_{-\infty}^{\infty}f_{i}(z)f_{j}(z)p_{0}(z)d z.}}\end{array}
$$ 

From these moments, we can derive the mean and covariance matrix for $p$ , which gives us precisely the M-projection. Thus, the M-projection task reduces to one of computing the expectation of some function $f$ (which may be $f_{i}$ or a product $f_{i}f_{j})$ relative to our distribution $p_{0}$ . Before we discuss the solution of these integrals, it is important to inject a note of caution. Even if $p$ is a valid density, its moments may be inﬁnite, preventing it from being approximated by a Gaussian. 

In some cases, it is possible to solve the integrals in closed form, leading to an efcient and optimal way of computing the best Gaussian approximation. For instance, in the case of numerical integration 

Gaussian quadrature 

integration rule precision 

example 14.11, equation (14.14) reduces to computing $E_{p}\big[Z^{2}\big]$   , where $p$ is $\mathcal{N}\left(0;1\right)$ , an integral that can be easily solved in closed form. Unfortunately, for many functions $f$ , these integrals have no closed-form solutions. However, because our goal is simply to estimate these quantities, we can use numerical integration methods. There are many such methods, with various trade-ofs. In our setting, we can exploit the fact that our task is to integrate the product of a function and a Gaussian. Two methods that are particularly efective in this setting are described in the following subsections. 

Gaussian Quadrature Gaussian quadrature is a method that was developed for the case of one-dimensional integrals. It approximates integrals of the form $\textstyle\int_{a}^{b}W(z){\bar{f}}(z)d z$ where $W(z)$ is a known nonnegative function (in our case a Gaussian). Based on the function $W$ , we choose $m$ points $z_{1},\dots,z_{m}$ and $m$ weights $w_{1},.\,.\,.\,,w_{m}$ and approximate the integral as: 

$$
\int_{a}^{b}W(z)f(z)d z\approx\sum_{j=1}^{m}w_{j}f(z_{j}).
$$ 

The points and weights are chosen such that the integral is exact if $f$ is a polynomial of degree $2m-1$ or less. Such rules are said to have precision $2m-1$ . 

To understand this construction, assume that we have chosen $m$ points $z_{1},\dots,z_{m}$ and $m$ weights $w_{1},\dots,w_{m}$ so that equation (14.16) holds with equality for any monomial $z^{i}$ for $i=$ $1,.\ldots,2m-1$ . Now, consider any polynomial of degree at most $2m-1$ : $\begin{array}{r}{f(z)=\sum_{i=0}^{2m-1}\alpha_{i}z^{i}}\end{array}$ . For such an f , we can show (exercise 14.8) that: 

$$
\int_{a}^{b}W(z)f(z)d z=\sum_{j=1}^{m}w_{j}z_{j}^{i}.
$$ 

Thus, if our points are exact for any monomial of degree up to $2m-1$ , it is also exact for any polynomial of this degree. 

Example 14.12 Consider the case of $m=2$ . In order for the rule to be exact for $f_{0},\ldots,f_{3}$ , it must be the case that for $i=0,\dots,3$ we have 

$$
\int_{a}^{b}W(z)f_{i}(z)d z=w_{1}f_{i}(z_{1})+w_{2}f_{i}(z_{2}).
$$ 

Assuming that $W(z)=\mathcal{N}\left(0;1\right)$ , $a=-\infty$ , and $b=\infty$ , we get the following set of four nonlinear equations 

$$
\begin{array}{r c l}{{w_{1}+w_{2}}}&{{=}}&{{\displaystyle\int_{-\infty}^{\infty}\mathcal{N}\left(z\mid0;1\right)d z=1}}\\ {{}}&{{}}&{{}}\\ {{w_{1}z_{1}+w_{2}z_{2}}}&{{=}}&{{\displaystyle\int_{-\infty}^{\infty}\mathcal{N}\left(z\mid0;1\right)z\;d z=0}}\\ {{}}&{{}}&{{}}\\ {{w_{1}z_{1}^{2}+w_{2}z_{2}^{2}}}&{{=}}&{{\displaystyle\int_{-\infty}^{\infty}\mathcal{N}\left(z\mid0;1\right)z^{2}\;d z=1}}\\ {{}}&{{}}&{{}}\\ {{w_{1}z_{1}^{3}+w_{2}z_{2}^{3}}}&{{=}}&{{\displaystyle\int_{-\infty}^{\infty}\mathcal{N}\left(z\mid0;1\right)z^{3}\;d z=0}}\end{array}
$$ 

The solution for these equations (up to swapping $z_{1}$ and $z_{2}$ ) is $w_{1}=w_{2}=0.5$ , $z_{1}=-1$ , $z_{2}=1$ . This solution gives rise to the following approximation, which we apply to any function $f$ : 

$$
\int_{-\infty}^{\infty}\mathcal{N}\left(0;1\right)f(z)d z\approx0.5f(-1)+0.5f(1).
$$ 

This approximation is exact for any polynomial of degree 3 or less, and approximate for other func- tions. The error in the approximation depends on the extent to which $f$ can be well approximated by a polynomial of degree 3. 

This basic idea generalizes to larger values of $m$ . 

Now, consider the more complex task of integrating a multidimensional function $f(Z_{1},.\,.\,.\,,Z_{d})$ One approach is to use the Gaussian quadrature grid in each dimension, giving rise to a $d$ - dimensional grid with $m^{d}$ points. We can then evaluate the function at each of the grid points, and combine the evaluations together using the appropriate weights. Viewed slightly diferently, this approach computes the $d$ -dimensional integral recursively, computing, for each point in dimension $i$ , the Gaussian-quadrature approximation of the integral up to dimension $i-1$ . This integration rule is accurate for any polynomial which is a sum of monomial terms, each of the form $\textstyle\prod_{i=1}^{d}z_{i}^{a_{i}}$ , where each $a_{i}\leq2m-1$ . Unfortunately, this grid grows exponentially with $d$ , which can be prohibitive in certain applications. 

unscented transformation exact monomials Unscented Transformation An alternative approach, called the unscented transformation , is based on the integration method of exact monomials . This approach uses grids designed specif- ically for Gaussians over $I\!\!R^{d}$ . Intuitively, it uses the symmetry of the Gaussian around its axes to reduce the density of the required grid. 

The simplest instance of the exact monomials framework uses $2d+1$ points, as compared to the $2^{d}$ points required for the rule derived from Gaussian quadrature. To apply this transforma- tion, it helps to assume that $p_{o}(Z)$ is a standard Gaussian $p_{0}(Z)=\mathcal{N}\left(Z\mid\mathbf{0};I\right)$ where $I$ is the identity matrix. In cases where $p_{0}$ is not of that form, s that $p_{0}(Z)=\mathcal{N}\left(Z\mid\mu;\Sigma\right)$ , e can do the following change of variable transformation: Let A be the matrix square root of Σ , that is, $A$ is a $d\times d$ matrix such that $\Sigma=A^{T}A$ . We deﬁne $\tilde{p}_{0}(Z^{\prime})=\mathcal{N}\left(Z^{\prime}\mid\mathbf{0};I\right)$    . We can now show that 

$$
p_{0}(Z)=\tilde{p}_{0}(Z^{\prime})\bigoplus[Z=A Z^{\prime}+\mu].
$$ 

We can now perform a change of variables for each of our functions, deﬁning $\tilde{f}_{i}(Z)=f_{i}(A Z+$ $\mu)$ , and perform our moment computation relative to the functions $\tilde{f}_{i}$ rather than $f_{i}$ . 

Now, for $i=1,\ldots,d,$ , let $z_{i}^{+}$ be the point in $I\!\!R^{d}$ which has $z_{i}=+1$ and $z_{j}=0$ for all $j\neq i$ . Similarly, let $z_{i}^{-}=-z_{i}^{+}$ − . Let $\lambda\neq0$ be any number. We then use the following integration rule: 

$$
\int_{-\infty}^{\infty}W(z)f(z)d z\approx\left(1-\frac{d}{\lambda^{2}}\right)f(0)+\sum_{i=1}^{d}\frac{1}{2\lambda^{2}}f(\lambda z_{i}^{+})+\sum_{i=1}^{d}\frac{1}{2\lambda^{2}}f(\lambda z_{i}^{-}).
$$ 

In other words, we evaluate $f$ at the mean of the Gaussian, 0 , and then at every point which is $\pm\lambda$ away from the mean for one of the variables $Z_{i}$ . We then take a weighted average of these points, for appropriately chosen weights. Thus, this rule, like Gaussian quadrature, is deﬁned in terms of a set of points $z_{0},\dots,z_{2d}$ and weights $w_{0},\ldots,w_{2d}$ , so that 

$$
\int_{-\infty}^{\infty}W(z)f(z)d z=\sum_{i=0}^{2d}w_{i}f(z_{i}).
$$ 

unscented Kalman ﬁlter This integration rule is used as the basis for the unscented Kalman ﬁlter (see section 15.4.1.2). 

The method of exact monomials can be used to provide exact integration for all polynomials of egree $p$ or less, that is, to all polynomials where each monom l term $\textstyle\prod_{i=1}^{d}z_{i}^{a_{i}}$ has $\textstyle\sum_{i=1}^{d}a_{i}\leq$ ≤ $p$ . Therefore, the method of exact monomials has precision p . In particular, equation (14.18) provides us with a rule of precision 3. Similar rules exist that achieve higher precision. For example, we can obtain a method of precision 5 by evaluating $f$ at 0 , at the $2d$ points that are $\pm\lambda$ away from the mean along one dimension, and at the $2d(d-1)$ points t $\pm\lambda$ away from the mean along two dimensions. The total number of points is therefore $2d^{2}+1$ . 

Note that the precision-3 rule is less precise than the one obtained by using Gaussian quadra- ture separately for each dimension: For example, if we combine one-dimensional Gaussian quadrature rules of precision 2, we will get a rule that is also exact for monomials such as $z_{1}^{2}z_{2}^{2}$ (but not for the degree 3 monomial $z_{1}^{3}$ ). However, the number of grid points used in this method is exponentially lower. 

The parameter $\lambda$ is a free parameter. Every choice of $\lambda\neq0$ results i a rule of precision 3, but diferent choices lead to diferent approximations. Small values of λ lead to more local approximations, which are based on the behavior of $f$ near the mean of the Gaussian and are less afected by the higher order terms of $f$ . 

# 14.4.1.3 Discussion 

We have suggested several diferent methods for approximating $q$ as a Gaussian distribution. What are the trade-ofs between them? We begin with two examples. 

Figure 14.7(top) illustrates the two diferent approximations in comparison to the optimal approx- imation (the correct mean and covariance) obtained by sampling. We can see that the unscented transformation is almost exact, whereas the linearization method makes signiﬁcant errors in both mean and covariance. 

The bottom row provides a more quantitative analysis for the simple nonlinear function $Y=$ $\sqrt{(\sigma Z_{1})^{2}+(\sigma Z_{2})^{2}}$ . The left panel presents results for $\sigma\,=\,2$ , showing the optimal Gaussian M-projection and the approximations using three methods: Taylor series, exact monomials with precision 3, and exact monomials with precision 5. The “optimal” approximation is estimated using a very accurate Gaussian quadrature rule with a grid of $100\times100$ integration points. We can see that the precision-5 rule is very accurate, but even the precision-3 rule is signiﬁcantly more accurate than the Taylor series. The right panel shows the KL-divergence between the diferent approximations and the optimal approximation. We see that the quality of approximation of every method degrades as $\sigma$ increases. This behavior is to be expected, since all of the methods are accurate for low-order polynomials, and the larger the $\sigma$ , the larger the contribution of the higher-order terms. For small and medium variances, the Taylor series is the least exact of the three methods. For large variances, the precision 3 rule becomes signiﬁcantly less accurate. The reason is that for $\sigma~>~0.23$ , the covariance matrices returned by the numerical integration procedure 

![](images/98773f8511065f1c64c346ce68aece6c3151061ce3f21c66bd6d76cc70722bd1.jpg) 
Figure 14.7 Comparisons of diferent Gaussian approximations for a nonlinear dependency. The top row (adapted with permission from van der Merwe et al. (2000a)) illustrates the process of diferent approximation methods and the results they obtain; the function being linearized is a feed-forward neural network with random weights. The bottom row shows a more quantitative analysis for the function $f(Z_{1},Z_{2})=\sqrt{(\sigma Z_{1})^{2}+\breve{(}\sigma Z_{2})^{2}}$ p . The left panel shows the diferent approximations when $\sigma^{2}=4$ , and the right panel the KL-divergence from optimal approximation as a function of $\sigma^{2}$ . 

are illegal, and must be corrected. The correction produces reasonable answers for values of $\sigma$ up to $\sigma\,=\,4$ , and then degrades. However, it is important to note that, for high variances, the Gaussian approximation is a poor approximation to $p$ in any case, so that the whole approach of using a Gaussian approximation in inference breaks down. For low variances, where the Gaussian approximation is reasonable, even the corrected precision-3 rule signiﬁcantly dominates the Taylor series approach. 

From a computational perspective, the Gaussian quadrature method is the most precise, but also the most expensive. In practice, one would only apply it in cases where precision was of critical important and the dimension was very low. The cost of the other two methods depends on the function $f$ that we are trying to linearize. The linearization method (equation (14.13)) requires that we evaluate $f$ and each of $f$ ’s $d$ partial derivatives at the point 0 . In cases where the partial derivative functions can be written in closed form, this process requires only $d+1$ function evaluations. By contrast, the precision 3 method requires $2d+1$ evaluations of the function $f$ , and the precision 5 method requires $2d^{2}+1$ function evaluations of $f$ . In addition, to use the numerical integration methods we need to convert our distribution to the form of equation (14.17), which is not always required for the Taylor series linearization. Finally, one subtle problem can arise when using numerical integration to perform the M- projection operation: Here, the quantities of equation (14.14)–(14.15) are computed using an approximate procedure. Thus, although for exact integration, the covariance matrix deﬁned by these equations is guaranteed to be positive deﬁnite; this is not the case for the approximate quantities, where the approximation may give rise to a matrix that is not positive deﬁnite. See section 14.7 for references to a modiﬁed approach that avoids this problem. 

Putting computational costs aside, the requirement in the linearization approach of com-  puting the gradient may be a signiﬁcant issue in some settings. Some functions may not be diferentiable (for example, the max function), preventing the use of the Taylor series expansion. Furthermore, even if $f$ is diferentiable, computing its gradient may still be difcult. In some applications, $f$ might not even be given in a parametric closed form; rather, it might be implemented as a lookup table, or as a function in some program- ming language. In such cases, there is no simple way to compute the derivatives of the Taylor series expansion, but it is easy to evaluate $f$ on a given point, as required for the numerical integration approach. 

# 14.4.2 Expectation Propagation with Gaussian Approximation 

The preceding section developed the basic tool of approximating a distribution as a Gaussian. We now show how these methods can be used to perform expectation propagation message passing inference in nonlinear graphical models. Roughly speaking, at each step, we take all of the factors that need to be multiplied, and we approximate as a Gaussian the measure derived by multiplying all of these factors. We can then use this Gaussian distribution to form the desired message. 

For the application of this method, we assume that each of the factors in our original distribution $\Phi$ deﬁnes a conditional distribution over a single random variable in terms of others. This assumption certainly holds in the case of standard Bayesian networks, where all CPDs are (by deﬁnition) of this form. It is also the case for many Markov networks, especially in the continuous case. We further assume that each of the factors in $\Phi$ can be written as a deterministic function: 

$$
X_{i}=f_{i}(Y_{i},W_{i}),
$$ 

where $Y_{i}\,\subseteq\,\mathcal{X}$ are the model variables on which $X_{i}$ ’s CPD depends, and $W_{i}$ ar new” standard Gaussian random variables that capture all of the stochasticity in the CPD of $X_{i}$ i . We call these W ’s exogenous variables, since they capture stochasticity that is outside the model. (See also section 21.4.) 

Although there are certainly factors that do not satisfy these assumptions, this representational class is quite general, and encompasses many of the factors used in practical applications. Most obviously, using the transformation of equation (14.17), this representation encompasses any Gaussian distribution. However, it also allows us to represent many nonlinear dependencies: 

Example 14.14 Consider the nonlinear CPD $\begin{array}{r}{\overline{{X}}\sim\mathcal{N}\left(\sqrt{Y_{1}^{2}+Y_{2}^{2}};\sigma^{2}\right)}\end{array}$ p  . We can reformulate this CPD in terms of a deterministic, nonlinear function, as follows: We introduce a new exogenous variable $W$ that captures the stochasticity in the CPD. We then deﬁne $X=f(Y_{1},Y_{2},W)$ where $f(Y_{1},Y_{2},W)=$ $\sqrt{Y_{1}^{2}+Y_{2}^{2}}+\sigma W$ . 

In this example, as in many real-world CPDs, the dependence of $X$ on the stochastic variable $W$ is linear. However, the same idea also applies in cases where the variance is a more complex function: 

# Example 14.15 

Returning again to example 5.20, assume we want the variance of the vehicle’s position at time $t+1$ to depend on its time $t$ velocity, so that we have more uncertainty about the vehicle’s next position if it is moving faster. Thus, for example, we might want to encode a distribution $\mathcal{N}\left(X^{\prime}\mid X+V;\rho V^{2}\right)$    . We can do so by introducing an exogenous standard Gaussian variable $Z$ and deﬁning 

$$
X^{\prime}=X+V+{\sqrt{\rho}}V Z.
$$ 

It is not difcult to verify that $X^{\prime}$ has the appropriate Gaussian distribution. 

We now apply the EP framework of section 11.4.3.2. As there, we maintain the potential at each cluster $C_{i}$ as a factor set $\vec{\phi}_{i}$ ; some of those factors are initial factors, whereas others are messages sent to cluster $C_{i}$ . Our initial potentials are all of the form equation (14.19), and since we project all messages into the Gaussian parametric family, all of the incoming messages are Gaussians, which we can reformulate as a standard Gaussian and a set of deterministic functions. 

In principle, it now appears that we can take all of the incoming messages, along with all of the exogenous Gaussian variables $W_{i}$ , to produce a single Gaussian distribution $p_{0}$ . We can then apply the linearization procedures of section 14.4.1 to obtain a Gaussian approximation. However, a closer examination reveals several important subtleties that must be addressed. 

14.4.2.1 Dealing with Evidence 

Our discussion so far has sidestepped the issue of evidence: nowhere did we discuss the place at which evidence is instantiated into the factors. In the context of discrete variables, this issue was resolved in a straightforward way by restricting the factors (as we discussed in section 9.3.2). This restriction could be done at any time during the course of the message passing algorithm (as long as the observed variable is never eliminated). 

In the context of continuous variables, the situation is more complex, since any assignment to a variable induces a density over a subspace of measure 0. Thus, when we observe $X=x$ , we must a priori restrict all factors to the space where $X=x$ . This operation is straightforward in the case of canonical forms, but it is somewhat subtler for nonlinear functions. For example, consider the nonlinear dependence of example 14.14. If we have evidence $Y_{1}=y_{1}$ , we can easily redeﬁne our model as $X=g(Y_{2},W)$ where $g(Y_{2},W)=\sqrt{y_{1}^{2}+Y_{2}^{2}}+\sigma W$ p . However, it is not so clear what to do with evidence on the dependent variable $X$ . 

The simplest solution to this problem, and one which is often used in practice, is to instantiate “downstream” the evidence in a cluster after the cluster is linearized. That is, in the preceding example, we would ﬁrst linearize the function $f$ in its cluster, resulting in a Gaussian distribution $p(X,Y_{1},Y_{2})$ ; we would then instantiate the evidence $X=x$ in the canonical form associated with this distribution, to obtain a new canonical form that is proportional to $p(Y_{1},Y_{2}\mid x)$ . 

This approach is simple, but it can be very inaccurate. In particular, the linearization operation (no matter how it is executed) depends on the distribution $p_{0}$ relative to which the linearization is performed. Our posterior distribution, given the evidence, can be very diferent from the prior distribution $p_{0}$ , leading to a very diferent linearization of $f$ . Better methods for taking the evidence into account during the linearization operation exist, but they are outside the scope of this book. 

# 14.4.2.2 Valid Linearization 

A second key issue is that all of the linearization procedures described earlier require that $p_{o}$ be a Gaussian distribution, and not a general canonical form. This requirement is a signiﬁcant one, and imposes constraints on one or more of: the structure of the cluster graph, the order in which messages are passed, and even the probabilistic model itself. 

# Example 14.16 

order-constrained message passing 

Most simply, consider a chain-structured Bayesia $X_{1}\rightarrow X_{2}\rightarrow X_{3}$ , have nonlinear CPDs. We thus have a function $X_{1}=f_{1}(W_{1})$ and functions $X_{2}=f_{2}(X_{1},W_{2})$ and $X_{3}=f_{3}(X_{2},W_{3})$ . In the obvious clique tree, we would have a clique $C_{1}$ with scope $X_{1},X_{2}$ , containing $f_{1}$ and $f_{2}$ , and a clique $C_{2}$ with scope $X_{2},X_{3}$ , containing $f_{3}$ . Further, assume that we have evidence $X_{3}=x_{3}$ . In the discrete setting, we can simply restrict the factor in $C_{2}$ with the observation over $X_{3}$ , producing a factor over $X_{2}$ . This factor can then be passed as a message to $C_{1}$ . In the nonlinear setting, however, we must ﬁrst linearize $f_{3}$ , and for that, we must have a distribution over $X_{2}$ . only obtain this distribution by multi ng into the factor $p(X_{1})$ , $p(X_{2}\mid X_{1})$ , and $p(X_{3}\mid X_{2})$ | . In other words, in order to deal with $C_{2}$ , we must ﬁrst pass a message from $C_{1}$ . 

In general, this requirement on order constrained message passing is precisely the same one that we faced for CLG distributions in section 14.3.3.1, with the same consequences. In a Bayesian network, this requirement constrains us to pass messages in a topological order. In other words, before we linearize a function $X_{i}\,=\,f_{i}(\mathrm{Pa}_{X_{i}},W_{i})$ , we must ﬁrst linearize and obtain a Gaussian for every $Y_{j}\in\mathrm{Pa}_{X_{i}}$ . 

Example 14.17 Consider the network of ﬁgure 14.6, but where we now assume that all variables (including $A,B,C)$ are continuous and utilize nonlinear CPDs. As in example 14.7, the clique tree of ﬁgure 14.6c does not allow messages to be passed at all, since none of the cliques respect the ordering constraint. The clique tree of (e) does allow messages to be passed, but only if the $\{A,B,X,Y\}$ clique is the ﬁrst to act, passing a m age o r $A,Y$ to the clique $\{A,C,Y,Z\}$ ; this message deﬁnes a distribution over the parents of C and Z , allowing them to be linearized. 

In the context of Markov networks, we again can partially circumvent the problem if we assume that the factors in the network are all factor normalizable. However, this requirement may not always hold in practice. 

# 14.4.2.3 Linearization of Multiple Functions 

A ﬁnal issue, related to the previous one, arises from our assumption in section 14.4.1 that all of the functions in a cluster depend only on a set of standard Gaussian random variables $Z$ . This assumption is overly restrictive in almost all cluster graphs. 

# Example 14.18 

incremental linearization 

simultaneous linearization 

sider agai ain-structured Baye n net rk $X_{1}\rightarrow X_{2}\rightarrow X_{3}$ of example 14.16. Here, $C_{1}$ has scope $X_{1},X_{2}$ , and contains both $f_{1}$ and $f_{2}$ . This structure does not satisfy the preceding requirements, since $X_{2}$ relies also on $X_{1}$ . 

There are two ways to address the issue in this case. The ﬁrst, which we call incremental linearization ﬁrst linearizes $f_{1}$ , and subsequently linearize $f_{2}$ . This approach can be implemented by making a separate clique containing just $X_{1}$ . In this clique, we have a Gaussian distribution $p_{1}(Z_{1})$ , and so we can compute a Gaussian approximation to $f_{1}(Z_{1})$ , producing a Gaussian message $\tilde{\delta}_{1\rightarrow2}(X_{1})$ . We can then pass this message to a clique containing $X_{1},X_{2}$ , but only → the $f_{2}(X_{2},Z_{2})$ function; we now have a Gaussian distribution $p_{2}(X_{2},Z_{2})$ , and can use the techniques of section 14.4.1 to linearize $f_{2}(X_{2},Z_{2})$ into this Gaussian, to produce a Gaussian distribution over $X_{1},X_{2}$ . (Note that $p_{2}(X_{2},Z_{2})$ is not a standard Gaussian, but we can use the trick of equation (14.17) to change the coordinate system; see exercise 14.7.) We can then marginalize the resulting Gaussian distribution onto $X_{2}$ , and send a message $\tilde{\delta}_{2\to3}(X_{2})$ to $C_{3}$ , continuing this → process. 

As a second alternative, called simultaneous linearization , we can linearize multiple nonlinear functions into the same cluster together. We can implement this solution by substituting the variable $X_{1}$ with its functional deﬁnition; that is, we can deﬁne $X_{2}=g_{2}(Z_{1},Z_{2})=f_{2}(f_{1}(Z_{1}),Z_{2})$ , and use $g_{2}$ rather than $f_{2}$ in the analysis of section 14.4.1. 

Both of these solutions generalize beyond this simple example. In the incremental approach, we simply deﬁne smaller clusters, each of which contains at most one nonlinear function. We then linearize one such function at a time, producing each time a Gaussian distribution. This Gaussian is passed on to another cluster, and used as the basis for linearizing another nonlinear function. The simultaneous approach linearizes several functions at once, substituting each variable with the function that deﬁnes it, so as to make all the functions depend only on Gaussian variables. 

These two approaches make diferent approximations: Going back to example 14.16, in the incremental approach, the approximation of $f_{2}$ uses a Gaussian approximation to the distribution over $X_{1}$ . If this approximation is poor, it may lead to a poor approximation for the distribution over $X_{2}$ . Conversely, in the simultaneous approach, we are performing an integral for the function $g_{2}$ , which may be more complicated than $f_{2}$ , possibly leading to a poor approximation of its moments. In general, the errors made by each method are going to depend on the speciﬁc case at hand, and neither necessarily dominates the other. 

# 14.4.2.4 Iterated Message Passing 

The general algorithm we described can be applied within the context of diferent message passing schemes. Most simply, we deﬁne a clique tree, and we then do a standard upward and downward pass. However, as we discussed in the context of the general expectation propagation algorithm, our approximation within a cluster depends on the contents of that factor. In our setting, the approximation of $_{p}\bigoplus f$ as a Gaussian depends on the distribution $p$ . Depending on the order in which CPDs are linearized and messages are passed, the resulting distribution might be very diferent. 

Example 14.19 Consider a network consisting of $a$ variable $X$ and its two children $Y$ and $Z$ , so that our two cliqu $C_{1}=\{X,Y\}$ and $C_{2}=\{X,Z\}$ ume that we observe $Y=y$ . If we include the prior $p(X)$ within $C_{1}$ , then we ﬁrst compute $C_{1}\mathit{\dot{s}}$ ’s message, producing a Gaussian approximation to the joint posterior ${\hat{p}}(X,Y)$ . This posterior is marginalized to produce ${\hat{p}}(Y)$ , which is then used as the basis for linearizing $Z$ . Conversely, if we include $p(X)$ within $C_{2}$ , then the linearization of $Z$ is done based on an approximation to $X\mathit{\dot{s}}$ prior distribution, generally leading to a very diferent linearization for $p(X,Z)$ . 

In general, the closer the distribution used for the approximation is to the true posterior, the higher the quality of the approximation. Thus, in this example, we would prefer to ﬁrst linearize $Y$ and condition on its value, and only then to linearize $Z$ . However, we cannot usually linearize all CPDs using the posterior. For example, assume that $Z$ has another child $W$ whose value is also observed; the constraint of using a topological ordering prevents us from linearizing $W$ before linearizing $Z$ , so we are forced to use a distribution over $X$ that does not take into account the evidence on $W$ . 

The iterative EP algorithm helps address this limitation. In particular, once we linearize all of the initial clusters (subject to the constraints we mentioned), we can now continue to pass messages between the clusters using the standard belief update algorithm. At any point in the algorithm where cluster $C_{i}$ must send a message (whether at every message or on a more intermittent basis), it can use the Gaussian deﬁned by the incoming messages and the exogenous Gaussian variables to relinearize the functions assigned to it. The revised messages arising from this new linearization are then sent to adjacent clusters, using the standard EP update rule of equation (11.41) (that is, by subtracting the sufcient statistics of the previously sent message). This generally has the efect of linearizing using a distribution that is closer to the posterior, and hence leading to improved accuracy. However, it is important to remember that, as in section 14.3.3.2, the messages that arise in belief-update message passing may not be positive deﬁnite, and hence can give rise to canonical forms that are not legal Gaussians, and for which integration is impossible. To avoid catastrophic failures in the algorithm, it is important to check that any canonical form used for integration is, indeed, a normalizable Gaussian. 

# 14.4.2.5 Augmented CLG Networks 

One very important consequence of this algorithm is its ability to address one of the main limitations of CLG networks — namely, that discrete variables cannot have continuous parents. This limitation is a signiﬁcant one. For example, it prevents us from modeling simple objects such as a thermostat, whose discrete on/of state depends on a continuous temperature variable. As we showed, we can easily model such dependencies, using, for example, a linear sigmoid model or its multinomial extension. The algorithm described earlier, which accommodates nonlinear CPDs, easily deals with this case. 

Consider the net ork $X\rightarrow A$ , where $X$ h $p(X)={\mathcal{N}}\left(X\mid\mu;\sigma\right)$ and the CPD of A is a softmax given by $P(A=a^{1}\mid X=x)=1/(1+e^{c_{1}x+c_{0}})$ | . The clique tree has a single clique $(X,A)$ , whose potential should contain the product of these two CPDs. Thus, it should contain two entries, one for $a^{1}$ and one for $a^{0}$ , each of which is a continuous function: $p(x)P(a^{1}\mid x)$ and $p(x)P(a^{0}\mid x)$ . As bef , we approximate this potential using a canonical table, with an entry for every assignment to A , and where each entry is a Gaussian distribution. Thus, we would approximate each entry $p(x)P(a\mid x)$ as a Gaussian. Once again, we have a product of a function $-\operatorname{\nabla}P(a\mid x)$ — with a Gaussian $-\,p(x)$ . We can therefore use any of the methods in section 14.4.1 to approximate the result as a single Gaussian. 

This simple extension forms the basis for a general algorithm that deals with hybrid networks involving both continuous and discrete variables; see exercise 14.11. 

# 14.5 Particle-Based Approximation Methods 

All of the message passing schemes described before now utilize the Gaussian distribution as a parametric representation for the messages and (in some cases) even the clique potentials. This representation allows for truly exact inference only in the case of pure Gaussian networks, and it is exact in a weak sense for a very restricted class of CLG networks. If our original factors are far from being Gaussian, and, most particularly if they are multimodal, the Gaussian approximation used in these algorithms can be a very poor one. Unfortunately, there is no general-purpose parametric class that allows us to encode arbitrarily continuous densities. 

An alternative approach is to use a semiparametric or nonparametric method, which allows us to avoid parametric assumptions that may not be appropriate in our setting. Such approaches are often applied in continuous settings, since there are many settings where the Gaussian approximation is clearly inappropriate. In this section, we discuss how such methods can be used in the context of inference in graphical models. 

# 14.5.1 Sampling in Continuous Spaces 

We begin by discussing the basic task of sampling from a continuous univariate measure. As we discussed in box 12.A, sampling from a discrete distribution can be done using straightfor- ward methods. Efcient solutions, albeit somewhat more complex, are also available for other parametric forms, including Gaussians, Gamma distributions, and others; see section 14.7. Un- fortunately, such solutions do not exist for all continuous densities. In fact, even distributions that can be characterized analytically may not have established sampling procedures. 

A number of general-purpose approaches have been designed for sampling from arbitrary continuous densities. Perhaps the simplest is rejection sampling (or sometimes acceptance- rejection sampling). The basic idea is to sample a value from a proxy distribution and then “accept” it with probability that is proportional to the skew introduced by the proxy distribution. Suppose that we want to sample a variable $X$ with density $p(x)$ . Now suppose that we have a function $q(x)$ such that $q(x)>0$ whenever $p(x)>0$ and $q(x)\geq p(x)$ . Moreover, suppose that we can sample from the distribution 

$$
Q(X)={\frac{1}{Z}}q(x),
$$ 

where $Z$ is a normalizing constant. (Note that $Z>1$ , as $q(x)$ is an upper bound on a density function whose integral is 1.) In this case, we repeatedly perform the following steps, until acceptance: 

The probability that this procedure returns the value $x$ is exactly $p(x)$ . The proof is analogous to the one we saw for importance sampling (see proposition 12.1). (The diference here is that we need to return a single sample rather than weight multiple samples.) In practice, we can speed up the procedure if we have also a lower bound $b(x)$ for $p(x)$ (that is $b(x)\leq p(x))$ ). In that case, if $\begin{array}{r}{u\,<\,\frac{b(x)}{q(x)}}\end{array}$ than we accept the sample $x$ without evaluating $p(x)$ . By ﬁnding easy to evaluate functions $q(x)$ and $b(x)$ that allow sampling from $Q$ , and provide relatively tight bounds on the density $p(x)$ , this method can accept samples within few iterations. 

This straightforward approach, however, can be very slow if $q(x)$ is not a good approximation to $p(x)$ . Fortunately, more sophisticated methods have also been designed. One example is described in exercise 12.22. See section 14.7 for other references. 

# 14.5.2 Forward Sampling in Bayesian Networks 

We now consider the application of forward sampling in a Bayesian network (algorithm 12.1). Assuming that we have a method for generating samples from the continuous density of each variable $X_{i}$ given an assignment to its parents, we can use this method to generate samples from a broad range of continuous and hybrid Bayesian networks — far greater than the set of networks for which message passing algorithms are applicable. This approach generates random samples from the prior distribution $p(\mathcal{X})$ . As we discussed, these samples can be used to estimate the expectation of any functio $f(\mathcal{X})$ , as in equation (12.1). For example, we can compute the mean $E[X]$ of any variable X by taking $f({\mathcal{X}})\,=\,X$ , or its variance by taking $f(\mathcal{X})=X^{2}$ and subtracting $E[X]^{2}$ . 

However, plain forward sampling is generally inapplicable to hybrid networks as soon as we have evidence over some of the network variables. Consider a simple network $X\rightarrow Y$ , where we have evidence $Y=y$ . Forward sampling generates $(x[m],y[m])$ samples, and rejects those in which $y[m]\neq y$ . Because the domain of $Y$ is a continuum, the probability that our sampling process will generate any particular value $y$ is zero. Thus, none of our samples will match the observations, and all will be rejected. 

A more relevant approach in the case of hybrid networks is a form of importance sampling, such as the likelihood weighting algorithm of algorithm 12.2. The likelihood weighting algorithm generalizes to the hybrid case in the most obvious way: Unobserved variables are sampled, based on the assignment to their parents, as described earlier. Observed variables are instantiated to their observed values, and the sample weight is adjusted. We can verify, using the same analysis as in section 12.2, that the appropriate adjustment to the importance weight for a continuous variable $X_{i}$ erved to be $x_{i}$ is the density $p(x_{i}\mid\mathbf{\mu}u_{i})$ , where $\mathbf{\mathbf{\mathit{u}}}_{i}$ is the assignment, in the sample, to $\mathrm{Pa}_{X_{i}}$ . The only implication of this change is that, unlike in the discrete case, this adjustment may be greater than 1 (because a density function is not bounded in the range $[0,1])$ . Although this diference does not inﬂuence the algorithm, it can increase the variance of the sample weights, potentially decreasing the overall quality of our estimator. 

# 14.5.3 MCMC Methods 

MCMC 

Gibbs sampling The second type of sampling algorithms is based on Markov chain Monte Carlo (MCMC) methods. In general, the theory of Markov chains for continuous spaces is quite complicated, and many of the basic theorems that hold for discrete spaces do not hold, or hold only under certain assumptions. A discussion of these issues is outside the scope of this book. However, simple variants of the MCMC algorithms for graphical models continue to apply in this setting. 

The Gibbs sampling algorithm (algorithm 12.4) can also be applied to the case of hybrid networks. As in the discrete case, we maintain a sample, which is a complete assignment to the network variables. We then select variables $X_{i}$ one at a time, and sample a new value for $X_{i}$ from $p(X_{i}\mid\mathbf{\mu}u_{i})$ , where $\mathbf{\mathbf{\mathit{u}}}_{i}$ is the current assignment to $X_{i}$ ’s Markov blanket. The only issue that might arise relates to this sampling step. In equation (12.23), we showed a particularly simple form for the probability of $X_{i}\,=\,x_{i}^{\prime}$ given the assignment $\mathbf{\mathbf{\Omega}}_{\mathcal{U}}$ to $X_{i}$ ’s. The same derivation applies to the continuous case, simply replacing the summation with an integral: 

$$
p(x_{i}^{\prime}\mid\mathbf{u}_{i})=\frac{\prod_{C_{j}\ni X_{i}}\beta_{j}(x_{i}^{\prime},\mathbf{u}_{i})}{\int_{x_{i}^{\prime\prime}}\prod_{C_{j}\ni X_{i}}\beta_{j}((x_{i}^{\prime\prime},\mathbf{u}_{i}))},
$$ 

where $C_{j}$ are the diferent potentials in the network. 

Metropolis- Hastings 

In the discrete case, we could simply multiply the relevant potentials as tables, renormalize the resulting factor, and use it as a sampling distribution. In the continuous case, matters may not be so simple: the product of factors might not have a closed form that allows sampling. In this case, we generally resort to the use of a Metropolis-Hastings algorithm. Here, as in the discrete case, changes to a variable $X_{i}$ are proposed by sampling from a local proposal distribution $\mathcal{T}_{i}^{Q}$ . The proposed local change is either accepted or rejected using the appropriate acceptance probability, as in equation (12.26): 

$$
\begin{array}{r l r}{\mathcal{A}(\pmb{u}_{i},x_{i}\rightarrow\pmb{u}_{i},x_{i}^{\prime})}&{=}&{\mathrm{min}\left[1,\frac{p(x_{i}^{\prime}\mid\pmb{u}_{i})}{p(x_{i}\mid\pmb{u}_{i})}\frac{\mathcal{T}^{Q}(\pmb{u}_{i},x_{i}^{\prime}\rightarrow\pmb{u}_{i},x_{i})}{\mathcal{T}^{Q}(\pmb{u}_{i},x_{i}\rightarrow\pmb{u}_{i},x_{i}^{\prime})}\right],}\end{array}
$$ 

where (as usual) probabilities are replaced with densities. 

The only question that needs to be addressed is the choice of the proposal distribution. One common choice is a Gaussian or student-t distribution centered on the current value $x_{i}$ . Another is a uniform distribution over a ﬁnite interval also centered on $x_{i}$ . These proposal random-walk chain 

distributions deﬁne a random walk over the space, and so a Markov chain that uses such a proposal distribution is often called a random-walk chain . These choices all guarantee that the Markov chain converges to the correct posterior, as long as the posterior is positive: for all $z\in\mathit{V a l}(\mathcal{X}-E)$ , we have that $p(z\mid e)>0$ . However, the rate of convergence can depend heavily on the window size: the variance of the proposal distribution. Diferent distributions, and even diferent regions within the same distribution, can require radically diferent window sizes. Picking an appropriate window size and adjusting it dynamically are important questions that can greatly impact performance. 

# 14.5.4 Collapsed Particles 

As we discussed in section 12.4, it is difcult to cover a large state space using full instantiations to the network variables. Much better estimates — often with much lower variance — can be obtained if we use collapsed particles. Recall that, when using collapsed particles, the variables

 $\mathcal{X}$ are partition bsets ${\mathcal X}\;=\;X_{p}\cup X_{d}$ . A collapsed particle then consists of tantiation $\pmb{x}_{p}\in V a l(\pmb{X}_{p})$ ∈ , coupled with some representation of the distribution $\cal P(\cal X_{d}\mid

$ $\mathbf{\nabla}x_{p},e)$ . The use of such particles relies on our ability to do two things: to generate samples from $X_{p}$ efectively, and to represent compactly and reason with the distribution $P(X_{d}\mid\mathbf{\sigma}_{\mathbf{X}_{p},\mathbf{\epsilon}}e)$ . 

The notion of collapsed particles carries over unchanged to the hybrid case, and virtually every algorithm that applied in the discrete case also applies here. Indeed, collapsed particles are often particularly suitable in the setting of continuous or hybrid networks: In many such networks, if we select an assignment to some of the variables, the conditional distribution over the remaining variables can be represented (or well approximated) as a Gaussian. Since we can efciently manipulate Gaussian distributions, it is generally much better, in terms of our time/accuracy trade-of, to try and maintain a closed-form Gaussian representation for the parts of the distribution for which such an approximation is appropriate. 

Although this property can be usefully exploited in a variety of networks, one particularly useful application of collapsed particles is motivated by the observation that inference in a purely continuous network is fairly tractable, whereas inference in the simplest hybrid networks — polytree CLGs — can be very expensive. Thus, if we can “erase” the discrete variables from the network, the result is a much simpler, purely continuous network, which can be manipulated using the methods of section 14.2 in the case of linear Gaussians, and the methods of section 14.4 in the more general case. 

Thus, CLG networks are often efectively tackled using a collapsed particles approach, where the instantiated variables in each particular are the discrete variables, $X_{p}=\Delta,$ , and the variables maintained in a closed-form distribution are the continuous variables, $\mathbf{\Delta}X_{d}=\Gamma$ . We can now apply any of the methods described in section 12.4, often with some useful shortcuts. As one example, likelihood weighting can be easily applied, since discrete variables cannot have continuous parents, so that the set $\boldsymbol{X}_{p}$ is upwardly closed, allowing for easy sampling (see exercise 14.12). The application of MCMC methods is also compelling in this case, and it can be made more efcient using incremental update methods such as those of exercise 12.27. 

# 14.5.5 Nonparametric Message Passing 

Yet another alternative is to use a hybrid approach that combines elements from both particle- based and message passing algorithms. Here, the overall algorithm uses the structure of a mes- sage passing (clique tree or cluster graph) approach. However, we use the ideas of particle-based inference to address the limitations of using a parametric representation for the intermediate factors in the computation. Speciﬁcally, rather than representing these factors using a single parametric model, we encode them using a nonparametric representation that allows greater ﬂexibility to capture the properties of the distribution. Thus, we are essentially still reparameter- izing the distribution in terms of a product of cluster potentials (divided by messages), but each cluster potential is now encoded using a nonparametric representation. 

The advantage of this approach over the pure particle-based methods is that the samples are generated in a much lower-dimensional space: single cluster, rather than the entire joint distri- bution. This can alleviate many of the issues associated with sampling in a high-dimensional space. On the other side, we might introduce additional sources of error. In particular, if we are using a loopy cluster graph rather than a clique tree as the basis for this algorithm, we have all of the same errors that arise from the representation of a distribution as a set of pseudo-marginals. 

One can construct diferent instantiations of this general approach, which can vary along several axes. The ﬁrst is the message passing algorithm used: clique tree versus cluster graph, sum-product versus belief update. The second is the form of the representation used for factors and messages: plain particles; a nonparametric density representation; or a semiparametric representation such as a histogram. Finally, we have the approach used to approximate the factor in the chosen representation: importance sampling, MCMC, or a deterministic approximation. 

# 14.6 Summary and Discussion 

In this chapter, we have discussed some of the issues arising in applying inference to networks involving continuous variables. While the semantics of such networks is easy to deﬁne, they raise considerable challenges for the inference task. 

The heart of the problem lies in the fact that we do not have a universal representation for factors involving continuous variables — one that is closed under basic factor operations such as multiplication, marginalization, and restriction with evidence. This limitation makes it very difcult to design algorithms based on variable elimination or message passing. Moreover, the difculty is not simply a matter of our inability to ﬁnd good algorithms. Theoretical analysis shows that classes of network structures that are tractable in the discrete case (such as polytrees) give rise to $\mathcal{N P}$ -hard inference problems in the hybrid case. 

Despite the difculties, continuous variables are ubiquitous in practice, and so signiﬁcant work has been done on the inference problem for such models. 

Most message passing algorithms developed in the continuous case use Gaussians as a lingua franca for factors. This representation allows for exact inference only in a very limited class of models: clique trees for linear Gaussian networks. However, the exact same factor operations provide a basis for a belief propagation algorithm for Gaussian networks. This algorithm is easy to implement and has several satisfying guarantees, such as guaranteed convergence under fairly weak conditions, producing the exact mean if convergence is achieved. This technique allows us to perform inference in Gaussians where manipulating the full covariance matrix is intractable. 

In cases where not all the potentials in the network are Gaussians, the Gaussian representation is generally used as an approximation. In particular, a standard approach uses an instantiation of the expectation propagation algorithm, using M-projection to approximate each non-Gaussian distribution as a Gaussian during message passing. In CLG networks, where factors represent a mixture of (possibly exponentially many) Gaussians derived from diferent instantiations of discrete variables, the M-projection is used to collapse the mixture components into a single Gaussian. In networks involving nonlinear dependencies between continuous variables, or be- tween continuous and discrete variables, the M-projection involves linearization of the nonlinear dependencies, using either a Taylor expansion or numerical integration. While simple in prin- ciple, this application of EP raises several important subtleties. In particular, the M-projection steps can only be done over intermediate factors that are legal distributions. This restriction imposes signiﬁcant constraints on the structure of the cluster graph and on the order in which messages can be passed. Finally, we note that the Gaussian approximation is good in some cases, but can be very poor in others. For example, when the distribution is multimodal, the Gaussian M-projection can be a very broad (perhaps even useless) agglomeration of the diferent peaks. 

This observation often leads to the use of approaches that use a nonparametric approximation. Commonly used approaches include standard sampling methods, such as importance sampling or MCMC. Even more useful in some cases is the use of collapsed particles, which avoid sampling a high-dimensional space in cases where parts of the distribution can be well approximated as a Gaussian. Finally, there are also useful methods that integrate message passing with nonparametric approximations for the messages, allowing us to combine some of the advantages (and some of the disadvantages) of both types of approaches. 

Our presentation in this chapter has only brieﬂy surveyed a few of the key ideas related to inference in continuous and hybrid models, focusing mostly on the techniques that are speciﬁcally designed for graphical models. Manipulation of continuous densities is a staple of statistical inference, and many of the techniques developed there could be applied in this setting as well. For example, one could easily imagine message passing techniques that use other representations of continuous densities, or the use of other numerical integration techniques. Moreover, the use of diferent parametric forms in hybrid networks tends to give rise to a host of “special-case” models where specialized techniques can be usefully applied. In particular, even more so than for discrete models, it is likely that a good solution for a given hybrid model will require a combination of diferent techniques. 

# 14.7 Relevant Literature 

Perhaps the earliest variant of inference in Gaussian networks is presented by Thiele (1880), who deﬁned what is the simplest special case of what is now known as the Kalman ﬁltering algorithm (Kalman 1960; Kalman and Bucy 1961). Shachter and Kenley (1989) proposed the general idea of network-based probabilistic models, in the context of Gaussian inﬂuence diagrams. The ﬁrst presentation of the general elimination algorithm for Gaussian networks is due to Normand and Tritchler (1992). However, the task of inference in pure Gaussian networks is highly related to the basic mathematical problem of solving a system of linear equations, and the elimination-based inference algorithms very similar to Gaussian elimination for solving such systems. Indeed, some of the early incarnations of these algorithms were viewed from that perspective; see Parter (1961) and Rose (1970) for some early work along those lines. 

Iterative methods from linear algebra (Varga 2000) can also be used for solving systems of linear equations; in efect, these methods employ a form of local message passing to compute the marginal means of a Gaussian distribution. Loopy belief propagation methods were ﬁrst proposed as a way of also estimating the marginal variances. Over the years, multiple au- thors (Rus me vi chien tong and Van Roy 2001; Weiss and Freeman 2001a; Wainwright et al. 2003a; Malioutov et al. 2006) have analyzed the convergence and correctness properties of Gaussian belief propagation, for larger and larger classes of models. All of these papers provide conditions that ensure convergence for the algorithm, and demonstrate that if the algorithm converges, the means are guaranteed to be correct. The recent analysis of Malioutov et al. (2006) is the most comprehensive; they show that their sufcient condition, called walk-summability , is equivalent to pairwise normalizability, and encompasses all of the classes of Gaussian models that were previously shown to be solvable via LBP (including attractive, nonfrustrated, and diagonally dominant models). They also show that the variances at convergence are an underestimate of the true variances, so that the LBP results are overconﬁdent; their results point the way to par- tially correcting these inaccuracies. The results also analyze LBP for non-walksummable models, relating convergence of the variance to validity of the LBP computation tree. 

The properties of conditional Gaussian distributions were studied by Lauritzen and Wermuth (1989). Lauritzen (1992) extended the clique tree algorithm to the task of inference in these models, and showed, for strongly rooted clique trees, the correctness of the discrete marginals and of the continuous means and variances. Lauritzen and Jensen (2001) and Cowell (2005) provided alternative variants of this algorithm, somewhat diferent representations, which are numerically more stable and better able to handle deterministic linear relationships, where the associated covariance matrix is not invertible. Lerner et al. (2001) extend Lauritzen’s algorithm to CLG networks where continuous variables can have discrete children, and provide conditions under which this algorithm also has the same correctness guarantees on the discrete marginals and the moments of the continuous variables. The $\mathcal{N P}$ -hardness of inference in CLG networks of simple structures (such as polytrees) was shown by Lerner and Parr (2001). Collapsed particles have been proposed by several researchers as a successful alternative to full collapsing of the potentials into a single Gaussian; methods include random sampling over particles (Doucet et al. 2000; Paskin 2003a), and deterministic search over the particle assignment (Lerner et al. 2000; Lerner 2002), a method particularly suitable in applications such as fault diagnosis, when the evidence is likely to be of low probability. 

The idea of adaptively modifying the approximation of a continuous cluster potential during the course of message passing was ﬁrst proposed by Kozlov and Koller (1997), who used a variable-resolution discretization approach (a semiparametric approximation). Koller et al. (1999) generalized this approach to other forms of approximate potentials. The expectation propagation algorithm, which uses a parametric approximation, was ﬁrst proposed by Minka (2001b), who also made the connection to optimizing the energy functional under expectation matching constraints. Opper and Winther (2005) present an alternative algorithm based on a similar idea. Heskes et al. (2005) provide a unifying view of these two works. Heskes and Zoeter (2003) discuss the use of weak marginalization within the generalized belief propagation in a network involving both discrete and continuous variables. 

The use of the Taylor series expansion to deal with nonlinearities in probabilistic models is a key component of the extended Kalman ﬁlter, which extends the Kalman ﬁltering method to nonlinear systems; see Bar-Shalom, Li, and Kirubarajan (2001) for a more in-depth presentation of these methods. The method of exact monomials, under the name unscented ﬁlter, was ﬁrst proposed by Julier and Uhlmann (1997). Julier (2002) shows how this approach can be modiﬁed to address the problem of producing approximations that are not positive deﬁnite. 

Sampling from continuous distributions is a core problem in statistics, on which exten- sive work has been done. Fishman (1996) provides a good overview of methods for various parametric families. Methods for sampling from other distributions include adaptive rejection sampling (Gilks and Wild 1992; Gilks 1992), adaptive rejection metropolis sampling (Gilks et al. 1995) and slice sampling (Neal 2003). 

nonparametric belief propagation 

The bugs system supports sampling for many continuous families, within their general MCMC framework. Several approaches combine sampling with message passing. Dawid et al. (1995); Hernández and Moral (1997); Kjærulf (1995b) propose methods that sample a continuous factor to turn it into a discrete factor, on which standard message passing can be applied. These methods vary on how the samples are generated, but the sampling is performed only once, in the initial message passing step, so that no adaptation to subsequent information is possible. Sudderth et al. (2003) propose nonparametric belief propagation , which uses a nonparametric approximation of the potentials and messages, as a mixture of Gaussians — a set of particles each with a small Gaussian kernel. They use MCMC methods to regenerate the samples multiple times during the course of message passing. 

Many of the ideas and techniques involving inference in hybrid systems were ﬁrst developed in a temporal setting; we therefore also refer the reader to the relevant references in section 15.6. 

# 14.8 Exercises 

# Exercise 14.1 

Let $_{X}$ and $\mathbf{Y}$ be two sets of continuous variables, with $|X|=n$ and $|Y|=m$ . Let 

$$
p(Y\mid X)=\mathcal{N}\left(Y\mid a+B X;C\right)
$$ 

where $^{a}$ is a vector of dimension $m,\,B$ is an $m\times n$ matrix, and $C$ is an $m\times m$ ix. This dependence is a multidimensional generalization of a linear Gaussian CPD. Show how $p(\pmb{Y}\mid\pmb{X})$ | can be represented as a canonical form. 

# Exercise 14.2 

Prove proposition 14.3. 

# Exercise 14.3 

Prove that setting evidence in a canonical form can be done as shown in equation (14.6). 

# Exercise $14.4\star$ 

Describe a method that efciently computes the covariance of any pair of variables $X,Y$ in a calibrated Gaussian clique tree. 

# Exercise $14.5\star$ 

Let $_{X}$ and $\mathbf{Y}$ be two sets of continuous variables, with $|X|=n$ and $|Y|=m$ . Let $p(X)$ be an arbitrary density, and let 

$$
p(Y\mid X)=\mathcal{N}\left(Y\mid a+B X;C\right)
$$ 

where $^{a}$ is a vector o $m,\,B$ $m\times n$ matrix, and $C$ is an $m\times m$ matrix. Sho t the ﬁrst two moments of $p(X,Y)=p(X)p(Y\mid X)$ | depend only on the ﬁrst two moments of $p(X)$ and not on the distribution $p(X)$ itself. 

Exercise $14.6\star$ Prove theorem 14.5. 

Exercise 14.7 Prove the equality in equation (14.17). 

Exercise 14.8 Prove equation (14.17). 

# Exercise $14.9\star$ 

Our derivation in section 14.4.1 assumes that $p$ and $Y\,=\,f(U)$ have the same scope $U$ . Now, assume that $S c o p e[f]=U$ , whereas our distribution $p$ has scope $U,Z$ . We can still use the same method if we deﬁne $\dot{g}(\dot{\pmb{u}},\dot{\pmb{u}}^{\prime})=f(\pmb{u})$ , and integrate $g$ . This solution, however, requires that we perform integration in dimension $|U\cup Z|$ , which is often much higher than $|U|$ . Since the cost of numeric ntegration grows with the dimension of the integrals, we can gain considerable savings by using only U to compute our approximation. 

In this exercise, you will use the interchange ability of the Gaussian and linear Gaussian representations to perform integration in higher dimension. 

a. For $Z\in Z$ , show how we ca $Z$ as a linear combination of variab $_{X}$ ith Gaussian noise. b. Use this expression to write $\mathbb{C}o v[Z;Y]$ as a function of the covariances $\;C o v[X_{i};Y]$ . c. Put these results together in order to show how we can obtain a Gaussian approximation to $p(Y,Z)$ . 

# Exercise 14.10 

In some cases, it is possible to decompose a nonlinear dependency $\boldsymbol{Y}~=~f(\boldsymbol{X})$ into ﬁner-grained dependencies. For example, we may be able to decompose the nonlinear function $f$ as ${\tilde{f(X)}}\ =$ $g\big(\dot{g}_{1}(\pmb{X}_{1}),g_{2}(\pmb{X}_{2})\big)$ , where $X_{1},X_{2}\subset X$ are smaller subsets of variables. 

Show how this decomposition can be used in the context of linearizing the function $f$ in several steps rather than in a single step. What are the trade-ofs for this approach versus linearizing $f$ directly? 

# Exercise $14.11\star\star$ 

Show how to combine the EP-based algorithms for CLGs and for nonlinear CPDs to address CLGs where discrete variables can have continuous parents. Your algorithm should specify any constraints on the message passing derived from the need to allow for valid M-projection. 

# Exercise $14.12\star$ 

Assume we have a CLG network with discrete variables $\Delta$ and continuous variables $\Gamma$ . In this exercise, we consider collapsed methods that sample the discrete variables and perform exact inference over the continuous variables. Let $e_{d}$ denote the discrete evidence and $e_{p}$ the continuous evidence. 

a. Given a set of weighted particles such as those described earlier, show how we can estimate the expectation of a function $\mathbf{\dot{}}f(X_{i})$ for some $X_{i}\in\Gamma$ . For what functions $f$ do you expect this analysis to give you drastically diferent answers from the “exact” CLG algorithm of section 14.3.4. (Ignore issues of inaccuracies arising from sampling noise or insufcient number of samples.) b. Show how we can efciently apply collapsed likelihood weighting, and show precisely how the impor- tance weights are computed. c. Now, consider a combined algorithm that generates a clique tree over $\Delta$ to generate particles $\pmb{x}_{p}$ [1] , . . . , $\pmb{x}_{p}[M]$ sampled exactly from $P(\Delta\mid\bar{\mathbf{\alpha}}_{d})$ . Show the computation of importance weights in this case. Explain the computational beneﬁt of this approach over doing clique tree inference over the entire network. 

# 15 Inference in Temporal Models 

knowledge-based model construction 

In chapter 6, we presented several frameworks that provide a higher-level representation lan- guage. We now consider the issue of performing probabilistic inference relative to these rep- resentations. The obvious approach is based on the observation that a template-based model can be viewed as a generator of ground graphical models: Given a skeleton, the template-based model deﬁnes a distribution over a ground set of random variables induced by the skeleton. We can then use any of our favorite inference algorithms to answer queries over this ground network. This process is called knowledge-based model construction , often abbreviated as KBMC . However, applying this simple idea is far from straightforward. 

First, these models can easily produce models that are very large, or even inﬁnite. Several approaches can be used to reduce the size of the network produced by KBMC; most obviously, given a set of ground query variables $Y$ and evidence $E=e$ , we can produce only the part of the network that is needed for answering the query $P(Y\mid e)$ . In other words, our ground network is a dynamically generated object, and we can generate only the parts that we need for our current query. While this approach can certainly give rise to considerable savings in certain cases, in many applications the network generated is still very large. Thus, we have to consider whether our various inference algorithms scale to this setting, and what additional approximations we must introduce in order to achieve reasonable performance. 

Second, the ground networks induced by a template-based model can often be densely con- nected; most obviously, both aggregate dependencies on properties of multiple objects and relational uncertainty can give rise to dense connectivity. Again, dense connectivity causes dif- culties for all exact and most approximate inference algorithms, requiring algorithmic treatment. 

Finally, these models give rise to new types of queries that are not easily expressible as standard probabilistic queries. For example, we may want to determine the probability that every person in our family tree has at least one close relative (for some appropriate deﬁnition of “close”) with a particular disease. This query involves both universal and existential quantiﬁers; while it can be translated into a ground-level conjunction (over people in the family tree) of disjunctions (over their close relatives), this translation is awkward and gives rise to a query over a very large number of variables. 

The development of methods that address these issues is very much an open area of research. In the context of general template-based models, the great variability of the models expressed in these languages limits our ability to provide general-purpose solutions; the existing approaches ofer only partial solutions whose applicability at the moment is somewhat limited. We therefore do not review these methods in this book; see section 15.6 for some references. In the more restricted context of temporal models, the networks have a uniform structure, and the set of relevant queries is better established. Thus, more work has been done on this setting. In the remainder of this chapter, we describe some of the exact and approximate methods that have been developed for inference in temporal models. 

# 15.1 Inference Tasks 

We now move to the question of inference in a dynamic Bayesian network (DBN). As we discussed, we can view a DBN as a “generator” for Bayesian networks for diferent time intervals. Thus, one might think that the inference task is solved. Once we generate a speciﬁc Bayesian network, we can simply run the inference algorithm of our choice to answer any queries. However, this view is overly simplistic in two diferent respects. 

First, the Bayesian networks generated from a DBN can be arbitrarily large. Second, the type of reasoning we want to perform in a temporal setting is often diferent from the reasoning applicable in static settings. In particular, many of the reasoning tasks in a temporal domain are executed online as the system evolves. 

ﬁltering 

belief state 

For example, a common task in temporal settings is ﬁltering (also called tracking ): at any time point $t$ , we compute our most informed beliefs about the current system state, given all of the evidence obtained so far. Formally, let $\boldsymbol{o}^{(t)}$ denote the observation at time $t$ ; we want to keep track of $P(\mathcal{X}^{(t)}\mid o^{(1:t)})$ (or of some marginal of this distribut n over some subset of variables). As a probabilistic query, we can deﬁne the belief state at time t to be: 

$$
\sigma^{(t)}(\mathcal{X}^{(t)})=P(\mathcal{X}^{(t)}\mid o^{(1:t)}).
$$ 

Note that the belief state is exponentially large in the number of unobserved variables in $\mathcal{X}$ . We therefore will not, in general, be interested in the belief state in its entirety. Rather, we must ﬁnd an efective way of encoding and maintaining the belief state, allowing us to query the current probability of various events of interest (for example, marginal distributions over smaller subsets of variables). 

prediction 

smoothing 

Example 15.1 

The tracking task is the task of maintaining the belief state over time. A related task is the prediction task: at time $t$ , given the observations $o^{(1:t)}$ , predict the distribution over (some subset of) the variables at time $t^{\prime}>t$ . 

A third task, often calle othing , involves computing the posterior probability of $\mathcal{X}^{(t)}$ given all of the evidence o $o^{(1:T)}$ in some longer trajectory. The term “smoothing” refers to the fact that, in tracking, the evidence accumulates gradually. In cases where new evidence can have signiﬁcant impact, the belief state can change drastically from one time slice to the next. By incorporating some future evidence, we reduce these temporary ﬂuctuations. This process is particularly important when the lack of the relevant evidence can lead to temporary “misconceptions” in our belief state. 

In cases of a sensor failure (such as example 6.5), a single anomalous observation may not be enough to cause the system to realize that a failure has occurred. Thus, the ﬁrst anomalous sensor reading, at time $t_{1}$ , may cause the system to conclude that the car did, in fact, move in an unexpected direction. It may take several anomalous observations to reach the conclusion that the sensor has failed. By passing these messages backward, we can conclude that the sensor was already broken at $t_{1}$ , allowing us to discount its observation and avoid reaching the incorrect conclusion about the vehicle location. 

We note that smoothing can be executed with diferent time horizons of evidence going forward. That is, we may want to use all of the available evidence, or perhaps just the evidence from some window of a few time slices ahead. 

A ﬁnal task is that of ﬁnding the most likely trajectory of the system, given the evidence — arg $\begin{array}{r}{\operatorname*{max}_{\xi^{(0:T)}}P(\xi^{(0:T)}\mid o^{(1:T)})}\end{array}$ . This task is an instance of the MAP problem. 

In all of these tasks, we are trying to compute answers to standard probabilistic queries. Thus, we can simply use one of the standard inference algorithms that we described earlier in this book. However, this type of approach, applied naively, would require us to run inference on larger and larger networks over time and to maintain our entire history of observations indeﬁnitely. Both of these requirements can be prohibitive in practice. Thus, alternative solutions are necessary to avoid this potentially unbounded blowup in the network size. 

In the remainder of our discussion of inference, we focus mainly on the tracking task, which presents us with many of the challenges that arise in other tasks. The solutions that we present for tracking can generally be extended in a fairly straightforward way to other inference tasks. 

# 15.2 Exact Inference 

We now consider the problem of exact inference in DBNs. We begin by focusing on the ﬁltering problem, showing how the Markovian independence assumptions underlying our representation provide a simple recursive rule that does not require maintaining an unboundedly large repre- sentation. We then show how this recursive rule corresponds directly to the upward pass of inference in the unrolled network. 

# 15.2.1 Filtering in State-Observation Models 

We begin by considering the ﬁltering task for state-observation models. Our goal here is to maintain the belief state $\sigma^{(t)}(X^{(t)})\stackrel{\smile}{=}P(X^{(t)}\mid o^{(1:t)})$ . As we now s e can ide a simple recursive algorithm for propagating these belief states, computing $\sigma^{(t+1)}$ from σ $\sigma^{(t)}$ . 

Initially, $P(X^{(0)})$ is precisely $\bar{\sigma^{(0)}}$ . Now, assume that we have already computed $\sigma^{(t)}(\boldsymbol{X}^{(t)})$ . To compute $\sigma^{(t+1)}$ based on $\sigma^{(t)}$ and the evidence $o^{(t+1)}$ , we ﬁrst propagate the state forward: 

$$
\begin{array}{l l l}{\sigma^{(\cdot t+1)}(X^{(t+1)})}&{=}&{P(X^{(t+1)}\mid o^{(1:t)})}\\ &{=}&{\displaystyle\sum_{X^{(t)}}P(X^{(t+1)}\mid X^{(t)},o^{(1:t)})P(X^{(t)}\mid o^{(1:t)})}\\ &{=}&{\displaystyle\sum_{X^{(t)}}P(X^{(t+1)}\mid X^{(t)})\sigma^{(t)}(X^{(t)}).}\end{array}
$$ 

In words, this expression is the beliefs over the state variables at time $_{t+1}$ , given the observations only up to time $t$ ( dicated by the $\cdot$ in the superscript). We can call this expression the prior belief state at time $t+1$ . In the next step, we condition this prior belief state to account for the 

![](images/a3a4d7af133e17d89f8a840582083bc6feea2ba24b61ac064d34bd8c75fb2bb0.jpg) 
Figure 15.1 Clique tree for HMM 

most recent observation $o^{(t+1)}$ : 

$$
\begin{array}{l c l}{\sigma^{(t+1)}(\pmb{X}^{(t+1)})}&{=}&{P(\pmb{X}^{(t+1)}\mid o^{(1:t)},o^{(t+1)})}\\ &{=}&{\displaystyle\frac{P(o^{(t+1)}\mid\pmb{X}^{(t+1)},o^{(1:t)})P(\pmb{X}^{(t+1)}\mid o^{(1:t)})}{P(o^{(t+1)}\mid o^{(1:t)})}}\\ &{=}&{\displaystyle\frac{P(o^{(t+1)}\mid\pmb{X}^{(t+1)})\sigma^{(\cdot t+1)}(\pmb{X}^{(t+1)})}{P(o^{(t+1)}\mid o^{(1:t)})}.}\end{array}
$$ 

recursive ﬁlter 

This simple recursive ﬁltering procedure maintains the belief state over time, without keeping track of a network or a sequence of observations of growing length. To analyze the cost of this operation, let $N$ be the number of states at each time point and $T$ the total number of time slices. The belief-state forward-propagation step considers every pair of states $s,s^{\prime}$ , and therefore it has a cost of $O(N^{2})$ . The conditioning step considers every state $s^{\prime}$ (multiplying it by the evidence likelihood and then renormalizing), and therefore it takes $O(N)$ time. Thus, the overall time cost of the message-passing algorithm is $O(N^{2}T)$ . The space cost of the algorithm is $O(N^{2})$ , which is needed to maintain the state transition model. 

# 15.2.2 Filtering as Clique Tree Propagation 

forward pass The simple recursive ﬁltering process is closely related to message passing in a clique tree. To understand this relationship, we focus on the simplest state-observation model — the HMM. Consider the process of exact clique tree inference, applied to the DBN for an HMM (as shown in ﬁgure 6.2). One possible clique tree for this network is shown in ﬁgure 15.1. Let us examine the messages passed in this tree in a sum-product clique tree algorithm, taking the last clique in the chain to be the root. In this context, the upward pass is also called the forward pass . 

ﬁrs $S^{(1)}$ nts $\begin{array}{r l r}{\sum_{{\cal S}^{(0)}}P({\cal S}^{(0)})\dot{P({\cal S}^{(1)}}}&{{}|}&{\dot{{\cal S}}^{(0)})\;=}\end{array}$ $P(S^{(1)})$ . The next message, sent from the S $S^{(1)},{\cal O}^{(1)}$ clique, also has the scope S $S^{(1)}$ , and represents ${\cal P}(S^{(1)}){\cal P}(o^{(1)}\mid S^{(1)})={\cal P}(S^{(1)},o^{(1)})$ | . Note renormalize this message to sum to 1, we obtain $P(S^{(1)}\mid o^{(1)})$ which is p $\sigma^{(1)}(S^{(1)})$ . Continuing, one that the message from the $S^{(1)},S^{(2)}$ clique is $P(S^{(2)},o^{(1)})$ , and the one from the $S^{(2)},O^{(2)}$ clique is $P(S^{(2)},o^{(1)},o^{(2)})$ . Once again, if we renormalize this last message to sum to 1, we obtain exactly $P(S^{(2)}\mid o^{(\overset{.}{1})},o^{(\overset{.}{2})})=\bar{\sigma}^{(2)}(S^{(2)})$ . 

We therefore see that the forward pass of the standard clique-tree message passing algorithm provides us with a solution to the ﬁltering problem. A slight variant of the algorithm gives us precisely the recursive update equations of equation (15.1) and (15.2). Speciﬁcally, assume that we normalize the messages from the $S^{(1)},O^{(1)}$ clique as they are sent, resulting in a probability distribution. (As we saw in exercise 9.3, such a normalization step has no efect on the belief state computed in later stages, and it is beneﬁcial in reducing underﬂow.) 1 

It is not hard to see that, with this slight modiﬁcation, the sum-product message passing algorithm results in precisely the recursive update equations shown here: The message pass- ing step executed by the $S^{\dot{(t)}},S^{(t+1)}$ clique is precisely equation (15.1), whereas the message passing step executed by the $S^{(t+1)},O^{(t+\bar{1})}$ clique is precisely equation (15.2), with the division corresponding to the renormalization step. 

Thus, the upward (forward) pass of the clique tree algorithm provides a solution to the ﬁltering task, with no need for a downward pass. Similarly, for the prediction task, we can use essentially the same message-passing algorithm, but without conditioning on the (unavailable future) evidence. In terms of the clique tree formulation, the unobserved evidence nodes are barren nodes, and they can thus be dropped from the network; thus, the $S^{(t)},O^{(t)}$ cliques would simply disappear. When viewed in terms of the iterative algorithm, the operation of equation (15.2) would be eliminated. 

For the smoothing task, however, we also need to propagate messages backward in time. Once again, this task is clearly an inference task in the unrolled DBN, which can be accomplished using a clique tree algorithm. In this case, messages are passed in both directions in the clique tree. The resulting algorithm is known as the forward-backward algorithm . In this algorithm, the backward messages also have semantics. Assume that our clique tree is over the time slices $0,\cdot\cdot\cdot,T$ . If we use the variable-elimination message passing scheme (without renormalization), the backward message sent to the clique $S^{(t)},S^{(t+1)}$ represents $P(o^{((t+1):T)}\mid S^{(t+1)})$ . If we use the belief propagation scheme, the backward message sent to this clique represents the fully informed (smoothed) distribution $P(S^{(t+1)}\mid o^{(1:T)})$ . (See exercise 15.1.) 

For the smoothing task, we need to keep enough information to reconstruct a full belief state at each time $t$ . Naively, we might maintain the entire clique tree at space cost $O(N^{2}T)$ . However, by more carefully analyzing the role of cliques and messages, we can reduce this cost considerably. Consider variable-elimination message passing; in this case, the cliques contain only the initial clique potentials, all of which can be read from the 2-TBN template. Thus, we can cache only the messages and the evidence, at a total space cost of $O(N T)$ . Unfortunately, space requirements that grow linearly in the length of the sequence can be computationally prohibitive when we are tracking the system for extended periods. (Of course, some linear growth is unavoidable if we want to remember the observation sequence; however, the size of the state space $N$ is usually much larger than the space required to store the observations.) Exercise 15.2 discusses one approach to reducing this computational burden using a time-space trade-of. 

# 15.2.3 Clique Tree Inference in DBNs 

The clique tree perspective provides us with a general algorithm for tracking in DBNs. To derive the algorithm, let us consider the clique tree algorithm for HMMs more closely. 

Although we can view the ﬁltering algorithm as performing inference on an unboundedly large clique tree, we never need to maintain more than a clique tree over two consecutive time slices. Speciﬁcally, we can create a (tiny) clique tree over the variables $S^{(t)},S^{(t+1)},O^{(t+1)}$ ; we then pass in the message $\sigma^{(t)}$ to the $\check{S}^{(\dot{t})},S^{(\bar{t}+1)}$ clique, pass the message to the $S^{(t+1)},O^{(t+1)}$ clique, and extract the outgoing message $\sigma^{(t+1)}$ . We can now forget this time slice’s clique tree and move on to the next time slice’s. 

template clique tree 

It is now apparent that the clique trees for all of the time slices are identical — only the messages passed into them difer. Thus, we can perform this propagation using a template clique tree $\Upsilon$ over the variables in the 2-TBN. In this setting, $\Upsilon$ would contain the two cliques $\{S,S^{\prime}\}$ and $\{S^{\prime},O^{\prime}\}$ , initialized with th potentia $P(S^{\prime}\mid S)$ and $P(O^{\prime}\mid S^{\prime})$ respectively. To propagate the belief state from time t to time t $t+1$ , we pass the time t belief state into $\Upsilon$ multiplying it into the clique $P(S^{\prime}\mid S)$ , taking $S$ to represent $S^{(t)}$ an $S^{\prime}$ sent $S^{(t+1)}$ . We then run inference over this clique tree, including conditioning on $O^{\prime}=o^{(t\bar{+}1)}$ . We can now extract the posterior distribution over $S^{\prime}$ , which is precisely the required belief state $P(S^{(t+1)}\mid o^{(1:(t+1))})$ . This belief state can be used as the input message for the next step of propagation. 

The generalization to arbitrary DBNs is now fairly straightforward. We maintain a belief state $\sigma^{(t)}(\mathcal{X}^{(t)})$ and propagate it forward from time $t$ to time $t+1$ . We perform t s propagation step using clique tree inference as follows: We construct a template clique tree Υ , deﬁned over the variables of the 2-TBN. We then pass a time $t$ message into $\Upsilon$ , and we obtain as the result of clique tree inference in $\Upsilon$ a time $t+1$ message that can be used as the input message for the next step. Most naively, the messages are full belief states, specifying the distribution over all of the unobserved variables. 

In general, however, we can often reduce the scope of the message passed: As can be seen from equation (6.2), only the time $t$ interface variables are relevant to the time $t+1$ distribution. For example, consider the two generalized HMM structures of ﬁgure 6.3. In the factorial HMM structure, all variables but the single observation variable are in the interface, providing little savings. However, in the coupled HMM, all of the private observation variables are not in the interface, leaving a much smaller belief state whose scope is $X_{1},X_{2},X_{3}$ . Observation variables are not the only ones that can be omitted from the interface; for example, in the network of ﬁgure 15.3a, the nonpersistent variable $B$ is also not in the interface. 

reduced belief state 

The algorithm is shown in algorithm 15.1. It passes messages corresponding to reduced belief states $\sigma^{(t)}(\mathcal{X}_{I}^{(t)})$ . At phase $t$ , it passes a me $t-1$ reduced belief state into the template clique tree, calibrates it, and extracts the time t reduced belief state to be used as input for the next step. Note that the clique-tree calibration step is useful not only for the propagation step. It also provides us with other useful conclusions, such as the marginal beliefs over all individual variables $X^{(t)}$ (and some subsets of variables) given the observations $o^{(1:t)}$ . 

# 15.2.4 Entanglement 

The use of a clique tree immediately suggests that we are exploiting structure in the algorithm, and so can expect the inference process to be tractable, at least in a wide range of situations. Unfortunately, that does not turn out to be the case. The problem arises from the need to represent and manipulate the (reduced) belief state $\sigma^{(t)}$ (a process not speciﬁed in the algorithm). Semantically, this belief state is a joint distribution over $\mathcal{X}_{I}^{(t)}$ ; if represented naively, 

![](images/cbad78d78026be1cc940e787b81d20f15b93fd8f4619bc92787dbe7a007a2eb4.jpg) 

it would require an exponential number of entries in the joint. At ﬁrst glance, this argument appears specious. After all, one of the key beneﬁts of graphical models is that high-dimensional distributions can be represented compactly by using factorization. It certainly appears plausible that we should be able to ﬁnd a compact representation for our belief state and use our structured inference algorithms to manipulate it efciently. As we now show, this very plausible impression turns out to be false. 

Example 15.2 

entanglement Consider our car network of ﬁgure 6.1, and consider our belief state at some time t . Intuitively, it seems as if there should be some conditional independence relations that hold in this network. For example, it seems as if Weather (2) and Location (2) should be uncorrelated. Unfortunately, they are not: if we examine the unrolled DBN, we see that there is an active trail between them (1) (0) (1) going through Velocity and Weather , Weather . This path is not blocked by any of the time 2 variables; in particular, Weather (2) and Location (2) are not conditionally independent given Velocity (2) . In general, a similar ysis can be used to show that, for $t\geq2$ , no conditional independence assumptions hold in σ $\sigma^{(t)}$ . 

This phenomenon, known as entanglement , has signiﬁcant implications. As we discussed, there is a direct relationship between conditional independence properties of a distribution and our ability to represent it as a product of factors. Thus, a distribution that has no independence properties does not admit a compact representation in a factored form. 

Unfortunately, the entanglement phenomenon is not speciﬁc to this example. Indeed, it holds for a very broad class of DBNs. We demonstrate it for a large subclass of DBNs that exhibit a very regular structure. We begin by introducing a few useful concepts. 

Deﬁnition 15.1 persistent independence 

Persistent independencies are independence properties of the belief state, and are therefore precisely what we need in order to provide a time-invariant factorization of the belief state. 

The following concept turns out to be a useful one, in this setting and others. 

Deﬁnition 15.2 inﬂuence graph 

et $\mathcal{B}_{\rightarrow}$ e a 2-TBN over $\mathcal{X}$ . We deﬁ the inﬂuence graph for $\mathcal{B}_{\rightarrow}$ to be ed ph $\mathcal{T}$ $\mathcal{X}$ whose nod orrespond to X , and that co directed arc $X\rightarrow Y$ → if $X\rightarrow Y^{\prime}$ → or $X^{\prime}\rightarrow Y^{\prime}$ → appear in B $\mathcal{B}_{\rightarrow}$ → . Note that a persistence arc $X\rightarrow X^{\prime}$ → induces a self-cycle in the inﬂuence graph. 

The inﬂuence graph corresponds to inﬂuence in the unrolled DBN: 

# Proposition 15.1 

fully persistent persistence edge 

# Theorem 15.1 

Let $\mathcal{T}$ be the inﬂuence graph for a 2-TBN $\mathcal{B}_{\rightarrow}$ . Then $\mathcal{T}$ cted from $X$ t $Y$ and only if, in the unrolled DBN, for every $t$ , there exists a path from $X^{(t)}$ to $\dot{Y}^{(t^{\prime})}$ for some $t^{\prime}\geq t$ ≥ . 

The following result demonstrates the inevitability of the entanglement phenomenon, by proving that it holds in a broad class of networks. A DBN is called fully persistent if it encodes a sta ation model, and, for each state variable $X\in X$ , the 2-TBN contains a persistence edge $X\rightarrow X^{\prime}$ . 

Let $\langle\mathcal{G}_{0},\mathcal{G}_{\rightarrow}\rangle$ be a fully persistent DBN structure over $\mathcal{X}=X\cup O$ , where the state variables $X^{(t)}$ are hidden in every time slice, and the observation variables $O^{(t)}$ are observed in every time slice. Furthermore, assume that, in the inﬂuence graph for $\mathcal{G}_{\rightarrow}$ : 

• there is a trail (not necessarily a directed path) between every pair of nodes, that is, the graph is connected; 

• every state variable $X$ has some directed path to some evidence variable in $^o$ . 

Then there is no persistent independence $(X\perp Y\mid Z)$ that holds for every DBN $\langle\mathcal{B}_{0},\mathcal{B}_{\rightarrow}\rangle$ over this DBN structure. 

The proof is left as an exercise (see exercise 15.4). Note that, as in every other setting, there may be spurious independencies that hold due to speciﬁc choices of the parameters. But, for almost all choices of the parameters, there will be no independence that holds persistently. 

# 

In fully persistent DBNs, the tracking problem is precisely one of maintaining a belief state — a distribution over $X^{(t)}$ . The entanglement theorem shows that the only exact representation for this belief state is as a full joint distribution, rendering any belief- state-algorithm computationally infeasible except in very small networks. 

More generally, if we want to track the system as it evolves, we need to maintain a repre- sentation that summarizes all of our information about the past. Speciﬁcally, as we showed in theorem 10.2, any sepset in a clique tree must render the two parts of the tree conditionally independent. In a temporal setting, we cannot allow the sepsets to grow unboundedly with the number of time slices. Therefore, there must exist some sepset over a scope $y$ that cuts across the network, in that any path that starts from a time 0 variable and continues to inﬁnity must intersect $\mathcal{Y}$ . In fully persistent networks, the set of state variables $X^{(t)}$ is a minimal set satisfying this condition. The entanglement theorem states that this set exhibits no persistent in- dependencies, and therefore the message over this sepset can only be represented as an explicit joint distribution. The resulting sepsets are therefore very large — exponential in the number of state variables. Moreover, as these large messages must be incorporated into some clique in the clique tree, the cliques also become exponentially large. 

![](images/0add1514068cd11b0738a65ba87d35c9f8cc6c4f0fa15f38a066f7a1098150b6.jpg) 

Consider again the Car network of ﬁgure 6.1. To support exact belief state propagation, our template clique tree must include a clique containing $W,V,L,F$ , where we can incorporate the previous belief state $\sigma^{(t)}(W^{(t)},V^{(t)},\dot{L^{(t)}},F^{(t)})$ . It must also contain a clique containing $W^{\prime},V^{\prime},L^{\prime},F^{\prime}.$ , from which we can extract $\sigma^{(t+1)}(W^{(t+1)},V^{(t+1)},L^{(t+1)},F^{(t+1)})$ . A minimally sized clique tree containing these cliques is shown in ﬁgure $l5.2a$ . All of the cliques in the tree are of size 5. By contrast, if we were simply to construct a template clique tree over the dependency structure deﬁned by the 2-TBN, we could obtain a clique tree where the maximal clique size is 4, as illustrated in ﬁgure $l5.2b$ . 

A clique size of 4 is the minimum we can hope for: In general, all of the cliques for a fully persistent network over $n$ variables contain at least $n+1$ variables: one representative (either $X$ or $X^{\prime}.$ ) of each variable $X$ in $\mathcal{X}$ , plus an additional variable that we are currently eliminating. (See exercise 15.5.) In many cases, the minimal induced width would actually be higher. For example, if we introduce an arc $L\to V^{\prime}$ into our 2-TBN (for example, because diferent locations have diferent speed limits), the smallest template clique tree allowing for exact ﬁltering has a clique size of 6. 

Even in networks when not all variables are persistent, entanglement is still an issue: We still need to represent a distribution that cuts across the entire width of the network. In most cases 

![](images/bdae79036c40b70a5419bddfa7a3f3500a8846ba653d50329d39665c1116709e.jpg) 
Figure 15.3 Nonpersistent 2-TBN and diferent possible clique trees : (a) A 2-TBN where not all of the unobserved variables are persistent. (b) A clique tree for this 2-TBN that allows exact ﬁltering; as before, $D^{\prime}$ is always observed, and hence it is not in the scope of any clique. (c) A clique tree for the 2-TBN, which does not allow exact ﬁltering. 

— except for speciﬁc structures and observation patterns — these distributions do not exhibit independence structure, and they must therefore be represented as an explicit joint distribution over the interface variables. Because the “width” of the network is often fairly large, this results in large messages and large cliques. 

Consider the 2-TBN shown in ﬁgure 15.3, where not all of the unobserved variables are persistent. In this network, our interface variables are $A,C$ . Thus, we can construct a template clique tree over $A,C,A^{\prime},B^{\prime},C^{\prime},D^{\prime}$ as the basis for our message passing step. To allow exact ﬁltering, we must have a clique whose scope contains $A,C$ and a clique whose scope contains $A^{\prime},C^{\prime}$ . A minimally sized clique tree satisfying these constraints is shown in ﬁgure 15.3b. It has a maximum clique size of 4; without these constraints, we can construct a clique tree where the maximal clique size is 3 (ﬁgure $\ 15.3c)$ ). 

We note that it is sometimes possible to ﬁnd better clique trees than those constrained to use $\mathcal{X}_{I}^{(t)}$ as the message. In fact, in some cases, the best sepset actually spans variables in multiple time slices. However, these improvements do not address the fundamental problem, which is that the computational cost of exact inference in DBNs grows exponentially with the “width” of the network. Speciﬁcally, we cannot avoid including in our messages at least the set of persistent variables. In many applications, a large fraction of the variables are persistent, rendering this approach intractable. 

# 15.3 Approximate Inference 

The computational problems with exact inference force us, in many cases, to fall back on approximate inference. In principle, when viewing the DBN as a large unrolled BN, we can apply any of the approximate inference algorithms that we discussed for BNs. Indeed, there has been signiﬁcant success, for example, in using a variational approach for reasoning about weakly coupled processes that evolve in parallel. (See exercise 15.6.) 

However, several complications arise in this setting. First, as we discussed in section 15.1, the types of tasks that we wish to address in the temporal setting often involve reasoning about arbitrarily large, and possibly unbounded, trajectories. Although we can always address these tasks using inference over the unrolled DBNs, as in the case of exact inference, algorithms that require us to maintain the entire unrolled BN during the inference process may be impractical. In the approximate case, we must also address an additional complication: An approximate inference algorithm that achieves reasonable errors for static networks of bounded size may not work well in arbitrarily large networks. Indeed, the quality of the approximation may degrade with the size of the network. 

For the remainder of this section, we focus on the ﬁltering task. As in the case of exact inference, methods developed for ﬁltering extend directly to prediction, and (with a little work) to smoothing with a bounded lookahead. There are many algorithms that have been proposed for approximate tracking in dynamic systems, and one could come up with several others based on the methods described earlier in this book. We begin by providing a high-level overview of a general framework that encompasses these algorithms. We then describe two speciﬁc methods that are commonly used in practice, one that uses a message passing approach and the other a sampling approach. 

# 15.3.1 Key Ideas 

# 15.3.1.1Bounded History Updates 

A general approach to addressing the ﬁltering (or prediction) task without maintaining a full history is related to the approach we used for exact ﬁltering. There, we passed messages in the clique tree forward in time, which allowed us to throw away the observations and the cliques in previous time slices once they have been processed. 

In principle, the same idea can be applied in the case of approximate inference: We can execute the appropriate inference steps for a time slice and then move forward to the next time slice. However, most approximate inference algorithms require that the same variable in the network be visited multiple times during the course of inference. For example, belief propagation algorithms (as in section 11.3 or section 11.4) send multiple messages through the same cluster. Similarly, structured variational approximation methods (as in section 11.5) are also iterative, running inference on the network multiple times, with diferent values for the variational parameters. Markov chain Monte Carlo algorithms also require that each node be visited and sampled multiple times. Thus, we cannot just throw away the history and apply our approximate inference to the current time slice alone. 

One common solution is to use a form of “limited history” in the inference steps. The various steps associated with the inference algorithm are executed not over the whole network, but over a subnetwork covering only the recent history. Most simply, the subnetwork for time $t$ is simply a bounded window covering some predetermined number $k$ of previous time slices $t-k,\ldots,t-1,t$ . More generally, the subnetwork can be determined in a dynamic fashion, using a variety of techniques. 

We will describe the two methods most commonly used in practice, one based on importance sampling, and the other on approximate message propagation. Both take $k\,=\,1$ , using only the current approximate belief state $\hat{\sigma}^{(t)}$ and the current time slice in estimating the next approximate belief state $\hat{\sigma}^{(t+1)}$ . In efect, these methods perform a type of approximate message propagation, as in equation (15.1) and equation (15.2). Although this type of approximation is clearly weak in some cases, it turns out to work fairly well in practice. 

# 15.3.1.2 Analysis of Convergence 

The idea of running approximate inference with some bounded history over networks of increas- ing size immediately raises concerns about the quality of the approximation obtained. Consider, for example, the simplest approximate belief-state ﬁltering process. Here, we maintain an ap- proximate belief state $\hat{\sigma}^{(t)}$ , which is (hopefully) similar to our true belief state $\sigma^{(t)}$ . We use $\bar{\hat{\sigma}}^{(t)}$ to compute the subsequent belief state $\hat{\sigma}^{(t+1)}$ . This step uses approximate inference and therefore introduces some additional error into our approximation. Therefore, as time evolves, our approximation appears to be accumulating more and more errors. In principle, it might be the case that, at some point, our approximate belief state $\hat{\sigma}^{(t)}$ bears no resemblance to the true belief state $\sigma^{(t)}$ . 

Although unbounded errors can occur, it turns out that such situations are rare in practice (for algorithms that are carefully designed). The main reason is that the dynamic system itself is typically stochastic. Thus, the efect of approximations that occur far in the past tends to diminish over time, and the overall error (for well-designed algorithms) tends to remain bounded indeﬁnitely. For several algorithms (including the two described in more detail later), one can prove a formal result along these lines. All of these results make some assumptions about the stochasticity of the system — the rate at which it “forgets” the past. Somewhat more formally, assume that propagating two distributions through the system dynamics (equation (15.1) and 15.2) reduces some notion of distance between them. In this case, discrepancies between $\hat{\sigma}^{(t)}$ and $\sigma^{(t)}$ , which result from approximations in previous time slices, decay over time. Of course, new errors are introduced by subsequent approximations, but, in stochastic systems, we can show that they do not accumulate unboundedly. 

Formal theorems proving a uniform bound on the distance between the approximate and true belief state — a bound that holds for all time points $t$ — exist for a few algorithms. These theorems are quite technical, and the actual bounds obtained on the error are fairly large. For this reason, we do not present them here. However, in practice, when the underlying system is stochastic, we do see a bounded error for approximate propagation algorithms. Conversely, when the system evolution includes a deterministic component — for example, when the state contains a variable that (once chosen) does not evolve over time — the errors of the approximate inference algorithms often do diverge over time. Thus, while the speciﬁc bounds obtain in the theoretical analyses may not be directly useful, they do provide a theoretical explanation for the behavior of the approximate inference algorithms in practice. 

# 15.3.2 Factored Belief State Methods 

 The issue underlying the entanglement result is that, over time, all variables in a belief state slice eventually become correlated via active trails through past time slices. In many cases, however, these trails can be fairly long, and, as a consequence, the resulting correlations can be quite weak. This raises the idea of replacing the exact, fully correlated, belief state, with an approximate, factorized belief state that imposes some independence assumptions. For a carefully chosen factorization structure, these independence assumptions may be a reasonable approximation to the structure in the belief state. 

belief-state projection 

This idea gives rise to the following general structure for a ﬁltering algorithm: At each time point $t$ , we have a factored representation $\hat{\sigma}^{(t)}$ of our time $t$ belief state. We then compute the correct update of this time $t$ belief state to produce a new time $t+1$ belief state $\sigma^{(\cdot t\bar{+}1)}$ . The update step consists of propagating the belief state forward through the system dynamics and conditioning on the time $t+1$ observations. Owing to the correlations induced by the system dynamics (as in section 15.2.4), $\sigma^{(\cdot t+1)}$ has more correlations than $\hat{\sigma}^{(t)}$ , and therefore it requires larger factors to represent correctly. If we continue this process, we rapidly end up with a belief state that has no independence structure and must be represented as a full joint distribution. Therefore, we introduce a projection step, where we approximation $\sigma^{(\cdot t+1)}$ using a more factored representation, giving rise to a new $\hat{\sigma}^{(t+1)}$ , with which we continue the process. This update- project cycle ensures that our approximate belief state remains in a class of distributions that we can tractably maintain and update. 

Most simply, we can represent the approximate belief state $\hat{\sigma}^{(t)}$ in terms of a set of factors

 $\Phi^{(t)}=\{\beta_{r}^{(\bar{t})}\dot{(}X_{r}^{(t)})\}$ } , where assume (for simplicity) that the factorization of the messages

 (that is, the choice of scopes $X_{r}$ ) does not change over time. Most simply, the scopes of the diferent factors are disjoint, in which case the belief state is simply a product of marginals over disjoint variables or subsets of variables. As a richer but more complex representation, we can represent $\hat{\sigma}^{(t)}$ using a calibrated cluster tree, or even a calibrated cluster graph $\mathcal{U}$ . Indeed, we can even use a general representation that uses overlapping regions and associated counting numbers: 

$$
\hat{\sigma}^{(t)}(\mathcal{X}^{(t)})=\prod_{r}(\beta_{r}^{(t)}(X_{r}^{(t)}))^{\kappa_{r}}.
$$ 

Example 15.5 Consider the task of monitoring a freeway with $k$ cars. As we discussed, after a certain amount of time, the states of the diferent cars become entangled, so our only option for representing the belief state is as a joint distribution over the states of all the cars. An obvious approximation is to assume that the correlations between the diferent cars are not very strong. Thus, although the cars do inﬂuence each other, the current state of one car does not tell us too much about the current state of another. Thus, we can choose to approximate the belief state over the entire system using an approximate belief state that ignores or approximates these weak correlations. Speciﬁcally, let $\mathbf{\boldsymbol{Y}}_{i}$ be the set of variables representing the state of car $i$ , and let $Z$ be a set of variables that encode global conditions, such as the weather or the current trafc density. Most simply, we can represent the belief state $\hat{\sigma}^{(t)}$ as a product of marginals 

$$
\beta_{g}^{(t)}(\mathbf{Z}^{(t)})\prod_{i=1}^{k}\beta_{i}^{(t)}(\mathbf{Y}_{i}^{(t)}).
$$ 

In a better approximation, we might preserve the correlations between the state of each individual vehicle and the global system state, by selecting as our factorization 

$$
\Big(\beta_{g}^{(t)}(Z^{(t)})\Big)^{-(k-1)}\prod_{i=1}^{k}\beta_{i}^{(t)}(Z^{(t)},Y_{i}^{(t)}),
$$ 

where the initial term compensates for the multiple counting of the probability of $Z^{(t)}$ in the other factors. Here, the representation of the approximate belief state makes the assumption that the states of the diferent cars are conditionally independent given the global state variables. 

assumed density ﬁlter expectation propagation 

We showed that exact ﬁltering is equivalent to a forward pass of message passing in a clique tree, with the belief states playing the role of messages. Hence, ﬁltering with factored belief states is simply a form of message passing with approximate messages. The use of an approximate belief state in a particular parametric class is also known as assumed density ﬁltering . This algorithm is a special case of the more general algorithm that we developed in the context of the expectation propagation (EP) algorithm of section 11.4. Viewed abstractly, each slice-cluster in our monolithic DBN clique tree (one that captures the entire trajectory) corresponds to a pair of adjacent time slices $t-1,t$ and contains the variables $\mathcal{X}_{I}^{(t)}\cup\mathcal{X}^{(t+1)}$ ∪X . As in the general EP algorithm, the potential in a slice-cluster is never represented explicitly, but in a decomposed form, as a product over factors. Each slice-cluster is connected to its predecessor and successor slice-clusters. The messages between these slice-clusters correspond to approximate belief states $\hat{\sigma}^{(t)}(\mathcal{X}_{I}^{(t)})$ X , which are represented in a factorized form. For uniformity of exposition, we assume that the initial state distribution — the time 0 belief state — also takes (or is approximated as) the same form. Thus, when propagating messages in this chain, each slice-cluster takes messages in this factorized form and produces messages in this form. 

As we discussed in section 11.4.2, the use of factorized messages allows us to perform the operations in each cluster much more efciently, by using a nested clique tree or cluster graph that exploits the joint structure of the messages and cluster potential. For example, if the belief state is fully factored as a product over the variables in the interface, the message structure imposes no constraints on the nested data structure used for inference within a time slice. In particular, we can use any clique tree over the 2-TBN structure; for instance, in example 15.3, we can use the structure of ﬁgure 15.2b. By contrast, for exact ﬁltering, the messages are full belief states over the interface variables, requiring the use of a nested clique tree with very large cliques. Of course, a fully factorized belief state generally provides a fairly poor approximation to the belief state. As we discussed in the context of the EP algorithm, we can also use much more reﬁned approximations, which use a clique tree or even a general region-based approximation to the belief state. 

The algorithm used for the message passing is precisely as we described in section 11.4.2, and we do not repeat it here. We make only three important observations. First, unlike a traditional application of EP, when doing ﬁltering, we generally do only a single upward pass of message propagation, starting at time 0 and propagating toward higher time slices. Because we do not have a backward pass, the distinctions between the sum-product algorithm (section 11.4.3.1) and the belief update algorithm (section 11.4.3.2) are irrelevant in this setting, since the diference arises only in the backward pass. Second, without a backward pass, we do not need to keep track of a clique once it has propagated its message forward. Thus, as in exact inference for DBNs, we can keep only a single (factored) message and single (factored) slice-cluster in memory at each point in time and perform the message propagation in space that is constant in the number of time slices. 

If we continue to assume that the belief state representation is the same for every time slice, then the factorization structure used in each of the message passing steps is identical. In this case, we can perform all the message passing steps using the same template cluster graph that has a ﬁxed cluster structure and ﬁxed initial factors (those derived from the 2-TBN); at each time $t$ , the factors representing $\hat{\sigma}^{(t)}$ are introduced into the template cluster graph, which is then calibrated and used to produce the factors representing $\hat{\sigma}^{(t+1)}$ . The reuse of the template can reduce the cost of the message propagation step. An alternative approach allows the structure used in our approximation to change over time. This ﬂexibility allows us to adapt our structure to reﬂect the strengths of the interactions between the variables in our domain. For example, in example 15.5, we might expect the variables associated with cars that are directly adjacent to be highly correlated; but the pairs of cars that are close to each other change over time. Section 15.6 describes some methods for dynamically adapting the representation to the current distribution. 

# 15.3.3 Particle Filtering 

Of the diferent particle-based methods that we discussed, forward sampling appears best suited to the temporal setting, since it generates samples incrementally, starting from the root of the network. In the temporal setting, this would correspond to generating trajectories starting from the beginning of time, and going forward. This type of sampling, we might hope, is more amenable to a setting where we do not have to keep sampled trajectories that go indeﬁnitely far back. Obviously, rejection sampling is not an appropriate basis for a temporal sampling algorithm. For an indeﬁnitely long trajectory, all samples will eventually be inconsistent with our observations, so we will end up rejecting all samples. In this section, we present a family of ﬁltering algorithms based on importance sampling and analyze their behavior. 

# 15.3.3.1 Naive Likelihood Weighting 

It is fairly straightforward to generalize likelihood weighting to the temporal setting. Recall that LW generates samples by sampling nodes that are not observed from their appropriate distribution, and instantiating nodes that are observed to their observed values. Every node that is instantiated in this way causes the weight of the sample to be changed. However, LW generates samples one at a time, starting from the root and continuing until a full assignment is generated. In an online setting, we can have arbitrarily many variables, so there is no natural end to this sampling process. Moreover, in the ﬁltering problem, we want to be able to answer queries online as the system evolves. Therefore, we ﬁrst adapt our sampling process to return intermediate answers. 

The LW algorithm for the temporal setting maintains a set of samples, each of which is a trajectory up to the current time slice $t$ : $\bar{\xi}^{(t)}[1],\dots,\xi^{(t)}[M]$ . Each sampled trajectory is associated with a weight $w[m]$ . At each time slice, the algorithm takes each of the samples, propagates it forward to sample the variables at time $t$ , and adjusts its weight to suit the new evidence at time $t$ . The algorithm uses a likelihood-weighting algorithm as a subroutine to propagate a time $t$ sample to time $t+1$ . The version of the algorithm for 2-TBNs is almost identical to the algorithm 12.2; it is shown in algorithm 15.2 primarily as a reminder. 

Algorithm 15.2 Likelihood-weighted particle generation for a 2-TBN Procedure LW-2TBN ( B // 2-TBN → $\xi$ tiation to time $t-1$ variables $\dot{\boldsymbol{O}}^{(t)}=\boldsymbol{o}^{(t)}$ // time $t$ evidence ) 1 Let $X_{1}^{\prime},\dots,X_{n}^{\prime}$ be a topological ordering of $\mathcal{X}^{\prime}$ in $\mathcal{B}_{\rightarrow}$ 2 $w\gets\ 1$ 3 for $i=1,\dots,n$ 4 $\mathbf{\alpha}\mathbf{\alpha}_{i}\gets\mathbf{\alpha}(\xi,\mathbf{x}^{\prime})\langle\mathrm{Pa}_{X_{i}^{\prime}}\rangle$ i ⟩ 5 // Assignment to $\mathrm{Pa}_{X_{i}^{\prime}}$ in $x_{1},.\.\,.\,,x_{n},x_{1}^{\prime},.\.\,.\,,x_{i-1}^{\prime}$ − 6 if $X_{i}^{\prime}\notin O^{(t)}$ ̸∈ then 7 Sample $x_{i}^{\prime}$ from $P(X_{i}^{\prime}\mid\mathbf{\mu}\mathbf{u}_{i})$ | 8 else 9 $x_{i}^{\prime}\gets\textbf{}o^{(t)}\langle X_{i}^{\prime}\rangle$ ⟩ // Assignment to $X_{i}^{\prime}$ in $\mathbf{\chi}_{o}^{(t)}$ 10 $w\gets\ w\cdot P(x_{i}^{\prime}\mid\mathbf{\em u}_{i})$ ← · // Multiply weight by probability of desired value 11 return $(x_{1}^{\prime},.\,.\,.\,,x_{n}^{\prime}),w$ 

# Algorithm 15.3 Likelihood weighting for ﬁltering in DBNs 

Procedure LW-DBN ( ⟨B , B ⟩ , // DBN 0 → M // Number of samples o (1) , o (2) , . . . // Observation sequence ) 1 for $m=1,\cdot\cdot\cdot,M$ 2 Sample $\xi^{(0)}[m]$ from $\mathcal{B}_{0}$ 3 $w[m]\leftarrow\mathrm{~1~}$ 4 for $t=1,2,\ldots.$ 5 for $m=1,\dots,M$ 6 $(\xi^{(t)}[m],w)\gets\mathsf{L W-2T B N}(\mathcal{B}_{\rightarrow},\xi^{(t-1)}[m],o^{(t)})$ 7 $//$ Sample time $t$ variables starting from time $t-1$ sample 8 $w[m]\gets\ w[m]\cdot w$ 9 // Multiply weight of $m$ ’th sample with weight of time $t$ evi- dence 10 $\begin{array}{r}{\hat{\sigma}^{(t)}(\boldsymbol{\xi})\gets\frac{\sum_{m=1}^{M}w[m]\pmb{I}\{\xi^{(t)}[m]=\xi\}}{\sum_{m=1}^{M}w[m]}}\end{array}$ 

Unfortunately, this extension of the basic LW algorithm is generally a very poor algorithm for DBNs. To understand why, consider the application of this algorithm to any state-observation 

![](images/5cd8907cf95f138b11f03c8db1dd3f09c761a7c63c69db9a9a9de83ae2c48567.jpg) 
Figure 15.4 Performance of likelihood weighting over time with diferent numbers of samples, for a state-observation model with one state and one observation variable. 

model. In this case, we have a very long network, where all of the evidence is at the leaves. Unfortunately, as we discussed, in such networks, LW generates samples according to the prior distribution, with the evidence afecting only the weights. In other words, the algorithm generates completely random state trajectories, which “match” the evidence only by chance. For example, in our Car example, the algorithm would generate completely random trajectories for the car, and check whether one of them happens to match the observed sensor readings for the car’s location. Clearly, the probability that such a match occurs — which is precisely the weight of the sample — decreases exponentially (and quite quickly) with time. This problem can also arise in a static BN, but it is particularly severe in this setting, where the network size grows unboundedly. In this case, as time evolves, more and more evidence is ignored in the sample-generation process (afecting only the weight), so that the samples become less and less relevant. Indeed, we can see in ﬁgure 15.4 that, in practice, the samples generated get increasingly irrelevant as $t$ grows, so that LW diverges rapidly as time goes by. From a technical perspective, this occurs because, over time, the variance of the weights of the samples grows very quickly, and unboundedly. Thus, the quality of the estimator obtained from this procedure — the probability that it returns an answer within a certain error tolerance — gets increasingly worse. 

particle ﬁlter 

sequential importance sampling 

One approach called particle ﬁltering (or sequential importance sampling ) for addressing this problem is based on the key observation that not all samples are equally “good.” In particular, samples that have higher weight explain the evidence observed so far much better, and are likely to be closer to the current state. Thus, rather than propagate all samples forward to the next time step, we should preferentially select “good” samples for propagation, where “good” samples are ones that have high weight. There are many ways of implementing this basic intuition: We can select samples for propagation deterministic ally or stochastically. 

We can use a ﬁxed number of samples, or vary the number of samples to achieve a certain quality of approximation (estimated heuristically). 

# 15.3.3.2 The Bootstrap Filter 

bootstrap ﬁlter The simplest and most common variant of particle ﬁltering is called the bootstrap ﬁlter . It maintains a set $\mathcal{D}^{(t)}$ of $M$ time $t$ trajectories $\bar{\pmb{x}}^{(0:t)}[m]$ , each associated with its own weight $w^{(t)}[m]$ . When propagating samples to the next time slice, each sample is chosen randomly for propagation, proportionately to its current weight. The higher the weight of the sample, the more likely it is to be selected for propagation; thus, higher-weight samples may “spawn” multiple copies, whereas lower-weight ones “die of” to make space for the others. 

More formally, consider a data set $\mathcal{D}^{(t)}$ consisting of $M$ weighted sample trajectories $(\bar{\pmb{x}}^{(0:t)}[m]$ , $w^{(t)}[m]_{\ell}$ ) . We can deﬁne the empirical distribution generated by the data set: 

$$
\hat{P}_{\mathcal{D}^{(t)}}(\pmb{x}^{(0:t)})\propto\sum_{m=1}^{M}w^{(t)}[m]\pmb{I}\{\bar{\pmb{x}}^{(0:t)}[m]=\pmb{x}^{(0:t)}\}.
$$ 

This distribution is a weighted sum of delta distributions, where the probability of each assign- ment is its total weight in $\mathcal{D}^{(t)}$ , ormalized to sum to 1. 

The algorithm then generates M new samples for time $t+1$ as follows: For each sample $m$ , it selects a time $t$ sample for propagation by randomly sampling from $\hat{P}_{\mathcal{D}^{(t)}}$ . Each of the $M$ D selected samples is used to generate a new time $t+1$ sample using the transition model, which is weighted using the observation model. Note that the weight of the sample $w^{(t)}[m]$ manifests in the relative proportion with which the $m$ th sample is propagated. Thus, we do not need to account for its previous weight when determining the weight of the time $t+1$ sample generated from it. If we did include its weight, we would efectively be double-counting it. The algorithm is shown in algorithm 15.4 and illustrated in ﬁgure 15.5. 

We can view $\hat{P}_{\mathcal{D}^{(t)}}$ as an approximation to the time $t$ belief state (one where only the sampled D states have nonzero probability), and the sampling step as using it to generate an approximate belief state for time $t+1$ . Thus, this algorithm can be viewed as performing a stochastic version of the belief-state ﬁltering process. 

Note that we view the algorithm as maintaining entire trajectories $\bar{\mathbfcal{x}}^{(0:t)}$ , rather than simply the current state. In fact, each sample generated does correspond to an entire trajectory. However, for the purpose of ﬁltering, the earlier parts of the trajectory are not relevant, and we can throw out all but the current state $\bar{\mathbfit{x}}^{(t)}$ . 

The bootstrap particle ﬁlter works much better than likelihood weighting, as illustrated in ﬁgure 15.6a. Indeed, the error seems to remain bounded indeﬁnitely over time (b). 

We can generalize the basic bootstrap ﬁlter along two dimensions. The ﬁrst modiﬁes the forward sampling procedure — the process by which we extend a partial trajectory $\bar{\pmb{x}}^{(0:t-1)}$ to include a time $t$ state variable assignment $\dot{\bar{\mathbfit{x}}}^{(t)}$ . The second modiﬁes the particle selection scheme, by which we take a set of weighted time $t$ samples $\mathcal{D}^{(t)}$ and use their weights to select a new set of time t samples. We will describe these two extensions in more detail. 

![](images/7ecd8377bb150dedf523c9c98d8a74d88a5d9a5e5a465c2d9fabb00313660c6b.jpg) 
Figure 15.5 Illustration of the particle ﬁltering algorithm. (Adapted with permission from van der Merwe et al. (2000a).) At each time slice, we begin with a set of weighted samples (dark circles), we sample from them to generate a set of unweighted samples (light circles). We propagate each sample forward through the system dynamics, and we update the weight of each sample to reﬂect the likelihood of the evidence (black line), producing a new set of weighted samples (some of which have weight so small as to be invisible). The process then repeats for the next time slice. 

# 15.3.3.3 Sequential Importance Sampling 

We can generalize our forward sampling process by viewing it in terms of importance sampling, as in section 12.2.2. Here, however, we are sampling entire trajectories rather than static states. Our goal is to sample a trajectory $\bar{\mathbfcal{x}}^{(0:t)}$ from the distribution $P(\pmb{x}^{(0:t)}\mid\pmb{o}^{(0:t)})$ . To use importance sampling, we must construct a proposal distribution α for trajectories and then use importance weights to correct for the diference between our proposal distribution and our target distribution. 

To maintain the ability to execute our ﬁltering algorithm in an online fashion, we must construct our proposal distribution so that trajectories are constructed incrementally. Assume that, at time $t_{j}$ , we have sampled some set of partial trajectories $\mathcal{D}^{(t)}$ , each possibly associated with some weight. If we want to avoid the need to maintain full trajectories and a full observation Algorithm 15.4 Particle ﬁltering for DBNs 

Procedure Particle-Filter-DBN ( $\langle\mathcal{B}_{0},\mathcal{B}_{\rightarrow}\rangle$ , // DBN $M\mathrm{~\ensuremath~{~\mu~}~}/I$ Number of samples o (1) , o (2) , . . . // Observation sequence ) 1 for $m=1,\cdot\cdot\cdot,M$ 2 Sample $\bar{\pmb x}^{(0)}[m]$ from $\mathcal{B}_{0}$ 3 $w^{(0)}[m]\gets\mathsf{\bar{1}}/\mathsf{\bar{M}}$ 4 for $t=1,2,\ldots.$ . 5 for $m=1,\dots,M$ 6 Sample $\bar{\pmb{x}}^{(0:t-1)}$ from the distribution $\hat{P}_{\mathcal{D}^{(t-1)}}$ . D 7 $//$ Select sample for propagation 8 $(\bar{\pmb x}^{(t)}[m],w^{(t)}[m])\gets\mathsf{L W-2T B N}(\mathscr{B}_{\rightarrow},\bar{\pmb x}^{(t-1)},o^{(t)})$ 9 // Generate time $t$ sample and weight from selected sample 10 $\begin{array}{c}{{\bar{\pmb x}^{(t-1)}}}\\ {{\pmb D^{(t)}\leftarrow~\{(\bar{\pmb x}^{(0:t)}[m],w^{(t)}[m])~:m=1,.\,.\,,M\}}}\\ {{\hat{\sigma}^{(t)}({\pmb x})\leftarrow~\hat{P}_{\pmb D^{(t)}}}}\end{array}$ 11 

![](images/ede394765cc75fbeb80f0bcbb710eaf4b4f0400ee7f6afbc5c7e7754439be843.jpg) 
Figure 15.6 Likelihood weighting and particle ﬁltering over time. (a) A comparison for 1,000 time slices. (b) A very long run of particle ﬁltering. 

history, each of our proposed sample trajectories for time $t+1$ must be an extension of one of our time $t$ sample trajectories. More precisely, each proposed trajectory at time $t+1$ must have the form $\bar{\mathbfit{x}}^{(0:t)},\mathbfit{x}^{(t+1)}$ , for some $\bar{\mathbf{\mathit{x}}}^{(0:t)}[m]\in\dot{\mathbf{\mathit{D}}}^{(t)}$ . 

Note that this requirement, while desirable from a computational perspective, does have disadvantages. In certain cases, our set of time $t$ sample trajectories might be unrepresentative of the true underlying distribution; this might occur simply because of bad luck in sampling, or because our evidence sequence up to time $t$ was misleading, causing us to select for trajectories that turn out to be a bad match to later observations. Thus, it might be desirable to rejuvenate our sample trajectories, allowing the states prior to time $t$ to be modiﬁed based on evidence observed later on. However, this type of process is difcult to execute efciently, and is not often done in practice. 

If we proceed under the previous assumption, we can compute the appropriate importance weights for our importance sampling process incrementally: 

$$
\begin{array}{r c l}{w(\bar{\mathbf{x}}^{(0:t)})}&{=}&{\displaystyle\frac{P(\pmb{x}^{(0:t)}\mid\pmb{o}^{(0:t)})}{\alpha^{(t)}(\pmb{x}^{(0:t)})}}\\ &{=}&{\displaystyle\frac{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t)})}{\alpha^{(t-1)}(\pmb{x}^{(0:t-1)})}\frac{P(\bar{\pmb{x}}^{(t)}\mid\pmb{x}^{(0:t-1)},\pmb{o}^{(0:t)})}{\alpha^{(t)}(\bar{\pmb{x}}^{(t)}\mid\pmb{x}^{(0:t-1)})}.}\end{array}
$$ 

As we have discussed, the quality of an importance sampler is a function of the variance of the weights: the lower the variance, the better the sampler. Thus, we aim to choose our proposal distribution $\alpha^{(t)}(\bar{\mathbf{\mathit{x}}}^{(t)}~\mid~\bar{\mathbf{\mathit{x}}}^{(0:t-1)})$ | so as to reduce the variance of the preceding expression. Note that only the second of the two terms in this product depends on our time $t$ proposal distribution $\overset{\cdot}{\alpha^{(t)}}\big(\bar{\pmb x}^{(t)}~|~\bar{\pmb x}^{(0:t-1)}\big)$ | . By assumption, the samples $\overset{\cdot}{\textbf{\em x}}^{(0:t-1)}$ are ﬁxed, and hence so is the ﬁrst term. It is now not difcult to show that the time $t$ proposal distribution that minimizes the overall variance is 

$$
\alpha^{(t)}(X^{(t)}\mid\mathbf{x}^{(0:t-1)})=P(X^{(t)}\mid\mathbf{x}^{(0:t-1)},\mathbf{o}^{(0:t)}),
$$ 

making the second term uniformly 1. In words, we should sample the time $t$ state variable assignment $\bar{\mathbfit{x}}^{(t)}$ from its posterior distribution given the chosen sample from the previous state and the time $t$ observations. 

Using this proposal, the appropriate importance weight for our time $t$ trajectory is 

$$
\frac{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t)})}{\alpha^{(t-1)}(\pmb{x}^{(0:t-1)})}.
$$ 

What is the proposal distribution we use for the time $t-1$ trajectories? If we use this idea in combination with resampling, we can make the approximation that our uniformly sampled particles at time $t-1$ are an approximation to $P(\pmb{x}^{(\bar{0}:\bar{t}-1)}\mid\pmb{o}^{(0:t-1)})$ . In this case, we have 

$$
\begin{array}{r c l}{\displaystyle\frac{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t)})}{\alpha^{(t-1)}\left(\bar{\pmb{x}}^{(0:t-1)}\right)}}&{\approx}&{\displaystyle\frac{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t)})}{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t-1)})}}\\ &{\propto}&{\displaystyle\frac{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t-1)})P(\pmb{o}^{(t)}\mid\pmb{x}^{(0:t-1)},\pmb{o}^{(0:t-1)})}{P(\pmb{x}^{(0:t-1)}\mid\pmb{o}^{(0:t-1)})}}\\ &{=}&{P(\pmb{o}^{(t)}\mid\bar{\pmb{x}}^{(t-1)}),}\end{array}
$$ 

where the last step uses the Markov independence properties. Thus, our importance weights re proportional to the probabi ty of the time $t$ observation given the time $t-1$ particle $\mathbfit{x}^{(t-1)}$ , marginalizing out the time t state variables. We call this approach posterior particle ﬁltering , because the samples are generated using our posterior over the time $t$ state, given the time $t$ observation, rather than using our prior. 

However, sampling from the posterior over the state variables given the time $t$ observations may not be tractable. Indeed, the whole purpose of the (static) likelihood-weighting algorithm is to address this problem, deﬁning a proposal distribution that is (perhaps) closer to the posterior while still allowing forward sampling according to the network structure. However, likelihood weighting is only one of many importance distributions that can be used in this setting. In many cases, signiﬁcant advantages can be gained by constructing proposal distributions that are even closer to this posterior; we describe some ideas in section 15.3.3.5 below. 

# 15.3.3.4 Sample Selection Scheme 

We can also generalize the particle selection scheme. A general selection procedure associates with each particle $\bar{\pmb{x}}^{(0:t)}[m]$ a number of ofspring ${K}_{m}^{(t)}$ . Each of the ofspring of this particle is a (possibly weighted) copy of it, which is then propagated independently to the next step, as discussed in the previous section. Let $\mathcal{D}^{(t)}$ be the original sample set, and $\tilde{\mathcal{D}}^{(t)}$ D be the new sample set. 

Assuming that we want to keep the total number of particles constant, we must have $\textstyle\sum_{m=1}^{M}K_{m}^{(t)}\;=\;M$ . There are many approaches to selecting the number of ofspring $K_{m}^{(t)}$ for each particle $\bar{\pmb{x}}^{(0:t)}[m]$ . The bootstrap ﬁlter implicitly selects $K_{m}^{(t)}$ using a multinomial distribution, where one performs $M$ IID trials, in each of which we obtain the outcome $m$ with probability $w(\bar{\pmb x}^{(0:t)}\bar{[m]})$ (assuming the weights have been renormalized). This distribution guarantees that the expectation of $K_{m}^{(t)}$ is $M\cdot w(\bar{\pmb x}^{(0:t)}[m])$ . Because each of the particles in $\bar{\mathcal{D}}^{(t)}$ D is weighed equally, roperty guarantees that the tation (relative to our random resampling procedure) of $\hat{P}_{\tilde{\mathcal{D}}^{(t)}}^{\;\;\;\;\;1}$ is our original distribution $\hat{P_{\mathcal{D}^{(t)}}}$ . Thus, the resampling proce- D D dure does not introduce any bias into our algorithm, in the sense that the expectation of any estimator relative to $\hat{P}_{\mathcal{D}^{(t)}}$ is the same as its expectation relative to $\hat{P}_{\tilde{\mathcal{D}}^{(t)}}$ . 

While the multinomial scheme is quite natural, there are many other selection schemes that also satisfy this constraint. In general, we can use some other method to select the number of ofspring $\dot{K}_{m}^{(t)}$ for each sample $m$ , so long as this number satisﬁes (perhaps approximately) the constraint on the expectation. Assuming $K_{m}^{(t)}>0$ , we then assign the weight of each of these $K_{m}^{(t)}$ ofspring to be: 

$$
\frac{w(\bar{\pmb x}^{(0:t)}[m])}{K_{m}^{(t)}\operatorname*{Pr}(K_{m}^{(t)}>0)};
$$ 

intuitively, we divide the original weight of the $m$ th sample between its $K_{m}^{(t)}$ ofspring. The second term in the denominator accounts for the fact that the sample was not eliminated entirely. To justify this expression, we observe that the total weight of the sample $m$ ofspring conditioned on the fact that $\dot{K}_{m}^{(t)}>0$ is precisely $w(\bar{\pmb x}^{(0:t)}[m])$ . Thus, the unconditional expectation of the total of these weights is $w(\bar{\mathbf{x}}^{(0:t)}[m])P(K_{m}^{(t)}>0)$ , causing us to divide by this latter probability in the new weights. 

There are many possible choices for generating the vector of ofspring $(K_{1}^{(t)},\cdot\cdot\cdot,K_{M}^{(t)})$ , which tells us how many copies (if any) of each of the $M$ samples in $\mathcal{D}^{(t)}$ we wish to propagate forward. Although the diferent schemes all have the same expectation, they can difer signiﬁcantly in terms of their variance. The higher the variance, the greater the probability of obtaining unrepresentative distributions, leading to poor answers. The multinomial sampling scheme induced by the bootstrap ﬁlter tends to have a fairly high variance, and other schemes often perform better in practice. Moreover, it is not necessarily optimal to perform a resampling step at every time slice. For example, one can monitor the weights of the samples, and only resample when the variance exceeds a certain threshold, or, equivalently, when the number of efective samples equation (12.15) goes below a certain minimal amount. 

Finally, we note that one can also consider methods that vary the number of samples $M$ over time. In certain cases, such as tracking a system in real time, we may be forced to maintain rigid constraints on the amount of time spent in each time slice. In other cases, however, it may be possible to spend more computational resources in some time slices, at the expense of using less in others. For example, if we have the ability to cache our observations a few time slices back (which we may be doing in any case for the purposes of performing smoothing), we can allow more samples in one time slice (perhaps falling a bit behind), and catch up in a subsequent time slice. If so, we can determine whether additional samples are required for the current time slice by using our estimate of the number of efective samples in the current time slice. Empirically, this ﬂexibility in the number of samples used per time slice can also improve the quality of the results, since it helps reduce the variance of the estimator and thereby reduces the harm done by a poor set of samples obtained at one time slice. 

# 15.3.3.5 Other Extensions 

As for importance sampling in the static case, there are multiple ways in which we can improve particle ﬁltering by utilizing other inference methods. For example, a key problem in particle ﬁltering is the fact that the diversity of particles often decreases over time, so that we are only generating samples from a relatively small part of our space. In cases where there are multiple reasonably likely hypotheses, this loss of diversity can result in bad situations, where a surprising observation (surprising relative to our current sample population) can suddenly rule out all or most of our samples. 

There are several ways of addressing that problem. For example, we can use MCMC methods within a time slice to obtain a more diverse set of samples. While this cannot regenerate hypotheses that are very far away from our current set of samples, it can build up and maintain a broader set of hypotheses that is less likely to become depleted in subsequent steps. A related approach is to generate a clique tree for the time slice in isolation, and then use forward sampling to generate samples from the clique tree (see exercise 12.3). We note that exact inference for a single time slice may be feasible, even if it is infeasible for the DBN as a whole due to the entanglement phenomenon. 

Another use for alternative inference methods is to reduce the variance of the generated samples. Here also, multiple approaches are possible. For example, as we discussed in sec- tion 15.3.3.3 (in equation (15.3)), we want to generate our time $t$ state variable assignment from its posterior distribution given the chosen sample from the previous state and the time $t$ ob- servations. We can generate this posterior using exact inference on the 2-TBN structure. Again, this approach may be feasible even if exact inference on the DBN is not. If exact inference is infeasible even for the 2-TBN, we may still be able to use some intermediate alternative. We might be able to reverse some edges that point to observed variables (see exercise 3.12), making the time $t$ distribution closer to the optimal sampling distribution at time $t$ . Alternatively, we might use approximate inference method (for example, the EP-based approach of the previous section) to generate a proposal distribution that is closer to the true posterior than the one obtained by the simple likelihood-weighting sampling distribution. 

Finally, we can also use collapsed particles rather than fully sampled states in particle ﬁltering. This method is often known as Rao-Blackwellized particle ﬁltering , or RBPF . As we observed in section 12.4, the use of collapsed particles reduces the bias of the estimator. The procedure is based on the collapsed importance-sampling procedure for static networks, as described in section 12.4.1. As there, we partition the state variables $X$ into two disjoint sets: the sampled variables $X_{p}$ , and the variables $X_{d}$ whose distribution we maintain in closed form. Each particle now consists of three components: $(\pmb{x}_{p}^{(t)}[m],w^{(t)}[m],q^{(t)}[m](\pmb{X}_{d}^{(t)}))$ . The particle structure is generally chosen so as to allow $q^{(t)}[m](\mathbf{\boldsymbol{X}}_{d}^{(t)})$ to be represented efectively, for example, in a factorized form. 

At a high level, we use importance sampling from some appropriate proposal distribution $Q$ (as described earlier) to sample the variables $X_{p}^{(t)}$ and exact inference to compute the importance weights and the distribution $q^{(t)}[m](\mathbf{\boldsymbol{X}}_{d}^{(t)})$ . This process is described in detail in section 12.4.1. When applying this procedure in the context of particle ﬁltering, we generate the time $t$ particle from a distribution deﬁned by a time $t-1$ particle and the 2-TBN. 

More precisely, consider a time $t-1$ particle $\pmb{x}_{p}^{(t-1)}[m],w^{(t-1)}[m],q^{(t-1)}[m](\pmb{X}_{d}^{(t-1)})$ . We deﬁne a joint probability distribution $P_{m}^{(t)}(X^{(t-1)}\cup X^{(t)})$ by taking the time $t-1$ particle

 $\mathbf{\bar{x}}_{p}^{(t-1)}[m],q^{(t-1)}[m](\mathbf{\bar{X}}_{d}^{(t-1)})$ as a distribution over $X^{(t-1)}$ (one that gives probability 1 to

 ${\pmb x}_{p}^{(t-1)}[m])$ ), and then using the 2-TBN to deﬁne $P(\mathcal{X}^{(t)}\mid X^{(t-1)})$ . The distribution ${P}_{m}^{(t)}$ is represented in a factored form, which is derived from the factorization of $q^{(t-1)}[m](X_{d}^{(t-1)})$ and from the structure of the 2-TBN. We can now use $P_{m}^{(t)}$ , and the time $t$ observation $o^{(t)}$ , as input to the procedure of section 12.4.1. The output is a new particle and weight $w$ ; the particle is added to the time $t$ data set $\mathcal{D}^{(t)}$ , with a weight $w\cdot w^{(\bar{t}-1)}[m]$ . The resulting data set, consisting now of collapsed particles, is then utilized as in standard particle ﬁltering. In particular, an additional sample selection step may be used to choose which particles are to be propagated to the next time step. 

As deﬁned, however, the collapsed importance-sampling procedure over $P_{m}^{(t)}$ computes a particle over all of the variables in this model: both time $t$ an time $t-1$ variables. For our purpose, we need to extract a particle involving only time t variables. This process is fairly straightforward. The particle speciﬁes as assignment to $X_{p}^{(t)}$ ; the marginal distribution over $X_{d}^{(t)}$ can be extracted using standard probabilistic inference techniques. We must take care, however: in general, the distribution over $X_{d}$ can be subject to the same entanglement phenomena as the distribution as a whole. Thus, we must select the factorization (if any) of $\bar{q}^{(t)}[m](X_{d}^{(t)})$ so as to be sustainable over time; that is, $q^{(t-1)}[m](X_{d}^{(t-1)})$ factorizes in a certain way, then so does the marginal distribution $q^{(t)}[m](\mathbf{\boldsymbol{X}}_{d}^{(t)})$ induced by ${P}_{m}^{(t)}$ . Box 15.A provides an example of such a model for the application of collapsed particle ﬁltering to the task of robot localization and mapping. 

# 15.3.4 Deterministic Search Techniques 

Random sampling methods such as particle ﬁltering are not always the best approach for generating particles that search the space of possibilities. In particular, if the transition model is discrete and highly skewed — some successor states have much higher probability than others — then a random sampling of successor states is likely to generate many identical samples. This greatly reduces sample diversity, wastes computational resources, and leads to a poor representation of the space of possibilities. In this case, search -based methods may provide a better alternative. Here, we aim to ﬁnd a set of particles that span the high-probability assignments and that we hope will allow us to keep track of the most likely trajectories through the system. 

speech recognition 

Viterbi algorithm 

These techniques are commonly used in applications such as speech recognition (see box 6.B), where the transitions between phonemes, and even between words, are often highly constrained, with most transitions having probability (close to) 0. Here, we often formulate the problem as that of ﬁnding the single highest-probability trajectory through the system. In this case, an exact solution can be found by running a variable elimination algorithm such as that of section 13.2. In the context of HMMs, this algorithm is known as the Viterbi algorithm . 

In many cases, however, the HMM for speech recognition does not ﬁt into memory. Moreover, if our task is continuous speech recognition, there is no natural end to the sequence. In such settings, we often resort to approximate techniques that are more memory efcient. A commonly used technique is beam search, which has the advantage that it can be applied in an online fashion as the data sequence is acquired. See exercise 15.10. 

Finally, we note that deterministic search in temporal models is often used within the frame- work of collapsed particles, combining search over a subset of the variables with marginalization over others. This type of approach provides an approximate solution to the marginal MAP problem, which is often a more appropriate formulation of the problem. For example, in the speech-recognition problem, the MAP solution ﬁnds the most likely trajectory through the speech HMM. However, this complete trajectory tells us not only which words were spoken in a given utterance, but also which phones and subphones were traversed; we are rarely interested in the trajectory through these ﬁner-grained states. A more appropriate goal is to ﬁnd the most likely sequence of words when we marginalize over the possible sequences of phonemes and subphones. Methods such as beam search can also be adapted to the marginal MAP problem, allowing it to be applied in this setting; see exercise 15.10. 

# 15.4 Hybrid DBNs 

So far, we have focused on inference in the context of discrete models. However, many (perhaps even most) dynamical systems tend to include continuous, as well as discrete, variables. From a representational perspective, there is no difculty incorporating continuous variables into the network model, using the techniques described in section 5.5. However, as usual, inference in models incorporating continuous variables poses new challenges. In general, the techniques developed in chapter 14 for the case of static networks also extend to the case of DBNs, in the same way that we extended inference techniques for static discrete networks in earlier sections in this chapter. 

We now describe a few of the combinations, focusing on issues that are speciﬁc to the combination between DBNs and hybrid models. We emphasize that many of the other techniques described in this chapter and in chapter 14 can be successfully combined. For example, one of the most popular combinations is the application of particle ﬁltering to continuous or hybrid systems; however, the combination does not raise signiﬁcant new issues, and so we omit a detailed presentation. 

# 15.4.1 Continuous Models 

We begin by considering systems composed solely of continuous variables. 

# 15.4.1.1 The Kalman Filter 

The simplest such system is the linear dynamical system (see section 6.2.3.2), where the variables are related using linear Gaussian CPDs. These systems can be tracked very efciently using a set of update equations called the Kalman ﬁlter . 

Recall that the key difculty with tracking a dynamical system is the entanglement phe- nomenon, which generally forces us to maintain, as our belief state, a full joint distribution over the state variables at time $t$ . For discrete systems, this distribution has size exponential in the number of variables, which is generally intractably large. By contrast, as a linear Gaus- sian network deﬁnes a joint Gaussian distribution, and Gaussian distributions are closed under conditioning and marginalization, we know that the posterior distribution over any subset of variables given any set of observations is a Gaussian. In particular, the belief state over the state variables $\Breve{\mathbf{X}}^{(t)}$ is a multivariate Gaussian. A Gaussian can be represented as a mean vector and covariance matrix, which requires (at most) quadratic space in the number of state variables. Thus, in a Kalman ﬁlter, we can represent the belief state fairly compactly. 

As we now show, we can also maintain the belief state efciently, using simple matrix operations over the matrices corresponding to the belief state, the transition model, and the observation model. Speciﬁcally, consider a linear Gaussian DBN deﬁned over a set of state variables $X$ with $n\;=\;|X|$ and a set of observation variables $O$ with $m\;=\;|O|$ . Let the probabilistic model be deﬁned as in equation (6.3) and equation (6.4), which we review for convenience: 

$$
\begin{array}{r c l}{{P(X^{(t)}\mid X^{(t-1)})}}&{{=}}&{{\mathcal{N}\left(A X^{(t-1)};Q\right),}}\\ {{}}&{{}}&{{}}\\ {{P(O^{(t)}\mid X^{(t)})}}&{{=}}&{{\mathcal{N}\left(H X^{(t)};R\right).}}\end{array}
$$ 

Kalman ﬁlter 

state transition update 

We now show the Kalman ﬁlter update equations, which provide an efcient implementation for equation (15.1) and equation (15.2). Assume that the Gaussian distribution encoding $\sigma^{(t)}$ is maintained using a mean vector $\mu^{(t)}$ and a covariance distribution $\Sigma^{(t)}$ . The state transition update equation is easy to implement: 

$$
\begin{array}{l c l}{{\mu^{(\cdot t+1)}}}&{{=}}&{{A\mu^{(t)}}}\\ {{\Sigma^{(\cdot t+1)}}}&{{=}}&{{A\Sigma^{(t)}A^{T}+Q,}}\end{array}
$$ 

observation update 

where $\mu^{(\cdot t+1)}$ and $\Sigma^{(\cdot t+1)}$ are the mean and covariance matrix for the prior belief state $\sigma^{(\cdot t+1)}$ . Intuitively, the new mean vector is simply the application of the linear transformation $A$ to the mean vector in the previous time step. The new covariance matrix is the transformation of the previous covariance matrix via $A$ , plus the covariance introduced by the noise. 

The observation update is somewhat more elaborate: 

$$
\begin{array}{r c l}{{K^{(t+1)}}}&{{=}}&{{\Sigma^{(\cdot t+1)}H^{T}(H\Sigma^{(\cdot t+1)}H^{T}+R)^{-1}}}\\ {{\mu^{(t+1)}}}&{{=}}&{{\mu^{(\cdot t+1)}+K^{(t+1)}(o^{(t+1)}-H\mu^{(\cdot t+1)})}}\\ {{\Sigma^{(t+1)}}}&{{=}}&{{(I-K^{(t+1)}H)\Sigma^{(\cdot t+1)}.}}\end{array}
$$ 

This update rule can be obtained using tedious but straightforward algebraic manipulations, by forming the joint Gaussian distribution over $X^{(t+1)},O^{(t+1)}$ deﬁned by the prior belief state $\sigma^{(\cdot t+1)}$ and the observation mod $P(O^{(t+1)}\mid X^{(t+1)})$ , and then conditioning the resulting joint Gaussian on the observation o $o^{(t+1)}$ . 

To understand the intuition behind this rule, note ﬁrst that the mean of $\sigma^{(t+1)}$ is simply the mean of $\sigma^{(\cdot t+1)}$ , plus a correction term arising from the evidence. The correction term involves the observation residual — the diference between our expected observation $H\mu^{(\cdot t+1)}$ and the actual observation $o^{(t+1)}$ . This residual is multiplied by a matrix called the Kalman gain $K^{(t+1)}$ , which dictates the importance that we ascribe to the observation. We can see, for example, that when the measurement error covariance $R$ approaches 0 , the Kalman gain approaches $H^{-1}$ ; in this case, we are exactly “reverse engineering” the residual in the observation and using it to correct the belief state mean. Thus, the actual observation is trusted more and more, and the predicted observation $H\mu^{(\cdot t+1)}$ is trusted less. We also then have that the covariance of the new belief state approaches 0, corresponding to the fact that the observation tells us the current state with close to certainty. Conversely, when the covariance in our belief state $\Sigma^{(\cdot t+1)}$ tends to 0, the Kalman gain approaches 0 as well. In this case, we trust our predicted distribution, and pay less and less attention to the observation: both the mean and the covariance of the posterior belief state $\sigma^{(t+1)}$ are the same as those of the prior belief state $\sigma^{(\cdot t+1)}$ . Finally, it can be shown that the posterior covariance matrix of our estimate approaches some limiting value as $T\longrightarrow\infty$ →∞ , hich reﬂects our “steady state” uncertainty about the system state. We note that both the time t covariance and its limiting value do not depend on the data. On one hand, this fact ofers computational savings, since it allows the covariance matrix to be computed ofine. However, it also points to a fundamental weakness of linear-Gaussian models, since we would naturally expect our uncertainty to depend on what we have seen. 

information form 

The Kalman ﬁltering process maintains the belief state as a mean and covariance matrix. An alternative is to maintain the belief state using information matrices (that is, a canonical form representation, as in equation (14.1)). The resulting set of update equations, called the information form of the Kalman ﬁlter, can be derived in a straightforward way from the basic operations on canonical forms described in section 14.2.1.2; the details are left as an exercise (exercise 15.11). We note that, in the Kalman ﬁlter, which maintains covariance matrices, the state transition update (equation (15.4)) is straightforward, and the observation update (equation (15.5)) is complex, requiring the inversion of an $n\times n$ matrix. In the information ﬁlter, which maintains information matrices, the situation is precisely the reverse. 

# 15.4.1.2 Nonlinear Systems 

The Kalman ﬁlter can also be extended to deal with nonlinear continuous dynamics, using the techniques described in section 14.4. In these methods, we maintain all of the intermediate fac- tors arising in the course of inference as multivariate Gaussian distributions. When encountering a nonlinear CPD, which would give rise to a non-Gaussian factor, we simply linearize the result to produce a new Gaussian. We described two main methods for performing the linearization, either by taking the Taylor series expansion of the nonlinear function, or by using one of several numerical integration techniques. The same methods apply without change to the setting of tracking nonlinear continuous DBNs. In this case, the application is particularly straightforward, as the factors that we wish to manipulate in the course of tracking are all distributions; in a general clique tree, some factors do not represent distributions, preventing us from applying these linearization techniques and constraining the order in which messages are passed. 

Concretely, assume that our nonlinear system has the model: 

$$
\begin{array}{r c l}{{P(X^{(t)}\mid X^{(t-1)})}}&{{=}}&{{f(X^{(t-1)},U^{(t-1)})}}\\ {{P(O^{(t)}\mid X^{(t)})}}&{{=}}&{{g(X^{(t)},W^{(t)}),}}\end{array}
$$ 

where $f$ and $g$ are deterministic nonlinear (continuous) functions, and $U^{(t)},W^{(t)}$ are Gaussian random variables, which explicitly encode the noise in the transition and observation models, respectively. (In other words, rather than modeling the system in terms of stochastic CPDs, we use an equivalent representation that partitions the model into a deterministic function and a noise component.) 

extended Kalman ﬁlter unscented Kalman ﬁlter 

To address the ﬁltering task here, we can apply either of the linearization methods described earlier. The Taylor-series linearization of section 14.4.1.1 gives rise to a method called the extended Kalman ﬁlter . The unscented transformation of section 14.4.1.2 gives rise to a method called the unscented Kalman ﬁlter . In this latter approach, we maintain our belief state using the same representation as in the Kalman ﬁlter: $\bar{\sigma^{\left(t\right)}}=\mathcal{N}\left(\boldsymbol{\mu}^{\left(t\right)};\boldsymbol{\Sigma}^{\left(t\right)}\right)$  . To perform the transition update, we construct a joint Gaussian distribution $p(X^{(t)},U^{(t)})$ by multiplying the Gaussians for $\sigma^{(t)}$ and $U^{(t)}$ . The result is a Gaussian distribution and a nonlinear function $f$ , to which we can now apply the unscented transformation of section 14.4.1.2. The result is a mean vector $\mu^{(\cdot t+1)}$ and covariance matrix $\Sigma^{(\cdot t+1)}$ for the prior belief state $\sigma^{(\cdot t+1)}$ . 

To obtain the posterior belief state, we must perform the observation update. We construct a joint Gaussian distribution $p(\mathbf{\cal{X}}^{(t+1)},\mathbf{\cal{W}}^{(t+1)})$ by multiplying the Gaussians for $\sigma^{(\cdot t+1)}$ and $W^{(t+1)}$ . We then estimate a joint Gaussian distribution over $X^{(t+1)},O^{(t+1)}$ , using the un- scented transformation of equation (14.18) to estimate the integrals required for computing the mean and covariance matrix of this joint distribution. We now have a Gaussian joint distri- bution over $X^{(t+1)},O^{(t+1)}$ , which we can condition on our observation $o^{(t+1)}$ in the usual way. The resulting posterior over $X^{(t+1)}$ is the new belief state $\sigma^{(t+1)}$ . Note that this approach computes a full joint covariance matrix over $X^{(t+1)},O^{(t+1)}$ . When the dependency model of the observation on the state is factored, where we have individual observation variables each of which depends only on a few state variables, we can perform this computation in a more structured way (see exercise 15.12). 

![](images/e003947ea8e33974efa3dc4c4c477aac9684c1154482f33707cd46e34b283bc0.jpg) 
Figure 15.A.1 — Illustration of Kalman ﬁltering for tracking (a) Raw data (dots) generated by an ob- ject moving to the right (line). (b) Estimated location of object: crosses are the posterior mean, circles are 95 percent conﬁdence ellipsoids. 

target tracking robot localization Box 15.A — Case Study: Tracking, Localization, and Mapping. A key application of proba- bilistic models is to the task of tracking moving objects from noisy measurements. One example is target tracking , where we measure the location of an object, such as an airplane, using an external sensor. Another example is robot localization , where the moving object itself collects measurements, such as sonar or laser data, that can help it localize itself on a map. 

Kalman ﬁltering was applied to this problem as early as the 1960s. Here, we give a simpliﬁed example to illustrate the key ideas. Consider an object moving in a two-dimensional plane. Let $X_{1}^{(t)}$ and $X_{2}^{(t)}$ be the horizontal and vertical locations of the object, and ${\dot{X}}_{1}^{(t)}$ and ${\dot{X}}_{2}^{(t)}$ be the corresponding velocity. We can represent this as a state vector $X^{(t)}\in I\!\!R^{4}$ . Let us assume that the object is moving at constant velocity, but is “perturbed” by random Gaussian noise (for example, due to the wind). Thus we can deﬁne $X_{i}^{\prime}\sim\mathcal{N}\left(X_{i}+\dot{X}_{i};\sigma^{2}\right)$   for $i\,=\,1,2$ . Assume we can obtain a (noisy) measurement of the location of the object but not its velocity. Let $Y^{(t)}\,\in\,I\!\!R^{2}$ represent our observation, where $\begin{array}{r}{Y^{(t)}\sim\mathcal{N}\left((X_{1}^{(t)},X_{2}^{(t)});\Sigma_{o}\right)}\end{array}$   , where $\Sigma_{o}$ is the covariance matrix that governs our observation noise model. Here, we do not necessarily assume that noise is added separately to each dimension of the object location. Finally, we need to specify our initial (prior) beliefs about the state of the object, $p(\dot{\boldsymbol X}^{(0)})$ . We assume that this distribution is also a Gaussian $p(X^{(0)})=\mathcal{N}\left(\mu^{(0)};\Sigma^{(0)}\right)$    . We can represent prior ignorance by making $\Sigma^{(0)}$ suitably “broad.” These parameters fully specify the model, allowing us to apply the Kalman ﬁlter, as described in section 15.4.1.1. 

Figure 15.A.1 gives an example. The object moves to the right and generates an observation at each time step (such as a “blip” on a radar screen). We observe these blips and ﬁlter out the noise by using the Kalman ﬁlter; the resulting posterior distribution is plotted on the right. Our best guess about the location of the object is the posterior mean, I $\pmb{{\cal E}}_{\sigma^{(t)}}\left[X_{1:2}^{(t)}\mid\pmb{{\mathscr{y}}}^{(1:t)}\right]$ h i , denoted as a cross. Our uncertainty associated with this estimate is represented as an ellipse that contains 95 percent of the probability mass. We see that our uncertainty goes down over time, as the efects of the initial uncertainty get “washed out.” As we discussed, the covariance converges to some steady-state value, which will remain indeﬁnitely. 

We have demonstrated this approach in the setting where the measurement is of the object’s location, from an external measurement device. It is also applicable when the measurements are collected by the moving object, and estimate, for example, the distance of the robot to various landmarks on a map. If the error in the measured distance to the landmark has Gaussian noise, the Kalman ﬁlter approach still applies. 

In practice, it is rarely possible to apply the Kalman ﬁlter in its simplest form. First, the dynamics and/or observation models are often nonlinear. A common example is when we get to observe the range and bearing to an object, but not its $X_{1}^{(t)}$ and $X_{2}^{(t)}$ coordinates. In this case, the observation model contains some trigonometric functions, which are nonlinear. If the Gaussian noise assumption is still reasonable, we apply either the extended Kalman ﬁlter or the unscented Kalman ﬁlter to linearize the model. Another problem arises when the noise is non-Gaussian, for example, when we have clutter or outliers. In this case, we might use the multivariate T distribution; this solution gains robustness at the cost of computational tractability. An alternative is to assume that each observation comes from a mixture model; one component corresponds to the observation being generated by the object, and another corresponds to the observation being generated by the background. However, this model is now an instance of a conditional linear Gaussian, raising all of the computational issues associated with multiple modes (see section 15.4.2). The same difculty arises if we are tracking multiple objects, where we are uncertain which observation was generated by which object; this problem is an instance of the data association problem (see box 12.D). 

In nonlinear settings, and particularly in those involving multiple modes, another very popular alternative is to use particle ﬁltering. This approach is particularly appealing in an online setting such as robot motion, where computations need to happen in real time, and using limited compu- tational resources. As a simple example, assume that we have a known map, $\mathcal{M}$ . The map can be encoded as a occupancy grid — a discretized grid of the environment, where each square is 1 if the environment contains an obstacle at that location, and 0 otherwise. Or we can encode it using a more geometric representation, such as a set of line segments representing walls. We can represent the robot’s location in the environment either in continuous coordinates, or in terms of the discretized grid (if we use that representation). In addition, the robot needs to keep track of its pose, or orientation, which we may also choose to discretize into an appropriate number of bins. The measurement $\mathbf{\boldsymbol{y}}^{(t)}$ is a vector of measured distances to the nearest obstacles, as described in box 5.E. Our goal is to maintain $P(X^{(t)}\mid\mathbf{\boldsymbol{y}}^{(1:t)},\mathcal{M})$ , which is our posterior over the robot location. 

Note that our motion model is nonlinear. Moreover, although the error model on the measured distance is a Gaussian around the true distance, the true distance to the nearest obstacle (in any given direction) is not even a continuous function of the robot’s position. In fact, belief states can easily become multimodal owing to perceptual aliasing , that is, when the robot’s percepts can match two or more locations. Thus, a Gaussian model is a very poor approximation here. 

Monte Carlo localization 

Thrun et al. (2000) propose the use of particle ﬁltering for localization, giving rise to an algorithm called Monte Carlo localization . Figure 15.A.2 demonstrates one sample trajectory of the particles over time. We see that, as the robot acquires more measurements, its belief state becomes more 

![](images/317f2b9f7458fe1d24cb99b483bdbbda6cc10a976e579aeb325daf8fa6d6d00c.jpg) 
Figure 15.A.2 — Sample trajectory of particle ﬁltering for robot localization 

sharply peaked. More importantly, we see that the use of a particle-based belief state makes it easy to model multimodal posteriors. 

One important issue when implementing a particle ﬁlter is the choice of proposal distribution. The simplest method is to use the standard bootstrap ﬁlter, where we propose directly from the dynamics model, and weight the proposals by how closely they match the evidence. However, when the robot is lost, so that our current belief state is very difuse, this approach will not work very well, since the proposals will literally be all over the map but will then get “killed of” (given essentially zero weight) if they do not match the (high-quality) measurements. As we discussed, the ideal solution is to use posterior particle ﬁltering, where we sample a particle $\pmb{x}^{(t+1)}[m]$ from the posterior distribution $P(\mathcal{X}^{(t+1)}\mid\pmb{x}^{(t)}[m],\pmb{y}^{(t+1)})$ . However, this solution requires that we be able to invert the evidence model using Bayes rule, a process that is not always feasible for complex, nonlinear models. One ad hoc ﬁx is to inﬂate the noise level in the measurement model artiﬁcially, giving particles an artiﬁcially high chance of surviving until the belief state has the chance to adapt to the evidence. A better approach is to use a proposal that takes the evidence into account; for example, we can compare $\mathbfit{y}^{(t)}$ with the map and then use a proposal that is a mixture of the bootstrap proposal $P(\mathcal{X}^{(t+1)}\mid\pmb{x}^{(t)}[m])$ and some set of candidate locations that are most consistent with the recent observations. 

robot mapping SLAM 

We now turn to the harder problem of localizing a robot in an unknown environment, while mapping the environment at the same time. This problem is known as simultaneous localization and mapping (SLAM) . In our previous terminology, this task corresponds to computing $p(\boldsymbol{X}^{(t)},\mathcal{M}\mid$ $\pmb{y}^{(1:t)}$ ) . Here, again, our task is much easier in the linear setting, where we represent the map in terms of $K$ landmarks whose locations, denoted $L_{1},\dots,L_{k}$ , are now unknown. Assume that we have a Gaussian prior over the location of each landmark, and that our observations $Y_{k}^{(t)}$ measure the Euclidean distance between the robot position $X^{(t)}$ and the k th landmark location $L_{k}$ , with some Gaussian noise. It is not difcult to see that $P(Y_{k}^{(t)}\mid X^{(t)},L_{k})$ | is a Gaussian distribution, so that our entire model is now a linear dynamical system. Therefore, we can naturally apply a Kalman ﬁlter to this task, where now our belief state represents $P(X^{(\hat{t})},L_{1},.\;.\;.\;,L_{k}\mid\pmb{y}^{(1:\hat{t})})$ . Figure $\mathit{15.A.3a}$ demonstrates this process for a simple two-dimensional map. We can see that the uncertainty of the landmark locations is larger for landmarks encountered later in the process, owing to the accumulation of uncertainty about the robot location. However, when the robot closes the loop and reencounters the ﬁrst landmark, the uncertainty about its position reduces dramatically; the 

![](images/0d8b1da01da5e05ee6c5bcda5d07cac1832ad8c0869ae2d06dfd83e073d8abaf.jpg) 
Figure 15.A.3 — Kalman ﬁlters for the SLAM problem (a) Visualization of marginals in Gaussian SLAM. (b) Visualization of information (inverse covariance) matrix in Gaussian SLAM, and its Markov network structure; very few entries have high values. 

landmark uncertainty reduces at the same point. If we were to use a smoothing algorithm, we would also be able to reduce much of the uncertainty about the robot’s intermediate locations, and hence also about the intermediate landmarks. 

Gaussian inference is attractive in this setting because even representing a posterior over the landmark positions would require exponential space in the discrete case. Here, because of our ability to use Gaussians, the belief state grows quadratically only in the number of landmarks. However, for large maps, even quadratic growth can be too inefcient, particularly in an online setting. To address this computational burden, two classes of methods based on approximate inference have been proposed. 

The ﬁrst is based on factored belief-state idea of section 15.3.2. Here, we utilize the observation that, although the variables representing the diferent landmark locations become correlated due to entanglement, the correlations between them are often rather weak. In particular, two landmarks that were just observed at the same time become correlated due to uncertainty about the robot position. But, as the robot moves, the direct correlation often decays, and two landmarks often become almost conditionally independent given some subset of other landmarks; this conditional independence manifests as sparsity in the information (inverse covariance) matrix, as illustrated in ﬁgure 15.A.3b. Thus, these approaches approximate the belief state by using a sparser representation that maintains only the strong correlations. We note that the set of strongly correlated landmark pairs changes over time, and hence the structure of our approximation must be adaptive. We can consider a range of sparser representations for a Gaussian distribution. One approach is to use a clique tree, which admits exact M-projection operations but grows quadratically in the maximum size of cliques in our clique tree. Another is to use the Markov network representation of the Gaussian (or, equivalently, its inverse covariance). The two main challenges are to determine dynamically the approximation to use at each step, and to perform the (approximate) M-projection in an efcient way, essential in this real-time setting. 

A second approach, and one that is more generally applicable, is to use collapsed particles. This approach is based on the important observation that the robot makes independent observations of the diferent landmarks. Thus, as we can see in ﬁgure 15.A.4a, the landmarks are conditionally independent given the robot’s trajectory. Importantly, the landmarks are not independent given the robot’s current location, owing to entanglement, but if we sample an entire robot trajectory $\mathbf{\mathcal{x}}^{(1:t)}$ , we can now maintain the conditional distribution over the landmark positions in a fully factored form. In this approach, known as FastSLAM , each particle is associated with a (factorized) distribution over maps. In this approach, rather than maintaining a Gaussian of dimension $2K+2$ (two coordinates for each landmark and two for the robot), we maintain a set of particles, each associated with $K$ two-dimensional Gaussians. Because the Gaussian representation is quadratic in the dimension and the matrix inversion operations are cubic in the dimension, this approach provides considerable savings. (See exercise 15.13.) Experimental results show that, in the settings where Kalman ﬁltering is suitable, this approximation achieves almost the same performance with a small number of particles (as few as ten); see ﬁgure 15.A.4b. 

This approach is far more general than the sparse Kalman-ﬁltering approach we have described, since it also allows us to deal with other map representations, such as the occupancy-grid map de- scribed earlier. Moreover, it also provides an integrated solution in cases where we have uncertainty about data association; here, we can sample over data-association hypotheses and still maintain a factored distribution over the map representation. Overall, by combining the diferent techniques we have described, we can efectively address most instances of the SLAM problem, so that this problem is now essentially considered solved. 

# 15.4.2 Hybrid Models 

We now move to considering a system that includes both discrete and continuous variables. As in the static case, inference in such a model is very challenging, even in cases where the continuous dynamics are simple. 

CLG dynamics 

switching linear dynamical system 

Consider a conditional linear Gaussian DBN, where all of the continuous variables have CLG dynamics , and the discrete variables have no continuous parents. This type of model is known as a switching linear dynamical system , as it can be viewed as a dynamical system where changes in the discrete state cause the continuous (linear) dynamics to switch. The unrolled DBN is a standard CLG network, and, as in any CLG network, given a full assignment to the discrete variables, the distribution over the continuous variables (or any subset of them) is a multivariate Gaussian. However, if we are not given an assignment to the discrete variables, the marginal distribution over the continuous variables is a mixture of Gaussians, where the number of mixture components is exponential in the number of discrete variables. (This phenomenon is precisely what underlies the $\mathcal{N P}$ -hardness proof for inference in CLG networks.) 

In the case of a temporal model, this problem is even more severe, since the number of mixture components grows exponentially, and unboundedly, over time. 

![](images/d290fbd2ad5d54d8fbeee57befc1bd5152c237bb2b50531511d617342efe066c.jpg) 
Figure 15.A.4 — Collapsed particle ﬁltering for SLAM (a) DBN illustrating the conditional indepen- dencies in the SLAM probabilistic model. (b) Sample predictions using a SLAM algorithm (solid path, star landmarks) in comparison to ground-truth measurements obtained from a GPS sensor, not given as input to the algorithm (dotted lines, circle landmarks): left — EKF; right — FastSLAM. The results show that both approaches achieve excellent results, and that there is very little diference in the quality of the estimates between these two approaches. 

Consider a one-dimensional switching linear-dynamical system whose state consists of a single discrete variable $D$ and a single continuous variable $X$ . In the SLDS, both $X$ and $D$ are persistent, and in addition we have $D^{\prime}\rightarrow X^{\prime}$ . The transition model is deﬁned by a CLG model for $X^{\prime}$ d a standard discrete CPD for D . In this example, the belief state at time t , marginalized over X $X^{(t)}$ , is a mixture distribution with a number of mixture components that is exponential in $t$ . 

In order to make the propagation algorithm tractable, we must make sure that the mixture of Gaussians represented by the belief state does not get too large. The two main techniques to do so are pruning and collapsing . 

mixture pruning 

beam search 

mixture collapsing 

Pruning algorithms reduce the size of the mixture distribution in the belief state by discarding some of its Gaussians. The standard pruning algorithm simply keeps the $N$ Gaussians with the highest probabilities, discards the rest, and then renormalizes the probabilities of the remaining Gaussian to sum to 1. This is a form of beam search for marginal MAP, as described in exercise 15.10. 

Collapsing algorithms partition the mixture of Gaussians into subsets, and then they collapse all the Gaussians in each subset into one Gaussian. Thus, if the belief state were partitioned into $N$ subsets, the result would be a belief state with exactly $N$ Gaussians. The diferent collapsing algorithms can difer in the choice of subsets of Gaussians to be collapsed, and on exactly when the collapsing is performed. The collapsed Gaussian $\hat{p}$ for a mixture $p$ is generally chosen to be its M-projection — the one that minimizes $D(p\|\hat{p})$ | | . Recall from proposition 14.3 that the M-projection can be computed simply and efciently by matching moments. 

We now describe some commonly used collapsing algorithms in this setting and show how they can be viewed within the framework of expectation propagation, as described in sec- tion 14.3.3. More precisely, consider a CLG DBN containing the discrete state variables $A$ and the continuous state variables $X$ . Let ${\cal M}\;=\;|V a l(A)|\;-\;$ the number of discrete states at any given time slice. Assume, for simplicity, that the state at time 0 is fully observed. We note that, throughout this section, we focus solely on techniques for CLG networks. It is not difcult to combine these techniques with linearization methods (as described earlier), allowing us to accommodate both nonlinear dynamics and discrete children of continuous parents (see section 14.4.2.5). 

general pseudo-Bayes 

GPB1 

As we discussed, after $t$ time slices, the total number of Gaussian mixture components in the belief state is $M^{t}$ — one for every assignment to $A^{(1)},\cdot\cdot\cdot,A^{(t)}$ . One common approximation is the class of general pseudo-Bayesian algorithms, which lump together components whose assignment in the recent past is identical. That is, for a positive integer $k$ , $\bar{\sigma}^{(t)}$ has $M^{k-1}$ mixture components — one for every assignment to $A^{((t-\hat{(k-2)}):t)}$ (which for $k=1$ we take to be a single “vacuous” assignment). If $\sigma^{(t)}$ has $M^{k-1}$ mixture components, then one step of forward propagation results in a distribution with $M^{k}$ components. Each of these Gaussian components is conditioned on the evidence, and its weight in the mixture is multiplied with the likelihood it gives the evidence. The resulting mixture is now partitioned into ${\bar{M}}^{k-1}$ subsets, and each subset is collapsed separately, producing a new belief state. 

For $k\,=\,1$ , the algorithm, known as GPB1 , maintains exactly one Gaussian in the belief state; it also maintains a distribution over $A^{(t)}$ . Thus, $\sigma^{(t)}$ is essentially a product of a discrete distribution $\sigma^{(t)}(A^{(t)})$ and a Gaussian $\sigma^{(t)}(\boldsymbol{X}^{(t)})$ . In the forward-propagation step (corresponding to equation (15.1)), we obtain a mixture of $M$ Gaussians: one Gaussian $p_{a^{(t+1)}}^{(t+1)}$ for each assignment $\pmb{a}^{(t+1)}$ , whose weight is 

$$
\sum_{\pmb{A}^{(t)}}P(\pmb{a}^{(t+1)}\mid\pmb{A}^{(t)})\sigma^{(t)}(\pmb{a}^{(t)}).
$$ 

In the conditioning step, each of these Gaussian components is conditioned on the evidence $o^{(t+1)}$ , and its weight is multiplied by the likelihood of the evidence relative to this Gaussian. The resulting weighted mixture is then collapsed into a single Gaussian, using the M-projection operation described in proposition 14.3. 

expectation propagation 

GPB2 

We can view this algorithm as an instance of the expectation propagation (EP) algorithm applied to the clique tree for the DBN described in section 15.2.3. The messages, which correspond to the belief states, are approximated using a factorization that decouples the discrete variables $A^{(t)}$ and the continuous variables $X^{(t)}$ ; the distribution over the continuous variables is maintained as a Gaussian. This approximation is illustrated in ﬁgure 15.7a. It is not hard to verify that the GPB1 propagation update is precisely equivalent to the operation of incorporating the time $t$ message into the $(t,t+1)$ clique, performing the in-clique computation, and then doing the M-projection to produce the time $t+1$ message. 

The GPB2 algorithm maintains $M$ Gaussians $\{p_{a^{(t)}}^{(t)}\}$ in the time $t$ belief state, each one corresponding to an assignment $\mathbf{\boldsymbol{a}}^{(t)}$ . After propagation, we get $M^{2}$ Gaussians, each one corresponding to an assignment to both $\mathbf{\alpha}a^{(t)}$ and $\bar{a^{(t+1)}}$ . We now partition this mixture into $M$ subsets, one for each assignment to $\pmb{a}^{(t+1)}$ . Each subset is collapsed, resulting in a new belief state with $M$ Gaussians. Once again, we can view this algorithm as an instance of EP, using the message structure $\mathbf{\boldsymbol{A}}^{(t)}\to\mathbf{\boldsymbol{X}}^{(t)}$ , where the distribution of $X^{(t)}$ given $A^{(t)}$ in the message has the form of a conditional Gaussian. This EP formulation is illustrated in ﬁgure 15.7b. 

A limitation of the GPB2 algorithm is that, at every propagation step we must generate $M^{2}$ Gaussians, a computation that may be too expensive. An alternative approach is called the inter- acting multiple model (IMM) algorithm. Like GPB2, it maintains a belief state with $M$ Gaussians $p_{a^{(t)}}^{(t)}$ ; but like GPB1, it collapses all of the Gaussians in the mixture into a single Gaussian, prior to incorporating them into the transition model $\mathcal{P}(\boldsymbol{X}^{(t+1)}\mid\boldsymbol{X}^{(t)})$ . Importantly, however, it performs the collapsing step after incorporating the discrete transition l $P(A^{(t+1)}\mid A^{(t)})$ . This produces a diferent mixture distribution for each assignment a $\pmb{a}^{(t+1)}$ — these distribu- tions are all mixtures of the same set of time $t$ Gaussians, but with diferent mixture weights, generally producing a better approximation. Each of these components, representing a condi- tional distribution over $X^{(t)}$ given $A^{(t+1)}$ , is then propagated using the continuous dynamics $P(X^{(t+1)}\mid X^{(t)},A^{(t+1)})$ , and conditioned on the evidence, producing the time $t+1$ be- lief state. The IMM algorithm can also be formulated in the EP framework, as illustrated in ﬁgure $15.7\mathrm{c}$ . 

Although we collapse our belief state in $M$ diferent ways when using the IMM algorithm, we only create $M$ Gaussians at time $t+1$ (as opposed to $M^{2}$ Gaussians in GPB2). The extra work compared to GPB1 is the computation of the new mixture probabilities and collapsing $M$ mixtures instead of just one, but usually this extra computational cost is small relative to the cost of computing the $M$ Gaussians at time $t+1$ . Therefore, the computational cost of the IMM algorithm is only slightly higher than the GPB1 algorithm, since both algorithms generate only $M$ Gaussians at every propagation step, and it is signiﬁcantly lower than GPB2. In practice, it seems that IMM often performs signiﬁcantly better than GPB1 and almost as well as GPB2. Thus, 

![](images/1bf6ee16a7d0af24cec4bde048de129ce45a7310b7d8656c77da29d3c4a70242.jpg) 
Figure 15.7 Three approaches to collapsing the Gaussian mixture obtained when tracking in a hybrid CLG DBN, viewed as instances of the EP algorithm. The ﬁgure illustrates the case where the network contains a single discrete variable $A$ and a single continuous variable $X$ : (a) the GPB1 algorithm; (b) the GPB2 algorithm; (c) the IMM algorithm. 

the IMM algorithm appears to be a good compromise between complexity and performance. 

We note that all of these clustering methods deﬁne the set of mixture components to corre- spond to assignments of discrete variables in the network. For example, in both GPB1 and IMM, each component in the mixture for $\sigma^{(t)}$ corresponds to an assignment to $A^{(t)}$ . In general, this approach may not be optimal, as Gaussians that correspond to diferent discrete assignments of $\hat{\pmb{A}}^{(t)}$ may be more similar to each other than Gaussians that correspond to the same assignment. In this case, the collapsed Gaussian would have a variance that is unnecessarily large, leading to a poorer approximation to the belief state. The solution to this problem is to select dynamically a partition of Gaussians in the current belief state, where the Gaussian components in the same partition are collapsed. 

Our discussion has focused on cases where the number of discrete states at each time step is tractably small. How do we extend these ideas to more complex systems, where the number of discrete states at each time step is too large to represent explicitly? The EP formulation of these collapsing strategies provides an easy route to generalization. In section 14.3.3, we discussed the application of the EP algorithm to CLG networks, using either a clique tree or a cluster-graph message passing scheme. When faced with a more complex DBN, we can construct a cluster graph or a clique tree for the 2-TBN, where the clusters contain both discrete and continuous variables. These clusters pass messages to each other, using M-projection to the appropriate parametric family chosen for the messages. When a cluster contains one or more discrete variables, the M-projection operation may involve one of the collapsing procedures described. We omit details. 

# 15.5 Summary 

In this chapter, we discussed the problem of performing inference in a dynamic Bayesian network. We showed how the most natural inference tasks in this setting map directly onto probabilistic queries in the ground DBN. Thus, at a high level, the inference task here is not diferent from that of any other Bayesian network: we can simply unroll the network, instantiate our observations, and run inference to compute the answers to our queries. A key challenge lies in the fact that the networks that are produced in this approach can be very (or even unboundedly) large, preventing us from applying many of our exact and approximate inference schemes. 

We showed that the tracking problem can naturally be formulated as a single upward pass of clique tree propagation, sending messages from time 0 toward later time slices. The messages naturally represent a belief state : our current beliefs about the state of the system. Importantly, this forward-propagation pass can be done in a way that does not require that the clique tree for the entire unrolled network be maintained. 

Unfortunately, the entanglement property that arises in all but the most degenerate DBNs typically implies that the belief state has no conditional independence structure, and therefore it does not admit any factorization. This fact generally prevents the use of exact inference, except in DBNs over a fairly small state space. 

As for exact inference, approximate inference techniques can be mapped directly to the unrolled DBN. Here also, however, we wish to avoid maintaining the entire DBN in memory during the course of inference. Some algorithms lend themselves more naturally than others to this “online” message passing. Two methods that have been speciﬁcally adapted to this setting are the factored message passing that lies at the heart of the expectation propagation algorithm, and the likelihood-weighting (importance-sampling) algorithm. 

The application of the factored message passing is straightforward: We represent the message as a product of factors, possibly with counting number corrections to avoid double-counting; we employ a nested clique tree or cluster graph within each time slice to perform approximate message propagation, mapping a time $t$ approximate belief state to a time $t+1$ approximate belief state. In the case of ﬁltering, this application is even simpler than the original EP, since no backward message passing is used. 

The adaptation of the likelihood-weighting algorithm is somewhat more radical. Here, if we simply propagate particles forward, using the evidence to adjust their weights, the weight of the particles will generally rapidly go to zero, leaving us with a set of particles that has little or no relevance to the true state. From a technical perspective, the variance of this estimator rapidly grows as a function of $t$ . We therefore introduce a resampling step, which allows “good” samples to duplicate while removing poor samples. We discussed several approaches for selecting new samples, including the simple (but very common) bootstrap ﬁlter, as well as more complex schemes that use MCMC or other approaches to generate a better proposal distribution for new samples. As in static networks, hybrid schemes that combine sampling with some form of (exact or approximate) global inference can be very useful in DBNs. Indeed, we saw examples of practical applications where this type of approach has been used successfully. 

Finally, we discussed the task of inference in continuous or hybrid models. The issues here are generally very similar to the ones we already tackled in the case of static networks. In purely Gaussian networks, a straightforward application of the basic Gaussian message propagation algorithm gives rise to the famous Kalman ﬁlter. For nonlinear systems, we can apply the techniques of chapter 14 to derive the extended Kalman ﬁlter and the unscented ﬁlter. More interesting is the case where we have a hybrid system that contains both discrete and continuous variables. Here, in order to avoid an unbounded blowup in the number of components in the Gaussian mixture representing the belief state, we must collapse multiple Gaussians into one. Various standard approaches for performing this collapsing procedure have been developed in the tracking community; these approaches use a window length in the trajectory to determine which Gaussians to collapse. Interestingly, these approaches can be naturally viewed as variants of the EP algorithm, with diferent message approximation schemes. 

Our presentation in this chapter has focused on inference methods that exploit in some way the speciﬁc properties of inference in temporal models. However, all of the inference methods that we have discussed earlier in this book (as well as others) can be adapted to this setting. 

If we are willing to unroll the network fully, we can use any inference algorithm that has been developed for static networks. But even in the temporal setting, we can adapt other inference algorithms to the task of inference within a pair of consecutive time slices for the purpose of propagating a belief state. Thus, for example, we can consider variational approximation over a pair of time slices, or MCMC sampling, or various combinations of diferent algorithms. Many such variants have been proposed; see section 15.6 for references to some of them. 

All of our analysis in this chapter has assumed that the structure of the diferent time slices is the same, so that a ﬁxed approximation structure could be reasonably used for all time slices $t$ . In practice, we might have processes that are not homogeneous, whether because the process itself is nonstationary or because of sparse events that can radically change the momentary system dynamics. The problem of dynamically adapting the approximation structure to changing circumstances is an exciting direction of research. Also related is the question of dealing with systems whose very structure changes over time, for example, a road where the number of cars can change dynamically. 

This last point is related, naturally, to the problem of inference in other types of template-based models, some of which we described in chapter 6 but whose inference properties we did not tackle here. Of course, one option is to construct the full ground network and perform standard inference. However, this approach can be quite costly and even intractable. An important goal is to develop methods that somehow exploit structure in the template-based model to reduce the computational burden. Ideally, we would run our entire inference at the template level, avoiding the step of generating a ground network. This process is called lifted inference , in analogy to terms used in ﬁrst-order logic theorem proving. As a somewhat less lofty goal, we would like to develop algorithms that use properties of the ground network, for example, the fact that it has repeated substructures due to the use of templates. These directions provide an important trajectory for current and future research. 

# 15.6 Relevant Literature 

The earliest instances of inference in temporal models were also some of the ﬁrst applications of dynamic programming for probabilistic inference in graphical models: the forward-backward algorithm of Rabiner and Juang (1986) for hidden Markov models, and the Kalman ﬁltering algorithm. 

Kjærulf (1992, 1995a) provided the ﬁrst algorithm for probabilistic inference in DBNs, based on a clique tree formulation applied to a moving window in the DBNs. Darwiche (2001a) studies the concept of slice-by-slice triangulation in a DBN, and suggests some new elimination strategies. Bilmes and Bartels (2003) extend on this work by providing a triangulation algorithm speciﬁcally designed for DBNs; they also show that it can be beneﬁcial to allow the inference data structure to span a window larger than two time slices. Binder, Murphy, and Russell (1997) show how exact clique tree propagation in a DBN can be performed in a space-efcient manner by using a time-space trade-of. 

Simple variants of particle-based ﬁltering were ﬁrst introduced by Handschin and Mayne (1969); Akashi and Kumamoto (1977). The popularity of these methods dates to the mid-1990s, where the resampling step was ﬁrst introduced to avoid the degeneracy problems inherent to the naive approaches. This idea was introduced independently in several communities under several names, including: dynamic mixture models (West 1993), bootstrap ﬁlters (Gordon et al. 1993), survival of the ﬁttest (Kanazawa et al. 1995), condensation (Isard and Blake 1998a), Monte Carlo ﬁlters (Kitagawa 1996), and sequential importance sampling (SIS) with resampling (SIR) (Doucet 1998). Kanazawa et al. (1995) propose the use of arc reversal for generating a better sampling distribution. Particle smoothing was ﬁrst proposed by Isard and Blake (1998b) and Godsill, Doucet, and West (2000). 

The success of these methods on a range of practical applications led to the development of multiple improvements, as well as some signiﬁcant analysis of the theoretical properties of these methods. Doucet, de Freitas, and Gordon (2001) and Ristic, Arulampalam, and Gordon (2004) provide an excellent overview of some of the key developments. 

The Viterbi algorithm for ﬁnding the MAP assignment in an HMM was proposed by Viterbi (1967); it is the ﬁrst incarnation of the variable elimination algorithm for MAP algorithm. 

The application of collapsed particles to switching linear systems were proposed by Akashi and Kumamoto (1977); Chen and Liu (2000); Doucet et al. (2000). Lerner et al. (2002) propose de- terministic particle methods as an alternative to the sampling based approach, and demonstrate signiﬁcant advantages in cases (such as fault diagnosis) where the distribution is highly peaked, so that sampling would generate the same particle multiple times. Marthi et al. (2002) describe an alternative sampling algorithm based on an MCMC approach with a decaying time window. 

Boyen and Koller (1998b) were the ﬁrst to study the entanglement phenomenon explicitly and to propose factorization of belief states as an approximation in DBN inference, describing an algorithm that came to be known as the BK algorithm . They also provided a theoretical analysis demonstrating that, under certain conditions, the error accumulated over time remains bounded. Boyen and Koller (1998a) extend these ideas to the smoothing task. Murphy and Weiss (2001) suggested an algorithm that uses belief propagation within a time slice rather than a clique tree algorithm, as well as an iterated variant of the BK algorithm. Boyen and Koller (1999) study the properties of diferent belief state factorizations and ofer some guidance on how to select a factorization that leads to a good approximation; Paskin (2003b); Frogner and Pfefer (2007) suggest concrete heuristics for this adaptive process. Several methods that combine factorization with particle-based methods have also been proposed (Koller and Fratkina 1998; Murphy 1999; Lerner et al. 2000; Montemerlo et al. 2002; Ng et al. 2002). 

The Kalman ﬁltering algorithm was proposed by (Kalman 1960; Kalman and Bucy 1961); a simpler version was proposed as early as the nineteenth century (Thiele 1880). Normand and Tritchler (1992) provide a derivation of the Kalman ﬁltering equations from a Bayesian network perspective. Many extensions and improvements have been developed over the years. Bar- Shalom, Li, and Kirubarajan (2001) provide a good overview of these methods, including the extended Kalman ﬁlter, and a variety of methods for collapsing the Gaussian mixture in a switching linear-dynamical system. Kim and Nelson (1998) reviews a range of deterministic and MCMC-based methods for these systems. 

Lerner et al. (2000) and Lerner (2002) describe an alternative collapsing algorithm that pro- vides more ﬂexibility in deﬁning the Gaussian mixture; they also show how collapsing can be applied in a factored system, where discrete variables are present in multiple clusters. Heskes and Zoeter (2002) apply the EP algorithm to switching linear systems. Zoeter and Heskes (2006) describe the relationship between the GPB algorithms and expectation propagation and provide an experimental comparison of various collapsing methods. The unscented ﬁlter, which we de- scribed in chapter 14, was ﬁrst developed in the context of a ﬁltering task by Julier and Uhlmann (1997). It has also been used as a proposal distribution for particle ﬁltering (van der Merwe et al. 2000b,a), producing a ﬁlter of higher accuracy and asymptotic correctness guarantees. 

When a temporal model is viewed in terms of the ground network it generates, it is amenable to the application of a range of other approximate inference methods. In particular, the global variational methods of section 11.5 have been applied to various classes of sequence-based models (Ghahramani and Jordan 1997; Ghahramani and Hinton 1998). 

Temporal models have been applied to very many real-world problems, too numerous to list. Bar-Shalom, Li, and Kirubarajan (2001) describe the key role of these methods to target tracking. Isard and Blake (1998a) ﬁrst proposed the use of particle ﬁltering for visual tracking tasks; this approach, often called condensation , has been highly inﬂuential in the computer- vision community and has led to much follow-on work. The use of probabilistic models has also revolutionized the ﬁeld of mobile robotics, providing greatly improved solutions to the tasks of navigation and mapping (Fox et al. 1999; Thrun et al. 2000); see Thrun et al. (2005) for a detailed overview of this area. The use of particle ﬁltering, under the name Monte Carlo localization , has been particularly inﬂuential (Thrun et al. 2000; Montemerlo et al. 2002). However, factored models, both separately and in combination with particle-based methods, have also played a role in these applications (Murphy 1999; Paskin 2003b; Thrun et al. 2004,?), in particular as a representation of complex maps. Dynamic Bayesian models are also playing a role in speech- recognition systems (Zweig and Russell 1998; Bilmes and Bartels 2005) and in fault monitoring and diagnosis of complex systems (Lerner et al. 2002). 

Finally, we also refer the reader to Murphy (2002), who provides an excellent tutorial on inference in DBNs. 

# 15.7 Exercises 

# Exercise 15.1 

Consider clique-tree message passing, described in section 15.2.2, where the messages in the clique tree of ﬁgure 15.1 are passed ﬁrst from the beginning of the clique tree chain toward the end. 

a. Show that if sum-product message passing is used, the backward messages (in the “downward” pass) represent $P(o^{((t+1):T)}\mid S^{(t+1)})$ . b. Show that if belief-update message passing is used, the backward messages (in the “downward” pass) represent $P(S^{(t+1)}\mid o^{(1:T)})$ . 

# Exercise ${15.2\star}$ 

Consider the smoothing task for HMMs, implemented using the clique tree algorithm described in sec- tion 15.2.2. As discussed, the $O(N T)$ space requirement may be computationally prohibitive in certain settings. Let $K$ be the space required to store the evidence at each time slice. Show how, by caching a certain subset of messages, we can trade of time for space, resulting in an algorithm whose time requirements are $O(N^{2}T\,\bar{\log T})$ and space requirements are $O(N\log T)$ . 

# Exercise 15.3 

Prove proposition 15.1. 

# Exercise $15.4\star$ 

a. Prove the entanglement theorem, theorem 15.1. b. 2-TBN (not necessarily fully persistent) where $(X\ \perp\ Y\ \mid\ Z)$ holds persistently but $(X\perp Y\mid\dot{\emptyset})$ does not? If so, give an example. If not, explain formally why not. 

# Exercise 15.5 

Consider a fully persistent DBN over $n$ state variables $_{X}$ . Show that any clique tree over $X^{(t)},X^{(t+1)}$ that we can construct for performing the belief-state propagation step has induced width at least $n+1$ . 

# Exercise $15.6\star$ 

Recall that a hidden Markov model!factorial factorial HMMfactorial HMM (see ﬁgure 6.3) is a DBN over $X_{1},\ldots,X_{n},O$ such that the only parent of $X_{i}^{\prime}$ in the 2-TBN is $X_{i}$ , and the parents of $O^{\prime}$ are $X_{1}^{\prime},\dots,X_{n}^{\prime}$ . (Typically, some structured model is used to encode this CPD compactly.) Consider the problem of using a structured variational approximation (as in section 11.5) to perform inference over the unrolled network for a ﬁxed number $T$ of time slices. 

a. ace of approximate distributions $Q$ composed of disjoint clusters $\{X_{i}^{(0)},\ldots,X_{i}^{(T)}\}$ } for $i\,=\,1,\dots,n$ . Show the variational update equations, describe the use of inference to compute the messages, and analyze the computational complexity of the resulting algorithm. b. pace of approximate distributions $Q$ composed of disjoint clusters $\{X_{1}^{(1)},\cdot\cdot\cdot,X_{n}^{(t)}\}$ } for $t\,=\,1\,.\,.\,.\,,T$ . Show the variational update equations, describe the use of inference to compute the messages, and analyze the computational complexity of the resulting algorithm. c. Discuss the circumstances when you would use one approximation over the other. 

# Exercise $15.7\star$ 

In this question, you will extend the particle ﬁltering algorithm to address the smoothing task, that is, computing $P(X^{(t)}\mid o^{(1:T)})$ where $T>t$ is the “end” of the sequence. 

a. Prove using formal probabilistic reasoning that 

$$
P(\boldsymbol{X}^{(t)}\mid\boldsymbol{o}^{(1:T)})=P(\boldsymbol{X}^{(t)}\mid\boldsymbol{o}^{(1:t)})\cdot\sum_{\boldsymbol{X}^{(t+1)}}\frac{P(\boldsymbol{X}^{(t+1)}\mid\boldsymbol{o}^{(1:T)})P(\boldsymbol{X}^{(t+1)}\mid\boldsymbol{X}^{(t)})}{P(\boldsymbol{X}^{(t+1)}\mid\boldsymbol{o}^{(1:t)})}.
$$ 

b. Based on this formula, construct an extension to the particle ﬁltering algorithm that: 

• Does a ﬁrst pass that is identical to particle ﬁltering, but where it keeps the samples $\bar{\pmb{x}}^{(0:t)}[m],w^{(t)}[m]$ generated for each time slice $t$ . • Has a second phase that updates the weights of the samples at the diferent time slices based on the formula in part 1, with the goal of getting an approximation to $P(X^{(t)}\mid o^{(1:T)})$ . 

Write your algorithm using a few lines of pseudo-code. Provide a brief description of how you would use your algorithm to estimate the probability of $P(X^{(t)}=\pmb{x}\mid o^{(1:T)})$ for some assignment $_{_{\pmb{x}}}$ . 

# Exercise $15.8\star$ 

One of the problems with the particle ﬁltering algorithm is that it uses only the observations obtained so far to select the samples that we continue to propagate. In many cases, the “right” choice of samples at time $t$ is not clear based on the evidence up to time $t$ , but it manifests a small number of time slices into the future. By that time, however, the relevant samples may have been eliminated in favor of ones that appeared (based on the limited evidence available) to be better. 

In this problem, your task is to extend the particle ﬁltering algorithm to deal with this problem. More precisely, assume that the relevant evidence usually manifests within $k$ time slices (for some small $k$ ). Consider performing the particle-ﬁltering algorithm with a lookahead of $k$ time slices rather than a single time slice. Present the algorithm clearly and mathematically, specifying exactly how the weights are computed and how the next-state samples are generated. Brieﬂy explain why your algorithm is sampling from (roughly) the right distribution (right in the same sense that standard particle ﬁltering is sampling from the right distribution). 

For simplicity of notation, assume that the process is structured as a state-observation model. 

(Hint: Use your answer to exercise 15.7.) 

# Exercise ${15.9\star\star}$ 

collapsed particle ﬁltering 

In this chapter, we have discussed only the application of particle ﬁltering that samples all of the network variables. In this exercise, you will construct a collapsed-particle-ﬁltering method. Consider a DBN where we have observed variables $^o$ , and the unobserved variables are divided into two disjoint groups $_{X}$ and $\mathbf{Y}$ . Assume that, in the 2-TBN, the parents of $X^{\prime}$ are only variables in $_{X}$ , and that we can efciently perform exact inference on $P(Y^{'}\mid X,X^{\prime},o^{\prime})$ . Describe an algorithm that uses collapsed particle ﬁltering for this type of DBN, where we represent the approximate belief state $\hat{\sigma}^{(t)}$ using a set of weighted collapsed particles, where the variables $\bar{\mathbf{\mathit{X}}}^{(t)}$ are sampled, and for each sample $X^{(\bar{t})}[m]$ , we have an exact distribution over the variables $Y^{(t)}$ . Speciﬁcally, describe how each sample $\pmb{x}^{(t+1)}[m]$ is generated from the time $t$ samples, how to compute the associated distribution over $Y^{(t)}$ , and how to compute the appropriate importance weights. Make sure your derivation is consistent with the analysis used in section 12.4.1. 

# Exercise 15.10 

beam search In this exercise, we apply the beam-search methods described in section 13.7 to the task of ﬁnding high- probability assignments in an HMM. Assume that our hidden state in the HMM is denoted $X^{(t)}$ and the observations are denoted $O^{(t)}$ . Our goal is to ﬁnd $x^{*\,(1:T)}\,=\,\arg\operatorname*{max}_{x^{(1:T)}}P(X^{(1:T)}\mid\,O^{(1:t)})$ . Importantly, however, we want to perform the beam search in an online fashion, adapting our set of candidates as new observations arrive. 

a. Describe how beam search can be applied in the context of an HMM. Specify the search procedure and suggest a number of pruning strategies. b. Now, assume we have an additional set of variables $Y^{(t)}$ that are part of our model and whose value we do not care about. That is, our goal has not changed. Assume that our 2-TBN model does not contain an arc $Y\rightarrow X^{\prime}$ . Describe how beam search can be applied in this setting. 

# Exercise 15.11 

Construct an algorithm that performs tracking in linear-Gaussian dynamical systems, maintaining the belief state in terms of the canonical form described in section 14.2.1.2. 

# Exercise $15.12\times$ 

Consider a nonlinear 2-TBN where we have a set of observation variables $O_{1},\ldots,O_{k}$ , where each $O_{i}^{\prime}$ i is a leaf in the 2-TBN and has the parents $U_{i}\in X^{\prime}$ . S we can use the methods of exercise 14.9 to perform the step of conditioning on the observation O $O^{\prime}=o^{\prime}$ , without forming an entire joint distribution over $X^{\prime}\cup O$ . Your method should perform the numerical integration over a space with as small a dimension as possible. 

# Exercise $15.13\star$ 

a. Write down the probabilistic model for the Gaussian SLAM problem with $K$ landmarks. b. Derive the equations for a collapsed bootstrap-sampling particle-ﬁltering algorithm in FastSLAM. Show how the samples are generated, how the importance weights are computed, and how the posterior is maintained. c. Derive the equations for the posterior collapsed-particle-ﬁltering algorithm, where $\pmb{x}^{(t+1)}[m]$ is gener- ated from ${\bar{P(X^{(t+1)}}}\mid\mathbf{x}^{(t)}{\bar{[m]}},\mathbf{y}^{(t+1)})$ . Show how the samples are generated, how the importance weights are computed, and how the posterior is maintained. d. Now, consider a diferent approach of applied collapsed-particle ﬁltering to this problem. Here, we t the landmark positions $\mathbf{\dot{L}}=\{L_{1},\,\,\hat{\cdot}\,\hat{\cdot}\,,L_{k}\}$ as our set of ampled variables, and for each particle $\imath[m]$ , we maintain a distribution over the robot’s state at time t . Without describing the details of this algorithm, explain qualitatively what will happen to the particles and their weights eventually (as $T$ grows). Are there conditions under which this algorithm will converge to the correct map? 

# Part III 

# Learning 

# 16 Learning Graphical Models: Overview 

# 16.1 Motivation 

In most of our discussions so far, our starting point has been a given graphical model. For example, in our discussions of conditional independencies and of inference, we assumed that the model — structure as well as parameters — was part of the input. 

There are two approaches to the task of acquiring a model. The ﬁrst, as we discussed in box 3.C, is to construct the network by hand, typically with the help of an expert. However, as we saw, knowledge acquisition from experts is a nontrivial task. The construction of even a modestly sized network requires a skilled knowledge engineer who spends several hours (at least) with one or more domain experts. Larger networks can require weeks or even months of work. This process also generally involves signiﬁcant testing of the model by evaluating results of some “typical” queries in order to see whether the model returns plausible answers. 

Such “manual” network construction is problematic for several reasons. In some domains, the amount of knowledge required is just too large or the expert’s time is too valuable. In others, there are simply no experts who have sufcient understanding of the domain. In many domains, the properties of the distribution change from one application site to another or over time, and we cannot expect an expert to sit and redesign the network every few weeks. In many settings, however, we may have access to a set of examples generated from the distribution we wish to model. In fact, in the Information Age, it is often easier to obtain even large amounts of data in electronic form than it is to obtain human expertise. For example, in the setting of medical diagnosis (such as box 3.D), we may have access to a large collection of patient records, each listing various attributes such as the patient’s history (age, sex, smoking, previous medical complications, and so on), reported symptoms, results of various tests, the physician’s initial diagnosis and prescribed treatment, and the treatment’s outcome. We may hope to use these data to learn a model of the distribution of patients in our population. In the case of pedigree analysis (box 3.B), we may have some set of family trees where a particular disease (for example, breast cancer) occurs frequently. We can use these family trees to learn the parameters of the genetics of the disease: the transmission model, which describes how often a disease genotype is passed from the parents to a child, and the penetrance model, which deﬁnes the probability of diferent phenotypes given the genotype. In an image segmentation application (box 4.B), we might have a set of images segmented by a person, and we might wish to learn the parameters of the MRF that deﬁne the characteristics of diferent regions, or those that deﬁne how strongly we believe that two neighboring pixels should be in the same segment. 

It seems clear that such instances can be of use in constructing a good model for the underlying distribution, either in isolation or in conjunction with some prior knowledge acquired from a human. This task of constructing a model from a set of instances is generally called model learning . In this part of the book, we focus on methods for addressing diferent variants of this task. In the remainder of this chapter, we describe some of these variants and some of the issues that they raise. 

To make this discussion more concrete, let us assume that the domain is governed by some underlying distribution $P^{*}$ , which is induced by some (directed or undirected) network model $\mathcal{M}^{*}\,=\,(\mathcal{K}^{*},\theta^{*})$ . We are given a data set ${\mathcal D}\,=\,\{d[1],.\,.\,.\,,d[M]\}$ of $M$ samples from $P^{*}$ . The standard assumption, whose statistical implications were brieﬂy discussed in appendix A.2, is that the data instances are sampled independently from $P^{*}$ ; as we discussed, such data instances are called independent and identically distributed (IID) . We are also given some family of models, and our task is to learn some model M in this family that deﬁnes a distribution $P_{\tilde{\mathcal{M}}}$ M (or $\tilde{P}$ when $\tilde{\mathcal{M}}$ M is clear from the context). We may want to learn only model parameters for a ﬁxed structure, or some or all of the structure of the model. In some cases, we might wish to present a spectrum of diferent hypotheses, and so we might return not a single model but rather a probability distribution over models, or perhaps some estimate of our conﬁdence in the model learned. 

We ﬁrst describe the set of goals that we might have when learning a model and the diferent evaluation metrics to which they give rise. We then discuss how learning can be viewed as an optimization problem and the issues raised by the design of that problem. Finally, we provide a detailed taxonomy of the diferent types of learning tasks and discuss some of their computational ramiﬁcations. 

# 16.2 Goals of Learning 

To evaluate the merits of diferent learning methods, it is important to consider our goal in learning a probabilistic model from data. A priori, it is not clear why the goal of the learning is important. After all, our ideal solution is to return a model M that precisely captures the distribution $P^{*}$ from which our data were sampled. Unfortunately, this goal is not generally achievable, because of computational reasons and (more importantly) because a limited data set provides only a rough approximation of the true underlying distribution. In practice, the amount of data we have is rarely sufcient to obtain an accurate representation of a high-dimensional distribution involving many variables. Thus, we have to select M so as to construct the “best” approximation to $\mathcal{M}^{*}$ . The notion of “best” depends on our goals. Diferent models will generally embody diferent trade-ofs. One approximate model may be better according to one performance metric but worse according to another. Therefore, to guide our development of learning algorithms, we must deﬁne the goals of our learning task and the corresponding metrics by which diferent results will be evaluated. 

# 16.2.1 Density Estimation 

The most common reason for learning a network model is to use it for some inference task that we wish to perform. Most obviously, as we have discussed throughout most of this book so far, a graphical model can be used to answer a range of probabilistic inference queries. In this density estimation setting, we can formulate our learning goal as one of density estimation : constructing a model M such that $\tilde{P}$ is “close” to the generating distribution $P^{*}$ . 

How do we evaluate the quality of an approximation $\tilde{\mathcal{M}}?$ M ? One commonly used option is to use the relative entropy distance measure deﬁned in deﬁnition A.5: 

$$
D(P^{*}\|\tilde{P})=E_{\xi\sim P^{*}}\biggl[\log\left(\frac{P^{*}(\xi)}{\tilde{P}(\xi)}\right)\biggr].
$$ 

Recall that this measure is zero when $\tilde{P}=P^{*}$ and positive otherwise. Intuitively, it measures the extent of the compression loss (in bits) of using $\tilde{P}$ rather than $P^{*}$ . 

To evaluate this metric, we need to know $P^{*}$ . In some cases, we are evaluating a learning algorithm on synthetically generated data, and so $P^{*}$ may be known. In real-world applications, however, $P^{*}$ is not known. (If it were, we would not need to learn a model for it from a data set.) However, we can simplify this metric to obtain one that is easier to evaluate: 

Proposition 16.1 For any distributions $P,P^{\prime}$ over $\mathcal{X}$ : 

$$
D(P\|P^{\prime})=-H_{P}(\mathcal{X})-E_{\xi\sim P}[\log P^{\prime}(\xi)].
$$ 

Proof 

$$
\begin{array}{r c l}{D(P\|P^{\prime})}&{=}&{E_{\xi\sim P}\bigg[\mathrm{log}\left(\frac{P(\xi)}{P^{\prime}(\xi)}\right)\bigg]}\\ &{=}&{E_{\xi\sim P}[\mathrm{log}\,P(\xi)-\mathrm{log}\,P^{\prime}(\xi)]}\\ &{=}&{E_{\xi\sim P}[\mathrm{log}\,P(\xi)]-E_{\xi\sim P}[\mathrm{log}\,P^{\prime}(\xi)]}\\ &{=}&{-H_{P}(\mathcal{X})-E_{\xi\sim P}[\mathrm{log}\,P^{\prime}(\xi)].}\end{array}
$$ 

expected log-likelihood 

likelihood log-likelihood 

log-loss 

loss function 

Applying this derivation to $P^{*},\tilde{P}$ , we see that the ﬁrst of these two terms is the negative entropy of $P^{*}$ ; because it does not depend on $\tilde{P}$ , it does not afect the comparison between diferent approximate models. We can therefore focus our evaluation metric on the second term $E_{\xi\sim P^{*}}\Bigl[\bar{\log\tilde{P}}(\xi)\Bigr]$ h i and prefer models that make this term as large as possible. This term is called an expected log-likelihood . It encodes our preference for models that assign high probability to instances sampled from $P^{*}$ . Intuitively, the higher the probability that M gives to points sampled from the true distribution, the more reﬂective it is of this distribution. We note that, in moving from the relative entropy to the expected log-likelihood, we have lost our baseline $E_{P^{*}}[\log P^{*}]$ , an inevitable loss since we do not know $P^{*}$ . As a consequence, although we ∗ can use the log-likelihood as a metric for comparing one learned model to another, we cannot evaluate a particular M in how close it is to the unknown optimum. 

More generally, in our discussion of learning we will be interested in the likelihood of the data, given a model $\mathcal{M}$ , which is $P(\mathcal{D}\,:\,\mathcal{M})$ , or for convenience using the log-likelihood $\ell(\mathcal{D}:\mathcal{M})=\log P(\mathcal{D}:\mathcal{M})$ . 

It is also customary to consider the negated form of the log-likelihood, called the log-loss . The log-loss reﬂects our cost (in bits) per instance of using the model $\tilde{P}$ . The log-loss is our ﬁrst example of a loss function , a key component in the statistical machine-learning paradigm. A loss function $l o s s(\xi\ :\ \mathcal{M})$ measures the loss that a model $\mathcal{M}$ makes on a particular instance risk 

empirical risk $\xi$ . When instances are sampled from some distribution $P^{*}$ , our goal is to ﬁnd a model that minimizes the expected loss , or the risk : 

$$
E_{\xi\sim P^{*}}[l o s s(\xi~:~\mathcal{M})].
$$ 

In general, of course, $P^{*}$ is unknown. However, we can approximate the expectation using an empirical average and estimate the risk relative to $P^{*}$ with an empirical risk averaged over a set $\mathcal{D}$ of instances sampled from $P^{*}$ : 

$$
E_{\mathcal{D}}[l o s s(\xi\ :\ \mathcal{M})]=\frac{1}{|\mathcal{D}|}\sum_{\xi\in\mathcal{D}}l o s s(\xi\ :\ \mathcal{M}).
$$ 

In the case of the log-loss, this expression has a very intuitive interpretation. Consider a data set $\mathcal{D}=\{\xi[1],\dots,\xi[M]\}$ composed of IID instances. The probability that $\mathcal{M}$ ascribes to $\mathcal{D}$ is 

$$
P(\mathcal{D}\ :\ \mathcal{M})=\prod_{m=1}^{M}P(\xi[m]\ :\ \mathcal{M}).
$$ 

Taking the logarithm, we obtain 

$$
\log P(\mathcal{D}~:~\mathcal{M})=\sum_{m=1}^{M}\log P(\xi[m]~:~\mathcal{M}),
$$ 

which is precisely the negative of the empirical log-loss that appears inside the summation of equation (16.1). 

The risk can be used both as a metric for evaluating the quality of a particular model and as a factor for selecting a model among a given class, given a training set $\mathcal{D}$ . We return to these ideas in section 16.3.1 and box 16.A. 

# 16.2.2Speciﬁc Prediction Tasks 

In the preceding discussion, we assumed that our goal was to use the learning model to perform probabilistic inference. With that assumption, we jumped to the conclusion that we wish to ﬁt the overall distribution $P^{*}$ as well as possible. However, that objective measures only our ability to evaluate the overall probability of a full instance $\xi$ . In reality, the model can be used for answering a whole range of queries of the form $P(Y\mid X)$ . In general, we can devise a test suite of queries for our learned model, which allows us to evaluate its performance on a range of queries. Most attention, however, has been paid to the special case where we have a particular set of variables $Y$ that we are interested in predicting, given a certain set of variables $X$ . 

classiﬁcation task 

Most simply, we may want to solve a simple classiﬁcation task , the goal of a large fraction of the work in machine learning. For example, consider the task of document classiﬁcation, where we have a set $X$ of words and other features characterizing the document, and a variable $Y$ that labels the document topic. In image segmentation, we are interested in predicting the class labels for all of the pixels in the image $(Y)$ , given the image features $(X)$ . There are many other such examples. 

A model trained for a prediction task should be able to produce for any instance characterized by $_{_{x}}$ , the probability distribution ${\tilde{P}}(Y\mid x)$ | . We might also wish to select the MAP assignment of this conditional distribution to produce a speciﬁc prediction: 

$$
h_{\tilde{P}}({\pmb x})=\arg\operatorname*{max}_{\pmb y}\tilde{P}({\pmb y}\mid{\pmb x}).
$$ 

classiﬁcation error 

0/1 loss 

Hamming loss 

conditional likelihood 

What loss function do we want to use for evaluating a model designed for a prediction task? We can, for example, use the classiﬁcation error , also called the 0/1 loss : 

$$
E_{(x,y)\sim\Tilde{P}}[I\{h_{\Tilde{P}}(x)\neq y\}],
$$ 

which is simply the probability, over all $(x,y)$ pairs sampled from $\tilde{P}$ , that our classiﬁer selects the wrong label. While this metric is suitable for labeling a single variable, it is not well suited to situations, such as image segmentation, where we simultaneously provide labels to a large number of variables. In this case, we do not want to penalize an entire prediction with an error of 1 if we make a mistake on only a few of the target variables. Thus, in this case, we might consider performance metrics such as the Hamming loss , which, instead of using the indicator function ${I\!\!\{h_{\Tilde{P}}({\pmb x})\neq{\pmb y}\}}$ { ̸ } , counts the number of variables $Y$ in which $h_{\Tilde{P}}(\pmb{x})$ difers from the ground truth $_{_y}$ . 

We might also wish to take into account the conﬁdence of the prediction. One such criterion is the conditional likelihood or its logarithm (sometimes called the conditional log-likelihood ): 

$$
E_{(x,y)\sim P^{*}}\Big[\log\tilde{P}(y\mid x)\Big].
$$ 

Like the log-likelihood criterion, this metric evaluates the extent to which our learned model is able to predict data generated from the distribution. However, it requires the model to predict only $Y$ given $X$ , and not the distribution of the $X$ variables. As before, we can negate this expression to deﬁne a loss function and compute an empirical estimate by taking the average relative to a data set $\mathcal{D}$ . 

If we determine, in advance, that the model will be used only to perform a particular task, we may want to train the model to make trade-ofs that make it more suited to that task. In particular, if the model is never evaluated on predictions of the variables $X$ , we may want to design our training regime to optimize the quality of its answers for $Y$ . We return to this issue in section 16.3.2. 

# 16.2.3 Knowledge Discovery 

Finally, a very diferent motivation for learning a model for a distribution $P^{*}$ is as a tool for discovering knowledge about $P^{*}$ . We may hope that an examination of the learned model can reveal some important properties of the domain: what are the direct and indirect dependencies, what characterizes the nature of the dependencies (for example, positive or negative correlation), and so forth. For example, in the genetic inheritance domain, it may be of great interest to discover the parameter governing the inheritance of a certain property, since this parameter can provide signiﬁcant biological insight regarding the inheritance mechanism for the allele(s) governing the disease. In a medical diagnosis domain, we may want to learn the structure of the model to discover which predisposing factors lead to certain diseases and which symptoms 

knowledge discovery 

are associated with diferent diseases. Of course, simpler statistical methods can be used to explore the data, for example, by highlighting the most signiﬁcant correlations between pairs of variables. However, a learned network model can provide parameters that have direct causal interpretation and can also reveal much ﬁner structure, for example, by distinguishing between direct and indirect dependencies, both of which lead to correlations in the resulting distribution. 

The knowledge discovery task calls for a very diferent evaluation criterion and a diferent set of compromises from a prediction task. In this setting, we really do care about reconstructing correct model $\mathcal{M}^{*}$ , rather than some other model M that induces a distribution similar to $\mathcal{M}^{*}$ M . Thus, in contrast to d mation, where our metric was on the distribution deﬁned by the model (for example, I $D(\dot{P^{*}}\|\tilde{P}))$ | | ), here our measure of success is in terms of the model, that is, diferences between M and M . Unfortunately, this goal is often not achievable, for several reasons. 

identiﬁability 

# 

First, even with large amounts of data, the true model may not be identiﬁable . Consider, for example, the task of recovering aspects of the correct network structure $\kappa^{*}$ . As one obvious difculty, recall that a given Bayesian network structure often has several I-equivalent structures. If such is the case for our target distribution $\kappa^{*}$ , the best we can hope for is to recover an I-equivalent structure. The problems are signiﬁcantly greater when the data are limited. Here, for example, if $X$ and $Y$ are directly related in $\kappa^{*}$ but the parameters relating them induce only a weak relationship, it may be very difcult to detect the correlation in the data and distinguish it from a random ﬂuctuations. This limitation is less of a problem for a density estimation task, where ignoring such weak correlations often has very few repercussions on the quality of our learned density; however, if our task focuses on correct reconstruction of structure, examples such as this reduce our accuracy. Conversely, when the number of variables is large relative to the amount of the training data, there may well be pairs of variables that appear strongly correlated simply by chance. Thus, we are also likely, in such settings, to infer the presence of edges that do not exist in the underlying model. Similar issues arise when attempting to infer other aspects of the model. 

The relatively high probability of making model identiﬁcation errors can be signiﬁcant if the goal is to discover the correct structure of the underlying distribution. For example, if our goal is to infer which genes regulate which other genes (as in box 21.D), and if we plan to use the results of our analysis for a set of (expensive and time-consuming) wet-lab experiments, we may want to have some conﬁdence in the inferred relationship. Thus, in a knowledge discovery application, it is far more critical to assess the conﬁdence in a prediction, taking into account the extent to which it can be identiﬁed given the available data and the number of hypotheses that would give rise to similar observed behavior. We return to these issues more concretely later on in the book (see, in particular, section 18.5). 

# 16.3 Learning as Optimization 

hypothesis space objective function 

The previous section discussed diferent ways in which we can evaluate our learned model. In many of the cases, we deﬁned a numerical criterion — a loss function — that we would like to optimize. This perspective suggests that the learning task should be viewed as an optimization problem: we have a hypothesis space , that is, a set of candidate models, and an objective function , a criterion for quantifying our preference for diferent models. Thus, our learning task can be formulated as one of ﬁnding a high-scoring model within our model class. The view of learning as optimization is currently the predominant approach to learning (not only of probabilistic models). 

In this section, we discuss diferent choices of objective functions and their ramiﬁcation on the results of our learning procedure. This discussion raises important points that will accompany us throughout this part of the book. We note that the formal foundations for this discussion will be established in later chapters, but the discussion is of fundamental importance to our entire discussion of learning, and therefore we introduce these concepts here. 

# 16.3.1 Empirical Risk and Overﬁtting 

empirical distribution Consider the task of constructing a model $\mathcal{M}$ that optimizes the expect n of a particular loss function $E_{\xi\sim P^{*}}[l o s s(\xi~:~\mathcal{M})]$ M . Of course, we nerally do not know P $P^{*}$ , but, as we have ∼ discussed, we can use a data set D sampled from $P^{*}$ to produce an empirical estimate this expectation. More formally, we can use the data D to deﬁne an empirical distribution $\hat{P}_{\mathcal{D}}$ D , as follows: 

$$
{\hat{P}}_{\mathcal{D}}(A)={\frac{1}{M}}\sum_{m}\mathbf{I}\{\xi[m]\in A\}.
$$ 

That is, the probability of the event $A$ is simply the fraction of training examples that satisfy $A$ . It is clear that $\hat{P}_{\mathcal{D}}^{\cdot}(A)$ is a probability distribution. Moreover, as the number of training D examples grows, the empirical distribution approaches the true distribution. 

Theorem 16.1 

$$
\operatorname*{lim}_{M\to\infty}\hat{P}_{\mathcal{D}_{M}}(A)=P^{*}(A)
$$ 

almost surely. 

Thus, for a sufciently large training set, $\hat{P}_{\mathcal{D}}$ will be quite close to the original distribution D $P^{*}$ with h probability that converges to 1 as $M\rightarrow\infty$ ). Since we do not have direct access to $P^{*}$ , we can use $\hat{P}_{\mathcal{D}}$ as the best proxy and try to minimize our loss function relative D to $\hat{P}_{\mathcal{D}}$ . Unfortunately, a naive application of this optimization objective can easily lead to very D poor results. 

Consider, for example, the use of the empirical log-loss (or log-likelihood) as the objective. It is not difcult to show (see section 17.1) that the distribution that maximizes the likelihood of a data set $\mathcal{D}$ is the empirical distribution $\hat{P}_{\mathcal{D}}$ itself. Now, assume that we have a distribution over D a probability space deﬁned by 100 binary random variables, for a total of $2^{100}$ possible joint assignments. If our data set $\mathcal{D}$ contains 1,000 instances (most likely distinct from each other), the empirical distribution will give probability 0 . 001 to each of the assignments that appear in $\mathcal{D}$ , and probability 0 to all $2^{100}\mathrm{~-~}1,000$ other assignments. While example is obviously extreme, the phenomenon is quite general. For example, assume that M is a Bayesian network where some variables, such as Fever , have a large number of parents $X_{1},\ldots,X_{k}$ . In a table- CPD, the number of parameters grows exponentially with the number of parents $k$ . For large $k$ , we are highly unlikely to encounter, in $\mathcal{D}$ , instances that to all possible parent instantiations, that is, all possible combinations of diseases $X_{1},\ldots,X_{k}$ . If we do not have enough data, many of the cases arising in our CPD will have very little (or no) relevant training data, leading to very poor estimates of the conditional probability of Fever in this context. In general, as we will see in later chapters, the amount of data required to estimate parameters reliably grows linearly with the number of parameters, so that the amount of data required can grow exponentially with the network connectivity. 

overﬁtting 

generalization 

As we see in this example, there is a signiﬁcant problem with using the empirical risk (the loss on our training data) as a surrogate for our true risk. In particular, this type of objective tends to overﬁt the learned model to the training data. However, our goal is to answer queries about examples that were not in our training set. Thus, for example, in our medical diagnosis example, the patients to which the learned network will be applied are new patients, not the ones on whose data the network was trained. In our image-segmentation example, the model will be applied to new (unsegmented) images, not the (segmented) images on which the model was trained. Thus, it is critical that the network generalize to perform well on unseen data. 

The need to generalize to unseen instances and the risk of overﬁtting to the training set raise an important trade-of that will accompany us throughout our discussion. On one hand, if our hypothesis space is very limited, it might not be able to represent our true target distribution $P^{*}$ . Thus, even with unlimited data, we may be unable to capture $P^{*}$ , and thereby remain with a suboptimal model. This type of limitation in a hypothesis space introduces inherent error in the result of the learning procedure, which is called bias , since the learning procedure is limited in how close it can approximate the target distribution. Conversely, if we select a hypothesis space that is highly expressive, we are more likely to be able to represent correctly the target distribution $P^{*}$ . However, given a small data set, we may not have the ability to select the “right” model among the large number of models in the hypothesis space, many of which may provide equal or perhaps even better loss on our limited (and thereby unrepresentative) training set $\mathcal{D}$ . Intuitively, when we have a rich hypothesis space and limited number of samples, small random ﬂuctuations in the choice of $\mathcal{D}$ can radically chang he properties of the selected model, often resulting in models that have little relationship to $P^{*}$ . As a result, the learning procedure will sufer from a high variance — running it on multiple data sets from the same $P^{*}$ will lead to highly variable results. Conversely, if we have a more limited hypothesis space, we are less likely to ﬁnd, by chance, a model that provides a good ﬁt to $\mathcal{D}$ . T , a high-scoring model within our limited hypothesis space is more likely to be a good ﬁt to P $P^{*}$ , and thereby is more likely to generalize to unseen data. 

bias-variance trade-of 

This bias-variance trade-of underlies many of our design choices in learning. When selecting a hypothesis space of diferent models, we must take care not to allow too rich a class of possible models. Indeed, with limited data, the error introduced by variance may be larger than the potential error introduced by bias, and we may choose to restrict our learning to models that are too simple to correctly encode $P^{*}$ . Although the learned model is guaranteed to be incorrect, our ability to estimate its parameters more reliably may well compensate for the error arising from incorrect structural assumptions. Moreover, when learning structure, although we will not correctly learn all of the edges, this restriction may allow us to more reliably learn the most important edges. In other words, restricting the space of possible models leads us to select models $\tilde{\mathcal{M}}$ M whose performance on the training objective is poorer, but whose distance to $P^{*}$ is better. 

Restricting our model class is one way to reduce overﬁtting. In efect, it imposes a hard constraint that prevents us from selecting a model that precisely captures the training data. A regularization second, more reﬁned approach is to change our training objective so as to incorporate a soft preference for simpler models. Thus, our learning objective will usually incorporate competing components: some components will tend to move us toward models that ﬁt well with our observed data; others will provide regularization that prevents us from taking the speciﬁcs of the data to extremes. In many cases, we adopt a combination of the two approaches, utilizing both a hard constraint over the model class and an optimization objective that leads us away from overﬁtting. 

The preceding discussion described the phenomenon of overﬁtting and the importance of ensuring that our learned model generalizes to unseen data. However, we did not discuss how to tell whether our model generalizes, and how to design our hypothesis space and/or objective function so as to reduce this risk. Box 16.A discusses some of the basic experimental protocols that one uses in the design and evaluation of machine learning procedures. Box 16.B discusses a basic theoretical framework that one can use to try and answer questions regarding the appropriate complexity of our model class. 

Box 16.A — Skill: Design and Evaluation of Learning Procedures. A basic question in learn- ing is to evaluate the performance of our learning procedure. We might ask this question in a rela- tive sense, to compare two or more alternatives (for example, diferent hypothesis spaces, or diferent training objectives), or in an absolute sense, when we want to test whether the model we have learned “captures” the distribution. Both questions are nontrivial, and there is a large literature on how to address them. We brieﬂy summarize some of the main ideas here. 

In both cases, we would ideally like to compare the learned model to the real underlying distri- bution that generated the data. This is indeed the strategy we use for evaluating performance when learning from synthetic data sets where we know (by design) the generating distribution (see, for example, box 17.C). Unfortunately, this strategy is infeasible when we learn from real-life data sets where we do not have access to the true generating distribution. And while synthetic studies can help us understand the properties of learning procedures, they are limited in that they are often not representative of the properties of the actual data we are interested in. 

holdout testing training set test set 

Evaluating Generalization Performance We start with the ﬁrst question of evaluating the per- formance of a given model, or a set of models, on unseen data. One approach is to use holdout testing . In this approach, we divide our data set into two disjoint sets: the training set $\mathcal{D}_{t r a i n}$ and test set $\mathcal{D}_{t e s t}.$ . To avoid artifacts, we usually use a randomized procedure to decide on this partition. We then learn a model using $\mathcal{D}_{t r a i n}$ (with some appropriate objective function), and we measure our pe rmance (using some appropriate loss function) on $\mathcal{D}_{t e s t}$ . Because $\mathcal{D}_{t e s t}$ is also sampled from $P^{*}$ , it provides us with an empirical estimate of the risk. Importantly, however, because $\mathcal{D}_{t e s t}$ is disjoint from $\mathcal{D}_{t r a i n},$ , we are measuring our loss using instances that were unseen during the training, and not on ones for which we optimized our performance. Thus, this approach provides us with an unbiased estimate of the performance on new instances. 

Holdout testing can be used to compare the performance of diferent learning procedures. It can also be used to obtain insight into the performance of a single learning procedure. In particular, we can compare the performance of the procedure (say the empirical log-loss per instance, or the classiﬁcation error) on the training set and on the held-out test set. Naturally, the training set performance will be better, but if the diference is very large, we are probably overﬁtting to the training data and may want to consider a less expressive model class, or some other method for discouraging overﬁtting. 

Holdout testing poses a dilemma. To get better estimates of our performance, we want to increase the size of the test set. Such an increase, however, decreases the size of the training set, which results in degradation of quality of the learned model. When we have ample training data, we can ﬁnd reasonable compromises between these two considerations. When we have few training samples, there is no good compromise, since decreasing either the training or the test set has large ramiﬁcations either on the quality of the learned model or the ability to evaluate it. 

An alternative solution is to attempt to use available data for both training and testing. Of course, we cannot test on our training data; the trick is to combine our estimates of performance from repeated rounds of holdout testing. That is, in each iteration we train on some subset of the instances and test on the remaining ones. If we perform multiple iterations, we can use a relatively small test set in each iteration, and pool the performance counts from all of them to estimate performance. The question is how to allocate the training and test data sets. A commonly used procedure is $k$ -fold cross-validation , where we use each instance once for testing. This is done by partitioning the original data into $k$ equally sized sets, and then in each iteration holding as test data one partition and training from all the remaining instances; see algorithm 16.A.1. An extreme case of cross-validation is leave one out cross-validation , where we set $k\,=\,M$ , that is, in each iteration we remove one instance and use it as a testing case. Both cross-validation schemes allow us to estimate not only the average performance of our learning algorithm, but also the extent to which this performance varies across the diferent folds. 

Both holdout testing and cross-validation are primarily used as methods for evaluating a learning procedure. In particular, a cross-validation procedure constructs $k$ diferent models, one for each partition of the training set into training/test folds, and therefore does not result in even a single model that we can subsequently use. If we want to write a paper on a new learning procedure, the results of cross-validation provide a good regime for evaluating our procedure and comparing it to others. If we actually want to end up with a real model that we can use in a given domain, we would probably use cross-validation or holdout testing to select an algorithm and ensure that it is working satisfactorily, but then train our model on the entire data set $\mathcal{D}$ , thereby making use of the maximum amount of available data to learn a single model. 

Selecting a Learning Procedure One common use for these evaluation procedures is as a mech- anism for selecting a learning algorithm that is likely to perform well on a particular application. That is, we often want to choose among a (possibly large) set of diferent options for our learning procedure: diferent learning algorithms, or diferent algorithmic parameters for the same algo- rithm (for example, diferent constraints on the complexity of the learned network structures). At ﬁrst glance, it is straightforward to use holdout testing or cross-validation for this purpose: we take each option LearnProc $j$ , evaluate its performance, and select the algorithm whose estimated loss is smallest. While this use is legitimate, it is also tempting to use the performance estimate that we obtained using this procedure as a measure for how well our algorithm will generalize to unseen data. This use is likely to lead to misleading and overly optimistic estimates of performance, since we have selected our particular learning procedure to optimize for this particular performance metric. Thus, if we use cross-validation or a holdout set to select a learning procedure, and we want to have an unbiased estimate of how well our selected procedure will perform on unseen data, we must hold back a completely separate test set that is never used in selecting any aspect of the 

Procedure Evaluate ( $\mathcal{M}$ , // parameters to evaluate $\mathcal{D}$ // test data set ) 1 $\begin{array}{r l}&{l o s s\leftarrow0}\\ &{\mathbf{for}\ m=1,.\,.\,.\,,M}\\ &{\quad l o s s\leftarrow\ l o s s+l o s s(\xi[m]\ :\ \mathcal{M})}\\ &{\mathbf{return}\ \frac{l o s s}{M}}\end{array}$ 2 3 4 Procedure Train-And-Test ( LearnProc , $//$ Learning procedure $\mathcal{D}_{\mathrm{train}}$ , $//$ Training data $\mathcal{D}_{\mathrm{test}}$ , // Test data ) 1 $\mathcal{M}\gets\mathrm{Lewrightsquigarrow}(\mathcal{D}_{\mathrm{train}})$ 2 return Evaluate $(\mathcal{M},\mathcal{D}_{\mathrm{test}})$ Procedure Holdout-Test ( LearnProc , $//$ Learning procedure $\mathcal{D}$ , $//$ Data Set $p$ test $//$ Fraction of data for testing ) 1 $\mathcal{D}$ 2 $M_{\mathrm{train}}\leftarrow\mathrm{\rounded{(M\cdot(1-p_{\mathrm{test}}))}}$ 3 $\mathcal{D}_{\mathrm{train}}\leftarrow\ \{\xi[1],\dots,\xi[M_{\mathrm{train}}]\}$ 4 $\mathcal{D}_{\mathrm{test}}\gets\{\xi[M_{\mathrm{train}}+1],.\,.\,,\xi[M]\}$ D ←{ 5 return Train-And-Test ( LearnProc , $\mathcal{D}_{\mathrm{train}},\mathcal{D}_{\mathrm{test}})$ D D Procedure Cross-Validation ( LearnProc , $//$ Learning procedure $\mathcal{D}$ $//$ Data Set $K$ , $//$ number of cross-validation folds ) 1 Randoml eshu instances in $\mathcal{D}$ 2 Partition D into K disjoint data sets $\mathcal{D}_{1},\dots,\mathcal{D}_{K}$ 3 $l o s s\gets\mathrm{~0~}$ 4 for $k=1,\cdot\cdot\cdot,K$ 5 $\mathcal{D}_{-k}\leftarrow\ \mathcal{D}-\mathcal{D}_{k}$ − 6 loss ← loss + Train-And-Test ( LearnProc , D , D ) − k k 7 return loss K 

validation set model, on which our model’s ﬁnal performance will be evaluated. In this setting, we might have: a training set, using which we learn the model; a validation set , which we use to evaluate diferent variants of our learning procedure and select among them; and a separate test set, on which our ﬁnal performance is actually evaluated. This approach, of course, only exacerbates the problem of fragmenting our training data, and so one can develop nested cross-validation schemes that achieve the same goal. 

Goodness of Fit Cross-validation and holdout tests allow us to evaluate performance of diferent learning procedures on unseen data. However, without a “gold standard” for comparison, they do not allow us to evaluate whether our learned model really captures everything there is to capture about the distribution. This question is inherently harder to answer. In statistics, methods for answering such questions fall under the category of goodness of ﬁt tests. The general idea is the following. After learning the parameters, we have a hypothesis about a distribution that generated the data. Now we can ask whether the data behave as though they were sampled from this distribution. To do this, we compare properties of the training data set to properties of simulated data sets of the same size that we generate according to the learned distribution. If the training data behave in a manner that deviates signiﬁcantly from what we observed in the majority of the simulations, we have reason to believe that the data were not generated from the learned distribution. 

More precisely, we consider some pr erty $f$ of data sets, and ev ate $f(\mathcal{D}_{t r a i n})$ for the training set. We then generate new data sets D from our learned model M and evaluate $f(\mathcal{D})$ for these randomly generated data sets. If $f(\mathcal{D}_{t r a i n})$ deviates signiﬁcantly from the distribution of the values $f(\mathcal{D})$ among our rand ly sampled data sets, we would probably reject the hypothesis that $\mathcal{D}_{t r a i n}$ was generated from M . Of course, there are many choices regarding which properties $f$ we should evaluate. One natural choice is to deﬁne $f$ as the empirical log-loss in the data set, $E_{\mathcal{D}}[l o s s(\xi\ :\ \mathcal{M})]$ D M , as per equation (16.1). We can then ask whether t empirical log-l for $\mathcal{D}_{t r a i n}$ difers signiﬁcantly from the expected empirical log-loss for data D sampled from M . Note that the expected value of this last expression is simply the entropy of M , and, as we saw in section 8.4, we can compute the entropy of a Bayesian network fairly efciently. To check for signiﬁcance, we also need to consider the tail distribution of the log-loss, which is more involved. However, we can approximate that computation by computing the variance of the log-loss as a function of $M$ . Alternatively, because generating samples from a Bayesian network is relatively inexpensive (as in section 12.1.1), we might ﬁnd it easier to generate a large number of data sets $\mathcal{D}$ of size $M$ sampled from the model and use those to estimate the distribution over $E_{\mathcal{D}}[l o s s(\xi\ :\ \mathcal{M})]$ . 

Box 16.B — Concept: PAC-bounds. As we discussed, given a target loss function, we can esti- mate the empirical risk on our training set $\mathcal{D}_{t r a i n}$ . However, because of possible overﬁtting to the training data, the performance of our learned model on the training set might not be representative of its performance on unseen data. One might hope, however, that these two quantities are related, so that a model that achieves low training loss also achieves low expected loss (risk). 

Before we tackle a proof of this type, however, we must realize that we cannot guarantee with tainty the quality of our learned model. Recall that the data set $\mathcal{D}$ is sampled stochastically from $P^{*}$ , so there is always a chance that we would have “bad luck” and sample a very unrepresentative data set from $P^{*}$ . For example, we might sample a data set where we get the same joint assignment in all of the instances. It is clear that we cannot expect to learn useful parameters from such a data set (assuming, of course, that $P^{*}$ is not degenerate). The probability of getting such a data set is very low, but it is not zero. Thus, our analysis must allow for the chance that our data set will be highly unrepresentative, in which case our learned model (which presumably performed well on the training set) may not perform well on expectation. 

probably approximately correct 

Our goal is then to prove that our learning procedure is probably approximately correct : that is, for most training sets $\mathcal{D}$ , the learning procedure will return a model whose error is low. Making this discussion concrete, assume we use relative entropy to the true distribution as our loss function. Let $P_{M}^{*}$ be the distribution ver data sets $\mathcal{D}$ of size $M$ sampled IID from $P^{*}$ . Now, assume that we have a learning procedure L that, given a data set D , returns a model $\mathcal{M}_{L(\mathcal{D})}$ . We want to prove results of the form: 

Let $\epsilon>0$ be our approximation parameter and $\delta>0$ our conﬁdence parameter . Then, for $M$ “large enough,” we have that 

$$
P_{M}^{*}\big(\{\mathcal{D}:D(P^{*}\|P_{\mathcal{M}_{L(\mathcal{D})}})\le\epsilon\}\big)\ge1-\delta.
$$ 

sample complexity PAC-bound 

excess risk That is, for sufciently large $M$ e have that, for most data sets $\mathcal{D}$ of si $M$ sampled from $P^{*}$ , the rning procedure, applied to D , will learn a close approximation to P $P^{*}$ . The number of samples $M$ required to achieve such a bound is called the sample complexity . This type of result is called $a$ PAC-bound . 

This type of bound can only be obtained if the hypothesis space contains a model that can correctly represent $P^{*}$ . In many cases, however, we are learning with a hypothesis space that is not guaranteed to be able to express $P^{*}$ . In this case, we cannot expect to learn a model whose relative entropy to $P^{*}$ is guaranteed to be low. In such a setting, the best we can hope for is to get a model whose error is at most $\epsilon$ worse than the lowest error found within our hypothesis space. The expected loss beyond the minimal possible error is called the excess risk . See section 17.6.2.2 for one example of $^a$ generalization bound for this case. 

# 16.3.2 Discriminative versus Generative Training 

generative training 

discriminative training 

In the previous discussion, we implicitly assumed that our goal is to get the learned model $\tilde{\mathcal{M}}$ M to be a good approximation to $P^{*}$ . However, as we discussed in section 16.2.2, we often know in advance that we want the model to perform well on a particular task, such as predicting $Y$ from $X$ . The training regime that we described would aim to get M close to the overall joint distribution $P^{*}(Y,X)$ . This type of objective is known as generative training , because we are training the model to generate all of the variables, both the ones that we care to predict and the features that we use for the prediction. Alternatively, we can train the model disc rim i natively , where our goal is to get ${\tilde{P}}({\bar{Y}}\mid X)$ | to be close to $P^{*}(Y\mid X)$ . The same model class can be trained in these two diferent ways, producing diferent results. 

naive Markov As the simplest example, consider a simple “star” Markov network structure with a single target variable $Y$ connected by edges to each of a set of features $X_{1},\dots,X_{n}$ . If we train the model generatively, we are learning a naive Markov model, which, because the network is singly connected, is equivalent to a naive Bayes model. On the other hand, we can train the same network structure disc rim i natively, to obtain a good ﬁt to $P^{*}(Y\mid X_{1},.\,.\,.\,,X_{n})$ . In this c , as we showed in example 4.20, we are learning a model that is a logistic regression model for $Y$ given its features. 

Note that a model that is trained generatively can still be used for speciﬁc prediction tasks. For example, we often train a naive Bayes model generatively but use it for classiﬁcation. However, a mod that is trained for a particular prediction task $P(Y\mid X)$ does not encode a distribution over X , and hence it cannot be used to reach any conclusions about these variables. 

conditional random ﬁeld 

Discriminative training can be used for any class of models. However, its application in the context of Bayesian networks is less appealing, since this form of training changes the inter- pretation of the parameters in the learned model. For example, if we disc rim i natively train a (directed) naive Bayes model, as in example 16.1, the resulting model would essentially represent the same logistic regression as before, except that the pairwise potentials between $Y$ and each $X_{i}$ would be locally normalized to look like a CPD. Moreover, most of the computational prop- erties that facilitate Bayesian network learning do not carry through to discriminative training. For this reason, discriminative training is usually performed in the context of undirected models. In this setting, we are essentially training a conditional random ﬁeld (CRF), as in section 4.6.1: a model that directly encodes a conditional distribution $P(Y\mid X)$ . 

There are various trade-ofs between generative and discriminative training, both statistical and computational, and the question of which to use has been the topic of heated debates. We now brieﬂy enumerate some of these trade-ofs. 

bias 

Generally speaking, generative models have a higher bias — they make more assumptions about the form of the distribution. First, they encode independence assumptions about the feature variables $X$ , whereas discriminative models make independence assumptions only about $Y$ and about their dependence on $X$ . An alternative intuition arises from the following view. A generative model deﬁnes $\tilde{P}(Y,X)$ , and thereby also induces ${\tilde{P}}(Y\mid X)$ | and ${\tilde{P}}(X)$ , using the same overall model for both. To obtain a good ﬁt to $P^{*}$ , we must therefore tune our model to get good ﬁts to both $P^{*}(Y\mid X)$ and $P^{*}(X)$ . Conversely, a discriminative model aims to get a good ﬁt only to $P^{*}(Y\mid X)$ , without constraining the same model to provide a good ﬁt to $P^{*}(X)$ as well. 

The additional bias in the setting ofers a standard trade-of. On one hand, it can help regularize and constrain the learned model, thereby reducing its ability to overﬁt the data. Therefore, generative training often works better when we are learning from limited amounts of data. However, imposing constraints can hurt us when the constraints are wrong, by preventing us from learning the correct model. In practice, the class of models we use always imposes some constraints that do not hold in the true generating distribution $P^{*}$ . For limited amounts of data, the constraints might still help reduce overﬁtting, giving rise to better generalization. However, as the amount of data grows, the bias imposed by the constraints starts to dominate the error of our learned model. Because discriminative models make fewer assumptions, they will tend to be less afected by incorrect model assumptions and will often outperform the generatively trained models for larger data sets. 

# Example 16.2 

Consider the problem of optical character recognition — identifying letters from handwritten im- ages. Here, the target variable $Y$ is the character label (for example, “A”). Most obviously, we can use the individual pixels as our feature variables $X_{1},\dots,X_{n}$ . We can then either generatively train a naive Markov model or disc rim i natively train a logistic regression model. The naive Bayes (or Markov) model separately learns the distribution over the 256 pixel values given each of the 26 labels; each of these is estimated independently, giving rise to a set of fairly low-dimensional estima- tion problems. Conversely, the discriminative model is jointly optimizing all of the approximately $26\times256$ parameters of the multinomial logit distribution, a much higher-dimensional estimation problem. Thus, for sparse data, the naive Bayes model may often perform better. 

However, even in this simple setting, the independence assumption made by the naive Bayes model — that pixels are independent given the image label — is clearly false. As a consequence, the naive Bayes model may be counting, as independent, features that are actually correlated, leading to errors in the estimation. The discriminative model is not making these assumptions; by ﬁtting the parameters jointly, it can compensate for redundancy and other correlations between the features. Thus, as we get enough data to ﬁt the logistic model reasonably well, we would expect it to perform better. 

A related beneﬁt of discriminative models is that they are able to make use a much richer feature set, where independence assumptions are clearly violated. These richer features can often greatly improve classiﬁcation accuracy. 

# Example 16.3 

# 

Continuing our example, the raw pixels are fairly poor features to use for the image classiﬁcation task. Much work has been spent by researchers in computer vision and image processing in devel- oping richer feature sets, such as the direction of the edge at a given image pixel, the value of a certain ﬁlter applied to an image patch centered at the pixel, and many other features that are even more reﬁned. In general, we would expect to be able to classify images much better using these features than using the raw pixels directly. However, each of these features depends on the values of multiple pixels, and the same pixels are used in computing the values of many diferent features. Therefore, these features are certainly not independent, and using them in the context of a naive Bayes classiﬁer is likely to lead to fairly poor answers. However, there is no reason not to include such correlated features within a logistic regression or other discriminative classiﬁer. 

Conversely, generative models have their own advantages. They often ofer a more natural interpretation of a domain. And they are better able to deal with missing values and unlabeled data. Thus, the appropriate choice of model is application dependent, and often a combination of diferent training regimes may be the best choice. 

16.4 Learning Tasks 

We now discuss in greater detail the diferent variants of the learning task. As we brieﬂy mentioned, the input of a learning procedure is: 

• Some prior knowledge or constraints about $\tilde{\mathcal{M}}$ M . • A set $\mathcal{D}$ of data instances $\{d[1],\,.\,.\,,d[M]\}$ , which are independent and identically dis- tributed (IID) samples from P $P^{*}$ . 

The output is a model $\tilde{\mathcal{M}}$ M , which may include the structure, the parameters, or both. 

There are many variants of this fairly abstract learning problem; roughly speaking, they vary along three axes, representing the two types of input and the type of output. First, and most obviously, the problem formulation depends on our output — the type of graphical model we are trying to learn — a Bayesian network or a Markov network. The other two axes summarize the input of the learning procedure. The ﬁrst of these two characterizes the extent of the constraints that we are given about M , and the second characterizes the extent to which the data in our training set are fully observed. We now discuss each of these in turn. We then present a taxonomy of the diferent tasks that are deﬁned by these axes, and we review some of their computational implications. 

# 16.4.1 Model Constraints 

hypothesis space The ﬁrst question is the extent to which our input constrains the hypothesis space — the class of models that we are allowed to consider as possible outputs of our learning algorithm. There is an almost unbounded set of options here, since we can place various constraints on the structure or on the parameters of the model. Some of the key points along the spectrum are: 

• At one extreme, we may be given a graph structure, and we have to learn only (some of) the parameters; note that we generally do not assume that the given structure is necessarily the correct one $\mathcal{K}^{*}$ .

 • We may not know the structure, and we have to learn both parameters and structure from the data.

 • Even worse, we may not even know the complete set of variables over which the distribution $P^{*}$ is deﬁned. In other words, we may only observe some subset of the variables in the domain and possibly be unaware of others. 

The less prior knowledge we are given, the larger the hypothesis space, and the more pos- sibilities we need to consider when selecting a model. As we discussed in section 16.3.1, the complexity of the hypothesis space deﬁnes several important trade-ofs. The ﬁrst is statistical. If we restrict the hypothesis space too much, it may be unable to represent $P^{*}$ adequately. Conversely, if we leave it too ﬂexible, our chances increase of ﬁnding a model within the hy- pothesis space that accidentally has high score but is a poor ﬁt to $P^{*}$ . The second trade-of is computational: in many cases (although not always), the richer the hypothesis space, the more difcult the search to ﬁnd a high-scoring model. 

# 16.4.2 Data Observability 

data observability 

complete data 

incomplete data 

Along the second input axis, the problem depends on the extent of the observability of our training set. Here, there are several options: 

• The data are complete , or fully observed , so that each of our training instances $d[m]$ is a full instantiation to all of the variables in $\mathcal{X}^{*}$ .

 • The data are incomplete , or partially observed , so that, in each training instance, some variables are not observed. 

hidden variable • The data contain hidden variables whose value is never observed in any training instance. This option is the only one compatible with the case where the set of variables $\mathcal{X}^{*}$ is unknown, but it may also arise if we know of the existence of a hidden variable but never have the opportunity to observe it directly. 

As we move along this axis, more and more aspects of our data are unobserved. When data are unobserved, we must hypothesize possible values for these variables. The greater the extent to which data are missing, the less we are able to hypothesize reliably values for the missing entries. 

Dealing with partially observed data is critical in many settings. First, in many settings, observing the values of all variables can be difcult or even impossible. For example, in the case of patient records, we may not perform all tests on all patients, and therefore some of the variables may be unobserved in some records. Other variables, such as the disease the patient had, may never be observed with certainty. The ability to deal with partially observed data cases is also crucial to adapting a Bayesian network using data cases obtained after the network is operational. In such situations, the training instances are the ones provided to the network as queries, and as such, are never fully observed (at least when presented as a query). 

A particularly difcult case of missing data occurs when we have hidden variables. Why should we worry about learning such variables? For the task of knowledge discovery, these variables may play an important role in the model, and therefore they may be critical for our understanding of the domain. For example, in medical settings, the genetic susceptibility of a patient to a particular disease might be an important variable. This might be true even if we do not know what the genetic cause is, and thus cannot observe it. As another example, the tendency to be an “impulse shopper” can be an important hidden variable in an application to supermarket data mining. In these cases, our domain expert can ﬁnd it convenient to specify a model that contains these variables, even if we never expect to observe their values directly. 

mixture distribution 

In other cases, we might care about the hidden variable even when it has no predeﬁned semantic meaning. Consider, for example, a naive Bayes model, such as the one shown in ﬁgure 3.2, but where we assume that the $X_{i}$ ’s are observed but the class variable $C$ is hidden. In this model, we have a mixture distribution : Each value of the hidden variable represents a separate distribution over the $X_{i}$ ’s, where each such mixture component distribution is “simple” — all of the $X_{i}$ ’s are independent in each of the mixture components. Thus, the population is composed of some number of separate subpopulations, each of which is generated by a distinct distribution. If we could learn this model, we could recover the distinct subpopulations, that is, ﬁgure out what types of individuals we have in our population. This type of analysis is very useful from the perspective of knowledge discovery. 

Finally, we note that the inclusion of a hidden variable in the network can greatly simplify the structure, reducing the complexity of the network that needs to be learned. Even a sparse model over some set of variables can induce a large number of dependencies over a subset of its variables; for example, returning to the earlier naive Bayes example, if the class variable $C$ is hidden and therefore is not included in the model, the distribution over the variables $X_{1},\dots,X_{n}$ has no independencies and requires a fully connected graph to be represented correctly. Figure 16.1 shows another example. (This ﬁgure illustrates another visual convention that will accompany us throughout this part of the book: Variables whose values are always hidden are shown as white ovals.) Thus, in many cases, ignoring the hidden variable 

![](images/1a300498b7f03271560b8e8e2a983ff529ae42cb2e82553215f9cc5b39993890.jpg) 
Figure 16.1 The efect of ignoring hidden variables. The model on the right is an I-map for the distribution represented by the model on the left, where the hidden variable is marginalized out. The counts indicate the number of independent parameters, under the assumption that the variables are binary-valued. The variable $H$ is hidden and hence is shown as a white oval. 

leads to a signiﬁcant increase in the complexity of the “true” model (the one that best ﬁts $P^{*}$ ), making it harder to estimate robustly. Conversely, learning a model that usefully incorporates a hidden variable is far from trivial. Thus, the decision of whether to incorporate a hidden variable is far from trivial, and it requires a careful evaluation of the trade-ofs. 

# 16.4.3 Taxonomy of Learning Tasks 

Based on these three axes, we can provide a taxonomy of diferent learning tasks and discuss some of the computational issues they raise. 

The problem of parameter estimation for a known structure is one of numerical optimization. Although straightforward in principle, this task is an important one, both because numbers are difcult to elicit from people and because parameter estimation forms the basis for the more advanced learning scenarios. 

In the case of Bayesian networks, when the data are complete, the parameter estimation problem is generally easily solved, and it often even admits a closed-form solution. Unfortunately, this very convenient property does not hold for Markov networks. Here, the global partition function induces entanglement of the parameters, so that the dependence of the distribution on any single parameter is not straightforward. Nevertheless, for the case of a ﬁxed structure and complete data, the optimization problem is convex and can be solved optimally using simple iterated numerical optimization algorithms. Unfortunately, each step of the optimization algorithm requires inference over the network, which can be expensive for large models. 

When the structure is not given, the learning task now incorporates an additional level of complexity: the fact that our hypothesis space now contains an enormous (generally superexpo- nentially large) set of possible structures. In most cases, as we will see, the problem of structure selection is also formulated as an optimization problem, where diferent network structures are given a score, and we aim to ﬁnd the network whose score is highest. In the case of Bayesian networks, the same property that allowed a closed-form solution for the parameters also allows the score for a candidate network to be computed in closed form. In the case of Markov network, most natural scores for a network structure cannot be computed in closed form because of the partition function. However, we can deﬁne a convex optimization problem that jointly searches over parameter and structure, allowing for a single global optimum. 

The problem of dealing with incomplete data is much more signiﬁcant. Here, the multiple hypotheses regarding the values of the unobserved variables give rise to a combinatorial range of diferent alternative models, and induce a nonconvex, multimodal optimization problem even in parameter space. The known algorithms generally work by iteratively using the current parameters to ﬁll in values for the missing data, and then using the completion to reestimate the model parameters. This process requires multiple calls to inference as a subroutine, making this process expensive for large networks. The case where the structure is not known is even harder, since we need to combine a discrete search over network structure with nonconvex optimization over parameter space. 

# 16.5 Relevant Literature 

Most of the topics reviewed here are discussed in greater technical depth in subsequent chapters, and so we defer the bibliographic references to the appropriate places. Hastie, Tibshirani, and Friedman (2001) and Bishop (2006) provide an excellent overview of basic concepts in machine learning, many of which are relevant to the discussion in this book. 

# 17 Parameter Estimation 

In this chapter, we discuss the problem of estimating parameters for a Bayesian network. We assume that the network structure ata set $\mathcal{D}$ consists of fully observed instances of the network variables: D ${\mathcal D}=\{\xi[1],\dots,\xi[M]\}$ { } . This problem arises fairly often in practice, since numerical parameters are harder to elicit from human experts than structure is. It also plays a key role as a building block for both structure learning and learning from incomplete data. As we will see, despite the apparent simplicity of our task deﬁnition, there is surprisingly much to say about it. 

As we will see, there are two main approaches to dealing with the parameter-estimation task: one based on maximum likelihood estimation , and the other using Bayesian approaches. For each of these approaches, we ﬁrst discuss the general principles, demonstrating their application in the simplest context: a Bayesian network with a single random variable. We then show how the structure of the distribution allows the techniques developed in this very simple case to generalize to arbitrary network structures. Finally, we show how to deal with parameter estimation in the context of structured CPDs. 

# 17.1 Maximum Likelihood Estimation 

In this section, we describe the basic principles behind maximum likelihood estimation. 

# 17.1.1 The Thumbtack Example 

We start with what may be considered the simplest learning problem: parameter learning for a single variable. This is a classical Statistics 101 problem that illustrates some of the issues that we will encounter in more complex learning problems. Surprisingly, this simple problem already contains some interesting issues that we need to tackle. 

Imagine that we have a thumbtack, and we conduct an experiment whereby we ﬂip the thumbtack in the air. It comes to land as either heads or tails, as in ﬁgure 17.1. We toss the thumbtack several times, obtaining a data set consisting of heads or tails outcomes. Based on this data set, we want to estimate the probability with which the next ﬂip will land heads or tails. In this description, we already made the implicit assumption that the thumbtack tosses are controlled by an (unknown) parameter $\theta$ , which describes the frequency of heads in thumbtack tosses. In addition, we also assume that the data instances are independent and identically distributed (IID). 

![](images/43fa5e6eec7dc95f2f76ad8b3dbff48176d3ce91820490ee91b4197f61a9294b.jpg) 
Figure 17.1 A simple thumbtack tossing experiment 

![](images/d8031cbca78bfc778d7783c1420b3fe9cd220ee02347d62c662db5cdf371930b.jpg) 
Figure 17.2 The likelihood function for the sequence of tosses $H,T,T,H,H$ 

Assume that we toss the thumbtack 100 times, of which 35 come up heads. What is our estimate for $\theta\colon$ Our intuition suggests that the best estimate is 0 . 35 . Had $\theta$ been 0 . 1 , for example, our chances of seeing $35/100$ heads would have been much lower. In fact, we examined a similar situation in our discussion of sampling methods in section 12.1, where we used samples from a distribution to estimate the probability of a query. As we discussed, the central limit theorem shows that, as the number of coin tosses grows, it is increasingly unlikely to sample a sequence of IID thumbtack ﬂips where the fraction of tosses that come out heads is very far from $\theta$ . Thus, for sufciently large $M$ , the fraction of heads among the tosses is a good estimate with high probability. 

hypothesis space objective function 

To formalize this intuition, assume that we have a set of thumbtack tosses $x[1],\cdot\cdot\cdot,x[M]$ that are IID, that is, each is sampled independently from the same distribution in which $X[m]$ is equal to $H$ (heads) or $T$ (tails) th probability $\theta$ and $1-\theta$ , respectively. Our task is to ﬁnd a good value for the parameter θ . As in many formulations of learning tasks, we deﬁne a hypothesis space $\Theta$ — a set of possibilities that we are considering — and an objective function that tells us how good dif nt hypotheses in the space are relative to our data set $\mathcal{D}$ . In this case, our hypothesis space Θ is the set of all param ers $\theta\in[0,1]$ . 

How do we score diferent possible parameters θ ? As we discussed in section 16.3.1, one way of evaluating $\theta$ is by how well it predicts the data. In other words, if the data are likely given the parameter, the parameter is a good predictor. For example, suppose we observe the sequence of outcomes $H,T,T,H,H$ . If we know $\theta$ , we could assign a probability to observing this particular sequence. The probability of the ﬁrst toss is $P(X[1]=H)=\theta$ . The probability of the second toss is $P(X[2]=T\mid X[1]=H)$ ) , but our assum es are independent allows us to conclude that this probability is simply $P(X[2]=T)=1-\theta$ − . This is also the probability of the third outcome, and so on. Thus, the probability of the sequence is 

$$
P(\langle H,T,T,H,H\rangle:\theta)=\theta(1-\theta)(1-\theta)\theta\theta=\theta^{3}(1-\theta)^{2}.
$$ 

likelihood function 

As expected, this probability depends on the particular value $\theta$ . As we consider diferent values of $\theta$ , we get diferent probabilities for the sequence. Thus, we can examine how the probability of the data changes as a function of $\theta$ . We thus deﬁne the likelihood function to be 

$$
L(\theta:\langle H,T,T,H,H\rangle)=P(\langle H,T,T,H,H\rangle:\theta)=\theta^{3}(1-\theta)^{2}.
$$ 

Figure 17.2 plots the likelihood function in our example. 

maximum likelihood estimator 

log-likelihood 

Clearly, parameter values with higher likelihood are more likely to generate the observed sequences. Thus, we can use the likelihood function as our measure of quality for diferent parameter values and select the parameter value that maximizes the likelihood; this value is called the maximum likelihood estimator (MLE) . By viewing ﬁgure 17.2 we see that $\hat{\theta}=0.6=3/5$ maximizes the likelihood for the sequence $H,T,T,H,H$ . 

Can we ﬁnd the MLE for the general case? Assume that r data set $\mathcal{D}$ of observations contains $M[1]$ heads and $M[0]$ tails. We want to ﬁnd the value $\hat{\theta}$ that maximizes the likelihood of $\theta$ relative to $\mathcal{D}$ . The likelihood function in this case is: 

$$
L(\theta:{\mathcal{D}})=\theta^{M[1]}(1-\theta)^{M[0]}.
$$ 

It turns out that it is easier to maximize the logarithm of the likelihood function. In our case, the log-likelihood function is: 

$$
\ell(\theta:{\mathcal{D}})=M[1]\log\theta+M[0]\log(1-\theta).
$$ 

Note that the log-likelihood is monotonically related to the likelihood. Therefore, maximizing the one is equivalent to maximizing the other. However, the log-likelihood is more convenient to work with, since products are converted to summations. 

Diferentiating the log-likelihood, setting the derivative to 0 , and solving for $\theta$ , we get that the maximum likelihood parameter, which we denote $\hat{\theta}$ , is 

$$
\hat{\theta}=\frac{M[1]}{M[1]+M[0]},
$$ 

as expected (see exercise 17.1). 

conﬁdence interval 

As we will see, the maximum likelihood approach has many advantages. However, the approach also has some limitations. For example, if we get 3 heads out of 10 tosses, the MLE estimate is 0 . 3 . We get the same estimate if we get 300 heads out of 1,000 tosses. Clearly, the two experiments are not equivalent. Our intuition is that, in the second experiment, we should be more conﬁdent of our estimate. Indeed, statistical estimation theory deals with conﬁdence intervals . These are common in news reports, for example, when describing the results of election polls, where we often hear that $"61\pm2$ percent” plan to vote for a certain candidate. The 2 percent is a conﬁdence interval — the poll is designed to select enough people so that the MLE estimate will be within 0 . 02 of the true parameter, with high probability. 

# 17.1.2 The Maximum Likelihood Principle 

We now generalize the discussion of maximum likelihood estimation to a broader range of learning problems. We then consider how to apply it to the task of learning the parameters of a Bayesian network. 

training set 

parametric model 

parameters 

parameter space 

We start by describing the setting of the learning problem. Assume that we observe several IID samples of a set of random variables $\mathcal{X}$ from an unknown distribution $P^{*}(\mathcal{X})$ . We assume we know in advance the sample space we are dealing with (that is, which random variables, and what values they can take). However, we do not make any additional assumptions about $P^{*}$ . We denote the training set of samples as $\mathcal{D}$ and assume that it consists of $M$ instances of $\mathcal{X}$ : $\xi[1],\cdot\cdot\cdot\xi[M]$ . 

Next, we need to consider what exactly we want to learn. We assume that we are given a parametric model for which we wish to estimate parameters . Formally, a parametric model (also known as a parametric family; see section 8.2) is deﬁned by a function $P(\xi:\theta)$ , speciﬁed in terms of a set of parameters . Given a particular set of parameter values $\theta$ and an instance $\xi$ of $\mathcal{X}$ , the model ss robability (or density) to $\xi$ . Of course, we require that for each choice of parameters θ , $P(\xi:\theta)$ is a legal distribution; that is, it is nonnegative and 

$$
\sum_{\xi}P(\xi:\theta)=1.
$$ 

In general, for each model, not all parameter values are legal. Thus, we need to deﬁne the parameter space $\Theta$ , which is the set of allowable parameters. 

To get some intuition, we consider concrete examples. The model we examined in section 17.1.1 has parameter space $\Theta_{t h u m b t a c k}=[0,1]$ and is deﬁned as 

$$
P_{t h u m b t a c k}(x:\theta)={\left\{\begin{array}{l l}{\theta}&{{\mathrm{if~}}x=H}\\ {1-\theta}&{{\mathrm{if~}}x=T.}\end{array}\right.}
$$ 

There are many additional examples. 

Example 17.1 multinomial 

Suppose that $X$ is a multinomial variable that can take values $x^{1},\cdot\cdot\cdot,x^{K}$ . The simplest represen- tation of a multinomial distribution is as a vector $\pmb{\theta}\in\mathbb{R}^{K}$ , such that 

$$
P_{m u l t i n o m i a l}(x:\theta)=\theta_{k}{\mathrm{~}}i f\,x=x^{k}.
$$ 

The parameter space of this model is 

$$
\Theta_{m u l t i n o m i a l}=\left\{\pmb{\theta}\in[0,1]^{K}:\sum_{i}\theta_{i}=1\right\}.
$$ 

Example 17.2 Gaussian 

Suppose that $X$ is a continuous variable that can take values in the real line. A Gaussian model for $X$ is 

$$
P_{G a u s s i a n}(x:\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}},
$$ 

where $\theta=\langle\mu,\sigma\rangle$ . The parameter space for this model is $\Theta_{G a u s s i a n}=I\!\!R\times I\!\!R^{+}$ × . That is, we allow any real value of $\mu$ and any positive real value for $\sigma$ . 

likelihood function 

The next step in maximum likelihood estimation is deﬁning the likelihood function . As we saw in our example, the likelihood function for a given choice of parameters $\theta$ is the probability (or density) the model assigns the training data: 

$$
L(\pmb\theta:\mathcal D)=\prod_{m}P(\xi[m]:\pmb\theta).
$$ 

In the thumbtack example, we saw that we can write the likelihood function using simpler terms. That is, using the counts $M[1]$ and $M[0]$ , we managed to have a compact description of the likelihood. More precisely, once we knew the values of $M[1]$ and $M[0]$ , we did not need to consider other aspects of training data (for example, the order of tosses). These are the sufcient statistics for the thumbtack learning problem. In a more general setting, a sufcient statistic is a function of the data that summarizes the relevant information for computing the likelihood. 

Deﬁnition 17.1 sufcient statistics 

A fu tion $\tau(\xi)$ from in of $\mathcal{X}$ to $I\!\!R^{\ell}$ (for some $\ell_{s}$ ) is $a$ sufcient statistic $i f,$ for any two data sets and and any $\theta\in\Theta$ , we have that 

$$
\sum_{\xi[m]\in{\mathcal D}}\tau(\xi[m])=\sum_{\xi^{\prime}[m]\in{\mathcal D}^{\prime}}\tau(\xi^{\prime}[m])\quad\Longrightarrow\quad L(\theta:{\mathcal D})=L(\theta:{\mathcal D}^{\prime}).
$$ 

We often refer to the tuple $\begin{array}{r}{\sum_{\xi[m]\in\mathcal{D}}\tau\big(\xi[m]\big)}\end{array}$ as the sufcient statistics of the data set $\mathcal{D}$ . 

# Example 17.3 

Let us reconsider the multinomial model of example 17.1. It is easy to see that a sufcient statistic for th ata set is the tuple of counts $\langle M[1],.\,.\,.\,,M[K]\rangle$ , such that $M[k]$ is number of times the value x $x^{k}$ appears in the training data. To obtain these counts by summing instance-level statistics, we deﬁne $\tau(x)$ to be a tuple of dimension $K$ , such that $\tau(x)$ has $a\;0$ in every position, except at the position $k$ for which $x=x^{k}$ , where its value is $\mathit{l.}$ : 

$$
\tau(x^{k})=(\overbrace{0,.\.\,.\,0}^{k-1},1,\overbrace{0,.\,.\,.\,0}^{n-k}).
$$ 

Given the vector of counts, we can write the likelihood function as 

$$
L(\pmb\theta:\mathcal{D})=\prod_{k}\theta_{k}^{M[k]}.
$$ 

Example 17.4 Let us reconsider the Gaussian model of example 17.2. In this case, it is less obvious how to construct sufcient statistics. However, if we expand the term $(x-\mu)^{2}$ in the exponent, we can rewrite the model as 

$$
P_{G a u s s i a n}(x:\mu,\sigma)=e^{-x^{2}\frac{1}{2\sigma^{2}}+x\frac{\mu}{\sigma^{2}}-\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log(2\pi)-\log(\sigma)}.
$$ 

We then see that the function 

$$
s_{G a u s s i a n}(x)=\langle1,x,x^{2}\rangle
$$ 

is a sufcient statistic for this model. Note that the ﬁrst element in the sufcient statistics tuple is “1,” which does not depend on the value of the data item; it serves, as in the multinomial case, to count the number of data items. 

We venture several comments about the likelihood function. First, we stress that the likelihood function measures the efect of the choice of parameters on the training data. Thus, for example, if we have two sets of parameters $\theta$ and $\theta^{\prime}$ , so that $L(\pmb\theta:\mathcal D)=L(\pmb\theta^{\prime}:\mathcal D)$ , then we cannot , given only the data, distinguish between the two choices of parameters. Moreover, if $L(\theta\,:$ $\mathcal{D})=L(\boldsymbol{\theta}^{\prime}:\mathcal{D})$ for all possible choices of $\mathcal{D}$ , then the two parameters are indistinguishable for any outcome. In such a situation, we can say in advance (that is, before seeing the data) that some distinctions cannot be resolved based on the data alone. 

Second, since we are maximizing the likelihood function, we usually want it to be continuous (and preferably smooth) function of $\theta$ . To ensure these properties, most of the theory of statistical estimation requires that $P(\xi:\theta)$ is a continuous and diferentiable function of $\theta$ , and moreover that $\Theta$ is a continuous set of points (which is often assumed to be convex). 

maximum likelihood estimation 

Once we have deﬁned the likelihood function, we can use maximum likelihood estimation to choose the parameter values. Formally, we state this principle as follows. 

Maximum Likelihood Estimation: Given a data set $\mathcal{D}$ , choose parameters $\hat{\pmb\theta}$ that satisfy 

$$
L({\hat{\theta}}:{\mathcal{D}})=\operatorname*{max}_{\theta\in\Theta}L(\theta:{\mathcal{D}}).
$$ 

Example 17.5 Consider estimating the parameters of the multinomial distribution of example 17.3. As one might guess, the maximum likelihood is attained when 

$$
{\hat{\theta}}_{k}={\frac{M[k]}{M}}
$$ 

(see exercise 17.2). That is, the probability of each value of $X$ corresponds to its frequency in the training data. 

Example 17.6 empirical mean, variance 

Consider estimating the parameters of a Gaussian distribution of example 17.4. It turns out that the maximum is attained when $\mu$ and $\sigma$ correspond to the empirical mean and variance of the training data: 

$$
\begin{array}{r c l}{{\hat{\mu}}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m}x[m]}}\\ {{}}&{{}}&{{}}\\ {{\hat{\sigma}}}&{{=}}&{{\displaystyle\sqrt{\frac{1}{M}\sum_{m}(x[m]-\hat{\mu})^{2}}}}\end{array}
$$ 

# 17.2 MLE for Bayesian Networks 

We now move to the more general problem of estimating parameters for a Bayesian network. It turns out that the structure of the Bayesian network allows us to reduce the parameter estimation problem to a set of unrelated problems, each of which can be addressed using the techniques of the previous section. We begin by considering a simple example to clarify our intuition, and then generalize to more complicated networks. 

# 17.2.1 A Simple Example 

The simplest example of a nontrivial network structure is a network consisting of two binary variables, say $X$ and $Y$ , with an arc $X\rightarrow Y$ . (A network without such an arc trivially reduces to the cases we already discussed.) 

As for a single parameter, our goal in maximum likelihood estimation is to maximize the likelihood (or log-likelihood) function. In this case, our network is parameterized by a parameter vector $\theta$ , which deﬁnes the set of parameters for all the CPDs in the network. In this example, our parameter iz ation would consist of the following parameters: $\theta_{x^{1}}$ , and $\theta_{x^{0}}$ specify the probability of the two values of $X$ ; $\theta_{y^{1}|x^{1}}$ , and $\theta_{y^{0}\mid x^{1}}$ specify the probability of $Y$ given that $X=x^{1}$ ; and $\theta_{y^{1}|x^{0}}$ , and $\theta_{y^{0}\mid x^{0}}$ describe the probability of $Y$ given that $X=x^{0}$ . For brevity, we also use the shorthand $\theta_{Y\mid x^{0}}$ to refer to the set $\{\theta_{y^{1}|x^{0}},\theta_{y^{0}|x^{0}}\}$ , and $\theta_{Y\mid X}$ to refer to $\pmb{\theta}_{Y\mid x^{1}}\cup\pmb{\theta}_{Y\mid x^{0}}$ . 

In this exam e, ea training instance is a tuple $\langle x[m],y[m]\rangle$ that describes a particular assignment to X and Y . Our likelihood function is: 

$$
L(\pmb\theta:\mathcal D)=\prod_{m=1}^{M}P(x[m],y[m]:\pmb\theta).
$$ 

Our network model speciﬁes that $P(X,Y:\theta)$ has a product form. Thus, we can write 

$$
L(\pmb\theta:\mathcal D)=\prod_{m}P(x[m]:\pmb\theta)P(y[m]\mid x[m]:\pmb\theta).
$$ 

Exchanging the order of multiplication, we can equivalently write this term as 

$$
L(\pmb\theta:\mathcal D)=\left(\prod_{m}{P(x[m]:\pmb\theta)}\right)\left(\prod_{m}{P(y[m]\mid x[m]:\pmb\theta)}\right).
$$ 

That is, the likelihood decomposes into two separate terms, one for each variable. Moreover, each of these terms is a local likelihood function that measures how well the variable is predicted given its parents. 

Now consider the two individual terms. Clearly, each one depends only on the parameters for that variable’s CPD. Thus, the ﬁrst is $\textstyle\prod_{m}{P(x[m]\,:\,\theta_{X})}$ . This term is identical to the multinomial likelihood function we discussed earlier. The second term is more interesting, since we can decompose it even further: 

$$
\begin{array}{r l r}{\lefteqn{\prod_{m}P(y[m]\mid x[m]:\theta_{Y|X})}}\\ &{=}&{\prod_{m:x[m]=x^{0}}P(y[m]\mid x[m]:\theta_{Y|X})\cdot\prod_{m:x[m]=x^{1}}P(y[m]\mid x[m]:\theta_{Y|X})}\\ &{=}&{\prod_{m:x[m]=x^{0}}P(y[m]\mid x[m]:\theta_{Y|x^{0}})\cdot\prod_{m:x[m]=x^{1}}P(y[m]\mid x[m]:\theta_{Y|x^{1}}).}\end{array}
$$ 

Thus, in this example, the likelihood function decomposes into a product of terms, one for each group of parameters in $\theta$ . This property is called the decomposability of the likelihood function. 

We can do one more simpliﬁcation by using the notion of sufcient statistics. Let us consider one term in this expression: 

$$
\prod_{m:x[m]=x^{0}}{\cal P}(y[m]\mid x[m]:\theta_{Y\mid x^{0}}).
$$ 

Each of the individual terms $P(y[m]\mid x[m]:\theta_{Y\mid x^{0}})$ can take one of two values, depending on the value of $y[m]$ . If $y[m]=y^{1}$ , it is equal to $\theta_{y^{1}|x^{0}}$ . If $y[m]=y^{0}$ , it is equal to $\theta_{y^{0}|x^{0}}$ . How many cases of each type do we get? First, we restrict attention only to those data cases where $x[m]=x^{0}$ . These, in turn, partition into the two categories. Thus, we get $\theta_{y^{1}|x^{0}}$ in those data cases where $x[m]\,=\,x^{0}$ and $y[m]\,=\,y^{1}$ ; we use $M[x^{0},y^{1}]$ to denote their number. We get $\theta_{y^{0}|x^{0}}$ in those data cases where $x[m]=x^{0}$ and $y[m]=x^{0}$ , and use $M[x^{0},y^{0}]$ to denote their number. Thus, the term in equation (17.2) is equal to: 

$$
\prod_{m:x[m]=x^{0}}{\cal P}(y[m]\mid x[m]:\theta_{Y|x^{0}})\;\;\;=\;\;\;\theta_{y^{1}|x^{0}}^{M[x^{0},y^{1}]}\cdot\theta_{y^{0}|x^{0}}^{M[x^{0},y^{0}]}.
$$ 

Based on our discussion of the multinomial likelihood in example 17.5, we know that we maximize $\theta_{Y\mid x^{0}}$ by setting: 

$$
\theta_{y^{1}|x^{0}}=\frac{M[x^{0},y^{1}]}{M[x^{0},y^{1}]+M[x^{0},y^{0}]}=\frac{M[x^{0},y^{1}]}{M[x^{0}]},
$$ 

and similarly for $\theta_{y^{0}|x^{0}}$ . Thus, we can ﬁnd the maximum likelihood parameters in this CPD by simply counting how many times each of the possible assignments of $X$ and $Y$ appears in the training data. It turns out that these counts of the various assignments for some set of variables are useful in general. We therefore deﬁne: 

Let $Z$ be some set of random variables, and $_z$ be some instantiation to these random variables. Let $\mathcal{D}$ be a data set. We deﬁne $M[z]$ to be the number of entries in $\mathcal{D}$ that have $Z[m]=z$ 

$$
M[z]=\sum_{m}I\{Z[m]=z\}.
$$ 

# 17.2.2 Global Likelihood Decomposition 

As we can expect, the arguments we used for deriving the MLE of $\theta_{Y\mid x^{0}}$ apply for the parameters of other CPDs in that example and indeed for other networks as well. We now develop, in several steps, the formal machinery for proving such properties in Bayesian networks. 

We start by examining the likelihood function of a Bayesian network. Suppose we want to learn the parameters for a Bayesian network with structure $\mathcal{G}$ and parameters $\theta$ . This means that we agree in advance on the type of CPDs we want to learn (say table-CPDs, or noisy-ors). As we discussed, we are also given a data set $\mathcal{D}$ consisting of samples $\xi[1],\cdot\cdot\cdot,\xi[M]$ . Writing the likelihood, and repeating the steps we performed in our example, we get 

$$
\begin{array}{r c l}{{{\cal L}(\pmb\theta:{\cal D})}}&{{=}}&{{\displaystyle\prod_{m}{\cal P}_{\mathcal G}(\xi[m]:\pmb\theta)}}\\ {{}}&{{=}}&{{\displaystyle\prod_{m}\prod_{i}{\cal P}(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m]:\pmb\theta)}}\\ {{}}&{{=}}&{{\displaystyle\prod_{i}\left[\displaystyle\prod_{m}{\cal P}(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m]:\pmb\theta)\right].}}\end{array}
$$ 

conditional likelihood 

local likelihood 

Note that each of the terms in the square brackets refers to the conditional likelihood of a particular variable given its parents in the network. We use $\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ to denote the subset of parameters that determines $P(X_{i}\mid\mathrm{Pa}_{X_{i}})$ in our model. Then, we can write 

$$
L(\pmb\theta:\mathcal{D})=\prod_{i}L_{i}(\pmb\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}:\mathcal{D}),
$$ 

where the local likelihood function for $X_{i}$ is: 

$$
L_{i}(\pmb{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}}:\mathcal{D})=\prod_{m}{P}(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m]:\pmb{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}}).
$$ 

This form is particularly useful when the parameter sets $\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ are disjoint . That is, each CPD is parameterized by a separate set of parameters that do not overlap. This assumption is quite natural in all our examples so far. (Although, as we will see in section 17.5, parameter sharing can be handy in many domains.) This analysis shows that the likelihood decomposes as a product of independent terms, one for each CPD in the network. This important property is called the global decomposition of the likelihood function. We can now immediately derive the following result: 

# 

global decomposability 

Let $\mathcal{D}$ be a complete data set for $X_{1},\dots,X_{n},$ , let $\mathcal{G}$ be a network structure hese variables, and suppose that the parameters $\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ are disjoint from $\theta_{X_{j}|\mathrm{Pa}_{X_{j}}}$ for all j $j\neq i$ ̸ . Let $\hat{\pmb{\theta}}_{X_{i}|\mathrm{Pa}_{X_{i}}}$ be the parameters that maximize $L_{i}(\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}:\mathcal{D})$ . Then, $\hat{\pmb\theta}=\langle\hat{\pmb\theta}_{X_{1}|\mathrm{Pa}_{1}},.\,.\,.\,,\hat{\pmb\theta}_{X_{n}|\mathrm{Pa}_{n}}\rangle$ ⟨ ⟩ maximizes | | $L(\theta:{\mathcal{D}})$ . 

Proposition 17.1 

In other words, we can maximize each local likelihood function independently of rest of the network, and then combine the solutions to get an MLE solution. This decomposition of the global problem to independent subproblems allows us to devise efcient solutions to the MLE problem. Moreover, this decomposition is an immediate consequence of the network structure and does not depend on any particular choice of parameter iz ation for the CPDs. 

# 

# 17.2.3 Table-CPDs 

Based on the preceding discussion, we know that the likelihood of a Bayesian network decom- poses into local terms that depend on the parameter iz ation of CPDs. The choice of parameters determines how we can maximize each of the local likelihood functions. We now consider what is perhaps the simplest parameter iz ation of the CPD: a table-CPD . 

table-CPD 

Suppose we have a variable $X$ with parents $U$ . If we represent that CPD $P(X\mid U)$ as a table, then we will have a parameter $\theta_{x|u}$ for each combination of $x\in V a l(X)$ and $u\in V a l(U)$ . In this case, we can rewrite the local likelihood function as follows: 

$$
\begin{array}{r c l}{{{\cal L}_{X}(\pmb{\theta}_{X|U}:\mathcal{D})}}&{{=}}&{{\displaystyle\prod_{m}\theta_{x[m]|\pmb{u}[m]}}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{\displaystyle\prod_{\pmb{u}\in V a l(\pmb{U})}\left[\displaystyle\prod_{x\in V a l(X)}\theta_{x|\pmb{u}}^{M[\pmb{u},x]}\right],}}\end{array}
$$ 

local decomposability where $M[{\boldsymbol{\mathbf{\mathit{u}}}},x]$ is the number of times $\xi[m]=x$ and ${\pmb u}[m]={\pmb u}$ in $\mathcal{D}$ . That is, we grouped together all the occurrences of $\theta_{x|u}$ in the product over all instances. This provides a further local decomposition of the likelihood function. 

We need to maximize this term under the constraints that, for each choice of value for the parents $U$ , the conditional probability is legal, that is: 

$$
\sum\theta_{x\mid u}=1\quad{\mathrm{~for~all~}}u.
$$ 

These constraints imply that the choice of value for $\theta_{x|u}$ can impact choice of value for $\theta_{x^{\prime}|u}$ . However, the choice of parameters given diferent values $\mathbfit{u}$ of U are independent of each other. Thus, we can maximize each of the terms in square brackets in equation (17.4) independently. 

We can thus further decompose the local likelihood function for a tabular CPD into a product of simple likelihood functions. Each of these likelihood functions is a multinomial likelihood, of the type that we examined in example 17.3. The counts in the data for the diferent outcomes $x$ are simply $\{M[{\pmb u},{\boldsymbol x}]:{\boldsymbol x}\in V a l({\boldsymbol X})\}$ . We can then immediately use the maximum likelihood estimation for multinomial likelihood of example 17.5 and see that the MLE parameters are 

$$
{\hat{\theta}}_{x\mid u}={\frac{M[{\pmb u},x]}{M[{\pmb u}]}},
$$ 

where we use the fact that $\begin{array}{r}{M[\pmb{u}]=\sum_{x}M[\pmb{u},x]}\end{array}$ . 

data fragmentation 

overﬁtting 

This simple formula reveals a key challenge when estimating parameters for a Bayesian net- works. Note that the number of data points used to estimate the parameter ${\hat{\theta}}_{x\mid u}$ is $M[\pmb{u}]$ . Data | points that do not agree with the parent assignment $\mathbfit{u}$ play no role in this computation. As the number of parents $U$ grows, the number of diferent parent assignments grows exponentially. Therefore, the number of data instances that we expect to have for a single parent assignment shrinks exponentially. This phenomenon is called data fragmentation , since the data set is par- titioned into a large number of small subsets. Intuitively, when we have a very small number of data instances from which we estimate a parameter, the estimates we get can be very noisy (this intuition is formalized in section 17.6), leading to overﬁtting . We are also more likely to get a large number of zeros in the distribution, which can lead to very poor performance. Our inability to estimate parameters reliably as the dimensionality of the parent set grows is one of the key limiting factors in learning Bayesian networks from data. This problem is even more severe when the variables can take on a large number of values, for example, in text applications. 

classiﬁcation 

Bayesian classiﬁer 

Box 17.A — Concept: Naive Bayes Classiﬁer. One of the basic tasks of learning is classiﬁcation . In this task, our goal is build a classiﬁer — a procedure that assigns instances into two or more categories, for example, deciding whether an email message is junk mail that should be discarded or a relevant message that should be presented to the user. In the usual setting, we are given a training example of instances from each category, where instances are represented by various features. In our email classiﬁcation example, a message might be analyzed by multiple features: its length, the type of attachments it contains, the domain of the sender, whether that sender appears in the user’s address book, whether a particular word appears in the subject, and so on. 

One general approach to this problem, which is referred to as Bayesian classiﬁer , is to learn a probability distribution of the features of instances of each class. In the language of probabilistic models, we use the random variables $X$ to represent the instance, and the random variable $C$ to represent the category of the instance. The distribution $P(X\mid C)$ is the probability of a particular combination of features given the category. Using Bayes rule, we have that 

$$
P(C\mid X)\propto P(C)P(X\mid C).
$$ 

Thus, if we have a good model of how instances of each category behave (that is, of $P(X\mid C)),$ we can combine it with our prior estimate for the frequency of each category (that is, $P(C))$ to estimate the posterior probability of each of the categories (that is, $P(C\mid X))$ . We can then decide either to predict the most likely category or to perform a more complex decision based on the strength of likelihood of each option. For example, to reduce the number of erroneously removed messages, a junk-mail ﬁlter might remove email messages only when the probability that it is junk mail is higher than a strict threshold. 

This Bayesian classiﬁcation approach is quite intuitive. Loosely speaking, it states that to classify objects successfully, we need to recognize the characteristics of objects of each category. Then, we can classify a new object by considering whether it matches the characteristic of each of the classes. More formally, we use the language of probability to describe each category, assigning higher probability to objects that are typical for the category and low probability to ones that are not. 

naive Bayes 

The main hurdle in constructing a Bayesian classiﬁer is the question of representation of the multivariate distribution $p(X\mid C)$ . The naive Bayes classiﬁer is one where we use the simplest representation we can think of. That is, we assume that each feature $X_{i}$ is independent of all the other features given the class variable $C$ . That is, 

$$
P(X\mid C)=\prod_{i}{\cal P}(X_{i}\mid C).
$$ 

Learning the distribution $P(C)P(X\mid C)$ is th s reduced to learning the parameters in the naive Bayes structure, with the category variable C rendering all other features as conditionally independent of each other. 

As can be expected, learning this classiﬁer is a straightforward application of the parameter estimation that we consider in this chapter. Moreover, classifying new examples requires simple computation, evaluating $\textstyle P(c)\prod_{i}P(x_{i}\mid c)$ for each category $c$ . 

Although this simple classiﬁer is often dismissed as naive, in practice it is often surprisingly efective. From a training perspective, this classiﬁer is quite robust, since in most applications, even with relatively few training examples, we can learn the parameters of conditional distribution $P(X_{i}\mid C)$ . However, one might argue that robust learning does not compensate for oversimpliﬁed independence assumption. Indeed, the strong independence assumption usually results in poor representation of the distribution of instances. However, errors in estimating the probability 

of an instance do not necessarily lead to classiﬁcation errors. For classiﬁcation, we are interested in the relative size of the conditional distribution of the instances given diferent categories. The ranking of diferent labels may not be that sensitive to errors in estimating the actual probability of the instance. Empirically, one often ﬁnds that the naive Bayes classiﬁer correctly classiﬁes an example to the right category, yet its posterior probability is very skewed and quite far from the correct distribution. 

In practice, the naive Bayes classiﬁer is often a good baseline classiﬁer to try before considering more complex solutions. It is easy to implement, it is robust, and it can handle diferent choices of descriptions of instances (for example, box 17.E). 

# 17.2.4 Gaussian Bayesian Networks $\star$ 

Our discussion until now has focused on learning discrete-state Bayesian networks with multi- nomial parameters. However, the concepts we have developed in this section carry through to a wide variety of other types of Bayesian networks. In particular, the global decomposition properties we proved for a Bayesian network apply, without any change, to any other type of CPD. That is, if the data are complete, the learning problem reduces to a set of local learning problems, one for each variable. The main diference is in applying the maximum likelihood estimation process to a CPD of a diferent type: how we deﬁne the sufcient statistics, and how we compute the maximum likelihood estimate from them. In this section, we demonstrate how MLE principles can be applied in the setting of linear Gaussian Bayesian networks. In section 17.2.5 we provide a general procedure for CPDs in the exponential family. 

Consider a variable $X$ with parents $U=\{U_{1},.\,.\,.\,,U_{k}\}$ with a linear Gaussian CPD: 

$$
P(X\mid\mathbf{\boldsymbol{u}})=\mathcal{N}\left(\beta_{0}+\beta_{1}u_{1}+\ldots,\beta_{k}u_{k};\sigma^{2}\right).
$$ 

Our task is to learn the parameters $\theta_{X|U}\;=\;\langle\beta_{0},.\,.\,.\,,\beta_{k},\sigma\rangle$ . To ﬁnd the MLE values of these parameters, we need to diferentiate the likelihood and solve the equations that deﬁne a stationary point. As usual, it will be easier to work with the log-likelihood function. Using the deﬁnition of the Gaussian distribution, we have that 

$$
\begin{array}{l l l}{\lefteqn{\ell_{X}\big(\pmb{\theta}_{X|U}:\mathcal{D}\big)=\log L_{X}\big(\pmb{\theta}_{X|U}:\mathcal{D}\big)}}\\ &{=}&{\sum_{m}\left[-\frac{1}{2}\log(2\pi\sigma^{2})-\frac{1}{2}\frac{1}{\sigma^{2}}\left(\beta_{0}+\beta_{1}u_{1}[m]+\ldots+\beta_{k}u_{k}[m]-x[m]\right)^{2}\right].}\end{array}
$$ 

We start by considering the gradient of the log-likelihood with respect to $\beta_{0}$ : 

$$
\begin{array}{c c}{\displaystyle\frac{\partial}{\partial\beta_{0}}\ell_{X}(\pmb{\theta}_{X|U}:\mathcal{D})=\sum_{m}-\frac{1}{\sigma^{2}}\left(\beta_{0}+\beta_{1}u_{1}[m]+\ldots+\beta_{k}u_{k}[m]-x[m]\right)}\\ {=}&{\displaystyle-\frac{1}{\sigma^{2}}\left(M\beta_{0}+\beta_{1}\sum_{m}u_{1}[m]+\ldots+\beta_{k}\sum_{m}u_{k}[m]-\sum_{m}x[m]\right).}\end{array}
$$ 

Equating this gradient to 0 , and multiplying both sides with $\frac{\sigma^{2}}{M}$ , we get the equation 

$$
{\frac{1}{M}}\sum_{m}x[m]=\beta_{0}+\beta_{1}{\frac{1}{M}}\sum_{m}u_{1}[m]+\ldots+\beta_{k}{\frac{1}{M}}\sum_{m}u_{k}[m].
$$ 

Each of the terms is the average value of one of the variables in the data. We use the notation 

$$
E_{\mathcal{D}}[X]=\frac{1}{M}\sum_{m}x[m]
$$ 

to denote this expectation. Using this notation, we see that we get the following equation: 

$$
\begin{array}{r}{{\pmb E}_{\mathcal{D}}[X]=\beta_{0}+\beta_{1}{\pmb E}_{\mathcal{D}}[U_{1}]+.\,.+\beta_{k}{\pmb E}_{\mathcal{D}}[U_{k}].}\end{array}
$$ 

Recall that theorem 7.3 speciﬁes the mean of a linear Gaussian variable $X$ in terms of the means of its parents $U_{1},\dots,U_{k}$ , using an expression that has precisely this form. Thus, equation (17.6) tells us that the MLE parameters should be such that the mean of $X$ in the data is consistent with the predicted mean of $X$ according to the parameters. 

Next, consider the gradient with respect to one of the parameters $\beta_{i}$ . Using similar arithmetic manipulations, we see that the equation $\begin{array}{r}{0=\frac{\partial}{\partial\beta_{i}}\ell_{X}(\bar{\pmb\theta_{X|U}}:\mathcal{D})}\end{array}$ can be formulated as: 

$$
\begin{array}{r}{E_{\mathcal{D}}[X\cdot U_{i}]=\beta_{0}E_{\mathcal{D}}[U_{i}]+\beta_{1}E_{\mathcal{D}}[U_{1}\cdot U_{i}]+\ldots+\beta_{k}E_{\mathcal{D}}[U_{k}\cdot U_{i}].}\end{array}
$$ 

At this stage, we have $k+1$ linear equations with $k+1$ unknowns, and we can use standard linear algebra techniques for solving for the value of $\beta_{0},\beta_{1},.\cdot\cdot,\beta_{k}$ . We can get additional intuition, however, by doing additional manipulation of equation (17.7). Recall that the covariance $\mathbf{C}o v[X;Y]=E[X\cdot Y]-E[X]\cdot E[Y]$ · − · . Thus, if we subtract $\pmb{E}_{\mathcal{D}}[X]\cdot\pmb{E}_{\mathcal{D}}[U_{i}]$ D · D from the left-hand side of equation (17.7), we would get the empirical covariance of X and $U_{i}$ . Using equation (17.6), we have that this term can also be written as: 

$$
\begin{array}{r}{\pmb{E}_{\mathcal{D}}[X]\cdot\pmb{E}_{\mathcal{D}}[U_{i}]=\beta_{0}\pmb{E}_{\mathcal{D}}[U_{i}]+\beta_{1}\pmb{E}_{\mathcal{D}}[U_{1}]\cdot\pmb{E}_{\mathcal{D}}[U_{i}]+.+.+\beta_{k}\pmb{E}_{\mathcal{D}}[U_{k}]\cdot\pmb{E}_{\mathcal{D}}[U_{i}].}\end{array}
$$ 

Subtracting this equation from equation (17.7), we get: 

$$
\begin{array}{r c l}{{{\cal E}_{\mathcal{D}}[X\cdot U_{i}]-{\cal E}_{\mathcal{D}}[X]\cdot{\pmb E}_{\mathcal{D}}[U_{i}]}}&{{=}}&{{\beta_{1}\left({\pmb E}_{\mathcal{D}}[U_{1}\cdot U_{i}]-{\pmb E}_{\mathcal{D}}[U_{1}]\cdot{\pmb E}_{\mathcal{D}}[U_{i}]\right)+\ldots+}}\\ {{}}&{{}}&{{\beta_{k}\left({\pmb E}_{\mathcal{D}}[U_{k}\cdot U_{i}]-{\pmb E}_{\mathcal{D}}[U_{k}]\cdot{\pmb E}_{\mathcal{D}}[U_{i}]\right).}}\end{array}
$$ 

Using $\mathbf{\it{C}}o v_{\mathcal{D}}[X;U_{i}]$ to denote the observed covariance of $X$ and $U_{i}$ in the data, we get: 

$$
\begin{array}{r}{\pmb{C}o v_{\mathcal{D}}[X;U_{i}]=\beta_{1}\pmb{C}o v_{\mathcal{D}}[U_{1};U_{i}]+.\,.+\beta_{k}\pmb{C}o v_{\mathcal{D}}[U_{k};U_{i}].}\end{array}
$$ 

In other words, the observed covariance of $X$ with $U_{i}$ should be the one predicted by theorem 7.3 given the parameters and the observed covariances between the parents of $X$ . 

Finally, we need to ﬁnd the value of the $\sigma^{2}$ parameter. Taking the derivative of the likelihood and equating to 0 , we get an equation that, after suitable reformulation, can be written as 

$$
\sigma^{2}={\bf C}o v_{\mathcal{D}}[X;X]-\sum_{i}\sum_{j}\beta_{i}\beta_{j}{\bf C}o v_{\mathcal{D}}[U_{i};U_{j}]
$$ 

(see exercise 17.4). Again, we see that the MLE estimate has to match the constraints implied by theorem 7.3. 

e glob picture that emerges is as follows. To estimate $P(X\mid U)$ , we estimate the means of X and U and covariance matrix of $\{X\}\cup U$ from the vector of means and covariance matrix deﬁnes a joint Gaussian distribution over { $\{X\}\cup U$ } ∪ . (In fact, this is the MLE estimate of the joint Gaussian; see exercise 17.5.) We then solve for the (unique) linear Gaussian that matches the joint Gaussian with these parameters. For this purpose, we can use the formulas provided by theorem 7.4. While these equations seem somewhat complex, they are merely describing the solution to a system of linear equations. 

This discussion also identiﬁes the sufcient statistics we need to collect to estimate linear Gaussians. These are the un the f $\textstyle\sum_{m}x[m]$ and $\textstyle\sum_{m}u_{i}[m]$ , and the interaction terms of the form $\textstyle\sum_{m}x[m]\cdot u_{i}[m]$ P · and P $\textstyle\sum_{m}u_{i}[m]\cdot u_{j}[m]$ · ] . From these, we can estimate the mean and covariance matrix of the joint distribution. 

nonparametric Bayesian estimation 

Box 17.B — Concept: Nonparametric Models. The discussion in this chapter has focused on estimating parameters for speciﬁc parametric models of CPDs: multinomials and linear Gaussians. However, a theory of maximum likelihood and Bayesian estimation exists for a wide variety of other parametric models. Moreover, in recent years, there has been a growing interest in the use of nonparametric Bayesian estimation methods, where a (conditional) distribution is not deﬁned to be in some particular parametric class with a ﬁxed number of parameters, but rather the complexity of the representation is allowed to grow as we get more data instances. In the case of discrete variables, any CPD can be described as a table, albeit perhaps a very large one; thus a nonparametric method is less essential (although see section 19.5.2.2 for a very useful example of a nonparametric method in the discrete case). In the case of continuous variables, we do not have a “universal” parametric distribution. While Gaussians are often the default, many distributions are not well ﬁt by them, and it is often difcult to determine which parametric family (if any) will be appropriate for a given variable. In such cases, nonparametric methods ofer a useful substitute. In such methods, we use the data points themselves as the basis for a probability distribution. Many nonparametric methods have been developed; we describe one simple variant that serves to illustrate this type of approach. 

Suppose we want to learn the distribution $P(X\mid U)$ from data. A reasonable assumption is that the CPD is smooth. Thus, if we observe $x,u$ in a training sample, it should increase the probability of seeing similar values of $X$ for similar values of $U$ . More precisely, we increase the density of $p(X=x+\epsilon\mid U=u+\delta)$ for small values of ϵ and $\delta$ . 

kernel density estimation 

One simple approach that captures this intuition is the use of kernel density estimation (also known as Parzen windows ). The idea is fairly simple: given the data $\mathcal{D}$ , we estimate a “local” joint density ${\tilde{p}}_{X}(X,U)$ by spreading out density around each example $x[m],\pmb{u}[m]$ . Formally, we write 

$$
\tilde{p}_{X}(\boldsymbol{x},\boldsymbol{u})=\frac{1}{M}\sum_{m}K(\boldsymbol{x},\boldsymbol{u};\boldsymbol{x}[m],\boldsymbol{u}[m],\alpha),
$$ 

where $K$ is $^a$ kernel density function and $\alpha$ is a parameter (or vector of parameters) controlling $K$ . A common choice of kernel is $^a$ simple round Gaussian distribution with radius $\alpha$ around $x[m],\pmb{u}[m]$ : 

$$
K(\boldsymbol{x},\boldsymbol{\mathbf{\mathit{u}}};\boldsymbol{x}[m],\boldsymbol{\mathbf{\mathit{u}}}[m],\boldsymbol{\mathbf{\mathit{a}}})=\mathcal{N}\left(\left(\begin{array}{c}{\boldsymbol{x}[m]}\\ {\boldsymbol{\mathbf{\mathit{u}}}[m]}\end{array}\right);\alpha^{2}I\right),
$$ 

where $I$ is the identity matrix and $\alpha$ is the width of the window. Of course, many other choices for kernel function are possible; in fact, if $K$ deﬁnes a probability measure (nonnegative and integrates to ${\mathit{l}}),$ then $\tilde{p}_{X}(x,\pmb{u})$ is also a probability measure. Usually we choose kernel functions that are local, in that they put most of the mass in the vicinity of their argument. For such kernels, the resulting density $\tilde{p}_{X}(x,\pmb{u})$ will have high mass in regions where we have seen many data instances $(x[m],\mathbf{u}[m])$ and low mass in regions where we have seen none. 

We can now reformulate this local joint distribution to produce a conditional distribution: 

$$
p(x\mid\mathbf{\boldsymbol{u}})=\frac{\sum_{m}K(x,\mathbf{\boldsymbol{u}};x[m],\mathbf{\boldsymbol{u}}[m],\alpha)}{\sum_{m}K(\mathbf{\boldsymbol{u}};\mathbf{\boldsymbol{u}}[m],\alpha)}
$$ 

where $K(\pmb{u};\pmb{u}[m],\alpha)$ is $K(x,\pmb{u};x[m],\pmb{u}[m],\alpha)$ marginalized over $x$ . 

Note that this learning procedure estimates virtually no parameters: the CPD is derived directly from the training instances. The only free parameter is $\alpha$ , which is the width of the window. Importantly, this parameter cannot be estimated using maximum likelihood: The α that maximizes the likelihood of the training set is $\alpha=0$ , which gives maximum density to the training instances themselves. This, of course, will simply memorize the training instances without any generalization. Thus, this parameter is generally selected using cross-validation. 

The learned CPD here is essentially the list of training instances, which has both advantages and disadvantages. On the positive side, the estimates are very ﬂexible and tailor themselves to the observations; indeed, as we get more training data, we can produce arbitrarily expressive representations of our joint density. On the negative side, there is no “compression” of the original data, which has both computational and statistical ramiﬁcations. Computationally, when there are many training samples the learned CPDs can become unwieldy. Statistically, this learning procedure makes no attempt to generalize beyond the data instances that we have seen. In high-dimensional spaces with limited data, most points in the space will be “far” from data instances, and therefore the estimated density will tend to be quite poor in most parts of the space. Thus, this approach is primarily useful in cases where we have a large number of training instances relative to the dimension of the space. 

Finally, while these approaches help us avoid parametric assumptions on the learning side, we are left with the question of how to avoid them on the inference side. As we saw, most inference procedures are geared to working with parametric representations, mostly Gaussians. Thus, when performing inference with nonparametric CPDs, we must generally either use parametric approximations, or resort to sampling. 

# 17.2.5 Maximum Likelihood Estimation as M-Projection $\star$ 

The MLE principle is a general one, in that it gives a recipe how to construct estimators for diferent statistical models (for example, multinomials and Gaussians). As we have seen, for simple examples the resulting estimators are quite intuitive. However, the same principle can be applied in a much broader range of parametric models. Indeed, as we now show, we have already discussed the framework that forms the basis for this generalization. 

In section 8.5, we deﬁned the notion of projection : ﬁnding the distribution, within a speciﬁed class, that is closest to a given target distribution. Parameter estimation is similar in the sense that we select a distribution from a given class — all of those that can be described by the model — that is “closest” to our data. Indeed, we can show that maximum likelihood estimation aims to ﬁnd the distribution that is “closest” to the empirical distribution $\hat{P}_{\mathcal{D}}$ (see equation (16.4)). 

We start by rewriting the likelihood function in terms of the empirical distribution. 

Proposition 17.2 

# Let $\mathcal{D}$ be a data set, then 

$$
\log L(\pmb\theta:\mathcal D)=M\cdot\pmb E_{\hat{P}_{\mathcal D}}[\log P(\mathcal X:\pmb\theta)].
$$ 

Proof We rewrite the likelihood by combining all identical instances in our training set and then writing the likelihood in terms of the empirical probability of each entry in our joint distribution: 

$$
\begin{array}{l c l}{\log L(\pmb\theta:\mathcal D)}&{=}&{\displaystyle\sum_{m}\log P(\xi[m]:\pmb\theta)}\\ &{=}&{\displaystyle\sum_{\xi}\left[\sum_{m}\pmb I\{\{\xi[m]=\xi\}\right]\log P(\xi:\pmb\theta)}}\\ &{=}&{\displaystyle\sum_{\xi}M\cdot\hat{P}_{\mathcal D}(\xi)\log P(\xi:\pmb\theta)}\\ &{=}&{M\cdot\pmb E_{\hat{P}_{\mathcal D}}[\log P(\mathcal X:\pmb\theta)].}\end{array}
$$ 

We can now apply proposition 16.1 to the empirical distribution to conclude that 

$$
\ell(\pmb\theta:{\mathcal D})=M\left(H_{\hat{P}_{\mathcal D}}(\mathcal X)-{\cal D}(\hat{P}_{\mathcal D}(\mathcal X)\|P(\mathcal X:\pmb\theta))\right).
$$ 

From this result, we immediately derive the following relationship between MLE and M-projections. 

Theorem 17.1 The MLE θ in a parametric family relative to a data set $\mathcal{D}$ is the $M\cdot$ -projection of $\hat{P}_{\mathcal{D}}$ onto the D parametric family 

$$
\hat{\pmb\theta}=\arg\operatorname*{min}_{\pmb\theta\in\Theta}\pmb D(\hat{P}_{\mathcal{D}}\|P_{\pmb\theta}).
$$ 

We see that MLE ﬁnds the distribution $P(\mathcal{X}:\theta)$ that is the M-projection of $\hat{P}_{\mathcal{D}}$ onto the set D of distributions representable in our parametric family. 

This result allows us to call upon our detailed analysis of M-projections in order to generalize MLE to other parametric classes in the exponential family. In particular, in section 8.5.2, we discussed the general notion of sufcient statistics and showed that the M-projection of a distribution $P$ into a class of distributions $\mathcal{Q}$ was deﬁned by e parameters $\theta$ such that $E_{Q_{\theta}}[\tau(\mathcal{X})]=E_{P}[\tau(\mathcal{X})]$ X X . In our setting, we seek the parameters θ whose expected sufcient statistics match those in $\hat{P}_{\mathcal{D}}$ , that is, the sufcient statistics in $\mathcal{D}$ . 

If our CPDs are in an exponential family where the mapping ess from parameters to sufcient statistics is invertible, we can simply take the sufcient statistic vector from $\hat{P}_{\mathcal{D}}$ , and invert this D mapping to produce the MLE. Indeed, this process is precisely the one that gave rise to our MLE for multinomials and for linear Gaussians, as described earlier. However, the same process can be applied to many other classes of distributions in the exponential family. 

This analysis provides us with a notion of sufcient statistics $\tau(\mathcal{X})$ and a clearly deﬁned path to deriving MLE parameters for any distribution in the exponential family. Somewhat more surprisingly, it turns out that a parametric family has a sufcient statistic only $i f$ it is in the exponential family. 

# 17.3 Bayesian Parameter Estimation 

# 17.3.1 The Thumbtack Example Revisited 

Although the MLE approach seems plausible, it can be overly simplistic in many cases. Assume again that we perform the thumbtack experiment and get 3 heads out of 10. It may be quite reasonable to conclude that the parameter $\theta$ is 0 . 3 . But what if we do the same experiment with a standard coin, and we also get 3 heads? We would be much less likely to jump to the conclusion that the parameter of the coin is 0 . 3 . Why? Because we have a lot more experience with tossing coins, so we have a lot more prior knowledge about their behavior. Note that we do not want our prior knowledge to be an absolute guide, but rather a reasonable starting assumption that allows us to counterbalance our current set of 10 tosses, under the assumption that they may not be typical. However, if we observe 1,000,000 tosses of the coin, of which 300,000 came out heads, then we may be more willing to conclude that this is a trick coin, one whose parameter is closer to 0 . 3 . 

Maximum likelihood allows us to make neither of these distinctions: between a thumbtack and a coin, and between 10 tosses and 1,000,000 tosses of the coin. There is, however, another approach, the one recommended by Bayesian statistics. 

# 17.3.1.1 Joint Probabilistic Model 

In this approach, we encode our prior knowledge about $\theta$ with a probability distribution; this distribution represents how likely we are a priori to believe the diferent choices of parameters. Once we quantify our knowledge (or lack thereof) about possible values of $\theta$ , we can create a joint distribution over the parameter $\theta$ and the data cases that we are about to observe $X[1],\cdot\cdot\cdot,X[M]$ . This joint distribution captures our assumptions about the experiment. 

Let us reconsider these assumptions. Recall that we assumed that tosses are independent of each other. Note, however, that this assumption was made when $\theta$ was ﬁxed. If we do not know $\theta$ , then the tosses are not marginally independent: Each toss tells us something about the parameter $\theta$ , and thereby about the probability of the next toss. However, once $\theta$ is known, we cannot learn about the outcome of one toss from observing the results of others. Thus, we assume that the tosses are conditionally independent given $\theta$ . We can describe these assumptions using the probabilistic model of ﬁgure 17.3. 

Having determined the model structure, it remains to specify the local probability models in this network. We begin by considering the probability $P(X[m]\mid\theta)$ . Clearly, 

$$
P(x[m]\mid\theta)={\left\{\begin{array}{l l}{\theta}&{{\mathrm{if~}}x[m]=x^{1}}\\ {1-\theta}&{{\mathrm{if~}}x[m]=x^{0}.}\end{array}\right.}
$$ 

Note that since we now treat $\theta$ as a random variable, we use the conditioning bar, instead of $P(x[m]:\theta)$ . 

prior parameter distribution 

To ﬁnish the description of the joint distribution, we need to describe $P(\theta)$ . This is our prior distribution over the value of $\theta$ . In our case, this is a continuous density over the interval $[0,1]$ . Before we discuss particular choices for this distribution, let us consider how we use it. 

The network structure implies that the joint distribution of a particular data set and $\theta$ 

![](images/9b78b0f78eb515c26ef81f2b3d67e69d88d261955057937e61c417447ab8644e.jpg) 
Figure 17.3 Meta-network for IID samples of a random variable $X$ . (a) Plate model; (b) Ground Bayesian network. 

factorizes as 

$$
\begin{array}{l l l}{P(x[1],\ldots,x[M],\theta)}&{=}&{P(x[1],\ldots,x[M]\mid\theta)P(\theta)}\\ &{=}&{P(\theta)\displaystyle\prod_{m=1}^{M}P(x[m]\mid\theta)}\\ &{=}&{P(\theta)\theta^{M[1]}(1-\theta)^{M[0]},}\end{array}
$$ 

where $M[1]$ is the number of heads in the data, and $M[0]$ is the number of tails. Note that the expression $P(x[1],.\,.\,.\,,x[M]\mid\theta)$ is simply the likelihood function $L(\theta:{\mathcal{D}})$ . 

posterior parameter distribution 

This network speciﬁes a joint probability model over parameters and data. There are several ys in which we can use this network. Most obviously, we can take an observed data set $\mathcal{D}$ of $M$ outcomes, and use it to instantiate the values of $x[1],\ldots,x[M]$ ; we can then compute the posterior distribution over $\theta$ : 

$$
P(\theta\mid x[1],\ldots,x[M])={\frac{P(x[1],\ldots,x[M]\mid\theta)P(\theta)}{P(x[1],\ldots,x[M])}}.
$$ 

In this posterior, the ﬁrst term in the numerator is the likelihood, the second is the prior over parameters, and the denominator is a normalizing factor that we will not expand on right now. We see that the posterior is (proportional to) a product of the likelihood and the prior. This product is normalized so that it will be a proper density function. In fact, if the prior is a uniform distribution (that is, $P(\theta)\,=\,1$ for all $\theta\,\in\,[0,1])$ , then the posterior is just the normalized likelihood function. 

17.3.1.2 Prediction 

If we do use a uniform prior, what then is the diference between the Bayesian approach and the MLE approach of the previous section? The main philosophical diference is in the use of the posterior. Instead of selecting from the posterior a single value for the parameter $\theta$ , we use it, in its entirety, for predicting the probability over the next toss. 

To derive this prediction in a principled fashion, we introduce the value of the next coin toss $x[M+1]$ to our network. We can then compute the probability over $x[M+1]$ given the observations of the ﬁrst $M$ tosses. Note that, in this model, the parameter $\theta$ is unknown, and we are considering all of its possible values. By reasoning over the possible values of $\theta$ and using the chain rule, we see that 

$$
{\begin{array}{r l}{\lefteqn{P(x[M+1]\mid x[1],\ldots,x[M])=}}\\ {=}&{\int P(x[M+1]\mid\theta,x[1],\ldots,x[M])P(\theta\mid x[1],\ldots,x[M])d\theta}\\ {=}&{\int P(x[M+1]\mid\theta)P(\theta\mid x[1],\ldots,x[M])d\theta,}\end{array}}
$$ 

where we use the conditional independencies implied by the meta-network to rewrite $P(x[M+$ 1] $\mid\theta,x[1],\cdot\cdot\cdot,x[M])$ as $P(x[M+1]\mid\theta)$ . In other words, we are integrating our posterior over θ to predict the probability of heads for the next toss. 

Let us go back to our thumbtack example. Assume that our prior is uniform over $\theta$ in the interval $[0,1]$ . Then $P_{\cdot}(\theta\mid x[1],\cdot\cdot\cdot,x[M])$ is proportional to the likelihood $P(x[1],\cdot\,\cdot\,\cdot\,,x[M]\mid$ $\theta)=\theta^{M[1]}(1-\theta)^{M[0]}$ . Plugging this into the integral, we need to compute 

$$
P(X[M+1]=x^{1}\mid x[1],\ldots,x[M])={\frac{1}{P(x[1],\ldots,x[M])}}\int\theta\cdot\theta^{M[1]}(1-\theta)^{M[1]}\,
$$ 

Doing all the math (see exercise 17.6), we get (for uniform priors) 

$$
P(X[M+1]=x^{1}\mid x[1],\ldots,x[M])={\frac{M[1]+1}{M[1]+M[0]+2}}.
$$ 

Bayesian estimator 

Laplace’s correction This prediction, called the Bayesian estimator , is quite similar to the MLE prediction of equa- tion (17.1), except that it adds one “imaginary” sample to each count. Clearly, as the number of samples grows, the Bayesian estimator and the MLE estimator converge to the same value. The particular estimator that corresponds to a uniform prior is often referred to as Laplace’s correction . 

# 17.3.1.3 Priors 

Beta distribution 

# Deﬁnition 17.3 

Beta hyperparameters We now want to consider nonuniform priors. The challenge here is to pick a distribution over this continuous space that we can represent compactly (for example, using an analytic formula), and update efciently as we get new data. For reasons that we will discuss, an appropriate prior in this case is the Beta distribution : 

$A$ Beta distribution is parameterized by two hyperparameters $\alpha_{1},\alpha_{0}$ , which are positive reals. The distribution is deﬁned as follows: 

$$
\theta\sim\mathrm{Beta}(\alpha_{1},\alpha_{0})\ i f\ p(\theta)=\gamma\theta^{\alpha_{1}-1}(1-\theta)^{\alpha_{0}-1}.
$$ 

The constant $\gamma$ is a normalizing constant, deﬁned as follows: 

$$
\gamma=\frac{\Gamma(\alpha_{1}+\alpha_{0})}{\Gamma(\alpha_{1})\Gamma(\alpha_{0})},
$$ 

![](images/2c82d2310c2d3b91959e390cf984fd767c02b4eab1c914eda3a55dbca1b5cb42.jpg) 
Figure 17.4 Examples of Beta distributions for diferent choices of hyperparameters 

Intuitively, the hyperparameters $\alpha_{1}$ and $\alpha_{0}$ correspond to the number of imaginary heads and tails that we have “seen” before starting the experiment. Figure 17.4 shows Beta distributions for diferent values of $\alpha$ . 

At ﬁrst glance, the normalizing constant for the Beta distribution might seem somewhat obscure. However, the Gamma function is actually a very natural one: it is simply a continuous generalization of factorials. More precisely, it satisﬁes the properties $\Gamma(1)=1$ and $\Gamma(x+1)=$ $x\Gamma(x)$ . As a consequence, we easily see that $\Gamma(n+1)=n!$ when $n$ is an integer. 

Beta distributions have properties that make them particularly useful for parameter estimation. Assume our distribution $P(\theta)$ is $B e t a(\alpha_{1},\alpha_{0})$ , and consider a single coin toss $X$ . Let us compute the marginal probability over $X$ , based on $P(\theta)$ . To compute the marginal probability, we need to integrate out $\theta$ ; standard integration techniques can be used to show that: 

$$
\begin{array}{c c l}{{P(X[1]=x^{1})}}&{{=}}&{{\displaystyle\int_{0}^{1}P(X[1]=x^{1}\mid\theta)\cdot P(\theta)d\theta}}\\ {{}}&{{=}}&{{\displaystyle\int_{0}^{1}\theta\cdot P(\theta)d\theta=\frac{\alpha_{1}}{\alpha_{1}+\alpha_{0}}.}}\end{array}
$$ 

This conclusion supports our intuition that the Beta prior indicates that we have seen $\alpha_{1}$ (imaginary) heads $\alpha_{0}$ (imaginary) tails. 

Now, let us see what happens as we get more observations. Speciﬁcally, we observe $M[1]$ heads and $M[0]$ tails. It follows easily that: 

$$
\begin{array}{l c l}{{P(\theta\mid x[1],\ldots,x[M])}}&{{\propto}}&{{P(x[1],\ldots,x[M]\mid\theta)P(\theta)}}\\ {{}}&{{\propto}}&{{\theta^{M[1]}(1-\theta)^{M[0]}\cdot\theta^{\alpha_{1}-1}(1-\theta)^{\alpha_{0}-1}}}\\ {{}}&{{=}}&{{\theta^{\alpha_{1}+M[1]-1}(1-\theta)^{\alpha_{0}+M[0]-1},}}\end{array}
$$ 

conjugate prior which is precisely $B e t a(\alpha_{1}+M[1],\alpha_{0}+M[0])$ . This result illustrates a key property of the Beta distribution: If the prior is a Beta distribution, then the posterior distribution, that is, the prior conditioned on the evidence, is also a Beta distribution. In this case, we say that the Beta distribution is conjugate to the Bernoulli likelihood function (see deﬁnition 17.4). 

An immediate consequence is that we can compute the probabilities over the next toss: 

$$
P(X[M+1]=x^{1}\mid x[1],\ldots,x[M])={\frac{\alpha_{1}+M[1]}{\alpha+M}},
$$ 

where $\alpha\,=\,\alpha_{1}+\alpha_{0}$ . In this case, our posterior Beta distribution tells us that we have seen $\alpha_{1}+M[1]$ heads (imaginary and real) and $\alpha_{0}+M[0]$ tails. 

It is interesting to examine the efect of the prior on the probability over the next coin toss. For example, the prior $B e t a(1,1)$ is very diferent than $B e t a(10,10)$ : Although both predict that the probability of heads in the ﬁrst toss is 0 . 5 , the second prior is more entrenched, and it requires more observations to deviate from the prediction 0 . 5 . To see this, suppose we observe 3 heads in 10 tosses. Using the ﬁrst prior, our estimate is $\begin{array}{r}{\frac{3+1}{10+2}=\frac{1}{3}\approx0.33.}\end{array}$ . On the other hand, using the second prior, our estimate is $\begin{array}{r}{\frac{3+10}{10+20}=\frac{13}{30}\approx0.{\stackrel{\ldots}{43}}}\end{array}$ . However, as we obtain more data, the efect of the prior diminishes. If we obtain $1,000$ tosses of which 300 are heads, the ﬁrst prior gives us an estimate of $\begin{array}{r}{\frac{300+1}{1,000+2}}\end{array}$ and the second an estimate of $\textstyle\frac{300+10}{1,000+20}$ , both of which are very close to 0 . 3 . Thus, the Bayesian framework allows us to capture both of the relevant distinctions. The distinction between the thumbtack and the coin can be captured by the strength of the prior: for a coin, we might use $\alpha_{1}=\alpha_{0}=100,$ , whereas for a thumbtack, we might use $\alpha_{1}=\alpha_{0}=1$ . The distinction between a few samples and many samples is captured by the peakedness of our posterior, which increases with the amount of data. 

# 17.3.2 Priors and Posteriors 

We now turn to examine in more detail the Bayesian approach to dealing with unknown pa- rameters. We start with a discussion of the general principle and deal with the case of Bayesian networks in the next section. 

As bef , we assume a general learning problem here we observe a training set $\mathcal{D}$ that contains M IID samples of a set of random variable X from an unknown distribution $P^{*}(\mathcal{X})$ . We also assume that we ave a parametric model $P(\xi\mid\theta)$ where we can choose parameters from a parameter space Θ . 

point estimate 

Recall that the MLE approach attempts to ﬁnd the parameters $\hat{\pmb\theta}$ in $\Theta$ that are “best” given the data. The Bayesian approach, on the other hand, does not attempt to ﬁnd such a point estimate . Instead, the underlying principle is that we should keep track of our beliefs about $\theta$ ’s values, and use these beliefs for reaching conclusions. That is, we should quantify the subjective probability we assign to diferent values of $\theta$ after we have seen the evidence. Note that, in representing such subjective probabilities, we now treat $\theta$ as a random variable. Thus, the Bayesian approach requires that we use probabilities to describe our initial uncertainty about the parameters $\theta$ , and then use probabilistic reasoning (that is, Bayes rule) to take into account our observations. 

To perform this task, we need to describe a joint distribution $P(\mathcal{D},\theta)$ over the data and the parameters. We can easily write 

$$
P(\mathcal{D},\boldsymbol{\theta})=P(\mathcal{D}\mid\boldsymbol{\theta})P(\boldsymbol{\theta}).
$$ 

parameter prior 

parameter posterior 

marginal likelihood 

The ﬁrst term is just the likelihood function we discussed earlier. The second term is the prior distribution over the possible values in $\Theta$ . This prior captures our initial uncertainty about the parameters. It can also capture our previous experience before starting the experiment. For example, if we study coin tossing, we might have prior experience that suggests that most coins are unbiased (or nearly unbiased). 

Once we have speciﬁed the likelihood function and the prior, we can use the data to derive the posterior distribution over the parameters. Since we have speciﬁed a joint distribution over all the quantities in question, the posterior is immediately derived by Bayes rule: 

$$
P(\theta\mid\mathcal{D})=\frac{P(\mathcal{D}\mid\theta)P(\theta)}{P(\mathcal{D})}.
$$ 

The term $P(\mathcal{D})$ is the marginal likelihood of the data 

$$
P(\mathcal{D})=\int_{\Theta}P(\mathcal{D}\mid\theta)P(\theta)d\theta,
$$ 

that is, the integration of the likelihood over all possible parameter assignments. This is the a priori probability of seeing this particular data set given our prior beliefs. 

As we saw, for some probabilistic models, the likelihood function can be compactly described by using sufcient statistics. Can we also compactly describe the posterior distribution? In general, this depends on the form of the prior. As we saw in the thumbtack example of section 17.1.1, we can sometimes ﬁnd priors for which we have a description of the posterior. 

As another example of the forms of priors and posteriors, let us examine the learning problem of example 17.3. Here we need to describe our uncertainty about the parameters of a multinomial distribution. The parameter space $\Theta$ is the space of all nonnegative vectors $\pmb{\theta}=\langle\theta_{1},.\,.\,.\,,\theta_{K}\rangle$ such that $\textstyle\sum_{k}\theta_{k}=1$ . As we saw in example 17.3, the likelihood function in this model has the form: 

$$
L(\pmb\theta:\mathcal{D})=\prod_{k}\theta_{k}^{M[k]}.
$$ 

Since the posterior is a product of the prior and the likelihood, it seems natural to require that the prior also have a form similar to the likelihood. 

Dirichlet distribution 

Dirichlet hyperparameters 

Dirichlet posterior 

One such prior is the Dirichlet distribution , which generalizes the Beta distribution we dis- cussed earlier. A Dirichlet distribution is speciﬁed by a set of hyperparameters $\alpha_{1},.\cdot\cdot\,,\alpha_{K}$ , so that 

$$
\pmb\theta\sim D i r i c h l e t(\alpha_{1},.\,.\,,\alpha_{K})\ \ \mathrm{if}\ P(\pmb\theta)\propto\prod_{k}\theta_{k}^{\alpha_{k}-1}.
$$ 

We use $\alpha$ to denote $\textstyle\sum_{j}\alpha_{j}$ . If we use a Dirichlet prior, then the posterior is also Dirichlet: 

# Proposition 17.3 

# If $P(\theta)$ is Dirichlet $\mathopen{}\mathclose\bgroup\left(\alpha_{1},\dots,\alpha_{K}\aftergroup\egroup\right)$ then $P(\pmb{\theta}\mid\mathcal{D})$ is Dirichlet $(\alpha_{1}+M$ [1] , . . . , $\alpha_{K}+M[K])$ , where $M[k]$ is the number of occurrences of $x^{k}$ . 

Priors such as the Dirichlet are useful, since they ensure that the posterior has a nice com- pact description. Moreover, this description uses the same representation as the prior. This phenomenon is a general one, and one that we strive to achieve, since it makes our computa- tion and representation much easier. 

Deﬁnition 17.4 conjugate prior 

family of priors $P(\pmb\theta:\pmb\alpha)$ is conjugate to a particular model $P(\xi\mid\theta)$ if f any possible data s t D of IID samples from $P(\xi\mid\theta)$ , and any choice of legal hyperparameters α for the prior over θ , there are hyperparameters α that describe the posterior. That is, 

$$
P(\pmb\theta:\pmb\alpha^{\prime})\propto P(\mathcal D\mid\pmb\theta)P(\pmb\theta:\pmb\alpha).
$$ 

For example, Dirichlet priors are conjugate to the multinomial model. We note that this does not preclude the possibility of other families that are also conjugate to the same model. See exercise 17.7 for an example of such a prior for the multinomial model. We can ﬁnd conjugate priors for other models as well. See exercise 17.8 and exercise 17.11 for the development of conjugate priors for the Gaussian distribution. 

This discussion shows some examples where we can easily update our beliefs about $\theta$ after observing a set of instances $\mathcal{D}$ . This update process results in a posterior that combines our prior knowledge and our observations. What can we do with the posterior? We can use the posterior to determine properties of the model at hand. For example, to assess our beliefs that a coin we experimented with is biased toward heads, we might compute the posterior probability that $\theta>t$ for some threshold $t$ , say 0.6. 

Bayesian estimator 

Another use of the posterior is to predict the probability of future examples. Suppose that we are about to sample a new instance $\xi[M+1]$ . Since we already have observations over previous instances, the Bayesian estimator is the posterior distribution over a new example: 

$$
\begin{array}{c c l}{P(\xi[M+1]\mid\mathcal{D})}&{=}&{\displaystyle\int P(\xi[M+1]\mid\mathcal{D},\theta)P(\theta\mid\mathcal{D})d\theta}\\ &{=}&{\displaystyle\int P(\xi[M+1]\mid\theta)P(\theta\mid\mathcal{D})d\theta}\\ &{=}&{E_{P(\theta\mid\mathcal{D})}[P(\xi[M+1]\mid\theta)],}\end{array}
$$ 

where, in the second step, we use the fact that instances are independent given $\theta$ . Thus, our prediction is the average over all parameters according to the posterior. 

Let us examine prediction with the Dirichlet prior. We need to compute 

$$
\begin{array}{r}{P(x[M+1]=x^{k}\mid\mathcal{D})=E_{P(\theta\mid\mathcal{D})}[\theta_{k}].}\end{array}
$$ 

To compute the prediction on a new data case, we need to compute the expectation of particular parameters with respect for a Dirichlet distribution over $\theta$ . 

# Proposition 17.4 

Recall that our posterior is $\mathit{D i r i c h l e t}(\alpha_{1}+M[1],.\,.\,,\alpha_{K}+M[K])$ where $M[1],\ldots,M[K]$ are the sufcient statistics from the data. Hence, the prediction with Dirichlet priors is 

$$
P(x[M+1]=x^{k}\mid\mathcal{D})=\frac{M[k]+\alpha_{k}}{M+\alpha}.
$$ 

pseudo-counts 

equivalent sample size mean prediction This prediction is similar to prediction with the MLE parameters. The only diference is that we added the hyperparameters to our counts when making the prediction. For this reason the Dirichlet hyperparameters are often called pseudo-counts . We can think of these as the number of times we have seen the diferent outcomes in our prior experience before conducting our current experiment. 

The total $\alpha$ of the pseudo-counts reﬂects how conﬁdent we are in our prior, and is often called the equivalent sample size . Using $\alpha$ , we can rewrite the hyperparameters as $\alpha_{k}=\alpha\theta_{k}^{\prime}$ , where $\theta^{\prime}=\{\theta_{k}^{\prime}:k=1,.\,.\,.\,,K\}$ } is a distribution describing the mean p iction of our prior. We can see that the prior prediction (before observing any data) is simply θ $\theta^{\prime}$ . Moreover, we can rewrite the prediction given the posterior as: 

$$
P(x[M+1]=x^{k}\mid\mathcal{D})=\frac{\alpha}{M+\alpha}\theta_{k}^{\prime}+\frac{M}{M+\alpha}\cdot\frac{M[k]}{M}.
$$ 

improper prior That is, the prediction is a weighted average (convex combination) of the prior mean and the MLE estimate. The combination weights are determined by the relative magnitude of $\alpha\mathrm{~-~}$ the conﬁdence of the prior (or total weight of the pseudo-counts) — and $M\mathrm{~-~}$ the number of observed samples. We see that the Bayesian prediction converges to the MLE estimate when $M\rightarrow\infty$ . Intuitively, when we have a very large training set the contribution of the prior is negligible, and the prediction will be dominated by the frequency of outcomes in the data. We also get convergence to th estimate when $\alpha\rightarrow0$ , so that we have only a very weak prior. Note that the case where α $\alpha=0$ is not achievable: the normalization constant for the Dirichlet prior grows to inﬁnity when the hyperparameters are close to 0 . Thus, the prior with $\alpha\,=\,0$ (that is, $\alpha_{k}=0$ for all $k$ ) is not well deﬁned. The prior with $\alpha=0$ is often called a improper prior . The diference between the Bayesian estimate and the MLE estimate arises when $M$ is not too large, and $\alpha$ is not close to 0 . In these situations, the Bayesian estimate is “biased” toward the prior probability $\theta^{\prime}$ . 

To gain some intuition for the interaction between these diferent factors, ﬁgure 17.5 shows the efect of the strength and means of the prior on our estimates. We can see that, as the amount of real data grows, our estimate converges to the true underlying distribution, regardless of the starting point. The convergence time grows both with the diference between the prior mean and the empirical mean, and with the strength of the prior. We also see that the Bayesian estimate is more stable than the MLE estimate, because with few instances, even single samples will change the MLE estimate dramatically. 

Example 17.7 Suppose we are trying to estimate the parameter associated with a coin, and we observe one head and one tail. Our MLE estimate of $\theta_{1}$ is $1/2=0.5\$ . Now, if the next observation is a head, we will change our estimate to be $2/3\approx0.66$ . On the other hand, if our next observation is a tail, we will change our te to $1/3\approx0.33$ . In contrast, consider the Bayesian estimate with a Dirichlet prior with $\alpha\,=\,1$ and $\theta_{1}^{\prime}\,=\,0.5$ . With this estimator, our original estimate is $1.5/3\,=\,0.5$ . If we observe another head, we revise to $2.5/4\,=\,0.625$ , and if observe another tail, we revise to 

![](images/bd089994e0a806c487f0fe014edcbf5a1f5338508cd0d158040cba40488e1b87.jpg) 
Figure 17.5 The efect of the strength and means of the Beta prior on our posterior estimates. Our data set is an idealized version of samples from a biased coin where the frequency of heads is 0 . 2 : for a given data set size $M$ , we assume that $\mathcal{D}$ $0.2M$ heads and $0.8M$ tails. The $x$ axis represents the number of samples $(M)$ in our data set D , and the $y$ axis the expecte bability of heads according to the Bayesian estimate. (a) shows the efect of varying the prior means $\theta_{1}^{\prime},\theta_{0}^{\prime}$ , for a ﬁxed prior strength $\alpha$ . (b) shows the efect of varying the prior strength for a ﬁxed prior mean $\theta_{1}^{\prime}=\theta_{0}^{\prime}=0.5$ . 

$1.5/4\,=\,0.375$ . We see that the estimate changes by slightly less after the update. If $\alpha$ is larger, then the smoothing is more aggressive. For example, when $\alpha=5$ , our estimate is $4.5/8=0.5625$ after observing a head, and $3.5/8\,=\,0.4375$ after observing a tail. We can also see this efect visually in ﬁgure 17.6, which shows our changing estimate for $P(\theta_{H})$ as we observe a particular sequence of tosses. 

This smoothing efect results in more robust estimates when we do not have enough data to reach deﬁnite conclusions. If we have good prior knowledge, we revert to it. Alternatively, if we do not have prior knowledge, we can use a uniform prior that will keep our estimate from taking extreme values. In general, it is a bad idea to have extreme estimates (ones where some of the parameters are close to 0 ), since these might assign too small probability to new instances we later observe. In particular, as we already discussed, probability estimates that are actually 0 are dangerous, since no amount of evidence can change them. Thus, if we are unsure about our estimates, it is better to bias them away from extreme estimates. The MLE estimate, on the other hand, often assigns probability 0 to values that were not observed in the training data. 

# 17.4 Bayesian Parameter Estimation in Bayesian Networks 

We now turn to Bayesian estimation in the context of a Bayesian network. Recall that the Bayesian framework requires us to specify a joint distribution over the unknown parameters and the data instances. As in the single parameter case, we can understand the joint distribution over parameters and data as a Bayesian network. 

![](images/874ddc5ff65efe0c598c8640a8aaf5760d7e0a0964b6835d22a8ddf39b93431f.jpg) 
Figure 17.6 The efect of diferent priors on smoothing our parameter estimates. The graph shows the stimate of $P(X=H|\mathcal{D})$ $(y$ -axis) after seeing diferent number of samples $\mathit{\check{x}}$ -axis). The graph below the x -axis shows the particular sequence of tosses. The solid line corresponds to the MLE estimate, and the remaining ones to Bayesian estimates with diferent strengths and uniform prior means. The large-dash line corresponds to $B e t a(1,1)$ , the small-dash line to $B e t a(5,5)$ , and the dotted line to $B e t a(10,10)$ . 

# 17.4.1 Parameter Independence and Global Decomposition 

# 17.4.1.1 A Simple Example 

meta-network Suppose we want to estimate parameters for a simple network with two variables $X$ and $Y$ so that $X$ is the parent of $Y$ . Our training data consist of observations $X[m],Y[m]$ for $m=$ $1,\cdot\cdot\cdot,M$ . In addition, we have unknown parameter vectors $\theta_{X}$ and $\theta_{Y\mid X}$ . The dependencies between these variables are described in the network of ﬁgure 17.7. This is the meta-network that describes our learning setup. 

This Bayesian network structure immediately reveals several points. For example, as in our simple thumbtack example, the instances are independent given the unknown parameters. A simple examination of active trails shows that $X[m]$ and $Y[m]$ are d-separated from $X[m^{\prime}]$ and $Y[m^{\prime}]$ once we observe the parameter variables. 

In addition, the network structure embodies the assumption that the priors for the individual parameters variables are a priori independent. That is, we believe that knowing the value of one parameter tells us nothing about another. More precisely, we deﬁne 

This assumption may not be suitable for all domains, and it should be considered with care. 

![](images/654911780adaac3796076adb6dcf5cd7b05ce5f969a1d4c18322860cba352133.jpg) 
Figure 17.7 Meta-network for IID samples from a network $X\,\rightarrow\,Y$ with global parameter inde- pendence. (a) Plate model; (b) Ground Bayesian network. 

Consider an extension of our student example, where our student takes multiple classes. For each class, we want to learn the distribution of Grade given the student’s Intelligence and the course Difculty. For classes taught by the same instructor, we might believe that the grade distribution is the same; for example, if two classes are both difcult, and the student is intelligent, his probability of getting an A is the same in both. However, under the global parameter independence assumption, these are two diferent random variables, and hence their parameters are independent. 

Thus, although we use the global parameter independence in much of our discussion, it is not always appropriate, and we relax it in some of our later discussion (such as section 17.5 and section 18.6.2). 

If we accept global parameter independence, we can draw an important conclusion. Complete data $\mathrm{d}$ -separates the parameters for diferent CPDs. For example, if $x[m]$ and $y[m]$ are observed for all $m$ , then $\theta_{X}$ and $\theta_{Y\mid X}$ are d-separated. To see this, note that any path between the two has the form 

$$
\theta_{X}\rightarrow X[m]\rightarrow Y[m]\leftarrow\theta_{Y|X},
$$ 

so that the observation of $x[m]$ blocks the path. Thus, if these two parameter variables are independent a priori, they are also independent a posteriori. Using the deﬁnition of conditional independence, we conclude that 

$$
P(\pmb\theta_{X},\pmb\theta_{Y\mid X}\mid\mathcal D)=P(\pmb\theta_{X}\mid\mathcal D)P(\pmb\theta_{Y\mid X}\mid\mathcal D).
$$ 

This decomposition has mediate practical ramiﬁcations. Given the data set $\mathcal{D}$ , we can deter- mine the posterior over $\theta_{X}$ independently of the posterior over $\theta_{Y\mid X}$ . Once we can solve each problem separately, we can combine the results. This is the analogous result to the likelihood decomposition for MLE estimation of section 17.2.2. In the Bayesian setting this property has additional importance. It tells us the posterior can be represented in a compact factorized form. 

# 17.4.1.2 General Networks 

We can generalize this conclusion to the general case of Bayesian network learning. Suppose we are given a network structure $\mathcal{G}$ with parameters $\theta$ . In the Bayesian framework, we need to specify a prior $P(\theta)$ over all possible parameter iz at ions of the network. The posterior distribution over parameters given the data samples $\mathcal{D}$ is simply 

$$
P(\theta\mid\mathcal{D})=\frac{P(\mathcal{D}\mid\theta)P(\theta)}{P(\mathcal{D})}.
$$ 

marginal likelihood The term $P(\theta)$ is our prior distribution, $P({\mathcal{D}}\mid\theta)$ is the probability of ta given a particular parameter settings, which is simply the likelihood function. Finally, $P(\mathcal{D})$ D is the normalizing constant. As we discussed, this term is called the marginal likelihood ; it will play an important role in the next chapter. For now, however, we can ignore it, since it does not depend on $\theta$ and only serves to normalize the posterior. 

As we discussed in section 17.2, we can decompose the likelihood into local likelihoods: 

$$
P(\mathcal{D}\mid\theta)=\prod_{i}L_{i}(\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}:\mathcal{D}).
$$ 

Moreover, if we assume that we have global parameter independence, then 

$$
P(\pmb\theta)=\prod_{i}P(\pmb\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}).
$$ 

Combining these two decompositions, we see that 

$$
P(\pmb\theta\mid\mathcal D)=\frac{1}{P(\mathcal D)}\prod_{i}\Big[L_{i}(\pmb\theta_{X_{i}\mid\mathrm{Pa}_{X_{i}}}:\mathcal D)P(\pmb\theta_{X_{i}\mid\mathrm{Pa}_{X_{i}}})\Big]\,.
$$ 

Now each subset $\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ of $\theta$ appears in just one term in the product. Thus, we have that the posterior can be represented as a product of local terms. 

Proposition 17.5 Let $\mathcal{D}$ be a complete data set for $\mathcal{X}$ , let $\mathcal{G}$ be a network structure over these variables. If $P(\theta)$ satisﬁes global parameter independence, then 

$$
P(\pmb\theta\mid\mathcal D)=\prod_{i}P(\pmb\theta_{X_{i}\mid\mathrm{Pa}_{X_{i}}}\mid\mathcal D).
$$ 

The proof of this property follows from the steps we discussed. It can also be derived directly from the structure of the meta-Bayesian network (as in the network of ﬁgure 17.7). 

# 17.4.1.3 Prediction 

This decomposition of the posterior allows us to simplify various tasks. For example, suppose that, in our simple two-variable network, we want to compute the probability of another instance $x[M+1],y[M+1]$ based on our previous observations $x[1],y[1],.\;.\;.\;,x[M],y[M]$ . According to the structure of our meta-network, we need to sum out (or more precisely integrate out) the unknown parameter variables 

$$
P(x[M+1],y[M+1]\mid\mathcal{D})=\int P(x[M+1],y[M+1]\mid\mathcal{D},\theta)P(\theta\mid\mathcal{D})d\theta,
$$ 

where the integration is over all legal parameter values. Since $\theta$ d-separates instances from each other, we have that 

$$
\begin{array}{r l}{\lefteqn{P(x[M+1],y[M+1]\mid\mathcal{D},\theta)}}\\ {=}&{P(x[M+1],y[M+1]\mid\theta)}\\ {=}&{P(x[M+1]\mid\theta_{X})P(y[M+1]\mid x[M+1],\theta_{Y\mid X}).}\end{array}
$$ 

Moreover, as we just saw, the posterior probability also decomposes into a product. Thus, 

$$
\begin{array}{l c l}{{P(x[M+1],y[M+1]\mid\mathcal{D})}}\\ {{=}}&{{{\displaystyle\int\int}P(x[M+1]\mid\theta_{X})P(y[M+1]\mid x[M+1],\theta_{Y\mid X})}}\\ {{}}&{{{\displaystyle P(\theta_{X}\mid\mathcal{D})P(\theta_{Y\mid X}\mid\mathcal{D})d\theta_{X}d\theta_{Y\mid X}}}}\\ {{=}}&{{\left({\displaystyle\int}P(x[M+1]\mid\theta_{X})P(\theta_{X}\mid\mathcal{D})d\theta_{X}\right)}}\\ {{}}&{{~~\left({\displaystyle\int}P(y[M+1]\mid x[M+1],\theta_{Y\mid X})P(\theta_{Y\mid X}\mid\mathcal{D})d\theta_{Y\mid X}\right).}}\end{array}
$$ 

In the second step, we use the fact that the double integral of two unrelated functions is the product of the integrals. That is: 

$$
\int\int f(x)g(y)d x d y=\left(\int f(x)d x\right)\left(\int g(y)d y\right).
$$ 

Thus, we can solve the prediction problem for the two variables $X$ and $Y$ separately. 

The same line of reasoning easily applies to the general case, and thus we can see that, in the setting of proposition 17.5, we have 

$$
\begin{array}{l}{{P(X_{1}[M+1],\ldots,X_{n}[M+1]\mid\mathcal{D})=}}\\ {{\displaystyle\prod_{i}\int P(X_{i}[M+1]\mid\mathrm{Pa}_{X_{i}}[M+1],\mathbf{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}})P(\theta_{X_{i}\mid\mathrm{Pa}_{X_{i}}}\mid\mathcal{D})d\mathbf{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}}.}}\end{array}
$$ 

We see that we can solve the prediction problem for each CPD independently and then combine the results. 

We stress that the discussion so far was based on the assumption that the priors over parameters for diferent CPDs are independent. We see that, when learning from complete data, this assumption alone sufces to get a decomposition of the learning problem to several “local” problems, each one involving one CPD. 

At this stage it might seem that the Bayesian framework introduces new complications that did not appear in the MLE setup. Note, however, that in deriving the MLE decomposition, we used the property that we can choose parameters for one CPD independently of the others. Thus, we implicitly made a similar assumption to get decomposition. The Bayesian treatment forces us to make such assumptions explicit, allowing us to more carefully evaluate their validity. We view this as a beneﬁt of the Bayesian framework. 

![](images/2aff6b1ca54744ece992ca56ba27597969024da3c207104b2722b9771c1e8f21.jpg) 
Figure 17.8 Meta-network for IID samples from a network $X\rightarrow Y$ with local parameter indepen- dence. (a) Plate model. (b) Ground Bayesian network. 

# 17.4.2 Local Decomposition 

Based on the preceding discussion, we now need to solve localized Bayesian estimation problems to get a global Bayesian solution. We now examine this localized estimation task for table-CPDs. The case for tree-CPDs is treated in section 17.5.2. 

Consider, for example, the learning setting described in ﬁgure 17.7, where we take both $X$ and $Y$ to be binary. As we have seen, we need to represent the posterior $\theta_{X}$ and $\theta_{Y\mid X}$ given the data. We already know how to deal with the posterior over $\theta_{X}$ . If we use a Dirichlet prior over $\theta_{X}$ , then the posterior $P(\theta_{X}\mid x[1],\ldots,x[M])$ is also represen a Dirichlet distribution. 

A less obvious question is how to deal with the posterior over $\theta_{Y\mid X}$ . If we are learning table- | CPDs, this parameter vector contains four parameters $\theta_{y^{0}|x^{0}},\cdot\cdot\cdot,\theta_{y^{1}|x^{1}}$ . In our discussion of maximum likelihood estimation, we saw how the local likelihood over these parameters can be further decomposed into two terms, one over the parameters $\theta_{Y\mid x^{0}}$ and one over the parameters $\theta_{Y\mid x^{1}}$ . Do we have a similar phenomenon in the Bayesian setting? 

We start with the prior over $\theta_{Y\mid X}$ . One obvious choice is a Dirichlet prior over $\theta_{Y\mid x^{1}}$ and another over $\theta_{Y\mid x^{0}}$ . More precisely, we have 

$$
P(\pmb\theta_{Y\mid X})=P(\pmb\theta_{Y\mid x^{1}})P(\pmb\theta_{Y\mid x^{0}}),
$$ 

where each of the terms on the right is a Dirichlet prior. Thus, in this case, we assume that the two groups of parameters are independent a priori. 

This independence assumption, in efect, allows us to replace the node $\theta_{Y\mid X}$ in ﬁgure 17.7 with two nodes, $\theta_{Y\mid x^{1}}$ and $\theta_{Y\mid x^{0}}$ that are both roots (see ﬁgure 17.8). What can we say about the posterior distribution of these parameter groups? At ﬁrst, it seems that the two are dependent on each other given the data. Given an observation of $y[m]$ , the path 

$$
\theta_{Y\mid x^{0}}\rightarrow Y[m]\leftarrow\theta_{Y\mid x^{1}}
$$ 

is active (since we observe the sink of a $\mathrm{V}\cdot$ -structure), and thus the two parameters are not d-separated. 

This, however, is not the end of the story. We get more insight if We examine how $y[m]$ depends on the two parameters. Clearly, 

$$
P(y[m]=y\mid x[m],\pmb\theta_{Y|x^{0}},\pmb\theta_{Y|x^{1}})=\left\{\begin{array}{l l}{\pmb\theta_{y|x^{0}}}&{\mathrm{if}\,\,x[m]=x^{0}}\\ {\pmb\theta_{y|x^{1}}}&{\mathrm{if}\,\,x[m]=x^{1}.}\end{array}\right.
$$ 

We see that $y[m]$ does not depends on the value of $\theta_{Y\mid x^{0}}$ when $x[m]=x^{1}$ . This example is an instance of the same type of context speciﬁc independence that we discussed in example 3.7. As discussed in section 5.3, we can perform a more reﬁned form of $\mathrm{d}$ -separation test in such a situation by removing arcs that are ruled inactive in particular contexts. For the CPD of $y[m]$ , we see that once we observe the value of $x[m]$ , one of the two arcs into $y[m]$ is inactive. If $x[m]=x^{0}$ , then the arc $\theta_{Y\mid x^{1}}\,\to\,y[m]$ is inactive, and if $x[m]=x^{1}$ , then $\theta_{Y\mid x^{0}}\rightarrow y[m]$ is inactive. In either case, the v-structure $\theta_{Y\mid x^{0}}\rightarrow y[m]\leftarrow\theta_{Y\mid x^{1}}$ is removed. Since this removal occurs for every $m\,=\,1,\cdot\cdot\cdot,M$ , we conclude that no active path exists between $\theta_{Y\mid x^{0}}$ and $\theta_{Y\mid x^{1}}$ and thus, the two are independent given the observation of the data. In other words, we can write 

$$
P(\pmb\theta_{Y\mid X}\mid\mathcal D)=P(\pmb\theta_{Y\mid x^{1}}\mid\mathcal D)P(\pmb\theta_{Y\mid x^{0}}\mid\mathcal D).
$$ 

Suppose that $P(\pmb{\theta}_{Y\mid x^{0}})$ is a Dirichlet prior with hyperparameters $\alpha_{y^{0}\mid x^{0}}$ and $\alpha_{y^{1}\mid x^{0}}$ . As in our discussion of the local decomposition for the likelihood function in section 17.2.3, we have that the likelihood terms that involve $\theta_{Y\mid x^{0}}$ are those that measure the probability of $P(y[m]\mid x[m],\theta_{Y\mid X})$ when $x[m]=x^{0}$ . Thus, we can decompose the joint distribution over parameters and data as follows: 

$$
\begin{array}{r c l}{P(\pmb{\theta},\mathcal{D})}&{=}&{P(\pmb{\theta}_{X})L_{X}(\pmb{\theta}_{X}:\mathcal{D})}\\ &&{P(\pmb{\theta}_{Y|x^{1}})\displaystyle\prod_{m:x[m]=x^{1}}P(y[m]\mid x[m]:\pmb{\theta}_{Y|x^{1}})}\\ &&{P(\pmb{\theta}_{Y|x^{0}})\displaystyle\prod_{m:x[m]=x^{0}}P(y[m]\mid x[m]:\pmb{\theta}_{Y|x^{0}}).}\end{array}
$$ 

Thus, this joint distribution is a product of three separate joint distributions with a Dirichlet prior for some multinomial parameter and data drawn from this multinomial. Our analysis for updating a single Dirichlet now applies, and we can conclude that the posterior $P(\pmb{\theta}_{Y\mid x^{0}}\mid\mathcal{D})$ is Dirichlet with hyperparameters $\alpha_{y^{0}|x^{0}}+M[x^{0},y^{0}]$ and $\alpha_{y^{1}|x^{0}}+M[x^{0},y^{1}]$ . 

We can generalize this discussion to arbitrary networks. 

Let $X$ be a variable with parents $U$ . We say that the prior $P(\pmb{\theta}_{X|U})$ satisﬁes local parameter independence if 

$$
P(\pmb\theta_{X|U})=\prod_{u}P(\pmb\theta_{X|u}).
$$ 

The same pattern of reasoning also applies to the general case. 

Let $\mathcal{D}$ be a complete data set for $\mathcal{X}$ , let $\mathcal{G}$ be a network structure over these variables with table- CPDs. If the prior $P(\theta)$ satisﬁes global and local parameter independence, then 

$$
P(\pmb\theta\mid\mathcal D)=\prod_{i}\prod_{\mathrm{pa}_{X_{i}}}P(\pmb\theta_{X_{i}\mid\mathrm{pa}_{X_{i}}}\mid\mathcal D).
$$ 

Moreover, if $P(\pmb{\theta}_{X|\pmb{u}})$ is a Dirichlet prior with hyperparameters $\alpha_{x^{1}|u},\cdot\cdot\cdot,\alpha_{x^{K}|u}$ , then the $P(\pmb{\theta}_{X\mid\pmb{u}_{-}}\mid\mathcal{D})$ is a Dirichlet distribution with hyperparameters $\alpha_{x^{1}|\pmb{u}}+\dot{M}[\pmb{u},x^{1}],\dots,$ $\alpha_{x^{K}|\pmb{u}}+M[\pmb{u},x^{K}]$ . 

As in the case of a single multinomial, this result induces a predictive model in which, for the next instance, we have that 

$$
P(X_{i}[M+1]=x_{i}\mid U[M+1]=\pmb{u},\mathcal{D})=\frac{\alpha_{x_{i}\mid\pmb{u}}+M[x_{i},\pmb{u}]}{\sum_{i}\alpha_{x_{i}\mid\pmb{u}}+M[x_{i},\pmb{u}]}.
$$ 

Plugging this result into equation (17.12), we see that for computing the probability of a new instance, we can use a single network parameterized as usual, via a set of multinomials, but ones computed as in equation (17.13). 

# 17.4.3 Priors for Bayesian Network Learning 

It remains only to address the question of assessing the set of parameter priors required for a Bayesian network. In a general Bayesian network, each node $X_{i}$ has a set of multinomial distributions $\theta_{X_{i}|\mathrm{pa}_{X_{i}}}$ , one for each instantiation $\mathrm{pa}_{X_{i}}$ of $X_{i}$ ’s parents $\operatorname{Pa}_{X_{i}}$ . Each of these parameters will have a separate Dirichlet prior, governed by hyperparameters 

$$
\begin{array}{r}{\alpha_{X_{i}\mid\mathrm{pa}_{X_{i}}}=(\alpha_{x_{i}^{1}\mid\mathrm{pa}_{X_{i}}},.\,.\,,\alpha_{x_{i}^{K_{i}}\mid\mathrm{pa}_{X_{i}}}),}\end{array}
$$ 

where $K_{i}$ is the number of values of $X_{i}$ . 

We can, of course, ask our expert to assign values to each of these hyperparameters based on his or her knowledge. This task, however, is rather unwieldy. Another approach, called the K2 prior, is to use a ﬁxed prior, say $\alpha_{x_{i}^{j}|\mathrm{pa}_{X_{i}}}=1$ , for all hyperparameters in the network. As we discuss in the next chapter, this approach has consequences that are conceptually unsatisfying; see exercise 18.10. 

A common approach to addressing the speciﬁcation task uses the intuitions we described in our discussion of Dirichlet priors in section 17.3.1. As we showed, we can think of the hyperparameter $\alpha_{x^{k}}$ as an imaginary count in our prior experience. This intuition suggests the following representation for a prior over a Bayesian network. Suppose we have an imaginary data set ${\mathcal{D}}^{\prime}$ of “prior” examples. Then, we can use counts from this imaginary data set as hyperparameters. More speciﬁcally, we set 

$$
\begin{array}{r}{\alpha_{x_{i}|\mathrm{pa}_{X_{i}}}=\alpha[x_{i},\mathrm{pa}_{X_{i}}],}\end{array}
$$ 

where $\alpha[x_{i},\mathrm{pa}_{X_{i}}]$ is the number of times $X_{i}=x_{i}$ and $\mathrm{Pa}_{X_{i}}=\mathrm{pa}_{X_{i}}$ in ${\mathcal{D}}^{\prime}$ . We can easily see that prediction with this setting of hyperparameters is equivalent to MLE prediction from the combined data set that contains instances of both $\mathcal{D}$ and ${\mathcal{D}}^{\prime}$ . 

One problem with this approach is that it requires storing a possibly large data set of pseudo- instances. Instead, we can store the size of the data set $\alpha$ and a representation $P^{\prime}(X_{1},.\,.\,.\,,X_{n})$ of the frequencies of events in this prior data set. If $P^{\prime}(X_{1},.\,.\,.\,,X_{n})$ is the distribution of events in ${\mathcal{D}}^{\prime}$ , then we get that 

$$
\alpha_{x_{i}|\mathrm{pa}_{X_{i}}}=\alpha\cdot P^{\prime}(x_{i},\mathrm{pa}_{X_{i}}).
$$ 

BDe prior 

ICU-Alarm How do we represent $P^{\prime}?$ Clearly, one natural choice is via a Bayesian network. Then, we can use Bayesian network inference to efciently compute the quantities $P^{\prime}(x_{i},\mathrm{pa}_{X_{i}})$ . Note that $P^{\prime}$ does not have to be structured in the same way as the network we learn (although it can be). It is, in fact, quite common to deﬁne $P^{\prime}$ as a set of independent marginals over the $X_{i}$ ’s. A prior that can be represented in this manner (using $\alpha$ and $P^{\prime}$ ) is called a BDe prior . Aside from being philosophically pleasing, it has some additional beneﬁts that we will discuss in the next chapter. 

Box 17.C — Case Study: Learning the ICU-Alarm Network. To give an example of the tech- niques described in this chapter, we evaluate them on a synthetic example. Figure 17.C.1 shows the graph structure of the real-world ICU-Alarm Bayesian network, hand-constructed by an expert, for monitoring patients in an Intensive Care Unit (ICU). The network has 37 nodes and a total of 504 parameters. We want to evaluate the ability of our parameter estimation algorithms to reconstruct the network parameters from data. 

We generated a training set from the network, by sampling from the distribution speciﬁed by the network. We then gave the algorithm only the (correct) network structure, and the generated data, and measured the ability of our algorithms to reconstruct the parameters. We tested the MLE approach, and several Bayesian approaches. All of the approaches used a uniform prior mean, but diferent prior strengths $\alpha$ . 

In performing such an experiment, there are many ways of measuring the quality of the learned network. One possible measure is the diference between the original values of the model parameters and the estimated ones. A related approach measures the distance between the original CPDs and the learned ones (in the case of table-CPDs, these two approaches are the same, but not for general parameter iz at ions). These approaches place equal weights on diferent parameters, regardless of the extent to which they inﬂuence the overall distribution. 

The approach we often take is the one described in section 16.2.1, where we measure the rel- ative entropy between the generating distribution $P^{*}$ and the learned distribution $\tilde{P}$ (see also section 8.4.2). This approach provides a global measure of the extent to which our learned distribu- tion resembles the true distribution. Figure 17.C.2 shows the results for diferent stages of learning. As we might expect, when more instances are available, the estimation is better. The improvement is drastic in early stages of learning, where additional instances lead to major improvements. When the number of instances in our data set is larger, additional instances lead to improvement, but $a$ smaller one. 

More surprisingly, we also see that the MLE achieves the poorest results, a consequence of its extreme sensitivity to the speciﬁc training data used. The lowest error is achieved with a very weak prior — $\alpha\,=\,5$ — which is enough to provide smoothing. As the strength of the prior grows, it starts to introduce a bias, not giving the data enough importance. Thus, the error of the estimated probability increases. However, we also note that the efect of the prior, even for $\alpha=50$ , disappears reasonably soon, and all of the approaches converge to the same line. Interestingly, the diferent 

![](images/e173e3084fe1c3d1b5eea82aa589a857031d802cb78619c105dd46a9e07d58c2.jpg) 
Figure 17.C.1 — The ICU-Alarm Bayesian network. 

Bayesian approaches converge to this line long before the MLE approach. Thus, at least in this example, an overly strong bias provided by the prior is still a better compromise than the complete lack of smoothing of the MLE approach. 

![](images/52471b454317def300edbf73029d93b67f3154d972ebdfc8faaab6574d16d55b.jpg) 
Figure 17.C.2 — Learning curve for parameter estimation for the ICU-Alarm network Relative en- tropy to true model as the amount of training data grows, for diferent values of the prior strength $\alpha$ . 

# 17.4.4 MAP Estimation $\star$ 

normal-Gamma distribution 

MAP estimation Our discussion in this chapter has focused solely on Bayesian estimation for multinomial CPDs. Here, we have a closed form solution for the integral required for Bayesian prediction, and thus we can perform it efciently. In many other representations, the situation is not so simple. In some cases, such as the noisy-or model or the logistic CPDs of section 5.4.2, we do not have a conjugate prior or a closed-form solution for the Bayesian integral. In those cases, Bayesian prediction requires numerical solutions for high-dimensional integrals. In other settings, such as the linear Gaussian CPD, we do have a conjugate prior (the normal-Gamma distribution ), but we may prefer other priors that ofer other desirable properties (such as the sparsity-inducing Laplacian prior described in section 20.4.1). 

When a full Bayesian solution is impractical, we can resort to using maximum a posteriori (MAP) estimation . Here, we search for parameters that maximize the posterior probability : 

$$
{\tilde{\theta}}=\arg\operatorname*{max}_{\theta}\log P(\theta\mid{\mathcal{D}}).
$$ 

When we have a large amount of data, the posterior is often sharply peaked around its maximum $\tilde{\theta}$ . In this case, the integral 

$$
P(X[M+1]\mid\mathcal{D})=\int P(X[M+1]\mid\theta)P(\theta\mid\mathcal{D})d\theta
$$ 

regularization will be roughly $P(X[M+1]\mid\tilde{\pmb\theta})$ . More generally, we can view the MAP estimate as a way of using the prior to provide regularization over the likelihood function: 

$$
\begin{array}{r c l}{\arg\operatorname*{max}_{\boldsymbol{\theta}}\log P(\boldsymbol{\theta}\mid\mathcal{D})}&{=}&{\arg\operatorname*{max}_{\boldsymbol{\theta}}\log\left(\displaystyle\frac{P(\boldsymbol{\theta})P(\mathcal{D}\mid\boldsymbol{\theta})}{P(\mathcal{D})}\right)}\\ &{=}&{\arg\operatorname*{max}_{\boldsymbol{\theta}}\left(\log P(\boldsymbol{\theta})+\log P(\mathcal{D}\mid\boldsymbol{\theta})\right).}\end{array}
$$ 

That is, $\tilde{\pmb{\theta}}$ is the maximum of a function that sums together the log-likelihood function and $\log{P(\theta)}$ . This latter term takes into account the prior on diferent parameters and therefore biases the parameter estimate away from undesirable parameter values (such as those involving conditional probabilities of 0 ) when we have few learning instances. When the number of samples is large, the efect of the prior becomes negligible, since $\ell(\pmb\theta:{\mathcal D})$ grows linearly with the number of samples whereas the prior does not change. 

Because our parameter priors are generally well behaved, MAP estimation is often no harder than maximum likelihood estimation, and is therefore often applicable in practice, even in cases where Bayesian estimation is not. Importantly, however, it does not ofer all of the same beneﬁts as a full Bayesian estimation. In particular, it does not attempt to represent the shape of the posterior and thus does not diferentiate between a ﬂat posterior and a sharply peaked one. As such, it does not give us a sense of our conﬁdence in diferent aspects of the parameters, and the predictions do not average over our uncertainty. This approach also sufers from issues regarding representation independence; see box 17.D. 

Box 17.D — Concept: Representation Independence. One important property we may want of an estimator is representation independence . To understand this concept better, suppose that in our thumbtack example, we choose to use a parameter $\eta$ , so that $\textstyle P^{\prime}(X=H\mid\eta)={\frac{1}{1+e^{-\eta}}}$ . We have that $\begin{array}{r}{\eta=\log\frac{\theta}{1-\theta}}\end{array}$ wher $\theta$ is the parameter we used earlier. Thus, there is a one-to-one − correspondence between a choice θ and a choice $\eta$ . Although one choice of parameters might seem more natural to us than another, there is no formal reason why we should prefer one over the other, since both can represent exactly the same set of distributions. 

More generally, a re parameter iz ation of $^a$ given family is a new set of parameter values $\eta$ in $^a$ space $\Upsilon$ and a mapping from the new parameters to the original one, that is, from $\eta$ to $\theta(\eta)$ so that $P(\cdot\mid\eta)$ in the new parameter iz ation is equal to $P(\cdot\mid\theta(\eta))$ in the original parameter iz ation. In addition, we require that the re parameter iz ation maintain the same set of distributions, that is, for each choice of $\theta$ there is $\eta$ such that $P(\cdot\mid\eta)=P(\cdot\mid\theta)$ . 

This concept immediately raises the question as to whether the choice of representation can impact our estimates. While we might prefer $^a$ particular way of parameter iz ation because it is more intuitive or interpretable, we may not want this choice to bias our estimated parameters. 

Fortunately, it is not difcult to see that maximum likelihood estimation is insensitive to repa- rameterization. If we have two diferent ways to represent the same family of the distribution, then the distributions in the family that maximize the likelihood using one parameter iz ation also maximize the likelihood with the other parameter iz ation. More precisely, if $\hat{\eta}$ is MLE, then the matching parameter values $\theta(\hat{\eta})$ are also MLE when we consider the likelihood function in the $\theta$ space. This property is a direct consequence of the fact that the likelihood function is a function of the distribution induced by the parameter values, and not of the actual parameter values. 

The situation with Bayesian inference is subtler. Here, instead of identifying the maximum parameter value, we now perform integration over all possible parameter values. Naively, it seems that such an estimation is more sensitive to the parameter iz ation than MLE, which depends only on the maximum of the likelihood surface. However, a careful choice of prior can account for the representation change and thereby lead to representation independence. Intuitively, if we consider a re parameter iz ation $\eta$ with a function $\theta(\eta)$ mapping to the original parameter space, then we would like the prior on $\eta$ to maintain the probability of events. That is, 

$$
P(A)=P(\{\pmb\theta(\eta):\eta\in A\}),\;\forall A\subset\Upsilon.
$$ 

This constraint implies that the prior over diferent regions of parameters is maintained. Under this assumption, Bayesian prediction will be identical under the two parameter iz at ions. 

We illustrate the notion of a reparameterized prior in the context of a Bernoulli distribution: 

Example 17.9 Consider a Beta prior over the parameter $\theta$ of a Bernoulli distribution: 

$$
P(\theta:\alpha_{0},\alpha_{1})=c\theta^{\alpha_{1}-1}(1-\theta)^{\alpha_{0}-1},
$$ 

where $c$ is the normalizing constant described in deﬁnition 17.3. Recall (example 8.5) that the natural parameter for a Bernoulli distribution is 

$$
\eta=\log{\frac{\theta}{1-\theta}}
$$ 

with the transformation 

$$
\theta=\frac{1}{1+e^{-\eta}},\quad1-\theta=\frac{1}{1+e^{\eta}}.
$$ 

What is the prior distribution on $\eta$ ? To preserve the probability of events, we want to make sure that for every interval $[a,b]$ 

$$
\int_{a}^{b}c\theta^{\alpha_{1}-1}(1-\theta)^{\alpha_{0}-1}d\theta=\int_{\log\frac{a}{1-a}}^{\log\frac{b}{1-b}}P(\eta)d\eta.
$$ 

To do so, we need to perform a change of variables. Using the relation between $\eta$ and $\theta$ , we get 

$$
d\eta=\frac{1}{\theta(1-\theta)}d\theta.
$$ 

Plugging this into the equation, we can verify that an appropriate prior is: 

$$
P(\eta)=c\left(\frac{1}{1+e^{-\eta}}\right)^{\alpha_{1}}\left(\frac{1}{1+e^{\eta}}\right)^{\alpha_{0}},
$$ 

where $c$ is the same constant as before. This means that the prior on $\eta$ , when stated in terms of $\theta$ , is $\theta^{\alpha_{1}}(1-\theta)^{\alpha_{0}}$ , in contrast to equation (17.16). At ﬁrst this disc pancy seems like a contradiction. However, we have to remember that the transformation from θ to $\eta$ takes the region $[0,1]$ and stretches it to the whole real line. Thus, the matching prior cannot be uniform. 

This example demonstrates that a uniform prior, which we consider to be unbiased or uninfor- mative, can seem very diferent when we consider a diferent parameter iz ation. 

Thus, both MLE and Bayesian estimation (when carefully executed) are representation-independent. This property, unfortunately, does not carry through to MAP estimation. Here we are not interested in the integral over all parameters, but rather in the density of the prior at diferent values of the parameters. This quantity does change when we reparameterize the prior. 

Consider the setting of example $l7.9$ and develop the MAP parameters for the priors we considered there. When we use the $\theta$ parameter iz ation, we can check that 

$$
\tilde{\pmb\theta}=\arg\operatorname*{max}_{\pmb\theta}\log P(\theta)=\frac{\alpha_{1}-1}{\alpha_{0}+\alpha_{1}-2}.
$$ 

On the other hand, 

$$
\tilde{\eta}=\arg\operatorname*{max}_{\eta}\log P(\eta)=\log\frac{\alpha_{1}}{\alpha_{0}}.
$$ 

To compare the two, we can transform $\tilde{\eta}$ to $\theta$ representation and ﬁnd 

$$
\theta(\tilde{\eta})=\frac{\alpha_{1}}{\alpha_{0}+\alpha_{1}}.
$$ 

In other words, the MAP of the $\eta$ parameter iz ation gives the same predictions as the mean param- eterization if we do the full Bayesian inference. 

# 

Thus, MAP estimation is more sensitive to choices in formalizing the likelihood and the prior than MLE or full Bayesian inference. This suggests that the MAP parameters involve, to some extent, an arbitrary choice. Indeed, we can bias the MAP toward diferent solutions if we construct a speciﬁc re parameter iz ation where the density is particularly large in speciﬁc regions of the parameter space. The parameter iz ation dependency of MAP is a serious caveat we should be aware of. 

# 17.5 Learning Models with Shared Parameters 

shared parameters In the preceding discussion, we focused on parameter estimation for Bayesian networks with table-CPDs. In this discussion, we made the strong assumption that the parameters for each conditional distribution $P(X_{i}\mid\textbf{\em u}_{i})$ can be estimated separately from parameters of other conditional distributions. In the Bayesian case, we also assumed that the priors on these distributions are independent. This assumption is a very strong one, which often does not hold in practice. In real-life systems, we often have shared parameters : parameters that occur in multiple places across the network. In this section, we discuss how to perform parameter estimation in networks where the same parameters are used multiple times. 

Analogously to our discussion of parameter estimation, we can exploit both global and local structure. Global structure occurs when the same CPD is used across multiple variables in the network. This type of sharing arises naturally from the template-based models of chapter 6. Local structure is ﬁner-grained, allowing parameters to be shared even within a single CPD; it arises naturally in some types of structured CPDs. We discuss each of these scenarios in turn, focusing on the simple case of MLE. 

We then discuss the issues arising when we want to use Bayesian estimation. Finally, we dis- cuss the hierarchical Bayes framework, a “softer” version of parameter sharing, where parametersare encouraged to be similar but do not have to be identical. 

# 17.5.1 Global Parameter Sharing 

Let us begin with a motivating example. 

Example 17.11 Let us return to our student, who is now taking two classes $c_{1},c_{2}$ , each of which is associated with a Difculty variable, $D_{1},D_{2}$ . We assume that the grade $G_{i}$ of our student in class $c_{i}$ depends on his intelligence and the class difculty. Thus, we model $G_{i}$ as having $I$ and $D_{i}$ as parents. Moreover, we might assume that these grades share the same conditional distribution. That is, the probability that an intelligent student receives an “A” in an easy class is the same regardless of the identity of the particular class. Stated diferently, we assume that the difculty variable summarizes all the relevant information about the challenge the class presents to the student. 

How do we formalize this assumption? A straightforward solution is to require that for all choices of grade $g$ , difculty $d$ and intelligence $i$ , we have that 

$$
P(G_{1}=g\mid D_{1}=d,I=i)=P(G_{2}=g\mid D_{2}=d,I=i).
$$ 

Importantly, this assumption does not imply that the grades are necessarily the same, but rather that the probability of getting a particular grade is the same if the class has the same difculty. 

This example is simply an instance of a network induced by the simple plate model described in example 6.11 (using $D_{i}$ to encode $D(c_{i})$ and similarly for $G_{i}$ ). Thus, as expected, template models give rise to shared parameters. 

# 17.5.1.1 Likelihood Function with Global Shared Parameters 

global parameter sharing 

As usual, the key to parameter estimation lies in understanding the structure of the likelihood function. To analyze this structure, we begin with some notation. Consider a network structure $\mathcal{G}$ over et of variables $\mathcal{X}\,=\,\{X_{1},.\,.\,.\,,X_{n}\}$ , parameterized by a set of parameters $\theta$ . Each variable $X_{i}$ is associated with a CP $P(X_{i}\mid U_{i},\theta)$ . Now, rather than assume that each such CPD has its own parameter iz ation $\theta_{X_{i}|U_{i}}$ , we assume that we have a certain set of shared | parameters that are used by multiple variables in the network. Thus, the sharing of parameters is global , over the entire network. 

More precisely, we assume that $\theta$ is partitioned into disjoint subsets $\boldsymbol{\theta}^{1},\dots,\boldsymbol{\theta}^{K}$ ; with each su subs ociate a set of variables $\mathcal{V}^{k}\subset\mathcal{X}$ such that $\mathcal{V}^{1},\dots,\mathcal{V}^{K}$ is a disjoint partition of . For $X_{i}\in\mathcal{V}^{k}$ , we assume that the CPD of $X_{i}$ depends only on θ $\theta^{k}$ ; that is, 

$$
P(X_{i}\mid U_{i},\theta)=P(X_{i}\mid U_{i},\theta^{k}).
$$ 

Moreover, we assume that the form of the CPD is the same for all $X_{i},X_{j}\in\mathcal{V}^{k}$ ; that is, 

$$
P(X_{i}\mid U_{i},\theta^{k})=P(X_{j}\mid U_{j},\theta^{k}).
$$ 

We note that this last statement makes sense only if $V a l(X_{i})\;=\;V a l(X_{j})$ and $V a l({\cal U}_{i})\;=\;\;$ $V a l(U_{j})$ . To avoi mbig s notation, for any variable $X_{i}\,\in\,\mathcal{V}^{k}$ , we use $y_{k}^{l}$ to range over possible values of $X_{i}$ and $\pmb{w}_{k}^{l}$ to range over the possible values of its parents. 

Consider the decomposition of the probability distribution in this case: 

$$
{\begin{array}{r c l}{P(X_{1},\ldots,X_{n}\mid\theta)}&{=}&{\displaystyle\prod_{i=1}^{n}P(X_{i}\mid\mathrm{Pa}_{X_{i}},\theta)}\\ &{=}&{\displaystyle\prod_{k=1}^{K}\prod_{X_{i}\in{\mathcal{V}}^{k}}P(X_{i}\mid\mathrm{Pa}_{X_{i}},\theta)}\\ &{=}&{\displaystyle\prod_{k=1}^{K}\prod_{X_{i}\in{\mathcal{V}}^{k}}P(X_{i}\mid\mathrm{Pa}_{X_{i}},\theta^{k}),}\end{array}}
$$ 

where the second equality follows from the fact that $\mathcal{V}^{1},\dots,\mathcal{V}^{K}$ deﬁnes a partition of $\mathcal{X}$ , and the third equality follows from equation (17.17). 

Now, let $\mathcal{D}$ be some assignment of values to the variables $X_{1},\dots,X_{n}$ ; our analysis can easily handle multiple IID instances, as in our earlier discussion, but this extension only clutters the notation. We can now write 

$$
L(\pmb\theta:\mathcal D)=\prod_{k=1}^{K}\prod_{X_{i}\in\mathcal V^{k}}P(x_{i}\mid\pmb u_{i},\pmb\theta^{k}).
$$ 

This expression is identical to the one we used in section 17.2.2 for the case of IID instances. There, for each set of parameters, we had multiple instances $\{(x_{i}[m],\mathbf{u}_{i}[m])\}_{m=1}^{M}$ , all of which were generated from the same conditional distribution. Here, we have multiple instances $\{(x_{i},\mathbf{u}_{i})\}_{X_{i}\in\mathcal{V}^{k}}$ , all of which are also generated from the same conditional distribution. Thus, it appears that we can use the same analysis as we did there. 

To provide a formal derivation, consider ﬁrst the case of table-CPDs. Here, our parameteriza- tion is a set of multinomial parameters $\theta_{y_{k}|w_{k}}^{k}$ , where we recall that $y_{k}$ ranges over the possible | values of each of the variables $X_{i}\,\in\,\mathcal{V}^{k}$ and $\mathbf{\nabla}w_{k}$ over the possible value assignments to its parents. Using the same derivation as in section 17.2.2, we can now write: 

$$
\begin{array}{r c l}{L(\pmb{\theta}:\mathcal{D})}&{=}&{\displaystyle\prod_{k=1}^{K}\prod_{y_{k},\pmb{w_{k}}}\quad\quad\prod_{X_{i}\in\mathcal{V}^{k}\ :\quad}\theta_{y_{k}|\pmb{w_{k}}}^{k}}\\ &&{\displaystyle\prod_{x_{i}=y_{k},\pmb{u_{i}}=\pmb{w_{k}}}}\\ &{=}&{\displaystyle\prod_{k=1}^{K}\prod_{y_{k},\pmb{w_{k}}}\big(\theta_{y_{k}|\pmb{w_{k}}}^{k}\big)^{\Tilde{M}_{k}[y_{k},\pmb{w_{k}}]},}\end{array}
$$ 

where we now have a new deﬁnition of our counts: 

$$
{\check{M}}_{k}[y_{k},{\pmb w}_{k}]=\sum_{X_{i}\in\mathcal{V}^{k}}{\pmb I}\{x_{i}=y_{k},{\pmb u}_{i}={\pmb w}_{k}\}.
$$ 

In other words, we now use aggregate sufcient statistics , which combine sufcient statistics from multiple variables across the same network. 

Given this formulation of the likelihood, we can now obtain the maximum likelihood solution for each set of shared parameters to get the estimate 

$$
\hat{\theta}_{y_{k}|\pmb{w}_{k}}^{k}=\frac{\check{M}_{k}[y_{k},\pmb{w}_{k}]}{\check{M}_{k}[\pmb{w}_{k}]}.
$$ 

Thus, we use the same estimate as in the case of independent parameters, using our aggregate sufcient statistics. Note that, in cases where our variables $X_{i}$ have no parents, ${\pmb w}_{k}$ is the empty tuple $\varepsilon$ . In this case, $\check{M}_{k}[\varepsilon]$ is the number of variables $X_{i}$ in $\mathcal{V}^{k}$ . 

linear exponential family 

This aggregation of sufcient statistics applies not only to multinomial distributions. Indeed, for any distribution in the linear exponential family , we can perform precisely the same aggre- gation of sufcient statistics over the variables in $\mathcal{V}^{k}$ . The result is a likelihood function in the same form as we had before, but written in terms of the aggregate sufcient statistics rather than the sufcient statistics for the individual variables. We can then perform precisely the same maximum likelihood estimation process and obtain the same form for the MLE, but using the aggregate sufcient statistics. (See exercise 17.14 for another simple example.) 

Does this aggregation of sufcient statistics make sense? Returning to our example, if we treat the grade of the student in each class as independent sample from the same parameters, then each data instance provides us with two independent samples from this distribution. It is important to clarify that, although the grades of the student are dependent on his intelligence, the samples are independent samples from the same distribution. More precisely, if $D_{1}=D_{2}$ , then both $G_{1}$ and $G_{2}$ are governed by the same multinomial distribution, and the student’s grades are two independent samples from this distribution. 

Thus, when we share parameters, multiple observations from within the same network con- tribute to the same sufcient statistic, and thereby help estimate the same parameter. Reducing the number of parameters allows us to obtain parameter estimates that are less noisy and closer to the actual generating parameters. This beneﬁt comes at a price, since it requires us to make an assumption about the domain. If the two distributions with shared parameters are actually diferent, the estimated parameters will be a (weighted) average of the estimate we would have had for each of them separately. When we have a small number of instances, that approxima- tion may still be beneﬁcial, since each of the separate estimates may be far from its generating parameters, owing to sample noise. When we have more data, however, the shared parame- ters estimate will be worse than the individual ones. We return to this issue in section 17.5.4, where we provide a solution that allows us to gradually move away from the shared parameter assumption as we get more data. 

# 17.5.1.2 Parameter Estimation for Template-Based Models 

As we mentioned, the template models of chapter 6 are speciﬁcally designed to encode global parameter sharing. Recall that these representations involve a set of template-level parameters, each of which is used multiple times when the ground network is deﬁned. For the purpose of our discussion, we focus mostly on plate models, since they are the simplest of the (directed) template-based representations and serve to illustrate the main points. 

As we discussed, it is customary in many plate models to explicitly encode the parameter sharing by including, in the model, random variables that encode the model parameters. This approach allows us to make clear the exact structure of the parameter sharing within the model. 

![](images/154f56988f6be4e76e641f4d895e55e48a69e8f7ae84e43ec55a7160b6e069c2.jpg) 
Figure 17.9 Two plate models for the University example, with explicit parameter variables. (a) Model where all parameters are global. (b) Model where the difculty is a course-speciﬁc parameter rather than a discrete random variable. 

As we mentioned, when parameters are global and shared across over all ground variables derived from a particular template variables, we may choose (purely as a notational convenience) not to include the parameters explicitly in the model. We begin with this simple setting, and then we extend it to the more general case. 

Example 17.12 Figure 17.9a is a representation of the plate model of example 6.11, except that we now explicitly encode the parameters as variables within the model. In this representation, we have made it clear that there is only a single parameter $\theta_{G|I,D}^{G}$ , which is the parent of the variables within the plates. | Thus, as we can see, the same parameters are used in every CPD $P(G(s,c)\mid I(s),D(c))$ in the ground network. 

When all of our parameters are global, the sharing structure is very simple. Let $A(U_{1},\dots,U_{k})$ be any attribute in the set of template attributes $\aleph$ . Recall from deﬁnition 6.10 that this attribute es a ground random variable $A(\gamma)$ for any assignment $\gamma=\left\langle U_{1}\mapsto u_{1},.\,.\,.\,,U_{k}\mapsto u_{k}\right\rangle\in$ $\Gamma_{\kappa}[A]$ . All of these variables share the same CPD, and hence the same parameters. Let $\theta_{A}$ be the parameters for the CPD for $A$ in the template-level model. We can now simply deﬁne $\mathcal{V}^{A}$ to be all of the variables of the form $A(\gamma)$ in the ground network. The analysis of section 17.5.1.1 now applies unchanged. 

Example 17.13 Continuing example 17.12, the likelihood function for an assignment $\xi$ to $^a$ ground network in the University domain would have the form 

$$
\prod_{i}(\theta_{i}^{I})^{\check{M}_{I}[i]}\prod_{d}(\theta_{d}^{D})^{\check{M}_{D}[d]}\prod_{g,i,d}(\theta_{g|i,d}^{G})^{\check{M}_{G}[g,i,d]}.
$$ 

Importantly, the counts are computed as aggregate sufcient statistics, each with its own appropriate set of variables. In particular, 

$$
\check{M}_{I}[i]=\sum_{s\in\mathcal{O}^{\kappa}[\mathsf{S t u d e n t}]}I\{I(s)=i\},
$$ 

whereas 

$$
\check{M}_{G}[g,i,d]=\sum_{\langle s,c\rangle\in\Gamma_{\kappa}[G r a d e]}I\{I(s)=i,D(c)=d,G(s,c)=g\}.
$$ 

For example, the counts for $g^{1},i^{1},d^{1}$ would be the number of all of the student,course pairs $s,c$ such that student $s$ has high intelligence, course c has high difculty, and the grade of s in $c$ is an A. The MLE for $\theta_{g^{1}|i^{1},d^{1}}^{G}$ is the fraction of those students among all (student,course) pairs. | 

To provide a concrete formula in the more general case, we focus on table-CPDs. We can now deﬁne, for any attribute $A(\mathcal{X})$ with parents $B_{1}(U_{1}),\dots,B_{k}(U_{k})$ , the following aggregate sufcient statistics: 

$$
{\tilde{M}}_{A}[a,b_{1},.\,.\,,b_{k}]=\sum_{\gamma\in\Gamma_{\kappa}[A]}{\pmb{I}}\{A(\gamma)=a,B_{1}(\gamma[\boldsymbol{U}_{1}])=b_{1},.\,.\,,\,\,B_{k}(\gamma[\boldsymbol{U}_{k}])=b_{k}\}
$$ 

where $\gamma[U_{j}]$ denotes the subtuple of the assignment to $U$ that corresponds to the logical variables in $U_{j}$ . We can now deﬁne the template-model log-likelihood for a given skeleton: 

$$
\ell(\pmb\theta:\kappa)=\sum_{A\in\aleph}\sum_{a\in V a l(A)}\sum_{\substack{b\in V a l(\mathrm{Pa}_{A})}}\check{M}_{A}[a,\pmb b]\log\theta_{A=a,\mathrm{Pa}_{A}=\pmb b}.
$$ 

From this formula, the maximum likelihood estimate for each of the model parameters follows easily, using precisely the same analysis as before. 

In our derivation so far, we focused on the setting where all of the model parameters are global. However, as we discussed, the plate representation can also encompass more local parameter iz at ions. Fortunately, from a learning perspective, we can reduce this case to the previous one. 

Figure 17.9b represents the setting where each course has its own model for how the course difculty afects the students’ grades. That is, we now have a set of parameters $\overset{\cdot}{\theta}^{G,c}$ , which are used in the CPDs of all of the ground variables $G(c,s)$ for diferent values of $s$ , but there is no sharing between $G(c,s)$ and $G(c^{\prime},s)$ . 

As we discussed, this setting is equivalent to one where we add the course ID c as a parent to the $D$ variable, forcing us to introduce a separate set of parameters for every assignment to c . In this case, the dependence on the speciﬁc course $I D$ subsumes the dependence on the difculty parameter $D$ , which we have (for clarity) dropped from the model. From this perspective, the parameter estimation task can now be handled in exactly the same way as before for the parameters in the (much larger) CPD for $G$ . In efect, this transformation converts global sharing to local sharing, which we will handle. 

There is, however, one important subtlety in scenarios such as this one. Recall that, in general, diferent skeletons will contain diferent objects. Parameters that are speciﬁc to objects in the model do not transfer from one skeleton to another. Thus, we cannot simply transfer the object- speciﬁc parameters learned from one skeleton to another. This limitation is important, since a major beneﬁt of the template-based formalisms is the ability to learn models in one instantiation of the template and use it in other instantiations. Nevertheless, the learned models are still useful in many ways. First, the learned model itself often provides signiﬁcant insight about the objects in the training data. For example, the LDA model of box 17.E tells us, for each of the documents in our training corpus, what the mix of topics in that particular document is. Second, the model parameters that are not object speciﬁc can be transferred to other skeletons. For example, the word multinomials associated with diferent topics that are learned from one document collection can also be used in another, leaving only the new document-speciﬁc parameters to be inferred. 

Although we have focused our formal presentation on plate models, the same analysis also applies to DBNs, PRMs, and a variety of other template-based languages that share parameters. See exercise 17.16 and exercise 17.17 for two examples. 

# 17.5.2 Local Parameter Sharing 

local parameter sharing 

Example 17.15 In the ﬁrst part of this section, we focused on cases where all of the parameter sharing occurred between diferent CPDs. However, we might also have shared parameters that are shared locally , within a single CPD. 

Consider the CPD of ﬁgure 5.4 where we model the probability of the student getting a job based on his application, recommendation letter, and SAT scores. As we discussed there, if the student did not formally apply for a position, then the recruiting company does not have access to the recommendation letter or SAT scores. Thus, for example, the conditional probability distribution $P(J\mid a^{0},s^{0},l^{0})$ is equal to $P(J\mid a^{0},s^{1},l^{1})$ . 

In fact, the representations we considered in section 5.3 can be viewed as encoding parameter sharing for diferent conditional distributions. That is, each of these representations (for example, tree-CPDs) is a language to specify which of the conditional distributions within a CPD are equal to each other. As we saw, these equality constraints had implications in terms of the independence statements encoded by a model and can also in some cases be exploited in inference. (We note that not all forms of local structure can be reduced to a simple set of equality constraints on conditional distributions within a CPD. For example, noisy-or CPDs or generalized linear models combine their parameters in very diferent ways and require very diferent techniques than the ones we discuss in this section.) 

Here, we focus on the setting where the CPD deﬁnes a set of multinomial distributions, but some of these distributions are shared across multiple contexts. In particular, we assume that our grap $\mathcal{G}$ now deﬁnes a set of multinomial distributions that make up CPDs for $\mathcal{G}$ : for each $X_{i}$ and each $u_{i}\in V a l(U_{i})$ we have a multinomial distribution. We can use the tuple $\langle X_{i},\mathbf{\mathbf{u}}_{i}\rangle$ to designate this multinomial distribution, and deﬁne 

$$
\mathcal{D}=\cup_{i=1}^{n}\{\langle X_{i},\mathbf{u}_{i}\rangle:\mathbf{u}_{i}\in V a l(\mathbf{U}_{i})\}
$$ 

to be the set conta n nomial distrib ns in $\mathcal{G}$ . We can now de of shared parameters θ $\theta^{k}$ , k $k\,=\,1,\ldots,K,$ , where each θ $\theta^{k}$ ed with a set D ${\mathcal{D}}^{k}\,\subseteq\,{\mathcal{D}}$ ⊆D of m inomial distributions. As before, we assume that D $\mathcal{D}^{1},\dots,\mathcal{D}^{K}$ D deﬁnes a disjoint parti n of D . We assume that all conditional distributions within D $\mathcal{D}^{k}$ share the same parameters θ $\theta^{k}$ . Thus, we have that if $\left<X_{i},\mathbf{u}_{i}\right>\in\mathcal{D}^{k}$ , then 

$$
P(x_{i}^{j}\mid\mathbf{\em u}_{i})=\theta_{j}^{k}.
$$ 

For this constraint to be coherent, we require that all the multinomial distributions within the same partition have the same set of values: for any $\langle X_{i},\mathbf{\mathit{u}}_{i}\rangle,\langle X_{j},\mathbf{\mathit{u}}_{j}\rangle\,\in\,\mathcal{D}^{k}$ , we have that $V a l(X_{i})=V a l(X_{j})$ . 

Clearly, the case where no parameter is shared can be represented by the trivial partition into singleton sets. However, we can also deﬁne more interesting partitions. 

To capture the tree-CPD of ﬁgure 5.4, we would deﬁne the following partition: 

$$
\begin{array}{r c l}{{\mathcal{D}^{a^{0}}}}&{{=}}&{{\{\langle J,(a^{0},s^{0},l^{0})\rangle,\langle J,(a^{0},s^{0},l^{1})\rangle,\langle J,(a^{0},s^{1},l^{0})\rangle,\langle J,(a^{0},s^{1},l^{1})\rangle\}}}\\ {{\mathcal{D}^{a^{1},s^{0},l^{0}}}}&{{=}}&{{\{\langle J,(a^{1},s^{0},l^{0})\rangle\}}}\\ {{\mathcal{D}^{a^{1},s^{0},l^{1}}}}&{{=}}&{{\{\langle J,(a^{1},s^{0},l^{1})\rangle\}}}\\ {{\mathcal{D}^{a^{1},s^{1}}}}&{{=}}&{{\{\langle J,(a^{1},s^{1},l^{0})\rangle,\langle J,(a^{1},s^{1},l^{1})\rangle\}.}}\end{array}
$$ 

More generally, this partition-based model can capture local structure in both tree-CPDs and in rule-based CPDs. In fact, when the network is composed of multinomial CPDs, this ﬁner-grained sharing also generalizes the sharing structure in the global partition models of section 17.5.1. 

We can now reformulate the likelihood function in terms of the shared parameters $\boldsymbol{\theta}^{1},\dots,\boldsymbol{\theta}^{K}$ . Recall that we can write 

$$
P(\mathcal{D}\mid\mathbf{\theta})=\prod_{i}\prod_{\mathbf{u}_{i}}\left[\prod_{x_{i}}P(x_{i}\mid\mathbf{u}_{i},\mathbf{\theta})^{M[x_{i},\mathbf{u}_{i}]}\right].
$$ 

Each of the terms in square brackets is the local likelihood of a single multinomial distribution. We now rewrite each of the terms in the innermost product in terms of the shared parameters, and we aggregate them according to the partition: 

$$
\begin{array}{c c l}{P(\mathcal{D}\mid\theta)}&{=}&{\displaystyle\prod_{i}\prod_{u_{i}}\left[\prod_{x_{i}}P(x_{i}\mid\mathbf{u}_{i},\theta)^{M[x_{i},\mathbf{u}_{i}]}\right]}\\ &{=}&{\displaystyle\prod_{k}\prod_{\langle X_{i},\mathbf{u}_{i}\rangle\in\mathcal{D}^{k}}\prod_{j}(\theta_{j}^{k})^{M[x_{i}^{j},\mathbf{u}_{i}]}}\\ &{=}&{\displaystyle\prod_{k}\left[\prod_{j}(\theta_{j}^{k})^{\sum_{(X_{i},\mathbf{u}_{i})\in\mathcal{D}^{k}}M[x_{i}^{j},\mathbf{u}_{i}]}\right].}\end{array}
$$ 

This ﬁnal expression is reminiscent of the likelihood in the case of independent parameters, except that now each of the terms in the square brackets involves the shared parameters. Once again, we can deﬁne a notion of aggregate sufcient statistics: 

$$
\check{M}_{k}[j]=\sum_{\langle X_{i},\pmb{u}_{i}\rangle\in\mathcal{D}^{k}}M[x_{i}^{j},\pmb{u}_{i}],
$$ 

and use those aggregate sufcient statistics for parameter estimation, exactly as we used the unaggregated sufcient statistics before. 

# 17.5.3 Bayesian Inference with Shared Parameters 

To perform Bayesian estimation, we need to put a prior on the parameters. In the case without parameter sharing, we had a separate (independent) prior for each parameter. This model is clearly in violation of the assumptions made by parameter sharing. If two parameters are shared, we want them to be identical, and thus it is inconsistent to assume they have independent prior. The right approach is to place a prior on the shared parameters. 

Consider, in particular, the local analysis of section 17.5.2, we would place a prior on each of the multinomial parameters $\theta^{k}$ . As before, it is very convenient to assume that each of these set of parameters are independent from each other. This assumption corresponds to the local parameter independence we made earlier, but applied in the context where we force the given parameter-sharing strategy. We can use a similar idea in the global analysis of section 17.5.1, introducing a prior over each set of parameters $\theta^{k}$ . If we impose an independence assumption for the priors of the diferent sets, we obtain a shared-parameter version of the global parameter independence assumption. 

One important subtlety relates to the choice of the prior. Given a model with shared param- eters, the analysis of section 17.4.3 no longer applies directly. See exercise 17.13 for one possible extension. 

As usual, if the prior decomposes as a product and the likelihood decomposes as a product along the same lines, then our posterior also decomposes. For example, returning to equa- tion (17.21), we have that: 

$$
P(\pmb\theta\mid\mathcal D)\propto\prod_{k=1}^{K}P(\pmb\theta^{k})\prod_{j}(\theta_{j}^{k})^{\check{M}_{k}[j]}.
$$ 

The actual form of the posterior depends on the prior. Speciﬁcally, if we use multinomial distributions with Dirichlet priors, then the posterior will also be a Dirichlet distribution with the appropriate hyperparameters. 

This discussion seems to suggest that the line of reasoning we had in the case of independent parameters is applicable to the case of shared parameters. However, there is one subtle point that can be diferent. Consider the problem of predicting the probability of the next instance, which can be written as: 

$$
P(\xi[M+1]\mid\mathcal{D})=\int P(\xi[M+1]\mid\theta)P(\theta\mid\mathcal{D})d\theta.
$$ 

To compute this formula, we argued that, since $P(\xi[M+1]\mid\theta)$ is a product of parameters $\textstyle\prod_{i}\theta_{x_{i}[M+1]|\pmb{u}_{i}[M+1]}$ , and since the posterior of these parameters are independent, then we can write (for multinomial parameters) 

$$
P(\xi[M+1]\mid\mathcal{D})=\prod_{i}E\big[\theta_{x_{i}[M+1]\mid\mathbf{u}_{i}[M+1]}\mid\mathcal{D}\big],
$$ 

where each of the expectations is based on the posterior over $\theta_{x_{i}[M+1]|\mathbf{u}_{i}[M+1]}$ . 

When we have shared parameters, we have to be more careful. If we consider the network of example 17.11, then when the $(M+1)\mathrm{st}$ instance has two courses of the same difculty, the likelihood term $P(\xi[M+1]\mid\theta)$ involves a product of two parameters that are not independent. More explicitly, the likelihood involves $P(G_{1}[M\!+\!1]\mid I[M\!+\!1],D_{1}[M\!+\!1])$ and $P(G_{2}[M\!+\!1]\mid$ 

![](images/cb423c654cf0f6627fd7b4cd1e62612add416a196330edf72e7503973234101c.jpg) 
Figure 17.10 Example meta-network for a model with shared parameters, corresponding to exam- ple 17.11. 

$I[M+1],D_{2}[M+1])$ ; if $D_{1}[M+1]=D_{2}[M+1]$ then the two parameters are from the same multinomial distribution. Thus, the posterior over these two parameters is not independent, and we cannot write their expectation as a product of expectations. 

Another way of understanding this problem is by examining the meta-network for learning in such a situation. The meta-network for Bayesian parameter learning for the network of example 17.11 is shown in ﬁgure 17.10. As we can see, in this network $G_{1}[M+1]$ is not independent of $G_{2}[M+1]$ given $I[M+1]$ because of the trail through the shared parameters. Stated diferently, observing $G_{1}[M+1]$ will cause us to update the parameters and therefore change our estimate of the probability of $G_{2}[M+1]$ . 

Note that this problem can happen only in particular forms of shared parameters. If the shared parameters are within the same CPD, as in example 17.15, then the $(M+1)\mathrm{st}$ instance can involve at most one parameter from each partitions of shared parameters. In such a situation, the problem does not arise and we can use the average of the parameters to compute the probability of the next instance. However, if we have shared parameters across diferent CPDs (that is, entries in two or more CPDs share parameters), this problem can occur. 

How do we solve this problem? The correct Bayesian prediction solution is to compute the average for the product of two (or more) parameters from the same posterior. This is essentially identical to the question of computing the probability of two or more test instances. See exercise 17.18. This solution, however, leads to many complications if we want to use the Bayesian posterior to answer queries about the distribution of the next instance. In particular, this probability no longer factorizes in the form of the original Bayesian network, and thus, we cannot use standard inference procedures to answer queries about future instances. For this reason, a pragmatic approximation is to use the expected parameters for each CPD and ignore the dependencies induced by the shared parameters. When the number of training samples is large, this solution can be quite a good approximation to the true predictive distribution. However, when the number of training examples is small, this assumption can skew the estimate; see exercise 17.18. 

# 17.5.4 Hierarchical Priors $\star$ 

In our discussion of Bayesian methods for table-CPDs, we made the strong independence as- sumption (global and local parameter independence) to decouple the estimation of parameters. 

Our discussion of shared parameters relaxed these assumptions by moving to the other end of the spectrum and forcing parameters to be identical. There are many situations where neither solution is appropriate. 

Example 17.17 

Example 17.18 bigram model Using our favorite university domain, suppose we learn a model from records of students, classes, teachers, and so on. Now suppose that we have data from several universities. Because of various factors, the data from each university have diferent properties. These diferences can be due to the diferent population of students in each one (for example, one has more engineering oriented students while the other has more liberal arts students), somewhat diferent grade scale, or other factors. This leads to a dilemma. We can learn one model over all the data, ignoring the university speciﬁc bias. This allows us to pool the data to get a larger and more reliable data set. Alternatively, we can learn a diferent model for each university, or, equivalently, add a University variable that is the parent of many of the variables in the network. This approach allows us to tailor the parameters to each university. However, this ﬂexibility comes at a price — learning parameters in one university does not help us learn better parameters from the data in the other university. This partitions the data into smaller sets, and in each one we need to learn from scratch that intelligent students tend to get an A in easy classes. 

A similar problem might arise when we consider learning dependencies in text domains. As we discussed in box 6.B, a standard model for word sequences is $^a$ bigram model , which views the words as forming a Markov chain, where we have a conditional probability over the next word given the current word. That is, we want to learn $P(W^{(t+1)}\;\mid\;W^{(t)})$ wh $W$ is a random variable taking values from the dictionary of words. Here again, the context $W^{(t)}$ can deﬁnitely change our distribution over $W^{(t+1)}$ , but we still want to share some information across these diferent conditional distributions; for example, the probability of common words, such as “the,” should be high in almost all of the conditional distributions we learn. 

In both of these examples, we aim to learn a conditional distribution, say $P(Y\mid X)$ . Moreover, we want the diferent conditional distributions $P(Y\mid x)$ to be similar to each other, yet not identical. Thus, the Bayesian learning problem assuming local independence (ﬁgure 17.11a) is not appropriate. 

One way to bias the diferent distributions to be similar to each other is to have the same prior over them. If the prior is very strong, it will bias the diferent estimates to the same values. In particular, in the domain of example 17.18, we want the prior to bias both distributions toward giving high probability to frequent words. Where do we get such a prior? One simple ad hoc solution is to use the data to set the prior. For example, we can use the frequency of words in training set to construct a prior where more frequent words have a larger hyperparameter. Such an approach ensures that more frequent words have higher posterior in each of the conditional distributions, even if there are few training examples for that particular conditional distribution. 

shrinkage 

This approach (which is a slightly more Bayesian version of the shrinkage of exercise 6.2) works fairly well, and is often used in practice. However, it seems to contradict the whole premise of the Bayesian framework: a prior is a distribution that we formulate over our parameters prior to seeing the data. This approach also leaves unaddressed some important questions, such as determining the relative strength of the prior based on the amount of data used to compute it. A more coherent and general approach is to stay within the Bayesian framework, and to 

![](images/a26f24ea2dd6d01e555653be8eeeb740c1da60eee99336436d8c04bc1e9c503b.jpg) 
Figure 17.11 Independent and hierarchical priors. (a) A plate model for $P(Y\mid X)$ under assumption of parameter independence. (b) A plate model for a simple hierarchical prior for the same CPD. (c) A plate model for two CPDs $P(Y\mid X)$ and $P(Z\mid X)$ that respond similarly to $X$ . 

hierarchical Bayes introduce explicitly our uncertainty about the joint prior as part of the model. Just as we introduced random variables to denote parameters of CPDS, we now take one step further and introduce a random variable to denote the hyperparameters. The resulting model is called a hierarchical Bayesian model . It uses a factored probabilistic model to describe our prior. This idea, of speciﬁcally deﬁning a probabilistic model over our priors, can be used to deﬁne rich priors in a broad range of settings. Exercise 17.7 provides another example. 

Figure 17.11b shows a simple example, where we have a variable that is the parent of both $\theta_{Y\mid x^{0}}$ and $\theta_{Y\mid x^{1}}$ . As a result, these two parameters are no longer independent in the prior, and consequently in the posterior. Intuitively, the efect of the prior will be to shift both $\theta_{Y\mid x^{0}}$ and $\theta_{Y\mid x^{1}}$ to be closer to each other. However, as usual when using priors, the efect of the prior diminishes as we get more data. In particular, as we have more data for the diferent contexts $x^{1}$ and $x^{0}$ , the efect of the prior will gradually decrease. Thus, the hierarchical priors (as for all priors) are particularly useful in the sparse-data regime. 

How do we represent the distribution over the hyperparameter $P(\vec{\alpha})\hat{\imath}$ ? One option is to create a prior where each component $\alpha_{y}$ is governed by some distribution, say a Gamma distribution (recall that components are strictly positive). That is, 

$$
P(\vec{\alpha})=\prod_{y}P(\alpha_{y}),
$$ 

Gamma distribution where $P(\alpha_{y})\sim\mathrm{Gamma}(\mu_{y})$ is a Gamma distribution with (hyp hyperparameter $\mu_{y}$ . The other option is to write ⃗ as the product of equivalent sample size $N_{0}$ with a probability distri- bution $p_{0}$ , the ﬁrst governed by a Gamma distribution and the other by a Dirichlet distribution. (These two representations are actually closely related; see box 19.E.) 

Moreover, the same general idea can be used in broader ways. In this example, we used a hierarchical prior to relate two (or more) conditional distributions in the same CPD. We can similarly relax the global parameter independence assumption and introduce dependencies between the parameters for two (or more) CPDs. For example, if we believe that two variables $Y$ and $Z$ depend on $X$ in a similar (but not identical) way, then we introduce a common prior on $\theta_{Y\mid x^{0}}$ and $\theta_{Z|x^{0}}$ , and similarly another common prior for $\theta_{Y\mid x^{1}}$ and $\theta_{Z|x^{1}}$ ; see ﬁgure 17.11c. 

The idea of a hierarchical structure can also be extended to additional levels of a hierarchy. For example, in the case of similar CPDs, we might argue that there is a similarity between the distributions of $Y$ and $Z$ given $x^{0}$ and $x^{1}$ . If we believe that this similarity is weaker than the similarity between the distributions of the two variables, we can introduce another hierarchy layer to relate the hyperparameters $\alpha.|x^{0}$ and $\alpha.|x^{1}$ . To do so, we might introduce hyper-hyperparameters $\mu$ that specify a joint prior over $\alpha.|x^{0}$ and $\alpha.|x^{1}$ . 

The notion of hierarchical priors can be readily applied to other types of CPDs. For example, if $X$ is a Gaussian variable without parents, then, as we saw in exercise 17.8, a conjugate prior for the mean of $X$ is simply a Gaussian prior on the mean parameter. This observation suggests that we can easily create a hierarchical prior that relates $X$ and another Gaussian variable $Y$ , by having a common Gaussian over the means of the variables. Since the distribution over the hyperparameter is a Gaussian, we can easily extend the hierarchy upward. Indeed, hierarchical priors over Gaussians are often used to model the dependencies of parameters of related populations. For example, we might have a Gaussian over the SAT score, where one level of the hierarchy corresponds to diferent classes in the same school, the next one to diferent schools in the same district, the following to diferent districts in the same state, and so on. 

The framework of hierarchical priors gives us a ﬂexible language to introduce depen- dencies in the priors over parameters. Such dependencies are particularly useful when we have small amount of examples relevant to each parameter but many such parameters that we believe are reasonably similar. In such situations, hierarchical priors “spread” the efect of the observations between parameters with shared hyperparameters. 

One question we did not discuss is how to perform learning with hierarchical priors. As with previous discussions of Bayesian learning, we want to compute expectations with respect to the posterior distribution. Since we relaxed the global and local independence assumptions, the posterior distribution no longer decomposes into a product of independent terms. From the perspective of the desired behavior, this lack of independence is precisely what we wanted to achieve. However, it implies that we need to deal with a much harder computational task. In fact, when introducing the hyperparameters as a variable into the model, we transformed our learning problem into one that includes a hidden variable. To address this setting, we therefore need to apply methods for Bayesian learning with hidden variables; see section 19.3 for a discussion of such methods. 

text classiﬁcation 

bag of words 

Box 17.E — Concept: Bag-of-Word Models for Text Classiﬁcation. Consider the problem of text classiﬁcation : classifying a document into one of several categories (or classes). Somewhat surprisingly, some of the most successful techniques for this problem are based on viewing the document as an unordered bag of words , and representing the distribution of this bag in diferent categories. Most simply, the distribution is encoded using a naive Bayes model. Even in this simple approach, however, there turn out to be design choices that can make important diferences to the performance of the model. 

Our ﬁrst task is to represent a document by a set of random variables. This involves various processing steps. The ﬁrst removes various characters such as punctuation marks as well as words that are viewed as having no content (such as “the,” “and,” and so on). In addition, most applica- tions use a variety of techniques to map words in the document to canonical words in a predeﬁned dictionary $\mathcal{D}$ (for example, replace “apples,” “used,” and “running” with “apple,” “use,” and “run” respectively). Once we ﬁnish this processing step, there are two common approaches to deﬁning the features that describe the document. 

Bernoulli naive Bayes 

multinomial naive Bayes 

In the Bernoulli naive Bayes model, we deﬁne a binary attribute (feature) $X_{i}$ to denote whether the i ’th dictionary word $w_{i}$ appears in the document. This representation assumes that we care only about the presence of a word, not about how many times it appeared. Moreover, when applying the naive Bayes classiﬁer with this representation we assume that the appearance of one word in the document is independent of the appearance of another (given the document’s topic). When learning a naive Bayes classiﬁer with this representation, we learn frequency (over a document) of encountering each dictionary word in documents of speciﬁc categories — for example, the probability that the word “ball” appears in a document of category “sports.” We learn such a parameter for each pair (dictionary word, category). 

In the multinomial naive Bayes model, we deﬁne attribute to describe the speciﬁc sequence of words in the document. The variable $X_{i}$ denotes which dictionary word appeared the i th word in the document. Thus, each $X_{i}$ can take on many values, one for each possible word. Here, when we use the naive Bayes classiﬁer, we assume that the choice of word in position i is independent of the choice of word in position $j$ (again, given the document’s topic). This model leads to a complication, since the distribution over $X_{i}$ is over all words in the document, which requires a large number of parameters. Thus, we further assume that the probability that a particular word is used in position $i$ does not depend on $i$ ; that is, the probability that $X_{i}=w$ (given the topic) is the same as the that $X_{j}\,=\,w$ . In other words, we use parameter sharing between $P(X_{i}\mid C)$ and $P(X_{j}\mid C)$ | . This implies that the total number of parameters is again one for each (dictionary word, category). 

In both models, we might learn that the word “quarterback” is much more likely in documents whose topic is “sports” than in documents whose topic is “economics,” the word “bank” is more associated with the latter subjects, and the word “dollar” might appear in both. Nevertheless, the two models give rise to quite diferent distributions. Most notably, if a word $w$ appears in several diferent positions in the document, in the Bernoulli model the number of occurrences will be ignored, while in the multinomial model we will multiply the probability $P(w\mid C)$ several times. If this probability is very small in one category, the overall probability of the document given that category will decrease to reﬂect the number of occurrences. Another diference is in how the document length plays a role here. In the Bernoulli model, each document is described by exactly the same number of variables, while the multinomial model documents of diferent lengths are associated with a diferent number of random variables. 

The plate model provides a compact and elegant way of making explicit the subtle distinctions between these two models. In both models, we have two diferent types of objects — documents, and individual words in the documents. Document objects $d$ are associated with the attribute $T$ , representing the document topic. However, the notion of “word objects” is diferent in the diferent models. 

In the Bernoulli naive Bayes model, our words correspond to words in some dictionary (for example, “cat,” “computer,” and so on). We then have a binary-valued attribute $A(d,w)$ for each document d and dictionary word w , which takes the value true if the word $w$ appears in the document d . We can model this case using a pair of intersecting plates, one for documents and the other for dictionary words, as shown in ﬁgure 17.E.1a. 

![](images/efe6d933698bd4e7de1c14a729c6182a2c5e8ed2e013d6ec077d11c775e0d850.jpg) 
Figure 17.E.1 — Diferent plate models for text (a) Bernoulli naive Bayes; (b) Multinomial naive Bayes. (c) Latent Dirichlet Allocation. 

In the multinomial naive Bayes model, our word objects correspond not to dictionary words, but to word positions $P$ within the document. Thus, we have an attribute $W$ of records representing pairs $(D,P)$ , where $D$ is a document and $P$ is a position within it (that is, ﬁrst word, second word, and so on). This attribute takes on values in the space of dictionary words, so that $W(d,p)$ is the random variable whose value is the actual dictionary word in position in document $d$ . However, $p$ all of these random variables are generated from the same multinomial distribution, which depends on the document topic. The appropriate plate model is shown in ﬁgure 17.E.1b. 

The plate representation of the two models makes explicit the fact that the Bernoulli parameter $\beta_{W}[w]$ in the Bernoulli model is diferent for diferent words, whereas in the multinomial model, the parameter $\beta_{W}$ is the same for all positions within the document. Empirical evidence suggests that the multinomial model is, in general, more successful than the Bernoulli. 

In both models, the parameters are estimated from data, and the resulting model used for classi- fying new documents. The parameters for these models measure the probability of a word given a topic, for example, the probability of “bank” given “economics.” For common words, such probabil- ities can be assessed reasonably well even from a small number of training documents. However, as the number of possible words is enormous, Bayesian parameter estimation is used to avoid over- ﬁtting, especially ascribing probability zero to words that do not appear in the training set. With Bayesian estimation, we can learn a naive Bayes model for text from a fairly small corpus, whereas more realistic models of language are generally much harder to estimate correctly. This ability to deﬁne reasonable models with a very small number of parameters, which can be acquired from a small amount of training data, is one of the key advantages of the naive Bayes model. 

We can also deﬁne much richer representations that capture more ﬁne-grained structure in the distribution. These models are often easily viewed in the plate representation. One such model, shown in ﬁgure 17.E.1c, is the latent Dirichlet allocation (LDA) model, which extends the multinomial naive Bayes model. As in the multinomial naive Bayes model, we have a set of topics associated with a set of multinomial distributions $\theta_{W}$ over words. However, in the LDA model, we do not assume that an entire document is about a single topic. Rather, we assume that each document $d$ is associated with a continuous mixture of topics, deﬁned using parameters $\theta(d)$ . These parameters are selected independently from each document $d$ , from a Dirichlet distribution parameterized by a set of hyperparameters $_{\alpha}$ . The word in position $p$ in the document $d$ is then selected by ﬁrst selecting a topic $T o p i c(d,p)\,=\,t$ from the mixture $\theta(d)$ , and then selecting a speciﬁc dictionary word from the multinomial $\beta_{t}$ associated with the chosen topic $t$ . The LDA model generally provides much better results (in measures related to log-likelihood of test data) than other unsupervised clustering approaches to text. In particular, owing to the ﬂexibility of assigning a mixture of topics to a document, there is no problem with words that have low probability relative to a particular topic; thus, this approach largely avoids the overﬁtting problems with the two naive Bayes models described earlier. 

# 17.6 Generalization Analysis $\star$ 

One intuition that permeates our discussion is that more training instances give rise to more accurate parameter estimates. This intuition is supported by our empirical results. In this section, we provide some formal analysis that supports this intuition. This analysis also allows us to quantify the extent to which the error in our estimates decreases as a function of the number of training samples, and increases as a function of the number of parameters we want to learn or the number of variables in our networks. 

We begin with studying the asymptotic behavior of our estimator at the large-sample limit. We then provide a more reﬁned analysis that studies the error as a function of the number of samples. 

# 17.6.1 Asymptotic Analysis 

We start by considering the asymptotic behavior of the maximum likelihood estimator. In this case, our analysis of section 17.2.5 provides an immediate conclusion: At the large sample limit, $\hat{P}_{\mathcal{D}}$ approaches $P^{*}$ ; thus, as the number of samples grows, $\hat{\pmb\theta}$ approaches $\theta^{*}$ — the projection D of $P^{*}$ onto the parametric family. 

consistent estimator 

A particular case of interest arises when $P^{*}(\mathcal{X})=P(\mathcal{X}:\theta^{*})$ , that is, $P^{*}$ is representable in the parametric family. Then, we have that ${\hat{\boldsymbol{\theta}}}\rightarrow{\boldsymbol{\theta}}^{*}$ → as $M\rightarrow\infty$ . An estimator with this property is called consistent estimator . In general, maximum likelihood estimators are consistent. 

We can make this analysis even more precise. Using equation (17.9), we can write 

$$
\log\frac{P(\mathcal{D}\mid\hat{\theta})}{P(\mathcal{D}\mid\theta)}=M\left(D(\hat{P}_{\mathcal{D}}\|P_{\theta})-D(\hat{P}_{\mathcal{D}}\|P_{\hat{\theta}})\right).
$$ 

This equality implies that the likelihood function is sharply peaked: the decrease in the likelihood for parameters that are not the MLE is exponential in $M$ . Of course, when we change $M$ the data set $\mathcal{D}$ and henc e distribution $\hat{P}_{\mathcal{D}}$ D also change, and thus this result does not guarantee exponential decay in M . However, for sufciently large $M$ $,\,\hat{P}_{\mathcal{D}}\,\rightarrow\,P^{*}$ → . Thus, diference in D log-likelihood of diferent choices of $\theta$ is roughly $M$ times their distance from P $P^{*}$ : 

$$
\log{\frac{P(\mathcal{D}\mid\hat{\theta})}{P(\mathcal{D}\mid\theta)}}\approx M\left(D(P^{*}\|P_{\theta})-D(P^{*}\|P_{\hat{\theta}})\right).
$$ 

The terms on the right depend only on $M$ and not on $\hat{P}_{\mathcal{D}}$ . Thus, we conclude that for large D values of $M$ , the likelihood function approaches a delta function, for which all values are virtually 0 when compared to the maximal value at $\theta^{*}$ , the M-projection of $P^{*}$ . 

To summarize, this argument basically allows us to prove the following result, asserting that the Bayesian estimator is consistent : 

Let $P^{*}$ be the g ution, $P(\cdot\mid\theta)$ be $a$ rametric family of distributions, and let θ ∗ = arg min θ I $D(P^{*}\|P(\cdot\mid\theta))$ be the M-projection of $P^{*}$ on this family. Then 

$$
\operatorname*{lim}_{M\rightarrow\infty}P(\cdot\mid\hat{\theta})=P(\cdot\mid\theta^{*})
$$ 

almost surely. 

That is, when $M$ grows larger, the estimated parameters describe a distribution that is close to the distribution with the “optimal” parameters in our parametric family. 

Is our Bayesian estimator consistent? Recall that equation (17.11) shows that the Bayesian estimate with a Dirichlet prior is an interpolation between the MLE and the prior prediction. The interpolation weight depends on the number of samples $M$ : as $M$ grows, the weight of the prior prediction diminishes and disappears in the limit. Thus, we can conclude that Bayesian learning with Dirichlet priors is also consistent. 

# 17.6.2 PAC-Bounds 

PAC-bound This consistency result guarantees that, at the large sample limit, our estimate converges to the true distribution. Though a satisfying result, its practical signiﬁcance is limited, since in most cases we do not have access to an unlimited number of samples. Hence, it is also important to evaluate the quality of our learned model as a function of the number of samples $M$ . This type of analysis allows us to know how much to trust our learned model as a function of $M$ ; or, from the other side, how many samples we need to acquire in order to obtain results of a given quality. Thus, using relative entropy to the true distribution as our notion of solution quality, we use PAC-bound analysis (as in box 16.B) to bound $D(P^{*}\|\hat{P})$ | | as a function of the number of data samples $M$ . 

# 17.6.2.1 Estimating a Multinomial 

convergence bound 

Hoefding bound We start with the simplest case, which forms the basis for our more extensive analysis. Here, our task is to estimate the multinomial parameters governing the distribution of a single random variable. This task is relevant to many disciplines and has been studied extensively. A basic tool used in this analysis are the convergence bounds described in appendix A.2. 

Consider a data set $\mathcal{D}$ deﬁned as a set of $M$ IID Bernoulli random variables ${\mathcal D}\,=\,\{X[1]$ . . . $,\,\,X[M]\}$ , where $P^{*}(X[m]\,=\,x^{1})\,=\,p^{*}$ for all $m$ . Note that we are now considering D itself to be a stochastic event (a random variable), sampled from the distribution $(P^{*})^{M}$ . Let $\begin{array}{r}{\hat{p}_{\mathcal{D}}=\frac{1}{M}\sum_{m}X[m]}\end{array}$ . Then an immediate consequence of the Hoefding bound (theorem A.3) is 

$$
\begin{array}{r}{P_{M}\big(|\hat{p}_{\mathcal{D}}-p^{*}|>\epsilon\big)\leq2e^{-2M\epsilon^{2}},}\end{array}
$$ 

where $P_{M}$ is a shorthand for $P_{\mathcal{D}\sim(P^{*})}M$ . Thus, the probability that the MLE $\hat{p}_{\mathcal{D}}$ deviates D from the true parameter by more than $\epsilon$ is bounded from above by a function that decays exponentially in $M$ . 

As an immediate corollary, we can prove a PAC-bound on estimating $p^{*}$ : 

Corollary 17.1 

$$
M>\frac{1}{2\epsilon^{2}}\log\frac{2}{\delta}.
$$ 

Then $P_{M}(|\hat{p}-p^{*}|\leq\epsilon)\geq1-\delta$ − ≥ − , where $P_{M}$ , as before, is a probability over data sets $\mathcal{D}$ of size $M$ sampled IID from P $P^{*}$ ∗ . 

Proof 

$$
\begin{array}{r c l}{{P_{M}(|\hat{p}-p^{*}|\leq\epsilon)}}&{{=}}&{{1-P_{M}(\hat{p}-p^{*}>\epsilon)-P_{M}(\hat{p}-p^{*}<-\epsilon)}}\\ {{}}&{{\geq}}&{{1-2e^{-2M\epsilon^{2}}\geq1-\delta.}}\end{array}
$$ 

The number of data instances $M$ required to obtain a PAC-bound grows quadratically in the error $1/\epsilon$ , and logarithmically in the conﬁdence $1/\delta$ . For example, setting $\epsilon=0.05$ and $\delta=0.01$ , we get that we need $M\geq1059.66$ . That is, we need a bit more than $1,000$ samples to conﬁdently estimate the probability of an event to within 5 percent error. 

relative entropy 

This result allows us to bound the absolute value of the error between the parameters. We, however, are interested in the relative entropy between the two distributions. Thus, we want to bound terms of the form $\textstyle\log{\frac{p^{*}}{\hat{p}}}$ . 

Lemma 17.1 

$$
P_{M}(\log\frac{p^{*}}{\hat{p}}>\epsilon)\leq e^{-2M p^{2}\epsilon^{2}\frac{1}{(1+\epsilon)^{2}}}.
$$ 

Proof The proof relies on the following fact: 

$$
\epsilon\leq x\leq y\leq1{\mathrm{~then~}}\quad(\log y-\log x)\leq{\frac{1}{\epsilon}}(y-x).
$$ 

Now, consider some $\epsilon^{\prime}$ . If $p^{*}-\hat{p}\leq\epsilon^{\prime}$ ≤ then $\hat{p}>p^{*}-\epsilon^{\prime}$ − . Applying equation (17.22), we get that $\begin{array}{r}{\log\frac{p^{*}}{\hat{p}}\leq\frac{\epsilon^{\prime}}{p^{*}-\epsilon^{\prime}}}\end{array}$ . Setting $\begin{array}{r}{\epsilon^{\prime}=\frac{\epsilon p^{*}}{1+\epsilon}}\end{array}$ , and taking the contrapositive, we conclude that, if $\begin{array}{r}{\log\frac{p^{*}}{\hat{p}}>\epsilon}\end{array}$ − then $\begin{array}{r}{p^{*}-\hat{p}>\frac{\epsilon p^{*}}{1+\epsilon}}\end{array}$ . Using the Hoefding bound to bound the probability of the latter event, we derive the desired result. 

This analysis applies to a binary-valued random variable. We now extend it to the case of a multivalued random variable. The result provides a bound on the relative entropy between $P^{*}(X)$ and the maximum likelihood estimate for $P(X)$ , which is simply its empirical probability ${\hat{P}}_{\mathcal{D}}(X)$ . D 

Proposition 17.7 Let $P^{*}(X)$ be a discrete dis ution such tha $P^{*}(x)\;\geq\;\lambda$ for all $x\ \in\ V a l(X)$ . Let $\mathcal{D}_{\mathrm{~}}={}$ $\{X[1],.\,.\,.\,,X[M]\}$ consist of M IID samples of X . Then 

$$
P_{M}(D(P^{\ast}(X)\|\hat{P}_{\mathcal{D}}(X))>\epsilon)\leq|V a l(X)|e^{-2M\lambda^{2}\epsilon^{2}\frac{1}{(1+\epsilon)^{2}}}.
$$ 

Proof We want to bound the error 

$$
D(P^{*}(X)\|\hat{P}_{\mathcal{D}}(X))=\sum_{x}P^{*}(x)\log\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}.
$$ 

This expression is a weighted average of log-ratios of the type we bounded in lemma 17.1. If we bound each of the terms in this average by $\epsilon$ , we can obtain a bound for the weighted average as a whole. That is, we say that a data set is well behaved if, for each $x$ , $\begin{array}{r}{\log\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}\leq\epsilon.}\end{array}$ . If the D data set is well behaved, we have a bound of $\epsilon$ on the term for each $x$ , and therefore an overall bound of $\epsilon$ for the entire relative entropy. 

With what probability is our data set not well behaved? It sufces that there is one $x$ for which $\begin{array}{r}{\log\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}>\epsilon}\end{array}$ . We can provide an upper bound on this probability using the union bound , D which bounds the probability of the union of a set of events as the sum of the probabilities of the individual events: 

$$
P\left(\exists x,\log{\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}}>\epsilon\right)\leq\sum_{x}P\left(\log{\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}}>\epsilon\right).
$$ 

The union bound is an overestimate of the probability, since it essentially represents the case where the diferent “bad” events are disjoint. However, our focus is on the situation where these events are unlikely, and the error due to such overcounting is not signiﬁcant. Formalizing this argument, we obtain: 

$$
\begin{array}{r c l}{{P_{M}(D(P^{*}(X)\|\hat{P}_{\mathcal{D}}(X))>\epsilon)}}&{{\le}}&{{P_{M}\left(\exists x\ :\ \log\displaystyle\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}>\epsilon\right)}}\\ {{}}&{{}}&{{}}\\ {{}}&{{\le}}&{{\displaystyle\sum_{x}P_{M}\left(\log\displaystyle\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}>\epsilon\right)}}\\ {{}}&{{\le}}&{{\displaystyle\sum_{x}e^{-2M P^{*}(x)^{2}\epsilon^{2}\frac{1}{(1+\epsilon)^{2}}}}}\\ {{}}&{{\le}}&{{|V a l(X)|e^{-2M\lambda^{2}\epsilon^{2}\frac{1}{(1+\epsilon)^{2}}},}}\end{array}
$$ 

where the second inequality is derived from the union bound, the third inequality from lemma 17.1, and the ﬁnal inequality from the assumption that $P^{*}(x)\geq\lambda$ . 

This result provides us with an error bound for estimating the distribution of a random variable. We can now easily translate this result to a PAC-bound: 

Corollary 17.2 

$$
M\geq\frac{1}{2}\frac{1}{\lambda^{2}}\frac{(1+\epsilon)^{2}}{\epsilon^{2}}\log\frac{|V a l(X)|}{\delta}.
$$ 

Then $\begin{array}{r}{P_{M}(D(P^{*}(X)\|\hat{P}_{\mathcal{D}}(X))\leq\epsilon)\geq1-\delta.}\end{array}$ | | D ≤ ≥ − 

As with the binary-valued case, the number of samples grows quadratically with $\textstyle{\frac{1}{\epsilon}}$ and logarithmically with $\frac{1}{\delta}$ . Here, however, we also have a quadratic dependency on $\textstyle{\frac{1}{\lambda}}$ . The value $\lambda$ is a measure of the “skewness” of the distribution $P^{*}$ . This dependence is not that surprising; we expect that, if some values of $X$ have small probability, then we need many more samples to get a good approximation of their probability. Moreover, underestimates of $\dot{P}_{\mathcal{D}}(x)$ for such D events can lead to a big error in estimating $\begin{array}{r}{\log\frac{P^{*}(x)}{\hat{P}_{\mathcal{D}}(x)}}\end{array}$ . Intuitively, we might suspect that when D $P^{*}(x)$ is small, it is harder to estimate, but at the same time it also less crucial for the total error. Although it is possible to use this intuition to get a better estimation (see exercise 17.19), the asymptotic dependence of $M$ on $\frac{1}{\lambda}$ remains quadratic. 

# 17.6.2.2 Estimating a Bayesian Network 

We now consider the problem of learning a Bayesian network. Suppose that $P^{*}$ is consistent with a Bayesian network $\mathcal{G}$ and that we learned parameters $\theta$ for $\mathcal{G}$ that deﬁne a distribution $P_{\theta}$ . Using theorem 8.5, we have that 

$$
D(P^{*}\|P_{\theta})=\sum_{i}D(P^{*}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})\|P_{\theta}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})),
$$ 

where (as shown in appendix A.1.3.2), we have that 

$$
D(P^{*}(X\mid Y)\|P(X\mid Y))=\sum_{y}P^{*}(y)D(P^{*}(X\mid y)\|P(X\mid y)).
$$ 

Thus, as we might expect, the error is a sum of errors in estimating the conditional probabilities. 

This error term, however, makes the strong assumption that our generating distribution $P^{*}$ is consistent with our target class — those distributions representable using the graph $\mathcal{G}$ . This assumption is usually not true in practice. When this assumption is false, the given network structure limits the ability of the learning procedure to generalize. For example, if we give a learning procedure a graph where $X$ and $Y$ are independent, then no matter how good our learning procedure, we cannot achieve low generalization error if $X$ and $Y$ are strongly dependent in $P^{*}$ . More broadly, if the given network structure is inaccurate, then there is inherent error in the learned distribution that the learning procedure cannot overcome. 

One approach to deal with this problem is to assume away the cases where $P^{*}$ does not conform with the given structure $\mathcal{G}$ . This solution, however, makes the analysis brittle and of excess risk little relevance to real-life scenarios. An alternative solution is to relax our expectations from the learning procedure. Instead of aiming for the error to become very small, we might aim to show that the error is not far away from the inherent error that our procedure must incur due to the limitations in expressive power of the given network structure. In other words, rather than bounding the risk, we provide a bound on the excess risk (see box 16.B). 

More formally, let $\Theta[\mathcal{G}]$ be the set of all possible parameter iz at ions for $\mathcal{G}$ . We now deﬁne 

$$
\pmb\theta^{\mathrm{opt}}=\arg\operatorname*{min}_{\pmb\theta\in\Theta[\mathcal G]}\pmb D(P^{*}\|P_{\pmb\theta}).
$$ 

M-projection That is, $\theta^{\mathrm{{opt}}}$ is the best result we might expect the learning procedure to return. (Using the terminology of section 8.5, $\theta^{\mathrm{{opt}}}$ is the $M$ -projection of $P^{*}$ on the family of distributions deﬁned by ${\mathcal{G}},\Theta[{\mathcal{G}}]$ .) 

The distance $D(P^{*}\|P_{\theta^{\mathrm{opt}}})$ | | reﬂects the minimal error we might achieve with networks with the structure $\mathcal{G}$ . Thus, deﬁning “success” for our learning procedure in terms of obtaining low values for I $D(P^{*}\|P_{\theta})$ | | (a goal which may not be achievable), we aim to obtain low values for $D(P^{*}\|P_{\theta})-D(P^{*}\|P_{\theta^{\mathrm{opt}}})$ | | − | | ) . 

What is the form of this error term? By solving for $P_{\theta^{\mathrm{opt}}}$ and using basic manipulations we can deﬁne it in precise terms. 

Theorem 17.3 Let $\mathcal{G}$ be a net ork structure, let $P^{*}$ be a distribution, and let $P_{\theta}\,=\,({\mathcal G},\theta)$ be a distribution consistent with . Then: 

$$
D(P^{*}\|P_{\theta})-D(P^{*}\|P_{\theta^{\mathrm{opt}}})=\sum_{i}D(P^{*}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})\|P_{\theta}(X_{i}\mid\mathrm{Pa}_{i}^{\mathcal{G}})).
$$ 

The proof is left as an exercise (exercise 17.20). 

# 

This theorem shows that the error in our learned network decomposes into two compo- nents. The ﬁrst is the error inherent in $\mathcal{G}$ , and the second the rror due to inaccuracies in estimating the conditional probabilities for parameterizing G . This theorem also shows that, in terms of error analysis, the treatment of the general case leads to exactly the same terms that we had to bound when we made the assumption that $P^{*}$ was consistent with $\mathcal{G}$ . Thus, in this learning task, the analysis of the inconsistent case is not more difcult than the analysis of the consistent case. As we will see in later chapters, this situation is not usually the case: we can often provide bounds for the consistent case, but not for the inconsistent case. 

To continue the analysis, we need to bound the error in estimating conditional probabilities. The preceding treatment showed that we can bound the error in estimating marginal probabilities of a variable or a group of variables. How diferent is the estimate of conditional probabilities from that of marginal probabilities? It turns out that the two are easily related. 

Lemma 17.2 

$$
D(P(X\mid Y)\|Q(X\mid Y))\leq D(P(X,Y)\|Q(X,Y)).
$$ 

See exercise 17.21. 

As an immediate corollary, we have that 

$$
D(P^{*}\|P)-D(P^{*}\|P_{\pmb\theta^{\mathrm{opt}}})\leq\sum_{i}D(P^{*}(X_{i},\mathrm{Pa}_{i}^{\mathcal{G}})\|P(X_{i},\mathrm{Pa}_{i}^{\mathcal{G}})).
$$ 

Thus, we can use proposition 17.7 to bound the error in estimating $P(X_{i},\mathrm{Pa}_{i}^{\mathcal{G}})$ for each $X_{i}$ (where we treat $X_{i},\mathrm{Pa}_{i}^{\mathcal{G}}$ as a single variable) and derive a bound on the error in estimating the probability of the whole network. 

Theorem 17.4 

$\mathcal{G}$ struct re, and $P^{*}$ dist bution consistent with some network $\mathcal{G}^{\ast}$ such that $P^{*}(x_{i}\mid\mathrm{pa}_{i}^{\mathcal{G}^{*}})\geq\lambda$ | ≥ for all i , $x_{i}$ , and $\mathrm{pa}_{i}^{\mathcal{G}^{\ast}}$ . If P is the distribution learned by maximum likelihood estimate for G , then 

$$
P\left(\pmb{D}(P^{*}\|P)-\pmb{D}(P^{*}\|P_{\pmb{\theta}^{\mathrm{opt}}})>n\epsilon\right)\le n K^{d+1}e^{-2M\lambda^{2(d+1)}\epsilon^{2}\frac{1}{(1+\epsilon)^{2}}},
$$ 

where $K$ is the maximal variable cardinality and $d$ is the maximum number of parents in $\mathcal{G}$ . Proof The proof uses the union bound 

$$
P\left(D(P^{*}\|P)-D(P^{*}\|P_{\theta^{\mathrm{opt}}})>n\epsilon\right)\leq\sum_{i}P\left(D(P^{*}(X_{i},\mathrm{Pa}_{i}^{\mathcal{G}})\|P(X_{i},\mathrm{Pa}_{i}^{\mathcal{G}}))>\epsilon\right)
$$ 

with application of proposition 17.7 to bound the probability of each of these latter events. The only technical detail we need to consider is to show that if conditional probabilities in $P^{*}$ are always larger than $\lambda$ , then $P^{*}(x_{i},\mathrm{pa}_{i}^{\mathcal{G}})\geq\lambda^{|\mathrm{Pa}_{i}^{\mathcal{G}}|+1}$ ≥ ; see exercise 17.22. 

This theorem shows that we can indeed learn parameters that converge to the optimal ones as the number of samples grows. As with previous bounds, the number of samples we need is 

Corollary 17.3 

$$
M\geq\frac{1}{2}\frac{1}{\lambda^{2(d+1)}}\frac{(1+\epsilon)^{2}}{\epsilon^{2}}\log\frac{n K^{d+1}}{\delta},
$$ 

then 

$$
P\left(D(P^{*}\|P\right)-D(P^{*}\|P_{\theta^{\mathrm{opt}}})<n\epsilon\}>1-\delta.
$$ 

As before, the number of required samples grows quadratically in $\frac{1}{\epsilon}$ . Conversely, we expect the error to decrease roughly with $\frac{1}{\sqrt{M}}$ , which is commensurate with the behavior we observe in practice (for example, see ﬁgure 17.C.2 in box 17.C). We see also that $\lambda$ and $d$ play a major role in determining $M$ . In practice, we often do not know $\lambda$ in advance, but such analysis allows us to provides guarantees under the assumption that conditional probabilities are not too small. It also allows us to predict the improvement in error (or at least in our upper bound on it) that we would obtain if we add more samples. 

Note that in this analysis we “allow” the error to grow with $n$ as we consider $D(P^{*}\|P)>n\epsilon$ | | . The argument is that, as we add more variables, we expect to incur some prediction error on each one. 

Example 17.19 Consider the network where we have n independent binary-valued variables $X_{1},\dots,X_{n}$ . In this case, we have $n$ independent Bernoulli estimation problems, and would expect a small number of samples to sufce. Indeed, we can obtain an $\epsilon$ -close estimate to each of them (with high-probability) using the bound of lemma 17.1. However, the overall relative entropy between $P^{*}$ and $\hat{P}_{\mathcal{D}}$ over D the joint space $X_{1},\dots,X_{n}$ will grow as the sum of the relative entropies between the individual marginal distributions $P^{*}(X_{i})$ and $\hat{P}_{\mathcal{D}}(X_{i})$ . Thus, even if we perform well at predicting each D variable, the total error will scale linearly with $n$ . 

Thus, our formulation of the bound in corollary 17.3 is designed to separate out this “inevitable” linear growth in the error from any additional errors that arise from the increase in dimension- ality of the distribution to be estimated. 

We provided a theoretical analysis for the generalization error of maximum likelihood estimate. A natural question is to carry similar analysis when we use Bayesian estimates. Intuitively, the asymptotic behavior (for $M\rightarrow\infty$ ) will be similar, since the two estimates are asymptotically identical. For small values of M we do expect to see diferences, since the Bayesian estimate is smoother and cannot be arbitrarily small and thus the relative entropy is bounded. See exercise 17.23 for an analysis of the Bayesian estimate. 

# 17.7 Summary 

In this chapter we examined parameter estimation for Bayesian networks when the data are complete. This is the simplest learning task for Bayesian networks, and it provides the basis for the more challenging learning problems we examine in the next chapters. We discussed two approaches for estimation: MLE and Bayesian prediction. Our primary emphasis here was on table-CPDs, although the ideas generalize to other representations. We touched on a few of these. 

As we saw, a central concept in both approaches is the likelihood function, which captures how the probability of the data depends on the choice of parameters. A key property of the likelihood function for Bayesian networks is that it decomposes as a product of local likelihood functions for the individual variables. If we use table-CPDs, the likelihood decomposes even further, as a product of the likelihoods for the individual multinomial distributions $P(X\mid\mathrm{pa}_{X_{i}})$ . This decomposition plays a central role in both maximum likelihood and Bayesian estimation, since it allows us to decompose the estimation problem and treat each of these CPDs or even each of the individual multinomials separately. 

When the local likelihood has sufcient statistics, then learning is viewed as mapping values of the statistics to parameters. For networks with discrete variables, these statistics are counts of the form $M[x_{i},\mathrm{pa}_{X_{i}}]$ . Thus, learning requires us to collect these for each combination $x_{i},\mathrm{pa}_{X_{i}}$ of a value of $X_{i}$ and an instantiation of values to its parents. We can collect all of these counts in one pass through the data using a data structure whose size is proportional to the representation of the network, since we need one counter for each CPD entry. 

Once we collect the sufcient statistics, the estimate of both methods is similar. The MLE estimate for table-CPDs has a simple closed form: 

$$
\hat{\theta}_{x_{i}\mid\mathrm{pa}_{X_{i}}}=\frac{M[x_{i},\mathrm{pa}_{X_{i}}]}{M[\mathrm{pa}_{X_{i}}]}.
$$ 

The Bayesian estimate is based on the use of a Dirichlet distribution, which is a conjugate prior to the multinomial distribution. In a conjugate prior, the posterior — which is proportional to the prior times the likelihood — has the same form as the prior. This property allows us to maintain posteriors in closed form. In particular, for a discrete Bayesian network with table-CPDs and a Dirichlet prior, the posterior of the local likelihood has the form 

$$
P(x_{i}\mid\mathrm{pa}_{X_{i}},D)=\frac{M[x_{i},\mathrm{pa}_{X_{i}}]+\alpha_{x_{i}\mid\mathrm{pa}_{X_{i}}}}{M[\mathrm{pa}_{X_{i}}]+\alpha_{\mathrm{pa}_{X_{i}}}}.
$$ 

Since all we need in order to learn are the sufcient statistics, then we can easily adapt them to learn in an online setting, where additional training examples arrive. We simply store a vector of sufcient statistics, and update it as new instances are obtained. 

In the more advanced sections, we saw that the same type of structure applies to other parameter iz at ions in the exponential family. Each family deﬁnes the sufcient statistics we need to accumulate and the rules for ﬁnding the MLE parameters. We developed these rules for Gaussian CPDs, where again learning can be done using a closed-form analytical formula. 

We also discussed networks where some of the parameters are shared, whether between CPDs or within a single CPD. We saw that the same properties described earlier — decomposition and sufcient statistics — allow us to provide an easy analysis for this setting. The likelihood function is now deﬁned in terms of sufcient statistics that aggregate statistics from diferent parts of the network. Once the sufcient statistics are deﬁned, the estimation procedures, whether MLE or Bayesian, are exactly the same as in the case without shared parameters. 

Finally, we examined the theoretical foundations of learning. We saw that parameter estimates are asymptotically correct in the following sense. If the data are actually generated from the given network structure, then, as the number of samples increases, both methods converge to the correct parameter setting. If not, then they converge to the distribution with the given structure that is “closest” to the distribution from which the data were generated. We further analyzed the rate at which the estimates converge. As $M$ grows, we see a concentration phenomenon ; for most samples, the empirical distribution is in a close neighborhood of the true distribution. Thus, the chances of sampling a data set in which the MLE estimates are far from the true parameters decays exponentially with $M$ . This analysis allowed us to provide a PAC-bound on the number of samples needed to obtain a distribution that is “close” to optimal. 

# 17.8 Relevant Literature 

The foundations of maximum likelihood estimation and Bayesian estimation have a long history; see DeGroot (1989); Schervish (1995); Hastie et al. (2001); Bishop (2006); Bernardo and Smith (1994) for some background material. The thumbtack example is adapted from Howard (1970). 

Heckerman (1998) and Buntine (1994, 1996) provide excellent tutorials and reviews on the basic principles of learning Bayesian networks from data, as well as a review of some of the key references. 

The most common early application of Bayesian network learning, and perhaps even now, is learning a naive Bayes model for the purpose of classiﬁcation (see, for example, Duda and Hart 1973). Spiegelhalter and Lauritzen (1990) laid the foundation for the problem of learning general Bayesian networks from data, including the introduction of the global parameter independence assumption, which underlies the decomposability of the likelihood function. This development led to a stream of signiﬁcant extensions, most notably by Buntine (1991); Spiegelhalter et al. (1993); Cooper and Herskovits (1992). Heckerman et al. (1995) deﬁned the BDe prior and showed its equivalence to a combination of assumptions about the prior. 

Many papers (for example, Spiegelhalter and Lauritzen 1990; Neal 1992; Buntine 1991; Diez 1993) have proposed the use of structured CPDs as an approach for reducing the number of parameters that one needs to learn from data. In many cases, the speciﬁc learning algo- rithms are derived from algorithms for learning conditional probability models for probabilistic classiﬁcation. The probabilistic derivation of tree-CPDs was performed by Buntine (1993) and introduced to Bayesian networks by Friedman and Goldszmidt (1998). The analysis of Bayesian learning of Bayesian networks with linear Gaussian dependencies was performed by Heckerman and Geiger (1995); Geiger and Heckerman (Geiger and Heckerman). Buntine (1994) emphasizes the important connection between the exponential family and the task of learning Bayesian networks. Bernardo and Smith (1994) describe conjugate priors for many distributions in the exponential family. Some material on nonparametric density estimation can be found in Hastie et al. (2001); Bishop (2006). Hofmann and Tresp (1995) use Parzen window to capture conditional distributions in continuous Bayesian networks. Imoto et al. (2003) learn semiparametric spline- regression models as CPDs in Bayesian networks. Rasmussen and Williams (2006) describe Gaussian processes , a state-of-the-art method for nonparametric estimation, which has also been used for estimating CPDs in Bayesian networks (Friedman and Nachman 2000). 

Plate models as a representation for parameter sharing in learning were introduced by Gilks et al. (1994) and Buntine (1994). Hierarchical Bayesian models have a long history in statistics; see, for example, Gelman et al. (1995). 

The generalization bounds for parameter estimation in Bayesian networks were ﬁrst analyzed by Höfgen (1993), and subsequently improved and reﬁned by Friedman and Yakhini (1996) and Dasgupta (1997). 

Beinlich et al. (1989) introduced the ICU-Alarm network, which has formed the benchmark for numerous studies of Bayesian network learning. 

# 17.9 Exercises 

# Exercise 17.1 

Show that the estimate of equation (17.1) is the maximum likelihood estimate. Hint: diferentiate the log-likelihood with respect to $\theta$ . 

# Exercise ${\bf17.2\star}$ 

Derive the MLE for the multinomial distribution (example 17.5). Hint, maximize the log-likelihood function using a Lagrange coefcient to enforce the constraint $\textstyle\sum_{k}\theta_{k}=1$ . 

# Exercise 17.3 

Derive the MLE for Gaussian distribution (example 17.6). Solve the equations 

$$
\begin{array}{r l r}{\displaystyle\frac{\partial\log L\big(\mathcal{D}:\mu,\sigma\big)}{\partial\mu}}&{=}&{0}\\ {\displaystyle\frac{\partial\log L\big(\mathcal{D}:\mu,\sigma\big)}{\partial\sigma^{2}}}&{=}&{0.}\end{array}
$$ 

# Exercise 17.4 

Derive equation (17.8) by diferentiating the log-likelihood function and using equation (17.6) and equa- tion (17.7). 

# Exercise $17.5\star$ 

In this exercise, we examine how to estimate a joint multivariate Gaussian. Consider two continuous variables $X$ and $Y$ , and assume we have a data set consisting of $M$ samples $\mathcal{D}=\{\langle x[m],y[m]\rangle:m=$ $1,\cdot\cdot\cdot,M\}$ . LE estimate for a joint Gaussian distribution over $X$ , $Y$ is the Gaussian with mean vector ⟨ $\langle E_{\mathcal{D}}[X],E_{\mathcal{D}}[Y]\rangle$ D D ⟩ , and covariance matrix 

$$
\Sigma_{X,Y}=\left(\begin{array}{l l}{{\pmb{C}o v_{\mathcal{D}}[X;X]}}&{{\pmb{C}o v_{\mathcal{D}}[X;Y]}}\\ {{\pmb{C}o v_{\mathcal{D}}[X;Y]}}&{{\pmb{C}o v_{\mathcal{D}}[Y;Y]}}\end{array}\right).
$$ 

# Exercise $17.6\star$ 

Derive equation (17.10) by solving the integral $\begin{array}{r}{\int_{0}^{1}\theta^{k}(1-\theta)^{M-k}d\theta}\end{array}$ for diferent values of $k$ . (Hint: use integration by parts.) 

# Exercise $17.7\star$ 

mixture of Dirichlets In this problem we consider the class of parameter priors deﬁned as a mixture of Dirichlets . These comprise a richer class of priors than the single Dirichlet that we discussed in this chapter. A mixture of Dirichlets represents another level of uncertainty, where we are unsure about which Dirichlet distribution is a more appropriate prior for our domain. For example, in a simple coin-tossing situation, we might be uncertain whether the coin whose parameter we are trying to learn is a fair coin, or whether it is a biased one. In this case, our prior might be a mixture of two Dirichlet distributions, representing those two cases. 

In this problem, our goal is to show that the family of mixture of Dirichlet priors is conjugate to the multinomial distribution; in other words, if our prior is a mixture of Dirichlets, and our likelihood function is multinomial, then our posterior is also a mixture of Dirichlets. 

a. Consider the simple possibly biased coin setting described earlier. Assume that we use a prior that is a mixture of two Dirichlet (Beta) distributions: $P(\theta)=0.95\cdot B e t a(5000,5000)+0.05\cdot\dot{B}e t a(1,1)$ ; the ﬁrst component represents a fair coin (for which we have seen many imaginary samples), and the second represents a possibly-biased coin, whose parameter we know nothing about. Show that the expected probability of heads given this prior (the probability of heads averaged over the prior) is $1/2$ . Suppose that we observe the data sequence $(H,H,T,H,H,H,H,H,H,H)$ . Calculate the posterior over $\theta$ , $P(\theta\mid\mathcal{D})$ . Sh xture of Beta distributions, by writing the posterior in the form λ $\lambda^{1}B e t a(\alpha_{1}^{1},\alpha_{2}^{1})+\lambda^{2}B\bar{e t a}(\alpha_{1}^{2},\alpha_{2}^{2})$ . Provide actual numeric values for the diferent parameters $\lambda^{1},\lambda^{2},\alpha_{1}^{1},\alpha_{2}^{1},\alpha_{1}^{2},\alpha_{2}^{2}$ . 

b. Now generalize your calculations from part (1) to the case of a mixture of $d$ Dirichlet priors over a $k$ -valued multinomial parameters. More precisely, assume that the prior has the form 

$$
P(\pmb\theta)=\sum_{i=1}^{d}\lambda^{i}D i r i c h l e t(\alpha_{1}^{i},.\,.\,.\,,\alpha_{k}^{i}),
$$ 

and prove that the posterior has the same form. 

# Exercise ${\bf17.8\star}$ 

We now consider a Bayesian approach for learning the mean of a Gaussian distribution. It turns out that in doing Bayesian inference with Gaussians, it is mathematically easier to use the precision $\textstyle\tau={\frac{1}{\sigma^{2}}}$ rather than the variance. Note that larger the precision, the narrower the distribution around the mean. 

Suppose that we have $M$ IID samples $x[1],\cdot\cdot\cdot,x[M]$ fr m $X\,\sim{\mathcal{N}}\left(\theta;\tau_{X}^{-1}\right)$    . Moreover, assume that we know the value of $\tau_{X}$ . Thus, the unknown parameter θ is the mean. Show that if the prior $P(\theta)$ is $\mathcal{N}\left(\mu;\tau_{\theta}^{-1}\right)$ , then the posterior $P(\theta\mid\mathcal{D})$ is $\mathcal{N}\left(\bar{\mu}^{\prime};(\tau_{\theta}^{\prime})^{-1}\right)$ where 

$$
\begin{array}{r c l}{{\tau_{\theta}^{\prime}}}&{{=}}&{{M\tau_{X}+\tau_{\theta}}}\\ {{\mu^{\prime}}}&{{=}}&{{\displaystyle\frac{M\tau_{X}}{\tau_{\theta}^{\prime}}{\cal E}_{\mathcal D}[X]+\frac{\tau_{\theta}}{\tau_{\theta}^{\prime}}\mu_{0}.}}\end{array}
$$ 

Hin : Start by proving $\begin{array}{r}{\sum_{m}(x[m]-\theta)^{2}=M(\theta-{\cal E}_{\mathcal{D}}[X])+c,}\end{array}$ , where $c$ is a constant that does not depend on θ . 

# Exercise 17.9 

We now consider making predictions with the posterior of exercise 17.8. Suppose we now want to compute the probability 

$$
P(x[M+1]\mid\mathcal{D})=\int P(x[M+1]\mid\theta)P(\theta\mid\mathcal{D})d\theta.
$$ 

Show that this distribution is Gaussian. What is the mean and precision of this distribution? 

# Exercise $17.10\star$ 

We now consider the complementary case to exercise 17.8, where we know the mean of $X$ , but do not know the precision. Suppose that $\stackrel{\cdot}{X}\sim\mathcal{N}\left(\mu;\theta^{-1}\right)$ , where $\theta$ is the unknown precision. 

We start with a deﬁnition. We say that $Y\sim\operatorname{Gamma}\left(\alpha;\beta\right)$ ∼ (for $\alpha,\beta>0$ ) if 

$$
P(y:\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}y^{\alpha}e^{-\beta y}.
$$ 

Gamma distribution This distribution is called a Gamma distribution . Here, we have that $\begin{array}{r}{{\pmb E}[Y]=\frac{\alpha}{\beta}}\end{array}$ and $\begin{array}{r}{\mathbb{W}a r[Y]=\frac{\alpha}{\beta^{2}}}\end{array}$ . 

a. Show that Gamma distributions are a conjugate family for this learning task. More precisely, show that if $P(\theta)\sim\mathrm{Gamma}\left(\alpha;\beta\right)$ , then $P(\theta\mid\mathcal{D})\stackrel{.}{\sim}\mathrm{Gamma}\left(\alpha^{\prime};\beta^{\prime}\right)$ where 

$$
\begin{array}{r c l}{{\alpha^{\prime}}}&{{=}}&{{\alpha+\displaystyle\frac{1}{2}{\cal M}}}\\ {{\beta^{\prime}}}&{{=}}&{{\beta+\displaystyle\frac{1}{2}\sum_{m}(x[m]-\mu)^{2}.}}\end{array}
$$ 

Hint: do not worry about the normalization constant, instead focus on the terms in the posterior that involve $\theta$ . 

b. Derive the mean and variance of $\theta$ in the posterior. What can we say about beliefs given the data? How do they difer from the MLE estimate? 

# Exercise ${\bf17.1l\star\star}$ 

Now consider the case where we know neither the mean nor the precision of $X$ . We examine a family of distributions that are conjugate in this case. 

normal-Gamma distribution 

A normal-Gamma distribution over $\mu,\tau$ is of the form: 

$$
P(\tau)\mathcal{P}(\mu\mid\tau)
$$ 

where $P(\tau)$ is Gamma $(\alpha;\beta)$ and $P(\mu\mid\tau)$ is $\mathcal{N}\left(\mu_{0};\lambda\tau\right)$ . That is, the distribution over precisions is a Gamma distribution (as in exercise 17.10), and the distribution over the mean is a Gaussian (as in exercise 17.8), except that the precision of this distribution depends on $\tau$ . 

Show that if $P(\mu,\tau)$ is normal-Gamma with parameters $\alpha,\beta,\mu_{0},\lambda,$ , then the posterior $P(\mu,\tau\mid\mathcal{D})$ is also a Normal-Gamma distribution. 

# Exercise 17.12 

Suppose that a prior on a parame ector is $p(\pmb\theta)\sim\mathit{D i r i c h l e t}(\alpha_{1},.\,.\,.\,,\alpha_{k})$ . What is the MAP value of the parameters, that is, arg max θ $p(\theta)$ ? 

# Exercise $17.13\star$ 

In this exercise, we will deﬁne a general-purpose prior for models with shared parameters, along the lines of the BDe prior of section 17.4.3. 

a. Following the lines of the derivation of the BDe prior, construct a parameter prior for a network with shared parameters, using the uniform distribution $P^{\prime}$ as the basis. b. Now, extend your analysis to construct a BDe-like parameter prior for a plate model, using, as the basis, a sample size of $\alpha(\mathsf Q)$ for each $\mathsf Q$ and a uniform distribution. 

# Exercise 17.14 

Perform the analysis of section 17.5.1.1 in the case where the network is a Gaussian Bayesian network. Derive the form of the likelihood function in terms of the appropriate aggregate sufcient statistics, and show how the MLE is computed from these statistics. 

# Exercise 17.15 

In section 17.5, we discussed sharing at both the global and the local level. We now consider an example where we have both. Consider the following elaboration of our University domain: Each course has an additional attribute Level , whose values are undergrad $(l^{0})$ or grad $(l^{1})$ . The grading curve (distribution for Grade ) now depends on Level : The curve is the same for all undergraduate courses, and depends on Difculty and Intelligence , as before. For graduate courses, the distribution is diferent for each course and, moreover, does not depend on the student’s intelligence. 

Specify the set of multinomial parameters in this model, and the partition of the multinomial distributions that correctly captures the structure of the parameter sharing. 

# Exercise 17.16 

a. Using the techniques and notation of section 17.5.1.2, describe the likelihood function for the DBN model of ﬁgure 6.2. Your answer should deﬁne the set of shared parameters, the partition of the variables in the ground network, the aggregate sufcient statistics, and the MLE. b. Now, assume that we want to use Bayesian inference for the parameter estimation in this case. Assuming local parameter independence and a Dirichlet prior, write down the form of the prior and the posterior. How would you use your learned model to compute the probability of a new trajectory? 

# Exercise 17.17 

Consider the application of the global sharing techniques of section 17.5.1.2 to the task of parameter estimation in a PRM. A key diference between PRMs and plate models is that the diferent instantiations of the same attribute in the ground network may not have the same in-degree. For instance, returning to example 6.16, let $J o b(S)$ be an attribute such that $S$ is a logical variable ranging over Student . Assume that $\hat{J o b}(S)$ depends on the average grade of all courses that the student has taken (where we round the grade-point-average to produce a discrete set of values). Show how we can parameterize such a model, and how we can aggregate statistics to learn its parameters. 

# Exercise 17.18 

Suppose we have a single multinomial variable $X$ with $K$ values and we have a prior on the parameters governing $X$ so that $\theta\sim\mathit{D i r i c h l e t}(\alpha_{1},.\,.\,.\,,\alpha_{K})$ . Assume we have some data set $\dot{\mathcal{D}}=\{x[1],\hat{.}\hat{.}\cdot,x[M]\}$ . 

a. Show how to compute 

$$
P(X[M+1]=x^{i},X[M+2]=x^{j}\mid\mathcal{D}).
$$ 

(Hint: use the chain rule for probabilities.) 

b. Suppose we decide to use the approximation 

$$
\begin{array}{r}{P(X[M+1]=x^{i},X[M+2]=x^{j}\mid\mathcal{D})\approx P(X[M+1]=x^{i},\mid\mathcal{D})P(X[M+2]=x^{j}\mid\mathcal{D}).}\end{array}
$$ 

That is, we ignore the dependencies between $X[M+1]$ and $X[M+2]$ . Analyze the error in this approximation (the ratio between the approximation and the correct probability). What is the quality of this approximation for small $M?$ What is th mptotic behavior of t proximation when $M\rightarrow\infty$ . (Hint: deal separately with the case where i $i=j$ and the case where i $i\neq j$ ̸ .) 

# Exercise $17.19\star$ 

We now prove a variant on proposition 17.7. Show that in the setting of that proposition 17.7 

$\begin{array}{r}{P(D(P\|\hat{P})>\epsilon)\le K e^{-2M\epsilon^{2}\frac{1}{K^{2}}\frac{2}{(1+\frac{\epsilon}{K\lambda})^{2}}}}\end{array}$ where $K=|V a l(X)|$ . 

a. Show that 

$$
P(\boldsymbol{D}(\boldsymbol{P}\|\hat{\boldsymbol{P}})>\epsilon)\le\sum_{x}P(\log\frac{P^{*}(x)}{\hat{P}(x)}>\frac{\epsilon}{K P(x)}).
$$ 

b. Use this result and lemma 17.1 to prove the stated bound. 

c. Show that the stated bound is tighter than the original bound of proposition 17.7. (Hint: examine the case when $\begin{array}{r}{\lambda=\frac{1}{K}}\end{array}$ .) 

# Exercise 17.20 

Prove theorem 17.3. Speciﬁcally, ﬁrst prove that $\begin{array}{r}{P_{\pmb\theta^{\mathrm{opt}}}=\prod_{i}P^{*}(X_{i}\ |\ \mathrm{Pa}_{X_{i}}^{\mathcal{G}})}\end{array}$ | and then use theorem 8.5. 

# Exercise 17.21 

Prove lemma 17.2. Hint: Show that $D(P(X,Y)\|Q(X,Y))=D(P(X\mid Y)\|Q(X\mid Y))+D(P(X)\|Q(X)),$ and then show that the inequality follows. 

# Exercise 17.22 

Suppose $P$ is a Bayesian network with $P(x_{i}\mid\mathrm{{pa}}_{i})\geq\lambda$ for all $i$ , $x_{i}$ and $\mathrm{pa}_{i}$ . Consider a family $X_{i},\mathrm{Pa}_{i}$ , show that 

$$
P(x_{i},\mathrm{{pa}}_{i})\geq\lambda^{|\mathrm{{Pa}}_{i}|+1}.
$$ 

# Exercise ${\bf17.23\star}$ 

We w prove a bound on the error when ing Ba sian e mates. Let $\mathcal{D}=\{X[1],\cdot\cdot\cdot,X[M]\}$ consist of M IID samples of a discrete variable X . Let α and P $P_{0}$ be the equivalent sample size and prior distribution for a Dirichlet prior. The Bayesian estimator will return the distribution 

$$
\tilde{P}(x)=\frac{M}{M+\alpha}\hat{P}(x)+\frac{\alpha}{M+\alpha}P_{0}(x).
$$ 

We now want to analyze the error of such an estimate. 

a. Prove the analogue of lemma 17.1. Show that 

$$
P(\log\frac{P^{*}(x)}{\tilde{P}(x)}>\epsilon)\leq e^{-\frac{2M(\frac{M}{M+\alpha}P^{*}(x)+\frac{\alpha}{M+\alpha}P_{0}(x))^{2}\epsilon_{x}^{2}}{(1+\epsilon_{x})^{2}}}.
$$ 

b. Use the union bound to show that if $P^{*}(x)\geq\lambda$ and $P_{0}(x)\ge\lambda_{0}$ for all $x\in V a l(X)$ , then 

$$
\begin{array}{r}{P(\mathcal{D}(P^{*}(X)\|\tilde{P}(X))>\epsilon)\le|V a l(X)|e^{-2M(\frac{M}{M+\alpha}\lambda+\frac{\alpha}{M+\alpha}\lambda_{0})^{2}\epsilon^{2}\frac{1}{(1+\epsilon)^{2}}}.}\end{array}
$$ 

c. Show that 

$$
\frac{M}{M+\alpha}\lambda+\frac{\alpha}{M+\alpha}\lambda_{0}\geq\operatorname*{max}(\lambda,\frac{\alpha}{M+\alpha}\lambda_{0}).
$$ 

d. Suppose that $\lambda_{0}>\lambda$ . That is, our prior is less extreme than the real distribution, which is deﬁnitely the case if we take $P_{0}$ to be the uniform distribution. What can you conclude about a PAC result for the Bayesian estimate? 

# 18 Structure Learning in Bayesian Networks 

# 18.1 Introduction 

# 18.1.1 Problem Deﬁnition 

In the previous chapter, we examined how to learn the parameters of Bayesian networks. We made a strong assumption that we know in advance the network structure, or at least we decide on one regardless of whether it is correct or not. In this chapter, we consider the task of learning in situations where do not know the structure of the Bayesian network in advance. Throughout this chapter, we continue with the (very) strong assumption that our data set is fully observed, deferring the discussion of learning with partially observed data to the next chapter. 

As in our ion so far, we assume that th ata $\mathcal{D}$ are generated IID from an underly distr tion $P^{*}(\mathcal{X})$ X . Here, we also assume that P $P^{*}$ is induced by som ayesian netw k G $\mathcal{G}^{\ast}$ over X . We begin by considering the extent to which independencies in G $\mathcal{G}^{*}$ manifest in D . 

# Example 18.1 

Consider an experiment where we toss two standard coins $X$ and $Y$ independently. We are given a data set with 100 instances of this experiment. We would like to learn a model for this scenario. A “typical” data set may have 27 head/head, 22 head/tail, 25 tail/head, and 26 tail/tail entries. In the empirical distribution, the two coins are not independent. This may seem reasonable, since the probability of tossing 100 pairs of fair coins and getting exactly 25 outcomes in each category is quite small (approximately $1/1,000)$ . Thus, even if the two coins are independent, we do not expect the observed empirical distribution to satisfy independence. 

Now suppose we get the same results in a very diferent situation. Say we scan the sports section of our local newspaper for 100 days and choose an article at random each day. We mark $X=x^{1}$ if the word “rain” appears in the article and $X=x^{0}$ otherwise. Similarly, $Y$ denotes whether the word “football” appears in the article. Here our intuitions as to whether the two random variables are independent are unclear. If we get the same empirical counts as in the coins described before, we might suspect that there is some weak connection. In other words, it is hard to be sure whether the true underlying model has an edge between $X$ and $Y$ or not. 

The importance of correctly reconstructing the network structure depends on our learning goal. As we discussed in chapter 16, there are diferent reasons for learning the model structure. One is for knowledge discovery : by examining the dependencies in the learned network, we can learn the dependency structure relating variables in our domain. Of course, there are other methods that reveal correlations between variables, for example, simple statistical independence tests . A Bayesian network structure, however, reveals much ﬁner structure. For instance, it can potentially distinguish between direct and indirect dependencies, both of which lead to correlations in the resulting distribution. 

I-equivalence 

identiﬁability 

density estimation generalization 

data fragmentation 

If our goal is to understand the domain structure, then, clearly, the best answer we can aspire to is recoverin $\mathcal{G}^{*}$ . Even here, must be careful. Recall that there can be many perfect maps for a distribution $P^{*}$ : all of th etworks in the same I-equivalence class as $\mathcal{G}^{\ast}$ . All of these are equally good structures for $P^{*}$ , and therefore we cannot distinguish between them based only on the data $D$ . In other words, $\mathcal{G}^{\ast}$ is not identiﬁable fro he data. Thus, the best we can hope for is an algorithm that, asymptotically, recovers $\mathcal{G}^{\ast}\mathbf{\ddot{s}}$ uivalence class. 

Unfortunately, as our example indicate he goal of learning G $\mathcal{G}^{*}$ (or an equivalent network) is hard to achieve. The data sampled from $P^{*}$ are noisy and do not reconstruct this distribution perfectly. We cannot detect with complete reliability which independencies are present in the underlying distribution. Therefore, we must generally make a decision about our willingness to include in our learned model edges about which we are less sure. If we include more of these edges, we will often learn a model that contains spurious edges. If we include fewer edges, we may miss dependencies. Both compromises lead to inaccurate structures that do not reveal the correct underlying structure. The decision of whether it is better to have spurious correlations or spurious independencies depends on the application. 

The second and more common reason to learn a network structure is in an attempt to per- form density estimation — that is, to estimate a statistical model of the underlying distribution. As we discussed, our goal is to use this model for reasoning about instances that were not in our training data. In other words, we want our network model to generalize to new instances. It seems intuitively reasonable that because $\mathcal{G}^{\ast}$ captures the true dependencies and indepen- dencies in the domain, the best generalization will be obtained if we recover the the structure $\mathcal{G}^{*}$ . Moreover, it seems that if we do make mistakes in the structure, it is better to e too many rather than too few edges. With an overly complex structure, we can still capture $P^{*}$ , and thereby represent the true distribution. 

Unfortunately, the situation is somewhat more complex. Let us go back to our coin example and assume that we had 20 data cases with the following frequencies: 3 head/head, 6 head/tail, 5 tail/head, and 6 tail/tail. We can introduce a spurious correlation between $X$ and $Y$ , which would give us, using maximum likelihood estimation, the parameters $P(X\;=\;H)\;=\;0.45$ , $P(Y\,=\,H\mid\,X\,=\,H)\,=\,1/3$ , and $P(Y\,=\,H\mid\,X\,=\,T)\,=\,5/11$ . On the ot r hand, in the independent structure (with no edge between X and Y ), the parameter of Y $Y$ would be $P(Y=H)=0.4.$ . All of these parameter estimates are imperfect, of course, but the ones in the more complex model are signiﬁcantly more likely to be skewed, because each is estimated from a much smaller data set. In particular, $P(Y\,=\,H\mid\,X\,=\,H)$ is estimated from a data set of 9 instances, as opposed to 20 for the estimation of $P(Y=H)$ . Recall that the standard deviation of the maximum likelihood estimate behaves as $1/\sqrt{M}$ . Thus, if the coins are fair, the standard deviation of the MLE estimate from 20 samples is approximately 0 . 11 , while the standard deviation from 9 samples is approximately 0 . 17 . This example is simply an instance of the data fragmentation issue that we discussed in section 17.2.3 in the previous chapter. As we discussed, when we add more parents to the variable $Y$ , the data used to estimate the CPD fragment into more bins, leaving fewer instances in each bin to estimate the parameters and reducing the quality of the estimated parameters. In a table-CPD, the number of bins grows exponentially with the number of parents, so the (statistical) cost of adding a parent can be very large; moreover, because of the exponential growth, the incremental cost of adding a parent grows with the number of parents already there. 

Thus, when doing density estimation from limited data, it is often better to prefer a sparser structure. The surprising fact is that this observation applies not only to networks that include spurious edges relative to $\mathcal{G}^{\ast}$ , but also to edges in $\mathcal{G}^{*}$ . That is, we can sometimes learn a better model in term of generalization by learning a structure with fewer edges, even if this structure is incapable of representing the true underlying distribution. 

# 18.1.2 Overview of Methods 

Roughly speaking, there are three approaches to learning without a prespeciﬁed structure. 

constraint-based structure learning 

score-based structure learning model selection hypothesis space 

Bayesian model averaging 

One approach utilizes constraint-based structure learning . These approaches view a Bayesian network as a representation of independencies. They try to test for conditional dependence and independence in the data and then to ﬁnd a network (or more precisely an equivalence class of networks) that best explains these dependencies and independencies. Constraint-based methods are quite intuitive: they decouple the problem of ﬁnding structure from the notion of independence, and they follow more closely the deﬁnition of Bayesian network: we have a distribution that satisﬁes a set of independencies, and our goal is to ﬁnd an I-map for this distribution. Unfortunately, these methods can be sensitive to failures in individual indepen- dence tests. It sufces that one of these tests return a wrong answer to mislead the network construction procedure. 

The second approach is score-based structure learning . Score-based methods view a Bayesian network as specifying a statistical model and then address learning as a model selection problem. These all operate on the same principle: We deﬁne a hypothesis space of potential models — the set of possible network structures we are willing to consider — and a scoring function that measures how well the model ﬁts the observed data. Our computational task is then to ﬁnd the highest-scoring network structure. The space of Bayesian networks is a combinatorial space, consisting of a super exponential number of structures $-\;2^{O(n^{2})}$ . Therefore, even with a scoring function, it is not clear how one can ﬁnd the highest-scoring network. As we will see, there are very special cases where we can ﬁnd the optimal network. In general, however, the problem is (as usual) $\mathcal{N P}$ -hard, and we resort to heuristic search techniques. Score-based methods consider the whole structure at once; they are therefore less sensitive to individual failures and better at making compromises between the extent to which variables are dependent in the data and the “cost” of adding the edge. The disadvantage of the score-based approaches is that they pose a search problem that may not have an elegant and efcient solution. 

Finally, the third approach does not attempt to learn a single structure; instead, it generates an ensemble of possible structures. These Bayesian model averaging methods extend the Bayesian reasoning we encountered in the previous chapter and try to average the prediction of all possible structures. Since the number of structures is immense, performing this task seems impossible. For some classes of models this can be done efciently, and for others we need to resort to approximations. 

# 18.2 Constraint-Based Approaches 

# 18.2.1 General Framework 

In constraint-based approaches, we attempt to reconstruct a network structure that best captures the independencies in the domain. In other words, we attempt to ﬁnd the best minimal I-map for the domain. 

Recall that in chapter 3 we discussed algorithms for building I-maps and P-maps that assume that we can test for independence statements in the distribution. The algorithms for constraint- based learning are essentially variants of these algorithms. The main technical question is how to answer independence queries. For now, assume that we have some procedure that can answer such queries. That is, for a given distribution $P$ , the learning algorithm can pose a question, such as “Does $P$ satisfy $(X_{1}\;\perp\;X_{2},X_{3}\;\mid\;X_{4})^{*}$ ?” and receive a yes/no answer. The task of the algorithm is to carry out some algorithm that interacts with this procedure and results in a network structure that is the minimal I-map of $P$ . 

minimal I-map 

class PDAG 

independence test 

We have already seen such an algorithm in chapter 3: Build-Minimal-I-Map constructs a minimal $I^{,}$ -map given a ﬁxed ordering. For each variable $X_{i}.$ , it then searches for the minimal subset of $X_{1},\dots,X_{i-1}$ that render $X_{i}$ independent of the others. This algorithm was useful in illustrating the deﬁnition of an I-map, but it sufers from several drawbacks in the context of learning. First, the input order over variables can have a serious impact on the complexity of the network we ﬁnd. Second, in learning the parents of $X_{i}$ , this algorithm poses indepen- dence queries of the form $(X_{i}\perp\{X_{1},.\,.\,.\,,X_{i-1}\}-U\mid U)$ . These conditional independence statements involve a large number of variables. Although we do not assume much about the independence testing procedure, we do realize that independence statements with many vari- ables are much more problematic to resolve from empirical data. Finally, Build-Minimal-I-Map performs a large number of queries. For determining the parents of $X_{i}$ , it must, in principle, examine all the $2^{i-1}$ possible subsets of $X_{1},\dots,X_{i-1}$ . 

To avoid these problems, we learn an I-equivalence class rather than a single network, and we use a class PDAG to represent this class. The algorithm that we use is a variant of the Build-PDAG procedure of algorithm 3.5. As we discuss, this algorithm reconstructs the network that best matches the domain without a prespeciﬁed order and uses only a polynomial number of independence tests that involve a bounded number of variables. 

To achieve these performance guarantees, we must make some assumptions: 

• The network $\mathcal{G}^{*}$ has bounded indegree, that is, for all $i$ , $|\mathrm{Pa}_{X_{i}}^{\mathcal{G}^{\ast}}|\leq d$ for some constant $d$ .

 • The independence procedure can perfectly answer any independence query that involves up to $2d+2$ variables.

 • The underlying distribution $P^{*}$ is faithful to $\mathcal{G}^{*}$ , as in deﬁnition 3.8. 

The ﬁrst assumption states the boundaries of when we expect the algorithm to work. If the network is simple in this sense, the algorithm will be able to learn it from the data. If the network is more complex, then we cannot hope to learn it with “small” independence queries that involve only a few variables. 

The second assumption is stronger, since it requires that the oracle can deal with queries up to a certain size. The learning algorithm does not depend on how the these queries are answered. They might be answered by performing a statistical test for conditional dependence on a training data, or by an active mechanism that gathers more samples until it can reach a signiﬁcant conclusion about this relations. We discuss how to construct such an oracle in more detail later in this chapter. Note that the oracle can also be a human expert who helps in constructing a model of the network. 

The third assumption is the strongest. It is required to ensure that the algorithm is not misled by spurious independencies that are not an artifact of the oracle but rather exist in the domain. By requiring that $\mathcal{G}^{*}$ is a perfect map of $P^{*}$ , we rule out quite a few situations, for example, the (noisy) XOR example of example 3.6, and various cases where additional independencies arise from structure in the CPDs. 

Once we make these assumptions, the setting is precisely the one we tackled in section 3.4.3. Thus, given an oracle that can answer independence statements perfectly, we can now simply apply Build-PMap-Skeleton . Of course, determining independencies from the data is not a trivial problem, and the answers are rarely guaranteed to be perfect in practice. We will return to these important questions. For the moment, we focus on analyzing the number of independence queries that we need to answer, and thereby the complexity of the algorithm. 

Recall that, in the construction of perfect maps, we perform independence queries only in the Build-PMAP-Skeleton procedure, when we search for a witness to the separation between every pair of variables. These witnesses are also used within Mark-Immoralities to determine whether the two parents in a v-structure are conditionally independent. According to lemma 3.2, if $X$ and $Y$ are not adjacent i $\mathcal{G}^{\ast}$ , then either $\mathrm{Pa}_{X}^{\mathcal{G}^{\ast}}$ or $\mathrm{Pa}_{Y}^{\mathcal{G}^{\ast}}$ is a witness set. If we assume that $\mathcal{G}^{\ast}$ has indegree of at most d , we can therefore limit our attention to witness sets of size at most $d$ . Thus, the number of independence queries in this step is polynomial in $n$ , the number of variables. Of course, this number is exponential in $d$ , but we assume that $d$ is a ﬁxed constant throughout the analysis. 

Thus, given our assumptions, we can perform a variant of Build-PDAG that performs a polyno- mial number of independence tests. We can also check all other operations; that is, applying the edge orientation rules, we can also require a polynomial number of steps. Thus, the procedure is polynomial in the number of variables. 

# 18.2.2 Independence Tests 

The only remaining question is how to answer queries about conditional independencies be- tween variables in the data. As one might expect, this question has been extensively studied in the statistics literature. We brieﬂy touch on some of the issues and outline one commonly-used methodology to answer this question. 

hypothesis testing 

The basic query of this type is to determine whether two variables are independent. As in the example in the introduction to this chapter, we are given joint samples of two variables $X$ and $Y$ , and we want to determine whether $X$ and $Y$ are independent. This basic question is often referred to as hypothesis testing . 

# 18.2.2.1 Single-Sided Hypothesis Tests 

null hypothesis In hypothesis testing, we have a base hypothesis that is usually denoted by $H_{0}$ and is referred to as the null hypothesis . In the particular case of the independence test, the null hypothesis is “the data were sampled from a distribution $P^{*}(X,Y)\,=\,P^{*}(X)P^{*}(Y)$ .” Note that this 

assumption states that the data were sampled from a particular distribution in which $X$ and $Y$ are independent. In real life, we do not have access to $P^{*}(X)$ and $P^{*}(Y)$ . As a substitute, we use $\hat{P}(\dot{X})$ and ${\hat{P}}(Y)$ as our best approximation for this distribution. Thus, we usually form $H_{0}$ as the assumption that $P^{*}(X,Y)\stackrel{\cdot\cdot}{=}\hat{P}(X)\hat{P}(Y)$ . 

decision rule 

We want to test whether the data conform to this hypothesis. More precisely, we want to ﬁnd a procedure that we will call a decision rule that will take as input a data set $\mathcal{D}$ , and return a verdict, either Accept or Reject. We will denote the function the procedure computes to be $R(\mathcal{D})$ . If $R(\mathcal D)=\mathrm{Accept}$ , then we consider that the data satisf e hypothesis. In our case, that would mean that we believe that the data were sampled from $P^{*}$ and that the two variables are independent. Otherwise, we decide to reject the hypothesis, which in our case would imply that the variables are dependent. 

The question is then, of course, how to choose a “good” decision rule. A liberal decision rule that accepts many data sets runs the risk of accepting ones that do not satisfy the hypothesis. A conservative rule that rejects many data sets runs the risk of rejecting many that satisfy the hypothesis. The common approach to evaluating a decision rule is analyze the probability of false rejection. Suppose we have access to the distribution $P(\mathcal{D}:H_{0},M)$ of data sets of $M$ instances given the null hypothesis. That is, we can evaluate the probability of seeing each particular data set if the hypothesis happens to be correct. In our case, since the hypothesis speciﬁes the distribution $P^{*}$ , this distribution is just the probability of sampling the particular instances in the data set (we assume that the size of the data set is known in advance). 

If we have access to this distribution, we can compute the probability of false rejection: 

$$
P(\{\mathcal{D}:R(\mathcal{D})=\mathrm{Re}\mathrm{jet}\}\mid H_{0},M).
$$ 

Then we can say that a decision rule $R$ has a probability of false rejection $p$ . We often refer to $1-p$ as the conﬁdence in the decision to reject an hypothesis. 1 

At this point we cannot evaluate the probability of false acceptances. Since we are not willing to assume a concrete distribution on data sets that violate $H_{0}$ , we cannot quantify this probability. For this reason, the decision is not symmetric. That is, rejecting the hypothesis $^{\circ}X$ and $Y$ are independent” is not the same as accepting the hypothesis $^(X$ and $Y$ are dependent.” In particular, to deﬁne the latter hypothesis we need to specify a distribution over data sets. 

# 18.2.2.2 Deviance Measures 

deviance 

$\chi^{2}$ statistic The preceding discussion suggests how to evaluate decision rules. Yet, it leaves open the question of how to design such a rule. A standard framework for this question is to deﬁne a measure of deviance from the null hypothesis. Such a measure $d$ is a function from possible data sets to the real line. Intuitively, large value of $d(\mathcal{D})$ implies that $\mathcal{D}$ is far away from the null hypothesis. 

To consider a concrete example, suppose we have discrete-valued, independent random variables $X$ and $Y$ . Typically, we expect that the counts $M[x,y]$ in the data are close to $M\cdot\hat{P}(x)\cdot\hat{P}(y)$ · (where $M$ is the number of samples). This is the expected value of the count, and, as we know, deviances from this value are improbable for large $M$ . Based on this intuition, we can measure the deviance of the data from $H_{0}$ in terms of these distances. A common measure of this type is the $\chi^{2}$ statistic : 

$$
d_{\chi^{2}}(\mathcal{D})=\sum_{x,y}\frac{(M[x,y]-M\cdot\hat{P}(x)\cdot\hat{P}(y))^{2}}{M\cdot\hat{P}(x)\cdot\hat{P}(y)}.
$$ 

A data set that perfectly ﬁts the independence assumption has $d_{\chi^{2}}(\mathcal{D})=0$ , and a data set where the empirical and expected counts diverge signiﬁcantly has a larger value. 

mutual information 

Another potential deviance measure for the same hypothesis is the mutual information $I_{\hat{P}_{\mathsf{D}}}(X;Y)$ in the empirical distribution deﬁned by the data set $\mathcal{D}$ . In terms of counts, this can be written as 

$$
d_{I}\!\left(\mathcal{D}\right)=I_{\hat{P}_{\mathsf{D}}}(X;Y)=\sum_{x,y}\frac{M[x,y]}{M}\log\frac{M[x,y]/M}{M[x]/M\cdot M[y]/M}.
$$ 

In fact, these two deviance measures are closely related to each other; see exercise 18.1. 

Once we agree on a deviance measure $d$ (say the $\chi^{2}$ statistic or the empirical mutual infor- mation), we can devise a rule for testing whether we want to accept the hypothesis 

$$
R_{d,t}(\mathcal{D})=\left\{\begin{array}{l l}{\mathrm{accept}}&{d(\mathcal{D})\leq t}\\ {\mathrm{Reject}}&{d(\mathcal{D})>t.}\end{array}\right.
$$ 

This rule accepts the hypothesis if the deviance is small (less than the predetermined threshold $t)$ and rejects the hypothesis if the deviance is large. 

p-value 

The choice of threshold $t$ determines the false rejection probability of the decision rule. The computational problem is to compute the false rejection probability for di￿erent values of $t$ . This value is called the $p$ -value of $t$ : 

$$
\operatorname{p-value}(t)=P(\{{\mathcal{D}}:d({\mathcal{D}})>t\}\mid H_{0},M).
$$ 

# 18.2.2.3 Testing for Independence 

Using the tools we developed so far, we can reexamine the independence test. The basic tool we use is a test to reject the null hypothesis that distribution of $X$ and $Y$ is the one we would estimate if we assume that they are independent. The typical signiﬁcance level we use is 95 percent. That is, we reject the null hypothesis if the deviance in the observed data has $\mathrm{p}$ -value of 0 . 05 or less. 

If we want to test the independence of discrete categorical variables, we usually use the $\chi^{2}$ statistic or the mutual information. The null hypothesis is that $P^{*}(X,Y)=\hat{P}(X)\hat{P}(Y)$ . 

We start by considering how to perform an exact test . The deﬁnition of $p$ -value requires summing over all possible data sets. In fact, since we care only about the su￿cient statistics of $X$ and $Y$ in the data set, we can sum over the smaller space of di￿erent su￿cient statistics vectors. Sup se w have $M$ samples; we deﬁne the space $\mathcal{C}_{X,Y}^{M}$ to be the set of all empirical counts over X and Y $Y$ , we might observe in a data set with M samples. Then we write 

$$
\mathrm{p-value}(t)=\sum_{C[X,Y]\in{\mathcal{C}}_{\times,\mathbb{Y}}^{\mathbb{M}}}{\mathbf{I}}\{d(C[X,Y])>t\}P(C[X,Y]\mid H_{0},M),
$$ 

where $d(C[X,Y])$ is the deviance measure (that is, $\chi^{2}$ or mutual information) computed with the counts $C[X,Y]$ , and 

$$
P(C[X,Y]\mid H_{0},M)=M!\prod_{x,y}{\frac{1}{C[x,y]!}}P(x,y\mid H_{0})^{C[x,y]}
$$ 

is the probability of seeing a data set with these counts given $H_{0}$ ; see exercise 18.2. 

This exact approach enumerates through all data sets. This is clearly infeasible except for small values of $M$ . A more common approach is to examine the asymptotic distribution of $M[x,y]$ under the null hypothesis. Since this count is a sum of binary indicator variables, its distribution is approximately normal when $M$ is large enough. Statistical theory develops the asymptotic distribution of the deviance measure under the null hypothesis. For the $\chi^{2}$ statistic, this distribution is called the $\chi^{2}$ distribution . We can use the tail probability of this distribution to approximate $p$ -values for independence tests. Numerical procedures for such computations are part of most standard statistical packages. 

A natural extension of this test exists for testing conditional independence. Suppose we want to test whether $X$ and $Y$ are independent given $Z$ . Then, $H_{0}$ is that $P^{*}(X,Y,Z)\,=$ ${\hat{P}}(Z){\hat{P}}(X\mid Z){\hat{P}}(Y\mid Z)$ | | , and the $\chi^{2}$ statistic is 

$$
d_{\chi^{2}}(\mathcal{D})=\sum_{x,y,z}\frac{(M[x,y,z]-M\cdot\hat{P}(z)\hat{P}(x\mid z)\hat{P}(y\mid z))^{2}}{M\cdot\hat{P}(z)\hat{P}(x\mid z)\hat{P}(y\mid z)}.
$$ 

This formula extends easily to conditioning on a set of variables $Z$ . 

# 18.2.2.4 Building Networks 

multiple hypothesis testing We now return to the problem of learning network structure. With the methods we just discussed, we can evaluate independence queries in the Build-PDAG procedure, so that whenever the test rejects the null hypothesis we treat the variables as dependent. One must realize, however, that these tests are not perfect. Thus, we run the risk of making wrong decisions on some of the queries. In particular, if we use signiﬁcance level of 95 percent, then we expect that on average 1 in 20 rejections is wrong. When testing a large number of hypotheses, a scenario called multiple hypothesis testing , the number of incorrect conclusions can grow large, reducing our ability to reconstruct the correct network. We can try to reduce this number by taking stricter signiﬁcance levels (see exercise 18.3). This, however, runs the risk of making more errors of the opposite type. 

In conclusion, we have to be aware that some of the independence tests results can be wrong. The procedure Build-PDAG can be sensitive to such errors. In particular, one misleading independence test result can produce multiple errors in the resulting PDAG (see exercise 18.4). When we have relatively few variables and large sample size (and “strong” dependencies among variables), the reconstruction algorithm we described here is efcient and often manages to ﬁnd a structure that is quite close to the correct structure. When the independence test results are less pronounced, the constraint-based approach can run into trouble. 

# 18.3 Structure Scores 

As discussed earlier, score-based methods approach the problem of structure learning as an optimization problem. We deﬁne a score function that can score each candidate structure with respect to the training data, and then search for a high-scoring structure. As can be expected, one of the most important decisions we must make in this framework is the choice of scoring function. In this section, we discuss two of the most obvious choices. 

# 18.3.1 Likelihood Scores 

# 18.3.1.1 Maximum Likelihood Parameters 

A natural choice for scoring function is the likelihood function, which we used for parameter estimation. Recall that this function measures the probability of the data given a model. Thus, it seems intuitive to ﬁnd a model that would make the data as probable as possible. 

Assume that we want to maximize the likelihood of the model. In this case, our model is a pair $\langle\mathcal{G},\theta_{\mathcal{G}}\rangle$ . Our goal is to ﬁnd both a graph $\mathcal{G}$ and parameters $\theta_{\mathcal{G}}$ that maximize the likelihood. 

In the previous chapter, we determined how to maximize the likelihood for a given structure $\mathcal{G}$ . We simply use the maximum likelihood parameters $\hat{\boldsymbol{\theta}}_{\mathcal{G}}$ for that graph. A simple analysis now G shows that: 

$$
\begin{array}{r c l}{\underset{\mathcal{G},\theta_{\mathcal{G}}}{\operatorname*{max}}\,L(\langle\mathcal{G},\theta_{\mathcal{G}}\rangle:\mathcal{D})}&{=}&{\underset{\mathcal{G}}{\operatorname*{max}}[\underset{\theta_{\mathcal{G}}}{\operatorname*{max}}\,L(\langle\mathcal{G},\theta_{\mathcal{G}}\rangle:\mathcal{D})]}\\ &{=}&{\underset{\mathcal{G}}{\operatorname*{max}}[L(\langle\mathcal{G},\hat{\theta}_{\mathcal{G}}\rangle:\mathcal{D})].}\end{array}
$$ 

In other words, to ﬁnd the maximum likelihood $({\mathcal{G}},{\boldsymbol{\theta}}_{\mathcal{G}})$ pair, we should ﬁnd the graph structure $\mathcal{G}$ that achieves the highest likelihood when we use the MLE parameters for $\mathcal{G}$ . We deﬁne: 

$$
\begin{array}{r}{\mathrm{score}_{L}(\mathcal{G}\ :\ \mathcal{D})=\ell(\hat{\pmb{\theta}}_{\mathcal{G}}:\mathcal{D}),}\end{array}
$$ 

where $\ell(\hat{\pmb{\theta}}_{\mathcal{G}}:\mathcal{D})$ D is the logarithm of the likelihood function and $\hat{\boldsymbol{\theta}}_{\mathcal{G}}$ are the maximum likelihood G G parameters for $\mathcal{G}$ . (As usual, it will be easier to deal with the logarithm of the likelihood.) 

# 18.3.1.2 Information-Theoretic Interpretation 

To get a better intuition of the likelihood score, let us consider the scenario of example 18.1. Consider the model $\mathcal{G}_{0}$ where $X$ and $Y$ are independent. In this case, we get 

$$
\mathrm{score}_{L}({\mathcal{G}}_{0}\ :\ {\mathcal{D}})=\sum_{m}\log{\hat{\theta}}_{x[m]}+\log{\hat{\theta}}_{y[m]}.
$$ 

On the other hand, we can consider the model $G_{1}$ where there is an arc $X~\rightarrow~Y$ . The log-likelihood for this model is 

$$
\mathrm{score}_{L}(\mathcal{G}_{1}\ :\ \mathcal{D})=\sum_{m}\log\hat{\theta}_{x[m]}+\log\hat{\theta}_{y[m]|x[m]},
$$ 

where $\hat{\theta}_{x}$ is again the maximum likelihood estimate for $P(x)$ , and ${\hat{\theta}}_{y\mid x}$ is the maximum likeli- | hood estimate for $P(y\mid x)$ . 

We see that the score of two models share a common component (the terms of the form $\log{\hat{\theta}_{x}})$ ). Thus, we can write the diference between the two scores as 

$$
\mathrm{score}_{L}(\mathcal G_{1}\ :\ \mathcal D)-\mathrm{score}_{L}(\mathcal G_{0}\ :\ \mathcal D)=\sum_{m}\log\hat{\theta}_{y[m]|x[m]}-\log\hat{\theta}_{y[m]}.
$$ 

By counting how many times each conditional probability parameter appears in this term, we can write this sum as: 

$$
\mathrm{score}_{L}(\mathcal G_{1}\ :\ \mathcal D)-\mathrm{score}_{L}(\mathcal G_{0}\ :\ \mathcal D)=\sum_{x,y}M[x,y]\log\widehat{\theta}_{y|x}-\sum_{y}M[y]\log\widehat{\theta}_{y}.
$$ 

Let $\hat{P}$ be the empirical distribution observed in the data; that is, $\hat{P}(x,y)$ is simply the empirical frequency of $x,y$ in $D$ . Then, we can write $M[x,y]\,=\,M\cdot\hat{P}(x,y)$ , and $\bar{M}[y]\,=\,\bar{M}\hat{P}(y)$ . Moreover, it is easy to check that ${\hat{\theta}}_{y\mid x}={\hat{P}}(y\mid x)$ , and that $\hat{\theta}_{y}=\hat{P}(y)$ . We get: | 

$$
\mathrm{score}_{L}(\mathcal G_{1}\ :\ \mathcal D)-\mathrm{score}_{L}(\mathcal G_{0}\ :\ \mathcal D)=M\sum_{x,y}\hat{P}(x,y)\log\frac{\hat{P}(y\mid x)}{\hat{P}(y)}=M\cdot\mathbf{I}_{\hat{P}}(X;\mathcal{M}).
$$ 

mutual information where $I_{\hat{P}}(X;Y)$ is the mutual information between $X$ and $Y$ in the distribution $\hat{P}$ . 

W see that the likelihood of the model $\mathcal{G}_{1}$ depends on the mutual information between $X$ and Y . Recall that higher mutual information implies stronger dependency. Thus, stronger dependency implies stronger preference for the model where $X$ and $Y$ depend on each other. 

Can we generalize this information-theoretic formulation of the maximum likelihood score to general network structures? Going through a similar arithmetic transformations, we can prove the following result. 

Proposition 18.1 

decomposable score 

The likelihood score decomposes as follows: 

$$
\mathrm{score}_{L}(\mathcal G\ :\ \mathcal D)=M\sum_{i=1}^{n}I_{\hat{P}}(X_{i};\mathrm{Pa}_{X_{i}}^{\mathcal G})-M\sum_{i=1}^{n}H_{\hat{P}}(X_{i}).
$$ 

Proof We have already seen that by combining all the occurrences of each parameter $\theta_{x_{i}\mid u}$ , we can rewrite the log-likelihood function as 

$$
\ell(\hat{\pmb\theta}_{\mathcal{G}}:\mathcal{D})=\sum_{i=1}^{n}\left[\sum_{\pmb{u}_{i}\in V a l(\mathrm{Pa}_{X_{i}}^{\mathcal{G}})}\sum_{x_{i}}M[x_{i},\pmb{u}_{i}]\log\hat{\theta}_{x_{i}|\pmb{u}_{i}}\right].
$$ 

Consider one of the terms in the square brackets, and let $U_{i}=\mathrm{Pa}_{X_{i}}$ 

$$
\begin{array}{r l}{\displaystyle\frac{1}{M}\displaystyle\sum_{u_{i}}\sum_{x_{i}}M[x_{i},{\mathbf{u}}_{i}]\log\hat{\theta}_{x_{i}|{\mathbf{u}}_{i}}}\\ {=}&{\displaystyle\sum_{u_{i}}\sum_{x_{i}}\hat{P}(x_{i},{\mathbf{u}}_{i})\log\hat{P}(x_{i}\mid{\mathbf{u}}_{i})}\\ {=}&{\displaystyle\sum_{u_{i}}\sum_{x_{i}}\hat{P}(x_{i},{\mathbf{u}}_{i})\log\left(\frac{\hat{P}(x_{i},{\mathbf{u}}_{i})}{\hat{P}(u_{i})}\frac{\hat{P}(x_{i})}{\hat{P}(x_{i})}\right)}\\ {=}&{\displaystyle\sum_{u_{i}}\sum_{x_{i}}\hat{P}(x_{i},{\mathbf{u}}_{i})\log\frac{\hat{P}(x_{i},{\mathbf{u}}_{i})}{\hat{P}(u_{i})\hat{P}(x_{i})}+\displaystyle\sum_{x_{i}}\left(\sum_{u_{i}}\hat{P}(x_{i},{\mathbf{u}}_{i})\right)\log\hat{P}(x_{i})}\\ {=}&{I_{P}(X_{i};U_{i})-\displaystyle\sum_{x_{i}}\hat{P}(x_{i})\log\frac{1}{\hat{P}(x_{i})}}\\ {=}&{I_{P}(X_{i};U_{i})-I_{P}(X_{i}),}\end{array}
$$ 

where (as implied by the deﬁnition) the mutual information $I_{\hat{P}}(X_{i};\mathrm{Pa}_{X_{i}})$ is $0$ if $\mathrm{Pa}_{X_{i}}=\varnothing$ . 

Note that the second sum in equation (18.4) does not depend on the network structure, and thus we can ignore it when we compare two structures with respect to the same data set. 

Recall that we can interpret $I\!\!I_{P}(X;Y)$ as the strength of the dependence between $X$ and $Y$ in $P$ . Thus, the likelihood of a network measures the strength of the dependencies between variables and their parents. In other words, we prefer networks where the parents of each variable are informative about it. 

This result can also be interpreted in a complementary manner. 

$$
{\frac{1}{M}}\mathrm{score}_{L}({\mathcal{G}}\ :\ {\mathcal{D}})=H_{{\hat{P}}}(X_{1},\ldots,X_{n})-\sum_{i=1}^{n}I_{\hat{P}}(X_{i};\{X_{1},\ldots X_{i-1}\}-\mathrm{{Pa}}_{X_{i}}^{{\mathcal{G}}}\ |\ \mathrm{{Pa}}_{X_{i-1}}^{{\mathcal{G}}}),
$$ 

For proof, see exercise 18.5. 

Again, this second reformulation of the likelihood has a term that does not depend on the structure, and one that does. This latter term involves conditional mutual-information expres- sions of the form $\pmb{I}_{\hat{P}}(X_{i};\{X_{1},.\,.\,.\,X_{i-1}\}-\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ ) . That is, the information between $X_{i}$ and the preceding variables in the order given $X_{i}$ ’s parents. Smaller conditional mutual- information terms imply higher scores. Recall that conditional independence is equivalent to having zero conditional mutual information. Thus, we can interpret this formulation as measur- ing to what extent the Markov properties implied by $\mathcal{G}$ are violated in the data. The smaller the violations of the Markov property, the larger the score. 

These two interpretations are complementary, one measuring the strength of dependence between $X_{i}$ and its parents $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}$ , and the other measuring the extent of the independence of $X_{i}$ from its predecessors given $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}$ . 

The process of choosing a network structure is often subject to constraints. Some constraints are a consequence of the acyclicity requirement, others may be due to a preference for simpler structures. Our previous analysis shows that the likelihood score provides valuable guidance in selecting between diferent candidate networks. 

# 18.3.1.3 Limitations of the Maximum Likelihood Score 

Based on the developments in the previous chapter and the preceding analysis, we see that the likelihood score is a good measure of the ﬁt of the estimated Bayesian network and the training data. In learning structure, however, we are also concerned about the performance of the learned network on new instances sampled from the same underlying distribution $P^{*}$ . Unfortunately, in this respect, the likelihood score can run into problems. 

To is, consider exa le 18.1. Let $\mathcal{G}_{\varnothing}$ be e network where $X$ and $Y$ are independent, and G ${\mathcal{G}}_{X\to Y}$ the one where X is the parent of Y . As we have seen, $\mathrm{score}_{L}({\mathcal{G}}_{X\to Y}\ :\ {\mathcal{D}})-$ D − → $\mathrm{score}_{L}({\mathcal{G}}_{\varnothing}\ :\ {\mathcal{D}})=M\cdot I_{\hat{P}}(X;Y)$ . Recall that the mutual information between two variables is nonnegative. Thus, $\mathrm{score}_{L}({\mathcal{G}}_{X\to Y}\ :\ {\mathcal{D}})\geq\mathrm{score}_{L}({\mathcal{G}}_{\varnothing}\ :\ {\mathcal{D}})$ D ≥ G D for any data set $D$ . This implies ∅ that the maximum likelihood score never prefers the simpler network over the more complex one. And it assigns both networks the same score only in these rare situations when $X$ and $Y$ are truly independent in the training data. 

As explained in the introduction to this chapter, there are situations where we should prefer to learn the simpler network (for example, when $X$ and $Y$ are nearly independent in the training data). We see that the maximum likelihood score would never lead us to make that choice. 

This observation applies to more complex networks as well. It is easy to show that adding an edge to a network structure can never decrease the maximum likelihood score. Furthermore, the more complex network will have a higher score in all but a vanishingly small fraction of cases. One approach to proving this follows directly from the notion of likelihood; see exercise 18.6. Another uses the fact that, for any $X,Y,Z$ and any distribution $P$ , we have that: 

$$
I_{P}(X;Y\cup Z)\geq I_{P}(X;Y),
$$ 

overﬁtting with equality holding only if $Z$ is conditionally independent of $X$ given $Y$ , see exercise 2.20. This inequality is fairly intuitive: if $Y$ gives us a certain amount of information about $X$ , adding $Z$ can only give us more information. Thus, the mutual information between a variable and its parents can only go up if we add another parent, and it will go up except in those few cases where we get a conditional independence assertion holding exactly in the empirical distribution . It follows that the maximum likelihood network will exhibit a conditional independence only when that independence happens to hold exactly in the empirical distribution. Due to statistical noise, exact independence almost never occurs, and therefore, in almost all cases, the maximum likelihood network will be a fully connected one. In other words, the likelihood score overﬁts the training data (see section 16.3.1), learning a model that precisely ﬁts the speciﬁcs of the empirical distribution in our training set. This model therefore fails to generalize well to new data cases: these are sampled from the underlying distribution, which is not identical to the empirical distribution in our training set. 

We note that the discussion of the maximum likelihood score was in the context of networks with table-CPDs. However, the same observations also apply to learning networks with other forms of CPDs (for example, tree-CPDs, noisy-ors, or Gaussians). In these cases, the information- theoretic analysis is somewhat more elaborate, but the general conclusions about the trade-ofs between models and about overﬁtting apply. 

Since the likelihood score does not provide us with tools to avoid overﬁtting, we have to be careful when using it. It is reasonable to use the maximum likelihood score when there are additional mechanisms that disallow overly complicated structures. For example, we will discuss learning networks with a ﬁxed indegree. Such a limitation can constrain the tendency to overﬁt when using the maximum likelihood score. 

# 18.3.2 Bayesian Score 

We now examine an alternative scoring function that is based on a Bayesian perspective; this approach extends ideas that we described in the context of parameter estimation in the previous chapter. We will start by deriving the score from the Bayesian perspective, and then we will try to understand how it avoids overﬁtting. 

Recall that the main principle of the Bayesian approach was that whenever we have uncer- tainty over anything, we should place a distribution over it. In this case, we have uncertainty both over structure and over parameters. We therefore deﬁne a structur $P(\mathcal G)$ that puts a prior probability on diferent graph structures, and a parameter prior $P(\theta_{\mathcal{G}}\mid\mathcal{G})$ , that puts a probability on diferent choice of parameters once the graph is given. By Bayes rule, we have 

$$
P(\mathcal{G}\mid\mathcal{D})=\frac{P(\mathcal{D}\mid\mathcal{G})P(\mathcal{G})}{P(\mathcal{D})},
$$ 

Bayesian score where, as usual, the denominator is simply a normalizing factor that does not help distinguish between diferent structures. Thus, we deﬁne the Bayesian score as: 

$$
\begin{array}{r}{\mathrm{score}_{B}(\mathcal{G}\ :\ \mathcal{D})=\log P(\mathcal{D}\mid\mathcal{G})+\log P(\mathcal{G}).}\end{array}
$$ 

The ability to ascribe a prior over structures gives us a way of preferring some structures over others. For example, we can penalize dense structures more than sparse ones. It turns out, however, that the structure-prior term in the score is almost irrelevant compared to the ﬁrst term. This term, $P({\mathcal{D}}\mid{\mathcal{G}})$ , takes into consideration our uncertainty over the parameters: 

$$
P(\mathcal{D}\mid\mathcal{G})=\int_{\Theta_{\mathcal{G}}}P(\mathcal{D}\mid\pmb{\theta}_{\mathcal{G}},\mathcal{G})P(\pmb{\theta}_{\mathcal{G}}\mid\mathcal{G})d\pmb{\theta}_{\mathcal{G}},
$$ 

marginal likelihood where $P({\mathcal{D}}\mid\theta_{\mathcal{G}},\mathcal{G})$ is the likelihood of the data given the network $\langle\mathcal{G},\theta_{\mathcal{G}}\rangle$ and $P(\theta_{\mathcal{G}}\mid\mathcal{G})$ is our prior distribution over diferent parameter values for the network G . Recall from section 17.4 that $P({\mathcal{D}}\mid{\mathcal{G}})$ is called the marginal likelihood of the data given the structure, since we marginalize out the unknown parameters. 

It is important to realize that the marginal likelihood is quite diferent from the maximum likelihood score. Both terms examine the likelihood of the data given the structure. The maximum likelihood score returns the maximum of this function. In contrast, the marginal likelihood is the average value of this function, where we average based on the prior measure $P(\theta_{\mathcal{G}}\mid\mathcal{G})$ . This diference will become apparent when we analyze the marginal likelihood term. 

One explanation of why the Bayesian score avoids overﬁtting examines the sensitivity of the likelihood to the particular choice of parameters. As we discussed, the maximal likelihood is overly “optimistic” in its evaluation of the score: It evaluates the likelihood of the training data using the best parameter values for the given data. This estimate is realistic only if these parameters are also reﬂective of the data in general, a situation that never occurs. 

The Bayesian approach tells us that, although the choice of parameter $\hat{\pmb\theta}$ is the most likely given the training set $D$ , it is not the only choice. The posterior over parameters provides us with a range of choices, along with a measure of how likely each of them is. By integrating $P({\mathcal{D}}\mid\theta_{{\mathcal{G}}},{\mathcal{G}})$ over the diferent choices of parameters $\theta_{\mathcal{G}}$ , we are measuring the expected likelihood, averaged over diferent possible choices of $\theta_{\mathcal{G}}$ . Thus, we are being more conservative in our estimate of the “goodness” of the model. 

holdout testing 

Another motivation can be derived from the holdout testing methods discussed in box 16.A. Here, we consider diferent network structures, parameterized by the training set, and test their predictiveness (likelihood) on the validation set. When we ﬁnd a network that best generalizes to the validation set (that is, has the best likelihood on this set), we have some reason to hope that it will also generalize to other unseen instances. As we discussed, the holdout method is sensitive to the particular split into training and test sets, both in terms of the relative sizes of the sets and in terms of which instances fall into which set. Moreover, it does not use all the available data in learning the structure, a potentially serious problem when we have limited amounts of data to learn from. 

![](images/d7c3bc8d70906eee0d26b7315687b5cf611333e51a946efe4b369849fc7189de.jpg) 
Figure 18.1 Marginal likelihood in training data as predictor of expected likelihood on underlying distribution. Comparison of the average log-marginal-likelihood per sample in training data ( $x$ -axis) to the expected log-likelihood of new samples from the underlying distribution $(y$ -axis) for two data sets sampled from the ICU-Alarm network. Each point corresponds to a network structure; the true network structure is marked by a circle. 

It turns out that the Bayesian approach can be viewed as performing a similar evaluation without explicitly splitting the data into two parts. Using the chain rule for probabilities, we can rewrite the marginal likelihood as 

$$
P(\mathcal{D}\mid\mathcal{G})=\prod_{m=1}^{M}P(\xi[m]\mid\xi[1],.\,.\,,\xi[m-1],\mathcal{G}).
$$ 

Eac of the terms in this product $-\ P(\xi[m]\ |\ \xi[1],.\,.\,,\xi[m-1],{\mathcal G})\ -$ is the probability of the m ’th instance using the parameters learned from the ﬁrst $m-1$ − instances (using Bayesian estimation). We see that in this term we are using the $m$ ’th instance as a test case, since we are computing its probability using what we learned from previous instances. Thus, it provides us with one data point for testing the ability of our model to predict a new data instance, based on the model learned from the previous ones. This type of analysis is called a prequential analysis . 

However, unlike the holdout approach, we are not holding out any data. Each instance is evaluated in incremental order, and contributes both to our evaluation of the model and to our ﬁnal model score. Moreover, the Bayesian score does not depend on the order of instances. Using the chain law of probabilities, we can generate a similar expansion for any ordering of the instances. Each one of these will give the same result (since these are diferent ways of expanding the term $P({\mathcal{D}}\mid{\mathcal{G}}))$ . 

This intuition suggests that 

$$
{\frac{1}{M}}\log P({\mathcal{D}}\mid{\mathcal{G}})\approx E_{P^{*}}[\log P({\mathcal{X}}\mid{\mathcal{G}},{\mathcal{D}})]
$$ 

is an estimator for the average log-likelihood of a new sample from the distribution $P^{*}$ . In 

![](images/79417c09b8e5b2b77ff7b129b60849160fbbbcd9fc5f2eeb2d7115e3f717ec5a.jpg) 
Figure 18.2 Maximal likelihood score versus marginal likelihood for the data $\langle H,T,T,H,H\rangle$ . 

practice, it turns out that for reasonable sample sizes this is indeed a fairly good estimator of the ability of a model to generalize to unseen data. Figure 18.1 demonstrates this property empirically for data sets sampled from the ICU-Alarm network. We generated a collection of network structures by sampling from the posterior distribution over structures given diferent data sets (see section 18.5). For each structure we evaluated the two sides of the preceding approximation: the average log-likelihood per sample, and the expected likelihood of new samples from the underlying distribution. As we can see, there is a general agreement between the estimate using the training data and the actual generalization error of each network structure. In particular, the diference in scores of two structures correlates with the diferences in generalization error. This phenomenon is particularly noticeable in the larger training set. 

We note that the Bayesian score is not the only way of providing “test set” performance using each instance. See exercise 18.12 for an alternative score with similar properties. 

# 18.3.3 Marginal Likelihood for a Single Variable 

We now examine how to compute the marginal likelihood for simple cases, and then in the next section treat the case of Bayesian networks. 

Consider a single binary random variable $X$ , and assume that we have a prior distribution Dirichlet $;\!\left(\alpha_{1},\alpha_{0}\right)$ over $X$ . Consider data set $\mathcal{D}$ that has $M[1]$ heads and $M[0]$ tails. Then, the maximum likelihood value given D is 

$$
P(\mathcal{D}\mid\hat{\theta})=\left(\frac{M[1]}{M}\right)^{M[1]}\cdot\left(\frac{M[0]}{M}\right)^{M[0]}.
$$ 

Now, consider the marginal likelihood. Here, we are not conditioning on the parameter. Instead, we need to compute the probability $P(X[1],\ldots,X[M])$ of the data given our prior. One approach to computing this term is to evaluate the integral equation (18.7). An alternative approach uses the chain rule 

$$
P(x[1],\ldots,x[M])=P(x[1])\cdot P(x[2]\mid x[1])\cdot\ldots\cdot P(x[M]\mid x[1],\ldots,x[M-1])
$$ 

Recall that if we use a Beta prior, then 

$$
P(x[m+1]=H\mid x[1],\ldots,x[m])={\frac{M^{m}[1]+\alpha_{1}}{m+\alpha}},
$$ 

where $M^{m}[1]$ is the number of heads in the ﬁrst $m$ examples. For example, if $\mathcal{D}=\left<H,T,T,H,H\right>$ , 

$$
\begin{array}{l c l}{P(x[1],\ldots,x[5])}&{=}&{\displaystyle\frac{\alpha_{1}}{\alpha}\cdot\frac{\alpha_{0}}{\alpha+1}\cdot\frac{\alpha_{0}+1}{\alpha+2}\cdot\frac{\alpha_{1}+1}{\alpha+3}\cdot\frac{\alpha_{1}+2}{\alpha+4}}\\ &{=}&{\displaystyle\frac{[\alpha_{1}(\alpha_{1}+1)(\alpha_{1}+2)][\alpha_{0}(\alpha_{0}+1)]}{\alpha\cdot\cdot(\alpha+4)}.}\end{array}
$$ 

Picking $\alpha_{1}=\alpha_{0}=1$ , so that $\alpha=\alpha_{1}+\alpha_{0}=2$ , we get 

$$
\frac{[1\cdot2\cdot3]\cdot[1\cdot2]}{2\cdot3\cdot4\cdot5\cdot6}=\frac{12}{720}=0.017
$$ 

(see ﬁgure 18.2), which is signiﬁcantly lower than the likelihood 

$$
\left({\frac{3}{5}}\right)^{3}\cdot\left({\frac{2}{5}}\right)^{2}={\frac{108}{3125}}\approx0.035.
$$ 

Thus, a model using maximum-likelihood parameters ascribes a much higher probability to this sequence than does the marginal likelihood. The reason is that the log-likelihood is making an overly optimistic assessment, based on a parameter that was designed with full retrospective knowledge to be an optimal ﬁt to the entire sequence. 

In general, for a binomial distribution with a Beta prior, we have 

$$
P(x[1],\ldots,x[M])=\frac{[\alpha_{1}\cdot\cdot\cdot(\alpha_{1}+M[1]-1)][\alpha_{0}\cdot\cdot\cdot(\alpha_{0}+M[0]-1)]}{\alpha\cdot\cdot\cdot(\alpha+M-1)}.
$$ 

Gamma function Each of the terms in square brackets is a product of a sequence of numbers such as $\alpha\cdot(\alpha+$ $1)\cdot\cdot\cdot(\alpha+M-1)$ . If $\alpha$ is an integer, we can write this product as $\textstyle{\frac{(\alpha+M-1)!}{(\alpha-1)!}}$ . However, we do − not necessarily know that $\alpha$ is an integer. It turns out that we can use a generalization of the facto purpose. Recall that the Gamma function is such that $\Gamma(m)=(m\!-\!1)!$ and $\Gamma(x+1)=x\cdot\Gamma(x)$ · . Using the latter property, we can rewrite 

$$
\alpha(\alpha+1)\cdot\cdot\cdot(\alpha+M-1)=\frac{\Gamma(\alpha+M)}{\Gamma(\alpha)}.
$$ 

Hence, 

$$
P(x[1],\cdot\,.\,,x[M])=\frac{\Gamma(\alpha)}{\Gamma(\alpha+M)}\cdot\frac{\Gamma(\alpha_{1}+M[1])}{\Gamma(\alpha_{1})}\cdot\frac{\Gamma(\alpha_{0}+M[0])}{\Gamma(\alpha_{0})}.
$$ 

A similar formula holds for a multinomial distribution over the space $x^{1},\cdot\cdot\cdot,x^{k}$ , with a Dirichlet prior with hyperparameters $\alpha_{1},.\,.\,.\,,\alpha_{k}$ : 

$$
P(x[1],\ldots,x[M])=\frac{\Gamma(\alpha)}{\Gamma(\alpha+M)}\cdot\prod_{i=1}^{k}\frac{\Gamma(\alpha_{i}+M[x^{i}])}{\Gamma(\alpha_{i})}.
$$ 

Note that the ﬁnal expression for the marginal likelihood is invariant to the order we selected in the expansion via the chain rule. In particular, any other order results in exactly the same ﬁnal expression. This property is reassuring, because the IID assumption tells us that the speciﬁc order in which we get data cases is insigniﬁcant. Also note that the marginal likelihood can be computed directly from the same sufcient statistics used in the computation of the likelihood function — the counts of the diferent values of the variable in the data. This observation will continue to hold in the general case of Bayesian networks. 

# 18.3.4 Bayesian Score for Bayesian Networks 

We now generalize the discussion of the Bayesian score to more general Bayesian networks. Consider two possible structures over two binary random variables $X$ and $Y$ . $\mathcal{G}_{\varnothing}$ is the graph with no edges. Here, we have: 

$$
P(\mathcal{D}\mid\mathcal{G}_{\emptyset})=\int_{\Theta_{X}\times\Theta_{Y}}P(\theta_{X},\theta_{Y}\mid\mathcal{G}_{\emptyset})P(\mathcal{D}\mid\theta_{X},\theta_{Y},\mathcal{G}_{\emptyset})d[\theta_{X},\theta_{Y}].
$$ 

parameter independence We know that likelihood term $P({\mathcal{D}}\mid\theta_{X},\theta_{Y},{\mathcal{G}}_{\varnothing})$ can be written as a product o erms, one involving θ $\theta_{X}$ and the observations of X in the data, and the other involving $\theta_{Y}$ and the observations of $Y$ in the data. If we also assume parameter independence , that is, that $P(\theta_{X},\theta_{Y}\mid\mathcal{G}_{\varnothing})$ decomposes as a product $P(\theta_{X}\mid\mathcal{G}_{\varnothing})P(\theta_{Y}\mid\mathcal{G}_{\varnothing})$ , then we can simplify the integral 

$$
\begin{array}{r c l}{P(\mathcal{D}\mid\mathcal{G}_{\emptyset})}&{=}&{\displaystyle\left(\int_{\Theta_{X}}P(\theta_{X}\mid\mathcal{G}_{\emptyset})\prod_{m}P(x[m]\mid\theta_{X},\mathcal{G}_{\emptyset})d\theta_{X}\right)}\\ &&{\displaystyle\left(\int_{\Theta_{Y}}P(\theta_{Y}\mid\mathcal{G}_{\emptyset})\prod_{m}P(y[m]\mid\theta_{Y},\mathcal{G}_{\emptyset})d\theta_{Y}\right),}\end{array}
$$ 

where we used the fact that the integral of a product of independent functions is the product of integrals. Now notice that each of the two integrals is the marginal likelihood of a single variable. Thus, if $X$ and $Y$ are multinomials, and each has a Dirichlet prior, then we can write each integral using the closed form of equation (18.9). 

Now consider the network ${\mathcal{G}}_{X\to Y}\;=\;(X\;\to\;Y)$ . Once again, if we assume parameter independence, we can decompose this integral into a product of three integrals, each over a single parameter family. 

$$
\begin{array}{r c l}{{\scriptstyle_{X\to Y}}}&{{\scriptstyle=}}&{{\displaystyle\left(\int_{\partial_{X}}{\cal P}(\theta_{X}\mid\mathcal{G}_{X\to Y})\prod_{m}{\cal P}(x[m]\mid\theta_{X},\mathcal{G}_{X\to Y})d\theta_{X}\right)}}\\ {{}}&{{}}&{{\displaystyle\left(\int_{\Theta_{Y\mid x^{0}}}{\cal P}(\theta_{Y\mid x^{0}}\mid\mathcal{G}_{X\to Y})\prod_{m:x[m]=x^{0}}{\cal P}(y[m]\mid\theta_{Y\mid x^{0}},\mathcal{G}_{X\to Y})d\theta_{Y\mid x^{0}}\right)}}\\ {{}}&{{}}&{{\displaystyle\left(\int_{\Theta_{Y\mid x^{1}}}{\cal P}(\theta_{Y\mid x^{1}}\mid\mathcal{G}_{X\to Y})\prod_{m:x[m]=x^{1}}{\cal P}(y[m]\mid\theta_{Y\mid x^{1}},\mathcal{G}_{X\to Y})d\theta_{Y\mid x^{0}}\right)}}\end{array}
$$ 

Again, each of these can be written using the closed form solution of equation (18.9). 

Comparing the marginal likelihood of the two structures, we see that the term that corre- sponds to $X$ is similar in both. In fact, the terms $P(x[m]\mid\theta_{X},\mathcal{G}_{\varnothing})$ and $P(x[m]\mid\theta_{X},\mathcal{G}_{X\rightarrow Y})$ are identical (both make the same predictions given the parameter values). Thus, if we choose the prior $P(\Theta_{X}\mid\mathcal{G}_{\varnothing})$ to be the same as $P(\Theta_{X}\mid\mathcal{G}_{X\rightarrow Y})$ , we have that the ﬁrst term in the marginal likelihood of both structures is identical. 

Thus, given this assumption about the prior, the diference between the marginal likelihood of $\mathcal{G}_{\varnothing}$ and ${\mathcal{G}}_{X\to Y}$ is due to the diference between the m inal likelihood of all the observations of $Y$ and the marginal likelihoods of the observations of $Y$ when we partition our examples based on the observed value of $X$ . Intuitively, if $Y$ has a diferent distribution in these two cases, then the latter term will have better marginal likelihood. On the other hand, if $Y$ is distributed in roughly the same manner in both subsets, then the simpler network will have better marginal likelihood. 

To see this behavior, we consider an idealized experiment where the empirical distribution is such that $P(x^{1})=0.5$ , and $P(y^{1}\mid x^{1})=0.5+p$ and $P(y^{1}\mid x^{0})=0.5-{\bar{p}},$ where $p$ is a free parameter. Larger values of $p$ imply stronger dependence between X and Y . Note, however, that the marginal distributions of $X$ and $Y$ are the same regardless of the value of $p$ . Thus, the score of pty structure $\mathcal{G}_{\varnothing}$ does not depend on $p$ . On the other hand, the score of the structure G ${\mathcal{G}}_{X\to Y}$ depends on $p$ . Figure 18.3 illustrates how these scores change as functions → of the number of training samples. The graph compares the average score per instance (of equation (18.8)) for both structures for diferent values of $p$ . 

We c see that, as we get more data, the Bayesian score prefers the structure ${\mathcal{G}}_{X\to Y}$ where $X$ and Y are dependent. When the dependency between them is strong, this preference arises very quickly. But as the dependency becomes weaker, more data are required in order to justify this selection. Thus, if the two variables are independent, small ﬂuctuations in the data, due to sampling noise, are unlikely to cause a preference for the more complex structure. By contrast, any ﬂuctuation from pure independence in the empirical distribution will cause the likelihood score to select the more complex structure. 

We now return to consider the general case. As we can expect, the same arguments we applied to the two-variable networks apply to any network structure. 

Let $\mathcal{G}$ be a network structure, and let $P(\pmb{\theta}_{\mathcal{G}}\mid\mathcal{G})$ be a parameter prior satisfying global parameter independence. Then, 

$$
P(\mathcal{D}\mid\mathcal{G})=\prod_{i}\sum_{\Theta_{X_{i}|\mathrm{Pa}_{X_{i}}}}\prod_{m}P(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m],\theta_{X_{i}|\mathrm{Pa}_{X_{i}}},\mathcal{G})P(\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}\mid\mathcal{G})d\pmb{\theta}_{X_{i}}
$$ 

Moreover, if $P(\pmb{\theta}_{\mathcal{G}})$ also satisﬁes local parameter independence, then 

$$
\mathcal{G})=\prod_{i}\prod_{\substack{\textbf{\em u}_{i}\in V a l(\mathrm{Pe}_{X_{i}}^{\mathcal{G}})\in\Omega_{X_{i}\mid u_{i}}}}\int\prod_{\substack{m,u_{i}[m]=u_{i}}}P(X_{i}[m]\mid\mathbf{u}_{i},\mathbf{\boldsymbol{\theta}}_{X_{i}\mid u_{i}},\mathcal{G})P(\mathbf{\boldsymbol{\theta}}_{X_{i}\mid u_{i}}\mid\mathcal{G})d\mathbf{\boldsymbol{\theta}}_{X_{i}\mid u_{i}}
$$ 

Using this proposition and the results about the marginal likelihood of Dirichlet priors, we conclude the following result: If we consider a network with Dirichlet priors where $P(\pmb{\theta}_{X_{i}\mid\mathrm{pa}_{X_{i}}}\mid$ 

![](images/5b7b66ecc467198ed0e21dcbea6ed1c912eef6ba86c950fd2b190fbe09c865ec.jpg) 
Figure 18.3 The efect of correlation on the Bayesian score. The solid line indicates the score of the independent model $\mathcal{G}_{\varnothing}$ . The remaining lines indicate the score of the more complex structure ${\mathcal{G}}_{X\to Y}$ , for diferent sampling distributions parameterized by $p$ . 

$\mathcal{G}$ ) has hyperparameters $\{\alpha_{x_{i}^{j}|u_{i}}^{\mathcal{G}}:j=1,.\,.\,.\,,|X_{i}|\}$ then 

$$
\mid\mathcal{G}\rangle=\prod_{i}\prod_{\substack{\mathbf{u}_{i}\in V a l(\mathrm{Pe}_{X_{i}}^{\mathcal{G}})}}\frac{\Gamma(\alpha_{X_{i}|\mathbf{u}_{i}}^{\mathcal{G}})}{\Gamma(\alpha_{X_{i}|\mathbf{u}_{i}}^{\mathcal{G}}+M[\mathbf{u}_{i}])}\prod_{x_{i}^{j}\in V a l(X_{i})}\left[\frac{\Gamma(\alpha_{x_{i}^{j}|\mathbf{u}_{i}}^{\mathcal{G}}+M[x_{i}^{j},\mathbf{u}_{i}])}{\Gamma(\alpha_{x_{i}^{j}|\mathbf{u}_{i}}^{\mathcal{G}})}\right]
$$ 

where $\begin{array}{r}{\alpha_{X_{i}|u_{i}}^{\mathcal{G}}=\sum_{j}\alpha_{x_{i}^{j}|u_{i}}^{\mathcal{G}}}\end{array}$ P . In practice, we use the logarithm of this formula, which is more | manageable to compute numerically. 

# 18.3.5 Understanding the Bayesian Score 

# 

overﬁtting 

Theorem 18.1 As we have just seen, the Bayesian score seems to be biased toward simpler structures, but as it gets more data, it is willing to recognize that a more complex structure is necessary. In other words, it appears to trade of ﬁt to data with model complexity, thereby reducing the extent of overﬁtting . To understand this behavior, it is useful to consider an approximation to the Bayesian score that better exposes its fundamental properties. 

If we use a Dirichlet parameter prior for all parameters in our network, then, when $M\rightarrow\infty$ , we have that: 

$$
\log P(\mathcal{D}\mid\mathcal{G})=\ell(\hat{\pmb{\theta}}_{\mathcal{G}}:\mathcal{D})-\frac{\log M}{2}\mathrm{min}[\mathcal{G}]+O(1),
$$ 

model dimension 

independent parameters 

where $\mathrm{Diim}[\mathcal{G}]$ is the model dimension , or the number of independent parameters in $\mathcal{G}$ . 

See exercise 18.7 for the proof. 

![](images/580ef662d3ce9395401a28642143873edc045a7bc1b848fd839e68dbc74c8c53.jpg) 
Figure 18.4 The Bayesian score of three structures, evaluated on synthetic data generated from the ICU-Alarm network. The solid line is the original structure, which has 509 parameters. The dashed line is a simpliﬁcation that has 359 parameters. The dotted line is a tree-structure and has 214 parameters. 

BIC score 

Thus, we see that the Bayesian score tends to trade of the likelihood — ﬁt to data — on one hand and some notion of model complexity on the other hand. This approximation is called the BIC score (for Bayesian information criterion): 

$$
\mathrm{score}_{B I C}(\mathcal{G}\ :\ \mathcal{D})=\ell(\hat{\pmb{\theta}}_{\mathcal{G}}:\mathcal{D})-\frac{\log M}{2}\mathrm{min}[\mathcal{G}].
$$ 

minimum description length 

We note that the negation of this quantity can be viewed as the number of bits required to encode both the model $\log M/2$ bits per model parameter, a derivation whose details we omit) and the data given the model (as per our discussion in section A.1.3). Thus, this objective is also known as minimum description length . 

We can decompose this score even further using our analysis from equation (18.4): 

$$
\mathrm{score}_{B I C}(\mathcal{G}\ :\ \mathcal{D})=M\sum_{i=1}^{n}I_{\hat{P}}(X_{i};\mathrm{Pa}_{X_{i}})-M\sum_{i=1}^{n}H_{\hat{P}}(X_{i})-\frac{\log M}{2}\mathrm{dim}[\mathcal{G}].
$$ 

We can observe several things about the behavior of this score function. First, the entropy terms do not depend on the graph, so they do not inﬂuence the choice of structure and can be ignored. The score exhibits a trade-of between ﬁt to data and model complexity: the stronger the dependence of a variable on its parents, the higher the score; the more complex the network, the lower the score. However, the mutual information term grows linearly in $M$ , whereas the complexity term grows logarithmically. Therefore, the larger $M$ is, the more emphasis will be given to the ﬁt to data. 

Figure 18.4 illustrates this theorem empirically. It shows the Bayesian score of three structures on a data set generated by the ICU-Alarm network. One of these structures is the correct one, and the other two are simpliﬁcations of it. We can see that, for small $M$ , the simpler structures have the highest scores. This is compatible with our analysis: for small data sets, the penalty term outweighs the likelihood term. But as $M$ grows, the score begins to exhibit an increasing preference for the more complex structures. With enough data, the true model is preferred. 

This last statement is a general observation about the BIC and Bayesian scores: Asymptotically, these scores will prefer a structure that exactly ﬁts the dependencies in the data. To make this statement precise, we introduce the following deﬁnition: 

Deﬁnition 18.1 consistent score Assume that our data are generated by some distribution $P^{*}$ for which the network $\mathcal{G}^{\ast}$ is a perfect map. We say that a scoring function is consistent if the following properties hold as the amount of data $M\rightarrow\infty$ , with probability that approaches 1 (over possible choices of data set $D$ ): 

• The structure $\mathcal{G}^{*}$ will maximize the score.

 • All structures $\mathcal{G}$ that are not $I\cdot$ -equivalent to $\mathcal{G}^{\ast}$ will have strictly lower score. 

# Theorem 18.2 

The BIC score is consistent. 

Proof Our goal is to prove that for sufciently large $M$ , if the graph that maximizes the BIC score is $\mathcal{G}$ , then $\mathcal{G}$ is I-e ivalent to $\mathcal{G}^{*}$ . We brieﬂy sketch this proof. 

Con der some graph G that implies an independence assumptio that $\mathcal{G}^{\ast}$ do s not support. Then G cannot be an I-map of the true underlying distributi $P$ . Hence, G cannot be a maximum likelihood model with respect to the true distribution $P^{*}$ , so that we must have: 

$$
\sum_{i}\mathbf{I}_{P^{*}}(X_{i};\mathrm{Pa}_{X_{i}}^{\mathcal{G}^{*}})>\sum_{i}\mathbf{I}_{P^{*}}(X_{i};\mathrm{Pa}_{X_{i}}^{\mathcal{G}}).
$$ 

As $M\rightarrow\infty$ , our empirical distribution $\hat{P}$ will converge to $P^{*}$ with probability 1. Therefore, for large M , 

$$
\mathrm{score}_{L}(\mathcal G^{*}\ :\ \mathcal D)-\mathrm{score}_{L}(\mathcal G\ :\ \mathcal D)\approx\Delta\cdot M,
$$ 

where $\begin{array}{r}{\Delta=\sum_{i}\mathbf{I}_{P^{*}}(X_{i};\mathrm{Pa}_{X_{i}}^{\mathcal{G}^{*}})-\sum_{i}\mathbf{I}_{P^{*}}(X_{i};\mathrm{Pa}_{X_{i}}^{\mathcal{G}})}\end{array}$ . Therefore, asymptotically we have that 

$$
\mathrm{score}_{B I C}(\mathcal{G}^{*}\ :\ \mathcal{D})-\mathrm{score}_{B I C}(\mathcal{G}\ :\ \mathcal{D})\approx\Delta M+\frac{1}{2}(\mathrm{dim}[\mathcal{G}]-\mathrm{dim}[\mathcal{G}^{*}])\log M
$$ 

The ﬁrst term grows much faster than the second, so that eventually its efect will dominate, and the score of $\mathcal{G}^{*}$ ill be better. 

Now, assume that G implies l the independence assump ons in $\mathcal{G}^{\ast}$ , but th $\mathcal{G}^{*}$ implies an dependence assumption that G doe ot. (In other words, G is a superset $\mathcal{G}^{*}$ G .) this case, G represent any distribution that G $\mathcal{G}^{\ast}$ can. In particular, it can represent $P^{*}$ . As $\hat{P}$ converges to $P^{*}$ , we will have that: 

$$
\mathrm{score}_{L}({\mathcal{G}}^{*}\ :\ {\mathcal{D}})-\mathrm{score}_{L}({\mathcal{G}}\ :\ {\mathcal{D}})\to0.
$$ 

Therefore, asymptotically we have that for 

$$
\mathrm{score}_{B I C}(\mathcal{G}^{*}\ :\ \mathcal{D})-\mathrm{score}_{B I C}(\mathcal{G}\ :\ \mathcal{D})\approx\frac{1}{2}(\mathrm{dim}[\mathcal{G}]-\mathrm{dim}[\mathcal{G}^{*}])\log M.
$$ 

Now, since $\mathcal{G}$ makes fewer independence assumptions than $\mathcal{G}^{*}$ , it must be param erized by a larger set of parameters. Thus, $\mathrm{dim}[\mathcal{G}]>\mathrm{dim}[\mathcal{G}^{*}]$ , so that G $\mathcal{G}^{*}$ will be preferred to G . 

As the Bayesian score is asymptotically identical to BIC (the remaining $O(1)$ terms do not grow with $M$ ), we get: 

# Corollary 18.2 

The Bayesian score is consistent. 

Note that consistency is an asymptotic property, and thus it does not imply much about the properties of networks learned with limited amounts of data. Nonetheless, the proof illustrates the trade-ofs that are playing a role in the deﬁnition of score. 

# 18.3.6 Priors 

Until now we did not specify the actual choice of priors we use. We now discuss possible choices of priors and their efect on the score. 

# 18.3.6.1 Structure Priors 

structure prior We begin with the prior over network structures, $P(\mathcal G)$ . Note that although this term seems to describe our bias for certain structure, in fact, it plays a relatively minor role. As we can see in theorem 18.1, the logarithm of the marginal likelihood grows linearly with the number of examples, while the prior over structures remains constant. Thus, the structure prior does not play an important role in asymptotic analysis as long as it does not rule out (that is, assign probability 0 ) any structure. 

For this reason, we often use a uniform prior over structures. Nonetheless, the structure prior can make some diference when we consider small samples. Thus, we might want to encode some of our preferences in this prior. For example, we might penalize edges in the graph, and use a prior $\dot{P}(\mathcal G)\propto c^{|\mathcal G|}$ , where $c$ is some constant smaller than 1 , and $|\mathcal{G}|$ is the number of edges in the graph. 

Note that in both these choices (the uniform and the penalty per edge) it sufces to use a value that is proportional to the prior, since the normalizing constant is the same for all choice of $\mathcal{G}$ and hence can be ignored. For this reason, we do not need to worry about the exact number of possible network structures in order to use these priors. 

structure modularity 

As we will immediately see, it will be mathematically convenient to assume that the structure prior satisﬁes structure modularity . This condition requires that the prior $P(\mathcal G)$ be proportional to a product of terms, where each term relates to one family. Formally, 

$$
P(\mathcal{G})\propto\prod_{i}P(\mathrm{Pa}_{X_{i}}=\mathrm{Pa}_{X_{i}}^{\mathcal{G}}),
$$ 

where $P(\mathrm{Pa}_{X_{i}}=\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ ) denotes the prior probability we assign to choosing the speciﬁc set of parents for $X_{i}$ . Structure priors that satisfy this property do not penalize for global properties of the graph (such as its depth) but only for local properties (such as the indegrees of variables). This is clearly the case for both priors we discuss here. 

In addition, it also seems reasonable to require that I-equivalent network structures are assigned the same prior. Again, this means that when two networks are equivalent, we do not distinguish between them by subjective preferences. 

# 18.3.6.2 Parameter Priors and Score Decomposability 

parameter prior 

decomposable score global parameter independence 

In order to use Bayesian scores, we also need to have parameter priors for the parameter iz ation corresponding to every possible structure. Before we discuss how to represent such priors, we consider the desired properties from these priors. 

Proposition 18.2 shows that the Bayesian score of a network structure $\mathcal{G}$ decomposes into a product of terms, one for each family. This is a consequence of the global parameter in- dependence assumption. In the case of parameter learning, this assumption was crucial for decomposing the learning problem into independent subproblems. Can we exploit a similar phenomenon in the case of structure learning? 

In the simple example we considered in the previous section, we compared the score of two networks $\mathcal{G}_{\varnothing}$ and ${\mathcal{G}}_{X\to Y}$ . We saw that if w hoose the priors $P(\Theta_{X}\mid\mathcal{G}_{\varnothing})$ and $P(\Theta_{X}\mid\mathcal{G}_{X\rightarrow Y})$ to be identical, the score associated with X is the same in both graphs. Thus, not only does the score of both structures have a product form, but in the case where the same variable has the same parents in both structures, the term associated with it also has the same value in both scores. 

Considering more general structures, if $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}=\mathrm{Pa}_{X_{i}}^{\mathcal{G}^{\prime}}$ then it would seem natural that the term that measures the score of $X_{i}$ given its parents in $\mathcal{G}$ wo be identical to the one in ${\mathcal{G}}^{\prime}$ . This seems reasonable. Recall that the score associated with $X_{i}$ measures how well it can be predicted given its parents. Thus, if $X_{i}$ has the same set of parents in both structures, this term should have the same value. 

Deﬁnition 18.2 decomposable score 

family score 

A structure score $\operatorname{score}(\mathcal G\ :\ \mathcal D)$ is decomposable if the score of a structure $\mathcal{G}$ can be written as 

$$
\mathrm{score}(\mathcal G\ :\ \mathcal D)=\sum_{i}\mathrm{Famscover}(X_{i}\ |\ \mathrm{Pa}_{X_{i}}^{\mathcal G}\ :\ \mathcal D),
$$ 

ere the family score $\operatorname{Famsccore}(X\mid U\ :\ {\mathcal{D}})$ is a score measuring how well a set of variables $U$ serves as parents of X in the data set . 

As an example, the likelihood score is decomposable. Using proposition 18.1, we see that in this decomposition 

$$
\operatorname{Famsccore}_{L}(X\mid U\;:\;{\mathcal{D}})=M\cdot\left[I_{\hat{P}}(X;U)-H_{\hat{P}}(X)\right].
$$ 

# 

Score decomposability has important ramiﬁcations when we search for structures that maximize the scores. The high-level intuition is that if we have a decomposable score, then a local change in the structure (such as adding an edge) does not change the score of other parts of the structure that remained the same. As we will see, the search algo- rithms we consider can exploit decomposability to reduce dramatically the computational overhead of evaluating diferent structures during search. 

Under what conditions is the Bayesian score decomposable? It turns out that a natural restriction on the prior sufces. 

Deﬁnition 18.3 parameter modularity 

Parameter modularity states that the prior over the CPD of $X_{i}$ depends only on the local structure of the network (that is, the set of parents of $X_{i}$ ), and not on other parts of the network. It is straightforward to see that parameter modularity implies that the score is decomposable. 

Proposition 18.3 Let $\mathcal{G}$ work structure, let $P(\mathcal G)$ be a structure prior satisfying structure modularity, and let P $P(\theta_{\mathcal{G}}\mid\mathcal{G})$ | G be a parameter prior satisfying global parameter independence and parameter G modularity. Then, the Bayesian score over network structures is decomposable. 

# 18.3.6.3 Representing Parameter Priors 

How do we represent our parameter priors? The number of possible structures is superexponen- tial, which makes it difcult to elicit separate parameters for each one. How do we elicit priors for all these networks? If we require parameter modularity, the number of diferent priors we need is somewhat smaller, since we need a prior for each choice of parents for each variable. This number, however, is still exponential. 

K2 prior 

A simpleminded approach is simply to take some ﬁxed Dirichlet distribution, for example, Dirichlet $\mathopen{}\mathclose\bgroup\left(\alpha,\ldots,\alpha\aftergroup\egroup\right)$ , for every parameter, where $\alpha$ is a predetermined constant. A typical choice is $\alpha\,=\,1$ . This prior is often referred to as the $K2$ prior , referring to the name of the software system where it was ﬁrst used. 

The K2 prior is simple to represent and efcient to use. However, it is somewhat inconsistent. Consider a structure where the binary variable $Y$ has no parents. If we take $\mathit{D i r i c h l e t}(1,1)$ for $\theta_{Y}$ , we are in efect stating that our imaginary sample size is two. But now, consider a diferent structure where $Y$ has the parent $X$ , which has 4 values. If we take $\,D i r i c h l e t(1,1)$ as our prior for all parameters $\theta_{Y\mid x^{i}}$ , we are efectively stating that we have seen two imaginary samples in each context $x^{i}$ , for a total of eight. It seems that the number of imaginary samples we have seen for diferent events is a basic concept that should not vary with diferent candidate structures. 

BDe prior 

A more elegant approach is one we already saw in the context of parameter estimation: the BDe prior . We elicit a prior distribution $P^{\prime}$ over the entire probability space and an equivalent sample size $\alpha$ for the set of imaginary samples. We then set the parameters as follows: 

$$
\alpha_{x_{i}|\mathrm{pa}_{X_{i}}}=\alpha\cdot P^{\prime}(x_{i},\mathrm{pa}_{X_{i}}).
$$ 

This choice will avoid the inconsistencies we just discussed. If we consider the prior over $\theta_{Y\mid x^{i}}$ in our example, then 

$$
\alpha_{y}=\alpha\cdot P^{\prime}(y)=\sum_{x^{i}}\alpha\cdot P^{\prime}(y,x^{i})=\sum_{x^{i}}\alpha_{y|x^{i}}.
$$ 

Thus, the number of imaginary samples for the diferent choices of parents for $Y$ will be identical. 

As we discussed, we can represent $P^{\prime}$ as a Bayesian network whose structure can represent our prior about the domain structure. Most simply, when we have no prior knowledge, we set $P^{\prime}$ to be the uniform distribution, that is, the empty Bayesian network with a uniform marginal distribution for each variable. In any case, it is important to note that the network structure is used only to provide parameter priors. It is not used to guide the structure search directly. 

# 18.3.7 Score Equivalence $\star$ 

The BDe score turns out to satisfy an important property. Recall that two networks are I- equivalent if they encode the same set of independence statements. Hence, based on observed independencies, we cannot distinguish between I-equivalent networks. This suggests that based on observing data cases, we do not expect to distinguish between equivalent networks. 

# Deﬁnition 18.4 score equivalence 

Let $\mathrm{score}(\mathcal G\mathrm{~\,~:~\,~}\mathcal D)$ D som scoring rule. We say that it satisﬁes score equivalence if all I-equivalent networks G and G ${\mathcal{G}}^{\prime}$ we have score $\cdot({\mathcal{G}}\ :\ {\mathcal{D}})=\operatorname{score}({\mathcal{G}}^{\prime}\ :\ {\mathcal{D}})$ for all data sets D . 

In other words, score equivalence implies that all networks in the same equivalence class have the same score. In general, if we view I-equivalent networks as equally good at describing the same probability distributions, then we want to have score equivalence. We do not want the score to introduce artiﬁcial distinctions when we choose networks. Do the scores discussed so far satisfy this condition? 

# Theorem 18.3 

The likelihood score and the BIC score satisfy score equivalence. 

For a proof, see exercise 18.8 and exercise 18.9 What about the Bayesian score? It turns out that the simpleminded K2 prior we discussed is not score-equivalent; see exercise 18.10. The BDe score, on the other hand, is score-equivalent. In fact, something stronger can be said. 

# Theorem 18.4 

Let $P(\mathcal G)$ be a structure prior that assigns I-equivalent networks identical prior. Let $P(\theta_{\mathcal{G}}\mid\mathcal{G})$ be a prior over parameters for networks with table-CPDs that satisﬁes global and local parameter independence and where for each $X_{i}$ and $\mathbf{\mathit{u}}_{i}\ \in\ \mathit{V a l}(\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ , we have that $P(\pmb{\theta}_{X_{i}\mid\mathbf{u}_{i}}\mid\mathcal{G})$ is $^a$ Dirichlet prior. The Bayesian score with this prior satisﬁes score equivalence if and only if the prior is a BDe prior for some choice of $\alpha$ and $P^{\prime}$ . 

We do not prove this theorem here. See exercise 18.11 for a proof that the BDe score in this case satisﬁes score equivalence. 

In other words, if we insist on using Dirichlet priors and also want the decomposition property, then to satisfy score equivalence, we must use a BDe prior. 

# 18.4 Structure Search 

In the previous section, we discussed scores for evaluating the quality of diferent candidate Bayesian network structures. These included the likelihood score, the Bayesian score, and the BIC score (which is an asymptotic approximation of the Bayesian score). We now examine how to ﬁnd a structure with a high score. We now have a well-deﬁned optimization problem. Our input is:

 • training set $\mathcal{D}$ ;

 • scoring function (including priors, if needed);

 • a set of possible network structures (incorporating any prior knowledge). 

Our desired output is a network structure (from the set of possible structures) that maximizes the score. 

It turns out that, for this discussion, we can ignore the speciﬁc choice of score. Our search algorithms will apply unchanged to all three of these scores. 

score decomposability 

As we will discuss, the main property of the scores that afect the search is their decompos- ability . That is, we assume we can write the score of a network structure $\mathcal{G}$ : 

$$
\mathrm{score}(\mathcal G\ :\ \mathcal D)=\sum_{i}\mathrm{Famscover}(X_{i}\ |\ \mathrm{Pa}_{X_{i}}^{\mathcal G}\ :\ \mathcal D).
$$ 

score equivalence 

Another property that is shared by all these scores is score equivalence : if $\mathcal{G}$ is I-equivalent to ${\mathcal{G}}^{\prime}$ then $\mathrm{score}(\mathcal G\ :\ \mathcal D)=\mathrm{score}(\mathcal G^{\prime}\ :\ \mathcal D)$ . This property is less crucial for search, but, as we will see, it can simplify several points. 

# 18.4.1 Learning Tree-Structured Networks 

We begin with the simplest variant of the structure learning task — the task of learning a tree-structured network . More precisely: 

Deﬁnition 18.5 tree network 

A network structure $\mathcal{G}$ is called tree-structured if each variable $X$ has at most one parent in $\mathcal{G}$ , that is, $\mid\mathrm{Pa}_{X}^{\mathcal{G}}\mid\leq1$ |≤ . 

Strictly speaking, the notion of tree-structured networks covers a broader class of graphs than those comprising a single tree; it also covers graphs composed of a set of disconnected trees, that is, a forest. In particular, the network of independent variables (no edges) also satisﬁes this deﬁnition. However, as the basic structure of these networks is still a collection of trees, we continue to use the term tree-structure. 

Note that the class of trees is narrower than the class of polytrees that we discussed in chapter 9. A polytree can have variables with multiple parents, whereas a tree cannot. In other words, a tree-structured network cannot have v-structures. In fact, the problem of learning polytree-structured networks has very diferent computational properties than that of learning trees (see section 18.8). 

Why do we care about learning trees? Most importantly, because unlike richer classes of structures, they can be learned efciently — in polynomial time. But learning trees can also be useful in themselves. They are sparse, and therefore they avoid most of the overﬁtting problems associated with more complex structures. They also capture the most important dependencies in the distribution, and they can therefore provide some insight into the domain. They can also provide a better baseline for approximating the distribution than the set of independent marginals of the diferent variables (another commonly used simple approximation). They are thus often used as a starting point for learning a more complex structure, or even on their own in cases where we cannot aford signiﬁcant computational resources. 

The key properties we are going to use for learning trees are the decomposability of the score on one hand and the restriction on the number of parents on the other hand. We start by examining the score of a network and performing slight manipulations. Instead of maximizing the score of a tree structure $\mathcal{G}$ , we will try to maximize the diference between its score and the score of the empty structure $\mathcal{G}_{\varnothing}$ . We deﬁne 

$$
\Delta(\mathcal{G})=\mathrm{score}(\mathcal{G}\ :\ \mathcal{D})-\mathrm{score}(\mathcal{G}_{\varnothing}\ :\ \mathcal{D}).
$$ 

We know that $\mathrm{score}(\mathcal{G}_{\varnothing}\ :\ \mathcal{D})$ is simply a sum of terms FamScore $\left(X_{i}\ :\ \mathcal{D}\right)$ for each $X_{i}$ . That is the score of $X_{i}$ if it does not have any parents. The score $\operatorname{score}(\mathcal G\ :\ \mathcal D)$ consists of terms FamScore $(X_{i}\ |\ \mathrm{Pa}_{X_{i}}^{\mathcal{G}}\ :\ \mathcal{D})$ . Now, there are two cases. If $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}=\mathring{\varnothing}$ ∅ , then the term for $X_{i}$ in both scores cancel out. If $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\,=\,X_{j}$ , then we are left with the diference between the two terms. Thus, we conclude that 

$$
\Delta(\mathcal{G})=\sum_{i,\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\neq\emptyset}\left(\mathrm{Famscone}(X_{i}\ |\ \mathrm{Pa}_{X_{i}}^{\mathcal{G}}\ :\ \mathcal{D})-\mathrm{Famscone}(X_{i}\ :\ \mathcal{D})\right).
$$ 

If we deﬁne the weight 

$$
w_{j\rightarrow i}=\operatorname{Famsccore}(X_{i}\mid X_{j}\ :\ \mathcal{D})-\operatorname{Famsccore}(X_{i}\ :\ \mathcal{D}),
$$ 

then we see that $\Delta(\mathcal{G})$ is the sum of weights on pairs $X_{i},X_{j}$ such that $X_{j}\rightarrow X_{i}$ in $\mathcal{G}$ 

$$
\Delta(\mathcal{G})=\sum_{X_{j}\rightarrow X_{i}\in\mathcal{G}}w_{j\rightarrow i}.
$$ 

maximum weight spanning forest 

We have transformed our problem to one of ﬁnding a maximum weight spanning forest in a directed weighted graph. Deﬁne a fully connected directed graph, where each vertex is labeled by a random variable in $\mathcal{X}$ , and the weight of the edge from vertex $X_{j}$ to vertex $X_{i}$ is $w_{j\rightarrow i}$ , and then search for a maximum-weight-spanning forest. Clearly, the sum of edge weights in a forest is exactly $\Delta(\mathcal{G})$ of the structure $\mathcal{G}$ with the corresponding set of edges. The graph structure that corresponds to that maximum-weight forest maximizes $\Delta(\mathcal{G})$ . 

How hard is the problem of ﬁnding a maximal-weighted directed spanning tree? It turns out that this problem has a polynomial-time algorithm. This algorithm is efcient but not simple. 

The task becomes simpler if the score satisﬁes score equivalence. In this case, we can show

 (see exercise 18.13) that $w_{i\to j}\,=\,w_{j\to i}$ . Thus, we can examine an undirected spanning tree

 (forest) problem, where we choose which edges participate in the forest, and only afterward determine their direction. (This can be done by choosing an arbitrary root and directing all edges away from it.) Finding a maximum spanning tree in undirected graph is an easy problem. One algorithm for solving it is shown in algorithm A.2; an efcient implementation of this algorithm requires time complexity of $O(n^{2}\log{n})$ , where $n$ is the number of vertices in the graph. 

Using this reduction, we end up with n alg hm whose complexity is $O(n^{2}\cdot M+n^{2}\log n)$ where $n$ is the number of variables in X and M is the number of data cases. This complexity is a result of two stages. In the ﬁrst stage we perform a pass over the data to collect the sufcient statistics of each of the $O(n^{2})$ edges. This step takes $O(n^{2}\cdot M)$ time. The spanning tree computation requires $O(n^{2}\log{n})$ using standard data structures, but it can be reduced to $O(n^{2}+n\log n)=O(n^{2})$ using more sophisticated approaches. We see that the ﬁrst stage dominates the complexity of the algorithm. 

# 18.4.2 Known Order 

variable ordering We now consider a special case that also turns out to be easier than the general case. Suppose we restrict attention to structures that are consistent with some predetermined variable ordering $\prec$ $\mathcal{X}$ . In other words, we restrict attention to structures $\mathcal{G}$ where, if $X_{i}\,\in\,\mathrm{Pa}_{X_{j}}^{\mathcal{G}}$ , then $X_{i}\prec X_{j}$ . 

This assumption was a standard one in the early work on learning Bayesian networks from data. In some domains the ordering is indeed known in advance. For example, if there is a clear temporal order by which the variables are assigned values, then it is natural to try to learn a network that is consistent with the temporal ﬂow. 

Before we proceed, we stress that choosing an ordering in advance may be problematic. As we have seen in the discussion of minimal I-maps in section 3.4, a wrong choice of order can result in unnecessarily complicated I-map. Although learning does not recover an exact I-map, the same reasoning applies. Thus, a bad choice of order can result in poor learning result. 

With this caveat in min ct an ordering $\prec;$ ; without loss of generality, assume that our order $X_{1}\prec X_{2}\prec.\,.\,.\prec X_{n}$ . We want to learn a structure that maximizes the score, but so that $\operatorname{Pa}_{X_{i}}\subseteq\{X_{1},.\,.\,.\,,X_{i-1}\}$ . 

The ﬁrst observation we make is the following. We need to ﬁnd the network that maximizes the score. This score is a sum of local scores, one per variable. Note that the choice of parents for one variable, say $X_{i}$ , does not restrict the choice of parents of another variable, say $X_{j}$ . Since we obey the ordering, none of our choices can create a cycle. Thus, in this scenario, learning the parents of each variable is independent of the other variables. Stated more formally: 

$L e t\prec$ be an ordering over $\mathcal{X}$ , and let score $(\mathcal{G}\ :\ \mathcal{D})$ be a decomposable score. If we choose $\mathcal{G}$ to be the network where 

$$
\mathrm{Pa}_{X_{i}}^{\mathcal{G}}=\arg\operatorname*{max}_{U_{i}\subseteq\{X_{j}:X_{j}\prec X_{i}\}}\mathrm{Famsccore}(X_{i}\mid U_{i}\;:\;{\mathcal{D}})
$$ 

for each $i$ , then $\mathcal{G}$ maximizes the score among the structures consistent with $\prec$ 

Based on this observation, we can learn the parents for each variable independently from the parents of other variables. In other words, we now face $n$ small learning problems. 

Let us consider these learning problems. Clearly, we are forced to make $X_{1}$ a root. In the case of $X_{2}$ we have a choice. We can either have the edge $X_{1}\rightarrow X_{2}$ or not. In this case, we can evaluate the diference in score between these two options, and choose the best one. Note that this diference is exactly the weight $w_{1\rightarrow2}$ we deﬁned when learned tree networks. If $w_{1\rightarrow2}>0$ , we ad he edge $X_{1}\rightarrow X_{2}$ ; otherwise we do not. 

Now consider $X_{3}$ . Now we have four options, corresponding to whether we add the edge $X_{1}\rightarrow X_{3}$ , and whether we add the edge $X_{2}\rightarrow X_{3}$ . A roach to making these choices is to he decision whether to ad edge $X_{1}\rightarrow X_{3}$ from the decision about the edge $X_{2}\rightarrow X_{3}$ → . Thus, we might evaluate $w_{1\rightarrow3}$ and $w_{2\rightarrow3}$ and based on these two numbers → → try to decide what is the best choice of parents. 

Unfortunately, this approach eral, the score $\operatorname{Famsccore}(X_{3}\mid X_{1},X_{2}\ :\ {\mathcal{D}})$ is not a function of FamScore( $(X_{3}\mid X_{1}\ :\ {\mathcal{D}})$ | D and FamScore( $(X_{3}\mid X_{2}\ :\ {\mathcal{D}})$ | D . An reme example is an XOR-like CPD where $X_{3}$ is a probabilistic function of the XOR of $X_{1}$ and $X_{2}$ . mScore $\begin{array}{r l r l}{\left(X_{3}\mid X_{1}}&{{}:}&{{}{\mathcal{D}}\right)}\end{array}$ D will be small (and potentially smaller than $\operatorname{Famsccore}(X_{3}\mid\mathbf{\theta}:\mathbf{\mathcal{D}})$ | D since the two variables are independ ), yet FamScore $(X_{3}\mid X_{1},X_{2}\ :$ D ) will be large. By choosing the particular dependence of $X_{3}$ on the XOR of $X_{1}$ and $X_{2}$ , we can change the magnitude of the latter term. 

We conclude that we need to consider all four possible parent sets before we choose the parents of $X_{3}$ . This does not seem that bad. However, when we examine $X_{4}$ we need to consider eight parent sets, and so on. For learning the parents of $X_{n};$ , we need to consider $2^{n-1}$ parent sets, which is clearly too expensive for any realistic number of variables. 

In practice, we do not want to learn networks with a large number of parents. Such networks are expensive to represent, most often are inefcient to perform inference with, and, most important, are prone to overﬁtting. So, we may ﬁnd it reasonable to restrict our attention to networks the indegree of each variable is at most $d$ . 

If we make this restriction, our situation is somewhat more reasonable. The number of possible parent sets for $X_{n}$ is $1+{\binom{n-1}{1}}+.\,.\,.+{\binom{n-1}{d}}=O(d{\binom{n-1}{d}})$          (when $d<n/2)$ ). Since the number of choices for all other variables is less than the number of choices for $X_{n}$ , the procedure has to evaluate $O(d n{\binom{n-1}{d}})\;=\;O(d{\binom{n}{d}})$       candidate parent sets. This number is polynomial in $n$ (for a ﬁxed $d)$ ). 

We conclude that learning given a ﬁxed order and a bound on the indegree is computationally tractable. However, the computational cost is exponential in $d$ . Hence, the exhaustive algorithm that c cks all parent sets of size $\leq d$ is impractical for values of $d$ larger than 3 or 4. When a larger d is required, we can use heuristic methods such as those described in the next section. 

# 18.4.3 General Graphs 

What happens when we consider the most general problem, where we do not have an ordering over the variables? Even if we restrict our attention to networks with small indegree, other pr ms arise. Suppo hat adding the edge $X_{1}\rightarrow X_{2}$ is beneﬁcial, for example, if the score of $X_{1}$ as a parent of $X_{2}$ is higher than all other alternatives. If we decide to add this edge, we cannot add other edges — for example, $X_{2}\rightarrow X_{1}$ — since this would introduce a cycle. The restriction on the immediate reverse of the edge we add might not seem so problematic. How g this edge also forbids us from adding tog of edges, such as $X_{2}\rightarrow X_{3}$ and $X_{3}\,\rightarrow\,X_{1}$ → . Thus, the decision on whether to add $X_{1}\rightarrow X_{2}$ → is not simple, since it has ramiﬁcations for other choices we make for parents of all the other variables. 

This discussion suggests that the problem of ﬁnding the maximum-score network might be more complex than in the two cases we examined. If fact, we can make this statement more precise. Let $d$ be an integer, we deﬁne $\pmb{\mathcal{G}}_{d}=\{\mathcal{G}:\forall i,|\mathrm{Pa}_{X_{i}}^{\mathcal{G}}|\leq d\}$ } . 

Given a data set $\mathcal{D}$ and a decomposable score function score , ﬁnd 

$$
{\mathcal G}^{*}=\arg\operatorname*{max}_{{\mathcal G}\in{\pmb{\mathscr G}}_{d}}\operatorname{score}({\mathcal G}\ :\ {\mathcal D}).
$$ 

The proof of this theorem is quite elaborate, and so we do not provide it here. 

Given this result, we realize that it is unlikely that there is an efcient algorithm that constructs the highest-scoring network structure for all input data sets. Unlike the situation in inference, for example, the known intermediate situations where the problem is easier are not the ones we usually encounter in practice; see exercise 18.14. 

As with many intractable problems, this is not the end of the story. Instead of aiming for an algorithm that will always ﬁnd the highest-scoring network, we resort to heuristic algorithms that attempt to ﬁnd the best network but are not guaranteed to do so. In our case, we are local search faced with a combinatorial optimization problem; we need to search the space of graphs (with bounded indegree) and return a high-scoring one. We solve this problem using a local search approach. To do so, we deﬁne three components: a search space, which deﬁnes the set of candidate network structures; a scoring function that we aim to maximize (for example, the BDe score given the data and priors); and the search procedure that explores the search space without necessarily seeing all of it (since it is super exponential in size). 

# 18.4.3.1 The Search Space 

search space We start by considering the search space . As discussed in appendix A.4.2, we can think of a search space as a graph over candidate solutions, connected by possible operators that the search procedure can perform to move between diferent solutions. In the simplest setting, we nsider the search space where each search state denotes a complete network structure $\mathcal{G}$ over $\mathcal{X}$ X . This is the search space we discuss for most of this chapter. However, we will see other formulations for search spaces. 

A crucial design choice that has large impact on the success of heuristic search is how the space is interconnected. If each state has few neighbors, then the search procedure has to consider only a few options at each point of the search. Thus, it can aford to evaluate each of these options. However, this comes at a price. Paths from the initial solution to a good one might be long and complex. On the other hand, if each state has many neighbors, we may be able to move quickly from the initial state to a good state, but it may be difcult to determine which step to take at each point in the search. A good trade-of for this problem chooses reasonably few neighbors for each state but ensures that the “diameter” of the search space remains small. A natural choice for the neighbors of a state representing a network structure is a set of structures that are identical to it except for small “local” modiﬁcations. Thus, we deﬁne the connectivity of our search space in terms of operators such as: 

edge addition edge deletion edge reversal • edge addition ;

 • edge deletion ;

 • edge reversal . 

In other words, the states adjacent to a state $\mathcal{G}$ are those where we change one edge, either by adding one, deleting one, or reversing the orientation of one. Note that we only consider operations that result in legal networks. That is, acyclic networks that satisfy the constraints we put in advance (such as indegree constraints). 

This deﬁnition of search space is quite natural and has several desirable properties. First, notice that the diameter of the search space is at most $n^{2}$ . That is, there is a relatively short path between any two networks we choose. To see this, note that if we consider traversing a path from $\mathcal{G}_{1}$ to $\mathcal{G}_{2}$ , we can start b eleting all ed s in $\mathcal{G}_{1}$ that do not appear in $\mathcal{G}_{2}$ , and then we can add the edges that are in G $\mathcal{G}_{2}$ and not in G $\mathcal{G}_{1}$ Clearly, the number of steps we take is bounded by the total number of edges we can have, $n^{2}$ . 

Second, recall that the score of a network $\mathcal{G}$ is a sum of local scores. The operations we consider result in changing only one local score term (in the case of addition or deletion of an edge) or two (in the case of edge reversal). Thus, they result in a local change in the score; most components in the score remain the same. This implies that there is some sense of “continuity” in the score of neighboring networks. 

![](images/119f5161a4a79181ba9b3de6a56f314b3409ff5f3629ffab4341907e1f0ec5d2.jpg) 
Figure 18.5 Example of a search problem requiring edge deletion. (a) original network that generated the data. (b) and (c) intermediate networks encountered during the search. 

The choice of the three particular operations we consider also needs some justiﬁcation. For example, if we always start the search from the empty graph $\mathcal{G}_{\varnothing}$ , we may wonder why we include the option to delete an edge. We can reach every network by adding the appropriate arcs to the empty network. In general, however, we want the search space to allow us to reverse our choices. As we will see, this is an important property in escaping local maxima (see appendix A.4.2). 

However, the ability to delete edges is important even if we perform only “greedy” operations that lead to improvement. To see this, consider the following example. Suppose the original network is the one shown in ﬁgure 18.5a, and that $A$ is highly informative about both $C$ and $B$ . S rom an empty network and adding edges greedily, we m the edges $A\rightarrow B$ and $A\,\rightarrow\,C$ → . How er, in some data sets, we ght a the edge $A\,\rightarrow\,D$ → . To see why, we need to realize that A is informative about both B and C , and since these are the two parents of $D$ , also about $D$ . Now $B$ and $C$ are also informative about $D$ . However, each of them provides part of the information, and thus neither $B$ nor $C$ by itself is the best parent of $D$ . At this stage, we thus end up with the network ﬁgure 18.5b. Continuing the search, we consider diferent operators, adding the edge $B\,\rightarrow\,D$ and $C\,\rightarrow\,D$ . Since $B$ is a rent of $D$ in the original network, it will improve the prediction of D when combined with A . Thus, the score of $A$ and $B$ together as parents of $D$ can be larger than the score of $A$ alone. Similarly, if there are enough data to support adding parameters, we will also add the edge $C\,\rightarrow\,D$ , and reach the structure shown in ﬁgure 18.5c. This is the correct structure, except for the redundant edge $A\,\rightarrow\,D$ the original distr tion $B$ and C together separate $A$ from $D$ , we expect that choosing $B,C$ as the parents of D will have higher score than choosing $A,B,C$ . To see this, note that $A$ cannot provide additional information on top of what $B$ and $C$ convey, and having it as an additional parent results in a penalty. After we delete the edge $A\rightarrow D$ we get the original structure. 

A similar question can be raised about the edge reversal operator. Clearly, we can achieve the efect of reve edge $X\rightarrow Y$ in two steps, ﬁrst deleting the ed $X\rightarrow Y$ and then adding the edge Y $Y\rightarrow X$ → . The problem is that when we delete the dge $X\rightarrow Y$ → , we usually reduce the score (assuming that there is some dependency between X and $Y$ ). Thus, these two operations require us to go “downhill” in the ﬁrst step in order to get to a better structure in the next step. The reverse operation allows us to realize the trade-of between a worse parent set for $Y$ and a better one for $X$ . 

![](images/8c5b83fff3a651339e0a6eb44fc4ad4fccc2b9434c744e51cb38eeaca0ad217e.jpg) 
Figure 18.6 Example of a search problem requiring edge reversal. (a) original network that generated the data. (b) and (c) intermediate networks encountered during the search. (d) an undesirable outcome. 

To see the utility of the edge reversal operator, consider the following simple example. Suppose the real network generating the data has the v-structure shown in ﬁgure 18.6a. Suppose that the dependency between $A$ and $C$ is stronger than that between $B$ and $C$ . Thus, a ﬁrst step in a greedy-search procedure would add an edge between $A$ and $C$ . Note, however, that score equivalence implies tha twork with the edge $A\rightarrow C$ has exactly the same score as the network with the edge C $C\rightarrow A$ → . At this stage, we cannot distinguish between the two choices. Thus, the decision between them is arbitrary (or, in some implementations, randomized). It is thus conceivable that at this stage we have the network shown in ﬁgure $18.6\mathrm{b}$ . The greedy procedure proceeds, and it decides to add the edge $B\ \rightarrow\ C$ , resultin e network of ﬁgure 18.6c. No we ar n the position to realize that reversing the edg $C\rightarrow A$ → can improve the score (since A and B together should make the best predictions of C ). However, if we do not have a reverse operator, a greedy procedure would not delete the edge $C\rightarrow A$ , since that would deﬁnitely hurt the score. 

In this example, note that when we do not perform the edge reversal, we might end up with the network shown in ﬁgure $18.6\mathrm{d}$ . To realize why, recall that although $A$ and $B$ are marginally independent, they are dependent given $C$ . Thus, $B$ and $C$ together make better predictions of $A$ than $C$ alone. 

# 18.4.3.2 The Search Procedure 

Once we deﬁne the search space, we need to design a procedure to explore it and search for high-scoring states. There is a wide literature on heuristic search. The vast majority of the search methods used in structure learning are local search procedures such as greedy hill climbing, as described in appendix A.4.2. In the structure-learning setting, we pick an initial network structure $\mathcal{G}$ as a starting point; this network can be the empty one, a random choice, the best tree, or a network obtained from some prior knowledge. We compute its score. We then consider all of the neighbors of $\mathcal{G}$ in the space — all of the legal networks obtained by applying a single operator to $\mathcal{G}$ — and compute the score for each of them. We then apply the change that leads to the best improvement in the score. We continue this process until no modiﬁcation improves the score. 

There are two questions we can ask. First, how expensive is this process, and second, what can we say about the ﬁnal network it returns? 

Computational Cost We start by brieﬂy considering the time complexity of the procedure. At each iteration, the procedure applies $|\mathcal O|$ operators and evaluates the resulting network. Recall that the space of operators we consider is quadratic in the number of variables. Thus, if we perform $K$ steps before convergence, then we perform $O(K\cdot n^{2})$ operator applications. Each operator application involves two steps. First, we need to check that the network is acyclic. This check can be done in time linear in the number of edges. If we are considering networks with indegree bounded by $d$ , then there are at most nd edges. Second, if the network is legal, we need to evaluate it. For this, we need to collect sufcient statistics from the data. These might be diferent for each network and require $O(M)$ steps, and so our rough time estimate is $O(K\cdot n^{2}\cdot(M+n d))$ . The number of iterations, $K$ , varies and depends on the starting netw and on how diferent the ﬁnal network is. However, we expect it not to be much larger than $n^{2}$ (since this is the diameter of the search space). We emphasize that this is a rough estimate, and not a formal statement. As we will show, we can make this process faster by using properties of the score that allow for smart caching. 

ﬁrst-ascent hill climbing 

local maximum plateau 

I-equivalence 

When $n$ is large, considering $O(n^{2})$ neighbors at each iteration may be too costly. How- ever, most operators attempt to perform a rather bad change to the network. So can we skip evaluating them? One way of avoiding this cost is to use search procedures that replace the exhaustive enumeration in line 5 of Greedy-Local-Search (algorithm A.5) by a randomized choice of operators. This ﬁrst-ascent hill climbing procedure samples operators from $\mathcal{O}$ and evaluates them one by one. Once it ﬁnds one that leads to a better-scoring network, it applies it without considering other operators. In the initial stages of the search, this procedure requires relatively few random trials before it ﬁnds such an operator. As we get closer to the local maximum, most operators hurt the score, and more trials are needed before an upward step is found (if any). 

Local Maxima What can we say about the network returned by a greedy hill-climbing search procedure? Clearly, the resulting network cannot be improved by applying a single operator (that is, changing one edge). This implies that we are in one of two situations. We might have reached a local maximum from which all changes are score-reducing. The other option is that we have reached a plateau : a large set of neighboring networks that have the same score. By design, the greedy hill-climbing procedure cannot “navigate” through a plateau, since it relies on improvement in score to guide it to better structures. 

Upon reﬂection, we realize that greedy hill climbing will encounter plateaus quite often. Recall that we consider scores that satisfy score equivalence. Thus, all networks in an I-equivalence class will have the same score. Moreover, as shown in theorem 3.9, the set of I-equivalent networks forms a contiguous region in the space, which we can traverse using a set of covered edge-reversal operations. Thus, any I-equivalence class necessarily forms a plateau in the search space. 

Recall that equivalence classes can potentially be exponentially large. Ongoing work stud- ies the average size of an equivalence class (when considering all networks) and the actual distributions of sizes encountered in realistic situations, such as during structure search. 

It is clear, however, that most networks we encounter have at least a few equivalent networks. Thus, we conclude that most often, greedy hill climbing will converge to an equivalence class. There are two possible situations: Either there is another network in this equivalence class from which we can continue the upward climb, or the whole equivalence class is a local maximum. Greedy hill climbing cannot deal with either situation, since it cannot explore without upward indications. 

As we discussed in appendix A.4.2, there are several strategies to improve on the network $\mathcal{G}$ returned by a greedy search algorithm. One approach that deals with plateaus induced by equivalence classes is to enumerate explicitly all the network structures that are I-equivalent to $\mathcal{G}$ , and for each one to examine whether it has neighbors with higher score. This enumeration, however, can be expensive when the equivalence class is large. An alternative solution, described in section 18.4.4, is to search directly over the space of equivalence classes. However, both of these approaches save us from only some of the plateaus, and not from local maxima. 

basin ﬂooding 

tabu search 

Appendix A.4.2 describes other methods that help address problem of local maxima. For example, basin ﬂooding keeps track of all previous networks and considers any operator leading from one of them to a structure that we have not yet visited. A key problem with this approach is that storing the list of networks we visited in the recent past can be expensive (recall that greedy hill climbing stores just one copy of the network). Moreover, we do not necessarily want to explore the whole region surrounding a local maximum, since it contains many variants of the same network. To see why, suppose that three diferent edges in a local maximum network can be removed with very little change in score. This means that all seven networks that contain at least one deletion will be explored before a more interesting change will be considered. 

A method that solves both problems is the tabu search of algorithm A.6. Recall that this procedure keeps a list of recent operators we applied, and in each step we do not consider operators that reverse the efect of recently applied operators. Thus, once the search decides to dd an edge, say $X\rightarrow Y$ , it cannot delete this edge in the next $L$ steps (for some prechosen $L$ ). Similarly, once an arc is reversed, it cannot be reversed again. As for the basin-ﬂooding approach, tabu search cannot use the termination criteria of greedy hill climbing. Since we want the search to proceed after reaching the local maxima, we do not want to stop when the score of the current candidate is smaller than the previous one. Instead, we continue the search with the hope of reaching a better structure. If this does not happen after a prespeciﬁed number of steps, we decide to abandon the search and select the best network encountered at any time during the search. 

Finally, as we discussed, one can also use randomization to increase our chances of escaping local maxima. In the case of structure learning, these methods do help. In particular, simulated annealing was reported to outperform greedy hill-climbing search. However, in typical example domains (such as the ICU-Alarm domain) it appears that simple methods such as tabu search with random restarts ﬁnd higher-scoring networks much faster. 

data perturbation Data Perturbation Methods So far, we have discussed only the application of general-purpose local search methods to the speciﬁc problem of structure search. We now discuss one class of methods — data-perturbation methods — that are more speciﬁc to the learning task. The idea is similar to random restarts: We want to perturb the search in a way that will allow it to overcome local obstacles and make progress toward the global maxima. Random restart methods achieve this perturbation by changing the network. Data perturbation methods, on the other hand, change the training data. 

To understand the idea, consider a perturbation that duplicates some instances (say by random choice) and removes others (again randomly). If we do a reasonable number of these mo ﬁcations, the resulting data set ${\mathcal{D}}^{\prime}$ has most of the characteristics of the original data set . For example, the value of sufcient statistics in the perturbed data are close 

![](images/dadb015bc3b645df01424118f30e40ef0803069ae2a56fa4b72750130d4c7136.jpg) 

to the values in the original data. Thus, we expect that big diferences between networks are preserved. That is, if $\begin{array}{r l}{\mathrm{score}(\mathcal{G}_{X\to Y}}&{:\quad\mathcal{D})\;\gg\;\mathrm{score}(\mathcal{G}_{2}\quad:\quad\mathcal{D})}\end{array}$ D ≫ G D , then we expect that score $(\mathcal{G}_{X\to Y}~:~\mathcal{D}^{\prime})\gg\mathrm{score}(\mathcal{G}_{2}~:~\mathcal{D}^{\prime})$ D ≫ G D . On the other hand, the perturbation does change comparison between networks that are simila The basic intuition is that the score using D ${\mathcal{D}}^{\prime}$ has the same broad outline as t e score using D , yet might have diferent ﬁne-grained pology. This suggests that a structure G tha s a local maximum when using the score on D is no longer a local maximum when using D ${\mathcal{D}}^{\prime}$ . The magnitude of perturbation determines the level of details that are preserved after the perturbation. 

weighted data instances 

We note that instead of duplicating and removing instances, we can achieve perturbation by weighting data instances. Much of the discussion on scoring networks and related topics applies without change if we assign weight to each instance. Formally, the only diference is the computation of sufcient statistics. If we have weights $w[m]$ for the $m$ ’th instance, then the sufcient statistics are redeﬁned as: 

$$
M[z]=\sum_{m}I\{Z[m]=z\}\cdot w[m].
$$ 

Note that when $w[m]=1$ , this reduces to the standard deﬁnition of sufcient statistics. Instance duplication and deletion lead to integer weights. However, we can easily consider perturbation that results in fractional weights. This leads to a continuous spectrum of data perturbations that range from small changes to weights to drastic ones. 

The actual search procedure is shown in algorithm 18.1. The heart of the procedure is the Perturb function. This procedure can implemented in diferent ways. A simple approach is to sample each $w[m]$ for a distribution whose variance is dictated by $t_{j}$ , for example, using a Gamma distribution, with mean 1 and variance $t$ . (Note that we need to use a distribution that attains nonnegative values. Thus, the Gamma distribution is more suitable than a Gaussian distribution.) 

# 18.4.3.3 Score Decomposition and Search 

The discussion so far has examined how generic ideas in heuristic search apply to structure learning. We now examine how the particulars of the problem impact the search. 

The dominant factor in the cost of the search algorithm is the evaluation of neighboring networks at each stage. As discussed earlier, the number of such networks is approximately $n^{2}$ . To evaluate each of these network structures, we need to score them. This process requires that we traverse all the diferent data cases, computing sufcient statistics relative to our new structure. This computation can get quite expensive, and it is the dominant cost in any structure learning algorithm. 

score decomposability 

delta score 

This key task is where the score decomposability property turns out to be useful. Recall that the scores we examine decompose into a sum of terms, one for each variable $X_{i}$ . Each of these family scores is computed relative only to the variables in the family of $X_{i}$ . A local change — adding, deleting, or reversing an edge — leaves almost all of the families in the network unchanged. (Adding and deleting changes one family, and reversing changes two.) For families whose composition does not change, the associated component of the score also does not change. To understand the importance of this observation, assume that our current candidate network is $\mathcal{G}$ . For each operator, we compute the improvement in the score that would result in making that change. We deﬁne the delta score 

$$
\delta(\mathcal{G}~:~o)=\mathrm{score}(o(\mathcal{G})~:~\mathcal{D})-\mathrm{score}(\mathcal{G}~:~\mathcal{D})
$$ 

to be the change of score associated with applying $O$ on $\mathcal{G}$ . Using score decomposition, we can compute this quantity relatively efciently. 

# Proposition 18.5 

Let $\mathcal{G}$ be a network structure and score be a decomposable score. 

$$
\delta(\mathcal{G}\ :\ o)=\mathrm{FastScone}(Y,\mathrm{Pa}_{Y}^{\mathcal{G}}\cup\{X\}\ :\ \mathcal{D})-\mathrm{FastScone}(Y,\mathrm{Pa}_{Y}^{\mathcal{G}}\ :\ \mathcal{D}).
$$ 

$$
\delta(\mathcal{G}\ :\ o)=\mathrm{FastScone}(Y,\mathrm{Pa}_{Y}^{\mathcal{G}}-\{X\}\ :\ \mathcal{D})-\mathrm{FastScone}(Y,\mathrm{Pa}_{Y}^{\mathcal{G}}\ :\ \mathcal{D}).
$$ 

$$
\begin{array}{r c l}{\delta(\mathcal{G}\ :\ o)}&{=}&{\mathrm{Famscover}(X,\mathrm{Pa}_{X}^{\mathcal{G}}\cup\{Y\}\ :\ \mathcal{D})+\mathrm{Famscover}(Y,\mathrm{Pa}_{Y}^{\mathcal{G}}-\{X\}\ :\ \mathcal{D})}\\ &&{-\mathrm{Famscover}(X,\mathrm{Pa}_{X}^{\mathcal{G}}\ :\ \mathcal{D})-\mathrm{Famscover}(Y,\mathrm{Pa}_{Y}^{\mathcal{G}}\ :\ \mathcal{D}).}\end{array}
$$ 

See exercise 18.18. 

Note that these computations involve only the sufcient statistics for the particular family that changed. This requires a pass over only the appropriate columns in the table describing the training data. 

Now, assume that we have an operator $O$ , say “Add $X\rightarrow Y$ ,” and instead of applying this edge addition, we have decided to apply another operator o $o^{\prime}$ that changes the family of some variable $Z$ (for $Z\neq Y)$ ), producing a new graph ${\mathcal{G}}^{\prime}$ . The key observation is that $\delta(\mathcal{G}^{\prime}\,\,\,\,:\,\,\,\,o)$ remains unch nged — we d not need to recompute it. We need only to recompute $\delta(\mathcal{G}^{\prime}~:~o^{\prime})$ for operators o $o^{\prime}$ that involve $Y$ . 

• If o is either “Add $X\rightarrow Y$ ” or “Delete $X\rightarrow Y^{\astrosun}$ and $\mathrm{Pa}_{Y}^{\mathcal{G}}=\mathrm{Pa}_{Y}^{\mathcal{G}^{\prime}}$ , then $\delta({\mathcal{G}}\ :\ o)=\delta({\mathcal{G}}^{\prime}\ :$ $O$ ) . • If o is “Reverse $X\rightarrow Y$ ,” $\mathrm{Pa}_{Y}^{\mathcal{G}}=\mathrm{Pa}_{Y}^{\mathcal{G}^{\prime}}$ , and $\mathrm{Pa}_{X}^{\mathcal{G}}=\mathrm{Pa}_{X}^{\mathcal{G}^{\prime}}$ , then $\delta({\mathcal{G}}\ :\ o)=\delta({\mathcal{G}}^{\prime}\ :\ o)$ . 

See exercise 18.19. 

# 

This shows that we can cache the computed $\delta(\mathcal{G}\quad:\quad o)$ for diferent operators and then reuse most of them in later search steps. The basic idea is to maintain a data structure that records for each operator $O$ the value of $\delta(\mathcal{G}\ \ :\ \ o)$ with respect to the current network $\mathcal{G}$ . After we apply a step in the search, we have a new current network, and we need to update this data structure. Using proposition 18.6 we see that most of the computed values do not need to be changed. We need to recompute $\delta(\mathcal{G}^{\prime}\,\,\,\,:\,\,\,\,o)$ only for operators that modify one of the families that we modiﬁed in the recent step. By careful data-structure design, this cache can save us a lot of computational time; see box 18.A for details. Overall, the decomposability of the scoring function provides signiﬁcant reduction in the amount of computation that we need to perform during the search. This observation is critical to making structure search feasible for high-dimensional spaces. 

Box 18.A — Skill: Practical Collection of Sufcient Statistics. The passes over the training data required to compute sufcient statistics generally turn out to be the most computationally intensive part of structure learning. It is therefore crucial to take advantage of properties of the score in order to ensure efcient computations as well as use straightforward organizational tricks. 

One important source of computational savings derives from proposition 18.6. As we discussed, this proposition allows us to avoid recomputing many of the delta-scores after taking a step in the search. We can exploit this observation in a variety of ways. For example, if we are performing greedy hill climbing, we know that the search will necessarily examine all operators. Thus, after each step we can update the evaluation of all the operators that were “damaged” by the last move. The number of such operators is $O(n)$ , and so this requires $O(n\cdot M)$ time (since we need to collect sufcient statistics from data). Moreover, if we keep the score of diferent operators in a heap, we spend $O(n\log{n})$ steps to update the heap but then can retrieve the best operator in constant time. Thus, although the cost of a single step in the greedy hill-climbing procedure seems to involve quadratic number of operations of $O(n^{2}\cdot M)$ , we can perform it in time $O(n\cdot M+n\log n)$ . 

We can further reduce the time consumed by the collection of sufcient statistics by considering additional levels of caching. For example, if we use table-CPDs, then the counts needed for evalu- 

![](images/19c9fbeec49f4404a14f2f89c1845d4abdeb18550033de1c994c4682752e7d83.jpg) 
Figure 18.7 Performance of structure and parameter learning for instances generated from the ICU-Alarm network. The graph shows the KL-divergence of the learned to the true network, and compares two learning tasks: learning the parameters only, using a correct network structure, and learning both parameters and structure. The curves shows average performance over 10 data sets of the same size, with error bars showing $+/.$ - one standard deviation. The error bars for parameter learning are much smaller and are not shown. 

ating $X$ as a parent of $Y$ and the counts needed to evaluate $Y$ as a parent of $X$ are the same. Thus, we can save time by caching previously computed counts, and also by marginalizing counts such as $M[x,y]$ to compute $M[x]$ . A more elaborate but potentially very efective approach is one where we plan the collection of the entire set of sufcient statistics needed. In this case, we can use efcient algorithms for the set cover problem to choose a smaller set of sufcient statistics that covers all the needed computations. There are also efcient data structures (such as AD-trees for discrete spaces and KD-trees or metric trees for continuous data) that are designed explicitly for maintaining and retrieving sufcient statistics; these data structures can signiﬁcantly improve the performance of the algorithm, particularly when we are willing to approximate sufcient statistics in favor of dramatic speed improvements. 

One cannot overemphasize the importance of these seemingly trivial caching tricks. In practice, learning the structure of a network without making use of such tricks is infeasible even for a modest number of variables. 

# 18.4.3.4 Empirical Evaluation 

In practice, relatively cheap and simple algorithms, such as tabu search, work quite well. Fig- ure 18.7 shows the results of learning a network from data generated from the ICU-Alarm network. The graph shows the KL-divergence to the true network and compares two learning tasks: learn- ing the parameters only, using a correct network structure, and learning both parameters and structure. Although the graph does show that it is harder to recover both the structure and the parameters, the diference in the performance achieved on the two tasks is surprisingly small. We see that structure learning is not necessarily a harder task than parameter estimation, although computationally, of course, it is more expensive. We note, however, that even the com- putational cost is not prohibitive. Using simple optimization techniques (such as tabu search with random restarts), learning a network with a hundred variables takes a few minutes on a standard machine. 

We stress that the networks learned for diferent sample sizes in ﬁgure 18.7 are not the same as the original networks. They are usually simpler (with fewer edges). As the graph shows, they perform quite similarly to the real network. This means that for the given data, these networks seem to provide a better score, which means a good trade-of between complexity and ﬁt to the data. As the graph suggests, this estimate (based on training data) is quite reasonable. 

# 18.4.4 Learning with Equivalence Classes $\star$ 

The preceding discussion examined diferent search procedures that attempt to escape local maxima and plateaus in the search space. An alternative approach to avoid some of these pitfalls is to change the search space. In particular, as discussed, many of the plateaus we encounter during the search are a consequence of score equivalence — equivalent networks have equivalent scores. This observation suggests that we can avoid these plateaus if we consider searching over equivalence classes of networks. 

class PDAG 

To carry out this idea, we need to examine carefully how to construct the search space. Recall that an equivalence class of networks can be exponential in size. Thus, we need a compact representation of states (equivalence classes) in our search space. Fortunately, we already encountered such a representation. Recall that a class PDAG is a partially directed graph that corresponds to an equivalence class of networks. This representation is relatively compact, and thus, we can consider the search space over all possible class PDAGs. 

Next, we need to answer the question how to score a given class PDAG. The scores we discussed are deﬁned for over network structures (DAGs) and not over PDAGs. Thus, to score a class PDAG $\mathcal{K}$ , we need to build a network $\mathcal{G}$ in the equivalence class represented by $\mathcal{K}$ and then score it. As we saw in section 3.4.3.3, this is a fairly straightforward procedure. 

Finally, we need to decide on our search algorithm. Once again, we generally resort to a hill-climbing search using local graph operations. Here, we need to deﬁne appropriate search operations on the space of PDAGs. One approach is to use operations at the level of PDAGs. In this case, we need operations that add, remove, and reverse edges; moreover, since PDAGs contain both directed edges and undirected ones, we may wish to consider operations such as adding an undirected edge, orienting an undirected edge, and replacing a directed edge by an undirected one. An alternative approach is to use operators in DAG space that are guaranteed to change the equivalence class. In particular, consider an equivalence c $\mathcal{E}$ (represented as a class PDAG). We can deﬁne as our operat s any step that takes a DAG G ∈E , adds or etes edge from $\mathcal{G}$ to produce a new DAG G ${\mathcal{G}}^{\prime}$ , and then constructs the equivalence class E $\mathcal{E}^{\prime}$ for ${\mathcal{G}}^{\prime}$ G (represented again as a class DAG). nce both edge addition and edge deletion change the skeleton, we are guaranteed that E and E $\mathcal{E}^{\prime}$ are distinct equivalence classes. 

GES algorithm 

One algorithm based on this last approach is called the GES algorithm , for greedy equivalence search . GES starts out with the equivalence class for the empty graph and then takes greedy edge-addition steps until no additional edge-addition steps improve the score. It then executes the reverse procedure, removing edges one at a time until no additional edge-removal steps improve the score. 

consistent score 

When used with a consistent score (as in deﬁnition 18.1), this simple two-pass algorithm has me satisfying guarantees. Assume that our distribution $P^{*}$ is fait l for the graph $\mathcal{G}^{*}$ over $\mathcal{X}$ X ; thus, as in section 18.2, there are no spurious independencies in P $P^{*}$ . Moreover, assume that we have (essentially) inﬁnite data. Under these assumptions, our (consistent) scoring function gives only the correct equivalence class — the equivalence class of $\mathcal{G}^{*}$ — the highest s e. For this setting, one can show that GES is guaranteed to produce the equivalence class of G $\mathcal{G}^{\ast}$ as its output. 

Although the assumptions here are fairly strong, this result is still important and satisfying. Moreover, empirical results suggest that GES works reasonably well even when some of its assumptions are violated (to an extent). Thus, it also provides a reasonable alternative in practice. 

Although simple in principle, there are two signiﬁcant computational issues associated with GES and other algorithms that work in the space of equivalence classes. The ﬁrst is the cost of generating the equivalence classes that result from the local search operators discussed before. The second is the cost of evaluating their scores while reusing (to the extent possible) the sufcient statistics from our current graph. Although nontrivial (and outside the scope of this book), local operations that address both of these tasks have been constructed, making this algorithm a computationally feasible alternative to search over DAG space. 

dependency networks 

Box 18.B — Concept: Dependency Networks. An alternative formalism for parameterizing a Markov network is by associating with each variable $X_{i}$ a conditional probability distribution (CPD) $P_{i}(X_{i}\mid\mathcal{X}-\{X_{i}\})\,=\,P_{i}(X_{i}\mid\mathrm{MB}_{\mathcal{H}}(X_{i}))$ . Networks parameterized in this way are sometimes called dependency networks and are drawn as a cyclic directed graph, with edges to each variable from all of the variables in its Markov blanket. 

This representation ofers certain trade-ofs over other representations. In terms of semantics, $a$ key limitation of this parameter iz ation is that a set of Ds $\{P_{i}(X_{i}\ |\ \mathrm{MB}_{\mathcal{H}}(X_{i}))\ :\ X_{i}\in\mathcal{X}\}$ may not be consistent with any probability distribution P ; that is, there may not be a distribution $P$ h that $P_{i}(X_{i}\mid\mathrm{MB}_{\mathcal{H}}(X_{i}))=P(X_{i}\mid\mathrm{MB}_{\mathcal{H}}(X_{i}))$ for all $i$ (hence the use of the subscrip $i$ on $P_{i}$ ). Moreover, determining whether such a set of CPDs is consistent with some distribution P is a computationally difcult problem. Thus, eliciting or learning a consistent dependency network can be quite difcult, and the semantics of an inconsistent network is unclear. 

However, in a noncausal domain, dependency networks arguably provide a more appropriate representation of the dependencies in the distribution than a Bayesian network. Certainly, for a lay user, understanding the notion of a Markov blanket in a Bayesian network is not trivial. On the other hand, in comparison to Markov networks, the CPD parameter iz ation is much more natural and easy to understand. (As we discussed, there is no natural interpretation for a Markov network factor in isolation.) 

From the perspective of inference, dependency networks provide a very easy mechanism for answering queries where all the variables except for a single query variable are observed (see box 18.C). However, answering other queries is not as obvious. The representation lends itself very nicely to Gibbs sampling, which requires precisely the distribution of individual variables given their Markov blanket. However, exact inference requires that we transform the network to a standard 

![](images/a411bb7934818b301e93600850a4b5c7538b93f7447b398877f01985ef8de510.jpg) 
Figure 18.C.1 — Learned Bayesian network for collaborative ﬁltering. A fragment of a Bayesian network for collaborative ﬁltering, learned from Nielsen TV rating data, capturing the viewing record of sample viewers. The variables denote whether a TV program was watched. 

parameter iz ation, a task that requires a numerical optimization process. 

The biggest advantage arises in the learning setting. If we are willing to relax the consistency requirement, the problem of learning such networks from data becomes quite simple: we simply have to learn a CPD independently for each variable, a task to which we can apply a wide variety of standard supervised learning algorithms. In this case, however, it is arguable whether the resulting network can be considered a uniﬁed probabilistic model, rather than a set of stand-alone predictors for individual variables. 

collaborative ﬁltering 

Box 18.C — Case Study: Bayesian Networks for Collaborative Filtering. In many marketing settings, we want to provide to a user a recommendation of an item that he might like, based on previous items that he has bought or liked. For example, a bookseller might want to recommend books that John might like to buy, using John’s previous book purchases. Because we rarely have enough data for any single user to determine his or her preferences, the standard solution is an approach called collaborative ﬁltering , which uses the observed preferences of other users to try to determine the preferences for any other user. There are many possible approaches to this problem, including ones that explicitly try to infer key aspects of a user’s preference model. 

One approach is to learn the dependency structure between diferent purchases, as observed in the population. We treat each item $i$ as a variable $X_{i}$ in a joint distribution, and each user as an instance. Most simply, we view a purchase of an item $i$ (or some other indication of preference) as one value for the variable $X_{i}$ , and the lack of a purchase as a diferent value. (In certain settings, we may get explicit ratings from the user, which can be used instead.) We can then use structure learning to obtain a Bayesian network model over this set of random variables. Dependency networks (see box 18.B) have also been used for this task; these arguably provide a more intuitive visualization of the dependency model to a lay user. 

Both models can be used to address the collaborative ﬁltering task. Given a set of purchases for a set of items $S$ , we can compute the probability that the user would like a new item i . In general, this question is reduced to a probabilistic inference task where all purchases other than $S$ and $i$ are set to false; thus, all variables other than the query variable $X_{i}$ are taken to be observed. In a Bayesian network, this query can be computed easily by simply looking at the Markov blanket of $X_{i}$ . In a dependency network, the process is even simpler, since we need only consider the CPD for $X_{i}$ . Bayesian networks and Markov networks ofer diferent trade-ofs. For example, the learning and prediction process for dependency networks is somewhat easier, and the models are arguably more understandable. However, Bayesian networks allow answering a broader range of queries — for example, queries where we distinguish between items that the user has viewed and chosen not to purchase and items that the user simply has not viewed (whose variables arguably should be taken to be unobserved). 

Heckerman et al. (2000) applied this approach to a range of diferent collaborative ﬁltering data sets. For example, ﬁgure 18.C.1 shows a fragment of a Bayesian network for TV-watching habits learned from Nielsen viewing data. They show that both the Bayesian network and the dependency network methods performed signiﬁcantly better than previous approaches proposed for this task. The performance of the two methods in terms of predictive accuracy is roughly comparable, and both were ﬁelded successfully as part of Microsoft’s E-Commerce software system. 

# 18.5 Bayesian Model Averaging $\star$ 

# 18.5.1 Basic Theory 

Bayesian estimation We now reexamine the basic principles of the learning problem. Recall that the Bayesian method- ology suggests that we treat unknown parameters as random variables and should consider all possible values when making predictions. When we do not know the structure, the Bayesian methodology suggests that we should consider all possible graph structures. Thus, according to Bayesian theory, given a data set $\mathcal{D}=\{\xi[1],\dots,\xi[M]\}$ we should make predictions according to the Bayesian estimation rule: 

$$
P(\xi[M+1]\mid\mathcal{D})=\sum_{\mathcal{G}}P(\xi[M+1]\mid\mathcal{D},\mathcal{G})P(\mathcal{G}\mid\mathcal{D}),
$$ 

where $P(\mathcal{G}\mid\mathcal{D})$ is posterior probability of diferent networks given the data 

$$
P(\mathcal{G}\mid\mathcal{D})=\frac{P(\mathcal{G})P(\mathcal{D}\mid\mathcal{G})}{P(\mathcal{D})}.
$$ 

In our discussion so far, we searched for a single structure $\mathcal{G}$ that maximized the Bayesian score, and thus also the posterior probability. When is the focus on a single structure justiﬁed? 

Recall that the logarithm of th marginal likelihood $\log P(\mathcal{D}\mid\mathcal{G})$ grows linearly with the number of samples. Thus, when M is large, there will be large diferences between the top- scoring structure and all the rest. We used this property in the proof of theorem 18.2 to show that for asymptotically large $M$ , the best-scoring structure is the true one. Even when $M$ is not that large, the posterior probability of this particular equivalence class of structures will be exponentially larger than all other structures and dominate most of the mass of the posterior. In such a case, the posterior mass is dominated by this single equivalence class, and we can approximate equation (18.10) with 

$$
P(\xi[M+1]\mid\mathcal{D})\approx P(\xi[M+1]\mid\mathcal{D},\tilde{\mathcal{G}}),
$$ 

where ${\tilde{\mathcal{G}}}=\arg\operatorname*{max}_{\mathcal{G}}P(\mathcal{G}\mid\mathcal{D})$ G . The intuition is that $P(\tilde{\mathcal{G}}\mid\mathcal{D})\approx1$ G | D ≈ , and $P(\mathcal{G}\mid\mathcal{D})\approx0$ for any G other structure. 

What happens when we consider learning with smaller number of samples? In such a situation, the posterior mass might be distributed among many structures (which may not include the true structure). If we are interested only in density estimation, this might not be a serious problem. If $P(\xi[M+1]\mid\mathcal{D},\mathcal{G})$ is similar for the diferent structure with high posterior, then picking one of them will give us reasonable performance. Thus, if we are doing density estimation, then we might get away with learning a single structure and using it. However, we need to remember that the theory suggests that we consider the whole set of structures when making predictions. 

structure discovery 

conﬁdence estimation 

network features 

If we are interested in structure discovery , we need to be more careful. The fact that several networks have similar scores suggests that one or several of them might be close to the “true” structure. However, we cannot really distinguish between them given the data. If this is the situation, then we should not be satisﬁed with picking one of these structures (say, even the one with the best score) and drawing conclusions about the domain. Instead, we want to be more cautious and quantify our conﬁdence about the conclusions we make. Such conﬁdence estimates are crucial in many domains where we use Bayesian network learning to learn about the structure of processes that generated data. This is particularly true when the available data is limited. 

To consider this problem, we need to specify more precisely what would help us understand the posterior over structures. In most cases, we do not want to quantify the posterior explicitly. Moreover, the set of networks with “high” posterior probability is usually large. To deal with this issue, there are various approaches we can take. A fairly general one is to consider network feature queries. Such a query can ask what is the probability that an edge, say $X\rightarrow Y$ , appears in the “true” network. Another possible query might be about separation in the “true” network — for example, whether $d{\textrm{\-}}s e p(X;Y\mid Z)$ holds in this network. In general, we can formulate such a query as a function $f(\mathcal{G})$ . For a binary feature, $f(\mathcal{G})$ can return 1 when the network structure contains the feature, and 0 otherwise. For other features, $f(\mathcal{G})$ may return a numeri quantity that is relevant to the graph structure (such as the length of the shortest trail from X to $Y$ , or the number of nodes whose outdegree is greater than some threshold $k$ ). 

Given a umerical feature $f(\mathcal{G})$ , we can compute its expectation over all possible network structures : 

$$
E_{P(\mathcal{G}\mid\mathcal{D})}[f(\mathcal{G})]=\sum_{\mathcal{G}}f(\mathcal{G})P(\mathcal{G}\mid\mathcal{D}).
$$ 

In particular, for a binary feature $f$ , this quantity is simply the posterior probability $P(f\mid{\mathcal{D}})$ . The problem in computing either equation (18.10) or equation (18.11), of course, is that the number of possible structures is super exponential; see exercise 18.20. 

We can reduce this number by restricting attention to structures $\mathcal{G}$ where there is a bound $d$ on the number of parents per variable. This assumption, which we will make throughout this section, is a fairly innocuous one. There are few applications in which very large families are called for, and there are rarely enough data to support robust parameter estimation for such families. From a more formal perspective, networks with very large families tend to have low score. Let ${\pmb G}_{d}$ G be the set all graphs with indegree bounded by some constant $d$ . Note that the number of structures in ${\pmb G}_{d}$ is still super exponential; see exercise 18.20. 

Thus, an exhaustive enumeration over the set of possible network structures is feasible only for tiny domains (4–5 variables). In the next sections we consider several approaches to address this problem. In the following discussion, we assume that we are using a Bayesian score that decomposes according to the assumptions we discussed in section 18.3. Recall that the Bayesian score, as deﬁned earlier, is equal to $\log P(\mathcal{D},\mathcal{G})$ . Thus, we have that 

$$
P(\mathcal{D},\mathcal{G})=\prod_{i}\exp\{\mathrm{Fastvarkappa}_{B}(X_{i}\ |\ \mathrm{Pa}_{X_{i}}^{\mathcal{G}}\ :\ \mathcal{D})\}.
$$ 

# 18.5.2 Model Averaging Given an Order 

In this section, we temporarily turn our attention to a somewhat easier problem. Rather than perform model averaging over the space of all structures, we restrict attention to structures that are consistent with so e predetermined variable ordering $\prec$ . As in section 18.4.2, we restrict attention to structures G such that there is an edge $X_{i}\rightarrow X_{j}$ , only if $X_{i}\prec X_{j}$ . 

# 18.5.2.1 Computing the marginal likelihood 

marginal likelihood We ﬁrst consider the problem of computing the marginal likelihood of the data given the order: 

$$
P(\mathcal{D}\mid\prec)=\sum_{\mathcal{G}\in\pmb{\mathscr{G}}_{d}}P(\mathcal{G}\mid\prec)P(\mathcal{D}\mid\mathcal{G}).
$$ 

Note that this summation, although restricted to networks with bounded indegree and consistent with $\prec,$ , is still exponentially large; see exercise 18.20. 

Before we compute the marginal likelihood, we note that computing the marginal likelihood is equivalent to making predictions with equation (18.10). To see this, we can use the deﬁnition of probability to see that 

$$
P(\xi[M+1]\mid\mathcal{D},\prec)=\frac{P(\xi[M+1],\mathcal{D}\mid\prec)}{P(\mathcal{D}\mid\prec)}.
$$ 

Now both terms on the right are marginal likelihood terms, one for the original data and the other for original data extended by the new instance. 

We now return to the computation of the marginal likelihood. The key insight is that when we restrict attention to structures consistent with a given order $\prec,$ the choice of family for one variable places no additional constraints on the choice of family for another. Note that this property does not hold without the restriction on the order; for example, if we pick $X_{i}$ to be a parent of $X_{j}$ , then $X_{j}$ cannot in turn be a parent of $X_{i}$ . 

Therefore, we can choose a structure $\mathcal{G}$ consistent with $\prec$ by choosing, independently, a family $U_{i}$ for each variable $X_{i}$ . Global parameter modularity assumption states that the choice of parameters for the family of $X_{i}$ is independent of the choice of family for another family in the network. Hence, summing over possible graphs consistent with $\prec$ is equivalent to summing over possible choices of family for each variable, each with its parameter prior. Given our constraint on the size of the family, the possible parent sets for the variable $X_{i}$ are 

$$
{\mathcal{U}}_{i,{\mathcal{I}}}=\{{\pmb{U}}\ :\ {\pmb{U}}\prec X_{i},|{\pmb{U}}|\leq d\},
$$ 

where $U\prec X_{i}$ is ned to hold wh all variables in $U$ precede $X_{i}$ in $\prec$ . Let $\pmb{\mathcal{G}}_{d,-}$ G be the ≺ set of structures in G ${\pmb G}_{d}$ consistent with ≺ . Using equation (18.12), we have that 

$$
\begin{array}{r c l}{P(\mathcal{D}\mid\prec)}&{=}&{\displaystyle\sum_{\mathcal{G}\in\pmb{\mathcal{G}}_{d,\prec}}\prod_{i}\exp\{\mathrm{Fastvarkappa}_{B}(X_{i}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\,:\,\mathcal{D})\}}\\ &{=}&{\displaystyle\prod_{i}\displaystyle\sum_{\boldsymbol{U}_{i}\in\mathcal{U}_{i,\prec}}\exp\{\mathrm{Fastvarkappa}_{B}(X_{i}\mid\boldsymbol{U}_{i}\,:\,\mathcal{D})\}.}\end{array}
$$ 

Intuitively, the equality states that we can sum over all network structures consistent with $\prec$ by summing over the set of possible families for each variable, and then multiplying the results for the diferent variables. This transformation allows us to compute $P(\mathcal{D}\mid\prec)$ efciently. The expression on the right-hand side consists of a product with a term for each variable $X_{i}$ , each of which is a summation over all possible families for $X_{i}$ . Given a bound $d$ over the number of parents, the number of possible families for a variable $X_{i}$ is at most ${\binom{n}{d}}\leq n^{d}$  . Hence, the total cost of computing equation (18.14) is at most $n\cdot n^{d}=n^{d+1}$ . 

# 18.5.2.2 Probabilities of features 

For certain types of features $f$ , we can use the technique of the previous section to compute, in closed form, the probability $P(f\mid\prec,{\mathcal{D}})$ that $f$ holds in a structure given the order and the data. 

In general, if $f$ is a feature. We want to compute 

$$
P(f\mid\prec,{\mathcal{D}})=\frac{P(f,{\mathcal{D}}\mid\prec)}{P({\mathcal{D}}\mid\prec)}.
$$ 

We have just shown how to compute the denominator. The numerator is a sum over all structures that contain the feature and are consistent with the order: 

$$
P(f,{\mathcal{D}}\mid\prec)=\sum_{{\mathcal{G}}\in{\pmb{\mathscr{G}}}_{d,\prec}}f({\mathcal{G}})P({\mathcal{G}}\mid\prec)P({\mathcal{D}}\mid{\mathcal{G}}).
$$ 

The computation of this term depends on the speciﬁc type of feature $f$ . 

The simplest situation is when we want to compute the posterior probability of a particular choice of parents $U$ . This in efect requires us to sum over all graphs where $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\stackrel{\cdot}{=}U$ . In this case, we can apply the same closed-form analysis to (18.15). The only diference is that we restrict $\mathcal{U}_{i,-}$ to be the singleton $\{U\}$ . Since the terms that sum over the parents of $X_{j}$ for $i\neq j$ are not disturbed by this constraint, they cancel out from the equation. 

Proposition 18.7 

$$
\begin{array}{r l r}{P(\mathrm{Pa}_{X_{i}}^{\mathcal{G}}=U\mid D,\prec)}&{=}&{\frac{\exp\{\mathrm{Famsccore}_{B}(X_{i}\mid U\;:\;\mathcal{D})\}}{\sum_{U^{\prime}\in\mathcal{U}_{i,\prec}}\exp\{\mathrm{Famsccore}_{B}(X_{i}\mid U^{\prime}\;:\;\mathcal{D})\}}.}\end{array}
$$ 

A slightly more complex situation is when we want to compute the posterior probability of the edge feature $X_{i}\rightarrow X_{j}$ . Agai can apply the same closed-form analy to (18.15). The only diference is that we restrict U $\mathcal{U}_{j,\prec}$ to consist only of subsets that contain $X_{i}$ . 

Proposition 18.8 

$$
P(X_{j}\in\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\mid\prec,\mathcal{D})=\frac{\sum_{\{U\in\mathcal{U}_{i,\prec}\ :\ X_{j}\in U\}}\exp\{\mathrm{ramSccore}_{B}(X_{i}\mid U\ :\ \mathcal{D})\}}{\sum_{U\in\mathcal{U}_{i,\prec}}\exp\{\mathrm{ramSccore}_{B}(X_{i}\mid U\ :\ \mathcal{D})\}}.
$$ 

Unfortunately, this approach cannot be used to compute the probability of arbitrary structural features. For example, we cannot compute the probability that there exists some directed path from $X_{i}$ to $X_{j}$ , as we would have to consider all possible ways in which a path from $X_{i}$ to $X_{j}$ could manifest itself through exponentially many structures. 

We can overcome this difculty using a simple sampling approach. Proposition 18.7 provides us with a closed-form expression for the exact posterior probability of the diferent possible families of the variable $X_{i}$ . We can therefore easily sample entire networks from the posterior distribution given the order: we simply sample a family for each variable, according to the distribution speciﬁed in proposition 18.7. We can then use the sampled networks to evaluate any feature, such as $X_{i}$ is ancestor of $X_{j}$ . 

# 18.5.3 The General Case 

In the previous section, we made the simplifying assumption that we were given a predetermined order. Although this assumption might be reasonable in certain cases, it is clearly too restrictive in domains where we have very little prior knowledge. We therefore want to consider structures consistent with all possible orders. Here, unfortunately, we have no elegant tricks that allow an efcient, closed-form solution. As with search problems we discussed, the choices of parents for one variable can interfere with the choices of parents for another. 

A general approach is try to approximate the exhaustive summation over structures in quan- tities of interest (that is, equation (18.10) and equation (18.11)) with approximate sums. For this purpose we utilize ideas that are similar to the ones we discuss in chapter 12 in the context of approximate inference. 

A ﬁrst-cut approach for approximating the sums in our case is to ﬁ d a set $\mathcal{G}^{\prime}$ G of high scoring structures, and then estimate the relative mass of the structures in G . Thus, for example, we would approximate equation (18.11) with 

$$
P(f\mid\mathcal{D})\approx\frac{\sum_{\mathcal{G}\in\pmb{\mathscr{G}}^{\prime}}P(\mathcal{G}\mid\mathcal{D})f(\mathcal{G})}{\sum_{\mathcal{G}\in\pmb{\mathscr{G}}^{\prime}}P(\mathcal{G}\mid\mathcal{D})}.
$$ 

This approach leaves open the question of how we construct $\mathcal{G}^{\prime}$ G . The simplest approach is to use model selection to pick a single high-scoring structure and then use that as our approximation. If the amount of data is large relative to the size of the model, then the posterior will be sharply peaked around a single model, and this approximation is a reasonable one. However, as we discussed, when the amount of data is small relative to the size of the model, there is usually a large number of high-scoring models, so that using a single model as our set $\pmb{\mathcal{G}}^{\prime}$ is a very poor approximation. 

We can ﬁnd a larger set of structures by recording all the structures examined during the search and returning the high-scoring ones. However, the set of structures found in this manner is quite sensitive to the search procedure we use. For example, if we use greedy hill climbing, then the set of structures we collect will all be quite similar. Such a restricted set of candidates also shows up when we consider multiple restarts of greedy hill climbing. This is a serious problem, since we run the risk of getting estimates of conﬁdence that are based on a biased sample of structures. 

# 18.5.3.1 MCMC Over Structures 

An alternative approach is based on the use of sampling. As in chapter 12, we aim to approximate the expectation over graph structures in equation (18.10) and equation (18.16) by an empirical average. Thus, if we manage to sample graphs $\mathcal{G}_{1},\dots,\mathcal{G}_{K}$ from $P(\mathcal{G}\mid\mathcal{D})$ , we can approximate equation (18.16) as 

$$
P(f\mid\mathcal{D})\approx\frac{1}{K}\sum_{k}f(\mathcal{G}_{k}).
$$ 

The question is how to sample from the posterior distribution. One possible answer is to use the general tool of Markov chain Monte Carlo (MCMC) simulation; see section 12.3. In this case, we deﬁne a Markov chain over the space of possible structures whose stationary distribution is the posterior distribution $P(G\mid{\mathcal{D}})$ . We then generate a set of possible structures by doing a random walk in this Markov chain. Assuming that we continue this process until the chain converges to the stationary distribution, we can hope to get a set of structures that is representative of the posterior. 

How do we construct a Markov chain over the space of structures? The idea is a fairly straightforward application of the principles discussed in section 12.3. The states of the Markov chain are graphs in the set $\pmb{\mathcal{G}}$ G of graphs we want to consider. We consider local operations (for example, add edge, delete edge, reverse edge) that transform one structure to another. We assume we have a proposal distribution $\mathcal{T}^{Q}$ over such operations. We then apply the Metropolis- stings acceptance rule. Suppose that the current state is $\mathcal{G}$ , and we sample the transition to ${\mathcal{G}}^{\prime}$ G from the proposal distribution, then we accept this transition with probability 

$$
\operatorname*{min}\left[1,\frac{P(\mathcal{G}^{\prime},\mathcal{D})\mathcal{T}^{Q}(\mathcal{G}^{\prime}\rightarrow\mathcal{G})}{P(\mathcal{G},\mathcal{D})\mathcal{T}^{Q}(\mathcal{G}\rightarrow\mathcal{G}^{\prime})}\right].
$$ 

As discussed in section 12.3, this strategy ensures that we satisfy the detailed balance condition. To ensure that we have a regular Markov chain, we so need to verify that the ace $\pmb{\mathcal{G}}$ G is connected, that is, that we can reach each structure in G from any other structure in G through a sequence of operations. This is usually easy to ensure with the set of operators we discussed. 

![](images/008f4ea2d1f28f3b9736a54a776b11c814baef97368601c9fea5137c067dba38.jpg) 
Figure 18.8 MCMC structure search using 500 instances from ICU-Alarm network. (a) & (b) Plots of the progression of the MCMC process for two runs for 500 instances sampled from the ICU-Alarm network. The $x$ -axis denotes the iteration number, and the $y$ -axis denotes the score of the current structure. (a) A run initialized from the empty structure, and (b) a run initialized from the structure found by structure search. (c) Comparison of the estimates of the posterior probability of edges using 100 networks sampled every 2,000 iterations from the each of the runs (after initial burn-in of 20,000 iterations). Each point denotes an edge, the $x$ -coordinate is the estimate using networks from run (a) and the $y$ -coordinate is the estimate using networks from run (b). 

It is important to stress that if the operations we apply are local (such as edge addition, deletion, and reversal), then we can efciently compute the ratio $\frac{P(\mathcal{G}^{\prime},\mathcal{D})}{P(\mathcal{G},\mathcal{D})}$ . To see this, note that G D the logarithm of this ratio is the diference in score between the two graphs. As we discussed in section 18.4.3.3, this diference involves only terms that relate to the families of the variables that are involved in the local move. Thus, performing MCMC over structures can use the same caching schemes discussed in section 18.4.3.3, and thus it can be executed efciently. 

The ﬁnal detail we need to consider is the choice the proposal distribution $\mathcal{T}^{Q}$ . Many choices are reasonable. The simplest one to use, and one that is often used in practice, is the uniform distribution over all possible operations (excluding ones that violate acyclicity or other constraints we want to impose). 

To demonstrate the use of MCMC over structures, we sampled 500 instances from the ICU- Alarm network. This is a fairly small training set, and so we do not hope to recover one network. Instead, we run the MCMC sampling to collect 100 networks and use these to estimate the posterior of diferent edges. One of the hard problems in using such an approach is checking whether the MCMC simulation converged to the stationary distribution. One ad-hoc test we can perform is to compare the results of running several independent simulations. For example, in ﬁgure 18.8a and 18.8b we plot the progression of two runs, one starting from the empty structure and the other from the structure found by structure search on the same data set. We see that initially the two runs are very diferent; they sample networks with rather diferent scores. In later stages of the simulation, however, the two runs are in roughly the same range of scores. This suggests that they might be exploring the same region. Another way of testing this is to compare the estimate we computed based on each of the two runs. As we see in ﬁgure 18.8c, 

![](images/063b02426e811a084873be9e76fcb6fbdf414d9ea9ffccd7674d24851b7f7405.jpg) 
Figure 18.9 MCMC structure search using 1,000 instances from ICU-Alarm network. The protocol is the same as in ﬁgure 18.8. (a) & (b) Plots two MCMC runs. (c) A comparison of the estimates of the posterior probability of edges. 

although the estimates are deﬁnitely not identical, they are mostly in agreement with each other. 

Variants of MCMC simulation have been applied successfully to this task for a variety of small domains, typically with 4–14 variables. However, there are several issues that potentially limit its efectiveness for large domains involving many variables. As we discussed, the space of network structures grows super exponentially with the number of variables. Therefore, the domain of the MCMC traversal is enormous for all but the tiniest domains. More importantly, the posterior distribution over structures is often quite peaked, with neighboring structures having very diferent scores. The reason is that even small perturbations to the structure — a removal of a single edge — can cause a huge reduction in score. Thus, the “posterior landscape” can be quite jagged, with high “peaks” separated by low “valleys.” In such situations, MCMC is known to be slow to mix, requiring many samples to reach the posterior distribution. 

To see this efect, consider ﬁgure 18.9, where we repeated the same experiment we performed before, but this time for a data set of 1000 instances. As we can see, although we considered a large number of iterations, diferent MCMC runs converge to quite diferent ranges of scores. This suggests that the diferent runs are sampling from diferent regions of the search space. Indeed, when we compare estimates in ﬁgure 18.9c, we see that some edges have estimated posterior of almost 1 in one run and of 0 in another. We conclude that each of the runs became stuck in a local “hill” of the search space and explored networks only in that region. 

# 18.5.3.2 MCMC Over Orders 

To avoid some of the problems with MCMC over structures, we consider an approach that utilizes collapsed particles , as described in section 12.4. Recall that a collapsed particle consists of two components, one an assignment to a set of random variables that we sampled, and the other a distribution over the remaining variables. 

In our case, we utilize the notion of collapsed particle as follows. Instead of working in the space of graphs $\mathcal{G}$ , we work in the space of pairs $\langle\prec,\mathcal{G}\rangle$ so that $\mathcal{G}$ is consistent with t ordering . As we have seen, we can use closed-form equations to deal the distribution over given an ordering $\prec$ . Thus, each ordering can represent a collapsed pa icle. 

We now construct a Markov chain over the state of all n ! orders. Our construction will guarantee that this chain has the stationary distribution $P(\prec|\ \mathcal{D})$ . We can then simulate this Markov chain, obtaining a sequence of samples $\prec_{1},\,\cdot\,\cdot\,\cdot\,,\,\prec_{T}$ . We can now approximate the expected value of any function $f$ as 

$$
P(f\mid\mathcal{D})\approx\frac{1}{T}\sum_{t=1}^{T}P(f\mid\mathcal{D},\prec_{t}),
$$ 

where we compute $P(f\mid\prec_{t},\mathcal{D})$ as described in section 18.5.2.2. 

Metropolis- Hastings 

It remains only to discuss the construction of the Markov chain. Again, we use a standard Metropolis-Hastings algorithm. For each order $\prec,$ , we deﬁne a proposal probability $\mathcal{T}^{Q}(\prec\rightarrow\prec^{\prime})$ , which deﬁnes the probability that the algorithm will “propose” a move from ≺ to ≺ . The algorithm then accepts this move with probability 

$$
\operatorname*{min}\bigg[1,\frac{P(\prec^{\prime},\mathcal{D})\mathcal{T}^{Q}(\prec^{\prime}\rightarrow\prec)}{P(\prec,\mathcal{D})\mathcal{T}^{Q}(\prec\rightarrow\prec^{\prime})}.\bigg]
$$ 

As we discussed in section 12.3, this sufces to ensure the detailed balance condition, and thus the resulting chain is reversible and has the desired stationary distribution. 

We can consider several speciﬁc constructions for the proposal distribution, based on diferent neighborhoods in the space of orders. In one very simple construction, we consider only operators that ﬂip two variables in the order (leaving all others unchanged): 

$$
\begin{array}{r}{(X_{i_{1}}\ldots X_{i_{j}}\ldots X_{i_{d}}\ldots X_{i_{n}})\mapsto(X_{i_{1}}\ldots X_{i_{d}}\ldots X_{i_{j}}\ldots X_{i_{n}}).}\end{array}
$$ 

Clearly, such operators allow to get from any ordering to another in relatively few moves. 

We note that, again, we can use decomposition to avoid repeated computation during the evaluation of candidate operators. Let $\prec$ be an order and let $\prec^{\prime}$ be the order obtained by ﬂipping $X_{i_{j}}$ and $X_{i_{k}}$ . Now, consider the terms in equation (18.14); those terms corresponding to variables $X_{i_{\ell}}$ in the order $\prec$ that precede $X_{i_{j}}$ or succeed $X_{i_{k}}$ do not change, since the set of potential parent sets $\mathcal{U}_{i_{l},\prec}$ is the same. 

Performing MCMC on the space of orderings is much more expensive than MCMC on the space of networks. Each proposed move requires performing summation over a fairly large set of possible parents for each variable. On the other hand, since each ordering corresponds to a large number of networks, a few moves in the space of orderings correspond to a much larger number of moves in the space of networks. 

Empirical results show that using MCMC over orders alleviate some of the problems we discussed with MCMC over network structures. For example, ﬁgure 18.10 shows two runs of this variant of MCMC for the same data set as in ﬁgure 18.9. Although these two runs involve much fewer iterations, they quickly converge to the same “area” of the search space and agree on the estimation of the posterior. This is an empirical indication that these are reasonably close to converging on the posterior. 

# 18.6 Learning Models with Additional Structure 

So far, our discussion of structure learning has deﬁned the task as one of ﬁnding the best graph structure $\mathcal{G}$ , where a graph structure is simply a speciﬁcation of the parents to each of the 

![](images/33bfc909ef9579bc1aec3c821d14151fce143af039dd885885f0ba5dad65244d.jpg) 
Figure 18.10 MCMC order search using 1,000 instances from ICU-Alarm network. The protocol is the same as in ﬁgure 18.8. (a) & (b) Plots two MCMC runs. (c) A comparison of the estimates of the posterior probability of edges. 

random variables in the network. However, some classes of models have additional forms of structure that we also need to identify. For example, when learning networks with structured CPDs, we may need to select the structure for the CPDs. And when learning template-based models, our model is not even a graph over $\mathcal{X}$ , but rather a set of dependencies over a set of template attributes. 

Although quite diferent, the approach in both of these settings is analogous to the one we used for learning a simple graph structure: we deﬁne a hypothesis space of potential models and a scoring function that allows us to evaluate diferent models. We then devise a search procedure that attempts to ﬁnd a high-scoring model. Even the choice of scoring functions is essentially the same: we generally use either a penalized likelihood (such as the BIC score) or a Bayesian score based on the marginal likelihood. Both of these scoring functions can be computed using the likelihood function for models with shared parameters that we developed in section 17.5. As we now discuss, the key diference in this new setting is the structure of the search space, and thereby of the search procedure. In particular, our new settings require that we make decisions in a space that is not the standard one of deciding on the set of parents for a variable in the network. 

# 18.6.1 Learning with Local Structure 

As we discussed, we can often get better performance in learning if we use more compact models of CPDs rather than ones that require a full table-based parameter iz ation. More compact models decrease the number of parameters and thereby reduce overﬁtting. Some of the techniques that we described earlier in this chapter are applicable to various forms of structured CPDs, including table-CPDs, noisy-or CPDs, and linear Gaussian CPDs (although the closed-form Bayesian score is not applicable to some of those representations). In this section, we discuss the problem of learning CPDs that explicitly encode local parameter sharing within a CPD, focusing on CPD trees as a prototypical (and much-studied) example. Here, we need to make decisions about the local structure of the CPDs as well as about the network structure. Thus, our search space is now much larger: it consists both of “global” decisions — the assignment of parents for each variable — and of “local” decisions — the structure of the CPD tree for each variable. 

# 18.6.1.1 Scoring Networks 

We ﬁrst consider how the development of the score of a network changes when we consider tree-CPDs instea f table-CPDs. Assume that we are given a work structure $\mathcal{G}$ ; moreover, for each variable $X_{i}$ , we are given a description of the CPD tree T $\mathcal{T}_{i}$ for $P(X_{i}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ . We view the choice of the trees as part of the speciﬁcation of the model. Thus, we consider the score of $\mathcal{G}$ with $\tau_{1},\dots,\tau_{n}$ 

$$
_{B}(\mathcal{G},\mathcal{T}_{1},\ldots,\mathcal{T}_{n}\ :\ \mathcal{D})=\log P(\mathcal{D}\mid\mathcal{G},\mathcal{T}_{1},\ldots,\mathcal{T}_{n})+\log P(\mathcal{G})+\sum_{i}\log P(\mathcal{T}_{i}\mid\mathcal{G},\mathcal{T}_{i},\ldots,\mathcal{T}_{n}).
$$ 

where $P(\mathcal{D}\mid\mathcal{G},\mathcal{T}_{1},\ldots,\mathcal{T}_{n})$ is the marginal likelihood when we integrate out the parameters in all the CPDs, and $P(\mathcal{T}_{i}\mid\mathcal{G})$ is the prior probability over the tree structure. 

structure prior 

We usually assume that the structure prior over trees does not depend on the graph, but only on the choice of parents. Possible priors include the uniform prior 

$$
P(\mathcal{T}_{i}\mid\mathcal{G})\propto1
$$ 

and a prior that penalizes larger trees 

$$
P(\mathcal{T}_{i}\mid\mathcal{G})\propto c^{|\mathcal{T}_{i}|}
$$ 

(for some constant $c<1$ ). 

We now turn to the marginal likelihood term. As in our previous discussion, we assume global and local parameter independence. This means that we have an independent prior over the parameters in each leaf of each tree. For each $X_{i}$ and each assignment $\mathrm{pa}_{X_{i}}$ to $X_{i}$ ’s parents, let $\lambda(\mathrm{{pa}}_{X_{i}})$ be the leaf in $\mathcal{T}_{i}$ to which $\mathrm{pa}_{X_{i}}$ is assigned. 

Using an argument that parallels our development of proposition 18.2, we have: 

Proposition 18.9 Let $\mathcal{G}$ be a network structure, $\tau_{1},\dots,\tau_{n}$ be CPD trees, and $P(\pmb\theta_{\mathcal{G}}\mid\mathcal{G},\mathcal{T}_{1},\dots,\mathcal{T}_{n})$ be a parameter prior satisfying global and local parameter independence. Then, 

$$
\begin{array}{r l}&{P(\mathcal{D}\mid\mathcal{G},\mathcal{T}_{1},\ldots,\mathcal{T}_{n})=}\\ &{\quad\prod_{i}\underset{\ell\in\mathrm{Leaves}(\mathcal{T}_{i})}{\prod}\int\underset{m:\lambda(\mathrm{pa}_{X_{i}}[m])=\ell}{\prod}P(X_{i}[m]\mid\ell,\pmb{\theta}_{X_{i}\mid\ell},\mathcal{G},\mathcal{T}_{i})P(\pmb{\theta}_{X_{i}\mid\ell}\mid\mathcal{G},\mathcal{T}_{i})d\mathcal{G},}\end{array}
$$ 

Thus, the score over a tree-CPD decomposes according to the structure of each of the trees. Each of the terms that corresponds to a leaf is the marginal likelihood of a single variable distribution and can be solved using standard techniques. Usually, we use Dirichlet priors at the leaves, and so the term for each leaf will be a term of the form of equation (18.9). 

Similar to the developments in table-CPDs, we can extend the notion of score decomposability to networks with tree-CPDs. Recall that score decomposability implies that identical substruc- tures receive the same score (regardless of structure of other parts of the network). Suppose we have two possible tree-CPDs for $X$ (not necessarily with the same set of parents in each), and suppose that there are two leaves $\ell_{1}$ and $\ell_{2}$ in the two trees such that $c_{\ell_{1}}=c_{\ell_{2}}$ ; that is, the same conditions on the parents of $X$ in each of the two CPDs lead to the respective leaves. Thus, the two leaves represent a similar situation (even though the tree structures can difer elsewhere), and intuitively they should receive the same score. 

To ensure this property, we need to extend the notion of parameter modularity: 

Deﬁnition 18.6 tree parameter modularity 

BDe prior 

Let $\{P(\pmb{\theta}_{\mathcal{G}}\ \mid\ \mathcal{G},T_{1},.\,.\,.\,,T_{n})\}$ be a set of parameter priors over networks with tree-CPDs that satisfy global and local parameter independence. The prior satisﬁes tree parameter modularity if for each $\mathcal{G},\mathcal{T}_{i}$ and $\mathcal{G}^{\prime},\mathcal{T}_{i}^{\prime}$ and $\ell\,\in\,\mathrm{Leaves}(\mathcal T_{i}),\;\ell^{\prime}\,\in\,\mathrm{Leaves}(\mathcal T_{i}^{\prime})$ such that ${\pmb{c}}_{\ell}\,=\,{\pmb{c}}_{\ell^{\prime}}$ , then $P(\pmb{\theta}_{X_{i}\mid\ell}\mid\mathcal{G},\mathcal{T}_{i})=P(\pmb{\theta}_{X_{i}\mid\ell^{\prime}}\mid\mathcal{G}^{\prime},\mathcal{T}_{i}^{\prime})$ . 

A natural extension of the BDe prior satisﬁes this condition. Suppose we choose a prior distribution $P^{\prime}$ and an equivalent sample size $\alpha$ ; then we choose hyperparameters for $P(\pmb\theta_{X_{i}|\ell}\ |$ $\mathcal{G},\mathcal{T}_{i})$ to be $\alpha_{x_{i}|\ell}=\alpha\cdot P^{\prime}(x_{i},\mathbf{\boldsymbol{c}}_{\ell})$ . Using this assumption, we can now decompose the Bayesian score for a tree-CPD; see exercise 18.17. 

# 18.6.1.2 Search with Local Structure 

Our next task is to describe how to search our space of possible hypotheses for one that has high score. Recall that we now have a much richer hypothesis space over which we need to search. 

The key question in the case of learning tree-CPDs is that of choosing a tree structure for $P(X\mid U)$ for some set of possible parents $U$ . (We will discuss diferent schemes for choosing $U$ .) There are two natural operators in this space. 

• Split – replace a leaf in the tree by an internal variable that leads to a leaf. This step increases the tree height by 1 on a selected branch.

 • Prune – replace an internal variable by a leaf. 

Starting from the “empty” tree (that consists of single leaf), we can reach any other tree by a sequence of split operations. 

The prune operations allow searches to retract some of the moves. This capability is critical, since there are many local maxima in growing trees. For example, it is often the case that a sequence of two splits leads to a high-scoring tree, but the intermediate step (after performing only the ﬁrst split) has low score. Thus, greedy hill-climbing search can get stuck in a local maximum early on. As a consequence, the search algorithm for tree-CPDs is often not a straightforward hill climbing. In particular, one common strategy is to choose the best-scoring split at each point, even if that split actually decreases the score. Once we have ﬁnished constructing the tree, we go back and evaluate earlier splitting decisions that we made. 

The search space over trees can explored using a “divide and conquer” approach. Once we decide to split our tree on one variable, say $Y$ , the choices we make in one subtree, say the one corresponding to $Y=y^{0}$ , are independent of the choices we make in other subtrees. Thus, we can explore for the best structure for each one of the subsets independently of the other. Thus, we can devise recursive search procedures for trees, which works roughly as follows. The procedure receives a set of instances to learn from, a target variable $X$ , and a set $U$ of possible parents. It evaluates each $Y\in U$ as a potential question by scoring the tree-CPD that has $Y$ as a root and has no further question. This myopic evaluation can miss pairs or longer sequences of questions that lead to high accuracy predictions and hence to a high scoring model. However, it can be performed very efciently (see exercise 18.17), and so it is often used, under the hope that other approaches (such as random restarts or tabu search) can help us deal with local optima. 

After choosing the root, the procedure divides the data set into smaller data sets, each one corresponding to one value of the variable $Y$ . Using the decomposition property we just discussed, the procedure is recursively invoked to learn a subtree in each $D_{y}$ . These subtrees are put together into a tree-CPD that has $Y$ as a root. The ﬁnal test compares the score of the constructed subtree to that of the empty subtree. This test is necessary, since our construction algorithm selected the best split at this point, but without checking that this split actually improves the score. This strategy helps avoid local maxima arising from the myopic nature of the search, but it does potentially lead to unnecessary splits that actually hurt our score. This ﬁnal check helps avoid those. In efect, this procedure evaluates, as it climbs back up the tree in the recursion, all of the possible merge steps relative to our constructed tree. 

There are two standard strategies for applying this procedure. One is an encapsulated search . Here, we perform one of the network-structure search procedures we described in earlier sections of this chapter. Whenever we perform a search operation (such as adding or removing an edge), we use a local procedure to ﬁnd a representation for the newly created CPD. The second option is to use a uniﬁed search , where we do not distinguish between operators that modify the network and operators that modify the local structure. Instead, we simply apply a local search that modiﬁes the joint representation of the network and local structure for each CPD. Here, each state in the search space consists of a collection of $n$ trees $\langle\mathcal{T}_{1},.\,.\,.\,,\mathcal{T}_{n}\rangle$ , which deﬁne both the structure and the parameters of the network. We can structure the operations in the search in many ways: we can update an entire CPD tree for one of the variables, or we can evaluate the delta-score of the various split and merge operations in tree-CPDs all across the network and choose the best one. In either case, we must remember that not every collection of trees deﬁnes an acyclic network structure, and so we must construct our search carefully to ensure acyclic structures, as well as any other constraints that we want to enforce (such as bounded indegree). 

Each of these two options for learning with local structure has its beneﬁts and drawbacks. Encapsulated search spaces decouple the problem of network structure learning from that of learning CPD structures. This modularity allows us to “plug in” any CPD structure learning procedure within generic search procedure for network learning. Moreover, since we consider the structure of each CPD as a separate problem, we can exploit additional structure in the CPD representation. A shortcoming of encapsulated search spaces is that they can easily cause us to repeat a lot of efort. In particular, we redo the local structure search for $X$ ’s CPD every time we consider a new parent for $X$ . This new parent is often irrelevant, and so we end up discarding the local structure we just learned. Uniﬁed search spaces alleviate this problem. In this search formulation, adding a new parent $Y$ to $X$ is coupled with a proposal as to the speciﬁc position in the tree-CPD of $X$ where $Y$ is used. This ﬂexibility comes at a cost: the number of possible operators at a state in the search is very large — we can add a split on any variable at each leaf of the $n$ trees. Moreover, key operations such as edge reversal or even edge deletion can require many steps in the ﬁner-grained search space, and therefore they are susceptible to forming local optima. Overall, there is no clear winner between these two methods, and the best choice is likely to be application-speciﬁc. 

# 18.6.2 Learning Template Models 

We now brieﬂy discuss the task of structure learning for template-based models. Here, once we deﬁne the hypothesis space, the solution becomes straightforward. Importantly, when learn- ing template-based models, our hypothesis space is the space of structures in the template representation, not in the ground network from which we are learning. 

For DBNs, we are l ing a representation as in deﬁnition 6.4: a time 0 network ${\mathcal B}_{0}$ , and a transition network B $\mathcal{B}_{\rightarrow}$ → . Importantly, the latter network is speciﬁed in terms of template attributes; thus, the learned structure will be used for all time slices in the unrolled DBN. The learned structure must satisfy the constraints on our template-based representation. For example, $\mathcal{B}_{\rightarrow}$ must be a valid conditional Bayesian network r $\mathcal{X}^{\prime}$ given $\mathcal{X}$ : it must be acyclic, and there must be no incoming edges into any variables in . 

For object-relational models, each hypothesis in our space speciﬁes an assignment $\mathrm{Pa}_{A}$ for every $A\,\in\,\aleph$ . H re also, the learned structure is at the template level and is applied to all instantiations of A in the ground network. The learned structure must satisfy the constraints of the template-based language with which we are dealing. For example, in a plate model, the structure must satisfy the constraints of deﬁnition 6.9, for example, the fact that for any template parent $B_{i}(U_{i})\in\mathrm{Pa}_{A}$ , ve that the parent’s arguments $U_{i}$ must be a subset of the attribute’s argument signature $\alpha(A)$ . 

Importantly, in both plate models and DBNs, the set of possible structures is ﬁnite. This fact is obvious for DBNs. For plate models, note that the set of template attributes is ﬁnite; the possible argument signatures of the parents is also ﬁnite, owing to the constraints that $U_{i}\,\subseteq\,\alpha(A)$ . However, this constraint does not necessarily hold for richer languages. We will return to this point. 

Given a hypothesis space, we need to determine a scoring function and a search algorithm. Based on our analysis in section 17.5.1.2 and section 17.5.3, we can easily generalize any of our standard scoring functions (say the BIC score or the marginal likelihood) to the case of template-based representations. The analysis in section 17.5.1.2 provides us with a formula for a decomposed likelihood function, with terms corresponding to each of the model parameters at the template level. From this formula, the derivation of the BIC score is immediate. We use the decomposed likelihood function and penalize with the number of parameters in the template model . For a Bayesian score, we can follow the lines of section 17.5.3 and assume global parameter independence at the template level. The posterior now decomposes in the same way as the likelihood, giving us, once again, a decomposable score. 

With a decomposable score, we can now search over the set of legal structures in our hypothesis space, as deﬁned earlier. In this case, the operators that we typically apply are the natural variants of those used in standard structure search: edge addition, deletion, and (possibly) reversal. The only slight subtlety is that we must check, at each step, that the model resulting from the operator satisﬁes the constraints of our template-based representation. In particular, in many cases for both DBNs and PRMs, edge reversal will lead to an illegal structure. For example, in DBNs, we cannot reverse an edge $A\rightarrow A^{\prime}$ . In plate models, we cannot reverse $B(U)~\to~A(U,U^{\prime})$ . Indeed, the notion of edge reversal only makes sense when $\alpha(A)=\alpha(B)$ . Thus, a more natural search space for plate models (and other object-relational template models) is one that simply searches for a parent set for each $A\in\aleph,$ , using operators such as parent addition and deletion. 

We conclude this discussion by commenting brieﬂy on the problem of structure learning for more expressive object-relational languages. Here, the complexity of our hypothesis space can be signiﬁcantly greater. Consider, for example, a PRM. Most obviously, we see that the speciﬁcation here involves not only possible parents but also possibly a guard and an aggregation function. Thus, our model speciﬁcation requires many more components. Subtler, but also much more important, is the fact that the space of possible structures is potentially inﬁnite. The key issue here is that the parent signature $\alpha(\operatorname{Pa}_{A})$ can be a superset of $\alpha(A)$ , and therefore the set of possible parents we can deﬁne is unboundedly large. 

Returning to the Genetics domain, we can, if we choose, allow the genotype of a person depending on the genotype of his or her mother, or of his or her grandmother, or of his or her paternal uncles, or on any type of relative arbitrarily far away in the family tree (as long as acyclicity is maintained). For example, the dependence on paternal uncle (not by marriage) can be written as a dependence of Genotype $\cdot(U)$ on Genotype $\left(U^{\prime}\right)$ with the guard 

$$
F a t h e r(V,U)\wedge B r o t h e r(U^{\prime},V),
$$ 

assuming Brother has already been deﬁned. 

In general, by increasing the number of logical variables in the attribute’s parent argument signature, we can introduce dependencies over objects that are arbitrarily far away. Thus, when learning PRMs, we generally want to introduce a structure prior that penalizes models that are more complex, for example, as an exponential penalty on the number of logical variables in $\alpha(\mathrm{Pa}_{A})-\alpha(A)$ , or on the number of clauses in the guard. 

In summary, the key aspect to learning structure of template-based models is that both the structure and the parameters are speciﬁed at the template level, and therefore that is the space over which our structure search is conducted. It is this property that allows us to learn a model from one skeleton (for example, one pedigree, or one university) and apply that same model to a very diferent skeleton. 

# 18.7 Summary and Discussion 

In this chapter, we considered the problem of structure learning from data. As we have seen, there are two main issues that we need to deal with: the statistical principles that guide the choice between network structures, and the computational problem of applying these principles. 

The statistical problem is easy to state. Not all dependencies we can see in the data are real. Some of them are artifacts of the ﬁnite sample we have at our disposal. Thus, to learn (that is, generalize to new examples), we must apply caution in deciding which dependencies to model in the learned network. 

We discussed two approaches to address this problem. The ﬁrst is the constraint-based approach. This approach performs statistical tests of independence to collect a set of depen- dencies that are strongly supported by the data. Then it searches for the network structure that “explains” these dependencies and no other dependencies. The second is the score-based approach. This approach scores whole network structures against the data and searches for a network structure that maximize the score. 

What is the diference between these two approaches? Although at the outset they seem quite diferent, there are some similarities. In particular, if we consider a choice between the two 

possible networks over two variables, then both approaches use a similar decision rule to make the choice; see exercise 18.27. 

When we consider more than two variables, the comparison is less direct. At some level, we  can view the score-based approach as performing a test that is similar to a hypothesis test. However, instead of testing each pair of variables locally, it evaluates a function that is somewhat like testing the complete network structure against the null hypothesis of the empty network. Thus, the score-based approach takes a more global perspective, which allows it to trade of approximations in diferent part of the network. 

The second issue we considered was the computational issue. Here there are clear diferences. In the constraint-based approach, once we collect the independence tests, the construction of the network is an efcient (low-order polynomial) procedure. On the other hand, we saw that the optimization problem in the score-based approach is NP-hard. Thus, we discussed various approaches for heuristic search. 

When discussing this computation issue, one has to remember how to interpret the theoretical results. In particular, the NP-hardness of score-based optimization does not mean that the problem is hopeless. When we have a lot of data, the problem actually becomes easier , since one structure stands out from the rest. In fact, recent results indicates that there might be search procedures that when applied to sufciently large data sets are guaranteed to reach the global optimum. This suggests that the hard cases might be the ones where the diferences between the maximal scoring network and that other local maxima might not be that dramatic. This is a rough intuition, and it is an open problem to characterize formally the trade-of between quality of solution and hardness of the score-based learning problem. 

Another open direction of research attempts to combine the best of both worlds. Can we use the efcient procedures developed for constraint-based learning to ﬁnd high-scoring network structure? The high-level motivation that the Build-PDAG we discussed uses knowledge about Bayesian networks to direct its actions. On the other hand, the search procedures we discussed so far are fairly uninformed about the problem. A simpleminded combination of these two approaches uses a constraint-based method to ﬁnd starting point for the heuristic search. More elaborate strategies attempt to use the insight from constraint-based learning to reformulate the search space — for example, to avoid exploring structures that are clearly not going to score well, or to consider global operators. 

Another issue that we touched on is estimating the conﬁdence in the structures we learned. We discussed MCMC approaches for answering questions about the posterior. This gives us a measure of our conﬁdence in the structures we learned. In particular, we can see whether a part of the learned network is “crucial” in the sense that it has high posterior probability, or closer to arbitrary when it has low posterior probability. Such an evaluation, however, compares structures only within the class of models we are willing to learn. It is possible that the data do not match any of these structures. In such situations, the posterior may not be informative about the problem. The statistical literature addresses such questions under the name of goodness of ﬁt tests, which we brieﬂy described in box 16.A. These tests attempt to evaluate whether a given model would have data such as the one we observed. This topic is still underdeveloped for models such as Bayesian networks. 

# 18.8 Relevant Literature 

We begin by noting that many of the works on learning Bayesian networks involve both param- eter estimation and structure learning; hence most of the references discussed in section 17.8 are still relevant to the discussion in this chapter. 

The constraint-based approaches to learning Bayesian networks were already discussed in chapter 3, under the guise of algorithms for constructing a perfect map for a given distribution. Section 3.6 provides the references relevant to that work; some of the key algorithms of this type include those of Pearl and Verma (1991); Verma and Pearl (1992); Spirtes, Glymour, and Scheines (1993); Meek (1995a); Cheng, Greiner, Kelly, Bell, and Liu (2002). The application of these methods to the task of learning from an empirical distribution requires the use of statistical independence tests (see, for example, Lehmann and Romano (2008)). However, little work has been devoted to analyzing the performance of these algorithms in that setting, when some of the tests may fail. 

Much work has been done on the development and analysis of diferent scoring functions for probabilistic models, including the BIC/MDL score (Schwarz 1978; Rissanen 1987; Barron et al. 1998) and the Bayesian score (Dawid 1984; Kass and Raftery 1995), as well as other scores, such as the AIC (Akaike 1974). These papers also establish the basic properties of these scores, such as the consistency of the BIC/MDL and Bayesian scores. 

The Bayesian score for discrete Bayesian networks, using a Dirichlet prior, was ﬁrst proposed by Buntine (1991) and Cooper and Herskovits (1992) and subsequently generalized by Spiegelhalter, Dawid, Lauritzen, and Cowell (1993); Heckerman, Geiger, and Chickering (1995). In particular, Heckerman et al. propose the BDe score, and they show that the BDe prior is the only one satisfying certain natural assumptions, including global and local parameter independence and score-equivalence. Geiger and Heckerman (1994) perform a similar analysis for Gaussian networks, resulting in a formal justiﬁcation for a Normal-Wishart prior that they call the BGe prior . The application of MDL principles to Bayesian network learning was developed in parallel (Bouckaert 1993; Lam and Bacchus 1993; Suzuki 1993). These papers also deﬁned the relationship between the maximum likelihood score and information theoretic scores. These connections in a more general setting were explored in early works in information theory Kullback (1959), as well as in early work on decomposable models Chow and Liu (1968). 

Buntine (1991) ﬁrst explored the use of nonuniform priors over Bayesian network structures, utilizing a prior over a ﬁxed node ordering, where all edges were included independently. Heckerman, Mamdani, and Wellman (1995) suggest an alternative approach that uses the extent of deviation between a candidate structure and a “prior network.” 

Perhaps the earliest application of structure search for learning in Bayesian networks was the work of Chow and Liu (1968) on learning tree-structured networks. The ﬁrst practical algorithm for learning general Bayesian network structure was proposed by Cooper and Herskovits (1992). Their algorithm, known as the K2 algorithm , was limited to the case where an ordering on the variables is given, allowing families for diferent variables to be selected independently. The approach of using local search over the space of general network structures was proposed and studied in depth by Chickering, Geiger, and Heckerman (1995) (see also Heckerman et al. 1995), although initial ideas along those lines were outlined by Buntine (1991). Chickering et al. compare diferent search algorithms, including K2 (with diferent orderings), local search, and simulated annealing. Their results suggest that unless a good ordering is known, local search ofers the best time-accuracy trade-of. Tabu search is discussed by Glover and Laguna (1993). Several works considered the combinatorial problem of searching over all network structures (Singh and Moore 2005; Silander and Myllymaki 2006) based on ideas of Koivisto and Sood (2004). 

Another line of research proposes local search over a diferent space, or using diferent operators. Best known in this category are algorithms, such as the GES algorithm, that search over the space of I-equivalence classes. The foundations for this type of search were developed by Chickering (1996b, 2002a). Chickering shows that GES is guaranteed to learn the optimal Bayesian network structure at the large sample limit, if the data is sampled from a graphical model (directed or undirected). Other algorithms that guarantee identiﬁcation of the correct structure at the large-sample limit include the constraint-based SGS method of Spirtes, Glymour, and Scheines (1993) and the KES algorithm of Nielsen, Koˇ cka, and Peña (2003). 

Other search methods that use alternative search operators or search spaces include the optimal reinsertion algorithm of Moore and Wong (2003), which takes a variable and moves it to a new position in the network, and the ordering-based search of Teyssier and Koller (2005), which searches over the space of orderings, selecting, for each ordering the optimal (bounded- indegree) network consistent with it. Both of these methods take much larger steps in the space than search over the space of network structures; although each step is also more expensive, empirical results show that these algorithms are nevertheless faster. More importantly, they are signiﬁcantly less susceptible to local optima. A very diferent approach to avoiding local optima is taken by the data perturbation method of Elidan et al. (2002), in which the sufcient statistics are perturbed to move the algorithm out of local optima. 

Much work has been done on efcient caching of sufcient statistics for machine learning tasks in general, and for Bayesian networks in particular. Moore and Lee (1997); Komarek and Moore (2000) present AD-trees and show their efcacy for learning Bayesian networks in high dimension. Deng and Moore (1989); Moore (2000); Indyk (2004) present some data structures for continuous spaces. 

Several papers also study the theoretical properties of the Bayesian network structure learning task. Some of these papers involve the computational feasibility of the task. In particular, Chickering (1996a) showed that the problem of ﬁnding the network structure with indegree $\leq~d$ that optimizes the Bayesian score for a given data set is $\mathcal{N P}$ -hard, for any $d\ge2$ . NP -hardness is also shown for ﬁnding the maximum likelihood structures within the class of polytree networks (Dasgupta 1999) and path-structured networks (Meek 2001). Chickering et al. (2003) show that the problem of ﬁnding the optimal structure is also $\mathcal{N P}$ -hard at the large-sample limit, even when: the generating distribution is perfect with respect to some DAG containing hidden variables, we are given an independence oracle, we are given an inference oracle, and we restrict potential solutions to structures in which each node has at most $d$ parents (for any $d\geq3.$ ). Importantly, all of these $\mathcal{N P}$ -hardness results hold only in the inconsistent case, that is, where the generating distribution is not perfect for some DAG. In the case where the generating distribution is perfect for some DAG over the observed variables, the problem is signiﬁcantly easier. As we discussed, several algorithms can be guaranteed to identify the correct structure. In fact, the constraint-based algorithms of Spirtes et al. (1993); Cheng et al. (2002) can be shown to have polynomial-time performance if we assume a bounded indegree (in both the generating and the learned network), providing a sharp contrast to the $\mathcal{N P}$ -hardness result in the inconsistent setting. 

Little work has been done on analyzing the PAC-learnability of Bayesian network structures, that is, their learnability as a function of the number of samples. A notable exception is the work of Höfgen (1993), who analyzes the problem of PAC-learning the structure of Bayesian networks with bounded indegree. He focuses on the maximum likelihood network subject to the indegree constraints and shows that this network has, with high probability, low KL-divergence to the true distribution, if we learn with a number of samples $M$ that grows logarithmically with the number of variables in the network. Friedman and Yakhini (1996) extend this analysis for search with penalized likelihood — for example, MDL/BIC scores. We note that, currently, no efcient algorithm is known for ﬁnding the maximum-likelihood Bayesian network of bounded indegree, although the work of Abbeel, Koller, and $\mathrm{Mg}$ (2006) does provide a polynomial-time algorithm for learning a low-degree factor graph under these assumptions. 

Bayesian model averaging has been used in the context of density estimation, as a way of gaining more robust predictions over those obtained from a single model. However, most often it is used in the context of knowledge discovery, to obtain a measure of conﬁdence in predictions relating to structural properties. In a limited set of cases, the space of legal networks is small enough to allow a full enumeration of the set of possible structures (Heckerman et al. 1999). Buntine (1991) ﬁrst observed that in the case of a ﬁxed ordering, the exponentially large summation over model structures can be reformulated compactly. Meila and Jaakkola (2000) show that one can efciently infer and manipulate the full Bayesian posterior over all the super exponentially many tree-structured networks. Koivisto and Sood (2004) suggest an exact method for summing over all network structure. Although this method is exponential in the number of variables, it can deal with domains of reasonable size. 

Other approaches attempt to approximate the super exponentially large summation by con- sidering only a subset of possible structures. Madigan and Raftery (1994) propose a heuristic approximation, but most authors use an MCMC approach over the space of Bayesian network structure (Madigan and York 1995; Madigan et al. 1996; Giudici and Green 1999). Friedman and Koller (2003) propose the use of MCMC over orderings, and show that it achieves much faster mixing and therefore more robust estimates than MCMC over network space. Ellis and Wong (2008) improve on this algorithm, both in removing bias in its prior distribution and in improving its computational efciency. 

The idea of using local learning of tree-structured CPDs for learning global network structure was proposed by Friedman and Goldszmidt (1996, 1998), who also observed that reducing the number of parameters in the CPD can help improve the global structure reconstruction. Chick- ering et al. (1997) extended these ideas to CPDs structured as a decision graph (a more compact generalization of a decision tree). Structure learning in dynamic Bayesian networks was ﬁrst proposed by Friedman, Murphy, and Russell (1998). Structure learning in object-relational mod- els was ﬁrst proposed by Friedman, Getoor, Koller, and Pfefer (1999); Getoor, Friedman, Koller, and Taskar (2002). Segal et al. (2005) present the module network framework, which combines clustering with structure learning. 

Learned Bayesian networks have also been used for speciﬁc prediction and classiﬁcation tasks. Friedman et al. (1997) deﬁne a tree-augmented naive Bayes structure, which extends the tradi- tional naive Bayes classiﬁer by allowing a tree-structure over the features. They demonstrate that this enriched model provides signiﬁcant improvements in classiﬁcation accuracy. Depen- dency networks were introduced by Heckerman et al. (2000) and applied to a variety of settings, including collaborative ﬁltering. This latter application extended the earlier work of Breese et al. (1998), which demonstrated the success of Bayesian network learning (with tree-structured CPDs) to this task. 

# 18.9 Exercises 

# Exercise 18.1 

Show that the $\chi^{2}(\mathcal{D})$ statistic of equation (18.1) is approximately twic of $d_{I}(\mathcal{D})$ of equation (18.2). Hint: examine the ﬁrst-order Taylor expansion of $\begin{array}{r}{\frac{z}{t}\approx1+\frac{z-t}{t}}\end{array}$ ≈ in $z$ around t . 

# Exercise 18.2 

Derive equation (18.3). Show that this value is the sum of the probability of all possible data sets that have the given empirical counts. 

# Exercise $18.3\star$ 

multiple hypothesis testing Suppose we are testing multiple hypotheses $1,\cdot\cdot\cdot,N$ for a large value. Each hypothesis has an observed deviance measure $D_{i}$ , and we computed the associated $\mathrm{p}$ -value $p_{i}$ . Recall that under the null hypothesis, $p_{i}$ has a uniform distribution between 0 and 1 . Thus, $\Bar{P(p_{i}<t\mid H_{0})}=t$ for every $t\in[0,1]$ . 

We are worried that one of our tests received a small $\mathrm{p}$ -value by chance. Thus, we want to consider the distribution of the best p-value out of all of our tests under the assumption that the null hypothesis is true in all of the tests. More formally, we want to examine the behavior of $\mathrm{min}_{i}\,p_{i}$ under $H_{0}$ . 

a. Show that 

$$
P(\operatorname*{min}_{i}p_{i}<t\mid H_{0})\leq t\cdot N.
$$ 

(Hint: Do not assume that the variables $p_{i}$ are independent of each other.) 

b. Suppose we want to ensure that the probability of a random rejection is below 0 . 05 , what p-value should we use in individual hypothesis tests? c. Suppose we assume that the tests (that is, the variables $p_{i.}$ ) are independent of each other. Derive the bound in this case. Does this bound give better (higher) decision $\mathrm{p}$ -value when we use $N=100$ and global rejection rate below 0 . 05 ? How about $N=1,000?$ ? 

Bonferroni correction The bound you derive in [b] is called Bonferroni bound , or more often Bonferroni correction for a multiple- hypothesis testing scenario. 

# Exercise $18.4\star$ 

Consider again the Build-PDAG procedure of algorithm 3.5, but now assume that we apply it in a setting where the independence tests might return incorrect answers owing to limited and noisy data. 

a. Provide an example where Build-PDAG can fail to reconstruct the true underlying graph $\mathcal{G}^{*}$ even in the presence of a single incorrect answer to an independence question. b. Now, assume that the algorithm constructs the correct skeleton but can encounter a single incorrect answer when extracting the immoralities. 

# Exercise 18.5 

Prove corollary 18.1. Hint: Start with the result of proposition 18.1, and use the chain rule of entropies and the chain rule mutual information. 

# Exercise 18.6 

Show that adding edges to a network increases the likelihood. 

# Exercise $18.7\star$ 

Stirling’s approximation Prove theorem 18.1. Hint: You can use Stirling’s approximation for the Gamma function 

$$
\Gamma(x)\approx\sqrt{2\pi}\,x^{x-\frac12}e^{-x}
$$ 

or 

$$
\log\Gamma(x)\approx{\frac{1}{2}}\log(2\pi)x\log(x)-{\frac{1}{2}}\log(x)-x
$$ 

# Exercise 18.8 

$\mathcal{G}$ is I-equivalent to ${\mathcal{G}}^{\prime}$ , then if we use table-CPDs, we have that $\begin{array}{r l}{\mathrm{score}_{L}(\mathcal{G}}&{{}:\quad\mathcal{D})\ =}\end{array}$ D $\mathrm{score}_{L}(\mathcal{G}^{\prime}\ :\ \mathcal{D})$ G D for any choice of D . 

Hint: Consider the set of distributions that can be represented by parameter iz ation each network structure. 

# Exercise 18.9 

$\mathcal{G}$ -equivalent to ${\mathcal{G}}^{\prime}$ , hen if we use table-CPDs, we have that $\begin{array}{r l r}{\mathrm{score}_{B I C}(\mathcal{G}}&{{};}&{\mathcal{D})\,=}\end{array}$ D $\mathrm{score}_{B I C}(\mathcal G^{\prime}~:~\mathcal D)$ G D for any choice of D . You can use the results of exercise 3.18 and exercise 18.8 in your proof. 

# Exercise 18.10 

Show that the Bayesian score with a K2 prior in which we have a Dirichlet prior Dirichlet $\left(1,1,\ldots,1\right)$ for each set of multinomial parameters is not score-equivalent. 

Hint: C a data set for which the score of the network $X\,\rightarrow\,Y$ difers from the score of the network $X\leftarrow Y$ . 

# Exercise $18.11\star$ 

We now examine how to prove score equivalence for the BDe score. Assume that we have a prior speciﬁed by an equivalent sample size $\alpha$ and prior distribution $P^{\prime}$ . Prove the following: 

a. Co etworks over the variables $X$ and $Y$ . Show that the BDe score of $X\rightarrow Y$ is equal to that of X $X\leftarrow Y$ ← . b. Show that if $\mathcal{G}$ and ${\mathcal{G}}^{\prime}$ are identical except for a covered edge reversal of $X\rightarrow Y$ , then the BDe score of both networks is equal. c. Show that the proof of score equivalence follows from the last result and theorem 3.9. 

# Exercise ${\bf18.12\star}$ 

In section 18.3.2, we have seen that the Bayesian score can be posed a sequential procedure that estimates the performance on new unseen examples. In this example, we consider another score that is based on this motivation. 

Recall that leave-one-out cross-validation (LOOCV), described in box 16.A, is a procedure for estimating the performance of a learning method on new samples. In our context, this deﬁnes the following score: 

$$
L O O C V(\mathcal{D}\mid\mathcal{G})=\prod_{m=1}^{M}P(\xi[m]\mid\mathcal{G},\mathcal{D}_{-m}),
$$ 

where $\mathcal{D}_{-m}$ is the data with the $m$ ’th instance removed. 

a. Consider the setting of section 18.3.3 where we observe a series of values of a binary variable. Develop a closed-form equation for $L O O C V({\mathcal{D}}\mid{\mathcal{G}})$ as a function of the number of heads and the number of tails. b. w co der the network ${\mathcal{G}}_{X\to Y}$ and a data set $\mathcal{D}$ ervations of two binary variables $X$ and Y . evelo a c ed-form equation for $L O O C V(\mathcal{D}\ |\ \mathcal{G}_{X\rightarrow Y})$ D | G as a function of the sufcient → statistics of X and Y in D . c. Based on these two examples, what are the properties of the LOOCV score? Is it decomposable? 

# Exercise 18.13 

Consider the algorithm for learning tree-structured networks in section 18.4.1. Show that the weight $w_{i\rightarrow j}=w_{j\rightarrow i}$ if the score satisﬁes score equivalence. 

# Exercise $18.14\star\star$ 

We now consider a situation where we can ﬁnd the high-scoring structure in a polynomial time. Suppose we are given a directed gr $\mathcal{C}$ hat mposes constr nts on the possible parent-child relati ships in the n edge $\bar{X}{-}Y$ in C implies that X might be considered as a parent of Y . e deﬁne $\pmb{\mathscr{G}}_{\mathcal{C}}=\{\mathcal{G}:\mathcal{G}\subseteq\mathcal{C}\}$ {G G ⊆C} to be the set of graphs that are consistent with the constraints imposed by C . 

Describe an gorithm that, given a decomposable score score and a data set $\mathcal{D}$ , ﬁnds t maximal scoring network in $\pmb{\mathscr{G}}_{\mathcal{C}}$ . Show that your algorithm is exponential in the tree-width of the graph C . 

# Exercise $18.15\star$ 

Consider the problem of learning a Bayesian network structure over two random variables $X$ and $Y$ . 

a. Show a data set — an empirical distribution and a number of samples $M\mathrm{~-~}$ where the optimal network structure according to the BIC scoring function is diferent from the optimal network structure according to the ML scoring function. b. Assume that we continue to get more samples that exhibit precisely the same empirical distribution. (For simplicity, we restrict attention to values of $M$ that allow that empirical distribution to be achieved; for example, an empirical distribution of 50 percent heads and 50 percent tails can be achieved only for an even number of samples.) At what value of $M$ will the network that optimizes the BIC score be the same as the network that optimizes the likelihood score? 

# Exercise 18.16 

This problem considers the performance of various types of structure search algorithms. Suppose we have a general network structure search algorithm, $\pmb{A}$ , that takes a set of basic operators on network structures as a parameter. This set of operators deﬁnes the search space for $\pmb{A}$ , since it deﬁnes the candidate network structures that are the “immediate successors” of any current candidate network structure—that is, the successor states of any state reached in the search. Thus, for example, if the set of operators is {add an edge not currently in the network}, then the successor ates of any candidate network $\mathcal{G}$ is the set of structures obtained by adding a single edge anywhere in (so long as acyclicity is maintained). 

Given a set of operators, $\pmb{A}$ does a simple greedy search over the set of network structures, starting from the empty network (no edges), using the BIC scoring function. Now, consider two sets of operators we can use in $\pmb{A}$ . Let $\mathbf{\cal{A}}_{[a d d]}$ be $\pmb{A}$ using the set of operations {add an edge not currently in the network}, and let $A_{[a d d,d e l e t e]}$ be $\pmb{A}$ using the set of operations {add an edge not currently in the network, delete an edge currently in the network}. 

a. Show a distribution where, regardless of the amount of data in our training set (that is, even with inﬁnitely many samples), the answer produced by $\mathbf{\cal{A}}_{[a d d]}$ is worse (that is, has a lower BIC score) than the answer produced by $A_{[a d d,d e l e t e]}\bar{]}$ . (It is easiest to represent your true distribution in the form of a Bayesian network; that is, a network from which the sample data are generated.) b. Show a distribution where, regardless of the amount of data in our training set, $A_{[a d d,d e l e t e]}$ will converge to a local maximum. In other words, the answer returned by the algorithm has a lower score than the optimal (highest-scoring) network. What can we conclude about the ability of our algorithm to ﬁnd the optimal structure? 

# Exercise $18.17\star$ 

This problem considers the problem of learning a CPD tree structure for a variable in a Bayesian network, using the Bayesian score. Assume that the network structure $\mathcal{G}$ includes a description of the CPD trees in it; that is, for each variable $X_{i}$ , we have a CPD tree $\mathcal{T}_{i}$ for $P(X_{i}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ ) . We view the choice of the trees as part of the speciﬁcation of the model. Thus, we consider the score of $\mathcal{G}$ with $\tau_{1},\dots,\tau_{n}$ 

$$
\mathrm{score}_{B}(\mathcal{G},\mathcal{T}_{1},\dots,\mathcal{T}_{n}\ :\ \mathcal{D})=\log P(\mathcal{D}\mid\mathcal{G},\mathcal{T}_{1},\dots,\mathcal{T}_{n})+\log P(\mathcal{G})+\sum_{i}\log P(\mathcal{T}_{i}\mid\mathcal{G})
$$ 

Here, we assume for simplicity that the two structure priors are uniform, so that we focus on the marginal likelihoo erm $P(\mathcal{D}\mid\mathcal{\bar{G}},\mathcal{T}_{1},.\,.\,.\,,\mathcal{T}_{n})$ . Assume elected a ﬁxed choice of parents $U_{i}$ for each variable $X_{i}$ . We would like to ﬁnd a set of trees T $\tau_{1},\dots,\tau_{n}$ T that together maximizes the Bayesian score. 

a. Show how you can decompose the Bayesian score in this case as a sum of simpler terms; make sure you state the assumptions necessary to allow this decomposition. b. Assume that we consider only a single type of operator in our search, a $s p l i t(X_{i},\ell,Y)$ operator, where $\ell$ is a leaf in the current CPD tree for $X_{i}$ , and $Y\,\in\,U_{i}$ is a possib parent of $X_{i}$ . This operator replaces the leaf ℓ by an internal variable that splits on the values of Y . Derive a simple formula for the delta-score $\delta(\mathcal{T}\,\colon\,o)$ of such an operator $o\stackrel{\cdot}{=}s p l i t(X_{i},\ell,Y)$ . (Hint: Use the representation of the decomposed score to simplify the formula.) c. Suppose our greedy search keeps track of the delta-score for all the operators. After we take a step in the search space by applying operator $o\,=\,s p l i t(X,\ell,Y)$ , how should we update the delta score for another operator $o^{\check{\prime}}=\dot{s p l i t(X^{\prime},\ell^{\prime},Y^{\prime})}\mathfrak{z}$ (Hint: Use the representation of the delta-score in terms of decomposed score in the previous question.) d. Now, consider applying the same process using the likelihood score rather than the Bayesian score. What will the resulting CPD trees look like in the general case? You can make any assumption you want about the behavior of the algorithm in case of ties in the score. 

# Exercise 18.18 

Prove proposition 18.5. 

# Exercise 18.19 

Prove proposition 18.6. 

# Exercise ${\bf18.20\star}$ 

Recall that the $\Theta(f(n))$ denotes both an asymptotic lower bound and an asymptotic upper bound (up to a constant factor). 

a. Show that the number of DAGs with $n$ vertices is $2^{\Theta(n^{2})}$ . 

b. Show that the number of DAGs with $n$ vertices and indegree bounded by $d$ is $2^{\Theta(d n\log n)}$ . c. Show that the number of DAGs with $n$ vertices and indegree bounded by $d$ that are consistent with a given order is $2^{\Theta(d n\log n)}$ . 

# Exercise 18.21 

Consider the problem of learning the structur of a 2-TBN over $\mathcal{X}\,=\,\{X_{1},.\,.\,.\,,X_{n}\}$ . Assume that we are learning a model with bounded indegree k . Explain, using the argument of asymptotic complexity, why the problem of learning the 2-TBN structure is considerably easier if we assume that there are no intra-time-slice edges in the 2-TBN. 

# Exercise ${\bf18.22\star}$ 

module network In this problem, we will consider the task of learning a generalized type of Bayesian networks that involves shared structure and rameters. Let $\mathcal{X}$ be a se f var les, which we assum -valued. A module network over X partitions the variables X into K disjoint clusters, for $K\ll n=|{\dot{\mathcal X}}|$ ≪ |X| . All of the variables assigned to the same cluster have precisely the same parents and CPD. More precisely, such a network deﬁnes: 

![](images/ccb2bd3b795bac08d6d68bb97caae47a9bfc56b6bdeb92143926e2b97776ea93.jpg) 
Figure 18.11 A simple module network 

• t function $\mathcal{A}$ , which deﬁnes for each variable $X$ , a cluster assignment ${\mathcal{A}}(X)\ \in$ $\{C_{1},.\,.\,.\,,C_{K}\}$ { } . • For e $C_{k}$ $(k=1,.\,.\,.\,,K)$ , a graph $\mathcal{G}$ that deﬁnes a set of parents $\mathrm{Pa}_{C_{k}}=U_{k}\subset\mathcal{X}$ and a CPD $P_{k}(X\mid U_{k})$ . 

The module network structure deﬁnes a ground Bayesian network where, for each variable $X$ , we have the parents $U_{k}$ for $k=\mathcal{A}(X)$ and the CPD $P_{k}(X\mid{\dot{U}}_{k})$ . Figure 18.11 shows an example of such a network. Assume that our goal is to learn a module network that maximizes the Bayesian score given a data set $\mathcal{D}$ , where we need to learn both the assignment of variables to clusters and the graph structure. 

a. Deﬁne an appropriate set of parameters and an appropriate notion of sufcient statistics for this class of models, and write down a precise formula for the likelihood function of a pair $({\mathcal{A}},{\mathcal{G}})$ in terms of the parameters and sufcient statistics. b. Draw the meta-network for the module network shown in ﬁgure 18.11. Assuming a uniform prior over each parameter, write down exactly (normalizing constants included) the appropriate parameter posterior given a data set $\mathcal{D}$ . c. We now turn to the problem of learning the structure of the cluster network. We will use local search, using the following types of operators: • Add operators that add a parent for a cluster; • Delete operators that delete a parent for a cluster; • Node-Move operators $o_{k\rightarrow k^{\prime}}(\bar{X})$ that change from ${\mathcal{A}}(X)=k$ to $\mathcal{A}(X)=k^{\prime}$ . Describe an efcient implementation of the Node-Move operator. d. For each type of operator, specify (precisely) which other operators need to be reevaluated once the operator has been taken? Brieﬂy justify your response. e. Why did we not include edge reversal in our set of operators? 

# Exercise ${\bf18.23\star}$ 

reinsertion operator 

It is often useful when learning the structure of a Bayesian network to consider more global search operations. In this problem we will consider an operator called reinsertion , which works as follows: For the current structure $\mathcal{G}$ , we choose a variable $X_{i}$ to be our target variable . The ﬁrst step is to remove the variable from the network by severing all connections to its children and parents. We then select the optimal set of at most $K_{p}$ parents and at most $K_{c}$ children for $X$ and reinsert it into the network with edges from the selected parents and to the selected children. Throughout this problem, assume the use of the BIC score for structure evaluation. 

a. Let $X_{i}$ be our current target variable, and assume for the moment that we have somehow chosen $U_{i}$ to be optimal parents of ${\bar{X_{i}}}$ . Consider the case of $K_{c}=1$ , where we want to choose the single optimal child for $X_{i}$ . Candidate children — those that do not introduce a cycle in the graph — are $Y_{1},.\,.\,.\,,Y_{\ell}$ . Write an argmax expression for ﬁnding the optimal child $C$ . Explain your answer. b. Now consider the case of $K_{c}=2$ . How do we ﬁnd the optimal pair of children? Assuming that our family score for any $\{X_{k},U_{k}\}$ can be computed in a constant time $f$ , what is the best asymptotic computational complexity of ﬁnding the optimal pair of children? Explain. Extend your analysis to larger values of $K_{c}$ . What is the computational complexity of this task? c. We now consider the choice of parents for $X_{i}$ . We now assume that we have already somehow chosen 

the optimal set of children and will hold them ﬁxed. Can we do the same trick when choosing the parents? If so, show how. If not, argue why not. 

# Exercise 18.24 

Prove proposition 18.7. 

# Exercise 18.25 

Prove proposition 18.8. 

# Exercise $\pmb{18.26\star}$ 

Consider the idea of searching for a high-scoring network by searching over the space of rderings $\prec$ over the variables. Our task is to search for a high-scoring network that has bounded indegree k . For simplicity, we focus on the li ihood score. For a giv n order $\prec$ , le $\mathcal{G}_{\prec}^{*}$ ≺ be the highest-likelihood network consistent with the ordering ≺ of bounded indegree k . We ﬁne $\mathrm{score}_{L}(\prec\ :\ \bar{\mathcal{D}})=\mathrm{score}_{L}(\mathcal{G}_{\prec}^{*}\ :\ \mathcal{D})$ ≺ D G D . We now ≺ search over the space of orderings using operators o that swap two adjacent nodes in the ordering, that is: 

$$
X_{1},\ldots,X_{i-1},X_{i},X_{i+1},\ldots,X_{n}\mapsto X_{1},\ldots,X_{i-1},X_{i+1},X_{i},\ldots,X_{n}.
$$ 

Show how to use score decomposition properties to search this space efciently. 

# Exercise $18.27\star$ 

Con er the choice between $\mathcal{G}_{\varnothing}$ and ${\mathcal{G}}_{X\to Y}$ given a data set of joint observations of binary variables $X$ and Y . 

a. Show that $\operatorname{score}_{B I C}({\mathcal{G}}_{X\to Y}\ :\ {\mathcal{D}})>\operatorname{score}_{B I C}({\mathcal{G}}_{\varnothing}\ :\ {\mathcal{D}})$ D G D if and only if $I\!\!I_{\hat{P}_{\mathcal{D}}}(X;Y)>c$ . What is ∅ D this constant c $c_{:}^{2}$ ? b. Suppose that we have BDe prior with uniform $P^{\prime}$ and $\alpha=0$ . Write the condition on the counts when $\hat{\mathrm{score}}_{B D e}(\mathcal{G}_{X\to Y}\ :\ \mathcal{D})>\hat{\mathrm{score}}_{B D e}(\mathcal{G}_{\varnothing}\ :\ \mathcal{D})$ . ∅ c. Consider empirical distributions of the form discussed in ﬁgure 18.3. That is, $\hat{P}(x,y)=0.25\!+\!0.5\cdot\!p$ · if $x$ and $y$ are equal, and $\hat{P}(x,y)=0.25-0.5\cdot p$ − · otherwise. For diferent values of $p$ , plot as a function of $M$ both the $\chi^{2}$ deviance measure and the mutual information. What do you conclude about these diferent functions? d. Implement the exact p-value test described in section 18.2.2.3 for the $\chi^{2}$ and the mutual information deviance measures. e. Using the same empirical distribution, plot for diferent values of $M$ the decision boundary between $\mathcal{G}_{\varnothing}$ and ${\mathcal{G}}_{X\to Y}$ for each of the three methods we considered in this exercise. That , ﬁnd the value of $p$ at which the two alternatives have (approximately) equal score, or at which the p -value of rejecting the null hypothesis is (approximately) 0 . 05 . 

What can you conclude about the diferences between these structure selection methods? 

# 19 Partially Observed Data 

hidden variable 

incomplete data Until now, our discussion of learning assumed that the training data are fully observed : each instance assigns values to all the variables in our domain. This assumption was crucial for some of the technical developments in the previous two chapters. Unfortunately, this assumption is clearly unrealistic in many settings. In some cases, data are missing by accident; for example, some ﬁelds in the data may have been omitted in the data collection process. In other cases, certain observations were simply not made; in a medical-diagnosis setting, for example, one never performs all possible tests or asks all of the possible questions. Finally, some variables are hidden , in that their values are never observed. For example, some diseases are not observed directly, but only via their symptoms. 

In fact, in many real-life applications of learning, the available data contain missing values. Hence, we must address the learning problem in the presence of incomplete data . As we will see, incomplete data pose both foundational problems and computational problems. The foundational problems are in formulating an appropriate learning task and determining when we can expect to learn from such data. The computational problems arise from the complications incurred by incomplete data and the construction of algorithms that address these complications. 

In the ﬁrst section, we discuss some of the subtleties encountered in learning from incomplete data and in formulating an appropriate learning problem. In subsequent sections, we examine techniques for addressing various aspects of this task. We focus initially on the parameter- learning task, assuming ﬁrst that the network structure is given, and then treat the more complex structure-learning question at the end of the chapter. 

# 19.1 Foundations 

# 19.1.1 Likelihood of Data and Observation Models 

A central concept in our discussion of learning so far was the likelihood function that measures the probability of the data induced by diferent choices of models and parameters. The likelihood function plays a central role both in maximum likelihood estimation and in Bayesian learning. In these developments, the likelihood function was determined by the probabilistic model we are learning. Given a choice of parameters, the model deﬁned the probability of each instance. In the case of fully observed data, we assumed that each instance $\xi[m]$ in our training set $\mathcal{D}$ is simply a random sample from the model. 

It seems straightforward to extend this idea to incomplete data. Suppose our domain consists of two random variables $X$ and $Y$ , and in one particular instance we observed only the value of $X$ to be $x[m]$ , but not the value of $Y$ . Then, it seems natural to assign the instance the probability $P(x[m])$ . More generally, the likelihood of an incomplete instance is simply the marginal probability given our model. Indeed, the most common approach to deﬁne the likelihood of an incomplete data set is to simply marginalize over the unobserved variables. 

This approach, however, embodies some rather strong assumptions about the nature of our data. To learn from incomplete data, we need to understand these assumptions and examine the situation much more carefully. Recall that when learning parameters for a model, we assume that the data were generated according to the model, so that each instance is a sample from the model. When we have missing data, the data-generation process actually involves two steps. In the ﬁrst step, data are generated by sampling from the model. In this step, values of all the variables are selected. The next step determines which values we get to observe and which ones are hidden from us. In some cases, this process is simple; for example, some particular variable may always be hidden. In other situations, this process might be much more complex. 

To analyze the probabilistic model of the observed training set, we must consider not only the data-generation mechanism, but also the mechanism by which data are hidden. Consider the following two examples. 

# Example 19.1 

We ﬂip a thumbtack onto a table, and every now and then it rolls of the table. Since a fall from the table to the ﬂoor is quite diferent from our desired experimental setup, we do not use results from these ﬂips (they are missing). How would that change our estimation? The simple solution is to ignore the missing values and simply use the counts from the ﬂips that we did get to observe. That is, we pretend that missing ﬂips never happened. As we will see, this strategy can be shown to be the correct one to use in this case. 

Now, assume that the experiment is performed by a person who does not like “tails” (because the point that sticks up might be dangerous). So, in some cases when the thumbtack lands “tails,” the experimenter throws the thumbtack on the ﬂoor and reports a missing value. However, if the thumbtack lands “heads,” he will faithfully report it. In this case, the solution is also clear. We can use our knowledge that every missing value is “tails” and count it as such. Note that this leads to very diferent likelihood function (and hence estimated parameters) from the strategy that we used in the previous case. 

While this example may seem contrived, many real-life scenarios have very similar properties. For example, consider a medical trial evaluating the efcacy of a drug, but one where patients can drop out of the trial, in which case their results are not recorded. If patients drop out at random, we are in the situation of example 19.1; on the other hand, if patients tend to drop out only when the drug is not efective for them, the situation is essentially analogous to the one in this example. 

Note that in both examples, we observe sequences of the form $H,T,H,\S,T,\S,\cdot\cdot\cdot,$ but never- theless we treat them diferently. The diference between these two examples is our knowledge about the observation mechanism. As we discussed, each observation is derived as a combi- nation of two mechanisms: the one that determines the outcome of the ﬂip, and the one that determines whether we observe the ﬂip. Thus, our training set actually consists of two variables for each ﬂip: the ﬂip outcome $X$ , and the observation variable $O_{X}$ , which tells us whether we observed the value of $X$ . 

![](images/5f2b1abf32ab7ba39b8533b77afcf4114aa0f980e35569e9574b7c6728e36c03.jpg) 

Figure 19.1 Observation models in two variants of the thumbtack example Deﬁnition 19.1 observability variable observability model 

Let $X\,=\,\{X_{1},.\,.\,.\,,X_{n}\}$ be some set of random variables, and let ${\cal O}_{X}\,=\,\{{\cal O}_{X_{1}},.\,.\,.\,,{\cal O}_{X_{n}}\}$ be their observability variable . The observability model is a joint distribution $P_{m i s s i n g}(X,{\cal O}_{X})\,=$ $P(X)\cdot P_{\mathit{m i s s i n g}}(O_{X}\mid X)$ , so th $P(X)$ is parameterized by parameters $\theta$ , and $P_{m i s s i n g}(O_{X}\mid X)$ is parameterized by parameters ψ . We deﬁne a new set of ndom variables $Y=\{Y_{1},.\,.\,.\,,Y_{n}\}$ , ere $V a l(Y_{i})=V a l(X_{i})\cup\{?\}$ . The actual observation is Y , which is a deterministic function of X and $O_{X}$ , 

$$
Y_{i}=\left\{\begin{array}{l l}{X_{i}}&{\qquad\phantom{X}O_{X_{i}}=o^{1}}\\ {?}&{\qquad\phantom{X}O_{X_{i}}=o^{0}.}\end{array}\right.
$$ 

The variables $Y_{1},\dots,Y_{n}$ represent the values we actually observe, either an actual value or a ? that represents a missing value. 

Thus, we observe the $Y$ variable. This observation always implies that we know the value of the $O_{X}$ variables, and whenever $Y_{i}\ \ne\ \S,$ we also observe the value of $X_{i}$ . To illustrate the deﬁnition of this concept, we consider the probability of the observed value Y $Y$ in the two preceding examples. 

Example 19.3 In the scenario of example 19.1, we have a parameter $\theta$ that describes the probability of $X=1$ (Heads), and another parameter $\psi$ that describes the probability of $O_{X}=o^{1}$ . Since we assume that the hiding mechanism is random, we can describe this scenario by the meta-network of ﬁgure 19.1a. This network describes how the probability of diferent instances (shown as plates) depend on the parameters. As we can see, this network consists of two independent subnetworks. The ﬁrst relates the values of $X$ in the diferent examples to the parameter $\theta$ , and the second relates the values of $O_{X}$ to $\psi$ . 

Recall from our earlier discussion that if we can show that $\theta$ and $\psi$ are independent given the evidence, then the likelihood decomposes into a product. We can derive this decomposition as follows. Consider the three values of $Y$ and how they could be attained. We see that 

$$
\begin{array}{r c l}{P(Y=1)}&{=}&{\theta\psi}\\ {P(Y=0)}&{=}&{(1-\theta)\psi}\\ {P(Y=?)}&{=}&{(1-\psi).}\end{array}
$$ 

Thus, if we see a data set $\mathcal{D}$ of tosses with $M$ [1] , $M[0]$ , and $M[\AA]$ instances that are Heads, Tails, and ?, respectively, then the likelihood is 

$$
L(\theta,\psi:\mathcal{D})=\theta^{M[1]}(1-\theta)^{M[0]}\psi^{M[1]+M[0]}(1-\psi)^{M[\natural]}.
$$ 

As we expect, the likelihood function in this example is a product of two functions: a function of $\theta$ , and a function of $\psi$ . We can easily see that the maximum likelihood estimate of $\theta$ is $\begin{array}{r}{\frac{M[1]}{M[1]+M[0]}}\end{array}$ , while the maximum likelihood estimate of $\psi$ is $\begin{array}{r}{\frac{M[1]+M[0]}{M[1]+M[0]+M[?]}}\end{array}$ . 

We can also reach the conclusion regarding independence using a more qualitative analysis. At ﬁrst glance, it appears that observing $Y$ activates the $\nu$ -structure between $X$ and $O_{X}$ , rendering them dependent. However, the CPD of $Y$ has a particular structure, which induces context-speciﬁc independence. In particular, we see that $X$ and $O_{X}$ are conditionally independent given both values of $Y$ : when $Y=2,$ then $O_{X}$ necessarily $o^{0}$ , in which case the edge $X\rightarrow Y$ s spu s (as in deﬁnition 5.7); if $Y\neq!,$ then Y deterministic ally establishes the values of both X and $O_{X}$ , in which case they are independent. 

Example 19.4 Now consider the scenario of example 19.2. Recall that in this example, the missing values are a consequence of an action of the experimenter after he sees the outcome of the toss. Thus, the probability of missing values depends on the value of $X$ . To deﬁne the likelihood function, suppose $\theta$ is the probability of $X\,=\,1$ . The observation parameters $\psi$ consist of two values: $\psi_{O_{X}|x^{1}}$ is probability $O_{X}=o^{1}$ when $X=1$ , and $\psi_{O_{X}|x^{0}}$ is the probability of $O_{X}=o^{1}$ when $X=0$ . 

We can describe this scenario by the meta-network of ﬁgure 19.1b. In this network, $O_{X}$ depends directly on $X$ . When we get an observation $Y=2,$ we essentially observe the value of $O_{X}$ but not of $X$ . In this case, due to the direct edge between $X$ and $O_{X}$ , the context-speciﬁc independence properties of $Y$ do not help: $X$ and $O_{X}$ are correlated, and therefore so are their associated parameters. Thus, we cannot conclude that the likelihood decomposes. 

Indeed, when we examine the form of the likelihood, this becomes apparent. Consider the three values of $Y$ and how they could be attained. We see that 

$$
\begin{array}{l c l}{P(Y=1)}&{=}&{\theta\psi_{O_{X}|x^{1}}}\\ {P(Y=0)}&{=}&{(1-\theta)\psi_{O_{X}|x^{0}}}\\ {P(Y=\?)}&{=}&{\theta(1-\psi_{O_{X}|x^{1}})+(1-\theta)(1-\psi_{O_{X}|x^{0}}).}\end{array}
$$ 

And so, if we see a data set $\mathcal{D}$ of tosses with $M$ [1] , $M[0],$ , and $M[\AA]$ instances that are Heads, Tails, and ?, respectively, then the likelihood is 

$$
\begin{array}{r l}{\lefteqn{L(\theta,\psi_{O_{X}|x^{1}},\psi_{O_{X}|x^{0}}:\mathcal{D})}}\\ &{=\phantom{\psi_{O_{X}|x^{1}},}\theta^{M[1]}(1-\theta)^{M[0]}\psi_{O_{X}|x^{1}}^{M[1]}\psi_{O_{X}|x^{0}}^{M[0]}}\\ &{\hphantom{=}(\theta(1-\psi_{O_{X}|x^{1}})+(1-\theta)(1-\psi_{O_{X}|x^{0}}))^{M[\mathcal{I}]}.}\end{array}
$$ 

As we can see, the likelihood function in this example is more complex than the one in the previous example. In particular, there is no easy way of decoupling the likelihood of $\theta$ from the likelihood of $\psi_{O_{X}|x^{1}}$ and $\psi_{O_{X}|x^{0}}$ . This makes sense, since diferent values of these parameters imply diferent possible values of $X$ when we see a missing value and so afect our estimate of $\theta$ ; see exercise 19.1. 

![](images/dca0c80c47fb66e44bb3d5f03e258428a658b5ec881b6dfa827d157f5c259348.jpg) 
Figure 19.2 An example satisfying MAR but not MCAR. Here, the observability pattern depends on the value of underlying variables. 

# 19.1.2 Decoupling of Observation Mechanism 

As we saw in the last two examples, modeling the observation variables, that is, the process that generated the missing values, can result in nontrivial modeling choices, which in some cases result in complex likelihood functions. Ideally, we would hope to avoid dealing with these issues and instead focus on the likelihood of the process that we are interested in (the actual random variables). When can we ignore the observation variables? In the simplest case, the observation mechanism is completely independent of the domain variables. This case is precisely the one we encountered in example 19.1. 

Deﬁnition 19.2 missing completely at random 

A missing data model $P_{m i s s i n g}$ is missing completely at random (MCAR) if ${}^{\cdot}P_{m i s s i n g}\models(X\perp O_{X}).$ ⊥ 

In this case, the likelihood of $X$ and $O_{X}$ decomposes as a product, and we can maximize each part separately. We have seen this decomposition in the likelihood function of example 19.3. The implications of the decoupling is that we can maximize the likelihood of the parameters of the distribution of $X$ without considering the values of the parameters governing the distribution of $O_{X}$ . Since we are usually only interested in the former parameters, we can simply ignore the later parameters. 

The MCAR assumption is a very strong one, but it holds in certain settings. For example, momentary sensor failures in medical/scientiﬁc imaging (for example, ﬂecks of dust) are typically uncorrelated with the relevant variables being measured, and they induce MCAR observation models. Unfortunately, in many other domains the MCAR simply does not hold. For example, in medical records, the pattern of missing values owes to the tests the patient underwent. These, however, are determined by some of the relevant variables, such as the patient’s symptoms, the initial diagnosis, and so on. 

As it turns out, MCAR is sufcient but not necessary for the decomposition of the likelihood function. We can provide a more general condition where, rather than assuming marginal inde- pendence between $O_{X}$ and the values of $X$ , we assume only that the observation mechanism is conditionally independent of the underlying variables given other observations. 

Example 19.5 for the corresponding model. In this case, $P_{m i s s i n g}\models(O_{X_{2}}\ \bot\ X_{2}\ |\ X_{1})$ ⊥ | . In other words, the true values of both coins are independent of whether they are hidden or not, given our observations. 

To understand the issue better, let us write the model and likelihood explicitly. Because we assume that the two coins are independent, we have two parameters $\theta_{X_{1}}$ and $\theta_{X_{2}}$ for the probability of the two coins. In this example, the ﬁrst coin is always observed, and the observation of the second one depends on the value of the ﬁrst. Thus, we have parameters $\psi_{O x_{2}|x_{1}^{1}}$ and $\psi_{O_{X_{2}}|x_{1}^{0}}$ that represent the probability of observing $X_{2}$ given that $X_{1}$ is heads or tails, respectively. 

To derive the likelihood function, we need to consider the probability of all possible observations. There are six possible cases, which fall in two categories. 

In the ﬁrst category are the four cases where we observe both coins. By way of example, consider the observation $Y_{1}\,=\,y_{1}^{1}$ and $Y_{2}\,=\,y_{2}^{0}$ . The probability of this observation is clearly $P(X_{1}=$ $x_{1}^{1},X_{2}=x_{2}^{0},O_{X_{1}}=o^{1},O_{X_{2}}=o^{1})$ ) . Using our modeling assumption, we see that this is simply the product $\theta_{X_{1}}(1-\theta_{X_{2}})\psi_{O_{X_{2}}|x_{1}^{1}}$ . 

In the second category are the two cases where we observe only the ﬁrst coin. By way of example, consider the observation $Y_{1}=y_{1}^{1},Y_{2}=z.$ ?. The probability of this observation is $P(X_{1}=$ $x_{1}^{1},O_{X_{1}}=o^{1},O_{X_{2}}=o^{0})$ ) . Note that the value of $X_{2}$ does not play a role here. This probability is simply the product $\theta_{X_{1}}(1-\psi_{O_{X_{2}}|x_{1}^{1}})$ . 

If we write all six possible cases and then rearrange the products, we see that we can write the likelihood function as 

$$
\begin{array}{l c l}{{{\cal L}(\pmb{\theta}:\mathcal{D})}}&{{=}}&{{\theta_{X_{1}}^{M[y_{1}^{1}]}(1-\theta_{X_{1}})^{M[y_{1}^{0}]}}}\\ {{}}&{{}}&{{\theta_{X_{2}}^{M[y_{2}^{1}]}(1-\theta_{X_{2}})^{M[y_{2}^{0}]}}}\\ {{}}&{{}}&{{\psi_{O_{X_{2}}|x_{1}^{1}}^{M[y_{1}^{1},y_{2}^{1}]+M[y_{1}^{1},y_{2}^{0}]}(1-\psi_{O_{X_{2}}|x_{1}^{1}})^{M[y_{1}^{1},y_{2}^{1}]}}}\\ {{}}&{{}}&{{\psi_{O_{X_{2}}|x_{1}^{0}}^{M[y_{1}^{0},y_{2}^{1}]+M[y_{1}^{0},y_{2}^{0}]}(1-\psi_{O_{X_{2}}|x_{1}^{0}})^{M[y_{1}^{0},y_{2}^{1}]}.}}\end{array}
$$ 

This likelihood is a product of four diferent functions, each involving just one parameter. Thus, we can estimate each parameter independently of the rest. 

As we saw in the last example, conditional independence can help us decouple the estimate of parameters of $P(X)$ from these of $P(O_{X}\mid X)$ . Is this a general phenomenon? To answer this question, we start with a deﬁnition. 

missing at random 

Let $_{_y}$ be a tuple of observations. These observations partition the variables $X$ into two sets, the observed variables $X_{\mathit{o b s}}^{y}\,=\,\{X_{i}\,:\,y_{i}\,\neq\,?\}$ { ̸ } and th hidden ones $X_{h i d d e n}^{y}\,=\,\{X_{i}\,:\,y_{i}\,=\,?\}$ { } . The values of the observed variables are determined by y , while the values of the hidden variables are not. We say that a missing data model $P_{m i s s i n g}$ is missing at random (MAR) if for all observations $_{_y}$ with $P_{m i s s i n g}(\pmb{y})>0$ , and for all $\pmb{x}_{h i d d e n}^{\pmb{y}}\in V a l(\pmb{X}_{h i d d e n}^{\pmb{y}})$ , we have that 

$$
P_{m i s s i n g}\models(o_{X}\perp\mathbf{\it{x}}_{h i d d e n}^{y}\mid\mathbf{\it{x}}_{o b s}^{y})
$$ 

where $_{O X}$ are the speciﬁc values of the observation variables given $Y$ 

In words, the MAR assumption requires independence between the events $_{O X}$ and $\boldsymbol{x}_{\mathrm{hidden}}^{y}$ given $\boldsymbol{x}_{\mathrm{obs}}^{y}$ . Note that this statement is written in terms of event-level conditional independence rather than conditional independence between random variables. This generality is necessary since every instance might have a diferent pattern of observed variables; however, if the set of observed variables is known in advance, we can state MAR as conditional independence between random variables. 

This statement implies that the observation pattern gives us no additional information about the hidden variables given the observed variables : 

$$
P_{m i s s i n g}(\pmb{x}_{\mathrm{hidden}}^{y}\mid\pmb{x}_{\mathrm{obs}}^{y},o_{X})=P_{m i s s i n g}(\pmb{x}_{\mathrm{hidden}}^{y}\mid\pmb{x}_{\mathrm{obs}}^{y}).
$$ 

Why should we require the MAR assumption? If $P_{m i s s i n g}$ satisﬁes this assumption, then we can write 

$$
\begin{array}{c c l}{{P_{m i s i n g}({\pmb y})}}&{{=}}&{{\displaystyle\sum_{{\pmb x}_{\mathrm{hidden}}^{y}}\left[P({\pmb x}_{\mathrm{obs}}^{y},{\pmb x}_{\mathrm{hidden}}^{y})P_{m i s i n g}(o_{X}\mid{\pmb x}_{\mathrm{hidden}}^{y},{\pmb x}_{\mathrm{obs}}^{y})\right]}}\\ {{}}&{{=}}&{{\displaystyle\sum_{{\pmb x}_{\mathrm{hidden}}^{y}}\left[P({\pmb x}_{\mathrm{obs}}^{y},{\pmb x}_{\mathrm{hidden}}^{y})P_{m i s i n g}(o_{X}\mid{\pmb x}_{\mathrm{obs}}^{y})\right]}}\\ {{}}&{{=}}&{{P_{m i s i n g}(o_{X}\mid{\pmb x}_{\mathrm{obs}}^{y})\displaystyle\sum_{{\pmb x}_{\mathrm{hidden}}^{y}}P({\pmb x}_{\mathrm{obs}}^{y},{\pmb x}_{\mathrm{hidden}}^{y})}}\\ {{}}&{{=}}&{{P_{m i s i n g}(o_{X}\mid{\pmb x}_{\mathrm{obs}}^{y})P({\pmb x}_{\mathrm{obs}}^{y}).}}\end{array}
$$ 

The ﬁrst term depends only on the parameters $\psi$ , and the second term depends only on the parameters $\theta$ . Since we write this product for every observed instance, we can write the likelihood as a product of two likelihoods, one for the observation process and the other for the underlying distribution. 

If $P_{m i s s i n g}$ satisﬁes MAR, then $L(\theta,\psi:{\mathcal{D}})$ can be written as a product of two likelihood functions $L(\theta:{\mathcal{D}})$ and $L(\psi:{\mathcal{D}})$ . 

This theorem implies that we can optimize the likelihood function in the parameters $\theta$ of the distribution $P(X)$ independently of the exact value the observation model parameters. In other words, the MAR assumption is a license to ignore the observation model while learning parameters. 

The MAR assumption is applicable in a broad range of settings, but it must be considered with care. For example, consider a sensor that measures blood pressure $B$ but can fail to record a measurement when the patient is overweight. Obesity is a very relevant factor for blood pressure, so that the sensor failure itself is informative about the variable of interest. However, if we always have observations $W$ of the patient’s body weight and $H$ of the height, then $O_{B}$ is conditionally independent of $B$ given $W$ and $H$ . As another example, consider the patient description in hospital records. If the patient does not have an X-ray result $X$ , he probably does not sufer from broken bones. Thus, $O_{X}$ gives us information about the underlying domain variables. However, assume that the patient’s chart also contains a “primary complaint” variable, which was the factor used by the physician in deciding which tests to perform; in this case, the MAR assumption does hold. 

In both of these cases, we see that the MAR assumption does not hold given a limited set of observed attributes, but if we expand our set of observations, we can get the MAR assumption to hold. In fact, one can show that we can always extend our model to produce one where the MAR assumption holds (exercise 19.2). Thus, from this point onward we assume that the data satisfy the MAR assumption, and so our focus is only on the likelihood of the observed data. However, before applying the methods described later in this chapter, we always need to consider the possible correlations between the variables and the observation variables, and possibly to expand the model so as to guarantee the MAR assumption. 

# 19.1.3 The Likelihood Function 

Throughout our discussion of learning, the likelihood function has played a major role, either on its own, or with the prior in the context of Bayesian estimation. Under the assumption of MAR, we can continue to use the likelihood function in the same roles. From now on, assume we have a network $\mathcal{G}$ over a set of variables $X$ . In general, each instance has a diferent set of observed variables. We will denote by ${\cal O}[m]$ and $o[m]$ the observed variables and their values in the $m$ ’th instance, and by $H[m]$ the missing (or hidden) variables in the $m$ ’th instance. We use $L(\theta:{\mathcal{D}})$ to denote the probability of the observed variables in the data, marginalizing out the hidden variables, and ignoring the observability model: 

$$
L(\pmb\theta:\mathcal D)=\prod_{m=1}^{M}P(\pmb o[m]\mid\pmb\theta).
$$ 

As usual, we use $\ell(\pmb\theta:{\mathcal D})$ to denote the logarithm of this function. 

With this deﬁnition, it might appear that the problem of learning with missing data does not difer substantially from the problem of learning with complete data. We simply use the likelihood function in exactly the same way. Although this intuition is true to some extent, the computational issues associated with the likelihood function are substantially more complex in this case. 

To und he complications, we consider a simple example on the network ${\mathcal{G}}_{X\to Y}$ with the edge X $X\rightarrow Y$ → . When we have complete data, the likelihood function for this network has the following form: 

$$
\begin{array}{r l}&{L(\pmb{\theta}_{X},\pmb{\theta}_{Y|x^{0}},\pmb{\theta}_{Y|x^{1}}:\mathcal{D})=}\\ &{\qquad\theta_{x^{1}}^{M[x^{1}]}\theta_{x^{0}}^{M[x^{0}]}\cdot\theta_{y^{1}|x^{0}}^{M[x^{0},y^{1}]}\theta_{y^{0}|x^{0}}^{M[x^{0},y^{0}]}\cdot\theta_{y^{1}|x^{1}}^{M[x^{1},y^{1}]}\theta_{y^{0}|x^{1}}^{M[x^{1},y^{0}]}.}\end{array}
$$ 

In the binary case, we can use the constraints to rewrite $\theta_{x^{0}}=1-\theta_{x^{1}}$ , $\theta_{y^{0}|x^{0}}=1-\theta_{y^{1}|x^{0}}$ , and $\theta_{y^{0}|x^{1}}=1-\theta_{y^{1}|x^{1}}$ . Thus, this is a function of three parameters. For example, if we have a data set with the following sufcient statistics: 

$$
\begin{array}{r l}{{}}&{{x^{1},y^{1};\ 13}}\\ {{}}&{{x^{1},y^{0}{:\ 16}}}\\ {{}}&{{x^{0},y^{1}{:\ 10}}}\\ {{}}&{{x^{0},y^{0}{:\ 4},}}\end{array}
$$ 

then our likelihood function has the form: 

$$
\theta_{x^{1}}^{29}(1-\theta_{x^{1}})^{14}\cdot\theta_{y^{1}|x^{0}}^{10}(1-\theta_{y^{1}|x^{0}})^{4}\cdot\theta_{y^{1}|x^{1}}^{13}(1-\theta_{y^{1}|x^{1}})^{16}.
$$ 

This function is well-behaved: it is log-concave, and it has a unique global maximum that has a simple analytic closed form. 

![](images/a9d3baf5b48e78c8f8deb51e92beb9df56ad5184b688735b25d76c78d8ee5372.jpg) 
Figure 19.3 A visualization of a multimodal likelihood function with incomplete data. The data likelihood is the sum of complete data likelihoods (shown in gray lines). Each of these is unimodal, yet their sum is multimodal. 

Assume that the ﬁrst instance in the data set was $X[1]\,=\,x^{0},Y[1]\,=\,y^{1}$ . Now, consider a situation where, rather than observing this instance, we observed only $Y[1]\,=\,y^{1}$ . We now have to reason that this particular data instance could have arisen in two cases: one where $X[1]=x^{0}$ and one where $X[1]=x^{1}$ . In the former case, our likelihood function is as before. In the second case, we have 

$$
\theta_{x^{1}}^{30}(1-\theta_{x^{1}})^{13}\cdot\theta_{y^{1}|x^{0}}^{9}(1-\theta_{y^{1}|x^{0}})^{4}\cdot\theta_{y^{1}|x^{1}}^{14}(1-\theta_{y^{1}|x^{1}})^{16}.
$$ 

When we do not observe $X[1]$ , the likelihood is the marginal probability of the observations. That is, we need to sum over possible assignments to the unobserved variables. This implies that the likelihood function is the sum of the two complete likelihood functions of equation (19.1) and equation (19.2). Since both likelihood functions are quite similar, we can rewrite this sum as 

$$
\begin{array}{r}{\theta_{x^{1}}^{29}(1-\theta_{x^{1}})^{13}\cdot\theta_{y^{1}|x^{0}}^{9}(1-\theta_{y^{1}|x^{0}})^{4}\cdot\theta_{y^{1}|x^{1}}^{13}(1-\theta_{y^{1}|x^{1}})^{16}\left[\theta_{x^{1}}\theta_{y^{1}|x^{1}}+(1-\theta_{x^{1}})\theta_{y^{1}|x^{0}}\right].}\end{array}
$$ 

This form seems quite nice, except for the last sum, which couples the parameter θ $\theta_{x^{1}}$ with $\theta_{y^{1}|x^{1}}$ and $\theta_{y^{1}|x^{0}}$ . 

If we have more missing values, there are other cases we have to worry about. For example, if $X[2]$ is also unobserved, we have to consider all possible combinations for $X[1]$ and $X[2]$ . This results in a sum over four terms similar to equation (19.1), each one with diferent counts. In general, the likelihood function with incomplete data is the sum of likelihood functions, one for each possible joint assignment of the missing values. Note that the number of possible assignments is exponential in the total number of missing values. 

We can think of the situation using a geometric intuition. Each one of the complete data likelihood deﬁnes a unimodal function. Their sum, however, can be multimodal. In the worst case, the likelihood of each of the possible assignments to the missing values contributes to a diferent peak in the likelihood function. The total likelihood function can therefore be quite complex. It takes the form of a “mixture of peaks,” as illustrated pictorially in ﬁgure 19.3. 

parameter independence 

likelihood decomposability 

To make matters even more complicated, we lose the property of parameter independence , and thereby the decomposability of the likelihood function. Again, we can understand this phenomenon either qualitatively, from the perspective of graphical models, or quantitatively, by 

![](images/d5ce1491061ce749826b8b140c7fc014c6e7fd4b5c276755e7764838bfc60658.jpg) 

![](images/c77291cf03a3444efba8417b506d0108e85963b8bbd124969e454273c0ba6210.jpg) 
Figure 19.4 The meta-network for parameter estimation for $X\,\rightarrow\,Y$ . When $X[m]$ is hidden but $Y[m]$ is observed, the trail $\theta_{X}\,\to\,X[m]\,\to\,Y[m]\,\leftarrow\,\theta_{Y|X}$ is active. Thus, the parameters are not independent in the posterior distribution. 

looking at the likelihood function. Qualitatively, recall from section 17.4.2 that, in the complete data case, $\theta_{Y\mid x^{1}}$ and $\theta_{Y\mid x^{0}}$ are independent given the data, because they are independent given $Y[m]$ and $X[m]$ . But if $X[m]$ is unobserved, they are clearly dependent. This fact is clearly illustrated by the meta-network (as in ﬁgure 17.7) that represents the learning problem. For example, in a simple network over two variables $X\rightarrow Y$ , we see that missing data can couple the two parameters’ variables; see ﬁgure 19.4. 

We can also see this phenomenon numerically. Assume for simplicity that $\theta_{X}$ is known. Then, our likelihood is a function of two parameters $\theta_{y^{1}|x^{1}}$ and $\theta_{y^{1}|x^{0}}$ . Intuitively, if our missing $X[1]$ is $H$ , then it cannot be $T$ . Thus, the likelihood functions of the two parameters are correlated; the more missing data we have, the stronger the correlation. This phenomenon is shown in ﬁgure 19.5. 

local decomposability global decomposability 

This example shows that we have lost the local decomposability property in estimating the CPD $P(Y\mid X)$ . What about global decom ability between diferent CPD Con er a simple model where there is one hidden variable H , and two observed variables X and Y , and edges $H\rightarrow X$ and $H\rightarrow Y$ . Thus, the probability of observing the values $x$ and $y$ is 

$$
P(x,y)=\sum_{h}P(h)P(x\mid h)P(y\mid h).
$$ 

The likelihood function is a product of such terms, one for each observed instance $x[m],y[m]$ , and thus has the form 

$$
L(\pmb\theta:\mathcal D)=\prod_{x,y}\left(\sum_{h}P(h)P(x\mid h)P(y\mid h)\right)^{M[x,y]}.
$$ 

When we had complete data, we rewrote the likelihood function as a product of local like- lihood functions, one for each CPD. This decomposition was crucial for estimating each CPD independently of the others. In this example, we see that the likelihood is a product of sum of products of terms involving diferent CPDs. The interleaving of products and sums means that we cannot write the likelihood as a product of local likelihood functions. Again, this result is intuitive: Because we do not observe the variable $H$ , we cannot decouple the estimation of $P(X\mid H)$ rom that of $P(Y\mid H)$ . Roughly speaking, both estimates depend on how we “reconstruct” H in each instance. 

We now consider the general case. Assume we have a network $\mathcal{G}$ over set of variables $X$ . In general, each instance has a diferent set of observed variables. We use D to denote, as before, the actual observed data values; we use $\mathcal{H}=\cup_{m}h[m]$ to denote a possible assignment to all of the missing values in the data set. Thus, the pair $(\mathcal{D},\mathcal{H})$ deﬁnes an assignment to all of the variables in all of our instances. 

The likelihood function is 

$$
L(\pmb\theta:\mathcal D)=P(\mathcal D\mid\pmb\theta)=\sum_{\mathcal H}P(\mathcal D,\mathcal H\mid\pmb\theta).
$$ 

Unfortunately, the number of possible assignments in this sum is exponential in the number of missing values in the entire data set. Thus, although each of the terms $P({\mathcal{D}},{\mathcal{H}}\mid{\boldsymbol{\theta}})$ is a unimodal distribution, the sum can have, in the worst case, an exponential number of modes. 

However, unimodality is not the only property we lose. Recall that our likelihood function in the complete data case was compactly represented as a product of local terms. This property was important both for the analysis of the likelihood function and for the task of evaluating the likelihood function. What about the incomplete data likelihood? If we use a straightforward representation, we get an exponential sum of terms, which is clearly not useful. Can we use additional properties of the data to help in representing the likelihood? Recall that we assume that diferent instances are independent of each other. This allows us to write the likelihood function as a product over the probability of each partial instance. 

Proposition 19.1 Assuming IID data, the likelihood can be written as 

$$
L(\pmb\theta:\mathcal D)=\prod_{m}P(o[m]\mid\pmb\theta)=\prod_{m}\sum_{h[m]}P(o[m],\pmb h[m]\mid\pmb\theta).
$$ 

This proposition shows that, to compute the likelihood function, we need to perform inference for each instance, computing the probability of the observations. As we discussed in section 9.1, this problem can be intractable, depending on the network structure and the pattern of missing values. Thus, for some learning problems, even the task of evaluating likelihood function for a particular choice of parameters is a difcult computational problem. This observation suggests that optimizing the choice of parameters for such networks can be computationally challenging. 

# 

To conclude, in the presence of partially observed data, we have lost all of the important properties of our likelihood function: its unimodality, its closed-form representation, and the decomposition as a product of likelihoods for the diferent parameters. Without these properties, the learning problem becomes substantially more complex. 

# 19.1.4 Identiﬁability 

Another issue that arises in the context of missing data is our ability to identify uniquely a model from the data. 

# Example 19.6 

Consider again our thumbtack tossing experiments. Suppose the experimenter can randomly choose to toss one of two thumbtacks (say from two diferent brands). Due to a mis communication between the statistician and the experimenter, only the toss outcomes were recorded, but not the brand of thumbtack used. 

To model the experiment, we assume that there is a hidden variable $H$ , so that if $H\,=\,h^{1}$ , the experimenter tossed the ﬁrst thumbtack, and if $H=h^{2}$ , the experimenter tossed the second thumbtack. The parameters of our model are $\theta_{H}$ , $\theta_{X|h^{1}}$ , and $\theta_{X|h^{2}}$ , denoting the probability of choosing the ﬁrst thumbtack, and the probability of heads in each thumbtack. This setting satisﬁes MCAR (since $H$ is hidden). It is straightforward to write the likelihood function: 

$$
L(\theta:{\mathcal{D}})=P(x^{1})^{M[1]}(1-P(x^{1}))^{M[0]},
$$ 

where 

$$
P(x^{1})=\theta_{H}\theta_{X|h^{1}}+(1-\theta_{H})\theta_{X|h^{2}}.
$$ 

If we examine this term, we see that $P(x^{1})$ is the weighted average of $\theta_{X|h^{1}}$ and $\theta_{X|h^{2}}$ . There are multiple choices of these two parameters and $\theta_{H}$ that achieve the same value of $P(x^{1})$ . For example, $\theta_{H}=0.5,\!\theta_{X\mid h^{1}}=0.5,\!\theta_{X\mid h^{2}}=0.5$ leads to the same behavior as $\theta_{H}=0.5,\!\theta_{X\mid h^{1}}=$ $0.8,\!\theta_{X\mid h^{2}}=0.2$ . Because the likelihood of the data is a function only of $P(x^{1})$ , we conclude that there is a continuum of parameter choices that achieve the maximum likelihood. 

This example illustrates a situation where the learning problem is under constrained: Given the observations, we cannot hope to recover a unique set of parameters. Recall that in previous sections, we showed that our estimates are consistent and thus will approach the true parameters when sufcient data are available. In this example, we cannot hope that more data will let us recover the true parameters. 

Before formally treating the issue, let us examine another example that does not involve hidden variables. 

# Example 19.7 

identiﬁability 

Deﬁnition 19.4 identiﬁability 

Suppose we conduct an experiment where we toss two coins $X$ and $Y$ that may be correlated with each other. After each toss, one of the coins is hidden from us using a mechanism that is totally unrelated to the outcome of the coins. Clearly, if we have sufcient observations (that is, the mechanism does not hide one of the coins consistently), then we can estimate the marginal probability of each of the coins. Can we, however, learn anything about how they depend on each other? Consider some pair of marginal probabilities $P(X)$ and $P(Y)$ ; because we never get to observe both coins together, any joint distribution that has these marginals has the same likelihood. In particular, a model where the two coins are independent achieves maximum likelihood but is not the unique point. In fact, in some cases a model where one is a deterministic function of the other also achieves the same likelihood (for example, if we have the same frequency of observed $X$ heads as of observed $Y$ heads). 

These two examples show that in some learning situations we cannot resolve all aspects of the model by learning from data. This issue has been examined extensively in statistics, and is known as identiﬁability , and we brieﬂy review the relevant notions here. 

Suppose w have a parametric model with parameters $\theta\in\Theta$ hat deﬁnes a distribution $P(X\mid\theta)$ over a set X of measurable variables. A choice of parameters θ is identiﬁable if there is $\pmb{\theta}^{\prime}\neq\pmb{\theta}$ such that $P(X\mid\theta)=P(X\mid\theta^{\prime})$ . A model is identiﬁable if all parameter choices θ $\theta\in\Theta$ ∈ are identiﬁable. 

In other words, a model is identiﬁable if each choice of parameters implies a diferent distribution over the observed variables. Non ident i ability implies that there are parameter settings that are indistinguishable given the data, and therefore cannot be identiﬁed from the data. Usually this is a sign that the parameter iz ation is redundant with respect to the actual observations. For example, the model we discuss in example 19.6 is unidentiﬁable, since there are regions in the parameters space that induce the same probability on the observations. Another source of non ident i ability is hidden variables. 

# Example 19.8 

Acme ( A ) and Bond ( B ). In each round, both thumbtacks are tossed and the entries are recorded. Unfortunately, due to scheduling constraints, two diferent experimenters participated in the ex- periment; each used a slightly diferent hand motion, changing the probability of heads and tails. Unfortunately, the experimenter name was not recorded, and thus we only have measurements of the outcome in each experiment. To model this situation, we have three random variables to describe each round. Suppose A denotes the outcome of the toss of the Acme thumbtack and $B$ the outcome of the toss of the Bond thumbtack. Because these outcomes depend on the experimenter, we add another (hidden) variable $H$ that denotes the name of the experimenter. We assume that the model is such that $A$ and $B$ are independent given $H$ . Thus, 

$$
P(A,B)=\sum_{h}P(h)P(A\mid h)P(B\mid h).
$$ 

Because we never observe $H$ , the parameters of this model can be reshufed by “renaming” the values of the hidden variable. If we exchange the roles of $h^{0}$ and $h^{1}$ , and change the corresponding entries in the CPDs, we get a model with exactly the same likelihood, but with diferent parameters. In this case, the likelihood surface is duplicated. For each parameter iz ation, there is an equivalent parameter iz ation by exchanging the names of the hidden variable. We conclude that this model is not identiﬁable. 

This type of un ident i ability exists in any model where we have hidden variables we never observe. When we have several hidden variables, the problem is even worse, and the number of equivalent “reﬂections” of each solution is exponential in the number of hidden variables. 

Although such a model is not identiﬁable due to “renaming” transformations, it is in some sense better than the model of example 19.6, where we had an entire region of equivalent parameter iz at ions. To capture this distinction, we can deﬁne a weaker version of identiﬁability. 

Suppose w ave a parametric model with parameters $\theta\in\Theta$ t at deﬁnes a distribution $P(X\mid\theta)$ over a set X of measurable variables. A choice of parameters θ is locally identiﬁable if there is a constant $\epsilon>0$ such that there is no $\pmb{\theta}^{\prime}\neq\pmb{\theta}$ such that $\lVert\theta-\theta^{\prime}\rVert_{2}<\epsilon$ | | | and $P(X\mid\theta)=P(X\mid\theta^{\prime})$ . A model is locally identiﬁable if all parameter choices θ $\theta\in\Theta$ ∈ are locally identiﬁable. 

In other words, a model is locally identiﬁable if each choice of parameters deﬁnes a distribu- tion that is diferent than the distribution of neighboring parameter iz ation in a sufciently small neighborhood. This deﬁnition implies that, from a local perspective, the model is identiﬁable. The model of example 19.8 is locally identiﬁable, while the model of example 19.6 is not. 

# 

It is interesting to note that we have encountered similar issues before: As we discussed in chapter 18, our data do not allow us to distinguish between structures in the same I-equivalence class. This limitation did not prevent us from trying to learn a model from data, but we needed to avoid ascribing meaning to directionality of edges that are not consistent throughout the I-equivalence class. The same approach holds for un ident i ability due to missing data: A nonidentiﬁable model does not mean that we should not attempt to learn models from data. But it does mean that we should be careful not to read into the learned model more than what can be distinguished given the available data. 

# 19.2 Parameter Estimation 

As for the fully observable case, we ﬁrst consider the parameter estimation task. As with complete data, we consider two approaches to estimation, maximum likelihood estimation (MLE), and Bayesian estimation. We start with a discussion of methods for MLE estimation, and then consider the Bayesian estimation problem in the next section. 

More precisely, suppose we are giv a network structure $\mathcal{G}$ and the form of the CPDs. Thus, we only need to set the parameters θ to deﬁne a distribution $P(\mathcal{X}\mid\theta)$ . We are also given a data set $\mathcal{D}$ that consists of $M$ par $\mathcal{X}$ e want to ﬁnd the values $\hat{\pmb\theta}$ that maximize the log-likelihood function: $\hat{\pmb{\theta}}=\arg\operatorname*{max}_{\pmb{\theta}}\ell(\pmb{\theta}:\mathcal{D})$ . As we discussed, in the presence of incomplete data, the likelihood does not decompose. And so the problem requires optimizing a highly nonlinear and multimodal function over a high-dimensional space (one consisting of parameter assignments to all CPDs). There are two main classes of methods for performing this optimization: a generic nonconvex optimization algorithm, such as gradient ascent; and expectation maximization , a more specialized approach for optimizing likelihood functions. 

# 19.2.1 Gradient Ascent 

gradient ascent One approach to handle this optimization task is to apply some variant of gradient ascent , a standard function-optimization technique applied to the likelihood function (see appendix A.5.2). These algorithms are generic and can be applied if we can evaluate the gradient function at diferent parameter choices. 

# 19.2.1.1 Computing the Gradient 

The main technical question we need to tackle is how to compute the gradient. We begin with considering the derivative relative to a single CPD entry $P(x\mid{\pmb u})$ . We can then use this result as the basis for computing derivatives relative to other parameters, which arise when we have structured CPDs. 

# Lemma 19.1 

Let $\mathcal{B}$ be a Bayesian network with structure $\mathcal{G}$ over $\mathcal{X}$ that ind probability distribution $P$ , let o be a tuple of obserations for some of the variables, and let $X\in{\mathcal{X}}$ ∈X be some random variable. Then 

$$
\frac{\partial}{\partial P(x\mid\mathbf{\nabla}u)}P(o)=\frac{1}{P(x\mid\mathbf{\nabla}u)}P(x,\mathbf{\nabla}u,o)
$$ 

Proof We start by considering the case where the evidence is a full assignment $\xi$ to all variables. The probability of such an assignment is a product of the relevant CPD entries. Thus, the gradient of this product with respect to the parameter $P(x\mid{\pmb u})$ is simply 

$$
\frac{\partial}{\partial P(\boldsymbol{x}\mid\mathbf{u})}P(\boldsymbol{\xi})=\left\{\begin{array}{l l}{\frac{1}{P(\boldsymbol{x}\mid\mathbf{u})}P(\boldsymbol{\xi})}&{\mathrm{if~}\boldsymbol{\xi}\langle\boldsymbol{X},\mathrm{Pa}_{\boldsymbol{X}}\rangle=\langle\boldsymbol{x},\mathbf{u}\rangle}\\ {0}&{\mathrm{otherwise}.}\end{array}\right.
$$ 

We now consider the general case where the evidence is a partial assignment. As usual, we can write $P(o)$ as a sum over all full assignments consistent with $P(o)$ 

$$
P(o)=\sum_{\xi:\xi\langle O\rangle=o}P(\xi).
$$ 

Applying the diferentiation formula to each of these full assignments, we get 

$$
\begin{array}{r c l}{\displaystyle\frac{\partial}{\partial P(x\mid u)}P(o)}&{=}&{\displaystyle\sum_{\xi:\xi\langle O\rangle=o}\frac{\partial}{\partial P(x\mid u)}P(\xi)}\\ &{=}&{\displaystyle\sum_{\xi:\xi\langle O\rangle=o,\xi\langle X,\mathrm{Pa}_{X}\rangle=\langle x,u\rangle}\frac{1}{P(x\mid u)}P(\xi)}\\ &{=}&{\displaystyle\frac{1}{P(x\mid u)}P(x,u,o).}\end{array}
$$ 

![](images/5e2f0f5ef7ea0debb5494151a88673deed58818c19a3e2de7fe8a3884a34e7f1.jpg) 
Figure 19.6 A simple network used to illustrate learning algorithms for missing data 

When $^o$ is inconsistent with $x$ or $\mathbfit{u}$ , then the gradient is 0 , since the probability $P(x,u,o)$ is 0 in this case. When $^o$ is consistent with $x$ and $\mathbfit{u}$ , the gradient is the ratio between the probability $P(x,u,o)$ and the parameter $P(x\mid{\pmb u})$ . Intuitively, this ratio takes into account the weight of the cases where $P(x\mid{\pmb u})$ is “used” in the computation of $P(o)$ . Increasing $P(x\mid{\pmb u})$ by a small amount will increase the probability of these cases by a multiplicative factor. 

The lemma does not deal with the case where $P(x\mid u)=0$ , since we cannot divide by 0 . Note, however, that the proof shows that this division is mainly a neat manner of writing the product of all terms except $P(x\mid{\pmb u})$ . Thus, even in this extreme case we can use a similar proof to compute the gradient, although writing the term explicitly is less elegant. Since in learning we usually try to avoid extreme parameter assignments, we will continue our discussion with the assumption that $P(x\mid u)>0$ . 

An immediate consequence of lemma 19.1 is the form of the gradient of the log-likelihood function. 

Theorem 19.2 Let $\mathcal{G}$ be a Bayesian n work structure ove $\mathcal{X}$ , and let ${\mathcal D}\,=\,\{o[1],\cdot\,\cdot\,,o[M]\}$ be a partially observable data set. Let X be a variable and $U$ its parents in $\mathcal{G}$ . Then 

$$
\frac{\partial\ell(\pmb\theta:\mathcal D)}{\partial P(\mathscr X\mid\pmb u)}=\frac{1}{P(\mathscr X\mid\pmb u)}\sum_{m=1}^{M}P(\mathscr X,\pmb u\mid\mathscr o[m],\pmb\theta).
$$ 

The proof is left as an exercise (exercise 19.5). 

chain rule of derivatives 

This theorem provides the form of the gradient for table-CPDs. For other CPDs, such as noisy-or CPDs, we can use the chain rule of derivatives to compute the gradient. Suppose that the CPD entries of $P(X\mid U)$ are written as functions of some set of parameters $\theta$ . Then, for a speciﬁc parameter $\theta\in\theta$ , we have 

$$
\frac{\partial\boldsymbol{\ell}(\pmb\theta:\mathcal{D})}{\partial\boldsymbol{\theta}}=\sum_{\boldsymbol{x},\pmb{u}}\frac{\partial\boldsymbol{\ell}(\pmb\theta:\mathcal{D})}{\partial P(\boldsymbol{x}\mid\pmb{u})}\frac{\partial P(\boldsymbol{x}\mid\pmb{u})}{\partial\boldsymbol{\theta}},
$$ 

where the ﬁrst term is the derivative of the log-likelihood function when parameterized in terms of the table-CPDs induced by $\theta$ . For structured CPDs, we can use this formula to compute the gradient with respect to the CPD parameters. For some CPDs, however, this may not be the most efcient way of computing these gradients; see exercise 19.4. 

# 19.2.1.2 An Example 

We consider a simple example to clarify the concept. Consider the network shown in ﬁgure 19.6, and a partially speciﬁed data case $o=\langle a^{1},\,\sharp,\,\sharp,d^{\bar{0}}\rangle$ . 

We want to compute the gradient of one family of parameters $P(D\mid c^{0})$ given the observation $^o$ . Using theorem 19.2, we know that 

$$
{\frac{\partial\log P({\boldsymbol{o}})}{\partial P(d^{0}\mid c^{0})}}={\frac{P(d^{0},c^{0}\mid{\boldsymbol{o}})}{P(d^{0}\mid c^{0})}},
$$ 

and similarly for other values of $D$ and $C$ . Assume that our current $\theta$ is: 

$$
\begin{array}{r l}{\theta_{a^{1}}}&{{}=0.3}\\ {\theta_{b^{1}}}&{{}=0.9}\\ {\theta_{c^{1}|a^{0},b^{0}}}&{{}=0.83}\\ {\theta_{c^{1}|a^{0},b^{1}}}&{{}=0.09}\\ {\theta_{c^{1}|a^{1},b^{0}}}&{{}=0.6}\\ {\theta_{c^{1}|a^{1},b^{1}}}&{{}=0.2}\\ {\theta_{d^{1}|c^{0}}}&{{}=0.1}\\ {\theta_{d^{1}|c^{1}}}&{{}=0.8.}\end{array}
$$ 

In this case, the probabilities of the four data cases that are consistent with $^o$ are 

$$
\begin{array}{r l r}{P(\langle a^{1},b^{1},c^{1},d^{0}\rangle)}&{=}&{0.3\cdot0.9\cdot0.2\cdot0.2=0.0108}\\ {P(\langle a^{1},b^{1},c^{0},d^{0}\rangle)}&{=}&{0.3\cdot0.9\cdot0.8\cdot0.9=0.1944}\\ {P(\langle a^{1},b^{0},c^{1},d^{0}\rangle)}&{=}&{0.3\cdot0.1\cdot0.6\cdot0.2=0.0036}\\ {P(\langle a^{1},b^{0},c^{0},d^{0}\rangle)}&{=}&{0.3\cdot0.1\cdot0.4\cdot0.9=0.0108.}\end{array}
$$ 

To compute the posterior probability of these instances given the partial observation $^o$ , we divide the probability of each instance with the total probability, which is 0 . 2196 , that is, 

$$
\begin{array}{l c l}{{P(\langle a^{1},b^{1},c^{1},d^{0}\rangle\mid o)}}&{{=}}&{{0.0492}}\\ {{P(\langle a^{1},b^{1},c^{0},d^{0}\rangle\mid o)}}&{{=}}&{{0.8852}}\\ {{P(\langle a^{1},b^{0},c^{1},d^{0}\rangle\mid o)}}&{{=}}&{{0.0164}}\\ {{P(\langle a^{1},b^{0},c^{0},d^{0}\rangle\mid o)}}&{{=}}&{{0.0492.}}\end{array}
$$ 

Using these computations, we see that 

$$
\begin{array}{r l}{\frac{\partial\log P(o)}{\partial P(d^{1}\mid c^{0})}}&{=\;\;\frac{P(d^{1},c^{0}\mid o)}{P(d^{1}\mid c^{0})}=\frac{0}{0.1}=0}\\ {\frac{\partial\log P(o)}{\partial P(d^{0}\mid c^{0})}}&{=\;\;\frac{P(d^{0},c^{0}\mid o)}{P(d^{0}\mid c^{0})}=\frac{0.8852+0.0492}{0.9}=1.0382}\\ {\frac{\partial\log P(o)}{\partial P(d^{1}\mid c^{1})}}&{=\;\;\frac{P(d^{1},c^{1}\mid o)}{P(d^{1}\mid c^{1})}=\frac{0}{0.8}=0}\\ {\frac{\partial\log P(o)}{\partial P(d^{0}\mid c^{1})}}&{=\;\;\frac{P(d^{0},c^{1}\mid o)}{P(d^{0}\mid c^{1})}=\frac{0.0492+0.0164}{0.2}=0.328.}\end{array}
$$ 

These computations show that we can increase the probability of the observations $^o$ by either increasing $P(d^{0}\mid c^{0})$ or $P(d^{0}\mid c^{1})$ . Moreover, increasing the former parameter will lead to a bigger change in the probability of $^o$ than a similar increase in the latter parameter. 

Now suppose we have an observation $o^{\prime}=\langle a^{0},\!\stackrel{\prime}{,}\!\stackrel{\prime}{,}d^{1}\rangle$ . We can repeat the same computation as before and see that 

$$
{\begin{array}{r l r l}{{\frac{\partial\log P(\,\boldsymbol{\sigma^{\prime}})}{\partial P(d^{1}\mid c^{0})}}}&{=}&{{\frac{P(d^{1},c^{0}\mid\boldsymbol{\sigma^{\prime}})}{P(d^{1}\mid c^{0})}}={\frac{0.2836}{0.1}}=2.8358}\\ {{\frac{\partial\log P(\,\boldsymbol{\sigma^{\prime}})}{\partial P(d^{0}\mid c^{0})}}}&{=}&{{\frac{P(d^{0},c^{0}\mid\boldsymbol{\sigma^{\prime}})}{P(d^{0}\mid c^{0})}}={\frac{0}{0.9}}=0}\\ {{\frac{\partial\log P(\,\boldsymbol{\sigma^{\prime}})}{\partial P(d^{1}\mid c^{1})}}}&{=}&{{\frac{P(d^{1},c^{1}\mid\boldsymbol{\sigma^{\prime}})}{P(d^{1}\mid c^{1})}}={\frac{0.7164}{0.8}}=0.8955}\\ {{\frac{\partial\log P(\,\boldsymbol{\sigma^{\prime}})}{\partial P(d^{0}\mid c^{1})}}}&{=}&{{\frac{P(d^{0},c^{1}\mid\boldsymbol{\sigma^{\prime}})}{P(d^{0}\mid c^{1})}}={\frac{0}{0.2}}=0.}\end{array}}
$$ 

Suppose our data set consists only of these two instances. The gradient of the log-likelihood function is the sum of the gradient with respect to the two instances. We get that 

Note that all the gradients are nonnegative. Thus, increasing any of the parameters in the CPD $P(D\mid C)$ will increase the likelihood of the data. It is clear, however, that we cannot increase both $P(d^{1}\mid c^{0})$ and $P(d^{0}\mid c^{0})$ at the same time, since this will lead to an illegal conditional probability. One way of solving this is to use a single parameter $\theta_{d^{1}|c^{0}}$ and write 

$$
P(d^{1}\mid c^{0})=\theta_{d^{1}\mid c^{0}}\;\;\;\;P(d^{0}\mid c^{0})=1-\theta_{d^{1}\mid c^{0}}.
$$ 

Using the chain rule of conditional probabilities, we have that 

$$
\begin{array}{r l r}{\displaystyle\frac{\partial\ell(\pmb\theta:\mathcal{D})}{\partial\theta_{d^{1}\mid c^{0}}}}&{=}&{\displaystyle\frac{\partial P(d^{1}\mid c^{0})}{\partial\theta_{d^{1}\mid c^{0}}}\frac{\partial\ell(\pmb\theta:\mathcal{D})}{\partial P(d^{1}\mid c^{0})}+\frac{\partial P(d^{0}\mid c^{0})}{\partial\theta_{d^{1}\mid c^{0}}}\frac{\partial\ell(\pmb\theta:\mathcal{D})}{\partial P(d^{0}\mid c^{0})}}\\ &{=}&{\displaystyle\frac{\partial\ell(\pmb\theta:\mathcal{D})}{\partial P(d^{1}\mid c^{0})}-\frac{\partial\ell(\pmb\theta:\mathcal{D})}{\partial P(d^{0}\mid c^{0})}}\\ &{=}&{2.8358-1.0382=1.7976.}\end{array}
$$ 

Thus, in this case, we prefer t increase $P(d^{1}\mid c^{0})$ and decrease $P(d^{0}\mid c^{0})$ , since the resulting increase in the probability of $o^{\prime}$ will be larger than the decrease in the probability of $^o$ . 

![](images/4c0ddaf39c3736c812edd48f1f04e181345b074354bd6a95302fd38b3c56a2dd.jpg) 

# 19.2.1.3 Gradient Ascent Algorithm 

We now generalize these ideas to case of an arbitrary network. For now we focus on the case of table-CPDs. In this case, the gradient is given by theorem 19.2. To compute the gradient for the CPD $P(X\mid U)$ , we need to compute the joint probability of $x$ and $\mathbfit{u}$ relative to our current parameter setting θ and each observed instance $o[m]$ . In other words, we need to compute the joint distribution $P(X[m],U[m]\mid o[m],\theta)$ for each $m$ . We can do this by running an inference procedure for each data case. Importantly, we can do all of the required inference for each data case using one clique tree calibration, since the family preservation property guarantees that $X$ and its parents $U$ will be together in some clique in the tree. Procedure Compute-Gradient , shown in algorithm 19.1, performs these computations. 

Once we have a procedure for computing the gradient, it seems that we can simply plug it into a standard package for gradient ascent and optimize the parameters. As we have illustrated, however, there is one issue that we need to deal with. It is not hard to conﬁrm that all components of the gradient vector are nonnegative. This is natural, since increasing each of the parameters will lead to higher likelihood. Thus, a step in the gradient direction will increase all the parameters. Remember, however, that we want to ensure that our parameters describe a legal probability distribution. That is, the parameters for each conditional probability are nonnegative and sum to one. 

In the preceding example, we saw one approach that works well when we have binary vari- ables. In general networks, there are two common approaches to deal with this issue. The ﬁrst approach is to modify the gradient ascent procedure we use (for example, conjugate gradient) to respect these constraints. First, we must project each gradient vector onto the hyperplane that satisﬁes the linear constraints on the parameters; this step is fairly straightforward (see exercise 19.6). Second, we must ensure that parameters are nonnegative; this requires restricting possible steps to avoid stepping out of the allowed bounds. 

reparameteriza- tion 

The second approach is to reparameterize the problem. Suppose we introduce new parameters $\lambda_{x\mid u}$ , and deﬁne 

$$
P(x\mid u)=\frac{e^{\lambda_{x\mid u}}}{\sum_{x^{\prime}\in V a l(X)}e^{\lambda_{x^{\prime}\mid u}}},
$$ 

for each $X$ and its parents $U$ . Now, any choice of values for the $\lambda$ parameters will lead to legal conditional probabilities. We can compute the gradient of the log-likelihood with respect to the $\lambda$ parameters using the chain rule of partial derivatives, and then use standard (unmodiﬁed) conjugate gradient ascent procedure. See exercise 19.7. 

Lagrange multipliers 

Another way of dealing with the constraints implied by conditional probabilities is to use the method of Lagrange multipliers , reviewed in appendix A.5.3. Applying this method to the optimization of the log-likelihood leads to the method we discuss in the next section, and we defer this discussion; see also exercise 19.8. 

Having dealt with this subtlety, we can now apply any gradient ascent procedure to ﬁnd a local maximum of the likelihood function. As discussed, in most missing value problems, the likelihood function has many local maxima. Unfortunately, gradient ascent procedures are guaranteed to achieve only a local maximum of the function. Many of the techniques we discussed earlier in the book can be used to avoid local maxima and increase our chances of ﬁnding a global maximum, or at least a better local maximum: the general-purpose methods of appendix A.4.2, such as multiple random starting points, or applying random perturbations to convergence points; and the more specialized data perturbation methods of algorithm 18.1. 

# 19.2.2 Expectation Maximization (EM) 

expectation maximization An alternative algorithm for optimizing a likelihood function is the expectation maximization algorithm. Unlike gradient ascent, EM is not a general-purpose algorithm for nonlinear function optimization. Rather, it is tailored speciﬁcally to optimizing likelihood functions, attempting to build on the tools we had for solving the problem with complete data. 

# 19.2.2.1 Intuition 

Recall that when learning from complete data, we can collect sufcient statistics for each CPD. We can then estimate parameters that maximize the likelihood with respect to these statistics. As we saw, in the case of missing data, we do not have access to the full sufcient statistics. Thus, we cannot use the same strategy for our problem. For example, in a simple $X\,\rightarrow\,Y$ network, if we see the training instance $\langle\mathring{z},y^{1}\rangle$ , then we do not know whether to count this instance toward the count $M[x^{1},y^{1}]$ or toward the count $M[x^{0},y^{1}]$ . 

data imputation 

A simple approach is to “ﬁll in” the missing values arbitrarily. For example, there are strategies that ﬁll in missing values with “default values” (say false ) or by randomly choosing a value. Once we ﬁll in all the missing values, we can use standard, complete data learning procedure. Such approaches are called called data imputation methods in statistics. 

The problem with such an approach is that the procedure we use for ﬁlling in the missing values introduces a bias that will be reﬂected in the parameters we learn. For example, if we ﬁll all missing values with false , then our estimate will be skewed toward higher (conditional) probability of false . Similarly, if we use a randomized procedure for ﬁlling in values, then the probabilities we estimate will be skewed toward the distribution from which we sample missing values. This might be better than a skew toward one value, but it still presents a problem. Moreover, when we consider learning with hidden variables, it is clear that an imputation procedure will not help us. The values we ﬁll in for the hidden variable are conditionally independent from the values of the other variables, and thus, using the imputed values, we will not learn any dependencies between the hidden variable and the other variables in the network. 

A diferent approach to ﬁlling in data takes the perspective that, when learning with missing data, we are actually trying to solve two problems at once: learning the parameters, and hypothesizing values for the unobserved variables in each of the data cases. Each of these tasks is fairly easy when we have the solution to the other. Given complete data, we have the statistics, and we can estimate parameters using the MLE formulas we discussed in chapter 17. Conversely, given a choice of parameters, we can use probabilistic inference to hypothesize the likely values (or the distribution over possible values) for unobserved variables. Unfortunately, because we have neither, the problem is difcult. 

data completion 

The EM algorithm solves this “chicken and egg” problem using a bootstrap approach. We start out with some arbitrary starting point. This can be either a choice of parameters, or some initial assignment to the hidden variables; these assignments can be either random, or selected using some heuristic approach. Assuming, for concreteness, that we begin with a parameter assignment, the algorithm then repeats two steps. First, we use our current parameters to complete the data, using probabilistic inference. We then treat the completed data as if it were observed and learn a new set of parameters. 

More precisely, suppose we have a guess $\theta^{0}$ about the parameters of the network. The resulting model deﬁnes a joint distribution over all the variables in the domain. Given a par- tial instance, we can compute the posterior (using our putative parameters) over all possible assignments to the missing values in that instance. The EM algorithm uses this probabilis- tic completion of the diferent data instances to estimate the expected value of the sufcient statistics. It then ﬁnds the parameters $\theta^{1}$ that maximize the likelihood with respect to these statistics. 

Somewhat surprisingly, this sequence of steps provably improves our parameters. In fact, as we will prove formally, unless our parameters have not changed due to these steps (such that $\pmb\theta^{0}=\pmb\theta^{\bar{1}}$ ), our new parameters $\theta^{1}$ necessarily have a higher likelihood than $\theta^{0}$ . But now we can iteratively repeat this process, using $\theta^{1}$ as our new starting point. Each of these operations can be thought of as taking an “uphill” step in our search space. More precisely, we will show (under very benign assumptions) that: each iteration is guaranteed to improve the log-likelihood function; that this process is guaranteed to converge; and that the convergence point is a ﬁxed point of the likelihood function, which is essentially always a local maximum. Thus, the guarantees of the EM algorithm are similar to those of gradient ascent. 

We start with a simple example to clarify the concepts. Consider the simple network shown in ﬁgure 19.6. In the fully observable case, our maximum likelihood parameter estimate for the parameter $\hat{\theta}_{d^{1}|c^{0}}$ is: 

$$
{\hat{\theta}}_{d^{1}|c^{0}}={\frac{M[d^{1},c^{0}]}{M[c^{0}]}}={\frac{\sum_{m=1}^{M}{\bf I}\{\xi[m]\langle D,C\rangle=\langle d^{1},c^{0}\rangle\}}{\sum_{m=1}^{M}{\bf I}\{\xi[m]\langle C\rangle=c^{0}\}}},
$$ 

where $\xi[m]$ is the $m$ ’th training example. In the fully observable case, we knew exactly whether the indicator variables were 0 or 1. Now, however, we do not have complete data cases, so we no longer know the value of the indicator variables. 

Consider a partially sp data case $o=\langle a^{1},\colon,\colon,\colon,d^{0}\rangle$ . There are four possible instantiations $B,C$ which could have given rise to this partial data case: $\langle b^{1},c^{1}\rangle$ , $\langle b^{1},c^{0}\rangle,\;\langle b^{0},c^{1}\rangle,\;\langle b^{0},c^{0}\rangle$ ⟨ ⟩ ⟨ ⟩ ⟨ ⟩ . We do not know which of them is true, or even which of them is more likely. 

However, assume that we have some estimate $\theta$ of the values of the parameters in the model. In this case, we can compute how likely each of these completions is, given our distribution. That is, we can deﬁne a distribution $Q(B,C)=P(B,C\mid o,\theta)$ that induces a distribution over the four data cases. For example, if our parameters θ are: 

$$
\begin{array}{l l l}{{\pmb{\theta}_{a^{1}}}}&{{=0.3}}&{{\qquad\displaystyle{\pmb{\theta}_{b^{1}}}}}&{{=0.9}}\\ {{\pmb{\theta}_{d^{1}|c^{0}}}}&{{=0.1}}&{{\qquad\displaystyle{\pmb{\theta}_{d^{1}|c^{1}}}}}&{{=0.8}}\\ {{\pmb{\theta}_{c^{1}|a^{0},b^{0}}}}&{{=0.83}}&{{\qquad\displaystyle{\pmb{\theta}_{c^{1}|a^{1},b^{0}}}=0.6}}\\ {{\pmb{\theta}_{c^{1}|a^{0},b^{1}}}}&{{=0.09}}&{{\qquad\displaystyle{\pmb{\theta}_{c^{1}|a^{1},b^{1}}}=0.2,}}\end{array}
$$ 

then $Q(B,C)=P(B,C\mid a^{1},d^{0},\theta)$ is deﬁned as: 

$Q(\langle b^{0},c^{0}\rangle)~=~0.3\cdot0.1\cdot0.4\cdot0.9/0.2196=0.0492$ 

where 0 . 2196 is a normalizing f $P(a^{1},d^{0}\mid\theta)$ 

If we have another example $o^{\prime}\,=\,\langle\!\,\!\mathfrak{z},b^{1},\mathfrak{z},d^{1}\rangle$ ⟨ ⟩ . Then $Q^{\prime}(A,C)\,=\,P(A,C\,\mid\,b^{1},d^{1},\theta)$ is deﬁned as: 

$\begin{array}{r l}{Q^{\prime}(\langle a^{1},c^{1}\rangle)~=}&{{}0.3\cdot0.9\cdot0.2\cdot0.8/0.1675=0.2579}\end{array}$ Q ′ ( ⟨ a 1 , c 0 ⟩ ) = $0.3\cdot0.9\cdot0.8\cdot0.1/0.1675=0.1290$ $Q^{\prime}(\langle a^{0},c^{1}\rangle)~=~0.7\cdot0.9\cdot0.09\cdot0.8/0.1675=0.2708$ $Q^{\prime}(\langle a^{0},c^{0}\rangle)~=~0.7\cdot0.9\cdot0.91\cdot0.1/0.1675=0.3423.$ 

weighted data instances 

Intuitively, now that we have estimates for how likely each of the cases is, we can treat these estimates as truth. That is, we view our partially observed data case $\langle a^{1},\cdot,\cdot,\cdot,d^{0}\rangle$ as consisting of four complete data cases, each of which has some weight lower than 1. The weights correspond to our estimate, based on our current parameters, on how likely is this particular completion of the partial instance. (This approach is somewhat reminiscent of the weighted particles in the likelihood weighting algorithm.) Importantly, as we will discuss, we do not usually explicitly generate these completed data cases; however, this perspective is the basis for the more sophisticated methods. 

More generally, let $H[m]$ denote the variables whose values are missing in the data instance $o[m]$ . We now have a data set $\mathcal{D}^{+}$ consisting of 

$$
\cup_{m}\{\langle o[m],h[m]\rangle\ :\ h[m]\in V a l(H[m])\},
$$ 

where each data case $\langle o[m],h[m]\rangle$ has weight $Q(h[m])=P(h[m]\mid o[m],\theta)$ . 

expected sufcient statistics 

We can now do standard maximum likelihood estimation using these completed data cases. We compute the expected sufcient statistics : 

$$
\bar{M}_{\pmb\theta}[{\pmb y}]=\sum_{m=1}^{M}\sum_{{\pmb h}[m]\in V a l({\pmb H}[m])}Q({\pmb h}[m]){\pmb I}\{\xi[m]\langle{\pmb Y}\rangle={\pmb y}\}.
$$ 

We then use these expected sufcient statistics as if they were real in the MLE formula. For example: 

$$
\tilde{\theta}_{d^{1}|c^{0}}={\frac{\bar{M}_{\theta}[d^{1},c^{0}]}{\bar{M}_{\theta}[c^{0}]}}.
$$ 

In our example, suppose the data consist of the two instances $o\;=\;\langle a^{1},\,\sharp,\,\sharp,d^{0}\rangle$ and $o^{\prime}\,=$ $\langle\mathcal{G},b^{1},\mathcal{G},d^{1}\rangle$ . Then, using the calculated $Q$ and $Q^{\prime}$ from above, we have that 

$$
\begin{array}{r c l}{{\bar{M}_{\theta}[d^{1},c^{0}]}}&{{=}}&{{Q^{\prime}(\langle a^{1},c^{0}\rangle)+Q^{\prime}(\langle a^{0},c^{0}\rangle)}}\\ {{}}&{{=}}&{{0.1290+0.3423=0.4713}}\\ {{\bar{M}_{\theta}[c^{0}]}}&{{=}}&{{Q(\langle b^{1},c^{0}\rangle)+Q(\langle b^{0},c^{0}\rangle)+Q^{\prime}(\langle a^{1},c^{0}\rangle)+Q^{\prime}(\langle a^{0},c^{0}\rangle)}}\\ {{}}&{{=}}&{{0.8852+0.0492+0.1290+0.3423=1.4057.}}\end{array}
$$ 

Thus, in this example, using these particular parameters to compute expected sufcient statistics, we get 

$$
\tilde{\pmb{\theta}}_{d^{1}|c^{0}}=\frac{0.4713}{1.4057}=0.3353.
$$ 

Note that this estimate is quite diferent from the parameter $\theta_{d^{1}|c^{0}}=0.1$ that we used in our estimate of the expected counts. The initial parameter and the estimate are diferent due to the incorporation of the observations in the data. 

This intuition seems nice. However, it may require an unreasonable amount of computation. To compute the expected sufcient statistics, we must sum over all the completed data cases. The number of these completed data cases is much larger than the original data set. For each $o[m]$ , the number of completions is exponential in the number of missing values. Thus, if we have more than few missing values in an instances, an implementation of this approach will not be able to ﬁnish computing the expected sufcient statistics. 

Fortunately, it turns out that there is a better approach to computing the expected sufcient statistic than simply summing over all possible completions. Let us reexamine the formula for an expected sufcient statistic, for example, $\bar{M}_{\theta}[c^{1}]$ . We have that 

$$
\bar{M}_{\pmb\theta}[c^{1}]=\sum_{m=1}^{M}\sum_{\pmb h[m]\in V a l(\pmb H[m])}Q(\pmb h[m])\pmb I\{\xi[m]\langle C\rangle=c^{1}\}.
$$ 

Let us consider the internal summation, say for a data case $o\,=\,\langle a^{1},\,\sharp,\,\sharp,d^{0}\rangle$ . We have four possible completions, as before, but we are only summing over the two that are consistent with $c^{1}$ , that is, $Q(b^{1},c^{1})+Q(b^{0},c^{1})$ . This expression is equal to $Q(c^{1})\,=\,P(c^{1}\,\mid\,a^{1},d^{0},\theta)\,=$ $P(c^{1}\mid o[1],\theta)$ . This idea clearly generalizes to our other data cases. Thus, we have that 

$$
\bar{M}_{\pmb\theta}[c^{1}]=\sum_{m=1}^{M}P(c^{1}\mid\pmb o[m],\pmb\theta).
$$ 

Now, recall our formula for sufcient statistics in the fully observable case: 

$$
M[c^{1}]=\sum_{m=1}^{M}{I\!\!\!/}\{\xi[m]\langle C\rangle=c^{1}\}.
$$ 

Our new formula is identical, except that we have substituted our indicator variable — either 0 or $1-$ with a probability that is somewhere between 0 and 1. Clearly, if in a certain data case we get to observe $C$ , the indicator variable and the probability are the same. Thus, we can view the expected sufcient statistics as ﬁlling in soft estimates for hard data when the hard data are not available. 

We stress that we use posterior probabilities in computing expected sufcient statistics. Thus, although our choice of $\theta$ clearly inﬂuences the result, the data also play a central role. This is in contrast to the probabilistic completion we discussed earlier that used a prior probability to ﬁll in values, regardless of the evidence on the other variables in the same instances. 

# 19.2.2.3 The EM Algorithm for Bayesian Networks 

We now present the basic EM algorithm and describe the guarantees that it provides. 

Networks with Table-CPDs Consider the application of the EM algorithm to a general Bayesian network with table-CPDs. Assume that the algorithm begins with some initial parameter assign- ment $\theta^{0}$ , which can be chosen either randomly or using some other approach. (The case where we begin with some assignment to the missing data is analogous.) The algorithm then repeatedly executes the following phases, for $t=0,1,\ldots.$ 

expected sufcient statistics 

Expectation (E-step): The algorithm uses the current parameters $\theta^{t}$ to compute the expected sufcient statistics . 

• For each data case $o[m]$ and each family $X,U$ , compute the joint distribution $P(X,U\mid$ $o[m],\pmb\theta^{t})$ .

 • Compute the expected sufcient statistics for each $x,u$ as: $\bar{M}_{\pmb\theta^{t}}[x,\pmb u]=\sum_{m}{P(x,\pmb u\mid o[m],\pmb\theta^{t})}.$ 

E-step This phase is called the $E$ -step ( expectation step ) because the counts used in the formula are the expected sufcient statistics, where the expectation is with respect to the current set of parameters. 

![](images/4536f8bdd85cc4e588e3bfd94fc6791a6fdd8d9c970f7e0a6ba54dc3c8911fae.jpg) 

Maximization (M-step): Treat the expected sufcient statistics as observed, and perform maximum likelihood estimation, with respect to them, to derive a new set of parameters. In other words, set 

$$
\theta_{x|u}^{t+1}=\frac{\bar{M}_{\theta^{t}}[x,u]}{\bar{M}_{\theta^{t}}[u]}.
$$ 

This phase is called the $M\cdot$ -step ( maximization step ), because we are maximizing the likelihood relative to the expected sufcient statistics. 

A formal version of the algorithm is shown fully in algorithm 19.2. 

The maximization step is straightforward. The more difcult step is the expectation step. How do we compute expected sufcient statistics? We must resort to Bayesian network inference over the network $\langle\mathcal{G},\pmb{\theta}^{\bar{t}}\rangle$ . Note that, as in the case of gradient ascent, the only expected sufcient statistics that we need involve a variable and its parents. Although one can use a variety of diferent inference methods to perform the inference task required for the E-step, we can, as in the case of gradient ascent, use the clique tree or cluster graph algorithm. Recall that the family-preservation property guarantees that $X$ and its parents $U$ will be together in some cluster in the tree or graph. Thus, once again, we can do all of the required inference for each data case using one run of message-passing calibration. 

exponential family 

General Exponential Family $\star$ The same idea generalizes to other distributions where the likelihood has sufcient statistics, in particular, all models in the exponential family (see deﬁni- tion 8.1). Recall that such families have a sufcient statistic function $\tau(\xi)$ that maps a complete instance to a vector of sufcient statistics. When learning parameters of such a model, we can summarize the data using the sufcient statistic function $\tau$ . For a complete data set $\mathcal{D}^{+}$ , we deﬁne 

$$
\tau(\mathcal{D}^{+})=\sum_{m}\tau(o[m],h[m]).
$$ 

We can now deﬁne the same E and M-steps described earlier for this more general case. 

E-step 

expected sufcient statistics 

M-step 

Expectation $^ Ḋ E Ḍ$ -step ): For each data case $o[m]$ , the algorithm uses the current parameters $\theta^{t}$ to deﬁne a model, and a posterior distribution: 

$$
Q(H[m])=P(H[m]\mid o[m],\pmb\theta^{t}).
$$ 

It then uses inference in this distribution to compute the expected sufcient statistics : 

$$
{\pmb E}_{Q}[\tau(\langle{\mathcal D},{\mathcal H}\rangle)]=\sum_{m}{\pmb E}_{Q}[\tau(o[m],{\pmb h}[m])].
$$ 

Maximization ( M-step ): As in the case of table-CPDs, once we have the expected sufcient statistics, the algorithm treats them as if they were real and uses them as the basis for maximum likelihood estimation, using the appropriate form of the ML estimator for this family. 

Convergence Results Somewhat surprisingly, this simple algorithm can be shown to have several important properties. We now state somewhat simpliﬁed versions of the relevant results, deferring a more precise statement to the next section. 

The ﬁrst result states that each iteration is guaranteed to improve the log-likelihood of the current set of parameters. 

Theorem 19.3 

$$
\ell(\pmb\theta^{t}:\mathcal D)\leq\ell(\pmb\theta^{t+1}:\mathcal D).
$$ 

Thus, the EM procedure is constantly increasing the log-likelihood objective function. Because the objective function can be shown to be bounded (under mild assumptions), this procedure is guaranteed to converge. By itself, this result does not imply that we converge to a maximum of the objective function. Indeed, this result is only “almost true”: 

![](images/77ad2b52c0e0eefa105f5711e34e4f8d1f0c07b4d1cfe1eae63c8266521249b4.jpg) 
Figure 19.7 The naive Bayes clustering model. In this model each observed variables $X_{i}$ is independent of the other observed variables given the value of the (unobserved) cluster variable $C$ . 

# Theorem 19.4 

# 

Suppose that $\theta^{t}$ is such that ${\pmb\theta}^{t+1}={\pmb\theta}^{t}$ during EM, and $\theta^{t}$ is also an interior point of the allowed parameter space. Then $\theta^{t}$ is a stationary point of the log-likelihood function. 

This result shows that EM converges to a stationary point of the likelihood function. Recall that a stationary point can be a local maximum, local minimum, or a saddle point. Although it seems counter intuitive that by taking upward steps we reach a local minimum, it is possible to construct examples where EM converges to such a point. However, nonmaximal convergence points can only be reached from very speciﬁc starting points, and are moreover not stable, since even small perturbations to the parameters are likely to move the algorithm away from this point. Thus, in practice, EM generally converges to a local maximum of the likelihood function. 

# 19.2.2.4 Bayesian Clustering Using EM 

clustering 

Bayesian clustering 

mixture distribution naive Bayes One important application of learning with incomplete data, and EM in particular, is to the problem of clustering . Here, we have a set of data points in some feature space $X$ . Let us even assume that they are fully observable. We want to classify these data points into coherent categories, that is, categories of points that seem to share similar statistical properties. 

The Bayesian clustering paradigm views this task as a learning problem with a single hidden variable $C$ that denotes the category or class from which an instance comes. Each class is associated with a probability distribution over the features of the instances in the class. In most cases, we assume that the instances in each class $c$ come from some coherent, fairly simple, distribution. In other words, we postulate a particular form for the class-conditional distribution $P(\pmb{x}\mid c)$ . For example, in the case of real-valued data, we typically assume that the class-conditional distribution is a multivariate Gaussian (see section 7.1). In discrete settings, we typically assume that the class-conditional distribution is a naive Bayes structure (section 3.1.3), where each feature is independent of the rest given the class variable. Overall, this approach views the data as coming from a mixture distribution and attempts to use the hidden variable to separate out the mixture into its components. 

Suppose we consider the case of a naive Bayes model (ﬁgure 19.7) where the hidden class variable is the single parent of all the observed feature. In this particular learning scenario, the E-step involves computing the probability of diferent values of the class variables for each instance. Thus, we can think of EM as performing a soft classiﬁcation of the instances, that is, each data instance belongs, to some degree, to multiple classes. 

In the M-step we compute the parameters for the CPDs in the form $P(X\mid C)$ and the prior $P(C)$ over the classes. These estimates depends on our expected sufcient statistics. These are: 

$$
\begin{array}{r c l}{{\bar{M}_{\theta}[c]}}&{{\leftarrow}}&{{\displaystyle\sum_{m}P(c\mid x_{1}[m],\ldots,x_{n}[m],\theta^{t})}}\\ {{\bar{M}_{\theta}[x_{i},c]}}&{{\leftarrow}}&{{\displaystyle\sum_{m}P(c,x_{i}\mid x_{1}[m],\ldots,x_{n}[m],\theta^{t}).}}\end{array}
$$ 

We see that an instance helps determine the parameters for all of the classes that it participates in (that is, ones where $P(c\mid\pmb{x}[m])$ is bigger than 0 ). Stated a bit diferently, each instance “votes” about the parameters of each cluster by contributing to the statistics of the conditional distribution given that value of the cluster variable. However, the weight of this vote depends on the probability with which we assign the instance to the particular cluster. 

Once we have computed the expected sufcient statistics, the M-step is, as usual, simple. The parameters for the class variable CPD are 

$$
\theta_{c}^{t+1}\gets\frac{\bar{M}_{\theta}[c]}{M},
$$ 

and for the conditional CPD are 

$$
\theta_{x_{i}|c}^{t+1}\leftarrow\frac{\bar{M}_{\theta}[x_{i},c]}{\bar{M}_{\theta}[c]}.
$$ 

We can develop similar formulas for the case where some of the observed variables are continuous, and we use a conditional Gaussian distribution (a special case of deﬁnition 5.15) to model the CPD $P(X_{i}\mid C)$ . The application of EM to this speciﬁc model results in a simple and efcient algorithm. 

We can think of the clustering problem with continuous observations from a geometrical per- spective, where each observed variable $X_{i}$ represents one coordinate, and instances correspond to points. The parameters in this case represent the distribution of coordinate values in each of the classes. Thus, each class corresponds to a cloud of points in the input data. In each iteration, we reestimate the location of these clouds. In general, depending on the particular starting point, EM will proceed to assign each class to a dense cloud. 

hard-assignment EM 

The EM algorithm for clustering uses a “soft” cluster assignment, allowing each instance to contribute part of its weight to multiple clusters, proportionately to its probability of belonging to each of them. As another alternative, we can consider “hard clustering,” where each instance contributes all of its weight to the cluster to which it is most likely to belong. This variant, called hard-assigment EM proceeds by performing the following steps. 

• Given par eters $\theta^{t}$ , we assign $c[m]\;=\;\arg\operatorname*{max}_{c}P(c\;\vert\;\,\pmb{x}[m],\pmb{\theta}^{t})$ for each instance $m$ . If we let H $\mathcal{H}^{t}$ comprise all of the assignments $c[m]$ , this results in a complete data set $(\mathcal{D}^{+})^{t}=\langle\mathcal{D},\mathcal{H}^{t}\rangle$ .

 • Set $\begin{array}{r}{\pmb{\theta}^{t+1}=\arg\operatorname*{max}_{\pmb{\theta}}\ell(\pmb{\theta}:(\mathcal{D}^{+})^{t})}\end{array}$ . This step requires collecting sufcient statistics from $(\mathcal{D}^{+})^{t}$ , and then choosing MLE parameters based on these. 

This approach is often used where the class-conditional distributions $P(X\mid c)$ are all “round” Gaussian distributions with unit variance. Thus, each class $c$ has its own mean vector $\pmb{\mu}_{c},$ but a unit covariance matrix. In this case, the most likely class for an instance $_{_{x}}$ is simply the k-means class $c$ such that the Euclidean distance between $_{_{x}}$ and $\mu_{c}$ is smallest. In other words, each point gravitates to the class to which it is “closest.” The reestimation step is also simple. It simply selects the mean of the class to be at the center of the cloud of points that have aligned themselves with it. This process iterates until convergence. This algorithm is called $k$ -means . 

Although hard-assignment EM is often used for clustering, it can be deﬁned more broadly; we return to it in greater detail in section 19.2.2.6. 

collaborative ﬁltering 

Box 19.A — Case Study: Discovering User Clusters. In box 18.C, we discussed the collabora- tive ﬁltering problem, and the use of Bayesian network structure learning to address it. A diferent application of Bayesian network learning to the collaborative ﬁltering data task, proposed by Breese et al. (1998), utilized a Bayesian clustering approach. Here, one can introduce a cluster variable $C$ denoting subpopulations of customers. In a simple model, the individual purchases $X_{i}$ of each user are taken to be conditionally independent given the user’s cluster assignment $C$ . Thus, we have $^a$ naive Bayes clustering model, to which we can apply the EM algorithm. (As in box $l8.C,$ items $i$ that the user did not purchase are assigned $X_{i}=x_{i}^{0}$ .) 

This learned model can be used in several ways. Most obviously, we can use inference to compute the probability that the user will purchase item $i$ , given a set of purchases $S$ . Empirical studies show that this approach achieves lower performance than the structure learning approach of box 18.C, probably because the “user cluster” variable simply cannot capture the complex preference patterns over a large number of items. However, this model can provide signiﬁcant insight into the types of users present in a population, allowing, for example, a more informed design of advertising campaigns. 

As one example, Bayesian clustering was applied to a data set of people browsing the MSNBC website. Each article was associated with a binary random variable $X_{i}$ , which took the value $x_{i}^{1}$ if the user followed the link to the article. Figure 19.A.1 shows the four largest clusters produced by Bayesian clustering applied to this data set. Cluster 1 appears to represent readers of commerce and technology news (a large segment of the reader population at that period, when Internet news was in its early stages). Cluster 2 are people who mostly read the top-promoted stories in the main page. Cluster 3 are readers of sports news. In all three of these cases, the user population was known in advance, and the website contained a page targeting these readers, from which the articles shown in the table were all linked. The fourth cluster was more surprising. It appears to contain readers interested in “softer” news. The articles read by this population were scattered all over the website, and users often browsed several pages to ﬁnd them. Thus, the clustering algorithm revealed an unexpected pattern in the data, one that may be useful for redesigning the website. 

# 19.2.2.5 Theoretical Foundations $\star$ 

So far, we used an intuitive argument to derive the details of the EM algorithm. We now formally analyze this algorithm and prove the results regarding its convergence properties. 

At each iteration, EM maintains the “current” set of parameters. Thus, we can view it as a local learning algorithm. Each iteration amounts to taking a step in the parameter space from 

Cluster 1 (36 percent) Cluster 2 (29 percent) E-mail delivery isn’t exactly guaranteed 757 Crashes at sea Should you buy a DVD player? Israel, Palestinians agree to direct talks Price low, demand high for Nintendo Fuhrman pleads innocent to perjury Cluster 3 (19 percent) Cluster 4 (12 percent) Umps refusing to work is the right thing The truth about what things cost Cowboys are reborn in win over eagles Fuhrman pleads innocent to perjury Did Orioles spend money wisely? Real astrology 

Figure 19.A.1 — Application of Bayesian clustering to collaborative ﬁltering. Four largest clusters found by Bayesian clustering applied to MSNBC news browsing data. For each cluster, the table shows the three news articles whose probability of being browsed is highest. 

$\theta^{t}$ to $\pmb{\theta}^{t+1}$ . This is similar to gradient-based algorithms, except that in those algorithms we have good understanding of the nature of the step, since each step attempts to go uphill in the steepest direction. Can we ﬁnd a similar justiﬁcation for the EM iterations? 

The basic outline of the analysis proceeds as follows. We will show that each iteration can be viewed as maximizing an auxiliary function , rather than the actual likelihood function. The choice of auxiliary function depends on the current parameters at the beginning of the iteration. The auxiliary function is nice in the sense that it is similar to the likelihood function in complete data problems. The crucial part of the analysis is to show how the auxiliary function relates to the likelihood function we are trying to maximize. As we will show, the relation is such that we can show that the parameters that maximize the auxiliary function in an iteration also have better likelihood than the parameters with which we started the iteration. 

The Expected Log-Likelihood Function Assume we are given a data set $\mathcal{D}$ that consists of partial observations. Recall t $\mathcal{H}$ denotes a possible assignm r data set. The combination of D $\mathcal{D},\mathcal{H}$ H deﬁnes a complete data set D $\mathcal{D}^{+}=\langle\mathcal{D},\mathcal{H}\rangle=\{o[m],h[m]\}_{m}$ ⟨D H⟩ { } , where in each instance we now have a full assignment to all the variables. We denote by $\ell(\pmb\theta:\langle\mathcal D,\mathcal H\rangle)$ the log-likelihood of the parameters $\theta$ ith respect to this completed data set. 

Suppose we are not sure abou he true value of H . Rather, we have a probabilistic timate that e denote by a distribution Q that assigns a probability to each possible value of H . Note that Q is a joint distribution over full assignments to all of the missing values in the entire data set. Thus, for example, in our earlier network, if $\mathcal{D}$ contains two instances $o[1]=\langle a^{1},\hat{\imath},\hat{\imath},d^{0}\rangle$ and $o[2]=\langle\P,b^{1},\P,d^{1}\rangle$ , then $Q$ is a joint distribution over $B[1],C[1],A[2],C[2]$ . 

expected log-likelihood 

In the fully observed case, our score for a set of parameters was the log-likelihood. In this case, given $Q$ , we can use it to deﬁne an average score, which takes into account the diferent possible completions of the data and their probabilities. Speciﬁcally, we deﬁne the expected log-likelihood as: 

$$
E_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]=\sum_{\mathcal{H}}Q(\mathcal{H})\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)
$$ 

This function has appealing characteristics that are important in the derivation of EM. 

The ﬁrst key property is a consequence of the linearity of expectation. Recall that when learning table-CPDs, we showed that 

$$
\ell(\pmb\theta:\langle\mathcal D,\mathcal H\rangle)=\sum_{i=1}^{n}\sum_{(x_{i},\mathbf{u}_{i})\in V a l(X_{i},\mathrm{Pa}_{X_{i}})}M_{\langle\mathcal D,\mathcal H\rangle}[x_{i},\pmb u_{i}]\log\theta_{x_{i}|\pmb u_{i}}.
$$ 

Because the only terms in this sum that depend on $\langle\mathcal{D},\mathcal{H}\rangle$ are the counts $M_{\langle\mathcal{D},\mathcal{H}\rangle}[x_{i},\mathbf{u}_{i}]$ , and these appear within a linear function, we can use linearity of expectations to show that 

$$
E_{Q}[\ell(\pmb\theta:\langle\mathscr{D},\mathscr{H}\rangle)]=\sum_{i=1}^{n}\sum_{(x_{i},\pmb u_{i})\in V a l(X_{i},\mathrm{Pa}_{X_{i}})}\pmb E_{Q}\big[M_{\langle\mathscr{D},\mathscr{H}\rangle}[x_{i},\pmb u_{i}]\big]\log\theta_{x_{i}|\pmb u_{i}}.
$$ 

If we now generalize our notation to deﬁne 

$$
\bar{M}_{Q}[x_{i},\mathbf{u}_{i}]=E_{\mathcal{H}\sim Q}\left[M_{\langle\mathcal{D},\mathcal{H}\rangle}[x_{i},\mathbf{u}_{i}]\right]
$$ 

we obtain 

$$
E_{Q}[\ell(\pmb\theta:\langle\mathcal D,\mathcal H\rangle)]=\sum_{i=1}^{n}\sum_{(x_{i},\pmb u_{i})\in V a l(X_{i},\mathrm{Pa}_{X_{i}})}\bar{M}_{Q}[x_{i},\pmb u_{i}]\log\theta_{x_{i}|\pmb u_{i}}.
$$ 

This expression has precisely the same form as the log-likelihood function in the complete data case, but using the expected counts rather than the exact full-data counts. The implication is that instead of summing over all possible completions of the data, we can evaluate the expected log-likelihood based on the expected counts. 

The crucial point here is that the log-likelihood function of complete data is linear in the counts. This allows us to use linearity of expectations to write the expected likelihood as a function of the expected counts. 

The same idea generalizes to any model in the exponential family, which we deﬁned in chapter 8. Recall that a model is in the exponential family if we can write: 

$$
P(\xi\mid\pmb\theta)=\frac{1}{Z(\pmb\theta)}A(\xi)\exp\left\{\langle\mathfrak t(\pmb\theta),\tau(\xi)\rangle\right\},
$$ 

where $\langle\cdot,\cdot\rangle$ is the inner product, $A(\xi),\,\mathtt{t}(\theta)$ , and $Z(\theta)$ are functions that deﬁne the family, and $\tau(\xi)$ is the sufcient statistics function that maps a complete instance to a vector of sufcient statistics. 

As discussed in section 17.2.5, when learning parameters of such a model, we can summarize the data using the sufcient statistic function $\tau$ . We deﬁne 

$$
\tau(\langle\mathcal{D},\mathcal{H}\rangle)=\sum_{m}\tau(o[m],h[m]).
$$ 

Because the model is in the exponential family, we can write the log-likelihood $\ell(\pmb\theta:\langle\mathcal D,\mathcal H\rangle)$ as a linear function of $\tau(\langle\mathcal{D},\mathcal{H}\rangle)$ 

$$
\ell(\pmb\theta:\langle\mathscr{D},\mathcal{H}\rangle)=\langle\mathrm t(\pmb\theta),\tau(\langle\mathscr{D},\mathcal{H}\rangle)\rangle+\sum_{m}\log A(\pmb\theta[m],\pmb h[m])-M\log Z(\pmb\theta).
$$ 

Using the linearity of expectation, we see that 

$$
E_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]=\langle\mathrm{t}(\pmb{\theta}),\pmb{E}_{Q}[\tau(\langle\mathcal{D},\mathcal{H}\rangle)]\rangle+\sum_{m}\pmb{E}_{Q}[\log A(o[m],\pmb{h}[m])]-M\log(m\delta).
$$ 

Because $A(o[m],h[m])$ does not depend on the choice of $\theta$ , we can ignore it. We are left with maximizing the function: 

$$
E_{Q}[\ell(\pmb\theta:\langle\mathscr{D},\mathscr{H}\rangle)]=\langle\mathfrak t(\pmb\theta),\pmb E_{Q}[\tau(\langle\mathscr{D},\mathscr{H}\rangle)]\rangle-M\log Z(\pmb\theta)+\mathrm{const}.
$$ 

In summary, the derivation here is directly analogous to the one for table-CPDs. The expected log-likelihood is a linear function of the expected sufcient statistics $E_{Q}[\tau(\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ . We can compute these as in equation (19.4), by aggregating their expectation in each instance in the training data. Now, maximizing the right-hand side of equation (19.6) is equivalent to maximum likelihood estimation in a complete data set where the sum of the sufcient statistics coincides with the expected sufcient statistics $E_{Q}[\tau(\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ . These two steps are exactly the E- step and M-step we take in each iteration of the EM procedure shown in algorithm 19.2. In the procedure, the distribution $Q$ that we are using is $P(\bar{\mathcal{H}}\mid\mathcal{D},\theta^{t})$ . Because instances are assumed to be independent given the parameters, it follows that 

$$
P(\mathcal{H}\mid\mathcal{D},\pmb{\theta}^{t})=\prod_{m}P(\pmb{h}[m]\mid\pmb{o}[m],\pmb{\theta}^{t}),
$$ 

where $h[m]$ are the missing variables in the $m$ ’th data instance, and $o[m]$ are the observations in the $m$ ’th instance. Thus, we see that in the $t$ ’th iteration of the EM procedure, we choose $\pmb{\theta}^{t+1}$ to be the ones that maximize $E_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ with $Q(\mathcal{H})=P(\bar{\mathcal{H}}\mid\mathcal{D},\theta^{t})$ . This discussion allows us to understand a single iteration as an (implicit) optimization step of a well-deﬁned target function. 

Choosing $Q$ The discussion so far has showed that we can use properties of exponential models to efciently maximize the expected log-likelihood function. Moreover, we have seen that the $t$ ’th EM iteration can be viewed as maximizing $E_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ where $Q$ is the conditional probability $P(\mathcal{H}\mid\mathcal{D},\theta^{t})$ . This discussion, however, does not provide us with guidance as to why we choose this particular auxiliary distribution $Q$ . Note that each iteration uses a diferent $Q$ distribution, and thus we cannot relate the optimization taken in one iteration to the ones made in the subsequent one. We now show why the choice $Q(\mathcal{H})=P(\mathcal{H}\mid\mathcal{D},\boldsymbol{\theta}^{t})$ allows us to prove that each EM iteration improves the likelihood function. 

To do this, we will deﬁne a new function that will be the target of our optimization. Recall that our ultimate goal is to maximize the log-likelihood function. The log-likelihood is a function only of $\theta$ ; however, in intermediate steps, we also have the current choice of $Q$ . Therefore, we will deﬁne a new function that accounts for both $\theta$ and $Q$ , and view each step in the algorithm as maximizing this function. 

We already encountered a similar problem in our discussion of approximate inference in chapter 11. Recall that in that setting we had a known distribution $P$ and attempted to ﬁnd an approximating distribution $Q$ . This problem is similar to the one we face, except that in learning we also change the parameters of target distribution $P$ to maximize the probability of the data. 

Let us brieﬂy summarize the main idea that we used in chapter 11. Suppose that $P=\tilde{P}/Z$ is some distribution, where $\tilde{P}$ is an unnormalized part of the distribution, speciﬁed by a product energy functional of factors, and $Z$ is the partition function that ensures that $P$ sums up to one. We deﬁned the energy functional as 

$$
F[P,Q]=E_{Q}\Bigl[\log\tilde{P}\Bigr]+H_{Q}(\mathcal{X}).
$$ 

We then showed that the logarithm of the partition function can be rewritten as: 

$$
\log Z=F[P,Q]+D(Q\|P).
$$ 

How does this apply to the case of learning from missing data? We can choose 

$$
P(\mathcal{H}\mid\mathcal{D},\pmb\theta)=P(\mathcal{H},\mathcal{D}\mid\pmb\theta)/P(\mathcal{D}\mid\pmb\theta)
$$ 

as our d tion over $\mathcal{H}$ (we hold $\mathcal{D}$ and $\theta$ ﬁxed or now). With this choice, the partition func $Z(\theta)$ likelihood $P({\mathcal{D}}\mid\theta)$ and $\tilde{P}$ is the joint probability $P({\mathcal{H}},{\mathcal{D}}\mid{\boldsymbol{\theta}})$ , so that $\log\tilde{P}=\check{\ell}(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)$ ⟨D H⟩ . Rewriting the energy functional for this new setting, we obtain: 

$$
F_{\mathcal{D}}[\pmb{\theta},Q]=\pmb{E}_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]+\pmb{H}_{Q}(\mathcal{H}).
$$ 

expected log-likelihood 

Corollary 19.1 Note that the ﬁrst term is precisely the expected log-likelihood relative to $Q$ . Applying our earlier analysis, we now can prove 

$$
\begin{array}{l l l}{\ell(\pmb\theta:\mathcal D)}&{=}&{F_{\mathcal D}[\pmb\theta,Q]+\pmb D(Q(\mathcal H)\|P(\mathcal H\mid\mathcal D,\pmb\theta))}\\ &{=}&{\pmb E_{Q}[\ell(\pmb\theta:\langle\mathcal D,\mathcal H\rangle)]+\pmb H_{Q}(\mathcal H)+\pmb D(Q(\mathcal H)\|P(\mathcal H\mid\mathcal D,\pmb\theta)).}\end{array}
$$ 

data completion Both equalities have important ramiﬁcations. Starting from the second equality, since both the entropy $H_{Q}(\mathcal{H})$ H and the relative entropy $D(Q({\mathcal{H}})\|P({\mathcal{H}}\mid{\mathcal{D}},\theta))$ H | | H | D are nonnegative, we conclude that the expected log-likelihood $E_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ wer bound on $\ell(\pmb\theta:\mathcal D)$ . This result is choice of distribution $Q$ . If we select $Q(\mathcal{H})$ H to be the data completion distribution $P(\mathcal{H}\mid\mathcal{D},\boldsymbol{\theta})$ H | D , the relative entropy term becomes zero. In this case, the remaining term $H_{Q}({\mathcal{H}})$ H captures to a certain extent the diference between the expected log-likelihood and the real log-likelihood. Intuitively, when $Q$ is close to being deterministic, the expected value is close to the actual value. 

The ﬁrst equality, for the same reasons, shows that, for any distribution $Q$ , the $F$ function is a lower bound on the log-likelihood. Moreover, this lower bound is tight for every choice of $\theta$ : if w choose $Q=P(\mathcal{H}\mid\mathcal{D},\theta)$ , the two functions have the same value. Thus, if we maximize the F function, we are bound to maximize the log-likelihood. 

coordinate ascent 

There many possible ways to optimize this target function. We now show that the EM procedure we described can be viewed as implicitly optimizing the EM functional $F$ using a particular optimization strategy. The strategy we are going to utilize is a coordinate ascent optimization. We start with some choice $\theta$ of parameters. We then search for $Q$ that maximizes $F_{\mathcal{D}}[\pmb{\theta},Q]$ while keeping $\theta$ ﬁxed. Next, we ﬁx $Q$ and search for parameters that maximize $F_{\mathcal{D}}[\pmb{\theta},Q]$ . We continue in this manner until convergence. We now consider each of these steps. 

• Optimizing $Q$ . Suppose that $\theta$ are ﬁxed, and we are searching for arg max Q $F_{\mathcal{D}}[\pmb{\theta},Q]$ . Using corollary 19.1, we know that, if $Q^{*}=P(\mathcal{H}\mid\mathcal{D},\theta)$ , then 

$$
F_{\mathcal{D}}[\pmb{\theta},Q^{*}]=\ell(\pmb{\theta}:\mathcal{D})\geq F_{\mathcal{D}}[\pmb{\theta},Q].
$$ 

![](images/7d584934cfa517c87bf80262468a3bfe9f6d85019af1bf91e784b438a4ba854e.jpg) 
Figure 19.8 An illustration of the hill-climbing process performed by the EM algorithm. The black line represents the log-likelihood function; the point on the left represents $\theta^{t}$ ; the gray line represents the expected log-likelihood derived from $\theta^{t}$ ; and the point on the right represents the parameters $\pmb{\theta}^{t+1}$ that maximize this expected log-likelihood. 

Thus, we maximize the EM functional by choosing the auxiliary distribution $Q^{*}$ . In other words, we can view the E-step as implicitly optimizing $Q$ by using $P(\mathcal{H}\mid\mathcal{D},\theta^{t})$ in computing the expected sufcient statistics. 

• Optimizing $\theta$ . Suppose $Q$ is ﬁxed, and that we wish to ﬁnd arg max θ $F_{\mathcal{D}}[\pmb{\theta},\mathbf{Q}]$ . Because the only term in $F$ that involves $\theta$ is $E_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ , the maximization is equivalent to maximizing the expected log-likelihood. As we saw, we can ﬁnd the maximum by comput- ing expected sufcient statistics and then solving the MLE given these expected sufcient statistics. 

Convergence of EM The discussion so far shows that the EM procedure can be viewed as maximizing an objective function; because the objective function can be shown to be bounded, this procedure is guaranteed to converge. However, it is not clear what can be said about the convergence points of this procedure. We now analyze the convergence points of this procedure in terms of our true objective: the log-likelihood function. Intuitively, as our procedure is optimizing the energy functional, which is a tight lower bound of the log-likelihood function, each step of this optimization also improves the log-likelihood. This intuition is illustrated in ﬁgure 19.8. In more detail, the E-step is selecting, at the current set of parameters, the distribution $Q^{t}$ for which the energy functional is a tight wer bound to $\ell(\pmb\theta:{\mathcal D})$ . The energy functional, which is a well-behaved concave function in θ , can be maximized efectively via the M-step, taking us to the parameters $\pmb{\theta}^{t+1}$ . Since the energy functional is guaranteed to remain below the log-likelihood function, this step is guaranteed to improve the log-likelihood. Moreover, the improvement is guaranteed to be at least as large as the improvement in the energy functional. More formally, using corollary 19.1, we can now prove the following generalization of theorem 19.3: 

$$
\begin{array}{r}{\ell(\pmb{\theta}^{t+1}:\mathcal{D})-\ell(\pmb{\theta}^{t}:\mathcal{D})\geq\pmb{E}_{P(\mathcal{H}|\mathcal{D},\pmb{\theta}^{t})}\big[\ell(\pmb{\theta}^{t+1}:\mathcal{D},\mathcal{H})\big]-\pmb{E}_{P(\mathcal{H}|\mathcal{D},\pmb{\theta}^{t})}\big[\ell(\pmb{\theta}^{t}:\mathcal{D},\mathcal{H})\big].}\end{array}
$$ 

As a consequence, we obtain that: 

$$
\ell(\pmb\theta^{t}:\mathcal D)\leq\ell(\pmb\theta^{t+1}:\mathcal D).
$$ 

gin with the ﬁrst statement. Using corollary 19.1, with the distribution $Q^{t}({\mathcal{H}})=$ $P(\mathcal{H}\mid\mathcal{D},\theta^{t})$ we have that 

$$
\begin{array}{r c l}{\ell(\pmb{\theta}^{t+1}:\mathscr{D})}&{=}&{\pmb{E}_{Q^{t}}\big[\ell(\pmb{\theta}^{t+1}:\langle\mathscr{D},\mathscr{H}\rangle)\big]+\pmb{H}_{Q^{t}}(\mathscr{H})+\pmb{D}(Q^{t}(\mathscr{H})\|P(\mathscr{H}\mid\mathscr{D},\pmb{\theta}^{t+1})}\\ {\ell(\pmb{\theta}^{t}:\mathscr{D})}&{=}&{\pmb{E}_{Q^{t}}\big[\ell(\pmb{\theta}^{t}:\langle\mathscr{D},\mathscr{H}\rangle)\big]+\pmb{H}_{Q^{t}}(\mathscr{H})+\pmb{D}(Q^{t}(\mathscr{H})\|P(\mathscr{H}\mid\mathscr{D},\pmb{\theta}^{t}))}\\ &{=}&{\pmb{E}_{Q^{t}}\big[\ell(\pmb{\theta}^{t}:\langle\mathscr{D},\mathscr{H}\rangle)\big]+\pmb{H}_{Q^{t}}(\mathscr{H}).}\end{array}
$$ 

The last step is justiﬁed by our choice of $Q^{t}(\mathcal{H})=P(\mathcal{H}\mid\mathcal{D},\theta^{t})$ . Subtracting these two terms, we have that 

$$
\begin{array}{r l}&{\ell(\pmb{\theta}^{t+1}:\mathcal{D})-\ell(\pmb{\theta}^{t}:\mathcal{D})=}\\ &{\qquad\pmb{E}_{\boldsymbol{Q}^{t}}\big[\ell(\pmb{\theta}^{t+1}:\mathcal{D},\mathcal{H})\big]-\pmb{E}_{\boldsymbol{Q}^{t}}\big[\ell(\pmb{\theta}^{t}:\mathcal{D},\mathcal{H})\big]+\pmb{D}(\boldsymbol{Q}^{t}(\mathcal{H})\|\boldsymbol{P}(\mathcal{H}\mid\mathcal{D},\pmb{\theta}^{t+1})).}\end{array}
$$ 

Because the last term is nonnegative, we get the desired inequality. 

To prove the second statement of the theorem, we note that $\pmb{\theta}^{t+1}$ is the value of $\theta$ that maximizes $E_{P(\mathcal{H}|\mathcal{D},\theta^{t})}[\ell(\pmb{\theta}:\mathcal{D},\mathcal{H})]$ . Hence the value obtained for this expression for $\pmb{\theta}^{t+1}$ is at H|D least at large as the value obtained for any other set of parameters, including $\theta^{t}$ . It follows that the right-hand side of the inequality is nonnegative, which implies the second statement. 

We conclude that EM performs a variant of hill climbing, in the sense that it improves the log-likelihood at each step. Moreover, the M-step can be understood as maximizing a lower-bound on the improvement in the likelihood. Thus, in a sense we can view the algorithm as searching for the largest possible improvement, when using the expected log-likelihood as a proxy for the actual log-likelihood. 

For most learning problems, we know that the log-likelihood is upper bounded. For example, if we have discrete data, then the maximal likelihood we can assign to the data is 1 . Thus, the log-likelihood is bounded by 0 . If we have a continuous model, we can construct examples where the likelihood can grow unboundedly; however, we can often introduce constraints on the parameters that guarantee a bound on the likelihood (see exercise 19.10). If the log-likelihood is bounded, and the EM iterations are nondecreasing in the log-likelihood, then the sequence of log-likelihoods at successive iterations must converge. 

The question is what can be said about this convergence point. Ideally, we would like to guar- antee convergence to the maximum value of our log-likelihood function. Unfortunately, as we mentioned earlier, we cannot provide this guarantee; however, we can now prove theorem 19.4, which shows convergence to a ﬁxed point of the log-likelihood function, that is, one where the gradient is zero. We restate the theorem for convenience: 

We now co sider the gradient of $\ell(\pmb\theta:{\mathcal D})$ with respect to $\theta$ . Since the term $H_{Q}({\mathcal{H}})$ H does not depend on θ , we get that 

$$
\nabla_{\boldsymbol{\theta}}\boldsymbol{\ell}(\boldsymbol{\theta}:\mathcal{D})=\nabla_{\boldsymbol{\theta}}E_{\boldsymbol{Q}}[\boldsymbol{\ell}(\boldsymbol{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]+\nabla_{\boldsymbol{\theta}}\boldsymbol{\mathcal{D}}(\boldsymbol{Q}(\mathcal{H})\|P(\mathcal{H}\mid\mathcal{D},\boldsymbol{\theta})).
$$ 

This observation is true for any choice of $Q$ . Now suppose we are in an EM iteration. In this case, we set $Q=P(\mathcal{H}\mid\mathcal{D},\pmb{\theta}^{t})$ and evaluate the gradient at $\theta^{t}$ 

A somewhat simpliﬁed proof runs as follows. Because θ ${\boldsymbol{\theta}}\,=\,{\boldsymbol{\theta}}^{t}$ is a minimum of the KL- divergence term, we know that $\nabla_{\boldsymbol{\theta}}D(Q(\mathcal{H})\|P(\mathcal{H}\mid\mathcal{D},\boldsymbol{\theta}^{t}))$ H | | H | D is 0 . This implies that 

$$
\nabla_{\pmb\theta}\ell(\pmb\theta^{t}:\mathcal D)=\nabla_{\pmb\theta}\pmb E_{Q}\big[\ell(\pmb\theta^{t}:\langle\mathcal D,\mathcal H\rangle)\big].
$$ 

Or, in other words, $\nabla_{\boldsymbol{\theta}}\ell(\boldsymbol{\theta}^{t}:\mathcal{D})=0$ if and only if $\nabla_{\boldsymbol{\theta}}E_{Q}\left[\boldsymbol{\ell}(\boldsymbol{\theta}^{t}:\langle\mathcal{D},\mathcal{H}\rangle)\right]=0$ . 

Recall that $\pmb{\theta}^{t+1}=\arg\operatorname*{max}_{\pmb{\theta}}\pmb{E}_{Q}\big[\ell(\pmb{\theta}^{t}:\langle\mathcal{D},\mathcal{H}\rangle)\big]$  ⟨D H⟩ . Hence the gradient of the expected likeli- hood at $\theta^{t+1}$ is 0 . Thus, we conclude that θ $\pmb\theta^{t+1}=\pmb\theta^{t}$ only if $\nabla_{\boldsymbol{\theta}}E_{Q}\left[\boldsymbol{\ell}(\boldsymbol{\theta}^{t}:\langle\mathcal{D},\mathcal{H}\rangle)\right]=0$   . And so, at this point, $\nabla_{\boldsymbol{\theta}}\ell(\boldsymbol{\theta}^{t}:\mathcal{D})=0$ . This implies that this set of parameters is a stationary point of the log-likelihood function. 

The actual argument has to be somewhat more careful. Recall that the parameters must lie within some allowable set. For example, the parameters of a discrete random variable must sum up to one. Thus, we are searching within a constrained space of parameters. When we have constraints, we often do not have zero gradient. Instead, we get to a stationary point when the gradient is orthogonal to the constraints (that is, local changes within the allowed space do not improve the likelihood). The arguments we have stated apply equally well when we replace statements about equality to 0 with orthogonality to the constraints on the parameter space. 

# 19.2.2.6 Hard-Assignment EM 

In section 19.2.2.4, we brieﬂy mentioned the idea of using a hard assignment to the hidden variables, in the context of applying EM to Bayesian clustering. We now generalize this simple idea to the case of arbitrary Bayesian networks. 

This algorithm, called hard-assignment EM , also iterates over two steps: one in which it com- pletes the data given the current parameters $\theta^{t}$ , and the other in which it uses the completion to estimate new parameters $\pmb{\theta}^{t+1}$ . However, rather than using a soft completion of the data, as in standard EM, it selects for each data instance $o[m]$ the single assignment $h[m]$ that maximizes $P(h\mid o[m],\theta^{t})$ . 

Although hard-assignment EM is similar in outline to EM, there are important diferences. In fact, hard-assignment EM can be described as optimizing a diferent objective function, one that involves both the learned parameters and the learned assignment to the hidden variables. This objective is to maximize the likelihood of the complete data $\langle\mathcal{D},\mathcal{H}\rangle$ , given the parameters: 

$$
\operatorname*{max}_{\pmb{\theta},\mathcal{H}}\ell(\pmb{\theta}:\mathcal{H},\mathcal{D}).
$$ 

See exercise 19.14. Compare this objective to the EM objective, which attempts to maximize $\ell(\pmb\theta:{\mathcal D})$ , averaging over all possible completions of the data. 

Does this observation provide us insight on these two learning procedures? The intuition is that these two objectives are similar if $P(\mathcal{H}\mid\mathcal{D},\theta)$ assigns most of the probability mass to 

![](images/2b5579e5c9cc38d41efcac48b4afc028e9e4d66d71532253c7c932be2a5443a5.jpg) 
Figure 19.B.1 — Convergence of EM run on the ICU Alarm network. (a) Training log-likelihood. (b) Progress of several sample parameters. (c) Test data log-likelihood. 

one completion of the data. In such a case, EM will efectively perform hard assignment during the E-step. However, if $P(\mathcal{H}\mid\mathcal{D},\theta)$ is difuse, the two algorithms will lead to very diferent solutions. In clustering, the hard-assignment version tends to increase the contrast between diferent classes, since assignments have to choose between them. In contrast, EM can learn classes that are overlapping, by having many instances contributing to two or more classes. 

Another diference between the two EM variants is in the way they progress during the learning. Note that for a given data set, at the end of an iteration, the hard-assignment EM can be in one of a ﬁnite number of parameter values. Namely, there is only one parameter assignment for each possible assignment to $\mathcal{H}$ . Thus, hard-assignment EM traverses a path in the combinatorial space of assignments to H . The soft-assignment EM, on the other hand, traverses the continuous space of parameter assignments. The intuition is that hard-assignment EM converges faster, since it makes discrete steps. In contrast, soft-assignment EM can converge very slowly to a local maximum, since close to the maximum, each iteration makes only small changes to the parameters. The ﬂip side of this argument is that soft-assignment EM can traverse paths that are infeasible to the hard-assignment EM. For example, if two clusters need to shift their means in a coordinated fashion, soft-assignment EM can progressively change their means. On the other hand, hard-assignment EM needs to make a “jump,” since it cannot simultaneously reassign multiple instances and change the class means. 

Box 19.B — Case Study: EM in Practice. The EM algorithm is guaranteed to monotonically im- prove the training log-likelihood at each iteration. However, there are no guarantees as to the speed of convergence or the quality of the local maxima attained. To gain a better perspective of how the algorithm behaves in practice, we consider here the application of the method to the ICU-Alarm network discussed in earlier learning chapters. 

We start by considering the progress of the training data likelihood during the algorithm’s it- erations. In this example, 1 , 000 samples were generated from the ICU-Alarm network. For each instance, we then independently and randomly hid 50 percent of the variables. As can be seen in ﬁgure 19.B.1a, much of the improvement over the performance of the random starting point is in the 

![](images/66d2de00c2049ce3a2a91a8386d951ff187e66c58ee9c9db897b43e0f28f0b06.jpg) 
Figure 19.B.2 — Local maxima in likelihood surface. (a) Number of unique local maxima $(\mathrm{in~25~}$ runs) for diferent sample sizes and missing value conﬁgurations. (b) Distribution of training likelihood of local maxima attained for 25 random starting points with 1,000 samples and one hidden variable. 

ﬁrst few iterations. However, examining the convergence of diferent parameters in (b), we see that some parameters change signiﬁcantly after the ﬁfth iteration, even though changes to the likelihood are relatively small. In practice, any nontrivial model will display a wide range of sensitivity to the network parameters. Given more training data, the sensitivity will, typically, overall decrease. Owing to these changes in parameters, the training likelihood continues to improve after the initial iterations, but very slowly. This behavior of fast initial improvement, followed by slow convergence, is typical of EM. 

overﬁtting 

We next consider the behavior of the learned model on unseen test data. As we can see in (c), early in the process, test-data improvement correlates with training-data performance. However, after the 10 th iterations, training performance continues to improve, but test performance decreases. This phenomenon is an instance of overﬁtting to the training data. With more data or fewer unobserved values, this phenomenon will be less pronounced. With less data or hidden variables, on the other hand, explicit techniques for coping with the problem may be needed (see box 19.C). 

A second key issue any type of optimization of the likelihood in the case of missing data is that of local maxima. To study this phenomenon, we consider the number of local maxima for 25 random starting points under diferent settings. As the sample size ( $\mathit{\check{x}}$ -axis) grows, the number of local maxima diminishes. In addition, the number of local maxima when more values are missing (dashed line) is consistently greater than the number of local maxima in the setting where more data is available (solid line). Importantly, in the case where just a single variable is hidden, the number of local maxima is large, and remains large even when the amount of training data is quite large. To see that this is not just an artifact of possible permutations of the values of the hidden variable, and to demonstrate the importance of achieving a superior local maxima, in (b) we show the training set log-likelihood of the 25 diferent local maxima attained. The diference between the best and worst local maxima is over 0 . 2 bit-per-instance. While this may not seem signiﬁcant, for a training set of 1 , 000 instances, this corresponds to a factor of $2^{0.2*1,000}\approx10^{60}$ in the training set likelihood. We also note that the spread of the quality in diferent local maxima is quite uniform, so that it is not easy to attain a good local maximum with a small number of random trials. 

# 19.2.3 Comparison: Gradient Ascent versus EM 

So far we have discussed two algorithms for parameter learning with incomplete data: gradient ascent and EM. As we will discuss (see box 19.C), there are many issues involved in the actual implementation of these algorithms: the choice of initial parameters, the stopping criteria, and so forth. However, before discussing these general points, it is worth comparing the two algorithms. 

There are several points of similarity in the overall strategy of both algorithms. Both algorithms are local in nature. At each iteration they maintain a “current” set of parameters, and use these to ﬁnd the next set. Moreover, both perform some version of greedy optimization based on the current point. Gradient ascent attempts to progress in the steepest direction from the current point. EM performs a greedy step in improving its target function given the local parameters. Finally, both algorithms provide a guarantee to converge to local maxima (or, more precisely, to stationary points where the gradient is 0 ). On one hand, this is an important guarantee, in the sense that both are at least locally maximal. On the other hand, this is a weak guarantee, since many real-world problems have multimodal likelihood functions, and thus we do not know how far the learned parameters are from the global maximum (or maxima). 

In terms of the actual computational steps, the two algorithms are also quite similar. For table-CPDs, the main component of either an EM iteration or a gradient step is computing the expected sufcient statistics of the data given the current parameters. This involves performing inference on each instance. Thus, both algorithms can exploit dynamic programming procedures (for example, clique tree inference) to compute all the expected sufcient statistics in an instance efciently. 

In term of implementation details, the algorithms provide diferent beneﬁts. On one hand, gradient ascent allows to use “black box” nonlinear optimization techniques, such as conjugate gradient ascent (see appendix A.5.2). This allows the implementation to build on a rich set of existing tools. Moreover, gradient ascent can be easily applied to various CPDs by using the chain rule of derivatives. On the other hand, EM relies on maximization from complete data. Thus, it allows for a fairly straightforward use of learning procedure for complete data in the case of incomplete data. The only change is replacing the part that accumulates sufcient statistics by a procedure that computes expected sufcient statistics. As such, most people ﬁnd EM easier to implement. 

A ﬁnal aspect for consideration is the convergence rate of the algorithm. Although we cannot predict in advance how many iterations are needed to learn parameters, analysis can show the general behavior of the algorithm in terms of how fast it approaches the convergence point. 

Suppose we denote by $\ell_{t}=\ell(\theta^{t}:\mathcal{D})$ the likelihood of the soluti $t$ ’th iteration (of either EM or gradient ascent). The algorithm converges toward $\begin{array}{r}{\ell^{*}=\operatorname*{lim}_{t\to\infty}\ell_{t}}\end{array}$ . The error →∞ at the $t^{\prime}$ ’th iteration is 

$$
\epsilon_{t}=\ell^{*}-\ell_{t}.
$$ 

Although we do not go through the proof, one can show that EM has linear convergence rate . This means that for each domain there exists a $t_{0}$ and $\alpha<1$ such that for all $t\geq t_{0}$ 

$$
\epsilon_{t+1}\leq\alpha\epsilon_{t}.
$$ 

On the face of it, this is good news, since it shows that the error decreases at each iteration. Such convergence rate means that $\ell_{t+1}-\ell_{t}=\epsilon_{t}-\epsilon_{t+1}\geq\epsilon_{t}(1-\alpha)$ . In other words, if we know α , we can bound the error 

$$
\epsilon_{t}\leq\frac{\ell_{t+1}-\ell_{t}}{1-\alpha}.
$$ 

While this result provides a bound on the error (and also suggests a way of estimating it), it is not always a useful one. In particular, if $\alpha$ is relatively close to 1 , then even when the diference is likelihood between successive iterations is small, the error can be much larger. Moreover, the number of iterations to convergence can be very large. In practice we see this behavior quite often. The ﬁrst iterations of EM show huge improvement in the likelihood. These are then followed by many iterations that slowly increase the likelihood; see box 19.B. Conjugate gradient often has opposite behavior. The initial iterations (which are far away from the local maxima) often take longer to improve the likelihood. However, once the algorithm is in the vicinity of maximum, where the log-likelihood function is approximately quadratic, this method is much more efcient in zooming on the maximum. Finally, it is important to keep in mind that these arguments are asymptotic in the number of iterations; the actual number of iterations required for convergence may not be in the asymptotic regime. Thus, the rate of convergence of diferent algorithms may not be the best indicator as to which of them is likely to work most efciently in practice. 

Box 19.C — Skill: Practical Considerations in Parameter Learning. There are a few practical considerations in implementing both gradient-based methods and EM for learning parameters with missing data. We now consider a few of these. We present these points mostly in the context of the EM algorithm, but most of our points apply equally to both classes of algorithms. 

In a practical implementation of EM, there are two key issues that one needs to address. The ﬁrst is the presence of local maxima. As demonstrated in box 19.B, the likelihood of even relatively simple networks can have a large number of local maxima that signiﬁcantly difer in terms of their quality. There are several adaptations of these local search algorithms that aim to consistently reach beneﬁcial local maxima. These adaptations include a judicious selection of initialization, and methods for modifying the search so as to achieve a better local maximum. The second key issue involves the convergence of the algorithm: determining convergence, and improving the rate of convergence. 

Local Maxima One of the main limitations of both the EM and the gradient ascent procedures is that they are only guaranteed to reach a stationary point, which is usually a local maximum. How do we improve the odds of ﬁnding a global — or at least a good local — maximum? 

The ﬁrst place where we can try to address the issue of local maxima is in the initialization of the algorithm. EM and gradient ascent, as well as most other “local” algorithms, require a starting point — a set of initial parameters that the algorithm proceeds to improve. Since both algo- rithms are deterministic, this starting point (implicitly) determines which local maximum is found. In practice, diferent initializations can result in radically diferent convergence points, sometimes with very diferent likelihood values. Even when the likelihood values are similar, diferent convergence points may represent semantically diferent conclusions about the data. This issue is particularly severe when hidden variables are involved, where we can easily obtain very diferent clusterings of the data. For example, when clustering text documents by similarity (for example, using a version of the model in box 17.E where the document cluster variable is hidden), we can learn one model where the clusters correspond to document topics, or another where they correspond to the style of the publication in which the document appeared (for example, newspaper, webpage, or blog). Thus, initialization should generally be considered very seriously in these situations, especially when the amount of missing data is large or hidden variables are involved. 

In general, we can initialize the algorithm either in the E-step, by picking an initial set of parameters, or in the M-step, by picking an initial assignment to the unobserved variables. In the ﬁrst type of approach, the simplest choices for starting points are either a set of parameters ﬁxed in advance or randomly chosen parameters. If we use predetermined initial parameters, we should exercise care in choosing them, since a misguided choice can lead to very poor outcomes. In particular, for some learning problems, the seemingly natural choice of uniform parameters can lead to disastrous results; see exercise 19.11. Another easy choice is applicable for parts of the network where we have only a moderate amount of missing data. Here, we can sometimes estimate parameters using only the observed data, and then use those to initialize the E-step. Of course, this approach is not always feasible, and it is inapplicable when we have a hidden variable. A diferent natural choice is to use the mean of our prior over parameters. On one hand, if we have good prior information, this might serve as a good starting position. Note that, although this choice does bias the learning algorithm to prefer the prior’s view of the data, the learned parameters can be drastically diferent in the end. On the other hand, if the prior is not too informative, this choice sufers from the same drawbacks we mentioned earlier. Finally, a common choice is to use a randomized starting point, an approach that avoids any intrinsic bias. However, there is also no reason to expect that a random choice will give rise to a good solution. For this reason, often one tries multiple random starting points, and the convergence point of highest likelihood is chosen. 

The second class of methods initializes the procedure at the M-step by completing the missing data. Again, there are many choices for completing the data. For example, we can use a uniform or a random imputation method to assign values to missing observations. This procedure is particularly useful when we have diferent patterns of missing observations in each sample. Then, the counts from the imputed data consist of actual counts combined with imputed ones. The real data thus bias the estimated parameters to be reasonable. Another alternative is to use a simpliﬁed learning procedure to learn initial assignment to missing values. This procedure can be, for example, hard- assignment EM. As we discussed, such a procedure usually converges faster and therefore can serve as a good initialization. However, hard-assignment EM also requires a starting point, or a selection among multiple random starting points. 

When learning with hidden variables, such procedures can be more problematic. For example, if we consider a naive Bayes clustering model and use random imputation, the result would be that we randomly assign instances to clusters. With a sufciently large data set, these clusters will be very similar (since they all sample from the same population). In a smaller data set the sampling noise might distinguish the initial clusters, but nonetheless, this is not a very informed starting point. We discuss some methods for initializing a hidden variable in section 19.5.3. 

Other than initialization, we can also consider modifying our search so as to reduce the risk of getting stuck at a poor local optimum. The problem of avoiding local maxima is a standard one, and we describe some of the more common solutions in appendix A.4.2. Many of these solutions are applicable in this setting as well. As we mentioned, the approach of using multiple random restarts is commonly used, often with a beam search modiﬁcation to quickly prune poor starting points. In particular, in this beam search variant, $K$ EM runs are carried out in parallel and every few iterations only the most promising ones are retained. A variant of this approach is to generate $K$ EM threads at each step by slightly perturbing the most beneﬁcial $k<K$ threads from the previous iteration. While such adaptations have no formal guarantees, they are extremely useful in practice in terms trading of quality of solution and computational requirements. 

Annealing methods (appendix A.4.2) have also been used successfully in the context of the EM algorithm. In such methods, we gradually transform from an easy objective with a single local maximum to the desired EM objective, and thereby we potentially avoid many local maxima that are far away from the central basin of attraction. Such an approach can be carried out by directly smoothing the log-likelihood function and gradually reducing the level to which it is smoothed, or implicitly by gradually altering the weights of training instances. 

Finally, we note that we can never determine with certainty whether the EM convergence point is truly the global maximum. In some applications this limitation is acceptable — for example, if we care only about ﬁtting the probability distribution over the training examples (say for detecting instances from a particular subpopulation). In this case, if we manage to learn parameters that assign high probability for samples in the target population, then we might be content even if these parameters are not the best ones possible. On the other hand, if we want to use the learned parameters to reveal insight about the domain, then we might care about whether the parameters are truly the optimal ones or not. In addition, if the learning procedure does not perform well, we have to decide whether the problem stems from getting trapped in a poor local maximum, or from the fact that the model is not well suited to the distribution in our particular domain. 

Stopping Criteria Both algorithms we discussed have the property that they will reach a ﬁxed point once they converged on a stationary point of the likelihood surface. In practice, we never really reach the stationary point, although we can get quite close to it. This raises the question of when we stop the procedure. 

The basic idea is that when solutions at successive iterations are similar to each other, additional iterations will not change the solution by much. The question is how to measure similarity of solutions. There are two main approaches. The ﬁrst is to compare the parameters from successive iterations. The second is to compare the likelihood of these choices of parameters. Somewhat surprisingly, these two criteria are quite diferent. In some situations small changes in parameters lead to dramatic changes in likelihood, and in others large changes in parameters lead to small changes in the likelihood. 

To understand how there can be a discrepancy between changes in parameters and changes in likelihood, consider the properties of the gradient as shown in theorem 19.2. Using a Taylor expansion of the likelihood, this gradient provides us with an estimate how the likelihood will change when we change the parameters. We see that if $P(x,\pmb{u}\mid\pmb{o[m],\theta})$ is small in most data instances, then the gradient $\frac{\partial\ell(\pmb{\theta}{:}\mathcal{D})}{\partial P(\boldsymbol{x}|\pmb{u})}$ will be small. This implies that relatively large changes in | $P(x\mid\pmb{u})$ will not change the likelihood by much. This can happen for example if the event u is uncommon in the training data, and the value of $P(x\mid\pmb{u})$ is involved in the likelihood only in a few instances. On the ﬂip side, if the event $x,u$ has a large posterior in all samples, then the gradient $\frac{\partial\ell(\pmb\theta;\mathcal D)}{\partial P(x|\pmb u)}$ will be of size proportional to $M$ . In such a situation a small change in the parameter can result in a large change in the likelihood. 

In general, since we are aiming to maximize the likelihood, large changes in the parameters that have negligible efect on the likelihood are of less interest. Moreover, measuring the magnitude of changes in parameters is highly dependent on our parameter iz ation. For example, if we use the re parameter iz ation of equation (19.3), the diference (say in Euclidean distance) between two sets of parameters can change dramatically. Using the likelihood for tracking convergence is thus less sensitive to these choices and more directly related to the goal of the optimization. 

Even once we decide what to measure, we still need to determine when we should stop the process. Some gradient-based methods, such as conjugate gradient ascent, build an estimate of the second-order derivative of the function. Using these derivatives, they estimate the improvement we expect to have. We can then decide to stop when the expected improvement is not more than a ﬁxed amount of log-likelihood units. We can apply similar stoping criteria to EM, where again, if the change in likelihood in the last iteration is smaller than a predetermined threshold we stop the iterations. 

overﬁtting 

Importantly, although the training set log-likelihood is guaranteed to increase mono- tonically until convergence, there is no guarantee that the generalization performance of the model — the expected log-likelihood relative to the underlying distribution — also increases monotonically. (See section 16.3.1.) Indeed, it is often the case that, as we ap- proach convergence, the generalization performance starts to decrease, due to overﬁtting of the parameters to the speciﬁcs of the training data. 

Thus, an alternative approach is to measure directly when additional improvement to the training set likelihood does not contribute to generalization. To do so we need to separate the available data into a training set and a validation set (see box 16.A). We run learning on the training set, but at the end of each iteration we evaluate the log-likelihood of the validation set (which is not seen during learning). We stop the procedure when the likelihood of the validation set does not improve. (As usual, the actual performance of the model would then need to be evaluated on a separate test set.) This method allows us to judge when the procedure stops learning the interesting phenomena and begins to overﬁt the training data. On the ﬂip side, such a procedure is both slower (since we need to evaluate likelihood on an additional data set at the end of iteration) and forces us to train on a smaller subset of data, increasing the risk of overﬁtting. Moreover, if the validation set is small, then the estimate of the generalization ability by the likelihood on this set is noisy. This noise can inﬂuence the stopping time. 

Finally, we note that in EM much of the improvement is typically observed in the ﬁrst few iterations, but the ﬁnal convergence can be quite slow. Thus, in practice, it is often useful to limit the number of EM iterations or use a lenient convergence threshold. This is particularly important when EM is used as part of a higher-level algorithm (for example, structure learning) and where, in the intermediate stages of the overall learning algorithm, approximate parameter estimates are often sufcient. Moreover, early stopping can help reduce overﬁtting, as we discussed. 

Accelerating Convergence There are also several strategies that can help improve the rate of convergence of EM to its local optimum. We brieﬂy list a few of them. 

The ﬁrst idea is to use hybrid algorithms that mix EM and gradient methods. The basic intuition is that EM is good at rapidly moving to the general neighborhood of a local maximum in few iterations but bad at pinpointing the actual maximum. Advanced gradient methods, on the other hand, quickly converge once we are close to a maximum. This observation suggests that we should run EM for few iterations and then switch over to using a method such as conjugate gradient. Such hybrid algorithms are often much more efcient. Another alternative is to use accelerated EM methods that take even larger steps in the search than standard EM (see section 19.7). 

Another class of variations comprises incremental methods . In these methods we do not perform a full $E$ -step or a full M-step. Again, the high-level intuition is that, since we view our procedure as maximizing the energy functional $F_{\mathcal{D}}[\pmb{\theta},Q]$ , we can consider steps that increase this functional but do not necessarily ﬁnd the maximum value parameters or $Q$ . For example, recall that $\theta$ consists of several subcomponents, one per CPD. Rather than maximizing all the parameters at once, we can consider a partial update where we maximize the energy functional with respect to one of the subcomponents while freezing the others; see exercise 19.16. Another type of partial update is based on writing $Q$ as a product of independent distributions — each one over the missing values in $a$ particular instance. Again, we can optimize the energy functional with respect to one of these while freezing the other; see exercise 19.17. These partial updates can provide two types of beneﬁt: they can require less computation than a full EM update, and they can propagate changes between the statistics and the parameters much faster, reducing the total number of iterations. 

Box 19.D — Case Study: EM for Robot Mapping. One interesting application of the EM algo- rithm is to robotic mapping. Many variants of this applications have been proposed; we focus on one by Thrun et al. (2004) that explicitly tries to use the probabilistic model to capture the structure in the environment. 

The data in this application are a point cloud representation of an indoor environment. The point cloud can be obtained by collecting a sequence of point clouds, measured along a robot’s motion trajectory. One can use a robot localization procedure to (approximately) assess the robot’s pose (position and heading) along each point in the trajectory, which allows the diferent measurements to be put on a common frame of reference. Although the localization process is not fully accurate, the estimates are usually reasonable for short trajectories. One can then take the points obtained over the trajectory, and ﬁt the points using polygons, to derive a 3D map of the surfaces in the robot’s environment. However, the noise in the laser measurements, combined with the errors in localization, leads adjacent polygons to have slightly diferent surface normals, giving rise to a very jagged representation of the environment. 

The EM algorithm can be used to ﬁt a more compact representation of the environment to the data, reducing the noise and providing a smoother, more realistic output. In particular, in this example, the model consists of a set of $3D$ planes $p_{1},\dots,p_{K}$ , each characterized by two parameters $\begin{array}{r}{\alpha_{k},\beta_{k}.}\end{array}$ , where $\alpha_{k}$ is a unit-length vector in $I\!\!R^{3}$ that encodes the plane’s surface normal vector, and $\beta_{k}$ is a scalar that denotes its distance to the origin of the global coordinate system. Thus, the distance of any point $_{_{x}}$ to the plane is $d({\pmb x},p_{k})=|\alpha_{k}{\pmb x}-\beta_{k}|$ . 

![](images/2283a6e3a3bdc2d2f5bf45696b760c24cc27242457cd4c15219ad72eaa66be38.jpg) 
Figure 19.D.1 — Sample results from EM-based 3D plane mapping (a) Raw data map obtained from range ﬁnder. (b) Planes extracted from map using EM. (c) Fragment of reconstructed surface using raw data. (d) The same fragment reconstructed using planes. 

correspondence variable data association 

The probabilistic model also needs to specify, for each point $\pmb{x}_{m}$ in the point cloud, to which plane $\pmb{x}_{m}$ belongs. This assignment can be modeled via a set of correspondence variables $C_{m}$ such that $C_{m}=k$ if the measurement point $\pmb{x}_{m}$ was generated by the k th plane. Each assignment to the correspondence variables, which are unobserved, encodes a possible solution to the data ee box 12.D for more details.) We deﬁne $P(\mathbf{X}_{m}\mid\mathbf{\mu}C_{m}=\boldsymbol{k}:\mathbf{\theta}_{k})$ ) to be $\propto\mathcal{N}\left(d({\pmb x},p_{k})\mid0;\sigma^{2}\right)$    . In addition, we also allow an additional value $C_{m}\,=\,0$ that encodes points that are not generated by any of the planes; the distribution $P(X_{m}\mid C_{m}=0)$ is taken to be uniform over the (ﬁnite) space. 

Given a probabilistic model, the EM algorithm can be applied to ﬁnd the assignment of points to planes — the correspondence variables, which are taken to be hidden; and the parameters $\alpha_{k},\beta_{k}$ that characterize the planes. Intuitively, the E-step computes the assignment to the correspondence variables by assigning the weight of each point proportionately to its distance to each of them. The M-step then recomputes the parameters of each plane to ﬁt the points assigned to it. See exer- cise 19.18 and exercise 19.19. The algorithm also contains an additional outer loop that heuristically suggests new surfaces to be added to the model, and removes surfaces that do not have enough support in the data (for example, one possible criterion can depend on the total weight that diferent data points assign to the surface). 

The results of this algorithm are shown in ﬁgure 19.D.1. One can see that the resulting map is considerably smoother and more realistic than the results derived directly from the raw data. 

# 19.2.4 Approximate Inference $\star$ 

The main computational cost in both gradient ascent and EM is in the computation of expected sufcient statistics. This step requires running probabilistic inference on each instance in the training data. These inference steps are needed both for computing the likelihood and for computing the posterior probability over events of the form $x,\mathrm{Da}_{X}$ for each variable and its parents. For some models, such as the naive Bayes clustering model, this inference step is almost trivial. For other models, this inference step can be extremely costly. In practice, we often want to learn parameters for models where exact inference is impractical. Formally, this happens when the tree-width of the unobserved parts of the model is large. (Note the contrast to learning from complete data, where the cost of learning the model did not depend on the complexity of inference.) In such situations the cost of inference becomes the limiting factor in our ability to learn from data. 

Recall the network discussed in example 6.11 and example 19.9, where we have n students taking m classes, and the grade for each student in each class depends on both the difculty of the class and his or her intelligence. In the ground network for this example, we have a set of variables $I=\{I(s)\}$ for the $n$ students (denoting the intelligence le el of each student $S_{.}$ ), $D=\{D(c)\}$ for the m courses (denoting the difculty level of each course c ), and $G=\{G(s,c)\}$ for the grades, where each variable $G(s,c)$ has as parents $I(s)$ and $D(c)$ . Since this network is derived from $^a$ plate model, the CPDs are all shared, so we only have three CPDs that must be learned: $P(I(S))$ , $^{\dag}(D(C)),P(G(S,C)\mid I(S),D(C))$ . 

Suppose we only observe the grades of the students but not their intelligence or the difculty of courses, and we want to learn this model. First, we note that there is no way to force the model to respect our desired semantics for the (hidden) variables $I$ and $D$ ; for example, a model in which we ﬂip the two values for $I$ is equally good. Nevertheless, we can hope that some value for $I$ will correspond to “high intelligence” and the other to “low intelligence,” and similarly for $D$ . 

To perform EM in this model, we need to infer the expected counts of assignments to triplets of variables of the form $I(s),D(c),G(s,c)$ . Since we have parameter sharing, we will aggregate these counts and then estimate the CPD $P(G(S,C)\mid I(S),D(C))$ from the aggregate counts. The problem is that observing a variable $G(s,c)$ couples its two parents. Thus, this network induces $a$ Markov network that has a pairwise potential between any pair of $I(s)$ and $D(c)$ variables that share an observed child. If enough grade variables are observed, this network will be close to a full bipartite graph, and exact inference about the posterior probability becomes intractable. This creates a serious problem in applying either EM or gradient ascent for learning this seemingly simple model from data. 

An obvious solution to this problem is to use approximate inference procedures. A simple approach is to view inference as a “black box.” Rather than invoking exact inference in the learning procedures shown in algorithm 19.1 and algorithm 19.2, we can simply invoke one of the approximate inference procedures we discussed in earlier chapters. This view is elegant because it decouples the choices made in the design of the learning procedure from the choices made in the approximate inference procedures. 

However, this decoupling can obscure important efects of the approximation on our learning procedure. For example, suppose we use approximate inference for computing the gradient in a gradient ascent approach. In this case, our estimate of the gradient is generally somewhat wrong, and the errors in successive iterations are generally not consistent with each other. Such inaccuracies can confuse the gradient ascent procedure, a problem that is particularly signiﬁcant when the procedure is closer to the convergence point and the gradient is close to 0 , so that the errors can easily dominate. A key question is whether learning with approximate inference results in an approximate learning procedure; that is, whether we are guaranteed to ﬁnd a local maximum of an approximation of the likelihood function. In general, there are very few cases where we can provide any types of guarantees on the interaction between approximate inference and learning. Nevertheless, in practice, the use of approximate 

# inference is often unavoidable, and so many applications use some form of approximate inference despite the lack of theoretical guarantees. 

structured variational 

One class of approximation algorithms for which a unifying perspective is useful is in the combination of EM with the global approximate inference methods of chapter 11. Let us con- sider ﬁrst the structured variational methods of section 11.5, where the integration is easiest to understand. In these methods, we are attempting to ﬁnd an approximate distribution $Q$ that is close to an unnormalized distribution $\tilde{P}$ in which we are interested. We saw that algorithms in this class can be viewed as ﬁnding a distribution $Q$ in a suitable family of distributions that maximizes the energy functional: 

$$
F[\tilde{P},Q]=E_{Q}\Bigl[\log\tilde{P}\Bigr]+H_{Q}(\mathcal{X}).
$$ 

Thus, in these approximate inference procedures, we search for a distribution $Q$ that maximizes 

$$
\operatorname*{max}_{Q\in{\mathcal{Q}}}F[\tilde{P},Q].
$$ 

variational EM 

We saw that we can view EM as an attempt to maximize the same energy functional, with the diference that we are also optimizing over the parameter iz ation $\theta$ of $\tilde{P}$ . We can combine both goals into a single objective by requiring that the distribution $Q$ used in the EM functional come from a particular family $\mathcal{Q}$ . Thus, we obtain the following variational $E M$ problem: 

$$
\operatorname*{max}_{\theta}\operatorname*{max}_{Q\in\mathcal{Q}}F_{\mathcal{D}}[\theta,Q],
$$ 

where $\mathcal{Q}$ is a family of approximate distributions we are considering for representing the distri- bution over the unobserved variables. 

To apply the variational EM framework, we need to choose the family of distributions $\mathcal{Q}$ that will be used to approximate the distribution $P(\mathcal{H}\mid\mathcal{D},\theta)$ . Importantly, because this posterior distribution is a product of the posteriors for the diferent training instance, our approximation $Q$ can take the same form without incurring any error. Thus, we need only to decide how to represent the posterior $P(h[m]\mid o[m],\theta)$ for each instance $m$ . We therefore deﬁne a class $\mathcal{Q}$ that we will use to approximate $P(h[m]\mid o[m],\theta)$ . Importantly, since the evidence $o[m]$ is diferent for each data instance $m$ , the posterior distribution for each instance is also diferent, and hence we need to use a diferent distribution $Q[m]\in{\mathcal{Q}}$ to approximate the posterior for each data instance. In principle, using the techniques of section 11.5, we can use any class $\mathcal{Q}$ that allows tractable inference. In actice, a common solution is to use the mean ﬁeld approximation, where we assume that Q is a product of marginal distributions (one per each unobserved value). 

Example 19.10 

Importantly, although the prior over the variables $I(s_{1}),\ldots,I(s_{n})$ is identical, their posterior is generally diferent. Thus, the marginal of each of the variable has diferent parameters in $Q$ (and similarly for the $D(c)$ variables). 

In our approximate $E$ -step, given a set of parameters $\theta$ for the model, we need to compute approximate expected sufcient statistics. We do so in two steps. First, we use iterations of the mean ﬁeld update equation equation (11.54) to ﬁnd the best choice of marginals in $Q$ to approximate $P(I(s_{1}),.\,.\,,I(s_{n}),D(c_{1}),.\,.\,,D(c_{m})\;\mid\;o,\theta)$ . We then use the distribution $Q$ to compute approximate expected sufcient statistics by ﬁnding: 

$$
\begin{array}{l l l}{{\bar{M}_{Q}[g_{(i,j)},I(s_{i}),D(c_{j})]}}&{{=}}&{{Q(I(s_{i}),D(c_{j}))I\{G(s_{i},c_{j})=g_{(i,j)}\}}}\\ {{}}&{{=}}&{{Q(I(s_{i}))Q(D(c_{j}))I\{G(s_{i},c_{j})=g_{(i,j)}\}.}}\end{array}
$$ 

Given our choice of $\mathcal{Q},$ , we can optimize the variational EM objective very similarly to the optimization of the exact EM objective, by iterating over two steps: 

variational E-step • Variational E-step For each $m$ , ﬁnd 

$$
Q^{t}[m]=\arg\operatorname*{max}_{Q\in Q}F_{o[m]}[\pmb\theta,Q].
$$ 

This step is identical to our deﬁnition of variational inference in chapter 11, and it can be implemented using the algorithms we discussed there, usually involving iterations of local updates until convergence. 

At the end of this step, we have an approximate distribution $\begin{array}{r}{Q^{t}=\prod_{m}Q^{t}[m]}\end{array}$ ] and can collect the expected sufcient statistics. To compute the expected sufcient statistics, we combine the observed values in the data with expected counts from the distribution $Q$ . This process requires answering queries about events in the distribution $Q$ . For some approximations, such as the mean ﬁeld approximation, we can answer such queries efciently (that is, by multiplying the marginal probabilities over each variables); see example 19.10. If we use a richer class of approximate distributions, we must perform a more elaborate inference process. Note that, because the approximation $Q$ is simpler than the original distribution $P$ , we have no guarantee that a clique tree for $Q$ will respect the family-preservation property relative to families in $P$ . Thus, in some cases, we may need to perform queries that are outside the clique tree used to perform the E-step (see section 10.3.3.2). 

• M-step We ﬁnd a new set of parameters 

$$
\pmb{\theta}^{t+1}=\arg\operatorname*{max}_{\pmb{\theta}}F_{\mathcal{D}}[\pmb{\theta},Q^{t}];
$$ 

this step is identical to the M-step in standard EM. 

The preceding algorithm is essentially performing coordinate-wise ascent alternating between optimization of $Q$ and $\theta$ . It opens up the way to alternative ways of maximizing the same objective function. For example, we can limit the number of iterations in the variational E-step. Since each such iteration improves the energy functional, we do not need to reach a maximum in the $Q$ dimension before making an improvement to the parameters. 

Importantly, regardless of the method used to optimize the variational EM functional of equation (19.7), we can provide some guarantee regarding the properties of our optimum. Recall that we showed that 

$$
\ell(\pmb\theta:{\mathcal D})=\operatorname*{max}_{Q}F_{{\mathcal D}}[\pmb\theta,Q]\geq\operatorname*{max}_{Q\in{\mathcal Q}}F_{{\mathcal D}}[\pmb\theta,Q].
$$ 

Thus, maximizing the objective of equation (19.7) maximizes a lower bound of the likelihood. When we limit the choice of $Q$ to be in a particular family, we cannot necessarily get a tight bound on the likelihood. However, since we are maximizing a lower bound, we know that we do not overestimate the likelihood of parameters we are considering. If the lower bound is relatively good, this property implies that we distinguish high-likelihood regions in the parameter space from very low ones. Of course, if the lower bound is loose, this guarantee is not very meaningful. 

We can try to extend these ideas to other approximation methods. For example, general- ized belief propagation section 11.3 is an attractive algorithm in this context, since it can be fairly efcient. Moreover, because the cluster graph satisﬁes the family preservation property, computation of an expected sufcient statistic can be done locally within a single cluster in the graph. The question is whether such an approximation can be understood as maximizing a clear objective. Recall that cluster-graph belief propagation can be viewed as attempting to maximize an approximation of the energy functional where we replace the term $H_{Q}(\mathcal X)$ X by approximate entropy terms. Using exactly the same arguments as before, we can then show that, if we use generalized belief propagation for computing expected sufcient statistics in the E-step, then we are efectively attempting to maximize the approximate version of the energy functional. In this case, we cannot prove that this approximation is a lower bound to the correct likelihood. Moreover, if we use a standard message passing algorithm to compute the ﬁxed points of the energy functional, we have no guarantees of convergence, and we may get oscillations both within an E-step and over several steps, which can cause signiﬁcant problems in practice. Of course, we can use other approximations of the energy functional, including ones that are guaranteed to be lower bounds of the likelihood, and algorithms that are guaranteed to be convergent. These approaches, although less commonly used at the moment, share the same beneﬁts of the structured variational approximation. 

More broadly, the ability to characterize the approximate algorithm as attempting to optimize a clear objective function is important. For example, an immediate consequence is that, to monitor the progress of the algorithm, we should evaluate the approximate energy functional, since we know that, at least when all goes well, this quantity should increase until the convergence point. 

# 19.3 Bayesian Learning with Incomplete Data $\star$ 

19.3.1 Overview 

In our discussion of parameter learning from complete data, we discussed the limitations of maximum likelihood estimation, many of which can be addressed by the Bayesian approach. In the Bayesian approach, we view the parameters as unobserved variables that inﬂuence the probability of all training instances. Learning then amounts to computing the probability of new examples based on the observation, which can be performed by computing the posterior probability over the parameters, and using it for prediction. 

More precisely, in Bayesian reasoning, we introduce a prior $P(\theta)$ over the parameters, and are interested in computing the posterior $P(\pmb\theta\mid\mathcal D)$ given the data. In the case of complete data, we saw that if the prior has some properties (for example, the priors over the parameters of diferent CPDs are independent, and the prior is conjugate), then the posterior has a nice form and is representable in a compact manner. Because the posterior is a product of the prior and the likelihood, it follows from our discussion in section 19.1.3 that these useful properties are lost in the case of incomplete data. In particular, as we can see from ﬁgure 19.4 and ﬁgure 19.5, the parameter variables are generally correlated in the posterior. Thus, we can no longer represent the posterior as a product of posteriors over each set of parameters. Moreover, the posterior will generally be highly complex and even multimodal. Bayesian inference over this posterior would generally require a complex integration procedure, which generally has no analytic solution. 

MAP estimation 

One approach, once we realize that incomplete data makes the prospects of exact Bayesian reasoning unlikely, is to focus on the more modest goal of MAP estimation , which we ﬁrst discussed in section 17.4.4. In this approach, rather than integrating over the entire posterior $P(\mathcal{D},\theta)$ , we search for a maximum of this distribution: 

$$
{\tilde{\theta}}=\arg\operatorname*{max}_{\theta}P(\theta\mid\mathcal{D})=\arg\operatorname*{max}_{\theta}\frac{P(\theta)P(\mathcal{D}\mid\theta)}{P(\mathcal{D})}.
$$ 

Ideally, the neighborhood of the MAP parameters is the center of mass of the posterior, and therefore, using them might be a reasonable approximation for averaging over parameters in their neighborhood. Using the same transformations as in equation (17.14), the problem reduces to one of computing the optimum: 

$$
\operatorname{score}_{\mathrm{MAP}}(\pmb\theta\ :\ \mathcal{D})=\ell(\pmb\theta:\mathcal{D})+\log P(\pmb\theta).
$$ 

MAP-EM This function is simply the log-likelihood function with an additional prior term. Because this prior term is usually well behaved, we can generally easily extend both gradient-based methods and the EM algorithm to this case; see, for example, exercise 19.20 and exercise 19.21. Thus, ﬁnding MAP parameters is essentially as hard or as easy as ﬁnding MLE parameters. As such, it is often applicable in practice. Of course, the same caveats that we discussed in section 17.4.4 — the sensitivity to parameter iz ation, and the insensitivity to the form of the posterior — also apply here. 

A second approach is to try to address the task of full Bayesian learning using an approximate method. Recall from section 17.3 that we can cast Bayesian learning as inference in the meta- network that includes all the variables in all the instances as well as the parameters. Computing the probability of future events amounts to performing queries about the posterior probability of the $(M+1)\mathrm{st}$ instance given the observations about the ﬁrst $M$ instances. In the case of complete data, we could derive closed-form solutions to this inference problem. In the case of incomplete data, these solutions do not exist, and so we need to resort to approximate inference procedures. 

In theory, we can apply any approximate inference procedure for Bayesian networks to this problem. Thus, all the procedures we discussed in the inference chapter can potentially be used for performing Bayesian inference with incomplete data. Of course, some are more suitable than others. 

For example, we can conceivably perform likelihood weighting, as described in section 12.2: we ﬁrst sample parameters from the prior, and then the unobserved variables. Each such sample will be weighted by the probability of the observed data given the sampled parameter and hidden variables. Such a procedure is relatively easy to implement and does not require running complex inference procedure. However, since the parameter space is a high-dimensional continuous region, the chance of sampling high-posterior parameters is exceedingly small. As a result, virtually all the samples will be assigned negligible weight. Unless the learning problem is relatively easy and the prior is quite informative, this inference procedure would provide a poor approximation and would require a huge number of samples. 

In the next two sections, we consider two approximate inference procedures that can be applied to this problem with some degree of success. 

# 19.3.2 MCMC Sampling 

A common strategy for dealing with hard Bayesian learning problems is to perform MCMC simulation (see section 12.3). Recall that, in these methods, we construct a Markov chain whose state is the assignment to all unobserved variables, such that the stationary distribution of the chain is posterior probability over these variables. In our case, the state of the chain consists of $\theta$ and $\mathcal{H}$ , and we need to ensure that the stationary distribution of the chain is the desired posterior distribution. 

# 19.3.2.1 Gibbs Sampling 

Gibbs sampling One of the simplest MCMC strategies for complex multivariable chains is Gibbs sampling . In Gibbs sampling, we choose one of the variables and sample its value given the value of all the other variables. In our setting, there are two types of variables: those in $\mathcal{H}$ and those in $\theta$ ; we deal with each separately. 

Suppose $X[m]$ is one of th ariables $\mathcal{H}$ . The current state of the MCMC sampler has a value for all other variables in H and for θ . Since the parameters are known, selecting a value for $X[m]$ requires a sampling step that is essentially the same as the one we did when we performed Gibbs sampling for inference in the $m$ ’th instance. This step can be performed using the same sampling procedure as in section 12.3. 

meta-network 

Now suppose that $\theta_{X|U}$ are the parameters for a particular CPD. Again, the current state of the sampler assigns value for all of the variables in $\mathcal{H}$ . Since the structure o he m a-network is such that $\theta_{X|U}$ are independent of the parameters of all other CPDs given D and H , then we need to sample fr $P(\pmb{\theta}_{X|U}\mid\mathcal{D},\mathcal{H})$ — the posterior distribution over the parameters given the complete data D $\mathcal{D},\mathcal{H}$ H . In section 17.4 we showed that, if the prior is of a particular form (for example, a product of Dirichlet priors), then the posterior based on complete data also has a compact form. Now, we can use these properties to sample from the posterior. To be concrete, if we consider table-CPDs with Dirichlet priors, then the posterior is a product of Dirichlet distributions, one for each assignment of values for $U$ . Thus, if we know how to sample from a Dirichlet distribution, then we can sample from this posterior. It turns out that sampling from a Dirichlet distribution can be done using a reasonably efcient procedure; see box 19.E. 

Thus, we can apply Gibbs sampling to the meta-network. If we simulate a sufciently long run, the samples we generate will be from the joint posterior probability of the parameters and the hidden variables. We can then use these samples to make predictions about new samples and to estimate the marginal posterior of parameters or hidden variables. The bugs system (see box 12.C) provides a simple, general-purpose tool for Gibbs-sampling-based Bayesian learning. 

Box 19.E — Skill: Sampling from a Dirichlet distribution. Suppose we have a parameter vec- tor random variable $\mathbf{\uptheta}\ =\ \langle\theta_{1},.\,.\,.\,,\theta_{k}\rangle$ that is distributed according to a Dirichlet distribution $\theta\sim D i r i c h l e t(\alpha_{1},.\.\ .\ ,\alpha_{K})$ . How do we sample a parameter vector from this distribution? 

An efective procedure for sampling such a vector relies on an alternative deﬁnition of the Dirichlet distribution. We need to start with some deﬁnitions. 

Deﬁnition 19.6 Gamma distribution 

A continuous random variable $\overline{{X}}$ has a Gamma distribution ${\mathrm{Gamma}}(\alpha,\beta)$ if it has the density 

$$
p(x)={\frac{\beta^{\alpha}}{\Gamma(\alpha)}}x^{\alpha-1}e^{-\beta x}.
$$ 

We can see that the $x^{\alpha-1}$ term is reminiscent of components of the Dirichlet distribution. Thus, it might not be too surprising that there is a connection between the two distributions. 

Theorem 19.7 Let $X_{1},\ldots,X_{k}$ be independent continuous random variables such that $X_{i}\,\sim\,\mathrm{Gamma}(\alpha_{i},1)$ . Deﬁne the random vector 

$$
\theta=\left\langle\frac{X_{1}}{X_{1}+\cdot\cdot\cdot+X_{k}},\cdot\cdot\cdot,\frac{X_{k}}{X_{1}+\cdot\cdot\cdot+X_{k}}\right\rangle.
$$ 

Then, $\theta\sim D i r i c h l e t(\alpha_{1},.\,.\,.\,,\alpha_{k})$ . 

Thus, we can think of a Dirichlet distribution as a two-step process. First, we sample $k$ indepen- dent values, each from a separate Gamma distribution. Then we normalize these values to get $^a$ distribution. The normalization creates the dependency between the components of the vector $\theta$ . 

This theorem suggests a natural way to sample from a Dirichlet distribution. If we can sam- ple from Gamma distributions, we can sample values $X_{1},\ldots,X_{k}$ from the appropriate Gamma distribution and then normalize these values. 

The only remaining question is how to sample from a Gamma distribution. We start with a special case. If we consider $a$ variable $X\sim\mathrm{Gamma}(1,1)$ , then the density function is 

$$
p(X=x)=e^{-x}.
$$ 

In this case, we can solve the cumulative distribution using simple integration and get that 

$$
P(X<x)=1-e^{-x}.
$$ 

From this, it is not hard to show the following result: 

# Lemma 19.2 

$$
f U\sim\operatorname{Unif}([0:1]),\,t h e n-\ln U\sim\operatorname{Gamma}(1,1).
$$ 

In particular, if we want to sample parameter vectors from Dirichlet $\mathopen{}\mathclose\bgroup\left(1,\ldots,1\aftergroup\egroup\right)$ , which is the uniform distribution over parameter vectors, we need to sample $k$ values from the uniform distri- bution, take their negative logarithm, and normalize. Since a sample from a uniform distribution can be readily obtained from a pseudo-random number generator, we get a simple procedure for sampling from the uniform distribution over multinomial distributions. Note that this procedure is not the one we intuitively would consider for this problem. When $\alpha\neq1$ the sampling problem is harder, and requires more sophisticated methods, often based on the rejection sampling approach described in section 14.5.1; these methods are outside the scope of this book. 

# 19.3.2.2 Collapsed MCMC 

Recall that, in many situations, we can make MCMC sampling more efcient by using collapsed particles that represent a partial state of the system. If we can perform exact inference over the remaining state, then we can use MCMC sampling over the smaller state space and thereby get more efcient sampling. 

We can apply this idea in the context of Bayesian inference in two diferent ways. In one approach, we have parameter collapsed particles , where each particle is an assignment to the parameters $\theta$ , associated with a distribution over $\mathcal{H}$ ; in the other, we have data complet distribution particles , where each article is an assignment to the unobserved variables H , associated with a distribution over θ . We now discuss each of these approaches in turn. 

Parameter Collapsed Particles Suppose we choose the collapsed particles to contain assign- ments to the parameters $\theta$ , accompanied by distributions over the hidden variables. Thus, we need to be able to deal with queries about $P({\mathcal{H}}\mid{\mathbf{\boldsymbol{\theta}}},{\mathcal{D}})$ and $P(\mathcal{D},\theta)$ . First note that given $\theta$ , the diferent training instances are conditionally independent. Thus, we can perform inference in each instance separately. The question now is whether we can perform this instance-level inference efciently. This depends on the structure of the network we are learning. 

Consider, for example, the task of learning the naive Bayes clustering model of section 19.2.2.4. In this case, each instance has a single hidden variable, denoting the cluster of the instance. Given the value of the parameters, inference over the hidden variable involves summing over all the values of the hidden variable, and computing the probability of the observation variables given each value. These operations are linear in the size of the network, and thus can be done efciently. This means that evaluating the likelihood of a proposed particle is quite fast. On the other hand, if we are learning parameters for the network of example 19.9, then the network structure is such that we cannot perform efcient exact inference. In this case the cost of evaluating the likelihood of a proposed particle is nontrivial and requires additional approximations. Thus, the ease of operations with this type of collapsed particle depends on the network structure. 

In addition to evaluating the previous queries, we need to be able to perform the sampling steps. In particular, for Gibbs sampling, we need to be able to sample from the distribution: 

$$
P(\pmb{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}}\mid\{\pmb{\theta}_{X_{j}\mid\mathrm{Pa}_{X_{j}}}\}_{j\neq i},\mathcal{D}).
$$ 

Unfortunately, sampling from this conditional distribution is unwieldy. Even though we assume the value of all other parameters, since we do not have complete data, we are not guaranteed to have a simple form for this conditional distribution (see example 19.11). As an alternative to Gibbs sampling, we can use Metropolis-Hastings. Here, in each proposal step, we suggest new parameter values and evaluate the likelihood of these new parameters relative to the old ones. This step requires that we evaluate $P(\mathcal{D},\theta)$ , which is as costly as computing the likelihood. This fact makes it critical that we construct a good proposal distribution, since a poor one can lead to many (expensive) rejected proposals. 

![](images/6d874367a9185510ab38ca5a110ed5d549ded5c01dc8c1ee862799c90203390d.jpg) 
Figure 19.9 Plate model for Bayesian clustering 

Example 19.11 Bayesian clustering 

Let us return to the setting of Bayesian clustering described in section 19.2.2.4. In this model, which is illustrated in ﬁgure .9, we have a set of data points ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ , which are taken from one of $^a$ set of K clusters. We assume that each cluster is characterized by a distribution $Q(X\mid\lambda)$ , which has the same form for each cluster, but diferent parameters. As we discussed, the form of the class-conditional distribution depends on the data; typical models include naive Bayes for discrete data, or Gaussian distributions for continuous data. This decision is orthogonal to our discussion here. Thus, we have a set of $K$ parameter vectors $\lambda_{1},\dots,\lambda_{K}$ , each sampled from $^a$ distribution $P(\lambda_{k}\mid\omega)$ . We use a hidden variable $C[m]$ to represen m ’th data point was sampled. Thus, the class-co itional distribution $P(X\mid C=c^{k},\lambda_{1,...,K})=$ | $Q(X\mid\lambda_{k})$ . We assume that the cluster variable C is sampled from a multinomial with parameters θ , sampled from a Dirichlet distribution $\theta\sim D i r i c h l e t(\alpha_{0}/K,.\,.\,.\,,\alpha_{0}/K)$ . The symmetry of the model relative to clusters reﬂects the fact that cluster identiﬁers are meaningless placeholders; it is only the partition of instances to clusters that is signiﬁcant. 

To consider the use of parameter collapsed particles, let us begin by writing down the data likelihood given the parameters. 

$$
P(\mathcal{D}\mid\lambda_{1},\ldots,\lambda_{K},\theta)=\prod_{m=1}^{M}\left(\sum_{k=1}^{K}P(C[m]=c^{k}\mid\theta)P(\pmb{x}[m]\mid C[m]=c^{k},\pmb{\lambda}_{k})\right)
$$ 

In this case, because of the simple structure of the graphical model, this expression is easy to evaluate for any ﬁxed set of parameters. 

Now, let us consider the task of sampling $\lambda_{k}$ given $\theta$ and $\lambda_{-k}$ , where we use a subscript of $-k$ to denote the set co ting of all of the values $k^{\prime}\in\{1,.\,.\,.\,,K\}-\{k\}$ . The distribution with which we wish to sample $\lambda_{k}$ is 

$$
P(\lambda_{k}\mid\lambda_{-k},\mathcal{D},\boldsymbol{\theta},\omega)\propto P(\mathcal{D}\mid\lambda_{1},\ldots,\lambda_{K},\boldsymbol{\theta})P(\lambda_{k}\mid\omega).
$$ 

Examining equation (19.8), we see that, given the data and the parameters other than $\lambda_{k}$ , all of the terms $P(\pmb{x}[m]\mid C[m]=c^{k^{\prime}},\lambda_{k^{\prime}})$ ) for $k^{\prime}\neq k$ can now be treated as a constant, and aggregated into a single number; similarly, the terms $P(C[m]\,=\,c^{k}\,\mid\,\theta)$ are also constant. Hence, each of the terms inside the outermost product can be written as a linear function $a_{m}P(\pmb{x}[m]\mid C[m]=$ $c^{k})+b_{m}$ . Unfortunately, the entire expression is a product of these linear functions, making the sampling distribution for $\lambda_{k}$ proportional to a degree M polynomial in its likelihood function (multiplied by $P(\lambda_{k}\mid\omega)_{\lambda}$ ). This distribution is rarely one from which we can easily sample. 

The Metropolis-Hastings approach is more feasible in this case. Here, as discussed in section 14.5.3, we can use a random-walk chain as a proposal distribution and use the data likelihood to compute the acceptance probabilities. In this case, the computation is fairly straightforward, since it involves the ratio of two expressions of the form of equation (19.8), which are the same except for the values of $\lambda_{k}$ . Unfortunately, although most of the terms in the numerator and denominator are identical, they appear within the scope of a summation over k and therefore do not cancel. Thus, to compute the acceptance probability, we need to compute the full data likelihood for both the current and proposed parameter choices; in this particular network, this computation can be performed fairly efciently. 

Data Completion Collapsed Particles An alternative choice is to use collapsed particles that assign a value to $\mathcal{H}$ . In this case, each particle represents a complete data set. Recall that if the prior distribution satisﬁes certain properties, then we can use closed-form formulas to compute parameter posteriors from $P(\lambda\mid\mathcal{D},\mathcal{H})$ and to evaluate the marginal likelihood $P(\mathcal{D},\mathcal{H})$ . This implies that if we are using a well-behaved prior, we can evaluate the likelihood of particles in time that does not depend on the network structure. 

For concreteness, consider the case where we are learning a Bayesian network with table- CPDs where we have an independent Dirichlet prior over each distribution $P(X_{i}\mid\mathrm{\mathfrak{p}}_{X_{i}})$ . In this case, if we have a particle that represents a complete data instance, we can summarize it by the sufcient statistics $M[x_{i},\mathrm{pa}_{x_{i}}]$ , and using these we can compute both the posterior over parameters and the marginal likelihood using closed-form formulas; see section 17.4 and section 18.3.4. 

Let us return to the Bayesian clustering task, but now consider the setting where each particle c is an assignment to the hidden variables $C[1],\ldots,C[M]$ . Given an assignment to these variables, we are now in the regime of complete data, in which the diferent parameters are independent in the posterior. In particular, let $I_{k}(c)$ be the t of indexes $\{m\,:\,c[m]=k\}$ . We can now compute the distribution associated with our particle c as a Dirichlet posterior 

$$
P(\pmb\theta\mid c)=D i r i c h l e t(\alpha_{0}/K+|I_{1}(\pmb c)|,.\,.\,,\alpha_{0}/K+|I_{K}(\pmb c)|).
$$ 

We also have that: 

$$
P(\lambda_{k}\mid c,\mathcal{D},\omega)=P(\lambda_{k}\mid\mathcal{D}_{I_{k}(c)},\omega)\propto P(\lambda_{k}\mid\omega)\prod_{m\in I_{k}(c)}P(x[m]\mid\lambda_{k}),
$$ 

that is, the posterior over $\lambda_{k}$ starting from the prior deﬁned by $\omega$ and conditioning on the data instances in $I_{k}(c)$ . If we now further assume that $P(\lambda\mid\omega)$ is a conjugate prior to $Q(X\mid\lambda)$ , this posterior can be computed in closed form. 

To apply Gibbs sampling, we also need to specify a distribution for sampling a new value for $C[m^{\prime}]$ given $c_{-m^{\prime}}$ , where again, we use the notation $-m^{\prime}$ to indicate all values $\{1,.\,.\,.\,,M\}\,-$ $\{m^{\prime}\}$ . Similarly, let $I_{k}\big(c_{-m^{\prime}}\big)$ denote the set of indexes $\{m\,\neq\,m^{\prime}\,\,:\,\,c[m]\,=\,k\}$ } . Due to the independencies represented by the model structure, we have: 

$$
\begin{array}{r l}&{P(C[m^{\prime}]=k\ |\ c_{-m^{\prime}},\mathcal{D},\omega)\propto}\\ &{\quad\quad P(C[m^{\prime}]=k\ |\ c_{-m^{\prime}})P(x[m^{\prime}]\ |\ C[m^{\prime}]=k,x[I_{k}(c_{-m^{\prime}})],\omega).}\end{array}
$$ 

The second term on the right-hand side is simply a Bayesian prediction over $X$ from the parameter posterior $P(\lambda_{k}\mid\mathcal{D}_{I_{k}(c_{-m^{\prime}})},\omega)$ , as deﬁned. ause of he symmetry of the parameters for the diferent clusters, the term does not depend on m ′ or on k , but only on the data set on which we condition. We can rewrite this term as $Q(X\mid\mathcal{D}_{I_{k}(c_{-m^{\prime}})},\omega)$ . The ﬁrst term on the right-hand side is the prior on cluster assignment for the instance m , as determined by the Dirichlet prior and the assignments of the other instances. Some algebra allows us to simplify this expression, resulting in: 

$$
P(C[m^{\prime}]=k\mid c_{-m^{\prime}},\mathcal{D},\omega)\propto(|I_{k}(c_{-m^{\prime}})|+\alpha_{0}/K)Q(X\mid\mathcal{D}_{I_{k}(c_{-m^{\prime}})},\omega).
$$ 

Assuming we have a conjugate prior, this expression can be easily computed. Overall, for most con- jugate priors, the cost of computing the sampling distribution for $C[m]$ in this model is $O(M K).$ 

It turns out that efcient sampling is also possible in more general models; see exercise 19.22. Alternatively we can use a Metropolis-Hastings approach where the proposal distribution can propose to modify several hidden values at once; see exercise 19.23. 

Comparison Both types of collapsed particles can be useful for learning in practice, but they have quite diferent characteristics. 

As we discussed, when we use parameter collapsed particles, the cost of evaluating a particle (for example, in a Metropolis-Hastings iteration) is determined by cost of inference in the net- work. In the worst case, this cost can be exponential, but in many examples it can be efcient. In contrast, the cost of evaluating data collapsed particles depends on properties of the prior. If the prior is properly chosen, the cost is linear in the size of the network. 

Another aspect is the space in which we perform MCMC. In the case of parameter collapsed particles, the MCMC procedure is performing integration over a high-dimensional continuous space. The simple Metropolis-Hastings procedures we discussed in this book are usually quite poor for addressing this type of task. However, there is an extensive literature of more efcient MCMC procedures for this task (these are beyond the scope of this book). In the case of data collapsed particles, we perform the integration over parameters in closed form and use MCMC to explore the discrete (but exponential) space of assignments to the unobserved variables. In this problem, relatively simple MCMC methods, such as Gibbs sampling, can be fairly efcient. 

To summarize, there is no clear choice between these two options. Both types of collapsed particles can speed up the convergence of the sampling procedure and the accuracy of the estimates of the parameters. 

# 19.3.3 Variational Bayesian Learning 

Another class of approximate inference procedures that we can apply to perform Bayesian infer- ence in the case of incomplete data are variational approximations. Here, we can use the meth- ods we developed in chapter 11 to the inference problem posed by Bayesian learning paradigm, resulting in an approach called variational Bayes . Recall that, in a variational approximation, we aim to ﬁnd a distribution $Q$ , from a predetermined family of distributions $\mathcal{Q}_{i}$ , that is close to the real posterior distri tion. In our case, we attempt to approximate $P({\mathcal{H}},\theta\mid{\mathcal{D}})$ ; thus, the unnormalized measure $\tilde{P}$ in equation (11.3) is $P({\mathcal{H}},\mathbf{\boldsymbol{\theta}},{\mathcal{D}})$ , and our approximating distribution $Q$ is over the parameters and the hidden variables. 

Plugging in the variational principle for our problem, and using the fact that $P({\mathcal{H}},\theta,{\mathcal{D}})=$ $P(\pmb\theta)P(\mathcal H,\mathcal D\mid\pmb\theta)$ , we have that the energy functional takes the form: 

$$
F[P,Q]=E_{Q}[\log P(\pmb\theta)]+E_{Q}[\log P(\mathcal{H},\mathcal{D}\mid\pmb\theta)]+H_{Q}(\pmb\theta,\mathcal{H}).
$$ 

The development of such an approximation requires that we decide on the class of approxi- mate distributions we want to consider. While there are many choices here, a natural one is to decouple the posterior over the parameters from the posterior over the missing data. That is, assume that 

$$
Q(\theta,{\mathcal{H}})=Q(\theta)Q({\mathcal{H}}).
$$ 

This is clearly a nontrivial assumption, since our previous discussion shows that these two posteriors are coupled by the data. Nonetheless, we can hope that an approximation that decouples the two distributions will be more tractable. 

Recall that, in our discussion of structured variational methods, we saw that the interactions between the structure of the approximation $Q$ and the true distribution $P$ can lead to further structural simpliﬁcations in $Q$ (see section 11.5.2.4). Using these tools, we can ﬁnd the following simpliﬁcation. 

Let $P(\theta)$ be a paramet l para ter inde endence, $\begin{array}{r}{P(\pmb{\theta})=\prod_{i}P(\pmb{\theta}_{X_{i}|U_{i}})}\end{array}$ . | Let D be a partially observable IID data set. If we consider a variational approximation with distributions satisfying $Q(\theta,{\mathcal{H}})=Q(\theta)Q({\mathcal{H}})$ H H , then Q can be decomposed as 

$$
Q(\pmb\theta,\mathcal{H})=\prod_{i}Q(\pmb\theta_{X_{i}|U_{i}})\prod_{m}Q(\pmb h[m]).
$$ 

The proof is by direct application of proposition 11.7 and is left as an exercise (exercise 19.24). 

This theorem shows that, once we decouple the posteriors over the parameters and missing data, we also lose the coupling between components of the two distributions (that is, diferent parameters or diferent instances). Thus, we can further decompose each of the two posteriors into a product of independent terms. This result matches our intuition, since the coupling between the parameters and missing data was the source of dependence between components of the two distributions. That is, the posteriors of two parameters were dependent due to incomplete data, and the posterior of missing data in two instances were dependent due to uncertainty about the parameters. 

This theorem does not necessarily justify the (strong) assumption of equation (19.11), but it does suggest that it provides signiﬁcant computational gains. In this case, we see that we can assume that the approximate posterior also satisﬁes global parameter independence, and similarly the approximate distribution over $\mathcal{H}$ consists of in ependent posteriors, one per instance. This simpliﬁcation already makes the representation of Q much more tractable. Other simpliﬁcations, following the same logic, are also possible. 

The variational Bayes approach often gives rise to very natural update rules. 

Example 19.13 Consider again the Bayesian clustering model of section 19.2.2.4. In this case, we aim to rep- resent the posterior over the parameters $\theta_{H},\theta_{X_{1}|H},.\,.\,.\,,\theta_{X_{n}|H}$ and over the hidden variables $H[1],\cdot\cdot\cdot,H[M]$ . The decomposition of theorem 19.8 allows us write $Q$ as a product distribution, with a term for each of these variables. Thus, we have that 

$$
Q=Q(\pmb\theta_{H})\left[\prod_{i}Q(\pmb\theta_{X_{i}|H})\right]\left[\prod_{m}Q(H[m])\right].
$$ 

mean ﬁeld This factorization is essentially $^a$ mean ﬁeld approximation. Using the results of section 11.5.1, we see that the ﬁxed-point equations for this approximation are of the form 

$$
\begin{array}{r c l}{{Q(\pmb{\theta}_{H})}}&{{\propto}}&{{\displaystyle\exp\left\{\ln P(\pmb{\theta}_{H})+\sum_{m}{\pmb{E}_{Q(H[m])}[\ln P(H[m]\mid\theta_{H})]}\right\}}}\\ {{}}&{{}}&{{}}\\ {{Q(\pmb{\theta}_{X_{i}[H)}}}&{{\propto}}&{{\displaystyle\exp\left\{\ln P(\pmb{\theta}_{X_{i}[H)}+\sum_{m}{\pmb{E}_{Q(H[m])}[\ln P(x_{i}[m]\mid H[m],\theta_{X_{i}[H})]}\right.}}\\ {{}}&{{}}&{{}}\\ {{Q(H[m])}}&{{\propto}}&{{\displaystyle\exp\left\{{\pmb{E}_{Q(\pmb{\theta}_{H})}[\ln P(H[m]\mid\theta_{H})]}\right.}}\\ {{}}&{{}}&{{}}\\ {{}}&{{}}&{{\displaystyle\left.+\sum_{i}{\pmb{E}_{Q(\pmb{\theta}_{X_{i}[H)})}[\ln P(x_{i}[m]\mid H[m],\theta_{X_{i}[H})]}\right\}.}}\end{array}
$$ 

The application of the mean-ﬁeld theory allows us to identify the structure of the update equation. To provide a constructive solution, we also need to determine how to evaluate the expectations in these update equations. We now examine these expectations in the case where all the variables are binary and the priors over parameters are simple Dirichlet distributions (Beta distributions, in fact). We start with the ﬁrst ﬁxed-point equation. A value for $\theta_{H}$ is $^a$ pair $\langle\theta_{h^{0}},\theta_{h^{1}}\rangle$ . Using the deﬁnition of the Dirichlet prior, we have that 

$$
\ln P(\theta_{H}=\langle\theta_{h^{0}},\theta_{h^{1}}\rangle)=\ln c+(\alpha_{h^{0}}-1)\ln\theta_{h^{0}}+(\alpha_{h^{1}}-1)\ln\theta_{h^{1}},
$$ 

where $\alpha_{h^{0}}$ and $\alpha_{h^{1}}$ are the hyperparameters of the prior $P(\pmb\theta_{H})$ , and $c$ is the normalizing constant of the prior (which we can ignore). Similarly, we can see that 

$$
E_{Q(H[m])}[\ln P(H[m]\mid\theta_{H}=\langle\theta_{h^{0}},\theta_{h^{1}}\rangle)]=Q(H[m]=h^{0})\ln\theta_{h^{0}}+Q(H[m]=h^{0}).
$$ 

Combining these results, we get that 

$$
\begin{array}{r c l}{{Q(\theta_{H}=\langle\theta_{h^{0}},\theta_{h^{1}}\rangle)}}&{{\propto}}&{{\displaystyle\exp\left\{\left(\alpha_{h^{0}}+\sum_{m}Q(H[m]=h^{0})-1\right)\ln\theta_{h^{0}}+\right.}}\\ {{}}&{{}}&{{\displaystyle\left.\left(\alpha_{h^{1}}+\sum_{m}Q(H[m]=h^{1})-1\right)\ln\theta_{h^{1}}\right\}}}\\ {{}}&{{=}}&{{\displaystyle\theta_{h^{0}}^{\alpha_{h^{0}}+\sum_{m}Q(H[m]=h^{0})-1}\theta_{h^{1}}^{\alpha_{h^{1}}+\sum_{m}Q(H[m]=h^{1})-1}.}}\end{array}
$$ 

In other words, $Q(\theta_{H})$ is a Beta distribution with hyperparameters 

$$
\begin{array}{r c l}{{\alpha_{h^{0}}^{\prime}}}&{{=}}&{{\alpha_{h^{0}}+\displaystyle\sum_{m}Q(H[m]=h^{0})}}\\ {{\alpha_{h^{1}}^{\prime}}}&{{=}}&{{\alpha_{h^{1}}+\displaystyle\sum_{m}Q(H[m]=h^{1}).}}\end{array}
$$ 

Note that this is exactly the Bayesian update for $\theta_{H}$ with the expected sufcient statistics given $Q(\mathcal{H})$ . 

A similar derivation shows that $Q(\theta_{X_{i}|H})$ is also a pair of independent Beta distributions (one for each value of $H$ ) that are updated with the expected sufcient statistics given $Q(\mathcal{H})$ . 

M-step 

E-step 

These updates are reminiscent of the EM-update ( M-step ), since we use expected sufcient statistics to update the posterior. In the EM M-step, we update the MLE using the expected sufcient statistics. If we carry the analogy further, the last ﬁxed-point equation, which updates $Q(H[m])$ , corresponds to the $E$ -step, since it updates the expectations over the missing values. Recall that, in the E-step of EM, we use the current parameters to compute 

$$
Q(H[m])=P(H[m]\mid x_{1}[m],\ldots x_{n}[m])\propto P(H[m]\mid\theta_{H})\prod_{i}P(x_{i}[m]\mid H[m],\theta_{H})
$$ 

If we were doing a Bayesian approach, we would not simply take our current values for the param- eters $\mathbf{\mathcal{\theta}}_{H},\mathbf{\mathcal{\theta}}_{X_{i}|H}$ ; rather, we would average over their posteriors. Examining this last ﬁxed-point equation, we see that we indeed average over the (approximate) posteriors $Q(\pmb\theta_{H})$ and $Q(\theta_{X_{i}|H})$ . However, unlike standard Bayesian averaging, where we compute the average value of the parameter itself, here we average its logarithm; that is, we evaluate terms of the form 

$$
E_{Q(\theta_{X_{i}|H})}\bigl[\ln P(x_{i}\mid H[m],\theta_{X_{i}|H})\bigr]=\intop_{0}^{1}Q(\theta_{x_{i}|H[m]})\ln\theta_{x_{i}|H[m]}d\theta_{x_{i}|H[m]}.
$$ 

Using methods that are beyond the scope of this book, one can show that this integral has $^a$ closed-form solution: 

$$
E_{Q(\theta_{X_{i}\mid H})}\left[\ln P(x_{i}\mid H[m],\theta_{X_{i}\mid H})\right]=\varphi(\alpha_{x_{i}\mid h}^{\prime})-\varphi(\sum_{x_{i}^{\prime}}\alpha_{x_{i}^{\prime}\mid h}^{\prime}),
$$ 

digamma function where $\alpha^{\prime}$ are the hyperparameters of the posterior approximation in $Q(\theta_{X_{i}|H})$ and $\varphi(z)\ =$ $\begin{array}{r}{(\ln\Gamma(z))^{\prime}=\frac{\Gamma^{\prime}(z)}{\Gamma(z)}}\end{array}$ is the digamma function , which is equal to $\ln(z)$ plus a polynomial function of $\frac{1}{z}$ . And so, for $z\gg1$ , $\varphi(z)\approx\ln(z)$ . Using this approximation, we see that 

$$
E_{Q(\theta_{X_{i}|H})}\bigl[\ln P(x_{i}\mid H[m],\theta_{X_{i}|H})\bigr]\approx\ln\frac{\alpha_{x_{i}|h}^{\prime}}{\sum_{x_{i}^{\prime}}\alpha_{x_{i}^{\prime}|h}^{\prime}},
$$ 

that is, the logarithm of the expected conditional probability according to the posterior $Q(\theta_{X_{i}|H})$ . This shows that if the posterior hyperparameters are large the variational update is almost identical to EM’s $E$ -step. 

To wrap up, we applied the structured variational approximation to the Bayesian learning prob- lem. Using the tools we developed in previous chapters, we deﬁned tractable ﬁxed-point equations. 

As with the mean ﬁeld approximation we discussed in section 11.5.1, we can ﬁnd a ﬁxed-point solution for $Q$ by iteratively applying these equations. 

The resulting algorithm is very similar to applications of EM. Applications of the update equations for the parameters are almost identical to standard EM of section 19.2.2.4 in the sense that we use expected sufcient statistics. However, instead of ﬁnding the MLE parameters given these expected sufcient statistics, we compute the posterior assuming these were observed. The update for $Q(H[m])$ is reminiscent to the computation of $P(H[m])$ when we know the parameters. However, instead of using parameter values we use expectations of their logarithm and then take the exponent. 

This example shows that we can ﬁnd a variational approximation to the Bayesian posterior using an EM-like algorithm in which we iterate between updates to the parameter posteriors and updates to the missing data posterior. These ideas generalize to other network structures in a fairly straightforward way. The update for the posterior over parameter is similar to Bayesian update with expected sufcient statistics, and the update of the posterior over hidden variable is similar to a computation with the expected parameters (with the diferences discussed earlier). In more complex examples we might need to make further assumptions about the distribution $Q$ in order to get a tractable approximation. For example, if there are multiple missing values per instance, then we might not be able to aford to represent their distribution by the joint distribution and would instead need to introduce structure into $Q$ . The basic ideas are similar to ones we explored before, and so we do not elaborate them. See exercise 15.6 for one example. 

Of course, this method has some clear drawbacks. Because we are representing the parameter posterior by a factored distribution, we cannot expect to represent a multimodal posterior. Unfortunately, we know that the posterior is often multimodal. For example, in the clustering problem, we know that change in names of values of $H$ would not change the prediction. Thus, the posterior in this example should be symmetric under such renaming. This implies that a unimodal distribution can only be a partial approximation to the true posterior. In multimodal cases, the efect of the variational approximation cannot be predicted. It may select one of the peaks and try to approximate it using $Q$ , or it may choose a “broad” distribution that averages over some or all of the peaks. 

# 19.4 Structure Learning 

We now move to discuss the more complex task of learning the network structure as well as the parameters, again in the presence of incomplete data. Recall that in the case of complete data, we started by deﬁning a score for evaluating diferent network structures and then examined search procedures that can maximize this score. As we will see, both components of structure  learning — the scoring function and the search procedure — are considerably more complicated in the case of incomplete data. Moreover, in the presence of hidden variables, even our search space becomes signiﬁcantly more complex, since we now have to select the value space for the hidden variables, and even the number of hidden variables that the model contains. 

# 19.4.1 Scoring Structures 

In section 18.3, we deﬁned three scores: the likelihood score, the BIC score, and the Bayesian score. As we discussed, the likelihood score does not penalize more complex models, and it is therefore not useful when we want to compare between models of diferent complexity. Both the BIC and Bayesian score have built-in penalization for complex models and thus trade of the model complexity with its ﬁt to the data. Therefore, they are far less likely to overﬁt. 

We now consider how to extend these scores to the case when some of the data are missing. On the face of it, the score we want to evaluate is the same Bayesian score we considered in the case of complete data: 

$$
\begin{array}{r}{\mathrm{score}_{\mathcal B}(\mathcal G\ :\ \mathcal D)=\log P(\mathcal D\mid\mathcal G)+\log P(\mathcal G)}\end{array}
$$ 

where $P({\mathcal{D}}\mid{\mathcal{G}})$ is the marginal likelihood of the data: 

$$
P(\mathcal{D}\mid\mathcal{G})=\int_{\Theta_{\mathcal{G}}}P(\mathcal{D}\mid\theta_{\mathcal{G}},\mathcal{G})P(\theta_{\mathcal{G}}\mid\mathcal{G})d\theta_{\mathcal{G}}.
$$ 

In the complete data case, the likelihood term inside the integral had a multiplicative factor- ization, and thus we could simplify it. In the case of incomplete data, the likelihood involves summing out over the unobserved variables, and thus it does not decompose. 

As we discussed, we can view the computation of the marginal likelihood as an inference problem. For most learning problems with incomplete data, this inference problem is a difcult one. We now consider diferent strategies for dealing with this issue. 

# 19.4.1.1 Laplace Approximation 

Laplace approximation One approach for approximating an integral in a high-dimensional space is to provide a simpler approximation to it, which we can then integrate in closed form. One such method is the Laplace approximation , described in box 19.F. 

Box 19.F — Concept: Laplace Approximation. The Laplace approximation can be applied to any function of the form $f(\pmb{w})=e^{g(\pmb{w})}$ for some vector $\mathbfit{w}$ . Our task is to compute the integral 

$$
F=\int f(w)d w.
$$ 

Using Taylor’s expansion, we can expand an approximation of $g$ around a point $\mathbf{\nabla}w_{\mathrm{0}}$ 

Hessian 

$(\pmb{w})\approx g(\pmb{w}_{0})+\left.\left[\frac{\partial g(\pmb{w})}{\partial x_{i}}\right]\right|_{\pmb{w}=\pmb{w}_{0}}(\pmb{w}-\pmb{w}_{0})+\frac{1}{2}(\pmb{w}-\pmb{w}_{0})^{T}\left.\left[\frac{\partial\partial g(\pmb{w})}{\partial x_{i}\partial x_{j}}\right]\right|_{\pmb{w}=\pmb{w}_{0}}(\pmb{w})$ w − w ) ,   0 where $\bigg[\frac{\partial g(\pmb{w})}{\partial x_{i}}\bigg]\bigg|_{\pmb{w}=\pmb{w}_{0}}$ h i denotes the vector of ﬁrst derivatives and $\bigg[\frac{\partial\partial g(\pmb{w})}{\partial x_{i}\partial x_{j}}\bigg]\bigg|_{\pmb{w}=\pmb{w}_{0}}$ h i denotes the Hessian — the matrix of second derivatives. If $\mathbf{\nabla}w_{\mathrm{0}}$ is the maximum of $g(w)$ , then the second term disappears. We now set 

$$
C=-\left.\left[\frac{\partial^{2}g(w)}{\partial x_{i}\partial x_{j}}\right]\right|_{w=w_{0}}
$$ 

to be the negative of the matrix of second derivatives of $g(w)$ at $\mathbf{\nabla}w_{\mathrm{0}}$ . Since $\mathbf{\nabla}w_{0}$ is a maximum, this matrix is positive semi-deﬁnitive. Thus, we get the approximation 

$$
g(\pmb{w})\approx g(\pmb{w}_{0})-\frac{1}{2}(\pmb{w}-\pmb{w}_{0})^{T}C(\pmb{w}-\pmb{w}_{0}).
$$ 

Plugging this approximation into the deﬁnition of $f(x)$ , we can write 

$$
\int f(\pmb{w})d\pmb{w}\approx f(\pmb{w}_{0})\int e^{-\frac{1}{2}(\pmb{w}-\pmb{w}_{0})^{T}C(\pmb{w}-\pmb{w}_{0})}d\pmb{w}.
$$ 

The integral is identical to the integral of an unnormalized Gaussian distribution with covariance matrix $\Sigma=C^{-1}$ . We can therefore solve this integral analytically and obtain: 

$$
\int f(\pmb{w})d\pmb{w}\approx f(\pmb{w}_{0})|C|^{-\frac{1}{2}}(2\pi)^{\frac{1}{2}\dim(C)},
$$ 

where $\dim(C)$ is the dimension of the matrix $C$ . 

At a high level, the Laplace approximation uses the value at the maximum and the curvature (the matrix of second derivatives) to approximate the integral of the function. This approximation works well when the function $f$ is dominated by a single peak that has roughly a Gaussian shape. 

How do we use the Laplace approximation in our setting? Taking $g$ to be the log-likelihood function combined with the prior $\log P(\mathcal{D}\mid\theta,\mathcal{G})+\log P(\theta|\mathcal{G})$ , we get that $\log P(\mathcal{D},\mathcal{G})$ can be approximated by the Laplace score : 

$$
\mathrm{score}_{L a p l a c e}(\mathcal{G}\ :\ \mathcal{D})=\log P(\mathcal{G})+\log P(\mathcal{D}\mid\tilde{\theta}_{\mathcal{G}},\mathcal{G})+\frac{\dim(C)}{2}\log2\pi-\frac{1}{2}\log|\mathcal{G}|.
$$ 

where $\tilde{\boldsymbol{\theta}}_{\mathcal{G}}$ are the MAP parameters and $C$ is the negative of the Hessian matrix of the log- G likelihood function. More precisely, the entries of $C$ are of the form 

$$
-\left.\frac{\partial^{2}\log P(\mathcal{D}\mid\theta,\mathcal{G})}{\partial\theta_{x_{i}|u_{i}}\partial\theta_{x_{j}|u_{j}}}\right|_{\Tilde{\theta}_{\mathcal{G}}}=-\sum_{m}\left.\frac{\partial^{2}\log P(o[m]\mid\theta,\mathcal{G})}{\partial\theta_{x_{i}|u_{i}}\partial\theta_{x_{j}|u_{j}}}\right|_{\Tilde{\theta}_{\mathcal{G}}},
$$ 

where $\theta_{x_{i}|\mathbf{u}_{i}}$ and $\theta_{x_{j}|\mathbf{\delta}\mathbf{u}_{j}}$ are two parameters (not necessarily from the same CPD) in the param- eterization of the network. 

The Laplace score takes into account not only the number of free parameters but also the curvature of the posterior distribution in each direction. Although the form of this expression arises directly by approximating the posterior marginal likelihood, it is also consistent with our intuitions about the desired behavior. Recall that the parameter posterior is a concave function, and hence has a negative deﬁnitive Hessian. Thus, the negative Hessian $C$ is positive deﬁnite and therefore has a positive determinant. A large determinant implies that the curvature at the MAP point is sharp; that is, the peak is relatively narrow and most of its mass is at the maximum. In this case, the model is probably overﬁtting to the training data, and we incur a large penalty. Conversely, if the curvature is small, the peak is wider, and the mass of the posterior is distributed over a larger set of parameters. In this case, overﬁtting is less likely, and, indeed, the Laplace score imposes a smaller penalty on the model. 

To compute the Laplace score, we ﬁrst need to use one of the methods we discussed earlier to ﬁnd the MAP parameters of the distribution, and then compute the Hessian matrix. The computation of the Hessian is somewhat involved. To compute the entry for the derivative relative to $\theta_{x_{i}|\mathbf{u}_{i}}$ and $\theta_{x_{j}|u_{j}}$ , we need to compute the joint distribution over $x_{i},x_{j},\mathbf{u}_{i},\mathbf{u}_{j}$ given the observation; see exercise 19.9. Because these variables are not necessarily together in a clique (or cluster), the cost of doing such computations can be much higher than computing the likelihood. Thus, this approximation, while tractable, is still expensive in practice. 

# 19.4.1.2 Asymptotic Approximations 

One way of avoiding the high cost of the Laplace approximation is to approximate the term $|C|^{-{\frac{1}{2}}}$ . Recall that the likelihood is the sum of the likelihood of each instance. Thus, the Hessian matrix is the sum of many Hessian matrixes, one per instance. We can consider asymptotic approximations that work well when the number of instances grows $M\to\infty$ ). For this analysis, we assume that all data instances have the same observation pattern; that is, the set of variables ${\cal O}[m]={\cal O}$ for all $m$ . 

Consider the matrix $C$ . As we just argued, this matrix has the form 

$$
C=\sum_{m=1}^{M}C_{m},
$$ 

where $C_{m}$ is the negative of the hessian of $\log P(\sigma[m]\mid\theta,\mathcal{G})$ . We can view each $C_{m}$ as a sample from a distribution that is induced by the (random) choice of assignment $^o$ to O ; each assignment $^o$ induces a diferent matrix $C_{o}$ . We can now rewrite: 

$$
C=M\frac{1}{M}\sum_{m=1}^{M}C_{m}.
$$ 

As $M$ grows, the term $\begin{array}{r}{\frac{1}{M}\sum_{m=1}^{M}C_{m}}\end{array}$ P approaches the expectation $E_{P^{*}}[C_{o}]$ . Taking the determinant of both sides, and recalling that $\mathrm{det}\left(\alpha A\right)=\alpha^{\mathrm{dim}\left(A\right)}\mathrm{det}\left(A\right)$ , we get 

$$
\mathrm{det}\left({C}\right)=M^{\mathrm{dim}\left(C\right)}\mathrm{det}\left(\frac{1}{M}\sum_{m=1}^{M}C_{m}\right)\approx M^{\mathrm{dim}\left(C\right)}\mathrm{det}\left(E_{P^{*}}\left[C_{o}\right]\right).
$$ 

Taking logarithms of both sides, we get that 

$$
\begin{array}{r}{\log\operatorname*{det}\left(\boldsymbol{C}\right)\approx\dim(\boldsymbol{C})\log M+\log\operatorname*{det}\left(E_{P^{*}}[C_{o}]\right).}\end{array}
$$ 

Notice that the last term does not grow with $M$ . Thus, when we consider the asymptotic behavior of the score, we can ignore it. This rough argument is the outline of the proof for the following result. 

# Theorem 19.9 

BIC score As $M\rightarrow\infty$ , we have that: $\mathrm{score}_{L a p l a c e}({\mathcal G}\ :\ {\mathcal D})=\mathrm{score}_{B I C}({\mathcal G}\ :\ {\mathcal D})+O(1)$ where $\mathrm{score}_{B I C}(\mathcal G\ :\ \mathcal D)$ is the BIC score 

$$
\mathrm{score}_{B I C}(\mathcal{G}\ :\ \mathcal{D})=\log P(\mathcal{D}\ |\ \tilde{\theta}_{\mathcal{G}},\mathcal{G})-\frac{\log M}{2}\mathrm{dim}[\mathcal{G}]+\log P(\mathcal{G})+\log P(\tilde{\theta}_{\mathcal{G}}
$$ 

This result shows that the BIC score is an asymptotic approximation to the Laplace score, a conclusion that is interesting for several important reasons. First, it shows that the intuition we had for the case of complete data, where the score trades of the likelihood of the data with a structure penalty, still holds. Second, as in the complete data case, the asymptotic behavior of this penalty is logarithmic in the number of samples; this relationship implies the rate at which more instances can lead us to introduce new parameters. 

An important subtlety in this analysis is hidden in the use of the notation $\mathrm{Diim}[\mathcal{G}]$ . In the case of complete data, this notation stood for the number of independent parameters in the network, a quantity that we could easily compute. Here, it turns out that for some models, the actual number of degrees of freedom is smaller than the space of parameters. This implies that the matrix $C$ is not of full rank, and so its determinant is 0 . In such cases, we need to perform a variant of the Laplace approximation in the appropriate subspace, which leads to a determinant of a smaller matrix. The question of how to determine the right number of degrees of freedom (and thus the magnitude of ${\mathrm{Min}}[\mathcal{G}])$ is still an open problem. 

# 19.4.1.3 Cheeseman-Stutz Approximation 

We can use the Laplace/BIC approximations to derive an even tighter approximation to the Bayesian score. The intuition is that, in the case of complete data, the full Bayesian score was more precise than the BIC score since it took into account the extent to which each parameter was used and how its range of values inﬂuenced the likelihood. These considerations are explicit in the integral form of the likelihood and implicit in the closed-form solution of the integral. When we use the BIC score on incomplete data, we lose these ﬁne-grained distinctions in evaluating the score. 

Recall that the closed-form solution of the Bayesian score is a function of the sufcient statistics of the data. An ad hoc approach for constructing a similar (approximate) score when we have incomplete data is to apply the closed-form solution of the Bayesian score on some approximation of the statistics of the data. A natural choice would be the expected sufcient statistics given the MAP parameters. These expected sufcient statistics represent the completion of the data given our most likely estimate of the parameters. 

More formally, for a network $\mathcal{G}$ and a set of parameters $\theta$ , we deﬁne $\mathcal{D}_{\mathcal{G},\theta}^{*}$ to be a ﬁctitious G “complete” data set whose actual counts are the same as the fractional expected counts relative to this network; that is, for every event $_{_{x}}$ : 

$$
M_{{\mathcal D}_{\mathcal G}^{*},\theta}[{\pmb x}]=\bar{M}_{P({\mathcal H}|{\mathcal D},\theta,{\mathcal G})}[{\pmb x}].
$$ 

Because the expected counts are based on a coherent distribution, there can be such a data set (although it might have instances with fractional weights). To evaluate a particular network $\mathcal{G}$ , we deﬁne the data set $\mathcal{D}_{\mathcal{G},\tilde{\boldsymbol{\theta}}_{\mathcal{G}}}^{*}$ induced by our network $\mathcal{G}$ and its MAP parameters $\tilde{\boldsymbol{\theta}}_{\mathcal{G}}$ G , and G G approximate the Bayesian score ${\mathcal{P}}({\mathcal{D}}\mid{\mathcal{G}})$ by $P(\mathcal{D}_{\mathcal{G},\tilde{\boldsymbol{\theta}}_{\mathcal{G}}}^{*}\mid\mathcal{G})$ , using the standard integration over G G the parameters. 

While straightforward in principle, a closer look suggests that this approximation cannot be a very good one. The ﬁrst term, 

$$
P(\mathcal{D}\mid\mathcal{G})=\int\sum_{\mathcal{H}}p(\mathcal{D},\mathcal{H}\mid\theta,\mathcal{G})P(\theta\mid\mathcal{G})d\theta=\sum_{\mathcal{H}}\int p(\mathcal{D},\mathcal{H}\mid\theta,\mathcal{G})P(\theta\mid\mathcal{G})d\theta
$$ 

involves a summation of exponentially many integrals over the parameter space — one for each assignment to the hidden variables $\mathcal{H}$ . On the other hand, the approximating term 

$$
P(\mathcal{D}_{\mathcal{G},\tilde{\boldsymbol{\theta}}_{\mathcal{G}}}^{*}\mid\mathcal{G})=\int p(\mathcal{D}_{\mathcal{G},\tilde{\boldsymbol{\theta}}_{\mathcal{G}}}^{*}\mid\boldsymbol{\theta},\mathcal{G})P(\boldsymbol{\theta}\mid\mathcal{G})d\boldsymbol{\theta}
$$ 

is only a single such integral. In both terms, the integrals are over a “complete” data set, so that one of these sums is on a scale that is exponentially larger than the other. 

One ad hoc solution is to simply correct for this discrepancy by estimating the diference: 

$$
\log P(\mathcal{D}\mid\mathcal{G})-\log P(\mathcal{D}_{\mathcal{G},\tilde{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G}).
$$ 

We use the asymptotic Laplace approximation to write each of these terms, to get: 

$$
\begin{array}{r c l}{{\log P(\mathcal{D}\mid\mathcal{G})-\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G})}}&{{\approx}}&{{\displaystyle\left[\log P(\mathcal{D}\mid\tilde{\theta}_{\mathcal{G}},\mathcal{G})-\frac{1}{2}\mathrm{Dim}[\mathcal{G}]\log M\right]}}\\ {{}}&{{}}&{{}}\\ {{}}&{{}}&{{-\left[\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\tilde{\theta}_{\mathcal{G}},\mathcal{G})-\frac{1}{2}\mathrm{Dim}[\mathcal{G}]\log M\right]}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{\log P(\mathcal{D}\mid\tilde{\theta}_{\mathcal{G}},\mathcal{G})-\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\tilde{\theta}_{\mathcal{G}},\mathcal{G})}}\end{array}
$$ 

The ﬁrst of these terms is the log-likelihood achieved by the MAP parameters on the observed data. The second is the log-likelihood on the ﬁctional data set, a term that can be computed in closed form based on the statistics of the ﬁctional data set. We see that the ﬁrst term is, again, a summation of an exponential number of terms representing diferent assignments to $\mathcal{H}$ . We note that the Laplace approximation is valid only at the large sample limit, but more careful arguments can show that this construction is actually fairly accurate for a large class of situations. 

Putting these arguments together, we can write: 

$$
\begin{array}{r c l}{{\log P(\mathcal{D}\mid\mathcal{G})}}&{{=}}&{{\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G})+\log P(\mathcal{D}\mid\mathcal{G})-\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G})}}\\ {{}}&{{\approx}}&{{\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G})+\log P(\mathcal{D}\mid\bar{\theta}_{\mathcal{G}},\mathcal{G})-\log P(\mathcal{D}_{\mathcal{G},\bar{\theta}_{\mathcal{G}}}^{*}\mid\bar{\theta}_{\mathcal{G}},\mathcal{G})}}\end{array}
$$ 

Cheeseman-Stutz score 

This approximation is the basis for the Cheeseman-Stutz score : 

$$
\mathrm{c}_{S}(\mathcal{G}~:~\mathcal{D})=\log P(\mathcal{G})+\log P(\mathcal{D}_{\mathcal{G},\widetilde{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G})+\log P(\mathcal{D}\mid\widetilde{\theta}_{\mathcal{G}},\mathcal{G})-\log P(\mathcal{D}_{\mathcal{G},\widetilde{\theta}_{\mathcal{G}}}^{*}\mid\mathcal{G}),
$$ 

The appealing property of the Cheeseman-Stutz score is that, unlike the BIC score, it uses the closed-form solution of the complete data marginal likelihood in the context of incomplete data. Experiments in practice (see box 19.G) show that this score is much more accurate than the BIC score and much cheaper to evaluate than the Laplace score. 

# 19.4.1.4 Candidate Method 

candidate method 

Another strategy for approximating the score is the candidate method ; it uses a particular choice of parameters (the candidate ) to evaluate the marginal likelihood. Consider any set of parameters $\theta$ . Using the chain law of probability, we can write $P(\mathcal{D},\boldsymbol{\theta}\mid\mathcal{G})$ in two diferent ways: 

$$
\begin{array}{r c l}{P(\mathcal D,\pmb\theta\mid\mathcal G)}&{=}&{P(\mathcal D\mid\pmb\theta,\mathcal G)P(\pmb\theta\mid\mathcal G)}\\ {P(\mathcal D,\pmb\theta\mid\mathcal G)}&{=}&{P(\pmb\theta\mid\mathcal D,\mathcal G)P(\mathcal D\mid\mathcal G).}\end{array}
$$ 

Equating the two right-hand terms, we can write 

$$
P(\mathcal{D}\mid\mathcal{G})=\frac{P(\mathcal{D}\mid\theta,\mathcal{G})P(\theta\mid\mathcal{G})}{P(\theta\mid\mathcal{D},\mathcal{G})}.
$$ 

The ﬁrst term in the numerator is the likelihood of the observed data given $\theta$ , which we should be able to evaluate using inference (exact or approximate). The second term in the numerator is the prior over the parameters, which is usually given. The denominator is the posterior over the parameters, the term most difcult to approximate. 

The candidate method reduces the problem of computing the marginal likelihood to the problem of generating a reasonable approximation to the parameter posterior $P(\pmb{\theta}\mid\mathcal{D},\mathcal{G})$ . It lets us estimate the likelihood when using methods such as MCMC sampling to approximate the posterior distribution. Of course, the quality of our approximation depends heavily on the design of the MCMC sampler. If we use a simple sampler, then the precision of our estimate of $P(\pmb\theta\mid\mathcal D,\mathcal G)$ will be determined by the number of sampled particles (since each particle either has these parameters or not). If, instead, we use collapsed particles, then each particle will have a distribution over the parameters, providing a better and smoother estimate for the posterior. 

The quality of our estimate also depends on the particular choice of candidate $\theta$ . We can obtain a more robust estimate by averaging the estimates from several choices of candidate parameters (say several likely parameter assignments based on our simulations). However, each of these requires inference for computing the numerator in equation (19.13), increasing the cost. 

An important property of the candidate method is that equation (19.13), on which the method is based, is not an approximation. Thus, if we could compute the denominator exactly, we would have an exact estimate for the marginal likelihood. This gives us the option of using more computational resources in our MCMC approximation to the denominator, to obtain increasingly accurate estimates. By contrast, the other methods all rely on an asymptotic approximation to the score and therefore do not ofer a similar trade-of of accuracy to computational cost. 

# 19.4.1.5 Variational Marginal Likelihood 

A diferent approach to estimating the marginal likelihood is using the variational approximations we discussed in section 19.3.3. Recall from corollary 19.1 that, for any distribution $Q$ , 

$$
\ell(\pmb\theta:{\mathcal D})=F_{\mathcal D}[\pmb\theta,Q]+\pmb D(Q(\mathcal H)\|P(\mathcal H\mid{\mathcal D},\pmb\theta)).
$$ 

It follows that the energy functional is a lower bound of the marginal likelihood, and the diference between them is the relative entropy between the approximate posterior distribution and the true one. Thus, if we ﬁnd a good approximation $Q$ of the posterior $P(\mathcal{H}\mid\mathcal{D},\boldsymbol{\theta})$ , then the relative entropy term is small, so that that energy functional is a good approximation of the marginal likelihood. As we discussed, the energy functional itself has the form: 

$$
F_{\mathcal{D}}[\pmb{\theta},Q]=\pmb{E}_{\mathcal{H}\sim Q}[\ell(\pmb{\theta}:\mathcal{D},\mathcal{H})]+\pmb{H}_{Q}(\mathcal{H}).
$$ 

Both of these terms can be computed using inference relative to the distribution $Q$ . Because this distribution was chosen to allow tractable inference, this provides a feasible approach for approximating the marginal likelihood. 

mixture distribution Box 19.G — Case Study: Evaluating Structure Scores. To study the diferent approximations to the Bayesian score in a restricted setting, Chickering and Heckerman (1997) consider learning a naive Bayes mixture distribution , as in section 19.2.2.4, where the cardinality $K$ of the hidden variable (the number of mixture components) is unknown. Adding more values to the class variables increases the representational power of the model, but also introduces new parameters and thus increases the ability of the model to overﬁt the data. Since the class of distributions that are representable with a cardinality of $K$ is contained within those that are representable with $^a$ cardinality of $K^{\prime}\,>\,K$ , the likelihood score increases monotonically with the cardinality of the class variable. The question is whether the diferent structure scores can pinpoint a good cardinality for the hidden variable. To do so, they perform MAP parameter learning on structures of diferent cardinality and then evaluate the diferent scores. Since the structure learning problem is one- dimensional (in the sense that the only parameter to learn is the cardinality of the class variable), there is no need to consider a speciﬁc search strategy in the evaluation. 

It is instructive to evaluate performance on both real data, and on synthetic data where the true number of clusters is known. However, even in synthetic data cases, where the true cardinality of the hidden variable is known, using this true cardinality as the “gold standard” for evaluating methods may not be appropriate, as with few data instances, the “optimal” model may be one with fewer parameters. Thus, Chickering and Heckerman instead compare all methods to the candidate method, using MCMC to evaluate the denominator; with enough computation, one can use this method to obtain high-quality approximations to the correct marginal likelihood. 

The data in the synthetic experiments were generated from a variety of models, which varied along several axes: the true cardinality of the hidden variable ( d ); the number of observed variables ( n ); and the number of instances $(M)$ . The ﬁrst round of experiments revealed few diferences between the diferent scores. An analysis showed that this was because synthetic data sets with random parameter choices are too easy. Because of the relatively large number of observed variables, such random models always had distinguished clusters. That is, using the true parameters, the posterior probability $P(c\mid x_{1},\cdot\cdot\cdot,x_{n})$ is close to 1 for the true cluster value and 0 for all others. Thus, the instances belonging to diferent clusters are easily separable, making the learning problem too easy. 

To overcome this problem, they considered sampling networks where the values of the parameters for $P(X_{i}\mid c)$ are correlated for diferent values of c . If the correlation is absolute, then the clusters overlap. For intermediate correlation the clusters were overlapping but not identical. By tuning the degree of correlation in sampling the distribution, they managed to generate networks with diferent degree of separation between the clusters. On data sets where the generating distribution did not have any separation between clusters, all the scores preferred to set the cardinality of the cluster variable to 1 , as expected. When they examined data sets where the generating distribution had partial overlap between clusters they saw diferentiating behavior between scoring methods. They also performed this same analysis on several real-world data sets. Figure 19.G.1 demonstrates the results for two data sets and summarizes the results for many of the synthetic data sets, evaluating the ability of the diferent methods to come close to the “optimal” cardinality, as determined by the candidate method. 

Overall, the results suggest that BIC tends to underﬁt badly, almost always selecting models with an overly low cardinality for the hidden variable; moreover, its score estimate for models of higher (and more appropriate) cardinality tended to decrease very sharply, making it very unlikely that 

![](images/29ac67c9bcdf87784c4969e81c51fe1c98a3cf28f5c3038de257abae18a512fa.jpg) 

Figure 19.G.1 — Evaluation of structure scores for a naive Bayes clustering model In the graphs in the top row, the $x$ axis denotes diferent cluster cardinalities, and the $y$ axis the marginal likeli- hood estimated by the method. The graph on the left represents synthetic data with $d=4$ , $n=128$ , and $M=400$ . The graph on the right represents a real-world data set, with $d=4$ , $n=35$ and $M=47$ . The table at bottom shows errors in model selection for the number of values of a hidden variable, as made by diferent approximations to the marginal likelihood. The errors are computed as diferences between the cardinality selected by the method and the “optimal” cardinality selected by the “gold standard” candidate method. The errors are averaged over ﬁve data sets. The blocks of lines correspond to experiments where one of the three parameters deﬁning the synthetic network varied while the others were held constant. (Adapted from Chickering and Heckerman (1997), with permission.) 

they would be chosen. The other asymptotic approximations were all reasonably good, although all of them tended to underestimate the marginal likelihood as the number of clusters grows. A likely reason is that many of the clusters tend to become empty in this setting, giving rise to a “ridge-shaped” likelihood surface, where many parameters have no bearing on the likelihood. In this case, the “peak”-shaped estimate of the likelihood used by the asymptotic approximations tends to underestimate the true value of the integral. Among the diferent asymptotic approximations, the Cheeseman-Stutz approximation using the MAP conﬁguration of the parameters had a slight edge over the other methods in its accuracy, and was more robust when dealing with parameters that are close to 0 or 1. It was also among the most efcient of the methods (other than the highly inaccurate BIC approach). 

# 19.4.2 Structure Search 

# 19.4.2.1 A Naive Approach 

Given a deﬁnition of the score, we can now consider the structure learning task. In the most general terms, we want to explore the set of graph structures involving the variables of interest, score each one of these, and select the highest-scoring one. For some learning problems, such as the one discussed in box 19.G, the number of structures we consider is relatively small, and thus we can simply systematically score each structure and ﬁnd the best one. 

This strategy, however, is infeasible for most learning problems. Usually the number of structures we want to consider is very large — exponential or even super exponential in the number of variables — and we do not want to score all them. In section 18.4, we discussed various optimization procedures that can be used to identify a high-scoring structures. As we showed, for certain types of constraints on the network structure — tree-structured networks or a given node ordering (and bounded indegree) — we can actually ﬁnd the optimal structure efciently. In the more general case, we apply a hill-climbing procedure, using search operators that consist of local network modiﬁcations, such as edge addition, deletion, or reversal. 

Unfortunately, the extension of these methods to the case of learning with missing data quickly hits a wall, since all of these methods relied on the decomposition of the score into a sum of individual family scores. This requirement is obvious in the case of learning tree-structured networks and in the case of learning with a ﬁxed ordering: in both cases, the algorithm relied explicitly on the decomposition of the score as a sum of family scores. 

The difculty is a little more subtle in the case of the hill-climbing search. There, in each iteration, we consider applying $O(n^{2})$ possible search operators (approximately 1–2 operators for each possible edge in the network). This number is generally quite large, so that the evaluation of the diferent possible steps in the search is a signiﬁcant computational bottleneck. Although the same issue arises in the complete data case, there we could signiﬁcantly reduce the computational burden due to two ideas. First, since the score is based on sufcient statistics, we could cache sufcient statistics and reuse them. Second, since the score is decomposable, the change in score of many of the operators is oblivious to modiﬁcations in another part of the network. Thus, as we showed in section 18.4.3.3, once we compute the delta-score of an operator $O$ relative to a candidate solution $\mathcal{G}$ : 

$$
\delta(\mathcal{G}\ :\ o)=\mathrm{score}(o(\mathcal{G})\ :\ \mathcal{D})-\mathrm{score}(\mathcal{G}\ :\ \mathcal{D}),
$$ 

the same quantity is also the delta-score $\delta(\mathcal{G}^{\prime}\,\,\,\,:\,\,\,\,o)$ fo any o ${\mathcal{G}}^{\prime}$ hat is similar to $\mathcal{G}$ in the local topology that is relevant f $O$ . For example, if o adds $X\rightarrow Y$ → , then the de -score remains unchanged for any graph G ${\mathcal{G}}^{\prime}$ for which the family of Y $Y$ is the same as in G . The decomposition property implied that the search procedure in the case of complete data could maintain a priority queue of the efect of diferent search operators from previous iterations of the algorithm and avoid repeated computations. 

When learning from incomplete data, the situation is quite diferent. As we discussed, local changes in the structure can result in global changes in the likelihood function. Thus, after a local structure change, the parameters of all the CPDs might change. As a consequence, the score is not decomposable; that is, the delta-score of one local modiﬁcation (for example, adding an arc) can change after we modify a remote part of the network. 

![](images/090cf9ca6aa432ea2acbbaa5ae408ca7da5c015a1730ead6f6d44121c6e68124.jpg) 
Figure 19.10 Non de com pos ability of structure scores in the case of missing data. (a) A training set over variables $X_{1},\dots,X_{4}$ . (b) Four possible networks over $X_{1},\dots,X_{4}$ and a hidden variable $C$ . Arrows from the top network to the other three are labeled with the change in log-likelihood (LL) and Cheeseman- Stutz (CS) score, respectively. The baseline score (for $\mathcal{G}_{00}.$ ) is: $-336.5$ r the LL score, and $-360$ f he CS score. We can se that the contribution of adding the arc $C\rightarrow X_{3}$ → is radically diferent when $X_{4}$ is added as a child of C . This example shows that both the log-likelihood and the Cheeseman-Stutz score are not decomposable. 

Consider a task of learning a network structure for clustering, where we are also trying to determine whether diferent features are relevant. More precisely, assume we have a hidden variable $C$ , and four possibly related variables $X_{1},\dots,X_{4}$ . Assume that we have already decided that both $X_{1}$ and $X_{2}$ are children of $C$ , and are trying to decide which (if any) of the edges $C\rightarrow X_{3}$ and $C\to X_{4}$ to include in the model, thereby giving rise to the four possible models in ﬁgure 19.10a. Our training set is as shown in ﬁgure 19.10b, and the resulting d relative to the baseline ne rk $\mathcal{G}_{00}$ are shown in (c). As we can see, adding the edge $C\,\rightarrow\,X_{3}$ → to the original structure G $\mathcal{G}_{00}$ leads only to a small improvement in the likelihood, and a slight decrease in the Cheeseman-Stutz score. However, adding the edge $C\rightarrow X_{3}$ to the structure $\mathcal{G}_{01}$ , where we also have $C\to X_{4}$ , leads to $^a$ substantial improvement. 

This example demonstrates a situation where the score is not decomposable. The intuition here is simple. In the structure $\mathcal{G}_{00}$ , the hidden variable $C$ is “tuned” to capture the dependency between $X_{1}$ and $X_{2}$ . In this network structure, there is a weak dependency between these two variables and $X_{3}$ . In $\mathcal{G}_{10}$ , the hidden variable has more or less the same role, and therefore there is l explanatory beneﬁt for $X_{3}$ in adding the edge to the hidden variable. However, when we add $X_{3}$ and $X_{4}$ together, the hidden variable shifts to capture the strong dependency between $X_{3}$ and $X_{4}$ while still capturing some of the dependencies between $X_{1}$ and $X_{2}$ . Thus, the score improves dramatically, and in a nonadditive manner. 

As a consequence of these problems, a search procedure that uses one of the scores we discussed has to evaluate the score of each candidate structure it considers, and it cannot rely on cached computations. In all of the scores we considered, this evaluation involves nontrivial computations (for example, running EM or an MCMC procedure) that are much more expensive than the cost of scoring a structure in the case of complete data. The actual cost of computation in these steps depends on the network structure (that is, the cost of inference in the network) and the number of iterations to convergence. Even in simple networks (for example, ones with a single hidden variable) this computation is an order of magnitude longer than evaluation of the score in complete data. 

The main problem is that, in this type of search, most of the computation results are discarded. To understand why, recall that to select a move $O$ from our current graph $\mathcal{G}$ , we ﬁrst evaluate all candidate successors $o(\mathcal{G})$ . To evaluate each candidate structure $o(\mathcal{G})$ , we compute the MLE or MAP parameters for $o(\mathcal{G})$ , score it, and then compare it to the score of other candidate we consider at this point. Since we select to apply only one of the proposed search operators o at each iteration, the parameters learned for other structures $o^{\prime}(\mathcal{G})$ are not needed. In practice, search using the modify-score-discard strategy is rarely feasible; it is useful only for learning in small domains, or when we have many constraints on the network structure, and so do not have many choices at each decision point. 

# 19.4.2.2 Heuristic Solutions 

There are several heuristics for avoiding this signiﬁcant computational cost. We list a few of them here. We note that non de com pos ability is a major issue in the context of Markov network learning, and so we return to these ideas in much greater length in section 20.7. 

One approach is to construct “quick and dirty” estimates of the change in score. We can employ such estimates in a variety of ways. In one approach, we can simply use them as our estimates of the delta-score in any of the search algorithms used earlier. Alternatively, we can use them as a pruning mechanism, focusing our attention on the moves whose estimated change in score is highest and evaluating them more carefully. This approach uses the estimates to prioritize our computational resources and invest computation on careful evaluation of the real change in score for fewer modiﬁcations. 

There are a variety of diferent approaches we can use to estimate the change in score. One approach is to use computations of delta-scores acquired in previous steps. More precisely, suppose we are at a structure $\mathcal{G}_{0}$ , and evaluate a search operator whose delta-score is $\delta(\mathcal{G}_{0}~:~o)$ . We can then assume for the next rounds of search that the delta-score for this operator has not changed, even though the network structure has changed. This approach allows us to cache the results computation for at least some number of subsequent iterations (as though we are learning from complete data). In efect, this approach approximates the score as decomposable, at least for the duration of a few iterations. Clearly, this approach is only an approximation, but one that may be quite reasonable: even if the delta-scores themselves change, it is not unreasonable to assume that a step that was good relative to one structure is often probably good also relative to a closely related structure. Of course, this assumption can also break down: as the score is not decomposable, applying a set of “beneﬁcial” search operators together can lead to structures with worse score. 

The implementation of such a scheme requires us to make various decisions. How long do we maintain the estimate $\delta(\mathcal{G}\ :\ o)^{\dagger}$ ? Which other search operators invalidate this estimate after they are applied? There is no clear right answer here, and the actual details of implementations of this heuristic approach difer on these counts. 

Another approach is to compute the score of the modiﬁed network, but assume that only the parameters of the changed CPD can be optimized. That is, we freeze all of the parameters except those of the s $P(X\mid U)$ whose family composition has changed, and optimize the parameters of $P(X\mid U)$ | using gradient ascent or EM. When we optimize only the parameters of a single CPD, EM or gradient ascent should be faster for two reasons. First, because we have only a few parameters to learn, the convergence is faster. Second, because we modify only the parameters of a single CPD, we can cache intermediate computations; see exercise 19.15. 

The set of parameter iz at ions where only the CPD $P(X\mid U)$ is allowed to change is a subset of the set of all possible parameter iz at ions for our network. Hence, any likelihood that we can achieve in this case would also be achievable if we ran a full optimization. As a consequence, the estimate of the likelihood in the modiﬁed network is a lower bound of the actual likelihood we can achieve if we can optimize all the parameters. If we are using a score such as the BIC score, this estimate is a proven lower bound on the score. If we are using a score such as the Cheeseman-Stutz score, this argument is not valid, but appears generally to hold in practice. That is, the score of the network with frozen parameters will be usually either smaller or very close to that of the one were we can optimize all parameters. 

More generally, if a heuristic estimate is a proven lower bound or upper bound, we can improve the search procedure in a way that is guaranteed not to lose the optimal candidates. In the case of a lower bound, an estimated value that is higher than moves that we have already evaluated allows us to prune those other moves as guaranteed to be suboptimal. Conversely, if we have a move with a guaranteed upper bound that is lower than previously evaluated candidates, we can safely eliminate it. In practice, however, such bounds are hard to come by. 

# 19.4.3 Structural EM 

We now consider a diferent approach to constructing a heuristic that identiﬁes helpful moves during the search. This approach shares some of the ideas that we discussed. However, by putting them together in a particular way, it provides signiﬁcant computational savings, as well as certain guarantees. 

# 19.4.3.1 Approximating the Delta-score 

One efcient approach for approximating the score of network is to construct some complete data set $\mathcal{D}^{*}$ , and then apply a score based on the complete data set. This was precisely the intuition that motivated the Cheeseman-Stutz approximation. However, the Cheeseman-Stutz approximation is computationally expensive. The data set we use $-\;\mathcal{D}_{\mathcal{G},\tilde{\boldsymbol{\theta}}_{\mathcal{G}}}^{*}\;-$ — is constructed G G by ﬁnding the MAP parameters for our current candidate $\mathcal{G}$ . We also needed to introduce a correction term that would improve the approximation to the marginal likelihood; this correction term required that we run inference over $\mathcal{G}$ . Because these steps must be executed for each candidate network, this approach quickly becomes infeasible for large search spaces. 

However, what if we do not want to obtain an accurate approximation to the marginal completed data likelihood? Rather, we want only a heuristic that would help identify useful moves in the space. In this case, one simple heuristic is to construct a single completed data set $\mathcal{D}^{*}$ and use it to evaluate multiple diferent search steps. That is, to evaluate a search operator $O$ , we deﬁne 

$$
\hat{\delta}_{\mathcal{D}^{*}}(\mathcal{G}\ :\ o)=\mathrm{score}(o(\mathcal{G})\ :\ \mathcal{D}^{*})-\mathrm{score}(\mathcal{G}\ :\ \mathcal{D}^{*}),
$$ 

where we can use any complete-data scoring function for the two terms on the right-hand side. The key observation is that this expression is simply a delta-score relative to a complete data set, and it can therefore be evaluated very efciently. We will return to this point. 

Clearly, the results of applying this heuristic depend on our choice of completed data set $\mathcal{D}^{*}$ , an observation immediately raises some important questions: How do we deﬁne our completed data set D $\mathcal{D}^{*}\boldsymbol{\underline{{\xi}}}$ ? Can we provide any guarantees on the accuracy of this heuristic? One compelling answer to these questions is obtained from the following result: 

Theorem 19.10 Let $\mathcal{G}_{0}$ be a gra structure and $\overline{{\tilde{\theta}}}_{0}$ be the MAP parameters for $\mathcal{G}_{0}$ given a data set $\mathcal{D}$ . Then for any graph structure G : 

$$
{}^{3I C}(\mathcal G\ :\ \mathcal D_{\mathcal G_{0},\bar{\theta}_{0}}^{*})-\mathrm{score}_{B I C}(\mathcal G_{0}\ :\ \mathcal D_{\mathcal G_{0},\bar{\theta}_{0}}^{*})\leq\mathrm{score}_{B I C}(\mathcal G\ :\ \mathcal D)-\mathrm{score}_{B I C}(\mathcal G_{0}\ :\ \mathcal D)
$$ 

This theorem states that the true improvement in the BIC score of network $\mathcal{G}$ , relative to the network $\mathcal{G}_{0}$ that we used to construct our completed data $\mathcal{D}_{\mathcal{G}_{0},\tilde{\boldsymbol{\theta}}_{0}}^{*}$ G , is at least as large as the estimated improvement of the score using the completed data $\mathcal{D}_{\mathcal{G}_{0},\tilde{\boldsymbol{\theta}}_{0}}^{*}$ . 

The proof of this theorem is essentially the same as the proof of theorem 19.5; see ex- ercise 19.25. Although the analogous result for the Bayesian score is not true (due to the nonlinearity of the $\Gamma$ function used in the score), it is approximately true, especially when we have a reasonably large sample size; thus, we often apply the same ideas in the context of the Bayesian score, albeit without the same level of theoretical guarantees. 

This result suggests the following scheme. Consider a graph structure $\mathcal{G}_{0}$ . We compute its MAP parameters $\theta_{0}$ , and construct a complete (fractional) data set $\mathcal{D}_{\mathcal{G}_{0},\tilde{\boldsymbol{\theta}}_{0}}^{*}$ G . We can now use the BIC score relative to this completed data set to evaluate the delta-score for any modiﬁcation $O$ to $\mathcal{G}$ . We can thus deﬁne 

$$
\begin{array}{r}{\hat{\delta}_{\mathcal{D}_{\mathcal{G}_{0},\tilde{\theta}_{0}}^{*}}(\mathcal{G}\ :\ o)=\mathrm{score}_{B I C}(o(\mathcal{G})\ :\ \mathcal{D}_{\mathcal{G}_{0},\tilde{\theta}_{0}}^{*})-\mathrm{score}_{B I C}(\mathcal{G}\ :\ \mathcal{D}_{\mathcal{G}_{0},\tilde{\theta}_{0}}^{*}).}\end{array}
$$ 

The theorem guarantees that our heuristic estimate for the delta-score is a lower bound on the true change in the BIC score. The fact that this estimate is a lower bound is signiﬁcant, since it guarantees that any change that we make that improves the estimated score will also improve the true score. 

# 19.4.3.2 The Structural EM Algorithm 

Importantly, the preceding guarantee holds not just for the application of a single operator, but also for any series of changes that modify $\mathcal{G}_{0}$ . Thus, we can use our completed data set $\mathcal{D}_{\mathcal{G}_{0},\tilde{\boldsymbol{\theta}}_{0}}^{*}$ G to estimate and apply an arbitrarily long sequence of operators to $\mathcal{G}_{0}$ ; as long as we have that 

$$
\mathrm{score}_{B I C}(\mathcal G\ :\ \mathcal D_{\mathcal G_{0},\tilde{\theta}_{0}}^{*})>\mathrm{score}_{B I C}(\mathcal G_{0}\ :\ \mathcal D_{\mathcal G_{0},\tilde{\theta}_{0}}^{*})
$$ 

for our new graph $\mathcal{G}$ , we are guaranteed that the true score of $\mathcal{G}$ is also better. 

![](images/11c3c57e69026089f3cb52fa78af4efddc1da07181160befeb154ab085ef410a.jpg) 

However, we must take care in interpreting this guarantee. Assume that we have already modiﬁed $\mathcal{G}_{0}$ in several ways, to obtain a new graph $\mathcal{G}$ . Now, we are considering a new operator $O_{i}$ , and are interested in determining whether that operator is an improvement; that is, we wish to estimate the delta-score: $\mathrm{score}_{B I C}(o(\mathcal G)\ :\ \mathcal D)-\mathrm{score}_{B I C}(\mathcal G\ :\ \mathcal D)$ . The theorem tells us that if $o(\mathcal{G})$ satisﬁes $\mathrm{score}_{B I C}(o(\mathcal{G})\ :\ \mathcal{D}_{\mathcal{G}_{0},\tilde{\theta}_{0}}^{*})>\mathrm{score}_{B I C}(\mathcal{G}_{0}\ :\ \mathcal{D}_{\mathcal{G}_{0},\tilde{\theta}_{0}}^{*})$ , then it is necessarily G G better than our original graph $\mathcal{G}_{0}$ . However, it does not follow that if $\hat{\delta}_{\mathcal{D}_{\mathcal{G}_{0},\tilde{\theta}_{0}}^{*}}(\mathcal{G}\;\;:\;\;o)\,>\,0$ , then $o(\mathcal{G})$ is necessarily better than $\mathcal{G}$ . In other words, we can verify that each of the graphs we construct improves over the graph used to construct the completed data set, but not that each operator improves over the previous graph in the sequence. Note that we are guaranteed that our estimate is a true lower bound for any operator applied directly to $\mathcal{G}_{0}$ . Intuitively, we believe that our estimates are likely to be reasonable for graphs that are “similar” to $\mathcal{G}_{0}$ . (This intuition was also the basis for some of the heuristics described in section 19.4.2.2.) However, as we move farther away, our estimates are likely to degrade. Thus, at some point during our search, we probably want to select a new graph and construct a more relevant complete data set. 

structural EM 

This observation suggests an EM-like algorithm, called structural EM , shown in algorithm 19.3. In structural EM, we iterate over a pair of steps. In the E-step, we use our current model to generate (perhaps implicitly) a completed data set, based on which we compute expected sufcient statistics. In the M-step, we use these expected sufcient statistics to improve our model. The biggest diference is that now our M-step can improve not only the parameters, but also the structure. (Note that the structure-learning step also reestimates the parameters.) The structure learning procedure in the M-step can be any of the procedures we discussed in section 18.4, whether a general-purpose heuristic search or an exact search procedure for a specialized subset of networks for which we have an exact solution (for example, a maximum weighted spanning tree procedure for learning trees). If we use the BIC score, theorem 19.10 guarantees that, if this search procedure ﬁnds a structure that is better than the one we used in the previous iteration, then the structural EM procedure will monotonically improve the score. 

Since the scores are upper-bounded, the algorithm must converge. Unlike the case of EM, we cannot, however, prove that the structure it ﬁnds is a local maximum. 

# 19.4.3.3 Structural EM for Selective Clustering 

We now illustrate the structural EM algorithm on a particular class of networks. Consider the task of learning structures that generalize our example of example 19.14; these networks are similar to the naive Bayes clustering of section 19.2.2.4, except that some observed variables may be independent of the cluster variable. Thus, in our structure, the class variable $C$ is a root, and each observed attribute $X_{i}$ is either a child of $C$ or a root by itself. This limited set of structures contains $2^{n}$ choices. 

Before discussing how to learn these structures using the ideas we just explored, let us consider why this problem is an interesting one. One might claim that, instead of structure learning, we can simply run parameter learning within the full structure (where each $X_{i}$ is a child of $C$ ); after all, if $X_{i}$ is independent of $C$ , then we can capture this independence within the parameters of the CPD $P(X_{i}\mid C)$ . However, as we discussed, statistical noise in the sampling process guarantees that we will never have true independence in the empirical distribution. Learning a more restricted model with fewer edges is likely to result in more robust clustering. Moreover, this approach allows us to detect irrelevant attributes during the clustering, providing insight into the domain. 

If we have a complete data set, learning in this class of models is trivial. Since this class of structures is such that we cannot have cycles, if the score is decomposable, the choice of family for $X_{i}$ does not impact the choice of parents for $X_{j}$ . Thus, we can simply select the optimal family for each $X_{i}$ separately: either $C$ is its only parent, or it has no parents. We can thus select the optimal structure using $2n$ local score evaluations. 

The structural EM algorithm applies very well in this setting. We initialize each iteration with our current structure $\mathcal{G}_{t}$ . We then perform the following steps: 

• Run parameter estimation (such as EM or gradient ascent) to learn parameters $\tilde{\pmb{\theta}}_{t}$ for $\mathcal{G}_{t}$ . 

• Construct a new structure $\mathcal{G}_{t+1}$ so that $\mathcal{G}_{t+1}$ contains the edge $C\to X_{i}$ if 

$$
\operatorname{Famsccore}(X_{i}\mid\{C\}\ :\ \mathcal{D}_{\mathcal{G}_{t},\tilde{\theta}_{t}}^{*})>\operatorname{Famsccore}(X_{i}\mid\varnothing\ :\ \mathcal{D}_{\mathcal{G}_{t},\tilde{\theta}_{t}}^{*}).
$$ 

We continue this procedure until convergence, that is, until an iteration that makes no changes to the structure. 

According to theorem 19.10, if we use the BIC score in this procedure, then any improvement to our expected score based on $\mathcal{D}_{\mathcal{G}_{t},\tilde{\boldsymbol{\theta}}_{t}}^{*}$ is guaranteed to give rise to an improvement in the true G BIC score; that is, $\mathrm{score}_{B I C}(\mathcal{G}_{t+1}\dot{\mathrm{~\tiny~\longrightarrow~}}\dot{\mathcal{D}})\,\ge\,\mathrm{score}_{B I C}(\mathcal{G}_{t}\,\,\,:\,\,\,\mathcal{D})$ D ≥ G D . Thus, each iteration (until convergence) improves the score of the model. 

One issue in implementing this procedure is how to evaluate the family scores in each iteration: $\operatorname{Famsccore}(X_{i}\mid\emptyset\ \ :\ \ {\mathcal{D}}_{{\mathcal{G}}_{t},{\tilde{\boldsymbol{\theta}}}_{t}}^{*})$ and $\operatorname{Famsccore}(X_{i}\mid\{C\}\ :\ \mathcal{D}_{\mathcal{G}_{t},\tilde{\boldsymbol{\theta}}_{t}}^{*})$ . The ﬁrst term G G depends on sufcient statistics for $X_{i}$ in the data set; as $X_{i}$ is fully observed, these can be collected once and reused in each iteration. The second term requires sufcient statistics of $X_{i}$ and $C$ in $\mathcal{D}_{\mathcal{G}_{t},\tilde{\boldsymbol{\theta}}_{t}}^{*};$ ; here, we need to compute: 

$$
\begin{array}{r c l}{{\bar{M}_{\mathcal{D}_{\mathcal{G}_{t},\bar{\boldsymbol{\theta}}_{t}}^{*}}[x_{i},c]}}&{{=}}&{{\displaystyle\sum_{m}P(C[m]=c,X_{i}[m]=x_{i}\mid\boldsymbol{o}[m],\mathcal{G}_{t},\tilde{\boldsymbol{\theta}}_{t})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{m,X_{i}[m]=x_{i}}P(C[m]=c\mid\boldsymbol{o}[m],\mathcal{G}_{t},\tilde{\boldsymbol{\theta}}_{t}).}}\end{array}
$$ 

We can collect all of these statistics with a singe pass over the data, where we compute the posterior over $C$ in each instance. Note that these are the statistics we need for parameter learning in the full naive Bayes network, where each $X_{i}$ is connected to $C$ . In some of the iterations of the algorithm, we will compute these statistics even though $X_{i}$ and $C$ are independent in $\mathcal{G}_{t}$ . Somewhat surprisingly, even when the joint counts of $X_{i}$ and $C$ are obtained from a model where these two variables are independent, the expected counts can show a dependency between them; see exercise 19.26. 

Note that this algorithm can take very large steps in the space. Speciﬁcally, the choice of edges in each iteration is made from scratch, independently of the choice in the previous structure; thus, $\mathcal{G}_{t+1}$ can be quite diferent from $\mathcal{G}_{t}$ . e, this observation is true only up to a point, since the use of the distribution based on $(\mathcal{G}_{t},\tilde{{\boldsymbol{\theta}}}_{t})$ G does bias the reconstruction to favor some aspects of the previous iteration. This point goes back to the inherent non de com pos ability of the score in this case, which we saw in example 19.14. To understand the limitation, consider the convergence point of EM for a particular graph structure where $C$ has a particular set of children $X$ . At this point, the learned model is optimized so that $C$ captures (as much as possible) the dependencies between its children in $X$ , to allow the variables in $X$ to be conditionally independent given $C$ . Thus, diferent choices of $X$ will give rise to very diferent models. When we change the set of children, we change the information that $C$ represents, and thus change the score in a global way. As a consequence, the choice of $\mathcal{G}_{t}$ that we used to construct the completed data does afect our ability to add certain edges into the graph. 

This issue brings up the important question of how we can initialize this search procedure. A simple initialization point is to use the full network, which is essentially the naive Bayes clustering network, and let the search procedure prune edges. An alternative is to start with a random subset of edges. Such a randomized starting point can allow us to discover “local maxima” that are not accessible from the full network. One might also tempted to use the empty network as a starting point, and then consider adding edges. It is not hard to show, however, that the empty network is a bad starting point: structural EM will never add a new edge if we initialize the algorithm with the empty network; see exercise 19.27. 

# 19.4.3.4 An Efective Implementation of Structural EM 

Our description of the structural EM procedure is at a somewhat abstract level, and it lends itself to diferent types of implementations. The big unresolved issue in this description is how to represent and manage the completed data set created in the E-step. Recall that the number of completions of each instance is exponential in the number of missing values in that instance. If we have a single hidden variable, as in the selective naive Bayes classiﬁer of section 19.4.3.3, then storing all completions (and their relative weights) might be a feasible implementation. However, if we have several unobserved variables in each instance, then this solution rapidly becomes impractical. 

We can, however, exploit the fact that procedures that learn from complete data sets do not need to access all the instances; they require only sufcient statistics computed from the data set. Thus, we do not need to maintain all the instances of the completed data set; we need only to compute the relevant sufcient statistics in the completed data set. These sufcient statistics are, by deﬁnition, the expected sufcient statistics based on the current model $(\mathcal{G}_{t},\theta_{t})$ and the observed data. This is precisely the same idea that we utilized in the E-step of standard EM for parameter estimation. However, there is one big diference. In parameter estimation, we know in advance the sufcient statistics we need. When we perform structure learning, this is no longer true. When we change the structure, we need a new set of sufcient statistics for the parts of the model we have changed. For example, if in the original network $X$ is a root, then, for parameter estimation, we need only sufcient statistics of $X$ alone. Now, if we consider adding $Y$ as a parent of $X$ , we need the joint statistics of $X$ and $Y$ together. If we do add the edge $Y\rightarrow X$ , and ow consider $Z$ as an additional parent of $X$ , we now need the joint statistics of $X,Y$ , and Z . 

This suggests that the number of sufcient statistics we may need can be quite large. One strategy is to compute in advance the set of sufcient statistics we might need. For specialized classes of structures, we may know this set exactly. For example, in the clustering scenario that we examined in section 19.4.3.3, we know the precise sufcient statistics that are needed for the M-step. Similarly, if we restrict ourselves to trees, we know that we are interested only in pairwise statistics and can collect all of them in advance. If we are willing to assume that our network has a bounded indegree of at most $k$ , then we can also decide to precompute all sufcient statistics involving $k+1$ or fewer variables; this approach, however, can be expensive for $k$ greater than two or three. An alternative strategy is to compute sufcient statistics “on demand” as the search progresses through the space of diferent structures. This approach allows us to compute only the sufcient statistics that the search procedure requires. However, it requires that we revisit the data and perform new inference queries on the instances; moreover, this inference generally involves variables that are not together in a family and therefore may require out-of-clique inference, such as the one described in section 10.3.3.2. 

Importantly, however, once we compute sufcient statistics, all of the decomposability prop- erties for complete data that we discussed in section 18.4.3.3 hold for the resulting delta-scores. Thus, we can apply our caching-based optimizations in this setting, greatly increasing the computational efciency of the algorithm. This property is key to allowing the structural EM algorithm to scale up to large domains with many variables. 

# 19.5 Learning Models with Hidden Variables 

In the previous section we examined searching for structures when the data are incomplete. In that discussion, we conﬁned ourselves to structures involving a given set of variables. Although this set can include hidden variables, we implicitly assumed that we knew of the existence of these variables, and could simply treat them as an extreme case of missing data. Of course, it is important to remember that hidden variables introduce important subtleties, such as our inability to identify the model. Nevertheless, as we discussed, in section 16.4.2, hidden variables are useful for a variety of reasons. 

In some cases, prior knowledge may tell us that a hidden variable belongs in the model, and perhaps even where we should place it relative to the other variables. In other cases (such as the naive Bayes clustering), the placement of the hidden variable is dictated by the goals of our learning (clustering of the instances into coherent groups). In still other cases, however, we may want to infer automatically that it would be beneﬁcial to introduce a hidden variable into the model. This opportunity raises a whole range of new questions: When should we consider introducing a hidden variable? Where in the network should we connect it? How many values should we allow it to have? 

In this section, we ﬁrst present some results that provide intuition regarding the role that a hidden variable can play in the model. We then describe a few heuristics for dealing with some of the computational questions described before. 

# 19.5.1 Information Content of Hidden Variables 

information content 

One can view the role of a hidden variable as a mechanism for capturing information about the interaction between other variables in the network. In our example of the network of ﬁgure 16.1, we saw that the hidden variable “conveyed” information from the parents $X_{1},X_{2},X_{3}$ to the children $Y_{1},Y_{2},Y_{3}$ . Similarly, in the naive Bayes clustering network of ﬁgure 3.2, the hidden variable captures information between its children. These examples suggest that, in learning a model for the hidden variable, we want to maximize the information that the hidden variable captures about its children. We now show that learning indeed maximizes a notion of information between the hidden variable and its children. We analyze a speciﬁc example, the naive Bayes network of ﬁgure 3.2, but the ideas can be generalized to other network structures. Suppose we observe $M$ samples of $X_{1},\dots,X_{n}$ and use maximum likelihood to learn the parameters of the network. Any choice of parameter set $\theta$ deﬁnes a distribution $\hat{Q}_{\theta}$ over $X_{1},\dots,X_{n},H$ so that 

$$
\hat{Q}_{\theta}(h,x_{1},\ldots,x_{n})=\hat{P}_{\mathcal{D}}(x_{1},\ldots,x_{n})P(h\mid x_{1},\ldots,x_{n},\theta),
$$ 

where $\hat{P}_{\mathcal{D}}$ is the empirical distribution of the observed variables in the data. This is essentially D the augmentation of the empirical distribution by our stochastic “reconstruction” of the hidden variable. 

Consider for a moment a complete data set $\langle\mathcal{D},\mathcal{H}\rangle$ , where $H$ is also observed. Proposition 18.1 shows that 

$$
\operatorname*{max}_{\pmb{\theta}}\frac{1}{M}\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)=\sum_{i}\pmb{I}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(X_{i};H)-\pmb{H}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(H)-\sum_{i}\pmb{H}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(X_{i}).
$$ 

We now show that a similar relationship holds in the case of incomplete data; in fact, this relationship holds not only at the maximum likelihood point but also in other points of the parameter space: 

# Proposition 19.2 

Let $\mathcal{D}$ be a data set where $X_{1},\ldots,X_{n}$ are observed and $\theta^{0}$ be a choice of parame s for the network of figure 3 . 2 . Deﬁne θ $\theta^{1}$ to be the result of an EM-iteration if we start with θ $\theta^{0}$ (that is, the result of an $M\cdot$ -step if we use sufcient statistics from $\hat{Q}_{\theta^{0}}$ ). Then 

$$
\frac{1}{M}\ell(\pmb\theta^{0}:\mathcal D)\leq\sum_{i}\pmb I_{\hat{Q}_{\pmb\theta^{0}}}(X_{i};H)-\sum_{i}\pmb H_{\hat{P}_{\mathcal D}}(X_{i})\leq\frac{1}{M}\ell(\pmb\theta^{1}:\mathcal D).
$$ 

Roughly speaking, this result states that the information-theoretic term is approximately equal to the likelihood. When $\theta^{0}$ is a local maxima of the likelihood, we have that $\pmb\theta^{1}=\pmb\theta^{0}$ , and so we have equality in the left-hand and right-hand sides of equation (19.16). For other parameter choices, the information-theoretic term can be larger than the likelihood, but not by “too much,” since it is bounded above by the next iteration of EM. Both the likelihood and the information- theoretic term have the same maxima. 

Because the entropy terms $H_{\hat{P}_{\mathcal{D}}}(X_{i})$ do not depend on $\theta$ his result implies that maximizing D the likelihood is equivalent to ﬁnding a hidden variable H that maximizes the information about each of the observed variables. Note that the information here is deﬁned in terms of the distribution $\hat{Q}_{\theta^{0}}$ , as in equation (19.14). This information measures what $H$ conveys about each of the observed variables in the posterior distribution given the observations. This is quite intuitive: For example, assume we learn a model, and after observing $x_{1},\ldots,x_{n}$ , our posterior over $H$ has $H\,=\,h^{1}$ with high probability. In this case, we are fairly sure about the cluster assignment of the cluster, so if the clustering is informative, we can conclude quite a bit of information about the value of each of the attributes. 

Finally, it is useful to compare this result to the complete data case of equation (19.15); there, we had an additional $-H(H)$ term, which accounts for the observations of $H$ . In the case of incomplete data, we do not observe $H$ and thus do not need to account for it. Intuitively, since we sum over all the possible values of $H$ , we are not penalized for more complex (higher entropy) hidden variables. This diference also shows that adding more values to the hidden variable will always improve the likelihood. As we add more values, the hidden variable can only become more informative about the observed variables. Since our likelihood function does not include a penalty term for the entropy of $H$ , this score does not penalize for the increased number of values of $H$ . 

We now turn to the proof of this proposition. Proof Deﬁne $Q\big(\mathcal{H}\big)=P\big(\mathcal{H}\mid\mathcal{D},\boldsymbol{\theta}^{0}\big)$ , then, by corollary 19.1 we have that 

$$
\ell(\pmb\theta^{0}:\mathcal D)=\pmb E_{Q}\big[\ell(\pmb\theta^{0}:\langle\mathcal D,\mathcal H\rangle)\big]+\pmb H_{Q}(\mathcal H).
$$ 

Moreover, if $\pmb{\theta}^{1}=\arg\operatorname*{max}_{\pmb{\theta}}\pmb{E}_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]$ ⟨D H⟩ , then 

$$
\pmb{E}_{Q}\left[\ell(\pmb{\theta}^{0}:\langle\mathcal{D},\mathcal{H}\rangle)\right]\leq\pmb{E}_{Q}\left[\ell(\pmb{\theta}^{1}:\langle\mathcal{D},\mathcal{H}\rangle)\right].
$$ 

Finally, we can use corollary 19.1 again and get that 

$$
E_{Q}\left[\ell(\pmb{\theta}^{1}:\langle\mathscr{D},\mathscr{H}\rangle)\right]+\pmb{H}_{Q}(\mathscr{H})\leq\ell(\pmb{\theta}^{1}:\mathscr{D}).
$$ 

Combining these three inequalities, we conclude that 

$$
\begin{array}{r}{\ell(\pmb{\theta}^{0}:\mathcal{D})\leq\pmb{E}_{Q}\big[\ell(\pmb{\theta}^{1}:\langle\mathcal{D},\mathcal{H}\rangle)\big]+\pmb{H}_{Q}(\mathcal{H})\leq\ell(\pmb{\theta}^{1}:\mathcal{D}).}\end{array}
$$ 

Since $\theta^{1}$ maximize the expected log-likelihood, we can apply equation (19.15) for the completed data set $\langle\mathcal{D},\mathcal{H}\rangle$ , and conclude that 

$$
\mathcal{E}_{Q}\big[\ell(\pmb{\theta}^{1}:\langle\mathcal{D},\mathcal{H}\rangle)\big]=M\left[\sum_{i}\pmb{E}_{Q}\Big[\pmb{I}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(X_{i};H)\Big]-\pmb{E}_{Q}\Big[\pmb{H}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(H)\Big]-\sum_{i}\pmb{H}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(X_{i};H)\right].
$$ 

Using basic rewriting, we have that $M E_{Q}\Big[H_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(H)\Big]=H_{Q}(\mathcal{H})$ and that $\begin{array}{r}{E_{\boldsymbol{Q}}\Big[\pmb{I}_{\hat{P}_{\langle\mathcal{D},\mathcal{H}\rangle}}(\boldsymbol{X}_{i};H)\Big]=\pmb{I}_{\hat{Q}_{\theta^{0}}}(\boldsymbol{X}_{i};H),}\end{array}$ which proves the result. 

# 19.5.2 Determining the Cardinality 

One of the key questions that we need to address for a hidden variable is that of its cardinality. 

# 19.5.2.1 Model Selection for Cardinality 

model selection The simplest approach is to use model selection , where we consider a number of diferent cardi- nalities for $H$ , and then select the best one. For our evaluation criterion, we can use a Bayesian technique, utilizing one of the approximate scores presented in section 19.4.1; box 19.G provides a comparative study of the diferent scores in precisely this setting. As another alternative, we can measure test generalization performance on a holdout set or using cross-validation. Both of these methods are quite expensive, even for a single hidden variable, since they both require that we learn a full model for each of the diferent cardinalities that we are considering; for multiple hidden variables, it is generally intractable. 

A cheaper approach is to consider a more focused problem of using $H$ to represent a clustering problem, where we cluster instances based only on the features $X$ in $H$ ’s (putative) Markov blanket. Here, the assumption is that if we give $H$ enough expressive power to capture the distinctions between diferent classes of instances, we have captured much of the information in $X$ . We can now use any clustering algorithm to construct diferent clusterings and to evaluate their explanatory power. Commonly used variants are EM with a naive Bayes model, the simpler k-means algorithm, or any other of many existing clustering algorithms. We can now evaluate diferent cardinalities for $H$ at much lower cost, using a score that measures only the quality of the local clustering. An even simpler approach is to introduce $H$ with a low cardinality (say binary-valued), and then use subsequent learning stages to tell us whether there is still information in the vicinity of $H$ . If there is, we can either increase the cardinality of $H$ , or add another hidden variable. 

# 19.5.2.2 Dirichlet Processes 

Bayesian model averaging 

A very diferent alternative is a Bayesian model averaging approach, where we do not select a cardinality, but rather average over diferent possible cardinalities. Here, we use a prior over the set of possible cardinalities of the hidden variable, and use the data to deﬁne a posterior. The Bayesian model averaging approach allows us to circumvent the difcult question of selecting the cardinality of the hidden variable. On the other side, because it fails to make a deﬁnitive decision on the set of clusters and on the assignment of instances to clusters, the results of the algorithm may be harder to interpret. Moreover, techniques 

# that use Bayesian model averaging are generally computationally even more expensive than approaches that use model selection. 

Dirichlet process 

One particularly elegant solution is provided by the Dirichlet process approach. We provide an intuitive, bottom-up derivation for this approach, which also has extensive mathematical foundations; see section 19.7 for some references. 

To understand the basic idea, consider what happens if we apply the approach of example 19.12 but allow the number of possible clusters $K$ to grow very large, much larger than the number of data instances. In this case, the bound $K$ does not limit the expressive power of model, since we can (in principle) put each instance in its own cluster. 

Our natural concern about this solution is the possibility of overﬁtting: after all, we certainly do not want to put each point in its own cluster. However, recall that we are using a Bayesian approach and not maximum likelihood. To understand the diference, consider the posterior distribution over the $(M+1)'$ ’st instance given a cluster assignment for the previous instances $1,\cdot\cdot\cdot,M$ . This formula is also proportional to equation (19.10), with $M+1$ playing the role of $m^{\prime}$ . (The normalizing constant is diferent, because here we are also interested in modeling the distribution over $X[M+1]$ , whereas there we took $x[m^{\prime}]$ as given.) The ﬁrst term in the equation, $|I_{k}(\pmb{c})|+\alpha_{0}/K$ , captures the relative prior probability t t the $(M+1)\mathrm{{'}s t}$ instance selects to join cluster k . Note that the more instances are in the k ’th cluster, the higher the probability that the new instance will select to join. Thus, the Dirichlet prior naturally causes instances to prefer to cluster together and thereby helps avoid overﬁtting. 

A second concern is the computational burden of maintaining a very large number of clusters. Recall that if we use the collapsed Gibbs sampling approach of example 19.12, the cost per sampling step grows linearly with $K$ . Moreover, most of this computation seems like a waste: with such a large $K$ , many of the clusters are likely to remain empty, so why should we waste our time considering them? 

partition 

The solution is to abstract the notion of a cluster assignment. Because clusters are completely symmetrical, we do not care about the speciﬁc assignment to the variables $C[m]$ , but only about the partition of the instances into groups. Moreover, we can collapse all of the empty partitions, treating them all as equivalent. We therefore deﬁne a particle $\sigma$ in our collapsed Gibbs process to encode a partition the data instances $\{1,.\,.\,.\,,M\}$ : an unordered set of non-empty subsets $\{I_{1},.\,.\,.\,,I_{l}\}$ . Each $I\in\sigma$ ∈ associated with a distribution over the parameters $\Theta_{\sigma}=\{\theta_{I}\}_{I\in\sigma}$ and over the multinomial θ . As usual, we deﬁne $\sigma_{-m^{\prime}}$ to denote the partition induced when we remove the instance $m^{\prime}$ . 

To deﬁne the sampling process, let $C[m^{\prime}]$ be the variable to be sampled. Let $L$ be the number of (non-empty) clusters in the partition $\sigma_{-m^{\prime}}$ . Introducing $C[m^{\prime}]$ (while keeping the other instances ﬁxed) induces $L+1$ possible partitions: joining one of the $L$ existing clusters, or opening a new one. We can compute the conditional probabilities of each of these outcomes. Let $I\in\sigma$ . 

$$
\begin{array}{r c l}{P(I\gets I\cup\{m^{\prime}\}\mid\sigma_{-m^{\prime}},\mathcal{D},\omega)}&{\propto}&{\left(|I|+\displaystyle\frac{\alpha_{0}}{K}\right)Q(\pmb{x}[m^{\prime}]\mid\mathcal{D}_{I},\omega)}\\ {P(\sigma\gets\sigma\cup\{\{m^{\prime}\}\}\mid\sigma_{-m^{\prime}},\mathcal{D},\omega)}&{\propto}&{(K-L)\displaystyle\frac{\alpha_{0}}{K}Q(\pmb{x}[m^{\prime}]\mid\omega),}\end{array}
$$ 

where the ﬁrst line denotes the event where $m^{\prime}$ joins an existing cluster $I$ , and the second the event where it forms a new singleton cluster (containing only $m^{\prime}$ ) that is added to $\sigma$ . To compute these transition probabilities, we needed to sum over all possible concrete cluster assignments that are consistent with $\sigma$ , but this computation is greatly facilitated by the symmetry of our prior (see exercise 19.29). Using abstract partitions as our particles provides signiﬁcant computational savings: we need only to compute $L+1$ values for computing the transition distribution, rather than $K$ , reducing the complexity of each Gibbs iteration to $O(N L)$ , independent of the number of classes $K$ . 

As long as $K$ is larger than the amount of data, it appears to play no real role in the model. Therefore, a more elegant approach is to remove it, allowing the number of clusters to grow to inﬁnity. At the limit, the sampling equation for $\sigma$ is now even simpler: 

$$
\begin{array}{r c l}{P(I\gets I\cup\{m^{\prime}\}\mid\sigma_{-m^{\prime}},\mathcal{D},\omega)}&{\propto}&{|I|\cdot Q(\pmb{x}[m^{\prime}]\mid\mathcal{D}_{I},\omega)}\\ {P(\sigma\gets\sigma\cup\{\{m^{\prime}\}\}\mid\sigma_{-m^{\prime}},\mathcal{D},\omega)}&{\propto}&{\alpha_{0}\cdot Q(\pmb{x}[m^{\prime}]\mid\omega).}\end{array}
$$ 

nonparametric Bayesian estimation 

Chinese restaurant process 

stick-breaking prior 

This scheme removes the bound on the number of clusters and induces a prior that allows any possible partition of the samples. Given the data, we obtain a posterior over the space of possible partitions. This posterior gives positive probability to partitions with diferent numbers of clusters, thereby averaging over models with diferent complexity. In general, the number of clusters tends to grow logarithmically with the size of the data. This type of model is called a nonparametric Bayesian model ; see also box 17.B. 

Of course, with $K$ at inﬁnity, our Dirichlet prior over $\theta$ is not a legal prior. Fortunately, it turns out that one can deﬁne a generalization of the Dirichlet prior that induces these conditional probabilities. One simple derivation comes from a sampling process known as the Chinese restaurant process . This process generates a random partition as follows: The guests (instances) enter a restaurant one by one, and each guest chooses between joining one of the non-empty tables (clusters) and opening a new table. The probability that a customer chooses to join a table $l$ at which $n_{l}$ customers are already sitting is $\propto n_{l}$ ; the probability that he opens a new tables is $\propto\alpha_{0}$ . The instances assigned to the same table all use the parameters of that table. It is not hard to show (exercise 19.30) that this prior induces precisely the update equations in equation (19.19) and equation (19.20). 

A second derivation is called the stick-breaking prior ; it is parameterized by $\alpha_{0}$ , and deﬁnes an inﬁnite sequence of random variables $\beta_{i}\,\sim\,B e t a(1,\alpha_{0})$ . We can now deﬁne an inﬁnite- dimensional vector deﬁned as: 

$$
\lambda_{k}=\beta_{k}\prod_{l=1}^{k-1}(1-\beta_{l}).
$$ 

This prior is called a stick-breaking prior because it can be viewed as deﬁning a process of breaking a stick into pieces: We ﬁrst break a piece of fraction $\beta_{1}$ , then the second piece is a fraction $\beta_{2}$ of the remainder, and so on. It is not difcult to see that $\textstyle\sum_{k}\lambda_{k}\;=\;1$ . It is also possible to show that, under the appropriate deﬁnitions, the limit of the distributions $D i r i c h l e t(\alpha_{0}/K,.\,.\,.\,,\alpha_{0}/K)$ as $K\longrightarrow\infty$ →∞ induces the stick-breaking prior. 

# 19.5.3 Introducing Hidden Variables 

Finally, we consider the question of determining when and where to introduce a hidden variable. The analysis of section 19.5.1 tells us that a hidden variable in a naive Bayes clustering network is optimized to capture information about the variables to which it is connected. Intuitively, 

![](images/1a66d7042dddfd41ab0360a51dfaf6b13fd252dbdf2f16a1659955cdfe50e86b.jpg) 
Figure 19.11 An example of a network with a hierarchy of hidden variables 

![](images/e5cb4a58c16922f5f69c089b7b60f15f843467a7af97682d3826fec9afdbf935.jpg) 
Figure 19.12 An example of a network with overlapping hidden variables 

this requirement imposes a signiﬁcant bias on the parameter iz ation of the hidden variable. This bias helps constrain our search and allows us to learn a hidden variable that plays a meaningful role in the model. Conversely, if we place a hidden variable where the search is not similarly constrained, we run the risk of learning hidden variables that are meaningless, and that only capture the noise in the training data. As a rough rule of thumb, we want the model with the hidden variables to have a lot fewer independent parameters than the number of degrees of freedom of the empirical distribution. 

hierarchical organization 

overlapping organization 

Thus, when selecting network topologies involving hidden variables, we must exercise care. One useful example of such a class of topologies are organized hierarchically (for example, ﬁgure 19.11), where the hidden variables form a treelike hierarchy. Since each hidden variable is a parent of several other variables (either observed or hidden), it serves to mediate the de- pendencies among its children and between these children and other variables in the network (through its parent). This constraint implies that the hidden variable can improve the likelihood by capturing such dependencies when they exist. The general topology leaves much freedom in determining what is the best hierarchy structure. Intuitively, distance in the hierarchy should roughly correspond to the degree of dependencies between the variables, so that strongly de- pendent variables would be closer in the hierarchy. This rule is not exact, of course, since the nature of the dependencies inﬂuences whether the hidden variable can capture them. 

Another useful class of networks are those with overlapping hidden variables; see ﬁgure 19.12. In this network, each hidden variable is the parent of several observed variables. The justiﬁcation is that each hidden variable captures aspects of the instance that several of the observed variables depend on. Such a topology encodes multiple “ﬂavors” of dependencies between the diferent variables, breaking up the dependency between them as some combination of independent axes. This approach often provides useful information about the structure of the domain. However, once we have an observed variable depending on multiple hidden ones, we might need to introduce many parameters, or restrict attention to some compact parameter iz ation of the CPDs. Moreover, while the tree structure of hierarchical networks ensures efcient inference, overlapping hidden variables can result in a highly intractable structure. 

In both of these approaches, as in others, we need to determine the placements of the hidden variables. As we discussed, once we introduce a hidden variable somewhere within our structure and localize it correctly in the model by connecting it to its correct neighbors, we can estimate parameters for it using EM. Even if we locate it approximately correctly, we can use the structural EM algorithm to adapt both the structure and the parameters. 

However, we cannot simply place a hidden variable arbitrarily in the model and expect our learning procedures to learn a reasonable model. Since these methods are based on iterative improvements, running structural EM with a bad initialization usually leads either to a trivial structure (where the hidden variable has few neighbors or disconnected from the rest of the variables) or to a structure that is very similar to the initial network structure. One extreme example of a bad initialization is to introduce a hidden variable that is disconnected from the rest of the variables; here, we can show that the variable will never be connected to the rest of the model (see exercise 19.27). 

This discussion raises the important question of how to induce the existence of a hidden variable, and how to assign it a putative position within the network. One approach is based on ﬁnding “signatures” that the hidden variable might leave. As we discussed, a hidden variable captures dependencies between the variables to which it is connected. Indeed, if we assume that a hidden variable truly exists in the underlying distribution, we expect its neighbors in the graph to be dependent. For example, in ﬁgure 16.1, marginalizing the hidden variable induces correlations among its children and between its parents and its children (see also exercise 3.11). Thus, a useful heuristic is to look for subsets of variables that seem to be highly interdependent. 

There are several approaches that one can use to ﬁnd such subsets. Most obviously, we can learn a structure over the observed variables and then search for subsets that are connected by many edges. An obvious problem with this approach is that most learning methods are biased against learning networks with large indegrees, especially given limited data. Thus, these methods may return sparser structures even when dependencies may exist, preventing us from using the learned structure to infer the existence of a hidden variable. Nevertheless, these methods can be used successfully given a reasonably large number of samples. Another approach is to avoid the structure learning phase and directly consider the dependencies in the empirical distribution. For example, a quick-and-dirty method is to compute a measure of dependency, such as mutual information, between all pairs of variables. This approach avoids the need to examine marginals over larger sets of variables, and hence it is applicable in the case of limited data. However, we note that children of an observed variable will also be highly correlated, so that this approach does not distinguish between dependencies that can be explained by the observed variables and ones that require introducing hidden variables. Nevertheless, we can use this approach as a heuristic for introducing a hidden variable, and potentially employ a subsequent pruning phase to eliminate the variable if it is not helpful, given the observed variables. 

# 19.6 Summary 

In this chapter, we considered the problem of learning in the presence of incomplete data. We saw that learning from such data introduces several signiﬁcant challenges. 

One set of challenges involves the statistical interpretation of the learning problem in this setting. As we saw, we need to be aware of the process that generated the missing data and the efect of nonrandom observation mechanisms on the interpretation of the data. Moreover, we also need to be mindful of the possibility of un ident i ability in the models we learn, and as a consequence, to take care when interpreting the results. 

A second challenge involves computational considerations. Most of the key properties

  that helped make learning feasible in the fully observable case vanish in the partially observed setting. In particular, the likelihood function no longer decomposes, and is even multimodal. As a consequence, the learning task requires global optimization over a high-dimensional space, with an objective that is highly susceptible to local optima. 

We presented two classes of approaches for performing parameter estimation in this setting: a generic gradient-based process, and the EM algorithm, which is speciﬁcally designed for maximizing likelihood functions. Both of these methods perform hill climbing over the parameter space, and are therefore guaranteed only to ﬁnd a local optimum (or rather, a stationary point) of the likelihood function. Moreover, each iteration in these algorithms requires that we solve an inference problem for each (partially observed) instance in our data set, a requirement that introduces a major computational burden. 

In some cases, we want not only a single parameter estimate, but also some evaluation of our conﬁdence in those estimates, as would be obtained from Bayesian learning. Clearly, given the challenges we mentioned, closed-form solutions to the integration are generally impossi- ble. However, several useful approximations have been developed and used in practice; most commonly used are the methods based on MCMC methods, and on variational approximations. 

 monly used are the score-based approaches, where we deﬁne the problem as one of ﬁnding a high-scoring structure. We presented several approximations to the Bayesian score; most of these are based on an asymptotic approximation, and hence should be treated with care given only a small number of samples. We then discussed the challenges of searching over the space of networks when the score is not decomposable, a setting that (in principle) forces us to apply a highly expensive evaluation procedure to every candidate that we are considering in the search. The structural EM algorithm provides one approach to reduce this cost. It uses an approximation to the score that is based on some completion of the data, allowing us to use the same efcient algorithms that we applied in the complete data case. 

Finally, we brieﬂy discussed some of the important questions that arise when we consider hidden variables: Where in the model should we introduce a hidden variable? What should we select as the cardinality of such a variables? And how do we initialize a variable so as to guide the learning algorithm toward “good” regions of the space? While we brieﬂy described some ideas here, the methods are generally heuristic, and there are no guarantees. 

Overall, owing to the challenges of this learning setting, the methods we discussed in this chapter are more heuristic and provide weaker guarantees than methods that we encountered in previous learning chapters. For this reason, the application of these methods is more of an art than a science, and there are often variations and alternatives that can be more efective for particular learning scenarios. This is an active area of study, and even for the simple clustering problem there is still much active research. Thus, we did not attempt to give a complete coverage and rather focused on the core methods and ideas. 

 However, while these complications mean that learning from incomplete data is often challenging or even impossible, there are still many real-life applications where the methods we discussed here are highly efective. Indeed, the methods that we described here are some of the most commonly used of any in the book. They simply require that we take care in their application, and generally that we employ a fair amount of hand-tuned engineering. 

# 19.7 Relevant Literature 

The problem of statistical estimation from missing data has received a thorough treatment in the ﬁeld of statistics. The distinction between the data generating mechanism and the observation mechanism was introduced by Rubin (1976) and Little (1976). Follow-on work deﬁned the notion of MAR and MCAR Little and Rubin (1987). Similarly, the question of identiﬁability is also a central in statistical inference Casella and Berger (1990); Tanner (1993). Treatment of the subject for Bayesian networks appears in Settimi and Smith (1998a); Garcia (2004). 

An early discussion that touches on the gradient of likelihood appears in Buntine (1994). Binder et al. (1997); Thiesson (1995) applied gradient methods for learning with missing values in Bayesian networks. They derived the gradient form and suggested how to compute it efciently using clique tree calibration. Gradient methods are often more ﬂexible in using models that do not have a closed-form MLE estimate even from complete data (see also chapter 20) or when using alternative objectives. For example, Greiner and Zhou (2002) suggest training using gradient ascent for optimizing conditional likelihood. 

The framework of expectation maximization was introduced by Dempster et al. (1977), who generalized ideas that were developed independently in several related ﬁelds (for example, the Baum-Welch algorithm in hidden Markov models (Rabiner and Juang 1986)). The use of expecta- tion maximization for maximizing the posterior was introduced by Green (1990). There is a wide literature of extensions of expectation maximization, analysis of convergence rates, and speedup methods; see McLachlan and Krishnan (1997) for a survey. Our presentation of the theoretical foundations of expectation maximization follows the discussion by Neal and Hinton (1998). 

The use of expectation maximization in speciﬁc graphical models ﬁrst appeared in various forms (Cheeseman, Self et al. 1988b; Cheeseman, Kelly et al. 1988; Ghahramani and Jordan 1993). Its adaptation for parameter estimation in general graphical models is due to Lauritzen (1995). Several approaches for accelerating EM convergence in graphical models were examined by Bauer et al. (1997) and Ortiz and Kaelbling (1999). The idea of incremental updates within expectation maximization was formulated by Neal and Hinton (1998). The application of expec- tation maximization for learning the parameters of noisy-or CPDs (or more generally CPDs with causal independence) was suggested by Meek and Heckerman (1997). The relationship between expectation maximization and hard-assignment EM was discussed by Kearns et al. (1997). 

There are numerous applications of expectation maximization to a wide variety of problems. The collaborative ﬁltering application of box 19.A is based on Breese et al. (1998). The application to robot mapping of box 19.D is due to Thrun et al. (2004). There is a rich literature combining expectation maximization with diferent types of approx- 

imate inference procedures. Variational EM was introduced by Ghahramani (1994) and further elaborated by Ghahramani and Jordan (1997). The combination of expectation maximization with various types of belief propagation algorithms has been used in many current applications (see, for example, Frey and Kannan (2000); Heskes et al. (2003); Segal et al. (2001)). Similarly, other combinations have been examined in the literature, such as Monte Carlo EM (Cafo et al. 2005). 

Applying Bayesian approaches with incomplete data requires approximate inference. A com- mon solution is to use MCMC sampling, such as Gibbs sampling, using data-completion particles (Gilks et al. 1994). Our discussion of sampling the Dirichlet distribution is based on (Ripley 1987). More advanced sampling is based on the method of Fishman (1976) for sampling from Gamma distributions. Bayesian Variational methods were introduced by MacKay (1997); Jaakkola and Jordan (1997); Bishop et al. (1997) and further elaborated by Attias (1999); Ghahramani and Beal (2000). Minka and Laferty (2002) suggest a Bayesian method based on expectation propagation. 

The development of Laplace-approximation structure scores is based mostly on the presenta- tion in Chickering and Heckerman (1997); this work is also the basis for the analysis of box 19.G. The BIC score was originally suggested by Schwarz (1978). Geiger et al. (1996, 2001) developed the foundations for the BIC score for Bayesian networks with hidden variables. This line of work was extended by several works (D. Rusakov 2005; Settimi and Smith 2000, 1998b). The Cheeseman-Stutz approximation was initially introduced for clustering models by Cheeseman and Stutz (1995) and later adapted for graphical models by Chickering and Heckerman (1997). Variational scores were suggested by Attias (1999) and further elaborated by Beal and Ghahramani (2006). 

Search based on structural expectation maximization was introduced by Friedman (1997, 1998) and further discussed in Meila and Jordan (2000); Thiesson et al. (1998). The selective clustering example of section 19.4.3.3 is based on Barash and Friedman (2002). Myers et al. (1999) suggested a method based on stochastic search. An alternative approach uses reversible jump MCMC methods that perform Monte Carlo search through both parameter space and structure space

 (Green 1995). More recent proposals use Dirichlet processes to integrate over potential structures

 (Rasmussen 1999; Wood et al. 2006). 

Introduction of hidden variables is a classic problem. Pearl (1988) suggested a method based on algebraic constraints in the distribution. The idea of using algebraic signatures of hidden variables has been proposed in several works (Spirtes et al. 1993; Geiger and Meek 1998; Robins and Wasserman 1997; Tian and Pearl 2002; Kearns and Mansour 1998). Using the structural signature was suggested by Martin and VanLehn (1995) and developed more formally by Elidan et al. (2000). Additional methods include hierarchical methods (Zhang 2004; Elidan and Friedman 2005), the introduction of variables to capture temporal correlations (Boyen et al. 1999), and introduction of variables in networks of continuous variables (Elidan et al. 2007). 

# 19.8 Exercises 

# Exercise 19.1 

Consider the estimation problem in example 19.4. 

a. Provide upper and lower bounds on the maximum likelihood estimate of $\theta$ . b. Prove that your bounds are tight; that is, there are values of $\psi_{O_{X}|x^{1}}$ and $\psi_{O_{X}|x^{0}}$ for which these estimates are equal to the maximum likelihood. 

# Exercise ${\bf19.2\star}$ 

Suppose we have a given model $P(\mathbf{X}\mid\mathbf{\theta})$ on a s $X=\{X_{1},.\,.\,.\,,X_{n}\}$ , and some inco ete data. Suppose we introduce additional variables $Y=\{Y_{1},.\,.\,.\,,Y_{n}\}$ { } so that $Y_{i}$ i has the value 1 if ${\bar{X}}_{i}$ is observed and 0 otherwise. We can extend the data, in the obvious way, to include complete observations of iables $\mathbf{Y}$ . Show how to augment the model to build a model ${\dot{P}}(X,Y\mid\theta,\theta^{\prime}){\dot{=}}\;P(X\mid\theta)P(Y\mid$ $X,\theta^{\prime})$ so that it satisﬁes the missing at random assumption. 

# Exercise 19.3 

Consider the problem of applying EM to parameter estimation for a variable $X$ whose local probabilistic model is a tree-CPD. We assum that e network structure $\mathcal{G}$ incl es the structure of the tree-CPDs in it, so that we have a structure T for X . We a given a data set D with some missing values, and we want to run EM to estimate the parameters of T . Explain how we can adapt the EM algorithm in order to accomplish this task. Describe what expected sufcient statistics are computed in the E-step, and how parameters are updated in the M-step. 

# Exercise 19.4 

Consider the problem of applying EM to parameter estimation for a variable $X$ whose local probabilistic model is a noisy-or. Assume that $X$ has parents $Y_{1},\ldots,Y_{k}$ , so that our task for $X$ is to estimate the noise parameters $\lambda_{0},\ldots,\lambda_{k}$ . Explain how we can use the EM algorithm to accomplish this task. (Hint: Utilize the structural decomposition of the noisy-or node.) 

# Exercise 19.5 

Prove theorem 19.2. (Hint: use lemma 19.1.) 

# Exercise 19.6 

Suppose we are using a gradient method to learn parameters for a network with table-CPDs. Let $X$ be one of the variables in the network with parents $U$ . One of the constraints we need to maintain is that 

$$
\sum_{x}\theta_{x\mid u}=1
$$ 

for every assignment $\mathbfit{u}$ for $U$ . Given the gradient $\frac{\partial}{\partial\theta_{x\mid u}}\ell(\theta:\mathcal{D})$ , show how to project it to null space of this constraint. That is, show how to ﬁnd a gradient direction that maximizes the likelihood while preserving this constraint. 

# Exercise 19.7 

Suppose we consider re parameter i zing table-CPDs using the representation of equation (19.3). Use the chain law of partial derivatives to ﬁnd the form of $\frac{\partial}{\partial\lambda_{x\mid u}}\bar{\ell}(\pmb\theta:\mathcal D\bar{)}$ . 

# Exercise ${\bf19.8\star}$ 

Suppose we have a Bayesian network with table-CPDs. Apply the method of Lagrange multipliers to characterize the maximum likelihood solution under the constraint that each conditional probability sums to one. How does your characterization relate to EM? 

# Exercise $19.9\star$ 

We now examine how to compute the Hessian of the likelihood function. Recall that the Hessian of the log-likelihood is the matrix of second derivatives. Assume that our model is a Bayesian network with table-CPDs. 

$\frac{\partial^{2}\log P(o)}{\partial\theta_{x_{i}\mid u_{i}}\partial\theta_{x_{j}\mid u_{j}}}=\frac{1}{\theta_{x_{i}\mid u_{i}}\theta_{x_{j}\mid u_{j}}}\left[P(x_{i},\mathbf{u}_{i},x_{j},\mathbf{u}_{j}\mid o)-P(x_{i},\mathbf{u}_{i}\mid o)P(x_{j},\mathbf{u}_{j}\mid o)\right].$ 

b. What is the cost of computing the full Hessian matrix of $\log P(o)$ if we use clique tree propagation? c. What is the computational cost if we are only interested in entries of the form 

$$
\frac{\partial^{2}}{\partial\theta_{x_{i}|u_{i}}\partial\theta_{x_{i}^{\prime}|u_{i}^{\prime}}}\log P(o);
$$ 

that is, we are interested in the “diagonal band” that involves only second derivatives of entries from the same family? 

# Exercise ${\bf19.10\star}$ 

a. Consider the task of estimating the parameters of a univariate Gaussian distribution $\mathcal{N}\left(\mu;\boldsymbol{\sigma}^{2}\right)$   from a data set $\mathcal{D}$ . Sh maximize likelihood subject to the constraint $\sigma^{2}\ge\epsilon$ for some $\epsilon>0$ , then the likelihood $L(\boldsymbol{\mu},\boldsymbol{\sigma}^{2}:\mathcal{D})$ D is guaranteed to remain bounded. b. Now, consider estimat g the parameters of a multivariate Gaussian $\mathcal{N}\left(\boldsymbol{\mu};\boldsymbol{\Sigma}\right)$ from a data set $\mathcal{D}$ . Provide constraints on Σ that achieve the same guarantee. 

# Exercise $19.11\star$ 

Consider learning the para $H\to X,H\to Y$ , where $H$ is a hidden variable. Show that the distribution where $P(H),P(X\mid H),P(Y\mid H)$ | | are uniform is a stationary point of the likelihood (gradient is 0). What does that imply about gradient ascent and EM starting from this point? 

# Exercise 19.12 

Prove theorem 19.5. Hint, note that $\ell(\pmb{\theta}^{t}:\mathcal{D})=F_{\mathcal{D}}[\pmb{\theta}^{t},P(\mathcal{H}\mid\mathcal{D},\pmb{\theta}^{t})]$ , and use corollary 19.1. 

# Exercise 19.13 

Consider the task of learning the parameters of a DBN with table-CPDs from a data set with missing data. In particular, assume that our data set consists of a sequence of observations $o_{0}^{(0)},o_{1}^{(1)},\cdot\cdot\cdot,o_{t}^{(T)}$ . (Note that we do not assume that the same variables are observed in every time-slice.) 

a. Describe precisely how you would run EM in this setting to estimate the model parameters; your algorithm should specify exactly how we run the E-step, which sufcient statistics we compute and how, and how the sufcient statistics are used within the M-step. b. Given a single trajectory, as before, which of the network parameters might you be able to estimate? 

# Exercise $19.14\star$ 

Show that, until convergence, each iteration of hard-assignment EM increases $\ell(\pmb\theta:\langle\mathcal D,\mathcal H\rangle)$ 

# Exercise $19.15\star$ 

Suppose that we have an incomplete data set $\mathcal{D}$ , and network structure $\mathcal{G}$ and m meters. Moreover, suppose that we are interested in learning the parameters of a single CPD $P(X_{i}^{-}\mid\mathbf{\bar{U}}_{i})$ | ) . That is, we assume that the parameters we were given for all other families are frozen and do not change during the learning. This scenario can arise for several reasons: we might have good prior knowledge about these parameters; or we might be using an incremental approach, as mentioned in box 19.C (see also exercise 19.16). 

We now consider how this scenario can change the computational cost of the EM algorithm. 

a. ume we have a clique tree for the network $\mathcal{G}$ and that the CPD $P(X_{i}\mid U_{i})$ assigned to clique $C_{j}$ . Analyze which messages change after we update the parameters for $\Dot{P}(X_{i}\mid U_{i})$ | . Use this analysis to show how, after an initial precomputation step, we can perform iterations of this single-family EM procedure with a computational cost that depends only on the size of $C_{j}$ and not the size of the rest of the cluster tree. 

b. Would this conclusion change if we update the parameters of several families that are all assigned to the same cluster in the cluster tree? 

# Exercise ${\bf9.16\star}$ 

incremental EM We can build on the idea of the single-family EM procedure, as described in exercise 19.15, to deﬁne an incremental EM procedure for learning all the parameters in the network. In this approach, at each step we optimize the parameters of a single CPD (or several CPDs) while freezing the others. We then iterate between these local EM runs until all families have converged. 

Is this modiﬁed version of EM still guaranteed to converge? In other words, does 

$$
\ell(\pmb{\theta}^{t+1}:\mathcal{D})\geq\ell(\pmb{\theta}^{t}:\mathcal{D})
$$ 

still hold? If so, prove the result. If not, explain why not. 

# Exercise $19.17\star$ 

We now consider how to use the interpretation of the EM as maximizing an energy functional to allow partial or incremental updates over the instances. Consider the EM algorithm of algorithm 19.2. In the Compute-ESS we collect the statistics from all the instances. This requires running inference on all the instances. 

We now consider a procedure that performs partial updates where it update the expected sufcient statistics for some, but not all, of the instances. In particular, suppose we replace this procedure by one that runs inference on a single instance and uses the update to replace the old contribution of the instance with a new one; see algorithm 19.4. This procedure, instead of computing all the expected sufcient statistics in each E-step, caches the contribution of each instance to the sufcient statistics, and then updates only a single one in each iteration. 

a. Show that the incremental EM algorithm converges to a ﬁxed point of the log-likelihood function. To do so, show that each iteration improves the EM energy functional. Hint: you need to deﬁne what is the efect of the partial E-step on the energy functional. b. How would that analysis generalize if in each iteration the algorithm performs a partial update for $k$ instances (instead of 1)? c. Assume that the computations in the M-step are relatively negligible compared to the inference in the E-step. Would you expect the incremental EM to be more efcient than standard EM? If so, why? 

# Exercise ${\bf9.18\star}$ 

Consider the model described in box 19.D. 

a. Assume we perform the E-step for each step $\pmb{x}_{m}$ by deﬁning 

$$
\tilde{P}(\pmb{x}_{m}\mid C_{m}=k:\pmb{\theta}_{k})=\mathcal{N}\left(d(\pmb{x},p_{k})\mid0;\sigma^{2}\right)
$$ 

and ${\tilde{P}}(\mathbf{x}_{m}\mid C_{m}=0:\theta_{k})=C$ | for some constant $C$ . Why is this formula not a correct application of EM? (Hint: Consider the normalizing constants.) 

We note that although this approach is mathematically not quite right, it seems to be a reasonable approximation that works in practice. 

b. Given a solution to the E-step, show how to perform maximum likelihood estimation of the model parameters $\alpha_{k},\beta_{k}$ , subject to the constraint that $\alpha_{k}$ be a unit-vector, that is, that $\pmb{\alpha}_{k}\cdot\pmb{\alpha}_{k}=1$ . (Hint: Use Lagrange multipliers.) 

# Algorithm 19.4 The incremental EM algorithm for network with table-CPDs 

) 1 Run inference on $\langle\mathcal{G},\theta\rangle$ using evidence o [ m ] 2 for each $i=1,\dots,n$ 3 for ch $x_{i},\mathbf{\mathit{u}}_{i}\in V a l(X_{i},\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ 4 $//$ Remove old contribution 5 $\bar{M}[x_{i},{\pmb u}_{i}]\gets\ \bar{M}[x_{i},{\pmb u}_{i}]-\bar{M}_{m}[x_{i},{\pmb u}_{i}]$ 6 $//$ Compute new contribution 7 $\bar{M}_{m}[x_{i},{\pmb u}_{i}]\gets\ P(x_{i},{\pmb u}_{i}\ |\ o[m])$ 8 $\bar{M}[x_{i},{\pmb u}_{i}]\gets\ \bar{M}[x_{i},{\pmb u}_{i}]+\bar{M}_{m}[x_{i},{\pmb u}_{i}]$ Procedure Incremental-EM ( $\mathcal{G}$ $//$ Bayesian network structure over $X_{1},\dots,X_{n}$ $\theta^{0}$ , $//$ Initial set of parameters for $\mathcal{G}$ $\mathcal{D}$ $//$ Partially observed data set ) 1 2 $\begin{array}{l}{\mathbf{for\each\}\iota=1,\dots,n}\\ {\mathbf{\four\each}\ x_{i},\mathbf{\mathit{u}}_{i}\in V a l(X_{i},\mathrm{Pa}_{X_{i}}^{\mathcal{G}})}\\ {\quad\bar{M}[x_{i},\mathbf{\mathit{u}}_{i}]\gets\ 0}\\ {\quad\mathbf{for\each}\ m=1\dots M}\\ {\quad\ \ \bar{M}_{m}[x_{i},\mathbf{\mathit{u}}_{i}]\gets\ 0}\end{array}$ 3 4 5 6 $//$ Initialize the expected sufcient statistics 7 for each $m=1\ldots M$ 8 ental- $\operatorname{E-Step}({\mathcal{G}},\theta^{0},{\mathcal{D}},m)$ 9 $m\gets\ 1$ 10 for each $t=0,1\ldots.$ , until convergence 11 // E-step 12 $\begin{array}{r l}&{\mathsf{\ddot{n c r e m e n t a l-E-S t e p}}(\mathcal{G},\pmb{\theta}^{t},\mathcal{D},m)}\\ &{m\xleftarrow{}\mathrm{~}(m\bmod M)+1}\\ &{\therefore}\end{array}$ 13 14 // M-step 15 for each $i=1,\dots,n$ 16 for each $x_{i},\mathbf{u}_{i}\in V a l(X_{i},\mathrm{Pa}_{X_{i}}^{\mathcal{G}})$ 17 $\begin{array}{r l}{\theta_{x_{i}|u_{i}}^{t+1}\leftarrow}&{{}\frac{\bar{M}[x_{i},u_{i}]}{\bar{M}[u_{i}]}}\\ {\bullet\;\theta^{t}}\end{array}$ 18 return 

# Exercise ${\bf19.19\star}$ 

Consider the setting of exercise 12.29, but now assume that we cannot (or do not wish to) maintain a distribution over the $A_{j}$ ’s. Rather, we want to ﬁnd the assignment $\pmb{a}_{1}^{*},\dots,\pmb{a}_{m}^{*}$ for which $P(\pmb{a}_{1},\cdot\cdot\cdot,\pmb{a}_{m})$ is maximized. 

In this exercise, we address this problem using the EM algorithm, treating the values $\mathbf{\alpha}_{1},\ldots,\mathbf{\alpha}_{m}$ as parameters. In the E-step, we compute the expected value of the $C_{i}$ variables; in the M-step, we maximize the value of the $\mathbf{\delta}_{\mathbf{\alpha}_{j}}$ ’s given the distribution over the $C_{j}$ ’s. 

a. Describe how one can implement this EM procedure exactly, that is, with no need for approximate inference. b. Why is approximate inference necessary in exercise 12.29 but not here? Give a precise answer in terms of the properties of the probabilistic model. 

# Exercise 19.20 

Suppose that a prior on a parameter vector is $p(\pmb\theta)\sim{\cal D}i r i c h l e t(\alpha_{1},.\,.\,.\,,\alpha_{k})$ . Derive $\begin{array}{r}{\frac{\partial}{\partial\theta_{i}}\log p(\boldsymbol{\theta})}\end{array}$ . 

# Exercise 19.21 

Consider the generalization of the EM procedure to the task of ﬁnding the MAP parameters. Let 

$\tilde{F}_{\mathcal{D}}[\pmb{\theta},Q]=F_{\mathcal{D}}[\pmb{\theta},Q]+\log{P(\pmb{\theta})}.$ a. Prove the following result: 

Corollary 19.2 

$F o r\;a\;d i s t r i b u t i o n\;Q,\;\mathrm{score}_{\mathrm{MAP}}(\pmb\theta\;:\;\mathcal{D})\geq\tilde{F}_{\mathcal{D}}[\pmb\theta,Q]$ 

b. Show that a coordinate ascent approach on $\tilde{F}_{\mathcal{D}}[\pmb{\theta},Q]$ requires only changing the M-step to perform D MAP rather than ML estimation, that is, to maximize: 

$$
\pmb{{\cal E}}_{Q}[\ell(\pmb{\theta}:\langle\mathcal{D},\mathcal{H}\rangle)]+\log{P(\pmb{\theta})}.
$$ 

c. Using exercise 17.12, provide a speciﬁc formula for the M-step in a network with table-CPDs. 

# Exercise 19.22 

In this case, we analyze the use of collapsed Gibbs with data completion particles for the purpose of sampling from a posterior in the case of incomplete data. 

a. Consider ﬁrst the simple case of example 19.12. Assuming that the data instances $_{_{\pmb{x}}}$ are sampled from a discrete naive Bayes model with a Dirichlet prior, derive a closed form for equation (19.9). b. Now, consider the general case of sampling from $P(\mathcal{H}\mid\mathcal{D})$ . Here, the key step would involve sampling from the distribution $P(X_{i}[m]\mid\langle{\mathcal D},{\mathcal H}\rangle_{-X_{i}[m]})\propto P(X_{i}[m],\langle{\mathcal D},{\mathcal H}\rangle_{-X_{i}[m]}),$ 

where $\langle\mathcal{D},\mathcal{H}\rangle_{-X_{i}[m]}$ is a complete data set from which the observation of $X_{i}[m]$ is removed. 

Assuming we have table-CPD and independent Dirichlet priors over the parameters, derive this con- ditional probability from the form of the marginal likelihood of the data. Show how to use sufcient statistics of the particle to perform this sampling efciently. 

# Exercise ${\bf19.23\star}$ 

We now consider a Metropolis-Hastings sampler for the same setting as exercise 19.22. For simplicity, we assume that the same variables are hidden in each instance. Consider the proposal distribution for variable $X_{i}$ speciﬁed in algorithm 19.5. (We are using a multiple-transition chain, as in section 12.3.2.4, where each variable has its own kernel.) In this proposal distribution, we resample a value for $X_{i}$ in all of the instances, based on the current parameters and the completion for all the other variables. 

Derive the form of the acceptance probability for this proposal distribution. Show how to use sufcient statistics of the completed data to evaluate this acceptance probability efciently. 

Algorithm 19.5 Proposal distribution for collapsed Metropolis-Hastings over data comple- 

Procedure Proposal-Distribution ( $\mathcal{G}$ $//$ Bayesian network structure over $X_{1},\dots,X_{n}$ D // completed data set $X_{i}$ // A variable to sample ) 1 $\theta\gets$ $(\mathcal{D},\mathcal{G})$ 2 $\mathcal{D}^{\prime}\leftarrow\ \mathcal{D}$ D ←D 3 for each $m=1\ldots M$ 4 Sample $x_{i}^{\prime}[m]$ from $P(X_{i}[m]\mid\mathbf{x}_{-i}[m],\pmb{\theta})$ 5 return ${\mathcal{D}}^{\prime}$ 

Exercise 19.24 Prove theorem 19.8. 

Exercise $19.25\star$ Prove theorem 19.10. Hint: Use the proof of theorem 19.5. 

# Exercise 19.26 

Consider learning structu n the etting discussed in section 19.4.3.3. Describe a data se $\mathcal{D}$ rame- ters for a network where $X_{1}$ and C are independent, yet the expected sufcient statistics ${\bar{M}}[X_{1},{\hat{C}}]$ show dependency between $X_{1}$ and $C$ . 

# Exercise 19.27 

Consider using the structural EM algorithm to learn the structure associated with a hidden variable $H$ ; all other variables are fully observed. Assume that we start our learning process by performing an E-step in a network where $H$ is not connected to any of $X_{1},\ldots,X_{n}$ . Show that, for any initial parameter assignment to $P(H)$ , the SEM algorithm will not connect $H$ to the rest of the variables in the network. 

# Exercise 19.28 

Consider the task of learning a model involving a binary-valued hidden variable $H$ using the EM algorithm. Assume that we initialize the EM algorithm using parameters that are symmetric in the two values of $H$ ; that is, for any variable $X_{i}$ that has $H$ has a parent, we have $P(X_{i}\mid{\dot{U}}_{i},h^{0})=P(X_{i}\mid U_{i},h^{1})$ . Show that, with this initialization, the model will remain symmetric in the two values of H , over all EM iterations. 

# Exercise 19.29 

Derive the sampling update equations for the partition-based Gibbs sampling of equation (19.17) and equation (19.18) from the corresponding update equations over particles deﬁned as ground assignments (equation (19.10)). Your update rules must sum over all assignments consistent with the partition. 

# Exercise 19.30 

Consider the distribution over partitions induced by the Chinese restaurant process. 

a. Find a closed-form formula for the probability induced by this process for any partition $\sigma$ of the guests. Show that this probability is invariant to the order the guests enter the restaurant. b. Show that a Gibbs sampling process over the partitions generated by this algorithm satisﬁes equa- tion (19.19) and equation (19.20). 

![](images/eec5016aa190bc52c475fd73724820c30f846ca20cdafe89efa7151306691f38.jpg) 

# Exercise ${\bf19.31\star}$ 

Algorithm 19.6 presents a Metropolis-Hastings proposal distribution over partitions in the Dirichlet process prior. Compute the acceptance probability of the proposed move. 

# 20 Learning Undirected Models 

# 20.1 Overview 

In previous chapters, we developed the theory and algorithms for learning Bayesian networks from data. In this chapter, we consider the task of learning Markov networks. Although many of the same concepts and principles arise, the issues and solutions turn out to be quite diferent. 

Perhaps the most important reason for the diferences is a key distinction between Markov networks and Bayesian networks: the use of a global normalization constant (the partition function) rather than local normalization within each CPD. This global factor couples all of the parameters across the network, preventing us from decomposing the problem and estimating local groups of parameters separately. This global parameter coupling has signiﬁcant computational ramiﬁcations. As we will explain, in contrast to the situation for Bayesian networks, even simple (maximum-likelihood) parameter estimation with complete data cannot be solved in closed form (except for chordal Markov networks, which are therefore also Bayesian networks). Rather, we generally have to resort to iterative methods, such as gradient ascent, for optimizing over the parameter space. The good news is that the likelihood objective is concave, and so these methods are guaranteed to converge to the global optimum. The bad news is that each of the steps in the iterative algorithm requires that we run inference on the network, making even simple parameter estimation a fairly expensive, or even intractable, process. Bayesian estimation, which requires integration over the space of parameters, is even harder, since there is no closed-form expression for the parameter posterior. Thus, the integration associated with Bayesian estimation must be performed using approximate inference (such as variational methods or MCMC), a burden that is often infeasible in practice. 

As a consequence of these computational issues, much of the work in this area has gone into the formulation of alternative, more tractable, objectives for this estimation problem. Other work has been focused on the use of approximate inference algorithms for this learning problem and on the development of new algorithms suited to this task. 

The same issues have signiﬁcant impact on structure learning. In particular, because a Bayesian parameter posterior is intractable to compute, the use of exact Bayesian scoring for model selection is generally infeasible. In fact, scoring any model (computing the likelihood) requires that we run inference to compute the partition function, greatly increasing the cost of search over model space. Thus, here also, the focus has been on approximations and heuristics that can reduce the computational cost of this task. Here, however, there is some good news, arising from another key distinction between Bayesian and Markov networks: the lack of a global acyclicity constraint in undirected models. Recall (see theorem 18.5) that the acyclicity constraint couples decisions regarding the family of diferent variables, thereby making the structure selection problem much harder. The lack of such a global constraint in the undirected case eliminates these interactions, allowing us to choose the local structure locally in diferent parts of the network. In particular, it turns out that a particular variant of the structure learning task can be formulated as a continuous, convex optimization problem, a class of problems generally viewed as tractable. Thus, elimination of global acyclicity removes the main reason for the $\mathcal{N P}$ -hardness of structure learning that we saw in Bayesian networks. However, this does not make structure learning of Markov networks efcient; the convex optimization process (as for parameter estimation) still requires multiple executions of inference over the network. 

A ﬁnal important issue that arises in the context of Markov networks is the overwhelmingly common use of these networks for settings, such as image segmentation and others, where we have a particular inference task in mind. In these settings, we often want to train a network disc rim i natively (see section 16.3.2), so as to provide good performance for our particular prediction task. Indeed, much of Markov network learning is currently performed for CRFs. 

The remainder of this chapter is structured as follows. We begin with the analysis of the properties of the likelihood function, which, as always, forms the basis for all of our discussion of learning. We then discuss how the likelihood function can be optimized to ﬁnd the maximum likelihood parameter estimates. The ensuing sections discuss various important extensions to these basic ideas: conditional training, parameter priors for MAP estimation, structure learning, learning with missing data, and approximate learning methods that avoid the computational bottleneck of multiple iterations of network inference. These extensions are usually described as building on top of standard maximum-likelihood parameter estimation. However, it is important to keep in mind that they are largely orthogonal to each other and can be combined. Thus, for example, we can also use the approximate learning methods in the case of structure learning or of learning with missing data. Similarly, all of the methods we described can be used with maximum conditional likelihood training. We return to this issue in section 20.8. 

We note that, for convenience and consistency with standard usage, we use natural logarithms throughout this chapter, including in our deﬁnitions of entropy or KL-divergence. 

# 20.2 The Likelihood Function 

As we saw in earlier chapters, the key component in most learning tasks is the likelihood function. In this section, we discuss the form of the likelihood function for Markov networks, its properties, and their computational implications. 

# 20.2.1 An Example 

As we suggested, the existence of a global partition function couples the diferent parameters in a Markov network, greatly complicating our estimation problem. To understand this issue, consider the very simple network $A{-}B{-}C$ , parameterized by two potentials $\phi_{1}(A,B)$ and $\phi_{2}(B,C)$ . Recall that the log-likelihood of an instance $\langle a,b,c\rangle$ is 

$$
\ln{\cal P}(a,b,c)=\ln\phi_{1}(a,b)+\ln\phi_{2}(b,c)-\ln Z,
$$ 

![](images/d0eb5f98a1cdc925706d5c1107096581cf1d0dc6ca389971d6b2c5909a720b45.jpg) 
Figure 20.1 Log-likelihood surface for the Markov network $A{-}B{-}C_{!}$ , as a function of $\ln\phi_{1}(a^{1},b^{1})$ ( $x$ -axis) and $\ln\phi_{2}(b^{0},c^{1})$ $(y$ -axis) ; all other parameters in both potentials are set to 1 . Surface is viewed from the $(+\infty,+\infty)$ point $(-,-)$ quadrant. The data set $\mathcal{D}$ has $M\,=\,100$ instances, for which $M[a^{1},b^{1}]=40$ and $M[b^{0},c^{1}]=40$ . (The other sufcient statistics are irrelevant, since all of the other log-parameters are 0.) 

where $Z$ is the partition function that ensures that the distribution sums up to one. Now, consider the log-likelihood function for a data set $\mathcal{D}$ containing $M$ instances: 

$$
\begin{array}{l l l}{\ell(\pmb\theta:\mathcal D)}&{=}&{\displaystyle\sum_{m}\left(\ln\phi_{1}(a[m],b[m])+\ln\phi_{2}(b[m],c[m])-\ln Z(\pmb\theta)\right)}\\ &{=}&{\displaystyle\sum_{a,b}M[a,b]\ln\phi_{1}(a,b)+\sum_{b,c}M[b,c]\ln\phi_{2}(b,c)-M\ln Z(\pmb\theta).}\end{array}
$$ 

Thus, we have sufcient statistics that summarize the data: the joint counts of variables that appear in each potential. This is analogous to the situation in learning Bayesian networks, where we needed the joint counts of variables that appear within the same family. This likelihood consists of three terms. The ﬁrst term involves $\phi_{1}$ alone, and the second term involves $\phi_{2}$ alone. The third term, however, is the log-partition function $\ln{\cal Z}$ , where: 

$$
Z(\theta)=\sum_{a,b,c}\phi_{1}(a,b)\phi_{2}(b,c).
$$ 

Thus, $\ln Z(\theta)$ is a function of both $\phi_{1}$ and $\phi_{2}$ . As a consequence, it couples the two potentials in the likelihood function. 

Speciﬁcally, consider maximum likelihood estimation, where we aim to ﬁnd parameters that maximize the log-likelihood function. In the case of Bayesian networks, we could estimate each conditional distribution independently of the other ones. Here, however, when we change one of the potentials, say $\phi_{1}$ , the partition function changes, possibly changing the value of $\phi_{2}$ that maximizes $-\ln Z(\theta)$ . Indeed, as illustrated in ﬁgure 20.1, the log-likelihood function in our simple example shows clear dependencies between the two potentials. 

In this particular example, we can avoid this problem by noting that the network $A{-}B{-}C$ is equivalent to a Bayesian network, say $A\rightarrow B\rightarrow C$ . Therefore, we can learn the parameters of this BN, and then deﬁne $\phi_{1}(A,B)=P(A)P(B\mid A)$ and $\phi_{2}(B,C)=P(C\mid B)$ . Because the two representations have equivalent expressive power, the same maximum likelihood is achievable in both, and so the resulting parameter iz ation for the Markov network will also be a maximum-likelihood solution. In general, however, there are Markov networks that do not have an equivalent BN structure, for example, the diamond-structured network of ﬁgure 4.13 (see section 4.5.2). In such cases, we generally cannot convert a learned BN parameter iz ation into an equivalent MN; indeed, the optimal likelihood achievable in the two representations is generally not the same. 

# 20.2.2 Form of the Likelihood Function 

log-linear model To provide a more general description of the likelihood function, it ﬁrst helps to provide a more convenient notational basis for the parameter iz ation of these models. For this purpose, we use the framework of log-linear models , as deﬁned in section 4.4.1.2. Given a set of features ${\mathcal F}=\{f_{i}(D_{i})\}_{i=1}^{k}$ , where $f_{i}(D_{i})$ is a feature function deﬁned over the variables in $D_{i}$ , we have: 

$$
P(X_{1},.\.\ ,X_{n}:\theta)={\frac{1}{Z(\theta)}}\exp\left\{\sum_{i=1}^{k}\theta_{i}f_{i}(D_{i})\right\}.
$$ 

As usual, we use $f_{i}(\xi)$ as shorthand for $f_{i}(\xi\langle D_{i}\rangle)$ parameters of this distribution corre- spond to the weight we put on each feature. When $\theta_{i}=0$ , the feature is ignored, and it has no efect on the distribution. 

As discussed in chapter 4, this representation is very generic and can capture Markov networks with global structure and local structure. A special case of particular interest is when $f_{i}(D_{i})$ is a binary indicator function that returns the value 0 or 1 . With such features, we can encode a “standard” Markov network by simply having one feature per potential entry. In more general, however, we can consider arbitrary valued features. 

Example 20.1 As a speciﬁc example, consider the simple diamond network of ﬁgure 3.10a, where we take all four variables to be binary-valued. The features that correspond to this network are sixteen indicator functions: four for each assignment of variables to each of our four clusters. For example, one such feature would be: 

$$
f_{a^{0},b^{0}}(a,b)={\bf{\cal I}}\{a=a^{0}\}{\bf{\cal I}}\{b=b^{0}\}.
$$ 

With this representation, the weight of each indicator feature is simply the natural logarithm of the corresponding potential entry. For example, $\theta_{a^{0},b^{0}}=\ln\phi_{1}(a^{0},b^{0})$ . 

Given a model in this form, the log-likelihood function has a simple form. 

Proposition 20.1 Let $\mathcal{D}$ be a data set of $M$ examples, and let ${\mathcal F}=\{f_{i}\,:\,i\,=\,1,.\,.\,.\,,k\}$ be a set of features that deﬁne a model. Then the log-likelihood is 

$$
\ell(\pmb\theta:{\mathcal D})=\sum_{i}\theta_{i}\left(\sum_{m}f_{i}(\xi[m])\right)-M\ln Z(\pmb\theta).
$$ 

sufcient statistics 

The sufcient statistics of this likelihood function are the sums of the feature values in the instances in $\mathcal{D}$ . We n derive a more elegant formulation if we divide the log-likelihood by the number of samples M . 

$$
\frac{1}{M}\ell(\pmb\theta:\mathcal D)=\sum_{i}\theta_{i}\pmb E_{\mathcal D}[f_{i}(\pmb d_{i})]-\ln Z(\pmb\theta),
$$ 

where $\pmb{E}_{\mathcal{D}}[f_{i}(\pmb{d}_{i})]$ is the empirical expectation of $f_{i}$ , that is, its average in the data set. 

# 20.2.3 Properties of the Likelihood Function 

The formulation of proposition 20.1 describes the likelihood function as a sum of two functions. The ﬁrst function is linear in the parameters; increasing the parameters directly increases this linear term. Clearly, because the log-likelihood function (for a ﬁxed data set) is upper-bounded (the probability of an event is at most 1), the second term $\ln Z(\theta)$ balances the ﬁrst term. 

Let us examine this second term in more detail. Recall that the partition function is deﬁned as 

$$
\ln Z(\pmb\theta)=\ln\sum_{\xi}\exp\left\{\sum_{i}\theta_{i}f_{i}(\xi)\right\}.
$$ 

convex partition function 

Hessian 

One important property of the partition function is that it is convex in the parameters $\theta$ . Recall that a function $f(\vec{x})$ is convex if for every $0\leq\alpha\leq1$ , 

$$
f(\alpha\vec{x}+(1-\alpha)\vec{y})\leq\alpha f(\vec{x})+(1-\alpha)f(\vec{y}).
$$ 

In other words, the function is bowl-like, and every interpolation between the images of two points is larger than the image of their interpolation. One way to prove formally that the function $f$ is convex is to show that the Hessian — the matrix of the function’s second derivatives — is positive semideﬁnite. Therefore, we now compute the derivatives of $Z(\theta)$ . 

Proposition 20.2 

$$
\begin{array}{r c l}{{\displaystyle\frac{\partial}{\partial\theta_{i}}\ln Z(\pmb\theta)}}&{{=}}&{{\pmb E_{\pmb\theta}[f_{i}]}}\\ {{\displaystyle\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}\ln Z(\pmb\theta)}}&{{=}}&{{\pmb C o v_{\pmb\theta}[f_{i};f_{j}],}}\end{array}
$$ 

Proof The ﬁrst derivatives are computed as: 

$$
\begin{array}{r c l}{{\displaystyle\frac{\partial}{\partial\theta_{i}}\ln Z(\pmb\theta)}}&{{=}}&{{\displaystyle\frac{1}{Z(\pmb\theta)}\sum_{\xi}\frac{\partial}{\partial\theta_{i}}\exp\left\{\sum_{j}\theta_{j}f_{j}(\xi)\right\}}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{\displaystyle\frac{1}{Z(\pmb\theta)}\sum_{\xi}f_{i}(\xi)\exp\left\{\sum_{j}\theta_{j}f_{j}(\xi)\right\}}}\\ {{}}&{{=}}&{{\displaystyle E_{\pmb\theta}[f_{i}].}}\end{array}
$$ 

We now consider the second derivative: 

$$
\begin{array}{r l}{\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{i}}\ln Z(\theta)}&{=\frac{\partial}{\partial\theta_{i}}\left[\frac{1}{Z(\theta)}\sum_{j\in J}i(j)\exp\left\{\sum_{i}b_{j}I_{i}(\xi)\right\}\right]}\\ &{=\ -\frac{1}{Z(\theta)^{2}}\left(\frac{\partial}{\partial\theta_{i}}Z(\theta)\right)\frac{\sum_{j}}{\sum_{i}}J_{i}(\xi)\exp\left\{\sum_{i}b_{j}I_{i}(\xi)\right\}}\\ &{\qquad+\frac{1}{Z(\theta)}\sum_{i}J_{i}(\xi)J_{i}(\xi)\exp\left\{\sum_{i}b_{j}I_{i}(\xi)\right\}}\\ &{=\ -\frac{1}{Z(\theta)^{2}}\frac{\partial}{\partial\theta_{i}}\langle\theta|\hat{J}_{i}|\sum_{j}f_{i}(\xi)\hat{P}(\xi;\theta)}\\ &{\qquad+\frac{1}{Z(\theta)}\sum_{i}J_{i}(\xi)J_{i}(\xi)\hat{P}(\xi;\theta)}\\ &{=\ -E_{\theta}J_{i}|\sum_{j}f_{i}(\xi)P_{i}(\xi;\theta)}\\ &{\qquad+\sum_{i}J_{i}(\xi)J_{i}(\xi)P_{i}(\xi;\theta)}\\ &{=\ E_{\theta}J_{i}|J_{i}-E_{\theta}|J_{i}|\hat{J}_{i}|J_{i}}\end{array}
$$ 

Thus, the Hessian of $\ln Z(\theta)$ is the covariance matrix of the features, viewed as random variables distributed according to distribution deﬁned by $\theta$ . Because a covariance matrix is always positive semideﬁnite, it follows that the Hessian is positive semideﬁnite, and hence that $\ln Z(\theta)$ is a convex function of $\theta$ . 

Because $\ln Z(\theta)$ is convex, its complement $(-\ln Z(\theta))$ is concave. The sum of a linear function and a concave function is concave, implying the following important result: 

redundant parameter iz ation 

This result implies that the log-likelihood is unimodal and therefore has no local op- tima. It does not, however, imply the uniqueness of the global optimum: Recall that a parameter iz ation of the Markov network can be redundant , giving rise to multiple representa- tions of the same distribution. The standard parameter iz ation of a set of table factors for a Markov network — a feature for every entry in the table — is always redundant. In our simple example, for instance, we have: 

$$
f_{a^{0},b^{0}}=1-f_{a^{0},b^{1}}-f_{a^{1},b^{0}}-f_{a^{1},b^{1}}.
$$ 

We thus have a continuum of parameter iz at ions that all encode the same distribution, and (necessarily) give rise to the same log-likelihood. Thus, there is a unique globally optimal value for the log-likelihood function, but not necessarily a unique solution. In general, because the function is concave, we are guaranteed that there is a convex region of continuous global optima. 

It is possible to eliminate the redundancy by removing some of the features. However, as we discuss in section 20.4, that turns out to be unnecessary, and even harmful, in practice. 

We note that we have deﬁned the likelihood function in terms of a standard log-linear param- eterization, but the exact same derivation also holds for networks that use shared parameters, as in section 6.5; see exercise 20.1 and exercise 20.2. 

# 20.3 Maximum (Conditional) Likelihood Parameter Estimation 

We now move to the question of estimating the parameters of a Markov network with a ﬁxed structure, given a fully observable data set $\mathcal{D}$ . We focus in this section on the simplest variant of this task — maximum-likelihood parameter estimation, where we select parameters that maximize the log-likelihood function of equation (20.2). In later sections, we discuss alternative objectives for the parameter estimation task. 

# 20.3.1 Maximum Likelihood Estimation 

As for any function, the gradient of the log-likelihood must be zero at its maximum points. For a concave function, the maxima are precisely the points at which the gradient is zero. Using proposition 20.2, we can compute the gradient of the average log-likelihood as follows: 

$$
\frac{\partial}{\partial\theta_{i}}\frac{1}{M}\ell(\pmb{\theta}:\mathcal{D})=\pmb{E_{\mathcal{D}}}[f_{i}(\mathcal{X})]-\pmb{E_{\theta}}[f_{i}].
$$ 

This analysis provides us with a precise characterization of the maximum likelihood parameters $\hat{\pmb\theta}$ : 

# Theorem 20.1 

expected sufcient statistics moment matching MLE consistency Let $\mathcal{F}$ be a set of features. Then, $\theta$ is a maximum-likelihood parameter assignment if and only if $\pmb{E}_{\mathcal{D}}[f_{i}(\mathcal{X})]=\pmb{E}_{\hat{\theta}}[f_{i}]$ for all $i$ . D 

In other words, at the maximal likelihood parameters $\hat{\pmb\theta}$ , the expected value of each feature relative to $P_{\hat{\theta}}$ matches its empirical expectation in $\mathcal{D}$ . In other words, we want the expected sufcient statistics in the learned distribution to match the empirical expectations. This type of equality constraint is also called moment matching . This theorem easily implies that maximum likelihood estimation is consistent in the same sense as deﬁnition 18.1: if the model is suf- ciently expressive to capture the data-generating distribution, then, at the large sample limit, the optimum of the likelihood objective is the true model; see exercise 20.3. 

By itself, this criterion does not provide a constructive deﬁnition of the maximum likelihood parameters. Unfortunately, although the function is concave, there is no analytical form for its maximum. Thus, we must resort to iterative methods that search for the global opti- mum. Most commonly used are the gradient ascent methods reviewed in appendix A.5.2, which iteratively take steps in parameter space to improve the objective. At each iteration, they compute the gradient, and possibly the Hessian, at the current point $\theta$ , and use those estimates to approximate the function at the current neighborhood. They then take a step in the right direction (as dictated by the approximation) and repeat the process. Due to the convexity of the problem, this process is guaranteed to converge to a global optimum, regardless of our starting point. 

To apply these gradient-based methods, we need to compute the gradient. Fortunately, equa- tion (20.4) provides us with an exact formula for the gradient: the diference between the feature’s empirical count in the data and its expected count relative to our current parame- terization $\theta$ . For example, consider again the fully parameterized network of example 20.1. Here, the features are simply indicator functions; the empirical count for a feature such as $f_{a^{0},b^{0}}(a,b)=I\!\!\!\{a=a^{0}\}I\!\!\!\{b=b^{0}\}$ { } { } is simply e empirical frequency, in the data set $\mathcal{D}$ , of the event $a^{0},b^{0}$ . At a particular parameter iz ation θ , the expected count is simply $P_{\theta}(a^{0},b^{0})$ . Very naturally, the gradient for the parameter associated with this feature is the diference between these two numbers. 

However, this discussion ignores one important aspect: the computation of the expected counts. In our example, for instance, we must compute the diferent probabilities of the form $P_{\theta^{t}}(a,b)$ . Clearly, this computation requires that we run inference over the network. As for the case of EM in Bayesian networks, a feature is necessarily part of a factor in the original network, and hence, due to family preservation, all of the variables involved in a feature must occur together in a cluster in a clique tree or cluster graph. Thus, a single inference pass that calibrates an entire cluster graph or tree sufces to compute all of the expected counts. Nevertheless, a full inference step is required at every iteration of the gradient ascent procedure. Because inference is almost always costly in time and space, the computational cost of parameter estimation in Markov networks is usually high, sometimes prohibitively so. In section 20.5 we return to this issue, considering the use of approximate methods that reduce the computational burden. 

log-likelihood Hessian 

L-BFGS algorithm 

Our discussion does not make a speciﬁc choice of algorithm to use for the optimization. In practice, standard gradient ascent is not a particularly good algorithm, both because of its slow convergence rate and because of its sensitivity to the step size. Much faster convergence is obtained with second-order methods, which utilize the Hessian to provide a quadratic approx- imation to the function. However, from proposition 20.2 we can conclude that the Hessian of the log-likelihood function has the form: 

$$
\frac{\partial}{\partial\theta_{i}\partial\theta_{j}}\ell(\pmb{\theta}:\mathcal{D})=-M\mathbb{C}o v_{\pmb{\theta}}[f_{i};f_{j}].
$$ 

To compute the Hessian, we must compute the joint expectation of two features, a task that is often computationally infeasible. Currently, one commonly used solution is the L-BFGS al- gorithm , a gradient-based algorithm that uses line search to avoid computing the Hessian (see appendix A.5.2 for some background). 

# 20.3.2 Conditionally Trained Models 

discriminative training 

conditional random ﬁeldconditional likelihood 

As we discussed in section 16.3.2, we often want to use a Markov network to perform a par- ticular inference task, where we have a known set of observed variables, or features, $X$ , and a predetermined set of variables, $Y$ , that we want to query. In this case, we may prefer to use discriminative training , where we train the network as a conditional random ﬁeld (CRF) that encodes a conditional distribution $P(Y\mid X)$ . 

More formally, in this s our training set consists of pairs ${\mathcal D}\,=\,\{(\pmb{y}[m],\pmb{x}[m])\}_{m=1}^{M}$ , specifying assignments to $Y,X$ . An appropriate objective function to use in this situation is the conditional likelihood or its logarithm, deﬁned in equation (16.3). In our setting, the log-conditional-likelihood has the form: 

$$
\ell_{Y\mid X}(\pmb\theta:\mathcal D)=\ln P(\pmb y[1,\dots,M]\mid\pmb x[1,\dots,M],\pmb\theta)=\sum_{m=1}^{M}\ln P(\pmb y[m]\mid\pmb x[m],\pmb\theta).
$$ 

In this objective, we are optimizing the likelihood of each observed assignment $\pmb{y}[m]$ given the corresponding observed assignment $\pmb{x}[m]$ . Each of the terms $\ln P(\pmb{y}[1,\dots,M]\mid\pmb{x}[1,.\,.\,,M],\pmb{\theta})$ is a log-likelihood of a Markov network model with a diferent set of factors — the factors in the original network, reduced by the observation ${\pmb x}[1,.\,.\,.\,,M]\ .$ — and its own partition function. Each term is thereby a concave function, and because the sum of concave functions is concave, we conclude: 

As for corollary 20.1, this result implies that the function has a global optimum and no local op- tima, but not that the global optimum is unique. Here also, redundancy in the parameter iz ation may give rise to a convex region of contiguous global optima. 

The approaches for optimizing this objective are similar to those used for optimizing the likelihood objective in the unconditional case. The objective function is a concave function, and so a gradient ascent process is guaranteed to give rise to the unique global optimum. The form of the gradient here can be derived directly from equation (20.4). We ﬁrst observe that the gradient of a sum is the sum of the gradients of the individual terms. Here, each term is, in fact, a log-likelihood — the log-likelihood of a single data case $\pmb{y}[m]$ in the Markov network obtained by reducing our original model to the context $\pmb{x}[m]$ . A reduced Markov network is itself a Markov network, and so we can apply equation (20.4) and conclude that: 

$$
\frac{\partial}{\partial\theta_{i}}\ell_{Y\mid X}(\pmb\theta:\mathcal D)=\sum_{m=1}^{M}\left(f_{i}(\pmb y[m],\pmb x[m])-\pmb E_{\pmb\theta}[f_{i}\mid\pmb x[m]]\right).
$$ 

This solution looks deceptively similar to equation (20.4). Indeed, if we aggregate the ﬁrst component in each of the summands, we obtain precisely the empirical count of $f_{i}$ in the data set $\mathcal{D}$ . There is, however, one key diference. In the unreduced Markov network, the expected feature counts are computed relative to a single model; in the case of the conditional Markov network, these expected counts are computed as the summation of counts in an ensemble of models, deﬁned by the diferent values of the conditioning variables $\pmb{x}[m]$ . This diference has signiﬁcant computational consequences. Recall that computing these expectations involves running inference over the model. Whereas in the unconditional case, each gradient step required only a single execution of inference, when training a CRF, we must (in general) execute inference for every single data case , conditioning on $\pmb{x}[m]$ . On the other hand, the inference is executed on a simpler model, since conditioning on evidence in a Markov network can only reduce the computational cost. For example, the network of ﬁgure 20.2 is very densely connected, whereas the reduced network over $Y$ alone (conditioned on $X$ ) is a simple chain, allowing linear-time inference. 

Discriminative training can be particularly beneﬁcial in cases where the domain of $X$ is very large or even inﬁnite. For example, in our image classiﬁcation task, the partition function in the 

![](images/8b4bd1f3bbcd7c838c04e1e2e1b6966dda3d19d814a8d439f0868cc350b9ab22.jpg) 
Figure 20.2 A highly connected CRF that allows simple inference when conditioned: The edges that disappear in the reduced Markov network after conditioning on $_{X}$ are marked in gray; the remaining edges form a simple linear chain. 

generative setting involves summation (or integration) over the space of all possible images; if we have an $N\times N$ image where each pixel can take 256 values, the resulting space has $256^{N^{2}}$ values, giving rise to a highly intractable inference problem (even using approximate inference methods). 

collective classiﬁcation 

sequence labeling 

activity recognition 

hidden Markov model maximum entropy Markov model conditional random ﬁeld 

Box 20.A — Concept: Generative and Discriminative Models for Sequence Labeling. One of the main tasks to which probabilistic graphical models have been applied is that of taking a set of interrelated instances and jointly labeling them, a process sometimes called collective clas- siﬁcation . We have already seen examples of this task in box 4.B and in box 4.E; many other examples exist. Here, we discuss some of the trade-ofs between diferent models that one can apply to this task. We focus on the context of labeling instances organized in a sequence, since it is simpler and allows us to illustrate another important point. 

In the sequence labeling task, we get as input a sequence of observations $X$ and need to label them with some joint label $Y$ . For example, in text analysis (box 4.E), we might have a sequence of words each of which we want to label with some label. In a task of activity recognition , we might obtain a sequence of images and want to label each frame with the activity taking place in it (for example, running, jumping, walking). We assume that we want to construct a model for this task and to train it using fully labeled training data, where both $Y$ and $X$ are observed. 

Figure 20.A.1 illustrates three diferent types of models that have been proposed and used for sequence labeling, all of which we have seen earlier in this book (see ﬁgure 6.2 and ﬁgure 4.14). The ﬁrst model is $a$ hidden Markov model (or HMM), which is a purely generative model: the model generates both the labels $Y$ and the observations $X$ . The second is called a maximum entropy Markov model (or MEMM). This model is also directed, but it represents a conditional distribution $P(\pmb{Y}\mid\pmb{X})$ ; hence, there is no attempt to model a distribution over the $X s$ . The ﬁnal model is the conditional random ﬁeld (or CRF) of section 4.6.1. This model also encodes a conditional distribution; hence the arrows from $X$ to $Y$ . However, here the interactions between the $Y$ are modeled as undirected edges. 

These diferent models present interesting trade-ofs in terms of their expressive power and learn- ability. First, from a computational perspective, HMMs and MEMMs are much more easily learned. As purely directed models, their parameters can be computed in closed form using either maximum- likelihood or Bayesian estimation (see chapter 17); conversely, the CRF requires that we use an 

![](images/4f58bf562d6646d853801bd631e6b3cf9219c92e5279993fc9fbececb0b4e314.jpg) 
Figure 20.A.1 — Diferent models for sequence labeling: HMM, MEMM, and CRF 

iterative gradient-based approach, which is considerably more expensive (particularly here, when inference must be run separately for every training sequence; see section 20.3.2). 

A second important issue relates to our ability to use a rich feature set. As we discussed in example 16.3 and in box 4.E, our success in a classiﬁcation task often depends strongly on the quality of our features. In an HMM, we must explicitly model the distribution over the features, including the interactions between them. This type of model is very hard, and often impossible, to construct correctly. The MEMM and the CRF are both discriminative models, and therefore they avoid this challenge entirely. 

The third and perhaps subtler issue relates to the independence assumptions made by the model. discusse ction 4.6.1.2, the MEMM makes the independence assumption that $(Y_{i}\perp X_{j}\mid$ $X_{-j}$ ) for any j > i . Thus, an observation from later in the sequence has absolutely no efect on − the posterior probability of the current state; or, in other words, the model does not allow for any smoothing. The implications of this can be severe in many settings. For example, consider the task of activity recognition from a video sequence; here, we generally assume that activities are highly persistent: if a person is walking in one frame, she is also extremely likely to be walking in the next frame. Now, imagine that the person starts running, but our ﬁrst few observations in the sequence are ambiguous and consistent with both running and walking. The model will pick one — the one whose probability given that one frame is highest — which may well be walking. Assuming that activities are persistent, this choice of activity is likely to stay high for a large number of steps; the posterior of the initial activity will never change. In other words, the best we can expect is a prediction where the initial activity is walking, and then (perhaps) transitions to running. The model is incapable of going back and changing its prediction about the ﬁrst few frames. This problem has been called the label bias problem . 

To summarize, the trade-ofs between these diferent models are subtle and non- deﬁnitive. In cases where we have many correlated features, discriminative models are probably better; but, if only limited data are available, the stronger bias of the generative model may dominate and allow learning with fewer samples. Among the discriminative models, MEMMs should probably be avoided in cases where many transitions are close to deterministic. In many cases, CRFs are likely to be a safer choice, but the computational cost may be prohibitive for large data sets. 

# 20.3.3 Learning with Missing Data 

We now turn to the problem of parameter estimation in the context of missing data. As we saw in section 19.1, the introduction of missing data introduces both conceptual and technical difculties. In certain settings, we may need to model explicitly the process by which data are observed. Parameters may not be identiﬁable from the data. And the likelihood function becomes signiﬁcantly more complex: there is coupling between the likelihood’s dependence on diferent parameters; worse, the function is no longer concave and generally has multiple local maxima. 

The same issues regarding observation processes (ones that are not missing at random) and identiﬁability arise equally in the context of Markov network learning. The issue regarding the complexity of the likelihood function is analogous, although not quite the same. In the case of Markov networks, of course, we have coupling between the parameters even in the likelihood function for complete data. However, as we discuss, in the complete data case, the log-likelihood function is concave and easily optimized using gradient methods. Once we have missing data, we lose the concavity of the function and can have multiple local maxima. Indeed, the example we used was in the context of a Bayesian network of the form $X\,\rightarrow\,Y$ , which can also be represented as a Markov network. Of course, the parameter iz ation of the two models is not the same, and so the form of the function may difer. However, one can verify that a function that is multimodal in one parameter iz ation will also be multimodal in the other. 

# 20.3.3.1 Gradient Ascent 

As in the case of Bayesian networks, if we assume our data is missing at random, we can perform maximum-likelihood parameter estimation by using some form of gradient ascent process to optimize the likelihood function. Let us therefore begin by analyzing the form of the gradient in the case of missing data. Let $\mathcal{D}$ be a data set w some entries are missing; let $o[m]$ be the observed entries in the m th data instance and H $\mathcal{H}[m]$ be the random variables that are the missing entries n that instance, so that for any $h[m]\in V a l({\mathcal{H}}[m])$ , $(o[m],h[m])$ is a complete assignment to X . 

As usual, the average log-likelihood function has the form: 

$$
\begin{array}{r c l}{{\displaystyle\frac{1}{M}\ln P(\mathcal{D}\mid\theta)}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m=1}^{M}\ln\left(\sum_{h[m]}P(o[m],h[m]\mid\theta)\right)}}\\ {{}}&{{}}&{{=\displaystyle\frac{1}{M}\sum_{m=1}^{M}\ln\left(\sum_{h[m]}\tilde{P}(o[m],h[m]\mid\theta)\right)-\ln Z.}}\end{array}
$$ 

Now, consider a single term within the summation, $\textstyle\sum_{\pmb{h}[m]}{\tilde{P}}(o[m],\pmb{h}[m]\ |\ \theta)$ . This expres- sion has the same form as a partition function; indeed, it is precisely the partition function for the Markov network that we would obtain by reducing our original Markov network with the observation $o[m]$ , to obtain a Markov network representing the conditional distribution $\tilde{P}(\mathcal{H}[m]\mid\mathbf{o}[m])$ H | . Therefore, we can apply proposition 20.2 and conclude that: 

$$
\frac{\partial}{\partial\theta_{i}}\ln\sum_{h[m]}\tilde{P}(o[m],h[m]\mid\theta)\quad=\quad E_{h[m]\sim P(\mathcal{H}[m]\mid o[m],\theta)}[f_{i}],
$$ 

that is, the gradient of this term is simply the conditional expectation of the feature, given the observations in this instance. 

Putting this together with previous computations, we obtain the following: 

Proposition 20.3 For a data set $\mathcal{D}$ 

$$
\frac{\partial}{\partial\theta_{i}}\frac{1}{M}\ell(\pmb{\theta}:\mathcal{D})=\frac{1}{M}\left[\sum_{m=1}^{M}\pmb{E}_{h[m]\sim P(\mathcal{H}[m]|\pmb{\theta}[m],\pmb{\theta})}[f_{i}]\right]-\pmb{E}_{\pmb{\theta}}[f_{i}].
$$ 

In other words, the gradient for feature $f_{i}$ in the case of missing data is the diference between two expectations — the feature expectation over the data and the hidden variables minus the feature expectation over all of the variables. 

It is instructive to compare the cost of this computation to that of computing the gradient in equation (20.4). For the latter, to compute the second term in the derivative, we need to run inference once, to compute the expected feature counts relative to our current distribution $P(\mathcal{X}\mid\theta)$ . The ﬁrst term is computed by simply aggregating the feature over the data. By comparison, to compute the derivative here, we actually need to run inference separately for every instance $m$ , conditioning on $o[m]$ . Although inference in the reduced network may be simpler (since reduced factors are simpler), the cost of this computation is still much higher than learning without missing data. Indeed, not surprisingly, the cost here is comparable to the cost of a single iteration of gradient descent or EM in Bayesian network learning. 

# 20.3.3.2 Expectation Maximization 

As for any other probabilistic model, an alternative method for parameter estimation in context of missing data is via the expectation maximization algorithm. In the case of Bayesian network learning, EM seemed to have signiﬁcant advantages. Can we deﬁne a variant of EM for Markov networks? And does it have the same beneﬁts? 

The answer to the ﬁrst question is clearly yes. We can perform an E-step by using our current parameters $\pmb{\theta}^{(t)}$ to compute the expected sufcient statistics, in this case, the expected feature counts. That is, at iteration $t$ of the EM algorithm, we compute, for each feature $f_{i}$ , the expected sufcient statistic: 

$$
\bar{M}_{\pmb\theta^{(t)}}[f_{i}]=\frac{1}{M}\left[\sum_{m=1}^{M}\pmb E_{\pmb h[m]\sim P(\mathcal{H}[m]|\pmb o[m],\pmb\theta)}[f_{i}]\right].
$$ 

With these expected feature counts, we can perform an M-step by doing maximum likelihood parameter estimation. The proofs of convergence and other properties of the algorithm go through unchanged. 

Here, however, there is one critical diference. Recall that, in the case of directed models, given the expected sufcient statistics, we can perform the M-step efciently, in closed form. By contrast, the M-step for Markov networks requires that we run inference multiple times, once for each iteration of whatever gradient ascent procedure we are using. At step $k$ of this “inner-loop” optimization, we now have a gradient of the form: 

$$
\bar{M}_{\pmb\theta^{(t)}}[f_{i}]-\pmb E_{\pmb\theta^{(t,k)}}[f_{i}].
$$ 

The trade-ofs between the two algorithms are now more subtle than in the case of Bayesian networks. For the joint gradient ascent procedure of the previous section, we need to run inference $M+1$ times in each gradient step: once without evidence, and once for each data case. If we use EM, we run inference $M$ times to compute the expected sufcient statistics in the E-step, and then once for each gradient step, to compute the second term in the gradient. Clearly, there is a computational savings here. However, each of these gradient steps now uses an “out-of-date” set of expected sufcient statistics, making it increasingly less relevant as our optimization proceeds. 

In fact, we can view the EM algorithm, in this case, as a form of caching of the ﬁrst term in the derivative: Rather than compute the expected counts in each iteration, we compute them every few iterations, take a number of gradient steps, and then recompute the expected counts. There is no need to run the “inner-loop” optimization until convergence; indeed, that strategy is often not optimal in practice. 

# 20.3.4 Maximum Entropy and Maximum Likelihood $\star$ 

We now return to the case of basic maximum likelihood estimation, in order to derive an alternative formulation that provides signiﬁcant insight. In particular, we now use theorem 20.1 to relate maximum likelihood estimation in log-linear models to another important class or problems examined in statistics: the problem of ﬁnding the distribution of maximum entropy subject to a set of constraints. 

To motivate this alternative formulation, consider a situation where we are given some sum- mary statistics of an empirical distribution, such as those that may be published in a census report. These statistics may include the marginal distributions of single variables, of certain pairs, and perhaps of other events that the researcher summarizing the data happened to con- sider of interest. As another example, we might know the average ﬁnal grade of students in the class and the correlation of their ﬁnal grade with their homework scores. However, we do not have access to the full data set. While these two numbers constrain the space of possible distributions over the domain, they do not specify it uniquely. Nevertheless, we might want to construct a “typical” distribution that satisﬁes the constraints and use it to answer other queries. 

maximum entropy 

expectation constraints 

One compelling intuition is that we should select a distribution that satisﬁes the given con- straints but has no additional “structure” or “information.” There are many ways of making this intuition precise. One that has received quite a bit of attention is based on the intuition that entropy is the inverse of information, so that we should search for the distribution of highest entropy. (There are more formal justiﬁcations for this intuition, but these are beyond the scope of this book.) More formally, in maximum entropy estimation, we solve the following problem: 

Maximum-Entropy : Find $\begin{array}{r c l}{{}}&{{}}&{{\mathcal{Q}(\mathcal{X})}}\\ {{}}&{{}}&{{H_{Q}(\mathcal{X})}}\\ {{}}&{{}}&{{}}\\ {{}}&{{}}&{{E_{Q}[f_{i}]=E_{\mathcal{D}}[f_{i}]\quad i=1,\ldots,k.}}\end{array}$ maximizing subject to 

The constraints of equation (20.10) are called expectation constraints , since they constrain us to the set of distributions that have a particular set of expectations. We know that this set is non-empty, since we have one example of a distribution that satisﬁes these constraints — the empirical distribution. 

Somewhat surprisingly, the solution to this problem is a Gibbs distribution over the features $\mathcal{F}$ that matches the given expectations. 

The distribution $Q^{*}$ is the maximum entropy distribution satisfying equation (20.10) if and only if $Q^{*}=P_{\hat{\theta}}$ , where 

$$
P_{\hat{\theta}}(\mathcal{X})=\frac{1}{Z(\hat{\theta})}\exp\left\{\sum_{i}\hat{\theta}_{i}f_{i}(\mathcal{X})\right\}
$$ 

and $\hat{\pmb\theta}$ is the maximum likelihood parameter iz ation relative to $\mathcal{D}$ . 

Proof For notational simplicity, let ${\cal P}\,=\,{\cal P}_{\hat{\theta}}$ . From theorem 20.1, it follows that $\pmb{E}_{P}[f_{i}]\;=\;$ $E_{D}[f_{i}(\mathcal{X})]$ X for $i\:=\:1,\ldots,k$ , and hence that $P$ satisﬁes the constraints of equation (20.10). D Therefore, to prove that $P\,=\,Q^{*}$ , we need only show that $H_{P}(\mathcal{X})\;\geq\;H_{Q}(\mathcal{X})$ X ≥ X for all other distributions $Q$ that satisfy these constraints. Consider any such distribution $Q$ . 

From proposition 8.1, it follows that: 

$$
H_{P}(\mathcal{X})=-\sum_{i}\hat{\theta}_{i}\pmb{E}_{P}[f_{i}]+\ln Z(\theta).
$$ 

Thus, 

$$
\begin{array}{r c l}{H_{P}(\mathcal{X})-H_{Q}(\mathcal{X})}&{=}&{-\displaystyle\left[\sum_{i}\hat{\theta}_{i}\pmb{E}_{P}[f_{i}(\mathcal{X})]\right]+\ln Z_{P}-\pmb{E}_{Q}[-\ln Q(\mathcal{X})]}\\ {(i)}&{=}&{-\displaystyle\left[\sum_{i}\hat{\theta}_{i}\pmb{E}_{Q}[f_{i}(\mathcal{X})]\right]+\ln Z_{P}+\pmb{E}_{Q}[\ln Q(\mathcal{X})]}\\ &{=}&{\pmb{E}_{Q}[-\ln P(\mathcal{X})]+\pmb{E}_{Q}[\ln Q(\mathcal{X})]}\\ &{=}&{\pmb{D}(Q\|P)\geq0,}\end{array}
$$ 

where (i) follows from the fact that both $P_{\hat{\theta}}$ and $Q$ satisfy the constraints, so that $E_{P_{\hat{\theta}}}[f_{i}]\,=$ $E_{Q}[f_{i}]$ for all $i$ . 

We conclude that $H_{P_{\hat{\theta}}}(\mathcal{X})\;\geq\;H_{Q}(\mathcal{X})$ with equality if and only if $P_{\hat{\theta}}\,=\,Q$ . Thus, the maximum entropy distribution $Q^{*}$ is necessarily equal to $P_{\hat{\theta}}$ , proving the result. 

duality 

One can also provide an alternative proof of this result based on the concept of duality discussed in appendix A.5.4. Using this alternative derivation, one can show that the two prob- lems, maximizing the entropy given expectation constraints and maximizing the likelihood given structural constraints on the distribution, are convex duals of each other. (See exercise 20.5.) 

Both derivations show that these objective functions provide bounds on each other, and are identical at their convergence point. That is, for the maximum likelihood parameters $\hat{\pmb\theta}$ , 

$$
H_{P_{\hat{\theta}}}(\mathcal{X})=-\frac{1}{M}\ell(\hat{\pmb{\theta}}:\mathcal{D}).
$$ 

As a consequence, we see that for any set of parameters $\theta$ and for any distribution $Q$ that satisfy the expectation constraints equation (20.10), we have that 

$$
H_{Q}(\mathcal{X})\leq H_{P_{\hat{\theta}}}(\mathcal{X})=-\frac{1}{M}\ell(\hat{\pmb{\theta}}:\mathcal{D})\leq-\frac{1}{M}\ell(\pmb{\theta}:\mathcal{D})
$$ 

with equality if and only if $Q\,=\,\mathcal{P}_{\theta}$ . We note that, while we provided a proof for this result from ﬁrst principles, it also follows directly from the theory of convex duality. 

Our discussion has shown an entropy dual only for likelihood. A similar connection can be shown between conditional likelihood and conditional entropy; see exercise 20.6. 

# 20.4 Parameter Priors and Regularization 

So far, we have focused on maximum likelihood estimation for selecting parameters in a Markov network. However, as we discussed in chapter 17, maximum likelihood estimation (MLE) is prone to overﬁtting to the training data. Although the efects are not as transparent in this case (due to the lack of direct correspondence between empirical counts and parameters), overﬁtting of the maximum likelihood estimator is as much of a problem here. 

MAP estimation 

As for Bayesian networks, we can reduce the efect of overﬁtting by introducing a prior distribution $P(\theta)$ over the model parameters. Note that, because we do not have a decomposable closed form for the likelihood function, we do not obtain a decomposable closed form for the posterior in this case. Thus, a fully Bayesian approach, where we integrate out the parameters to compute the next prediction, is not generally feasible in Markov networks. However, we can aim to perform MAP estimation — to ﬁnd the parameters that maximize $P(\theta)P(\mathcal{D}\mid\theta)$ . 

Given that we have no constraints on the conjugacy of the prior and the likelihood, we can consider virtually any reasonable distribution as a possible prior. However, only a few priors have been applied in practice. 

# 20.4.1 Local Priors 

Most commonly used is a Gaussian prior on the log-linear parameters $\theta$ . The most standard form of this prior is simply a zero-mean diagonal Gaussian, usually with equal variances for each of the weights: 

$$
P(\theta\mid\sigma^{2})=\prod_{i=1}^{k}\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{\theta_{i}^{2}}{2\sigma^{2}}\right\},
$$ 

hyperparameter 

$L_{2}$ -regularization for some choice of the variance $\sigma^{2}$ . This variance is a hyperparameter , as were the $\alpha_{i}$ ’s in the Dirichlet distribution (section 17.3.2). Converting to log-space (in which the optimization is typically done), this prior gives rise to a term of the form: 

$$
-\frac{1}{2\sigma^{2}}\sum_{i=1}^{k}\theta_{i}^{2},
$$ 

This term places a quadratic penalty on the magnitude of the weights, where the penalty is measured in Euclidean, or $L_{2}$ -norm, generally called an $L_{2}$ -regularization term. This term is 

![](images/2f272936c7dcf98d7e134b0142ad512d46abc646d0460cbeebfc80bb84c1268f.jpg) 
Figure 20.3 Laplacian distribution $(\beta=1)$ ) and Gaussian distribution $(\sigma^{2}=1)$ ) 

concave, and therefore it gives rise to a concave objective, which can be optimized using the same set of methods as standard MLE. 

Laplacian distribution 

A diferent prior that has been used in practice uses the zero-mean Laplacian distribution , which, for a single parameter, has the form 

$$
P_{L a p l a c i a n}(\theta\mid\beta)=\frac{1}{2\beta}\exp\left\{-\frac{|\theta|}{\beta}\right\}.
$$ 

One example of the Laplacian distribution is shown in ﬁgure 20.3; it has a non diferent i able point at $\theta=0$ , arising from the use of the absolute value in the exponent. As for the Gaussian case, one generally assumes that the diferent parameters $\theta_{i}$ are independent, and often (but not always) that they are identically distributed with the same hyperparameter $\beta$ . Taking the logarithm, we obtain a term 

$$
-\frac{1}{\beta}\sum_{i=1}^{k}|\theta_{i}|
$$ 

$L_{1}$ -regularization that also penalizes weights of high magnitude, measured using the $L_{1}$ -norm. Thus, this approach is generally called $L_{1}$ -regularization . 

Both forms of regularization penalize parameters whose magnitude (positive or negative) is large. Why is a bias in favor of parameters of low magnitude a reasonable one? Recall from our discussion in section 17.3 that a prior often serves to pull the distribution toward an “uninformed” one, smoothing out ﬂuctuations in the data. Intuitively, a distribution is “smooth” if the probabilities assigned to diferent assignments are not radically diferent. Consider two assignments $\xi$ and $\xi^{\prime}$ ; their relative probability is 

$$
\frac{P(\xi)}{P(\xi^{\prime})}=\frac{\tilde{P}(\xi)/Z_{\theta}}{\tilde{P}(\xi^{\prime})/Z_{\theta}}=\frac{\tilde{P}(\xi)}{\tilde{P}(\xi^{\prime})}.
$$ 

$\tilde{P}$ Moving to log-space and expanding the unnormalized measure , we obtain: 

$$
\begin{array}{r c l}{\displaystyle\ln\frac{P(\xi)}{P(\xi^{\prime})}}&{=}&{\displaystyle\sum_{i=1}^{k}\theta_{i}f_{i}(\xi)-\sum_{i=1}^{k}\theta_{i}f_{i}(\xi^{\prime})}\\ &{=}&{\displaystyle\sum_{i=1}^{k}\theta_{i}(f_{i}(\xi)-f_{i}(\xi^{\prime})).}\end{array}
$$ 

When all of the $\theta_{i}$ ’s have small magnitude, this log-ratio is also bounded, resulting in a smooth distribution. Conversely, when the parameters can be large, we can obtain a “spiky” distribution with arbitrarily large diferences between the probabilities of diferent assignments. 

In both the $L_{2}$ and the $L_{1}$ case, we penalize the magnitude of the parameters. In the Gaussian case, the penalty grows quadratically with the parameter magnitude, implying that an increase in magnitude in a large parameter is penalized more than a similar increase in a small parameter. For example, an increase in $\theta_{i}$ from 0 to 0 . 1 is penalized less than an increase from 3 to 3 . 1 . In the Laplacian case, the penalty is linear in the parameter magnitude, so that the penalty growth is invariant over the entire range of parameter values. This property has important ramiﬁcations. In the quadratic case, as the parameters get close to 0 , the efect of the penalty diminishes. Hence, the models that optimize the penalized likelihood tend to have many small weights. Although the resulting models are smooth, as desired, they are structurally quite dense. By comparison, in the $L_{1}$ case, the penalty is linear all the way until the parameter value is 0 . This penalty provides a continued incentive for parameters to shrink until they actually hit 0 . As a consequence, the models learned with an $L_{1}$ penalty tend to be much sparser than those learned with an $L_{2}$ penalty, with many parameter weights achieving a value of 0 . From a structural perspective, this efect gives rise to models with fewer edges and sparser potentials, which are potentially much more tractable. We return to this issue in section 20.7. 

Importantly, both the $L_{1}$ and $L_{2}$ regularization terms are concave. Because the log-likelihood is also concave, the resulting posterior is concave, and can therefore be optimized efciently using the gradient-based methods we described for the likelihood case. Moreover, the introduc- tion of these penalty terms serves to reduce or even eliminate multiple (equivalent) optima that arise when the parameter iz ation of the network is redundant. For example, consider the trivial example where we have no data. In this case, the maximum likelihood solution is (as desired) the uniform distribution. However, due to redundancy, there is a continuum of parameteriza- tions that give rise to the uniform distribution. However, when we introduce either of the earlier prior distributions, the penalty term drives the parameters toward zero, giving rise to the unique optimum $\mathbf{\nabla}\theta={\bf{0}}$ . Although one can still construct examples where multiple optima occur, they are very rare in practice. Conversely, methods that eliminate redundancies by reexpressing some of the parameters in terms of others can produce undesirable interactions with the regularization terms, giving rise to priors where some parameters are penalized more than others. 

The regularization hyperparameters — $\sigma^{2}$ in the $L_{2}$ case, and $\beta$ in the $L_{1}$ case — encode the strength in our belief that the model weights should be close to 0 . The larger these parameters (both in the denominator), the broader our parameter prior, and the less strong our bias toward 0 . In principle, any choice of hyperparameter is legitimate, since a prior is simply a reﬂection of our beliefs. In practice, however, the choice of prior can have a signiﬁcant efect on the quality of our learned model. A standard method for selecting this parameter is via a cross-validation procedure, as described in box 16.A: We repeatedly partition the training set, learn a model over one part with some choice of hyperparameter, and measure the performance of the learned model (for example, log-likelihood) on the held-out fragment. 

# 20.4.2 Global Priors 

An alternative approach for deﬁning priors is to search for a conjugate prior . Examining the likelihood function, we see that the posterior over parameters has the following general form: 

$$
\begin{array}{r c l}{P(\pmb{\theta}\mid\mathcal{D})}&{\propto}&{P(\pmb{\theta})P(\mathcal{D}\mid\pmb{\theta})}\\ &{=}&{P(\pmb{\theta})\exp\left\{\displaystyle\sum_{i}M\pmb{E}_{\mathcal{D}}[f_{i}]\theta_{i}-M\ln Z(\pmb{\theta})\right\}.}\end{array}
$$ 

This expression suggests that we use a family of prior distributions of the form: 

$$
P(\pmb\theta)\propto\exp\left\{\sum_{i}M_{0}\alpha_{i}\theta_{i}-M_{0}\ln Z(\pmb\theta)\right\}.
$$ 

This form deﬁnes a family of priors with hyperparameters $\{\alpha_{i}\}$ . It is easy to see that the posterior is from the same family with $\alpha_{i}^{\prime}=\alpha_{i}+E_{\mathcal{D}}[f_{i}]$ and $M_{0}^{\prime}=M_{0}+M$ , so that this prior D is conjugate to the log-linear model likelihood function. 

We can think of hyperparameters $\{\alpha_{i}\}$ as specifying the sufcient statistics from prior observations and of $M_{0}$ as specifying the number of these prior observations. This formulation is quite similar to the use of pseudocounts in the BDe priors for directed models (see section 17.4.3). The main diference from directed models is that this conjugate family (both the prior and the likelihood) does not decompose into independent priors for the diferent features. 

# 20.5 Learning with Approximate Inference 

The methods we have discussed here assume that we are able to compute the partition function $Z(\theta)$ and expectations such as $\pmb{E}_{P_{\theta}}[f_{i}]$ . In many real-life applications the structure of the network does not allow for exact computation of these terms. For example, in applications to image segmentation (box 4.B), we generally use a grid-structured network, which requires exponential size clusters for exact inference. 

The simplest approach for learning in intractable networks is to apply the learning procedure (say, conjugate gradient ascent) using an approximate inference procedure to compute the re- quired queries about the distribution $P_{\theta}$ . This view decouples the question of inference from learning and treats the inference procedure as a black box during learning. The success of such an approach depends on whether the approximation method interferes with the learning. In particular, nonconvergence of the inference method, or convergence to approximate answers, can lead to inaccurate and even oscillating estimates of the gradient, potentially harming convergence of the overall learning algorithm. This type of situation can arise both in particle-based methods (say MCMC sampling) and in global algorithms such as belief propagation. In this section, we describe several methods that better integrate the inference into the learning outer loop in order to reduce problems such as this. 

A second approach for dealing with inference-induced costs is to come up with alternative (possibly approximate) objective functions whose optimization does not require (as much) in- ference. Some of these techniques are reviewed in the next section. However, one of the main messages of this section is that the boundary between these two classes of methods is 

approximate inference algorithm to compute the gradient can often be reformulated as exactly optimizing an approximate objective. When applicable, this view is often more insightful and also more usable. First, it provides more insight about the outcome of the opti- mization. Second, it may allow us to bound the error in the optimum in terms of the distance between the two functions being optimized. Finally, by formulating a clear objective to be optimized, we can apply any applicable optimization algorithm, such as conjugate gradient or Newton’s method. 

Importantly, while we describe the methods in this section relative to the plain likelihood objective, they apply almost without change to the generalizations and extensions we describe in this chapter: conditional Markov networks; parameter priors and regularization; structure learning; and learning with missing data. 

# 20.5.1 Belief Propagation 

A fairly popular approach for approximate inference is the belief propagation algorithm and its variants. Indeed, in many cases, an algorithm in this family would be used for inference in the model resulting from the learning procedure. In this case, it can be shown that we should learn the model using the same inference algorithm that will be used for querying it. Indeed, it can be shown that using a model trained with the same approximate inference algorithm is better than using a model trained with exact inference. 

belief propagation 

At ﬁrst glance, the use of belief propgation for learning appears straightforward. We can simply run BP within every iteration of gradient ascent to compute the expected feature counts used in the gradient computation. Due to the family preservation property, each feature $f_{i}$ must be a subset of a cluster $C_{i}$ in the cluster graph. Hence, to compute the expected feature count $\pmb{{\cal E}}_{\theta}[f_{i}]$ , we can compute the BP marginals over $C_{i}$ , and then compute the expectation. In practice, however, this approach can be highly problematic. As we have seen, BP often does not converge. The marginals that we derive from the algorithm therefore oscillate, and the ﬁnal results depend on the point at which we choose to stop the algorithm. As a result, the gradient computed from these expected counts is also unstable . This instability can be a signiﬁcant problem in a gradient-based procedure, since it can gravely hurt the convergence properties of the algorithm. This problem is even more severe in the context of line-search methods, where the function evaluations can be inconsistent at diferent points in the line search. 

unstable gradient 

There are several solutions to this problem: One can use one of the convergent alternatives to the BP algorithm that still optimizes the same Bethe energy objective; one can use a convex energy approximation, such as those of section 11.3.7.2; or, as we now show, one can reformulate the task of learning with approximate inference as optimizing an alternative objective, allowing the use of a range of optimization methods with better convergence properties. 

# 20.5.1.1 Pseudo-moment Matching 

Let us begin by a simple analysis of the ﬁxed points of the learning algorithm. At convergence, the approximate expectations must satisfy the condition of theorem 20.1; in particular, the converged BP beliefs for $C_{i}$ must satisfy 

$$
E_{\beta_{i}(C_{i})}[f_{C_{i}}]=E_{\cal D}[f_{i}(C_{i})].
$$ 

Now, let us consider the special case where our feature model deﬁnes a set of fully param- eterized potentials that precisely match the clusters used in the BP cluster graph. That is, for every cluster $C_{i}$ in the cluster graph, and every assignment $c_{i}^{j}$ to $C_{i},$ , we have a feature which is an indicator function $I\{c_{i}^{j}\}$ { } , that is, it is 1 when $C_{i}=c_{i}^{j}$ and 0 otherwise. In this case, the preceding set of equalities imply that, for every assignment $c_{i}^{j}$ to $C_{i}$ , we have that 

$$
\beta_{i}(c_{i}^{j})=\hat{P}(c_{i}^{j}).
$$ 

That is, at convergence of the gradient ascent algorithm, the convergence point of the underlying belief propagation must be to a set of beliefs that exactly matches the empirical marginals in the data. But if we already know the outcome of our convergence, there is no point to running the algorithm! 

This derivation gives us a closed form for the BP potentials at the point when both algorithms — BP inference and parameter gradient ascent — have converged. As we have already discussed, the full-table parameter iz ation of Markov network potentials is redundant, and therefore there are multiple solutions that can give rise to this set of beliefs. One of these solutions can be obtained by dividing each sepset in the calibrated cluster graph into one of the adjacent clique potentials. More precisely, for each sepset $\boldsymbol{S}_{i,j}$ between $C_{i}$ and $C_{j}$ , we select the endpoint for which $i<j$ (in some arbitrary ordering), and we then deﬁne: 

$$
\phi_{i}\leftarrow\frac{\beta_{i}}{\mu_{i,j}}.
$$ 

We perform this transformation for each sepset. We use the ﬁnal set of potentials as the parameter iz ation for our Markov network. We can show that a single pass of message passing in a particular order gives rise to a calibrated cluster graph whose potentials are precisely the ones in equation (20.13). Thus, in this particular special case, we can provide a closed-form solution to both the inference and learning problem. This approach is called pseudo-moment matching . 

While it is satisfying that we can ﬁnd a solution so efectively, the form of the solution should be considered with care. In particular, we note that the clique potentials are simply empirical cluster marginals divided by empirical sepset marginals. These quantities depend only on the local structure of the factor and not on any global aspect of the cluster graph, including its structure. For example, the $B C$ factor is estimated in exactly the same way within the diamond network of ﬁgure 11.1a and within the chain network $_{A-B-C-D}$ . Of course, potentials are also estimated locally in a Bayesian network, but there the local calibration ensures that the distribution can be factorized using purely local computations. As we have already seen, this is not the case for Markov networks, and so we expect diferent potentials to adjust to ﬁt each other; however, the estimation using loopy BP does not accommodate that. In a sense, this observation is not surprising, since the BP approach also ignores the more global information. 

We note, however, that this purely local estimation of the parameters only holds under the very restrictive conditions described earlier. It does not hold when we have parameter priors (regularization), general features rather than table factors, any type of shared parameters (as in section 6.5), or conditional random ﬁelds. We discuss this more general case in the next section. 

# 20.5.1.2 Belief Propagation and Entropy Approximations $\star$ 

We now provide a more general derivation that allows us to reformulate maximum-likelihood learning with belief propagation as a uniﬁed optimization problem with an approximate objec- tive. This perspective opens the door to the use of better approximation algorithms. 

maximum entropy 

Our analysis starts from the maximum-entropy dual of the maximum-likelihood problem. 

Maximum-Entropy : Find Q ( X ) maximizing $H_{Q}(\mathcal X)$ subject to $\pmb{E}_{Q}[f_{i}]=\pmb{E}_{\mathcal{D}}[f_{i}]\quad i=1,.\,.\,.\,,k.$ 

local consistency polytope factored entropy 

We can obtain a tractable approximation to this problem by applying the same sequence of transformations that we used in section 11.3.6 to derive belief propagation from the energy optimization problem. More precisely, assume we have a cluster graph $\mathcal{U}$ consisting of a set of clusters $\{C_{i}\}$ connecte by sepsets $\boldsymbol{S}_{i,j}$ . Now, rather than optimize Maximum-Entropy over the space of distributions Q , we optimize over the set of possible pseudo-marginals in the local consistency polytope Local [ U ] , as deﬁned in equation (11.16). Continuing as in the BP derivation, we also approximate the entropy as in its factored form (deﬁnition 11.1): 

$$
H_{Q}(\mathcal{X})\approx\sum_{\boldsymbol{C}_{i}\in\mathcal{U}}H_{\beta_{i}}(\boldsymbol{C}_{i})-\sum_{(\boldsymbol{C}_{i}-\boldsymbol{C}_{j})\in\mathcal{U}}H_{\mu_{i,j}}(\boldsymbol{S}_{i,j}).
$$ 

As before, this reformulation is exact when the cluster graph is a tree but is approximate otherwise. 

Putting these approximations together, we obtain the following approximation to the maximum- entropy optimization problem: 

Approx-Maximum-Entropy : Find $\begin{array}{r}{\underset{\sum_{\boldsymbol{C}_{i}\in\mathcal{U}}}{Q}H_{\beta_{i}}(\boldsymbol{C}_{i})-\sum_{(\boldsymbol{C}_{i}-\boldsymbol{C}_{j})\in\mathcal{U}}H_{\mu_{i,j}}(\boldsymbol{S}_{i,j})}\end{array}$ maximizing subject to $\begin{array}{r c l}{{{\pmb E}_{\beta_{i}}[f_{i}]}}&{{=}}&{{{\pmb E}_{\mathcal{D}}[f_{i}]\quad i=1,.\,.\,.\,,k}}\\ {{{\pmb Q}}}&{{\in}}&{{L o c a l[{\mathcal{U}}].}}\end{array}$ 

CAMEL This approach is called CAMEL , for constrained approximate maximum enropy learning. 

To illustrate this reformulation, consider a simple pairwise Markov network over the binary variables $A,B,C;$ , with three clusters: $C_{1}=\{A,B\},C_{2}=\{B,C\},C_{3}=\{A,C\}$ . We assume that the log-linear model is deﬁned by the following two features, both of which are shared over all clusters: $f_{00}(x,y)=1$ if $x\,=\,0$ and $y\,=\,0$ , and 0 otherwise; and $f_{11}(x,y)=1$ if $x=1$ and $y=1$ . Assume we have 3 data instances $[0,0,0]$ , $[0,1,0]$ , $[1,0,0]$ . The unnormalized empirical counts of each feature, pooled over all clusters, is then ${\pmb E}_{\hat{P}}[f_{00}]=(3+1+1)/3=5/3$ , $E_{\hat{P}}[f_{11}]=0$ . In this case, the optimization of equation (20.15) would take the following form: 

![](images/f54457592a0cb98f9a416ed691129fc0ee6e7ceeb2dddf8a568612630aedd583.jpg) 

The CAMEL optimization problem of equation (20.15) is a constrained maximization problem with linear constraints and a nonconcave objective. The problem actually has two distinct sets of constraints: the ﬁrst set encodes the moment-matching constraints and comes from the learning problem; and the second set encodes the constraint that $Q$ be in the marginal polytope and arises from the cluster-graph approximation. It thus forms a uniﬁed optimization problem that encompasses both the learning task — moment matching — and the inference task — obtaining a set of consistent pseudo-marginals over a cluster graph. Analogously, if we introduce Lagrange multipliers for these constraints (as in appendix A.5.3), they would have very diferent interpretations. The multipliers for the ﬁrst set of constraints would correspond to weights $\theta$ in the log-linear model, as in the max-likelihood $/$ max-entropy duality ; those in the second set would correspond to messages $\delta_{i\to j}$ in the cluster graph, as in the BP algorithm. 

This observation leads to several solution algorithms for this problem. In one class of methods, we could introduce Lagrange multipliers for all of the constraints and then optimize the resulting problem over these new variables. If we perform the optimization by a double-loop algorithm where the outer loop optimizes over $\theta$ (say using gradient ascent) and the inner loops “optimizes” the $\delta_{i\to j}$ by iterating their ﬁxed point equations, the result would be precisely gradient ascent over parameters with BP in the inner loop for inference. 

# 20.5.1.3 Sampling-Based Learning $\star$ 

The partition function $Z(\theta)$ is a summation over an exponentially large space. One approach to approximating this summation is to reformulate it as an expectation with respect to some distribution $Q(\mathcal X)$ : 

$$
\begin{array}{c c l}{{Z(\pmb\theta)}}&{{=}}&{{\displaystyle\sum_{\boldsymbol{\xi}}\exp\left\{\sum_{i}\theta_{i}f_{i}(\boldsymbol{\xi})\right\}}}\\ {{}}&{{=}}&{{\displaystyle\sum_{\boldsymbol{\xi}}\frac{Q(\boldsymbol{\xi})}{Q(\boldsymbol{\xi})}\exp\left\{\sum_{i}\theta_{i}f_{i}(\boldsymbol{\xi})\right\}}}\\ {{}}&{{=}}&{{\displaystyle{{\cal E}_{Q}\Bigg[\frac{1}{Q(\mathcal{X})}\exp\left\{\sum_{i}\theta_{i}f_{i}(\mathcal{X})\right\}\Bigg]}.}}\end{array}
$$ 

importance sampling 

This is precisely the form of the importance sampling estimator described in section 12.2.2. Thus, we can approximate it by generating samples from $Q$ , and correcting appropriately via weights. We can simplify this expression if we choose $Q$ to be $P_{\theta^{0}}$ for some set of parameters $\theta^{0}$ : 

$$
\begin{array}{r c l}{{Z(\pmb\theta)}}&{{=}}&{{{\cal E}_{P_{\theta^{0}}}\Bigg[\displaystyle\frac{Z(\pmb\theta^{0})\exp\left\{\sum_{i}\theta_{i}f_{i}(\mathcal X)\right\}}{\exp\left\{\sum_{i}\theta_{i}^{0}f_{i}(\mathcal X)\right\}}\Bigg]}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{Z(\pmb\theta^{0}){\cal E}_{P_{\theta^{0}}}\Bigg[\displaystyle\exp\left\{\sum_{i}(\theta_{i}-\theta_{i}^{0})f_{i}(\mathcal X)\right\}\Bigg].}}\end{array}
$$ 

If we can sample instances $\xi^{1},\cdot\cdot\cdot,\xi^{K}$ from $P_{\theta_{0}}$ , we can approximate the log-partition function as: 

$$
\ln Z(\pmb\theta)\approx\ln\left(\frac{1}{K}\sum_{k=1}^{K}\exp\left\{\sum_{i}(\theta_{i}-\theta_{i}^{0})f_{i}(\xi^{k})\right\}\right)+\ln Z(\pmb\theta^{0}).
$$ 

We can plug this approximation of $\ln Z(\theta)$ into the log-likelihood of equation (20.3) and op- timize it. Note that $\ln{Z(\theta^{0})}$ is a constant that we can ignore in the optimization, and the resulting expression is therefore a simple function of $\theta$ , which can be optimized using methods such as gradient ascent or one of its extensions. Interestingly, gradient ascent over $\theta$ relative to equation (20.16) is equivalent to utilizing an importance sampling estimator directly to approxi- mate the expected counts in the gradient of equation (20.4) (see exercise 20.12). However, as we discussed, it is generally more instructive and useful to view such methods as exactly optimizing an approximate objective rather than approximately optimizing the exact likelihood. 

Of course, as we discussed in section 12.2.2, the quality of an importance sampling estima- tor depends on the diference between $\theta$ and $\theta^{0}$ : the greater the diference, the larger the variance of the importance weights. Thus, this type of approximation is reasonable only in a neighborhood surrounding $\theta^{0}$ . 

MCMC 

How do we use this approximation? One possible strategy is to iterate between two steps. In one we run a sampling procedure, such as MCMC , to generate samples from the current parameter set $\theta^{t}$ . Then in the second iteration we use some gradient procedure to ﬁnd $\pmb{\theta}^{t+1}$ that improve the approximate log-likelihood based on these samples. We can then regenerate samples and repeat the process. As the samples are regenerated from a new distribution, we can hope that they are generated from a distribution not too far from the one we are currently optimizing, maintaining a reasonable approximation. 

# 20.5.2 MAP-Based Learning $\star$ 

MAP assignment As another approximation to the inference step in the learning algorithm, we can consider approximating the expected feature counts with their counts in the single MAP assignment to the current Markov network. As we discussed in chapter 13, in many classes of models, computing a single MAP assignment is a much easier computational task, making this a very appealing approach in many settings. 

More precisely, to approximate the gradient at a given parameter assignment $\theta$ , we compute 

$$
\pmb{E}_{\mathcal{D}}[f_{i}(\mathcal{X})]-f_{i}(\xi^{\mathrm{MAP}}(\pmb{\theta})),
$$ 

Viterbi training 

here $\begin{array}{r}{\xi^{\mathrm{MAP}}(\pmb{\theta})=\arg\operatorname*{max}_{\xi}P(\xi\mid\pmb{\theta})}\end{array}$ is the MAP assignment given the current set of parameters θ . This approach is also called Viterbi training . 

Once again, we can gain considerable intuition by reformulating this approximate inference step as an exact optimization of an approximate objective. Some straightforward algebra shows that this gradient corresponds exactly to the approximate objective 

$$
\frac{1}{M}\ell(\pmb\theta:\mathcal{D})-\ln P(\xi^{\mathrm{MAP}}(\pmb\theta)\mid\pmb\theta),
$$ 

or, due to the cancellation of the partition function: 

$$
\frac{1}{M}\sum_{m=1}^{M}\ln\tilde{P}(\xi[m]\mid\theta)-\ln\tilde{P}(\xi^{\mathrm{MAP}}(\theta)\mid\theta).
$$ 

To see this, consider a single data instance $\xi[m]$ : 

$$
\begin{array}{r l}{\ln P(\xi[m]\mid\pmb{\theta})-\ln P(\xi^{\mathrm{MAP}}(\pmb{\theta})\mid\pmb{\theta})}&{}\\ {=}&{[\ln\Tilde{P}(\xi[m]\mid\pmb{\theta})-\ln Z(\pmb{\theta})]-[\ln\Tilde{P}(\xi^{\mathrm{MAP}}(\pmb{\theta})\mid\pmb{\theta})-\ln Z(\pmb{\theta})]}\\ {=}&{\ln\Tilde{P}(\xi[m]\mid\pmb{\theta})-\ln\Tilde{P}(\xi^{\mathrm{MAP}}(\pmb{\theta})\mid\pmb{\theta})}\\ {=}&{\sum_{i}\theta_{i}[f_{i}(\xi[m])-f_{i}(\xi^{\mathrm{MAP}}(\pmb{\theta}))].}\end{array}
$$ 

If we average this expression over all data instances and take the partial derivative relative to $\theta_{i}$ , we obtain an expression whose gradient is precisely equation (20.17). 

The ﬁrst term in equation (20.19) is an average of expressions of the form $\ln{\tilde{P}}(\xi\mid\theta)$ | . Each such expression is a linear function in $\theta$ , and hence their average is also linear in θ . The second term, $\Tilde{P}(\xi^{\mathrm{MAP}}(\pmb{\theta})\mid\pmb{\theta})$ | , may appear to be the log robability of an instance. However, as indicated by the notation, $\xi^{\mathrm{MAP}}(\theta)$ is itself a function of θ : in diferent regions of the parameter space, the MAP assignment changes. In fact, this term is equal to: 

$$
\ln P(\xi^{\mathrm{MAP}}(\theta)\mid\theta)=\operatorname*{max}_{\xi}\ln P(\xi\mid\theta).
$$ 

This is a maximum of linear functions, which is a convex, piecewise-linear function. Therefore, its negation is concave, and so the entire objective of equation (20.19) is also concave and hence has a global optimum. 

Although reasonable at ﬁrst glance, a closer examination reveals some important issues with this objective. Consider again a single data instance $\xi[m]$ . Because $\xi^{\mathrm{MAP}}(\theta)$ is the MAP assignment, it follows that $\ln P(\xi[m]\mid\theta)\,\le\,\ln P(\xi^{\mathrm{MAP}}(\theta)\mid\theta)$ , and therefore the objective is always nonpositive. The maximal value of 0 can be achieved in two ways. The ﬁrst is if we manage to ﬁnd a setting of $\theta$ in which the empirical feature counts match the feature counts in $\xi^{\mathrm{MAP}}(\theta)$ . This optimum may be hard to achieve: Because the counts in $\xi^{\mathrm{MAP}}(\theta)$ are discrete, they take on only a ﬁnite set of values; for example, if we have a feature that is an indicator function for the event $X_{i}\,=\,x_{i}$ , its count can take on only the values 0 or 1 , depending on whether the MAP assignment has $X_{i}=x_{i}$ or not. Thus, we may never be able to match the feature counts exactly. The second way of achieving the optimal value of 0 is to set all of the parameters $\theta_{i}$ to 0 . In this case, we obtain the uniform distribution over assignments, and the objective achieves its maximum value of 0 . This possible behavior may not be obvious when we consider the gradient, but it becomes apparent when we consider the objective we are trying to optimize. 

That said, we note that in the early stages of the optimization, when the expected counts are far from the MAP counts, the gradient still makes progress in the general direction of increasing the relative log-probability of the data instances. This approach can therefore work fairly well in practice, especially if not optimized to convergence. 

Box 20.B — Case Study: CRFs for Protein Structure Prediction. One interesting application of CRFs is to the task of predicting the three-dimensional structure of proteins. Proteins are con- structed as chains of residues, each containing one of twenty possible amino acids. The amino acids are linked together into a common backbone structure onto which amino-speciﬁc side-chains are attached. An important computational problem is that of predicting the side-chain conformations given the backbone. The full conﬁguration for a side-chain consists of up to four angles, each of which takes on a continuous value. However, in practice, angles tend to cluster into bins of very similar angles, so that the common practice is to discretize the value space of each angle into a small number (usually up to three) bins, called rotamers . 

With this transformation, side-chain prediction can be formulated as a discrete optimization problem, where the objective is an energy over this discrete set of possible side-chain conformations. Several energy functions have been proposed, all of which include various repulsive and attractive terms between the side-chain angles of nearby residues, as well as terms that represent a prior and internal constraints within the side chain for an individual residue. Rosetta, a state-of-the- art system, uses a combination of eight energy terms, and uses simulated annealing to search for the minimal energy conﬁguration. However, even this highly engineered system still achieves only moderate accuracies (around 72 percent of the discretized angles predicted correctly). An obvious question is whether the errors are due to suboptimal answers returned by the optimization algorithm, or to the design of the energy function, which may not correctly capture the true energy “preferences” of protein structures. 

Yanover, Schueler-Furman, and Weiss (2007) propose to address this optimization problem using MAP inference techniques. The energy functions used in this type of model can also be viewed as the log-potentials of a Markov network, where the variables represent the diferent angles to be inferred, and their values the discretized rotamers. The problem of ﬁnding the optimal conﬁguration is then simply the MAP inference problem, and can be tackled using some of the algorithms described in chapter 13. Yanover et al. show that the TRW algorithm of box 13.A ﬁnds the provably global optimum of the Rosetta energy function for approximately 85 percent of the proteins in a standard benchmark set; this computation took only a few minutes per protein on a standard workstation. They also tackled the problem by directly solving the LP relaxation of the MAP problem using a commercial LP solver; this approach found the global optimum of the energy function for all proteins in the test set, but at a higher computational cost. However, ﬁnding the global minimum gave only negligible improvements on the actual accuracy of the predicted angles, suggesting that the primary source of inaccuracy in these models is in the energy function, not the optimization. 

Thus, this problem seems like a natural candidate for the application of learning methods. The task was encoded as a CRF, whose input is a list of amino acids that make up the protein as well as the three-dimensional shape of the backbone. Yanover et al. encoded this distribution as a log-linear model whose features were the (eight) diferent components of the Rosetta energy function, and whose parameters were the weights of these features. Because exact inference for this model is intractable, it was trained by using a TRW variant for sum-product algorithms (see section 11.3.7.2). This variant uses a set of convex counting numbers to provide a convex approximation, and a lower bound, to the log-partition function. These properties guarantee that the learning process is stable and is continually improving a lower bound on the true objective. This new energy function improves performance from 72 percent to 78 percent, demonstrating that learning can signiﬁcantly improve models, even those that are carefully engineered and optimized by a human expert. Notably, for the learned energy function, and for other (yet more sophisticated) energy functions, the use of globally optimal inference does lead to improvements in accuracy. Overall, a combination of these techniques gave rise to an accuracy of 82.6 percent, a signiﬁcant improvement. 

# 20.6 Alternative Objectives 

Another class of approximations can be obtained directly by replacing the objective that we aim to optimize with one that is more tractable. To motivate the alternative objectives we present in this chapter, let us consider again the form of the log-likelihood objective, focusing, for simplicity, on the case of a single data instance $\xi$ : 

$$
\begin{array}{r c l}{{\ell(\pmb\theta:\xi)}}&{{=}}&{{\ln\tilde{P}(\xi\mid\pmb\theta)-\ln Z(\pmb\theta)}}\\ {{}}&{{=}}&{{\ln\tilde{P}(\xi\mid\pmb\theta)-\ln\left(\displaystyle\sum_{\xi^{\prime}}\tilde{P}(\xi^{\prime}\mid\pmb\theta)\right).}}\end{array}
$$ 

Considering the ﬁrst term, this objective aims to increase the log-measure (logarithm of the unnormalized probability) of the observed data instance $\xi$ . Of course, because the log-measure is a linear function of the parameters in our log-linear representation, that goal can be achieved simply by increasing all of the parameters associated with positive empirical expectations in $\xi$ , and decreasing all of the parameters associated with negative empirical expectations. Indeed, we can increase the ﬁrst term unboundedly using this approach. The second term, however, balances the ﬁrst, since it is the logarithm of a sum of the unnormalized measures of instances, in this case, all possible instances in $V a l(\mathcal{X})$ . In a sense, then, we can view the log-likelihood objective as aiming to increasing the distance between the log-measure of $\xi$ and the aggregate of the measures of all instances. We can thus view it as contrasting two terms. The key difculty with this formulation, of course, is that the second term involves a summation over the exponentially many instances in $V a l(\mathcal{X})$ , and therefore requires inference in the network. 

contrastive objective 

This formulation does, however, suggest one approach to approximating this objective: perhaps we can still move our parameters in the right direction if we aim to increase the diference between the log-measure of the data instances and a more tractable set of other instances, one that does not require summation over an exponential space. The contrastive objectives that we describe in this section all take that form. 

# 20.6.1 Pseudo likelihood and Its Generalizations 

Perhaps the earliest method for circumventing the intractability of network inference is the pseudo likelihood objective. As one motivation for this approximation, consider the likelihood of a single instance $\xi$ . Using the chain rule, we can write 

$$
P(\xi)=\prod_{j=1}^{n}P(x_{j}\mid x_{1},\ldots,x_{j-1}).
$$ 

We can approximate this ormulation by replacing each term $P(x_{i}\mid\;x_{1},.\,.\,,x_{i-1})$ by the conditional probability of x i given all other variables: 

$$
P(\xi)\approx\prod_{j}P(x_{j}\mid x_{1},\ldots,x_{j-1},x_{j+1},\ldots,x_{n}).
$$ 

pseudolikelihood This approximation leads to the pseudolikelihood objective: 

$$
\ell_{\mathrm{PL}}(\pmb\theta:\mathcal{D})=\frac{1}{M}\sum_{m}\sum_{j}\ln P(x_{j}[m]\mid\pmb x_{-j}[m],\pmb\theta),
$$ 

multinomial logistic CPD where $\pmb{x}_{-j}$ stands for $x_{1},.\.\ ,x_{j-1},x_{j+1},.\ .\ .\ ,x_{n}$ . Intuitively, this objective measures our ability to predict each variable in the model given a full observation over all other variables. The predictive model takes a form that generalizes the multinomial logistic CPD of deﬁnition 5.10 and is identical to it in the case where the network contains only pairwise features — factors over edges in the network. As usual, we can use the conditional independence properties in the network to simplify this expressi removing from the right-hand side of $P(X_{j}\mid X_{-j})$ any variable that is not a neighbor of $X_{j}$ . 

At ﬁrst glance, this objective may appear to be more complex than the likelihood objective. However, a closer examination shows that we have eliminated the exponential summation over instances with several summations, each of which is far more tractable. In particular: 

$$
\begin{array}{r c l}{P(x_{j}\mid\pmb{x}_{-j})=\displaystyle\frac{P(x_{j},\pmb{x}_{-j})}{P(\pmb{x}_{-j})}}&{=}&{\displaystyle\frac{\tilde{P}(x_{j},\pmb{x}_{-j})}{\tilde{P}(\pmb{x}_{-j})}}\\ &{=}&{\displaystyle\frac{\tilde{P}(x_{j},\pmb{x}_{-j})}{\sum_{x_{j}^{\prime}}\tilde{P}(x_{j}^{\prime},\pmb{x}_{-j})}.}\end{array}
$$ 

The critical feature of this expression is that the global partition function has disappeared, and instead we have a local partition function that requires summing only over the values of $X_{j}$ . 

The contrastive perspective that we described earlier provides an alternative insight on this derivation. Consider the pseudo likelihood objective applied to a single data instance $\xi$ : 

$$
\begin{array}{r c l}{{\displaystyle\sum_{j}\ln P(x_{j}\mid\mathbf{x}_{-j})}}&{{=}}&{{\displaystyle\sum_{j}\left(\ln\tilde{P}(x_{j},\mathbf{x}_{-j})-\ln\sum_{x_{j}^{\prime}}\tilde{P}(x_{j}^{\prime},\mathbf{x}_{-j})\right)}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{\displaystyle\sum_{j}\left(\ln\tilde{P}(\xi)-\ln\sum_{x_{j}^{\prime}}\tilde{P}(x_{j}^{\prime},\mathbf{x}_{-j})\right).}}\end{array}
$$ 

Each of the terms in this ﬁnal summation is a contrastive term, where we aim to increase the diference between the log-measure of our training instance $\xi$ and an aggregate of the log- measures of instances that difer from $\xi$ in the assignment to precisely one variable. In other words, we are increasing the contrast between our training instance $\xi$ and the instances in a local neighborhood around it. 

We can further simplify each of the summands in this expression, obtaining: 

$$
\begin{array}{r l}&{\mathrm{ln}P(x_{j}\mid x_{-j})=}\\ &{\quad\left(\displaystyle\sum_{\substack{i\,:\,S c o p e[f_{i}]\,\ni\,X_{j}}}\theta_{i}f_{i}(x_{j},\mathbf{x}_{-j})\right)-\mathrm{ln}\left(\displaystyle\sum_{x_{j}^{\prime}}\exp\left\{\displaystyle\sum_{\substack{i\,:\,S c o p e[f_{i}]\,\ni\,X_{j}}}\theta_{i}f_{i}(x_{j}^{\prime},\mathbf{x}_{-j})\right\}\right)}\end{array}
$$ 

Each of these terms is precisely a log-conditional-likelihood term for a Markov network over a single variable $X_{j}$ , conditioned on all the remaining variables. Thus, it follows from corol- lary 20.2 that the function is concave in the parameters $\theta$ . Since a sum of concave functions is also concave, we have that the pseudo likelihood objective of equation (20.20) is concave. Thus, we are guaranteed that gradient ascent over this objective will converge to the global maximum. To compute the gradient, we use equation (20.21), to obtain: 

$$
\frac{\partial}{\partial\theta_{i}}\ln P(x_{j}\mid\mathbf{x}_{-j})=f_{i}(x_{j},\mathbf{x}_{-j})-\mathbf{E}_{x_{j}^{\prime}\sim P_{\theta}(X_{j}\mid\mathbf{x}_{-j})}\left[f_{i}(x_{j}^{\prime},\mathbf{x}_{-j})\right].
$$ 

If $X_{j}$ is not in the scope of $f_{i}$ , then $f_{i}(x_{j},\pmb{x}_{-j})=f_{i}(x_{j}^{\prime},\pmb{x}_{-j})$ for any $x_{j}^{\prime};$ , and the two terms are identical, making the derivative 0. Inserting this expression into equation (20.20), we obtain: 

Proposition 20.4 

$$
\frac{\partial}{\partial\theta_{i}}\ell_{\mathrm{PL}}(\pmb{\theta}:\mathcal{D})=\sum_{j:X_{j}\in S o p e[f_{i}]}\left(\frac{1}{M}\sum_{m}f_{i}(\xi[m])-\pmb{E}_{x_{j}^{\prime}\sim P_{\theta}(X_{j}|\pmb{x}_{-j}[m])}\left[f_{i}(x_{j}^{\prime},\pmb{x}_{-j}|\pmb{x}_{-i})-\pmb{E}_{x_{j}^{\prime}\sim P_{\theta}(X_{j}|\pmb{x}_{-j}[m])}\right]\right)
$$ 

While this term looks somewhat more involved than the gradient of the likelihood in equa- tion (20.4), it is much easier to compute: each of the expectation terms requires a summation over only a single random variable $X_{j}$ , conditioned on all of its neighbors, a computation that can generally be performed very efciently. 

What is the relationship between maximum likelihood estimation and maximum pseudo- likelihood? In one speciﬁc situation, the two estimators return the same set of parameters. 

Assume that our data are generated by a log-linear model $P_{\theta^{*}}$ that is of the form of equation (20.1). Then, as the number of data instances $M$ goes to inﬁnity, with probability that approaches 1, $\theta^{*}$ is a global optimum of the pseudo likelihood objective of equation (20.20). 

Proof To prove the result, we need to show that because the size of the data set tends to inﬁnity, the gradient of the pseudo likelihood objective at $\theta^{*}$ tends to zero. Owing to the concavity of the objective, this equality implies that $\theta^{*}$ is necessarily an optimum of the pseudo likelihood objective. We provide a somewhat informal sketch of the gradient argument, but one that contains all the essential ideas. 

Because $M\,\longrightarrow\,\infty$ →∞ , the empirical distribution $\hat{P}$ gets arbitrarily close to $P_{\theta^{*}}$ . Thus, the statistics in the data are precisely representative of their expectations relative to $P_{\theta^{*}}$ . Now, consider one of the summands in equation (20.23), associated with a feature $f_{i}$ . Due to the convergence of the sufcient statistics, 

$$
\frac{1}{M}\sum_{m}f_{i}(\xi[m])\longrightarrow E_{\xi\sim P_{\theta^{*}}(\mathcal{X})}[f_{i}(\xi)].
$$ 

Conversely, 

$$
\begin{array}{r l}{\lefteqn{\frac{1}{M}\sum_{m}E_{x_{j}^{\prime}\sim P_{\theta^{*}}(X_{j}\mid x_{-j}\mid m)}\left[f_{i}(x_{j}^{\prime},\mathbf{x}_{-j}[m])\right]}}\\ {=\;}&{\sum_{x_{-j}}P_{\mathcal{D}}(x_{-j})\sum_{x_{j}^{\prime}}P_{\theta^{*}}(x_{j}^{\prime}\mid x_{-j})f_{i}(x_{j}^{\prime},\mathbf{x}_{-j})}\\ {\longrightarrow}&{\sum_{x_{-j}}P_{\theta^{*}}(x_{-j})\sum_{x_{j}^{\prime}}P_{\theta^{*}}(x_{j}^{\prime}\mid x_{-j})f_{i}(x_{j}^{\prime},\mathbf{x}_{-j})}\\ {=\;}&{E_{\xi\sim P_{\theta^{*}}}[f_{i}(\xi)].}\end{array}
$$ 

Thus, at the limit, the empirical and expected counts are equal, so that the gradient is zero. 

consistent 

This theorem states that, like the likelihood objective, the pseudo likelihood objective is also consistent . If we assume that the models are nondegenerate so that the two objectives are strongly concave, the maxima are unique, and hence the two objectives have the same maxi- mum. 

While this result is an important one, it is important to be cognizant of its limitations. In particular, we note that the two assumptions are central to this argument. First, in order for the empirical and expected counts to match, the model being learned needs to be sufciently expressive to represent the generating distribution. Second, the data distribution needs to be close enough to the generating distribution to be well captured within the model, a situation that is only guaranteed to happen at the large-sample limit. Without these assumptions, the two objectives can have quite diferent optima that lead to diferent results. 

In practice, these assumptions rarely hold: our model is never a perfect representation of the true underlying distribution, and we often do not have enough data to be close to the large sample limit. Therefore, one must consider the question of how good this objective is in practice. The answer to this question depends partly on the types of queries for which we intend to use the model. If we plan to run queries where we condition on most of the variables and query the values of only a few, the pseudo likelihood objective is a very close match to the type of predictions we would like to make, and therefore pseudo likelihood may well provide a better training objective than likelihood. For example, if we are trying to learn a Markov network for collaborative ﬁltering (box 18.C), we generally take the user’s preference for all items except the query item to be observed. Conversely, if a typical query involves most or all of the variables in the model, the likelihood objective is more appropriate. For example, if we are trying to learn a model for image segmentation (box 4.B), the segment value of all of the pixels is unobserved. (We note that this last application is a CRF, where we would generally use a conditional likelihood objective, conditioned on the actual pixel values.) In this case, a (conditional) likelihood is a more appropriate objective than the (conditional) pseudo likelihood. 

However, even in cases where the likelihood is the more appropriate objective, we may have to resort to pseudo likelihood for computational reasons. In many cases, this objective performs surprisingly well. However, in others, it can provide a fairly poor approximation. 

Example 20.3 Consider a Markov network over three variables $X_{1},X_{2},Y$ , where each pair is connected by an edge. Assume that $X_{1},X_{2}$ are very highly correlated (almost identical) and both are somewhat (but not as strongly) correlated with $Y$ . In this case, the best predictor for $X_{1}$ is $X_{2}$ , and vice versa, so the pseudo likelihood objective is likely to overestimate signiﬁcantly the parameters on the $X_{1}{-}X_{2}$ , and almost entirely dismiss the $X_{1}{-}Y$ and $X_{2}{-}Y$ edges. The resulting model would be an excellent predictor for $X_{2}$ when $X_{1}$ is observed, but virtually useless when only $Y$ and not $X_{1}$ is observed. 

This example is typical of a general phenomenon: Pseudo likelihood, by assuming that each variable’s local neighborhood is fully observed, is less able to exploit information obtained from weaker or longer-range dependencies in the distribution. 

generalized pseudolikelihood 

This limitation also suggests a spectrum of approaches known as generalized pseudo likelihood , which can reduce the extent of this problem. In particular, in the objective of equation (20.20), rather than using a product of terms over individual variables, we can consider terms where the left-hand side consists of several variables, conditioned on the rest. More precisely, we can deﬁne a set of subsets of variables $\{X_{s}\ :\ s\in\mathcal{S}\}$ , and then deﬁne an objective: 

$$
\ell_{\mathrm{GPL}}(\pmb\theta:\mathcal D)=\frac{1}{M}\sum_{m}\sum_{s}\ln P(\pmb x_{s}[m]\mid\pmb x_{-s}[m],\pmb\theta),
$$ 

where $X_{-s}=\mathcal{X}-X_{s}$ . 

Clearly, there are many possible choices of subsets $\{{\cal X}_{s}\}$ . For diferent such choices, this expression generalizes several objectives: the likelihood, the pseudo likelihood, and even the conditional likelihood. When variables are together in the same subset $X_{s}$ , the relationship between them is subject (at least in part) to a likelihood-like objective, which tends to induce a more correct model of the joint distribution over them. However, as for the likelihood, this objective requires that we compute expected counts over the variables in each $X_{s}$ given an assignment to $X_{-s}$ . Thus, the choice of $X_{s}$ ofers a trade-of between “accuracy” and computational cost. One common choice of subsets is the set of all cliques in the Markov networks, which guarantees that the factor associated with each clique is optimized in at least one likelihood-like term in the objective. 

# 20.6.2 Contrastive Optimization Criteria 

As we discussed, both likelihood and pseudo likelihood can be viewed as attempting to increase the “log-probability gap” between the log-probability of the observed instances in $\mathcal{D}$ and the logarithm of the aggregate probability of a set of instances. Building on this perspective, one can construct a range of methods that aim to increase the log-probability gap between $\mathcal{D}$ and some other instances. The intuition is that, by driving the probability of the observed data higher relative to other instances, we are tuning our parameters to predict the data better. 

More precisely, consider again the case of a single training instance $\xi$ . We can deﬁne a “contrastive” objective where we aim to maximize the log-probability gap: 

$$
\left(\ln\tilde{P}(\xi\mid\theta)-\ln\tilde{P}(\xi^{\prime}\mid\theta)\right),
$$ 

where $\xi^{\prime}$ is some other instance, whose selection we discuss shortly. Importantly, this expression takes a very simple form: 

$$
\left(\ln\tilde{P}(\boldsymbol{\xi}\mid\boldsymbol{\theta})-\ln\tilde{P}(\boldsymbol{\xi}^{\prime}\mid\boldsymbol{\theta})\right)=\boldsymbol{\theta}^{T}[\boldsymbol{f}(\boldsymbol{\xi})-\boldsymbol{f}(\boldsymbol{\xi}^{\prime})].
$$ 

Note that, for a ﬁxed instantiation $\xi^{\prime}$ , this expression is a linear function of $\theta$ and hence is unbounded. Thus, in order for this type of function to provide a coherent optimization objective, the choice of $\xi^{\prime}$ will generally have to change throughout the course of the optimization. Even then, we must take care to prevent the parameters from growing unboundedly, an easy way of arbitrarily increasing the objective. 

One can construct many variants of this type of method. Here, we brieﬂy survey two that have been particularly useful in practice. 

# 20.6.2.1 Contrastive Divergence 

contrastive divergence One approach whose popularity has recently grown is the contrastive divergence method. In this method, we “contrast” our data instances $\mathcal{D}$ with a set of randomly perturbed “neighbors” $\mathcal{D}^{-}$ . In particular, we aim to maximize: 

$$
\ell_{\mathrm{CD}}(\pmb{\theta}:\mathcal{D}\|\mathcal{D}^{-})=\left[\pmb{E}_{\xi\sim\hat{P}_{\mathcal{D}}}\left[\ln\tilde{P}_{\pmb{\theta}}(\xi)\right]-\pmb{E}_{\xi\sim\hat{P}_{\mathcal{D}^{-}}}\left[\ln\tilde{P}_{\pmb{\theta}}(\xi)\right]\right],
$$ 

where $\hat{P}_{\mathcal{D}}$ and $\hat{P}_{\mathcal{D}^{-}}$ are the empirical distributions tive to $\mathcal{D}$ and $\mathcal{D}^{-}$ , respectively. 

As we discussed, the set of “contrasted” instan es D $\mathcal{D}^{-}$ will necessarily difer at diferent stages in the search. Given a current parameter iz ation θ , what is a good choice of instances to which e want to contrast our data instances $\mathcal{D}\mathfrak{L}$ One intuition is that e want to move our parameters $\theta$ in a direction that increases the probability of instances in D relative to “typical” instances in our current distribution; that is, we want to increase the probability gap between instances $\xi\in\mathcal{D}$ and instances $\xi$ sampled randomly from $P_{\theta}$ . Thus, we can generate a contrastive set $\mathcal{D}^{-}$ by sampling from $P_{\theta}$ , and then maximizing the objective in equation (20.26). 

How do we sample from $\mathcal{P}_{\theta}?$ As in section 12.3, we can run a Markov chain deﬁned by the Markov network $P_{\theta}$ , using, for example, Gibbs sampling, and initializing from the instances in $\mathcal{D}$ ; once the chain mixes, we can collect samples from the distribution $P_{\theta}$ . Unfortunately, sampling from the chain for long enough to achieve mixing usually takes far too long to be feasible as the inner loop of a learning algorithm. However, there is an alternative approach, which is both less expensive and more robust. Rather than run the chain deﬁned by $P_{\theta}$ to convergence, we initialize from the instances in $\mathcal{D}$ , and run the chain o for a few steps; we then use the instances generated by these short sampling runs to deﬁne $\mathcal{D}^{-}$ . 

Intuitively, this approach has signiﬁcant appeal: We want our model to give high probability to the instances in $\mathcal{D}$ ur current parameters, initialized at $\mathcal{D}$ , are causing us to move away from the instances in D . Thus, w want to move our parameters in a direc that increases the probability of the instances in D relative to the “perturbed” instances in D $\mathcal{D}^{-}$ . The gradient of this objective is also very intuitive, and easy to compute: 

$$
\frac{\partial}{\partial\theta_{i}}\ell_{\mathrm{CD}}({\pmb\theta}:\mathcal{D}\|\mathcal{D}^{-})={\pmb E}_{\hat{P}_{\mathcal{D}}}[f_{i}(\mathcal{X})]-{\pmb E}_{\hat{P}_{\mathcal{D}^{-}}}[f_{i}(\mathcal{X})].
$$ 

Note that, if we run the Markov chain to the limit, the sample $\mathcal{D}^{-}$ are generated from $P_{\theta}$ ; in this case, the second term in this diference converges to I $\pmb{E}_{P_{\theta}}[f_{i}]$ , which is precisely the second term in the gradient of the log-likelihood objective in equation (20.4). Thus, at the limit of the Markov chain, this learning procedure is equivalent (on expectation) to maximizing the log-likelihood objective. However, in practice, the approximation that we get by taking only a few steps in the Markov chain provides a good direction for the search, at far lower computational cost. In fact, empirically it appears that, because we are taking fewer sampling steps, there is less variance in our estimation of the gradient, leading to more robust convergence. 

# 20.6.2.2 Margin-Based Training $\star$ 

A very diferent intuition arises in settings where our goal is to use the learned network for predicting a MAP assignment. For example in our image segmentation application of box 4.B, we want to use the learned network to predict a single high-probability assignment to the pixels that will encode our ﬁnal segmentation output. This type of reasoning only arises in the context of conditional queries, since otherwise there is only a single MAP assignment (in the unconditioned network). Thus, we describe the objective in this section in the context of conditional Markov networks. 

Recall that, in this setting, our training set consists of a set of pairs $\mathcal{D}=\{(\pmb{y}[m],\pmb{x}[m])\}_{m=1}^{M}$ . Given an observation $\pmb{x}[m]$ , we would like our learned model to give the highest probability to $\pmb{y}[m]$ . In other words, we would like the probability $P_{\theta}(\pmb{y}[m]\mid\pmb{x}[m])$ to be higher than any other probability $P_{\theta}(\pmb{y}\mid\pmb{x}[m])$ for ${\pmb y}\neq{\pmb y}[m]$ . In fact, to increase our conﬁdence in this prediction, we would like to increase the log-probability gap as much as possible, by increasing: 

$$
\ln P_{\pmb\theta}(\pmb y[m]\mid\pmb x[m])-\biggl[\operatorname*{max}_{\pmb y\neq\pmb y[m]}\ln P_{\pmb\theta}(\pmb y\mid\pmb x[m])\biggr].
$$ 

This diference between the log-probability of the target assignment $\pmb{y}[m]$ and that of the “next best” assignment is called the margin . The higher the margin, the more conﬁdent the model is in selecting $\pmb{y}[m]$ . Roughly speaking, margin-based estimation methods usually aim to maximize the margin. 

One way of formulating this of max-margin objective as an optimization problem is as follows: 

Find $\gamma,\theta$ maximizing γ subject to 

$$
\ln P_{\theta}({\pmb y}[m]\mid{\pmb x}[m])-\ln P_{\theta}({\pmb y}\mid{\pmb x}[m])\quad\geq\quad\gamma\qquad\forall m,{\pmb y}\neq{\pmb y}[m].
$$ 

The objective here is to maximize a single parameter $\gamma$ , which encodes the worst-case margin over all data instances, by virtue of the constraints, which impose that the log-probability gap between $\pmb{y}[m]$ and any other assignment $_{_y}$ (given $\pmb{x}[m])$ is at least $\gamma$ . Importantly, due to equation (20.25), the ﬁrst set of constraints can be rewritten in a simple linear form: 

$$
{\pmb\theta}^{T}({\pmb f}({\pmb y}[m],{\pmb x}[m])-{\pmb f}({\pmb y},{\pmb x}[m]))\geq\gamma.
$$ 

With this reformulation of the constraints, it becomes clear that, if we ﬁnd any solution that achieves a positive margin, we can increase the margin unboundedly simply by multiplying all the parameters through by a positive constant factor. To make the objective coherent, we can bound the magnitude of the parameters by constraining their $L_{2}$ -norm: $||\pmb\theta||_{2}^{2}=\pmb\theta^{T}\pmb\theta=$ $\textstyle\sum_{i}\theta_{i}^{2}=1$ ; or, equivalently, we can decide on a ﬁxed margin and try to reduce the magnitude of the parameters as much as possible. With the latter approach, we obtain the following optimization problem: 

Simple-Max-Margin : Find θ minimizing $||\pmb{\theta}||_{2}^{2}$ subject to $\theta^{T}(\pmb{f}(\pmb{y}[m],\pmb{x}[m])-\pmb{f}(\pmb{y},\pmb{x}[m]))\geq1\qquad\forall m,\pmb{y}\neq\pmb{y}[m]$ 

quadratic program convex optimization 

constraint generation 

At some level, this objective is simple: it is a quadratic program (QP) with linear constraints, and hence is a convex problem that can be solved using a variety of convex optimization methods. However, a more careful examination reveals that the problem contains a constraint for every $m$ , and (more importantly) for every assig ment ${\pmb y}\,\neq\,{\pmb y}[m]$ . Thus, the number of constraints is exponential in the number of variables Y , generally an intractable number. 

However, these are not arbitrary constraints: the structure of the underlying Markov network is reﬂected in the form of the constraints, opening the way toward efcient solution algorithms. One simple approach uses constraint generation , a general-purpose method for solving optimiza- tion problems with a large number of constraints. Constraint generation is an iterative method, which repeatedly solves for $\theta$ , each time using a larger set of constraints. Assume we have some algorithm for performing constrained optimization. We initially run this algorithm using none of the margin constraints, and obtain the optimal solution $\theta^{0}$ . In most cases, this solution will not satisfy many of the margin constraints, and it is thus not a feasible solution to our original QP. We add one or more constraints that are violated by $\theta^{0}$ into a set of active constraints . We now repeat the constrained optimization process to obtain a new solution $\theta^{1}$ , which is guaranteed to satisfy the active constraints. We again examine the constraints, ﬁnd ones that are violated, and add them to our active constraints. This process repeats until no constraints are violated by our solution. Clearly, since we only add constraints, this procedure is guaranteed to terminate: eventually there will be no more constraints to add. Moreover, when it terminates, the solution is guaranteed to be optimal: At any iteration, the optimization procedure is solving a relaxed problem, whose value is at least as good as that of the fully constrained problem. If the optimal solution to this relaxed problem happens to satisfy all of the constraints, no better solution can be found to the fully constrained problem. 

This description leaves unanswered two important questions. First, how many constraints we will have to add before this process terminates? Fortunately, it can be shown that, under reasonable assumptions, at most a polynomial number of constraints will need to be added prior to termination. Second, how do we ﬁnd violated constraints without exhaustively enumerating and checking every one? As we now show, we can perform this computation by running MAP inference in the Markov network induced by our current parameter iz ation $\theta$ . To see how, recall that we either want to show that 

$$
\ln\tilde{P}(\pmb{y}[m],\pmb{x}[m])\geq\ln\tilde{P}(\pmb{y},\pmb{x}[m])+1
$$ 

for every $\textbf{\em y}\in\mathsf{\Gamma}V a l(Y)$ except $\pmb{y}[m]$ , or we want to ﬁnd an assignment $_{_y}$ that violates this inequality constraint. Let 

$$
{\pmb y}^{m a p}=\arg\operatorname*{max}_{{\pmb y}\neq{\pmb y}[m]}\Tilde{P}({\pmb y},{\pmb x}[m]).
$$ 

There are now two cases: If $\ln\tilde{P}(\pmb{y}[m],\pmb{x}[m])<\ln\tilde{P}(\pmb{y}^{m a p},\pmb{x}[m])+1$ , then this is a violated constraint, which can be added to our constraint set. Alternatively, if $\ln\tilde{P}({\pmb y}[m],{\pmb x}[m])\ >$ $\ln\tilde{P}({\pmb y}^{m a p},{\pmb x}[m])+1$ , then, due to the selection of $\pmb{y}^{m a p}$ , we are guaranteed that 

$$
\ln\tilde{P}(\pmb{y}[m],\pmb{x}[m])>\ln\tilde{P}(\pmb{y}^{m a p},\pmb{x}[m])+1\geq\ln\tilde{P}(\pmb{y},\pmb{x}[m])+1,
$$ 

for ery $\pmb{y}\neq\pmb{y}[m]$ . That is, in this second case, all of the exponentially many constraints for the m ’th data instance are guaranteed to be satisﬁed. As written, the task of ﬁnding $\pmb{y}^{m a p}$ is not a simple MAP computation, due to the constraint t ${\pmb y}^{m a p}\neq\pmb y[m]$ . However, this difculty arises only in the case where the MAP assignment is $\pmb{y}[m]$ , in which case we need only ﬁnd the second-best assignment. Fortunately, it is not difcult to adapt most MAP solution methods to the task of ﬁnding the second-best assignment (see, for example, exercise 13.5). 

The use of MAP rather than sum-product as the inference algorithm used in the inner loop of the learning algorithm can be of signiﬁcance. As we discussed, MAP inference admits the use of more efcient optimization algorithms that are not applicable to sum-product. In fact, as we discussed in section 13.6, there are even cases where sum-product is intractable, whereas MAP can be solved in polynomial time. 

However, the margin constraints we use here fail to address two important issues. First, we are not guaranteed that there exists a model that can correctly select $\pmb{y}[m]$ as the MAP assignment for every data instance $m$ : First, our training data may be noisy, in which case $\pmb{y}[m]$ may not be the actual desired assignment. More importantly, our model may not be expressive enough to always pick the desired target assignment (and the “simple” solution of increasing its expressive power may lead to overﬁtting). Because of the worst-case nature of our optimization objective, when we cannot achieve a positive margin for every data instance, there is no longer any incentive in getting a better margin for those instances where a positive margin can be achieved. Thus, the solution we obtain becomes meaningless. To address this problem, we must allow for instances to have a nonpositive margin and simply penalize such exceptions in the objective; the penalization takes the form of slack variables $\eta_{m}$ that measure the extent of the violation for the $m$ ’th data instances. This approach allows the optimization to trade of errors in the labels of a few instances for a better solution overall. 

A second, related problem arises from our requirement that our model achieve a uniform margin for all ${\pmb y}\,\neq\,{\pmb y}[m]$ . To see why t quirement can be problematic, consid ain our image segmentation problem. Here, $\pmb{x}[m]$ are features derived from the image, $\pmb{y}[m]$ is our “ground truth” segmentation, and other assignments $_{_y}$ are other candidate segmentations. Some of these candidate segmentations difer from $\pmb{y}[m]$ only in very limited ways (perhaps a few pixels are assigned a diferent label). In this case, we expect that a reasonable model $P_{\theta}$ will ascribe a probability to these “almost-correct” candidates that is very close to the probability of the ground truth. If so, it will be difcult to ﬁnd a good model that achieves a high margin. Again, due to the worst-case nature of the objective, this can lead to inferior models. We address this concern by allowing the required margin $\ln P(\pmb{y}[m]\mid\pmb{x}[m])-\ln P(\pmb{y}\mid\pmb{x}[m])$ to vary with the “distance” between $\pmb{y}[m]$ and $_{_y}$ , with assignments $_{_y}$ that are more similar to $\pmb{y}[m]$ requiring a smaller margin. In particular, using the ideas of the Hamming loss , we can deﬁne $\Delta_{m}(\pmb{y})$ to be the number of variables $Y_{i}\in Y$ such that $y_{i}\neq y_{i}[m]$ , and require that the margin increase linearly in this discrepancy. 

Putting these two modiﬁcations together, we obtain our ﬁnal optimization problem: 

Max-Margin : Find $\begin{array}{r l}&{\pmb{\theta}}\\ &{\|\pmb{\theta}\|_{2}^{2}+C\sum_{m}\eta_{m}}\\ &{}\\ &{\pmb{\theta}^{T}(\pmb{f}(\pmb{y}[m],\pmb{x}[m])-\pmb{f}(\pmb{y},\pmb{x}[m]))\ge\Delta_{m}(\pmb{y})-\eta_{m}\quad\forall m,\pmb{y}\neq\pmb{y}[m].}\end{array}$ maximizing subject to 

Here, $C$ is a constant that determines the balance between the two parts of the objective: how much we choose to penalize mistakes (negative margins) for some instances, versus achieving a higher margin overall. 

Fortunately, the same constraint generation approach that we discussed can also be applied in this case (see exercise 20.14). 

# 20.7 Structure Learning 

model selection We now move to the problem of model selection : learning a network structure from data. As usual, there are two types of solution to this problem: the constraint-based approaches, which search for a graph structure satisfying the independence assumptions that we observe in the empirical distribution; and the score-based approaches, which deﬁne an objective function for diferent models, and then search for a high-scoring model. 

From one perspective, the constraint-based approaches appear relatively more advantageous here than they did in the case of Bayesian network learning. First, the independencies associated with separation in a Markov network are much simpler than those associated with d-separation in a Bayesian network; therefore, the algorithms for inferring the structure are much simpler here. Second, recall that all of our scoring functions were based on the likelihood function; here, unlike in the case of Bayesian networks, even evaluating the likelihood function is a computationally expensive procedure, and often an intractable one. 

On the other side, the disadvantage of the constraint-based approaches remains: their lack of robustness to statistical noise in the empirical distribution, which can give rise to incorrect independence assumptions. We also note that the constraint based approaches produce only a structure, and not a fully speciﬁed model of a distribution. To obtain such a distribution, we need to perform parameter estimation, so that we eventually encounter the computational costs associated with the likelihood function. Finally, in the context of Markov network learning, it is not clear that learning the global independence structure is necessarily the appropriate problem. In the context of learning Bayesian networks we distinguished between learning the global structure (the directed graph) and local structure (the form of each CPD). In learning undirected models we can similarly consider both the problem of learning the undirected graph structure and the particular set of factors or features that represent the parameter iz ation of the graph. Here, however, it is quite common to ﬁnd distributions that have a compact factorization yet have a complex graph structure. One extreme example is the fully connected network with pairwise potentials. Thus, in many domains we want to learn the factorization of the joint distribution, which often cannot be deduced from the global independence assumptions. 

We will review both types of approach, but we will focus most of the discussion on score- based approaches, since these have received more attention. 

# 20.7.1 Structure Learning Using Independence Tests 

constraint-based structure learning independence tests 

We ﬁrst consider the idea of constraint-based structure learning . Recall that the structure of a Markov network speciﬁes a set of independence assertions. We now show how we can use independence tests to reconstruct the Markov network structure. For this discussion, assume that the generating distr tion $P^{*}$ is positive and can be represented as a Markov netw $\mathcal{H}^{*}$ that is a perfect map of $P^{*}$ . Thus, we want to perform a set of independence tests on P $P^{*}$ and recover ${\mathcal{H}}^{*}$ To make the problem tractable, we further assume that the degree of nodes in $\mathcal{H}^{*}$ is at most d $d^{*}$ ∗ . 

Recall that in section 4.3.2 we considered three set sets of independencies that characterize a Markov network: global independencies that include all consequences of separation in the graph; Markov independencies that describe the independence of each variable $X$ from the rest of the variables given its Markov blanket; and pairwise independencies that describe the independence of each nonadjacent pair of variables $X,Y$ given all other variables. We showed there that these three deﬁnitions are equivalent in positive distributions. 

local Markov independencies 

pairwise independencies 

Can we use any of these concepts to recover the structure of $\mathcal{H}^{\ast2}$ Intuitively, we would prefer to examine a smaller set of independencies, since they would require fewer independence tests. Thus, we should focus either on the local Markov independencies or pairwise independencies. Recall that local Markov independencies are of the form 

$(X\perp{\mathcal{X}}-\{X\}-{\mathrm{MB}}_{{\mathcal{H}}^{*}}(X)\mid{\mathrm{MB}}_{{\mathcal{H}}^{*}}(X))\quad\forall X$ and pairwise independencies are of the form $(X\bot Y\mid{\mathcal{X}}-\{X,Y\})\;\;\forall(X{-}Y)\not\in{\mathcal{H}}.$ 

Unfortunately, as written, neither of these sets of independencies can be checked tractably, since both involve the entire set of variables $\mathcal{X}$ and hence require measuring the probability of exponentially many events. The computational infeasibility of this requirement is obvious. But equally problematic are the statistical issues: these independence assertions are evaluated not on the true distribution, but on the empirical distribution. Independencies that involve many variables lead to fragmentation of the data, and are much harder to evaluate without error. To estimate the distribution sufciently well as to evaluate these independencies reliably, we would need exponentially many data points. 

Thus, we need to consider alternative sets of independencies that involve only smaller subsets of variables. Several such approaches have been proposed; we review only one, as an example. Consider the network $\mathcal{H}^{*}$ . Clearly, if $X$ and $Y$ are not neighbors in $\mathcal{H}^{*}$ , then they are se rated by the Markov blanket $\mathrm{MB}_{{\mathcal{H}}^{*}}(X)$ and also by $\mathrm{MB}_{{\mathcal{H}}^{*}}(Y)$ . Thus, we can ﬁnd a set Z with $|Z|\leq\operatorname*{min}(|\mathrm{MB}_{{\mathcal{H}}^{\ast}}(X)|,|\mathrm{MB}_{{\mathcal{H}}^{\ast}}(Y)|)$ so that $s e p_{\mathcal{H}^{\ast}}(X;Y\mid Z)$ holds. O he other hand, if $X$ d $Y$ are neighbors in H ${\mathcal{H}}^{*}$ , then we cannot ﬁnd such a set Z . Because H $\mathcal{H}^{*}$ is a perfect map of $P^{*}$ , we can show that 

$$
X{\mathrm{-}}Y\notin{\mathcal{H}}^{*}\;{\mathrm{~if~and~only~if~}}\;\exists Z,|z|\leq d^{*}\&P^{*}=(X\perp Y\;|\;Z).
$$ 

Thus, we can determine whether $X{-}Y$ is $\mathcal{H}^{*}$ using ${\textstyle\sum_{k=0}^{d^{*}}{\binom{n-2}{k}}}$ P    independence tests. Each of these independence tests involves only d $d^{*}+2$ variables, which, for low values of $d^{*}$ , can be tractable. We have already encountered this test in section 3.4.3.1, as part of our Bayesian network construction procedure. If fact, it is not hard to show that, given our assumptions and perfect independence tests, the Build-PMap-Skeleton procedure of algorithm 3.3 reconstructs the correct Markov structure $\mathcal{H}^{*}$ (exercise 20.15). 

This procedure uses a polynomial number of tests. Thus, the procedure runs in polynomial time. Moreover, if the probability of a false answer in any single independence test is at most ϵ , then the probability th n one of the independence tests fails is at most ${\textstyle\sum_{k=0}^{d^{*}}{\binom{n-2}{k}}}\epsilon$    . Therefore, for sufciently small ϵ , we can use this analysis to prove that we can reconstruct the correct network structure $\mathcal{H}^{*}$ with high probability. 

While this result is satisfying at some level, there are signiﬁcant limitations. First, the number of samples required to obtain correct answers for all of the independence tests can be very large in practice. Second, the correctness of the algorithm is based on several important assumptions: that there is a Markov network that is a perfect map of $P^{*}$ ; that this network has a bounded degree; and that we have enough data to obtain reliable answers to the independence tests. When these assumptions are violated, this algorithm can learn incorrect network structures. 

Example 20.4 Assume that the underlying distribution $P^{*}$ is a Bayesian network with a v-structure $X\rightarrow Z\leftarrow Y$ . We showed in section 3.4.3 that, assuming perfect independence tests, Build-PMap-Skeleton learns the skeleton of $\mathcal{G}^{\ast}$ . However, the Markov network ${\mathcal{H}}^{*}$ that is an I-map for $P^{*}$ is the moralized network, which contains, in addition to the skeleton edges, edges between parents of a joint child. These edges will not be learned correctly by this procedure. In particular, have at $(X\perp Y\mid\emptyset)$ holds, and so the algorithm will allow us to remove the edge between X and Y , even though it exists in the true network ${\mathcal{H}}^{*}$ . 

The failure in this example results from the fact that the distribution $P^{*}$ does not have a perfect map that is a Markov network. Because many real-life distributions do not have a perfect map that is a compact graph, the applicability of this approach can be limited. 

Moreover, as we discussed, this approach focuses solely on reconstructing the network struc- ture and does not attempt to learn the the structure of the factorization, or to estimate the parameters. In particular, we may not have enough data to reliably estimate parameters for the structure learned by this procedure, limiting its usability in practice. Nevertheless, as in the case of Bayesian network structure learning, constraint-based approaches can be a useful tool for obtaining qualitative insight into the global structure of the distribution, and as a starting point for the search in the score-based methods. 

# 20.7.2 Score-Based Learning: Hypothesis Spaces 

hypothesis space We now move to the score-based structure learning approach. As we discussed earlier, this approach formulates structure learning as an optimization problem: We deﬁne a hypothesis space consisting of a set of possible networks; we also deﬁne an objective function, which is used to score diferent candidate networks; and then we construct a search algorithm that attempts to identify a high-scoring network in the hypothesis space. We begin in this section by discussing the choice of hypothesis space for learning Markov networks. We discuss objective functions and the search strategy in subsequent sections. 

There are several ways of formulating the search space for Markov networks, which vary in terms of the granularity at which they consider the network parameter iz ation. At the coarsest- grained, we can pose the hypothesis space as the space of diferent structures of the Markov network itself and measure the model complexity in terms of the size of the cliques in the network. At the next level, we can consider parameter iz at ions at the level of the factor graph, and measure complexity in terms of the sizes of the factors in this graph. At the ﬁnest level of granularity, we can consider a search space at the level of individual features in a log-linear model, and measure sparsity at the level of features included in the model. 

The more ﬁne-grained our hypothesis space, the better it allows us to select a parameter iz ation that matches the properties of our distribution without overﬁtting. For example, the factor-graph approach allows us to distinguish between a single large factor over $k$ variables and a set of $\textstyle{\binom{k}{2}}$  pairwise factors over the same variables, requiring far fewer parameters. The feature-based approach also allows us to distinguish between a full factor over $k$ variables and a single log-linear feature over the same set of variables. 

Conversely, the ﬁner-grained spaces can obscure the connection to the network structure, in that sparsity in the space of features selected does not correspond directly to sparsity in the model structure. For example, introducing even a single feature $f(d)$ into the model has the structural efect of introducing edges between all of the variables in $^d$ . Thus, even models with a fairly small number of features can give rise to dense connectivity in the induced network. While this is not a problem from the statistical perspective of reliably estimating the model parameters from limited data, it can give rise to signiﬁcant problems from the perspective of performing inference in the model. Moreover, a ﬁner-grained hypothesis space also means that search algorithms take smaller steps in the space, potentially increasing the cost of our learning procedure. We will return to some of these issues. 

We focus our presentation on the formulation of the search space in terms of log-linear models. Here, we have a set of features $\Omega$ , which are those that can potentially have nonzero weight. Our task is to select a log-linear model structur $\mathcal{M}$ , which is deﬁned by some subset $\Phi[\mathcal{M}]\subseteq\Omega$ . Let $\Theta[\mathcal{M}]$ be the s rameterizations θ that are compatible with the model structure: that is, those where θ $\theta_{i}\,\neq\,0$ ̸ only if $f_{i}\ \in\ \Phi[{\mathcal{M}}]$ . A structure and a compatible parameter iz ation deﬁne a log-linear distribution via: 

$$
P(\mathcal{X}\mid\mathcal{M},\pmb\theta)=\frac{1}{Z}\exp\left\{\sum_{i\in\Phi[\mathcal{M}]}\theta_{i}f_{i}(\boldsymbol\xi)\right\}=\frac{1}{Z}\exp\left\{\pmb f^{T}\pmb\theta\right\},
$$ 

where, because of the compatibility of $\theta$ with $\mathcal{M}$ , a feature not in $\Phi[\mathcal{M}]$ does not inﬂuence in the ﬁnal vector product, since it is multiplied by a parameter that is 0 . 

bounded tree-width 

Regardless of the formulation chosen, we may sometimes wish to impose structural con- straints that restrict the set of graph structures that can be selected, in order to ensure that we learn a network with certain sparsity properties. In particular, one choice that has received some attention is to restrict the class of networks learned to those that have a certain bound on the tree-width . By placing a tight bound on the tree-width, we prevent an overly dense network from being selected, and thereby reduce the chance of overﬁtting. Moreover, because models of low tree-width allow exact inference to be performed efciently (to some extent), this restriction also allows the computational steps required for evaluating the objective during the search to be performed efciently. However, this approach also has limitations. First, it turns out to be non- trivial to implement, since computing the tree-width of a graph is itself an intractable problem (see theorem 9.7); even keeping the graph under the required width is not simple. Moreover, many of the distributions that arise in real-world applications cannot be well represented by networks of low tree-width. 

# 20.7.3 Objective Functions 

We now move to considering the objective function that we aim to optimize in the score-based approach. We note that our discussion in this section uses the likelihood function as the basis for the objectives we consider; however, we can also consider similar objectives based on various approximations to the likelihood (see section 20.6); most notably, the pseudo likelihood has been used efectively as a substitute for the likelihood in the context of structure learning, and most of our discussion carries over without change to that setting. 

# 20.7.3.1 Likelihood Function 

The most straightforward objective function is the likelihood of the training data. As before, we take the score to be the log-likelihood, deﬁning: 

$$
\mathrm{score}_{L}\big(\mathcal{M}\ :\ \mathcal{D}\big)=\operatorname*{max}_{\theta\in\Theta[\mathcal{M}]}\ln P\big(\mathcal{D}\mid\mathcal{M},\theta\big)=\ell\big(\langle\mathcal{M},\hat{\theta}_{\mathcal{M}}\rangle:\mathcal{D}\big),
$$ 

where $\widehat{\pmb{\theta}}_{\mathcal{M}}$ are the maximum likelihood parameters compatible with $\mathcal{M}$ . 

The likelihood score measures the ﬁtness of the model to the data. However, for the same reason discussed in chapter 18, it prefers more complex models. In particular, if $\Phi[\mathcal{M}_{1}]\subset$ $\Phi[\mathcal{M}_{2}]$ then $\mathrm{score}_{L}(\mathcal{M}_{1}\ :\ \mathcal{D})\leq\mathrm{score}_{L}(\mathcal{M}_{2}\ :\ \mathcal{D})$ . Typically, this inequality is strict, due to the ability of the richer model to capture noise in the data. 

Therefore, the likelihood score can be used only with very strict constraints on the expressive model of the model class that we are considering. Examples include bounds on the structure of the Markov network (for example, networks with low tree-width) or on the number of features used. A second option, which also provides some regularization of parameter values, is to use an alternative objective that penalizes the likelihood in order to avoid overﬁtting. 

# 20.7.3.2 Bayesian Scores 

Bayesian score 

BIC score 

Recall that, for Bayesian networks, we used a Bayesian score , whose primary term is a marginal likelihood that integrates the likelihood over all possible network parameter iz at ions: $\textstyle\int P(\mathcal{D}\mid$ D | ${\mathcal{M}},{\boldsymbol{\theta}})P({\boldsymbol{\theta}}\mid{\mathcal{M}})d{\boldsymbol{\theta}}$ . This score accounts for our uncertainty over parameters using a Bayesian prior; it avoided overﬁtting by preventing overly optimistic assessments of the model ﬁt to the training data. In the case of Bayesian networks, we could efciently evaluate the marginal likelihood. In contrast, in the case of undirected models, this quantity is difcult to evaluate, even using approximate inference methods. 

Instead, we can use asymptotic approximations of the marginal likelihood. The simplest approximation is the BIC score : 

$$
\mathrm{score}_{B I C}(\mathcal{M}\ :\ \mathcal{D})=\ell(\langle\mathcal{M},\hat{\theta}_{\mathcal{M}}\rangle:\mathcal{D})-\frac{\dim(\mathcal{M})}{2}\ln M,
$$ 

model dimension 

Laplace approximation 

where $\mathrm{dim}(\mathcal{M})$ is the dimension of the model and $M$ the number of instances in $\mathcal{D}$ . This quantity measures the degrees of freedom of our parameter space. When the model has nonredundant features, $\mathrm{dim}(\mathcal{M})$ is exactly the number of features. When there is redundancy, the dimension is smaller than the number of features. Formally, it is the rank of the matrix whose rows are complete assignments $\xi_{i}$ to $\mathcal{X}$ , whose columns are features $f_{j}$ , and whose entries are $f_{j}(\xi_{i})$ . This matrix, however, is exponential in the number of variables, and therefore its rank cannot be computed efciently. Nonetheless, we can often estimate the number of nonredundant parameters in the model. As a very coarse upper bound, we note that the number of nonredundant features is always upper-bounded by the size of the full table representation of the Markov network, which is the total number of entries in the factors. 

The BIC approximation penalizes each degree of freedom (that is, free parameter) by a ﬁxed amount, which may not be the most appropriate penalty. Several more reﬁned alternatives have been proposed. One common choice is the Laplace approximation , which provides a more explicit approximation to the marginal likelihood: 

$$
\mathrm{core}_{L a p l a c e}(\mathcal{M}\ :\ \mathcal{D})=\ell(\langle\mathcal{M},\tilde{\theta}_{\mathcal{M}}\rangle:\mathcal{D})+\ln P(\tilde{\theta}_{\mathcal{M}}\ |\ \mathcal{M})+\frac{\mathrm{dim}(\mathcal{M})}{2}\ln(2\pi)-\mathrm{~i~n~}\langle\tilde{\theta}_{\mathcal{M}},\tilde{\theta}_{\mathcal{M}}\rangle
$$ 

MAP estimation 

Hessian 

where $\tilde{\pmb{\theta}}_{\mathcal{M}}$ are the parameters for $\mathcal{M}$ obtained from MAP estimation : 

$$
\tilde{{\boldsymbol{\theta}}}_{\mathcal{M}}=\arg\operatorname*{max}_{{\boldsymbol{\theta}}}P(\mathcal{D}\mid{\boldsymbol{\theta}},\mathcal{M})P({\boldsymbol{\theta}}\mid\mathcal{M}),
$$ 

and $A$ is the negative Hessian matrix: 

$$
A_{i,j}=-\frac{\partial}{\partial\theta_{i}\partial\theta_{j}}\left(\boldsymbol{\ell}(\left<\mathcal{M},\boldsymbol{\theta}\right>:\mathcal{D})+\ln P(\boldsymbol{\theta}\mid\mathcal{M})\right),
$$ 

evaluated at the point $\tilde{\pmb{\theta}}_{\mathcal{M}}$ . 

As we discussed in section 19.4.1.1, the Laplace score also takes into account the local shape of the posterior distribution around the MAP parameters. It therefore provides a better approxi- mation than the BIC score. However, as we saw in equation (20.5), to compute the Hessian, we need to evaluate the pairwise covariance of every feature pair given the model, a computation that may be intractable in many cases. 

# 20.7.3.3 Parameter Penalty Scores 

An alternative to approximations of the marginal likelihood are methods that simply evaluate the maximum posterior probability 

$$
\mathrm{score}_{M A P}(\mathcal{M}\ :\ \mathcal{D})=\operatorname*{max}_{\theta\in\Theta[\mathcal{M}]}\ell(\langle\mathcal{M},\tilde{\theta}_{\mathcal{M}}\rangle:\mathcal{D})+\ln P(\tilde{\theta}_{\mathcal{M}}\mid\mathcal{M}),
$$ 

MAP score where $\tilde{\pmb{\theta}}_{\mathcal{M}}$ M are the MAP parameters for $\mathcal{M}$ , as deﬁned in equation (20.28). One intuition for this type of MAP score is that the prior “regularizes” the likelihood, moving it away from the maximum likelihood values. If the likelihood of these parameters is still high, it implies that the model is not too sensitive to particular choice of maximum likelihood parameters, and thus it is more likely to generalize. 

Although the regularized parameters may achieve generalization, this approach achieves model selection only for certain types of prior. To understand why, note that the MAP score is based on a distribution not over structures, but over parameters. We can view any parameter iz ation $\theta_{\mathcal{M}}$ as a parameter iz ation to the “universal” model deﬁned over our entire set of features $\Omega$ : one where features not in $\Phi[\mathcal{M}]$ receive weight 0 . Assuming that our parameter prior simply ignores zero weights, we can view our score as simply evaluating diferent choices of parameter iz at ions $\theta_{\Omega}$ to this universal model. 

$L_{2}$ -regularization 

$L_{1}$ -regularization 

$L_{1}$ -MAP score 

block- $L_{1}$ - regularization 

We have already discussed several parameter priors and their efect on the learned parameters. Most parameter priors are associated with the magnitude of the parameters, rather than the complexity of the graph as a discrete data structure. In particular, as we discussed, although $L_{2}$ -regularization will tend to drive the parameters toward zero, few will actually hit zero, and so structural sparsity will not be achieved. Thus, like the likelihood score, the $L_{2}$ -regularized MAP objective will generally give rise to a fully connected structure. Therefore, this approach is generally not used in the context of model selection (at least not in isolation). 

A more appropriate approach for this task is $L_{1}$ -regularization , which does have the efect of driving model parameters toward zero, and thus can give rise to a sparse set of features. In other words, the structure that optimizes the $L_{1}$ -MAP score is not, in general, the universal structure $\Omega$ . Indeed, as we will discuss, an $L_{1}$ prior has other useful properties when used as the basis for a structure selection objective. 

However, as we have discussed, feature-level sparsity does not necessarily induce sparsity in the network. An alternative that does tend to have this property is the block- $\cdot L_{1}$ -regularization . Here, we partition all the pa meters into groups $\theta_{i}=\{\theta_{i,1},.\,.\,.\,,\theta_{i,k_{i}}\}$ (for $i=1,\dots,l)$ . We now deﬁne a variant of the $L_{1}$ penalty that tends to make each parameter group either go to zero together, or not: 

$$
-\sum_{i=1}^{l}\left\lvert\sqrt{\sum_{j=1}^{k_{i}}\theta_{i,j}^{2}}\right\rvert.
$$ 

To understand the behavior of this penalty term, let us consider its derivative for the simple case where we have two parameters in the same group, so that our expression takes the form $\sqrt{\theta_{1}^{2}+\theta_{2}^{2}}$ . We now have that: 

$$
\frac{\partial}{\partial\theta_{1}}\left[-\sqrt{\theta_{1}^{2}+\theta_{2}^{2}}\right]=-\frac{\theta_{1}}{\sqrt{\theta_{1}^{2}+\theta_{2}^{2}}}.
$$ 

We therefore see that, when $\theta_{2}$ is large, the derivative relative to $\theta_{1}$ is fairly small, so that there is no pressure on $\theta_{1}$ to go to 0 . Conversely, when $\theta_{2}$ is small, the derivative relative to $\theta_{1}$ tends to $-1$ , which essentially gives the same behavior as $L_{1}$ regularization. Thus, this prior tends to have the following behavior: if the overall magnitude of the parameters in the group is small, all of them will be forced toward zero; if the overall magnitude is large, there is little downward pressure on any of them. 

In our setting, we can naturally apply this prior to give rise to sparsity in network structure. Assume that we are willing to consider, within our network, factors over scopes $Y_{1},\dots,Y_{l}$ . For each $Y_{i}$ , let $f_{i,j}$ , for $j=1,\ldots,k_{i}$ , be all of the features whose scope is $Y_{i}$ . We now deﬁne a block- $\cdot L_{1}$ prior where we have a block for each set of parameters $\theta_{i1},\ldots,\theta_{i k_{i}}$ . The result of this prior would be to select together nonzero parameters for an entire set of features associated with a particular scope. 

Finally, we note that one can also use multiple penalty terms on the likelihood function. For example, a combination of a parameter penalty and a structure penalty can often provide both regularization of the parameters and a greater bias toward sparse structures. 

# 20.7.4 Optimization Task 

Having selected an objective function for our model structure, it remains to address the opti- mization problem. 

# 20.7.4.1 Greedy Structure Search 

As in the approach used for structure learning of general Bayesian networks (section 18.4.3), the external search over structures is generally implemented by a form of local search . Indeed, the general form of the algorithms in appendix A.4.2 applies to our feature-based view of Markov network learning. The general template is shown in algorithm 20.1. Roughly speaking, the algorithm maintains a current structure, deﬁned in terms of a set of features $\mathcal{F}$ in our log-linear model. At each point in the search, the algorithm optimizes the model parameters relative to the current feature set and the structure score. Using the current structure and parameters, it estimates the improvement of diferent structure modiﬁcation steps. It then selects some subset of modiﬁcations to implement, and returns to the parameter optimization step, initializing from the current parameter setting. This process is repeated until a termination condition is reached. This general template can be instantiated in many ways, including the use of diferent hypothesis spaces (as in section 20.7.2) and diferent scoring functions (as described in section 20.7.3). 

# 20.7.4.2 Successor Evaluation 

Although this approach is straightforward at a high level, there are signiﬁcant issues with its implementation. Importantly, the reasons that made this approach efcient in the context of 

![](images/69282ea6807f32238790f750141e65609575287f303ab334bc89dbe4c5645911.jpg) 

Bayesian networks do not apply here. In the case of Bayesian networks, evaluating the score of a candidate structure is a very easy task, which can be executed in closed form, at very low computation cost. Moreover, the Bayesian network score satisﬁes an important property: it decomposes according to the structure of the network. As we discussed, this property has two major implications. First, a local modiﬁcation to the structure involves changing only a single term in the score (proposition 18.5); second, the change in score incurred by a particular change (for example, adding an edge) remains unchanged after modiﬁcations to other parts of the network (proposition 18.6). These properties allowed us to design efcient search procedure that does not need to reevaluate all possible candidates after every step, and that can cache intermediate computations to evaluate candidates in the search space quickly. 

Unfortunately, none of these properties hold for Markov networks. For concreteness, consider the likelihood score, which is comparable across both network classes. First, as we discussed, even computing the likelihood of a fully speciﬁed model — structure as well as parameters — requires that we run inference for every instance in our training set. Second, to score a structure, we need to estimate the parameters for it, a problem for which there is no closed-form solution. Finally, none of the decomposition properties hold in the case of undirected models. By adding a new feature (or a set of features, for example, a factor), we change the weight $(\sum_{i}\theta_{i}f_{i}(\boldsymbol{\xi}))$ associated with diferent instances. This change can be decomposed, since it is a linear function of the diferent features. However, this change also afects the partition function, and, as we saw in the context of parameter estimation, the partition function couples the efects of changes in one parameter on the other. We can clearly see this phenomenon in ﬁgure 20.1, where the efect on the likelihood of modifying $f_{1}(b^{1},c^{1})$ clearly depends on the current value of the parameter for $f_{2}(a^{1},b^{1})$ . 

As a consequence, a local search procedure is considerably more expensive in the context of Markov networks. At each stage of the search, we need to evaluate the score for all of the candidates we wish to examine at that point in the search. This evaluation requires that we estimate the parameters for the structure, a process that itself requires multiple iterations of a numerical optimization algorithm, each involving inference over all of the instances in our training set. We can reduce somewhat the computational cost of the algorithm by using the observation that a single change to the structure of the network often does not result in drastic changes to the model. Thus, if we begin our optimization process from the current set of parameters, a reasonably small number of iterations often sufces to achieve convergence to the new set of parameters. Importantly, because all of the parameter objectives described are convex (when we have fully observable data), the initialization has no efect, and convergence to the global optimum remains guaranteed. Thus, this approach simply provides a way of speeding up the convergence to the optimal answer. (We note, however, that this statement holds only when we use exact inference; the choice of initialization can afect the accuracy of some approximate inference algorithms, and therefore the answers that we get.) 

Unfortunately, although this observation does reduce the cost, the number of candidate hy- potheses at each step is generally quite large. The cost of running inference on each of the candidate successors is prohibitive, especially in cases where, to ﬁt our target dis- tribution well, we need to consider nontrivial structures. Thus, much of the work on the problem of structure learning for Markov networks has been devoted to reducing the computational cost of evaluating the score of diferent candidates during the search. In particular, when evaluating diferent structure-modiﬁcation operators in line 11, most algorithms use some heuristic to rank diferent candidates, rather than computing the exact delta-score of each operator. These heuristic estimates can be used either as the basis for the ﬁnal selection, or as a way of pruning the set of possible successors, where the high-ranking candidates are then evaluated exactly. This design decision is a trade-of between the quality of our operator selection and the computational cost. 

Even with the use of heuristics, the cost of taking a step in the search can be prohibitive, since it requires a reestimation of the network parameters and a reevaluation of the (approximate) delta-score for all of the operators. This suggests that it may be beneﬁcial to select, at each structure modiﬁcation step (line 12), not a single operator but a subset $\mathcal{O}$ of operators. This approach can greatly reduce the computational cost, but at a cost: our (heuristic) estimate of each operator can deteriorate signiﬁcantly if we fail to take into account interactions between the efects of diferent operators. Again, this is a trade-of between the quality and cost of the operator selection. 

As we mentioned, the rough template of algorithm 20.1 can be applied to any objective function. However, the choice of objective function has signiﬁcant implications on our ability to efectively optimize it. Let us consider several of the choices discussed earlier. 

We ﬁrst recall that both the log-likelihood objective and the $L_{2}$ -regularized log-likelihood generally give nonzero values to all parameters. In other words, if we allow the model to consider a set of atures $\mathcal{F}$ , an optimal model (maximum-likelihood or maximum $L_{2}$ -regularized likelihood) over F will give a nonzero value to the parameters for all of these features. In other words, we cannot rely on these objectives to induce sparsity in the model structure. Thus, if we simply want to optimize these objectives, we should simply choose the richest model available in our hypothesis space and then optimize its parameters relative to the chosen objective. 

One approach for deriving more compact models is to restrict the class of models to ones with a certain bound on the complexity (for example, networks of bounded tree-width, or with a bound on the number of edges or features allowed). However, these constraints generally introduce nontrivial combinatorial trade-ofs between features, giving rise to a search space with multiple local optima, and making it generally intractable to ﬁnd a globally optimal solution. A second approach is simply to halt the search when the improvement in score (or an approxi- mation to it) obtained by a single step does not exceed a certain threshold. This heuristic is not unreasonable, since good features are generally introduced earlier, and so there is a general trend of diminishing returns. However, there is no guarantee that the solution we obtain is even close to the optimum, since there is no bound on how much the score would improve if we continue to optimize beyond the current step. 

Scoring functions that explicitly penalize structure complexity — such as the BIC score or Laplace approximation — also avoid this degeneracy. Here, as in the case of Bayesian networks, we can consider a large hypothesis space and attempt to ﬁnd the model in this space that optimizes the score. However, due to the discrete nature of the structure penalty, the score is discontinuous and therefore nonconcave. Thus, there is generally no guarantee of convergence to the global optimum. Of course, this limitation was also the case when learning Bayesian networks; as there, it can be somewhat alleviated by methods that avoid local maxima (such as tabu search, random restarts, or data perturbation). 

However, in the case of Markov networks, we have another solution available to us, one that avoids the prospect of combinatorial search spaces and the ensuing problem of local optima. This solution is the use of $L_{1}$ -regularized likelihood. As we discussed, the $L_{1}$ -regularized likelihood is a concave function that has a unique global optimum. Moreover, this objective function naturally gives rise to sparse models, in that, at the optimum, many parameters have value 0 , corresponding to the elimination of features from the model. We discuss this approach in more detail in the next section. 

# 20.7.4.4 $L_{1}$ -Regularization for Structure Learning 

Recall that the $L_{1}$ -regularized likelihood is simply the instantiation of equation (20.29) to the case of an $L_{1}$ -prior: 

$$
\begin{array}{r}{\mathrm{score}_{L_{1}}(\pmb\theta\ :\ \mathcal{D})=\ell(\langle\mathcal{M},\pmb\theta\rangle:\mathcal{D})-\|\pmb\theta\|_{1}.}\end{array}
$$ 

Somewhat surprisingly, the $L_{1}$ -regularized likelihood can be optimized in a way that guar- antees convergence to the globally optimal solution. To understand why, recall that the task of optimizing the $L_{1}$ -regularized log-likelihood is a convex optimization problem that has no local optima. Indeed, in theory, we can entirely avoid the combinatorial search component when using this objective. We can simply introduce all of the possible features into the model and optimize the resulting parameter vector $\theta$ relative to our objective. The sparsifying efect of the $L_{1}$ penalty will drive some of the parameters to zero. The parameters that, at convergence, have zero values correspond to features that are absent from the log-linear model. In this approach, we are efectively making a structure selection decision as part of our parameter optimization procedure. Although appealing, this approach is not generally feasible. In most cases, the num- ber of potential features we may consider for inclusion in the model is quite large. Including all of them in the model simultaneously gives rise to an intractable structure for the inference that we use as part of the computation of the gradient. 

Therefore, even in the context of the $L_{1}$ -regularized likelihood, we generally implement the optimization as a double-loop algorithm where we separately consider the structure and param- eters. However, there are several beneﬁts to the $L_{1}$ -regularized objective: 

• We do not need to consider feature deletion steps in our combinatorial search.

 • We can consider feature introduction steps in any (reasonable) order, and yet achieve conver- gence to the global optimum.

 • We have a simple and efcient test for determining convergence.

 • We can prove a PAC-learnability generalization bound for this type of learning. 

We now discuss each of these points. 

For the purpose of this discussion, assume that we currently have a model over a set of features $\mathcal{F}$ , and assume that $\theta^{l}$ optimizes our $L_{1}$ -regularized objective, subject to the constraint that $\theta_{k}^{l}$ can be nonzero only if $f_{k}\in\mathcal{F}$ . convergence poin ny feature deletion step cannot improve the score: Consider any $f_{k}\,\in\,\mathcal{F}$ ∈F ; the case where $f_{k}$ is de s already in the class of models that was considered when we optimized the choice of $\theta^{l}\mathrm{~-~}$ — it is simply the model where $\theta_{k}^{l}\,=\,0$ . Indeed, the algorithm already discards features whose parameter was zeroed by the continuous optimization procedure (line 7 of algorithm 20.1). If our current optimized model $\theta^{l}$ has $\theta_{k}^{l}\neq0$ , it follows that setting $\theta_{k}$ to 0 is suboptimal, and so deleting $f_{k}$ can only reduce the score. Thus, there is no value to considering discrete feature deletion steps: features that should be deleted will have their parameters set to 0 by the continuous optimization procedure. We note that this property also holds, in principle, for other smooth objectives, such as the likelihood or the $L_{2}$ -regularized likelihood; the diference is that for those objectives, parameters will generally not be set to 0 , whereas the $L_{1}$ objective does tend to induce sparsity. 

The second beneﬁt arises directly from the fact that optimizing the $L_{1}$ -regularized objective is a convex optimization problem. In such problems, any sequence of steps that continues to improve the objective (when possible) is guaranteed to converge to the global optimum. The restriction imposed by the set $\mathcal{F}$ induces a coordinate ascent approach: at each step, we are optimizing only the features in F , leaving at 0 those parameters $\theta_{k}$ for $f_{k}\notin\mathcal{F}$ . As long as each step continues to improve the objective, we are making progress toward the global optimum. At each point in the search, we consider the steps that we can take. If some step leads to an improvement in the score, we can take that step and continue with our search. If none of the steps lead to an improvement in the score, we are guaranteed that we have reached convergence to the global optimum. Thus, the decision on which operators to consider at each point in the algorithm (line 12 of algorithm 20.1) is not relevant to the convergence of the 

# algorithm to the true global optimum: As long as we repeatedly consider each operator until convergence, we are guaranteed that the global optimum is reached regardless of the order in which the operators are applied. 

While this guarantee is an important one, we should interpret it with care. First, when we add features to the model, the underlying network becomes more complex, raising the cost of infer- ence. Because inference is executed many times during the algorithm, adding many irrelevant features, even if they were eventually eliminated, can greatly degrade the computational perfor- mance time of the algorithm. Even more problematic is the efect when we utilize approximate inference, as is often the case. As we discussed, for many approximate inference algorithms, not only the running time but also the accuracy tend to degrade as the network becomes more complex. Because inference is used to compute the gradient for the continuous optimization, the degradation of inference quality can lead to models that are suboptimal. Moreover, because the resulting model is generally also used to estimate the beneﬁt of adding new features, any inaccuracy can propagate further, causing yet more suboptimal features to be introduced into the model. Hence, especially when the quality of approximate inference is a concern, it is worthwhile to select with care the features to be introduced into the model rather than blithely relying on the “guaranteed” convergence to a global optimum. 

Another important issue to be addressed is the problem of determining convergence in line 14 of Greedy-MN-Structure-Search . In other words, how do we test that none of the search operators we currently have available can improve the score? A priori, this task appears daunting, since we certainly do not want to try all possible feature addition/deletion steps, reoptimize the parameters for each of them, and then check whether the score has improved. Fortunately, there is a much more tractable solution. Speciﬁcally, we can show the following proposition: 

Let $\Delta_{L}^{\mathrm{grad}}(\theta_{k}\ :\ \theta^{l},\mathcal{D})$ denote the gradient of the l elihood relative to $\theta_{k}$ , evaluated at $\theta^{l}$ . Let $\beta$ be the hyperparameter deﬁning the $L_{1}$ prior. Let θ $\theta^{l}$ be a parameter assignment for which the following conditions hold: 

$$
\Delta_{L}^{\mathrm{grad}}(\theta_{k}\,:\,\theta^{l},\mathcal{D})-\frac{1}{\beta}\mathrm{sign}(\theta_{k}^{l})=0.
$$ 

• For any $k$ for which $\theta_{k}^{l}=0$ we have that 

$$
|\Delta_{L}^{\mathrm{grad}}(\theta_{k}\ :\ \theta^{l},\mathcal{D})|<\frac{1}{2\beta}.
$$ 

Then $\theta^{l}$ is $a$ global optimum of the $L_{1}$ -regularized log-likelihood function: 

$$
\frac{1}{M}\ell(\pmb\theta:\mathcal D)-\frac{1}{\beta}\sum_{i=1}^{k}|\theta_{i}|.
$$ 

Proof We provide a rough sketch of the proof. The ﬁrst condition guarantees that the gradient relative to any parameter for which $\theta_{k}^{l}\neq0$ is zero, and hence the objective f cannot be improved by changing its value. The second condition deals with parameters $\theta_{k}^{l}=0$ , for which the gradient is discontinuous at the convergence point. However, consider a point $\theta^{\prime}$ in the nearby vicinity of $\theta$ , so that $\theta_{k}^{\prime}\neq0$ . At $\theta^{\prime}$ , the gradient of the function relative to $\theta_{k}$ is very close to 

$$
\Delta_{L}^{\mathrm{grad}}(\theta_{k}\ :\ \pmb\theta^{l},\mathcal{D})-\frac{1}{\beta}\mathrm{sign}(\theta_{k}^{\prime}).
$$ 

The value of this expression is positive if $\theta_{k}^{\prime}<0$ and negative if $\theta_{k}^{\prime}\,>\,0$ . Thus, $\theta^{l}$ is a local optimum of the $L_{1}$ -regularized objective function. Because the function has only global optima, $\bar{\theta^{l}}$ must be a global optimum. 

Thus, we can test convergence easily as a direct by-product of the continuous parameter optimization procedure executed at each step. We note that we still have to consider every feature that is not included in the model and compute the relevant gradient; but we do not have to go through the (much more expensive) process of trying to introduce the feature, optimizing the resulting model, and evaluating its score. 

L-BFGS algorithm 

PAC-bound 

Theorem 20.4 

So far, we have avoided the discussion of optimizing this objective. As we mentioned in section 20.3.1, a commonly used method for optimizing the likelihood is the L-BFGS algorithm , which uses gradient descent combined with line search (see appendix A.5.2). The problem with applying this method to the $L_{1}$ -regularized likelihood is that the regularization term is not continuously diferentiable: the gradient relative to any parameter $\theta_{i}$ changes at $\theta_{i}=0$ from $+1$ to $-1$ . Perhaps the simplest solution to t s problem is to adjust the l e-search procedure to avoid changing the sign of any parameter θ $\theta_{i}$ i : If, during our line search, $\theta_{i}$ crosses from positive to negative (or vice versa), we simply ﬁx it to be 0 , and continue with the line search for the remaining parameters. Note that this decision corresponds to taking $f_{i}$ out of the set of active features in this iteration. If the optimal parameter assignment has a nonzero value for $\theta_{i}$ , we are guaranteed that $f_{i}$ will be introduced again in a later stage in the search, as we have discussed. Finally, as we mentioned, we can prove a useful theoretical guarantee for the results of $L_{1}$ -regularized Markov network learning. Speciﬁcally, we can show the following PAC-bound : 

$\mathcal{X}$ a set variables such that $|\mathit{V a l}(X_{i})|\;\leq\;d$ for all $i$ . Let $P^{*}$ be a tion, and

 $\delta,\epsilon,B>0$ . Let F be a set of all indicator features over all subsets of variables $X\subset{\mathcal{X}}$ ⊂X such that

 $|X|\leq c_{!}$ , and let 

$$
\begin{array}{r}{\Theta_{c,B}=\{\pmb{\theta}\in\Theta[\mathcal{F}]\ :\ \lVert\pmb{\theta}\rVert_{1}\leq B\}}\end{array}
$$ 

be all parameter iz at ions of $\mathcal{F}$ whose $L_{1}$ -norm is at most $B$ . Let $\beta=\sqrt{c\ln(2n d/\delta)/(2M)}$ . Let 

$$
\pmb{\theta}_{c,B}^{*}=\arg\operatorname*{max}_{\pmb{\theta}\in\Theta_{c,B}}D(P^{*}\|P_{\pmb{\theta}})
$$ 

be the best parameter iz ation achievable within the class $\Theta_{c,B}$ . For any data set $\mathcal{D}$ , let 

$$
\hat{\pmb\theta}=\arg\operatorname*{max}_{\pmb\theta\in\Theta[\mathcal{F}]}\mathrm{score}_{L_{1}}(\pmb\theta\ :\ \mathcal{D}).
$$ 

Then, for 

$$
M\geq\frac{2c B^{2}}{\epsilon^{2}}\ln\left(\frac{2n d}{\delta}\right),
$$ 

with probability at least $1-\delta$ , 

$$
D(P^{*}\|P_{\hat{\theta}})\leq D(P^{*}\|P_{{\theta}^{*}c,B})+\epsilon.
$$ 

In other w s, this theorem states that, with high probability over ata sets $\mathcal{D}$ , the relative entropy to $P^{*}$ achieved by the best $L_{1}$ -regularized model is at most ϵ worse than the relative entropy achieved by the best model within the class of limited-degree Markov networks. This guarantee is achievable with a number of samples that is polynomial in $\epsilon,\textit{c,}$ and $B$ , and logarithmic in $\delta$ and $d.$ . The logarithmic dependence on $n$ may feel promising, but we note that $B$ is a sum of the absolute values of all network parameters; assuming we bound the magnitude of individual parameters, this terms grows linearly with the total number of network parameters. Thus, $L_{1}$ -regularized learning provides us with a model that is close to optimal (within the class $\Theta_{c,B})$ , using a polynomial number of samples. 

# 20.7.5 Evaluating Changes to the Model 

We now consider in more detail the candidate evaluation step that takes place in line 11 of Greedy-MN-Structure-Search . As we discussed, the standard way to reduce the cost of the candidate evaluation step is simply to avoid computing the exact score of each of the candidate successors, and rather to select among them using simpler heuristics. Many approximations are possible, ranging from ones that are very simple and heuristic to ones that are much more elaborate and provide certain guarantees. 

Most simply, we can examine statistics of the data to determine features that may be worth including. For example, if two variables $X_{i}$ and $X_{j}$ are strongly correlated in the data, it may be worthwhile to consider introducing a factor over $X_{i},X_{j}$ (or a pairwise feature over one or more combinations of their values). The limitation of this approach is that it does not take into account the features that have already been introduced into the model and the extent to which they already explain the observed correlation. 

grafting 

gradient heuristic 

A somewhat more reﬁned approach, called grafting , estimates the beneﬁt of introducing a feature $f_{k}$ by compute the gradient of the likelihood relative to $\theta_{k}$ , evaluated at the current model. More precisely, assume tha our current model is $(\mathcal{F},\boldsymbol{\theta}^{0})$ . The gradient heuristic estimate to the delta-score (for score X ) obtained by adding $f_{k}\notin\mathcal{F}$ is deﬁned as: 

$$
\Delta_{X}^{\mathrm{grad}}(\theta_{k}\ :\ \pmb\theta^{0},\mathcal{D})=\frac{\partial}{\partial\theta_{k}}\mathrm{score}_{X}(\pmb\theta\ :\ \mathcal{D}),
$$ 

evaluated at the current parameters $\theta^{0}$ 

The gradient heuristic does account for the parameters already selected; thus, for example, it can avoid introducing features that are not relevant given the parameters already introduced. Intuitively, features that have a high gradient can induce a signiﬁcant immediate improvement in the score, and therefore they are good candidates for introduction into the model. Indeed, we are guaranteed that, if $\theta_{k}$ has a positive gradient, introducing $f_{k}$ into $\mathcal{F}$ will result in some improvement to the score. The problem with this approach is that it does not attempt to evaluate how large this improvement can be. Perhaps we can increase $\theta_{k}$ only by a small amount before further changes stop improving the score. 

An even more precise approximation is to evaluate a change to the model by computing the score obtained in a model where we keep all other parameters ﬁxed. Consider a step where gain heuristic we introduce or delete a single feature $f_{k}$ in our model. We can obtain an approximation to the score by evaluating the change in score when we change only the parameter $\theta_{k}$ associated with $f_{k}$ , keeping all other parameters unchanged. To formalize this idea, let $(\mathcal{F},\theta^{0})$ be our current model, and consider changes involving $f_{k}$ . We deﬁne the gain heuristic estimate to be the change in the score of the model for diferent values for $\theta_{k}$ , assuming the other parameters are kept ﬁxed: 

$$
\Delta_{X}^{\mathrm{gain}}(\boldsymbol{\theta}_{k}~:~\boldsymbol{\theta}^{0},\mathcal{D})=\mathrm{score}_{X}((\boldsymbol{\theta}_{k},\boldsymbol{\theta}_{-k}^{0})~:~\mathcal{D})-\mathrm{score}_{X}(\boldsymbol{\theta}^{0}~:~\mathcal{D}),
$$ 

where $\theta_{-k}^{0}$ is the vector of all parameters other than $\theta_{k}^{0}$ . As we discussed, due to the nonde- − composability of the likelihood, when we change $\theta_{k}$ , the current assignment $\theta_{-k}^{0}$ to the other − parameters is generally no longer optimal. However, it is still reasonable to use this function as a heuristic to rank diferent steps: Parameters that give rise to a larger improvement in the objective by themselves often also induce a larger improvement when other parameters are optimized. Indeed, changing those other parameters to optimize the score can only improve it further. Thus, the change in score that we obtain when we “freeze” the other parameters and change only $\theta_{k}$ is a lower bound on the change in the score. 

The gain function can be used to provide a lower bound on the improvement in score derived from the deletion of a feature $f_{k}$ currently in the model: we simply evaluate the gain function setting $\theta_{k}=0$ . We can also obtain a lower bound on the value of a step where we introduce into the model a new feature $f_{k}$ (that is, one for which the current parameter $\theta_{k}^{0}=0$ ). The improvement we can get if we freeze all parameters but one is clearly a lower bound on the improvement we can get if we optimize over all of the parameters. Thus, the value of $\Delta_{X}^{\mathrm{gain}}({\dot{\theta}}_{k}~:~{\theta}^{0},{\mathcal{D}})$ D is a lower bound on the improvement in the objective that can be gained by setting $\theta_{k}$ to its chosen value and optimizing all other parameters. To compute the best lower bound, we must maximize the function relative to diferent possible values of $\theta_{k}$ , giving us the score of the best possible model when all parameters other than $\theta_{k}$ are frozen. In particular, we can deﬁne: 

$$
{\mathrm{Gain}}_{X}(\pmb\theta^{0}\ :\ f_{k},{\mathcal D})=\operatorname*{max}_{\theta_{k}}\Delta_{X}^{\mathrm{gain}}(\theta_{k}\ :\ \pmb\theta^{0},{\mathcal D}).
$$ 

This is a lower bound on the change in the objective obtained from introducing a $f_{k}\notin\mathcal{F}$ . 

In principle, lower bounds are more useful than simple approximations. If our lower bound of the candidate’s score is higher than that of our current best model, then we deﬁnitely want to evaluate that candidate; this will result in a better current candidate and allow us to prune additional candidates and focus on the ones that seem more promising for evaluation. Upper bounds are useful as well. If we have a candidate model and obtain an upper bound on its score, then we can remove it from consideration once we evaluate another candidate with higher score; thus, upper bounds help us prune models for which we would never want to evaluate the true score. In practice, however, fully evaluating the score for all but a tiny handful of candidate structures is usually too expensive a proposition. Thus, the gain is generally used simply as an approximation rather than a lower bound. 

How do we evaluate the gain function efciently, or ﬁnd its optimal value? The gain function is a univariate function of $\theta_{k}$ , which is a projection of the score function onto this single dimension. Importantly, all of our scoring functions — including any of the penalized likelihood functions we described — are concave (for a given set of active features). The projection of a concave function onto a single dimension is also concave, so that this single-parameter delta-score is also concave and therefore has a global optimum. 

Nevertheless, given the complexity of the likelihood function, it is not clear how this global optimum can be efciently found. We now show how the diference between two log-likelihood terms can be considerably simpliﬁed, even allowing a closed-form solution in certain important cases. Recall from equation (20.3) that 

$$
\frac{1}{M}\ell(\pmb\theta:{\mathcal D})=\sum_{k}\theta_{k}\pmb E_{\mathcal D}[f_{k}]-\ln Z(\pmb\theta).
$$ 

Because parameters other than $\theta_{k}$ are the same in the two models, we have that: 

$$
\frac{1}{M}[\ell((\theta_{k},\theta_{-k}^{0}):\mathcal{D})-\ell(\theta^{0}:\mathcal{D})]=(\theta_{k}-\theta_{k}^{0})E_{\mathcal{D}}[f_{k}]-\left[\ln Z(\theta_{k},\theta_{-k}^{0})-\ln Z(\theta_{-k}^{0})\right]=0,
$$ 

The ﬁrst term is a linear function in $\theta_{k}$ , whose coefcient is the empirical expectation of $f_{k}$ in the data. For the second term, we have: 

$$
\begin{array}{r c l}{{\ln\displaystyle\frac{Z(\theta_{k},\theta_{-k}^{0})}{Z(\theta^{0})}}}&{{=}}&{{\ln\displaystyle\left[\frac{1}{Z(\theta^{0})}\sum_{\xi}\exp\left\{\sum_{j}\theta_{j}^{0}f_{j}(\xi)+(\theta_{k}-\theta_{k}^{0})f_{k}(\xi)\right\}\right]}}\\ {{}}&{{=}}&{{\ln\displaystyle\sum_{\xi}\frac{\tilde{P}_{\theta^{0}}(\xi)}{Z(\theta^{0})}\left[\exp\left\{(\theta_{k}-\theta_{k}^{0})f_{k}(\xi)\right\}\right]}}\\ {{}}&{{=}}&{{\ln E_{\theta^{0}}\Big[\exp\left\{(\theta_{k}-\theta_{k}^{0})f_{k}(d_{k})\right\}\Big].}}\end{array}
$$ 

Thus, the diference of these two log-partition functions can be rewritten as a log-expectation relative to our original distribution. We can convert this expression into a univariate function of $\theta_{k}$ by computing (via inference in our current model $\theta^{0}$ ) the marginal distribution over the variables $d_{k}$ . Altogether, we obtain that: 

$$
\begin{array}{l}{\displaystyle\frac{1}{M}[\ell((\theta_{k},\theta_{-k}^{0}):\mathcal{D})-\ell(\theta^{0}:\mathcal{D})]=\ }\\ {\displaystyle\ \ \ \ \ \ (\theta_{k}-\theta_{k}^{0})E_{\mathcal{D}}[f_{k}]-\ln\sum_{d_{k}}P_{\theta^{0}}(d_{k})\left[\exp\left\{(\theta_{k}-\theta_{k}^{0})f_{k}(d_{k})\right\}\right].}\end{array}
$$ 

We can incorporate this simpliﬁed form into equation (20.33) for any penalized-likelihood scoring function. We can now easily provide our lower-bound estimates for feature deletions. For introducing a feature $f_{k}$ , the optimal lower bound can be computed by optimizing the univariate function deﬁned by equation (20.33) over the parameter $\theta_{k}$ . Because this function is concave, it can be optimized using a variety of univariate numerical optimization algorithms. For example, to compute the lower bound for an $L_{2}$ -regularized likelihood, we would compute: 

$$
\operatorname*{max}_{\theta_{k}}\left\{\theta_{k}E_{\mathcal{D}}[f_{k}]-\ln\sum_{d_{k}}P_{\theta^{0}}(\boldsymbol{d}_{k})\left[\exp\left\{(\theta_{k}-\theta_{k}^{0})f_{k}(\boldsymbol{d}_{k})\right\}\right]-\frac{\theta_{k}^{2}}{2\sigma^{2}}\right\}.
$$ 

However, in certain special cases, we can actually provide a closed-form solution for this optimization problem. We note that this derivation applies only in restricted cases: only in the case of generative training (that is, not for CRFs); only for the likelihood or $L_{1}$ -penalized objective; and only for binary-valued features. 

# Proposition 20.6 

Let $f_{k}$ be a binary-valued feature and let $\theta^{0}$ be a current setting of parameters for a log-linear model. Let $\hat{p}_{k}\,=\,E_{D}[f_{k}]$ be the empirical probability of $f_{k}$ in $\mathcal{D}$ , and $p_{k}^{0}\,=\,P_{\theta}(f_{k})$ be its D probability relative to the current model. Then: 

$$
\operatorname*{max}_{\theta_{k}}\big[\mathrm{score}_{L}\big((\theta_{k},\pmb{\theta}_{-k}^{0})\ :\ \mathcal{D}\big)-\mathrm{score}_{L}\big(\pmb{\theta}^{0}\ :\ \mathcal{D}\big)\big]=\pmb{D}\big(\hat{p}_{k}\|p_{k}^{0}\big),
$$ 

where the KL-divergence is the relative entropy between the two Bernoulli distributions parameter- ized by $\hat{p}_{k}$ and $p_{k}^{0}$ respectively. 

The proof is left as an exercise (exercise 20.16). 

To see why this result is intuitive, recall that when we maximize the likelihood relative to some log-linear model, we obtain a model where the expected counts match the empirical counts. In the case of a binary-valued feature, in the optimized model we have that the ﬁnal probability of $f_{k}$ would be the same as the empirical probability $\hat{p}_{k}$ . Thus, it is reasonable that the amount of improvement we obtain from this optimization is a function of the discrepancy between the empirical probability of the feature and its probability given the current model. The bigger the discrepancy, the bigger the improvement in the likelihood. 

A similar analysis applies when we consider several binary-valued features $f_{1},\ldots,f_{k}$ , as long as they are mutually exclusive and exhaustive; that is, as long as there are no assignments for which both $f_{i^{\downarrow}}(\xi)=1$ and $f_{j}(\boldsymbol{\xi})=1$ . In particular, we can show the following: 

# Proposition 20.7 

Let $\theta^{0}$ be a current setting of parameters for a log-linear model, and consider introducing into the model a complete factor $\phi$ over scope $^{d,}$ , parameterized with $\theta_{k}$ that correspond to the diferent assignments to $^d$ . Then 

$$
\operatorname*{max}_{\theta_{k}}\left[\mathrm{score}_{L}((\theta_{k},\theta_{-k}^{0})\ :\ \mathcal{D})-\mathrm{score}_{L}(\theta^{0}\ :\ \mathcal{D})\right]=D(\hat{P}(d)\|P_{\theta}(d)).
$$ 

The proof is left as an exercise (exercise 20.17). 

Although the derivations here were performed for the likelihood function, a similar closed- form solution, in the same class of cases, can also be performed for the $L_{1}$ -regularized likelihood (see exercise 20.18), but not for the $L_{2}$ -regularized likelihood. Intuitively, the penalty in the $L_{1}$ - regularized likelihood is a linear function in each $\theta_{k}$ , and therefore it does not complicate the form of equation (20.34), which already contains such a term. However, the $L_{2}$ penalty is quadratic, and introducing a quadratic term into the function prevents an analytic solution. 

One issue that we did not address is the task of computing the expressions in equation (20.34), or even the closed-form expressions in proposition 20.6 and proposition 20.7. All of these expressions involve expectations over the scope $D_{k}$ of $f_{k}$ , where $f_{k}$ is the feature that we want to eliminate from or introduce into the model. Let us consider ﬁrst the case where $f_{k}$ is already in the model. In this case, if we use a belief propagation algorithm (whether a clique tree or a loopy cluster graph), the family preservation property guarantees that the feature’s scope $D_{k}$ is necessarily a subset of some cluster in our inference data structure. Thus, we can easily compute the necessary expectations. However, for a feature not currently in the model, we would not generally expect its scope to be included in any cluster. If not, we must somehow compute expectations of sets of variables that are not together in the same cluster. In the case of clique trees, we can use the out-of-clique inference methods described in section 10.3.3.2. For the case of loopy cluster graphs, this problem is more challenging (see exercise 11.22). 

# 20.8 Summary 

In this chapter, we discussed the problem of learning undirected graphical models from data. The key challenge in learning these models is that the global partition function couples the parameters, with signiﬁcant consequences: There is no closed-form solution for the optimal parameters; moreover, we can no longer optimize each of the parameters independently of the others. Thus, even simple maximum-likelihood parameter estimation is no longer trivial. For the same reason, full Bayesian estimation is computationally intractable, and even approximations are expensive and not often used in practice. 

Following these pieces of bad news, there are some good ones: the likelihood function is concave, and hence it has no local optima and can be optimized using efcient gradient-based methods over the space of possible parameter iz at ions. We can also extend this method to MAP estimation when we are given a prior over the parameters, which allows us to reduce the overﬁtting to which maximum likelihood is prone. 

The gradient of the likelihood at a point $\theta$ has a particularly compelling form: the gradient relative to the parameter $\theta_{i}$ corresponding to the feature $f_{i}$ is the diference between the empirical expectation of $f_{i}$ in the data and its expectation relative to the distribution $P_{\theta}$ . While very intuitive and simple in principle, the form of the gradient immediately gives rise to some bad news: to compute the gradient at the point $\theta$ , we need to run inference over the model $P_{\theta}$ , a costly procedure to execute at every gradient step. 

This complexity motivates the use of myriad alternative approaches: ones involving the use of approximate inference for computing the gradient; and ones that utilize a diferent objective than the likelihood. Methods in the ﬁrst class included using message passing algorithms such as belief propagation, and methods based on sampling. We also showed that many of the methods that use approximate inference for optimizing the likelihood can be reformulated as exactly optimizing an approximate objective. This perspective can ofer signiﬁcant insight. For example, we showed that learning with belief propagation can be reformulated as optimizing a joint objective that involves both inference and learning; this alternative formulation is more general and allows the use of alternative optimization methods that are more stable and convergent than using BP to estimate the gradient. 

Methods that use an approximate objective include pseudo likelihood, contrastive divergence, and maximum-margin (which is speciﬁcally geared for discriminative training of conditional models). Importantly, both likelihood and these objectives can be viewed as trying to increase the distance between the log-probability of assignments in our data and those of some set other assignments. This “contrastive” view provides a diferent view of these objectives, and it suggests that they are only representatives of a much more general class of approximations. 

The same analysis that we performed for optimizing the likelihood can also be extended to other cases. In particular, we showed a very similar derivation for conditional training, where the objective is to maximize the likelihood of a set of target variables $Y$ given some set of observed feature variables $X$ . 

We also showed that similar approaches can be applied to learning with missing data. Here, the optimization task is no longer convex, but the gradient has a very similar form and can be optimized using the same gradient-ascent methods. However, as in the case of Bayesian network learning with missing data, the likelihood function is generally multimodal, and so the gradient ascent algorithm can get stuck in local optima. Thus, we may need to resort to techniques such as data perturbation or random restarts. 

We also discussed the problem of structure learning of undirected models. Here again, we can use both constraint-based and score-based methods. Owing to the difculties arising from the form of the likelihood function, full Bayesian scoring, where we score a model by integrating over all of the parameters, is intractable, and even approximations are generally impractical. Thus, we generally use a simpler scoring function, which combines a likelihood term (measuring ﬁt to data) with some penalty term. We then search over some space of structures for ones that optimize this objective. For most objectives, the resulting optimization problem is combinatorial with multiple local optima, so that we must resort to heuristic search. One notable exception is the use of an $L_{1}$ -regularized likelihood, where the penalty on the absolute value of the parameters tends to drive many of the parameters to zero, and hence often results in sparse models. This objective allows the structure learning task to be formulated as a convex optimization problem over the space of parameters, allowing the optimization to be performed efciently and with guaranteed convergence to a global optimum. Of course, even here inference is still an unavoidable component in the inner loop of the learning algorithm, with all of the ensuing difculties. 

As we mentioned, the case of discriminative training is a setting where undirected models are particularly suited, and are very commonly used. However, it is important to carefully weigh the trade-ofs of generative versus discriminative training. As we discussed, there are signiﬁcant diferences in the computational cost of the diferent forms of training, and the trade-of can go either way. More importantly, as we discussed in section 16.3.2, generative models incorporate a higher bias by making assumptions — ones that are often only approximately correct — about the underlying distribution. Discriminative models make fewer assumptions, and therefore tend to require more data to train; generative models, due to the stronger bias, often perform better in the sparse-data regime. But incorrect modeling assumptions also hurt performance; therefore, as the amount of training data grows, the discriminative model, which makes fewer assumptions, often performs better. This diference between the two classes of models is particularly signiﬁcant when we have complex features whose correlations are hard to model. However, it is important to remember that models trained disc rim i natively to predict $Y$ given $X$ will perform well primarily in this setting, and even slight changes may lead to a degradation in performance. For example, a model for predicting $P(Y\mid X_{1},X_{2})$ would not be useful for predicting $P(Y\mid X_{1})$ in situations where $X_{2}$ is not observed. In general, discriminative models are much less ﬂexible in their ability to handle missing data. 

We focused most of our discussion of learning on the problem of learning log-linear models deﬁned in terms of a set of features. Log-linear models are a ﬁner-grained representation than a Markov network structure or a set of factors. Thus, they can make better trade-ofs between model complexity and ﬁt to data. However, sparse log-linear models (with few features) do not directly correspond to sparse Markov network structures, so that we might easily end up learning a model that does not lend itself to tractable inference. It would be useful to consider the development of Markov network structure learning algorithms that more easily support efcient inference. Indeed, some work has been done on learning Markov networks of bounded tree-width, but networks of low tree-width are often poor approximations to the target distribution. Thus, it would be interesting to explore alternative approaches that aim at structures that support approximate inference. 

This chapter is structured as a core idea with a set of distinct extensions that build on it: The core idea is the use of the likelihood function and the analysis of its properties. The extensions include conditional likelihood, learning with missing data, the use of parameter priors, approximate inference and/or approximate objectives, and even structure learning. In many cases, these extensions are orthogonal, and we can easily combine them in various useful ways. For example, we can use parameter priors with conditional likelihood or in the case of missing data; we can also use them with approximate methods such as pseudo likelihood, contrastive divergence or in the objective of equation (20.15). Perhaps more surprising is that we can easily perform structure learning with missing data by adding an $L_{1}$ -regularization term to the likelihood function of equation (20.8) and then using the same ideas as in section 20.7.4.4. In other cases, the combination of the diferent extensions is more involved. For example, as we discussed, structure learning requires that we be able to evaluate the expected counts for variables that are not in the same family; this task is not so easy if we use an approximate algorithm such as belief propagation. As another example, it is not immediately obvious how we can extend the pseudo likelihood objective to deal with missing data. These combinations provide useful directions for future work. 

# 20.9 Relevant Literature 

iterative proportional scaling iterative proportional ﬁtting 

Log-linear models and contingency tables have been used pervasively in a variety of communi- ties, and so key ideas have often been discovered multiple times, making a complete history too long to include. Early attempts for learning log-linear models were based on the iterative propor- tional scaling algorithm and its extension, iterative proportional ﬁtting . These methods were ﬁrst developed for contingency tables by Deming and Stephan (1940) and applied to log-linear models by Darroch and Ratclif (1972). The convex duality between the maximum likelihood and maxi- mum entropy problems appears to have been proved independently in several papers in diverse communities, including (at least) Ben-Tal and Charnes (1979); Dykstra and Lemke (1988); Berger, Della-Pietra, and Della-Pietra (1996). It appears that the ﬁrst application of gradient algorithms to maximum likelihood estimation in graphical models is due to Ackley, Hinton, and Sejnowski (1985) in the context of Boltzmann machines. The importance of the method used to optimize the likelihood was highlighted in the comparative study of Minka (2001a); this study focused on learning for logistic regression, but many of the conclusions hold more broadly. Since then, sev- eral better methods have been developed for optimizing likelihood. Successful methods include conjugate gradient, L-BFGS (Liu and Nocedal 1989), and stochastic meta-descent (Vishwanathan et al. 2006). 

Conditional random ﬁelds were ﬁrst proposed by Laferty et al. (2001). They have since been applied in a broad range of applications, such as labeling multiple webpage on a website (Taskar et al. 2002), image segmentation (Shental et al. 2003), or information extraction from text (Sutton and McCallum 2005). The application to protein-structure prediction in box 20.B is due to Yanover et al. (2007). 

The use of approximate inference in learning is an inevitable consequence of the intractability of the inference problem. Several papers have studied the interaction between belief propaga- tion and Markov network learning. Teh and Welling (2001) and Wainwright et al. (2003b) present methods for certain special cases; in particular, Wainwright, Jaakkola, and Willsky (2003b) derive the pseudo-moment matching argument. Inspired by the moment-matching behavior of learn- ing with belief propagation, Sutton and McCallum (2005); Sutton and Minka (2006) deﬁne the piecewise training objective that directly performs moment matching on all network potentials. Wainwright (2006) provides a strong argument, both theoretical and empirical, for using the same approximate inference method in training as will be used in performing the prediction using the learned model. Indeed, he shows that, if an approximate method is used for inference, then we get better performance guarantees if we use that same method to train the model than if we train a model using exact inference. He also shows that it is detrimental to use an un- stable inference algorithm (such as sum-product BP) in the inner loop of the learning algorithm. Ganapathi et al. (2008) deﬁne the uniﬁed CAMEL formulation that encompasses learning and inference in a single joint objective, allowing the nonconvexity of the BP objective to be taken out of the inner loop of learning. 

Although maximum (conditional) likelihood is the most commonly used objective for learning Markov networks, several other objectives have been proposed. The earliest is pseudo likelihood, proposed by Besag (1977b), of which several extensions have been proposed (Huang and Ogata 2002; McCallum et al. 2006). The asymptotic consistency of both the likelihood and the pseu- dolikelihood objectives is shown by Gidas (1988). The statistical efciency (convergence as a function of the number of samples) of the pseudo likelihood estimator has also been analyzed (for example, (Besag 1977a; Geyer and Thompson 1992; Guyon and Künsch 1992; Liang and Jordan 2008)). 

The use of margin-based estimation methods for probabilistic models was ﬁrst proposed by Collins (2002) in the context of parsing and sequence modeling, building on the voted-perceptron algorithm (Freund and Schapire 1998). The methods described in this chapter build on a class of large-margin methods called support vector machines (Shawe-Taylor and Cristianini 2000; Hastie et al. 2001; Bishop 2006), which have the important beneﬁt of allowing a large or even inﬁnite feature space to be used and trained very efciently. This formulation was ﬁrst proposed by Altun, Tsochantaridis, and Hofmann (2003); Taskar, Guestrin, and Koller (2003), who proposed two diferent approaches for addressing the exponential number of constraints. Altun et al. use a constraint-generation scheme, which was subsequently proven to require at most a polynomial number of steps (Tsochantaridis et al. 2004). Taskar et al. use a closed-form polynomial-size reformulation of the optimization problem that uses a clique tree-like data structure. Taskar, Chatalbashev, and Koller (2004) also show that this formulation also allows tractable training for networks where conditional probability products are intractable, but the MAP assignment can be found efciently. The contrastive divergence approach was introduced by Hinton (2002); Teh, Welling, Osindero, and Hinton (2003), and was shown to work well in practice in various studies (for example, (Carreira-Perpignan and Hinton 2005)). This work forms part of a larger trend of training using a range of alternative, often contrastive, objectives. LeCun et al. (2007) provide an excellent overview of this area. 

Much discussion has taken place in the machine learning community on the relative merits of discriminative versus generative training. Some insightful papers of particular relevance to graphical models include the work of Minka (2005) and LeCun et al. (2007). Also of interest are the theoretical analyses of $\mathrm{Mg}$ and Jordan (2002) and Liang and Jordan (2008) that discuss the statistical efciency of discriminative versus generative training, and provide theoretical support for the empirical observation that generative models, even if not consistent with the true underlying distribution, often work better in the sparse data case, but discriminative models tend to work better as the amount of data grows. 

The work of learning Markov networks with hidden variables goes back to the seminal paper of Ackley, Hinton, and Sejnowski (1985), who used gradient ascent to train Boltzmann machines with hidden variables. This line of work, largely dormant for many years, has seen a resurgence in the work on deep belief networks (Hinton et al. 2006; Hinton and Salakhutdinov 2006), a training regime for a multilayer restricted Boltzmann machine that iteratively tries to learn deeper and deeper hidden structure in the data. 

Parameter priors and regularization methods for log-linear models originate in statistics, where they have long been applied to a range of statistical models. Many of the techniques described here were ﬁrst developed for traditional statistical models such as linear or logistic regression, and then extended to the general case of Markov networks and CRFs. See Hastie et al. (2001) for some background on this extensive literature. 

The problem of learning the structure of Markov networks has not received as much attention as the task of Bayesian network structure learning. One line of work has focused on the problem of learning a Markov network of bounded tree-width, so as to allow tractable inference. The work of Chow and Liu (1968) shows that the maximum-likelihood tree-structured network can be found in quadratic time. 

A tree is a network of tree-width 1. Thus, the obvious generalization of is to learning the class of Markov networks whose tree-width is at most $k$ . Unfortunately, there is a sharp threshold phenomenon, since Srebro (2001) proves that for any tree-width $k$ greater than 1, ﬁnding the maximum likelihood tree-width- $k$ network is $\mathcal{N P}$ -hard. Interestingly, Narasimhan and Bilmes (2004) provide a constraint-based algorithm for PAC-learning Markov networks of tree-width at most $k$ : Their alg rithm is guaranteed to ﬁnd, with probability $1-\delta$ , a network whose relative entropy is within ϵ of optimal, in polynomial time, and using a polynomial number of samples. Importantly, their result does not contradict the hardness result of Srebro, since their analysis applies only in the consistent case, where the data is derived from a $k$ -width network. This discrepancy highlights again the signiﬁcant diference between learnability in the consistent and the inconsistent case. Several search-based heuristic algorithms for learning models with small tree-width have been proposed (Bach and Jordan 2001; Deshpande et al. 2001); so far, none of these algorithms have been widely adopted, perhaps because of the limited usefulness of bounded tree-width networks. 

Abbeel, Koller, and $\mathrm{Mg}$ (2006) provide a diferent PAC-learnability result in the consistent case, for networks of bounded connectivity. Their constraint-based algorithm is guaranteed to learn, with high probability, a network $\tilde{\mathcal{M}}$ M whose (symmetric) relative entropy to the true distribution $(D(\tilde{P}\|\bar{P^{*}})\bar{+D(P^{*}\|\tilde{P})})$ | | | | ) is at most $\epsilon$ . The complexity, both in time and in number of samples, grows exponentially in the maximum number of assignments to any local neighborhood (a factor and its Markov blanket). This result is somewhat surprising, since it shows that the class of low- connectivity Markov networks (such as grids) is PAC-learnable, even though inference (including computing the partition function) can be intractable. 

Of highest impact has been the work on using local search to optimize a (regularized) like- lihood. This line of work originated with the seminal paper of Della Pietra, Della Pietra, and Laferty (1997), who deﬁned the single-feature gain, and proposed the gain as an efective heuristic for feature selection in learning Markov network structure. McCallum (2003) describes some heuristic approximations that allow this heuristic to be applied to CRFs. The use of $L_{1}$ -regularization for feature selection originates from the Lasso model proposed for linear re- gression by Tibshirani (1996). It was ﬁrst proposed for logistic regression by Perkins et al. (2003); Goodman (2004). Perkins et al. also suggested the gradient heuristic for feature selection and the $L_{1}$ -based stopping rule. $L_{1}$ -regularized priors were ﬁrst proposed for learning log-linear distributions by Riezler and Vasserman (2004); Dudík, Phillips, and Schapire (2004). The use of $L_{1}$ -regularized objectives for learning the structure of general Markov networks was proposed by Lee et al. (2006). Building on the results of Dudík et al., Lee et al. also showed that the number of samples required to achieve close-to-optimal relative entropy (within the target class) grows only polynomially in the size of the network. Importantly, unlike the PAC-learnability results mentioned earlier, this result also holds in the inconsistent case. 

Pseudo likelihood has also been used as a criterion for model selection. Ji and Seymour (1996) deﬁne a pseudo likelihood-based objective and show that it is asymptotically consistent, in that the probability of selecting an incorrect model goes to zero as the number of training examples goes to inﬁnity. However, they did not provide a tractable algorithm for ﬁnding the highest scoring model in the super exponentially large set of structures. Wainwright et al. (2006) suggested the use of an $L_{1}$ -regularized pseudo likelihood for model selection, and also proved a theorem that provides guarantees on the near-optimality of the learned model, using a polynomial number of samples. Like the result of Lee et al. (2006), this result applies also in the inconsistent case. 

This chapter has largely omitted discussion of the Bayesian learning approach for Markov networks, for both parameter estimation and structure learning. Although an exact approach is computationally intractable, some interesting work has been done on approximate methods. Some of this work uses MCMC methods to sample from the parameter posterior. Murray and Ghahramani (2004) propose and study several diverse methods; owing to the intractability of the posterior, all of these methods are approximate, in that their stationary distribution is only an approximation to the desired parameter posterior. Of these, the most successful methods appear to be a method based on Langevin sampling with approximate gradients given by contrastive divergence, and a method where the acceptance probability is approximated by replacing the log partition function with the Bethe free energy. Two more restricted methods (Møller et al. 2006; Murray et al. 2006) use an approach called “perfect sampling” to avoid the need for estimating the partition function; these methods are elegant but of limited applicability. Other approaches approximate the parameter posterior by a Gaussian distribution, using either expectation prop- agation (Qi et al. 2005) or a combination of a Bethe and a Laplace approximation (Welling and Parise 2006a). The latter approach was also used to approximate the Bayesian score in order to perform structure learning (Welling and Parise 2006b). Because of the fundamental intractability of the problem, all of these methods are somewhat complex and computationally expensive, and they have therefore not yet made their way into practical applications. 

# 20.10 Exercises 

# Exercise 20.1 

Consider the network of ﬁgure 20.2, where we assume that some of the factors share parameters. Let $\theta_{i}^{y}$ be the parameter vector associated with all of the features whose scope is $Y_{i},Y_{i+1}$ . Let $\mathbf{\bar{\theta}}_{i,j}^{x y}$ be the parameter vector associated with all of the features whose scope is $Y_{i},X_{j}$ . 

a. Assume that, for all $i,i^{\prime}$ , $\pmb\theta_{i}^{y}=\pmb\theta_{i^{\prime}}^{y}$ , and that for all $i,i^{\prime}$ and $j$ , $\pmb{\theta}_{i,j}^{x y}=\pmb{\theta}_{i^{\prime},j}^{x y}$ . Derive the gradient update ′ for this model. 

b. Now (without the previous assumptions) assume for all $i$ and $j,j^{\prime}$ , $\pmb{\theta}_{i,j}^{x y}=\pmb{\theta}_{i,j^{\prime}}^{x y}$ . Derive the gradient ′ update for this model. 

# Exercise 20.2 

relational Markov network 

In this exercise, we show how to learn Markov networks with shared parameters, such as a relational Markov network (RMN). 

a. Consider the log-linear model of example 6.18, where we assume that the Study-Pair relationship is determined in the relational skeleton. Thus, we have a single template feature, with a single weight, which is applied to all study pairs. Derive the likelihood function for this model, and the gradient. b. Now provide a formula for the likelihood function and the gradient for a general RMN, as in deﬁni- tion 6.14. 

# Exercise 20.3 

Assume that our data are generated by a log-linear model $P_{\theta^{*}}$ that is of the form of equation (20.1). Show that, as the number of data instances $M$ goes to inﬁnity, with probability that approaches 1, $\pmb{\theta}^{*}$ is a global optimum of the likelihood objective of equation (20.3). (Hint: Use the characterization of theorem 20.1.) 

# Exercise 20.4 

Use the techniques described in this chapter to provide a method for performing maximum likelihood estimation for a CPD whose parameter iz ation is a generalized linear model, as in deﬁnition 5.10. 

# Exercise $20.5\star$ 

Show using Lagrange multipliers and the deﬁnitions of appendix A.5.4 that the problem of maximizing $H_{Q}(\mathcal X)$ X subject to equation (20.10) is dual to the problem of maximizing the log likelihood max $\ell(\pmb\theta:{\mathcal D})$ . 

# Exercise ${\bf20.6\star}$ 

In this problem, we will show an analogue to theorem 20.2 for the problems of maximizing conditional likelihood and maximizing conditional entropy. 

Consider a data set $\mathcal{D}=\{(\pmb{y}[m],\pmb{x}[m])\}_{m=1}^{M}$ as in section 20.3.2, and deﬁne the following conditional entropy maximization problem: 

Maximum-Conditional-Entropy : 

$$
\begin{array}{l r l}{{\mathrm{Find}}}&{Q(Y\mid X)}\\ {\operatorname{maximize}}&{\sum_{m=1}^{M}H_{Q}(Y\mid\pmb{x}[m])}\\ {\mathbf{subject~to}}&{}\\ &{\displaystyle\sum_{m=1}^{M}E_{Q(Y\mid\pmb{x}[m])}[f_{k}]=\sum_{m=1}^{M}f_{k}(\pmb{y}[m],\pmb{x}[m])\quad i=1,\ldots,k.}\end{array}
$$ 

Show that $Q^{*}(Y\mid X)$ optimizes this objective if and only if $\boldsymbol{Q}^{*}=P_{\hat{\boldsymbol{\theta}}}$ where $P_{\hat{\theta}}$ maximizes $\ell_{Y\mid X}(\pmb\theta:\mathcal D)$ as in equation (20.6). 

# Exercise $20.7\star$ 

iterative proportional scaling 

One of the earliest approaches for ﬁnding maximum likelihood parameters is called iterative proportional scaling (IPS). The idea is essentially to use coordinate ascent to improve the match between the empirical feature counts and the expected feature counts. In other words, we change $\theta_{k}$ so as to make $E_{P_{\theta}}[f_{k}]$ closer to $\pmb{E_{\mathcal{D}}}[f_{k}]$ ] . Because our model is multiplicative, it seems natural to multiply the weight of instances D where $f_{k}(\xi)=1$ by the ratio between the two expectations. This intuition leads to the following update rule: 

$$
\theta_{k}^{\prime}\leftarrow\theta_{k}+\ln\frac{E_{\mathcal{D}}[f_{k}]}{E_{P_{\theta}}[f_{k}]}.
$$ 

The IPS algorithm iterates over the diferent parameters and updates each of them in turn, using this update rule. 

Somewhat surprisingly, one can show that each iteration increases the likelihood until it reaches a maxi- mum point. Because the likelihood function is concave, there is a single maximum, and the algorithm is guaranteed to ﬁnd it. 

# Theorem 20.5 

Let $\theta$ be a parameter vector, and $\theta^{\prime}$ the vector that results from it after an application of equation (20.36). Then $\ell(\pmb\theta^{\prime}:\mathcal D)\geq\ell(\pmb\theta:\mathcal D)$ with equality if only if $\begin{array}{r}{\frac{\partial}{\partial\boldsymbol{\theta}_{k}}\ell(\boldsymbol{\theta}^{t}:\mathcal{D})=\overset{\cdot}{0}}\end{array}$ . 

In this exercise, you will prove this theorem for the special case where $f_{k}$ is binary-valued. More precisely, let $\Delta(\theta_{k})$ denote the change in likelihood obtained from modifying a single parameter $\theta_{k}$ , keeping the others ﬁxed. This expression was computed in equation (20.34). You will now show that the IPS update step for $\theta_{k}$ maximizes a lower bound on this single parameter gain. 

Deﬁne 

$$
\begin{array}{r c l}{{\tilde{\Delta}(\theta_{k})}}&{{=}}&{{(\theta_{k}^{\prime}-\theta_{k})\pmb{{\cal E}}_{\mathcal{D}}[f_{k}]-\displaystyle\frac{Z({\pmb\theta}^{\prime})}{Z({\pmb\theta})}+1}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{(\theta_{k}^{\prime}-\theta_{k})\pmb{{\cal E}}_{\mathcal{D}}[f_{k}]-\pmb{{\cal E}}_{P_{\theta}}[1-f_{k}]-e^{\theta_{k}^{\prime}-\theta_{k}}\pmb{{\cal E}}_{P_{\theta^{\prime}}}[f_{k}]+1.}}\end{array}
$$ 

a. Show that $\Delta(\theta_{k}^{\prime})\ge\tilde{\Delta}(\theta_{k}^{\prime})$ . (Hint: use the bound $\ln(x)\leq x-1.)$ 

b. Show that $\theta_{k}+\ln\frac{E_{\mathcal{D}}\left[f_{k}\right]}{E_{P_{\theta}}\left[f_{k}\right]}=\arg\operatorname*{max}_{\theta_{k}^{\prime}}\tilde{\Delta}\big(\theta_{k}^{\prime}\big)$ . 

c. Use these two facts to conclude that IPS steps are monotonically nondecreasing in the likelihood, and that convergence is achieved only when the log-likelihood is maximized. 

d. This result shows that we can view IPS as performing coordinatewise ascent on the likelihood surface. At each iteration we make progress along on dimension (one parameter) while freezing the others. Why is coordinate ascent a wasteful procedure in the context of optimizing the likelihood? 

# Exercise 20.8 

hyperbolic prior Consider the following hyperbolic prior for parameters in log-linear models. 

$$
P(\theta)=\frac{1}{\left(e^{\theta}+e^{-\theta}\right)/2}.
$$ 

a. Derive a gradient-based update rule for this parameter prior. 

b. Qualitatively describe the expected behavior of this parameter prior, and compare it to those of the $L_{2}$ or $L_{1}$ priors discussed in section 20.4. In particular, would you expect this prior to induce sparsity? 

# Exercise 20.9 

piecewise training 

We now consider an alternative local training method for Markov networks, known as piecewise training . For simplicity, we focus on Markov networks parameterized via full table factors. Thus, we have a set of factors $\phi_{c}(\boldsymbol{X}_{c})$ , where $X_{c}$ is the scope of factor $c$ , and $\phi_{c}({\pmb x}_{c}^{j})=\exp(\theta_{c j})$ . For a particular parameter assignment $\theta$ , we deﬁne $Z_{c}(\pmb{\theta})$ to be the local partition function for this factor in isolation: 

$$
Z_{c}({\pmb\theta}_{c})=\sum_{{\pmb x}_{c}}\phi_{c}({\pmb x}_{c}),
$$ 

where $\theta_{c}$ is the parameter vector associated with the factor $\phi_{c}(\boldsymbol{X}_{c})$ . We can approximate the global partition function in the log-likelihood objective of equation (20.3) as a product of the local partition functions, replacing $Z(\theta)$ with $\textstyle\prod_{c}Z_{c}(\pmb{\theta}_{c})$ . 

a. Write down the form of the resulting objective, simplify it, and derive the assignment of parameters that optimizes it. b. Compare the result of this optimization to the result of the pseudo-moment matching approach de- scribed in section 20.5.1. 

# Exercise ${\bf20.10\star}$ 

CAMEL In this exercise, we analyze the following simpliﬁcation of the CAMEL optimization problem of equa- tion (20.15): 

Simple-Approx-Maximum-Entropy : Find Q maximizingP Hβ(Ci)C ∈U i i subject to $\begin{array}{r c l}{{E_{\beta_{i}}[f_{i}]}}&{{=}}&{{E_{\mathcal{D}}[f_{i}]\quad i=1,.\,.\,.\,,k}}\\ {{\displaystyle\sum_{c_{i}}\beta_{i}(\pmb{c}_{i})}}&{{=}}&{{1\quad i=1,.\,.\,,k}}\\ {{\quad\quad Q\ge0}}&{{}}&{{}}\end{array}$ 

Here, we approximate both the objective and the constraints. The objective is approximated by the removal of all of the negative entropy terms for the sepsets. The constraints are relaxed by removing the requirement that the potentials in $Q$ be locally consistent (sum-calibrated) — we now require only that they be legal probability distributions. 

Show that this optimization problem is the Lagrangian dual of the piecewise training objective in exer- cise 20.9. 

# Exercise $20.11\star$ 

multi conditional training 

Consider a setting, as in section 20.3.2, where we have two sets of variables $\mathbf{Y}$ and $_{X}$ . Multi conditional training provides a spectrum between pure generative and pure discriminative training by maximizing the following objective: 

$$
\alpha\ell_{Y\mid X}(\boldsymbol{\theta}:\mathcal{D})+(1-\alpha)\ell_{X\mid Y}(\boldsymbol{\theta}:\mathcal{D}).
$$ 

Consider he model structure shown in ﬁgure 2 d a partially labeled data set $\mathcal{D}$ ere in each instance m we observe all of the feature variables $\pmb{x}[m]$ , but only the target variables in ${\cal O}[m]$ . 

Write down the objective of equation (20.37) for this case and compute its derivative. 

# Exercise $20.12\star$ 

Consider the problem of maximizing the approximate log-likelihood shown in equation (20.16). 

a. Derive the gradient of the approximate likelihood, and show that it is equivalent to utilizing an impor- tance sampling estimator directly to approximate the expected counts in the gradient of equation (20.4). b. Characterize properties of the maximum point (when the gradient is 0 ). Is such a maximum always attainable? Prove or suggest a counterexample. 

# Exercise $20.13\!\star\!\star$ 

One approach to providing a lower bound to the log-likelihood is by upper-bounding the partition function. Assume that we can decompose our model as a convex combination of (hopefully) simpler models, each with a weight $\alpha_{k}$ and a set of param ters $\psi^{k}$ . We deﬁne these submodels as follows: $\bar{\psi}^{k}(\pmb\theta)=\pmb w^{k}\bullet\pmb\theta$ , where we require that, for any feature i , 

$$
\sum_{k}\alpha_{k}w_{i}^{k}=1.
$$ 

a. Under this assumption, prove that 

$$
\ln Z(\pmb\theta)\leq\sum_{k}\alpha_{k}\ln Z(\pmb w^{k}\bullet\pmb\theta).
$$ 

This result allows us to deﬁne an approximate log-likelihood function: 

$$
\frac{1}{M}\ell(\pmb\theta:\mathcal D)\geq\ell_{\mathrm{convex}}(\pmb\theta:\mathcal D)=\sum_{i}\theta_{i}\pmb E_{\mathcal D}[f_{i}]-\sum_{k}\alpha_{k}\ln Z(\pmb w^{k}\bullet\pmb\theta).
$$ 

b. Assuming that the submodels are more tractable, we can efciently evaluate this lower bound, and also compute its derivatives to be used during optimization. Show that 

$$
\frac{\partial}{\partial\theta_{i}}\ell_{\mathrm{convex}}(\pmb{\theta}:\mathcal{D})=\pmb{E}_{\mathcal{D}}[f_{i}]-\sum_{k}\alpha_{k}\pmb{w}_{i}^{k}\pmb{E}_{P_{\pmb{w}^{k}}\bullet\pmb{\theta}}[f_{i}].
$$ 

c. We can provide a bound on the error of this approximation. Speciﬁcally, show that: 

$$
\frac{1}{M}\ell(\pmb\theta:\mathcal D)-\ell_{\mathrm{convex}}(\pmb\theta:\mathcal D)=\sum_{k}\alpha_{k}\pmb D(P_{\pmb\theta}\|P_{\pmb w^{k}\bullet\pmb\theta}),
$$ 

where the KL-divergence measures are deﬁned in terms of the natural logarithm. Thus, we see that the error is an average of the divergence between the true distribution and each of the approximating submodels. 

d. The justiﬁcation for this approach is that we can make the submodels simpler than the original model by having some parameters be equal to 0 , thereby eliminating the resulting feature from the model structure. Other than this constraint, however, we still have considerable freedom in choosing the submodel weight vectors $\pmb{w}^{k}$ . Assume that each weight vector $\scriptstyle\left\{{\pmb w}_{k}\right\}$ maximizes $\ell_{\mathrm{convex}}(\theta:\mathcal{D})$ ject to the constraint of equation (20.38) plus additional constraints requiring that certain entries $\boldsymbol{w}_{i}^{k}$ be equal to 0 . Show that if $i,\,k$ and $l$ are such that $\theta_{i}\neq0$ and neither $w_{i}^{k}$ nor $w_{i}^{l}$ is constrained to be zero, then 

$$
\pmb{{\cal E}}_{P_{w^{k}\bullet\theta}}[f_{i}]=\pmb{{\cal E}}_{P_{w^{l}\bullet\theta}}[f_{i}].
$$ 

Conclude from this result that for each such $i$ and $k$ , we have that 

$$
\pmb{{\cal E}}_{P_{w^{k}\bullet\theta}}[f_{i}]=\pmb{{\cal E}}_{\mathcal{D}}[f_{i}].
$$ 

# Exercise 20.14 

Consider a particular parameter iz ation $(\theta,\eta)$ to Max-margin . Show how we can use second-best MAP inference to either ﬁnd a violated constraint or guarantee that all constraints are satisﬁed. 

# Exercise 20.15 

Let $\mathcal{H}^{*}$ be a Markov twork where the aximum degree of a node is $d^{*}$ . Show that if we have an inﬁnitely large data set D generated from H $\mathcal{H}^{*}$ (so that independence tests are evaluated per tly), then the Build-PMap-Skeleton procedure of algorithm 3.3 reconstructs the correct Markov structure ∗ . 

# Exercise $20.16\star$ 

Prove proposition 20.6. (Hint: Take the derivative of equation (20.34) and set it to zero.) 

# Exercise 20.17 

In this exercise, you will prove proposition 20.7, which allows us to ﬁnd a closed-form optimum to multiple features in a log-linear model. 

a. Prove the following proposition. 

# Proposition 20.8 

Let $\pmb{\theta}^{0}$ be a current setting of parameters for a log-linear model, and suppose that $f_{1},\ldots,f_{l}$ are mutually exclusive binary features, that is, there is no $\xi$ and $i\neq j$ , so that $f_{i}(\xi)=1$ and $f_{j}(\boldsymbol{\xi})=1$ . Then, $\operatorname*{max}_{\theta_{1},\ldots,\theta_{l}}\left[\mathrm{score}_{L}((\theta_{1},\ldots,\theta_{l},\theta_{-\{1,\ldots,l\}}^{0})\ :\ \mathcal{D})-\mathrm{score}_{L}(\theta^{0}\ :\ \mathcal{D})\right]=D(\hat{p}\|p^{0}),$ where $\hat{p}$ is a distribution over $_{l+1}$ values with $\hat{p}_{i}=\pmb{E}_{\mathcal{D}}[f_{i}]$ ] , and $p^{0}$ is a distribution with $p^{0}(i)=P\theta(f_{i})$ . D b. Use this proposition to prove proposition 20.7. 

# Exercise 20.18 

Derive an analog to proposition 20.6 for the case of the $L_{1}$ regularized log-likelihood objective. 

# Part IV 

# Actions and Decisions 

# 21.1 Motivation and Overview 

So far, we have been somewhat ambivalent about the relation between Bayesian networks and causality. On one hand, from a formal perspective, all of the deﬁnitions refer only to probabilistic properties such as conditional independence. The BN structure may be directed, but the directions of the arrows do not have to be meaningful. They can even be antitemporal. Indeed, we saw in our discussion of I-maps that we can take any ordering on the nodes and create a BN for any distribution. On the other hand, it is common wisdom that a “good” BN ructure should correspond to causality, in that an edge $X\rightarrow Y$ often suggests that $X$ “causes” $Y$ , either directly or indirectly. The motivation for this statement is pragmatic: Bayesian networks with a causal structure tend to be sparser and more natural. However, as long as the network structure is capable of representing the underlying joint distribution correctly, the answers that we obtain to probabilistic queries are the same, regardless of whether the network structure corresponds to some notion of causal inﬂuence. 

Given this observation, is there any deeper value to imposing a causal semantics on a Bayesian network? In this chapter, we discuss a type of reasoning for which a causal interpretation of the network is critical — reasoning about situations where we intervene in the world, thereby interfering in the natural course of events. For example, we may wish to know if an intervention where we prevent smoking in all public places is likely to decrease the frequency of lung cancer. To answer such queries, we need to understand the causal relationships between the variables in our model. 

In this chapter, we provide a framework for interpreting a Bayesian network as a causal model whose edges have causal signiﬁcance. Not surprisingly, this interpretation distinguishes between models that are equivalent in their ability to represent probabilistic correlations. Thus, although the two networks $X\rightarrow Y$ and $Y\rightarrow X$ are equivalent as probabilistic models , they will turn out to be very diferent as causal models . 

# 21.1.1 Conditioning and Intervention 

As we discussed, for standard probabilistic queries it does not matter whether our model is causal or not. It matters only that it encode the “right” distribution. The diference between causal models and probabilistic models arise when we care about interventions in the model — situations where we do not simply observe the values that variables take but can take actions that can manipulate these values. 

ideal intervention 

intervention query 

In general, actions can afect the world in a variety of ways, and even a single action can have multiple efects. Indeed, in chapter 23, we discuss models that directly incorporate agent actions and allow for a range of efects. In this chapter, however, our goal is to isolate the speciﬁc issue of understanding causal relationships between variables. One approach to modeling causal relationships is using the notion of ideal interventions — interventions of the form $\pmb{d o}(Z\,:=\,z)$ , which force the variable $Z$ to take the value $z$ , and have no other immediate efect. An ideal intervention is equivalent to a dedicated action whose only efect is setting $Z$ to $z$ . However, we can consider such an ideal intervention even when such an action does not exist in the world. For example, consider the question of whether a particular mutation in a person’s DNA causes a particular disease. This causal question can be formulated as the question of whether an ideal intervention, whose only efect is to generate this mutation in a person’s DNA, would lead to the disease. Note that, even if such a process were ethical, current technology does not permit an action whose only efect is to mutate the DNA in all cells of a human organism. However, understanding the causal connection between the mutation and the disease can be a critical step toward ﬁnding a cure; the ideal intervention provides us with a way of formalizing this question and trying to provide an answer. 

More formally, we consider a new type of “conditioning” on an event of the form $d o(Z:=$ $z$ ) , often abbreviated $d o(z)$ ; this information corresponds to settings where an agent directly manipulated the world, to set the variable $Z$ to take the value $z$ with probability 1 . We are now interested in answering queries of the form $P(Y\mid d o(z))$ , or, more generally, $P(Y\mid$ $~d o(z),X\,=\,x)$ . These queries are called intervention queries . They correspond to settings where we set the variables in $Z$ to take the value $_z$ , observe the values $_{_{x}}$ for the variables in $X$ , and wish to ﬁnd the distribution over the variables $Y$ . Such queries arise naturally in a variety of settings: 

• Diagnosis and Treatment: “If we get a patient to take this medication, what are her chances of getting well?” Th can be formulated as $P(H\mid d o(M:=m^{1}))$ ) , where $H$ is the patient’s health, and $M=m^{1}$ corresponds to her taking the medication. Note that this query is not the same as $P(H\mid m^{1})$ . For example, if patients who take the medication on their own are more likely to be health-conscious, and therefore healthier in general, the chances of $P(H\mid m^{1})$ may be higher than is warranted for the patient in question.

 • Marketing: “If we lower the price of hamburgers, will people buy more ketchup?” Once again, this query is not a standard observational query, but rather one in which we intervene in the model, and thereby possibly change its behavior.

 • Policy Making: “If we lower the interest rates, will that give rise to inﬂation?”

 • Scientiﬁc Discovery: “Does smoking cause cancer?” When we formalize it, this query is an intervention query, meaning: “If we were to force someone to smoke, would they be more likely to get cancer?” 

counterfactual query 

A diferent type of causal query arises in situations where we already have some information about the true state of the world, and want to inquire about the state the world would be in had we been able to intervene and set the values of certain variables. For example, we might want to know “Would the U.S. have joined World War II had it not been for the attack on Pearl Harbor?” Such queries are called counterfactual queries , because they refer to a world that we know did not happen. Intuitively, our interpretation for such a query is that it refers to a world that difers only in this one respect. Thus, in this counterfactual world, Hitler would still have come into power in Germany, Poland would still have been invaded, and more. On the other hand, events that are direct causal consequences of the variable we are changing are clearly going to be diferent. For example, in the counterfactual world, the USS Arizona (which sank in the attack) would not (with high probability) currently be at the bottom of Pearl Harbor. 

At ﬁrst glance, counterfactual analysis might seem somewhat pointless and convoluted (who cares about what would have happened?). However, such queries actually arise naturally in several settings: 

• Legal liability cases: “Did the driver’s intoxicated state cause the accident?” In other words, would the accident have happened had the driver not been drunk? Here, we may want to preserve many other aspects of the world, for example, that it was a rainy night (so the road was slippery). 

• Treatment and Diagnosis: “We are faced with a car that does not start, but where the lights work; will replacing the battery make the car start?” Note that this is not an intervention query; it is a counterfactual query: we are actually asking whether the car would be working now had we replaced the battery. As in the previous example, we want to preserve as much of our scenario as possible. For example, given our observation that the lights work, the problem probably is not with the battery; we need to account for this conclusion when reasoning about the situation where the battery has been replaced. 

Even without a formal semantics for a causal model, we can see that the answer to an intervention query $P(Y\mid d o(z),X\,=\,x)$ is generally quite diferent from the answer to its corresponding probabilistic query $P(Y\mid Z=z,X=x)$ ) . 

Let us revisit our simple Student example of section 3.1.3.1, and consider a particular student Gump. As we have already discussed, conditioning on an observation that Gump receives an A in the class increases the probability that he has high intelligence, his probability of getting a high SAT score, and his probability of getting a good job. By contrast, consider a situation where Gump is lazy, and rather than working hard to get an A in the class, he pays someone to hack into the university registrar’s database and change his grade in the course to an A. In this case, what is his probability of getting a good job? Intuitively, the company where Gump is applying only has access to Gump’s transcript; thus, the company’s response to a manipulated grade would be the same as the response to an authentic grade. Therefore, we would expect $P(J\mid\operatorname{do}(g^{1}))=P(J\mid g^{1})$ . What about the other two probabilities? Intuitively, we feel that the manipulation to Gump’s grade should not afect our beliefs about his intelligence, nor about his SAT score. Thus, we would expect $P(i^{1}\mid\mathrm{d}\mathrm{o}(g^{1}))=P(i^{1})$ and $P(s^{1}\mid\operatorname{do}(g^{1}))=P(s^{1})$ . 

Why is our response to these queries diferent? In all three cases, there is a strong correlation between Gump’s grade and the variable of interest. However, we perceive the correlation between Gump’s grade and his job prospects as being causal . Thus, changes to his grade will directly afect his chances of being hired. The correlation between intelligence and grade arises because of an opposite causal connection: intelligence is a causal factor in grade. The correlation between Gump’s SAT score and grade arises due to a third mechanism — their joint dependence on Gump’s intelligence. Manipulating Gump’s grade does not change his intelligence or his chances of doing well in the class. In this chapter, we describe a formal framework of causal models that provides a rigorous basis for answering such queries and allows us to distinguish between these diferent cases. As we will see, this framework can be used to answer both intervention and counterfactual queries. However, the latter require much ﬁner-grained information, which may be difcult to acquire in practice. 

# 21.1.2 Correlation and Causation 

As example 21.1 illustrates, a correlation between two variables $X$ and $Y$ can arise in multiple settings: when $X$ causes $Y$ , when $Y$ causes $X$ , or when $X$ and $Y$ are both efects of a single cause. This observation immediately gives rise to the question of identiﬁability of causal models: If we observe two variables $X,Y$ to be probabilistic ally correlated in some observed distribution, what can we infer about the causal relationship between them. As we saw, diferent relationships give rise to very diferent answers to causal queries. 

This problem is greatly complicated by the broad range of reasons that may lead to an observed correlation between two variables $X$ and $Y$ . As we saw in example 21.1, when some variable $W$ causally afects both $X$ and $Y$ , we generally observe a correlation between them. If we know about the existence of $W$ and can observe it, we can disentangle the correlation between $X$ and $Y$ that is induced by $W$ and compute the residual correlation between $X$ and $Y$ that may be attributed to a direct causal relationship. In practice, however, there is latent variable confounding factor 

a huge set of possible latent variables , representing factors that exist in the world but that we cannot observe and often are not even aware of. A latent variable may induce correlations between the observed variables that do not correspond to causal relations between them, and hence forms a confounding factor in our goal of determining causal interactions. 

As we discussed in section 19.5, when our task is pure probabilistic reasoning, latent variables need not be modeled explicitly, since we can always represent the joint distribution over the observable variables only using a probabilistic graphical model. Of course, this marginalization process can lead to more complicated models (see, for example, ﬁgure 16.1), and may therefore be undesirable. We may therefore choose to model certain latent variables explicitly, in order to simplify the resulting network structure. Importantly, however, for the purpose of answering probabilistic queries, we do not need to model all latent variables. As long as our model $\mathcal{B}_{o b s}$ over the observable variables allows us to capture exactly the correct marginal distribution over the observed variables, we can answer any query as accurately with $\mathcal{B}_{o b s}$ as with the true network, where the latent variables are included explicitly. 

However, as we saw, the answer to a causal query over $X,Y$ is quite diferent when a correlation between them is due to a causal relationship and when it is induced by a latent variable. Thus, for the purposes of causal inference, it is critical to disentangle the component in the correlation between $X$ and $Y$ that is due to causal relationships and the component due to these confounding factors. Unfortunately, this requirement poses a major challenge, since it is virtually impossible, in complex real-world settings, to identify all of the relevant latent variables and quantify their efects. 

stomach ulcer ( O ). Because taking PeptAid precedes the ulcer, we might be tempted to conclude that PeptAid causes stomach ulcers. However, an alternative explanation is that the correlation can be attributed to a latent common cause — preulcer discomfort: individuals sufering from preulcer discomfort were more likely to take PeptAid and ultimately more likely to develop ulcers. Even if we account for this latent variable, there are many others that can have a similar efect. For example, some patients who live a more stressful lifestyle may be more inclined to eat irregular meals and therefore more likely to require antacid medication; the same patients may also be more susceptible to stomach ulcers. 

selection bias 

# Example 21.3 

Latent variables are only one type of mechanism that induces a noncausal correlation between variables. Another important class of confounding factors involves selection bias . Selection bias arises when the population that the distribution represents is a segment of the population that exhibits atypical behavior. 

Consider a university that sends out a survey to its alumni, asking them about their history at the institution. Assume that the observed distribution reveals a negative correlation between students who participated in athletic activities $(A)$ and students whose GPA was high $(G)$ . Can we conclude from this ﬁnding that participating in athletic activities reduces one’s GPA? Or that students with a high GPA tend not to participate in athletic activities? An alternative explanation is that the respondents to the survey $\scriptstyle(S\ =\ s^{1}$ ) are not a representative segment population: Students who did well in courses tended to respond, as did students who participated in athletic activities (and therefore perhaps enjoyed their time at school more); students who did neither tended not to respond. In other words, we have a causal link from $A$ to $S$ and from $G$ to $S$ . In this case, even if $A$ and $G$ are independent in the overall distribution over the student population, we may have a correlation in the subpopulation of respondents. This is an instance of standard intercausal reasoning, where $P(a^{1}\mid s^{1})>P(a^{1}\mid g^{1},s^{1})$ . But without accounting for the possible bias in selecting our population, we may falsely explain the correlation using a causal relationship. 

There are many other examples where correlations might arise due to noncausal reasons. One reason involves a mixture of diferent populations. 

# Example 21.4 

# Example 21.5 

It is commonly accepted that young girls develop verbal ability at an earlier age than boys. Con- versely, boys tend to be taller and heavier than girls. There is certainly no (known) correlation between height and verbal ability in either girls or boys separately. However, if we simply measure height and verbal ability across all children (of the same age), then we may well see a negative correlation between verbal ability and height. This type of situation is a special case of a latent variable, denoting the class to which the instance belongs (gender, in this case). However, it deserves special mention both because it is quite common, and because these class membership variables are often not perceived as “causes” and may therefore be ignored when looking for a confounding common cause. A similar situation arises when the distribution we obtain arises from two time series, each of which has a particular trend. 

Because average population height has been increasing (due to improved nutrition), and the total size of the polar caps has been decreasing (due to global warming), we would observe a negative correlation between $H$ and $S$ in these data. However, we would not want to conclude that the size of the polar caps causally inﬂuences average population height. 

In a sense, this situation is also an instance of a latent variable, which in this case is time. 

# 

causal efect 

Thus, we see that the correlation between a pair of variables $X$ and $Y$ may be a consequence of multiple mechanisms, where some are causal and others are not. To answer a causal query regarding an intervention at $X$ , we need to disentangle these diferent mechanisms, and to isolate the component of the correlation that is due to the causal efect of $X$ on $Y$ . A large part of this chapter is devoted to addressing this challenge. 

# 21.2 Causal Models 

causal model 

causal mechanism 

We begin by providing a formal framework for viewing a Bayesian network as a causal model. A causal model has the same form as a probabilistic Bayesian network. It consists of a directed acyclic graph over the random variables in the domain. The model asserts that each variable $X$ is governed by a causal mechanism that (stochastically) determines its value based on the values of its parents. That is, the value of $X$ is a (stochastic) function of the values of its parents. 

A causal mechanism takes the same form as a standard CPD. For a node $X$ and its parents $U$ , the causal model has a stochastic function from the values of $U$ to the values of $X$ . In other words, for each value $\mathbfit{u}$ of $U$ , it speciﬁes a distribution over the values of $X$ . The diference is in the interpretation of the edges. In a causal model, we assume that $X$ ’s parents are its direct causes (relative to the variables represented in the model). In other words, we assume that causality ﬂows in the direction of the edges, so that $X$ ’s value is actually determined via the stochastic function implied by X ’s CPD. 

The assumption that CPDs correspond to causal mechanisms forms the basis for the treatment of intervention queries. When we intervene at a variable $X$ , setting its value to $x$ , we replace its original causal mechanism with one that dictates that it take the value $x$ . This manipulation corresponds to replacing $X$ ’s CPD with a diferent one, where $X\ =\ x$ with probability 1 , regardless of anything else. 

Example 21.6 

mutilated network 

For instance, in example 21.1, if Gump changes his grade to an A by hacking into the registrar’s database, the result is a model where his grade is no longer determined by his performance in the class, but rather set to the value A, regardless of any other aspects of the situation. An appropriate graphical model for the post intervention situation is shown in ﬁgure 21.1a. In this network, the Grade variable no longer depends on Intelligence or Difculty, nor on anything else. It is simply set to take the value A with probability 1. 

The model in ﬁgure 21.1a is an instance of the mutilated network , a concept introduced in deﬁnition 12.1 that, in the mutilated network $\mathcal{B}_{Z=z}$ , we eliminate all incoming edges into each variable $Z_{i}\in Z$ ∈ , and set its value to be $z_{i}$ with probability 1 . Based on this intuition, we can now deﬁne a causal model as a model that can answer intervention queries using the appropriate mutilated network. 

![](images/87745848257792fbf85d745e735481894f0d09d32d34641d76afbdd4ce5ef7ff.jpg) 

Figure 21.1 Mutilated Student networks representing interventions (a) Mutilated Student network with an intervention at $G$ . (b) An expanded S dent network, with an additional arc $S\,\rightarrow\,J$ . (c) A mutilated network from (b), with an intervention at G . 

intervention query 

Example 21.7 bilistic queries, can also answer intervention queries $P(Y\mid\mathrm{d}o(z),\pmb{x})$ , as follows: 

$$
P_{\mathcal{C}}(Y\mid\mathrm{do}(z),\boldsymbol{x})=P_{\mathcal{C}_{Z=z}}(Y\mid x).
$$ 

It is easy to see that this approach deals appropriately with example 21.1. Let C student be the appropriate causal model. When we intervene in this model by setting Gump’s grade to an A, we obtain the mutilated network shown in ﬁgure 21.1a. The distribution induced by this network over Gump’s SAT score is the same as the prior distribution over his SAT score in the original network. Thus, 

$$
P(S\mid{\mathrm{do}}(G:=g^{1}))=P_{\mathcal{C}_{G=g^{1}}^{\mathrm{stunder}}}(S)=P_{\mathcal{C}^{\mathrm{stunder}}}(S),
$$ 

as we would expect. Conversely, the distribution induced by this network on Gump’s job prospects is $P_{\mathcal{C}^{\mathrm{stunder}}}(J\mid G=g^{1})$ . 

Note that, in general, the answer to an intervention query does not necessarily reduce to the answer to some observational query. 

# Example 21.8 

Simpson’s paradox 

Example 21.9 Assume that we start out with a somewhat diferent Student network, as shown in ﬁgure 21.1b, which contains an edge from the student’s SAT score to his job prospects (for example, because the recruiter can also base her hiring decision on the student’s SAT scores). Now, the query $P_{\mathcal{C}^{\mathrm{stunder}}}(J\mid\mathrm{d}\mathfrak{o}(g^{1}))$ is answered by the mutilated network of ﬁgure 21.1c. In this case, the answer to the query is clearly not $P_{\mathcal{C}^{\mathrm{stunder}}}(J)$ , due to the direct causal inﬂuence of his grade on his job prospects. On the other hand, it is also not equ $P_{\mathcal{C}^{\mathrm{stunder}}}(J\mid g^{1})$ , because this last expression also includes the inﬂuence via the evidential trail $G\gets I\to S\to J_{z}$ ← → → , which does not apply in the mutilated model. 

The ability to provide a formal distinction between observational and causal queries can help resolve some apparent paradoxes that have been the cause of signiﬁcant debate. One striking example is Simpson’s paradox , a variant of which is the following: 

![](images/c84763d6016e70cb47186b0facb39df35a1f7e1c2c002fd49bacf0a935572241.jpg) 
Figure 21.2 Causal network for Simpson’s paradox 

of patients who took the drug $(D)$ are cured $(\!(C)\!,$ whereas only 50 percent of the patients who did not take the drug are cured. Given these statistics, we might be inclined to believe that the drug is beneﬁcial. However, more reﬁned statistics show that within the subpopulation of male patients, 70 percent who took the drug are cured, whereas 80 percent of those who did not take the drug are cured. Moreover, within the subpopulation of female patients, 20 percent of who took the drug are cured, whereas 40 percent of those who did not take the drug are cured. Thus, despite the apparently beneﬁcial efect of the drug on the overall population, the drug appears to be detrimental to both men and women! More precisely, we have that: 

$$
\begin{array}{r c l}{{P(c^{1}\mid d^{1})}}&{{>}}&{{P(c^{1}\mid d^{0})}}\\ {{P(c^{1}\mid d^{1},G=m a l e)}}&{{<}}&{{P(c^{1}\mid d^{0},G=m a l e)}}\\ {{P(c^{1}\mid d^{1},G=f e m a l e)}}&{{<}}&{{P(c^{1}\mid d^{0},G=f e m a l e).}}\end{array}
$$ 

How is this possible? 

This case can occur because taking the drug is correlated with gender: men are much more likely to take the drug than women. In this particular example, 75 percent of men take the drug, whereas only 25 percent of women do. With these parameters, even if men and women are equally represented in the population of patients, we obtain the surprising behavior described earlier. 

The conceptual difculty behind this paradox is that it is not clear which statistics one should use when deciding whether to prescribe the drug to a patient: the general ones or the ones conditioned on gender. In particular, it is not difcult to construct examples where this reversal continues, in that conditioning on yet another variable leads one to the conclusion that the drug is beneﬁcial after all, and conditioning on one more reverses the conclusion yet again. So how can we decide which variables we should condition on? 

The causal framework provides an answer to this question. The appropriate query we need to answer in determining whether to prescribe the drug is not $P(c^{1}\mid d^{1})$ , but rather $P(c^{1}\mid\mathrm{d}\mathrm{o}(d^{1}))$ . Figure 21.2 shows the causal network corresponding to this situation, and the mutilated network required for answering the query. We will show how we can use the structure of the causal network to answer queries such as $P(c^{1}\mid\mathrm{d}\mathrm{o}(d^{1}))$ . As we will see in example 21.14, the answer to this query shows that the drug is not beneﬁcial, as expected. 

# 21.3 Structural Causal Identiﬁability 

The framework of the previous section provides us with a mechanism for answering intervention queries, given a fully speciﬁed causal model. However, fully specifying a causal model is often impossible. As we discussed, there are often a multitude of (generally unknown) factors that are latent, and that can induce correlations between the variables. In most cases, we cannot fully specify the causal connection between the latent variables and the variables in our model. 

In general, the only thing that we can reasonably hope to obtain is the marginal distribution over the observed variables in the model. As we discussed, for probabilistic queries, this marginal distribution sufces (assuming it can be acquired with reasonable accuracy). However, for intervention queries, we must disentangle the causal inﬂuence of $X$ and $Y$ from other factors leading to correlations between them. It is far from clear how we could ever accomplish this goal. 

Consider a pair of variables $X,Y$ with an observed correlation between them, and imagine that our task is to determine $P(Y\mid\operatorname{do}(X))$ . t us even assume that $X$ temporally precedes $Y$ , and therefore we know that Y cannot cause X . However, if we consider the possibility that at least some of the correlation between $X$ and $Y$ is due to a hidden common cause, we have no way of determining how much efect perturbing $X$ would have on $Y$ . If all of the correlation is due to a causal link, then $P(Y\mid{\mathrm{do}}(X))=P(Y\mid X),$ ; conversely, if all of the correlation is due to the hidden common cause, then $P(Y\mid{\mathrm{do}}(X))=P(Y)$ . And, in general, any value between those two distributions is possible. 

# 

identiﬁability 

Thus, given that latent variables are inevitable in many settings, it appears that the situation regarding causal queries is hopeless. Fortunately, as we now show, we can sometimes answer causal questions in models involving latent variables using observed correlations alone . More precisely, in this section, we attempt to address to question of which intervention queries are identiﬁable , that is, can be answered using only conditional probabilities involving observable variables. Probabilities over observed variables can be estimated from data or elicited from an expert. Thus, if we can reduce the answer to a query to an expression involving only such probabilities, we may be able to provide a robust and accurate answer to it. 

# 21.3.1 Query Simpliﬁcation Rules 

The key observation in this section is that the structure of a causal model give rise to certain equivalence rules over interventional queries, which allow one query to be replaced by an equiv- alent one that may have a simpler form. By applying one or more of these simpliﬁcation steps, we may be able to convert one causal query to another query that involves no interventions, and can therefore be answered using observational data alone. 

augmented causal model decision variable 

These rules can be deﬁned in terms of an augmented causal model that encodes the possible efect of interventions explicitly within the graph structure. More precisely, we view the process of rventi in terms of a new decision variable (see ch ter 23) $\widehat{Z}$ that determines whether we in $Z$ , a o, what its value is. The variable $\hat{\widehat{Z}}$ b takes on values in $\{\epsilon\}\cup V a l(Z)$ . If $\widehat{Z}=\epsilon$ b , then Z behaves as a random variable whose distribution is determined by its usual CPD $P(Z\mid\mathrm{Pa}_{Z})$ | ; if ${\widehat{Z}}=z$ b , then it deterministic ally sets the value of $Z$ to be $z$ with probability 1 . Let $\widehat{\boldsymbol{Z}}$ denote the set $\{{\widehat{Z}}\ :\ Z\in Z\}$ ∈ } . 

Note that, in those cases where $Z$ ’s value is deterministic ally set by one parent, all of $Z$ ’s other parents $U$ become irrelevant, and so we can efectively remove all edges from $U$ to $Z$ . Let $\mathcal{G}^{\dagger}$ be the augmented model for $\mathcal{G}$ . Let $\mathcal{G}_{\overline{{Z}}}^{\dagger}$ be the graph obtained from $\mathcal{G}^{\dagger}$ except that every $Z\in Z$ has only the single parent $\widehat{Z}$ . Note that $\mathcal{G}_{\overline{{Z}}}^{\dagger}$ is similar to the mutilated network used in deﬁnition 21.1 to deﬁne the semantics of intervention queries; the only diference is that we now make the interventions themselves explicit. As we will now see, this diference allows us to study the efect of one intervention within a model that contain others. 

intervention query simpliﬁcation 

Proposition 21.1 

Based on this construction, we now deﬁne three query simpliﬁcation rules. The ﬁrst simply allows us to insert or delete observations into a query. 

Let $\mathcal{C}$ be a causal model over the graph structure $\mathcal{G}$ . Then: 

$$
P(Y\mid{\mathrm{do}}(Z:=z),X=x,W=w)=P(Y\mid{\mathrm{do}}(Z:=z),X=x),
$$ 

if $W$ is $d$ -separated from $Y$ given $Z,X$ in the graph $\mathcal{G}_{\overline{{Z}}}^{\dagger}.$ . 

This rule is a simple consequence of the fact that probabilities of intervention queries are deﬁned relative to the graph $\mathcal{G}_{\overline{{Z}}}^{\dagger},$ , and the correspondence between independence and $\mathrm{d}$ -separation in this graph. 

The second rule is subtler, and it allows us to replace an intervention with the corresponding observation. 

Proposition 21.2 Let $\mathcal{C}$ be a causal model over the graph structure $\mathcal{G}$ . Then: 

$$
P(Y\mid\operatorname{do}(Z:=z),\operatorname{do}(X:=x),W=w)=P(Y\mid\operatorname{do}(Z:=z),X=x,W=y).
$$ 

if $Y$ is $d$ -separated from $\widehat{X}$ given $X,Z,W$ in the graph $\mathcal{G}_{\overline{{Z}}}^{\dagger}$ . 

requisite CPD Intuitively, this rule holds because it tells us that we get no more information regarding $Y$ from the fact that an intervention took place at $X$ than the values $_{_{x}}$ themselves. In other words, knowing that $X\,=\,x$ , we do not care whether these values were obtained as a result of an intervention or not. This criterion is also equivalent to asking whether $X$ have requisite CPD for the query $P(Y\mid d o(Z:=z),X=x,W=w)$ ) , as n exercise 3.20. The relationship is not surprising, because an intervention at a variable $X\in X$ ∈ co sponds exactly to changing its CPD; if our query is oblivious to changes in the CPD (given X ), then we should not care whether there was an intervention at $X$ or not. 

As a simple example, consider the case where $Z=\emptyset$ and $W=\emptyset$ , and where we have single variables $X,Y$ . In this case, the rule reduces to the assertion that 

$$
P(Y\mid d o(X:=x))=P(Y\mid X=x),
$$ 

if $Y$ is $\mathrm{d}$ -separ ed fr $\widehat{X}$ iven $X$ in the graph $\mathcal{G}^{\dagger}$ trails between X and $Y$ in $\mathcal{G}$ emanate causally from $X$ (that is, go thr gh its children). Indeed, in this case, intervening at X has the same efect on Y as observing X . Conversely, if we have an active ail from $\widehat{X}$ to $Y$ given $X$ , then it must go through a v-structure activated by $X$ . In this case, Y $Y$ is not a descendant of $X$ , and an observation of $X$ has a very diferent efect than an intervention at X . The proof of this theorem is left as an exercise (exercise 21.1). 

Example 21.11 Consider again the network of ﬁgure 21.1b, and assume that the student somehow manages to cheat on the SAT exam and get a higher SAT score. We are interested in the efect on the student’s grade, that is, in evaluating a query of the form $P(G\mid\mathrm{\mathrm{d}}\mathrm{o}(S),J,I)$ . Consider the augmented mod $\mathcal{G}^{\dagger}$ ren $\widehat{S}$ b for the node $S$ . In this graph, we would have that G is d-separated from $\widehat{S}$ b given S , J and I . Thus, the theorem would allow us to conclude that $P(G\mid{\mathrm{do}}(S),J,I)\,=\,P(G\mid S,J,I)$ | | . To provide another tuitio fo note riginal graph, there are two possible trails between G and S : G $G\,\rightarrow\,J\,\leftarrow\,S$ → ← and $G\gets I\to S$ ← → . The efects of the ﬁrst t il remain unchanged i the mutilated netwo o diference between an intervention at S and an observation at S , r ail $G\rightarrow J\leftarrow S$ → ← . There is a diference between hese two cases relative to the trail G $G\,\leftarrow\,I\,\rightarrow\,S$ ← → , but that tr l is blocked by the observation at I , and hence there is no efect to changing the intervention at S to an observation. Note that this last argument would not apply to the query $P(G\mid{\mathrm{d}}\mathbf{o}(S),J)$ , and indeed, the theorem would not allow us to conclude that $P(G\mid\mathrm{do}(S),J)=P(G\mid S,J)$ . 

The ﬁnal rule allows us to introduce or delete interventions, in the same way that proposi- tion 21.1 allows us to introduce or delete observations. 

Proposition 21.3 Let $\mathcal{C}$ be a causal model over the graph structure $\mathcal{G}$ . Then: 

$$
P(Y\mid{\mathrm{do}}(Z:=z),{\mathrm{do}}(X:=x),W=w)=P(Y\mid{\mathrm{do}}(Z:=z),W=w),
$$ 

if $Y$ is $d$ -separated from $\widehat{X}$ given $Z,W$ in the graph $\mathcal{G}_{\overline{{Z}}}^{\dagger}.$ . 

This analysis can also be interpreted in terms of requisite CPDs. Here, the premise is equivalent to stating that the CPDs of the variables in $X$ are not requisite for the query even when their values are not observed. In this case, we can ignore both the knowledge of the intervention and the knowledge regarding the values imposed by the intervention. 

Again, we can obtai intuition by considering the simpler case where $Z=\emptyset$ , $W=\emptyset$ , and $X$ is a single variable X . Here, the rule reduces to: 

$$
P(Y\mid d o(X:=x))=P(Y),
$$ 

if $Y$ is $\mathrm{d}$ -separated from $\widehat{X}$ in the graph $\mathcal{G}^{\dagger}$ . The intuition behind this rule is fair straight- forward. Conditioning on $\widehat{X}$ b corresponds to changing the causal mechanism for X . If the d-separation condition holds, this operation provably has no efect on $Y$ . From a diferent per- spective, changing the causal mechanism for $X$ can only afect $Y$ causally, via $X$ ’s children. If ere e no trails from $\widehat{X}$ to $Y$ without conditioning on $X$ , then there are no causal paths from $X$ to Y . Not surprisingly, this condition is equivalent to the graphical criterion for identifying requisite probability nodes (see exercise 21.2), which also test whether the CPD of a variable $X$ can afect the outcome of a given query; in this case, the CPDs of the variable $X$ is determined by whether there is an intervention at $X$ . 

Example 21.12 (so that now $W\,=\,J_{.}$ ), then $G$ itself is an ancestor of our evidence J . In this case, there is an $\widehat{G}$ to $S$ in the network; hence, the rule does not apply, and we cannot conclude $P(S\mid\operatorname{do}(G),J)=P(S\mid J)$ | . Again, this is as w would expect, because when we observe $J$ , the fact that we intervened at G is clearly relevant to S , due to standard intercausal reasoning. 

# 21.3.2 Iterated Query Simpliﬁcation 

The rules in the previous section allow us to simplify a query in certain cases. But their applicability appears limited, since there are many queries where none of the rules apply directly. A key insight, however, is that we can also perform other transformations on the query, allowing the rules to be applied. 

Example 21.13 To illustrate this approach, consider again the example of example 21.8, which involves the query $P(J\mid\operatorname{do}(G))$ in the network of ﬁgure 21.1b. As we discussed, directly to this query: Obviously we cannot eliminate the intervention — $P(J\mid\operatorname{do}(G))\neq P(J)$ | ̸ . We also cannot turn the intervention nto an obser tion $-\operatorname{\rho}P(J\mid\operatorname{do}(G))\neq P(J\mid G)$ ; intuitively, t reason is that i ervening at G only af $J$ edge $G\rightarrow J$ → , where s conditioning G also inﬂuence $J$ by the indirect trail G $G\gets I\to S\to J$ ← → → . This trail is called a back-door trail , since it leaves G by the “back door.” 

However, we can use standard probabilistic reasoning to conclude that: 

$$
P(J\mid{\mathrm{do}}(G))=\sum_{S}P(J\mid{\mathrm{do}}(G),S)P(S\mid{\mathrm{do}}(G)).
$$ 

Both of the terms in the summation can be further simpliﬁed. For the ﬁrst term, we have that the o active rail from $G$ to $J$ is now the direct edge $G\rightarrow J$ . More formally, $J$ is $d$ -separated from G given S in the graph where outgoing arcs from G have been deleted. Thus, we can apply proposition 21.2, and conclude: 

$$
P(J\mid\operatorname{do}(G),S)=P(J\mid G,S).
$$ 

For the second term, we have already shown in example 21.12 that $P(S\mid\mathrm{do}(G))=P(S)$ . Putting the two together, we obtain that: 

$$
P(J\mid{\mathrm{do}}(G))=\sum_{S}P(J\mid G,S)P(S).
$$ 

This example illustrates a process whereby we introduce conditioning on some set of variables: 

$$
P(Y\mid d o(X),Z)=\sum_{W}P(Y\mid d o(X),Z,W)P(W\mid d o(X),Z).
$$ 

Even when none of the transformation rule apply to the query $P(Y\mid d o(X),Z)$ , they may apply to each of the two terms in the summation of the transformed expression. 

back-door trail 

back-door criterion 

The example illustrates one special case of this transformation. A back-door trail from $X$ to $Y$ is an active trail that leaves $X$ via a p t of $X$ . For a query $P(Y\mid d o(X))$ , a set $W$ satisﬁes the back-door criterion if no node in W is a descendant of X , and W blocks all back-door paths from $X$ to $Y$ . Using an argument identical to the one in the example, we can show that if a set $W$ satisﬁes the back-door criterion for a query $P(Y\mid d o(X))$ , then 

$$
P(\boldsymbol{Y}\mid d o(\boldsymbol{X}))=\sum_{\boldsymbol{W}}P(\boldsymbol{Y}\mid\boldsymbol{X},\boldsymbol{W})P(\boldsymbol{W}).
$$ 

The back-door criterion can be used to address Simpson’s paradox, as described in exam- ple 21.9. 

Consider gain e query $P(c^{1}\mid\mathrm{{do}}(d^{1}))$ . The variable $G$ (Gender) introduces a back-door trail between C and D . We can account for its inﬂuence using equation (21.1): 

$$
P(c^{1}\mid\operatorname{do}(d^{1}))=\sum_{g}P(c^{1}\mid d^{1},g)P(g).
$$ 

We therefore obtain that: 

$$
{\begin{array}{l l l}{P(c^{1}\mid{\mathrm{do}}(d^{1}))}&{=}&{0.7\cdot0.5+0.2\cdot0.5=0.45}\\ {P(c^{1}\mid{\mathrm{do}}(d^{0}))}&{=}&{0.8\cdot0.5+0.4\cdot0.5=0.6.}\end{array}}
$$ 

And therefore, we should not prescribe the drug. 

More generally, by repeated application of these rules, we can sometimes simplify fairly complex queries, obtaining answers in cases that are far from obvious at ﬁrst glance. 

Box 21.A — Case Study: Identifying the Efect of Smoking on Cancer. In the early 1960s, fol- lowing a signiﬁcant increase in the number of smokers that occurred around World War II, people began to notice a substantial increase in the number of cases of lung cancer. After a great many studies, a correlation was noticed between smoking and lung cancer. This correlation was noticed in both directions: the frequency of smokers among lung cancer patients was substantially higher than in the general population, and the frequency of lung cancer patients within the population of smokers was substantially higher than within the population of nonsmokers. 

These results, together with some experiments of injecting tobacco products into rats, led the Surgeon General, in 1964, to issue a report linking cigarette smoking to cancer and, most particularly, lung cancer. His claim was that the correlation found is causal, namely: If we ban smoking, the rate of cancer cases will be roughly the same as the one we ﬁnd among nonsmokers in the population. 

These studies came under severe attacks from the tobacco industry, backed by some very promi- nent statisticians. The claim was that the observed correlations can also be explained by a model in which there is no causal connection between smoking and lung cancer. Instead, an unobserved genotype might exist that simultaneously causes cancer and produces an inborn craving for nicotine. In other words, there were two hypothesized models, shown in ﬁgure 21.A.1a,b. 

The two models can express precisely the same set of distributions over the observable variables $S,C$ . Thus, they can do an equally good job of representing the empirical distribution over these variables, and there is no way to distinguish between them based on observational data alone. Moreover, both models will provide the same answer to standard probabilistic queries such as $P(c^{1}\mid$ 

![](images/9741ba02e003206c88ba745540659d72f3666695bad8afdf876f9da668401f1f.jpg) 
Figure 21.A.1 — Three candidate models for smoking and cancer. (a) a direct causal inﬂuence; (b) indirect inﬂuence via a latent common parent Genotype ; (c) incorporating both types of inﬂuence. 

$s^{1})$ . However, relative to interventional queries, these models have very diferent consequences. According to the Surgeon General’s model, we would have: 

$$
P(c^{1}\mid\operatorname{do}(S:=s^{1}))=P(c^{1}\mid s).
$$ 

In other words, if we force people to smoke, their probability of getting cancer is the same as the probability conditioned on smoking, which is much higher than the prior probability. On the other hand, according to the tobacco industry model, we have that 

$$
P(c^{1}\mid\mathrm{do}(S:=s^{1}))=P(c^{1}).
$$ 

In other words, making the population smoke or stop smoking would have no efect on the rate of cancer cases. 

Pearl (1995) proposes a formal analysis of this dilemma, which we now present. He proposes that we combine these two models into a single joint model, which accommodates for both possible types of interactions between smoking and cancer, as shown in ﬁgure 21.A.1c. We now need to assess, from the marginal distribution over the observed variables alone, the parameter iz ation of the various links. Unfortunately, it is impossible to determine the parameters of these links from observational data alone, since both of the original two models (in ﬁgure 21.A.1a,b) can explain the data perfectly. 

However, if we reﬁne the model somewhat, introducing an additional assumption, we can provide such estimates. Assume that we determine that the efect of smoking on cancer is not a direct one, but occurs through the accumulation of tar deposits in the lungs, as shown in ﬁgure 21.A.2a. Note that this model makes the assumption that the accumulation of tar in the lungs is not directly afected by the latent Genotype variable. As we now show, if we can measure the amount of tar deposits in the lungs of various individuals (for example, by $X$ -ray or in autopsies), we can determine the probability of the intervention query $P(c^{1}\mid\mathrm{d}o(s^{1}))$ using observed correlations alone . 

We are interested in $P(c^{1}\mid\mathrm{d}o(s^{1}))$ , which is an intervention query whose mutilated network is $\mathcal{G}_{\overline{{S}}}^{\dagger}$ in ﬁgure 21.A.2b. Standard probabilistic reasoning shows that: 

$$
P(C\mid{\mathrm{d}}{\mathsf{o}}(s^{1}))\quad=\quad\sum_{t}P(C\mid{\mathrm{d}}{\mathsf{o}}(s^{1}),t)P(t\mid{\mathrm{d}}{\mathsf{o}}(s^{1})).
$$ 

We now consider and simplify each term in the summation separately. 

![](images/a1f050a1203381d502b80de71ebd3652c0744c5cb83988e9f0af52312dce20e1.jpg) 
Figure 21.A.2 — Determining causality between smoking and cancer. Augmented network for a model in which the efect of smoking on cancer is indirect, mediated via tar accumulation in the lungs, and mutilated variants for two possible interventions. 

The second term, which measures the efect of Smoking on Tar, can be simpliﬁed directly using our rule f conve ing interventi to observations, stated in proposition 21.2. Here, $\widehat{S}$ is $d$ -separated from T given S in the graph $\mathcal{G}^{\dagger}$ , shown in ﬁgure 21.A.2a. It follows that: 

$$
P(t\mid\operatorname{do}(s))\quad=\quad P(t\mid s).
$$ 

Intuitively, the only active trail from $\widehat{S}$ to $T$ goes via $S$ , and the efect of that trail is identical regardless of whether we condition on S or intervene at $S$ . 

Now, let us examine the ﬁrst term, $P(C\mid{\mathrm{d}}\mathbf{o}(s^{1}),t)$ | , which measures the efect of Tar on Cancer in the presence of our intervention on S . Unfortunately, we cannot directly convert the intervention at $S$ t n observation, s ce $C$ is not $d$ -separated fro $\widehat{S}$ given $S,T$ in $\mathcal{G}^{\dagger}$ However, we can convert the observation at T to an intervention, because C is $d$ -separated from $\widehat{T}$ b given $S,T$ in the graph G $\mathcal{G}_{\overline{{S}}}^{\dagger}$ . 

$$
P(C\mid{\mathrm{do}}(s^{1}),t)=P(C\mid{\mathrm{do}}(s^{1}),{\mathrm{do}}(t)).
$$ 

We can now eliminate the intervention at $S$ from this expression using proposition 21.3, which applies because $C$ is $d$ -separated from $\widehat{S}$ given $T$ in the graph $\mathcal{G}_{\overline{{T}}}^{\dagger}$ (ﬁgure 21.A.2c), obtaining: 

$$
P(C\mid{\mathrm{do}}(s^{1}),{\mathrm{do}}(t))=P(C\mid{\mathrm{do}}(t)).
$$ 

Considering this last expression, we can apply standard probabilistic reasoning and introduce conditioning on $S$ : 

$$
{\begin{array}{r c l}{P(C\mid{\mathrm{do}}(t))}&{=}&{\displaystyle\sum_{s^{\prime}}P(C\mid{\mathrm{do}}(t),s^{\prime})P(s^{\prime}\mid{\mathrm{do}}(t))}\\ &{=}&{\displaystyle\sum_{s^{\prime}}P(C\mid t,s^{\prime})P(s^{\prime}\mid{\mathrm{do}}(t))}\\ &{=}&{\displaystyle\sum_{s^{\prime}}P(C\mid t,s^{\prime})P(s^{\prime}).}\end{array}}
$$ 

The second equality is an application of proposition 21.2, which applies because $C$ is $d$ -separated from $\widehat{T}$ given $T,S$ in $\mathcal{G}^{\dagger}$ . The ﬁnal equality is a consequence of proposition 21.3, which holds because $S$ is $d$ -separated from $\widehat{T}$ in $\mathcal{G}^{\dagger}$ . 

Putting everything together, we get: 

$$
\begin{array}{r c l}{{P(c\mid\mathrm{do}(s^{1}))}}&{{=}}&{{\displaystyle\sum_{t}P(c\mid\mathrm{do}(s^{1}),t)P(t\mid\mathrm{do}(s^{1}))}}\\ {{}}&{{=}}&{{\displaystyle\sum_{t}P(c\mid\mathrm{do}(s^{1}),t)P(t\mid s^{1})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{t}P(t\mid s^{1})\sum_{s^{\prime}}P(c\mid t,s^{\prime})P(s^{\prime}).}}\end{array}
$$ 

Thus, if we agree that tar in the lungs is the intermediary between smoking and lung cancer, we can uniquely determine the extent to which smoking causes lung cancer even in the presence of a confounding latent variable! 

Of course, this statement is more useful as a thought experiment than as a practical computa- tional tool, both because it is unlikely that smoking afects cancer only via tar accumulation in the lungs and because it would be very difcult in practice to measure this intermediate variable. Nevertheless, this type of analysis provides insight on the extent to which understanding of the underlying causal model allows us to identify the value of intervention queries even in the presence of latent variables. 

bow pattern 

These rules provide a powerful mechanism for answering intervention queries, even when the causal model involves latent variables (see, for example, box 21.A). More generally, using these rules, we can show that the query $P(Y\mid d o(x))$ is identiﬁable in each of the models shown in ﬁgure 21.3 (see exercise 21.4). In these ﬁgures, we have used a bidirected dashed arrow, known as a bow pattern , to denote the existence of a latent common cause between two variables. For example, the model in (d) has the same structure as in our Smoking model of ﬁgure 21.A.2a. (box 21.A). This notation simpliﬁes the diagrams considerably, and it is therefore quite commonly used. Note that none of the diagrams contain a bow pattern between $X$ and one of its children. In general, a necessary condition for identiﬁability is that the graph contain no bow pattern between $X$ and a child of $X$ that is an ancestor of $Y$ . The reason is that, if such a bow pattern exists between $X$ and one of its children $W$ , we have no mechanism for distinguishing X ’s direct inﬂuence on $W$ and the indirect correlation induced by the latent variable, which is their common parent. 

It is interesting to note that, in the models shown in (a), (b), and (e), $Y$ has a parent $Z$ whose efect on $Y$ is not identiﬁable, yet the efect of $X$ on $Y$ , including its efect via $Z$ , is identiﬁable. For example, in (a), $P(Y\mid X)=P(Y\mid d o(X))$ , and so there is no need to disentangle the inﬂuence that ﬂows through the $Z\rightarrow Y$ → edge, and the inﬂuence that ﬂows through the bow pattern. These examples demonstrate that, to identify the inﬂuence of one variable on another, it is not necessary to identify every edge involved in the interactions between them. 

Figure 21.4 shows a few examples of models where the inﬂuence of $X$ on $Y$ is not identi- ﬁable. Interestingly, the model in (g) illustrates the converse to the observation we just stated: identiﬁcation of every edge involved in the interaction between $X$ and $Y$ does not sufce to 

![](images/0db178d8a0998d0ddd42185cab4534454084924b500a11c4f74220f75109bf0c.jpg) 
Figure 21.3 Examples of models where $P(Y\mid d{\pmb o}(X))$ is identiﬁable. The bidirected dashed arrows denote cases where a latent variable afects both of the linked variables. 

![](images/0d7aba846a8ac28031aabcf996ae7fcb8e229e1371130da715c3a2b63f1c5eb9.jpg) 
Figure 21.4 Examples of models where $P(Y\mid d{\pmb o}(X))$ is not identiﬁable. The bidirected dashed arrows denote cases where a latent variable afects both of the linked variables. 

t ticula , we can identify all of $P(Z_{1}\mid d o(X))$ $P(Z_{2}\mid d o(X))$ | , $P(Y\mid d o(Z_{1}))$ | , and $P(Y\mid d o(Z_{2}))$ | , but we cannot identify $P(Y\mid d o(X))$ | (see exercise 21.5). 

Note that the removal of any edge (directed or bidirected) from a causal diagram of this type can only help in the identiﬁability of causal efects, since it can only deactivate active trails. Conversely, adding edges can only reduce identiﬁability. Hence, any subgraph of the graphs in ﬁgure 21.3 is also identiﬁable, and any extension of the graphs in ﬁgure 21.4 is nonidentiﬁable. Moreover, it is possible to show that the graphs in ﬁgure 21.3 are maximal, in the sense that adding any edge renders $P(Y\;\mid\;d o(X))$ unidentiﬁable; similarly, the graphs in ﬁgure 21.4 are minimal, in that the query is identiﬁable in any of their (strict) subgraphs. Similarly, the introduction of mediating observed variables onto any edge in a causal graph — transforming an arc $A\rightarrow B$ into a path $A\rightarrow Z\rightarrow B$ for a new observed variable $Z$ — can only increase our ability to identify causal efects. 

Finally, we note that the techniques in this section provide us with methods for answering only queries that are identiﬁable. Unfortunately, unidentiﬁable queries are quite common in practice. In section 21.5, we describe methods that allow us to provide partial answers for queries that are not identiﬁable. 

# 21.4 Mechanisms and Response Variables $\star$ 

The underlying intuition for our deﬁnition of a causal model is that the graph structure deﬁnes a causal progression, where the value of each variable is selected, in order, based on the values of its parents, via some causal mechanism. So far, the details of this causal mechanism have remained implicit. We simply assumed that it induces, for each variable $X$ , a conditional probability distribution $P(X\mid\mathrm{Pa}_{X})$ . 

This approach sufces in cases, as in the previous section, where we can compute the value of a causal query in terms of standard probabilistic queries. In general, however, this identiﬁcation may not be possible. In such cases, we have to reason explicitly about the mechanism governing the behavior of the variables in the model and the ways in which the latent variables may inﬂuence the observed variables. By obtaining a ﬁner-grained analysis of these mechanisms, we may be able to provide some analysis for intervention queries that are not identiﬁable from probabilities over the observable variables alone. 

A second reason for understanding the mechanism in more detail is to answer additional types of queries: counterfactual queries, and certain types of diagnostic queries. As we discussed, our intuition for a counterfactual query is that we want to keep as much as we can between the real and the counterfactual world. By understanding the exact mechanism that governs each variable, we can obtain more reliable inferences about what took place in the true world, so as to preserve as much as possible when reasoning about the counterfactual events. 

populations of companies. Those in the ﬁrst population are not in hiring mode, and they always reject Gump without looking at his grade. Companies in the second population are desperate for employees and will take anyone, regardless of their grade. The basic intuition of counterfactual queries is that, when we move to the counterfactual world, Gump does not get another “random draw” of company. We want to answer the counterfactual query assuming that Acme’s recruiting response model is the same in both cases. Under this assumption, we can conclude that Acme was not in hiring mode, and that Gump’s outcome would have been no diferent had he managed to change his grade. 

In a second model, there are also two (equally likely) populations of companies. In the ﬁrst, the company is highly selective, and it hires a student if and only if his grades are high. In the second population, the recruiter likes to hire underdogs because he can pay them less, and he hires Gump if and only if his grades are low. In this setting, if we again preserve the hiring response of the company, we would conclude that Acme must use a selective recruiting policy, and so Gump would have been hired had he managed to change his grade to a high one. 

Note that these two models are identical from a probabilistic perspective. In both cases, $P(J\mid$ $G)=(0.5,0.5)$ . But they are very diferent with respect to answering counterfactual queries. 

troubleshooting 

endogenous variable 

The same type of situation applies in a setting where we act in the world, and where we wish to reason about the probabilities of diferent events before and after our action. For example, consider the problem of troubleshooting a broken device with multiple components, where a fault in each can lead to the observed failure mode. Assume that we replace one of the components, and we wish to reason about the probabilities in the system after the replacement action. Intuitively, if the problem was not with the component we replaced, then the replacement action should have no efect: the probability that the device remains broken should be 1. The mechanism by which diferent combinations of faults can lead to the observed behavior is quite complex, and we are generally uncertain about which one is currently the case. However, we wish to have this mechanism persist between the prerepair and postrepair state. Box 21.C describes the use of this approach in a real-world diagnostic setting. 

As these examples illustrate, to answer certain types of queries, we need to obtain a detailed speciﬁcation of the causal mechanism that determines the values of diferent variables in the model. In general, we can argue that (quantum efects aside) any randomness in the model is the cumulative efect of latent variables that are simply unmodeled. For example, consider even a seemingly random event such as the outcome of a coin toss. We can imagine this event depending on a large number of exogenous variables, such as the rotation of the coin when it was tossed, the forces applied to it, any air movement in the room, and more. Given all of these factors, the outcome of the coin toss is arguably deterministic. As another example, a company’s decision on whether to hire Gump can depend on the company’s current goals and funding level, on the existence of other prospective applicants, and even on whether the recruiter likes the color of Gump’s tie. Based on this intuition, we can divide the variables into two groups. The endogenous variables are those that we choose to include in the model; the exogenous variables are unmodeled, latent variables. We can now argue, as a hypothetical thought experiment, that the exogenous variables encompass all of the stochasticity in the world; therefore, given all of the exogenous variables, each endogenous variable (say the company’s hiring decision) is fully determined by its endogenous parents (Gump’s grade in our example). 

# 

Example 21.16 

Clearly, we cannot possibly encode a complete speciﬁcation of a causal model with all of the relevant exogenous variables. In most cases, we are not even aware of the entire set of relevant exogenous variables, far less able to specify their inﬂuence on the model. However, for our purposes, we can abstract away from speciﬁc exogenous variables and simply focus on their efect on the endogenous variables. To understand this transformation, consider the following example. 

Consider again the simple setting of example 21.15. Let $U$ be the entire set of exogenous variables afecting the utcome of the variable $J$ in the causal model $G\,\rightarrow\,J$ . We can n assume t t the value of J is a deterministic function of $G,U$ — for each assignment $\mathbfit{u}$ to U and $g$ to G , the variable $J$ deterministic ally takes the value $f_{J}(g,\pmb{u})$ . We are interested in modeling only the interaction between $G$ and $J$ . Thus, we can partition the set of assignments $\mathbfit{u}$ into four classes, based on the mapping $\mu$ that they induce between $G$ and $J$ : 

• $\mu_{1\mapsto1,0\mapsto1}^{J}$ — those where $f_{J}({\pmb u},g^{1})=j^{1}$ and $f_{J}({\boldsymbol u},g^{0})=j^{1}$ ; these are the “always hire” 7→ 7→ cases, where Gump is hired regardless of his grade. • $\mu_{1\mapsto1,0\mapsto0}^{J}$ — those where $f_{J}({\pmb u},g^{1})\ =\ j^{1}$ and $f_{J}({\boldsymbol u},g^{0})\;=\;j^{0}$ ; these are the “rational 7→ 7→ recruiting” cases, where Gump is hired if and only if his grade is high. • $\mu_{1\mapsto0,0\mapsto1}^{J}$ — those where $f_{J}({\boldsymbol{\mathbf{\mathit{u}}}},{\boldsymbol{g}}^{1})\,=\,j^{0}$ and $f_{J}({\boldsymbol u},g^{0})\,=\,j^{1}$ ; these are the “underdog” 7→ 7→ cases, where Gump is hired if and only if his grade is low. • $\mu_{1\mapsto0,0\mapsto0}^{J}$ — those where $f_{J}({\boldsymbol{\mathbf{\mathit{u}}}},{\boldsymbol{g}}^{1})\,=\,j^{0}$ and $f_{J}({\boldsymbol u},g^{0})\,=\,j^{0}$ ; these are the “never hire” 7→ 7→ cases, where Gump is not hired regardless of his grade. 

Each assignment $\mathbfit{u}$ induces precisely one of these four diferent mappings between $G$ and $J$ . For example, we might have a situation where, if Gump wears $a$ green tie with pink elephants, he is never hired regardless of his grade, which is the last case in the previous list. 

For the purposes of reasoning about the interaction between $G$ and $J$ , we can abstract away from modeling speciﬁc exogenous variables $U$ , and simply reason about how likely we are to encounter each of the four categories of functions induced by assignments $\mathbfit{u}$ . 

Generalizing from this example, we deﬁne as follows: 

# Deﬁnition 21.2 

response variable 

# Example 21.17 

Let $X$ be a variable and $Y$ be a set of parents. $A$ response variable for $X$ given $Y$ is a variable $U^{X}$ whose domain is the set of all possible functions $\mu(Y)$ from $V a l(\pmb{Y})$ to $V a l(X)$ . That is, let $\pmb{y}_{1},\cdot\cdot\cdot,\pmb{y}_{m}$ $V a l(Y)$ (for $m=|V a l(Y)|)$ . For a tuple of (generally not distinct) values $x_{1},\ldots,x_{m}\in V a l(X)$ ∈ , we use $\mu_{(\pmb{y}_{1},\dots,\pmb{y}_{m})\mapsto(x_{1},\dots,x_{m})}^{X}$ to denote the function that 7→ assigns $x_{i}$ to $\mathbf{\boldsymbol{y}}_{i},$ , for $i=1,\ldots,m$ . The domain of $U^{X}$ contains one such function for each such tuple, giving a total of $k^{m}$ functions of this form (for $|\mathit{V a l}(X)|=k_{\lambda}$ ). 

In example 21.16, we introduce two response variables, $U^{G}$ and $U^{J}$ . Because $G$ has no endogenous parents, the variable $U^{G}$ is degenerate: its values are simply $g^{1}$ and $g^{0}$ . The response variable $U^{J}$ takes one of the four values $\mu_{1\mapsto1,1\mapsto1}^{J},\,\mu_{1\mapsto1,1\mapsto0}^{J},\,\mu_{1\mapsto0,1\mapsto1}^{J},\,\mu_{1\mapsto0,1\mapsto0}^{J}$ , each of which deﬁnes a 7→ 7→ 7→ 7→ 7→ 7→ 7→ 7→ function from $G$ to $J$ . Thus, given $U^{J}$ and $G$ , the value of $J$ is fully determined. For example, if $U^{J}=\mu_{1\mapsto1,1\mapsto0}^{J}$ , and $G=g^{1}$ , then $J=j^{1}$ . 

# Deﬁnition 21.3 

functional causal model 

$A$ functional causal model $\mathcal{C}$ over et of endogenous variables $\mathcal{X}$ is a er two sets of va the variables X , and sponse variables U ${\mathcal{U}}=\{U^{X}~:~X\in{\mathcal{X}}\}$ { X Each ble $X\in{\mathcal{X}}$ ∈X has a set of paren $\mathrm{Pa}_{X}\subset\mathcal{X}$ ⊂X , and a response v arent $U^{X}$ $X$ given $\mathrm{Pa}_{X}$ . The model for each variable X $X\in{\mathcal{X}}$ ∈X is deterministic: When $U^{X}=\mu$ and $\mathrm{Pa}_{X}={\pmb{y}}.$ , then $X=\mu({\pmb y})$ with probability 1. 

The model $\mathcal{C}$ also deﬁ s a joint probability distributio over the response va ﬁned a Ba rk over U . Thus, each response variable U has a set of parents $\mathrm{Pa}_{U}\subset\mathcal{U}$ ⊂U , and a CPD $P(U\mid\mathrm{Pa}_{U})$ | . 

Functional causal models provide a much ﬁner-grained speciﬁcation of the underlying causal mechanisms than do standard causal models, where we only specify a CPD for each endogenous variable. In our $G\ \rightarrow\ J$ example, rather than specifying a CPD for $J$ , we must specify a distribution over $U^{J}$ . A table representation of the CPD has two independent parameters — one for each assignment to $G$ . The distribution $P(U^{J})$ has three independent parameters — there are four possible values for $U^{J}$ , and their probabilities must sum to 1 . While, in this case, the blowup in the representation may not appear too onerous, the general case is far worse. In general, consider a variable $X$ with parents $Y$ , and let $k=|\mathit{V a l}(X)|$ and $m=|V a l(\pmb{Y})|$ . The total number of possible mappings from $V a l(\pmb{Y})$ to $V a l(X)$ is $k^{m}$ — each function selects one of X ’s $k$ values for each of the $m$ assignments to $Y$ . Thus, the total number of independent parameters in (an explicit representation of) $P(U^{X})$ is $k^{m}-1$ . By comparison, a table-CPD $P(X\mid Y)$ requires $m(k-1)$ independent parameters only. 

# Example 21.18 

Consider the network in ﬁgure $2l.5a,$ representing a causal model for a randomized clinical trial, where some patients are randomly assigned a medication and others a placebo. The model contains three binary-valued endogenous variables: $A$ indicates the treatment assigned to the patient; $T$ indicates the treatment actually received; and $O$ indicates the observed outcome (positive or nega- tive). The model contains three response variables: $U^{A}$ , $U^{T}$ , and $U^{O}$ . Intuitively, $U^{A}$ (which has $^a$ uniform distribution) represents the stochastic event determining the assignment to the two groups (medication and placebo); $U^{T}$ determines the patient’s model for complying with the prescribed course of treatment; and $U^{O}$ encodes the form of the patient’s response to diferent treatments. The domain of $U^{T}$ consists of the following four functions: 

• $\mu_{1\mapsto1,0\mapsto1}^{T}$ — always taker; 7→ 7→ • $\mu_{1\mapsto1,0\mapsto0}^{T}$ — complier (takes medicine if and only if prescribed); 7→ 7→ • $\mu_{1\mapsto0,0\mapsto1}^{T}$ — deﬁer (takes medicine if and only if not prescribed); 7→ 7→ • $\mu_{1\mapsto0,0\mapsto0}^{T}$ — never taker. 7→ 7→ 

Similarly, the domain of $U^{O}$ consists of the following four functions: 

• $\mu_{1\mapsto1,0\mapsto1}^{O}$ — always well; 7→ 7→ • $\mu_{1\mapsto1,0\mapsto0}^{O}$ — helped (recovers if and only if takes medicine); 7→ 7→ • $\mu_{1\mapsto0,0\mapsto1}^{O}$ — hurt (recovers if and only if does not take medicine); 7→ 7→ • $\mu_{1\mapsto0,0\mapsto0}^{O}$ — never well. 

![](images/0e183fb824bef3a295439ee1a403e6151a99e122e3f68add666a5c5b2e3c0de8.jpg) 
Figure 21.5 A simple functional causal model for a clinical trial. (a) Network structure. (b) Sample CPDs and resulting joint distribution for the response variables $U^{T}$ and $U^{O}$ . 

The model makes explicit two important assumptions. First, the assigned treatment $A$ does not inﬂuence the response $O$ directly, but only through the actual treatment $T$ ; this conditional independence is achieved by the use of a placebo in the control group, so that the patients do not know to which of the two groups they were assigned. The second assumption is that $A$ is marginally independent from $\{U^{T},U^{O}\}$ , w h is e red via the randomization of the assignment to the two groups. Note, however, that U $U^{T}$ and $U^{O}$ are not independent, reﬂecting the fact that factors determining a patient’s decision to comply with the treatment can also afect his ﬁnal outcome. (For example, patients who are not very ill may neglect to take their medication, and may be more likely to get well regardless.) Thus, to specify this model fully, we need to deﬁne a joint probability distribution over $U^{T},U^{O}-a$ total of ﬁfteen independent parameters. 

We see that a full speciﬁcation of even a simple functional causal model is quite complex. 

# 21.5 Partial Identiﬁability in Functional Causal Models $\star$ 

In section 21.3, we considered the task of answering causal queries involving interventions. As we discussed, unlike probability queries, to answer intervention queries we generally need to consider the efect of latent variables on the observable variables. A functional causal model $\mathcal{C}$ provides a complete speciﬁcation of a causal model, including the efect of the latent variables, and can thus be used to answer any intervention query. As in deﬁnition 21.1, we mutilate the network by eliminating all incoming arcs to intervened variables, and then do inference on the resulting network. 

However, this “solution” does not address the fundamental problem. As we discussed in section 21.3, our accumulated statistics are generally only over the observable variables. We will rarely have any direct observations of the ﬁner-grained distributions over the response variables. Thus, it is unrealistic to assume that we can construct a fully speciﬁed functional causal model. So, what is the point in deﬁning these constructs if we can never obtain them? As we show in this section and the next one, these networks, even if never fully elicited, can help us provide at least partial answers to both intervention and counterfactual queries. 

The basis for this approach rests on the fact that the distributions of response variables are related to the conditional probabilities for the endogenous variables. Thus, we can use our information about the latter to constrain the former. To understand this relationship, consider the following example: 

# Example 21.19 

Let us revisit example 21.16, and consider the observed ity $P(j^{1}\mid g^{1})$ — the probability that Gump gets a job if he gets a high grade. Given $G\,=\,g^{1}$ , we get $j^{1}$ in two cases: in the “always hire” case — $U^{J}=\mu_{1\mapsto1,0\mapsto1}^{J}$ , and in the “rational recruiting” case — $U^{J}=\mu_{1\mapsto1,0\mapsto0}^{J}$ . 7→ 7→ 7→ 7→ These two choices for $U^{J}$ are indistinguishable in the context $g^{1}$ , but would have led to diferent outcomes had Gump had a low grade. As a consequence, we have that: 

$$
\begin{array}{r c l}{P(j^{1}\mid g^{1})}&{=}&{P(\mu_{1\mapsto1,0\mapsto1}^{J})+P(\mu_{1\mapsto1,0\mapsto0}^{J})}\\ {P(j^{1}\mid g^{0})}&{=}&{P(\mu_{1\mapsto1,0\mapsto1}^{J})+P(\mu_{1\mapsto0,0\mapsto1}^{J})}\\ {P(j^{0}\mid g^{1})}&{=}&{P(\mu_{1\mapsto0,0\mapsto1}^{J})+P(\mu_{1\mapsto0,0\mapsto0}^{J})}\\ {P(j^{0}\mid g^{0})}&{=}&{P(\mu_{1\mapsto1,0\mapsto0}^{J})+P(\mu_{1\mapsto0,0\mapsto0}^{J}).}\end{array}
$$ 

We see that each conditional probability is a sum of some subset of the probabilities associated with the response variable. 

In the case where the response variable $U^{X}$ is marginally independent of other response variables, we can generalize this example to provide a full formulation of the constraints that relate the observed conditional probabilities $P(X\mid Y)$ and the distributions over $U^{X}$ : 

$$
P(x_{i}\mid\pmb{y}_{i})=\sum_{\bar{x}_{-i}\in V a l(X)^{m-1}}P(\mu_{\pmb{y}\mapsto x_{i},\bar{\pmb{y}}_{-i}\mapsto\bar{x}_{-i}}^{X}),
$$ 

where $\bar{\pmb{y}}_{-i}$ is the tuple of all assignments $\pmb{y}_{j}$ except for $j=i$ , and similarly for $\bar{x}_{-i}$ . The more − − general case, where response variables may be correlated, is a little more complex, due to the richer parameter iz ation of the model. 

# Example 21.20 

Consider again example 21.18. In this case, our functional causal model has sixteen unknown re- sponse parameters, specifying the joint distribution $P(U^{T},U^{O})$ . By observing the statistics over the variables, we can evaluate the distribution $P({\cal T},{\cal O}\mid A)$ , which can be used to constrain $P(U^{T},U^{O})$ . Let $\nu_{(1,0)\mapsto(i,j),(1,0)\mapsto(k,l)}$ (for $i,j,k,l\,\in\,\{0,1\}$ denote $P(\mu_{1\mapsto i,0\mapsto j}^{T},\mu_{1\mapsto k,0\mapsto l}^{O})$ . 7→ 7→ 7→ 7→ Using reasoning similar to that of example 21.19, we can verify that these two sets of probabilities are related by the following linear equalities: 

$$
P(t^{i},o^{j}\mid a^{k})=\sum_{i^{\prime},j^{\prime}\in\{0,1\}}\nu_{(k,1-k)\mapsto(i,i^{\prime}),(i,1-i)\mapsto(j,j^{\prime})}\;\;\;\;\;\;\forall i,j,k.
$$ 

For example, 

$$
P(t^{1},o^{0}\mid a^{1})=\sum_{i^{\prime},j^{\prime}\in\{0,1\}}\nu_{(1,0)\mapsto(1,i^{\prime}),(1,0)\mapsto(0,j^{\prime})};
$$ 

that is, to be consistent with the assignment $a^{1},t^{1},o^{0}$ , the $U^{T}$ function should map $a^{1}$ to $t^{1}$ , but it can map $a^{0}$ arbitrarily, and the $U^{O}$ function should map $t^{1}$ to $o^{0}$ , but it can map $t^{0}$ arbitrarily. 

Thus, we see that the observed probabilities over endogenous variables impose constraints on the possible distributions of the response variables. These constraints, in turn, impose constraints over the possible values of various queries of interest. By reasoning about the possible values for the distributions over the response variables, we can often obtain reasonably precise bounds over the values of queries of interest. 

# Example 21.21 

Continuing our clinical trial example, assume that we are interested in determining the extent to which taking the medication improves a patient’s chances of a cure. Naively, we might think to answer this question by evaluating $P(o^{1}\mid t^{1})-P(o^{1}\mid t^{0})$ . This approach would be incorrect, since it does not account for the correlation between compliance and cure. For example, if patients who choose to comply with the treatment are generally sicker and therefore less likely to get well regardless, then the cure rate in the population of patients for which $T=t^{1}$ will be misleadingly low, giving a pessimistic estimate of the efcacy of treatment. 

A correct answer is obtained by the corresponding intervention query, 

$$
P(o^{1}\mid\mathrm{do}(t^{1}))-P(o^{1}\mid\mathrm{do}(t^{0})),
$$ 

average causal efect 

which measures the increase in cure probability between a patient who was forced not to take the treatment and one who was forced to take it. This query is also known as the average causal efect of $T$ on $O$ and is denoted $\mathrm{ACC}(T\rightarrow O)$ . Unfortunately, the causal model for this situation contains a bidirected arc between T and O , due to the correlation between their responses. Therefore, the inﬂuence of $T$ on $O$ is not identiﬁable in this model, and, indeed, none of the simpliﬁcation rules of section 21.3 apply in this case. However, it turns out that we can still obtain surprisingly meaningful bounds over the value of this query. 

We begin by noting that 

$$
\begin{array}{r c l}{P(o^{1}\mid\mathrm{{do}}(t^{1}))}&{=}&{P(\mu_{1\mapsto1,0\mapsto1}^{O})+P(\mu_{1\mapsto1,0\mapsto0}^{O})}\\ {P(o^{1}\mid\mathrm{{do}}(t^{0}))}&{=}&{P(\mu_{1\mapsto1,0\mapsto1}^{O})+P(\mu_{1\mapsto0,0\mapsto1}^{O}),}\end{array}
$$ 

so that 

$$
\mathrm{ACC}(T\rightarrow O)=P(o^{1}\mid\mathrm{do}(t^{1}))-P(o^{1}\mid\mathrm{do}(t^{0}))=P(\mu_{1\mapsto1,0\mapsto0}^{O})-P(\mu_{1\mapsto0,0\mapsto0}^{O})
$$ 

These are just marginal probabilities of the distribution $P(U^{T},U^{O})$ , and they can therefore be written as a linear combination of the sixteen $\nu$ parameters representing the entries in this joint distribution. 

The set of possible values for $\nu$ is determined by the constraints equation (21.3), which deﬁnes eight linear equations constraining various subsets of these parameters to sum up to probabilities in the observed distribution. We also need to require that $\nu$ be nonnegative. 

Altogether, we have a linear formulation of ACE( $T\,\rightarrow\,{\cal O})$ in terms of the $\nu$ parameters, and a set of linear constraints on these parameters — the equalities of equation (21.3) and the nonnegativity inequalities. We can now use linear programming techniques to obtain both the maximum and the minimum value of the function representing ACE $T\,\rightarrow\,O)$ ) subject to the constraints. This gives us bounds over the possible value that $\mathrm{ACC}(T\,\rightarrow\,O)$ can take in any functional causal model consistent with our observed probabilities . 

In this fairly simple problem, we can even provide closed-form expressions for these bounds. Somewhat simpler bounds that are correct but not tight are: 

$$
\begin{array}{r l r}{\mathrm{ACC}(T\rightarrow O)}&{\geq}&{P(o^{1}\mid a^{1})-P(o^{1}\mid a^{0})-P(o^{1},t^{0}\mid a^{1})-P(o^{0},t^{1}\mid a^{0}).}\\ &{}&\\ {\mathrm{ACC}(T\rightarrow O)}&{\leq}&{P(o^{1}\mid a^{1})-P(o^{1}\mid a^{0})+P(o^{0},t^{0}\mid a^{1})+P(o^{1},t^{1}\mid a^{0}).}\end{array}
$$ 

Both bounds have a base quantity of $P(o^{1}\mid a^{1})-P(o^{1}\mid a^{0}),$ ; this quantity, sometimes called the encouragement , represents the diference in cure rate between the group that was prescribed the medication and the group that was not, ignoring the issue of how many in each group actually took the prescribed treatment. In a regime of full compliance with the treatment, the encouragement would be equivalent to $\mathrm{ACC}(T\rightarrow O)$ . The correction factors in each of these equations provide a bound on the extent to which noncompliance can afect this estimate. Note that the total diference between the upper and lower bounds is 

$$
P(o^{0},t^{0}\mid a^{1})+P(o^{1},t^{1}\mid a^{0})+P(o^{1},t^{0}\mid a^{1})+P(o^{0},t^{1}\mid a^{0})=P(t^{0}\mid a^{1})+P(o^{1},t^{1}\mid a^{0}),
$$ 

natural bounds which is precisely the total rate of noncompliance. These bounds are known as the natural bounds . They are generally not as tight as the bounds we would obtain from the linear program, but they are tight in cases where no patient is a deﬁer, that is, $P(\mu_{1\mapsto0,0\mapsto1}^{T})=0$ . In many cases, they provide 7→ 7→ surprisingly informative bounds on the result of the intervention query, as shown in box 21.B. 

Box 21.B — Case Study: The Efect of Cholestyramine. A study conducted as part of the Lipid Research Clinics Coronary Primary Prevention Trial produced data about a drug trial relating to a drug called cholestyramine. In a portion of this data set (337 subjects), subjects were randomized into two treatment groups of roughly equal size; in one group, all subjects were prescribed the drug $(a^{1}),$ , while in the other group, all subjects were prescribed a placebo $(a^{0})$ . Patients who were in the control group did not have access to the drug. The cholesterol level of each patient was measured several times over the years of treatment, and the average was computed. In this case, both the actual consumption of the drug and the resulting cholesterol level are continuous-valued. 

Balke and Pearl (1994a) provide a formal analysis of this data set. In their analysis, a patient was said to have received treatment $(\!t^{1}$ ) if his consumption was above the midpoint between minimal and maximal consumption among patients in the study. A patient was said to have responded to treatment $(o^{1}$ ) if his average cholesterol level throughout the treatment was at least 28 points lower than his measurement prior to treatment. 

The resulting data exhibited the following statistics: 

![](images/3b71427f817afbfbe255d82a4f534c0f0652be2e6105e86759eef28f74a07183.jpg) 

The encouragement in this case is $P(o^{1}\mid a^{1})-P(o^{1}\mid a^{0})=0.0473+0.073-0.081=0.465.$ . According to equation (21.5) and 21.6, we obtain: 

The diference between these bounds represents the noncompliance rate of $P(t^{0}\mid a^{1})\,=\,0.388$ . Thus, despite the fact that 38.8 percent of the subjects deviated from their treatment protocol, we can still assert (ignoring possible errors resulting from using statistics over a limited population) that, when applied uniformly to the population, cholestyramine increases by at least 39.2 percent the probability of reducing a patient’s cholesterol level by 28 points or more. 

# 21.6 Counterfactual Queries $\star$ 

Part of our motivation for moving to functional causal models derived from the issue of an- swering counterfactual queries. We now turn our attention to such queries, and we show how functional causal models can be used to address them. As we discussed, similar issues arise in the context of diagnostic reasoning, where we wish to reason about the probabilities of various events after taking a repair action; see box 21.C. 

# 21.6.1 Twinned Networks 

Recall that a counterfactual query considers a scenario that actually took place in the real world and a corresponding scenario in a counterfactual world, where we modify the chain of events via some causal intervention. Thus, we are actually reasoning in parallel about two diferent worlds: the real one, and the counterfactual one. A variable $X$ may take on one value in the real world, and a diferent value in the counterfactual world. To distinguish between these two values, we use $X$ to denote the random variable $X$ in the true world, and $X^{\prime}$ to denote $X$ in the counterfactual world . Intuitively, both the real and the counterfactual worlds are governed by the same causal model, except that one involves an intervention. 

However, a critical property of the desired output for a counterfactual query was that it involves minimal modiﬁcation to the true world. As a degenerate case, consider the following counterfactual query relating to example 21.18: “A patient in the study took the prescribed treatment and did not get well. Would he have gotten well had we forced him to comply with the treatment?” Formally, we can write this query as $P(O^{\prime}=o^{1}\mid T=t^{1},O=o^{0},d o(T^{\prime}:=t^{1}))$ . Our intuition in this case says that we change nothing in the counterfactual world, and so the outcome should be exactly the same. But even this obvious assertion has signiﬁcant consequences. Clearly, we cannot simply generate a new random assignment to the variables in the counterfactual world. For example, we want the patient’s prescribed course of treatment to be the same in both worlds. We also want his response to the treatment to be the same. 

The notion of response variables allows us to make this intuition very precise. A response variable $U_{X}$ in a functional causal model summarizes the efect of all of the stochastic choices that can inﬂuence the choice of value for the endogenous variable $X$ . When moving to the counterfactual world, all of these stochastic choices should remain the same. Thus, we assume that the values of the response variables are selected at random in the real world and preserved unchanged in the counterfactual world. Note that, when the counterfactual query includes a nondegenerate intervention $d o(X\,:=\,x^{\prime})\ -$ — one where $x^{\prime}$ is diferent from the value of $x$ in the true world — the values of endogenous variables can change in the counterfactual world. However, the mechanism by which they choose these values remains constant. 

Consider the counterfactual query $P(O^{\prime}\,=\,o^{1}\;\mid\;T\,=\,t^{0},O\,=\,o^{0},\mathrm{do}(T^{\prime}\,:=\,t^{1}))$ ) . This query represents a situation where the patient did not take the prescribed treatment and did not get well, and we wish to determine the probability that the patient would have gotten well had he complied with the treatment. Assume that the true world is such that the patient was assigned to the treatment group, so that $U^{A}\,=\,a^{1}$ . Assume also that he is in the “helped” category, so that the response variable $U^{O}\,=\,\mu_{1\mapsto1,0\mapsto0}^{O}$ . As we assumed, both of these are conserved in the 7→ 7→ counterfactual world. Thus, given $T^{\prime}=t^{1}$ , and applying the deterministic functions speciﬁed by the response variables, we obtain that $A^{\prime}=a^{1}$ , $T^{\prime}=t^{1}$ , and $O^{\prime}=o^{1}$ , so that the patient would have recovered had he complied. 

In general, of course, we do not know the value of the response variables in the real world. However, a functional causal model speciﬁes a prior distribution over their values. Some values are not consistent with our observations in the real scenario, so the distribution over the response variables is conditioned accordingly. The resulting posterior can be used for inference in the counterfactual world. 

Example 21.23 Consider again our clinical trial of example 21.18. Assume that the trial is randomized, so that $P(U^{A}=a^{1})=P(U^{A}=a^{0})=0.5$ . Also, assume that the medication is unavailable outside the treatment group, so that $P(U^{T}=\mu_{1\mapsto1,0\mapsto1}^{T})=0$ . One possible joint distribution for $P(U^{T},U^{O})$ 7→ 7→ is shown in ﬁgure 21.5b. Assume that we have a patient who, in the real world, was assigned to the treatment group, did not comply with the treatment, and did not get well; we are interested in the probability that he would have gotten well had he complied. Thus, our query is: 

$$
P(O^{\prime}=o^{1}\mid A=a^{1},T=t^{0},O=o^{0},\mathrm{do}(T^{\prime}:=t^{1})).
$$ 

Our observations in the real world are consistent only with the following values for the response 

![](images/0e155cb25db02c1be1f8a3e3952331d0664043da815df84338358a39175fcfec.jpg) 
Figure 21.6 Twinned counterfactual network for ﬁgure 21.5, with an intervention at $T^{\prime}$ . 

variables: $U^{T}$ can be “deﬁer” or “never taker”; $U^{O}$ can be “helped” or “never well.” Conditioning our joint distribution, we obtain the following joint posterior: 

![](images/840a1f6ca8d330bee7e694c0981f22918b3271e10f9c2849d251317a43b56cf2.jpg) 

In the counterfactual world, we intervene by forcing the patient to comply with the treatment. Therefore, his outcome is determined by the value that $U^{O}$ deﬁnes for $t^{1}$ . This value is $o^{1}$ when the patient is of category “helped,” and $o^{0}$ when he is of category “never well.” Using the posterior we computed, we conclude that the answer to the query is $10/13$ . 

This type of reasoning can be obtained as a direct consequence of inference in a joint causal network that incorporates both the real and the counterfactual world. Speciﬁcally, consider the following construction: 

Deﬁnition 21.4 counterfactual twinned network Let $\mathcal{C}$ be a functional causal model over the endogenous variables $\mathcal{X}$ and the corresponding response variables ${\mathcal{U}}=\{U^{X}~:~X\in{\mathcal{X}}\}$ . We deﬁne the counterfactual twinned network to be a functional causal model over the variables $\mathcal{X}\cup\mathcal{U}\cup\{X^{\prime}\ :\ X\in\mathcal{X}\}$ , such that: 

• $i f\mathrm{Pa}_{X}=Y$ , then $\mathrm{Pa}_{X^{\prime}}=Y^{\prime}$ , • $X^{\prime}$ also has the response variable $U^{X}$ as $a$ parent, • the deterministic function governing $X^{\prime}$ is identical to that of $X$ . 

We can answer counterfactual queries for our original causal model by constructing the twinned causal model and then answering the counterfactual query as an intervention query in that network, in exactly the same way as we would for any functional causal network. 

Example 21.24 Returning to the query of example 21.23, we ﬁrst construct the twinned network for ﬁgure 21.5, and then use it to answer the intervention query $P(O^{\prime}=o^{1}\mid A=a^{1},T=t^{0},O=o^{0},\mathsf{d o}(T^{\prime}:=t^{1}))$ as a simple intervention query in the resulting network. Speciﬁcally, we mutilate the counterfactual network so as to eliminate incoming arcs into $T^{\prime}$ , condition on our evidence $A=a^{1},T=t^{0},O=$ $o^{0}$ , and simply run probabilistic inference. The network is shown in ﬁgure 21.6. It is straightforward to verify that the answer is precisely what we obtained in the preceding analysis. 

Note that this approach is very general, and it allows us to answer a broad range of queries. We can account for interventions in the real network as well as the counterfactual one and allow for evidence in both. For example, we can ask, “Assume that we had a patient in the control (placebo) group who did not get well; if I had assigned that patient to the treatment group and observed that he did not get well, what is the probability that he complied with treatment ld)?” This query is formally written as ${\cal P}({\cal T}^{\prime}\,=\,t^{1}\,\mid\,{\cal A}\,=\,a^{0},{\cal O}\,=$ $o^{0},d o(A^{\prime}:=a^{1}),O^{\prime}=o^{0})$ ) , and it can be answered using inference in the twinned network. 

Box 21.C — Case Study: Persistence Networks for Diagnosis. One real-world application of the twinned-network analysis arises in the setting of troubleshooting (see also box 5.A). In this ap- plication, described in Breese and Heckerman (1996), we have a network with multiple (unobserved) variables $X_{1},\ldots,X_{k}$ that denote the possible failure of various components in a system. Our task is to repair the system. In an ideal setting, our repair actions can correspond to ideal interventions: we take one of the failure variables $X_{k}$ and set it to a value denoting no failure: $\mathrm{d}\mathrm{o}(X_{i}:=x_{i}^{0})$ ) . 

Now, assume that we have some set of observations $e$ about the current state of the system, and want to evaluate the beneﬁt of some repair action $\operatorname{do}(X_{i}:=x_{i}^{0})$ ) . How do we compute the probability that the repair action ﬁxes the problem, or the distribution over the remaining failure variables following the repair action? It is not difcult to see that an intervention query is not appropriate in this setting: The evidence that we had about the failure symptoms prior to the repair is highly relevant for determining these probabilities, and it must be taken into account when computing the posterior following the intervention. For example, if the probability of $x_{i}^{0}$ ( i not broken) was very high given $e$ , chances are the system is still broken following the repair. The right query, to determine the post intervention distribution for a variable $Y$ , is a counterfactual one: 

$$
P(Y^{\prime}\mid e,\mathrm{do}(X_{i}^{\prime}:=x_{i}^{0})).
$$ 

Under certain (fairly strong) assumptions — noisy-or CPDs and a single fault only — one can avoid constructing the twinned network and signiﬁcantly reduce the cost of this computation. (See exercise 21.6.) We return to the issue of troubleshooting networks in box 23.C. 

# 21.6.2 Bounds on Counterfactual Queries 

Although a functional causal model gives us enormous ability to answer sophisticated counter- factual queries, such a model is rarely available, as we discussed. However, we can apply the same idea as in section 21.5 to provide bounds on the probabilities of the response variables, and hence on the answers to counterfactual queries. We demonstrate this approach on a ﬁcti- tious example, which also serves to illustrate the diferences between diferent types of causal analysis. 

The marketer of PeptAid, an antacid medication, randomly mailed out product samples to 10 percent of the population. A follow-on market survey determined, for each individual, whether he or she received the sample ( A ), whether he or she took it $(T),$ and whether he or she subsequently developed an ulcer $(\mathcal{O}).$ . The accumulated data exhibited the following statistics: 

The functional causal model for this situation is identical to that of ﬁgure 21.5a. 

Examining these numbers, we see a strong correlation between individuals who consumed Pep- tAid and those who developed ulcers: $P(o^{1}\mid t^{1})=0.5$ , whereas $P(o^{1}\mid t^{0})=0.26$ . Moreover, the probability of developing ulcers was 45 percent greater in individuals who received the PeptAid samples than in those who did not: $P(o^{1}\mid a^{1})\,=\,0.81$ , whereas $P(o^{1}\mid a^{0})\,=\,0.36$ . Thus, using the observational data alone, we might conclude that PeptAid causes ulcers, and that its manufacturer is legally liable for damages to the afected population. 

As we discussed in example 21.2, an immediate counterargument is that the high positive corre- lation is due to some latent common cause such as preulcer discomfort. Indeed, one can show that, in this case, PeptAid actually helps reduce the risk of ulcers: a causal analysis along the lines of example 21.21 shows that 

$$
-0.23\leq\mathrm{ACC}(T\rightarrow O)\leq-0.15.
$$ 

That is, the average causal efect of PeptAid consumption is to reduce an individual’s chance of getting an ulcer by at least 15 percent. 

However, now consider a particular patient George who received the sample, consumed it, and subsequently developed an ulcer. We would like to know whether the patient would not have devel- oped the ulcer had he not received the sample. In this case, the relevant query is a counterfactual, which has the form: 

$$
P(O^{\prime}=o^{0}\mid A=a^{1},T=t^{1},O=o^{1},\mathsf{d o}(A^{\prime}:=a^{0})).
$$ 

Given our evidence $A=a^{1},T=t^{1},O=o^{1}$ , only the responses “complier” and “always taker” are possible for $U^{T}$ , and only the responses “hurt” and “never well” for $U^{O}$ (where here “never well” means “always ulcer”, or $\mu_{1\mapsto1,0\mapsto1}^{O}).$ ). Of these, only the combination (“complier,”“hurt”) is consistent 7→ 7→ with the query assertion ${\cal O}^{\prime}\,=\,{\o{o}{o}}^{0}$ : if George is $^a$ “never well,” he would have developed ulcers regardless; similarly, if George is an “always taker,” he would have taken PeptAid regardless, with the same outcome. We conclude that the probability of interest is equal to: 

$$
\frac{P(U^{T}=c o m p l i e r,U^{O}=h u r t)}{P(T=t^{1},O=o^{1}\mid A=a^{1})}.
$$ 

Because the numerator is ﬁxed, this expression is linear in the $\nu$ parameters, and so we can compute bounds using linear programming. The resulting bounds show that: 

$$
P(O^{\prime}=o^{0}\mid A=a^{1},T=t^{1},O=o^{1},\mathsf{d o}(A^{\prime}:=a^{0}))\ge0.93;
$$ 

thus, at least 93 percent of patients in George’s category — those who received PeptAid, consumed it, and developed an ulcer — would not have developed an ulcer had they not received the sample! 

This example illustrates the subtleties of causal analysis, and the huge diferences between the answers to apparently similar queries. Thus, care must be taken when applying causal analysis to understand precisely which query it is that we really wish to answer. 

# 21.7 Learning Causal Models 

So far, we have focused on the problem of using a given causal model to answer causal queries such as intervention or counterfactual queries. In this section, we discuss the problem of learning a causal model from data. 

As usual, there are several axes along which we can partition the space of learning tasks. 

Perhaps the most fundamental axis is the notion of what we mean by a causal model. Most obvious is a causal network with standard CPDs. Here, we are essentially learning a standard Bayesian network, except that we are willing to ascribe to it causal semantics and use it to answer interventional queries. However, we can also consider other choices. For example, at one extreme, we can consider a functional causal model, where our network has response variables and a fully speciﬁed parameter iz ation for them; clearly, this problem is far more challenging, and such rich models are much more difcult to identify from available data. At the other extreme, we can simplify our problem considerably by abstracting away all details of the parameter iz ation and focus solely on determining the causal structure. 

Second, we need to determine whether we are given the structure and so have to deal only with parameter estimation, or whether we also have to learn the structure. 

observational data interventional data 

A third axis is the type of data from which we learn the model. In the case of probabilistic models, we assumed that our data consist of instances sampled randomly from some generating distribution $P^{*}$ . Such data are called observational . In the context of causal models, we may also have access to interventional data , where (some or all of) our data instances correspond to cases where we intervene in the model by setting the values of some variables and observe the values of the others. As we will discuss, interventional data provide signiﬁcant power in disambiguating diferent causal models that can lead to identical observational patterns. Unfortunately, in many cases, interventions are difcult to perform, and sometimes are even illegal (or immoral), whereas observational data are usually much more plentiful. 

A ﬁnal axis involves the assumptions that we are willing to make regarding the presence of factors that can confound causal efects. If we assume that there are no confounding factors — latent variables or selection bias — the problem of identifying causal structure becomes signiﬁcantly simpler. In particular, under fairly benign assumptions, we can fully delineate the cases in which we can infer causal direction from observational data. When we allow these confounding factors, the problem becomes signiﬁcantly harder, and the set of cases where we can reach nontrivial conclusions becomes much smaller. 

Of course, not all of the entries in this many-dimensional grid are interesting, and not all have been explored. To structure our discussion, we begin by focusing on the task of learning a causal model, which allows us to infer the causal direction for the interactions between the variables and to answer interventional queries. We consider ﬁrst the case where there are no confounding factors, so that all relevant variables are observed in the data. We discuss both the case of learning only from observational data, then introduce the use of interventional data. We then discuss the challenges associated with latent variables and approaches for dealing with them, albeit in a limited way. Finally, we move to the task of learning a functional causal model, where even determining the parameter iz ation for a ﬁxed structure is far from trivial. 

# 21.7.1 Learning Causal Models without Confounding Factors 

We now consider the problem of learning a causal model from data. At some level, it appears that there is very little to say here that we have not already said. After all, a causal model is essentially a Bayesian network, and we have already devoted three chapters in this book to the problem of learning the structure and the parameters of such networks from data. Although many of the techniques we developed in these chapters will be useful to us here, they do not provide a full solution to the problem of learning causal models. To understand why, recall that our objective in the task of learning probabilistic models was simply to provide a good ﬁt to the true underlying distribution. Any model that ﬁt that distribution (reasonably well) was an adequate solution. 

As we discussed in section 21.1.2, there are many diferent probabilistic models that can give rise to the same marginal distribution over the observed variables. While these models are (in some sense) equivalent with respect to probabilistic queries, as causal models, they generally give rise to very diferent conclusions for causal queries. Unfortunately, distinguishing between these structures is often impossible. Thus, our task in this section is to obtain the strongest possible conclusion about the causal models that could have given rise to the observed data. 

# 21.7.1.1 Key Assumptions 

There are two key assumptions that underlie methods that learn causal models from obser- vational data. They relate the independence assumptions that hold in the true underlying distribution $P^{*}$ with the causal relationships between the variables in the domain. Assume that $P^{*}$ is derived from a causal network whose structure is the graph $\mathcal{G}^{*}$ . 

The ﬁrst, known as the causal Markov assumption , asserts that, in $P^{*}$ , each variable is conditionally independent of its non-efects (direct or indirect) given its direct causes. Thus, each variable is conditionally independent of its nondescendants given its parents in $\mathcal{G}^{\ast}$ . This assumption is precisely the same as the local Markov assumptions of deﬁnition 3.1, except that arcs are given a causal interpretation. We can thus restate this assumption as asserting that the causal network $\mathcal{G}^{\ast}$ is an I-map for the distribution $P^{*}$ . 

While the diference between the local Markov and causal Markov assumptions might appear purely syntactic, it is fundamental from a philosophical perspective. The local Markov assump- tions for Bayesian networks are simply phenom eno logical: they state properties that a particular distribution has. The causal Markov assumption makes a statement about the world: If we relate variables by the “causes” relationship, these independence assumptions will hold in the empirical distribution we observe in the world. 

The justiﬁcation for this assumption is that causality is local in time and space, so that the direct causes of a variable (stochastically) determine its value. Current quantum theory and experiments show that this assumption does not hold at the quantum level, where there are nonlocal variables that appear to have a direct causal efect on others. While these cases do not imply that the causal Markov assumption does not hold, they do suggest that we may see more violations of this assumption at the quantum level. However, in practice, the causal Markov assumption appears to be a reasonable working assumption in most macroscopic systems. 

faithfulness assumption 

The second assumption is the faithfulness assumption , which states that the only conditional independencies in $P^{*}$ are those that arise from d-separation in the corresponding causal graph $\mathcal{G}^{*}$ . Wh combined with the causal Markov assumption, the consequence is that $\mathcal{G}^{\ast}$ is a perfect map of P $P^{*}$ . As we discussed in section 3.4.2, there are many examples where the faithfulness assumption is violated, so that there are independencies in $P^{*}$ not implied by the structure of $\mathcal{G}^{*}$ ; however, these correspond to particular parameter values, which (as stated in theorem 3.5) are a set of measure zero within the space of all possible parameter iz at ions. As we discussed, there are still cases where one of these parameter iz at ions arises naturally. However, for the purposes of learning causality, we generally need to make both of these assumptions. 

# 21.7.1.2 Identifying the Model 

With these assumptions in hand, we can now assume that our data set consists of samples from $P^{*}$ , and our task is simply to identify a perfect map for $P^{*}$ . Of course, as we observed in section 3.4.2, the perfect map for a distribution $P^{*}$ is not unique. Because we might have several diferent structures that are equivalent in the independence assumptions they impose, we cannot, based on observational data alone, distinguish between them. Thus, we cannot, in general, determine a unique causal structure for our domain, even given inﬁnite data. For the purpose of answering probabilistic queries, this limitation is irrelevant: any of these structures will perform equally well. However, for causal queries, determining the correct direction of the edges in the network is critical. Thus, at best, we can hope to identify the equivalence class of $\mathcal{G}^{\ast}$ . 

constraint-based structure learning 

class PDAG 

The class of constraint-based structure learning methods is suitable to this task. Here, we take the independencies observed in the empirical distribution and consider them as representative of $P^{*}$ . Given enough data instances, the empirical distribution $\hat{P}$ will reﬂect exactly the independencies that hold in $P^{*}$ , so that an independence oracle will provide accurate answers about independencies in $P^{*}$ . The task now reduces to that of identifying an I-equivalence class that is consistent with these observed independencies. For the case without confounding factors, we have already discussed such a method: the Build-PDAG procedure described in section 18.2. Recall that Build-PDAG constructs a class PDAG , which represents an entire I-equivalence class of network structures. In the class PDAG, an edge is oriented $X\rightarrow Y$ if and only if it is oriented in that way in every graph that is a member of the I-equivalence class. Thus, the algorithm does not make unwarranted conclusions about directionality of edges. 

Even with this conservative approach, we can often infer the directionality of certain edges. 

Example 21.26 Assume that our underlying distribution $P^{*}$ is represented by the causal structure of the Student network shown in ﬁgure 3.3. Assuming that the empirical distribution $\hat{P}$ reﬂects the independencies in $P^{*}$ , the Build-PDAG will return the PDAG shown in ﬁgure 21.7a. Intuitively, we can infer the causal directions for $D\to G$ and $I\rightarrow G$ because of t $\nu$ cture at $G$ ; we can infer the causal dir tion for $G\,\rightarrow\,J$ → because the opposite orientation J $J\rightarrow G$ → would create a $\nu$ -structure involving J , which induces a diferent set of independencies. However, we are unable to infer a causal direction for the edge $I{-}S,$ , because both orientations are consistent with the observed independencies. 

In some sense, the constraint-based approaches are ideally suited to inferring causal direction; 

![](images/c0b5b71aac6b7a8e292d4e74cd79217a8343f16851a77bf0dcd7f1adccadbce6.jpg) 
Figure 21.7 Models corresponding to the equivalence class of the Student network. (a) A PDAG, representing its equivalence class when all relevant variables are observed. (b) A PAG, representing its equivalence class when latent variables are a possibility. (c) An unsuccessful attempt to “undirect” the $G\rightarrow J$ arc. 

in fact, these algorithms were mostly developed for the purpose of causal discovery. However, as we discussed in section 18.2, these approaches are fairly sensitive to mistakes in the inde- pendence tests over the distribution. This property can make them somewhat brittle, especially when the data set size is small relative to the number of variables. 

Score-based methods allow us to factor in our conﬁdence in diferent independencies, ﬁnding a solution that is more globally consistent. Thus, an alternative approach is to apply model selection with some appropriate score and then construct the I-equivalence class of the network $\mathcal{G}$ produced by the structure learning algorithm. The I-equivalence class can be rep sented by a PDAG, which can be constructed simply by applying the procedure Build-PDAG to . 

Bayesian model averaging 

A better solution, however, accounts for the fact that the highest-scoring network is not gener- ally the only viable hypothesis. In many cases, especially when data are scarce, there can be several non-equivalent network structures that all have a reasonably high posterior prob- ability. Thus, a better approach for causal discovery is to use Bayesian model averaging , as described in section 18.5. The result is a distribution over diferent network structures that allows us to encode our uncertainty not only about the orientation of an edge, but also about the presence or absence of an edge. Furthermore, we obtain numerical conﬁdence measures in these network features. Thus, whereas the PDAG representation prevents us from orienting an edge $X\rightarrow Y$ even if there is only a single member in the equivalence class that has the opposite orientation, the Bayesian model averaging approach allows us to quantify the overall probability mass of structures in which an edge exists and is oriented in a particular direction. 

Although constraint-based methods and Bayesian methods are often viewed as competing solutions, one successful approach is to combine them, using a constraint-based method to initialize a graph structure, and then using Bayesian methods to reﬁne it. This approach exploits the strengths of both methods. It uses the global nature of the constraint-based methods to avoid local maxima, and it uses the ability of the Bayesian methods to avoid making irreversible decisions about independencies that may be incorrect due to noise in the data. 

# 21.7.2 Learning from Interventional Data 

So far, we have focused on the task of learning causal models from observational data alone. In the causal setting, a natural question is to consider data that are obtained (at least partly) from interventional queries. That is, some of our data cases are obtained in a situation where we intervene in the model, sampled from the mutilated network corresponding to an intervention. One important situation where such data are often available is scientiﬁc discovery, where the data we obtain can be either a measurement of an existing system or a system that was subjected to perturbations. 

Although both constraint-based and score-based approaches can be applied in this setting, constraint-based approaches are not as commonly used. Although it is straightforward to deﬁne the independencies associated with the mutilated network, we generally do not have enough data instances for any given type of intervention to measure reliably whether these independencies hold. Score-based approaches are much more ﬂexible at combining data from diverse interventions. We therefore focus our discussion on that setting. 

To for , we assume that each data instance in $\mathcal{D}$ is speciﬁed by an inter- vention $d o(Z[m]:=z[m])$ ; for each such data case, we have a fully observed data instance $\chi[m]\,=\,\xi[m]$ . As usual in a score-based setting, our ﬁrst task is to deﬁne the likelihood function. Consider ﬁrst the probability of a s nstance $P(\xi\mid d o(Z:=Z),\mathcal{C})$ . This term is deﬁne rms of the mutilated network C $\mathcal{C}_{Z=z}$ . I network, the distribution of each variable $Z\in Z$ ∈ is deﬁned by the intervention, so that Z $Z=z$ w h probability 1 and therefore is also the value we necessarily see in $\xi$ . The variables not in Z are subject to their normal probabilistic model, as speciﬁed in $\mathcal{C}$ . Letting $\mathbf{\mathbf{\mathit{u}}}_{i}$ be the assignment to $\mathrm{Pa}_{X_{i}}$ in $\xi$ , we obtain: 

$$
P(\xi\mid d o(Z:=z),\mathcal{C})=\prod_{X_{i}\notin Z}P(x_{i}\mid\mathbf{u}_{i}).
$$ 

sufcient statistics 

In the case of table-CPDs (see exercise 21.8 for another example), it follows that the sufcient statistics for this type of likelihood function are: 

$$
M[x_{i};{\pmb u}_{i}]=\sum_{m\ :\ X_{i}\not\in{\pmb Z}[m]}{\pmb I}\{X_{i}[m]=x_{i},\mathrm{Pa}_{X_{i}}[m]={\pmb u}_{i}\},
$$ 

for an assignment $x_{i}$ to $X_{i}$ and $\mathbf{\mathbf{\mathit{u}}}_{i}$ to $\mathrm{Pa}_{X_{i}}$ . This sufcient statistic counts the number of occurrences of this event, in data instances where there is no intervention at $X_{i}$ . Unlike our original sufcient statistic $M[x_{i},{\pmb u}_{i}]$ , this deﬁnition of the sufcient statistic treats $X_{i}$ diferently from its parents, hence the change in notation. 

It now follows that: 

$$
L(\mathcal{C}:\mathcal{D})=\prod_{i=1}^{n}\prod_{\substack{x_{i}\in V a l(X_{i}),{\bf u}_{i}\in V a l(\mathrm{Pa}_{X_{i}})}}\theta_{x_{i}|{\bf u}_{i}}^{M[x_{i};{\bf u}_{i}]}.
$$ 

Because this likelihood function has the same functional form as our original likelihood function, we can proceed to apply any of the likelihood-based approaches described in earlier chapters. We can perform maximum likelihood estimation or incorporate a prior to deﬁne a Bayesian posterior over the parameters. We can also deﬁne a Bayesian (or other likelihood-based) score in order to perform model selection or model averaging. The formulas and derivations are exactly the same, only with the new sufcient statistics. 

Example 21.27 

Importantly, in this framework, we can now distinguish between I-equivalent models, which are indistinguishable given observational data alone. In particular, consider a net- work over two variables $X,Y$ , and assume that we have interventional data at either $X,\,Y$ , or both. As we just mentioned, the sufcient statistics are asymmetrical in the parent and child, so that $M[X;Y]$ , for the network $Y\rightarrow X$ , is diferent from $M[Y;X]$ , for the network $X\rightarrow Y$ . Therefore, although the two networks are I-equivalent, the likelihood function can be diferent for the two networks, allowing us to select one over the other. 

Consider the task of learning a causal model over the variables $X,Y$ . Assume that we have the samples with the sufcient statistics shown in the following table: 

![](images/df33c31159c42892601d6b9564c159547d05a31d820f4be69233df31b675d377.jpg) 

The observational data suggest that each of $X$ and $Y$ are (roughly) uniformly distributed, but that they are correlated with each other. The interventional data, although limited, suggest that, when we intervene at $X$ , $Y$ tends to follow, but when we intervene at $Y$ , $X$ is unafected. These intuitions suggest that th model $X\rightarrow Y$ is more plausible. Indeed, computing the sufcient statistics for the model X $X\rightarrow Y$ and these data instances, we obtain: 

$$
{\begin{array}{r c l}{M[x^{1}]}&{=}&{M[x^{1}y^{1}\mid{\mathrm{None}}]+M[x^{1}y^{0}\mid{\mathrm{Rone}}]+M[x^{1}y^{1}\mid\operatorname{do}(y^{1})]=4+1}\\ {M[x^{0}]}&{=}&{M[x^{0}y^{1}\mid{\mathrm{Rone}}]+M[x^{0}y^{0}\mid{\mathrm{Rone}}]+M[x^{0}y^{1}\mid\operatorname{do}(y^{1})]=1+4}\\ {M[y^{1};x^{1}]}&{=}&{M[x^{1}y^{1}\mid{\mathrm{Rone}}]+M[x^{1}y^{1}\mid\operatorname{do}(x^{1})]=4+2}\\ {M[y^{0};x^{1}]}&{=}&{M[x^{1}y^{0}\mid{\mathrm{Rone}}]=1}\\ {M[y^{1};x^{0}]}&{=}&{M[x^{0}y^{1}\mid{\mathrm{Rone}}]=1}\\ {M[y^{0};x^{0}]}&{=}&{M[x^{0}y^{0}\mid{\mathrm{Rone}}]=4.}\end{array}}
$$ 

Importantly, we note that, unlike the purely observational case, $M[y^{1};x^{1}]+M[y^{0};x^{1}]\ne M[x^{1}].$ ; this is because diferent data instances contribute to the diferent counts, depending on the variable at which the intervention takes place. 

We can now compute the maximum likelihood parameters for this model as $\theta_{x^{1}}=0.5,\theta_{y^{1}|x^{1}}=$ $6/7,\,\theta_{y^{1}|x^{0}}=1/5$ . The log-likelihood of the data is then: 

$$
\begin{array}{l}{{M[x^{1}]\log\theta_{x^{1}}+M[x^{0}]\log\theta_{x^{0}}+}}\\ {{M[y^{1};x^{1}]\log\theta_{y^{1}|x^{1}}+M[y^{0};x^{1}]\log\theta_{y^{0}|x^{1}}+M[y^{1};x^{0}]\log\theta_{y^{1}|x^{0}}+M[y^{0};x^{0}]\log\theta_{x^{0}}+}}\end{array}
$$ 

which equals $-19.75$ . 

We can analogously execute the same steps for the causal model $Y\rightarrow X$ , where our sufcient statistics would have the form $M[y]$ and $M[x;y]$ , each utilizing a diferent set of instances. Overall, we would obtain a log-likelihood of $-21.41$ , which is lower than for the causal model $X\rightarrow Y$ . Thus, the log-likelihood of the two causal models is diferent, even though they are $I\cdot$ -equivalent as probabilistic models. Moreover, the causal model that is consistent with our intuitions is the one that obtains the highest score. 

We also note that an intervention at $X$ can help disambiguate parts of the network cent to $X$ . or example, assume that the true network is a chain $X_{1}\rightarrow$ $X_{2}\,\rightarrow\,.\,.\,\rightarrow\,X_{n}$ . There are n I-equiva t directed graphs ot the graph at $X_{i}$ for i $i\,=\,1,.\,.\,.\,,n)$ ). Interventions at any $X_{i}$ can reveal that $X_{i+1},.\,.\,.\,,X_{n}$ all respond to an intervention at $X_{i}$ , whereas $X_{1},.\ldots,X_{i-1}$ do not. Although these experiments would not fully disambiguate the causal structure, they would help direct all of the edges from $X_{i}$ toward any of its descendants. Perhaps less intuitive is that they also help direct edges that are not downstream of our intervention. For example, if $X_{i-1}$ does not respond to an intervention at $X_{i}$ , but we are convinced that they are directly correlated, we now have more conﬁdence that we can direct the edge $X_{i-1}\rightarrow X_{i}$ . Importantly, directing some edges can have repercussions on others, as we saw in section 3.4.3. Indeed, in practice, a series of interventions at some subset of variables can signiﬁcantly help disambiguate the directionality of many of the edges; see box 21.D. 

cellular network reconstruction 

Box 21.D — Case Study: Learning Cellular Networks from Intervention Data. As we men- tioned earlier, one of the important settings where interventional data arise naturally is scientiﬁc discovery. One application where causal network discovery has been applied is to the task of cellular network reconstruction . In the central paradigm of molecular biology, a gene in a cell (as encoded in DNA) is expressed to produce mRNA, which in turn is translated to produce protein, which performs its cellular function. The diferent steps of this process are carefully regulated. There are proteins whose task it is to regulate the expression of their target genes ; others change the activity level of proteins by a physical change to the protein itself. Unraveling the structure of these networks is a key problem in cell biology. Causal network learning has been successfully applied to this task in a variety of ways. 

One important type of cellular network is a signaling network , where a signaling protein phys- ically modiﬁes the structure of a target in a process called phosphorylation , thereby changing its activity level. Fluorescence microscopy can be used to measure the level of $a$ phosphoprotein — a particular protein in a particular phosphorylation state. The phosphoprotein is fused to a ﬂuorescent marker of a particular color. The ﬂuorescence level of a cell, for a given color channel, indicates the level of the phosphoprotein fused with that color marker. Current technology allows us to measure simultaneously the levels of a small number of phosphoproteins, at the level of single cells. These data provide a unique opportunity to measure the activity levels of several proteins within individual cells, and thereby, we hope, to determine the causal network that underlies their interactions. 

bootstrap 

Sachs et al. (2005) measured eleven phosphoproteins in a signaling pathway in human T-cells un- der nine diferent perturbation conditions. Of these conditions, two were general perturbations, but the remaining seven activated or inhibited particular phosphoproteins, and hence could be viewed as ideal interventions. Overall, 5,400 measurements were obtained over these nine conditions. The continuous measurements for each gene were discretized using a $k$ -means algorithm, and the sys- tem was modeled as a Bayesian network where the variables are the levels of the phosphoproteins and the edges are the causal connections between them. The network was learned using standard score-based search, using a Bayesian score based on the interventional likelihood of equation (21.8). To obtain a measure of conﬁdence in the edges of the learned structure, conﬁdence estimation was performed using a bootstrap method, where the same learning procedure was applied to diferent training sets, each sampled randomly, with replacement, from the original data set. This procedure gave rise to an ensemble of networks, each with its own structure. The conﬁdence associated with an edge was then estimated as the fraction of the learned networks that contained the edge. 

The result of this procedure gave rise to a network with seventeen high-conﬁdence causal arcs between various components. A comparison to the known literature for this pathway revealed that ﬁfteen of the seventeen edges were well established in the literature; the other two were novel but had supporting evidence in at least one literature citation. Only three well-established connections were missed by this analysis. Moreover, in all but one case, the direction of causal inﬂuence was correctly inferred. One of the two novel predictions was subsequently tested and validated in a wet-lab experiment. This ﬁnding suggested a new interaction between two pathways. 

The use of a single global model for inferring the causal connections played an important role in the quality of the results. For example, because causal directions of arcs are often compelled by their interaction with other arcs within the overall structure, the learning algorithm was able to detect correctly causal inﬂuences from proteins that were not perturbed in the assay. In other cases, strong correlations did not lead to the inclusion of direct arcs in the model, since the correlations were well explained by indirect pathways. Importantly, although the data were modeled as fully observed, many relevant proteins were not actually measured. In such cases, the resulting indirect paths gave rise to direct edges in the learned network. 

Various characteristics of this data set played an important role in the quality of the results. For example, the application of learning to a curtailed data set consisting solely of 1,200 observational data points gave rise to only ten arcs, all undirected, of which eight were expected or reported; ten of the established arcs were missing. Thus, the availability of interventional data played an important role in the accurate reconstruction of the network, especially the directionality of the edges. Another experiment showed the value of single-cell measurements, as compared to an equal-size data set each of whose instances is an average over a population of cells. This result suggests that the cell population is heterogeneous, so that averaging destroys much of the signal present in the data. 

Nevertheless, the same techniques were also applied, with some success, to a data set that lacks these enabling properties. These data consist of gene expression measurements — measurements of mRNA levels for diferent genes, each collected from a population of cells. Here, data were collected from approximately 300 experiments, each acquired from a strain with a diferent gene deleted. Such perturbations are well modeled as ideal interventions, and hence they allow the use of the same techniques. This data set poses signiﬁcant challenges: there are only 300 measurements (one for each perturbation experiment), but close to 6,000 variables (genes); the population averaging of each sample obscures much of the signal; and mRNA levels are a much weaker surrogate for activity level than direct protein measurements. Nevertheless, by focusing attention on subgraphs where many edges had high conﬁdence, it was possible to reconstruct correctly some known pathways. 

The main limitation of these techniques is the assumption of acyclicity, which does not hold for cellular networks. Nevertheless, these results suggest that causal-network learning, combined with appropriate data, can provide a viable approach for uncovering cellular pathways of diferent types. 

# 21.7.3 Dealing with Latent Variables $\star$ 

So far, we have discussed the learning task in the setting where we have no confounding factors. The situation is more complicated if we have confounding efects such as latent variables or selection bias. In this case, the samples in our empirical distribution are generated from a “partial view” of $P^{*}$ , where some variables have been marginalized out, and others perhaps instantiated to particular values. Because we do not know the set of confounding variables, there is an inﬁnite set of networks that could have given rise to exactly the same set of dependencies over the observable variables. For example, $X\rightarrow Y$ , $X\rightarrow H\rightarrow Y$ $X\rightarrow H\rightarrow H^{\prime}\rightarrow Y$ , and so on are completely indistinguishable in terms of their efect on $X,Y$ (assuming that the hidden variables $H,H^{\prime}$ do not afect other variables). The task of determining a causal model in this case seems unreasonably daunting. 

In the remainder of this section, we describe solutions to the problem of discovering causal structure in presence of confounding efects that induce noncausal correlations between the ob- served variables. These confounding efects include latent variables and selection bias. Although there are methods that cover both of these problems, the treatment of the latter is signiﬁcantly more complex. Moreover, although selection bias clearly occurs, latent variables are almost ubiquitous, and they have therefore been the focus of more work. We therefore restrict our discussion to the case of learning models with latent variables. 

# 21.7.3.1 Score-Based Approaches 

A ﬁrst thought is to try to learn a model with hidden variables using score-based methods such as those we discussed in chapter 19. The implementation of this idea, however, requires signiﬁcant care. First, we generally do not know where in the model we need to introduce hidden variables. In fact, we can have an unbounded number of latent variables in the model. Moreover, as we saw, causal conclusions can be quite sensitive to local maxima or to design decisions such as the number of values of the hidden variable. We note, again, that probabilistic conclusions are also somewhat sensitive to these issues, but signiﬁcantly less so, since models that achieve the same marginals over the observed variables are equivalent with respect to probabilistic queries, but not with respect to causal queries. Nevertheless, when we have some strong prior knowledge about the possible number and placement of hidden variables, and we apply our analysis with care, we can learn informative causal models. 

# 21.7.3.2 Constraint-Based Approaches 

An alternative solution is to use constraint-based approaches that try to use the independence properties of the distribution to learn the structure of the causal model, including the placement of the hidden variables. Here, as for the fully observable case, the independencies in a distribu- tion only determine a structure up to equivalence. However, in the case of latent variables, we observe independence relationships only between the observable variables. We therefore need to introduce a notion of the independencies induced over the observable variables. We say that a dir cyclic g h $\mathcal{G}$ is a latent variable network over $\mathcal{X}$ if it is a causal netwo structure over X ∪H , where H is some arbitrarily large set of (latent) variables disjoint from X . 

# Deﬁnition 21.5 

Let $\mathcal{G}$ be $^a$ latent variable $\mathcal{X}$ . We deﬁne $\mathcal{Z}_{\mathcal{X}}(\mathcal{G})$ to be the set of independencies $(X\perp Y\mid Z)\in{\mathcal{Z}}(\mathcal{G})$ for $X,Y,Z\subset\mathcal{X}$ ⊂X . 

We can now deﬁne a notion of I-equivalence over the observable variables. 

Deﬁnition 21.6 $\operatorname{I}_{\mathcal{X}}$ -equivalence Let $\mathcal{G}_{1},\mathcal{G}_{2}$ be two lat vari ne orks over $\mathcal{X}$ (not necessarily over the same set of latent variables). We say that $\mathcal{G}_{1}$ and $\mathcal{G}_{2}$ are $\operatorname{I}_{\mathcal{X}}$ -equivalent if $\mathscr{T}_{\mathcal{X}}(\mathcal{G}_{1})=\mathscr{T}_{\mathcal{X}}(\mathcal{G}_{2})$ . 

Clearly, we cannot explicitly enumerate the $\mathrm{I}_{\mathcal{X}}$ -equivalence class of a graph. Even in the fully observable case, this equivalence class can be quite large; when we allow latent variables, the equivalence class is generally inﬁnite, since we can have an unbounded number of latent variables placed in a variety of conﬁgurations. The constraint-based methods sidestep this difculty by searching over a space of graphs over the observable variables alone. Like PDAGs, an edge in these graphs can represent diferent types of relationships between its endpoints. 

Deﬁnition 21.7 partial ancestral graph 

t G b n $I_{\mathcal{X}}$ -equivalence class of latent variabl etworks over $\mathcal{X}$ . $A$ partial ancestral graph (PAG)

 $\mathcal{P}$ over X is a graph whose nodes co spond o X nd whose edges represent the depende

 $\pmb{\mathscr{G}}.$ . The presence of an edg etwee $X$ and Y $Y$ in P corresponds to the existence, in each G ∈G , of an active trail between X and Y $Y$ that utilizes only latent variables. Edges have three types of endpoints: $-,\,>$ , and $\bigcirc$ ; these endpoints on the $Y$ end of an edge between $X$ and $Y$ have the following meanings: 

• An arrowhead $>$ implies that $Y$ is not an ancestor of $X$ in any graph in G . • A straight end − implies that $Y$ is an ancestor of $X$ in all graphs in G . • A circle $\bigcirc$ implies that neither of the two previous cases holds. 

The interpretatio f the diferent edg ypes is as follows An ed $X\rightarrow Y$ has (almost) t standard meaning: X is an ancesto of Y in all graphs in G , and Y no an ancestor of X in any graph. Thus, each graph in G contains a directed path from X to Y $Y$ . However, some graphs may also contain trail where a latent variable is an ancestor of both. Thus, for example, the edge $S\rightarrow C$ would represent both of the networks in ﬁgure 21.A.1a,b when both $G$ and $T$ are latent. 

An edge $X\leftrightarrow Y$ means that $X$ is never an ancestor of $Y$ , and $Y$ is never an ancestor of $X$ ; thus, the edge must be due to the presence of a latent common cause. Note that an undirected edge $X{-}Y$ is illegal relative to this deﬁnition, since it is inconsistent with the acyclicity of the graphs in G n edge $X\circ\rightarrow Y$ means that $Y$ is not an ancestor of $X$ in any graph, but $X$ is an ancestor of Y $Y$ in some, but not all, graphs. 

Figure 21.8 shows an example PAG, along with several members of the (inﬁnite) equivalence class that it represents. All of the graphs in the equivalence class have one or more active trails between $X$ and $Y$ , none of which are directed from $Y$ to $X$ . 

At ﬁrst glance, it might appear that the presence of latent variables completely eliminates our ability to infer causal direction. After all, any edge can be ascribed to an indirect correlation via a latent variable. However, somewhat surprisingly, there are conﬁgurations where we can infer a causal orientation to an edge. 

![](images/a8a4448427e5b16b403e11dd87e25d8bdf9635cc7a11f331ade9f62bbc5fd65e.jpg) 
Figure 21.8 Example PAG (left), along with several members of the (inﬁnite) equivalence class that it represents. All of the graphs in the equivalence class have one or more active trails between $X$ and $Y$ , none of which are directed from $Y$ to $X$ . 

Consider again the learning problem in example 21.26, but where we now allow for the presence of latent variables. Figure 21.7b shows the PAG reﬂecting the equivalence class of our original network of ﬁgure 3.3. Not surprisingly, we cannot reach any conclusions about the edge between $I$ and $S$ . The edge between $D$ and $G$ can arise both from a directed path from $D$ to $G$ and from the presence of a latent variable that is the parent of both. However, a directed path from $G$ to $D$ would not result in a marginal independence between $D$ and $I$ , and a dependence given $G$ . Thus, we have an arrowhead $>$ on the $G$ side of this edge. The same analysis holds for the edge between $I$ and $G$ . 

Most interesting, however, is he d ected edge $G\,\rightarrow\,J$ , which asserts that, in any graph in G , there is a directed path from G to J . To understand why, let us try to explain the correlation between $G$ and $J$ by introducing a common latent parent (see ﬁgure 21.7c). This model is not $I_{\mathcal{X}}$ - equivalent to our original network, because it implies that $J$ is marginally independent of $I$ and $D$ . More generally, we can conclude that $J$ must be a descendant of $I$ and $D$ , because observing $J$ renders them dependent. Because $G$ renders $J$ independent of $I$ , it must block any directed path from $I$ to $J$ . It follows that there is a directed path from $G$ to $J$ in every member of G . In fact, in this case, we can reach the stronger conclusions that all the trails between these two variables are directed paths from $G$ to $J$ . Thus, in this particular case, the causal inﬂuence of $G$ on $J$ is simply $P(J\mid G)$ , which we can obtain directly from observational data alone. 

This example gives intuition for how we might determine a PAG structure for a given distri- bution. The algorithm for constructing a PAG for a distribution $P$ proceeds along similar lines to the algorithm for constructing PDAGs, described in section 3.4.3 and 18.2. The full algorithm for learning PAGs is quite intricate, and we do not provide a full description of it, but only give some high-level intuition. The algorithm has two main phases. The ﬁrst phase constructs an undirected graph over the observed variables, representing direct probabilistic interactions between them in $P$ . In general, we want to connect $X$ and $Y$ with a direct edge if and only if there is no subset $Z$ of ${\mathcal{X}}-\{X,Y\}$ such that $P\models(X\ \bot\ Y\ |\ Z)$ ⊥ Of course, we cannot actually enumerate over the exponentially many possible subsets $Z\subset{\mathcal{X}}$ ⊂X . As in Build-PMap-Skeleton , we both bound the size of the possible separating set and prune sets that cannot be separating sets given our current knowledge about the adjacency structure of the graph. The second phase of the algorithm orients as many edges as possible, using reasoning similar to the ideas used for PDAGs, but extended to deal with the confounding efect of latent variables. 

The PAG-learning algorithm ofers similar (albeit somewhat weaker) guarantees than the PDAG construction algorithm. In particular, one cannot show that the edge orientation rules are complete, that is, produce the strongest possible conclusion about edge orientation that is consistent with the equivalence class. However, one can show that all of the latent variable networks over $\mathcal{X}$ that are consistent with a PAG produced by this algorithm are $\mathrm{I}_{\mathcal{X}}$ -equivalent. 

Importantly, we note that a PAG is only a partial graph structure, and not a full model; thus, it cannot be used directly for answering causal queries. One possible solution is to use the score- based techniques we described to parameterize the causal model. This approach, however, is fraught with difculties: First, we have the standard difculties of using EM to learn parameters for hidden variables; an even bigger problem is that the PAG provides no guidance about the number of latent variables, their domain, or the edges between them. 

Another alternative is to use the methods of section 21.3 and 21.5, which use a learned causal structure with latent variables, in conjunction with statistics over the observable data, to answer causal queries. We note, however, that these methods require a known connectivity structure among the hidden variables, whereas the learned PAG does not specify this structure. Never- theless, if we are willing to introduce some assumptions about this structure, these algorithms may be usable. We return to this option in section 21.7.4, where we discuss more robust ways of estimating the answers to such queries. 

# 21.7.4 Learning Functional Causal Models $\star$ 

Finally, we turn to the question of learning a much richer class of models: functional causal models, where we have a set of response variables with their associated parameters. As we discussed, these models have two distinct uses. The ﬁrst is to answer a broader range of causal queries, such as counterfactual queries or queries regarding the average causal efect. The second is to avoid, to some extent, the inﬁnite space of possible conﬁgurations of latent variables. As we discussed in section 21.4, a fully speciﬁed functional causal model summarizes the efect of all of the exogenous variables on the variables in our model, and thereby, within a ﬁnite description, speciﬁes the causal behavior of our endogenous variables. Thus, rather than select a set of concrete latent variables with a particular domain and parameter iz ation for each one, we use response variables to summarize all of the possibilities. Our conclusions in this case are robust, and they apply for any true underlying model of the latent variables. 

The difculty, of course, is that a functional causal model is a very complex object. The pa- rameterization of a response variable is generally exponentially larger than the parameter iz ation of a CPD for the corresponding endogenous variable. Moreover, the data we are given provide the outcome in only one of the exponentially many counterfactual cases given by the response variable. In this section, we describe one approach for learning with these issues. 

Recall that a functional causal model is parameterized by a joint distribution $P(\mathcal{U})$ over the response variables. The local models of the endogenous variables are, by deﬁnition, deterministic functions of the response variables. A response variable $U^{X}$ for a variable $X$ with parents $Y$ is a discrete random variable, whose domain is the space of all functions $\mu(Y)$ from $V a l(Y)$ to $V a l(X)$ . The joint distribution $P(\mathcal{U})$ is encoded by a Bayesian network. We ﬁrst focus on the case where the structure of the network is known, and our task is only to learn the parameter iz ation. We then brieﬂy discuss the issue of structure learning. 

Consider ﬁrst the simple case, where $U^{X}$ has no parents. In this case, we can parameterize $U^{X}$ using a multinomial distribution $\pmb{\nu}^{X}\,=\,(\nu_{1}^{X},\bar{.}\cdot\cdot\,,\nu_{m}^{X})$ , where $m\,=\,|\mathit{V a l}(\bar{U}^{X})|$ . In the Bayesian approach, we would take this parameter to be itself a random variable and introduce an appropriate prior, such as a Dirichlet distribution, over $\nu^{X}$ . More generally, we can use the techniques of section 17.3 to parameterize the entire Bayesian network over $\mathcal{U}$ . 

Our goal is then to compute the posterior distribution $P(\mathcal{U}\mid\mathcal{D})$ ; this posterior deﬁnes an answer to both intervention queries and counterfactual queries. Consider a general causal query $P(\phi\mid\psi)$ , where $\phi$ and $\psi$ may contain both real and counterfactual variables, and $\psi$ may also contain interventions. We have that: 

$$
P(\phi\mid\psi,\mathcal{D})=\int P(\phi\mid\psi,\nu)P(\nu\mid\psi,\mathcal{D})d\nu.
$$ 

Assuming that $\mathcal{D}$ is reasonably large, we can approximate $P(\nu\mid\psi,\mathcal{D})$ as $P(\nu\mid\mathcal{D})$ . Thus, to answer a causal query, we simply take the expectation of the answer to the query over the posterior parameter distribution $P(\nu\mid\mathcal{D})$ . 

The main difculty in this procedure is that e data set $\mathcal{D}$ is only part observable: even if we fully observe the endogenous variables X , e response variables U are not di tly observed. As we saw, an observed assig ment $\xi$ to X limits the set of possible value $\mathcal{U}$ to the subset of functions consistent with ξ . In particular, if $x,y$ is the assignment to $X,Y$ in $\xi;$ , then $U^{X}$ is restricted to the set of possible functions $\mu$ for which $\mu(\pmb{y})\,=\,x$ , a set that is exponentially large. Thus, to apply Bayesian learning, we must use techniques that approximate the posterior parameter distribution $P(\nu\mid\mathcal{D})$ . In section 19.3, we discussed several approaches to approximating this posterior, including variational Bayesian learning and MCMC methods. Both can be applied in this setting as well. 

Thus, in principle, this approach is a straightforward application of techniques we have already discussed. However, because of the size of the space, the use of functional causal models in general and in this case in particular is feasible only for fairly small models. 

When we also need to learn the structure of the functional causal model, the situation be- comes even more complex, since the problem is one of structure learning in the presence of hidden variables. One approach is to use the constraint-based approach of section 21.7.3.2 to learn a structure involving latent variables, and then the approach described here for ﬁlling in the parameters. A second approach is to use one of the methods of section 19.4. However, there is an important issue that arises in this approach: Recall that a response variable for a variable $X$ speciﬁes the value of $X$ for each conﬁguration of its endogenous parents $U$ . Thus, as our structure learning algorithm adapts the structure of the network, the domain of the response variables changes; for example, if our search adds a parent to $X$ , the domain of $U^{X}$ changes. Thus, when performing the search, we would need to recompute the posterior parameter dis- tribution and thereby the score after every structure change to the model. However, under certain independence assumptions, we can use score decomposability to reduce signiﬁcantly the amount of recomputation required; see exercise 21.10. 

# 21.8 Summary 

In this chapter, we addressed the issue of ascribing a causal interpretation to a Bayesian network. While a causal interpretation does not provide any additional capabilities in terms of answering standard probabilistic queries, it provides the basic framework for answering causal queries — queries involving interventions in the world. We provided semantics for causal models in terms of the causal mechanism by which a variable’s value is generated. An intervention query can then be viewed as a substitution of the existing causal mechanism with one that simply forces the intervened variable to take on a particular value. 

We discussed the greater sensitivity of causal queries to the speciﬁcs of the model, including the speciﬁc orientations of the arcs and the presence of latent variables. Latent variables are particularly tricky, since they can induce correlations between the variables in the model that are hard to distinguish from causal relationships. These issues make the identiﬁcation of a causal model much more difcult than the selection of an adequate probabilistic model. 

We presented a class of situations in which a causal query can be answered exactly, using only a distribution over the observable variables, even when the model as a whole is not identiﬁable. In other cases, even if the query is not fully identiﬁable, we can often provide surprisingly strong bounds over the answer to a causal query. 

Besides intervention queries, causal models can also be used to answer counterfactual queries — queries about a sequence of events that we know to be diferent from the sequence that actually took place in the world. To answer such queries, we need to make explicit the random choices made in selecting the values of variables in the model; these random choices need to be preserved between the real and counterfactual worlds in order to maintain the correct semantics for the idea of a counterfactual. Functional causal models allow us to represent these random choices in a ﬁnite way, regardless of the (potentially unbounded) number of latent variables in the domain. We showed how to use functional causal models to answer counterfactual queries. While these models are even harder to identify than standard causal models, the techniques for partially identifying causal queries can also be used in this case. 

Finally, we discussed the controversial and challenging problem of learning causal models from data. Much of the work in this area has been devoted to the problem of inferring causal models from observational data alone. This problem is very challenging, especially when we allow for the possible presence of latent variables. We described both constraint-based and Bayesian methods for learning causal models from data, and we discussed their advantages and disadvantages. 

Causality is a fundamental concept when reasoning about many topics, ranging from speciﬁc scientiﬁc applications to commonsense reasoning. Causal networks provide a framework for performing this type of reasoning in a systematic and principled way. On the other side, the learning algorithms we described, by combining prior knowledge about domain structure with empirical data, can help us identify a more accurate causal structure, and perhaps obtain a better understanding of the domain. There are many possible applications of this framework in the realm of scientiﬁc discovery, both in the physical and life sciences and in the social sciences. 

# 21.9 Relevant Literature 

The use of functional equations to encode causal processes dates back at least as far as the work of Wright (1921), who used them to model genetic inheritance. Wright (1934) also used directed graphs to represent causal structures. 

The view of Bayesian networks as encoding causal processes was present throughout much of their history, and certainly played a signiﬁcant role in early work on constraint-based methods for learning network structure from data (Verma and Pearl 1990; Spirtes et al. 1991, 1993). The formal framework for viewing a Bayesian network as a causal graph was developed in the early and mid 1990s, primarily by two groups: by Spirtes, Glymour, and Scheines, and by Pearl and his students Balke and Galles. Much of this work is summarized in two seminal books: the early book of Spirtes, Glymour, and Scheines (1993) and the more recent book by Pearl (2000), on which much of the content of this chapter is based. The edited collection of Glymour and Cooper (1999) also reviews other important developments. 

The use of a causal model for analyzing the efect of interventions was introduced by Pearl and Verma (1991) and Spirtes, Glymour, and Scheines (1993). The formalization of the causal calculus, which allows the simpliﬁcation of intervention queries and their reformulation in terms of purely observable queries, was ﬁrst presented in detail in Pearl (1995). The example on smoking and cancer was also presented there. Based on these ideas, Galles and Pearl

 (1995) provide an algorithm for determining the identiﬁability of an intervention query. Dawid

 (2002, 2007) provides an alternative formulation of causal intervention that makes explicit use of decision variables. This perspective, which we used in section 21.3, signiﬁcantly simpliﬁes certain aspects of causal reasoning. 

The idea of making mechanisms explicit via response variables is based on ideas proposed in Rubin’s theory of counterfactuals (Rubin 1974). It was introduced into the framework of causal networks by Balke and Pearl (1994b,a), and in parallel by Heckerman and Shachter (1994), who use a somewhat diferent framework based on inﬂuence diagrams. Balke and Pearl (1994a) describe a method that uses the distribution over the observed variables to constrain the distribution of the response variables. The PeptAid example (example 21.25) is due to Balke and Pearl (1994a), who also performed the analysis of the cholesterol y mine example (box 21.B). Chickering and Pearl (1997) present a Gibbs sampling approach to Bayesian parameter estimation in causal settings. 

The work on constraint-based structure learning (described in section 18.2) was ﬁrst presented as an approach for learning causal networks. It was proposed and developed in the work of Verma and Pearl (1990) and in the work of Spirtes et al. (1993). Even this very early work was able to deal with latent variables. Since then, there has been signiﬁcant work on extending and improving these early algorithms. Spirtes, Meek, and Richardson (1999) present a state-of-the- art algorithm for identifying a PAG from data and show that it can accommodate both latent variables and selection bias. 

Heckerman, Meek, and Cooper (1999) proposed a Bayesian approach to causal discovery and the use of a Markov chain Monte Carlo algorithm for sampling structures in order to obtain probabilities of causal features. 

The extension of Bayesian structure learning to a combination of observational and inter- ventional data was ﬁrst developed by Cooper and Yoo (1999). These ideas were extended and applied by Pe’er et al. (2001) to the problem of identifying regulatory networks from gene ex- pression data, and by Sachs et al. (2005) to the problem of identifying signaling networks from ﬂuorescent microscopy data, as described in box 21.D. Tong and Koller (2001a,b) build on these ideas in addressing the problem of active learning — choosing a set of interventions so as best to learn a causal network model. 

# 21.10 Exercises 

# Exercise $21.1\star$ 

a. Prove proposition 21.2, which allows us to convert causal interventions in a query into observations. b. An alt ative condition for this proposition works in terms of the original graph $\mathcal{G}$ rather than the raph G $\mathcal{G}^{\dagger}$ . Let ${\mathcal{G}}_{X}$ denote the graph $\mathcal{G}$ , minus all edges going out of no in X . Show that d-separation criterion used in the proposition is equivalent to requiring that Y is d-separated from X given $Z,W$ in the graph $\mathcal{G}_{\overline{{Z}}\underline{{X}}}$ . 

# Exercise $21.2\times$ 

Prove proposition 21.3, which allows us to drop causal interventions from a query entirely. 

# Exercise ${\bf21.3\star}$ 

For probabilistic queries, we have that 

$$
\operatorname*{min}_{x}P(y\mid x)\le P(y)\le\operatorname*{max}_{x}P(y\mid x).
$$ 

Show that the same property does not hold for intervention queries. Speciﬁcally, provide an example where it is not the case that: 

$$
\operatorname*{min}_{x}P(y\mid d o(x))\le P(y)\le\operatorname*{max}_{x}P(y\mid d o(x)).
$$ 

# Exercise $21.4\star\star$ 

Show that every one of the diagrams in ﬁgure 21.3 is identiﬁable via the repeated application of proposi- tion 21.1, 21.2, and 21.3. 

# Exercise $21.5\star$ 

a. he ca gure $21.4\mathrm{g},$ , each of the queries $P(Z_{1}\mid d o(X))$ , $P(Z_{2}\mid d o(X))$ , $P(Y\mid d o(Z_{1}))$ | , and $P(Y\mid d o(Z_{2}))$ are identiﬁable. b. Explain why the efect of X on Y cannot be identiﬁable in this model. c. Show that we can identify both $P(Y\mid d o(X),d o(Z_{1}))$ and $P(Y\mid d o(X),d o(Z_{2}))$ . This example illustrates that the efect of a joint intervention may be more easily identiﬁed than the efect of each of its components. 

# Exercise 21.6 

As we discussed in box 21.C, under certain assumptions, we can reduce the cost of performing counterfac- tual inference to that of a standard probabilistic query. In particular, assume that we have a system status variable $X$ that is a noisy-or of the failure variables $X_{1},\ldots,X_{k}$ , and that there is no leak probability, so that $X=x^{0}$ when all $X_{i}=x_{i}^{0}$ (that is, $X$ is normal when all its components are normal). Furthermore, assume that only a single $X_{i}$ is in the failure mode $(X_{i}=x_{i}^{1}$ ). Show that 

$$
P(x^{\prime0}\mid x^{1},d o(x_{i}^{0}),e)=P(d_{i}^{1}\mid x^{1},e),
$$ 

where $Z_{i}$ is the noisy version of $X_{i}$ , as in deﬁnition 5.11. 

This exercise demonstrates computation of sufcient statistics with interventional data. The following table shows counts for diferent interventions. 
![](images/2fe81cb5d805a9b5f27679b158f83f96c28f250ac1ba92f4b7188029f22648e4.jpg) 
Calculate $M[x^{0};y^{0}z^{0}]$ , $M[y^{0};x^{0}z^{0}]$ , and $M[x^{0}]$ . 

# Exercise 21.8 

Consider the problem of learning a Gaussian yesian network from interventio $\mathcal{D}$ sec- tion 21.7.2, assume that each data instance in D is speciﬁed b ion $d o(Z[m]:=z[m])$ ; for each such data case, we have a fully observed data inst ce X $\mathcal{X}[m]=\xi[m]$ . Write down the sufcient statistics that would be used to score a network structure G from this data set. 

# Exercise ${\bf21.9\star}$ 

Consider e problem of Bayesian lear g for a functional causal mode $\mathcal{C}$ over a set of endogenous variables X . Assume we have a data set D where nous variables X are fully observed. Describe a way for approximating the parameter posterior $P(\nu\mid\mathcal{\tilde{X}})$ X using collapsed Gibbs sampling. Speciﬁcally, your algorit m should sample the response variables U and compute a closed-form distribution over the parameters ν . 

# Exercise $21.10\star\star$ 

Consider e problem of learning the structure of a functional causal model $\mathcal{C}$ over a set of endogenous variables . 

a. Using your answer from exercise 21.9, construct an algorithm for learning the structure of a causal model. Describe precisely the key steps used in the algorithm, including the search steps and the use of the Gibbs sampling algorithm to evaluate the score at each step. 

b. Now, assume that we are willing to stipulate that the response variables $U^{X}$ for each variable $X$ are independent. (This assumption is a very strong one, but it may be a reasonable approximation in some cases.) How can you signiﬁcantly improve the learning algorithm in this case? Provide a new pseudo-code description of the algorithm, and quantify the computational gains. 

# Exercise $21.11\star$ 

causal independence As for probabil e, we can deﬁ $(X\ \bot_{C}\ Y\ |\ Z)$ if, for any values $x,x^{\prime}\,\in\,\mathrm{~\hat{V}a l}(X)$ ∈ , we have $P(Y\mid d o(Z),d o({\pmb x}))=\dot{P}(Y\mid d o(Z),d o({\pmb x}^{\prime}))$ | . (Note nlike probabilistic independence — $(X\perp\dot{Y}\mid Z)$ ⊥ | — causal independence is not symmetric over $X,Y.)$ 

a. he statement: “For any value $\textbf{\em x}\in\mathsf{\Gamma}V a l(X)$ , we have that $P(Y\mid d o(Z),\hat{d o(x)})=\hat{P(Y\mid d o(Z))}$ | | .” (Hint: Use your result from exercise 21.3.) 

b. Prove that $(X\perp_{C}Y\mid Z,W)$ and $(W\perp_{C}Y\mid X,Z)$ mplies t $(X,W\perp_{C}Y\mid Z)$ . uitively, this property states that if changing X cannot afect $P(\dot{\mathbf{\calY}})$ when W is ﬁxed, and changing W cannot afect $\dot{P}(\pmb{Y})$ when $_{X}$ is ﬁxed, then changing $_{X}$ and $\mathbf{Y}$ together cannot afect $P(\pmb{Y})$ . 

# Exercise $21.12\star$ 

We discussed the issue of trying to use data to extract causal knowledge, that is, the directionality of an inﬂuence. In this problem, we will consider the interaction between this problem and both hidden variables and selection bias. 

![](images/94f13ca884d99726bdbe38d1bb547784a5589614763010969e9fb5e7d4264e21.jpg) 
Figure 21.9 Learned causal network for exercise 21.12 

Assume that our learning algorithm came up with the network in ﬁgure 21.9, which we are willing to assume is a perfect map for the distribution over the variables $A,B,C,D,E$ . Under this assumption, among which pairs of variables between which a causal path exists in this model does there also necessarily exist a causal path . . . 

a. . . . if we assume there are no hidden variables? b. . . . if we allow the possibility of one or more hidden variables? c. . . . if we allow for the possibility of selection bias? 

For each of these options, specify the pairs for which a causal path exists, and explain why it exists in every $\mathrm{I}_{\mathcal{X}}$ -equivalent structure. For the other pairs, provide an example of an $\mathrm{I}_{\mathcal{X}}$ -equivalent structure for which no causal path exists. 

# 22 Utilities and Decisions 

We now move from the task of simply reasoning under uncertainty — reaching conclusions about the current situation from partial evidence — to the task of deciding how to act in the world. In a decision-making setting, an agent has a set of possible actions and has to choose between them. Each action can lead to one of several outcomes, which the agent can prefer to diferent degrees. 

Most simply, the outcome of each action is known with certainty. In this case, the agent must simply select the action that leads to the outcome that is most preferred. Even this problem is far from trivial, since the set of outcomes can be large and complex and the agent must weigh diferent factors in determining which of the possible outcomes is most preferred. For example, when deciding which computer to buy, the agent must take into consideration the CPU speed, the amount of memory, the cost, the screen size, and many other factors. Deciding which of the possible conﬁgurations he most prefers can be quite difcult. 

Even more difcult is the decision-making task in situations where the outcome of an action is not fully determined. In this case, we must take into account both the probabilities of various outcomes and the preferences of the agent between these outcomes. Here, it is not enough to determine a preference ordering between the diferent outcomes. We must be able to ascribe preferences to complex scenarios involving probability distributions over possible outcomes. The framework of decision theory provides a formal foundation for this type of reasoning. This framework requires that we assign numerical utilities to the various possible outcome, encoding the agent’s preferences. In this chapter, we focus on a discussion of utilities functions and the principle of maximum expected utility , which is the foundation for decision making under uncertainty. In the next chapter, we discuss computationally tractable representations of an agent’s decision problem and the algorithmic task of ﬁnding an optimal strategy. 

# 22.1 Foundations: Maximizing Expected Utility 

In this section, we formally describe the basic decision-making task and deﬁne the principle of maximum expected utility. We also provide a formal justiﬁcation for this principle from basic axioms of rationality. 

# 22.1.1 Decision Making Under Uncertainty 

We begin with a simple motivating example. 

# Example 22.1 

expected utility 

Deﬁnition 22.1 lottery 

preference over lotteries 

Example 22.2 Consider a decision maker who encounters the following situation. She can invest in a high-tech company ( A ), where she can make a proﬁt of $\S4$ million with 20 percent probability and $\S O$ with 80 percent probability; or she can invest in pork belly futures $(\boldsymbol{B})$ ), where she can make $\S3$ million with 25 percent probability and $\S O$ with 75 percent probability. (That is, the pork belly investment is less proﬁtable but also less risky.) In order to choose between these two investment opportunities, the investor must compare her preferences between two scenarios, each of which encodes a probability distribution over outcomes: the ﬁrst scenario, which we denote $\pi_{A}$ , can be written as [\$4 million : $\mathrm{0.2;}\,\S0:0.8]$ ; the second scenario, denoted $\pi_{B}$ , has the form $[\S3m i l l i o n:0.25;\S0:0.75]$ . 

In order to ascertain which of these scenarios we prefer, it is not enough to determine that we prefer $\S4$ million to $\S3$ million to $\S0$ . We need some way to aggregate our preferences for these outcomes with the probabilities with which we will get each of them. One approach for doing this aggregation is to assign each outcome a numerical utility , where a higher utility value associated with an outcome indicates that this outcome is more preferred. Importantly, however, utility values indicate more than just an ordinal preference ranking between outcomes; their numerical value is signiﬁcant by itself, so that the relative values of diferent states tells us the strength of our preferences between them. This property allows us to combine the utility values of diferent states, allowing us to ascribe an expected utility to situations where we are uncertain about the outcome of an action. Thus, we can compare two possible actions using their expected utility, an ability critical for decision making under uncertainty. We now formalize these intuitions. 

$\pi$ over an ou me space $\mathcal{O}$ is a s $[\pi_{1}:\alpha_{1};.\,.\,.\,;\pi_{k}:\alpha_{k}]$ ch that $\alpha_{1},.\,.\,.\,,\alpha_{k}\in[0,1].$ , $\textstyle\sum_{i}\alpha_{i}=1$ P , and each $\pi_{i}$ is an outcome in O . For two lotteries $\pi_{1},\pi_{2},$ , if the agent prefers $\pi_{1}$ , we say that $\pi_{1}\succ\pi_{2}$ . If the agent is indiferent between the two lotteries, we say that $\pi_{1}\sim\pi_{2}$ . 

A comparison between two diferent scenarios involving uncertainty over the outcomes is quite difcult for most people. At ﬁrst glance, one might think that the “right” decision is the one that optimizes a person’s monetary gain. However, that approach rarely reﬂects the preferences of the decision maker. 

Consider a slightly diferent decision-making situation. Here, the investor must decide between company $C$ , where she earns $\S3$ million with certainty, and company $D$ , where she can earn $\S4$ million with probability 0 . 8 and $\S O$ with probability 0 . 2 . In other words, she is now comparing two lotteries $\pi_{C}=[\S3m i l l i o n:1]$ and $\pi_{D}=[\S4m i l l i o n:0.8;\S0:0.2]$ ] . The expected proﬁt of lottery $D$ is $\S3.2$ million, which is larger than the proﬁt of $\S3$ million from lottery $C$ . However, a vast majority of people prefer the option of lottery $C$ to that of lottery $D$ . 

The problem becomes far more complicated when one accounts for the fact that many decision-making situations involve aspects other than ﬁnancial gain. 

A general framework that allows us to make decisions such as these ascribes a numer- ical utility to diferent outcomes. An agent’s utilities describe her overall preferences, which can depend not only on monetary gains and losses, but also on all other relevant aspects. Each outcome $O$ is associated with a numerical value $U(o)$ , which is a numerical encoding of the agent’s “happiness” for this outcome. Importantly, utilities are not just ordinal values, denoting the agent’s preferences between the outcomes, but are actual numbers whose magnitude is meaningful. Thus, we can probabilistic ally aggregate utilities and compute their expectations over the diferent possible outcomes. 

We now make these intuitions more formal. 

Deﬁnition 22.2 decision-making situation outcome action 

utility function 

A decision-making situation $\mathcal{D}$ is deﬁned by the following elements: 

• a set of outcomes $\mathcal{O}=\left\{o_{1},.\,.\,.\,,o_{N}\right\}$ ; • a set of possible actions that the agent can take, $\mathcal{A}=\{a_{1},.\,.\,.\,,a_{K}\}$ ; • a probabilistic outcome model $P:\mathcal{A}\mapsto\Delta_{\mathcal{O}}$ , which deﬁnes a lottery $\pi_{a},$ which speciﬁes $^a$ probability distribution over outcomes given that the action a was taken; • $a$ utility function $U:\mathcal{O}\mapsto I\!\!R$ , where $U(o)$ is the agent’s preferences for the outcome $O$ . 

Note that the deﬁnition of an outcome can also include the action taken; outcomes that involve one action $a$ would then get probability 0 in the lottery induced by another action $a^{\prime}$ . 

Deﬁnition 22.3 MEU principle expected utility 

Example 22.3 The principle of maximum expected utility ( MEU principle ) asserts that, in a decision-making situation $\mathcal{D}$ , we should choose the action $a$ that maximizes the expected utility : 

$$
\mathrm{EU}[\mathcal{D}[a]]=\sum_{o\in\mathcal{O}}\pi_{a}(o)U(o).
$$ 

Consider a decision situation $\mathcal{T}_{F}$ where a college graduate is trying to decide whether to start up a company that builds widgets. The potential entrepreneur does not know how large the market demand for widgets really is, but he has a distribution: the demand is either $m^{0}$ —nonexistent, $m^{1}$ —low, or $m^{2}$ —high, with probabilities 0 . 5 , 0 . 3 , and 0 . 2 respectively. The entrepreneur’s proﬁt, if he founds the startup, depends on the situation. If the demand is nonexistent, he loses a signiﬁcant amount of money (outcome $o_{1}$ ); if it is low, he sells the company and makes a small proﬁt (outcome $O_{2.}$ ); if it is high, he goes public and makes a fortune (outcome $O_{3}$ ). If he does not found the startup, he loses nothing and earns nothing (outcome $O_{\mathrm{0}}$ ). These outcomes might involve attributes other than money. For example, if he loses a signiﬁcant amount of money, he also loses his credibility and his ability to start another company later on. Let us assume that the agent’s utilities for the four outcomes are: $7(o_{0})=0;U(o_{1})=-7;U(o_{2})=5;U(o_{3})=20$ . The agent’s expected utility for the action of founding the company (denoted $f^{1}$ ) is 

$$
\operatorname{EU}[\mathcal{D}[f^{1}]]=0.5\cdot(-7)+0.3\cdot5+0.2\cdot20=2.
$$ 

His expected utility for the action of not founding the company (denoted $f^{0}.$ ) is 0 . The action choice maximizing the expected utility is therefore $f^{1}$ . 

Our deﬁnition of a decision-making situation is very abstract, resulting in the impression that the setting is one where an agent takes a single simple action, resulting in a single simple outcome. In fact, both actions and outcomes can be quite complex. Actions can be complete strategies involving sequences of decisions, and outcomes (as in box 22.A) can also involve multiple aspects. We will return to these issues later on. 

# 22.1.2 Theoretical Justiﬁcation $\star$ 

What justiﬁes the principle of maximizing expected utility, with its associated assumption re- garding the existence of a numerical utility function, as a deﬁnition of rational behavior? It turns out that there are several theoretical analyses that can be used to prove the existence of such a function. At a high level, these analyses postulate some set of axioms that characterize the behavior of a rational decision maker. They then show that, for any agent whose decisions abide by these postulates, there exists some utility function $U$ such that the agent’s decisions are equivalent to maximizing the expected utility relative to $U$ . 

# 

Deﬁnition 22.4 compound lottery 

# Example 22.4 

rationality postulates 

The analysis in this chapter is based on the premise that a decision maker under uncertainty must be able to decide between diferent lotteries. We then make a set of assumptions about the nature of the agent’s preferences over lotteries; these assumptions arguably should hold for the preferences of any rational agent. For an agent whose preferences satisfy these axioms, we prove that there exists a utility function $U$ such that the agent’s preferences are equivalent to those obtained by maximizing the expected utility relative to $U$ . We ﬁrst extend the concept of a lottery. 

$A$ compound lottery $\pi$ over an outcome ace $\mathcal{O}$ is a set $[\pi_{1}\,:\,\alpha_{1};.\,.\,.\,;\pi_{k}\;:\;\alpha_{k}]$ such that $\begin{array}{r}{\alpha_{1},.\,.\,.\,,\alpha_{k}\in[0,1],\,\sum_{i}\alpha_{i}=1,\;}\end{array}$ , and each $\pi_{i}$ is either an outcome in O or another lottery. 

One example of a compound lottery is a game where we ﬁrst toss a coin; if it comes up heads, we get $\S3\ (o_{1});$ if it comes up tails, we participate in another subgame where we draw a random card from a deck, and if it comes out spades, we get $\S5O\ (O_{2}),$ otherwise we get nothing $(O_{3,i}$ . This lottery would be represented as $[o_{1}:0.5;[o_{2}:0.25;o_{3}:0.75]:0.5$ 5] . 

We can now state the postulates of rationality regarding the agent’s preferences over lotteries. At ﬁrst glance, each these postulates seems fairly reasonable, but each of them has been subject to signiﬁcant criticism and discussion in the literature. 

• (A1) Orderability: For all lotteries $\pi_{1},\pi_{2}$ , either 

$$
(\pi_{1}\succ\pi_{2})\ \mathrm{or}\ (\pi_{1}\sim\pi_{2})
$$ 

This postulate asserts that an agent must know what he wants; that is, for any pair of lotteries, he must prefer one, prefer the other, or consider them to be equivalent. Note that this assumption is not a trivial one; as we discussed, it is hard for people to come up with preferences over lotteries. 

• (A2) Transitivity: For all lotteries $\pi_{1},\pi_{2},\pi_{3}$ , we have that: 

The transitivity postulate asserts that preferences are transitive, so that if the agent prefers lottery 1 to lottery 2 and lottery 2 to lottery 3, he also prefers lottery 1 to lottery 3. Although transitivity seems very compelling on normative grounds, it is the most frequently violated axiom in practice. One hypothesis is that these “mistakes” arise when a person is forced to make choices between inherently incomparable alternatives. The idea is that each pairwise comparison invokes a preference response on a diferent “attribute” (for instance, money, time, health). Although each scale itself may be transitive, their combination need not be. A similar situation arises when the overall preference arises as an aggregate of the preferences of several individuals. 

• (A3) Continuity: For all lotteries $\pi_{1},\pi_{2},\pi_{3}$ , 

This postulate asserts that if $\pi_{2}$ is somewhere between $\pi_{1}$ and $\pi_{3}$ , then there should be some lottery between $\pi_{1}$ and $\pi_{3}$ , which is equivalent to $\pi_{2}$ . For our simple Entrepreneur example, we might have that $o_{0}\sim[o_{1}:0.8;o_{3}:0.2]$ . This axiom excludes the possibility that one alternative is “inﬁnitely better” than another one, in the sense that any probability mixture involving the former is preferable to the latter. It therefore captures the relationship between probabilities and preferences and the form in which they compensate for each other. 

• (A4) Monotonicity: For all lotteries $\pi_{1},\pi_{2}$ , and probabilities $\alpha,\beta$ , 

$$
(\pi_{1}\succ\pi_{2}),(\alpha\geq\beta)\Rightarrow([\pi_{1}:\alpha;\pi_{2}:(1-\alpha)]\succ[\pi_{1}:\beta;\pi_{2}:(1-\beta)]).
$$ 

This postulate asserts that an agent prefers that better things happen with higher probability. Again, although this attribute seems unobjectionable, it has been argued that risky behavior such as Russian roulette violates this axiom. People who choose to engage in such behavior seem to prefer a probability mixture of “life” and “death” to “life,” even though they (pre- sumably) prefer “life” to “death.” This argument can be resolved by revising the outcome descriptions, incorporating the aspect of the thrill obtained by playing the game. 

• (A5) Subst it ut ability: For all lotteries $\pi_{1},\pi_{2},\pi_{3}$ , and probabilities $\alpha$ , 

$$
(\pi_{1}\sim\pi_{2})\Rightarrow([\pi_{1}:\alpha;\pi_{3}:(1-\alpha)]\sim[\pi_{2}:\alpha;\pi_{3}:(1-\alpha)]).
$$ 

This axiom states that if $\pi_{1}$ and $\pi_{2}$ are equally preferred, we can substitute one for the other without changing our preferences. 

• (A6) Decomposability: For all lotteries $\pi_{1},\pi_{2}$ , and probabilities $\alpha,\beta$ , 

$$
[\pi_{1}:\alpha,[\pi_{2}:\beta,\pi_{3}:(1-\beta)]:(1-\alpha)]\sim[\pi_{1}:\alpha,\pi_{2}:(1-\alpha)\beta,\pi_{3}:(1-\alpha)(1-\beta)]:(1-\alpha)!]
$$ 

This postulate says that compound lotteries are equivalent to ﬂat ones. For example, our lottery in example 22.4 would be equivalent to the lottery 

$$
[o_{1}:0.5;o_{2}:0.125;o_{3}:0.375].
$$ 

Intuitively, this axiom implies that the preferences depend only on outcomes, not the process in which they are obtained. It implies that a person does not derive any additional pleasure (or displeasure) from suspense or participation in the game. 

If we are willing to accept these postulates, we can derive the following result: 

Theorem 22.1 Assume that we have an agent whose preferences over lotteries satisfy the axioms (A1)–(A6). Then there exists a function $U:{\mathcal{O}}\mapsto I\!\!R,$ , such that, for any pair of lotteries $\pi,\pi^{\prime}$ , we have that $\pi\prec\pi^{\prime}$ if and only if $U(\pi)<U(\pi^{\prime})$ , where we deﬁne (recursively) the expected utility of any lottery as: 

$$
U{\big(}[\pi_{1}:\alpha_{1},.\,.\,,\pi_{k}:\alpha_{k}]{\big)}=\sum_{i=1}^{k}\alpha_{i}U(\pi_{i}).
$$ 

That is, the utility of a lottery is simply the expectation of the utilities of its components. 

anchor outcome Proof Our goal is take a preference relation $\prec$ that satisﬁes these axioms, and to construct a utility function U over consequ ces such that $\prec$ is equivalent to implementing the MEU principle over the utility function U . We take the least and most preferred outcomes $o_{\mathrm{min}}$ and $O_{\mathrm{max}}$ ; these outcomes are typically known as anchor outcomes . By orderability (A1) and transitivity (A2), such outcomes must exist. We assign $U(o_{\mathrm{min}})\,:=\,0$ and $U(o_{\mathrm{max}}):=1$ . By orderability, we have that for any other outcome $O$ : 

$$
o_{\operatorname*{min}}\preceq o\preceq o_{\operatorname*{max}}.
$$ 

By continuity (A3), there must exist a probability $\alpha$ such that 

$$
[o:1]\sim[o_{\mathrm{min}}:(1-\alpha);o_{\mathrm{max}}:\alpha]
$$ 

We assign $U(o):=\,\alpha$ . The axioms can then be used to show that the assignment of util- ities to lotteries resulting from applying the expected utility-principle results in an ordering that is consistent with our preferences. We leave the completion of this proof as an exercise (exercise 22.1). 

From an operational perspective, this discussion gives us a formal justiﬁcation for the principle of maximum expected utility. When we have a set of outcomes, we ascribe a numerical utility to each one. If we have a set of actions that induce diferent lotteries over outcomes, we should choose the action whose expected utility is largest; as shown by theorem 22.1, this choice is equivalent to choosing the action that induces the lottery we most prefer. 

# 22.2 Utility Curves 

The preceding analysis shows that, under certain assumptions, a utility function must exist. However, it does not provide us with an understanding of utility functions. In this section, we take a more detailed look at the form of a utility functions and its connection to the utility function properties. 

A utility function assigns numeric values to various possible outcomes. These outcomes can vary along multiple dimensions. Most obvious is monetary gain, but most settings involve other attributes as well. We begin in this section by considering the utility of simple outcomes, involving only a single attribute. We discuss the form of a utility function over a single attribute and the efects of the utility function on the agent’s behavior. We focus on monetary outcomes, which are the most common and easy to understand. However, many of the issues we discuss in this section — those relating to risk attitudes and rationality — are general in their scope, and they apply also to other types of outcomes. 

# 22.2.1 Utility of Money 

Consider a decision-making situation where the outcomes are simply monetary gains or losses. In this simple setting, it is tempting to assume that the utility of an outcome is simply the amount of money gained in that outcome (with losses corresponding to negative utilities). However, as we discussed in example 22.2, most people do not always choose the outcome that maximizes their expected monetary gain. Making such a decision is not irrational; it simply implies that, for most people, their utility for an outcome is not simply the amount of money they have in that outcome. 

utility curve 

Example 22.5 

Saint Petersburg paradox 

Example 22.6 

Consider a graph whose $X$ -axis is the monetary gain a person obtains in an outcome (with losses corresponding to negative amounts), and whose $Y$ -axis is the person’s utility for that outcome. In general, most people’s utility is monotonic in money, so that they prefer outcomes with more money to outcomes with less. However, if we draw a curve representing a person’s utility as a function of the amount of money he or she gains in an outcome, that curve is rarely a straight line. This nonlinearity is the “justiﬁcation” for the rationality of the preferences we observe in practice in example 22.2. 

Let $c_{0}$ represent the agent’s current ﬁnancial status, and assume for simplicity that he assigns a utility of $O$ to $c_{0}$ . If he assigns a utility of 10 to the consequence $c_{0}+3$ million and $l2$ to the $c_{0}+4$ million , then the expected utility of the gamble in example 22.2 is $0.2\cdot0+0.8$ · $12=9.6<10$ . Therefore, with this utility function, the agent’s decision is completely rational. 

A famous example of the nonlinearity of the utility of money is the Saint Petersburg paradox : 

$$
\sum_{n=1}^{\infty}P(H_{n})\mathrm{Payoff}(H_{n})=\sum_{n=1}^{\infty}{\frac{1}{2^{n}}}2^{n}=1+1+1+\ldots=\infty.
$$ 

Therefore, you should be willing to pay any amount to play this game. However, most people are willing to pay only about $\S2$ . 

Empirical psychological studies show that people’s utility functions in a certain range often grow logarithmically in the amount of monetary gain. That is, the utility of the outcome $c_{k}$ , corresponding to an agent’s current ﬁnancial status plus $\S k$ , looks like $\alpha+\beta\log(k+\gamma)$ . In the Saint Petersburg example, if we take $U(c_{k})=\log_{2}k$ , we get: 

$$
\sum_{n=1}^{\infty}P(H_{n})U(P a y o f\!\!f(H_{n}))=\sum_{n=1}^{\infty}{\frac{1}{2^{n}}}U(c_{2^{n}})=\sum_{n=1}^{\infty}{\frac{n}{2^{n}}}=2,
$$ 

which is precisely the amount that most people are willing to pay in order to play this game. In general, most people’s utility function tends to be concave for positive amount of money, so that the incremental value of additional money decreases as the amount of wealth grows. 

![](images/075af0c3d91cfaa631e420e5e8cc5c56e8b7c2d161dca940b61e98260e46f600.jpg) 
Figure 22.1 Example curve for the utility of money 

Conversely, for negative amounts of money (debts), the shape of the curve often has the opposite shape, as shown in ﬁgure 22.1. Thus, for many people, going into debt of $\S1$ million has signiﬁcant negative utility, but the additional negative utility incurred by an extra $\S1$ million of debt is a lot lower. Formally, $\vert U(-\S2,000,000)\,-\,U(-\S1,000,000)\vert$ is often signiﬁcantly less than $\vert U(-\S1,000,000)-U(\S0)\vert$ . 

# 22.2.2 Attitudes Toward Risk 

risk risk-averse 

certainty equivalent 

insurance premium There is a tight connection between the form of a person’s utility curve and his behavior in diferent decision-making situations. In particular, the shape of this curve determines the person’s attitude toward risk . A concave function, as in ﬁgure 22.2, indicates that the agent is risk-averse : he prefers a sure thing to a gamble with the same payof. Consider in more detail the risk-averse curve of ﬁgure 22.2. We see that the utility of a lottery such as $\pi\,=\,[\S1000\,:\,0.5,\S0\,:\,0.5]$ is lower than the utility of getting $\S500$ with certainty. Indeed, risk-averse preferences are characteristic of most people, especially when large sums of money are involved. In particular, recall example 22.2, where we compared a lottery where we win $\S3$ million with certainty to one where we win $\S4$ million with probability 0 . 8 . As we discussed, most people prefer the ﬁrst lottery to the second, despite the fact that the expected monetary gain in the ﬁrst lottery is lower. This behavior can be explained by a risk-averse utility function in that region. 

Returning to the lottery $\pi$ , empirical research shows that many people are indiferent between playing $\pi$ and the outcome where they get (around) $\S400$ with certainty; that is, the utilities of the lottery and the outcome are similar. The amount $\S400$ is called the certainty equivalent of the lottery. It is the amount of “sure thing” money that people are willing to trade for a lottery. The diference between the expected monetary reward of $\S500$ and the certainty equivalent of $\S500$ is called the insurance premium , and for good reason. The premium people pay to the insurance company is precisely to guarantee a sure thing (a sure small loss) as opposed to a lottery where one of the consequences involves a large negative utility (for example, the price of rebuilding the house if it burns down). 

As we discussed, people are typically risk-averse. However, they often seek risk when the certain loss is small (relative to their ﬁnancial situation). Indeed, lotteries and other forms of gambling exploit precisely this phenomenon. When the agent prefers the lottery to the certainty 

![](images/424d027409fd8b42e2d0f8c6da857030b253cad7a3837480eae0f12e9e9eec84.jpg) 
Figure 22.2 Utility curve and its consequences to an agent’s attitude toward risk 

risk-seeking risk-neutral equivalent, he is said to be risk-seeking , a behavior that corresponds to a curve whose shape is convex. Finally, if the agent’s utility curve is linear, he is said to be risk-neutral . Most utility curves are locally linear, which means we can assume risk neutrality for small risks and rewards. Finally, as we noted, people are rarely consistent about risk throughout the entire monetary range: They are often risk-averse for positive gains, but can be risk-seeking for large negative amounts (going into debt). Thus, in our example, someone who is already $\S10$ million in debt might choose to accept a gamble on a fair coin with $\S10$ million payof on heads and a $\S20$ million loss on tails. 

# 22.2.3 Rationality 

The framework of utility curves provides a rich language for describing complex behaviors, including risk-averse, risk-seeking, or risk-neutral behaviors. They can even change their risk preferences over the range. One may thus be tempted to conclude that, for any behavior proﬁle, there is some utility function for which that behavior is rational. However, that conclusion turns out to be false; indeed, empirical evidence shows that people’s preferences are rarely rational under our deﬁnitions. 

Example 22.7 Consider again the two simple lotteries in example 22.2 and example 22.1. In the two examples, we had four lotteries: 

$$
\begin{array}{r l}{\pi_{A}\ :\ [\S4m i l l i o n:0.2;\S0:0.8]}\\ {\pi_{B}\ :\ [\S3m i l l i o n:0.25;\S0:0.75]}\\ {\pi_{C}\ :\ [\S3m i l l i o n:1]}\\ {\pi_{D}\ :\ [\S4m i l l i o n:0.8;\S0:0.2].}\end{array}
$$ 

Most people, by an overwhelming majority, prefer $\pi_{C}$ to $\pi_{D}$ . The opinions on $\pi_{A}$ versus $\pi_{B}$ are more divided, but quite a number of people prefer $\pi_{A}$ to $\pi_{B}$ . Each of these two preferences $-\:\pi_{D}\;\succ\;\pi_{C}$ and $\pi_{A}~\succ~\pi_{B}~\;.$ — is rational relative to some utility functions. However, their combination is not — there is no utility function that is consistent with both of these preferences. $\mathit{T o}$ understand why, assume (purely to simplify the presentation) that $U(\S0)\,=\,0$ . In this case, preferring $\pi_{C}$ to $\pi_{D}$ is equivalent to saying that 

$U(c_{3,000,000})>0.8\cdot U(c_{4,000,000}).$ On the other hand, preferring $\pi_{A}$ to $\pi_{B}$ is equivalent to: $\begin{array}{l l l}{{0.2\cdot U\bigl(c_{4,000,000}\bigr)}}&{{>}}&{{0.25\cdot U\bigl(c_{3,000,000}\bigr)}}\\ {{0.8\cdot U\bigl(c_{4,000,000}\bigr)}}&{{>}}&{{U\bigl(c_{3,000,000}\bigr).}}\end{array}$ 

Multiplying both sides of the ﬁrst inequality by 4, we see that these two statements are directly contradictory, so that these preferences are inconsistent with decision-theoretic foundations, for any utility function . 

Thus, people are often irrational, in that their choices do not satisfy the principle of maximum expected utility relative to any utility function. When confronted with their “irrationality,” the responses of people vary. Some feel that they have learned an important lesson, which often afects other decisions that they make. For example, some subjects have been observed to cancel their automobile collision insurance and take out more life insurance. In other cases, people stick to their preferences even after seeing the expected utility analysis. These latter cases indicate that the principle of maximizing expected utility is not, in general, an adequate descriptive model of human behavior. As a consequence, there have been many proposals for alternative deﬁnitions of rationality that attempt to provide a better ﬁt to the behavior of human decision makers. Although of great interest from a psychological perspective, there is no reason to believe that these frameworks will provide a better basis for building automated decision- making systems. Alternatively, we can view decision theory as a normative model that provides the “right” formal basis for rational behavior, regardless of human behavior. One can then argue that we should design automated decision-making systems based on these foundations; indeed, so far, most such systems have been based on the precepts of decision theory. 

# 22.3 Utility Elicitation 

# 22.3.1 Utility Elicitation Procedures 

How do we acquire an appropriate utility function to use in a given setting? In many ways, this problem is much harder than acquiring a probabilistic model. In general, we can reasonably assume that the probabilities of chance events apply to an entire population and acquire a single probabilistic model for the whole population. For example, when constructing a medical diagnosis network, the probabilities will usually be learned from data or acquired from a human expert who understands the statistics of the domain. By contrast, utilities are inherently personal, and people often have very diferent preference orderings in the same situation. Thus, the utility function we use needs to be acquired for the individual person or entity for whom the decision is being made. Moreover, as we discussed, probability values can be learned from data by observing empirical frequencies in the population. The individuality of utility values, and the fact that they are never observed directly, makes it difcult to apply similar learning methods to the utility acquisition task. 

utility elicitation standard gamble 

indiference point 

There have been several methods proposed for eliciting utilities from people. The most classical method is the standard gamble method, which is based directly on the axioms of utility theory. In the proof of theorem 22.1, we selected two anchor states — our least preferred and most preferred states $s_{\perp}$ and $s\top$ . We then used the continuity axiom (equation (22.3)) to place each state on a continuous spectrum between these two anchor states, by ﬁnding the indiference point $\alpha$ — a probability value $\alpha\in[0,1]$ such that $s\sim[s_{\perp}:(1-\alpha);s_{\top}:\alpha]$ . 

We can convert this idea to a utility elicitation procedure as follows. We select a pair of anchor states. In most cases, these are determined in advance, independently of the user. For example, in a medical decision-making situation, $s_{\perp}$ is often “death,” whereas $s\top$ is an immediate and complete cure. For any outcome $s$ , we can now try to ﬁnd the indiference point. It is generally assumed that we cannot ask a user to assess the value of $\alpha$ directly. We therefore use some procedure that s arches over the space of possible $\alpha$ ’s. If $s\prec[s_{\bot}:(1-\alpha);s\tau:\alpha]_{;}$ , we consider lower values of α , and if $s\succ[s_{\bot}:(1-\alpha);s_{\top}:\alpha]$ , we consider higher values, until we ﬁnd the indiference point. Taking $U(s_{\perp})=0$ and $U(s_{\top})=1$ , we simply take $U(s)=\alpha$ . 

The standard gamble procedure is satisfying because of its sound theoretical foundations. However, it is very difcult for people to apply in practice, especially in situations involving large numbers of outcomes. Moreover, many independent studies have shown that the ﬁnal values obtained in the process of standard gamble elicitation are sensitive to the choice of anchors and to the choice of the search procedure. 

time trade-of 

visual-analog scale 

Several other methods for utility elicitation have been proposed to address these limitations. For example, time trade-of tries to compare two outcomes: (1) $t$ years (where $t$ is the patient’s life expectancy) in the current state of health (state $s$ ), and (2) $t^{\prime}$ years (where $t^{\prime}<t)$ ) in perfect health (the outcome $s\top$ ). As in standard gamble, $t^{\prime}$ is varied until the indiferent point is reached, and the utility of the state $s$ is taken to be proportional to $t^{\prime}$ at that point. Another method, the visual-analog scale , simply asks users to point out their utilities on some scale. 

Overall, each of the methods proposed has signiﬁcant limitations in practice. Moreover, the results obtained for the same individual using diferent methods are usually quite diferent, putting into question the results obtained by any method. Indeed, one might wonder whether there even exists such an object as a person’s “true utility value.” Nevertheless, one can still argue that decisions made for an individual using his or her own utility function (even with the imprecisions involved in the process) are generally better for that individual than decisions made using some “global” utility function determined for the entire population. 

# 22.3.2 Utility of Human Life 

Attributes whose utility function is particularly difcult to acquire are those involving human life. Clearly, such factors play a key role in medical decision-making situations. However, they also appear in a wide variety of other settings. For example, even a simple decision such as whether to replace worn-out tires for a car involves the reduced risk of death or serious injury in a car with new tires. 

Because utility theory requires that we reduce all outcomes to a single numerical value, we are forced to place a utility value on human life, placing it on the same scale as other factors, such as money. Many people ﬁnd this notion morally repugnant, and some simply refuse to do so. However, the fact of the matter is that, in making decisions, one makes these trade-ofs, whether consciously or unconsciously. For example, airplanes are not overhauled after each trip, even though that would clearly improve safety. Not all cars are made with airbags, even though they are known to save lives. Many people accept an extra stopover on a ﬂight in order to save money, even though most airplane accidents happen on takeof and landing. 

Placing a utility on human life raises severe psychological and philosophical difculties. One such difculty relates to actions involving some probability of death. The naive approach would be to elicit the utility of the outcome death and then estimate the utility of an outcome involving some probability $p$ of death as $p\cdot U(d e a t h)$ . However, this approach implies that people’s utility is linear in their probability of death, an assumption which is generally false. In other words, even if a person is willing to accept $\S50$ for an outcome involving a one-in-a-million chance of death, it does not mean that he would be willing to accept $\S50$ million for the outcome of death with certainty. Note that this example shows that, at least for this case, people violate the basic assumption of decision theory: that a person’s preference for an uncertain outcome can be evaluated using expected utility, which is linear in the probabilities. 

A more appropriate approach is to encode explicitly the chance of death. Thus, a key metric used to measure utilities for outcomes involving risk to human life is the micromort — a one-in- a-million chance of death. Several studies across a range of people have shown that a micromort is worth about $\S20$ in 1980 dollars, or under $\S50$ in today’s dollars. We can consider a utility curve whose $X$ -axis is micromorts. As for monetary utility, this curve behaves diferently for positive and negative values. For example, many people are not willing to pay very much to remove a risk of death, but require signiﬁcant payment in order to assume additional risk. 

Micromorts are useful for evaluating situations where the primary consideration is the proba- bility that death will occur. However, in many situations, particularly in medical decision making, the issue is not the chance of immediate death, but rather the amount of life that a person has remaining. In one approach, we can evaluate outcomes using life expectancy, where we would construct a utility curve whose $X$ axis was the number of expected years of life. However, our preferences for outcomes are generally much more complex, since they involve not only the quantity but also the quality of life . In some cases, a person may prefer to live for fewer years, but in better health, than to live longer in a state where he is in pain or is unable to perform certain activities. The trade-ofs here are quite complex, and highly personal. 

QALY 

One approach to simplifying this complex problem is by measuring outcomes using units called QALY s — quality-adjusted life years . A year of life in good health with no inﬁrmities is worth 1 QALY. A year of life in poor health is discounted, and it is worth some fraction (less than 1 ) of a QALY. (In fact, some health states — for example, those involving signiﬁcant pain and loss of function — may even be worth negative QALYs.) Using QALYs, we can assign a single numerical score to complex outcomes, where a person’s state of health can evolve over time. QALYs are much more widely used than micromorts as a measure of utility in medical and social-policy decision making. 

# 22.4 Utilities of Complex Outcomes 

So far, we have largely focused on outcomes involving only a single attribute. In this case, we can write down our utility function as a simple table in the case of discrete outcomes, or as a curve in the case of continuous-valued outcomes (such as money). In practice, however, outcomes often involve multiple facets. In a medical decision-making situation, outcomes might involve pain and sufering, long-term quality of life, risk of death, ﬁnancial burdens, and more. Even in a much “simpler” setting such as travel planning, outcomes involve money, comfort of accommodations, availability of desired activities, and more. A utility function must incorporate the importance of these diferent attributes, and the preferences for various values that they might take, in order to produce a single numeric value for each outcome. 

Our utility function in domains such as this has to construct a single number for each outcome that depends on the values of all of the relevant variables. More precisely, assume that an outcome is described by an assignment o alues to some set of variables $V=\{V_{1},.\,.\,.\,,V_{k}\}$ ; we then have to deﬁne a utility function $\begin{array}{r l}{U}&{:\quad V a l(V)\:\mapsto\:I\!\!R}\end{array}$ 7→ . As usual, the size of this representation is exponential in $k$ . 

In the case of probabilities, we addressed the issue of exponential blowup by exploiting structure in the distribution. We showed a direct connection between independence properties of the distribution and our ability to represent it compactly as a product of smaller factors. As we now discuss, very similar ideas apply in the setting of utility functions. 

Speciﬁcally, we can show a correspondence between “independence properties” among utility attributes of an agent and our ability to factor his utility function into a combi- nation of subutility functions , each deﬁned over a subset of utility attributes. A subutility function is a function $f:V a l(Y)\mapsto I\!\!R_{\mathfrak{z}}$ , for some $Y\subseteq V$ , where $Y$ is the scope of $f$ . 

However, the notion of independence in this setting is somewhat subtle. A utility function on its own does not induce behavior; it is a meaningful entity only in the context of a decision- making setting. Thus, our independence properties must be deﬁned in that context as well. As we will see, there is not a single deﬁnition of independence that is obviously the right choice; several deﬁnitions are plausible, each with its own properties. 

# 22.4.1 Preference and Utility Independence $\star$ 

To understand the notion of independence in decision making, we begin by considering the simpler setting, where we are making decisions in the absence of uncertainty. Here, we need only consider preferences on outcomes. Let $X,Y$ be a disjoint partition of our set of utility attributes $V$ . We thus hav preference ordering $\prec$ $(x,y)$ . 

When can we say that X is “independent” of $Y?$ ? Intuitively, if we are given $Y\,=\,y$ , we can now consider our preferences over the possible values $_{_{x}}$ , given that $_{_y}$ holds. Thus, we ha ference ordering $\prec_{y}$ over values $\pmb{x}\in V a l(\pmb{X})$ , wh e we write ${\pmb x}_{1}\prec_{\pmb y}{\pmb x}_{2}$ if ( $({\pmb x}_{1},{\pmb y})\,\prec\,({\pmb x}_{2},{\pmb y})$ ≺ . In general, the ord ng induced by one value y is d erent from the ordering induced by another. We say that X is preferentially independent of Y $Y$ if all values $_{_y}$ induce the same ordering over $V a l(X)$ . More precisely: 

$V a l(Y)$ , and for all $x_{1},x_{2}\in V a l(X)$ , we have that 

$$
x_{1}\prec_{y}x_{2}\quad\iff\quad x_{1}\prec_{y^{\prime}}x_{2}.
$$ 

Note that preferential independence is not a symmetric relation: 

# Example 22.8 

Consider an entrepreneur whose utility function $U(S,F)$ involves two binary-valued attributes: the success of his company $(S)$ and the fame he gets $(F_{\cdot}$ ). One reasonable preference ordering over outcomes might be: 

$$
(s^{0},f^{1})\prec(s^{0},f^{0})\prec(s^{1},f^{0})\prec(s^{1},f^{1}).
$$ 

That is, the most-preferred state is where he is successful and famous; the next-preferred state is where he is successful but not famous; then the second-next-preferred state is where he is unsuccessful but unknown; and the least-preferred state is where he is unsuccessful and (in)famous. In this preference ordering, we have that $S$ is preferentially independent of $F$ , since the entrepreneur prefers to be successful w ther he is famous or not $(\!(s^{0},f^{1})\,\prec\,(s^{1},f^{1})$ and $(s^{0},f^{0})\prec(s^{1},f^{0}))$ . On the other hand, F is not preferentially independent of S , since the entrepreneur prefers to be famous if successful but unknown if he is unsuccessful. 

When we move to the more complex case of reasoning under uncertainty, we compare decisions that induce lotteries over outcomes. Thus, our notion of independence must be deﬁned relative to this omplex setting. From now on, let $\prec,U$ be a pair, where $U$ is function over V $V a l(V)$ , and $\prec$ is the a ciated preference ordering for lotteries over $V a l(V)$ . We deﬁne independence properties for U in terms of $\prec$ . 

Our ﬁrst task is to deﬁne the notion of a conditional preference structure, where we “ﬁx” the value of some subset of variables $Y$ . This structure deﬁn a preference ordering $\prec_{y}$ for lotteries over $V a l(X)$ , given some particular instantiation $_{_y}$ to $Y$ . The deﬁnition is a straightforward generalization of the one we used for preferences over outcomes: 

Deﬁnition 22.6 conditional preference structure 

$$
\pi_{1}^{X}\prec_{\pmb{y}}\pi_{2}^{X}\ \ \ \ \ i f\ \ \ \ (\pi_{1}^{X},{\bf1}_{\pmb{y}})\prec(\pi_{2}^{X},{\bf1}_{\pmb{y}}),
$$ 

where $(\pi^{X},{\bf1}_{y})$ assigns probability $\pi^{X}({\pmb x})$ to any assignment $(x,y)$ and probability 0 to any assignment $(x,y^{\prime})$ for $\pmb{y}^{\prime}\neq\pmb{y}$ . 

In other words, the preference ord ng $\prec_{y}$ “expands” lotteries over $V a l(X)$ by having $Y=y$ with probability 1 , and then using . 

With this deﬁnition, we can now generalize preferential independence in the obvious way: $X$ is utility independent of $Y=V-X$ whe conditional preferences for lotteries over $X$ do not depend on the particular value $_{_y}$ given to $Y$ . 

Deﬁnition 22.7 utility independence 

We say tha $X$ tility independent of $Y=V-X$ if, for all $y,y^{\prime}\in V a l(Y)$ , and for any pair of lotteries $\pi_{1}^{X},\pi_{2}^{X}$ over $V a l(X)$ , we have that: 

$$
\pi_{1}^{X}\prec_{\pmb{y}}\pi_{2}^{X}\quad\Leftrightarrow\quad\pi_{1}^{X}\prec_{\pmb{y}^{\prime}}\pi_{2}^{X}.
$$ 

Because utility independence is a straight generalization of preference independence, it, too, is not symmetric. 

Note that utility independence is only deﬁned for a set of variables and its complement. This limitation is inevitable in the context of decision making, since we can deﬁne preferences only over entire outcomes, and therefore every variable must be assigned a value somehow. 

Diferent sets of utility independence assumptions give rise to diferent decompositions of the utility function. Most basically, for a pair $(\prec,U)$ as before, we have that: 

Proposition 22.1 A set $X$ is utility independent of $Y=V-X$ in $\prec$ if and only if $U$ has the form: $U(V)=f(Y)+g(Y)h(X).$ 

Note that each of the functions $f,g,h$ has a smaller scope than our original $U$ , and hence this representation requires (in general) fewer parameters. From this basic theorem, we can obtain two conclusions. 

# Proposition 22.2 

very subset of variables $X\subset V$ is utility independent of its complement if and only if there exist $k$ functions $U_{i}(V_{i})$ and a constant c such that 

$$
U(V)=\prod_{i=1}^{k}U_{i}(V_{i})+c,
$$ 

or $k$ functions $U_{i}(V_{i})$ such that 

$$
U(V)=\sum_{i=1}^{k}U_{i}(V_{i}).
$$ 

utility decomposition In other words, when every subset is utility independent of its complement, the utility function decomposes either as a sum or as a product of subutility functions over individual variables. In this case, we need only elicit a linear number of parameters, exponentially fewer than in the general case. 

If we weaken our assumption, requiring only that each variable in isolation is utility indepen- dent of its complement, we obtain a much weaker result: 

# Proposition 22.3 

$V_{i}\in V$ , $V_{i}$ s utility independent of $V\mathrm{~-~}\{V_{i}\}$ , then there exist $k$ $U_{i}(V_{i})\ (\ddot{\imath}=1,.\.\.\,.\,k)$ ) such that U is a multilinear function (a sum of products) of the $U_{i}\,{\mathrm{\boldmath~\acute{s}~}}$ ’s. For example, if $V=\{V_{1},V_{2},V_{3}\}$ , then this theorem would imply only that $U(V_{1},V_{2},V_{3})$ can be written as 

$$
\begin{array}{r l}&{c_{1}U_{1}(V_{1})U_{2}(V_{2})U_{3}(V_{3})+c_{2}U_{1}(V_{1})U_{2}(V_{2})+c_{3}U_{1}(V_{1})U_{3}(V_{3})+c_{4}U_{2}(V_{2})U_{3}(V_{3})}\\ &{c_{5}U_{1}(V_{1})+c_{6}U_{2}(V_{2})+c_{7}U_{3}(V_{3}).}\end{array}
$$ 

In this case, the number of subutility functions is linear, but we must elicit (in the worst case) exponentially many coefcients. Note that, if the domains of the variables are large, this might still result in an overall savings in the number of parameters. 

# 22.4.2 Additive Independence Properties 

Utility independence is an elegant assumption, but the resulting decomposition of the utility function can be difcult to work with. The case of a purely additive or purely multiplicative decomposition is generally too limited, since it does not allow us to express preferences that relate to combinations of values for the variables. For example, a person might prefer to take a vacation at a beach destination, but only if the weather is good; such a preference does not easily decompose as a sum or a product of subutilities involving only individual variables. 

In this section, we explore progressively richer families of utility factorizations, where the utility is encoded as a sum of subutility functions: 

$$
U(V)=\sum_{i=1}^{k}U_{i}(Z_{i}).
$$ 

We also study how these decompositions correspond to a form of independence assumption about the utility function. 

# 22.4.2.1 Additive Independence 

additive independence 

# Deﬁnition 22.8 

In our ﬁrst decomposition, we restrict attention to decomposition as in equation (22.8), where $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ is a disjoint partition of $V$ . This decomposition is more restrictive than the one allowed by utility independence, since we allow a decomposition only as a sum, and not as a product. This decomposition turns out to be equivalent to a notion called additive independence , which has much closer ties to probabilistic independence. Roughly speaking, $X$ and $Y$ are additively independent if our preference function for lotteries over $V$ depends only on the marginals over $X$ and $Y$ . More generally, we deﬁne: 

Let $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ be a disjoint partition of $V$ . We say that $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ are additively independent $i n\prec i f,$ for any lotter s $\pi_{1},\pi_{2}$ that have the same marginals on all $Z_{i}$ , we have that $\pi_{1}$ and $\pi_{2}$ are indiferent under ≺ . 

Additive independence is strictly stronger than utility i ependence: For two subset $X\cup Y=$ $V$ that are additively independent, we have both that X is utility independent of Y $Y$ and $Y$ is utility independent of $X$ . It then follows, for example, that the preference ordering in example 22.8 does not have a corresponding additively independent utility function. Additive independence is equivalent to the decomposition of $U$ as a sum of subutilities over the $Z_{i}$ ’s: 

# Theorem 22.2 

Let $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ be a disjoint partition of $V$ , and let $\prec,U$ be a corresponding pa of a preference ordering and a utility function. Then $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ are additively independent in ≺ if and only if $U$ can itten as: $\begin{array}{r}{U(V)=\sum_{i=1}^{k}U_{i}(Z_{i})}\end{array}$ . 

Proof The “if” direction is straightforward. For the “only if” direction, consider ﬁrst the case where $X,Y$ is a disjoint partition of $V$ , and $X,Y$ e additively independent in $\prec.$ . Let $\mathbfit{\Delta}x,\mathbfit{\Delta}y$ be some arbitrary ﬁxed assignment to $X,Y$ . Let $x^{\prime},y^{\prime}$ be any other assignment to $X,Y$ . Let $\pi_{1}$ be the distribution that assigns probability 0 . 5 to each of $\mathbfit{\Delta}x,\mathbfit{\Delta}y$ and $x^{\prime},y^{\prime}$ , and $\pi_{2}$ be the distribution that assigns probability 0 . 5 to each of $x,y^{\prime}$ and $\pmb{x}^{\prime},\pmb{y}$ . These two distributions have the same marginals over $X$ and $Y$ . Therefore, by the assumption of additive independence, $\pi_{1}\sim\pi_{2}$ , so that 

$$
\begin{array}{r c l}{0.5U(x,y)+0.5U(x^{\prime},y^{\prime})}&{=}&{0.5U(x,y^{\prime})+0.5U(x^{\prime},y)}\\ {U(x^{\prime},y^{\prime})}&{=}&{U(x,y^{\prime})-U(x,y)+U(x^{\prime},y).}\end{array}
$$ 

Now, deﬁne $U_{1}(X)=U(X,y)$ and $U_{2}(Y)=U({\pmb x},{\pmb Y})-U({\pmb x},{\pmb y})$ . It follows directly from equation (22.9) that for any $x^{\prime},y^{\prime}$ , $U(x^{\prime},y^{\prime})\,=\,U_{1}(x^{\prime})+U_{2}(y^{\prime})$ , as desired. The case of a decomposition $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ follows by a simple induction on $k$ . 

# Example 22.9 

Consider a student who is deciding whether to take a difcult course. Taking the course will require a signiﬁcant time investment during the semester, so it has a cost. On the other hand, taking the course will result in a more impressive résumé, making the student more likely to get a good job with a high salary after she graduates. The student’s utility might depend on the two attributes $T$ (taking the course) and $J$ (the quality of the job obtained). The two attributes are plausibly additively independent, so that we can express the student’s utility as $U_{1}(T)\;+$ $U_{2}(J)$ . Note that this independence of the utility function is completely unrelated to any possible probabilistic (in)dependencies. For example, taking the class is deﬁnitely correlated probabilistic ally with the student’s job prospects, so $T$ and $J$ are dependent as probabilistic attributes but additively independent as utility attributes. 

In general, however, additive independence is a strong notion that rarely holds in practice. 

# Example 22.10 

# Example 22.11 

Consider a student planning his course load for the next semester. His utility might depend on two attributes — how interesting the courses are ( I ), and how much time he has to devote to class work versus social activities $(T)$ . It is quite plausible that these two attributes are not utility independent, because the student might be more willing to spend signiﬁcant time on class work if the material is interesting. 

Consider the task of making travel reservations, and the two attributes H — the quality of one’s hotel — and W — the weather. Even these two seemingly unrelated attributes might not be additively independent, because the pleasantness of one’s hotel room is (perhaps) more important when one has to spend more time in it on account of bad weather. 

# 22.4.2.2 Conditional Additive Independence 

The preceding discussion provides a strong argument for extending additive independence to the case of nondisjoint subsets. For this extension, we turn to probability distributions for intuition: In a sense, additive independence is analogous to marginal independence. We therefore wish to construct a notion analogous to conditional independence: 

# Deﬁnition 22.9 CA-independence 

Let $X,Y,Z$ be a disjoint partition of $V$ . We say that $X$ and $Y$ are conditionally additively independent (CA-independent) given $Z$ in $\prec i f,$ for every assignment $_z$ to $Z$ , $X$ and $Y$ are additively independent in the conditional preference structure $\prec_{z}$ . 

The CA-independence condition is equivalent to an assumption that the utility decomposes with overlapping subsets: 

# Proposition 22.4 

Let $X,Y,Z$ be a disjoint partition $V$ , a let $\prec,U$ be a correspondin pai f a preference dering and a utility function. Then X and $Y$ are CA-independent given Z in ≺ if and only if $U$ can be written as: 

$$
U(X,Y,Z)=U_{1}(X,Z)+U_{2}(Y,Z).
$$ 

The proof is straightforward and is left as an exercise (exercise 22.2). 

# Example 22.12 

# 

Deﬁnition 22.10 CAI-map 

Deﬁnition 22.11 

utility factorization 

Consider again example 22.10, but now we add an attribute $F$ representing how much fun the student has in his free time (for example, does he have a lot of friends and hobbies that he enjoys?). Given an assignment to $T$ , which determines how much time the student has to devote to work versus social activities, it is quite reasonable to assume that $I$ and $F$ are additively independent. Thus, we can write $U(I,T,F)$ as $U_{1}(I,T)+U_{2}(T,F)$ . 

Based on this result, we can prove an important theorem that allows us to view a utility function in terms of a graphical model. Speciﬁcally, we associate a utility function with an undirected graph, like a Markov network. As in probabilistic graphical models, the separation properties in the graph encode the CA-independencies in the utility function. Conversely, the utility function decomposes additively along the maximal cliques in the network. Formally, we deﬁne the two types of relationships between a pair $(\prec,U)$ and an undirected graph: 

We say that $\mathcal{H}$ is an I-map for $\prec i f,$ r any isjoint partition $X,Y,Z$ of $V$ , $X$ and $Y$ are separated in H given Z , we have that X and $Y$ are CA-independent in ≺ given Z . 

We say that a utility function $U$ factorizes according to $\mathcal{H}$ if we can write $U$ as a sum 

$$
U(V)=\sum_{c=1}^{k}U_{c}(C_{c}),
$$ 

where $C_{1},\ldots,C_{k}$ are the maximal cliques in $\mathcal{H}$ . 

We can now show the same type of equivalence between these two deﬁnitions as we did for probability distributions. The ﬁrst theorem goes from factorization to independencies, showing that a factorization of the utility function according to a network $\mathcal{H}$ implies that it satisﬁes the independence properties implied by the network. It is analogous to theorem 3.2 for Bayesian networks and theorem 4.1 for Markov networks. 

# Theorem 22.3 

Let $(\prec,U)$ b a corre onding pair of a p ference function and a utility function. If $U$ factorizes according to H , then H is a CAI-map for ≺ . 

Proof The proof of this result follows immediately from proposition 22.4. Assume that $U$ toriz according to $\mathcal{H}$ , so that $\begin{array}{r}{U=\sum_{c}U_{c}(C_{c})}\end{array}$ . Any $C_{c}$ cannot involve variables from both $X$ and $Y$ . Thus, we can divide the cliques into two subsets: $\mathcal{C}_{1}$ , which involve only variables in $X,Z$ , and $\mathcal{C}_{2}$ $Y,Z$ . Letting $\begin{array}{r}{U_{i}=\sum_{c\in\mathcal{C}_{i}}U_{c}(C_{c})}\end{array}$ , for $i=1,2$ , ∈C we have that $U(V)=U_{1}(X,Z)+U_{2}(Y,Z)$ , precisely the condition in proposition 22.4. The desired CA-independence follows. 

The converse result asserts that any utility function that satisﬁes the CA-independence prop- erties associated with the network can be factorized over the network’s cliques. It is analogous to theorem 3.1 for Bayesian networks and theorem 4.2 for Markov networks. 

# Theorem 22.4 

Hammersley- Cliford theorem 

# Lemma 22.1 

Let $(\prec,U)$ be a cor ponding pair of a pref ence function and a utility function. If $\mathcal{H}$ is $^a$ CAI-map for $\prec,$ then U factorizes according to . 

Although it is possible to prove this result directly, it also follows from the analogous result for probability distributions (the Hammersley-Cliford theorem — theorem 4.2). The basic idea is to construct a probability distribution by exponentiating $U$ and then show that CA-independence properties for $U$ imply corresponding probabilistic conditional independence properties for $P$ : 

Let $U$ be a utility fu tion, d deﬁ $P(V)\propto\exp(U(V))$ . F a disjoint part on $X,Y,Z$ of V , we have that X and Y $Y$ are CA-independent given Z in U if and only if X and Y are conditionally independent given $Z$ in $P$ . 

The proof is left as an exercise (exercise 22.3). 

Based on this correspondence, many of the results and algorithms of chapter 4 now apply without change to utility functions. In particular, the proof of theorem 22.4 follows immediately (see exercise 22.4). 

minimal CAI-map 

perfect CAI-map 

# Theorem 22.5 

completeness 

As for probabilistic models, we can consider the task of constructing a graphical model that reﬂects the independencies that hold for a utility function. Speciﬁcally, we deﬁne $\mathcal{H}$ to be a minimal CAI-map if it is a CAI-map from which no further edges can be removed without rendering it not a CAI-map. Our goal is the construction of an undirected graph which is a minimal CAI-map for a utility function $U$ . We addressed exactly the same problem in the context of probability functions in section 4.3.3 and provided two algorithms. One was based on checking pairwise independencies of the form $(X\perp Y\mid{\mathcal{X}}-\{X,Y\})$ on checking local (Markov blanket) independencies of the form ( $(X\ \bot\ \mathcal{X}-\{X\}-U\ |\ U)$ ⊥X −{ } − | . Importantly, both of these types of independencies involve a disjoint and exhaustive partition of the set of variables into three subsets. Thus, we can apply these procedures without change using CA-independencies. 

Because of the equivalence of lemma 22.1, and because $P\propto\exp(U)$ is a positive distribution, all of the results in section 4.3.3 hold without change. In particular, we can show that either of the two procedures described produces the unique minimal CAI-map for $U$ . Indeed, we can ove an even stronger result: The unique minimal CAI- p $\mathcal{H}$ for $U$ is a perfect CAI- ap for $U$ , in the sense that any CA-independence that holds for U is implied by separation in H : 

that holds in the utility function from a graph. One might wonder why a similar result was so elu- sive for probability distributions. The reason is not that utility functions are better expressed as graphical models than are probability distributions. Rather, the language of CA-independencies is substantially more limited than that of conditional independencies in the probabilistic case: For probability distributions, we can evaluate any statement of the form $^{\alpha}X$ is independent of $Y$ given $Z$ ,” whereas for utility functions, the corresponding (CA-independence) statement is well deﬁned only when $X,Y,Z$ form a disjoint partition of $V$ . In other words, although any CA-independence statement that holds in the utility function can be read from the graph, the set of such statements is signiﬁcantly more restricted. In fact, a similar weak completeness statement can also be shown for probability distributions. 

# 22.4.2.3 Generalized Additive Independence 

Because of its limited expressivity, the notion of conditional additive independence allows us only to make fairly coarse assertions regarding independence — independence of two subsets of variables given all of the rest. As a consequence, the associated factorization is also quite coarse. In particular, we can only use CA-independencies to derive a factorization of the utility function over the maximal cliques in the Markov network. As was the case in probabilistic models (see section 4.4.1.1), this type of factorization can obscure the ﬁner-grained structure in the function, and its parameter iz ation may be exponentially larger. 

In this section, we present the most general additive decomposition: the decomposition of equation (22.8), but with arbitrarily overlapping subsets. This type of decomposition is the utility analogue to a Gibbs distribution (deﬁnition 4.3), with the factors here combining additively rather than multiplicative ly. 

Once again, we can provide an independence-based formulation for this decomposition: 

Deﬁnition 22.12 GA-independence Let $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ be (not necessarily disjoint) subsets of $V$ . We say that $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ are generalized additively indep ent (GA-independent) $i n\;\prec\;i f,$ for any lotteri $\pi_{1},\pi_{2}$ that have the same marginals on all $Z_{i}$ , we have that $\pi_{1}$ and $\pi_{2}$ are indiferent under ≺ . 

This deﬁnition is identical to that of additive independence (deﬁnition 22.8), with the exception that the subsets $Z_{1},.\,.\,.\,,Z_{k}$ are not necessarily mutually exclusive nor exhaustive. Thus, this deﬁnition allows us to consider cases where our preferences between two distributions depend only on some arbitrary set of marginals. It is also not hard to show that GA-independence subsumes CA-independence (see exercise 22.6). 

Satisfyingly, a factorization theorem analogous to theorem 22.2 holds for GA-independence: 

Theorem 22.6 Let $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ be (not necessarily disjoint) subsets of $V$ , and let $\prec,U$ be a correspondi pair of a prefe nce ordering and a utility function. Then $\boldsymbol{Z}_{1},\ldots,\boldsymbol{Z}_{k}$ are GA-independent in ≺ if and only if U can be written as: 

$$
U(V)=\sum_{i=1}^{k}U_{i}(Z_{i}).
$$ 

Thus, the set of possible factorizations associated with GA-independence strictly subsumes the set of factorizations associated with CA-independence. For example, using GA-independence, we can obtain a factorization $U(X,Y,Z)=U_{1}(X,Y)+U_{2}(Y,Z)+U_{3}(X,Z)$ . The Markov network associated with this factorization is a full clique over $X,Y,Z$ , and therefore no CA- independencies hold for this utility function. Overall, GA-independence provides a rich and natural language for encoding complex utility functions (see, for example, box 22.A). 

prenatal diagnosis Box 22.A — Case Study: Prenatal Diagnosis. An important problem involving utilities arises in the domain of prenatal diagnosis , where the goal is to detect chromosomal abnormalities present in a fetus in the early stages of the pregnancy. There are several tests available to diagnose these diseases. These tests have diferent rates of false negatives and false positives, costs, and health risks. The task is to decide which tests to conduct on a particular patient. This task is quite difcult. The patient’s risk for having a child with a serious disease depends on the mother’s age, child’s sex and race, and the family history. Some tests are not very accurate; others carry a signiﬁcant risk of inducing miscarriages. Both a miscarriage (spontaneous abortion or SAB) and an elective termination of the pregnancy (induced abortion or IAB) can afect the woman’s chances of conceiving again. 

Box 23.A describes a decision-making system called PANDA (Norman et al. 1998) for assisting the parents in deciding on a course of action for prenatal testing. The PANDA system requires that we have a utility model for the diferent outcomes that can arise as part of this process. Note that, unlike for probabilistic models, we cannot simply construct a single utility model that applies to all patients. Diferent patients will typically have very diferent preferences regarding these outcomes, and certainly regarding lotteries over them. Interestingly, the standard protocol

 (and the one followed by many health insurance companies), which recommends prenatal diagnosis

 (under normal circumstances) only for women over the age of thirty-ﬁve, was selected so that the risk (probability) of miscarriage is equal to that of having a Down syndrome baby. Thus, this recommendation essentially assumes not only that all women have the same utility function, but also that they have equal utility for these two events. 

The outcomes in this domain have many attributes, such as the inconvenience and expense of fairly invasive testing, the disease status of the fetus, the possibility of test-induced miscarriage, knowledge of the status of the fetus, and future successful pregnancy. Speciﬁcally, the utility could be viewed as a function of ﬁve attributes: pregnancy loss $L$ , with domain {no loss, miscarriage, elective termination}; Down status $D$ of the fetus, with domain: {normal, Down}; mother’s knowledge $K$ , with domain {none, accurate, inaccurate}; future pregnancy $F$ , with domain {yes, no}; type of test $T$ with domain {none, CVS, amnio}. An outcome is an assignment of values to all the attributes. For example, ⟨ no loss, normal, none, yes, none ⟩ is one possible outcomes. It represents the situation in which the fetus is not afected by Down syndrome, the patient decides not to take any tests (as a consequence, she is unaware of the Down status of the fetus until the end of the pregnancy), the pregnancy results in normal birth, and there is a future pregnancy. Another outcome, ⟨ miscarriage, normal, accurate, no, CVS ⟩ represents a situation where the patient decides to undergo the CVS test. The test result correctly asserts that the fetus is not afected by Down syndrome. However, a miscarriage occurs as a side efect of the procedure, and there is no future pregnancy. Our decision-making situation involves comparing lotteries involving complex (and emotionally difcult) outcomes such as these. 

In this domain, we have three ternary attributes and two binary ones, so the total number of outcomes is 108 . Even if we remove outcomes that have probability zero (or are very unlikely), a 

![](images/ee37d0665785ee665f59a9ab9a787b26c1bee215a59fc2eeae8ef464c34b9934.jpg) 
Figure 22.A.1 — Typical utility function decomposition for prenatal diagnosis 

large number of outcomes remain. In order to perform utility elicitation, we must assign a numerical utility to each one. A standard utility elicitation process such as standard gamble involves a fairly large number of comparisons for each outcome. Such a process is clearly infeasible in this case. 

However, in this domain, many of the utility functions elicited from patients decompose additively in natural ways. For example, as shown by Chajewska and Koller (2000), many patients have utility functions where invasive testing $(T)$ and knowledge $(K)$ are additively independent of other attributes; pregnancy loss $(L)$ is correlated both with Down syndrome $(D)$ and with future pregnancy $(F),$ but there is no direct interaction between Down syndrome and future pregnancy. Thus, for example, one common decomposition is $U_{1}(T)+U_{2}(K)+U_{3}(D,L)+U_{4}(L,F)$ , as encoded in the Markov network of ﬁgure 22.A.1. 

Box 22.B — Case Study: Utility Elicitation in Medical Diagnosis. In box 3.D we described the Pathﬁnder system, designed to assist a pathologist in diagnosing lymph-node diseases. The Pathﬁnder system was purely probabilistic — it produced only a probability distribution over possible diag- noses. However, the performance evaluation of the Pathﬁnder system accounted for the implications of a correct or incorrect diagnosis on the patient’s utility. For example, if the patient has a viral infection and is diagnosed as having a bacterial infection, the consequences are not so severe: the patient may take antibiotics unnecessarily for a few days or weeks. On the other hand, if the patient has Hodgkin’s disease and is incorrectly diagnosed as having a viral infection, the consequences — such as delaying chemotherapy — may be lethal. Thus, to evaluate more accurately the implications of Pathﬁnder’s performance, a utility model was constructed that assigned, for every pair of diseases $d,d^{\prime}$ , a utility value $u_{d,d^{\prime}}$ , which denotes the patient’s utility for having the disease d and being diagnosed with disease $d^{\prime}$ . 

We might be tempted to evaluate the system’s performance on a particular case by computing $u_{d^{*},d}-u_{d^{*},d^{*}}$ , where $d^{*}$ is the true diagnosis, and $d$ is the most likely diagnosis produced by the system. However, this metric ignores the important distinction between the quality of the decision and the quality of the outcome : A bad decision is one that is not optimal relative to the agent’s state of knowledge, whereas a bad outcome can arise simply because the agent is unlucky. In this case, the set of observations may suggest (even to the most knowledgeable expert) that one disease is the most likely, even when another is actually the case. Thus, a better metric is the inferential loss — the diference in the expected utility between the gold standard distribution produced by an expert and the distribution produced by the system, given exactly the same set of observations. 

Estimating the utility values $u_{d,d^{\prime}}$ is a nontrivial task. One complication arises from the fact that this situation involves outcomes whose consequences are fairly mild, and others that involve a signiﬁcant risk of morbidity or mortality. Putting these on a single scale is quite challenging. The approach taken in Pathﬁnder is to convert all utilities to the micromort scale — a one-in-a- million chance of death. For severe outcomes (such as Hodgkins disease), one can ask the patient what probability of immediate, painless death he would be willing to accept in order to avoid both the disease d and the (possibly incorrect) diagnosis $d^{\prime}$ . For mild outcomes, where the micromort equivalent may be too low to evaluate reliably, utilities were elicited in terms of monetary equivalents — for example, how much the patient would be willing to pay to avoid taking antibiotics for two weeks. At this end of the spectrum, the “conversion” between micromorts and dollars is fairly linear (see section 22.3.2), and so the resulting dollar amounts can be converted into micromorts, putting these utilities on the same scale as that of severe outcomes. 

The number of distinct utilities that need to be elicited even in this simple setting is impractically large: with sixty diseases, the number of utilities is $60^{2}\,=\,3,600$ . Even aggregating diseases that have similar treatments and prognoses, the number of utilities is $36^{2}\,=\,1,296$ . However, utility independence can be used to decompose the outcomes into independent factors, such as the disutility of a disease d when correctly treated, the disutility of delaying the appropriate treatment, and the disutility of undergoing an unnecessary treatment. This decomposition reduced the number of assessments by 80 percent, allowing the entire process of utility assessment to be performed in approximately sixty hours. 

The Pathﬁnder IV system (based on a full Bayesian network) resulted in a mean inferential loss of 16 micromorts, as compared to 340 micromorts for the Pathﬁnder III system (based on a naive Bayes model). At a rate of \$20/micromort (the rate elicited in the 1980s), the improvement in the expected utility of Pathﬁnder IV over Pathﬁnder III is equivalent to around $\S6,O O O$ per case . 

# 22.5 Summary 

In this chapter, we discussed the use of probabilistic models within the context of a decision task. The key new element one needs to introduce in this context is some representation of the preferences of the agent (the decision maker). Under certain assumptions about these preferences, one can show that the agent, whether implicitly or explicitly, must be following the principle of maximum expected utility, relative to some utility function. It is important to note that the assumptions required for this result are controversial, and that they do not necessarily hold for human decision makers. Indeed, there are many examples where human decision makers do not obey the principle of maximum expected utility. Much work has been devoted to developing other precepts of rational decision making that better match human decision making. Most of this study has taken place in ﬁelds such as economics, psychology, or philosophy. Much work remains to be done on evaluating the usefulness of these ideas in the context of automated decision-making systems and on developing computational methods that allow them to be used in complex scenarios such as the ones we discuss in this book. 

In general, an agent’s utility function can involve multiple attributes of the state of the world. In principle, the complexity of the utility function representation grows exponentially with the number of attributes on which the utility function depends. Several representations have been developed that assume some structure in the utility function and exploit it for reducing the number of parameters required to represent the utility function. 

Perhaps the biggest challenge in this setting is that of acquiring an agent’s utility functions. Unlike the case of probability distributions, where an expert can generally provide a single model that applies to an entire population, an agent’s utility function is often highly personal and even idiosyncratic. Moreover, the introspection required for a user to understand her preferences and quantify them is a time-consuming and even distressing process. Thus, there is signiﬁcant value in developing methods that speed up the process of utility elicitation. 

A key step in that direction is in learning better models of utility functions. Here, we can attempt to learn a model for the structure of the utility function (for example, its decomposition), or for the parameters that characterize it. We can also attempt to learn richer models that capture the dependence of the utility function on the user’s background and perhaps even its evolution over time. Another useful direction is to try to learn aspects of the agent’s utility function by observing previous decisions that he made. 

These models can be viewed as narrowing down the space of possibilities for the agent’s utility function, allowing it to be elicited using fewer questions. One can also try to develop algorithms that intelligently reduce the number of utility elicitation questions that one needs to ask in order to make good decisions. It may be hoped that a combination of these techniques will make utility-based decision making a usable component in our toolbox. 

# 22.6 Relevant Literature 

The principle of maximum expected utility dates back at least to the seventeenth century, where it played a role in the famous Pascal’s wager (Arnauld and Nicole 1662), a decision-theoretic analysis concerning the existence of God. Bernoulli (1738), analyzing the St. Petersburg paradox , made the distinction between monetary rewards and utilities. Bentham (1789) ﬁrst proposed the idea that all outcomes should be reduced to numerical utilities. 

Ramsey (1931) was the ﬁrst to provide a formal derivation of numerical utilities from prefer- ences. The axiomatic derivation described in this chapter is due to von Neumann and Morgen- stern (1944). A Bayesian approach to decision theory was developed by Ramsey (1931); de Finetti (1937); Good (1950); Savage (1954). Ramsey and Savage both deﬁned axioms that provide a simultaneous justiﬁcation for both probabilities and utilities, in contrast to the axioms of von Neumann and Morgenstern, that take probabilities as given. These axioms motivate the Bayesian approach to probabilities in a decision-theoretic setting. The book by Kreps (1988) provides a good review of the topic. 

The principle of maximum expected utility has also been the topic of signiﬁcant criticism, both on normative and descriptive grounds. For example, Kahneman and Tversky, in a long series of papers, demonstrate that human behavior is often inconsistent with the principles of rational decision making under uncertainty for any utility function (see, for example, Kahneman, Slovic, and Tversky 1982). Among other things, Tversky and Kahneman show that people commonly use heuristics in their probability assessments that simplify the problem but often lead to serious errors. Speciﬁcally, they often pay disproportionate attention to low-probability events and treat high-probability events as though they were less likely than they actually are. 

Motivated both by normative limitations in the MEU principle and by apparent inconsistencies minimax risk between the MEU principle and human behavior, several researchers have proposed alternative criteria for optimal decision making. For example, Savage (1951) proposed the minimax risk criterion, which asserts that we should associate with each outcome not only some utility value but also a regret value. This approach was later reﬁned by Loomes and Sugden (1982) and Bell (1982), who show how regret theory can be used to explain such apparently irrational behaviors as gambling on negative-expected-value lotteries or buying costly insurance. 

A discussion of utility functions exhibited by human decision makers can be found in von Win- terfeldt and Edwards (1986). Howard (1977) provides an extensive discussion of attitudes toward risk and deﬁnes the notions of risk-averse, risk-seeking, and risk-neutral behaviors. Howard (1989) proposes the notion of micromorts for eliciting utilities regarding human life. The Pathﬁnder system is one of the ﬁrst automated medical diagnosis systems to use a carefully constructed utility function; a description of the system, and of the process used to construct the utility function, can be found in Heckerman (1990) and Heckerman, Horvitz, and Nathwani (1992). 

The basic framework of multiattribute utility theory is presented in detail in the seminal book of Keeney and Raifa (1976). These ideas were introduced into the AI literature by Wellman (1985). The notion of generalized additive independence (GAI), under the name interdependent value additivity , was proposed by Fishburn (1967, 1970), who also provided the conditions under which a GAI model provides an accurate representation of a utility function. The idea of using a graphical model to represent utility decomposition properties was introduced by Wellman and Doyle (1992). The rigorous development was performed by Bacchus and Grove (1995), who also proposed the idea of GAI-networks. These ideas were subsequently extended in various ways by La Mura and Shoham (1999) and Boutilier, Bacchus, and Brafman (2001). 

Much work has been done on the problem of utility elicitation. The standard gamble method was ﬁrst proposed by von Neumann and Morgenstern (1947), based directly on their axioms for utility theory. Time trade-of was proposed by Torrance, Thomas, and Sackett (1972). The visual analog scale dates back at least to the 1970s (Patrick et al. 1973); see Drummond et al. (1997) for a detailed presentation. Chajewska (2002) reviews these diferent methods and some of the documented difculties with them. She also provides a fairly recent overview of diferent approaches to utility elicitation. 

There has been some work on eliciting the structure of decomposed utility functions from users (Keeney and Raifa 1976; Anderson 1974, 1976). However, most often a simple (for exam- ple, fully additive) structure is selected, and the parameters are estimated using least-squares regression from elicited utilities of full outcomes. Chajewska and Koller (2000) show how the problem of inferring the decomposition of a utility function can be viewed as a Bayesian model selection problem and solved using techniques along the lines of chapter 18. Their work is based on the idea of explicitly representing an explicit probabilistic model over utility parameters, as proposed by Jimison et al. (1992). The prenatal diagnosis example is taken from Chajewska and Koller (2000), based on data from Kuppermann et al. (1997). 

Several authors (for example, Heckerman and Jimison 1989; Poh and Horvitz 2003; Ha and Haddawy 1997, 1999; Chajewska et al. 2000; Boutilier 2002; Braziunas and Boutilier 2005) have proposed that model reﬁnement, including reﬁnement of utility assessments, should be viewed in terms of optimizing expected value of information (see section 23.7). In general, it can be shown that a full utility function need not be elicited to make optimal decisions, and that close-to-optimal decisions can be made after a small number of utility elicitation queries. 

# 22.7 Exercises 

Exercise 22.1 

Complete the proof of theorem 22.1. In particular, let $U(s):=p$ , as deﬁned in equation (22.7), be our utility assignment for outcomes. Show that, if we use the MEU principle for selecting between two lotteries, the resulting preference over lotteries is equivalent to $\prec$ . (Hint: Do not forget to address the case of compound lotteries.) 

Exercise 22.2 Prove proposition 22.4. 

# Exercise 22.3 

Prove lemma 22.1. 

Exercise 22.4 Complete the proof of theorem 22.4. 

Exercise $22.5\star$ Prove theorem 22.5. (Hint: Use exercise 4.11.) 

# Exercise $\mathbf{22.6}\star$ 

Prove the following result without using the factorization properties of $U$ . Let $X,Y,Z$ be a disjoint partition of $V$ . T n $X,Z$ and $Y,{\bar{Z}}$ are GA-independent in $\prec$ if and only if $_{X}$ and $\mathbf{Y}$ are CA- independent given Z in . 

This result shows that CA-independence and GA-independence are equivalent over the scope of independence assertions to which CA-independence applies (those involving disjoint partitions of $V$ ). 

# Exercise 22.7 

Consider the problem of computing the optimal action for an agent whose utility function we are uncertain about. In particular, assum t, rather than a known utility function over outcomes $\mathcal{O}$ a probability density function $P(U)$ , which assigns a density for each possible utility function $U:\mathcal{O}\mapsto I\!\!R$ . 

a. What is the expected utility for a given action $a$ , taking an expectation both over the outcomes of $\pi_{a}$ , and over the possible utility functions that the agent might have? b. Use your answer to provide an efcient computation of the optimal action for the agent. 

# Exercise ${\bf22.8\star}$ 

As we discussed, diferent people have diferent utility functions. Consider the problem of learning a probability distribution over the utility functions found in a population. Assume that we have a set of samples $\dot{U}[1],\dots,U[M]$ of users from a population, where for each user $m$ we have elicited a utility function $U[\dot{m}]:V a l(\mathbf{\dot{V}})\dot{\mapsto}I\!\!R$ . 

a. Assume that we want to model our utility function as in equation (22.10). We want to use the same factorization for all users, but where diferent users have diferent subutility functions; that is, $U_{i}[m]$ and $U_{i}[m^{\prime}]$ are not the same. Moreover, the elicited values $U(\pmb{v})[m]$ are noisy, so that they may not decompose exactly as in equation (22.10). We can model the actual elicited values for a given user as the sum in equation (22.10) plus Gaussian noise. Formulate the distribution over the utility functions in the population as a linear Gaussian graphical model. Using the techniques we learned earlier in this book, provide a learning algorithm for the parameters in this model. b. Now, assume that we allow diferent users in the population to have one of several diferent factoriza- tions of their utility functions. Show how you can extend your graphical model and learning algorithm accordingly. Show how your model allows you to infer which factorization a user is likely to have. 

# 23 Structured Decision Problems 

In the previous chapter, we described the basic principle of decision making under uncertainty — maximizing expected utility. However, our deﬁnition for a decision-making problem was completely abstract; it deﬁned a decision problem in terms of a set of abstract states and a set of abstract actions. Yet, our overarching theme in this book has been the observation that the world is structured, and that we can obtain both representational and computational efciency by exploiting this structure. In this chapter, we discuss structured representations for decision- making problems and algorithms that exploit this structure when addressing the computational task of ﬁnding the decision that maximizes the expected utility. 

We begin by describing decision trees — a simple yet intuitive representation that describes a decision-making situation in terms of the scenarios that the decision maker might encounter. This representation, unfortunately, scales up only to fairly small decision tasks; still, it provides a useful basis for much of the later development. We describe inﬂuence diagrams , which extend Bayesian networks by introducing decisions and utilities. We then discuss algorithmic techniques for solving and simplifying inﬂuence diagrams. Finally, we discuss the concept of value of information , which is very naturally encoded within the inﬂuence diagram framework. 

# 23.1 Decision Trees 

# 23.1.1 Representation 

A decision tree is a representation of the diferent scenarios that might be encountered by the decision maker in the context of a particular decision problem. A decision tree has two types of internal nodes (denoted t-nodes to distinguish them from nodes in a graphical model) — one set encoding decision points of the agent, and the other set encoding decisions of nature. The outgoing edges at an agent’s t-node correspond to diferent decisions that the agent might make. The outgoing edges at one of nature’s t-nodes correspond to random choices that are made by nature. The leaves of the tree are associated with outcomes, and they are annotated with the agent’s utility for that outcome. 

A decision tree $\mathcal{T}$ is a rooted tree with a et of internal t-nodes $\nu$ $\mathcal{V}_{L}$ . The set $\nu$ is partitioned into two disjoint sets — agent t-nodes $\mathcal{V}_{A}$ and nature t-nodes $\mathcal{V}_{N}$ . Each t-node has ome set of choices $\mathcal{C}[v]$ , associated with its outgoing edges. We let $s u c c(v,c)$ denote the child of $v$ reached via the edge labeled with c . Each of nature’s $t$ -nodes v is associated with a probability 

![](images/427ee9ea5672f239ac6de5579ab054bc50b7eb83b5c6d07b75b5cf9b764e40d9.jpg) 
Figure 23.1 Decision trees for the Entrepreneur example. (a) one-stage scenario; (b) two-stage scenario, with the solution (optimal strategy) denoted using thicker edges. 

distribution $P_{v}$ over $\mathcal{C}[v]$ . Each leaf $v\in\mathcal{V}_{L}$ in the tree is annotated with a numerical value $U(v)$ corresponding to the agent’s utility for reaching that leaf. 

Most simply, in our basic decision-making scenario of deﬁnition 22.2, a lottery $\ell$ induces a two-layer tree. The root is an agent $\dagger$ -node $v$ , and it has an outgoing edge for each possible action $a\in\mathcal A$ , leading to some child $s u c c(v,a)$ . Each node $s u c c(v,a)$ is a na $\mathrm{t}$ e; its children are leaves in the tree, with one leaf for outcome in O for which $\ell_{a}(o)>0$ ; the corresponding edge is labeled with the probability ${\ell}_{a}(o)$ . The leaf associated with some outcome $O$ is annotated with $U(o)$ . Most simply, in our basic Entrepreneur scenario of example 22.3, the corresponding decision tree would be as shown in ﬁgure 23.1a. Note that if the agent decides not to found the company, there is no dependence of the outcome on the market demand, and the agent simply gets a utility of 0 . 

The decision-tree representation allows us to encode decision scenarios in a way that reveals much more of their internal structure than the abstract setting of outcomes and utilities. In particular, it allows us to encode explicitly sequential decision settings, where the agent makes several decisions; it also allows us to encode information that is available to the agent at the time a decision must be made. 

tion about its outcome, which can be one of three values: a negative reaction, $s^{0}$ , indicating almost no hope of widget sales; a neutral reaction, $s^{1}$ , indicating some hope of sales; and an enthusiastic reaction, $s^{2}$ , indicating a lot of potential demand. The probability distribution over the market demand is diferent for the diferent outcomes of the survey. If the agent conducts the survey, his decision on whether to found the company can depend on the outcome. 

The decision tree is shown in ﬁgure 23.1b. At the root, the agent decides whether to conduct the survey $(c^{1})$ ) or not $(c^{0}$ ). If he does not conduct the survey, the next t-node is another decision by the agent, where he decides whether to found the company $(f^{1})$ or not $(f^{0})$ . If the agent decides to found the company, nature decides on the market demand for widgets, which determines the ﬁnal outcome. The situation if the agent decides to conduct the survey is more complex. Nature then probabilistic ally chooses the value of the survey. For each choice, the agent has a t-node where he gets to decide whether he founds the company or not. If he does, nature gets to decide on the distribution of market demand for widgets, which is diferent for diferent outcomes of the survey. 

strategy 

Deﬁnition 23.2 decision-tree strategy 

We can encode the agent’s overall behavior in a decision problem encoded as a decision tree as a strategy . There are several possible deﬁnitions of a strategy. One that is simple and suitable for our purposes is a mapping from agent t-nodes to possible choices at that t-node. 

A decision-tree strategy $\sigma$ speciﬁes, for each $v\;\in\;\mathcal{V}_{A}$ , one of the choices labeling its outgoing edges. 

For example, in the decision tree of ﬁgure 23.1b, a strategy has to designate an action for the agent t-node, labeled $C$ , and the four agent t-nodes, labeled $F$ . One possible strategy is illustrated by the thick lines in the ﬁgures. 

Decision trees provide a structured representation for complex decision problems, potentially involving multiple decisions, taken in sequence, and interleaved with choices of nature. However, they are still instances of the abstract framework deﬁned in deﬁnition 22.2. Speciﬁcally, the outcomes are the leaves in the tree, each of which is annotated with a utility; the set of agent actions is the set of all strategies; and the probabilistic outcome model is the distribution over leaves induced by nature’s random choices given a strategy (action) for the agent. 

# 23.1.2 Backward Induction Algorithm 

backward induction Expectimax As in the abstract decision-making setting, our goal is to select the strategy that maximizes the agent’s expected utility. This computational task, for the decision-tree representation, can be solved using a straightforward tree-traversal algorithm. This approach is an instance of an approach called backward induction in the game-theoretic and economic literature, and the Expectimax algorithm in the artiﬁcial intelligence literature. 

The algorithm proceeds from the leaves of the tree upward, computing the maximum expected utility ${\mathrm{MEU}}_{v}$ achievable by the agent at each t-node $v$ in the tree — his expected utility if he plays the optimal strategy from that point on. At a leaf $v$ , ${\mathrm{MEU}}_{v}$ is simply the utility $U(v)$ associated with that leaf’s outcome. Now, consider an internal t-node $v$ for whose children we have already computed the MEU. If $v$ belongs to nature, the expected utility accruing to the agent if $v$ is reached is simply the weighted average of the expected utilities at each of v ’s children, where the weighted average is taken relative to the distribution deﬁned by nature over $v$ ’s children. If $v$ belongs to the agent, the agent has the ability to select the action at $v$ . 

![](images/a5104d49855e9a2fe585ede8534f0fe304be824b023877090237e354ff2752cd.jpg) 

The optimal action for the agent is the one leading to the child whose MEU is largest, and the MEU accruing to the agent is the MEU associated with that child. The algorithm is shown in algorithm 23.1. 

# 23.2 Inﬂuence Diagrams 

The decision-tree representation is a signiﬁcant improvement over representing the prob- lem as a set of abstract outcomes; however, much of the structure of the problem is still not made explicit. For example, in our simple Entrepreneur scenario, the agent’s utility if he founds the company depends only on the market demand $M$ , and not on the results of the survey $S$ . In the decision tree, however, the utility values appear in four separate subtrees: one for each value of the $S$ variable, and one for the subtree where the survey is not performed. An examination of the utility values shows that they are, indeed, identical, but this is not apparent from the structure of the tree. 

The tree also loses a subtler structure, which cannot be easily discerned by an examination of the parameters. The tree contains four nodes that encode a probability distribution over the values of the market demand $M$ . These four distributions are diferent. We can presume that neither the survey nor the agent’s decision has an efect on the market demand itself. The reason for the change in the distribution presumably arises from the efect of conditioning the distribution on diferent observations (or no observation) on the survey variable $S$ . In other words, these distributions represent $P(M\mid s^{0}),\,P(M\mid s^{1}),\,P(M\mid s^{2})$ , and $P(M)$ (in the branch where the survey was not performed). These interactions between these diferent parameters are obscured by the decision-tree representation. 

![](images/6a61ab808309d85d24a33799aab0a37be75e7fbeb091c1fb5c79f12ca62475b6.jpg) 
Figure 23.2 Inﬂuence diagram $\mathcal{T}_{F}$ for the basic Entrepreneur example 

# 23.2.1 Basic Representation 

inﬂuence diagram 

An alternative representation is the inﬂuence diagram (sometimes also called a decision network ), a natural extension of the Bayesian network framework. It encodes the decision scenario via a set of variables, each of which takes on values in some space. Some of the variables are random variables, as we have seen so far, and their values are selected by nature using some probabilistic model. Others are under the control of the agent, and their value reﬂects a choice made by him. Finally, we also have numerically valued variables encoding the agent’s utility. 

This type of model can be encoded graphically, using a directed acyclic graph containing three types of nodes — corresponding to chance variables , decision variables , and utility variables . These diferent node types are represented as ovals, rectangles, and diamonds, respectively. An inﬂuence diagram $\mathcal{T}$ is a directed acyclic graph over these nodes, such that the utility nodes have no children. 

Example 23.2 The inﬂuence diagram $\mathcal{T}_{F}$ for our entrepreneur example is shown in ﬁgure 23.2. The utility variable $V_{E}$ encodes the utility of the entrepreneur’s earnings, which are a deterministic function of the utility variable’s parents. This function speciﬁes the agent’s real-valued utility for each combination of the parent nodes; in this case, the utility is a function from $V a l(M)\times V a l(F)$ to $I\!\!R$ . We can represent this function as a table: 

where $f^{1}$ represents the decision to found the company and $f^{0}$ the decision not to do so. The CPD for the $M$ node is: 

$$
\frac{m^{0}\quad m^{1}\quad m^{2}}{0.5\quad0.3\quad0.2.}
$$ 

chance variable decision variable 

More formally, in an inﬂuence diagram, the world in which the agent acts is represented by the set $\mathcal{X}$ of chance variables , and by a set $\mathcal{D}$ of decision variables . Chance variables are those whose values are chosen by nature. The decision variables are variables whose values the agent gets to choose. Each variable $V\in\mathcal{X}\cup\mathcal{D}$ has a ﬁnite domain $V a l(V)$ of possible values. We can place this representation within the context of the abstract framework of deﬁnition 22.2: The possible actions $\mathcal{A}$ are all of the possible assignments $V a l(\mathcal{D})$ ; the possible outcomes are all of the joint assignments in $V a l(\mathcal{X}\cup\mathcal{D})$ . Thus, this framework provides a factored representation of both the action and the outcome space. 

utility variable 

outcome 

We can also decompose the agent’s utility function. A standard decomposition (see discussion in section 22.4) is as a linear sum of terms, each of which represents a certain component of the agent’s utility. More precisely, we have a set of utility variab s $\mathcal{U}$ , wh e on real numbers as val s. The agent’s ﬁnal utility is the sum of the value of $V$ for all $V\in{\mathcal{U}}$ . 

Let Z be the set of all variables in the network — chance, dec on, and utility variables. We expand the notion of outcome to en mpass a full assignment to , which we denote as $\zeta$ . 

The parents of a chance variable $X$ represent, as usual, the direct inﬂuences on the choice of $X$ ’s value. Note that the parents of $X$ can be both other chance variables as well as decision variables, but they cannot be utility variables, since we assumed that utility nodes have no children. Each chance node $X$ is ass iated with a CPD, which represents $P(X\mid\mathrm{Pa}_{X})$ . 

The parents of a utility variable V $V$ represent the set of variables on which the utility $V$ depends. The value of a utility variable $V$ is a deterministic function of the values of $\mathrm{Pa}_{V}$ ; we use $V(w)$ to denote the value that node $V$ takes when $\mathrm{Pa}_{V}\,=\,{\pmb w}$ . Note that, as for any deterministic function, we can also view $V$ as deﬁning a CPD, where for each parent assignment, some value gets probability 1 . When convenient, we will abuse notation and interpret a utility node as deﬁning a factor. 

Summarizing, we have the following deﬁnition: 

# Deﬁnition 23.3 

inﬂuence diagram 

An inﬂuence diagram $\mathcal{T}$ over $\overline{{\mathcal{Z}}}$ is a directed acyclic graph whose nodes correspond $\mathcal{Z}$ nd where nodes corresponding to utility variables have no ch Each chance variable $X\in{\mathcal{X}}$ ∈X is associated with a CPD $P(X\mid\mathrm{Pa}_{X})$ . Each utility variable $V\in{\mathcal{U}}$ ∈U is associated with a deterministic function $V(\mathrm{Pa}_{V})$ . 

# 23.2.2 Decision Rules 

information edge 

Example 23.3 

So far, we have not discussed the semantics of the decision node. For a decision variable $D\in{\mathcal{D}}$ , $\mathrm{Pa}_{D}$ is the set of variables whose values the agent knows when he chooses a value for $D$ . The edges incoming into a decision variable are often called information edges . 

Let us return to the setting of example 23.1. Here, we have the chance variable $M$ that represents the market demand, and the chance variable $S$ that represents the results of the survey. The variable $S$ has the values $s^{0}$ , $s^{1}$ , $s^{2}$ , and an additional value $s^{\perp}$ , denoting that the survey was not taken. This additional value is needed in this case, because we allow the agent’s decision to depend on the value of $S$ , and therefore we need to allow some value for this variable when the survey is not taken. The variab $S$ has two parents, $C$ and $M$ . W have that $P(s^{\perp}\mid c^{0},m)=1$ , for any value of m . In the case c $c^{1}$ , the probabilities over values of S are: 

![](images/34ed280969c7ee51f37794d7978fd3971995bfbf2ff46574f5bc3c40c1156cc6.jpg) 

The entrepreneur knows the result of the survey before making his decision whether to found the company. Thus, there is an edge between $S$ and his decision $F$ . We also assume that conducting 

![](images/fd686501c671cc796bf8526e555eabdbc9dba8e16e7ef550a083cfb20e6b054d.jpg) 
Figure 23.3 Inﬂuence diagram $\mathcal{T}_{F,C}$ for Entrepreneur example with market survey 

the survey has some cost, so that we have an additional utility node $V_{S}$ , with the parent $C$ ; $V_{S}$ takes on the value $-1$ if $C=c^{1}$ and 0 otherwise. The resulting inﬂuence diagram $\mathcal{T}_{F,C}$ is shown in ﬁgure 23.3. 

The inﬂuence diagram representation captures the causal structure of the problem and its parameter iz ation in a much more natural way than the decision tree. It is clear in the inﬂuence diagram that $S$ depends on $M$ , that $M$ is parameterized via a simple (unconditional) prior distribution, and so on. 

information state 

Example 23.4 

The choice that the agent makes for a decision variable $D$ can be contingent only on the values of its parents. More precisely, in any trajectory through the decision scenario, the agent will encounter $D$ in some particular information states , where each information state is an assignment of values to $\mathrm{Pa}_{D}$ . An agent’s strategy for $D$ must tell the agent how to act at $D$ , at each of these information states. 

In example 23.3, for instance, the agent’s strategy must tell him whether to found the company or not in each possible scenario he may encounter; the agent’s information state at this decision is deﬁned by the possible values of the decision variable $C$ and the survey variable $S$ . The agent must therefore decide whether to found the company in four diferent information states: if he chose not to conduct the survey, and in each of the three diferent possible outcomes of the survey. 

A decision rule tells the agent how to act in each possible information state. Thus, the agent is choosing a local conditional model for the decision variable $D$ . In efect, the agent has the ability to choose a CPD for $D$ . 

# Deﬁnition 23.4 

decision rule deterministic decision rule complete strategy 

# Example 23.5 

A decision rule for $C$ is simply a distribution over its two values. A decision rule for $F$ must deﬁne, for every value of $C$ , and for every value $s^{0},s^{1},s^{2},s^{\perp}$ of $S$ , a probability distribution over values of $F$ . Note, however, that there is a deterministic relationship between $C$ and $S$ , so that many of the combinations are inconsistent (for example, $c^{1}$ and $s^{\perp}$ , or $c^{0}$ and $s^{1}$ ). For example, in the case $c^{1},s^{1}$ , one possible decision rule for the agent is $f^{0}$ with probability 0 . 7 and $f^{1}$ with probability 0 . 3 . 

As we will see, in the case of single-agent decision making, one can always choose an optimal deterministic strategy for the agent. However, it is useful to view a strategy as an assignment of CPDs to the decision variables. Indeed, in this case, the parents of a decision node have the same semantics as the parents of a chance node: the agent’s strategy can depend only on the values of the parent variables. Moreover, randomized decision rules will turn out to be a useful concept in some of our constructions that follow. In the common case of deterministic decision rules, which pick a single action $\textit{d}\in\mathit{V a l}(D)$ for each assignment $\pmb{w}\,\in\,V a l(\mathrm{Pa}_{D})$ , we sometimes abuse notation and use $\delta_{D}$ to refer to the decision-rule function, in which case $\delta_{D}({\boldsymbol{w}})$ denotes the single action $d$ that has probability 1 given the parent assignment $\mathbfit{w}$ . 

# 23.2.3 Time and Recall 

intervention Unlike a Bayesian network, an inﬂuence diagram has an implicit causal semantics. One assumes that the agent can intervene at a decision variable $D$ by selecting its value. This intervention will afect the values of variables downstream from $D$ . By choosing a decision rule, the agent determines how he will intervene in the system in diferent situations. 

The acyclicity assumption for inﬂuence diagrams, combined with the use of information edges, ensures that an agent cannot observe a variable that his action afects. Thus, acyclicity implies that the network respects some basic causal constraints. In the case of multiple decisions, we often want to impose additional constraints on the network structure. In many cases, one assumes that the decisions in the network are all made by a single agent in some sequence over time; in this case, we have a total ordering $\prec$ on $\mathcal{D}$ . An additional assumption that is often made in this case is that the agent does not forget his previous decisions or information it once had. This assumption is typically called the perfect recall assumption (or sometime the no forgetting assumption), formally deﬁned as follows: 

Deﬁnition 23.5 temporal ordering perfect recall recall edge 

n inﬂuence diagram $\mathcal{T}$ is said to have $^a$ temporal ordering if there i some total ordering $\prec$ ov D , which is consistent with partial ordering im d by the edges in I . The inﬂuence diagram I satisﬁes the per mp n relative to ≺ $\prec i f,$ if, whenever $D_{i}\prec D_{j}$ , $\mathrm{Pa}_{D_{j}}\supset(\mathrm{Pa}_{D_{i}}\cup\{D_{i}\})$ . The edges from $\mathrm{Pa}_{D_{i}}\cup\{D_{i}\}$ ∪{ } to $D_{j}$ are called recall edges . 

Intuitively, a recall edge is an edge from a variable $X$ (chance or decision) to a decision variable $D$ whose presence is implied by the perfect recall assumption. In particular, if $D^{\prime}$ is n th $D$ in the temporal ordering, then we have recall edges $D^{\prime}\rightarrow D$ and $X\,\rightarrow\,D$ → for $X\,\in\,\mathrm{Pa}_{D^{\prime}}$ ∈ . To reduce visual clutter, we often omit recall edges in an inﬂuence diagram when the temporal ordering is known. For example, in ﬁgure 23.3, we omitted the edge from $C$ to $F$ . 

Although the perfect recall assumption appears quite plausible at ﬁrst glance, there are several arguments against it. First, it is not a suitable model for situations where the “agent” is actually a compound entity, with individual decisions made by diferent “subagents.” For example, our agent might be a large organization, with diferent members responsible for various decisions. It is also not suitable for cases where an agent might not have the resources (or the desire) to remember an entire history of all previous actions and observations. 

limited memory inﬂuence diagram 

The perfect recall assumption also has signiﬁcant representational and computational ramiﬁcations. The size of the decision rule at a decision node is, in general, exponential in the number of parents of the decision node. In the case of perfect recall, the number of parents grows with every decision, resulting in a very high-dimensional space of possible decision rules for decision variables later in the temporal ordering. This blowup makes computations involving large inﬂuence diagrams with perfect recall intractable in many cases. The computational burden of perfect recall leads us to consider also inﬂuence diagrams in which the perfect recall assumption does not hold, also known as limited memory inﬂuence diagrams (or LIMIDs). In these networks, all information edges must be represented explicitly, since perfect recall is no longer universally true. We return to this topic in section 23.6. 

# 23.2.4 Semantics and Optimality Criterion 

A choice of a decision rule $\delta_{D}$ efectively turns $D$ from a decision variable into a chance variable. Let $\sigma_{D}$ be any partial strategy that speciﬁes a decision rule for the decision variables $D\,\in\,D$ . We can replace each decision variable in $_D$ with the CPD deﬁn its decision rule in $\sigma$ , resulting in a nce diagram $\mathcal{Z}[\sigma]$ who chance variables are X ∪ $\mathcal X\cup D$ and whose decision variables are D − $\mathcal{D}-\mathcal{D}$ . In particular, when σ is a complete strategy, $\mathcal{Z}[\sigma]$ is simply a Bayesian network, which we denote by $\mathcal{B}_{\mathcal{Z}[\sigma]}$ . This Bayesian network deﬁnes a probability distribution over possible outcomes $\zeta$ . 

expected utility The agent’s expected utility in this setting is simply: 

$$
\operatorname{EU}[\mathcal{Z}[\sigma]]=\sum_{\zeta}P_{\mathcal{B}_{\mathcal{Z}[\sigma]}}(\zeta)U(\zeta)
$$ 

where the utility of an outcome is the sum of the individual utility variables in that outcome: 

$$
U(\zeta)=\sum_{V\in{\mathcal{U}}}\zeta\langle V\rangle.
$$ 

The linearity of expectation allows us to simplify equation (23.1) by considering each utility variable separately, to obtain: 

$$
\begin{array}{r c l}{\mathrm{EU}[\mathcal{Z}[\sigma]]}&{=}&{\displaystyle\sum_{V\in\mathcal{U}}\pmb{E}_{\mathcal{B}_{\mathcal{Z}[\sigma]}}[V]}\\ &{=}&{\displaystyle\sum_{V\in\mathcal{U}}\sum_{v\in V a l(V)}P_{\mathcal{B}_{\mathcal{Z}[\sigma]}}(V=v)v.}\end{array}
$$ 

We often drop the subscript $\mathcal{B}_{\mathcal{Z}[\sigma]}$ where it is clear from context. 

An alternative useful formulation for this expected utility makes explicit the dependence on the factors parameterizing the network: 

$$
\operatorname{EU}[\mathcal{Z}[\sigma]]=\sum_{\mathcal{X}\cup\mathcal{D}}\left[\left(\prod_{X\in\mathcal{X}}P(X\mid\operatorname{Pa}_{X})\right)\left(\prod_{D\in\mathcal{D}}\delta_{D}\right)\left(\sum_{i\ :\ V_{i}\in\mathcal{U}}V_{i}\right)\right].
$$ 

The expression inside the summation is constructed as a product of three components. The ﬁrst is a product of all of the CPD factors in the network; the second is a product of all of the factors corresponding to the decision rules (also viewed as CPDs); and the third is a factor that captures the agent’s utility function as a sum of the subutility functions $V_{i}$ . As a whole, the expression inside the summation is a single factor who e is $\mathcal X\cup\mathcal D$ . The value of the entry in the factor corresponding to an assignment $O$ t X ∪D is a product of the probability of this outcome (using the decision rules speciﬁed by σ ) and the utility of this outcome. The summation over this factor is simply the overall expected utility $\operatorname{EU}[{\mathcal{Z}}[\sigma]]$ . 

Returning to example 23.3, our outcomes are complete assignments $m,c,s,f,\,u_{s},\,u_{f}$ . The agent’s utility in such an outcome is $\boldsymbol{u}_{s}+\boldsymbol{u}_{f}$ . The agent’s expected utility given a strategy $\sigma$ is 

$$
\begin{array}{r}{P_{\mathcal{B}}(V_{S}=-1)\cdot-1+P_{\mathcal{B}}(V_{S}=0)\cdot0+}\\ {P_{\mathcal{B}}(V_{E}=-7)\cdot-7+P_{\mathcal{B}}(V_{E}=5)\cdot5+P_{\mathcal{B}}(V_{E}=20)\cdot20+P_{\mathcal{B}}(V_{E}=5)\cdot5+P_{\mathcal{B}}(V_{E}=40)\cdot20+P_{\mathcal{B}}(V_{E}=1)}\end{array}
$$ 

where $\mathcal{B}=\mathcal{B}_{\mathcal{Z}_{F,C}[\sigma]}$ . a th timizes the expected utility is: $\delta_{C}\,=\,c^{\mathrm{\footnotesize{1}}}$ ; δ $\delta_{F}(c^{1},s^{0})\,=\,f^{0}$ , δ $\delta_{F}(c^{1},s^{1})\,=\,f^{1}$ , δ $\delta_{F}(c^{1},s^{2})\,=\,f^{1}$ . Because the event $C=c^{0}$ has probability $O$ in this strategy, any choice of probability distributions for $\delta_{F}(c^{0},S)$ is optimal. By following the deﬁnition, we can compute the overall expected utility for this strategy, which is 3 . 22 , so that MEU $[\mathcal{T}_{F,C}]=3.22$ . 

According to the basic postulate of statistical decision theory, the agent’s goal is to maximize his expected utility for a given decision setting. Thus, he should choose the strategy $\sigma$ that maximizes $\operatorname{EU}[{\mathcal{Z}}[\sigma]]$ . 

Deﬁnition 23.6 MEU strategy MEU value 

An MEU strategy $\sigma^{*}$ for an inﬂuence diagram $\mathcal{T}$ is one that maximizes $\operatorname{EU}[{\mathcal{Z}}[\sigma]]$ . The MEU value $\mathrm{MSU}[\mathcal{Z}]$ is $\operatorname{EU}[{\mathcal{Z}}[\sigma^{*}]]$ . 

In general, there may be more than one MEU strategy for a given inﬂuence diagram, but they all have the same expected utility. 

This deﬁnition lays out the basic computational task associated with inﬂuence diagrams: Given an inﬂuence diagram $\mathcal{T}$ , our goal is to ﬁnd the MEU strategy $\mathrm{MSU}[\mathcal{Z}]$ . Recall that a strategy is an assignment of decision rules to all the decision variables in the network; thus, our goal is to ﬁnd: 

$$
\arg\operatorname*{max}_{\delta_{D_{1}},\ldots,\delta_{D_{k}}}\mathrm{EU}[\mathcal{Z}[\delta_{D_{1}},\ldots,\delta_{D_{k}}]].
$$ 

Each decision rule is itself a complex function, assigning an action (or even a distribution over actions) to each information state. This complex optimization task appears quite daunting at ﬁrst. Here we present two diferent ways of tackling it. 

prenatal diagnosis Box 23.A — Case Study: Decision Making for Prenatal Testing. As we discussed in box 22.A, prenatal diagnosis ofers a challenging domain for decision making. It incorporates a sequence of interrelated decisions, each of which has signiﬁcant efects on variables that determine the patient’s preferences. Norman et al. (1998) construct a system called PANDA (which roughly stands 

for “Prenatal Testing Decision Analysis”). PANDA uses an inﬂuence diagram to model the sequential decision process, the relevant random variables, and the patient’s utility. The inﬂuence diagram contains a sequence of six decisions: four types of diagnostic test (CVS, triple marker screening, ultrasound, and amniocentesis), as well as early and late termination of the pregnancy. The model focuses on ﬁve diseases that are serious, relatively common, diagnosable using prenatal testing, and not readily correctable: Down syndrome, neural-tube defects, cystic ﬁbrosis, sickle-cell anemia, and fragile X mental retardation. The probabilistic component of the network (43 variables) includes predisposing factors that afect the probability of these diseases, and it models the errors in the diagnostic ability of the tests (both false positive and false negative). Utilities were elicited for every patient and placed on a scale of 0 – 100 , where a utility of 100 corresponds to the outcome of a healthy baby with perfect knowledge throughout the course of the pregnancy, and a utility of 0 corresponds to the outcome of both maternal and fetal death. 

The strategy space in this model is very complex, since any decision (including a decision to take a test) can depend on the outcome of one or more earlier tests, As a consequence, there are about $1.62\times10^{272}$ diferent strategies, of which $3.85\times10^{38}$ are “reasonable” relative to a set of constraints. This enormous space of options highlights the importance of using automated methods to guide the decision process. 

The system can be applied to diferent patients who vary both on their predisposing factors and on their utilities. Both the predisposing factors and the utilities give rise to very diferent strategies. However, a more relevant question is the extent to which the diferent strategy choices make a diference to the patient’s ﬁnal utility. To provide a reasonable scale for answering this question, the algorithm was applied to select for each patient their best and worst strategy. As an example, for one such patient (a young woman with predisposing factors for sickle-cell anemia), the optimal strategy achieved an expected utility of 98 . 77 and the worst strategy an expected utility of 87 . 85 , for a diference of 10 . 92 utility points. Other strategies were then evaluated in terms of the percentage of these 10 . 92 points that they provided to the patient. For many patients, most of the reasonable strategies performed fairly well, achieving over 99 percent of the utility gap for that patient. However, for some patients, even reasonable strategies gave very poor results. For example, for the patient with sickle-cell anemia, strategies that were selected as optimal for other patients in the study provided her only 65–70 percent of the utility gap. Notably, the “recommended” strategy for women under the age of thirty-ﬁve, which is to perform no tests, performed even worse, achieving only 64.7 percent of the utility gap. 

Overall, this study demonstrates the importance of personalizing medical decision making to the information and the utility for individual patients. 

# 23.3 Backward Induction in Inﬂuence Diagrams 

We now turn to the problem of selecting the optimal strategy in an inﬂuence diagram. Our ﬁrst approach to addressing this problem is a fairly simple algorithm that mirrors the backward induction algorithm for decision trees described in section 23.1.2. As we will show, this algorithm can be implemented efectively using the techniques of variable elimination of chapter 9. This algorithm applies only to inﬂuence diagrams satisfying the perfect recall assumption, a restriction that has signiﬁcant computational ramiﬁcations. 

![](images/2955c2e8c9e03255a33125d77b056bd110be405bf6eb222c9d46e82e855be42e.jpg) 
Figure 23.4 Decision tree for the inﬂuence diagram $\mathcal{T}_{F,C}$ in the Entrepreneur example. For clarity, probability zero events are omitted, and edges are labeled only at representative nodes. 

# 23.3.1 Decision Trees for Inﬂuence Diagrams 

Our starting point for the backward induction algorithm is to view an inﬂuence diagram as deﬁning a set of possible trajectories, deﬁned from the perspective of the agent. A trajectory includes both the observations made by the agent and the decisions he makes. The set of possible trajectories can be organized into a decision tree, with a split for every chance variable and every decision variable. We note that this construction gives rise to an exponentially large decision tree. Importantly, we never have to construct this tree explicitly. As we will show, we can use this tree as a conceptual construct, which forms the basis for deﬁning a variable elimination algorithm. The VE algorithm works directly on the inﬂuence diagram, never constructing the exponentially large tree. 

We begin by illustrating the decision tree construction on a simple example. 

Consider the possible trajectories that might be encountered by our entrepreneur of example 23.3. Initially, he has to decide whether to conduct the survey or not $(\!C)$ . He then gets to observe the value of the survey $(S)$ . He then has to decide whether to found the company or not $(F)$ . The variable $M$ inﬂuences his utility, but he never observes it (at least not in a way that inﬂuences any of his decisions). Finally, the utility is selected based on the entire trajectory. We can organize this set of trajectories into a tree, where the ﬁrst split is on the agent’s decision $C$ , the second split (on every branch) is nature’s decision regarding the value of $S$ , the third split is on the agent’s decision $F$ , and the ﬁnal split is on $M$ . At each leaf, we place the utility value corresponding to the scenario. Thus, for example, the agent’s utility at the leaf of the trajectory $c^{1},s^{1},f^{1},m^{1}$ is $V_{S}(c^{1})+V_{E}(f^{1},m^{1})=\,-1+5$ . The decision tree for this example is the same one shown in ﬁgure 23.4. 

Note that the ordering of the nodes in the tree is deﬁned by the agent’s observations, not by the topological ordering of the underlying inﬂuence diagram. Thus, in this example, $S$ precedes $M$ , despite the fact that, viewed from the perspective of generative causal model, $M$ is “determined by nature” before $S$ . 

More generally, we assume (as stated earlier) that the inﬂuence diagram satisﬁes perfect recall relative to some temporal or ring $\prec$ on decisions. Wit ss of generality, assume that $D_{1}\prec.\,.\,.\prec D_{k}$ . We extend ≺ to a partial ordering over X ∪ $\mathcal X\cup\mathcal D$ which is consist t with the information edges in the inﬂuence diagrams; that is, whenever W is a parent of D for some $D\in{\mathcal{D}}$ and $W\in\mathcal{X}\cup\mathcal{D}$ , we have that $W\prec D$ . This ordering is guaranteed to extend th otal ordering $\prec$ over D postulated in deﬁnition 23.5, allowing us to abuse notation and use ≺ for both. 

This partial ordering constrains the orderings that we can use to deﬁne the decision tree. Let $X_{1}$ be the set of variables $X$ such th $X\prec D_{1}$ ; these variab are the ones that t agent observes for the ﬁrst time at decision $D_{1}$ 1 . More generally, let $X_{i}$ be those variables X such that $X\prec D_{i}$ but not $X\prec D_{i-1}$ . These variables are the ones that the agent observes for ﬁrst time at decision $D_{i}$ . With the perfect recall assumption, the agent’s decision rule at $D_{i}$ can depend on all of $X_{1}\cup\ldots\cup X_{i}\cup\{D_{1},\ldots,D_{i-1}\}$ Let $Y$ be the variables that e not observed prior to any decision. The sets $X_{1},.\.\ .\ ,X_{k},Y$ form a disjoint partition o $\mathcal{X}$ We can then deﬁne a tree where the ﬁrst split is on the set of possible assignments $_{x_{1}}$ to $X_{1}$ , the second is on possible decisions in $V a l(D_{1})$ , and so on, and where the ﬁnal split is on possible assignments $_{_y}$ to $Y$ . 

The choices at nature’s chance moves are associated with probabilities. These probabilities are not the same as the generative probabilities (as reﬂected in the CPDs in the inﬂuence diagrams), but reﬂect the agent’s subjective beliefs in nature’s choices given the evidence observed so far. 

Consider the decision tree of ﬁgure 23.4, and consider nature’s choice for the branch $S~=~s^{1}$ at the node corresponding to the trajectory $C\,=\,c^{1}$ . The probability that the survey returns $s^{1}$ rgina ility $\textstyle\sum_{M}P(M)\cdot P(s^{1}\mid M,c^{1})$ · | Continuing down the same branch, and assuming $F\,=\,f^{1}$ , the branching probability for $M=m^{1}$ is the conditional probability of $M=m^{1}$ given s $s^{1}$ (and the two decision variables, although these are irrelevant to this probability). 

In general, consider a branch down the tree associated with the choices $x_{1},d_{1},.\ .\ .\ ,x_{i-1},d_{i-1}$ At this vertex, we have a decision of nature, splitting on possible instantiations $\mathbf{\Phi}_{\mathbf{x}_{i}}$ to $X_{i}$ . We associate with this vertex a distribution $P(\pmb{x}_{i}\mid\pmb{x}_{1},d_{1},.\,.\,.\,,\pmb{x}_{i-1},d_{i-1})$ . 

As written, this probability expression is not well deﬁned, since we have not speciﬁed a distribution relative to which it is computed. Speciﬁcally, because we do not have a decision rule for the diferent decision variables in the inﬂuence diagram, we do not yet have a fully speciﬁed Bayesian network. We can ascribe semantics to this term using the following lemma: 

Lemma 23.1 Let $x_{1},.\,.\,,x_{i-1},d_{1},.\,.\,.\,,d_{i-1}$ be an assignment to $X_{1},.\.\.,X_{i-1},D_{1},.\.\.,D_{i-1}$ respectively. Let $\sigma_{1},\sigma_{2}$ be any two strategies in which $P_{\mathcal{B}_{\mathcal{Z}[\sigma_{i}]}}(\mathbf{x}_{1},.\,.\,.\,,\mathbf{x}_{i-1},d_{1},.\,.\,.\,,d_{i-1})\,\neq\,0$ $(i\,=\,1,2)$ . Then 

$$
P_{\mathcal{B}_{\mathcal{Z}[\sigma_{1}]}}(X_{i}\mid\mathbf{x}_{1},\ldots,\mathbf{x}_{i-1},d_{1},\ldots,d_{i-1})=P_{\mathcal{B}_{\mathcal{Z}[\sigma_{2}]}}(X_{i}\mid\mathbf{x}_{1},\ldots,\mathbf{x}_{i-1},d_{1},\ldots,d_{i-1}).
$$ 

The proof is left as an exercise (exercise 23.4). Thus, the probability of $X_{i}$ given $\mathbfit{x}_{1},\dots,\mathbfit{x}_{i-1}$ , $d_{1},.\ldots,d_{i-1}$ does not depend on the choice of strategy $\sigma$ , and so we can deﬁne a probability for this event without deﬁning a particular strategy $\sigma$ . We use $P(X_{i}\mid\mathbf{\sigma}x_{1},d_{1},.\,.\,.\,,\mathbf{\sigma}x_{i-1},d_{i-1})$ as shorthand for this uniquely deﬁned probability distribution. 

# 23.3.2 Sum-Max-Sum Rule 

Given an inﬂuence diagram $\mathcal{T}$ , we can construct the decision tree using the previous procedure and then simply run MEU-for-Decision-Trees (algorithm 23.1) over the resulting tree. The algo- rithm computes both the MEU value of the tree and the optimal strategy. We now show that this MEU value and strategy are also the optimal value and strategy for the inﬂuence diagram. 

Our ﬁrst key observation is that, in the decision tree we constructed, we can choose a diferent action at each t-node in the layer for a decision variable $D$ . In other words, the decision tree strategy allows us to take a diferent action at $D$ for each assignment to the decision and observation variables precedin $D$ in $\prec$ . The perfect-recall ssumption asserts that th variables are precisely the parents of D in the inﬂuence diagram I . Thus, a decision rule at D is precisely as expressive as the set of individual decisions at the t-nodes corresponding to $D$ , and the decision tree algorithm is simply selecting a set of decision rules for all of the decision variables in $\mathcal{T}$ — that is, a complete strategy for $\mathcal{T}$ . 

Example 23.9 In the $F$ layer (the third layer) of the decision tree in ﬁgure 23.4, we maximize over diferent possible values of the decision variable $F$ . Importantly, this layer is not selecting a single decision, but a (possibly) diferent action at each node in the layer. Each of these nodes corresponds to an information state — an assignment to $C$ and $S$ . Altogether, the set of decisions at this layer selects the entire decision rule $\delta_{F}$ . 

Note that the perfect recall assumption is critical here. The decision tree semantics (as we deﬁned it) makes the implicit assumption that we can make an independent decision at each t-node in the tree. Hence, if $D^{\prime}$ follows $D$ in the decision tree, then every variable on the path to $D$ t-nodes also appears on the path to $D^{\prime}$ t-nodes. Thus, the decision tree semantics can be consistent with the inﬂuence diagram semantics only when the inﬂuence diagram satisﬁes the perfect recall assumption. 

We now need to show that the strategy selected by this algorithm is the one that maximizes the expected utility for $\mathcal{T}$ . To do so, let us examine more closely the expression computed by MEU-for-Decision-Trees when applied to the decision tree constructed before. 

Example 23.10 In example 23.3, our computation for the value of the entire tree can be written using the following expression: 

$$
\operatorname*{max}_{C}\sum_{S}P(S\mid C)\operatorname*{max}_{F}\sum_{M}P(M\mid S,F,C)[V_{S}(C)+V_{E}(M,F)].
$$ 

Note that we can simplify some of the conditional probability terms in this expression using the conditional independence properties of the network (which are also invariant under any choice of ecision rules). For example, $M$ is independent of $F,C$ given $S$ , so that $P(M\mid S,F,C)=P(M\mid$ $S$ ) . 

sum-max-sum rule 

More generally, consider an inﬂuence diagram where, as before, the sequence of chance and decision variables is: $X_{1},D_{1},.\,.\,.\,,X_{k},D_{k},Y$ . We can write the value of the decision-making situation using the following expression, known as the sum-max-sum rule : 

$$
\begin{array}{r l}&{[\mathrm{EU}[\mathcal{Z}]=\sum_{\pmb{X}_{1}}P(\pmb{X}_{1})\operatorname*{max}_{D_{1}}\sum_{\pmb{X}_{2}}P(\pmb{X}_{2}\mid\pmb{X}_{1},D_{1})\operatorname*{max}_{D_{2}}.\,.\,.}\\ &{\quad\sum_{\pmb{X}_{k}}P(\pmb{X}_{k}\mid\pmb{X}_{1},\ldots,\pmb{X}_{k-1},D_{1},\ldots,D_{k-1})}\\ &{\quad\quad\operatorname*{max}_{D_{k}}\sum_{\pmb{Y}}P(\pmb{Y}\mid\pmb{X}_{1},\ldots,\pmb{X}_{k},D_{1},\ldots,D_{k})U(\pmb{Y},\pmb{X}_{1},\ldots,\pmb{X}_{k},D_{1},\ldots,D_{k})}\end{array}
$$ 

![](images/b22f35a6791696ab6ac95da1fec44856a889d6853046a22454cb47b5acb83557.jpg) 
Figure 23.5 Iterated optimization versus variable elimination. An inﬂuence diagram that allows an efcient solution using iterated optimization, but where variable elimination techniques are considerably less efcient. 

This expression is efectively performing the same type of backward induction that we used in decision trees. 

We can now push in the conditional probabilities into the summations or maximizations. This operation is the inverse to the one we have used so often earlier in the book, where we move probability factors out of a summation or maximization; the same equivalence is used to justify both. Once all the probabilities are pushed in, all of the conditional probability expressions cancel each other, so that we obtain simply: 

$$
\begin{array}{r l}&{\mathrm{MeV}[\mathcal{Z}]=\displaystyle\sum_{\boldsymbol{X}_{1}}\operatorname*{max}_{D_{1}}\sum_{\boldsymbol{X}_{2}}\operatorname*{max}_{D_{2}}\dots\cdot\sum_{\boldsymbol{X}_{k}}\operatorname*{max}_{D_{k}}}\\ &{\qquad\qquad\qquad\displaystyle\sum_{\boldsymbol{Y}}P(\boldsymbol{X}_{1},\dots,\boldsymbol{X}_{k},\boldsymbol{Y}\mid D_{1},\dots,D_{k})U(\boldsymbol{X}_{1},\dots,\boldsymbol{X}_{k},\boldsymbol{Y},D_{1},\dots,D_{k})}\end{array}
$$ 

If we view this expression in terms of factors (as in equation (23.2)), we can decompose the joint probability $P(X_{1},.\,.\,,X_{k},Y\mid D_{1},.\,.\,,D_{k})\,=\,P(\mathcal{X}\mid\mathcal{D})$ as the product of all of the factors corresponding to the CPDs of the variables X in the inﬂuence diagram. The joint utility $U(X_{1},\dots,X_{k},Y,D_{1},\dots,D_{k})\,=\,U({\mathcal{X}},{\mathcal{D}})$ is the sum of all of the utility variables in the network. 

Now, consider a strategy $\sigma$ — an assignment of actions to all of the agent t-nodes in the tree. Given a ﬁxed strategy, the maximizations become vacuous, and we are simply left with a set of summations over the diferent chance variables in the network. It follows directly from the deﬁnitions that the result of this summation is simply the expected utility of $\sigma$ in the inﬂuence diagram, as in equation (23.2). The fact that the sum-max-sum computation results in the MEU strategy now follows directly from the optimality of the strategy produced by the decision tree algorithm. 

The form of equation (23.4) suggests an alternative method for computing the MEU value and strategy, one that does not require that we explicitly form a decision tree. Rather, we can apply a variable elimination algorithm that directly computes the the sum-max-sum expression: We eliminate both the chance and decision variables, one at a time, using the $\sum$ or max operations, as appropriate. At ﬁrst glance, this approach appears straightforward, but the details are somewhat subtle. Unlike most of our applications of the variable elimination algorithm, which involve only two operations (either sum-product or max-product), this expression involves four — sum-marginalization, max-marginalization, factor product (for probabilities and utilities), and factor addition (for utilities). The interactions between these diferent operations require careful treatment, and the machinery required to handle them correctly has a signiﬁcant efect on the design and efciency of the variable elimination algorithm. The biggest complication arises from the fact that sum-marginalization and max-marginalization do not commute, and therefore elimination operations can be executed only in an order satisfying certain constraints; as we showed in section 13.2.3 and section 14.3.1, such constraints can cause inference even in simple networks to become intractable. The same issues arise here: 

Consider a setting where a student must take a series of exams in a course. The hardness $H_{i}$ of each exam $i$ is not known in advance, but one can assume that it depends on the hardness of the previous exam $H_{i-1}$ (if the class performs well on one exam, the next one tends to be harder, and if the class performs poorly, the next one is often easier). It also depends on the overall meanness of the instructor. The student needs to decide how much to study for each exam $(D_{i});$ studying more makes her more likely to succeed in the exam, but it also reduces her quality of life. At the time the student needs to decide on $D_{i}$ , she knows the difculty of the previous one and whether she studied for it, but she does not remember farther back than that. The meanness of the instructor is never observed. The inﬂuence diagram is shown in ﬁgure 23.5. 

If we apply a straightforward variable elimination algorithm based on equation (23.4), we would have to work from the inside out in an order that is consistent with the operations in the equation. Thus, we would ﬁrst have to eliminate $M$ , which is never observed. This step has the efect of creating a single factor over all of the $H_{i}$ variables, whose size is exponential in $k$ . 

Fortunately, as we discuss in section 23.5, there are better solution methods for inﬂuence diagrams, which are not based on variable elimination and hence avoid some of these difculties. 

# 23.4 Computing Expected Utilities 

In constructing a more efcient algorithm for ﬁnding the optimal decision in an inﬂuence diagram, we ﬁrst consider the special case of an inﬂuence diagram with no decision variables. This problem is of interest in its own right, since it allows us to evaluate the expected utility of a given strategy. More importantly, it is also a key subroutine in the algorithm for ﬁnding an optimal strategy. 

We begin our discussion with the even more restricted setting, where there is a single utility variable, and then discuss how it can be extended to the case of several utility variables. As we will see, although there are straightforward generalizations, an efcient implementation for this extension can involve some subtlety. 

# 23.4.1 Simple Variable Elimination 

Assume we have a single utility factor $U$ . In this case, the expected utility is simply a product of factors: the CPDs of the chance variables, the decision rules, and the utility function of the 

![](images/3d414315b9e2767547686bb9c9dcff25ea41b0a960317ae6fb0fecfd0892c341.jpg) 
Figure 23.6 An inﬂuence diagram with multiple utility variables 

# 

single utility factor $U$ , summed out over all of the variables in the network. Thus, in the setting of a single utility variable, we can apply our standard variable elimination algorithm in a straightforward way, to the set of factors deﬁning the expected utility. Because variable elimination is well deﬁned for any set of factors (whether derived from probabilities or not), there is no obstacle to applying it in this setting. 

Consider the inﬂuence diagram in ﬁgure 23.6. The inﬂuence diagram is drawn with two utility vari- ables, but (proceeding with our assumption of a single utility variable) we analyze the computation for each of them in isolation, assuming it is the only utility variable in the network. We begin with the utility variable $V_{1}$ , and use the elimination ordering $C,A,E,B,D$ . Note that $C$ is barren relative to $V_{1}$ (that is, it has no efect on the utility $V_{1}$ ) and can therefore be ignored. (Eliminating $C$ would simply produce the all 1 ’s factor.) Eliminating $A$ , we obtain 

$$
\mu_{1}^{1}(B,E)=\sum_{A}V_{1}(A,E)P(B\mid A)P(A).
$$ 

Eliminating $E$ , we obtain 

$$
\mu_{2}^{1}(B,D)=\sum_{E}P(E\mid D)\mu_{1}^{1}(B,E).
$$ 

We can now proceed to eliminate $D$ and $B$ , to compute the ﬁnal expected utility value. 

Now, consider the same variable elimination algorithm, with the same ordering, applied to the utility variable $V_{2}$ . In this case, $C$ is not barren, so we compute: 

$$
\mu_{1}^{2}(E)=\sum_{C}V_{2}(C,E)P(C).
$$ 

The variable $A$ does not appear in the scope of $\mu_{1}^{2}$ , and hence we do not use the utility factor in this step. Rather, we obtain a standard probability factor: 

$$
\phi_{1}^{2}(B)=\sum_{A}{\cal P}(A){\cal P}(B\mid A).
$$ 

Eliminating $E$ , we obtain: 

$$
\mu_{2}^{2}(D)=\sum_{E}P(E\mid D)\mu_{1}^{2}(E).
$$ 

To eliminate $B$ , we multiply $P(D\mid B)$ (which is a decision rule, and hence simply a CPD) with $\phi_{1}^{2}(B)$ , and then marginalize out B from the resulting probability factor. Finally, we multiply the result with $\mu_{2}^{2}(D)$ to obtain the expected utility of the inﬂuence diagram, given the decision rule for $D$ . 

# 23.4.2 Multiple Utility Variables: Simple Approaches 

An efcient extension to multiple utility variables is surprisingly subtle. One obvious solution is to collapse all of the utility factors into a single large factor $\begin{array}{r}{U=\sum_{V\in\mathcal{U}}V}\end{array}$ . We are now back ∈U to the same situation as above, and we can run the variable elimination algorithm unchanged. Unfortunately, this solution can lead to unnecessary computational costs: 

# Example 23.13 

Let us return to the inﬂuence diagram of example 23.12, but where we now assume that we have both $V_{1}$ and $V_{2}$ . In this simple solution, we add both together to obtain $U(A,E,C)$ . If we now run our variable elimination process (with the same ordering), it produces the following factors: $\begin{array}{r}{\mu_{1}^{U}(A,E)=\sum_{C}P(C)U(A,C,E)}\end{array}$ ; $\begin{array}{r}{\mu_{2}^{U}(B,E)=\sum_{A}P(A)P(B\mid A)\mu_{1}^{U}(A,E)}\end{array}$ P ; and $\begin{array}{r}{\mu_{3}^{U}(B,D)\;=\;\sum_{E}P(E\;\mid\;D)\mu_{2}^{U}(B,E).}\end{array}$ P | . Thus, this process produces a factor over the scope $A,C,E,$ , which is not created by either of the two preceding subcomputations; if, for example, both $A$ and C have a large domain, this factor might result in high computational costs. 

Thus, this simple solution requires that we sum up the individual subutility functions to con- struct a single utility factor whose scope is the union of the scopes of the subutilities. As a consequence, this transformation loses the structure of the utility function and creates a factor that may be exponentially larger. In addition to the immediate costs of creating this larger factor, factors involving more variables can also greatly increase the cost of the variable elimination algorithm by forcing us to multiply in more factors as variables are eliminated. A second simple solution is based on the linearity of expectations: 

$$
\sum_{\substack{\chi-\mathrm{Pa}D\;X\in\mathcal{X}}}\prod_{X\in\mathcal{X}}P(X\mid\mathrm{Pa}_{X})(\sum_{i\ :\ V_{i}\in\mathcal{U}}V_{i})=\sum_{i\ :\ V_{i}\in\mathcal{U}}\left(\sum_{\mathcal{X}-\mathrm{Pa}D\;X\in\mathcal{X}}P(X\mid\mathrm{Pa}_{X})V_{i}\right).
$$ 

Thus, we can run multiple separate executions of variable elimination, one for each utility factor $V_{i}$ , computing for each of them an expected utility factor $\mu_{-D}^{i}$ ; we then sum up these expected − utility factors and optimize the decision rule relative to the resulting aggregated utility factor. The limitation of this solution is that, in some cases, it forces us to replicate work that arises for multiple utility factors. 

# Example 23.14 

Returning to example 23.12, assume that we replace the single variable $E$ between $D$ and the two utility variables with a long chain $D\to E_{1}\to\dots\to E_{k}$ , whe $E_{k}$ is e parent of $V_{1}$ and $V_{2}$ . If we do a separate variable elimination computation for each of $V_{1}$ and $V_{2}$ , we would be executing twice the steps involved in eliminating $E_{1},\ldots,E_{k}$ , rather than reusing the computation for both utility variables. 

# 23.4.3 Generalized Variable Elimination $\star$ 

A solution that addresses both the limitations described before is to perform variable elimination with multiple utility factors simultaneously, but allow the algorithm to add utility factors to each other, as called for by the variable elimination algorithm. In other words, just as we multiply factors together when we eliminate a variable that they have in common, we would combine two utility factors together in the same situation. 

Let us return to example 23.12 using the same elimination ordering $C,A,E$ . The ﬁrst steps, of eliminating $C$ and $A$ , are exactly those we took in that example, as applied to each of the two utility factors separately. In other words, the elimination of $C$ does not involve $V_{1}$ , and hence produces precisely the same factor $\mu_{1}^{1}(B,E)$ as before; similarly, the elimination of $A$ does not involve $V_{2}$ , and produces $\mu_{1}^{2}(E),\phi_{1}^{2}(B)$ . However, when we now eliminate $E$ , we must somehow combine the two utility factors in which $E$ appears. At ﬁrst glance, it appears as if we can simply add these two factors together. However, a close examination reveals an important subtlety. The utility factor $\begin{array}{r}{\mu_{1}^{2}(E)=\sum_{\boldsymbol{C}}P(\boldsymbol{C})V_{2}(\boldsymbol{C},E)}\end{array}$ is a function that deﬁnes, for each assignment to $E$ , an expected utility given E . However, the entries in the other utility factor, 

$$
\begin{array}{l l l}{\mu_{1}^{1}(B,E)}&{=}&{\displaystyle\sum_{A}P(A)P(B\mid A)V_{1}(A,E)}\\ &{=}&{\displaystyle\sum_{A}P(B)P(A\mid B)V_{1}(A,E)=P(B)\sum_{A}P(A\mid B)V_{1}(A,E),}\end{array}
$$ 

do not represent an expected utility; rather, they are a product of an expected utility with the probability $P(B)$ . Thus, the two utility factors are on “diferent scales,” so to speak, and cannot simply be added together. To remedy this problem, we must convert both utility factors into the “utility scale” before adding them together. To do so, we must keep track of $P(B)$ as we do the elimination and divide $\mu_{1}^{1}(B,E)$ by $P(B)$ to rescale it appropriately, before adding it to $\mu_{1}^{2}$ . 

Thus, in order to perform the variable elimination computation correctly with multiple utility variables, we must keep track not only of utility factors, but also of the probability factors neces- sary to normalize them. This intuition suggests an algorithm where our basic data structures — our factors — are actually pairs of factors $\gamma=(\phi,\mu)$ , where $\phi$ is a probability factor, and $\mu$ is a utility factor. Intuitively, $\phi$ is the probability factor that can bring $\mu$ into the “expected utility scale.” More precisely, assume for simplicity that the probability and utility factors in a joint factor have the same scope; we can make this assumption without loss of generality by simply increasing the scope of either or both factors, duplicating entries as required. Intuitively, the probability factor is maintained as an auxiliary factor, used to normalize the utility factor when necessary, so as to bring it back to the standard “utility scale.” Thus, if we have a joint factor $(\phi(Y),\mu(Y))$ ) , then $\mu(Y)/\phi(Y)$ is a factor whose entries are expected utilities associated with the diferent assignments $_{_y}$ . 

Our goal is to deﬁne a variable-elimination-style algorithm using these joint factors. As for any variable elimination algorithm, we must deﬁne operations that combine factors, and operations that marginalize — sum out — variables out of a factor. We now deﬁne both of these steps. We consider, for the moment, factors associated only with probability variables and utility variables; we will discuss later how to handle decision variables. 

Initially, each variable $W$ that is associated with a CPD induces a probability factor $\phi_{W}$ ; such variables include both chance variables in $\mathcal{X}$ and decision variab s associated with a ecision rule. (As we discussed, a decision rule for a decision variable D essentially turns D into a chance variable.) We convert $\phi_{W}$ into a joint factor $\gamma_{W}$ by attaching to it an all-zeros utility factor over the same scope, ${\bf0}_{S c o p e[\phi_{W}]}$ . Similarly, each utility variable $V\in{\mathcal{U}}$ is associated with a utility factor $\mu_{V}$ , which we convert to a joint factor by attaching to it an all-ones probability factor: $\gamma_{V}=({\bf1}_{\mathrm{Pa}_{V}},V)$ for $V\in{\mathcal{U}}$ . 

Intuitively, we want to multiply probability components (as usual) and add utility components. Thus, we deﬁne the joint factor combination operation as follows: 

• For two joint factors $\gamma_{1}\,=\,\left(\phi_{1},\mu_{1}\right)$ , $\gamma_{2}=(\phi_{2},\mu_{2})$ , we deﬁne the joint factor combination operation: 

$$
\gamma_{1}\bigoplus\gamma_{2}=(\phi_{1}\cdot\phi_{2},\mu_{1}+\mu_{2}).
$$ 

We see that, if all of the joint factors in the inﬂuence diagram are combined, we obtain a single (exponentially large) probability factor that deﬁnes the joint distribution over outcomes, and a single (exponentially large) utility factor that deﬁnes the utilities of the outcomes. Of course, this procedure is not one that we would ever execute; rather, as in variable elimination, we want to interleave combination steps and marginalization steps in a way that preserves the correct semantics. 

The deﬁnition of the marginalization operation is subtler. Intuitively, we want the probability of an outcome to be multiplied with its utility. However, as suggested by example 23.15, we must take care that the utility factors derived as intermediate results all maintain the same scale, so that they can be correctly added in the factor combination operation. Thus, when marginalizing a variable $W$ , we divide the utility factor by the associated probability factor, ensuring that it maintains its expected utility interpretation: 

• For a joint factor $\gamma\;=\;(\phi,\mu)$ over scope $W$ , we deﬁne the joint factor marginalization operation for $W^{\prime}\subset W$ as follows: 

$$
m a r g_{W^{\prime}}(\gamma)=\left(\sum_{W^{\prime}}\phi,\frac{\sum_{W^{\prime}}\phi\cdot\mu}{\sum_{W^{\prime}}\phi}\right).
$$ 

Intuitively, this operation marginalizes out (that is, eliminates) the variables in $W^{\prime}$ , handling both utility and probability factors correctly. 

Finally, at the end of the process, we can combine the probability and utility factors to obtain a single factor that corresponds to the overall expected utility: 

• For a joint factor $\gamma\,=\,(\phi,\mu)$ , we deﬁne the joint factor contraction operation as the factor product of the two components: 

$$
\operatorname{cont}(\gamma)=\phi\cdot\mu.
$$ 

To understand these deﬁnitions, consider again the problem of computing the expected utility for some (complete) strategy $\sigma$ for the inﬂuence diagram $\mathcal{T}$ . Thus, we now have a probability 

![](images/6a81f868a23eb9c6302442049fe00a30a620dc04064fed705c9e217f647ba05b.jpg) 

factor for each decision variable. Recall that our expected utility is deﬁned as: 

$$
\mathrm{EU}[\mathcal{I}[\sigma]]=\sum_{W\in\mathcal{X}\cup\mathcal{D}}\prod_{W\in\mathcal{X}\cup\mathcal{D}}\phi_{W}\cdot(\sum_{V\in\mathcal{U}}\mu_{V}).
$$ 

Let $\gamma^{*}$ be the marginalization over all variables of the combination of all of the joint factors: 

$$
\gamma^{*}=(\phi^{*},\mu^{*})=m a r g_{\emptyset}(\bigoplus_{(W\in\mathcal{X}\cup\mathcal{U})}[\gamma_{W}]).
$$ 

Note that the factor has empty scope and is therefore simply a pair of numbers. We can now show the following simple result: 

# Proposition 23.1 

For $\gamma^{*}$ deﬁned in equation (23.8), we have: $\gamma^{*}=(1,\operatorname{EU}[\mathcal{Z}[\sigma]])$ . 

The proof follows directly from the deﬁnitions and is left as an exercise (exercise 23.2). 

Of course, as we discussed, we want to interleave the marginalization and combination steps. An algorithm implementing this idea is shown in algorithm 23.2. The algorithm returns a single joint factor $(\phi,\mu)$ . 

Example 23.16 Let us consider the behavior of this algorithm on the inﬂuence diagram of example 23.12, assuming again that we have a decision rule for $D$ , so that we have only chance variables and utility variables. Thus, we initially have ﬁve joint factors derived from the probability factors for $A,B,C,D,E,$ ; for example, we have $\gamma_{B}=(P(B\mid A),\mathbf{0}_{A,B})$ . We have two joint factors $\gamma_{1},\gamma_{2}$ derived from the utility variables $V_{1},V_{2}$ ; for example, we have $\gamma_{2}=({\bf1}_{C,E},V_{2}(C,E))$ . 

Now, consider running our generalized variable elimination algorithm, using the elimination ordering $C,\,A,\,E,\,B,\,D$ . Eliminating $C$ , we ﬁrst combine $\gamma_{C},\gamma_{2}$ to obtain: 

$$
\gamma_{C}\bigoplus\gamma_{2}=(P(C),V_{2}(C,E)),
$$ 

where the scope of both components is taken to be $C,E$ . We then marginalize $C$ to obtain: 

$$
\begin{array}{c c l}{{\gamma_{3}(E)}}&{{=}}&{{\displaystyle\left({\bf1}_{E},\frac{\sum_{C}(P(C)V_{2}(C,E))}{{\bf1}_{E}}\right)}}\\ {{}}&{{=}}&{{({\bf1}_{E},{\cal E}_{P(C)}[V_{2}(C,E)]).}}\end{array}
$$ 

Continuing to eliminate $A$ , we combine $\gamma_{A},\,\gamma_{B}$ , and $\gamma_{1}$ and marginalize $A$ to obtain: 

$$
\begin{array}{r c l}{{\gamma_{4}(B,E)}}&{{=}}&{{\displaystyle\left(\sum_{A}P(A)P(B\mid A),\frac{\sum_{A}P(A)P(B\mid A)V_{1}(A,E)}{\sum_{A}P(A)P(B\mid A)}\right)}}\\ {{}}&{{=}}&{{\displaystyle(P(B),\sum_{A}P(A\mid B)V_{1}(A,E))}}\\ {{}}&{{=}}&{{\displaystyle(P(B),E_{P(A\mid B)}[V_{1}(A,E)]).}}\end{array}
$$ 

Importantly, the utility factor here can be interpreted as the expected utility over $V_{1}$ given $B$ , where the expectation is taken over values of $A$ . It therefore keeps this utility factor on the same scale as the others, avoiding the problem of incomparable utility factors that we had in example 23.15. 

We next eliminate $E$ . We ﬁrst combine $\gamma_{E},\,\gamma_{3}$ , and $\gamma_{4}$ to obtain: 

$$
(P(E\mid D)P(B),E_{P(C)}[V_{2}(C,E)]+E_{P(A\mid B)}[V_{1}(A,E)]).
$$ 

Marginalizing $E$ , we obtain: 

$$
\begin{array}{r l r}{\gamma_{5}(B,D)}&{=}&{(P(B),E_{P(C,E|D)}[V_{2}(C,E)]+E_{P(A,E|B,D)}[V_{1}(A,E)]).}\end{array}
$$ 

To eliminate $B$ , we ﬁrst combine $\gamma_{5}$ and $\gamma_{D}$ , to obtain: 

$$
(P(D\mid B)P(B),E_{P(C,E\mid D)}[V_{2}(C,E)]+E_{P(A,E\mid D)}[V_{1}(A,E)]).
$$ 

We then marginalize $B$ , obtaining: 

$$
\begin{array}{r c l}{\gamma_{6}(D)}&{=}&{\left(P(D),\frac{\sum_{B}\left(E_{P(C,E,D,B)}\left[V_{2}(C,E)\right]+E_{P(A,E,D,B)}\left[V_{1}(A,E)\right]\right)}{P(D)}\right)}\\ &{=}&{\left(P(D),E_{P(C,E|D)}[V_{2}(C,E)]+E_{P(A,E|B,D)}[V_{1}(A,E)]\right).}\end{array}
$$ 

Finally, we have only to marginalize $D$ , obtaining: 

$$
\begin{array}{r l r}{\gamma_{7}(\emptyset)}&{=}&{\left(1,E_{P(C,E)}[V_{2}(C,E)]+E_{P(A,E)}[V_{1}(A,E)]\right),}\end{array}
$$ 

as desired. 

How do we show that it is legitimate to reorder these marginalization and combination operators? In exercise 9.19, we deﬁned the notion of generalized marginalize-combine factor operators and stated a result showing that, for any pair of operators satisfying certain conditions, any legal reordering of the operators led to the same result. In particular, this result implied, as special cases, correctness of sum-product, max-product, and max-sum variable elimination. The same analysis can be used for the operators deﬁned here, showing the following result: 

Let $\Phi$ be a set of joint factors over $Z$ . Generalized-VE-for- $|\mathrm{D}\mathrm{s}(\Phi,W)|$ returns the joint factor 

$$
m a r g_{W}(\bigoplus_{(\gamma\in\Phi)}\gamma).
$$ 

The proof is left as an exercise (exercise 23.3). 

Note that the complexity of the algorithm is the same (up to a constant factor) as that of a standard VE algorithm, applied to an analogous set of factors — with the same scope — as our initial probability and utility factors. In other words, for a given elimination ordering, the cost of the algorithm grows as the induced tree-width of the graph generated by this initial set of factors. 

So far, we have discussed the problem of computing the expected utility of a complete strategy. How can we apply these ideas to our original task, of optimizing a single decision rule? The idea is essentially the same as in section 23.4.1. As there, we apply Generalized-VE-for-IDs to eliminate all of the variables other than $\mathrm{uniformly}_{D}=\{D\}\cup\mathrm{Pa}_{D}$ . In this process, the probability factor induced by the decision rule for D is only combined with the other factors at the ﬁnal step of the algorithm, when the remaining factors are all combined. It thus has no efect on the factors produced up to that point. We can therefore omit $\phi_{D}$ from our computation, and produce a joint factor $\gamma_{-D}=\left(\phi_{-D},\mu_{-D}\right)$ over $\mathrm{vartheta}_{D}$ based only on the other factors in the network. 

For any decision rule $\delta_{D}$ , if we run Generalized-VE-for-IDs on the factors in the original inﬂuence diagram plus a joint factor $\gamma_{D}=(\delta_{D},\mathbf{0}_{\mathrm{FImij}_{D}})$ , we would obtain the factor 

$$
\gamma_{\delta_{D}}=\gamma_{-D}\bigoplus\gamma_{D}.
$$ 

Rewriting this expression, we see that the overall expected utility for the inﬂuence diagram given the decision rule $\delta_{D}$ is then: 

$$
\sum_{\pmb{w}\in V a l(\mathrm{Pa}_{D}),d\in V a l(D)}\mathrm{cont}(\gamma_{-D})(\pmb{w},d)\delta_{D}(\pmb{w}).
$$ 

Based on this observation, and on the fact that we can always select an optimal decision rule that is a deterministic function from $V a l(\mathrm{Pa}_{D})$ to $V a l(D)$ , we can easily optimize $\delta_{D}$ . For each assignment $\mathbfit{w}$ to $\mathrm{Pa}_{D}$ , we select 

$$
\delta_{D}({\pmb w})=\arg\operatorname*{max}_{d\in V a l(D)}\operatorname{cont}(\gamma_{-D})({\pmb w},d).
$$ 

As before, the problem of optimizing a single decision rule can be solved using a standard variable elimination algorithm, followed by a simple optimization. In this case, we must use a generalized variable elimination algorithm, involving both probability and utility factors. 

# 23.5 Optimization in Inﬂuence Diagrams 

We now turn to the problem of selecting an optimal strategy in an inﬂuence diagram. We begin with the simple case, where we have only a single decision variable. We then show how to extend these ideas to the more general case. 

# 23.5.1 Optimizing a Single Decision Rule 

We ﬁrst make the important observation that, for the case of a single decision variable, the task of ﬁnding an optimal decision rule can be reduced to that of computing a single utility factor. 

We begin by rewriting the expected utility of the inﬂuence diagram in a diferent order: 

$$
\operatorname{EU}[\mathcal{Z}[\sigma]]=\sum_{D,\operatorname{Pa}_{D}}\delta_{D}\sum_{\mathcal{X}-\operatorname{Pa}_{D}}\prod_{X\in\mathcal{X}}P(X\mid\operatorname{Pa}_{X})(\sum_{V\in\mathcal{U}}V).
$$ 

Our task is to select $\delta_{D}$ . 

expected utility factor 

We now deﬁne the expected utility factor to be the value of the internal summation in equation (23.9): 

$$
\mu_{-D}=\sum_{\mathcal{X}-\mathrm{Pa}_{D}}\prod_{X\in\mathcal{X}}P(X\mid\mathrm{Pa}_{X})(\sum_{V\in\mathcal{U}}V).
$$ 

This expression is the marginalization of this product on the variables $D\cup\mathrm{Pa}_{D}$ ; importantly, it does not depend on our choice of decision rule for D . Given $\mu_{-D}$ , we can compute the expected utility for any decision rule $\delta_{D}$ as: 

$$
\sum_{D,\mathrm{Pa}_{D}}\delta_{D}\mu_{-D}\bigl(D,\mathrm{Pa}_{D}\bigr).
$$ 

Our goal is to ﬁnd $\delta_{D}$ that maximizes this expression. 

# Proposition 23.2 

Consider an inﬂuence diagram $\overline{{\mathcal{Z}}}$ with a ngl decision variable $D$ . Letting $\mu_{-D}$ be as in equa- tion (23.10), the optimal decision rule for D in is deﬁned as: 

$$
\delta_{D}({\pmb w})=\arg\operatorname*{max}_{d\in V a l(D)}\mu_{-D}(d,{\pmb w})
$$ 

$$
\forall w\in V a l(\mathrm{Pa}_{D}).
$$ 

The proof is left as an exercise (see exercise 23.1). 

Thus, we have shown how the problem of optimizing a single decision rule can be solved very simply, once we have computed the utility factor $\mu_{-D}(D,\mathrm{Pa}_{D})$ . 

Importantly, any of the algorithms described before, whether the simpler ones in section 23.4.1 and 23.4.2, or the more elaborate generalized variable elimination algorithm of section 23.4.3, can be used to compute this expected utility factor. We simply structure our elimination ordering to eliminate only the variables other than $D,\mathrm{Pa}_{D}$ ; we then combine all of the factors that are computed via this process, to produce a single integrated factor $\mu_{-D}(D,\mathrm{Pa}_{D})$ . We can then use this factor as in proposition 23.2 to ﬁnd the optimal decision rule for $D$ , and thereby solve the inﬂuence diagram. 

How do we generalize this approach to the case of an inﬂuence diagram with multiple decision rules $D_{1},\ldots,D_{k};$ ? In principle, we could generate an expected utility factor where we eliminated all variables other than the union $Y=\cup_{i}(\{D_{i}\}\cup\mathrm{Pa}_{D_{i}})$ of all of the decision variables and all of their parents. Intuitively, this factor would specify the expected utility of the inﬂuence diagram given an assignment to $Y$ . However, in this case, the optimization problem is much more complex, in that it requires that we consider simultaneously the decisions at all of the decision variables in the network. Fortunately, as we show in the next section, we can perform this multivariable optimization using localized optimization steps over single variables. 

# 23.5.2 Iterated Optimization Algorithm 

In this section, we describe an iterated approach that breaks up the problem into a series of simpler ones. Rather than optimize all of the decision rules at the same time, we ﬁx all of the decision rules but one, and then optimize the remaining one. The problem of optimizing a single decision rule is signiﬁcantly simpler, and admits very efcient algorithms, as shown in section 23.5.1. This algorithm is very similar in its structure to the local optimization approach for marginal MAP problems, presented in section 13.7. Both algorithms are intended to deal with the same computational bottleneck: the exponentially large factors generated by a constrained elimination ordering. They both do so by optimizing one variable at a time, keeping the others ﬁxed. The diference is that here we are optimizing an entire decision rule for the decision variable, whereas there we are simply picking a single value for the MAP variable. 

We will show that, under certain assumptions, this iterative approach is guaranteed to converge to the optimal strategy. Importantly, this approach also applies to inﬂuence diagrams with imperfect recall, and can therefore be considerably more efcient. 

locally optimal decision rule 

The basic idea behind this algorithm is as follows. The algorithm proceeds by sequentially optimizing individual decision rules. We begin with some (almost) arbitrary strategy $\sigma$ , which assigns a decision rule to all decision variables in the network. We then optimize a single decision rule relative to our current assignment to the others. This decision rule is used to update $\sigma$ , and another decision rule is now optimized relative to the new strategy. More precisely, let $\sigma_{-D}$ denote the decision rules in a strategy $\sigma$ other than the one for $D$ . We say that a decision rule $\delta_{D}$ is locally optimal for a strategy $\sigma$ if, for any other decision rule $\delta_{D}^{\prime}$ , 

$$
\mathrm{EU}[\mathcal{Z}[(\sigma_{-D},\delta_{D})]]\ge\mathrm{EU}[\mathcal{Z}[(\sigma_{-D},\delta_{D}^{\prime})]].
$$ 

Our algorithm starts with some strategy $\sigma$ , and then iterates over diferent decision variables $D$ . It then selects a locally optimal decision rule $\delta_{D}$ for $\sigma$ , and updates $\sigma$ by replacing $\sigma_{D}$ with the new $\delta_{D}$ . Not that the inﬂuence diagram $\mathcal{T}[\sigma_{-D}]$ is an inﬂuence diagram with the single decision variable D , which can be solved using a variety of methods, as described earlier. The algorithm terminates when no decision rule can be improved by this process. 

Perhaps the most important property of this algorithm is its ability to deal with the main computational limitation of the simple variable elimination strategy described in section 23.3.2: the fact that the constrained variable elimination ordering can require creation of large factors even when the network structure does not force them. 

Example 23.17 Consider again example 23.11; here, we would begin with some set of decision rules for all of $D_{1},\ldots,D_{k}$ . We would then iteratively compute the expected utility factor $\mu_{-D_{i}}$ for one of the $D_{i}$ variables, using the (current) decision rules for the others. We could then optimize the decision rule for $D_{i}$ , and continue the process. Importantly, the only constraint on the variable elimination ordering is that $D_{i}$ and its parents be eliminated last. With these constraints, the largest factor induced in any of these variable elimination procedures has size 4, avoiding the exponential blowup in $k$ that we saw in example 23.11. 

In a naive implementation, the algorithm runs variable elimination multiple times — once for each iteration — in order to compute $\mu_{-D_{i}}$ . However, using the approach of joint (probability, utility) factors, as described in section 23.4.3, we can provide a very efcient implementation as a clique tree. See exercise 23.10. 

So far, we have ignored several key questions that afect both the algorithm’s complexity and its correctness. Most obviously, we can ask whether this iterative algorithm even converges. When we optimize $D$ , we either improve the agent’s overall expected utility or we leave the 

decision rule unchanged. Because the expected utility is bounded from above and the total number of strategies is discrete, the algorithm cannot improve the expected utility indeﬁnitely. Thus, at some point, no additional improvements are possible, and the algorithm will terminate. A second question relates to the quality of the solution obtained. Clearly, this solution is locally optimal, in that no change to a single decision rule can improve the agent’s expected utility. However, local optimality does not, in general, imply that the strategy is globally optimal. 

Consider an inﬂuence diagram containing only two decision variables $D_{1}$ and $D_{2}$ , and a utility variable $V(D_{1},D_{2})$ deﬁned as follows: 

$$
V(d_{1},d_{2})=\left\{\begin{array}{l l}{{2\qquad}}&{{d_{1}=d_{2}=1}}\\ {{1\qquad}}&{{d_{1}=d_{2}=0}}\\ {{0\qquad}}&{{d_{1}\neq d_{2}.}}\end{array}\right.
$$ 

The strategy $(0,0)$ is locally optimal for both decision variables, since the unique optimal decision for $D_{i}$ when $D_{j}=0$ $(j\neq i)$ ) is $D_{i}=0$ . On the other hand, the globally optimal strategy is $(1,1).$ 

 However, under certain conditions, local optimality does imply global optimality, so that the iterated optimization process is guaranteed to converge to a globally optimal solution. These conditions are more general than perfect recall, so that this algorithm works in every case where the algorithm of the previous section applies. In this case, we can provide an ordering for applying the local optimization steps that guarantees that this process converges to the globally optimal strategy after modifying each decision rule exactly once. However, this algorithm also applies to networks that do not satisfy the perfect recall assumption, and in certain such cases it is even guaranteed to ﬁnd an optimal solution. By relaxing the perfect recall assumption, we can avoid some of the exponential blowup of the decision rules in terms of the number of decisions in the network. 

# 23.5.3 Strategic Relevance and Global Optimality $\star$ 

The algorithm described before iteratively changes the decision rule associated with individual decision variables. In general, changing the decision rule for one variable $D^{\prime}$ can cause a decision rule previously optimal for another variable $D$ to become suboptimal. Therefore, the algorithm must revisit $D$ and possibly select a new decision rule for it. In this section, we provide conditions under which we can guarantee that changing the decision rule for $D^{\prime}$ will not necessitate a change in the decision rule for $D$ . In other words, we deﬁne conditions under which the decision rule for $D^{\prime}$ may not be relevant for optimizing the decision rule for $D$ . Thus, if we choose a decision rule for $D$ and later select one for $D^{\prime}$ , we do not have to revisit the selection made for $D$ . As we show, under certain conditions, this criterion allows us to optimize all of the decision rules using a single iteration through them. 

23.5.3.1 Strategic Relevance 

Intuitively, we would like to deﬁne a decision variable $D^{\prime}$ as strategically relevant to $D$ if, to optimize the decision rule at $D$ , the decision maker needs to consider the decision rule at $D^{\prime}$ . That is, we want to say that $D^{\prime}$ is is relevant to $D$ if there is a partial strategy proﬁle $\sigma$ over $\mathcal{D}-\left\{D,D^{\prime}\right\}$ , two decision rules $\delta_{D^{\prime}}$ and $\delta_{D^{\prime}}^{\prime}$ , and a decision rule $\delta_{D}$ , such that $(\sigma,\delta_{D},\delta_{D^{\prime}})$ is optimal, but $(\sigma,\delta_{D},\delta_{D^{\prime}}^{\prime})$ is not. 

Example 23.19 

imple inﬂuence diagram where we have two decision variables $D_{1}\to D_{2}$ , and a ut $V(D_{1},D_{2})$ that is the same as the one used in example 23.18. Pick an arbitrary decision rule $\delta_{D_{1}}$ 1 (not necessarily deterministic), and consider the problem of optimizing $\delta_{D_{2}}$ relative to $\delta_{D_{1}}$ . The overall expected utility for the agent is 

$$
\sum_{d_{1}}\delta_{D_{1}}(d_{1})\sum_{d_{2}}\delta_{D_{2}}(d_{2}\mid d_{1})V(d_{1},d_{2}).
$$ 

An optimal decision for $D_{2}$ given the information state $d_{1}$ is arg $\mathrm{max}_{d_{2}}\,V(d_{1},d_{2})$ , regardless of the choice of decision rule for $D_{1}$ . Thus, in this setting, we can pick an arbitrary decision rule $\delta_{D_{1}}$ and optimize $\delta_{D_{2}}$ relative to it; our selected decision rule will then be locally optimal relative to any decision rule for $D_{1}$ . However, there is a subtlety that makes the previous statement false in certain settings. Let $\delta_{D_{1}}^{\prime}\,=\,d_{1}^{0}$ . Then one optimal decision rule for $D_{2}$ is $\delta_{D_{2}}^{\prime}(d_{1}^{0})\,=\,\delta_{D_{2}.}^{\prime}(d_{1}^{1})\,=\,d_{2}^{0}$ . Clearly, $d_{2}^{0}$ is the right choice when $D_{1}\,=\,d_{1}^{0}$ , but it is suboptimal when $D_{1}\,=\,d_{1}^{1}$ . However, because $\delta_{D_{1}}$ gives this latter event probability 0 , this choice for $\delta_{D_{2}}$ is locally optimal relative to $\delta_{D_{1}}$ . 

fully mixed decision rule 

Deﬁnition 23.7 strategic relevance 

As this example shows, a decision rule can make arbitrary choices in information states that have probability zero without loss in utility. In particular, because $\delta_{D_{1}}^{\prime}$ assigns probability zero to $d_{1}^{1}$ , the “suboptimal” $\delta_{D_{2}}^{\prime}$ is locally optimal relative to $\delta_{D_{1}}^{\prime}$ ; however, $\delta_{D_{2}}^{\prime}$ is not locally optimal relative to other decision rules for $D_{1}$ . Thus, if we use the previous deﬁnition, $D_{1}$ appears relevant to $D_{2}$ despite our intuition to the contrary. We therefore want to avoid probability-zero events, which allow situations such as this. We say that a decision rule is fully mixed if each probability distribution $\delta_{D}(D\ |\ \mathrm{pa}_{D})$ assigns nonzero probability to all values of $D$ . We can now formally deﬁne strategic relevance. 

Le $D$ and $D^{\prime}$ e decision nodes in an i uence diagram $\mathcal{T}$ . We say that $D^{\prime}$ is strategically relevant to D (or that D strategically relies on $D^{\prime}$ ) if there exist: 

• a partial strategy proﬁle $\sigma$ over $\mathcal{D}-\left\{D,D^{\prime}\right\}$ ; • two decision rules $\delta_{D^{\prime}}$ and $\delta_{D^{\prime}}^{\prime}$ such that $\delta_{D^{\prime}}$ is fully mixed; • a decision rule $\delta_{D}$ that is optimal for $(\sigma,\delta_{D^{\prime}})$ but not for $(\sigma,\delta_{D^{\prime}}^{\prime})$ . 

This deﬁnition does not provide us with an operative procedure for determining relevance. We can obtain such a procedure by considering an alternative mathematical characterization of the notion of local optimality. 

# Proposition 23.3 

Let $\delta_{D}$ be a decision ule for a decision variable $D$ in $\mathcal{T}$ , a let $\sigma$ be a strategy for $\mathcal{T}$ . Then $\delta_{D}$ is locally optimal for σ if and only if for every instantiation w of $\mathrm{Pa}_{D}$ where $P_{\mathcal{B}_{\mathcal{Z}[\sigma]}}(\pmb{w})>0$ , the probability distribution $\delta_{D}(D\mid\pmb{w})$ is a solution to 

$$
\arg\operatorname*{max}_{q(D)}\sum_{d\in V a l(D)}q(d)\sum_{V\in\mathcal{U}\succ_{D}}\sum_{v\in V a l(V)}P_{\mathcal{B}_{\mathcal{Z}[\sigma]}}(v\mid d,\mathbf{w})\cdot v,
$$ 

where $\mathcal{U}_{\succ D}$ is the set of utility nodes in $\mathcal{U}$ that are descendants of $D$ in $\mathcal{T}$ . 

![](images/08867e172add3ef16e1bd790c1d48988965209c5d072123b5909a76673d841d7.jpg) 
Figure 23.7 Inﬂuence diagrams, augmented to test for s-reachability 

The proof is left as an exercise (exercise 23.6). 

The signiﬁcance of this result arises from two key points. First, the only probability expres- sions appearin in the optimization cri rion are of the form $P_{\mathcal{B}_{\mathcal{Z}[\sigma]}}(V\,\mid\,\mathrm{varphi}_{D})$ for some utility variable V and decision variable D . Thus, we care about a decision rule $\delta_{D^{\prime}}$ only if the CPD induced by this decision rule afects the value of one of these probability expressions. Sec- ond, the only utility variables that participate in these expressions are those that are descendants of $D$ in the network. 

# 23.5.3.2 S-Reachability 

requisite CPD We have reduced our problem to one of determining which decision rule CPDs might afect the value of some expression $P_{\mathcal{B}_{\mathcal{Z}[\sigma]}}(V\,\mid\,\mathrm{varphi}_{D})$ , for $V\,\in\mathcal{U}_{\succ D}$ . In other words, we need to determine whether the decision variable is a requisite CPD for this query. We also encountered this question in a very similar context in section 21.3.1, when we wanted to determine whether an intervention (that is, a decision) was relevant to a query. As described in exercise 3.20, we can determine whether the CPD for a variable $Z$ is requisite for answer g a query $P(X\mid Y)$ with a simple gr hical criterion: We in duce a w “dummy” parent $\widehat{Z}$ b whose values correspond to diferent choices for the CPD of Z . Then Z is a requisite probability node for $P(X\mid Y)$ if and only if $\widehat{Z}$ has an active trail to X given Y . 

Based on this concept and equation (23.12), we can deﬁne $s$ -reachability — a graphical criterion for detecting strategic relevance. 

Deﬁnition 23.8 s-reachable 

A decision variable $D^{\prime}$ is s-reachable from decision variable $D$ in an $I\!D\,\mathcal{T}$ if there is some utility ode $V\in\mathcal{U}_{\succ D}$ such t ew parent c were added to $D^{\prime}$ , there would be an active path in $\mathcal{T}$ I from c to $V$ given $\mathrm{Family}_{D}$ , where a path is active in an $I D$ if it is active in the same graph, viewed as a BN. 

Note that unlike d-separation, s-reachability is not necessarily a symmetric relation. 

# Example 23.20 

on the decision rule for $D^{\prime}$ , he does not need to know the decision rule for $D$ in order to evaluate his options at $D^{\prime}$ . Thus, $D^{\prime}$ does not strategically rely on $D$ . Indeed, if we add a dummy parent $\widehat{D}$ to $D$ we hav hat $V$ is $d$ -separated from $\widehat{D}$ given $\mathrm{Family}_{D^{\prime}}\:=\:\{D,D^{\prime}\}$ . Thus, $D$ is t $s$ -reachable from D ′ . Conversely, the agent’s decision rule at $D^{\prime}$ does inﬂuence his payof at D , nd so D ′ is rele t to D , if we add a du y parent ${\widehat{D^{\prime}}}$ to $D^{\prime}$ , we have that $V$ is not d-separated from given $D,\mathrm{Pa}_{D}$ . 

By contrast, in (b), the agent forgets his action at c D when observing $D^{\prime}$ ; as his utility node is inﬂuenced by both decisions, we have that each decision is relevant to the other. The s-reachability analysis using $d$ -separation from the dummy parents supports this intuition. 

The notion of s-reachability is sound and complete for strategic relevance (almost) in the same sense that d-separation is sound and complete for independence in Bayesian networks. As for d-separation, the soundness result is very strong: without s-reachability, one decision cannot be relevant to another. 

If $D$ and $D^{\prime}$ are two decisi nodes in an $I\!D\,\mathcal{T}$ and $D^{\prime}$ is not $s$ -reachable from $D$ in $\mathcal{T}$ , then $D$ does not strategically rely on $D^{\prime}$ ′ . 

roof Let $\sigma$ be a strategy proﬁle for $\mathcal{T}$ , and let $\delta_{D}$ be a decision rule for $D$ that is optimal for σ . Let $\mathcal{B}=\mathcal{B}_{\mathcal{Z}[\sigma]}$ . By proposition 23.3, for every $\pmb{w}\,\in\,V a l(\mathrm{Pa}_{D})$ such that $P_{\mathcal{B}}(w)\,>\,0$ , the distribution $\delta_{D}(D\mid\pmb{w})$ must be a solution of the maximization problem: 

$$
\arg\operatorname*{max}_{P(D)}\sum_{d\in V a l(D)}P(d)\sum_{V\in\mathcal{U}_{\succ D}}\sum_{v\in V a l(V)}P_{\mathcal{B}}(v\mid d,\mathbf{w})\cdot v.
$$ 

Now, let $\sigma^{\prime}$ be any strategy proﬁle for $\mathcal{T}$ t t difers from $\sigma$ y at $D^{\prime}$ nd let $\mathcal B^{\prime}=\mathcal B_{\mathcal T[\sigma^{\prime}]}$ . We must construct a decision rule $\delta_{D}^{\prime}$ for D that agrees with $\delta_{D}$ on all w where $P_{\mathcal{B}}(w)>0$ , and that is optimal for $\sigma^{\prime}$ . By proposition 23.3, it sufces to show that for every $\mathbfit{w}$ where $P_{\mathcal{B}^{\prime}}(w)>0$ , $\delta_{D}^{\prime}(D\mid{\pmb w})$ is a solution of: 

$$
\arg\operatorname*{max}_{P(D)}\sum_{d\in V a l(D)}P(d)\sum_{V\in\mathcal{U}_{\succ D}}\sum_{v\in V a l(V)}P_{\mathcal{B}^{\prime}}(v\mid d,\mathbf{w})\cdot v.
$$ 

If $P_{\mathcal B}({\pmb w})\;=\;0$ , then our choice of $\delta_{D}^{\prime}(D\ \mid\ w)$ | is unconstrained; we can simply select a distribution that satisﬁes equation (23.14). For other $\mathbfit{w}$ , we must let $\delta_{D}^{\prime}(D\mid\pmb{w})=\delta_{D}(D\mid\pmb{w})$ | | . We know that $\delta_{D}(D\mid\pmb{w})$ is a solu on (23.13), and the two expr ferent only in that e uatio 23.13) uses $P_{\mathcal{B}}(v\mid d,\pmb{w})$ B | a equation 14) uses $P_{\mathcal{B}^{\prime}}(v\mid d,w)$ B | . The two networks B and B $\mathcal{B}^{\prime}$ difer only in the CPD for D $D^{\prime}$ . Because $D^{\prime}$ is not a requisite probability node for any $V\in\mathcal{U}_{\succ D}$ given $D,\mathrm{Pa}_{D}$ , we have that $P_{\mathcal{B}}(v\mid d,\pmb{w})=P_{\mathcal{B}^{\prime}}(v\mid d,\pmb{w})$ , and that $\delta_{D}^{\prime}(D\mid\pmb{w})=\delta_{D}(D\mid\pmb{w})$ | | is a solution of equation (23.14), as required. 

Thus, s-reachability provides us with a sound criterion for determining which decision vari- ables $D^{\prime}$ are strategically relevant for $D$ . As for $\mathrm{d}$ -separation, the completeness result is not as strong: s-reachability does not imply relevance in every ID. We can choose the probabilities and utilities in the ID in such a way that the inﬂuence of one decision rule on another does not manifest itself. However, s-reachability is the most precise graphical criterion we can use: it will not identify a strategic relevance unless that relevance actually exists in some ID that has the given graph structure. 

![](images/56d854032744823e7ce8e9fb3349b2100dbbec1ed1def1054a87bd55f523f4d1.jpg) 
Figure 23.8 Four simple inﬂuence diagrams (top), and their relevance graphs (bottom). 

# Theorem 23.3 

If a node $D^{\prime}$ is s-reachable from a node $D$ in an ID, then there is some ID with the same graph structure in which $D$ strategically relies on $D^{\prime}$ . 

This result is roughly analogous to theorem 3.4, which states that there exists some parame- terization that manifests the dependencies not induced by d-separation. A result analogous to the strong completeness result of theorem 3.5 is not known for this case. 

# 23.5.3.3 The Relevance Graph 

We can get a global view of the strategic dependencies between diferent decision variables in an inﬂuence diagram by putting them within a single graphical data structure. 

Deﬁnition 23.9 relevance graph 

To construct the graph for a given ID, we need to determine, for each decision node $D$ , the set of nodes $D^{\prime}$ that are s-reachable from $D$ . Using standard methods from chapter 3, we can ﬁnd this set for any given $D$ in time linear in the number of chance and decision variables in the ID. By repeating the algorithm for each $D$ , we can derive the relevance graph in time $O((n+k)k)$ where $n=|{\mathcal{X}}|$ and $k=|\mathcal{D}|$ . Recall our original statement that a decision node $D$ strategically relies on a decision node $D^{\prime}$ if one needs to know the decision rule for $D^{\prime}$ in order to evaluate possible decision rules for $D$ . Intuitively, if the relevance graph is acyclic, we have a decision variable that has no parents in the graph, and hence relies on no other decisions. We can optimize the decision rule at this variable relative to some arbitrary strategy for the other decision rules. Having optimized that decision rule, we can ﬁx its strategy and proceed to optimize the next one. Conversely, if we have a cycle in the relevance graph, then we have some set of decisions all of which rely on each other, and their decision rules need to be optimized together. In this case, the simple iterative approach we described no longer applies. 

However, before we describe this iterative algorithm formally and prove its correctness, it is instructive to examine some simple IDs and see when one decision node relies on another. 

Consider the four examples shown in ﬁgure 23.8, all of which relate to a setting where the agent ﬁrst makes decision $D$ and then $D^{\prime}$ . Examples (a) and (b) are the ones we previously saw in example 23.20, showing the resulting relevance graphs. As we saw, in (a), we have that $D$ relies on $D^{\prime}$ but not vice versa, leading to the structure shown on the bottom. In (b) we have that each decision relies on the other, leading to a cyclic relevance graph. Example (c) represents a situation where the agent does not remember $D$ when making the decision $D^{\prime}$ . However, the agent knows everything he needs to about $D$ : his utility does not depend on $D$ directly, but only on the chance node, which he can observe. Hence $D^{\prime}$ does not rely on $D$ . 

One might conclude that a decision node $D^{\prime}$ never relies on another $D$ when $D$ is observed by $D^{\prime}$ , but the situation is subtler. Consider example (d), which represents a simple card game: the agent observes a card and decides whether to bet $(D);$ at a later stage, the agent remembers only his bet but not the card, and decides whether to raise his bet $(D^{\prime});$ the utility of both depends on the total bet and the value of the card. Even though the agent does remember the actual decision at $D$ , he needs to know the decision rule for $D$ in order to know what the value of $D$ tells him about the value of the card. Thus, $D^{\prime}$ relies on $D$ ; indeed, when $D$ is observed, there is an active trail from a hypothetical parent $\hat{D}$ that runs through the chance node to the utility node. 

However, it is the case that perfect recall — remembering both the previous decisions and the previous observations, does imply that the underlying relevance graph is acyclic. 

# Theorem 23.4 

The proof follows directly from properties of d-separation, and it is left as an exercise (exer- cise 23.7). We note that the ordering of the decisions in the relevance graph will be the opposite of the ordering in the original ID, as in ﬁgure $23.8a$ . 

23.5.3.4 Global Optimality 

Using the notion of a relevance graph, we can now provide an algorithm that, under certain conditions, is guaranteed to ﬁnd an MEU strategy for the inﬂuence diagram. In particular, consider an inﬂuence diagram $\mathcal{T}$ whose relevance graph is acyclic, and let $D_{1},\ldots,D_{k}$ be a topological ordering of the decision variables according to the relevance graph. We now simply execute the algorithm of section 23.5.2 in the order $D_{1},\ldots,D_{k}$ . 

Why does this algorithm guarantee global optimality of the inferred strategy? When selecting the decision rule for $D_{i}$ , we have two cases: for $j<i$ , by induction, the decision rules for $D_{j}$ 

![](images/8363afdc80880e792ba2131436de966c5028a118683ad942a0698bc4e9ce74b8.jpg) 

are already stable, and so will never need to change; for $j>i$ , the decision rules for $D_{j}$ are irrelevant, so that changing them will not require revisiting $D_{i}$ . 

One subtlety with this argument relates, once again, to the issue of probability-zero events. If our arbitrary starting strategy $\sigma$ assigns probability zero to a cert decision $d\in\mathit{V a l}(D)$ (in some setting), then the local optimization of another decision rule $D^{\prime}$ might end up selecting a suboptimal decision for the zero probability cases. If subsequently, when optimizing the decision rule for $D$ , we ascribe nonzero probability to $D=d$ , our overall strategy will not be optimal. To avoid this problem, we can use as our starting point any fully mixed strategy $\sigma$ . One obvious choice is simply the strategy that, at each decision $D$ and for each assignment to $\mathrm{Pa}_{D}$ , selects uniformly at random between all of the possible values of $D$ . The overall algorithm is shown in algorithm 23.3. 

Theorem 23.5 Applying Iterated-Optimization-for-ID on an inﬂuence diagram $\mathcal{T}$ whose relevance graph is acyclic, returns a globally optimal strategy for I . 

The proof is not difcult and is left as an exercise (exercise 23.8). Thus, this algorithm, by iteratively optimizing individual decision rules, ﬁnds a globally optimal solution. The algorithm applies to any inﬂuence diagram whose relevance graph is acyclic, and hence to any inﬂuence diagrams satisfying the perfect recall assumption. Hence, it is at least as general as the variable elimination algorithm of section 23.3. However, as we saw, some inﬂuence diagrams that violate the perfect recall assumption have acyclic relevance graphs nonetheless; this algorithm also applies to such cases. 

network, an s-reachability analysis shows that each decision variable $D_{i}$ strategically relies only n $D_{j}$ for $\textit{j}>\textit{i}$ . or ex ple, if arent $\widehat{D_{1}}$ to $D_{1}$ , we can verify that it is d-separated from $V_{3}$ and V $V_{4}$ given $\mathrm{Pa}_{D_{3}}\,=\,\{H_{2},D_{2}\}$ { } , so that the resulting relevance graph is acyclic. 

The ability to deal with problems where the agent does not have to remember his entire history can provide very large computational savings in large problems. Speciﬁcally, we can solve this inﬂuence diagram using the clique tree of ﬁgure 23.9, at a cost that grows linearly rather than exponentially in the number of decision variables. 

This algorithm is guaranteed to ﬁnd a globally optimal solution only in cases where the rele- vance graph is acyclic. However, we can extend this algorithm to ﬁnd a globally optimal solution in more general cases, albeit at some computational cost. In this extension, we simultaneously optimize the rules for subsets of interdependent decision variables. Thus, for example, in ex- ample 23.18, we would optimize the decision rules for $D_{1}$ and $D_{2}$ together, rather than each in isolation. (See exercise 23.9.) This approach is guaranteed to ﬁnd the globally optimal strategy, but it can be computationally expensive, depending on the number of interdependent decisions that must be considered together. 

joint action Box 23.B — Case Study: Coordination Graphs for Robot Soccer. One subclass of problem in decision making is that of making a joint decision for a team of agents with a shared utility function et the world state be deﬁned by a set of variables $X=\{X_{1},\cdot\cdot\cdot,X_{n}\}$ . We now have $^a$ team of m agents, each with a decision variable $A_{i}$ . The team’s utility function is described by $^a$ function $U(\pmb{X},\pmb{A})$ (for $A=\{A_{1},.\,.\,.\,,A_{m}\})$ . Given an assignment $_{_{x}}$ to $X_{1},\dots,X_{n},$ our goal is to ﬁnd the optimal joint action $\begin{array}{r}{\operatorname*{arg\,max}_{a}U({\boldsymbol{x}},{\boldsymbol{a}})}\end{array}$ . 

In a naive encoding, the representation of the utility function grows exponentially both in the number of state variables and in the number of agents. However, we can come up with more efcient algorithms by exploiting the same type of factorization that we have utilized so far. In particular, we assume that we can decompose $U$ as a sum of subutility functions, each of which depends only on the actions of some subset of the agents. More precisely, 

$$
U{\big(}X_{1},\dots,X_{n},A_{1},\dots,A_{m}{\big)}=\sum_{i}V_{i}(X_{i},A_{i}),
$$ 

where $V_{i}$ is some subutility function with scope $X_{i},A_{i}$ . 

coordination graph RoboSoccer 

This optimization problem is simply a max-sum problem over a factored function, a problem that is precisely equivalent to the MAP problem that we addressed in chapter 13. Thus, we can apply any of the algorithms we described there. In particular, max-sum variable elimination can be used to produce optimal joint actions, whereas max-sum belief propagation can be used to construct approximate max-marginals, which we can decode to produce approximate solutions. The application of these message passing algorithms in this type of distributed setting is satisfying, since the decomposition of the utility function translates to a limited set of interactions between agents who need to coordinate their choice of actions. Thus, this approach has been called a coordination graph . 

Kok, Spaan, and Vlassis (2003), in their UvA (Universiteit van Amsterdam) Trilearn team, applied coordination graphs to the RoboSoccer domain, a particularly challenging application of decision making under uncertainty. RoboSoccer is an annual event where teams of real or simulated robotic agents participate in a soccer competition. This application requires rapid decision making under uncertainty and partial observability, along with coordination between the diferent team members. The simulation league allows teams to compete purely on the quality of their software, eliminating the component of hardware design and maintenance. However, key challenges are faithfully simulated in this environment. For example, each agent can sense its environment via only three sensors: a visual sensor, a body sensor, and an aural sensor. The visual sensor measures relative distance, direction, and velocity of the objects in the player’s current ﬁeld of view. Noise is added to the true quantities and is larger for objects that are farther away. The agent has only $^a$ partial view of the world and needs to take viewing actions (such as turning its neck) deliberately in order to view other parts of the ﬁeld. Players in the simulator have diferent abilities; for example, some can be faster than others, but they will also tire more easily. Overall, this tournament provides a challenge for real-time, multiagent decision-making architectures. 

Kok et al. hand-coded a set of utility rules, each of which represents the incremental gain or loss to the team from a particular combination of joint actions. At every time point t they instantiate the variables representing the current state and solve the resulting coordination graph. Note that there is no attempt to address the problem of sequential decision making, where our choice of action at time t should consider its efect on actions at subsequent time points. The myopic nature of the decision making is based on the assumption that the rules summarize the long-term beneﬁt to the team from a particular joint action. 

To apply this framework in this highly dynamic, continuous setting, several adaptations are required. First, to reduce the set of possible actions that need to be considered, each agent is assigned a role: interceptor, passer, receiver, or passive. The assignment of roles is computed directly from the current state information. For example, the fastest player close to the ball will be assigned the passer role when he is able to kick the ball, and the interceptor role otherwise. The assignment of roles deﬁnes the structure of the coordination graph: interceptors, passers, and receivers are connected, whereas passive agents do not need to be considered in the joint-action selection process. The roles also determine the possible actions for each agent, which are discrete, high-level actions such as passing a ball to another agent in a given direction. The state variables are also deﬁned as a high- level abstraction of the continuous game state; for example, there is a variable pass-blocked $(i,j,d)$ that indicates whether a pass from agent $i$ to agent $j$ in direction $d$ is blocked by an opponent. With this symbolic representation, one can write value rules that summarize the value gained by $^a$ particular combination of actions. For example, one rule says: 

$$
e\mathrm{-}r e c e i v e r(j)\wedge\neg i s P a s s B l o c k e d(i,j,d)\wedge A_{i}=p a s s T o(j,d)\wedge A_{j}=m o v e T o(d):V(j)=V a s s
$$ 

where $V(j,d)$ depends on the position where the receiving agent $j$ receive the pass — the closer to the opponent goal the better. 

A representation of a utility function as a set of rules is equivalent to a feature-based represen- tation of a Markov network. To perform the optimization efciently using this representation, we can easily adapt the rule-based variable-elimination scheme described in section 9.6.2.1. Note that the actual rules used in the inference are considerably simpler, since they are conditioned on the state variables, which include the role assignment of the agents and the other aspects of the state (such as isPassBlocked). However, this requirement introduces other complications: because of the limited communication bandwidth, each agent needs to solve the coordination graph on its own. Moreover, the state of the world is not generally fully observed to the agent; thus, one needs to ensure that the agents take the necessary observation actions (such as turning the neck) to obtain enough information to condition the relevant state variables. Depending on the number of agents and their action space, one can now solve this problem using either variable elimination or belief propagation. 

The coordination graph framework allows the diferent agents in the team to conduct complex maneuvers, an agent $j$ would move to receive a pass from agent $i$ even before agent i was in position to kick the ball; by contrast, previous methods required $j$ to observe the trajectory of the ball before being able to act accordingly. This approach greatly increased the capabilities of the UVA Trilearn team. Whereas their entry took fourth place in the RoboSoccer 2002 competition, in 2003 it took ﬁrst place among the forty-six qualifying team, with a total goal count of 177–7. 

# 23.6 Ignoring Irrelevant Information $\star$ 

As we saw, there are several signiﬁcant advantages to reducing the amount of information that the agent considers at each decision. Eliminating an information edge from a variable $W$ into a decision variable $D$ reduces the complexity of its decision rule, and hence the cognitive load on the decision maker. Computationally, it decreases the cost of manipulating its factor and of computing the decision rule. In this section, we consider a procedure for removing information edges from an inﬂuence diagram. 

Of course, removing information edges reduces the agent’s strategy space, and therefore can potentially signiﬁcantly decrease his maximum expected utility value. If we want to preserve the agent’s MEU value, we need to remove information edges with care. We focus here on removing only information edges that do not reduce the agent’s MEU. We therefore study when a variable $W\,\in\,\mathrm{Pa}_{D}$ is irrelevant to making the mal decision at $D$ . In this section, we provide a graphical criterion for guaranteeing that W is irrelevant and can be dropped without penalty from the set $\mathrm{Pa}_{D}$ . 

Intuitively, $W$ is not relevant when it has no efect on utility nodes that participate in determining the decision at $D$ . 

# Example 23.23 

Deﬁnition 23.10 irrelevant information edge Consider the inﬂuence diagram $\mathcal{L}_{S}$ of ﬁgure 23.10. Intuitively, the edge from Difculty $(D)$ to Apply ( A ) is irrelevant. To understand why, consider its efect on the diferent utility variables in the network. On one hand, it inﬂuences $V_{S}$ ; however, given the variable Grade, which is also observed at $A$ , $D$ is irrelevant to $V_{S}$ . On the other hand, it inﬂuences $V_{Q}$ ; however, $V_{Q}$ cannot be inﬂuenced by the decision at $A$ , and hence is not considered by the decision maker when determining the strategy at $A$ . Overall, $D$ is irrelevant to $A$ given A ’s other parents. 

We can make this intuition precise as follows: 

An infor ation edge $W\rightarrow D$ from a (ch ce or decision) variable $W$ is irrelevant for a decision variable D if there is no active trail from W to $\mathcal{U}_{\succ D}$ given $\mathrm{Pa}_{D}-\{W\}$ . 

According to this criterion, $D$ is irrelevant for $A$ , supporting our intuitive argument. We note that certain recall edges can also be irrelevant according to this deﬁnition. For example, assume that we add an edge from Difculty to the decision variable Take . The Difculty $\rightarrow\mathit{T a k e}$ edge 

![](images/31aa02a6cbe94cf4f4bcb5bfec20aee4cadffc837f7619c6aedc322921ade269.jpg) 
Figure 23.10 More complex inﬂuence diagram $\mathcal{L}_{S}$ for the Student scenario. Recall edges that follow from the deﬁnition are omitted for clarity. 

is not irrelevant, but the Difculty $\rightarrow$ Apply edge, which would be implied by perfect recall, is irrelevant. 

We can show that irrelevant edges can be removed without penalty from the network. 

# Proposition 23.4 

reduction 

Theorem 23.6 Let $\mathcal{T}$ be an inﬂuence diagram, and $W\,\rightarrow\,D$ irrelevant edge in $\mathcal{T}$ . Let ${\mathcal{L}}^{\prime}$ e the inﬂuence diagram bta d by removing the edge $W\,\rightarrow\,D$ → . Then for any strategy $\sigma$ in I , there exists $^a$ strategy σ $\sigma^{\prime}$ in I ${\mathcal{L}}^{\prime}$ such that $\mathrm{EU}[\mathcal{Z}^{\prime}[\sigma^{\prime}]]\ge\mathrm{EU}[\mathcal{Z}[\sigma]]$ . 

The proof follows from proposition 23.3 and is left as an exercise (exercise 23.11). An inﬂuenc diagram ${\mathcal{L}}^{\prime}$ that is obtained from $\mathcal{T}$ via the removal of irrelevant edges is called a reduction of I . An immediate consequence of proposition 23.4 is the following result: 

If ${\mathcal{L}}^{\prime}$ is a reduction of $\mathcal{T},$ , then any strategy $\sigma$ that is optimal for ${\mathcal{L}}^{\prime}$ is also optimal for $\mathcal{T}$ . 

The more edges we remove from $\mathcal{T}$ , the simpler our computational problem. We would thus like to ﬁnd a reduction that has the fewest possible edges. One simple method for obtaining a minimal reduction — one that does not admit the removal of any additional edges — is to remove irrelevant edges iteratively from the network one at a time until no further edges can be removed. An obvious question is whether the order in which edges are removed makes a diference to the ﬁnal result. Fortunately, the following result implies otherwise: 

# Theorem 23.7 

This theorem implies that we can examine each edge independently, and test whether it is irrelevant in $\mathcal{T}$ . All such edges can then be removed at once. Thus, we can ﬁnd all irrelevant edges using a single global computation of d-separation on the original ID. 

The removal of irrelevant edges has several important computational beneﬁts. First, it de- creases the size of the strategy representation in the ID. Second, by removing edges in the network, it can reduce the complexity of the variable-elimination-based algorithms described in section 23.5. Finally, as we now show, it also has the efect of removing edges from the relevance graph associated with the ID. By breaking cycles in the relevance graph, it allows more decision rules to be optimized in sequence, reducing the need for iterations or for jointly optimizing the decision rules at multiple variables. 

If ${\mathcal{L}}^{\prime}$ is a reductio of $\mathcal{T},$ , then the relevance graph of ${\mathcal{L}}^{\prime}$ is a subset (not necessarily strict) of the relevance graph of . 

Proof It sufces to show the result f the case where ${\mathcal{L}}^{\prime}$ is a duct n of $\mathcal{T}$ by a single irrelevant edge. We will show that if D $D^{\prime}$ is not s-reachable from D in I , then it is a not s-reachable from $D$ in ${\mathcal{L}}^{\prime}$ $D^{\prime}$ s-reach le from $D$ in ${\mathcal{L}}^{\prime}$ , then a mmy p ${\widehat{D^{\prime}}}$ ′ , we have that there is some $V\in\mathcal{U}_{\succ D}$ ∈U ail in ${\mathcal{L}}^{\prime}$ from $\widehat{D}$ b to V given $D,\mathrm{Pa}_{D}^{\mathcal{T}^{\prime}}$ . By assumption, that same trail is not active in I . Since removal of edges cannot make a il active, this situation can occur only if $\mathrm{Pa}_{D}^{\mathcal{Z}^{\prime}}=\mathrm{Pa}_{D}^{\mathcal{Z}}-\{W\}$ −{ } , an $W$ rail from c to $V$ in $\mathcal{T}$ . Because o rvi $W$ locks the trail, it must be part of the trail, in which case there is a btrail from W to V in I . This subtrail is acti given $(\mathrm{Pa}_{D}^{\mathcal{Z}}-\{W\}),D$ −{ } . However, observing $D$ cannot activate a trail where we condition on D ’s parents (because then v-structures involving $D$ are blocked). Thus, this subtrail must form an active trail from $W$ to $V$ given $\mathrm{Pa}_{D}^{\mathcal{Z}}-\{W\}$ −{ } , violating the assumption that $W\to D$ is an irrelevant edge. 

# 23.7 Value of Information 

So far, we have focused on the problem of decision making. Inﬂuence diagrams provide us with a representation for structured decision problems, and a basis for efcient decision-making algorithms. 

One particularly useful type of task, which arises in a broad range of applications, is that of determining which variables we want to observe. Most obviously, in any diagnostic task, we usually have a choice of diferent tests we can perform. Because tests usually come at a cost (whether monetary or otherwise), we want to select the tests that are most useful in our particular setting. For example, in a medical setting, a diagnostic test such as a biopsy may involve signiﬁcant pain to the patient and risk of serious injury, as well as high monetary costs. In other settings, we may be interested in determining if and where it is worthwhile to place sensors — such as a thermostat or a smoke alarm — so as to provide the most useful information in case of a ﬁre. 

The decision-theoretic framework provides us with a simple and elegant measure for the value of making a particular observation. Moreover, the inﬂuence diagram represen- tation allows us to formulate this measure using a simple, graph-based criterion, which also provides considerable intuition. 

# 23.7.1 Single Observations 

We begin with the question of evaluating the beneﬁt of a single observation. In the setting of inﬂuence diagrams, we can model this question as one of computing the value of observing the value of some variable. Our Survey variable in the Entrepreneur example is precisely such a situation. Although we could (and did) analyze this type of decision using our general framework, it is useful to consider such decisions as a separate (and simpler) class. By doing so, we can gain insight into questions such as these. 

The key idea is that the beneﬁt of making an observation is the utility the agent can gain by observing the associated variable, assuming he acts optimally in both settings. 

# Example 23.24 

# Deﬁnition 23.11 

value of perfect information 

Let us revisit the Entrepreneur example, and consider the value to the entrepreneur of conducting the survey, that is, of observing the value of the Survey variable. In efect, we are comparing two scenarios and the utility to the entrepreneur in each of them: One where he conducts the survey, and one where he does not. If the agent does not observe the $S$ variable, that node is barren in the network, and it can therefore be simply eliminated. This would result precisely in the inﬂuence diagram of ﬁgure 23.2. In example 22.3 we analyzed the agent’s optimal action in this setting and showed that his MEU is 2 . The second case is one in which the agent conducts the survey. This situation is equivalent to the inﬂuence diagram of ﬁgure 23.3, where we restrict to strategies where $C\,=\,c^{1}$ . As we have already discussed, $C\,=\,c^{1}$ is the optimal strategy in this setting, so that the optimal utility obtainable by the agent in this situation is 3 . 22 , as computed in example 23.6. Hence, the improvement in the entrepreneur’s utility, assuming he acts optimally in both cases, is 1 . 22 . 

More generally, we deﬁne: 

Let $\mathcal{T}$ be an inﬂuence di ram $X$ a c nce variable, and $D$ a decision variable such that there is no (caus ) pa from D to X . Let I ${\mathcal{L}}^{\prime}$ be the sa e as I , except that we add an information edge from X to D , and to all decisions that follow D (that is, we have perfect information about $X$ from $D$ onwards). The valu f perfect informa on for $X$ at $D$ , denoted $\mathrm{VPI}_{\mathcal{Z}}(D\mid X)$ , is the diference between the MEU of I ${\mathcal{L}}^{\prime}$ and the MEU of I . 

Let us analyze the concept of value of perfect information. First, it is not difcult to see that it cannot be negative; if the information is free, it cannot hurt to have it. 

# Proposition 23.6 

# 

Let $\mathcal{T}$ be an inﬂu ce di am, $D$ a decision variable n $\mathcal{T}$ , a $X$ able that is $^a$ nondescendant of D Let $\sigma^{*}$ be the optimal strategy in I . Then $\begin{array}{r}{\mathrm{VPI}_{\mathcal{Z}}(D\mid X)\ge0,}\end{array}$ ≥ , and e ality I holds if and only if σ $\sigma^{*}$ is still optimal in the new inﬂuence diagram with X as a parent of D . The proof is left as an exercise (exercise 23.13). Does information always help? What if the numbers had been such that the entrepreneur would have founded the company regardless of the survey? In that case, the expected utility with the survey and without it would have been identical; that is, the VPI of $S$ would have been zero. This property is an important one: there is no value to information if it does not change the selected action(s) in the optimal strategy. 

Let us analyze more generally when information helps. To do that, consider a diferent decision problem. 

![](images/5245e1e4144c036c58dfe21a624cf912881699c75b18b05c044599b50899189e.jpg) 
Figure 23.11 Inﬂuence diagram for VPI computation in example 23.25. We can compute the value of information for each of the two State variables by comparing the value with/without the dashed information edges. 

Our budding entrepreneur has decided that founding a startup is not for him. He is now choosing between two job opportunities at existing companies. Both positions are ofering a similar starting salary, so his utility depends on his salary a year down the road, which depends on whether the company is still doing well at that point. The agent has the option of obtaining some information about the current state of the companies. 

More formally, the entrepreneur has a decision variable $C$ , whose value $c_{i}$ is accepting a job with company $i$ $(i=1,2)$ . For each company, we have a variable $S_{i}$ that represents the current state of the company (quality of the management, the engineering team, and so on); this value takes three values, with $s_{i}^{3}$ being a very high-quality company and $s_{i}^{1}$ a poor company. We also have a binary-valued variable $F_{i}$ , which represents the funding status of the company in the future, with $f_{i}^{1}$ representing the state of having funding. We assume that the utility of the agent is 1 if he takes a job with a company for which $F_{i}=f_{i}^{1}$ and 0 otherwise. We want to evaluate the value of information of observing $S_{1}$ (the case of observing $S_{2}$ is essentially the same). The structure of the inﬂuence diagram is shown in ﬁgure 23.11; the edges that would be added to compute the value of information are shown as dashed. 

We now consider three diferent scenarios and compute the value of information in each of them. Scenario 1: Company 1 is well established, whereas Company 2 is a small startup. Thus, $P(S_{1})=$ (0 . 1 , 0 . 2 , 0 . 7) (that is, $P(s_{1}^{1})\;=\;0.1)$ ), and $P(S_{2})\;=\;(0.4,0.5,0.1)$ . The economic climate is poor, so the chances of getting funding are not great. Thus, for both companies, $P(f_{i}^{1}\mid S_{i})\,=$ | (0 . 1 , 0 . 4 , 0 . 9) (that is, $P(f_{i}^{1}\mid s_{1}^{1})=0.1)$ | ). Without additional information, the optimal strategy is $c_{1}$ , with MEU value 0 . 72 . Intuitively, in this case, the information obtained by observing $S_{1}$ does not have high value. Although it is possible that $c_{1}$ will prove less reliable than $c_{2}$ , this outcome is very unlikely; with very high probability, $c_{1}$ will turn out to be the better choice even with the information. Thus, the probability that the information changes our decision is low, and the value of the information is also low. More formally, a simple calculation shows that the optimal strategy changes to $c_{2}$ only if we observe $s_{1}^{1}$ , which happens with probability 0 . 1 . The MEU value in this scenario is 0 . 743 , which is not a signiﬁcant improvement over our original MEU value. If observing $S_{1}$ costs more than 0 . 023 utility points, the agent should not make the observation. 

Scenario 2: The economic climate is still bad, but now $c_{1}$ and $c_{2}$ are both small startups. In this case, we might have $P(S_{2})$ as in Scenario 1, and $P(S_{1})=(0.3,0.4,0.3);P(F_{i}\mid S_{i})$ is also as in Scenario 1. Intuitively, our value of information in this case is quite high. There is a reasonably high probability that the observation will change our decision, and therefore a high probability that we would gain a lot of utility by ﬁnding out more information and making a better decision. Indeed, if we go through the calculation, the MEU strategy in the case without the additional observation is $c_{1}$ , and the MEU value is 0 . 546 . However, with the observation, we change our decision to $c_{2}$ both when $S_{1}=s_{1}^{1}$ and when $S_{1}=s_{1}^{2}$ , events that are fairly probable. The MEU value in this case is 0 . 6882 , a signiﬁcant increase over the uninformed MEU. 

Scenario 3: In this case, $c_{1}$ and $c_{2}$ are still both small startups, but the time is the middle of the Internet boom, so both companies are likely to be funded by investors desperate to get into this area. Formally, $P(S_{1})$ and $P(S_{2})$ are as above, but $P(f_{i}^{1}\mid S_{i})=(0.6,0.8,0.99)$ | . In this case, the probability that the observation changes the agent’s decision is reasonably high, but the change to the agent’s expected utility when the decision changes is low. Speciﬁcally, the uninformed optimal strategy is $c_{1}$ , with MEU value 0 . 816 . Observing $s_{1}^{1}$ changes the decision to $c_{2}$ ; but, while this observation occurs with probability 0 . 3 , the diference in the expected utility between the two decisions in this case is less than 0 . 2 . Overall, the MEU of the informed case is 0 . 8751 , which is not much greater than the uninformed MEU value. 

Overall, we see that our deﬁnition of VPI allows us to make fairly subtle trade-ofs. 

The value of information is critical in many applications. For example, in medical or fault diagnosis, it often serves to tell us which diagnostic tests to perform (see box 23.C). Note that its behavior is exactly appropriate in this case. We do not want to perform a test just because it will help us narrow down the probability of the problem. We want to perform tests that will change our diagnosis. For example, if we have an invasive, painful test that will tell us which type of ﬂu a patient has, but knowing that does not change our treatment plan (lie in bed and drink a lot of ﬂuids), there is no point in performing the test. 

# 23.7.2 Multiple Observations 

We now turn to the more complex setting where we can make multiple simultaneous observa- tions. In this case, we must decide which subset of the $m$ potentially observable variables we choose to observe. For each such subset, we can evaluate the MEU value with the observations, as in the single variable case, and select the subset whose MEU value is highest. However, this approach is overly simplistic in several ways. First, the number of possible subsets of observa- tions is exponentially large $(2^{m})$ . A doctor, for example, might have available a large number of tests that she can perform, so the number of possible subsets of tests that she might select is huge. Even if we place a bound on the number of observations that can be performed or on the total cost of these observations, the number of possibilities can be very large. 

More importantly, in practice, we often do not select in advance a set of observations to be performed, and then perform all of them at once. Rather, observations are typically made in sequence, so that the choice of which variable to observe next can be made with knowledge about the outcome of the previous observations. In general, the value of an observation can depend strongly on the outcome of a previous one. For example, in example 23.25, if we observe that the current state of Company 1 is excellent — $S_{1}=s_{1}^{3}$ , observing the state of Company 2 is signiﬁcantly less useful than in a situation where we observe that $S_{1}=s_{1}^{2}$ . Thus, the optimal choice of variable to observe generally depends on the outcomes of the previous observations. 

Therefore, when we have the ability to select a sequence of observations, the optimal selection has the form of a conditional plan : Start by observing $X_{1}$ ; if we observe $X_{1}=x_{1}^{1}$ , observe $X_{2}$ ; if we observe $X_{1}=x_{1}^{2}$ , observe $X_{3}$ ; and so on. Each such plan is exponentially large in the number $k$ of possible observations that we are allowed to perform. The total number of such plans is therefore doubly exponential. Selecting an optimal observation plan is computationally a very difcult task, for which no good algorithms exist in general. 

myopic value of information 

The most common solution to this problem is to approximate the solution using myopic value of information , where we incrementally select at each stage the optimal single observation, ignoring its efect on the later choices that we will have to make. The optimal single observation can be selected easily using the methods described in the previous section. This myopic approximation can be highly suboptimal. For example, we might have an observation that, by itself, provides very little information useful for our decision, but does tell us which of two other observations is the most useful one to make. 

In situations where the myopic approximation is complex, we can try to generate a condi- tional plan, as described before. One approach for solving such a problem is to formulate it as an inﬂuence diagram, with explicit decisions for which variable to observe. This type of transformation is essentially the one underlying the very simple case of example 23.3, where the variable $C$ represents our decision on whether to observe $S$ or not. The optimal strategy for this extended inﬂuence diagram also speciﬁes the optimal observation plan. However, the resulting inﬂuence diagram can be quite complex, and ﬁnding an optimal strategy for it can be very expensive and often infeasible. 

Box 23.C — Case Study: Decision Making for Troubleshooting. One of the most commonly used applications of Bayesian network technology is to the task of fault diagnosis and repair. Here, we construct a probabilistic model of the device in question, where random variables correspond to diferent faults and diferent types of observations about the device state. Actions in this type of domain correspond both to diagnostic tests that can help indicate where the problem lies, and to actions that repair or replace a broken component. Both types of actions have a cost. One can now apply decision-theoretic techniques to help select a sequence of observation and repair actions. 

counterfactual twinned network 

One of the earliest and largest ﬁelded applications of this type was the decision-theoretic trou- bleshooting system incorporated into the Microsoft’s Windows $95^{T M}$ operating system. The system, described in Heckerman, Breese, and Rommelse (1995) and Breese and Heckerman (1996), included hundreds of Bayesian networks, each aimed at troubleshooting a type of fault that commonly arises in the system (for example, a failure in printing, or an application that does not launch). Each fault had its own Bayesian network model, ranging in size from a few dozen to a few hundred variables. To compute the probabilities required for an analysis involving repair actions, which intervene in the model, one must take into account the fact that the system state from before the repair also persists afterward (except for the component that was repaired). For this computation, a counterfactual twinned network model was used, as described in box 21.C. 

The probabilistic models were augmented with utility models for observing the state of a com- ponent in the system (that is, whether it is faulty) and for replacing it. Under carefully crafted assumptions (such as a single fault hypothesis), it was possible to deﬁne an optimal series of re- pair/observation actions, given a current state of information $e$ , and thereby compute an exact formula for the expected cost of repair $E C R(e)$ . (See exercise 23.15.) This formula could then be used to compute exactly the beneﬁt of any diagnostic test $D$ , using a standard value of information computation: 

$$
\sum_{d\in V a l(D)}P(D=d\mid e)E C R(e,D=d).
$$ 

One can then add the cost of the observation of $D$ to choose the optimal diagnostic test. Note that the computation of $E C R(e,D=d)$ estimates the cost of the full trajectory of repair actions following the observation, a trajectory that is generally diferent for diferent values of the observation $d$ . Thus, although this analysis is still myopic in considering only a single observation action $D$ at a time, it is nonmyopic in evaluating the cost of the plan of action following the observation. 

Empirical results showed that this technique was very valuable. One test, for example, was applied to the printer diagnosis network of box 5.A. Here, the cost was measured in terms of minutes to repair. In synthetic cases with known failures, sampled from the network, the system saved about 20 percent of the time over the best predetermined plan. Interestingly, the system also performed well, providing equal or even better savings, in cases where there were multiple faults, violating the assumptions of the model. 

At a higher level, decision-theoretic techniques are particularly valuable in this setting for several reasons. The standard system used up to that point was a standard static ﬂowchart where the answer to a question would lead to diferent places in the ﬂowchart. From the user side, the experience was signiﬁcantly improved in the decision-theoretic system, since there was considerably greater ﬂexibility: diagnostic tests are simply treated as observed variables in the network, so if a user chooses not to answer a question at a particular point, the system can still proceed with other questions or tests. Users also felt that the questions they were asked were intuitive and made sense in context. Finally, there was also signiﬁcant beneﬁt for the designer of the system, because the decision-theoretic system allowed modular and easily adaptable design. For example, if the system design changes slightly, the changes to the corresponding probabilistic models are usually small (a few CPDs may change, or maybe some variables are added/deleted); but the changes to the “optimal” ﬂowchart are generally quite drastic. Thus, from a software engineering perspective, this approach was also very beneﬁcial. 

This application is one of the best-known examples of decision-theoretic troubleshooting, but similar techniques have been successfully used in a large number of applications, including in a decision-support system for car repair shops, in tools for printer and copier repair, and many others. 

# 23.8 Summary 

In this chapter, we placed the task of decision making using decision-theoretic principles within the graphical modeling framework that underlies this entire book. Whereas a purely proba- bilistic graphical model provides a factorized description of the probability distribution over possible states of the world, an inﬂuence diagram provides such a factorized representation for the agent’s actions and utility function as well. The inﬂuence diagram clearly encodes the breakdown of these three components of the decision-making situation into variables, as well as the interactions between these variables. These interactions are both probabilistic, where one variable afects the distribution of another, and informational, where observing a variable allows an agent to actively change his action (or his decision rule). 

We showed that dynamic programming algorithms, similar to the ones used for pure proba- bilistic inference, can be used to ﬁnd an optimal strategy for the agent in an inﬂuence diagram. However, as we saw, inference in an inﬂuence diagram is more complex than in a Bayesian net- work, both conceptually and computationally. This complexity is due to the interactions between the diferent operations involved: products for deﬁning the probability distribution; summation for aggregating utility variables; and maximization for determining the agent’s optimal actions. 

The inﬂuence diagram representation provides a compact encoding of rich and complex decision problems involving multiple interrelated factors. It provides an elegant framework for considering such important issues as which observations are required to make optimal decisions, the deﬁnition of recall and the value of the perfect recall assumption, the dependence of a particular decision on particular observations or components of the agent’s utility function, and the like. Value of information — a concept that plays a key role in many practical applications — is particularly easy to capture in the inﬂuence diagram framework. 

However, there are several factors that can cause the complexity of the inﬂuence diagram to grow unreasonably large, and signiﬁcantly reduce its usability in many real-world settings. One such limitation is the perfect recall assumption, which can lead the decision rules to grow exponentially large in the number of actions and observations the agent makes. We note that this limitation is not one of the representation, but rather of the requirements imposed by the notion of optimality and by the algorithms we use to ﬁnd solutions. A second source of blowup arises when the scenario that arises following one decision by the agent is very diferent from the scenario following another. For example, imagine that the agent has to decide whether to go from San Francisco to Los Angeles by air or by car. The subsequent decisions he has to make and the variables he may observe in these two cases are likely to be very diferent. This example is an instance of context-speciﬁcity, as described in section 5.2.2; however, the simple solution of modifying our CPD structure to account for context-speciﬁcity is usually insufcient to capture compactly these very broad changes in the model structure. The decision-tree structure is better able to capture this type of structure, but it too has its limitations; several works have tried to combine the beneﬁts of both representations (see section 23.9). 

Finally, the basic formalism for sequential decision making under uncertainty is only a ﬁrst step toward a more general formalism for planning and acting under uncertainty in many settings: single-agent, multiagent distributed decision making, and multiagent strategic (game- theoretic) interactions. A complete discussion of the ideas and methods in any of these areas is a book in itself; we encourage the reader who is interested in these topics to pursue some additional readings, some of which are mentioned in section 23.9. 

# 23.9 Relevant Literature 

The inﬂuence diagram representation was introduced by Howard and Matheson (1984a), albeit more as a guide to formulating a decision problem than as a formal language with well-deﬁned semantics. See also Oliver and Smith (1990) for an overview. 

Olmsted (1983) and Shachter (1986, 1988) provided the ﬁrst algorithm for decision making in inﬂuence diagrams, using local network transformations such as edge reversal. This algorithm was gradually improved and reﬁned over the years in a series of papers (Tatman and Shachter 1990; Shenoy 1992; Shachter and Ndilikilikesha 1993; Ndilikilikesha 1994). The most recent algo- rithm of this type is due to Jensen, Jensen, and Dittmer (1994); their algorithm utilizes the clique tree data structure for addressing this task. All of these solutions use a constrained elimination ordering, and they are therefore generally feasible only for fairly small inﬂuence diagrams. 

A somewhat diferent approach is based on reducing the problem of solving an inﬂuence diagram to inference in a standard Bayesian network. The ﬁrst algorithm along these lines is due to Cooper (1988), whose approach applied only to a single decision variable. This idea was subsequently extended and improved considerably by Shachter and Peot (1992) and Zhang (1998). 

Nilsson and Lauritzen (2000) and Lauritzen and Nilsson (2001) provide an algorithm based on the concept of limited memory inﬂuence diagrams, which relaxes the perfect recall assumption made in almost all previous work. This relaxation allows them to avoid the constraints on the elimination ordering, and thereby leads to a much more efcient clique tree algorithm. Similar ideas were also developed independently by Koller and Milch (2001). The clique-tree approach was further improved by Madsen and Nilsson (2001). 

The simple inﬂuence diagram framework poses many restrictions on the type of decision- making situation that can be expressed naturally. Key restrictions include the perfect recall assumption (also called “no forgetting”), and the assumption of the uniformity of the paths that traverse the inﬂuence diagram. 

Regarding this second point, a key limitation of the basic inﬂuence diagram representation is that it is designed for encoding situations where all trajectories through the system go through the same set of decisions in the same ﬁxed order. Several authors (Qi et al. 1994; Covaliu and Oliver 1995; Smith et al. 1993; Shenoy 2000; Nielsen and Jensen 2000) propose extensions that deal with asymmetric decision settings, where a choice taken at one decision variable can lead to diferent decision being encountered later on. Some approaches (Smith et al. 1993; Shenoy 2000) use an approach based on context-speciﬁc independence, along the lines of the tree-CPDs of section 5.3. These approaches are restricted to cases where the sequence of observations and decisions is ﬁxed in all trajectories of the system. The approach of Nielsen and Jensen (1999, 2000) circumvents this limitation, allowing for a partial ordering over observations and decisions. The partial ordering allows them to reduce the set of constraints on the elimination ordering in a variable elimination algorithm, resulting in computational savings. This approach was later extended by Jensen and Vomlelová (2003). 

In a somewhat related trajectory, Shachter (1998, 1999) notes that some parents of a decision node may be irrelevant for constructing the optimal decision rule, and provided a graphical procedure, based on his BayesBall algorithm, for identifying such irrelevant chance nodes. The LIMID framework of Nilsson and Lauritzen (2000); Lauritzen and Nilsson (2001) makes these notions more explicit by speciﬁcally encoding in the inﬂuence diagram representation the subset of potentially observable variables relevant to each decision. This allows a relaxation of the ordering constraints induced by the perfect recall assumption. They also deﬁne a graphical procedure for identifying which decision rules depend on which others. This approach forms the basis for the recursive algorithm presented in this chapter, and for its efcient implementation using clique trees. 

The concept of value of information was ﬁrst deﬁned by Howard (1966). Over the years, various algorithms (Zhang et al. 1993; Chávez and Henrion 1994; Ezawa 1994) have been proposed for performing value of information computations efciently in an inﬂuence diagram, culminating in the work of Dittmer and Jensen (1997) and Shachter (1999). All of these papers focus on the myopic case and provide an algorithm for computing the value of information only for all single variables in this network (allowing the decision maker to decide which one is best to observe). Recent work of Krause and Guestrin (2005a,b) addresses the nonmyopic problem of selecting an entire sequence of observations to make, within the context of a particular class of utility functions. 

There have been several ﬁelded systems that use the decision-theoretic approach described in this chapter, although many use a Bayesian network and a simple utility function rather than a full-ﬂedged inﬂuence diagram. Examples of this latter type include the Pathﬁnder system of Heckerman (1990); Heckerman et al. (1992), and Microsoft’s system for decision-theoretic troubleshooting (Heckerman et al. 1995; Breese and Heckerman 1996) that was described in box 23.C. The Vista system of Horvitz and Barry (1995) used an inﬂuence diagram to make decisions on display of information at NASA Mission Control Center. Norman et al. (1998) present an inﬂuence-diagram system for prenatal testing, as described in box 23.A. Meyer et al. (2004) present a ﬁelded application of an inﬂuence diagram for selecting radiation therapy plans for prostate cancer. 

A framework closely related to inﬂuence diagrams is that of Markov decision processes (MDPs) and its extension to the partially observable case (partially observable Markov decision processes, or POMDPs). The formal foundations for this framework were set forth by Bellman (1957); Bertsekas and Tsitsiklis (1996) and Puterman (1994) provide an excellent modern introduction to this topic. Although both an MDP and an inﬂuence diagram encode decision problems, the focus of inﬂuence diagrams has been on richly spaces that involve rich structure in terms of the state description (and sometimes the utility function), but only a few decisions; conversely, much of the focus of MDPs has been on state spaces that are fairly unstructured (encoded simply as a set of states), but on complex decision settings with long (often inﬁnite) sequences of decisions. 

Several groups have worked on the synthesis of these two ﬁelds, tackling the problem of sequential decision making in large, richly structured state spaces. Boutilier et al. (1989, 2000) were the ﬁrst to explore this extension; they used a DBN representation of the MDP, and relied on the use of context-speciﬁc structure both in the system dynamics (tree-CPDs) and in the form of the value function. Boutilier, Dean, and Hanks (1999) provide a comprehensive survey of the representational issues and of some of the earlier algorithms in this area. Koller and Parr (1999); Guestrin et al. (2003) were the ﬁrst to propose the use of factored value functions, which decompose additively as a sum of subutility functions with small scope. Building on the rule-based variable elimination approach described in section 9.6.2.1, they also show how to make use of both context-speciﬁc structure and factorization. 

Another interesting extension that we did not discuss is the problem of decision making in multiagent systems. At a high level, one can consider two diferent types of multiagent systems: ones where the agents share a utility function, and need to cooperate in a decentralized setting with limited communication; and ones where diferent agents have diferent utility functions, and must optimize their own utility while accounting for the other agents’ actions. Guestrin et al. (2003) present some results for the cooperative case, and introduce the notion of coordination graph; they focus on issues that arise within the context of MDPs, but some of their ideas can also be applied to inﬂuence diagrams. The coordination graph structure was the basis for the RoboSoccer application of Kok et al. (2003); Kok and Vlassis (2005), described in box 23.B. 

The problem of optimal decision making in the presence of strategic interactions is the focus of most of the work in the ﬁeld of game theory . In this setting, the notion of a “rational strategy” is somewhat more murky, since what is optimal for one player depends on the actions taken by others. Fudenberg and Tirole (1991) and Osborne and Rubinstein (1994) provide a good introduction to the ﬁeld of game theory, and to the standard solution concepts used. Generally, work in game theory has represented multiagent interactions in a highly unstructured way: either in the normal form , which lists a large matrix indexed by all possible strategies of all agents, or in the extensive form — a game tree, a multiplayer version of a decision tree. More recently, there have been several proposals for game representations that build on ideas in graphical models. These proposals include graphical games (Kearns et al. 2001), multiagent inﬂuence diagrams (Koller and Milch 2003), and game networks (La Mura 2000). Subsequent work (Vickrey and Koller 2002; Blum et al. 2006) has shown that ideas similar to those used for inference in graphical models and inﬂuence diagrams can be used to provide efcient algorithms for ﬁnding Nash equilibria (or approximate Nash equilibria) in these structured game representations. 

# 23.10 Exercises 

# Exercise 23.1 

Show that the decision rule $\delta_{D}$ that maximizes: $\begin{array}{r}{\sum_{D,\mathrm{Pa}_{D}}\delta_{D}\mu_{-D}\bigl(D,\mathrm{Pa}_{D}\bigr)}\end{array}$ P is deﬁned as: $\delta_{D}(\pmb{w})=\arg\operatorname*{max}_{d\in V a l(D)}\mu_{-D}(d,\pmb{w})\quad\mathrm{~for~all~}\pmb{w}\in V a l(\mathrm{Pa}_{D}).$ 

Exercise 23.2 

Prove proposition 23.1. In particular: 

a. Sho $\gamma^{*}$ deﬁned as in equation (23.8), we have that $\begin{array}{r}{\phi^{*}=\prod_{W\in\mathcal{X}\cup\mathcal{D}}\phi_{W},\,\mu^{*}=\prod_{V\in\mathcal{U}}\mu_{V}}\end{array}$ Q Q . ∈X∪D b. For $W^{\prime}\subset W$ ⊂ , show that $\operatorname{cont}(m a r g_{W^{\prime}}(\gamma))=\sum_{W-W^{\prime}}\operatorname{cont}(\gamma),$ that is, that contraction and marginalization interchange appropriately. c. Use your previous results to prove proposition 23.1. 

# Exercise $23.3\star$ 

Prove theorem 23.1 by showing that the combination and marginalization operations deﬁned in equa- tion (23.5) and equation (23.6) satisfy the axioms of exercise 9.19: 

a. Commutativity and associativity of combination: 

$$
\begin{array}{r c l}{{\gamma_{1}\bigoplus\gamma_{2}}}&{{=}}&{{\gamma_{2}\bigoplus\gamma_{1}}}\\ {{\gamma_{1}\bigoplus(\gamma_{2}\bigoplus\gamma_{3})}}&{{=}}&{{(\gamma_{1}\bigoplus\gamma_{2})\bigoplus\gamma_{3}.}}\end{array}
$$ 

b. Consonance of marginalization: Let $\gamma$ be a factor over scope $W$ and let $W_{2}\subseteq W_{1}\subseteq W$ . Then: $m a r g_{W_{2}}(m a r g_{W_{1}}(\gamma))=m a r g_{W_{2}}(\gamma).$ 

c. Interchanging marginalization and combination: Let $\gamma_{1}$ and $\gamma_{2}$ be potentials over $W_{1}$ and $W_{2}$ respectively. Then: 

$$
m a r g_{W_{1}}((\gamma_{1}\bigoplus\gamma_{2}))=\gamma_{1}\bigoplus m a r g_{W_{1}}(\gamma_{2}).
$$ 

# Exercise $23.4\star$ 

Prove lemma 23.1. 

# Exercise $23.5\star\star$ 

Extend the variable elimination algorithm of section 23.3 to the case of multiple utility variables, using the mechanism of joint factors used in section 23.4.3. (Hint: Deﬁne an operation of max-marginalization, as required for optimizing a decision variable, for a joint factor.) 

# Exercise $\mathbf{23.6\star}$ 

Prove proposition 23.3. (Hint: The proof is based on algebraic manipulation of the expected utility $\mathrm{EU}[\mathcal{Z}[\bar{(\sigma_{-D}},\delta_{D})]]$ .) 

# Exercise 23.7 

Prove theorem 23.4, as follows: 

a. Show at if $D_{i}$ and $D_{j}$ are two decisions such that $D_{i},\mathrm{Pa}_{D_{i}}\subseteq\mathrm{Pa}_{D_{j}}$ , then $D_{i}$ is not s-reachable from $D_{j}$ . b. Use this result to conclude the theorem. c. Show that the nodes in the relevance graph in this case will be totally ordered, in the opposite order to the temporal ordering $\prec$ over the decisions in the inﬂuence diagram. 

# Exercise ${\bf23.8\star}$ 

In this exercise, you will prove theorem 23.5, using two steps. 

a. We ﬁrst need to prove a result analogous to theorem 23.2, but showing that a decision rule $\delta_{D}$ remains optimal even if the decision rules at several decisions $D^{\prime}$ change. Let $\sigma$ be a fully mixed strategy, and $\delta_{D}$ a decision rule for $D$ that is locally optimal for $\sigma$ . Let $\sigma^{\prime}$ be other strategy such at, whenever $\sigma^{\prime}(D^{\prime})\neq\sigma(D^{\prime})$ , then $D^{\prime}$ is not s-reachable from $D$ . Prove that $\delta_{D}$ is also optimal for σ $\sigma^{\prime}$ . b. Now, let $\sigma^{k}$ be the strategy returned by Iterated-Optimization-for-IDs , and $\sigma^{\prime}$ be some other strategy for the agent. Let $D_{1},\ldots,D_{k}$ be the ordering on decisions used by the algorithm. Show that $\mathrm{EU}[\mathcal{Z}[\sigma^{n}]]\ge\mathrm{EU}[\mathcal{Z}[\sigma^{\prime}]]$ . (Hint: Use induction on the number of variables $l$ at which $\sigma^{k}$ and $\sigma^{\prime}$ difer.) 

# Exercise $\mathbf{23.9\star\star}$ 

Extend the algorithm of algorithm 23.3 to ﬁnd a globally optimal solution even in inﬂuence diagrams with cyclic relevance graphs. Your algorithm will have to optimize several decision rules simultaneously, but it should not always optimize all decision rules simultaneously. Explain precisely how you jointly optimize multiple decision rules, and how you select the order in which decision rules are optimized. 

# Exercise $23.10\star\star$ 

In this exercise, we will deﬁne an efcient clique tree implementation of the algorithm of algorithm 23.3. 

a. Describe a clique tree algorithm for a setting where cliques and sepsets are each parameterized with a joint (probability, utility) potential, as described in section 23.4.3. Deﬁne: (i) the clique tree initialization in terms of the network parameter iz ation and a complete strategy $\sigma$ , and (ii) the message passing operations. 

b. Show how we can use the clique-tree data structure to reuse computation between diferent steps of the iterated optimization algorithm. In particular, show how we can easily retract the current decision rule $\delta_{D}$ from the calibrated clique tree, compute a new optimal decision rule for $D$ , and then update the clique tree accordingly. (Hint: Use the ideas of section 10.3.3.1.) 

# Exercise $23.11\star$ 

Prove proposition 23.4. 

# Exercise 23.12 

Prove theore 23.7: Let $\mathcal{T}$ be an inﬂuence diagram and ${\mathcal{T}}^{\prime}$ be any reduction of it, and let $W\,\rightarrow\,D$ be some arc in ′ . 

a. (easy) Prove that if $W\to D$ is irrelevant in $\mathcal{T}$ then it is also irrelevant in ${\mathcal{L}}^{\prime}$ . b. (hard) Prove that if $W\to D$ → is irrelevant in I ${\mathcal{L}}^{\prime}$ , then it is also irrelevant in I . 

# Exercise 23.13 

a. Prove proposition 23.6. 

b. Is the value of learning the values of two variables equal to the sum of the values of learning each of them? That is to say, is $\operatorname{VPI}(\mathcal{Z},D,\{X,Y\})=\operatorname{VPI}(\mathcal{Z},D,X)+\operatorname{VPI}(\mathcal{Z},D,Y)?$ 

# Exercise $23.14\star$ 

Consider an inﬂuence diagram $\mathcal{L}$ , and assu that we have compu the o imal strategy for $\mathcal{T}$ using the clique tree algorithm of section 23.5.2. Let D be some decision in D , and X some variable not observed at $D$ in $\mathcal{T}$ . Show how we can efciently compute $\mathrm{VPI}_{\mathcal{Z}}(D\mid X)$ , using the results of our original clique tree computation, when: 

a. $D$ is the only decision variable in $\mathcal{T}$ . b. The inﬂuence diagram contains additional decision variables, but the relevance graph is acyclic. 

# Exercise $23.15\star$ 

Consider a setting where we have a faulty device. Assume that the failure can be caused by a failure in one of $n$ components, exactly one of which is faulty. The probability that repairing component $c_{i}$ will repair the device $p_{i}$ . By the single-fault hypo sis, we have that $\textstyle\sum_{i=1}^{n}p_{i}\;{\widehat{=}}\;1$ . Further assume that each component c can be examined with cost $C_{i}^{o}$ and then repaired (if faulty) with cost $C_{i}^{r}$ . Finally, assume that the costs of observing and repairing any component do not depend on any previous actions taken. 

a. Show that if we observe and repair components in the order $c_{1},\ldots,c_{n}$ , then the expected cost until the device is repaired is: 

$$
\sum_{i=1}^{n}\left[\left(1-\sum_{j=1}^{i-1}p_{j}\right)C_{i}^{o}+p_{i}C_{i}^{r}\right].
$$ 

b. Use that to show that the optimal sequence of actions is the one in which we repair components in order of their $p_{i}/C_{i}^{o}$ ratio. c. Extend your analysis to the case where some components can be replaced, but not observed; that is, we cannot determine whether they are broken or not. 

# Exercise 23.16 

value of control The value of perfect information measures the change in our MEU if we allow observing a variable that was not observed before. In the same spirit, deﬁne a notion of a value of control , which is the gain to the agent if she is allowed to intervene at a chance variable $X$ and set its value. Make reasonable assumptions about the space of strategies available to the agent, but state your assumptions explicitly. 

# Why Probabilistic Graphical Models? 

In this book, we have presented a framework of structured probabilistic models. This framework rests on two foundations: 

• the use of a probabilistic model — a joint probability distribution — as a representation of our domain knowledge;

 • the use of expressive data structures (such as graphs or trees) to encode structural properties of these distributions. 

# 

declarative representation 

The ﬁrst of these ideas has several important ramiﬁcations. First, our domain knowledge is encoded declaratively , using a representation that has its own inherent semantics. Thus, the conclusions induced by the model are intrinsic to it, and not dependent on a speciﬁc implementation or algorithm. This property gives us the ﬂexibility to develop a range of inference algorithms, which may be appropriate in diferent settings. As long as each algorithm remains faithful to the underlying model semantics, we know it to be correct. 

Moreover, because the basic operations of the calculus of probabilities (conditioning, marginal- ization) are generally well accepted as being sound reasoning patterns, we obtain an important guarantee: If we obtain surprising or undesirable conclusions from our probabilistic model, the problem is with our model, not with our basic formalism. Of course, this conclusion relies on the assumption that we are using exact probabilistic inference, which implements (albeit ef- ciently) the operations of this calculus; when we use approximate inference, errors induced by the algorithm may yield undesirable conclusions. Nevertheless, the existence of a declarative representation allows us to separate out the two sources of error — modeling error and algorithmic error — and consider each separately. We can ask separately whether our model is a correct reﬂection of our domain knowledge, and whether, for the model we have, approxi- mate inference is introducing overly large errors. Although the answer to each of these questions may not be trivial to determine, each is more easily considered in isolation. For example, to test the model, we might try diferent queries or perform sensitivity analysis. To test an approximate inference algorithm, we might try the algorithm on fragments of the network, try a diferent (approximate or exact) inference algorithm, or compare the probability of the answer obtained to that of an answer we may expect. 

A third beneﬁt to the use of a declarative probabilistic representation is the fact that the same representation naturally and seamlessly supports multiple types of reasoning. We can compute the posterior probability of any subset of variables given observations about any others, subsuming reasoning tasks such as prediction, explanation, and more. We can compute the most likely joint assignment to all of the variables in the domain, providing a solution to a problem known as abduction . With a few extensions to the basic model, we can also answer causal queries and make optimal decisions under uncertainty. 

The second of the two ideas is the key to making probabilistic inference practical. The ability to exploit structure in the distribution is the basis for providing a compact representa- tion of high-dimensional (or even inﬁnite-dimensional) probability spaces. This compact representation is highly modular, allowing a ﬂexible representation of domain knowledge that can easily be adapted, whether by a human expert or by an automated algorithm. This property is one of the key reasons for the use of probabilistic models. For example, as we discussed in box 23.C, a diagnostic system designed by a human expert to go through a certain set of menus asking questions is very brittle: even small changes to the domain knowledge can lead to a complete reconstruction of the menu system. By contrast, a system that uses inference relative to an underlying probabilistic model can easily be modiﬁed simply by revising the model (or small parts of it); these changes automatically give rise to a new interaction with the user. 

The compact representation is also the key for the construction of efective reasoning al- gorithms. All of the inference algorithms we discussed exploit the structure of the graph in fundamental ways to make the inference feasible. Finally, the graphical representation also provides the basis for learning these models from data. First, the smaller parameter space uti- lized by these models allows parameter estimation even of high-dimensional distributions from a reasonable amount of data. Second, the space of sparse graph structures deﬁnes an efec- tive and natural bias for structure learning, owing to the ubiquity of (approximate) conditional independence properties in distributions arising in the real world. 

# The Modeling Pipeline 

The framework of probabilistic graphical models provides support for natural representation, efective inference, and feasible model acquisition. Thus, it naturally leads to an integrated methodology for tackling a new application domain — a methodology that relies on all three of these components. 

Consider a new task that we wish to address. We ﬁrst deﬁne a class of models that encode the key properties of the domain that are critical to the task. We then use learning to ﬁll in the missing details of the model. The learned model can be used as the basis for knowledge discovery, with the learned structure and parameters providing important insights about prop- erties of the domain; it can also be used for a variety of reasoning tasks: diagnosis, prediction, or decision making. 

Many important design decisions must be made during this process. One is the form of the graphical model. We have described multiple representations throughout this book — directed and undirected, static and temporal, ﬁxed or template-based, with a variety of models for local interactions, and so forth. These should not be considered as mutually exclusive options, but rather as useful building blocks. Thus, a model does not have to be either a Bayesian network or a Markov network — perhaps it should have elements of both. A model may be neither a full dynamic Bayesian network nor a static one: perhaps some parts of the system can be modeled as static, and others as dynamic. 

In another decision, when designing our class of models, we can provide a fairly speciﬁc description of the models we wish to consider, or one that is more abstract, specifying only high- level properties such as the set of observed variables. Our prior knowledge can be incorporated in a variety of ways: as hard constraints on the learned model, as a prior, or perhaps even only as an initialization for the learning algorithm. Diferent combinations will be appropriate for diferent applications. 

These decisions, of course, inﬂuence the selection of our learning algorithm. In some cases, we will need to ﬁll in only (some) parameters; in others, we can learn signiﬁcant aspects of the model structure. In some cases, all of the variables will be known in advance; in others, we will need to infer the existence and role of hidden variables. 

When designing a class of models, it is critical to keep in mind the basic trade-of between faithfulness — accurately modeling the variables and interactions in the domain — and identiﬁability — the ability to reliably determine the details of the model. Given the richness of the representations one can encode in the framework of probabilistic models, it is often very tempting to select a highly expressive representation, which really captures everything that we think is going on in the domain. Unfortunately, such models are often hard to identify from training data, owing both to the potential for overﬁtting and to the large number of local maxima that can make it difcult to ﬁnd the optimal model (even when enough training data are available). Thus, one should always keep in mind Einstein’s maxim: 

Everything should be made as simple as possible, but not simpler. 

There are many other design decisions that inﬂuence our learning algorithm. Most obviously, there are often multiple learning algorithms that are applicable to the same class of models. Other decisions include what priors to use, when and how to introduce hidden variables, which features to construct, how to initialize the model, and more. Finally, if our goal is to use the model for knowledge discovery, we must consider issues such as methods for evaluating our conﬁdence in the learned model and its sensitivity to various choices that we made in the design. Currently, these decisions are primarily made using individual judgment and experience. 

Finally, if we use the model for inference, we also have various decisions to make. For any class of models, there are multiple algorithms — both exact and approximate — that one can apply. Each of these algorithms works well in certain cases and not others. It is important to remember that here, too, we are not restricted to using only a pure version of one of the inference algorithms we described. We have already presented hybrid methods such as collapsed sampling methods, which combine exact inference and sampling. However, many other hybrids are possible and useful. For example, we might use collapsed particle methods combined with belief propagation rather than exact inference, or use a variational approximation to provide a better proposal distribution for MCMC methods. 

Overall, it is important to realize that what we have provided is a set of ideas and tools. One can be ﬂexible and combine them in diferent ways. Indeed, one can also extend these ideas, constructing new representations and algorithms that are based on these concepts. This is precisely the research endeavor in this ﬁeld. 

# Some Current and Future Directions 

Despite the rapid advances in this ﬁeld, there are many directions in which signiﬁcant open problems remain. Clearly, one cannot provide a comprehensive list of all of the interesting open problems; indeed, identifying an open problem is often the ﬁrst step in a research project. However, we describe here some broad categories of problems where there is clearly much work that needs to be done. 

On the pragmatic side, probabilistic models have been used as a key component in addressing some very challenging applications involving automated reasoning and decision making, data analysis, pattern recognition, and knowledge discovery. We have mentioned some of these applications in the case studies provided in this book, but there are many others to which this technology is being applied, and still many more to which it could be applied. There is much work to be done in further developing these methods in order to allow their efective application to an increasing range of real-world problems. 

However, our ability to easily apply graphical models to solve a range of problems is limited by the fact that many aspects of their application are more of an art than a science. As we discussed, there are many important design decisions in the selection of the representation, the learning procedure, and the inference algorithm used. Unfortunately, there is no systematic procedure that one can apply in navigating these design spaces. Indeed, there is not even a comprehensive set of guidelines that tell us, for a particular application, which combination of ideas are likely to be useful. At the moment, the design process is more the result of trial- and-error experimentation, combined with some rough intuitions that practitioners learn by experience. It would be an important achievement to turn this process from a black art into a science. 

At a higher level, one can ask whether the language of probabilistic graphical models is ade- quate for the range of problems that we eventually wish to address. Thus, a diferent direction is to extend the expressive power of probabilistic models to incorporate a richer range of concepts, such as multiple levels of abstractions, complex events and processes, groups of objects with a rich set of interactions between them, and more. If we wish to construct a representation of general world knowledge, and perhaps to solve truly hard problems such as perception, nat- ural language understanding, or commonsense reasoning, we may need a representation that accommodates concepts such as these, as well as associated inference and learning algorithms. Notably, many of these issues were tackled, with varying degrees of success, within the dis- ciplines of philosophy, psychology, linguistics, and traditional knowledge representation within artiﬁcial intelligence. Perhaps some of the ideas developed in this long-term efort can be inte- grated into a probabilistic framework, which also supports reasoning from limited observations and learning from data, providing an alternative starting point for this very long-term endeavor. 

The possibility that these models can be used as the basis for solving problems that lie at the heart of human intelligence raises an entirely new and diferent question: Can we use models such as these as a tool for understanding human cognition? In other words, can these structured models, with their natural information ﬂow over a network of concepts, and their ability to integrate intelligently multiple pieces of weak evidence, provide a good model for human cognitive processes? Some preliminary evidence on this question is promising, and it suggests that this direction is worthy of further study. 

# A Background Material 

# A.1 Information Theory 

Information theory deals with questions involving efcient coding and transmission of informa- tion. To address these issues, one must consider how to encode information so as to maximize the amount of data that can sent on a given channel, and how to deal with noisy channels. We brieﬂy touch on some technical deﬁnitions that arise in information theory, and use com- pression as our main motivation. Cover and Thomas (1991) provides an excellent introduction to information theory, including historical perspective on the development and applications of these notions. 

# A.1.1 Compression and Entropy 

Suppose that one plans to transmit a large corpus of say English text over a digital line. One option is to send the text using standard (for example, ASCII) encoding that uses a ﬁxed number of bits per character. A somewhat more efcient approach is to use a code that is tailored to the task of transmitting English text. For example, if we construct a dictionary of all words, we can use binary encoding to describe each word; using 16 bits per word, we can encode a dictionary of up to 65,536 words, which covers most English text. 

compression 

We can gain an additional boost in compression by building a variable-length code , which encodes diferent words in bit strings of diferent length. The intuition is that words that are frequent in English should be encoded by shorter code words, and rare words should be encoded by longer ones. To be unambiguously decodable, a variable-length code must be preﬁx free : no codeword can be a strict preﬁx of another. Without this property, we would not be able to tell (at least not using a simple scan of the data) when one code word ends and the next begins. It turns out that variable-length codes can signiﬁcantly improve our compression rate: 

expected number of bits used is: 

$$
{\frac{1}{2}}\cdot1+{\frac{1}{4}}\cdot2+{\frac{1}{8}}\cdot3+{\frac{1}{8}}\cdot3=1.75.
$$ 

One might ask whether a diferent encoding would give us better compression performance in this example. It turns out that this encoding is the best we can do, relative to the word-frequency distribution. To provide a formal analysis for this statement, suppose we have a random variable $X$ that denotes the next item we need to encode (for example, a word). In order to analyze the performance of a compression scheme, we need to know the distribution over diferent values of $X$ . So we assume that we have a distribution $P(X)$ (for example, frequencies of diferent words in a large corpus of English documents). 

The notion of the entropy of a distribution provides us with a precise lower bound for the expected number of bits required to encode instances sampled from $P(X)$ . 

Deﬁnition A.1 entropy 

Let $P(X)$ be a distribution over a random variable $X$ . The entropy of $X$ is deﬁned as 

$$
H_{P}(X)=E_{P}{\Bigg[}\mathrm{log}\,{\frac{1}{P(x)}}{\Bigg]}=\sum_{x}P(x)\log{\frac{1}{P(x)}},
$$ 

where we treat $0\log{1/0}=0$ . 

When discussing entropies (and other information-theoretic measures) we use logarithms of base 2. We can then interpret the entropy in terms of bits. 

The central result in information theory is a theorem by Shannon showing that the entropy of $X$ is the lower bound on the average number of bits that are needed to encode values of $X$ . That is, if we consider a proper codebook for values of $X$ (one that can be decoded unambiguously), then the expected code length, relative to the distribution $P(X)$ , cannot be less than $H_{P}(X)$ bits. 

Going back to our example, we see that the average number of bits for this code is precisely the entropy. Thus, the lower bound is tight in this case, in that we can construct a code that achieves precisely that bound. As another example, consider a uniform distribution $P(X)$ . In this case, the optimal encoding is to represent each word using the same number of bits, $\log|V a l(X)|$ . Indeed, it is easy to verify that $H_{P}(X)=\log|V a l(X)|$ | | , so again the bound is tight (at least for cases where $|V a l(X)|$ is a power of 2.) Somewhat surprisingly, the entropy bound is tight in general, in that there are codes that come very close to the “optimum” of assigning the value $x$ a code of length $-\log P(x)$ . 

Another way of viewing the entropy is as a measure of our uncertainty about the value of $X$ . Consider a game where we are allowed to ask yes/no questions until we pinpoint the value $X$ . Then the entropy of $X$ is average number of questions we need to ask to get to the answer (if we have a good strategy for asking them). If we have little uncertainty about $X$ , then we get to the value with few questions. An extreme case is when $H_{P}(X)=0$ . It is easy to verify that this can happen only when one value of $X$ has probability 1 and the rest probability 0 . In this case, we do not need to ask any questions to get to the value of $X$ . On the other hand, if the value of $X$ is very uncertain, then we need to ask many questions. This discussion in fact identiﬁes the two boundary cases for $H_{P}(X)$ . 

Proposition A.1 

$$
0\leq H_{P}(X)\leq\log|\mathit{V a l}(X)|
$$ 

The deﬁnition of entropy naturally extends to multiple variables. 

Deﬁnition A.2 joint entropy 

Suppose we have a joint distribution over random variables $X_{1},\ldots,X_{n}$ . Then the joint entropy of $X_{1},\dots,X_{n}$ is 

$$
H_{P}(X_{1},.\,.\,,X_{n})=E_{P}\biggl[\log\frac{1}{P(X_{1},.\,.\,,X_{n})}\biggr].
$$ 

The joint entropy captures how many bits are needed (on average) to encode joint instances of the variables. 

# A.1.2 Conditional Entropy and Information 

Suppose we are encoding the values of $X$ and $Y$ . A natural question is what is the cost of encoding $X$ if we are already encoding $Y$ . Formally, we can examine the diference between $H_{P}(X,Y)$ — the number of bits needed (on average) to encode of both variables, and $H_{P}(Y)$ — the number of bits needed to encode $Y$ alone. 

Deﬁnition A.3 conditional entropy 

entropy chain rule 

# Proposition A.2 

The conditional entropy of $X$ given $Y$ is 

$$
H_{P}(X\mid Y)=H_{P}(X,Y)-H_{P}(Y)=E_{P}\biggl[\log{\frac{1}{P(X\mid Y)}}\biggr].
$$ 

This quantity captures the additional cost (in terms of bits) of encoding $X$ when we are already encoding $Y$ . The deﬁnition gives rise to the chain rule of entropy : 

For any distribution $P(X_{1},\cdot\cdot\cdot,X_{n}).$ , we have that 

$$
H_{P}(X_{1},\ldots,X_{n})=H_{P}(X_{1})+H_{P}(X_{2}\mid X_{1})+\ldots+H_{P}(X_{n}\mid X_{1},\ldots,X_{n-1}).
$$ 

That is, to encode a joint value of $X_{1},\dots,X_{n};$ , we ﬁrst need to encode $X_{1}$ , then encode $X_{2}$ given that we know the value of $X_{1}$ , then encode $X_{3}$ given the ﬁrst two, and so on. Note that, similarly to the chain rule of probabilities, we can expand the chain rule in any order we prefer; that is, all orders result in precisely the same value. 

Intuitively, we would expect $H_{P}(X\mid Y)$ | , the additional c of encoding $X$ when we already encode $Y$ , to be at least as small as the cost of encoding X alone. To motivate that, we see that the worst case scenario is where we encode $X$ as though we did not know the value of $Y$ . Indeed, one can formally show 

Proposition A.3 

$$
H_{P}(X\mid Y)\leq H_{P}(X).
$$ 

The diference between these two quantities is of special interest. 

Deﬁnition A.4 

mutual information 

The mutual information between $X$ and $Y$ is 

$$
I_{P}(X;Y)=H_{P}(X)-H_{P}(X\mid Y)=E_{P}\biggl[\log\frac{P(X\mid Y)}{P(X)}\biggr].
$$ 

The mutual information captures how many bits we save (on average) in the encoding of $X$ if we know the value of $Y$ . Put in other words, it represents the extent to which the knowledge of $Y$ reduces our uncertainty about $X$ . 

The mutual information satisﬁes several nice properties. 

Proposition A.4 • $0\leq\mathbf{\mathit{I}}_{P}(X;Y)\leq\mathbf{\mathit{H}}_{P}(X).$ ≤ .

 • $I_{P}(X;Y)=I_{P}(Y;X).$ .

 • $I_{P}(X;Y)=0$ if and only if $X$ and $Y$ are independent. Thus, the mutual information is nonnegative, and equal to 0 if and only if the two variables 

are independent of each other. This is fairly intuitive, since if $X$ and $Y$ are independent, then learning the value of $Y$ does not tell us any thing new about the value of $X$ . In fact, we can view the mutual information as a quantitative measure of the strength of the dependency between $X$ and $Y$ . The bigger the mutual information, the stronger the dependency. The extreme upper value of the mutual information is when $X$ is a deterministic function of $Y$ (or vice versa). In this case, once we know $Y$ we are certain about the value of $X$ , and so $I_{P}(X;Y)=I H_{P}(X)$ . That is, $Y$ supplies the maximal amount of information about $X$ . 

# A.1.3 Relative Entropy and Distances Between Distributions 

In many situations when doing probabilistic reasoning, we want to compare two distributions. For example, we might want to approximate a distribution by one with desired qualities (say, simpler representation, more efcient to reason with, and so on) and want to evaluate the quality of a candidate approximation. Another example is in the context of learning a distribution from data, where we want to compare the learned distribution to the “true” distribution from which the data was generated. 

distance measure 

Thus, we want to construct a distance measure $d$ that evaluates the distance between two distributions. There are some properties that we might wish for in such a distance measure: 

Positivity: $d(P,Q)$ is always nonnegative, and is zero if and only if $P=Q$ ; 

Symmetry: $d(P,Q)=d(Q,P)$ . 

Triangle inequality: for any three distributions $P,Q,R,$ we have that 

$$
d(P,R)\leq d(P,Q)+d(Q,R).
$$ 

distance metricWhen a distance measure $d$ satisﬁes these criteria, it is called a distance metric . 

We now review several common approaches used to compare distributions. We begin by describing one important measure that is motivated by information-theoretic considerations. It also turns out to arise very naturally in a wide variety of probabilistic settings. 

# A.1.3.1 Relative Entropy 

Consider the preceding discussion of compression. As we discussed, the entropy measures the performance of “optimal” code that assigns the value $x$ a code of length $-\log P(x)$ . However, in many cases in practice, we do not have access to the true distribution P that generates the data we plan to compress. Thus, instead of using $P$ we use another distribution $Q$ (say one we estimated from prior data, or supplied by a domain expert), which is our best guess for $P$ . 

Suppose we build a code using $Q$ . Treating $Q$ as a proxy to the real distribution, we use $-\log Q(x)$ bits o encode the value $x$ . Thus, the expected number of bits we use on data generated from P is 

$$
E_{P}\left[\log\frac{1}{Q(x)}\right].
$$ 

A natural question is how much we lost, due to the inaccuracy of using $Q$ . Thus, we can examine the diference between this encoding and the best achievable one, $H_{P}(X)$ . This diference is called the relative entropy. 

Deﬁnition A.5 relative entropy Let $P$ and $Q$ be two distributions over random variables $X_{1},\ldots,X_{n}$ . The relative entropy of $P$ and $Q$ is 

$$
D(P(X_{1},\ldots,X_{n})\|Q(X_{1},\ldots,X_{n}))=E_{P}\biggl[\log\frac{P(X_{1},\ldots,X_{n})}{Q(X_{1},\ldots,X_{n})}\biggr].
$$ 

When the set of variables in question is clear from the context, we use the shorthand notation $D(P\|Q)$ . This measure is also often known as the Kullback-Liebler divergence (or KL-divergence ). 

This discussion suggests that the relative entropy measures the additional cost imposed by using a wrong distribution $Q$ instead of $P$ . Thus, $Q$ is close, in the sense of relative entropy, to $P$ if this cost is small. As we expect, the additional cost of using the wrong distribution is always positive. Moreover, the relative entropy is 0 if and only if the two distributions are identical: 

Proposition A.5 $D(P\|Q)\geq0$ | | ≥ , and is equal to zero if and only if $P=Q$ . 

It is also natural to ask whether the relative entropy is also bounded from above. As we can quickly convince ourselves, if there is a value $x$ such that $P(x)\,>\,0$ and $Q(x)=0$ , then the relative entropy $D(P\|Q)$ | | is inﬁnite. More precisely, if we consider a sequence of distributions $Q_{\epsilon}$ such that $Q_{\epsilon}(x)=\epsilon$ , then $\begin{array}{r}{\operatorname*{lim}_{\epsilon\to0}D(P\|Q_{\epsilon})=\infty}\end{array}$ | | ∞ . 

It is natural ask whether the relative entropy deﬁnes a distance measure over distributions. Proposition A.5 shows that the relative entropy satisﬁes the positivity property speciﬁed above. Unfortunately, positivity is the only property of distances that relative entropy satisﬁes; it satisﬁes neither symmetry nor the triangle inequality. Given how natural these properties are, one might wonder why relative entropy is used at all. Aside from the fact that it arises very naturally in many settings, it also has a variety of other useful properties, that often make up for the lack of symmetry and the triangle inequality. 

# A.1.3.2 Conditional Relative Entropy 

As with entropies, we can deﬁne a notion of conditional relative entropy. 

# Deﬁnition A.6 

conditional relative entropy Let $P$ and $Q$ be two distributions over random variables $X,Y$ . The conditional relative entropy of $P$ and $Q$ , is 

$$
D(P(X\mid Y)\|Q(X\mid Y))=E_{P}\biggl[\log{\frac{P(X\mid Y)}{Q(X\mid Y)}}\biggr].
$$ 

We can think of the conditional relative entropy $D(P(X\mid Y)\|Q(X\mid Y))$ | | | | as the weighted sum of the relative entropies between the conditional distributions given diferent values of $y$ 

$$
D(P(X\mid Y)\|Q(X\mid Y))=\sum_{y}P(y)D(P(X\mid y)\|Q(X\mid y)).
$$ 

relative entropy chain rule 

# Proposition A.6 

Using the conditional relative entropy, we can write the chain rule of relative entropy : Let $P$ and $Q$ be distributions over $X_{1},\dots,X_{n}.$ , then 

$$
\begin{array}{l l l}{{D(P\|Q)}}&{{=}}&{{D(P(X_{1})\|Q(X_{1}))\;+}}\\ {{}}&{{}}&{{D(P(X_{2}\mid X_{1})\|Q(X_{2}\mid X_{1}))+\ldots+}}\\ {{}}&{{}}&{{D(P(X_{n}\mid X_{1},\ldots,X_{n-1})\|Q(X_{n}\mid X_{1},\ldots,X_{n-1})).}}\end{array}
$$ 

Using the chain rule, we can prove additional properties of the relative entropy. First, using the chain rule and the fact that $D(P(Y\mid X)\|Q(Y\mid X))\ge0$ | | | | ≥ , we can get the following property. 

# Proposition A.7 

$$
D(P(X)\|Q(X))\leq D(P(X,Y)\|Q(X,Y)).
$$ 

That is, the relative entropy of a marginal distributions is upper-bounded by the relative entropy of the joint distributions. This observation generalizes to situations where we consider sets of variables. That is, 

$$
D(P(X_{1},\ldots,X_{k})\|Q(X_{1},\ldots,X_{k}))\le D(P(X_{1},\ldots,X_{n})\|Q(X_{1},\ldots,X_{n}))
$$ 

for $k\leq n$ . 

pose that $X$ an $Y$ nt in both $P$ and $Q$ . Then, we have that $P(Y\mid X)=$ $P(Y)$ , and similarly, $Q(Y\mid X)=Q(Y)$ | . Thus, we conclude that $D(P(Y\mid X)\|Q(Y\mid X))=$ | | | | $D(P(Y)\|Q(Y))$ | | . Combining this observation with the chain rule, we can prove an additional property. 

# Proposition A.8 

If both $P$ and $Q$ satisfy $(X\perp Y)$ , then $D(P(X,Y)\|Q(X,Y))=D(P(X)\|Q(X))+D(P(Y)\|Q(Y)).$ 

# A.1.3.3 Other Distance Measures 

There are several diferent metric distances between distributions that we may consider. Several simply treat a probability distribution as a vector in ${\mathit{I\!R}}^{N}$ (where $N$ is the dimension of our probability space), and use standard distance metrics for Euclidean spaces. More precisely, let $P$ and $Q$ be two distributions over $X_{1},\dots,X_{n}$ . The three most commonly used distance metrics of this type are: 

$$
\begin{array}{r l}&{\|P-Q\|_{1}=\sum_{x_{1},\ldots,x_{n}}|P(x_{1},\ldots,x_{n})-Q(x_{1},\ldots,x_{n})|.}\\ &{\|P-Q\|_{2}=\Big(\sum_{x_{1},\ldots,x_{n}}(P(x_{1},\ldots,x_{n})-Q(x_{1},\ldots,x_{n}))^{2}\Big)^{\frac{1}{2}}.}\\ &{\therefore\ \|P-Q\|_{\infty}=\operatorname*{max}_{x_{1},\ldots,x_{n}}|P(x_{1},\ldots,x_{n})-Q(x_{1},\ldots,x_{n})|.}\end{array}
$$ 

variational distance 

An apparently diferent distance measure is the variational distance , which seems more specif- ically tailored to probability distributions, rather than to general real-valued vectors. It is deﬁned as the maximal diference in the probability that two distributions assign to any event that can be described by the distribution. For two distributions $P,Q$ over an event space $s$ , we deﬁne: 

$$
D_{v a r}(P;Q)=\operatorname*{max}_{\alpha\in\mathcal{S}}|P(\alpha)-Q(\alpha)|.
$$ 

Interestingly, this distance turns out to be exactly half the $\mathrm{L_{1}}$ distance: 

Proposition A.9 

$$
D_{\mathrm{var}}(P;Q)={\frac{1}{2}}\|P-Q\|_{1}.
$$ 

These distance metrics are all useful in the analysis of approximations, but, unlike the relative entropy, they do not decompose by a chain-rule-like construction, often making the analytical analysis of such distances harder. However, we can often use an analysis in terms of relative entropy to provide bounds on the $\mathrm{L_{1}}$ distance, and hence also on the variational distance: 

Theorem A.1 

$$
\|P-Q\|_{1}\leq((2\ln2)D(P\|Q))^{1/2}\,.
$$ 

# A.2 Convergence Bounds 

In many situations that we cover in this book, we are given a set of samples generated from a distribution, and we wish to estimate certain properties of the generating distribution from the samples. We now review some properties of random variables that are useful for this task. The derivation of these convergence bounds is central to many aspects of probability theory, statistics, and randomized algorithms. Motwani and Raghavan (1995) provide one good introduction on this topic and its applications to the analysis of randomized algorithms. 

Speciﬁcally, suppose we have a biased coin that has an unknown probability $p$ of landing heads. We can estimate the value of $p$ by tossing the coin several times and counting the IID frequency of heads. More precisely, assume we have a ta set $\mathcal{D}$ consisting of $M$ coin tosses, that is, $M$ trials from a Bernoulli distribution. The m ’th coin toss is represented by a binary variable $X[m]$ that has value 1 if the coin lands heads, and 0 otherwise. Since each toss is separate from the previous one, we are assuming that all these random variables are independent. Thus, these variables are independence and identically distribution , or $I D.$ It is easy to compute the expectation and variance of each $X[m]$ : 

• $E[X[m]]=p.$ .

 • $\begin{array}{r}{{\cal{W a r}}[X[m]]=p(1-p).}\end{array}$ 

# A.2.1 Central Limit Theorem 

We are interested in the sum of all the variables $S_{\mathcal{D}}=X[1]+\cdot\cdot\cdot+X[M]$ and in the fraction of successful trials $\begin{array}{r}{T_{\mathcal{D}}=\frac{1}{M}S_{\mathcal{D}}}\end{array}$ . Note that $S_{\mathcal{D}}$ and $T_{\mathcal{D}}$ are functions of the data set $\mathcal{D}$ . As $\mathcal{D}$ is chosen randomly, they can be viewed as random variables over the probability space deﬁned by diferent possible data sets $\mathcal{D}$ . Using properties of expectation and variance, we can analyze the properties of these random variables. 

• $E[S_{\mathcal{D}}]=M\cdot p,$ D · , by linearity of expectation.

 • $W a r[S_{\mathcal{D}}]=M\cdot p(1-p)$ · − , since all the all the $X[i]$ ’s are independent. D

 • $E[T_{\mathcal{D}}]=p$ . D

 • $\begin{array}{r}{{\mathbb{V}}\!a r[T_{\mathcal{D}}]=\frac{1}{M}p(1-p),\,\mathrm{since}\,\,{\mathbb{V}}\!a r\big[\frac{1}{M}S_{\mathcal{D}}\big]=\frac{1}{M^{2}}{\mathbb{V}}\!a r[S_{\mathcal{D}}].}\end{array}$ 

The fact that $\mathbb{W}a r[T_{\mathcal{D}}]\rightarrow0$ → as $M\rightarrow\infty$ suggests that for sufciently large $M$ the distribution D of $T_{\mathcal{D}}$ is concentrated around $p$ . In fact, a general result in probability theory allows us to conclude that this distribution has a particular form: 

Theorem A.2 central limit theorem 

(Central Limit Theorem) Let $X[1],X[2],\ldots.$ be a series of IID random variables, where each $X[m]$ is sampled from a distribution such that $E[X[m]]\,=\,\mu_{*}$ , and variance $\mathbb{W}a r[X[m]]\,=\,\sigma^{2}\,$ $(0<\sigma<\infty)$ ). Then 

$$
\operatorname*{lim}_{M\to\infty}P\left(\frac{\sum_{m}(X[m]-\mu)}{\sqrt{M}\sigma}<r\right)=\Phi(r),
$$ 

where $\Phi(r)=P(Z<r)$ for a Gaussian variable $Z$ with distribution $\mathcal{N}\left(0;1\right)$ . 

Gaussian Thus, if we collect a large number of repeated samples from the same distribution, then the distribution of the rando variable $(\grave{S_{\mathcal{D}}}-E[S_{\mathcal{D}}])\grave{/\sqrt{\mathbb{W}a r[S_{\mathcal{D}}]}}$ p is roughly Gaussian . In other D D words, the distribution of $S_{\mathcal{D}}$ is, at the limit, close to a Gaussian with the appropriate expectation D and variance: $\mathcal{N}\left(E[S_{\mathcal{D}}];\,\mathbb{W}a r[S_{\mathcal{D}}]\right)$ . 

There are variants of the central limit theorem for the case where each $X[m]$ has a diferent distribution. These require additional technical conditions that we do not go into here. However, the general conclusion is similar — the sum of many independent random variables has a distribution that is approximately Gaussian. This is often a justiﬁcation for using a Gaussian distribution in modeling quantities that are the cumulative efect of many independent (or almost independent) factors. 

estimator 

unbiased estimator 

The quantity $T_{\mathcal{D}}$ is an estimator for the mean $\mu$ : a statistical function that we can use to estimate the value of $\mu$ . The mean and variance of an estimator are the two key quantities for evaluating it. The mean of the estimator tells us the value around which its values are going to be concentrated. When the mean of the estimator is the target value $\mu$ , it is called an unbiased estimator for the quantity $\mu\:-$ an estimator whose mean is precisely the desired value. In general, lack of bias is a desirable property in an estimator: it tells us that, although they are noisy, at least the values obtained by the estimator are centered around the right value. The variance of the estimator tells us the “spread” of values we obtain from it. Estimators with high variance are not very reliable, as their value is likely to be far away from their mean. 

Applying the central limit theorem to our problem, we see that, for sufciently large $M$ , the variable $T_{\mathcal{D}}$ has a roughly Gaussian distribution with mean $p$ and variance $\textstyle{\frac{p(1-p)}{M}}$ . 

# A.2.2 Convergence Bounds 

Hoefding bound 

Theorem A.3 

In many situations, we are interested not only in the asymptotic distribution of $T_{\mathcal{D}}$ , but also in the probability that $T_{\mathcal{D}}$ is close to $p$ for a concrete choice of $M$ . We can bound this probability in several ways. One of the simplest is by using Chebyshev’s inequality; see exercise 12.1. This bound, however, is quite loose, as it assumes quadratic decay in the distance $|T_{\mathcal{D}}-p|$ . Other, more reﬁned bounds, can be used to prove an exponential rate of decay in this distance. There are many variants of these bounds, of which we describe two. 

The ﬁrst, called Hoefding bound , measures error in terms of the absolute distance . 

Let $\mathcal{D}=\{X[1],\cdot\cdot\cdot,X[M]\}$ be a sequence of $M$ independent Bernoulli trials with probability of success $p$ . Let $\begin{array}{r}{T_{\mathcal{D}}=\frac{1}{M}\sum_{m}X[m]}\end{array}$ P . Then $\begin{array}{r c l}{{P_{\mathcal{D}}(T_{\mathcal{D}}>p+\epsilon)}}&{{\le}}&{{e^{-2M\epsilon^{2}}}}\\ {{P_{\mathcal{D}}(T_{\mathcal{D}}<p-\epsilon)}}&{{\le}}&{{e^{-2M\epsilon^{2}}.}}\end{array}$ 

The bound asserts that, with very high probability, $T_{\mathcal{D}}$ is within an additive error $\epsilon$ of the true probability $p$ . The probability here is ta n relative to possible data sets $\mathcal{D}$ . Intuitively, we might end up with really unlikely choices of D , for example, ones where we get the same value all the time; these choices will clearly give wrong results, but they are very unlikely to arise as a result of a random sampling process. Thus, the bound tells us that, for most data sets $\mathcal{D}$ 

# 

Chernof bound 

Theorem A.4 

that we generate at random, we obtain a good estimate. Furthermore, the fraction of “bad” sample sets $\mathcal{D}$ , those for which the estimate more than $\epsilon$ from the true value, diminishes exponentially as the number of samples M grows. 

The second bound, called the Chernof bound , measures error in terms of the relative size of this distance to the size of $p$ . 

Let $\sigma_{M}=\sqrt{\mathbb{W}a r[T_{\mathcal{D}}]}$ p be the standard deviation of $T_{\mathcal{D}}$ for $\mathcal{D}$ of size $M$ . Using the multiplica- D tive Chernof bound, we can show that 

$$
\begin{array}{r}{P_{\mathcal{D}}\big(|T_{\mathcal{D}}-p|\ge k\sigma\big)\le2e^{-k^{2}/6}.}\end{array}
$$ 

This inequality should be contrasted with the Chebyshev inequality. The big diference owes to the fact that the Chernof bound exploits the particular properties of the distribution of $T_{\mathcal{D}}$ . 

# A.3 Algorithms and Algorithmic Complexity 

In this section, we brieﬂy review relevant algorithms and notions from algorithmic complexity. Cormen et al. (2001) is a good source for learning about algorithms, data structures, graph algorithms, and algorithmic complexity; Papadimitriou (1993) and Sipser (2005) provide a good introduction to the key concepts in computational complexity. 

# A.3.1 Basic Graph Algorithms 

Given a graph structure, there are many useful operations that we might want to perform. For example, we might want to determine whether there is a certain type of path between two nodes. In this section, we survey algorithms for performing two key tasks that will be of use in several places throughout this book. Additional algorithms, for more speciﬁc tasks, are presented as they become relevant. 

![](images/ecc02fea659cb92fcb703ed1e9a5947a00ed52c965c64fb57cb847301f01c558.jpg) 

topological ordering 

maximum weight spanning tree 

One algorithm, shown in algorithm A.1, ﬁnds a topological ordering of the nodes in the graph, as deﬁned in deﬁnition 2.19. 

Another useful algorithm is one that ﬁnds, in a weighted undirected graph $\mathcal{H}$ with nonnegative edge weights, a maximum weight spanning tree . More precisely, a subgraph is said to be a spanning tree if it is a tree and it spans all vertices in the graph. Similarly, a spanning forest is a forest that spans all vertices in the graph. A maximum weight spanning tree (or forest) is the tree (forest) whose edge-weight sum is largest among all spanning trees (forests). 

![](images/5794ba8ffd6aa2abfed02c1b82783ea60dc747d8ea7bd95e468720b60f59a0e1.jpg) 

# A.3.2 Analysis of Algorithmic Complexity 

A key step in evaluating the usefulness of an algorithm is to analyze its computational cost: the amount of time it takes to complete the computation and the amount of space (memory) required. To evaluate the algorithm, we are usually not interested in the cost for a particular input, but rather in the algorithm’s performance over a set of inputs. Of course, we would expect most algorithms to run longer when applied to larger problems. Thus, the complexity of an algorithm is usually measured in terms of its performance, as a function of the size of the input given to it. Of course, to determine the precise cost of the algorithm, we need to know exactly how it is implemented and even which machine it will be run on. However, we can often determine the scalability of an algorithm at a more abstract level, without worrying about the details of its implementation. We now provide a high-level overview of some of the basic concepts underlying such analysis. 

Consider an algorithm that takes a list of $n$ numbers and adds them together to compute their sum. Assuming the algorithm simply traverses the list and computes the sum as it goes along, it has to perform some ﬁxed number of basic operations for each element in the list. The precise operations depend on the implementation: we might follow a pointer in a linked list, or simply increment a counter in an array. Thus, the precise cost might vary based on the implementation. But, the total number of operations per list element is some ﬁxed constant factor. Thus, for any reasonab implementation, the running time of the algorithm will be bounded by $C\!\cdot\!n$ for some constant C . In this case, we say that the asymptotic complexity of the algorithm is $O(n)$ , where the $O()$ notation makes implicit the precise nature of the constant factor, which can vary from one implementation to another. This idea only makes sense if we consider the running time as a function of $n$ . For any ﬁxed problem size, say up to 100, we can always ﬁnd a constant $C$ (for instance, a million years) such that the algorithm takes time no more than $C$ . However, even if we are not interested in problems of unbounded size, evaluating the way in which the running time varies as a function of the problem size is the ﬁrst step to understanding how well it will scale to large problems. 

To take a more relevant example, consider the maximum weight spanning tree procedure of algorithm A.2. A (very) naive implementation of this algorithm traverses all of the edges in the graph every time a node is added to the spanning tree; the resulting cost is $O(m n)$ where $m$ is the number of edges and $n$ the number of nodes. A more careful implementation of the data structures, however, maintains the edges in a sorted data structure known as a heap, and the list of edges adjacent to a node in an adjacency list . In this case, the complexity of the algorithm can be $O(m\log n)$ or (with a yet more sophisticated data structure) $O(m+n\log n)$ . Surprisingly, even more sophisticated implementations exist whose complexity is very close to linear time in $m$ . 

More generally, we can provide the following deﬁnition: 

Consider an algorithm $\mathcal{A}$ that takes as input problems $\Pi$ from particular class, and returns an output. Assume that the size of each possible input problem Π is measured using some set of parameters $n_{1},\ldots,n_{k}$ . We say that the running time of $\mathcal{A}$ is $O(f(n_{1},.\,.\,.\,,n_{k}))$ f some function $f$ (called “big $O$ of $f^{\prime\prime}),$ if, for $n_{1},\ldots,n_{k}$ sufciently large, there exists a constant C such that, for any possible input problem $\Pi$ , the running time of $\mathcal{A}$ on $\Pi$ is at most $C\cdot f(n_{1},.\,.\,.\,,n_{k})$ . 

In our example, each problem $\Pi$ is a graph, and its size is deﬁned by two parameters: the number of nodes $n$ and the number of edges $m$ . The function $f(n,m)$ is simply $n+m$ . 

running time 

polynomial time exponential time 

When the function $f$ is linear in each of the input size parameters, we say that the running time of the algorithm is linear, or that the algorithm has linear time . We can similarly deﬁne notions of polynomial time and exponential time . It may be useful to distinguish diferent rates of growth in the diferent parameters. For example, if we have a function that has the form $f(n,m)=n^{2}+2^{m}$ , we might say that the function is polynomial in $n$ but exponential in $m$ . 

Although one can ﬁnd algorithms at various levels of complexity, the key cutof between feasible and infeasible computations is typically set between algorithms whose complexity is polynomial and those whose complexity is exponential. Intuitively, an algorithm whose com- plexity is exponential allows virtually no useful scalability to larger problems. For example, assume we have an algorithm whose complexity is $O(2^{n})$ , and that we can now solve instances whose size is $N$ . If we wait a few years and get a computer that is twice as fast as the one we have now, we will be able to solve only instances whose size is $N+1$ , a negligible improvement. 

We can also see this phenomenon by comparing the growth curves for various cost functions, as in ﬁgure A.1. We see that the constant factors in front of the polynomial functions have some impact on very small problem sizes, but even for moderate problem sizes, such as 20, the exponential function quickly dominates and grows to the point of infeasibility. Thus, a major distinction is made between algorithms that run in polynomial time and those whose running time is exponential. While the exponential-polynomial distinction is a critical one, there is also a tendency to view polynomial-time algorithms as tractable. This view, unfortunately, is overly simpliﬁed: an algorithm whose running time is $O(n^{3})$ is not generally tractable for problems where $n$ is in the thousands. 

Algorithmic theory ofers a suite of tools for constructing efcient algorithms for certain types of problems. One such tool, which we shall use many times throughout the book, is dynamic programming , which we describe in more detail in appendix A.3.3. Unfortunately, not all problems are not amenable to these techniques, and a broad class of highly important problems fall into a category for which polynomial-time algorithms are extremely unlikely to 

![](images/819ba5d0cf1d41c8ecc4148a52ecbcbbbca6ac68dc53596ff5cc75bc3d041625.jpg) 
Figure A.1 Illustration of asymptotic complexity. The growth curve of three functions: The solid line is $100n^{2}$ , the dashed line is $30n^{3}$ , and the dotted line is $2^{n}/5$ . 

exist; see appendix A.3.4. 

# A.3.3 Dynamic Programming 

dynamic programming As we discussed earlier, several techniques can be used to provide efcient solutions to appar- ently challenging computational problems. One important tool is dynamic programming , a general method that we can apply when the solution to a problem requires that we solve many smaller subproblems that recur many times. In this case, we are often better of precomputing the solution to the subproblems, storing them, and using them to compute the values to larger problems. 

Perhaps the simplest application of dynamic programming is the problem of computing Fibonacci numbers , deﬁned via the recursive equations: 

$$
\begin{array}{r c l}{F_{0}}&{=}&{1}\\ {F_{1}}&{=}&{1}\\ {F_{n}}&{=}&{F_{n-1}+F_{n-2}.}\end{array}
$$ 

Thus, we have that $F_{2}=2$ , $F_{3}=3$ , $F_{4}=5$ , $F_{5}=8$ , and so on. 

One simple algorithm to compute Fibonacci( n ) is to use the recursive deﬁnition directly, as shown in algorithm A.3. Unrolling the computation, we see that the ﬁrst of these recursive calls, Fibonacci $(n-1)$ lls Fibonacc $(n-2)$ a nacci $(n-3)$ . Thus, w lready have two calls to Fibonacci( $(n-2)$ − ). Similarly, Fibonacci( $(n-2)$ − ) also calls Fibonacci( $(n-3)$ − ), another redundant computation. If we carry through the entire recursive analysis, we can show that the running time of the algorithm is exponential in $n$ . 

On the other hand, we can compute “bottom up”, as in algorithm A.4. Here, we start with $F_{0}$ 

![](images/0784f24eec79279fd8786a65a3c92f2915297a853a6ff0e833aa60941d789faf.jpg) 

![](images/3627c5bb24f44ebba94ed3d54e219eb51b6ec18f6715a61379067670773b7e71.jpg) 

and $F_{1}$ , compute $F_{2}$ from $F_{0}$ and $F_{1}$ , compute $F_{3}$ from $F_{1}$ and $F_{2}$ , and so forth. Clearly, this process computes $F_{n}$ in time $O(n)$ . We can view this alternative algorithm as precomputing and then caching (or storing) the results of the intermediate computations performed on the way to each $F_{i}$ , so that each only has to be performed once. 

More generally, if we can deﬁne the set of intermediate computations required and how they depend on each other, we can often use this caching idea to avoid redundant computation and provide signiﬁcant savings. This idea underlies most of the exact inference algorithms for graphical models. 

# A.3.4 Complexity Theory 

In appendix A.3.3, we saw how the same problem might be solvable by two algorithms that have radically diferent complexities. Examples like this raise an important issue regarding the algorithm design process: If we come up with an algorithm for a problem, how do we know whether its computational complexity is the best we can achieve? In general, unfortunately, we cannot tell. There are very few classes of problems for which we can give nontrivial lower bounds on the amount of computation required for solving them. 

However, there are certain types of problems for which we can provide, not a guarantee, but at least a certain expectation regarding the best achievable performance. Complexity theory has deﬁned classes of problems that are, in a sense, equivalent to each other in terms of their computational cost. In other words, we can show that an algorithm for solving one problem can be converted into an algorithm that solves another problem. Thus, if we have an efcient algorithm for solving the ﬁrst problem, it can also be used to solve the second efciently. 

The most prominent such class of problems is that of $\mathcal{N P}$ -complete problems; this class contains many problems for which researchers have unsuccessfully tried, for decades, to ﬁnd efcient algorithms. Thus, by provin at a problem is $\mathcal{N P}$ -complete, we are essentially showing that it is “as easy” as all other NP -complete problems. Finding an efcient (polyn l time) algorithm for this problem would therefore give rise to efcient algorithms for all NP - complete problems, an extremely unlikely event. In other words, by showing that a problem is $\mathcal{N P}$ -complete, we are essentially showing that it is extremely unlikely to have an efcient solution. We now provide some of the formal basis for this type of discussion. 

# A.3.4.1 Decision Problems 

A decision problem $\Pi$ is a task that has the following form: The program must accept an input $\omega$ and decide whether it satisﬁes a certain condition or not. A prototypical decision problem is the $S A T$ problem, which is deﬁned as the problem of taking as input a formula in propositional logic, and returning true if the formula has a satisfying assignment and false if it does not. For example, an algorithm for the SAT problem should return true for the formula 

$$
\bigl(q_{1}\vee\neg q_{2}\vee q_{3}\bigr)\wedge\bigl(\neg q_{1}\vee q_{2}\vee\neg q_{3}\bigr),
$$ 

which has (among others) the satisfying assignment $q_{1}=t r u e;q_{2}=t r u e;q_{3}=t r u e.$ . It would return false for the formula 

$$
\left(\neg q_{1}\lor\neg q_{2}\right)\wedge\left(q_{2}\lor q_{3}\right)\wedge\left(\neg q_{1}\lor\neg q_{3}\right),
$$ 

which has no satisfying assignments. We often use a somewhat restricted version of the SAT problem, called 3-SAT . 

Deﬁnition A.8 3-SAT 

A formula $\phi$ is said to be a 3-SAT formula over the Boolean (binary-valued) variables $q_{1},\ldots,q_{n}$ if ng form $\phi$ n $\phi=C_{1}\wedge...\wedge C_{m}$ . Each $C_{i}$ is $^a$ clau of t orm $\ell_{i,1}\vee\ell_{i,2}\vee\ell_{i,3}$ ∨ Each $\ell_{i,j}$ ( $(i=1,.\,.\,.\,,m$ ; j $j=1,2,3)$ ) is a literal , which is either $q_{k}$ or ¬ $\neg q_{k}$ for some $k=1,\ldots,n$ . 

A decision problem $\Pi$ is associated wit a language ${\mathcal{L}}_{\Pi}$ that deﬁnes the precis of instances for which a correct algorithm for Π must return true . In the case of 3-SAT, L $\mathcal{L}_{3S A T}$ is the set of all correct encodings of propositional 3-SAT formulas that are satisﬁable. 

A.3.4.2 $\mathcal{P}$ and $\mathcal{N P}$ 

A decision problem is said to be in the class $\mathcal{P}$ if ists a deterministic algorithm that takes an i tance $\omega$ and determines whether or not $\omega\in{\mathcal{L}}_{\Pi}$ ∈L , in polynomial time in the size of the input ω . In SAT, for example, the input is the formula, and its size is simply its length. 

We can also deﬁne a signiﬁcantly more powerful type of computation that allows us to provide a formal foundation for a very rich class of problems. Consider again our SAT algorithm. The naive algorithm for determining whether a formula is satisﬁable enumerates all of the assignments, and returns true if one of them satisﬁes the formula. Imagine that we allow the algorithm a notion of a “lucky guess”: the algorithm is allowed to guess an assignment, and then verify whether it satisﬁes the formula. The algorithm can determine if the formula is satisﬁable simply by having one guess that works out. In other words, we assume that the algorithm asserts that the formula is in $\mathcal{L}_{3S A T}$ if there is some guess that works out. This type of computation is called a non deterministic computation . A fully formal deﬁnition requires that we introduce a range of concepts (such as Turing Machines) that are outside the scope of this book. Roughly speaking, a non deterministic decision algorithm has the following form. The ﬁrst stage is a guessing stage, where the algorithm non deterministic ally produces some guess $\gamma$ . The second stage is a deterministic verifying stage that either accepts its input $\omega$ based on $\gamma$ or not. The algorithm as a whole is said to accept $\omega$ if it accepts $\gamma$ using any one of its guesses. A decision problem $\Pi$ is in th $\mathcal{N P}$ if there exists a non deterministic algorithm that accepts an instance $\omega$ if and o ly if $\omega\in{\mathcal{L}}_{\Pi}$ ∈L , and if eriﬁcation stage can be executed in polynomial time in the length of ω . Clearly, SAT is in NP : the guesses $\gamma$ are possible assignments, and they are veriﬁed in polynomial time simply by testing whether the assignment $\gamma$ satisﬁes the input formula $\phi$ . 

Because deterministic computations are a special case of non deterministic ones, we have that $\mathcal{P}\subseteq\mathcal{N P}$ . The converse of this inclusion is the biggest open problem in computational complexity. In other words, can every problem that can be solved in polynomial time using a lucky guess also be solved in polynomial time without guessing? 

As stated, it seems impossible to get a handle on this problem: The number of problems in $\mathcal{N P}$ is potentially unlimited, and even if we ﬁnd an efcie gorithm for one problem, what does that tell us about the class in general? The notion of NP -complete problems gives us a tool for reducing this unmanageable question into a much more compact one. Roughly speaking, the class $\mathcal{N P}$ has a set of problems that are the “hardest pro s in ${\mathcal{N P}}"$ : if we can solve them in polynomial time can provably solve any problem in NP in polynomial time. These problems are known as -complete pro em 

$\mathcal{N P}$ -hard 

reduction 

Max-Clique Problem 

formally, we say that a decision problem Π is NP -hard if for every decision blem $\Pi^{\prime}$ in NP , there is a polynomial-time transformation of inputs such that an input for $\Pi^{\prime}$ belongs to ${\mathcal{L}}_{\Pi^{\prime}}$ if and only if the transformed instance belongs to ${\mathcal{L}}_{\Pi}$ . This type of transformation called a reduction of one problem o another. When we h such a reduction, any algorithm A that solves the decision problem Π can be used to solve $\Pi^{\prime}$ : We simply convert each instance of $\Pi^{\prime}$ to the corresponding nce of $\Pi$ , and apply $\mathcal{A}$ . An $\mathcal{N P}$ -hard problem be used in this way for any problem in NP . Thus, it p es a universal sol ion for any N $\mathcal{N P}$ roblem. It is possible to w that the SA blem is NP -hard. A pro Π is said to be NP -complete if it is both NP -hard and in NP . The 3-SAT problem is NP -complete, as are many other important problems. For example, the Max-Clique Problem of deciding whether an undirected graph has a clique of size at least $K$ (where $K$ is r to the algorithm) is also $\mathcal{N P}$ -hard. At the moment, it is not yet known whether P $\mathcal{P}=\mathcal{N P}$ NP . Much work has been devoted to investigating both sides of this conjecture. In particular, decades of research have been spent on failed attempts to ﬁnd polynomial-time algorithms for many $\mathcal{N P}$ -complete problems, such as SAT or Max-Clique. The lack of success suggests that probably no such algorithm exists for any $\mathcal{N P}$ ard problem, and that $\mathcal{P}\neq\mathcal{N P}$ . Thus, a standard way of showing that a part r problem Π probably is unlikely to have a polynomial time algorithm i show that it is NP - hard. In other words, we try to ﬁnd a reduction from some known NP -hard problem, such as SAT, to the problem of interest. If we construct such a reduction, then we have shown the following: If we ﬁnd a polynomial-time algorithm for $\Pi_{i}$ , we have also provided a polynomial- time algorithm for all $\mathcal{N P}$ -complete problems, and shown that $\mathcal{N P}=\mathcal{P}$ . Although this is not impossible, it is currently believed to be highly unlikely. 

 Thus, if we show that a problem is $\mathcal{N P}$ -hard, we should probably resign ourselves to algorithms that are exponential-time in the worst case. However, as we will see, there are many cases where algorithms can be exponential-time in the worst case, yet achieve signiﬁcantly better performance in practice. Because many of the problems we encounter are $\mathcal{N P}$ -hard, ﬁnding tractable cases and providing algorithms for them is where most of the interesting work takes place. 

A.3.4.3 Other Complexity Classes 

The classes $\mathcal{P}$ and $\mathcal{N P}$ are the most important and commonly used classes used to describe the computational complexity of problems, but they are only part of a rich framework used for classifying problems based on their time or space complexity. In particular, the class $\mathcal{N P}$ is only the ﬁrst level in an inﬁnite hierarchy of increasingly larger classes. Classes higher in the hierarchy might or might not be harder than the lower classes; this problem also is a major open problem in complexity theory. 

A diferent dimension along which complexity can vary relates to the existential nature of the deﬁnition of the class $\mathcal{N P}$ . A problem is in $\mathcal{N P}$ if there is some guess on which a polynomial time computation succeeds (returns true ). In our SAT example, the guesses were diferent assignments that could satisfy the formula $\phi$ deﬁned in the problem instance. However, we might want to know what fraction of the computations succeed. In our SAT example, we may want to compute the exact number (or fraction) of assignments satisfying $\phi$ . This problem is no longer a decision problem, but rather a counting problem that returns a numeric output. 

The class $\#\mathcal{P}$ is deﬁned precisely for problems that return a numerical value. Such a problem is in $\#\mathcal{P}$ P if the number can be computed as the number of accepting guesses of a non deterministic polynomial time algorithm. The problem of counting the number of satisfying assignments to a 3-SAT formula is clearly in $\#\mathcal{P}$ . Like class of $\mathcal{N P}$ -hard problems, there are problems that are at least as as any problem in $\#\mathcal{P}$ P . The problem unting satisfying assignments is the canonical $\#\mathcal{P}$ P -hard problem. This problem is clearly NP -hard: if we can solve it, we can immediately solve the 3-SAT decision problem. For trivial reasons, it is not in $\mathcal{N P}$ , because it is a counting problem, not a decision problem. However, it is generally believed that the counting version of the 3-SAT problem is inherently more difcult than the original decision problem, in that we can use them to solve problems that are “harder” than $\mathcal{N P}$ . 

Finally, another, quite diferent, complexity class is the class of randomized polynomial time algorithms — those that can be solved using a polynomial time algorithm that makes random guesses. There are several ways of deﬁning when a randomized algorithm accepts a particular input; we provide one of them. A decision problem $\Pi$ is in the class $\mathcal{R P}$ if there exists a randomized algorithm that makes a guess probabilistic ally, and then processes it in polynomial time, such that following holds: The algorithm always returns false for an input not in ${\mathcal{L}}_{\Pi}$ ; for an input in L ${\mathcal{L}}_{\Pi}$ , the algorithm returns true with probability greater than $1/2$ . Thus, the algorithm only has to get the “right” answer in half of its guesses; this requirement is much more stringent than that of non deterministic polynomial time, where the algorithm only had to ge e guess rig any problems are known to be in $\mathcal{N P}$ but are not known to be in RP . Whether NP $\mathcal{N P}=\mathcal{R P}$ RP is another important open question, where the common belief is also that the answer is no. 

# A.4 Combinatorial Optimization and Search 

# A.4.1 Optimization Problems 

optimization problem objective function 

Many of the problems we address in this book and in other settings can be formulated as an optimization problem . Here, we are given a solution space $\Sigma$ of possible solutions $\sigma$ , and an objective function $f_{\mathrm{obj}}\ :\ \Sigma\mapsto I\!\!R$ 7→ that allows us to evaluate the “quality” of each candidate solution. Our aim is then to ﬁnd the solution that achieves the maximum score: 

$$
\sigma^{*}=\arg\operatorname*{max}_{\sigma\in\Sigma}f\!\!\!\!/(\sigma).
$$ 

This optimization task is a maximization problem; we can similarly deﬁne a minimization problem, where our goal is to minimize a loss function . One can easily convert one problem to another (by negating the objective), and so, without loss of generality, we focus on maximization problems. 

Optimization problems can be discrete, where the solution space $\Sigma$ consists of a certain (ﬁnite) number of discrete hypotheses. In most such cases, this space is (at least) exponentially large in the size of the problem, and hence, for reasonably sized problems, it cannot simply be enumerated to ﬁnd the optimal solution. In other problems the solution space is continuous, so that enumeration is not even an option. 

The available tools for solving an optimization problem depend both on the form of the solution space $\Sigma$ and on the form of the objective. For some classes of problems, we can identify the optimum in terms of a closed-form expression; for others, there exist algorithms that can provably ﬁnd the optimum efciently (in polynomial time), even when the solution space is large (or inﬁnite); others are $\mathcal{N P}$ -hard; and yet others do not (yet) have any theoretical analysis of their complexity. Throughout this book, multiple optimization problems arise, and we will see examples of all of these cases. 

# A.4.2Local Search 

local search search space search state 

search operators Many optimization problems do not appear to admit tractable solution algorithms exist, and we are forced to fall back on heuristic methods that have no guarantees of actually ﬁnding the optimal solution. One such class of methods that are in common use is the class of local search methods. Such search procedures operate over a search space . A search space is a collection of candidate solutions, often called search states . Each search state is associated with a score and a set of neighboring states. A search procedure is a procedure that, starting from one state, explores search space in attempt to ﬁnd a high-scoring state. 

Local search algorithms keep track of a “current” state. At each iteration they consider several states that are “similar” to the current one, and therefore are viewed as adjacent to it in the search space. These states are often generated by a set of search operators , each of which takes a state and makes a small modiﬁcation to it. They select one of these neighboring states and make it the current candidate. These iterations are repeated until some termination condition. These local search procedures can be thought of as moving around in the solution space by taking small steps. Generally, these steps are taken in a direction that tends to improve the objective. If we assume that “similar” solutions tend to have similar values, this approach is likely to move toward better regions of the space. 

MAP assignment 

structure search 

This approach can be applied to a broad range of problems. For example, we can use it to ﬁnd a MAP assignment relative to a distribution $P$ : the space of solutions is the set of assignments $\xi$ to a set of ran m variables $\mathcal{X}$ ; the objective function is $P(\xi)$ ; and the search operators take one assignment x and change the value of one variable $X_{i}$ from $x_{i}$ to $x_{i}^{\prime}$ . As we discuss in section 18.4, it can also be used to perform structure search over the space of Bayesian network structures to ﬁnd one that optimizes a certain “goodness” function: the search space is the set of network structures, and the search operators make small changes to the current structure, such as adding or deleting an edge. 

![](images/8c2220de205088f49e64f3e80ff19d07168369c345559ab4d0031b30add31961.jpg) 

# A.4.2.1 Local Hill Climbing 

greedy hill-climbing 

ﬁrst-ascent hill climbing 

One of the simplest, and often used, search procedures is the greedy hill-climbing procedure. As the name suggests, at each step we take the step that leads to the largest improvement in the score. This is the search analogue of a continuous gradient-ascent method; see appendix A.5.2. The actual details of the procedure are shown in algorithm A.5. We initialize the search with some solution $\sigma_{0}$ . Then we repeatedly execute the following steps: We consider all of the solutions that are neighbors of the current one, and we compute their score. We then select the neighbor that leads to the best improvement in the score. We continue this process until no modiﬁcation improves the score. One issue with this algorithm is that the number of operators that can be applied may be quite large. A slight variant of this algorithm, called ﬁrst-ascent hill climbing , samples operators from $\mathcal{O}$ and evaluates them one at a time. Once it ﬁnds one that leads to better scoring network, it applies it without considering other operators. In the initial stages of the search, this procedure requires relatively few random trials before it ﬁnds such an operator. As we get closer to the local maximum, most operators hurt the score, and more trials are needed before an upward step is found (if any). 

local maximum 

plateau 

What can we say about the solution returned by Greedy-Local-Search ? From our stopping criterion, it follows that the score of this solution is no lower than that of its neighbors. This implies that we are in one of two situations. We might have reached a local maximum from which all changes are score-reducing. Except in rare cases, there is no guarantee that the local maximum we ﬁnd via local search is actually the global optimum $\sigma^{*}$ . Indeed, it may be a very poor solution. The other option is that we have reached a plateau : a large set of neighboring solutions that have the same score. By design, greedy hill-climbing procedure cannot “navigate” through a plateau, since it relies on improvement in score to guide it to better solutions. Once again, we have no guarantee that this plateau achieves the highest possible score. 

There are many modiﬁcations to this basic algorithm, mostly intended to address this problem. We now discuss some basic ideas that are applicable to all local search algorithms. We defer to the main text any detailed discussion of algorithms speciﬁc to problems of interest to us. 

# A.4.2.2 Forcing Exploration in New Directions 

basin ﬂooding One common approach is to try to escape a suboptimal convergence point by systematically exploring the region around that point with the hope of ﬁnding an “outlet” that leads to a new direction to climb up. This can be done if we are willing to record all networks we “visited” during the search. Then, instead of choosing the best operator in Line 7 of Greedy-Local-Search , we select the best operator that leads to a solution we have not visited. We then allow the search to continue even when the score does not improve (by changing the termination condition). This variant can take steps that explore new territories even if they do not improve the score. Since it is greedy in nature, it will try to choose the best network that was not visited before. To understand the behavior of this method, visualize it climbing to the hilltop. Once there, the procedure starts pacing parts of the hill that were not visited before. As a result, it will start circling the hilltop in circles that grow wider and wider until it ﬁnds a ridge that leads to a new hill. (This procedure is often called basin ﬂooding in the context of minimization problems.) 

In this variant, even when no further progress can be made, the algorithm keeps moving, trying to ﬁnd new directions. One possible termination condition is to stop when no progress has been made for some number of steps. Clearly, the ﬁnal solution produced should not necessarily be the one at which the algorithm stops, but rather the best solution found anywhere during the search. Unfortunately, the computational cost of this algorithm can be quite high, since it needs to keep track of all solutions that have been visited in the past. 

tabu search 

Tabu search is a much improved variant of this general idea utilizes the fact that the steps in our search space take the form of local modiﬁcations to the current solution. In tabu search, we keep a list not of solutions that have been found, but rather of operators that we have recently applied. In each step, we do not consider operators that reverse the efect of operators applied within a history window of some predetermined length $L$ . Thus, if we ﬂip a variable $X_{i}$ from $x_{i}$ to $x_{i}^{\prime}$ , we cannot ﬂip it back in the next $L$ steps. These restrictions force the search procedure i to explore new directions in the search space, instead of tweaking with the same parts of the solution. The size $L$ determines the amount of memory retained by the search. 

The tabu search procedure is shown in algorithm A.6. The “tabu list” is the list of operators 

# Algorithm A.6 Local search with tabu list 

Procedure LegalOp ( o , // Search operator to check TABU // List of recently applied operators ) 1 if exists ${\cal{o}}^{\prime}\in\mathit{T A B U}$ such that $O$ reverses $o^{\prime}$ then return false 2 else return true 3 Procedure Tabu-Structure-Search ( $\sigma_{0}$ , $//$ initial candidate solution score , // Score $\mathcal{O}$ // A set of search operators $L$ , // Size of tabu list $N$ , $//$ Stopping criterion ) 1 $\begin{array}{r l}&{\sigma_{\mathrm{best}}\leftarrow\sigma_{0}}\\ &{\sigma\leftarrow\sigma_{\mathrm{best}}}\\ &{t\leftarrow1}\end{array}$ 2 3 4 LastImprovement $t\gets\ 0$ 5 while LastImprovement $<N$ 6 $\boldsymbol{o}^{(t)}\leftarrow\ \epsilon$ // Set current operator to be uninitialized 7 for e $o\in\mathcal{O}$ // Search for best allowed operator 8 if $\mathsf{L e g a l o p}(o,\{o^{(t-L)},.\,.\,,o^{(t-1)}\})$ { } ) then 9 $\sigma_{o}\gets\ o(\sigma)$ 10 if $\sigma_{o}$ is legal solution then 11 if $o^{(t)}\overset{\vartriangle}{=}\epsilon$ or $\mathrm{score}(\sigma_{o})>\mathrm{score}(\sigma_{o^{t}})$ then 12 $\mathbf{\Omega}_{O}(t)\gets\mathbf{\Omega}_{O}$ 13 $\sigma\gets\;\sigma_{o^{t}}$ 14 if $\mathrm{score}(\sigma)>\mathrm{score}(\sigma_{\mathrm{best}})\ \mathbf{th}$ en 15 $\sigma_{\mathrm{best}}\gets\ \sigma_{o}$ 16 LastImprovement ← 0 17 else 18 LastImprovement $\leftarrow$ LastImprovement + 1 19 $t\gets\ t+1$ 20 21 return σ best 

applied in the last $L$ steps. The procedure LegalOp checks if a new operator is legal given the current tabu list. The implementation of this procedure depends on the exact nature of operators we use. As in the basin-ﬂooding approach, tabu search does not stop when it reaches a solution that cannot be improved, but rather continues the search with the hope of reaching a better structure. If this does not happen after a prespeciﬁed number of steps, we decide to abandon the search. 

![](images/0267eb7f6d082f3ebefea349091bd201ab1c020f410ec2b462e2b99cfe1d8b1f.jpg) 

beam search 

Another variant that forces a more systematic search of the space is beam search . In beam search, we conduct a hill-climbing search, but we keep track of a certain ﬁxed number $K$ of states. The value $K$ is called the beam width . At each step in the search, we take all of the current states and generate and evaluate all of their successors. The best $K$ are kept, and the algorithm repeats. The algorithm is shown in algorithm A.7. Note that with a beam width of 1, beam search reduces to greedy hill-climbing search, and with an inﬁnite beam width, it reduces to breadth-ﬁrst search. Note that this version of beam search assumes that the (best) steps taken during the search always improve the score. If that is not the case, we would also have to compare the current states in our beam Beam to the new candidates in $H$ in order to determine the next set of states to put in the beam. The termination condition can be an upper bound on the number of steps or on the improvement achieved in the last iteration. 

# A.4.2.3 Randomization in Search 

randomization Another approach that can help in reducing the impact of local maxima is randomization . Here, multiple approaches exist. We note that most randomization procedures can be applied as a wrapper to a variety of local search algorithm, including both hill climbing and tabu search. Most simply, we can initialize the algorithm at diferent random starting points, and then use a hill-climbing algorithm from each one. Another strategy is to interleave random steps and hill-climbing steps. Here, many strategies are possible. In one approach, we can “revitalize” the search by taking the best network found so far and applying several randomly chosen operators 

![](images/b622bf9333a2fbb53c8374fea5b312b7eabc64334f2c5f69df87298930f44ce1.jpg) 

random restart to get a network that is fairly similar, yet perturbed. We then restart our search procedure from the new network. If we are lucky, this random restart step moves us to a network that belongs to a better “basin of attraction,” and thus the search will converge to a better structure. A simple random restart procedure is shown in algorithm A.8; it can be applied as wrapper to plain hill climbing, tabu search, or any other search algorithm. 

This approach can be efective in escaping from fairly local maxima (which can be thought of as small bumps on the slope of a larger hill). However, it is unlikely to move from one wide hill to another. There are diferent choices in applying random restart, the most important one is how many random “steps” to take. If we take too few, we are unlikely to escape the local maxima. If we take too many, than we move too far of from the region of high scoring network. One possible strategy is to applying random restarts of growing magnitude. That is, each successive random restart applies more random operations. 

simulated annealing 

To make this method concrete, we need a way of determining how to apply random restarts, and how to interleave hill-climbing steps and randomized moves. A general framework for doing is simulated annealing . The basic idea of simulated annealing is similar to Metropolis-Hastings MCMC methods that we discuss in section 12.3, and so we only brieﬂy touch it. 

In broad outline, the simulated annealing procedure attempts to mix hill-climbing steps with temperature parameter 

proposal distribution moves that can decrease the score. This mixture is controlled by a so-called temperature parameter . When the temperature is “hot,” the search tries many moves that decrease the score. As the search is annealed (the temperature is slowly reduced) it starts to focus only on moves that improve the score. The intuition is that during the “hot” phase the search explores the space and eventually gets trapped in a region of high scores. As the temperature reduces it is able to distinguish between ﬁner details of the score “landscape” and eventually converge to a good maximum. 

To carry out this intuition, a simulated annealing procedure uses a proposal distribution over operators to propose candidate operators to apply in the search. At each step, the algorithm selects an operator $O$ using this distribution, and evaluates $\delta(o)$ — the change in score incurred by applying $O$ at the current state. The search accepts this move with probability $\operatorname*{min}(1,e^{\frac{\delta(o)}{\tau}})$ , where $\tau$ is the current temperature. Note that, if $\delta(o)>0$ , the move is automatically accepted. If $\delta(o)<0$ , the move is accepted with probability that depends both on the decrease in score and on the temperature $\tau$ . For large value of $\tau$ (hot) all moves are applied with probability close to 1 . For small values of $\tau$ (cold), all moves that decrease the score are applied with small probability. The search procedure anneals $\tau$ every ﬁxed number of move attempts. There are various strategies for annealing; the simplest one is simply to have $\tau$ decay exponentially. One can actually show that, if the temperature is annealed sufciently slowly, simulated annealing converges to the globally optimal solution with high probability. However, in practice, this “guaranteed” annealing schedule is both unknown and much too slow to be useful in practice. In practice, the success of simulated annealing depends heavily on the design of the proposal distribution and annealing schedule. 

# A.4.3 Branch and Bound Search 

branch and bound 

Here we discussed one class of solutions to discrete optimization problems: the class of local hill-climbing search. Those methods are very broadly useful, since they apply to any discrete optimization problem for which we can deﬁne a set of search operators. In some cases, however, we may know some additional structure within the problem, allowing more informed methods to be applied. One useful type of information is a mechanism that allows us to evaluate a partial assignment $\mathbf{\mathcal{Y}}_{1\ldots i},$ and to place a bound bound $(\boldsymbol{y}_{1\ldots i})$ on the best score of any complete assignment that extends $\mathbf{\mathit{y}}_{1...i}$ . In this case, we can use an algorithm called branch and bound search , shown in algorithm A.9 for the case of a maximization problem. 

Roughly speaking, branch and bound searches the space of partial assignments, beginning with the empty assignment, and assigning the variables $X_{1},\dots,X_{n},$ one at a time (in some order), using depth-ﬁrst search. At each point, when considering the current partial assignment

 $\mathbf{\mathcal{Y}}_{1\dots i}$ , the algorithm evaluates it using bound $(\boldsymbol{y}_{1\ldots i})$ and compares it to the best full assignment

 $\xi$ found so far. If $\operatorname{score}(\xi)$ is better than the best score that can possibly be achieved starting from $\mathbf{\mathit{y}}_{1...i},$ then there is no point continuing to explore any of those assignments, and the algorithm backtracks to try a diferent partial assignment. Because the bound is correct, it is not difcult to show that the assignments that were pruned without being searched cannot possibly be optimal. When the bound is reasonably tight, this algorithm can be very efective, pruning large parts of the space without searching it. 

The algorithm shows the simplest variant of the branch-and-bound procedure, but many extensions exist. One heuristic is to perform the search so as to try and ﬁnd good assignments 

![](images/cdfa6f5dc793aa5178fff2bb76456e1a8eec93d0f4ea94c20d89521f063a9c40.jpg) 

early. The better the current assignment $\sigma_{\mathrm{best}}$ , the better we can prune suboptimal trajectories. Other heuristics intelligently select, at each point in the search, which variable to assign next, allowing this choice to vary across diferent points in the search. When available, one can also use a lower bound as well as an upper bound, allowing pruning to take place based on partial (not just full) trajectories. Many other extensions exist, but are outside the scope of this book. 

# A.5 Continuous Optimization 

In the preceding section, we discussed the problem of optimizing an objective over a discrete space. In this section we brieﬂy review methods for solving optimization problems over a continuous space. See Avriel (2003); Bertsekas (1999) for more thorough discussion of nonlin- ear optimization, and see Boyd and Vandenberghe (2004) for an excellent overview of convex optimization methods. 

# A.5.1 Characterizing Optima of a Continuous Function 

At several points in this book we deal with maximization (or minimization) problems. In these problems, we have a function $f_{\mathrm{obj}}(\theta_{1},\cdot\cdot\cdot,\theta_{n})$ for several parameters , and we wish to ﬁnd joint values of the parameters that maximizes the value of $f_{\mathrm{obj}}$ . 

Formally, we face the following problem: 

Find values $\theta_{1},.\cdot\cdot\,,\theta_{n}$ such that f $f_{\mathrm{obj}}(\theta_{1},.\,.\,,\theta_{n})=\mathrm{max}_{\theta_{1}^{\prime},\ldots,\theta_{n}^{\prime}}\,f_{\mathrm{obj}}(\theta_{1}^{\prime},.\,.\,.\,,\theta_{n}^{\prime}).$ 

# Example A.2 

Assume we are given a set of points $(x[1],y[1]),\.\.\ ,(x[m],y[m])$ . Our goal is to ﬁnd the “centroid” of these points, deﬁned as a point $(\theta_{x},\theta_{y})$ that minimizes the square distance to all of the points. We can formulate this problem into a maximization problem by considering the negative of the sum of squared distances: 

$$
f_{\mathrm{obj}}(\theta_{x},\theta_{y})=-\sum_{i}\left((x[i]-\theta_{x})^{2}+(y[i]-\theta_{y})^{2}\right).
$$ 

gradient 

Theorem A.5 

stationary point 

One way of ﬁnding the maximum of a function is to use the fact that, at the maximum, the gradient of the function is 0 . Recall that the gradient of a function $f_{\mathrm{obj}}(\theta_{1},\cdot\cdot\cdot,\theta_{n})$ is the vector of partial derivatives 

$$
\nabla f=\left\langle{\frac{\partial f}{\partial\theta_{1}}},\ldots\cdot{\frac{\partial f}{\partial\theta_{n}}}\right\rangle.
$$ 

If $\langle\theta_{1},.\cdot\cdot,\theta_{n}\rangle$ is an interior maximum point of $f_{\mathrm{obj}}$ , then 

$$
\nabla f(\theta_{1},.\,.\,.\,,\theta_{n})=0.
$$ 

This property, however, does not characterize maximum points. Formally, a point $\langle\theta_{1},.\cdot\cdot,\theta_{n}\rangle$ where $\nabla f_{\mathrm{obj}}(\theta_{1},\dots,\theta_{n})=0$ is a stationary point of $f_{\mathrm{obj}}$ . Such a point can be either a local maximum, a local minimum, or a saddle point. However, ﬁnding such a point can often be the ﬁrst step toward ﬁnding a maximum. 

To satisfy the requirement that $\nabla f=0$ we need to solve the set of equations 

$$
{\frac{\partial}{\partial\theta_{k}}}f_{\mathrm{obj}}(\theta_{1},\dots,\theta_{n})=0\,\,\,\,\,\,\,\,\,k=1,\dots,n
$$ 

Example A.3 Consider the task of example A.2. We can easily verify that: 

$$
{\frac{\partial}{\partial\theta_{x}}}f_{\mathrm{obj}}(\theta_{x},\theta_{y})=2\sum_{i}(x[i]-\theta_{x}).
$$ 

Equating this term to 0 and performing simple arithmetic manipulations, we get the equation: 

$$
\theta_{x}=\frac{1}{m}\sum_{i}x[i].
$$ 

The exact same reasoning allows us to solve for $\theta_{y}$ . 

In this example, we conclude that $f_{\mathrm{obj}}$ has a unique stationary point. We next need to verify that this point is a maximum point (rather than a minimum or a saddle point). In our example, we can check that, for any sequence that extends from the origin to inﬁnity (that is, $\theta_{x}^{2}+\bar{\theta}_{y}^{2}\rightarrow\infty)$ ), we have $f_{\mathrm{obj}}\to-\infty$ . Thus, the single stationary point is a maximum. 

In general, to verify that the stationary point is a maximum, we can check that the second derivative is negative. To see this, recall the multivariate Taylor expansion of degree two: 

$$
f_{\mathrm{obj}}(\vec{\theta})=f_{\mathrm{obj}}(\vec{\theta_{0}})+(\vec{\theta}-\vec{\theta_{0}})^{T}\nabla f_{\mathrm{obj}}(\vec{\theta_{0}})+\frac{1}{2}\left[\vec{\theta}-\vec{\theta_{0}}\right]^{T}A(\vec{\theta}_{0})\left[\vec{\theta}-\vec{\theta_{0}}\right],
$$ 

Hessian 

negative deﬁnite 

Theorem A.6 

Example A.4 

where $A(\ensuremath{\vec{\theta}}_{0})$ is the Hessian — the matrix of second derivatives at $\ensuremath{\vec{\theta}}_{0}$ . If we use this expansion around a stationary point, then the gradient is 0, and we only need to examine the term $[\vec{\theta}-\vec{\theta_{0}}]^{T}A(\vec{\theta_{0}})[\vec{\theta}-\dot{\vec{\theta_{0}}}]$ − − . In the univariate case, we can verify that a point is a local maximum by testing that the second derivative is negative. The analogue to this condition in the multivariate case is that $A(\ensuremath{\vec{\theta}}_{0})$ is negative deﬁnite at the point $\ensuremath{\vec{\theta_{0}}}$ , that is, that $\ensuremath{\vec{\theta}}^{T}A(\ensuremath{\vec{\theta}}_{0})\ensuremath{\vec{\theta}}<0$ for all $\vec{\theta}$ . 

Suppose $\overline{{\vec{\theta}=\langle\theta_{1},.\,.\,.\,,\theta_{n}\rangle}}$ ⟩ is an interior point of $f_{\mathrm{obj}}$ with $\nabla\vec{\theta}=0$ . Let $\begin{array}{r}{A(\vec{\theta})=\{\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}f_{\mathrm{obj}}(\vec{\theta})\}}\end{array}$ be the Hessian matrix of second derivatives of $f_{\mathrm{obj}}$ at $\vec{\theta.}$ . $A(\vec{\theta})$ is negative deﬁnite at $\theta$ if and only if $\vec{\theta}$ is a local maximum of $f_{\mathrm{obj}}$ . 

Continuing example A.2, we can verify that 

$$
\begin{array}{r c l}{{\displaystyle\frac{\partial^{2}}{\partial\theta_{x}}f_{\mathrm{obj}}(\theta_{x},\theta_{y})}}&{{=}}&{{-2m}}\\ {{\displaystyle\frac{\partial^{2}}{\partial\theta_{y}^{2}}f_{\mathrm{obj}}(\theta_{x},\theta_{y})}}&{{=}}&{{-2m}}\\ {{\displaystyle\frac{\partial^{2}}{\partial\theta_{y}\partial\theta_{x}}f_{\mathrm{obj}}(\theta_{x},\theta_{y})}}&{{=}}&{{0}}\\ {{\displaystyle\frac{\partial^{2}}{\partial\theta_{x}\partial\theta_{y}}f_{\mathrm{obj}}(\theta_{x},\theta_{y})}}&{{=}}&{{0.}}\end{array}
$$ 

$$
A=\left(\begin{array}{r r}{{-2m}}&{{0}}\\ {{0}}&{{-2m}}\end{array}\right).
$$ 

It is easy to verify that this matrix is negative deﬁnite. 

# A.5.2 Gradient Ascent Methods 

The characterization of appendix A.5.1 allows us to provide closed-form solutions for certain continuous optimization problems. However, there are many problems for which such solutions cannot be found. In these problems, the equations posed by $\nabla f_{\mathrm{obj}}(\pmb\theta)\,=\,0$ do not have an analytical solution. Moreover, in many practical problems, there are multiple local maxima, and then this set of equations does not even have a unique solution. 

One approach for dealing with problems that do not yield analytical solutions is to search for a (local) maximum. The idea is very analogous to the discrete local search of appendix A.4.2: We begin with an initial point $\theta^{0}$ , which can be an arbitrary choice, a random guess, or an approximation of the solution based on other considerations. From this starting point, we want to “climb” to a maximum. A great many techniques roughly follow along these lines. In this section, we survey some of the most common ones. 

# A.5.2.1 Gradient Ascent 

![](images/3d8461ca95b8be4fd7132c41a4625c0f3f12017e7e9e44df3b5dc821051291f0.jpg) 

search of algorithm A.5 (see appendix A.4.2). Using the Taylor expansion of a function, we know that, in the neighborhood of $\bar{\theta}^{\bar{0}}$ , the function can be approximated by the linear equation 

$$
f_{\mathrm{obj}}(\pmb\theta)\approx f_{\mathrm{obj}}(\pmb\theta^{0})+(\pmb\theta-\pmb\theta^{0})^{T}\nabla f_{\mathrm{obj}}(\pmb\theta^{0}).
$$ 

Using basic properties of linear algebra, we can check that the slope of this linear function, that is, $\bar{\nabla f}_{\mathrm{{obj}}}(\boldsymbol{\theta}^{\bar{0}})$ , points to the direction of the steepest ascent. This observation suggests that, if we take a step in the direction of the gradient, we increase the value of $f_{\mathrm{obj}}$ . This reasoning leads to the simple gradient ascent algorithm shown in algorithm A.10. Here, $\eta$ is a constant that determines the rate of ascent at each iteration. Since the gradient $\nabla f_{\mathrm{obj}}$ approaches 0 as we approach a maximum point, the procedure will converge if $\eta$ is sufciently small. 

Note that, in order to apply gradient ascent, we need to be able to evaluate the function $f_{\mathrm{obj}}$ at diferent points, and also to evaluate its gradient. In several examples we encounter in this book, we can perform these calculations, although in some cases these are costly. Thus, a major objective is to reduce the number of points at which we evaluate $f_{\mathrm{obj}}$ or $\nabla f_{\mathrm{obj}}$ . 

The performance of gradient ascent depends on the choice of $\eta$ . If $\eta$ is too large, then the algorithm can “overshoot” the maximum in each iteration. For sufciently small value of $\eta,$ , the gradient ascent algorithm will converge, but if $\eta$ is too small, we will need many iterations to converge. Thus, one of the difcult points in applying this algorithm is deciding on the value of $\eta$ . Indeed, in practice, one typically needs to begin with a large $\eta$ , and decrease it over time; this approach leaves us with the problem of choosing an appropriate schedule for shrinking $\eta$ . 

# A.5.2.2 Line Search 

An alternative approach is to adaptively choose the step size $\eta$ at each step. The intuition is that we choose a direction to climb and continue in that direction until we reach a point where we start to descend. In this procedure, at each point $\theta^{t}$ in the search, we deﬁne a “line” in the direction of the gradient: 

$$
g(\eta)=\vec{\theta^{t}}+\eta\nabla f_{\mathrm{obj}}(\pmb{\theta}^{t}).
$$ 

line search We now use a line search procedure to ﬁnd the value of $\eta$ that deﬁnes a (local) maximum of 

![](images/39d1fbd812297fb58ff240f182716cd038741eecbd8436e63c93fc3fe28b6f48.jpg) 
Figure A.2 Illustration of line search with Brent’s method. The solid line shows a one-dimensional function. The three points, $\eta_{1},\,\eta_{2}$ , and $\eta_{3}$ , bracket the maximum of this function. The dashed line shows the quadratic ﬁt to these three points and the choice of $\eta^{\prime}$ proposed by Brent’s method. 

$f_{\mathrm{obj}}$ along the line; that is, we ﬁnd: 

$$
\eta^{t}=\arg\operatorname*{max}_{\eta}g(\eta).
$$ 

We now take an $\eta^{t}$ -sized step in the direction of the gradient; that is, we deﬁne: 

$$
\pmb\theta^{t+1}\leftarrow\ \pmb\theta^{t}+\eta^{t}\nabla f_{\mathrm{obj}}(\pmb\theta^{t}).
$$ 

And the process repeats. 

There are several methods for performing the line search. The basic idea is to ﬁnd three points $\eta_{1}~<~\eta_{2}~<~\eta_{3}$ so that $f_{\mathrm{obj}}(g(\eta_{2}))$ is larger than both $f_{\mathrm{obj}}(g(\eta_{1}))$ and $f_{\mathrm{obj}}(g(\eta_{3}))$ . In this case, we know that there is at least one local maximum between $\eta_{1}$ and $\eta_{3}$ , and we say that $\eta_{1},\eta_{2}$ and $\eta_{3}$ bracket a maximum; see ﬁgure A.2 for an illustration. Once we have a method for ﬁnding a bracket, we can zoom in on the maximum. If we choose a point $\eta^{\prime}$ so that $\eta_{1}<\eta^{\prime}<\eta_{2}$ we can ﬁnd a new, tighter, bracket. To see this, we consider the two possible cases. If $f_{\mathrm{obj}}\big(g(\eta^{\prime})\big)>f_{\mathrm{obj}}\big(g(\eta_{2})\big)$ , then $\eta_{1},\eta^{\prime},\eta_{2}$ bracket a maximum. Alternatively, if $f_{\mathrm{obj}}\big(g(\eta^{\prime})\big)\leq f_{\mathrm{obj}}\big(g(\eta_{2})\big)$ , then $\eta^{\prime},\eta_{2},\eta_{3}$ bracket a maximum. In bot ases, the new bracket is smaller than the original one. Similar reasoning applies if we choose $\eta^{\prime}$ between $\eta_{2}$ and $\eta_{3}$ . 

The question is how to choose $\eta^{\prime}$ . One approach is to perform a binary search and choose $\eta^{\prime}=(\eta_{1}+\eta_{3})/2$ . This ensures that the size of the new bracket is half of the old one. A faster approach, known as Brent’s method , ﬁts a quadratic function based on the values of $f_{\mathrm{obj}}$ at the three points $\eta_{1},\;\eta_{2}$ , and $\eta_{3}$ . We then choose $\eta^{\prime}$ to be the maximum point of this quadratic approximation. See ﬁgure A.2 for an illustration of this method. 

# A.5.2.3 Conjugate Gradient Ascent 

Line search attempts to maximize the improvement along the direction deﬁned by $\nabla f_{\mathrm{obj}}(\pmb\theta^{t})$ . This approach, however, often has undesired consequences on the convergence of the search. To understand the problem, we start by observing that $\nabla f_{\mathrm{obj}}(\pmb\theta^{t+1})$ must be orthogonal to 

![](images/b4205ecce3ccedd685db4db620dd60f3a3768050183b9a1452c5445a14ede5d0.jpg) 
Figure A.3 Two examples of the convergence problem with line search. The solid line shows the progression of gradient ascent with line search. The dashed line shows the progression of the conjugate a quadratic function $f_{\mathrm{obj}}(x,y)\,=\,-(x^{2}+10y^{2}).$ ; (b) its exponential $f_{\mathrm{obj}}(x,y)=$ $\mathrm{exp}\{-(x^{2}+10y^{2})\}$ {− } . In both cases, the two search procedures start from the same initial point (bottom left of the ﬁgure), and diverge after the ﬁrst line search. 

$\nabla f_{\mathrm{obj}}(\pmb\theta^{t})$ . To see why, observe that $\pmb{\theta}^{t+1}$ was chosen to be a local maximum along the

 $\nabla f_{\mathrm{obj}}(\pmb\theta^{t})$ direction. Thus, the gradient of $f_{\mathrm{obj}}$ at $\pmb{\theta}^{t+1}$ must be 0 in this direction. This implies that the two consecutive gradient vectors are orthogonal. As a consequence, the progress of the gradient ascent will be in a zigzag line. As the procedure approaches a maximum point, the size of each step becomes smaller, and the progress slows down. See ﬁgure A.3 for an illustration of this phenomenon. 

A possible solution is to “remember” past directions of search and to bias the new direction to be a combination of the gradient at the current point and the direction implied by previous steps. This intuitive idea can be developed into a variety of algorithms. It turns out, however, that one variant of this algorithm can be shown to be optimal for ﬁnding the maximum of quadratic functions. Since, by the Taylor expansion, all functions are approximately quadratic in the neighborhood of a maximum, it follows that the ﬁnal steps of the algorithm will converge to a maximum relatively quickly. 

conjugate gradient ascent 

The algorithm, known as conjugate gradient ascent , is shown in algorithm A.11. The vector $h^{t}$ is the “corrected” direction for search. It combines the gradient $g^{t}$ with the previous direction of search $h^{t-1}$ . The efect of previous search directions on the new one depends on the relative sizes of the gradients. 

If our function $f_{\mathrm{obj}}$ is a quadratic function, the conjugate gradient ascent procedure is guaranteed to converge in $n$ steps, where $n$ is the dimension of the space. Indeed, in ﬁgure A.3a we see that the conjugate method converges in two steps. When the function is not quadratic, conjugate gradient ascent might require more steps, but is still much faster than standard gradient ascent. For example, in ﬁgure A.3b, it converges in four steps (the last step is too small to be visible in the ﬁgure). 

Finallly, we note that gradient ascent is the continuous analogue of the local hill-climbing approaches described in section A.4.2. As such, it is susceptible to the same issues of local maxima and plateaus. The approaches used to address these issues in this setting are similar to those outlined in the discrete case. 

![](images/3e7965bbae9a7e9b3acbe8245bcb96ef39938b4fb8815add074aef9616d374f1.jpg) 

# A.5.3 Constrained Optimization 

constrained optimization 

Example A.5 In appendix A.5.1, we considered the problem of optimizing a continuous function over its entire domain (see also appendix A.5.2). In many cases, however, we have certain constraints that the desired solution must satisfy. Thus, we have to optimize the function within a constrained space. We now review some basic methods that address this problem of constrained optimization . 

Suppose we want to ﬁnd the maximum entropy distribution over a variable $X$ , with ${\mathit{V a l}}(X)=$ $\{x^{1},\cdot\cdot\cdot,x^{K}\}$ . Consider the entropy of $X$ : 

$$
H(X)=-\sum_{k=1}^{K}P(x^{k})\log P(x^{k}).
$$ 

We can maximize this function using the gradient method by treating each $P(x^{k})$ as a separate parameter $\theta_{k}$ . We compute the gradient of $H_{P}(X)$ with respect to each of these parameters: 

$$
{\frac{\partial}{\partial\theta_{k}}}H(X)=-\log(\theta_{k})-1.
$$ 

Setting this partial derivative to $O_{i}$ , we get that $\log(\theta_{k})=-1$ , and thus $\theta_{k}=1/2$ . This solution seems ﬁne until we realize that the numbers do not sum up to 1, and hence our solution does not deﬁne a probability distribution! 

The ﬂaw in our analysis is that we want to maximize the entropy subject to a constraint on eters, namely, $\textstyle\sum_{k}\theta_{k}=1$ . In addition, we also remember that we need to require that $\theta_{k}\geq0$ ≥ . In this case we see that the gradient drives the solution away from from $0\;(\!-\log(\theta_{k})\rightarrow\infty$ as $\theta_{k}\rightarrow0$ ), and thus we do not need to enforce this constraint actively. 

equality constraint 

Problems of this type appear in many settings, where we are interested in maximizing a function $\pmb{f}$ under a set of equality constraints . This problem is posed as follows: 

$$
\begin{array}{r l r}{c_{1}(\pmb\theta)}&{{}=}&{0}\\ {}&{{}}&{\ddots}\\ {c_{m}(\pmb\theta)}&{{}=}&{0.}\end{array}
$$ 

Note that any equality constraint (such as the one in our example above) can be rephrased as constraining a function $c$ to 0 . Formally, we are interested in the behavior of $\pmb{f}$ in the region of points that satisﬁes all the constraints 

$$
\mathcal{C}=\{\pmb{\theta}:\forall j=1,.\,.\,.\,,n,\,c_{j}(\pmb{\theta})=0\}.
$$ 

To deﬁne our goal, remember that we want to ﬁnd a maxima point within $\mathcal{C}$ . Since $\mathcal{C}$ is a constrained “surface” we need to adopt the basic deﬁnition of maxima (and similarly minima, stationary point, etc.) to this situation. We can deﬁne local maxima in two ways. The ﬁrst deﬁniti n term of neighborhood. We deﬁne the $\epsilon$ -nei borhood of $\theta$ in $\mathcal{C}$ to e all the points θ $\theta^{\prime}\,\in\,\mathcal{C}$ ∈C such that $\|\theta-\theta^{\prime}\|_{2}\,<\,\epsilon$ | − | | . W then s y that θ is a local maxima in C if there is an $\epsilon>0$ such that ${\pmb f}({\boldsymbol{\theta}})>{\pmb f}({\pmb\theta}^{\prime})$ for all θ $\theta^{\prime}$ in its ϵ -neighborhood. An alternative deﬁnition that will be easier for the following is in terms of derivatives. Recall that a stationary point (local maximum, local minimum, or a saddle point) of a function if the derivative is 0 . In the constraint case we have a similar deﬁnition, but we must ensure that the derivatives are ones that do not take us outside the constrained surface. Stated diferently, if we consider a derivative in the direction $\delta$ , we want to ensure that the constraints remain 0 if we take a small step in direction $\delta$ . Formally, this means that the derivative has to be tangent to each constraint $c_{i}$ , that is $\delta^{T}\nabla c_{i}(\theta)=0$ . 

Lagrange multipliers 

Theorem A.7 

A general approach to solving such constrained optimization problems is the method of Lagrange multipliers . We deﬁne a new function, called the Lagrangian , of $\theta$ and of a new vector of parameters $\pmb{\lambda}=\langle\lambda_{1},.\,.\,.\,,\lambda_{m}\rangle$ 

$$
\mathcal{I}(\pmb{\theta},\pmb{\lambda})=\pmb{f}\!\!\!\!/(\pmb{\theta})-\sum_{j=1}^{m}\lambda_{j}c_{j}(\pmb{\theta}).
$$ 

If $\langle\theta,\lambda\rangle$ is a stationary point of the Lagrangian $\mathcal{I}$ , then $\theta$ is a stationary point of f f subject to the constraints $c_{1}(\pmb\theta)=0,.\;.\;.\;,c_{m}(\pmb\theta)=0$ . 

Proof We brieﬂy outline the proof. A formal proof requires the use of more careful tools from functional analysis. 

We start by showing that $\theta$ satisﬁes the constraints. Since $\langle\theta,\lambda\rangle$ is a stationary point of $\mathcal{I}$ , we have that for each $j$ 

$$
\frac{\partial}{\partial\lambda_{j}}\mathscr{I}(\pmb{\theta},\pmb{\lambda})=-c_{j}(\pmb{\theta}).
$$ 

Thus, at stationary points of $\mathcal{I}$ , the constraint $c_{j}(\theta)=0$ must be satisﬁed. Now consider $\nabla\!f\!\!\left(\theta\right)$ . For each component $\theta_{i}$ of θ , we have that 

$$
0={\frac{\partial}{\partial\theta_{i}}}{\mathcal{I}}(\theta,\lambda)={\frac{\partial}{\partial\theta_{i}}}{\pmb{\hat{f}}}(\theta)-\sum_{j}\lambda_{j}{\frac{\partial}{\partial\theta_{i}}}c_{j}(\theta).
$$ 

Thus, 

$$
\nabla\pmb{f}(\theta)=\sum_{j}\lambda_{j}\nabla c_{j}(\theta).
$$ 

In other words, the gradient of $\pmb{f}$ is a linear combination of the gradients of $c_{j}$ . 

We now use this property to prove that $\theta$ is a stationary point of $\pmb{f}$ when constrained to region $\mathcal{C}$ . Consider a direction $\delta$ that is tangent to the region $\mathcal{C}$ at $\theta$ . As $\delta$ is tangent to $\mathcal{C}$ , we expect that moving inﬁnitesimally in this direction will maintain the constraint that $c_{j}$ is 0 ; that is, $c_{j}$ should not change its value when we move in this direction. More formally, the derivative of $c_{j}$ the direction $\delta$ is 0 . The derivative of $c_{j}$ in a direction $\delta$ is $\delta^{T}\nabla c_{j}$ . Thus, if $\delta$ is tangent to $\mathcal{C}$ C , we have 

$$
\delta^{T}\nabla c_{j}(\pmb\theta)=0
$$ 

for all $j$ . Using equation (A.6), we get 

$$
\pmb{\delta}^{T}\nabla\pmb{f}(\pmb{\theta})=\sum_{j}\lambda_{j}\pmb{\delta}^{T}\nabla c_{j}(\pmb{\theta})=0.
$$ 

Thus, the derivative of $\pmb{f}$ in a direction at is tangent to $\mathcal{C}$ is 0 . This implies th t when moving away from $\theta$ within the allow d region C the value of $\pmb{f}$ has 0 derivative. Thus, θ is a stationary point of $\pmb{f}$ when restricted to C . 

We also have the converse property: If $\pmb{f}$ satisﬁes some regularity conditions, then for every stationary point of $\pmb{f}$ in $\mathcal{C}$ there is a choice of $\lambda$ so that $\langle\theta,\lambda\rangle$ is a stationary point of $\mathcal{I}$ . 

We see that the Lagrangian construction allows us to solve constrained optimization problems using tools for unconstrained optimization. We note that a local maximum of $\pmb{f}$ always corresponds to a stationary point of $\mathcal{I}$ , but this stationary point is not necessarily a local maximum of $\mathcal{I}$ . If, however, we restrict attention to nonnega e constraint functions $c_{i}$ , then a local maximum of $\pmb{f}$ must correspond to a local maximum of J . 

We now consider two examples of using this technique. 

Let us return to example A.5. In order to ﬁnd the maximum entropy distribution over $X$ , we need to solve the Lagrangian 

$$
\mathcal{I}=-\sum_{k}\theta_{k}\log\theta_{k}-\lambda\left(\sum_{k}\theta_{k}-1\right).
$$ 

Setting $\nabla\mathcal{T}=0$ implies the following system of equations: 

$$
\begin{array}{r c l}{{0}}&{{=}}&{{-\log\theta_{1}-1-\lambda}}\\ {{}}&{{}}&{{}}\\ {{0}}&{{=}}&{{-\log\theta_{K}-1-\lambda}}\\ {{0}}&{{=}}&{{\displaystyle\sum_{k}\theta_{k}-1.}}\end{array}
$$ 

Each of the ﬁrst $K$ equations can be rewritten as $\theta_{k}\,=\,2^{-1-\lambda}$ . Plugging this term into the last equation, we get that $\lambda=\log(K)-1$ , and thus $P(x^{k})=1/K$ . We conclude that we achieve maximum entropy with the uniform distribution. 

To see an example with more than one constraint, consider the following problem. 

# Example A.7 

M-projection Suppose we have a distribution $P(X,Y)$ over two random variables, and we want to ﬁnd the closest distribution $Q(X,Y)$ in which $X$ is independent of $Y$ . As we discussed in section 8.5, this process is called M-projection (see deﬁnition 8.4). Since $X$ and $Y$ are independent in $Q$ , we must have that $Q(X,Y)=Q(X)Q(Y)$ . Thus, we are searching for parameters $\theta_{x}=Q(x)$ and $\theta_{y}=Q(y)$ for diferent values $x\in V a l(X)$ and $y\in V a l(Y)$ . 

Formally, we want to solve the following problem: 

Find $\{\theta_{x}:x\in V a l(X)\}$ and $\{\theta_{y}:y\in V a l(y)\}$ that minimize 

$$
D(P(X,Y)\|Q(X)Q(Y))=\sum_{x}\sum_{y}P(x,y)\log\frac{P(x,y)}{\theta_{x}\theta_{y}},
$$ 

subject to the constraints 

$$
\begin{array}{r c l}{{0}}&{{=}}&{{\displaystyle\sum_{x}\theta_{x}-1}}\\ {{0}}&{{=}}&{{\displaystyle\sum_{y}\theta_{y}-1.}}\end{array}
$$ 

We deﬁne the Lagrangian 

$$
\mathcal{I}=\sum_{x}\sum_{y}P(x,y)\log\frac{P(x,y)}{\theta_{x}\theta_{y}}-\lambda_{x}\left(\sum_{x}\theta_{x}-1\right)-\lambda_{y}\left(\sum_{y}\theta_{y}-1\right).
$$ 

To simplify the computation of derivatives, we notice that 

$$
\log{\frac{P(x,y)}{\theta_{x}\theta_{y}}}=\log P(x,y)-\log\theta_{x}-\log\theta_{y}.
$$ 

Using this simpliﬁcation, we can compute the derivative with respect to the probability of a par- ticular value of $X$ , say $\theta_{x^{k}}$ . We note that this parameter appears only when the value of x in the summation equals $x^{k}$ . Thus, 

$$
\frac{\partial}{\partial\theta_{x^{k}}}\mathcal{I}=-\sum_{y}\frac{P(x^{k},y)}{\theta_{x^{k}}}-\lambda_{x}.
$$ 

Equating this derivative to 0 , we get 

$$
\theta_{x^{k}}=-{\frac{\sum_{y}P(x^{k},y)}{\lambda_{x}}}=-{\frac{P(x^{k})}{\lambda_{x}}}.
$$ 

To solve for the value of $\lambda_{x}$ , we use the ﬁrst constraint, and get that 

$$
1=\sum_{x}\theta_{x}=-\sum_{x}\frac{P(x)}{\lambda_{x}}.
$$ 

t that $\begin{array}{r}{\lambda_{x}=-\sum_{x}P(x)}\end{array}$ . Thus, we can conclude that $\lambda_{x}=-1$ , and consequently that $\theta_{x}=P(x)$ . An analogous reasoning shows that $\theta_{y}=P(y)$ . 

This solution is very natural. The closest distribution to $P(X,Y)$ in which $X$ and $Y$ are independent is $Q(X,Y)=P(X)P(Y)$ . This distribution preserves the marginal distributions of both $X$ and $Y$ , but loses all information about their joint behavior. 

# A.5.4 Convex Duality 

convex duality The concept of convex duality plays a central role in optimization theory. We brieﬂy review the main results here for equality-constrained optimization problems with nonnegativity constraints (although the theory extends quite naturally to the case of general inequality constraints). 

In appendix A.5.3, we considered an optimization problem of maximizing ${\pmb f}({\pmb\theta})$ subject to certain constraints, which we now call the primal problem . We showed how to formulate a Lagrangian ${\mathcal{I}}(\theta,\lambda)$ , and proved that if $\langle\theta,\lambda\rangle$ is a stationary point of $\mathcal{I}$ then $\theta$ is a stationary point of the objective function $\pmb{f}$ that we are trying to maximize. 

We can extend this idea further and deﬁne the dual function $\pmb{g}(\lambda)$ as 

$$
\displaystyle\mathbf{g}(\lambda)=\operatorname*{sup}_{\theta\geq0}{\mathcal{I}}(\theta,\lambda).
$$ 

That is, the dual function $\pmb{g}(\lambda)$ , is the supremum , or maximum, over the parameters $\theta$ for a given $\lambda$ . In general, we allow the dual function to take the value $\infty$ when $\mathcal{I}$ is unbound above (which can occur when the primal constraints are unsatisﬁed), and refer to the points λ at which this happens as dual infeasible . 

Example A.8 Let us return to example A.6, where our task is to ﬁnd the distribution $P(X)$ of maximum entropy. Now, however, we also want the distribution to satisfy the constraint that ${\cal E}_{P}[X]=\,\mu$ . Treating each $P(X=k)$ as a separate parameter $\theta_{k}$ , we can write our problem formally as: 

Constrained-Entropy : Find $P$ maximizing $H_{P}(X)$ subject to $\begin{array}{l}{\sum_{k=1}^{K}k\theta_{k}=\mu}\\ {\sum_{k=1}^{K}\theta_{k}=1}\\ {\theta_{k}\geq0\qquad\forall k=1,.\,.\,.\,,K}\end{array}$ 

Introducing Lagrange multipliers for each of the constraints we can write 

$$
\mathcal{I}(\pmb{\theta},\lambda,\nu)=-\sum_{k=1}^{K}\theta_{k}\log\theta_{k}-\lambda\left(\sum_{k=1}^{K}k\theta_{k}-\mu\right)-\nu\left(\sum_{k=1}^{K}\theta_{k}-1\right).
$$ 

Maximizing over $\theta$ for each $\langle\lambda,\nu\rangle$ we get the dual function 

$$
\begin{array}{l}{\displaystyle\mathbf{g}(\lambda,\nu)=\operatorname*{sup}_{\pmb{\theta}\geq0}\mathcal{I}(\pmb{\theta},\lambda,\nu)}\\ {\displaystyle\quad\quad\quad=\lambda\mu+\nu+e^{-\nu-1}\sum_{k}e^{-k\lambda}.}\end{array}
$$ 

Thus, the convex ual (to be minimized) is $\begin{array}{r}{\lambda\mu+\nu+e^{-\nu-1}\sum_{k}e^{-k\lambda}}\end{array}$ . We can minimize over $\nu$ analytically by taking derivatives and setting them equal to zero, giving $\begin{array}{r}{\nu=\log\pmb{g}(\sum_{k}e^{-k\lambda})-1}\end{array}$ P − . Substituting into g g, we arrive at the dual optimization problem 

$$
\begin{array}{r l}{i n i m i z e}&{{}\lambda\mu+\log\left(\sum_{k=1}^{K}e^{-k\lambda}\right).}\end{array}
$$ 

This form of optimization problem is known as a geometric program . The convexity of the objective function can be easily veriﬁed by taking second derivatives. Taking the ﬁrst derivative and setting it to zero provides some insight into the solution to the problem: 

$$
\frac{\sum_{k=1}^{K}k e^{-k\lambda}}{\sum_{k=1}^{K}e^{-k\lambda}}=\mu,
$$ 

indicating that the solution has $\theta_{k}\propto\alpha^{k}$ for some ﬁxed $\alpha$ . 

Importantly, as we can see in this example, the dual function is a pointwise maximization over a family of linear functions (of the dual variables). Thus, the dual function is always convex even when the primal objective function $\pmb{f}$ is not. 

One of the most important results in optimization theory is that the dual function gives an upper bound on the optimal value of the optimization problem; that is, for any primal feasible point $\theta$ and any dual feasible point $\lambda$ , we have $\mathbf{g}(\lambda)\,\geq\,f_{\mathrm{obj}}(\theta)$ ≥ . This leads directly to the property of weak duality , which states that the minimum value of the dual function is at least as large as the maximum value of the primal problem; that is, 

$$
\pmb{\mathrm{g}}(\lambda^{\star})=\operatorname*{inf}_{\lambda}\pmb{\mathrm{g}}(\lambda)\geq\pmb{\mathrm{f}}\!\!\!\!\!\slash(\theta^{\star}).
$$ 

The diference ${\pmb f}({\pmb\theta}^{\star})-{\pmb g}(\lambda^{\star})$ − is known as the duality gap . Under certain conditions the duality gap is zero, that is, ${\pmb f}\!\!\left({\pmb\theta}^{\star}\right)={\pmb g}(\lambda^{\star})$ , in which case we have strong duality . Thus, duality can be used to provide a certiﬁcate of optimality. That is, if we can show that ${\pmb g}(\lambda)={\pmb f}({\pmb\theta})$ for some value of $\langle\theta,\lambda\rangle$ , then we know that ${\pmb f}(\theta)$ is optimal. 

The concept of a dual function plays an important role in optimization. In a number of situations, the dual objective function is easier to optimize than the primal. Moreover, there are methods that solve the primal and dual together, using the fact that each bounds the other to improve the search for an optimal solution. 

# Bibliography 

Abbeel, P., D. Koller, and A. Ng (2006, August). Learning factor graphs in polynomial time & sample complexity. Journal of Machine Learning Research 7 , 1743–1788. Ackley, D., G. Hinton, and T. Sejnowski (1985). A learning algorithm for Boltzmann machines. Cognitive Science 9 , 147–169. Aji, S. M. and R. J. McEliece (2000). The generalized distributive law. IEEE Trans. Information Theory 46 , 325–343. Akaike, H. (1974). A new look at the statistical identiﬁcation model. IEEE Transactions on Automatic Control 19 , 716–723. Akashi, H. and H. Kumamoto (1977). Random sampling approach to state estimation in switching environments. Automatica 13 , 429–434. Allen, D. and A. Darwiche (2003a). New advances in inference by recursive conditioning. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 2–10. Allen, D. and A. Darwiche (2003b). Optimal time–space tradeof in probabilistic inference. In Proc. 18th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 969–975. Altun, Y., I. Tsochantaridis, and T. Hofmann (2003). Hidden Markov support vector machines. In Proc. 20th International Conference on Machine Learning (ICML) . Andersen, S., K. Olesen, F. Jensen, and F. Jensen (1989). HUGIN—a shell for building Bayesian belief universes for expert systems. In Proc. 11th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1080–1085. Anderson, N. (1974). Information integration theory: A brief survey. In Contemporary develop- ments in Mathematical Psychology , Volume 2, pp. 236–305. San Francisco, California: W.H. Freeman and Company. Anderson, N. (1976). How functional measurement can yield validated interval scales of mental quantities. Journal of Applied Psychology 61 (6), 677–692. Andreassen, S., F. Jensen, S. Andersen, B. Falck, U. Kjærulf, M. Woldbye, A. R. Sørensen, A. Rosen- falck, and F. Jensen (1989). MUNIN — an expert EMG assistant. In J. E. Desmedt (Ed.), Computer-Aided Electro myo graph y and Expert Systems , Chapter 21. Amsterdam: Elsevier Sci- ence Publishers. Anguelov, D., D. Koller, P. Srinivasan, S. Thrun, H.-C. Pang, and J. Davis (2004). The correlated correspondence algorithm for unsupervised registration of nonrigid surfaces. In Proc. 18th Conference on Neural Information Processing Systems (NIPS) . Arnauld, A. and P. Nicole (1662). Port-royal logic. 

Arnborg, S. (1985). Efcient algorithms for combinatorial problems on graphs with bounded, decomposability—a survey. BIT 25 (1), 2–23. Arnborg, S., D. Corneil, and A. Proskurowski (1987). Complexity of ﬁnding embeddings in a k-tree. SIAM J. Algebraic Discrete Methods 8 (2), 277–284. Attias, H. (1999). Inferring parameters and structure of latent variable models by variational Bayes. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 21–30. Avriel, M. (2003). Nonlinear Programming: Analysis and Methods . Dover Publishing. Bacchus, F. and A. Grove (1995). Graphical models for preference and utility. In Proc. UAI–95 , pp. 3–10. Bach, F. and M. Jordan (2001). Thin junction trees. In Proc. 15th Conference on Neural Information Processing Systems (NIPS) . Balke, A. and J. Pearl (1994a). Counterfactual probabilities: Computational methods, bounds and applications. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 46–54. Balke, A. and J. Pearl (1994b). Probabilistic evaluation of counterfactual queries. In Proc. 10th Conference on Artiﬁcial Intelligence (AAAI) , pp. 230–237. Bar-Shalom, Y. (Ed.) (1992). Multitarget multisensor tracking: Advanced applications . Norwood, Massachusetts: Artech House. Bar-Shalom, Y. and T. Fortmann (1988). Tracking and Data Association . New York: Academic Press.Bar-Shalom, Y., X. Li, and T. Kirubarajan (2001). Estimation with Application to Tracking and Navigation . John Wiley and Sons. Barash, Y. and N. Friedman (2002). Context-speciﬁc Bayesian clustering for gene expression data. Journal of Computational Biology 9 , 169–191. Barber, D. and W. Wiegerinck (1998). Tractable variational structures for approximating graphical models. In Proc. 12th Conference on Neural Information Processing Systems (NIPS) , pp. 183–189. Barbu, A. and S. Zhu (2005). Generalizing Swendsen-Wang to sampling arbitrary posterior probabilities. IEEE Trans. on Pattern Analysis and Machine Intelligence 27 (8), 1239–1253. Barnard, S. (1989). Stochastic stero matching over scale. International Journal of Computer Vision 3 , 17–32. Barndorf-Nielsen, O. (1978). Information and Exponential Families in Statistical Theory . Wiley. Barron, A., J. Rissanen, and B. Yu (1998). The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory 44 (6), 2743–2760. Bartlett, M. (1935). Contingency table interactions. Journal of the Royal Statistical Society, Series B 2 , 248–252. Bauer, E., D. Koller, and Y. Singer (1997). Update rules for parameter estimation in Bayesian networks. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 3–13. Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London 53 , 370–418. Beal, M. and Z. Ghahramani (2006). Variational Bayesian learning of directed graphical models with hidden variables. Bayesian Analysis 1 , 793–832. Becker, A., R. Bar-Yehuda, and D. Geiger (1999). Random algorithms for the loop cutset problem. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 49–56. Becker, A. and D. Geiger (1994). The loop cutset problem. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 60–68. Becker, A. and D. Geiger (2001). A sufciently fast algorithm for ﬁnding close to optimal clique 

trees. Artiﬁcial Intelligence 125 (1–2), 3–17. Becker, A., D. Geiger, and C. Meek (2000). Perfect tree-like Markovian distributions. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 19–23. Becker, A., D. Geiger, and A. Schäfer (1998). Automatic selection of loop breakers for genetic linkage analysis. Human Heredity 48 , 49–60. Beeri, C., R. Fagin, D. Maier, and M. Yannakakis (1983). On the desirability of acyclic database schemes. Journal of the Association for Computing Machinery 30 (3), 479–513. Beinlich, L., H. Suermondt, R. Chavez, and G. Cooper (1989). The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. In Proceedings of the Second European Conference on Artiﬁcial Intelligence in Medicine , pp. 247–256. Springer Verlag. Bell, D. (1982). egret in decision making under uncertainty. Operations Research 30 , 961–981. Bellman, R. E. (1957). Dynamic Programming . Princeton, New Jersey: Princeton University Press. Ben-Tal, A. and A. Charnes (1979). A dual optimization framework for some problems of infor- mation theory and statistics. Problems of Control and Information Theory 8 , 387–401. Bentham, J. (1789). An introduction to the principles of morals and legislation. Berger, A., S. Della-Pietra, and V. Della-Pietra (1996). A maximum entropy approach to natural language processing. Computational Linguistics 16 (2). Bernardo, J. and A. Smith (1994). Bayesian Theory . New York: John Wiley and Sons. Bernoulli, D. (1738). Specimen theoriae novae de mensura sortis (exposition of a new theory on the measurement of risk). English Translation by L. Sommer, Econometrica , 22:23–36, 1954. Berrou, C., A. Glavieux, and P. Thitimajshima (1993). Near Shannon limit error-correcting coding: Turbo codes. In Proc. International Conference on Communications , pp. 1064–1070. Bertelé, U. and F. Brioschi (1972). Nonserial Dynamic Programming . New York: Academic Press. Bertsekas, D. (1999). Nonlinear Programming (2nd ed.). Athena Scientiﬁc. Bertsekas, D. P. and J. N. Tsitsiklis (1996). Neuro-Dynamic Programming . Athena Scientiﬁc. Besag, J. (1977a). Efciency of pseudo-likelihood estimation for simple Gaussian ﬁelds. Biometrika 64 (3), 616–618. Besag, J. (1977b). Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society, Series B 36 , 192–236. Besag, J. (1986). On the statistical analysis of dirty pictures (with discussion). Journal of the Royal Statistical Society, Series B 48 , 259–302. Bethe, H. A. (1935). Statistical theory of superlattices. in Proceedings of the Royal Society of London A , 552. Bidyuk, B. and R. Dechter (2007). Cutset sampling for bayesian networks. Journal of Artiﬁcial Intelligence Research 28 , 1–48. Bilmes, J. and C. Bartels (2003). On triangulating dynamic graphical models. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Bilmes, J. and C. Bartels (2005, September). Graphical model architectures for speech recognition. IEEE Signal Processing Magazine 22 (5), 89–100. Binder, J., D. Koller, S. Russell, and K. Kanazawa (1997). Adaptive probabilistic networks with hidden variables. Machine Learning 29 , 213–244. Binder, J., K. Murphy, and S. Russell (1997). Space-efcient inference in dynamic probabilistic networks. In Proc. 15th International Joint Conference on Artiﬁcial Intelligence (IJCAI) . Bishop, C. (2006). Pattern Recognition and Machine Learning . Information Science and Statistics 

(M. Jordan, J. Kleinberg, and B. Schökopf, editors). New York: Springer-Verlag. Bishop, C., N. Lawrence, T. Jaakkola, and M. Jordan (1997). Approximating posterior distributions in belief networks using mixtures. In Proc. 11th Conference on Neural Information Processing Systems (NIPS) . Blalock, Jr., H. (1971). Causal Models in the Social Sciences . Chicago, Illinois: Aldine-Atheson. Blum, B., C. Shelton, and D. Koller (2006). A continuation method for nash equilibria in structured games. Journal of Artiﬁcial Intelligence Resarch 25 , 457–502. Bodlaender, H., A. Koster, F. van den Eijkhof, and L. van der Gaag (2001). Pre-processing for triangulation of probabilistic networks. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 32–39. Boros, E. and P. Hammer (2002). Pseudo-Boolean optimization. Discrete Applied Mathemat- ics 123 (1-3). Bouckaert, R. (1993). Probabilistic network construction using the minimum description length principle. In Proc. European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty , pp. 41–48. Boutilier, C. (2002). A POMDP formulation of preference elicitation problems. In Proc. 18th Conference on Artiﬁcial Intelligence (AAAI) , pp. 239–46. Boutilier, C., F. Bacchus, and R. Brafman (2001). UCP-Networks: A directed graphical represen- tation of conditional utilities. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 56–64. Boutilier, C., T. Dean, and S. Hanks (1999). Decision theoretic planning: Structural assumptions and computational leverage. Journal of Artiﬁcial Intelligence Research 11 , 1 – 94. Boutilier, C., R. Dearden, and M. Goldszmidt (1989). Exploiting structure in policy construction. In Proc. 14th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1104–1111. Boutilier, C., R. Dearden, and M. Goldszmidt (2000). Stochastic dynamic programming with factored representations. Artiﬁcial Intelligence 121 (1), 49–107. Boutilier, C., N. Friedman, M. Goldszmidt, and D. Koller (1996). Context-speciﬁc independence in Bayesian networks. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 115–123. Boyd, S. and L. Vandenberghe (2004). Convex Optimization . Cambridge University Press. Boyen, X., N. Friedman, and D. Koller (1999). Discovering the hidden structure of complex dynamic systems. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 91–100. Boyen, X. and D. Koller (1998a). Approximate learning of dynamic models. In Proc. 12th Conference on Neural Information Processing Systems (NIPS) . Boyen, X. and D. Koller (1998b). Tractable inference for complex stochastic processes. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 33–42. Boyen, X. and D. Koller (1999). Exploiting the architecture of dynamic systems. In Proc. 15th Conference on Artiﬁcial Intelligence (AAAI) . Boykov, Y., O. Veksler, and R. Zabih (2001). Fast approximate energy minimization via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence 23 (11), 1222–1239. Braziunas, D. and C. Boutilier (2005). Local utility elicitation in GAI models. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 42–49. Breese, J. and D. Heckerman (1996). Decision-theoretic troubleshooting: A framework for repair and experiment. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 

124–132. Breese, J., D. Heckerman, and C. Kadie (1998). Empirical analysis of predictive algorithms for collaborative ﬁltering. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 43–52. Breiman, L., J. Friedman, R. Olshen, and C. Stone (1984). Classiﬁcation and Regression Trees . Monterey,CA: Wadsworth & Brooks. Buchanan, B. and E. Shortlife (Eds.) (1984). Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project . Reading, MA: Addison-Wesley. Bui, H., S. Venkatesh, and G. West (2001). Tracking and surveillance in wide-area spatial environ- ments using the Abstract Hidden Markov Model. International Journal of Pattern Recognition and Artiﬁcial Intelligence . Buntine, W. (1991). Theory reﬁnement on Bayesian networks. In Proc. 7th Conference on Uncer- tainty in Artiﬁcial Intelligence (UAI) , pp. 52–60. Buntine, W. (1993). Learning classiﬁcation trees. In D. J. Hand (Ed.), Artiﬁcial Intelligence Frontiers in Statistics , Number III in AI and Statistics. Chapman & Hall. Buntine, W. (1994). Operations for learning with graphical models. Journal of Artiﬁcial Intelligence Research 2 , 159–225. Buntine, W. (1996). A guide to the literature on learning probabilistic networks from data. IEEE Transactions on Knowledge and Data Engineering 8 , 195–210. Cafo, B., W. Jank, and G. Jones (2005). Ascent-based Monte Carlo Expectation-Maximization. Journal of the Royal Statistical Society, Series B . Cannings, C., E. A. Thompson, and H. H. Skolnick (1976). The recursive derivation of likelihoods on complex pedigrees. Advances in Applied Probability 8 (4), 622–625. Cannings, C., E. A. Thompson, and M. H. Skolnick (1978). Probability functions on complex pedigrees. Advances in Applied Probability 10 (1), 26–61. Cano, J., L.D., Hernández, and S. Moral (2006). Importance sampling algorithms for the propaga- tion of probabilities in belief networks. International Journal of Approximate Reasoning 15 (1), 77–92. Carreira-Perpignan, M. and G. Hinton (2005). On contrastive divergence learning. In Proc. 11thWorkshop on Artiﬁcial Intelligence and Statistics . Casella, G. and R. Berger (1990). Statistical Inference . Wadsworth. Castillo, E., J. Gutiérrez, and A. Hadi (1997a). Expert Systems and Probabilistic Network Models . New York: Springer-Verlag. Castillo, E., J. Gutiérrez, and A. Hadi (1997b). Sensitivity analysis in discrete Bayesian networks. IEEE Transactions on Systems, Man and Cybernetics 27 , 412–23. Chajewska, U. (2002). Acting Rationally with Incomplete Utility Information . Ph.D. thesis, Stanford University. Chajewska, U. and D. Koller (2000). Utilities as random variables: Density estimation and structure discovery. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 63–71. Chajewska, U., D. Koller, and R. Parr (2000). Making rational decisions using adaptive utility elicitation. In Proc. 16th Conference on Artiﬁcial Intelligence (AAAI) , pp. 363–369. Chan, H. and A. Darwiche (2002). When do numbers really matter? Journal of Artiﬁcial Intelligence Research 17 , 265–287. Chávez, T. and M. Henrion (1994). Efcient estimation of the value of information in Monte Carlo 

models. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 119–127. Cheeseman, P., J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman (1988). Autoclass: a Bayesian classiﬁcation system. In Proc. 5th International Conference on Machine Learning (ICML) . Cheeseman, P., M. Self, J. Kelly, and J. Stutz (1988). Bayesian classiﬁcation. In Proc. 4th Conference on Artiﬁcial Intelligence (AAAI) , Volume 2, pp. 607–611. Cheeseman, P. and J. Stutz (1995). Bayesian classiﬁcation (AutoClass): Theory and results. In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD- 95) . AAAI Press. Chen, L., M. Wainwright, M. Cetin, and A. Willsky (2003). Multitarget-multisensor data association using the tree-reweighted max-product algorithm. In Proceedings SPIE Aerosense Conference , Orlando, Florida. Chen, R. and S. Liu (2000). Mixture Kalman ﬁlters. Journal of the Royal Statistical Society, Series B . Cheng, J. and M. Druzdzel (2000). AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks. Journal of Artiﬁcial Intelligence Research 13 , 155–188. Cheng, J., R. Greiner, J. Kelly, D. Bell, and W. Liu (2002). Learning bayesian networks from data: An information-theory based approach. Artiﬁcial Intelligence . Chesley, G. (1978). Subjective probability elicitation techniques: A performance comparison. Journal of Accounting Research 16 (2), 225–241. Chickering, D. (1996a). Learning Bayesian networks is NP-Complete. In D. Fisher and H. Lenz (Eds.), Learning from Data: Artiﬁcial Intelligence and Statistics V , pp. 121–130. Springer-Verlag. Chickering, D. (2002a, February). Learning equivalence classes of Bayesian-network structures. Journal of Machine Learning Research 2 , 445–498. Chickering, D., D. Geiger, and D. Heckerman (1995, January). Learning Bayesian networks: Search methods and experimental results. In Proceedings of the Fifth International Workshop on Artiﬁcial Intelligence and Statistics , pp. 112–128. Chickering, D., C. Meek, and D. Heckerman (2003). Large-sample learning of Bayesian networks is hard. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 124–133. Chickering, D. and J. Pearl (1997). A clinician’s tool for analyzing non-compliance. Computing Science and Statistics 29 , 424–31. Chickering, D. M. (1995). A transformation al characterization of equivalent Bayesian network structures. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 87–98. Chickering, D. M. (1996b). Learning equivalence classes of Bayesian network structures. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 150–157. Chickering, D. M. (2002b, November). Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research 3 , 507–554. Chickering, D. M. and D. Heckerman (1997). Efcient approximations for the marginal likelihood of Bayesian networks with hidden variables. Machine Learning 29 , 181–212. Chickering, D. M., D. Heckerman, and C. Meek (1997). A Bayesian approach to learning Bayesian networks with local structure. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 80–89. Chow, C. K. and C. N. Liu (1968). Approximating discrete probability distributions with depen- dence trees. IEEE Transactions on Information Theory 14 , 462–467. Collins, M. (2002). Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. Conference on Empirical Methods in Natural 

Language Processing (EMNLP) . Cooper, G. (1990). Probabilistic inference using belief networks is NP-hard. Artiﬁcial Intelli- gence 42 , 393–405. Cooper, G. and E. Herskovits (1992). A Bayesian method for the induction of probabilistic networks from data. Machine Learning 9 , 309–347. Cooper, G. and C. Yoo (1999). Causal discovery from a mixture of experimental and observational data. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 116–125. Cooper, G. F. (1988). A method for using belief networks as inﬂuence diagrams. In Proceedings of the Fourth Workshop on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 55–63. Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein (2001). Introduction to Algorithms . Cambridge, Massachusetts: MIT Press. 2nd Edition. Covaliu, Z. and R. Oliver (1995). Representation and solution of decision problems using sequen- tial decision diagrams. Management Science 41 (12), 1860–81. Cover, T. M. and J. A. Thomas (1991). Elements of Information Theory . John Wiley & Sons. Cowell, R. (2005). Local propagation in conditional gaussian Bayesian networks. Journal of Machine Learning Research 6 , 1517–1550. Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter (1999). Probabilistic Networks and Expert Systems . New York: Springer-Verlag. Cox, R. (2001). Algebra of Probable Inference . The Johns Hopkins University Press. Cozman, F. (2000). Credal networks. Artiﬁcial Intelligence 120 , 199–233. Csiszàr, I. (1975). I-divergence geometry of probability distributions and minimization problems. The Annals of Probability 3 (1), 146–158. Culotta, A., M. Wick, R. Hall, and A. McCallum (2007). First-order probabilistic models for coreference resolution. In Proc. Conference of the North American Association for Computational Linguistics . D. Rusakov, D. G. (2005). Asymptotic model selection for naive Bayesian networks. Journal of Machine Learning Research 6 , 1–35. Dagum, P. and M. Luby (1993). Appoximating probabilistic inference in Bayesian belief networks in NP-hard. Artiﬁcial Intelligence 60 (1), 141–153. Dagum, P. and M. Luby (1997). An optimal approximation algorithm for Baysian inference. Artiﬁcial Intelligence 93 (1–2), 1–27. Daneshkhah, A. (2004). Psychological aspects inﬂuencing elicitation of subjective probability. Technical report, University of Shefeld. Darroch, J. and D. Ratclif (1972). Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics 43 , 1470–1480. Darwiche, A. (1993). Argument calculus and networks. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 420–27. Darwiche, A. (2001a). Constant space reasoning in dynamic Bayesian networks. International Journal of Approximate Reasoning 26 , 161–178. Darwiche, A. (2001b). Recursive conditioning. Artiﬁcial Intelligence 125 (1–2), 5–41. Darwiche, A. (2003). A diferential approach to inference in Bayesian networks. Journal of the ACM 50 (3), 280–305. Darwiche, A. and M. Goldszmidt (1994). On the relation between Kappa calculus and probabilistic reasoning. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Dasgupta, S. (1997). The sample complexity of learning ﬁxed-structure Bayesian networks. Ma- 

chine Learning 29 , 165–180. Dasgupta, S. (1999). Learning polytrees. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 134–141. Dawid, A. (1979). Conditional independence in statistical theory (with discussion). Journal of the Royal Statistical Society, Series B 41 , 1–31. Dawid, A. (1980). Conditional independence for statistical operations. Annals of Statistics 8 , 598–617. Dawid, A. (1984). Statistical theory: The prequential approach. Journal of the Royal Statistical Society, Series A 147 (2), 278–292. Dawid, A. (1992). Applications of a general propagation algorithm for probabilistic expert system. Statistics and Computing 2 , 25–36. Dawid, A. (2002). Inﬂuence diagrams for causal modelling and inference. International Statistical Review 70 , 161–189. Corrections p437. Dawid, A. (2007, September). Fundamentals of statistical causality. Technical Report 279, RSS/EPSRC Graduate Training Programme, University of Shefeld. Dawid, A., U. Kjærulf, and S. Lauritzen (1995). Hybrid propagation in junction trees. In Advances in Intelligent Computing , Volume 945. Springer-Verlag. de Bombal, F., D. Leaper, J. Staniland, A. McCann, and J. Harrocks (1972). Computer-aided diagnosis of acute abdominal pain. British Medical Journal 2 , 9–13. de Finetti, B. (1937). Foresight: Its logical laws, its subjective sources. Annals Institute H. Poincaré 7 , 1–68. Translated by H. Kyburg in Kyburg et al. (1980). de Freitas, N., P. Højen-Sørensen, M. Jordan, and S. Russell (2001). Variational MCMC. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 120–127. Dean, T. and K. Kanazawa (1989). A model for reasoning about persistence and causation. Computational Intelligence 5 (3), 142–150. Dechter, R. (1997). Mini-Buckets: A general scheme for generating approximations in automated reasoning. In Proc. 15th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1297–1303. Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artiﬁcial Intelli- gence 113 (1–2), 41–85. Dechter, R. (2003). Constraint Processing . Morgan Kaufmann. Dechter, R., K. Kask, and R. Mateescu (2002). Iterative join-graph propagation. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 128–136. Dechter, R. and I. Rish (1997). A scheme for approximating probabilistic inference. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . DeGroot, M. H. (1989). Probability and Statistics . Reading, MA: Addison Wesley. Della Pietra, S., V. Della Pietra, and J. Laferty (1997). Inducing features of random ﬁelds. IEEE Trans. on Pattern Analysis and Machine Intelligence 19 (4), 380–393. Dellaert, F., S. Seitz, C. Thorpe, and S. Thrun (2003). EM, MCMC, and chain ﬂipping for structure from motion with unknown correspondence. Machine Learning 50 (1–2), 45–71. Deming, W. and F. Stephan (1940). On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. Annals of Mathematical Statistics 11 , 427–444. Dempster, A., N. M. Laird, and D. Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B 39 (1), 1–22. Deng, K. and A. Moore (1989). Multiresolution instance-based learning. In Proc. 14th International 

Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1233–1239. Deshpande, A., M. Garofalakis, and M. Jordan (2001). Efcient stepwise selection in decomposable models. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 128–135. Diez, F. (1993). Parameter adjustment in Bayes networks: The generalized noisy OR-gate. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 99–105. Dittmer, S. L. and F. V. Jensen (1997). Myopic value of information in inﬂuence diagrams. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 142–149. Doucet, A. (1998). On sequential simulation-based methods for Bayesian ﬁltering. Technical Report CUED/FINFENG/TR 310, Department of Engineering, Cambridge University. Doucet, A., N. de Freitas, and N. Gordon (Eds.) (2001). Sequential Monte Carlo Methods in Practice . New York: Springer-Verlag. Doucet, A., N. de Freitas, K. Murphy, and S. Russell (2000). Rao-Blackwellised particle ﬁltering for dynamic Bayesian networks. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Doucet, A., S. Godsill, and C. Andrieu (2000). On sequential Monte Carlo sampling methods for Bayesian ﬁltering. Statistics and Computing 10 (3), 197–208. Drummond, M., B. O’Brien, G. Stoddart, and G. Torrance (1997). Methods for the Economic Evaluation of Health Care Programmes, 2nd Edition . Oxford, UK: Oxford University Press. Druzdzel, M. (1993). Probabilistic Reasoning in Decision Support Systems: From Computation to Common Sense . Ph.D. thesis, Carnegie Mellon University. Dubois, D. and H. Prade (1990). Inference i possibilistic hypergraphs. In Proc. of the 6th Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems . Duchi, J., D. Tarlow, G. Elidan, and D. Koller (2006). Using combinatorial optimization within max-product belief propagation. In Proc. 20th Conference on Neural Information Processing Systems (NIPS) . Duda, R., J. Gaschnig, and P. Hart (1979). Model design in the prospector consultant system for mineral exploration. In D. Michie (Ed.), Expert Systems in the Microelectronic Age , pp. 153–167. Edinburgh, Scotland: Edinburgh University Press. Duda, R. and P. Hart (1973). Pattern Classiﬁcation and Scene Analysis . New York: John Wiley & Sons. Duda, R., P. Hart, and D. Stork (2000). Pattern Classiﬁcation, Second Edition . Wiley. Dudík, M., S. Phillips, and R. Schapire (2004). Performance guarantees for regularized maximum entropy density estimation. In Proc. Conference on Computational Learning Theory (COLT) . Durbin, R., S. Eddy, A. Krogh, and G. Mitchison (1998). Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids . Cambridge: Cambridge University Press. Dykstra, R. and J. Lemke (1988). Duality of I projections and maximum likelihood estimation for log-linear models under cone constraints. Journal of the American Statistical Associa- tion 83 (402), 546–554. El-Hay, T. and N. Friedman (2001). Incorporating expressive graphical models in variational approximations: Chain-graphs and hidden variables. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 136–143. Elfadel, I. (1995). Convex potentials and their conjugates in analog mean-ﬁeld optimization. Neural Computation 7 , 1079–1104. Elidan, G. and N. Friedman (2005). Learning hidden variable networks: The information bottle- neck approach. Journal of Machine Learning Research 6 , 81–127. 

Elidan, G., I. McGraw, and D. Koller (2006). Residual belief propagation: Informed scheduling for asynchronous message passing. In Proc. 22nd Conf. on Uncertainty in Artiﬁcial Intelligence . Elidan, G., N. Lotner, N. Friedman, and D. Koller (2000). Discovering hidden variables: A structure-based approach. In Proc. 14th Conf. on Neural Information Processing Systems (NIPS) . Elidan, G., I. Nachman, and N. Friedman (2007). “Ideal Parent” structure learning for continuous variable networks. Journal of Machine Learning Research 8 , 1799–1833. Elidan, G., M. Ninio, N. Friedman, and D. Schuurmans (2002). Data perturbation for escaping local maxima in learning. In Proc. 18th National Conference on Artiﬁcial Intelligence (AAAI) . Ellis, B. and W. Wong (2008). Learning causal Bayesian network structures from experimental data. Journal of the American Statistical Association 103 , 778–789. Elston, R. C. and J. Stewart (1971). A general model for the analysis of pedigree data. Human Heredity 21 , 523–542. Ezawa, K. (1994). Value of evidence on inﬂuence diagrams. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 212–220. Feller, W. (1970). An Introduction to Probability Theory and Its Applications (third ed.), Volume I. New York: John Wiley & Sons. Felzenszwalb, P. and D. Huttenlocher (2006, October). Efcient belief propagation for early vision. International Journal of Computer Vision 70 (1). Fertig, K. and J. Breese (1989). Interval inﬂuence diagrams. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Fine, S., Y. Singer, and N. Tishby (1998). The hierarchical Hidden Markov Model: Analysis and applications. Machine Learning 32 , 41–62. Fishburn, P. (1967). Interdependence and additivity in multivariate, unidimensional expected utility theory. International Economic Review 8 , 335–42. Fishburn, P. (1970). Utility Theory for Decision Making . New York: Wiley. Fishelson, M. and D. Geiger (2003). Optimizing exact genetic linkage computations. In Proc. International Conf. on Research in Computational Molecular Biology (RECOMB) , pp. 114–121. Fishman, G. (1976, July). Sampling from the gamma distribution on a computer. Communications of the ACM 19 (7), 407–409. Fishman, G. (1996). Monte Carlo — Concept, Algorithms, and Applications . Series in Operations Research. Springer. Fox, D., W. Burgard, and S. Thrun (1999). Markov localization for mobile robots in dynamic environments. Journal of Artiﬁcial Intelligence Research 11 , 391–427. Freund, Y. and R. Schapire (1998). Large margin classiﬁcation using the perceptron algorithm. In Proc. Conference on Computational Learning Theory (COLT) . Frey, B. (2003). Extending factor graphs so as to unify directed and undirected graphical models. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 257–264. Frey, B. and A. Kannan (2000). Accumulator networks: suitors of local probability propagation. In Proc. 14th Conference on Neural Information Processing Systems (NIPS) . Frey, B. and D. MacKay (1997). A revolution: Belief propagation in graphs with cycles. In Proc. 11th Conference on Neural Information Processing Systems (NIPS) . Frey, B. J. (1998). Graphical Models for Machine Learning and Digital Communication . Cambridge, Massachusetts: MIT Press. Friedman, N. (1997). Learning belief networks in the presence of missing values and hidden variables. In Proc. 14th International Conference on Machine Learning (ICML) , pp. 125–133. 

Friedman, N. (1998). The Bayesian structural em algorithm. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 129–138. Friedman, N., D. Geiger, and M. Goldszmidt (1997). Bayesian network classiﬁers. Machine Learning 29 , 131–163. Friedman, N., D. Geiger, and N. Lotner (2000). Likelihood computations using value abstraction. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Friedman, N., L. Getoor, D. Koller, and A. Pfefer (1999). Learning probabilistic relational models. In Proc. 16th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1300–1307. Friedman, N. and M. Goldszmidt (1996). Learning Bayesian networks with local structure. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 252–262. Friedman, N. and M. Goldszmidt (1998). Learning Bayesian networks with local structure. See Jordan (1998), pp. 421–460. Friedman, N. and D. Koller (2003). Being Bayesian about Bayesian network structure: A Bayesian approach to structure discovery in Bayesian networks. Machine Learning 50 (1–2), 95–126. Friedman, N., K. Murphy, and S. Russell (1998). Learning the structure of dynamic probabilistic networks. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Friedman, N. and I. Nachman (2000). Gaussian process networks. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 211–219. Friedman, N. and Z. Yakhini (1996). On the sample complexity of learning Bayesian networks. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Frogner, C. and A. Pfefer (2007). Discovering weakly-interacting factors in a complex stochastic process. In Proc. 21st Conference on Neural Information Processing Systems (NIPS) . Frydenberg, J. (1990). The chain graph Markov property. Scandinavian Journal of Statistics 17 , 790–805. Fudenberg, D. and J. Tirole (1991). Game Theory . MIT Press. Fung, R. and K. C. Chang (1989). Weighting and integrating evidence for stochastic simulation in Bayesian networks. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , San Mateo, California. Morgan Kaufmann. Fung, R. and B. del Favero (1994). Backward simulation in Bayesian networks. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 227–234. Galles, D. and J. Pearl (1995). Testing identiﬁability of causal models. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 185–95. Gamerman, D. and H. Lopes (2006). Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference . Chapman & Hall, CRC. Ganapathi, V., D. Vickrey, J. Duchi, and D. Koller (2008). Constrained approximate maximum entropy learning. In Proc. 24th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Garcia, L. D. (2004). Algebraic statistics in model selection. In Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 177–18. Geiger, D. and D. Heckerman. A characterization of the bivariate normal-Wishart distribution. Probability and Mathematical Statistics 18 , 119–131. Geiger, D. and D. Heckerman (1994). Learning gaussian networks. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 235–243. Geiger, D. and D. Heckerman (1996). Knowledge representation and inference in similarity networks and Bayesian multinets. Artiﬁcial Intelligence 82 (1-2), 45–74. Geiger, D., D. Heckerman, H. King, and C. Meek (2001). Stratiﬁed exponential families: Graphical 

models and model selection. Annals of Statistics 29 , 505–529. Geiger, D., D. Heckerman, and C. Meek (1996). Asymptotic model selection for directed networks with hidden variables. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 283–290. Geiger, D. and C. Meek (1998). Graphical models and exponential families. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 156–165. Geiger, D., C. Meek, and Y. Wexler (2006). A variational inference procedure allowing internal structure for overlapping clusters and deterministic constraints. Journal of Artiﬁcial Intelligence Research 27 , 1–23. Geiger, D. and J. Pearl (1988). On the logic of causal models. In Proc. 4th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 3–14. Geiger, D. and J. Pearl (1993). Logical and algorithmic properties of conditional independence and graphical models. Annals of Statistics 21 (4), 2001–21. Geiger, D., T. Verma, and J. Pearl (1989). d-separation: From theorems to algorithms. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 139–148. Geiger, D., T. Verma, and J. Pearl (1990). Identifying independence in Bayesian networks. Net- works 20 , 507–534. Gelfand, A. and A. Smith (1990). Sampling based approaches to calculating marginal densities. Journal of the American Statistical Association 85 , 398–409. Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (1995). Bayesian Data Analysis . London: Chapman & Hall. Gelman, A. and X.-L. Meng (1998). Simulating normalizing constants: From importance sampling to bridge sampling to path sampling. Statistical Science 13 (2), 163–185. Gelman, A. and D. Rubin (1992). Inference from iterative simulation using multiple sequences. Statistical Science 7 , 457–511. Geman, S. and D. Geman (1984, November). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence $6(6)$ , 721–741. Getoor, L., N. Friedman, D. Koller, A. Pfefer, and B. Taskar (2007). Probabilistic relational models. See Getoor and Taskar (2007). Getoor, L., N. Friedman, D. Koller, and B. Taskar (2002). Learning probabilistic models of link structure. Journal of Machine Learning Research 3 (December), 679–707. Getoor, L. and B. Taskar (Eds.) (2007). Introduction to Statistical Relational Learning . MIT Press. Geweke, J. (1989). Bayesian inference in econometric models using Monte Carlo integration. Econometrica 57 , 1317–1339. Geyer, C. and E. Thompson (1992). Constrained Monte Carlo maximum likelihood for dependent data. Journal of the Royal Statistical Society, Series B . Geyer, C. and E. Thompson (1995). Annealing Markov chain Monte Carlo with applications to ancestral inference. Journal of the American Statistical Association 90 (431), 909–920. Geyer, C. J. (1991). Markov chain Monte Carlo maximum likelihood. In Computing Science and Statistics: Proceedings of 23rd Symposium on the Interface Interface Foundation , pp. 156–163. Fairfax Station. Ghahramani, Z. (1994). Factorial learning and the em algorithm. In Proc. 8th Conference on Neural Information Processing Systems (NIPS) , pp. 617–624. Ghahramani, Z. and M. Beal (2000). Propagation algorithms for variational Bayesian learning. In 

Proc. 14th Conference on Neural Information Processing Systems (NIPS) . Ghahramani, Z. and G. Hinton (1998). Variational learning for switching state-space models. Neural Computation 12 (4), 963–996. Ghahramani, Z. and M. Jordan (1993). Supervised learning from incomplete data via an EM approach. In Proc. 7th Conference on Neural Information Processing Systems (NIPS) . Ghahramani, Z. and M. Jordan (1997). Factorial hidden Markov models. Machine Learning 29 , 245–273. Gibbs, J. (1902). Elementary Principles of Statistical Mechanics . New Haven, Connecticut: Yale University Press. Gidas, B. (1988). Consistency of maximum likelihood and pseudo-likelihood estimators for Gibb- sian distributions. In W. Fleming and P.-L. Lions (Eds.), Stochastic diferential systems, stochastic control theory and applications . Springer, New York. Gilks, W. (1992). Derivative-free adaptive rejection sampling for Gibbs sampling. In J. Bernardo, J. Berger, A. Dawid, and A. Smith (Eds.), Bayesian Statistics 4 , pp. 641–649. Oxford, UK: Claren- don Press.Gilks, W., N. Best, and K. Tan (1995). Adaptive rejection Metropolis sampling within Gibbs sampling. Annals of Statistics 44 , 455–472. Gilks, W., S. Richardson, and D. Spiegelhalter (Eds.) (1996). Markov Chain Monte Carlo in Practice . Chapman & Hall, London. Gilks, W., A. Thomas, and D. Spiegelhalter (1994). A language and program for complex Bayesian modeling. The Statistician 43 , 169–177. Gilks, W. and P. Wild (1992). Adaptive rejection sampling for Gibbs sampling. Annals of Statistics 41 , 337–348. Giudici, P. and P. Green (1999, December). Decomposable graphical Gaussian model determina- tion. Biometrika 86 (4), 785–801. Globerson, A. and T. Jaakkola (2007a). Convergent propagation algorithms via oriented trees. In Proc. 23rd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Globerson, A. and T. Jaakkola (2007b). Fixing max-product: Convergent message passing al- gorithms for MAP LP-relaxations. In Proc. 21st Conference on Neural Information Processing Systems (NIPS) . Glover, F. and M. Laguna (1993). Tabu search. In C. Reeves (Ed.), Modern Heuristic Techniques for Combinatorial Problems , Oxford, England. Blackwell Scientiﬁc Publishing. Glymour, C. and G. F. Cooper (Eds.) (1999). Computation, Causation, Discovery . Cambridge: MIT Press.Godsill, S., A. Doucet, and M. West (2000). Methodology for Monte Carlo smoothing with application to time-varying autoregressions. In Proc. International Symposium on Frontiers of Time Series Modelling . Golumbic, M. (1980). Algorithmic Graph Theory and Perfect Graphs . London: Academic Press. Good, I. (1950). Probability and the Weighing of Evidence . London: Grifn. Goodman, J. (2004). Exponential priors for maximum entropy models. In Proc. Conference of the North American Association for Computational Linguistics . Goodman, L. (1970). The multivariate analysis of qualitative data: Interaction among multiple classiﬁcation. Journal of the American Statistical Association 65 , 226–56. Gordon, N., D. Salmond, and A. Smith (1993). Novel approach to nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings-F 140 (2), 107–113. 

Gorry, G. and G. Barnett (1968). Experience with a model of sequential diagnosis. Computers and Biomedical Research 1 , 490–507. Green, P. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika 82 , 711–732. Green, P. J. (1990). On use of the EM algorithm for penalized likelihood estimation. Journal of the Royal Statistical Society, Series B 52 (3), 443–452. Greig, D., B. Porteous, and A. Seheult (1989). Exact maximum a posteriori estimation for binary images. Journal of the Royal Statistical Society, Series B 51 (2), 271–279. Greiner, R. and W. Zhou (2002). Structural extension to logistic regression: Discriminant pa- rameter learning of belief net classiﬁers. In Proc. 18th Conference on Artiﬁcial Intelligence (AAAI) . Guestrin, C. E., D. Koller, R. Parr, and S. Venkataraman (2003). Efcient solution algorithms for factored MDPs. Journal of Artiﬁcial Intelligence Research 19 , 399–468. Guyon, X. and H. R. Künsch (1992). Asymptotic comparison of estimators in the Ising model. In Stochastic Models, Statistical Methods, and Algorithms in Image Analysis, Lecture Notes in Statistics , Volume 74, pp. 177–198. Springer, Berlin. Ha, V. and P. Haddawy (1997). Problem-focused incremental elicitation of multi-attribute utility models. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 215–222. Ha, V. and P. Haddawy (1999). A hybrid approach to reasoning with partially elicited preference models. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 263–270. Haberman, S. (1974). The General Log-Linear Model . Ph.D. thesis, Department of Statistics, University of Chicago. Halpern, J. Y. (2003). Reasoning about Uncertainty . MIT Press. Hammer, P. (1965). Some network ﬂow problems solved with pseudo-Boolean programming. Operations Research 13 , 388–399. Hammersley, J. and P. Cliford (1971). Markov ﬁelds on ﬁnite graphs and lattices. Unpublished manuscript. Handschin, J. and D. Mayne (1969). Monte Carlo techniques to estimate the conditional expecta- tion in multi-stage non-linear ﬁltering. International Journal of Control 9 (5), 547–559. Hartemink, A., D. Giford, T. Jaakkola, and R. Young (2002, March/April). Bayesian methods for elucidating genetic regulatory networks. IEEE Intelligent Systems 17 , 37–43. special issue on Intelligent Systems in Biology. Hastie, T., R. Tibshirani, and J. Friedman (2001). The Elements of Statistical Learning . Springer Series in Statistics. Hazan, T. and A. Shashua (2008). Convergent message-passing algorithms for inference over general graphs with convex free energies. In Proc. 24th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Heckerman, D. (1990). Probabilistic Similarity Networks . MIT Press. Heckerman, D. (1993). Causal independence for knowledge acquisition and inference. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 122–127. Heckerman, D. (1998). A tutorial on learning with Bayesian networks. See Jordan (1998). Heckerman, D. and J. Breese (1996). Causal independence for probability assessment and in- ference using Bayesian networks. IEEE Transactions on Systems, Man, and Cybernetics 26 , 826–831. Heckerman, D., J. Breese, and K. Rommelse (1995, March). Decision-theoretic troubleshooting. 

Communications of the ACM 38 (3), 49–57. Heckerman, D., D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie (2000). Dependency networks for inference, collaborative ﬁltering, and data visualization. jmlr 1 , 49–75. Heckerman, D. and D. Geiger (1995). Learning Bayesian networks: a uniﬁcation for discrete and Gaussian domains. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 274–284. Heckerman, D., D. Geiger, and D. M. Chickering (1995). Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning 20 , 197–243. Heckerman, D., E. Horvitz, and B. Nathwani (1992). Toward normative expert systems: Part I. The Pathﬁnder project. Methods of Information in Medicine 31 , 90–105. Heckerman, D. and H. Jimison (1989). A Bayesian perspective on conﬁdence. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 149–160. Heckerman, D., A. Mamdani, and M. Wellman (1995). Real-world applications of Bayesian net- works. Communications of the ACM 38 . Heckerman, D. and C. Meek (1997). Embedded Bayesian network classiﬁers. Technical Report MSR-TR-97-06, Microsoft Research, Redmond, WA. Heckerman, D., C. Meek, and G. Cooper (1999). A Bayesian approach to causal discovery. See Glymour and Cooper (1999), pp. 141–166. Heckerman, D., C. Meek, and D. Koller (2007). Probabilistic entity-relationship models, PRMs, and plate models. See Getoor and Taskar (2007). Heckerman, D. and B. Nathwani (1992a). An evaluation of the diagnostic accuracy of Pathﬁnder. Computers and Biomedical Research 25(1) , 56–74. Heckerman, D. and B. Nathwani (1992b). Toward normative expert systems. II. Probability-based representations for efcient knowledge acquisition and inference. Methods of Information in Medicine 31 , 106–16. Heckerman, D. and R. Shachter (1994). A decision-based view of causality. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 302–310. Morgan Kaufmann. Henrion, M. (1986). Propagation of uncertainty in Bayesian networks by probabilistic logic sampling. In Proc. 2nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 149–163. Henrion, M. (1991). Search-based algorithms to bound diagnostic probabilities in very large belief networks. In Proc. 7th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 142–150. Hernández, L. and S. Moral (1997). Mixing exact and importance sampling propagation algorithms in dependence graphs. International Journal of Intelligent Systems 12 , 553–576. Heskes, T. (2002). Stable ﬁxed points of loopy belief propagation are minima of the Bethe free energy. In Proc. 16th Conference on Neural Information Processing Systems (NIPS) , pp. 359–366. Heskes, T. (2004). On the uniqueness of loopy belief propagation ﬁxed points. Neural Computa- tion 16 , 2379–2413. Heskes, T. (2006). Convexity arguments for efcient minimization of the Bethe and Kikuchi free energies. Journal of Machine Learning Research 26 , 153–190. Heskes, T., K. Albers, and B. Kappen (2003). Approximate inference and constrained optimization. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 313–320. Heskes, T., M. Opper, W. Wiegerinck, O. Winther, and O. Zoeter (2005). Approximate infer- ence techniques with expectation constraints. Journal of Statistical Mechanics: Theory and Experiment . Heskes, T. and O. Zoeter (2002). Expectation propagation for approximate inference in dynamic 

Bayesian networks. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Heskes, T. and O. Zoeter (2003). Generalized belief propagation for approximate inference in hybrid Bayesian networks. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics . Heskes, T., O. Zoeter, and W. Wiegerinck (2003). Approximate expectation maximization. In Proc. 17th Conference on Neural Information Processing Systems (NIPS) , pp. 353–360. Higdon, D. M. (1998). Auxiliary variable methods for Markov chain Monte Carlo with applications. Journal of the American Statistical Association 93 , 585–595. Hinton, G. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation 14 , 1771–1800. Hinton, G., S. Osindero, and Y. Teh (2006). A fast learning algorithm for deep belief nets. Neural Computation 18 , 1527–1554. Hinton, G. and R. Salakhutdinov (2006). Reducing the dimensionality of data with neural net- works. Science 313 , 504–507. Hinton, G. and T. Sejnowski (1983). Optimal perceptual inference. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pp. 448–453. Hinton, G. E., P. Dayan, B. Frey, and R. M. Neal (1995). The wake-sleep algorithm for unsupervised neural networks. Science 268 , 1158–1161. Höfgen, K. (1993). Learning and robust learning of product distributions. In Proc. Conference on Computational Learning Theory (COLT) , pp. 77–83. Hofmann, R. and V. Tresp (1995). Discovering structure in continuous variables using bayesian networks. In Proc. 9th Conference on Neural Information Processing Systems (NIPS) . Horn, G. and R. McEliece (1997). Belief propagation in loopy bayesian networks: experimental results. In Proceedings if IEEE International Symposium on Information Theory , pp. 232. Horvitz, E. and M. Barry (1995). Display of information for time-critical decision making. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 296–305. Horvitz, E., J. Breese, and M. Henrion (1988). Decision theory in expert systems and artiﬁcial intelligence. International Journal of Approximate Reasoning 2 , 247–302. Special Issue on Uncertainty in Artiﬁcial Intelligence. Horvitz, E., H. Suermondt, and G. Cooper (1989). Bounded conditioning: Flexible inference for decisions under scarce resources. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 182–193. Howard, R. (1970). Decision analysis: Perspectives on inference, decision, and experimentation. Proceedings of the IEEE 58 , 632–643. Howard, R. (1977). Risk preference. In R. Howard and J. Matheson (Eds.), Readings in Decision Analysis , pp. 429–465. Menlo Park, California: Decision Analysis Group, SRI International. Howard, R. and J. Matheson (1984a). Inﬂuence diagrams. See Howard and Matheson (1984b), pp. 721–762. Howard, R. and J. Matheson (Eds.) (1984b). The Principle and Applications of Decision Analysis . Menlo Park, CA, USA: Strategic Decisions Group. Howard, R. A. (1966). Information value theory. IEEE Transactions on Systems Science and Cybernetics SSC-2 , 22–26. Howard, R. A. (1989). Microrisks for medical decision analysis. International Journal of Technology Assessment in Health Care 5 , 357–370. Huang, C. and A. Darwiche (1996). Inference in belief networks: A procedural guide. International 

Journal of Approximate Reasoning 15 (3), 225–263. Huang, F. and Y. Ogata (2002). Generalized pseudo-likelihood estimates for Markov random ﬁelds on lattice. Annals of the Institute of Statistical Mathematics 54 , 1–18. Ihler, A. (2007). Accuracy bounds for belief propagation. In Proc. 23rd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Ihler, A. T., J. W. Fisher, and A. S. Willsky (2003). Message errors in belief propagation. In Proc. 17th Conference on Neural Information Processing Systems (NIPS) . Ihler, A. T., J. W. Fisher, and A. S. Willsky (2005). Loopy belief propagation: Convergence and efects of message errors. Journal of Machine Learning Research 6 , 905–936. Imoto, S., S. Kim, T. Goto, S. Aburatani, K. Tashiro, S. Kuhara, and S. Miyano (2003). Bayesian network and nonparametric heteroscedastic regression for nonlinear modeling of genetic net- work. Journal of Bioinformatics and Computational Biology 1 , 231–252. Indyk, P. (2004). Nearest neighbors in high-dimensional spaces. In J. Goodman and J. O’Rourke (Eds.), Handbook of Discrete and Computational Geometry (2nd ed.). CRC Press. Isard, M. (2003). PAMPAS: Real-valued graphical models for computer vision. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 613–620. Isard, M. and A. Blake (1998a). Condensation — conditional density propagation for visual tracking. International Journal of Computer Vision 29 (1), 5–28. Isard, M. and A. Blake (1998b). A smoothing ﬁlter for condensation. In Proc. European Conference on Computer Vision (ECCV) , Volume 1, pp. 767–781. Isham, V. (1981). An introduction to spatial point processes and Markov random ﬁelds. Interna- tional Statistical Review 49 , 21–43. Ishikawa, H. (2003). Exact optimization for Markov random ﬁelds with convex priors. IEEE Trans. on Pattern Analysis and Machine Intelligence 25 (10), 1333–1336. Ising, E. (1925). Beitrag zur theorie des ferromagnetismus. Z. Phys. 31 , 253–258. Jaakkola, T. (2001). Tutorial on variational approximation methods. In M. Opper and D. Saad (Eds.), Advanced mean ﬁeld methods , pp. 129–160. Cambridge, Massachusetts: MIT Press. Jaakkola, T. and M. Jordan (1996a). Computing upper and lower bounds on likelihoods in intractable networks. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 340–348. Jaakkola, T. and M. Jordan (1996b). Recursive algorithms for approximating probabilities in graphical models. In Proc. 10th Conference on Neural Information Processing Systems (NIPS) , pp. 487–93. Jaakkola, T. and M. Jordan (1997). A variational approach to bayesian logistic regression models and their extensions. In Proc. 6thWorkshop on Artiﬁcial Intelligence and Statistics . Jaakkola, T. and M. Jordan (1998). Improving the mean ﬁeld approximation via the use of mixture models. See Jordan (1998). Jaakkola, T. and M. Jordan (1999). Variational probabilistic inference and the QMR-DT network. Journal of Artiﬁcial Intelligence Research 10 , 291–322. Jarzynski, C. (1997, Apr). Nonequilibrium equality for free energy diferences. Physical Review Letters 78 (14), 2690–2693. Jaynes, E. (2003). Probability Theory: The Logic of Science . Cambridge University Press. Jensen, F., F. V. Jensen, and S. L. Dittmer (1994). From inﬂuence diagrams to junction trees. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 367–73. Jensen, F. and M. Vomlelová (2003). Unconstrained inﬂuence diagrams. In Proc. 19th Conference 

on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 234–41. Jensen, F. V. (1995). Cautious propagation in Bayesian networks. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 323–328. Jensen, F. V. (1996). An introduction to Bayesian Networks . London: University College London Press.Jensen, F. V., K. G. Olesen, and S. K. Andersen (1990, August). An algebra of Bayesian belief universes for knowledge-based systems. Networks 20 (5), 637–659. Jerrum, M. and A. Sinclair (1997). The Markov chain Monte Carlo method. In D. Hochbaum (Ed.), Approximation Algorithms for NP-hard Problems . Boston: PWS Publishing. Ji, C. and L. Seymour (1996). A consistent model selection procedure for Markov random ﬁelds based on penalized pseudo likelihood. Annals of Applied Probability . Jimison, H., L. Fagan, R. Shachter, and E. Shortlife (1992). Patient-speciﬁc explanation in models of chronic disease. AI in Medicine 4 , 191–205. Jordan, M., Z. Ghahramani, T. Jaakkola, and L. K. Saul (1998). An introduction to variational approximations methods for graphical models. See Jordan (1998). Jordan, M. I. (Ed.) (1998). Learning in Graphics Models . Cambridge, MA: The MIT Press. Julier, S. (2002). The scaled unscented transformation. In Proceedings of the American Control Conference , Volume 6, pp. 4555–4559. Julier, S. and J. Uhlmann (1997). A new extension of the Kalman ﬁlter to nonlinear systems. In Proc. of AeroSense: The 11th International Symposium on Aerospace/Defence Sensing, Simulation and Controls . Kahneman, D., P. Slovic, and A. Tversky (Eds.) (1982). Judgment under Uncertainty: Heuristics and Biases . Cambridge: Cambridge University Press. Kalman, R. and R. Bucy (1961). New results in linear ﬁltering and prediction theory. Trans. ASME, Series D, Journal of Basic Engineering . Kalman, R. E. (1960). A new approach to linear ﬁltering and prediction problems. Transactions of the ASME–Journal of Basic Engineering 82 (Series D), 35–45. Kanazawa, K., D. Koller, and S. Russell (1995). Stochastic simulation algorithms for dynamic probabilistic networks. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 346–351. Kass, R. and A. Raftery (1995). Bayes factors. Journal of the American Statistical Association 90 (430), 773–795. Kearns, M., M. L. Littman, and S. Singh (2001). Graphical models for game theory. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 253–260. Kearns, M. and Y. Mansour (1998). Exact inference of hidden structure from sample data in noisy-or networks. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 304–31. Kearns, M., Y. Mansour, and A. Ng (1997). An information-theoretic analysis of hard and soft assignment methods for clustering. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelli- gence (UAI) , pp. 282–293. Keeney, R. L. and H. Raifa (1976). Decisions with Multiple Objectives: Preferences and Value Tradeofs . John Wiley & Sons, Inc. Kersting, K. and L. De Raedt (2007). Bayesian logic programming: Theory and tool. See Getoor and Taskar (2007). Kikuchi, R. (1951). A theory of cooperative phenomena. Physical Review Letters 81 , 988–1003. 

Kim, C.-J. and C. Nelson (1998). State-Space Models with Regime-Switching: Classical and Gibbs- Sampling Approaches with Applications . MIT Press. Kim, J. and J. Pearl (1983). A computational model for combined causal and diagnostic reasoning in inference systems. In Proc. 7th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 190–193. Kirkpatrick, S., C. Gelatt, and M. Vecchi (1983). Optimization by simulated annealing. Science 220 , 671–680. Kitagawa, G. (1996). Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space models. Journal of Computational and Graphical Statistics 5 (1), 1–25. Kjærulf, U. (1990, March). Triangulation of graph — Algorithms giving small total state space. Technical Report R90-09, Aalborg University, Denmark. Kjærulf, U. (1992). A computational scheme for reasoning in dynamic probabilistic networks. In Proc. 8th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 121–129. Kjærulf, U. (1995a). dHugin: A computational system for dynamic time-sliced Bayesian networks. International Journal of Forecasting 11 , 89–111. Kjærulf, U. (1995b). HUGS: Combining exact inference and Gibbs sampling in junction trees. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 368–375. Kjaerulf, U. (1997). Nested junction trees. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 294–301. Kjærulf, U. and L. van der Gaag (2000). Making sensitivity analysis computationally efcient. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 317–325. Koivisto, M. and K. Sood (2004). Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research 5 , 549–573. Kok, J., M. Spaan, and N. Vlassis (2003). Multi-robot decision making using coordination graphs. In Proc. International Conference on Advanced Robotics (ICAR) , pp. 1124–1129. Kok, J. and N. Vlassis (2005). Using the max-plus algorithm for multiagent decision making in coordination graphs. In RoboCup-2005: Robot Soccer World Cup IX , Osaka, Japan. Koller, D. and R. Fratkina (1998). Using learning for approximation in stochastic processes. In Proc. 15th International Conference on Machine Learning (ICML) , pp. 287–295. Koller, D., U. Lerner, and D. Anguelov (1999). A general algorithm for approximate inference and its application to hybrid Bayes nets. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 324–333. Koller, D. and B. Milch (2001). Multi-agent inﬂuence diagrams for representing and solving games. In Proc. 17th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1027–1034. Koller, D. and B. Milch (2003). Multi-agent inﬂuence diagrams for representing and solving games. Games and Economic Behavior 45 (1), 181–221. Full version of paper in IJCAI $'03$ . Koller, D. and R. Parr (1999). Computing factored value functions for policies in structured MDPs. In Proc. 16th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1332–1339. Koller, D. and A. Pfefer (1997). Object-oriented Bayesian networks. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 302–313. Kolmogorov, V. (2006). Convergent tree-reweighted message passing for energy minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence . Kolmogorov, V. and C. Rother (2006). Comparison of energy minimization algorithms for highly connected graphs. In Proc. European Conference on Computer Vision (ECCV) . Kolmogorov, V. and M. Wainwright (2005). On the optimality of tree reweighted max-product 

message passing. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Kolmogorov, V. and R. Zabih (2004). What energy functions can be minimized via graph cuts? IEEE Transactions on Pattern Analysis and Machine Intelligence 26 (2). Komarek, P. and A. Moore (2000). A dynamic adaptation of AD-trees for efcient machine learning on large data sets. In Proc. 17th International Conference on Machine Learning (ICML) , pp. 495–502. Komodakis, N., N. Paragios, and G. Tziritas (2007). MRF optimization via dual decomposition: Message-passing revisited. In Proc. International Conference on Computer Vision (ICCV) . Komodakis, N. and G. Tziritas (2005). A new framework for approximate labeling via graph-cuts. In Proc. International Conference on Computer Vision (ICCV) . Komodakis, N., G. Tziritas, and N. Paragios (2007). Fast, approximately optimal solutions for single and dynamic MRFs. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR) . Kong, A. (1991). Efcient methods for computing linkage likelihoods of recessive diseases in inbred pedigrees. Genetic Epidemiology 8 , 81–103. Korb, K. and A. Nicholson (2003). Bayesian Artiﬁcial Intelligence . CRC Press. Koster, J. (1996). Markov properties of non-recursive causal models. The Annals of Statistics 24 (5), 2148–77. Koˇ cka, T., R. Bouckaert, and M. Studený (2001). On characterizing inclusion of Bayesian networks. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 261–68. Kozlov, A. and D. Koller (1997). Nonuniform dynamic discretization in hybrid networks. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 314–325. Krause, A. and C. Guestrin (2005a). Near-optimal nonmyopic value of information in graphical models. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Krause, A. and C. Guestrin (2005b). Optimal nonmyopic value of information in graphical models: Efcient algorithms and theoretical limits. In Proc. 19th International Joint Conference on Artiﬁcial Intelligence (IJCAI) . Kreps, D. (1988). Notes on the Theory of Choice . Boulder, Colorado: Westview Press. Kschischang, F. and B. Frey (1998). Iterative decoding of compound codes by probability propa- gation in graphical models. IEEE Journal on Selected Areas in Communications 16 , 219–230. Kschischang, F., B. Frey, and H.-A. Loeliger (2001a). Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory 47 , 498–519. Kschischang, F., B. Frey, and H.-A. Loeliger (2001b). Factor graphs and the sum-product algorithm. IEEE Trans. Information Theory 47 , 498–519. Kullback, S. (1959). Information Theory and Statistics . New York: John Wiley & Sons. Kumar, M., V. Kolmogorov, and P. Torr (2007). An analysis of convex relaxations for MAP estimation. In Proc. 21st Conference on Neural Information Processing Systems (NIPS) . Kumar, M., P. Torr, and A. Zisserman (2006). Solving Markov random ﬁelds using second order cone programming relaxations. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1045–1052. Kuppermann, M., S. Shiboski, D. Feeny, E. Elkin, and A. Washington (1997, Jan–Mar). Can preference scores for discrete states be used to derive preference scores for an entire path of events? An application to prenatal diagnosis. Medical Decision Making 17 (1), 42–55. Kyburg, H., , and H. Smokler (Eds.) (1980). Studies in Subjective Probability . New York: Krieger. La Mura, P. (2000). Game networks. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence 

(UAI) , pp. 335–342. La Mura, P. and Y. Shoham (1999). Expected utility networks. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 366–73. Lacoste-Julien, S., B. Taskar, D. Klein, and M. Jordan (2006, June). Word alignment via quadratic assignment. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference , pp. 112–119. Laferty, J., A. McCallum, and F. Pereira (2001). Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conference on Machine Learning (ICML) . Lam, W. and F. Bacchus (1993). Using causal information and local measures to learn Bayesian networks. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 243–250. Lange, K. and R. C. Elston (1975). Extensions to pedigree analysis. I. Likelihood calculations for simple and complex pedigrees. Human Heredity 25 , 95–105. Laskey, K. (1995). Sensitivity analysis for probability assessments in Bayesian networks. IEEE Transactions on Systems, Man, and Cybernetics 25 (6), 901 – 909. Lauritzen, S. (1982). Lectures on contingency tables (2 ed.). Aalborg: Denmark: University of Aalborg Press.Lauritzen, S. (1992). Propagation of probabilities, means, and variances in mixed graphical association models. Journal of the American Statistical Association 87 (420), 1089–1108. Lauritzen, S. (1996). Graphical Models . New York: Oxford University Press. Lauritzen, S. and D. Nilsson (2001). Representing and solving decision problems with limited information. Management Science 47 (9), 1235–51. Lauritzen, S. L. (1995). The EM algorithm for graphical association models with missing data. Computational Statistics and Data Analysis 19 , 191–201. Lauritzen, S. L. and F. Jensen (2001). Stable local computation with conditional Gaussian distri- butions. Statistics and Computing 11 , 191–203. Lauritzen, S. L. and D. J. Spiegelhalter (1988). Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, Series B B 50 (2), 157–224. Lauritzen, S. L. and N. Wermuth (1989). Graphical models for associations between variables, some of which are qualitative and some quantitative. Annals of Statistics 17 , 31–57. LeCun, Y., S. Chopra, R. Hadsell, R. Marc’Aurelio, and F.-J. Huang (2007). A tutorial on energy- based learning. In G. Bakir, T. Hofmann, B. Schölkopf, A. Smola, B. Taskar, and S. Vishwanathan (Eds.), Predicting Structured Data . MIT Press. Lee, S.-I., V. Ganapathi, and D. Koller (2006). Efcient structure learning of Markov networks using L1-regularization. In Proc. 20th Conference on Neural Information Processing Systems (NIPS) . Lehmann, E. and J. Romano (2008). Testing Statistical Hypotheses . Springer Texts in Statistics. Leisink, M. A. R. and H. J. Kappen (2003). Bound propagation. Journal of Artiﬁcial Intelligence Research 19 , 139–154. Lerner, U. (2002). Hybrid Bayesian Networks for Reasoning about Complex Systems . Ph.D. thesis, Stanford University. Lerner, U., B. Moses, M. Scott, S. McIlraith, and D. Koller (2002). Monitoring a complex physical system using a hybrid dynamic Bayes net. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 301–310. 

Lerner, U. and R. Parr (2001). Inference in hybrid networks: Theoretical limits and practical algorithms. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 310–318. Lerner, U., R. Parr, D. Koller, and G. Biswas (2000). Bayesian fault detection and diagnosis in dynamic systems. In Proc. 16th Conference on Artiﬁcial Intelligence (AAAI) , pp. 531–537. Lerner, U., E. Segal, and D. Koller (2001). Exact inference in networks with discrete children of continuous parents. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 319–328. Li, S. (2001). Markov Random Field Modeling in Image Analysis . Springer. Liang, P. and M. Jordan (2008). An asymptotic analysis of generative, discriminative, and pseu- dolikelihood estimators. In Proc. 25th International Conference on Machine Learning (ICML) . Little, R. J. A. (1976). Inference about means for incomplete multivariate data. Biometrika 63 , 593–604. Little, R. J. A. and D. B. Rubin (1987). Statistical Analysis with Missing Data . New York: John Wiley & Sons. Liu, D. and J. Nocedal (1989). On the limited memory method for large scale optimization. Mathematical Programming 45 (3), 503–528. Liu, J., W. Wong, and A. Kong (1994). Covariance structure of the Gibbs sampler with applications to the comparisons of estimators and sampling schemes. Biometrika 81 , 27–40. Loomes, G. and R. Sugden (1982). Regret theory: An alternative theory of rational choice under uncertainty. The Economic Journal 92 , 805–824. MacEachern, S. and L. Berliner (1994, August). Subsampling the Gibbs sampler. The American Statistician 48 (3), 188–190. MacKay, D. J. C. (1997). Ensemble learning for hidden markov models. Unpublished manuscripts, http://wol.ra.phy.cam.ac.uk/mackay. MacKay, D. J. C. and R. M. Neal (1996). Near shannon limit performance of low density parity check codes. Electronics Letters 32 , 1645–1646. Madigan, D., S. Andersson, M. Perlman, and C. Volinsky (1996). Bayesian model averaging and model selection for Markov equivalence classes of acyclic graphs. Communications in Statistics: Theory and Methods 25 , 2493–2519. Madigan, D. and E. Raftery (1994). Model selection and accounting for model uncertainty in graphical models using Occam’s window. Journal of the American Statistical Association 89 , 1535–1546. Madigan, D. and J. York (1995). Bayesian graphical models for discrete data. International statistical Review 63 , 215–232. Madsen, A. and D. Nilsson (2001). Solving inﬂuence diagrams using HUGIN, Shafer-Shenoy and lazy propagation. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 337–45. Malioutov, D., J. Johnson, and A. Willsky (2006). Walk-sums and belief propagation in Gaussian graphical models. Journal of Machine Learning Research 7 , 2031–64. Maneva, E., E. Mossel, and M. Wainwright (2007, July). A new look at survey propagation and its generalizations. Journal of the ACM 54 (4), 2–41. Manning, C. and H. Schuetze (1999). Foundations of Statistical Natural Language Processing . MIT Press.Marinari, E. and G. Parisi (1992). Simulated tempering: A new Monte Carlo scheme. Europhysics Letters 19 , 451. 

Marinescu, R., K. Kask, and R. Dechter (2003). Systematic vs. non-systematic algorithms for solving the MPE task. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Marthi, B., H. Pasula, S. Russell, and Y. Peres (2002). Decayed MCMC ﬁltering. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Martin, J. and K. VanLehn (1995). Discrete factor analysis: Learning hidden variables in Bayesian networks. Technical report, Department of Computer Science, University of Pittsburgh. McCallum, A. (2003). Efciently inducing features of conditional random ﬁelds. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 403–10. McCallum, A., C. Pal, G. Druck, and X. Wang (2006). Multi-conditional learning: Genera- tive/discriminative training for clustering and classiﬁcation. In Proc. 22nd Conference on Arti- ﬁcial Intelligence (AAAI) . McCallum, A. and B. Wellner (2005). Conditional models of identity uncertainty with application to noun coreference. In Proc. 19th Conference on Neural Information Processing Systems (NIPS) , pp. 905–912. McCullagh, P. and J. Nelder (1989). Generalized Linear Models . London: Chapman & Hall. McEliece, R., D. MacKay, and J.-F. Cheng (1998, February). Turbo decoding as an instance of Pearl’s “belief propagation” algorithm. IEEE Journal on Selected Areas in Communications 16 (2). McEliece, R. J., E. R. Rodemich, and J.-F. Cheng (1995). The turbo decision algorithm. In Proc. 33rd Allerton Conference on Communication Control and Computing , pp. 366–379. McLachlan, G. J. and T. Krishnan (1997). The EM Algorithm and Extensions . Wiley Interscience. Meek, C. (1995a). Causal inference and causal explanation with background knowledge. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 403–418. Meek, C. (1995b). Strong completeness and faithfulness in Bayesian networks. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 411–418. Meek, C. (1997). Graphical Models: Selecting causal and statistical models . Ph.D. thesis, Carnegie Mellon University. Meek, C. (2001). Finding a path is harder than ﬁnding a tree. Journal of Artiﬁcial Intelligence Research 15 , 383–389. Meek, C. and D. Heckerman (1997). Structure and parameter learning for causal independence and causal interaction models. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 366–375. Meila, M. and T. Jaakkola (2000). Tractable Bayesian learning of tree belief networks. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Meila, M. and M. Jordan (2000). Learning with mixtures of trees. Journal of Machine Learning Research 1 , 1–48. Meltzer, T., C. Yanover, and Y. Weiss (2005). Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation. In Proc. International Conference on Computer Vision (ICCV) , pp. 428–435. Metropolis, N., A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller (1953). Equation of state calculation by fast computing machines. Journal of Chemical Physics 21 , 1087–1092. Meyer, J., M. Phillips, P. Cho, I. Kalet, and J. Doctor (2004). Application of inﬂuence diagrams to prostate intensity-modulated radiation therapy plan selection. Physics in Medicine and Biology 49 , 1637–53. Middleton, B., M. Shwe, D. Heckerman, M. Henrion, E. Horvitz, H. Lehmann, and G. Cooper (1991). Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base. 

II. Evaluation of diagnostic performance. Methods of Information in Medicine 30 , 256–67. Milch, B., B. Marthi, and S. Russell (2004). BLOG: Relational modeling with unknown objects. In ICML 2004 Workshop on Statistical Relational Learning and Its Connections to Other Fields . Milch, B., B. Marthi, S. Russell, D. Sontag, D. Ong, and A. Kolobov (2005). BLOG: Probabilis- tic models with unknown objects. In Proc. 19th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1352–1359. Milch, B., B. Marthi, S. Russell, D. Sontag, D. Ong, and A. Kolobov (2007). BLOG: Probabilistic models with unknown objects. See Getoor and Taskar (2007). Miller, R., H. Pople, and J. Myers (1982). Internist-1, an experimental computer-based diagnostic consultant for general internal medicine. New England Journal of Medicine 307 , 468–76. Minka, T. (2005). Discriminative models, not discriminative training. Technical Report MSR-TR- 2005-144, Microsoft Research. Minka, T. and J. Laferty (2002). Expectation propagation for the generative aspect model. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Minka, T. P. (2001a). Algorithms for maximum-likelihood logistic regression. Available from http://www.stat.cmu.edu/\~minka/papers/logreg.html . Minka, T. P. (2001b). Expectation propagation for approximate Bayesian inference. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 362–369. Møller, J. M., A. Pettitt, K. Berthelsen, and R. Reeves (2006). An efcient Markov chain Monte Carlo method for distributions with intractable normalisation constants. Biometrika 93 (2), 451–458. Montemerlo, M., S. Thrun, D. Koller, and B. Wegbreit (2002). FastSLAM: A factored solution to the simultaneous localization and mapping problem. In Proc. 18th Conference on Artiﬁcial Intelligence (AAAI) , pp. 593–598. Monti, S. and G. F. Cooper (1997). Learning Bayesian belief networks with neural network estimators. In Proc. 11th Conference on Neural Information Processing Systems (NIPS) , pp. 579– 584. Mooij, J. M. and H. J. Kappen (2007). Sufcient conditions for convergence of the sum-product algorithm. IEEE Trans. Information Theory 53 , 4422–4437. Moore, A. (2000). The anchors hierarchy: Using the triangle inequality to survive high- dimensional data. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 397–405. Moore, A. and W.-K. Wong (2003). Optimal reinsertion: A new search operator for accelerated and more accurate bayesian network structure learning. In Proc. 20th International Conference on Machine Learning (ICML) , pp. 552–559. Moore, A. W. and M. S. Lee (1997). Cached sufcient statistics for efcient machine learning with large datasets. Journal of Artiﬁcial Intelligence Research 8 , 67–91. Morgan, M. and M. Henrion (Eds.) (1990). Uncertainty: A Guide to Dealing with Uncertainty in Quantitative Risk and Policy Analysis . Cambridge University Press. Motwani, R. and P. Raghavan (1995). Randomized Algorithnms . Cambridge University Press. Muramatsu, M. and T. Suzuki (2003). A new second-order cone programming relaxation for max-cut problems. Journal of Operations Research of Japan 43 , 164–177. Murphy, K. (1999). Bayesian map learning in dynamic environments. In Proc. 13th Conference on Neural Information Processing Systems (NIPS) . Murphy, K. (2002). Dynamic Bayesian Networks: A tutorial. Technical report, Mas- 

sachussetts Institute of Technology. Available from http://www.cs.ubc.ca/\~murphyk/ Papers/dbnchapter.pdf . Murphy, K. and M. Paskin (2001). Linear time inference in hierarchical HMMs. In Proc. 15th Conference on Neural Information Processing Systems (NIPS) . Murphy, K. and Y. Weiss (2001). The factored frontier algorithm for approximate inference in DBNs. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Murphy, K. P. (1998). Inference and learning in hybrid Bayesian networks. Technical Report UCB/CSD-98-990, University of California, Berkeley. Murphy, K. P., Y. Weiss, and M. Jordan (1999). Loopy belief propagation for approximate inference: an empirical study. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 467–475. Murray, I. and Z. Ghahramani (2004). Bayesian learning in undirected graphical models: Ap- proximate MCMC algorithms. In Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Murray, I., Z. Ghahramani, and D. MacKay (2006). MCMC for doubly-intractable distributions. In Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Myers, J., K. Laskey, and T. Levitt (1999). Learning Bayesian networks from incomplete data with stochastic search algorithms. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 476–485. Narasimhan, M. and J. Bilmes (2004). PAC-learning bounded tree-width graphical models. In Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Ndilikilikesha, P. (1994). Potential inﬂuence diagrams. International Journal of Approximate Reasoning 10 , 251–85. Neal, R. (1996). Sampling from multimodal distributions using tempered transitions. Statistics and Computing 6 , 353–366. Neal, R. (2001). Annealed importance sampling. Statistics and Computing 11 (2), 25–139. Neal, R. (2003). Slice sampling. Annals of Statistics 31 (3), 705–767. Neal, R. M. (1992). Asymmetric parallel Boltzmann machines are belief networks. Neural Com- putation 4 (6), 832–834. Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, University of Toronto. Neal, R. M. and G. E. Hinton (1998). A new view of the EM algorithm that justiﬁes incremental and other variants. See Jordan (1998). Neapolitan, R. E. (2003). Learning Bayesian Networks . Prentice Hall. Ng, A. and M. Jordan (2000). Approximate inference algorithms for two-layer Bayesian networks. In Proc. 14th Conference on Neural Information Processing Systems (NIPS) . Ng, A. and M. Jordan (2002). On discriminative vs. generative classiﬁers: A comparison of logistic regression and naive Bayes. In Proc. 16th Conference on Neural Information Processing Systems (NIPS) . Ng, B., L. Peshkin, and A. Pfefer (2002). Factored particles for scalable monitoring. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 370–377. Ngo, L. and P. Haddawy (1996). Answering queries from context-sensitive probabilistic knowledge bases. Theoretical Computer Science . Nielsen, J., T. Koˇ cka, and J. M. Peña (2003). On local optima in learning Bayesian networks. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 435–442. 

Nielsen, T. and F. Jensen (1999). Welldeﬁned decision scenarios. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 502–11. Nielsen, T. and F. Jensen (2000). Representing and solving asymmetric Bayesian decision prob- lems. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 416–25. Nielsen, T., P.-H. Wuillemin, F. Jensen, and U. Kjærulf (2000). Using robdds for inference in Bayesian networks with troubleshooting as an example. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 426–35. Nilsson, D. (1998). An efcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert systems. Statistics and Computing 8 (2), 159–173. Nilsson, D. and S. Lauritzen (2000). Evaluating inﬂuence diagrams with LIMIDs. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 436–445. Nodelman, U., C. R. Shelton, and D. Koller (2002). Continuous time Bayesian networks. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 378–387. Nodelman, U., C. R. Shelton, and D. Koller (2003). Learning continuous time Bayesian networks. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Norman, J., Y. Shahar, M. Kuppermann, and B. Gold (1998). Decision-theoretic analysis of prenatal testing strategies. Technical Report SMI-98-0711, Stanford University, Section on Medical Informatics. Normand, S.-L. and D. Tritchler (1992). Parameter updating in a Bayes network. Journal of the American Statistical Association 87 , 1109–1115. Nummelin, E. (1984). General Irreducible Markov Chains and Non-Negative Operators . Cambridge University Press. Nummelin, E. (2002). Mc’s for mcmc’ists. International Statistical Review 70 (2), 215–240. Olesen, K. G., U. Kjærulf, F. Jensen, B. Falck, S. Andreassen, and S. Andersen (1989). A Munin network for the median nerve — A case study on loops. Applied Artiﬁcial Intelligence 3 , 384–403. Oliver, R. M. and J. Q. Smith (Eds.) (1990). Inﬂuence Diagrams, Belief Nets and Decision Analysis . New York: John Wiley & Sons. Olmsted, S. (1983). On Representing and Solving Inﬂuence Diagrams . Ph.D. thesis, Stanford University. Opper, M. and O. Winther (2005). Expectation consistent free energies for approximate inference. In Proc. 19th Conference on Neural Information Processing Systems (NIPS) . Ortiz, L. and L. Kaelbling (1999). Accelerating em: An empirical study. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 512–521. Ortiz, L. E. and L. P. Kaelbling (2000). Adaptive importance sampling for estimation in structured domains. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 446–454. Osborne, M. and A. Rubinstein (1994). A Course in Game Theory . The MIT Press. Ostendorf, M., V. Digalakis, and O. Kimball (1996). From HMMs to segment models: A uniﬁed view of stochastic modeling for speech recognition. IEEE Transactions on Speech and Audio Processing 4 (5), 360–378. Pakzad, P. and V. Anantharam (2002). Minimal graphical representation of Kikuchi regions. In Proc. 40th Allerton Conference on Communication Control and Computing , pp. 1585–1594. Papadimitriou, C. (1993). Computational Complexity . Addison Wesley. Parisi, G. (1988). Statistical Field Theory . Reading, Massachusetts: Addison-Wesley. Park, J. (2002). MAP complexity results and approximation methods. In Proc. 18th Conference on 

Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 388–396. Park, J. and A. Darwiche (2001). Approximating MAP using local search. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 403–â A¸ S410. Park, J. and A. Darwiche (2003). Solving MAP exactly using systematic search. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Park, J. and A. Darwiche (2004a). Complexity results and approximation strategies for MAP explanations. Journal of Artiﬁcial Intelligence Research 21 , 101–133. Park, J. and A. Darwiche (2004b). A diferential semantics for jointree algorithms. Artiﬁcial Intelligence 156 , 197–216. Parter, S. (1961). The user of linear graphs in Gauss elimination. SIAM Review 3 , 119–130. Paskin, M. (2003a). Sample propagation. In Proc. 17th Conference on Neural Information Processing Systems (NIPS) . Paskin, M. (2003b). Thin junction tree ﬁlters for simultaneous localization and mapping. In Proc. 18th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 1157–1164. Pasula, H., B. Marthi, B. Milch, S. Russell, and I. Shpitser (2002). Identity uncertainty and citation matching. In Proc. 16th Conference on Neural Information Processing Systems (NIPS) , pp. 1401–1408. Pasula, H., S. Russell, M. Ostland, and Y. Ritov (1999). Tracking many objects with many sensors. In Proc. 16th International Joint Conference on Artiﬁcial Intelligence (IJCAI) . Patrick, D., J. Bush, and M. Chen (1973). Methods for measuring levels of well-being for a health status index. Health Services Research 8 , 228–45. Pearl, J. (1986a). A constraint-propagation approach to probabilistic reasoning. In Proc. 2nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 357–370. Pearl, J. (1986b). Fusion, propagation and structuring in belief networks. Artiﬁcial Intelli- gence 29 (3), 241–88. Pearl, J. (1987). Evidential reasoning using stochastic simulation of causal models. Artiﬁcial Intelligence 32 , 245–257. Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems . San Mateo, California: Morgan Kaufmann. Pearl, J. (1995). Causal diagrams for empirical research. Biometrika 82 , 669–710. Pearl, J. (2000). Causality: Models, Reasoning, and Inference . Cambridge Univ. Press. Pearl, J. and R. Dechter (1996). Identifying independencies in causal graphs with feedback. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 420–26. Pearl, J. and A. Paz (1987). GRAPHOIDS: A graph-based logic for reasoning about relevance relations. In B. Du Boulay, D. Hogg, and L. Steels (Eds.), Advances in Artiﬁcial Intelligence , Volume 2, pp. 357–363. Amsterdam: North Holland. Pearl, J. and T. S. Verma (1991). A theory of inferred causation. In Proc. Conference on Knowledge Representation and Reasoning (KR) , pp. 441–452. Pe’er, D., A. Regev, G. Elidan, and N. Friedman (2001). Inferring subnetworks from preturbed expression proﬁles. Bioinformatics 17 , S215–S224. Peng, Y. and J. Reggia (1986). Plausibility of diagnostic hypotheses. In Proc. 2nd Conference on Artiﬁcial Intelligence (AAAI) , pp. 140–45. Perkins, S., K. Lacker, and J. Theiler (2003, March). Grafting: Fast, incremental feature selection by gradient descent in function space. Journal of Machine Learning Research 3 , 1333–1356. Peterson, C. and J. R. Anderson (1987). A mean ﬁeld theory learning algorithm for neural 

networks. Complex Systems 1 , 995–1019. Pfefer, A., D. Koller, B. Milch, and K. Takusagawa (1999). spook : A system for probabilistic object-oriented knowledge representation. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 541–550. Poh, K. and E. Horvitz (2003). Reasoning about the value of decision-model reﬁnement: Methods and application. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 174– 182. Poland, W. (1994). Decision Analysis with Continuous and Discrete Variables: A Mixture Distribution Approach . Ph.D. thesis, Department of Engineering-Economic Systems, Stanford University. Poole, D. (1989). Average-case analysis of a search algorithm for estimating prior and posterior probabilities in Bayesian networks with extreme probabilities. In Proc. 13th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 606–612. Poole, D. (1993a). Probabilistic Horn abduction and Bayesian networks. Artiﬁcial Intelligence 64 (1), 81–129. Poole, D. (1993b). The use of conﬂicts in searching Bayesian networks. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 359–367. Poole, D. and N. Zhang (2003). Exploiting causal independence in Bayesian network inference. Journal of Artiﬁcial Intelligence Research 18 , 263–313. Poon, H. and P. Domingos (2007). Joint inference in information extraction. In Proc. 23rd Conference on Artiﬁcial Intelligence (AAAI) , pp. 913–918. Pradhan, M. and P. Dagum (1996). Optimal Monte Carlo estimation of belief network inference. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 446–453. Pradhan, M., M. Henrion, G. Provan, B. Del Favero, and K. Huang (1996). The sensitivity of belief networks to imprecise probabilities: An experimental investigation. Artiﬁcial Intelligence 85 , 363–97. Pradhan, M., G. M. Provan, B. Middleton, and M. Henrion (1994). Knowledge engineering for large belief networks. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 484–490. Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming . John Wiley and Sons, New York. Qi, R., N. Zhang, and D. Poole (1994). Solving asymmetric decision problems with inﬂuence diagrams. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 491–497. Qi, Y., M. Szummer, and T. Minka (2005). Bayesian conditional random ﬁelds. In Proc. 11thWork- shop on Artiﬁcial Intelligence and Statistics . Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE 77 (2), 257–286. Rabiner, L. R. and B. H. Juang (1986, January). An introduction to hidden Markov models. IEEE ASSP Magazine , 4–15. Ramsey, F. (1931). The Foundations of Mathematics and other Logical Essays . London: Kegan, Paul, Trench, Trubner & Co., New York: Harcourt, Brace and Company. edited by R.B. Braithwaite. Rasmussen, C. and C. Williams (2006). Gaussian Processes for Machine Learning . MIT Press. Rasmussen, C. E. (1999). The inﬁnite gaussian mixture model. In Proc. 13th Conference on Neural Information Processing Systems (NIPS) , pp. 554–560. Ravikumar, P. and J. Laferty (2006). Quadratic programming relaxations for metric labelling and Markov random ﬁeld MAP estimation. In Proc. 23rd International Conference on Machine 

Learning (ICML) . Renooij, S. and L. van der Gaag (2002). From qualitative to quantitative probabilistic networks. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 422–429. Richardson, M. and P. Domingos (2006). Markov logic networks. Machine Learning 62 , 107–136. Richardson, T. (1994). Properties of cyclic graphical models. Master’s thesis, Carnegie Mellon University. Riezler, S. and A. Vasserman (2004). Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling. In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP) . Ripley, B. D. (1987). Stochastic Simulation . New York: John Wiley & Sons. Rissanen, J. (1987). Stochastic complexity (with discussion). Journal of the Royal Statistical Society, Series B 49 , 223–265. Ristic, B., S. Arulampalam, and N. Gordon (2004). Beyond the Kalman Filter: Particle Filters for Tracking Applications . Artech House Publishers. Robert, C. and G. Casella (1996). Rao-Blackwell is ation of sampling schemes. Biometrika 83 (1), 81–94. Robert, C. and G. Casella (2005). Monte Carlo Statistical Methods (2nd ed.). Springer Texts in Statistics. Robins, J. M. and L. A. Wasserman (1997). Estimation of efects of sequential treatments by re parameter i zing directed acyclic graphs. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 409–420. Rose, D. (1970). Triangulated graphs and the elimination process. Journal of Mathematical Analysis and Applications 32 , 597–609. Ross, S. M. (1988). A First Course in Probability (third ed.). London: Macmillan. Rother, C., S. Kumar, V. Kolmogorov, and A. Blake (2005). Digital tapestry. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR) . Rubin, D. (1974). Estimating causal efects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology 66 (5), 688–701. Rubin, D. R. (1976). Inference and missing data. Biometrika 63 , 581–592. Rus me vi chien tong, P. and B. Van Roy (2001). An analysis of belief propagation on the turbo decoding graph with Gaussian densities. IEEE Transactions on Information Theory 48 (2). Russell, S. and P. Norvig (2003). Artiﬁcial Intelligence: A Modern Approach (2 ed.). Prentice Hall. Rustagi, J. (1976). Variational Methods in Statistics . New York: Academic Press. Sachs, K., O. Perez, D. Pe’er, D. Laufenburger, and G. Nolan (2005, April). Causal protein-signaling networks derived from multiparameter single-cell data. Science 308 (5721), 523–529. Sakurai, J. J. (1985). Modern Quantum Mechanics . Reading, Massachusetts: Addison-Wesley. Santos, A. (1994). A linear constraint satisfaction approach to cost-based abduction. Artiﬁcial Intelligence 65 (1), 1–28. Santos, E. (1991). On the generation of alternative explanations with implications for belief revision. In Proc. 7th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 339–347. Saul, L., T. Jaakkola, and M. Jordan (1996). Mean ﬁeld theory for sigmoid belief networks. Journal of Artiﬁcial Intelligence Research 4 , 61–76. Saul, L. and M. Jordan (1999). Mixed memory Markov models: Decomposing complex stochastic processes as mixture of simpler ones. Machine Learning 37 (1), 75–87. Saul, L. K. and M. I. Jordan (1996). Exploiting tractable substructures in intractable networks. In 

Proc. 10th Conference on Neural Information Processing Systems (NIPS) . Savage, L. (1951). The theory of statistical decision. Journal of the American Statistical Associa- tion 46 , 55–67. Savage, L. J. (1954). Foundations of Statistics . New York: John Wiley & Sons. Schäfer, A. (1996). Faster linkage analysis computations for pedigrees with loops or unused alleles. Human Heredity , 226–235. Scharstein, D. and R. Szeliski (2003). High-accuracy stereo depth maps using structured light. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR) , Volume 1, pp. 195–202. Schervish, M. (1995). Theory of Statistics . Springer-Verlag. Schlesinger, M. (1976). Sintaksicheskiy analiz dvumernykh zritelnikh singnalov v usloviyakh pomekh (syntactic analysis of two-dimensional visual signals in noisy conditions). Kiber- netika 4 , 113–130. Schlesinger, M. and V. Giginyak (2007a). Solution to structural recognition $(\mathrm{max,+)}$ )-problems by their equivalent transformations (part 1). Control Systems and Computers 1 , 3–15. Schlesinger, M. and V. Giginyak (2007b). Solution to structural recognition $(\mathrm{max,+)}$ -problems by their equivalent transformations (part 2). Control Systems and Computers 2 , 3–18. Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6 (2), 461–464. Segal, E., D. Pe’er, A. Regev, D. Koller, and N. Friedman (2005, April). Learning module networks. Journal of Machine Learning Research 6 , 557–588. Segal, E., B. Taskar, A. Gasch, N. Friedman, and D. Koller (2001). Rich probabilistic models for gene expression. Bioinformatics 17 (Suppl 1), S243–52. Settimi, R. and J. Smith (2000). Geometry, moments and conditional independence trees with hidden variables. Annals of Statistics . Settimi, R. and J. Q. Smith (1998a). On the geometry of Bayesian graphical models with hidden variables. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 472–479. Settimi, R. and J. Q. Smith (1998b). On the geometry of Bayesian graphical models with hidden variables. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 472–479. Shachter, R. (1988, July–August). Probabilistic inference and inﬂuence diagrams. Operations Research 36 , 589–605. Shachter, R. (1999). Efcient value of information computation. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 594–601. Shachter, R., S. K. Andersen, and P. Szolovits (1994). Global conditioning for probabilistic inference in belief networks. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 514–522. Shachter, R. and D. Heckerman (1987). Thinking backwards for knowledge acquisition. Artiﬁcial Intelligence Magazine 8 , 55 – 61. Shachter, R. and C. Kenley (1989). Gaussian inﬂuence diagrams. Management Science 35 , 527–550. Shachter, R. and P. Ndilikilikesha (1993). Using inﬂuence diagrams for probabilistic inference and decision making. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 276–83. Shachter, R. D. (1986). Evaluating inﬂuence diagrams. Operations Research 34 , 871–882. Shachter, R. D. (1989). Evidence absorption and propagation through evidence reversals. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 173–190. Shachter, R. D. (1998). Bayes-ball: The rational pastime. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 480–487. 

Shachter, R. D., B. D’Ambrosio, and B. A. Del Favero (1990). Symbolic probabilistic inference in belief networks. In Proc. 6th Conference on Artiﬁcial Intelligence (AAAI) , pp. 126–131. Shachter, R. D. and M. A. Peot (1989). Simulation approaches to general probabilistic inference on belief networks. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 221–230. Shachter, R. D. and M. A. Peot (1992). Decision making using probabilistic inference methods. In Proc. 8th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 276–83. Shafer, G. and J. Pearl (Eds.) (1990). Readings in Uncertain Reasoning . Representation and Reasoning. San Mateo, California: Morgan Kaufmann. Shafer, G. and P. Shenoy (1990). Probability propagation. Annals of Mathematics and Artiﬁcial Intelligence 2 , 327–352. Shannon, C. (1948). A mathematical theory of communication. Bell System Technical Journal 27 , 379–423; 623–656. Shawe-Taylor, J. and N. Cristianini (2000). Support Vector Machines and other kernel-based learning methods . Cambridge University Press. Shenoy, P. (1989). A valuation-based language for expert systems. International Journal of Ap- proximate Reasoning 3 , 383–411. Shenoy, P. (2000). Valuation network representation and solution of asymmetric decision prob- lems. European Journal of Operational Research 121 (3), 579–608. Shenoy, P. and G. Shafer (1990). Axioms for probability and belief-function propagation. In Proc. 6th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 169–198. Shenoy, P. P. (1992). Valuation-based systems for Bayesian decision analysis. Operations Re- search 40 , 463–484. Shental, N., A. Zomet, T. Hertz, and Y. Weiss (2003). Learning and inferring image segmentations using the GBP typical cut algorithm. In Proc. International Conference on Computer Vision . Shimony, S. (1991). Explanation, irrelevance and statistical independence. In Proc. 7th Conference on Artiﬁcial Intelligence (AAAI) . Shimony, S. (1994). Finding MAPs for belief networks in NP-hard. Artiﬁcial Intelligence 68 (2), 399–410. Shoikhet, K. and D. Geiger (1997). A practical algorithm for ﬁnding optimal triangulations. In Proc. 13th Conference on Artiﬁcial Intelligence (AAAI) , pp. 185–190. Shwe, M. and G. Cooper (1991). An empirical analysis of likelihood-weighting simulation on a large, multiply connected medical belief network. Computers and Biomedical Research 24 , 453–475. Shwe, M., B. Middleton, D. Heckerman, M. Henrion, E. Horvitz, H. Lehmann, and G. Cooper (1991). Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base. I. The probabilistic model and inference algorithms. Methods of Information in Medicine 30 , 241–55. Silander, T. and P. Myllymaki (2006). A simple approach for ﬁnding the globally optimal Bayesian network structure. In Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Singh, A. and A. Moore (2005). Finding optimal bayesian networks by dynamic programming. Technical report, Carnegie Mellon University. Sipser, M. (2005). Introduction to the Theory of Computation (Second ed.). Course Technology. Smith, A. and G. Roberts (1993). Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods. Journal of the Royal Statistical Society, Series B 55 , 3–23. 

Smith, J. (1989). Inﬂuence diagrams for statistical modeling. Annals of Statistics 17 (2), 654–72. Smith, J., S. Holtzman, and J. Matheson (1993). Structuring conditional relationships in inﬂuence diagrams. Operations Research 41 (2), 280–297. Smyth, P., D. Heckerman, and M. Jordan (1997). Probabilistic independence networks for hidden Markov probability models. Neural Computation 9 (2), 227–269. Sontag, D. and T. Jaakkola (2007). New outer bounds on the marginal polytope. In Proc. 21st Conference on Neural Information Processing Systems (NIPS) . Sontag, D., T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss (2008). Tightening LP relaxations for MAP using message passing. In Proc. 24th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Speed, T. and H. Kiiveri (1986). Gaussian Markov distributions over ﬁnite graphs. The Annals of Statistics 14 (1), 138–150. Spetzler, C. and C.-A. von Holstein (1975). Probabilistic encoding in decision analysis. Manage- ment Science , 340–358. Spiegelhalter, D. and S. Lauritzen (1990). Sequential updating of conditional probabilities on directed graphical structures. Networks 20 , 579–605. Spiegelhalter, D. J., A. P. Dawid, S. L. Lauritzen, and R. G. Cowell (1993). Bayesian analysis in expert systems. Statistical Science 8 , 219–283. Spirtes, P. (1995). Directed cyclic graphical representations of feedback models. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 491–98. Spirtes, P., C. Glymour, and R. Scheines (1991). An algorithm for fast recovery of sparse causal graphs. Social Science Computer Review 9 , 62–72. Spirtes, P., C. Glymour, and R. Scheines (1993). Causation, Prediction and Search . Number 81 in Lecture Notes in Statistics. New York: Springer-Verlag. Spirtes, P., C. Meek, and T. Richardson (1999). An algorithm for causal inference in the presence of latent variables and selection bias. See Glymour and Cooper (1999), pp. 211–52. Srebro, N. (2001). Maximum likelihood bounded tree-width Markov networks. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Srinivas, S. (1993). A generalization of the noisy-or model. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 208–215. Srinivas, S. (1994). A probabilistic approach to hierarchical model-based diagnosis. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Studený, M. and R. Bouckaert (1998). On chain graph models for description of conditional independence structures. Annals of Statistics 26 . Sudderth, E., A. Ihler, W. Freeman, and A. Willsky (2003). Nonparametric belief propagation. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 605–612. Sutton, C. and T. Minka (2006). Local training and belief propagation. Technical Report MSR-TR- 2006-121, Microsoft Research. Sutton, C. and A. McCallum (2004). Collective segmentation and labeling of distant entities in information extraction. In ICML Workshop on Statistical Relational Learning and Its Connections to Other Fields . Sutton, C. and A. McCallum (2005). Piecewise training of undirected models. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Sutton, C. and A. McCallum (2007). An introduction to conditional random ﬁelds for relational learning. In L. Getoor and B. Taskar (Eds.), Introduction to Statistical Relational Learning . MIT 

Press.Sutton, C., A. McCallum, and K. Rohanimanesh (2007, March). Dynamic conditional random ﬁelds: Factorized probabilistic models for labeling and segmenting sequence data. Journal of Machine Learning Research 8 , 693–723. Suzuki, J. (1993). A construction of Bayesian networks from databases based on an MDL scheme. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 266–273. Swendsen, R. and J. Wang (1987). Nonuniversal critical dynamics in Monte Carlo simulations. Physical Review Letters 58 (2), 86–88. Swendsen, R. H. and J.-S. Wang (1986, Nov). Replica Monte Carlo simulation of spin-glasses. Physical Review Letters 57 (21), 2607–2609. Szeliski, R., R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother (2008, June). A comparative study of energy minimization methods for Markov random ﬁelds with smoothness-based priors. IEEE Trans. on Pattern Analysis and Machine Intelligence 30 (6), 1068–1080. See http://vision.middlebury.edu/MRF for more detailed results. Szolovits, P. and S. Pauker (1992). Pedigree analysis for genetic counseling. In Proceedings of the Seventh World Congress on Medical Informatics (MEDINFO ’92) , pp. 679–683. North-Holland. Tanner, M. A. (1993). Tools for Statistical Inference . New York: Springer-Verlag. Tarjan, R. and M. Yannakakis (1984). Simple linear-time algorithms to test chordality of graphs, test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM Journal of Computing 13 (3), 566–579. Taskar, B., P. Abbeel, and D. Koller (2002). Discriminative probabilistic models for relational data. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 485–492. Taskar, B., P. Abbeel, M.-F. Wong, and D. Koller (2007). Relational Markov networks. See Getoor and Taskar (2007). Taskar, B., V. Chatalbashev, and D. Koller (2004). Learning associative Markov networks. In Proc. 21st International Conference on Machine Learning (ICML) . Taskar, B., C. Guestrin, and D. Koller (2003). Max margin Markov networks. In Proc. 17th Conference on Neural Information Processing Systems (NIPS) . Tatikonda, S. and M. Jordan (2002). Loopy belief propagation and Gibbs measures. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Tatman, J. A. and R. D. Shachter (1990). Dynamic programming and inﬂuence diagrams. IEEE Transactions on Systems, Man and Cybernetics 20(2) , 365–379. Teh, Y. and M. Welling (2001). The uniﬁed propagation and scaling algorithm. In Proc. 15th Conference on Neural Information Processing Systems (NIPS) . Teh, Y., M. Welling, S. Osindero, and G. Hinton (2003). Energy-based models for sparse over- complete representations. Journal of Machine Learning Research 4 , 1235–1260. Special Issue on ICA. Teyssier, M. and D. Koller (2005). Ordering-based search: A simple and efective algorithm for learning bayesian networks. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 584–590. Thiele, T. (1880). Sur la compensation de quelques erreurs quasisystematiques par la methode des moindres carrees . Copenhagen: Reitzel. Thiesson, B. (1995). Accelerated quantiﬁcation of Bayesian networks with incomplete data. In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD- 

95) , pp. 306–311. AAAI Press. Thiesson, B., C. Meek, D. M. Chickering, and D. Heckerman (1998). Learning mixtures of Bayesian networks. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Thomas, A., D. Spiegelhalter, and W. Gilks (1992). BUGS: A program to perform Bayesian inference using Gibbs sampling. In J. Bernardo, J. Berger, A. Dawid, and A. Smith (Eds.), Bayesian Statistics 4 , pp. 837–842. Oxford, UK: Clarendon Press. Thrun, S., W. Burgard, and D. Fox (2005). Probabilistic Robotics . Cambridge, MA: MIT Press. Thrun, S., D. Fox, W. Burgard, and F. Dellaert (2000). Robust Monte Carlo localization for mobile robots. Artiﬁcial Intelligence 128 (1–2), 99–141. Thrun, S., Y. Liu, D. Koller, A. Ng, Z. Ghahramani, and H. Durrant-Whyte (2004). Simultaneous localization and mapping with sparse extended information ﬁlters. International Journal of Robotics Research 23 (7/8). Thrun, S., C. Martin, Y. Liu, D. Hähnel, R. Emery-Montemerlo, D. Chakrabarti, and W. Burgard (2004). A real-time expectation maximization algorithm for acquiring multi-planar maps of indoor environments with mobile robots. IEEE Transactions on Robotics 20 (3), 433–443. Thrun, S., M. Montemerlo, D. Koller, B. Wegbreit, J. Nieto, and E. Nebot (2004). FastSLAM: An efcient solution to the simultaneous localization and mapping problem with unknown data association. Journal of Machine Learning Research . Tian, J. and J. Pearl (2002). On the testable implications of causal models with hidden variables. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 519–527. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B 58 (1), 267–288. Tierney, L. (1994). Markov chains for exploring posterior distributions. Annals of Statistics 22 (4), 1701–1728. Tong, S. and D. Koller (2001a). Active learning for parameter estimation in Bayesian networks. In Proc. 15th Conference on Neural Information Processing Systems (NIPS) , pp. 647–653. Tong, S. and D. Koller (2001b). Active learning for structure in Bayesian networks. In Proc. 17th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp. 863–869. Torrance, G., W. Thomas, and D. Sackett (1972). A utility maximization model for evaluation of health care programs. Health Services Research 7 , 118–133. Tsochantaridis, I., T. Hofmann, T. Joachims, and Y. Altun (2004). Support vector machine learning for interdependent and structured output spaces. In Proc. 21st International Conference on Machine Learning (ICML) . Tversky, A. and D. Kahneman (1974). Judgment under uncertainty: Heuristics and biases. Sci- ence 185 , 1124–1131. van der Merwe, R., A. Doucet, N. de Freitas, and E. Wan (2000a, Aug.). The unscented particle ﬁlter. Technical Report CUED/F-INFENG/TR 380, Cambridge University Engineering Depart- ment. van der Merwe, R., A. Doucet, N. de Freitas, and E. Wan (2000b). The unscented particle ﬁlter. In Proc. 14th Conference on Neural Information Processing Systems (NIPS) . Varga, R. (2000). Matrix Iterative Analysis . Springer-Verlag. Verma, T. (1988). Causal networks: Semantics and expressiveness. In Proc. 4th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 352–359. Verma, T. and J. Pearl (1988). Causal networks: Semantics and expressiveness. In Proc. 4th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 69–76. 

Verma, T. and J. Pearl (1990). Equivalence and synthesis of causal models. In Proc. 6th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 255 –269. Verma, T. and J. Pearl (1992). An algorithm for deciding if a set of observed independencies has a causal explanation. In Proc. 8th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 323–330. Vickrey, D. and D. Koller (2002). Multi-agent algorithms for solving graphical games. In Proceed- ings of the Eighteenth National Conference on Artiﬁcial Intelligence (AAAI-02) , pp. 345–351. Vishwanathan, S., N. Schraudolph, M. Schmidt, and K. Murphy (2006). Accelerated training of conditional random ﬁelds with stochastic gradient methods. In Proc. 23rd International Conference on Machine Learning (ICML) , pp. 969–976. Viterbi, A. (1967, April). Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory 13 (2), 260–269. von Neumann, J. and O. Morgenstern (1944). Theory of games and economic behavior (ﬁrst ed.). Princeton, NJ: Princeton Univ. Press.von Neumann, J. and O. Morgenstern (1947). Theory of games and economic behavior (second ed.). Princeton, NJ: Princeton Univ. Press.von Winterfeldt, D. and W. Edwards (1986). Decision Analysis and Behavioral Research . Cambridge, UK: Cambridge University Press. Vorobev, N. (1962). Consistent families of measures and their extensions. Theory of Probability and Applications 7 , 147–63. Wainwright, M. (2006). Estimating the “wrong” graphical model: Beneﬁts in the computation- limited setting. Journal of Machine Learning Research 7 , 1829–1859. Wainwright, M., T. Jaakkola, and A. Willsky (2003a). Tree-based re parameter iz ation framework for analysis of sum-product and related algorithms. IEEE Transactions on Information Theory 49 (5). Wainwright, M., T. Jaakkola, and A. Willsky (2003b). Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching. In Proc. 9thWorkshop on Artiﬁcial Intelligence and Statistics . Wainwright, M., T. Jaakkola, and A. Willsky (2004, April). Tree consistency and bounds on the performance of the max-product algorithm and its generalizations. Statistics and Computing 14 , 143–166. Wainwright, M., T. Jaakkola, and A. Willsky (2005). MAP estimation via agreement on trees: Message-passing and linear programming. IEEE Transactions on Information Theory . Wainwright, M., T. Jaakkola, and A. S. Willsky (2001). Tree-based re parameter iz ation for approx- imate estimation on loopy graphs. In Proc. 15th Conference on Neural Information Processing Systems (NIPS) . Wainwright, M., T. Jaakkola, and A. S. Willsky (2002a). Exact map estimates by (hyper)tree agreement. In Proc. 16th Conference on Neural Information Processing Systems (NIPS) . Wainwright, M., T. Jaakkola, and A. S. Willsky (2002b). A new class of upper bounds on the log partition function. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Wainwright, M. and M. Jordan (2003). Graphical models, exponential families, and variational inference. Technical Report 649, Department of Statistics, University of California, Berkeley. Wainwright, M. and M. Jordan (2004). Semideﬁnite relaxations for approximate inference on graphs with cycles. In Proc. 18th Conference on Neural Information Processing Systems (NIPS) . Wainwright, M., P. Ravikumar, and J. Laferty (2006). High-dimensional graphical model selec- tion using $\ell_{1}$ -regularized logistic regression. In Proc. 20th Conference on Neural Information 

Processing Systems (NIPS) . Warner, H., A. Toronto, L. Veasey, and R. Stephenson (1961). A mathematical approach to medical diagnosis — application to congenital heart disease. Journal of the American Madical Association 177 , 177–184. Weiss, Y. (1996). Interpreting images by propagating bayesian beliefs. In Proc. 10th Conference on Neural Information Processing Systems (NIPS) , pp. 908–914. Weiss, Y. (2000). Correctness of local probability propagation in graphical models with loops. Neural Computation 12 , 1–41. Weiss, Y. (2001). Comparing the mean ﬁeld method and belief propagation for approximate inference in MRFs. In M. Opper and D. Saad (Eds.), Advanced mean ﬁeld methods , pp. 229– 240. Cambridge, Massachusetts: MIT Press. Weiss, Y. and W. Freeman (2001a). Correctness of belief propagation in Gaussian graphical models of arbitrary topology. Neural Computation 13 . Weiss, Y. and W. Freeman (2001b). On the optimality of solutions of the max-product belief propagation algorithm in arbitrary graphs. IEEE Transactions on Information Theory 47 (2), 723–735. Weiss, Y., C. Yanover, and T. Meltzer (2007). MAP estimation, linear programming and belief propagation with convex free energies. In Proc. 23rd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Welling, M. (2004). On the choice of regions for generalized belief propagation. In Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Welling, M., T. Minka, and Y. Teh (2005). Structured region graphs: Morphing EP into GBP. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Welling, M. and S. Parise (2006a). Bayesian random ﬁelds: The Bethe-Laplace approximation. In Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Welling, M. and S. Parise (2006b). Structure learning in Markov random ﬁelds. In Proc. 20th Conference on Neural Information Processing Systems (NIPS) . Welling, M. and Y.-W. Teh (2001). Belief optimization for binary networks: a stable alternative to loopy belief propagation. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) . Wellman, M. (1985). Reasoning about preference models. Technical Report MIT/LCS/TR-340, Laboratory for Computer Science, MIT. Wellman, M., J. Breese, and R. Goldman (1992). From knowledge bases to decision models. Knowledge Engineering Review 7 (1), 35–53. Wellman, M. and J. Doyle (1992). Modular utility representation for decision-theoretic planning. In Procec. First International Conference on AI Planning Systems , pp. 236–42. Morgan Kaufmann. Wellman, M. P. (1990). Foundamental concepts of qualitative probabilistic networks. Artiﬁcial Intelligence 44 , 257–303. Wellner, B., A. McCallum, F. Peng, and M. Hay (2004). An integrated, conditional model of information extraction and coreference with application to citation matching. In Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 593–601. Wermuth, N. (1980). Linear recursive equations, covariance selection and path analysis. Journal of the American Statistical Association 75 , 963–975. Werner, T. (2007). A linear programming approach to max-sum problem: A review. IEEE Trans. on Pattern Analysis and Machine Intelligence 29 (7), 1165–1179. West, M. (1993). Mixture models, Monte Carlo, Bayesian updating and dynamic models. Comput- 

ing Science and Statistics 24 , 325–333. Whittaker, J. (1990). Graphical Models in Applied Multivariate Statistics . Chichester, United King- dom: John Wiley and Sons. Wiegerinck, W. (2000). Variational approximations between mean ﬁeld theory and the junction tree algorithm. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 626–636. Wold, H. (1954). Causality and econometrics. Econometrica 22 , 162–177. Wood, F., T. Grifths, and Z. Ghahramani (2006). A non-parametric bayesian method for inferring hidden causes. In Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 536– 543. Wright, S. (1921). Correlation and causation. Journal of Agricultural Research 20 , 557–85. Wright, S. (1934). The method of path coefcients. Annals of Mathematical Statistics 5 , 161–215. Xing, E., M. Jordan, and S. Russell (2003). A generalized mean ﬁeld algorithm for variational in- ference in exponential families. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 583–591. Yanover, C., T. Meltzer, and Y. Weiss (2006, September). Linear programming relaxations and belief propagation — an empirical study. Journal of Machine Learning Research 7 , 1887–1907. Yanover, C., O. Schueler-Furman, and Y. Weiss (2007). Minimizing and learning energy functions for side-chain prediction. In Proc. International Conference on Research in Computational Molecular Biology (RECOMB) , pp. 381–395. Yanover, C. and Y. Weiss (2003). Finding the M most probable conﬁgurations using loopy belief propagation. In Proc. 17th Conference on Neural Information Processing Systems (NIPS) . Yedidia, J., W. Freeman, and Y. Weiss (2005). Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Trans. Information Theory 51 , 2282–2312. Yedidia, J. S., W. T. Freeman, and Y. Weiss (2000). Generalized belief propagation. In Proc. 14th Conference on Neural Information Processing Systems (NIPS) , pp. 689–695. York, J. (1992). Use of the Gibbs sampler in expert systems. Artiﬁcial Intelligence 56 , 115–130. Yuille, A. L. (2002). CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent alternatives to belief propagation. Neural Computation 14 , 1691–1722. Zhang, N. (1998). Probabilistic inference in inﬂuence diagrams. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 514–522. Zhang, N. and D. Poole (1994). A simple approach to Bayesian network computations. In Proceedings of the 10th Biennial Canadian Artiﬁcial Intelligence Conference , pp. 171–178. Zhang, N. and D. Poole (1996). Exploiting contextual independence in probabilistic inference. Journal of Artiﬁcial Intelligence Research 5 , 301–328. Zhang, N., R. Qi, and D. Poole (1993). Incremental computation of the value of perfect information in stepwise-decomposable inﬂuence diagrams. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , pp. 400–407. Zhang, N. L. (2004). Hierarchical latent class models for cluster analysis. Journal of Machine Learning Research 5 , 697–723. Zoeter, O. and T. Heskes (2006). Deterministic approximate inference techniques for conditionally Gaussian state space models. Statistical Computing 16 , 279–292. Zweig, G. and S. J. Russell (1998). Speech recognition with dynamic Bayesian networks. In Proc. 14th Conference on Artiﬁcial Intelligence (AAAI) , pp. 173–180. 

# Notation Index 

$|A|$ — Cardinality of the set $A$ , 20

 $\phi_{1}\times\phi_{2}

$ $\gamma_{1}\oplus\gamma_{2}$ L γ — Joint factor combi n, 1104 ( Z ) — Marginal of $g(Z)$ based on $p(Z)$ , 631

 $\textstyle\sum_{Y}\phi$ — Factor marginalization, 297 $X\rightleftharpoons Y$ — Bi-directional edge, 34 $X\rightarrow Y$ — Directed edge, 34 $X{-}Y$ — Undirected edge, 34 $X\leftrightarrow Y$ — Non-ancestor edge (PAGs), 1049 $X\circ\to Y$ ◦→ — Ancestor edge (PAGs), 1049

 $\langle x,y\rangle$ — Inner product of vectors $x$ and $y$ , 262

 $\|P-Q\|_{1}~-L_{1}$ | − | | distance, 1143

 $\|P-Q\|_{2}~-L_{2}$ | − | | distance, 1143

 $\|P-Q\|_{\infty}\ -L_{\infty}$ | − | | ∞ ∞ distance, 1143

 $(X\perp Y)$ — Independence of random variables, 24

 $(X\perp Y\mid Z)$ — Conditional independence of random variables, 24

 $(X\perp_{c}Y\mid Z,c)$ — Context-speciﬁcindependence, 162 

$I\{\cdot\}$ — Indicator function, 32 

$\mathcal{A}(\pmb{x}\rightarrow\pmb{x}^{\prime})$ — Acceptance probability, 517

 $\aleph$ ℵ — Template attributes, 214

 $\alpha(A)$ — The argument signature of attribute A , 213 Ancestors X — Ancestors of $X$ (in graph), 36 argmax, 26 $A\mathrm{~-~A~}$ template attribute, 213 

$B e t a(\alpha_{1},\alpha_{0})$ — Beta distribution, 735

 $\beta_{i}$ — Belief potential, 352

 $\mathcal{B}_{\mathcal{Z}[\sigma]}$ — Induced Bayesian network, 1093

 $\mathcal{B}$ B — Bayesian network, 62

 ${\mathcal B}_{0}$ — Initial Bayesian network (DBN), 204 

$\mathcal{B}_{\rightarrow}$ — Transition Bayesian network (DBN), 204

 $\mathcal{B}_{Z=z}$ — Mutilated Bayesian network, 499 Boundary $X$ — Boundary around $X$ (in graph), 34 

$\mathcal{C}\left(K,h,g\right)$ — Canonical form, 609

 $\mathcal{C}\left(X;K,h,g\right)$ — Canonical form, 609

 $\mathcal{C}[v]$ — Choices, 108

 $\mathrm{Ch}_{X}$ — Children of X (in graph), 34 $C_{i}$ — Clique, 346

 ${\pmb x}\sim{\pmb c}$ — Compatability of values , 20

 $\operatorname{cont}(\gamma)$ — Joint factor contraction, 1104

 $\mathbb{C}o v[X;Y]$ — Covariance of $X$ and $Y$ , 248 

$_{D\mathrm{~-~A~}}$ subclique, 104

 $\Delta$ — Discrete variables (hybrid models), 605

 $^d$ — Value of a subclique, 104

 $\mathcal{D}^{+}$ — Complete data, 871

 $\mathcal{D}$ — Empirical samples (data), 698

 $\mathcal{D}$ — Sampled data, 489

 $\mathcal{D}^{*}$ — Complete data, 912 D — Decisions, 1089 Descendants $X$ — Descendants of $X$ (in graph), 36

 $\tilde{\delta}_{i\to j}$ — Approximate sum-product message, → 435

 $\delta_{i\to j}$ — Sum-product message, 352

 $\mathrm{Diim}[\mathcal{G}]$ — Dimension of a graph, 801 Dirichlet $\left(\alpha_{1},.\,.\,.\,,\alpha_{K}\right)$ — Dirichlet distribution, 738 $D(P\|Q)$ | | — Relative entropy, 1141 $D_{u a r}(P;Q)$ — Variational distance, 1143

 $\mathbf{D o w n}^{*}(r)$ — Downward closure, 422

 $\mathbf{D o w n}^{+}(r)$ — Extended downward closure, 422

 $\mathbf{D o w n}(r)$ — Downward regions, 422 $d o(Z:=z),d o(z)$ — Intervention, 1010 

# $\begin{array}{r l}{~}&{{}d\mathbf{\boldsymbol{-}}s e p_{\mathcal{G}}(\boldsymbol{X};Y\mid Z)\mathbf{\Sigma}-\operatorname{d}}\end{array}$ -separation, 71 

$\mathcal{E}$ — Edges in MRF, 127 EU $^{\intercal}\!\left[\mathcal{D}[a]\right]$ — Expected utility, 1061

 $\operatorname{EU}[{\mathcal{Z}}[\sigma]]$ — Expected utility of $\sigma$ , 1093

 $\hat{\boldsymbol E}_{\mathcal{D}}(f)$ — Empirical expectation, 490 D

 $\pmb{{\cal E}}_{\mathcal{D}}[f]$ — Empirical expectation, 700 D

 $\pmb{{\cal E}}_{P}[X]$ — Expectation (mean) of $X$ , 31

 $\pmb{{\cal E}}_{P}[X\mid\pmb{{y}}]$ | — Conditional exp 32 E [ · ] — Expectation when $X\sim P$ ∼ , 387 X∼P$f(D)$ — A feature, 124 $F[\tilde{P},Q]$ — Energy functional, 385, 881

 $\tilde{F}[\tilde{P}_{\Phi},Q]$ — Region Free Energy functional, 420

 $\tilde{F}[\tilde{P}_{\Phi},Q]$ — Factored energy functional, 386 FamScore( X i | Pa X : D ) — Family score, i 805

 $\mathcal{F}

$ $\mathcal{F}$ F — Factor graph, 123

 $\mathcal{G}$ — Directed graph, 34

 $\pmb{\mathscr{G}}$ G — Partial ancestral graph, 1049

 $\Gamma$ — Continuous variables (hybrid models), 605

 $\gamma$ — Template assignment, 215 Gamma $(\alpha,\beta)$ — Gamma distribution, 900

 $\Gamma(x)$ — Gamma function, 736

 $\mathcal{H}

$ $\mathcal{H}$ H — Undirected graph, 34 $H_{P}(X)$ — Entropy, 1138 $H_{P}(X\mid Y)$ | — Conditional entropy, 1139

 $\tilde{\cal H}_{Q}^{\kappa}(\mathcal{X})$ — Weighted approximate entropy, 415

 $\mathcal{T}$ — Inﬂuence diagram, 1090

 $\mathcal{T}(\mathcal{G})$ — Markov independencies of $\mathcal{G}$ , 72

 $\mathcal{T}_{\ell}(\mathcal{G})$ — Local Markov independencies of $\mathcal{G}$ , 57

 $\mathcal{Z}(P)$ — The independencies satisﬁed by $P$ , 60 $I_{P}(X;Y)$ — Mutual infromation, 1140 Interfa $c e_{\mathcal{H}}(X;Y)\;-Y$ -interface of $_{X}$ , 464 $\mathcal{I}$ — Lagrangian, 1168 $J$ — Precision matrix, 248

 $\mathcal{K}$ artially directed graph, 34

 $\mathcal{K}^{+}[X]$ K — Upward closed subgraph, 35

 $\kappa$ — Object skeleton (template models), 214

 $\kappa_{r}$ — Counting number of region $r$ , 415 

# $K_{i}$ — Member of a chain, 37 $\mathcal{K}[X]$ — Induced subgraph, 35 

$L(\theta:{\mathcal{D}})$ — Likelihood function, 721 — Local polytope, 412

 $\ell(\hat{\pmb{\theta}}_{\mathcal{G}}\,\dot{:}\,\dot{\mathcal{D}})$ G D — Maximum likelihood value, 791

 $\ell(\theta:{\mathcal{D}})$ — Log-likelihood function, 719

 $\ell_{Y\mid X}(\pmb\theta:\mathcal D)$ — Conditional log-likelihood function, 951 $l o s s(\xi\ :\ \mathcal{M})$ — Loss function, 699 $\mathcal{M}^{*}$ — Model that generated the data, 698 M-project- $\mathrm{{distr}}_{i,j}$ — M-projection, 436 $M[\pmb{x}]$ — Counts of event $_{_{\pmb{x}}}$ in data, 724 Marg [ U ] — Marginal polytope, 411 $m a r g_{W}(\gamma)$ — Joint factor marginalization, 1104 MaxMarg ( x ) — Max marginal of $f$ , 553 f

 $\mathcal{M}[\mathcal{G}]$ $\mathcal{G}$ , 134

 $\mathcal{M}\mathrm{~-~A~}$ A model, 699

 $\bar{M}_{\theta}[{\pmb x}]$ — Expected counts, 871

 $\tilde{\mathcal{M}}^{\perp}-$ M — Learned/estimated model, 698

 $\mathcal{N}\left(\mu;\boldsymbol{\sigma^{2}}\right)$  — A Gaussian distribution, 28

 ${\mathcal{N}}\left(X\mid{\dot{\mu}};\sigma^{2}\right)$ N   |  — Gaussian distribution over $X$ , 616 Nb X — Neighbors of $X$ (in graph), 34 NonDescendants $X$ — Non-descendants of $X$ (in graph), 36 NP , 1151 O — Outcome space, 1060

 ${\cal O}(f(\cdot))~-"\mathrm{Big}\;\mathrm{O}"$ $f$ , 1148

 ${\mathcal{O}}^{\kappa}[\mathsf{Q}]\ -\mathsf{O b j e c}$ κ (template models), 214

 $\mathcal{P}$ , 1151 $P(X\mid Y)$ — Conditional distribution, 22 $P(x),P(x,y)\ \ .$ — Shorthand for $P(X=x)$ , $P(X=x,Y=y)$ , 21 $P^{*}\_$ $P\models.\rightharpoonup P$ satisﬁes . . . , 23

 $\mathrm{Pa}_{X}$ — Parents of $X$ (in graph), 34

 $\mathrm{Da}_{X}$ — Value of $\mathrm{Pa}_{X}$ , 157

 $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}$ — Parents of $X_{i}$ in $\mathcal{G}$ , 57

 $\hat{P}_{\mathcal{D}}(A)$ — Empirical distribution, 703 D

 $\hat{P}_{\mathcal{D}}(\pmb{x})$ — Empirical distribution, 490 D

 $\theta$ — Parameters, 262, 720 — MLE parameters, 726

 $\phi$ — A factor (Markov network), 104 

$\phi[U=u]$ — Factor reduction, 110

 $\pi$ — Lottery, 1060

 $\pi(X)$ — Stationary probability, 509

 $\tilde{P}_{\Phi}(\mathcal X)$ — Unnormalized measure deﬁned by $\Phi$ , 345

 $\psi_{i}(C_{i})$ — Initial potential, 349

 $\tilde{P}$ — Learned/estimated distribution, 698 $Q$ — Approximating distribution, 383

 $\mathcal{Q}$ — Template classes, 214

 $\mathcal{R}$ $I\!\!R$ — Real numbers, 27

 $\rho$ — A rule, 166

 $\mathcal{R}$ — Rule set, 168 $s

$ $\sigma$ — Std of a Gaussian distribution, 28

 $\sigma$ — Strategy, 1092

 $\sigma^{(t)}(\cdot)$ — Belief state, 652 $S c o p e[\phi]$ — Scope of a factor, 104 score $_B\big(\mathcal{G}\ :\ \mathcal{D}\big)$ — Bayesian score, 795

 $\mathrm{score}_{B I C}(\mathcal G\ :\ \mathcal D)$ — BIC score, 802

 $\mathrm{score}_{C S}(\mathcal G\ :\ \mathcal D)$ — Cheeseman-Stutz score, 913

 $\mathrm{score}_{L}(\mathcal G\ :\ \mathcal D)$ — Likelihood score, 791

 $\mathrm{score}_{L_{1}}(\theta\ :\ \mathcal{D})\ -L_{1}$ score, 988

 $\mathrm{score}_{L a p l a c e}(\mathcal{G}~:~\mathcal{D})$ — Laplace score, 910

 $\operatorname{score}_{\operatorname{MAP}}(\pmb{\theta}\ :\ \mathcal{D})\ -\operatorname{MAP}$ score, 98 $s e p_{\mathcal{H}}(X;Y\mid Z)$ — Separation in H , 114 sigmoid( $(x)$ ) — Sigmoid function, 145 $\boldsymbol{S}_{i,j}$ — Sepset, 140, 346 $s u c c(v,c)$ — Successor (decision trees), 1085

 $\mathcal{T}$ — Clique tree, 140, 347

 $\Upsilon$ — Template clique tree, 656

 $\mathcal{T}$ — Decision tree, 1085

 ${\mathfrak{t}}(\theta)$ — Natural parameters function, 261

 $\tau(\xi)$ — Sufcient statistics function, 261, 721

 $\Theta$ — Parameter space, 261, 720

 $\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime})$ — Transition probability, 507 

$\mathbf{Up}(r)$ — Upward regions, 422 — Utility variables, 1090 $U^{X}$ — Response variable, 1029 

$V a l(X)$ — Possible values of $X$ , 20

 $\mathbb{W}a r_{P}[X]$ — Variance of $X$ , 33

 $\mathrm{VPI}_{\mathcal{Z}}(D\mid X)$ — Value of perfect information, 1122

 $\nu_{r},\nu_{i},\nu_{r,i}$ — Convex counting numbers, 416 $W_{<(i,j)}$ , 348

 $\mathcal{X}$ — The set of all var les in the domain, 21

 $\xi$ X , 79 $X,Y,Z$ — Random variables, 20 $X,Y,Z$ — Random variable sets, 20 ${\mathbfit{x}},{\mathbfit{y}},{\mathbfit{z}}$ — Values of random variable sets, 20 $x^{0},x^{1}$ — False/True values of $X$ , 20

 $_{x\langle Y\rangle}$ — Assignment in $_{_{\pmb{x}}}$ to variables in $\mathbf{Y}$ , 21

 ${\bf x}[{\bf m}]{\bf x}[{m}]\;-m$ ’th data instance (i.i.d. samples), 698 $x^{i}$ — The $i$ ’th value of $X$ , 20

 $\mathcal{X}_{\kappa}[A]$ — Ground random variables, 214

 $\xi[m]~-m$ ’th data instance (i.i.d. samples), 488

 $\xi^{m a p}\ -{\mathrm{MAP}}$ assignment, 552 $X^{(t)}~-X$ at time $t$ , 200 $X^{(t_{1}:t_{2})}~-X$ in the interval $[t_{1},t_{2}]$ , 200 $X\sim...\mathrm{~-~}X$ is distributed according to . . . , 28 $Z$ — Partition function, 105 

$\mathcal{U}

$ $\mathcal{U}$ U — Response variables, 1029

 $\mu$ — Mean of a Gaussian distribution, 28 $U(o)$ — Utility function, 1060 $\mu_{i,j}$ — Sepset beliefs, 358 Unif[a , b] — Uniform distribution on $[a,b]$ , 28 $\mathbf{Up}^{*}(r)$ — Upward closure, 422 

# Subject Index 

2-TBN, 202 3-SAT, 288, 1151 

abduction, 1134 action, 1061 joint, 1117 active learning, 1055 activity recognition, 952 Algorithm Alpha-Expand , 593, 593 Alpha-Expansion , 593 BU-Message , 367, 367 , 368, 440, 441 BU-message , 400 Beam-Search , 1158 Branch-and-Bound , 1161, 1161 Build-Minimal-I-Map , 80, 80 , 142, 786 Build-PDAG , 89 , 90–92, 786, 787, 790, 839, 843, 1042, 1043 Build-PMAP-Skeleton , 787 Build-PMap-Skeleton , 85 , 86, 89, 90, 101, 787, 980, 1005, 1051 Build-Saturated-Region-Graph , 423 CGraph-BU-Calibrate , 398, 400, 413 CGraph-SP-Calibrate , 397 , 413, 428 CLG-M-Project-Distr , 622 , 628 CSI-sep , 173 CTree-BU-Calibrate, 367, 398, 628CTree-BU-calibrate, 391CTree-Filter-DBN , 657 CTree-Query , 371 CTree-SP-Calibrate , 357 , 364, 365, 368, 398, 413, 436 CTree-SP-Upward , 353 , 378, 612 CTree-SP-calibrate , 388, 413 Compute-ESS , 873, 873 , 938 Compute-Gradient , 867, 867 Cond-Prob-VE , 304 , 317 

Conditioning , 604 Conjugate-Gradient-Ascent , 1167 Convex-BP-Msg , 418 Cross-Validation , 707 DP-Merge-Split-Proposal , 942 Data-Dependent-LW , 502, 502 , 504 EP-Message , 440, 441 , 443, 628 Estimate-Parameters , 922, 941 Evaluate , 707, 707 Expectation-Maximization , 873 , 922 Factor-Product , 359 Factored-Project , 434 , 435 Fibonacci , 1150, 1150 Find-Immoralities , 89 Forward-Sample , 489 Generalized-MP-BP , 573 Generalized-VE-for-IDs , 1105 , 1106, 1107 Gibbs-Sample , 506 Gradient-Ascent , 1164 Greedy-Local-Search , 815, 1155 , 1156 Greedy-MN-Structure-Search , 986 , 990, 992 Greedy-Ordering , 314 , 340 Holdout-Test , 707 Incremental-E-Step , 939, 939 Incremental-EM , 939 Initialize-CGraph , 397, 397 , 573 Initialize-CTree , 367, 367 Initialize-Cliques , 353, 353 , 357 Iterated-Optimization-for-IDs , 1116, 1116 , 1131 K-Best , 1158 LW-2TBN , 666, 666 , 670 LW-DBN , 666 LW-Sample , 493, 493 , 502 LearnProc , 706, 707 LegalOp , 1157, 1157 , 1158 M-Project-Distr , 443 , 621 

MCMC-Sample , 509 MEU-for-Decision-Trees , 1088 , 1098 Mark-Immoralities , 86 , 87, 89, 102, 787 Max-Cardinality , 312, 312 , 313 Max-Message , 562 Max-Product-Eliminate-Var , 557, 557 Max-Product-VE , 557 Max-Weight-Spanning-Tree , 1147 Mean-Field , 455, 455 , 459 MinCut-MAP , 591 , 593 MinCut , 591 Msg-Truncated-1-Norm , 603 Parameter-Optimize , 986 Particle-Filter-DBN , 670 Perturb , 817, 818 Proposal-Distribution , 941 Reachable , 75 , 76, 102 Rule-Split , 332, 332 , 333 Rule-Sum-Product-Eliminate-Var , 333 , 601 SP-Message , 353, 353 , 357, 368, 378, 397, 397 , 407, 437, 567, 612 Search-with-Data-Perturbation , 817 Search-with-Restarts , 1159 Search , 817, 1159 Structural-EM , 922 Structure-Learn , 922 Sum-Product-Conditioning , 317 Sum-Product-Eliminate-Var , 298, 298 , 306, 347, 611 Sum-Product-VE , 298 , 299, 304, 313, 331, 371, 611 Tabu-Structure-Search , 1157 Topological-Sort , 1146 Traceback-MAP , 557, 557 , 558, 561, 601 Train-And-Test , 707, 707 alignment, see correspondence alpha-beta swap, 592, 602 alpha-expansion, 592 ancestor, 36 argument, 213 factor, 216 feature, 229 signature, 213, 223 parent, 221, 223 assignment local optimality, 566–567, 569 MAP, 26, 967 strong local maximum, 570–572, 602 attribute, 213 

object-valued, 234 average causal efect, 1032 back-door criterion, 1020–1021 trail, 1020 backward induction, 1098 decision tree, 1087 bag of words, 766 barren node, 98, 136 basin ﬂooding, 816, 1156 Bayes’ rule, 18 BayesBall, 94 Bayesian classiﬁer, 727 Bayesian estimation, 735, 739, 752, 781, 782, 824 Bayesian networks, 741–750 BDE prior, see BDe prior BGe prior, see BGe prior Dirichlet prior, 739, 740 Gaussian, 779–780 incomplete data, 898–908, 1052 MCMC, 899–904 variational, see variational Bayes nonparametric, 730–731, 928–930 shared parameters, 762–763 Bayesian model averaging, 785, 824–832, 928, 1043 computational complexity, 827 MCMC, 829–832 Bayesian network, 5, 62 conditional, see conditional Bayesian network gradient, 339, 483 structure, 57 Bayesian score, 983 BDe prior, 749, 806, 835, 844, 848 shared parameters, 780 beam search, 890 belief propagation asynchronous, 408, 417 clique tree, 355–358 cluster graph, 396–399 convergence, 392, 401–403, 407–411, 417–419 convergence point, 412–413, 479 stability, 408, 413 convex, 416–419damping, 408, 479 EM, 897 frustrated loop, 568 Gaussian, see Gaussian, belief propagation 

local maxima, 409 loopy, 393, 405, 962 Markov network learning, 963–965 max-product, 562, 593, 602 convergence, 602 message scheduling, 408 nonparametric, 646, 649 operator, 402 region graph, 423–428 residual, 408 sum-product, 356 synchronous, 402, 408 tree re parameter iz ation, 408 tree-CPDs, 478 tree-reweighted, 418, 576, 593, 968 belief state, 652 prior, 653 projection, 663 reduced, 656 beliefs, 358 Beta distribution, 735–737 BGe prior, 840 bias, 710 bias-variance trade-of, 704 bigram model, 764 bipartite matching, 534 BK algorithm, 690 BN2O network, 177, 197 Boltzmann distribution, 126 Bonferroni correction, 843 bootstrap, 1046 bow pattern, 1024 BUGS system, 525–526, 543 c-separation, 150, 156 CAI-map, 1076 minimal, 1077 perfect, 1077 calibrated, 358 CAMEL, 964, 1004 canonical form, 609, 649 division, 610 marginalization, 610 well-deﬁned, 611 operations, 610–611 product, 610 reduction, 611 vacuous, 610 canonical table, 618 

marginalization, 619–621 weak, 620 operations, 618–621 causal efect, 1014 independence, 1056 mechanism, 175, 1014 model, 1014–1030 augmented, 1017–1020, 1022–1024 functional, 1029–1030 identiﬁability, 1042 causal Markov assumption, 1041 causal model learning, 1040–1053 Bayesian model averaging, 1043 constraint-based, 1042–1043 functional causal model, 1051–1053, 1056 interventional data, 1044–1047 latent variables, 1048–1051 constraint-based, 1048–1051, 1056 score-based, 1048 cellular network reconstruction, 1046–1047 central limit theorem, 1144 Markov chain, 521 certainty equivalent, 1066 chain component, 37, 148 chain graph, 37, 148 c-separation, see c-separation distribution, 149 model, 148 chain rule Bayesian networks, 54, 62 conditional probabilities, 18, 47 entropy, 1139 mutual information, see mutual information, chain rule relative entropy, 1142 chance variable, 1089 Chebyshev’s inequality, 33 Cheeseman-Stutz score, see marginal likelihood approximation, Cheeseman-Stutz Chernof bound, 491, 501, 1145 $\chi^{2}$ distribution, 790 statistic, 788, 843, 848 child, 34 Chinese restaurant process, 930 chordal graph, 311 clarity test, 64 class, 213 

classiﬁcation, 50, 727 collective, 952 error, 701 task, 700 text, 766 CLG, see Gaussian, conditional linear CLG network, 190, 645, 684 computational complexity, 615–617 clique, 35 clique potentials, 109 clique tree, 140, 346–348, 481, 549, 673, 937 algorithm correctness, 353–355 beliefs, 352, 357, 365 calibrated, 355–358, 384 CLG network, 626–630 clique, 348 downstream, 347 initial potential, 351 ready, 350, 356 upstream, 347 computational complexity, 358, 374 construction, 372–376, 379, 380 downward pass, 356, 655 family preservation, see cluster graph, family preservation incremental update, 369–370, 379 inference as optimization, 387–390 inﬂuence diagram, 1109, 1117, 1131, 1132 invariant, 361–363, 368 max-product, 564, 568 max-calibrated, 563 max-product, 562–565 traceback, 566 measure, 361–364, 383–384, 564 message, 345 scheduling, 357 message passing, see message passing multiple queries, 371–372 nested, 377 out-of-clique inference, 370–371, 379 re parameter iz ation, 362 rule-based CPDs, 379 running intersection property, 347–348, 353 sampling, 544 sepset, 140 strong root, 627 structure changes, 378, 379 sum-product, 352 

template, 656 upward pass, 356, 378, 654 cluster graph, 346, 396 Bayesian network, 478 beliefs, 396 low-temperature-limit, 583 Bethe, 405, 414, 415, 573 calibrated, 396–398, 412 construction, 404–411 family preservation, 346, 420 induced subgraph, 570 invariant, 399–400 max-calibrated, 583 message passing, see message passing out-of-cluster inference, 481 residual, 401, 477 running intersection property, 396, 407 sepset, 346 template, 664 tree consistency, 401 clustering, 875 Bayesian, see naive Bayes, clustering, 875, 902–908, 915–916 collaborative ﬁltering, 823, 877 collapsed sampling, see Gibbs, collapsed, see importance sampling, collapsed, see MCMC, collapsed, 526–532, 645, 650 compression, 1137 computational complexity, 1147–1149 asymptotic, 1147 running time, 1148 theory, 1150 concentration phenomenon, 777 condensation, see ﬁlter, particle conditional Bayesian network, 191 conditional covariance, 259 conditional expectation, 32, 451 conditional independence, see independence conditional preference structure, 1072 conditional probability, 18 conditional probability distribution, see CPD conditional probability table, see table-CPD conditional random ﬁeld, 113, 143, 191, 197, 710, 950, 952 linear-chain, 146 skip-chain, 146 conditioning, 315–325 bounded, 540 computational complexity, 320–325 

cutset, 318 incremental, 540 induced graph, 322 marginal MAP, 604 rule-based CPDs, 334 conﬁdence interval, 719 confounding factor, 1012–1014 constraint, 388 equality, 1168 expectation consistency, 446, 447 local polytope, see local polytope marginal consistency, 384, 387, 416 region graph, 421 marginal polytope, see marginal polytope mean ﬁeld, 455 constraint generation, 976, 1005 constraint propagation, 89 constraint satisfaction problem, 569 context-speciﬁc independence, seeindependence, context-speciﬁccontingency table, 152 contingent dependency model, 223 contraction, 402 contrastive divergence, 974–975 objective, 970 convergence bound, 489, 771, 1145–1146 convergence rate, 888 convex optimization, 976 coordinate ascent, 881 coordination graph, 1117 correspondence, 165, 236, 532–536, 544, 550 correlated, 535 EM, 534 Metropolis Hastings, 534 mutual exclusion, 533–534 variable, 166, 533, 893 counterfactual query, 1010, 1026–1027, 1034–1040 twinned network, 1035–1037 world, 1034 counterfactual twinned network, 1125 counting numbers, 415, 420, 573 convex, 416, 419, 574 CPD, 47, 53, 62 aggregator, 225, 245 conditional linear Gaussian, 190, 618 decomposition causal independence, 325–329 

context-speciﬁc independence, 341deterministic, 158 encapsulated, 192 Gaussian, see Gaussian, linear linear Gaussian, 187 logistic, 145, 179, 197, 225, 483 multinomial, 181, 970 multiplexer, 165 noisy-and, 196 noisy-max, 183, 196 noisy-or, 176, 196, 197, 225, 936, 1037 requisite, 100, 1018, 1112 rule-based, 168, 195, 601 inference, see variable elimination, rule-based CPDs table-CPD, 157, 725 tree-CPD, 164, 195, 196 CPT, see CPD, table CRF, see conditional random ﬁeld cross-validation, 706, 844, 960 CSI-separation, 173, 196 computational complexity, 196 cycle, 37 cyclic graphical model, 95 d-separation, 71 completeness, 72 soundness, 72 DAG, 37, 57 data complete, 712 completion, 869, 881, 912, 921 incomplete, 712, 849 interventional, 1040, 1044, 1056 observability, 712 observational, 1040 weighted, 817, 870 data association, see correspondence, 165, 244, 532, 550, 680, 893, 940 data fragmentation, 726, 784 data imputation, 869 data perturbation, 816 data-driven approach, 6 decision diagram, 170 decision rule, 1091 deterministic, 1091 fully mixed, 1111 locally optimal, 1109, 1110 optimization, 1107, 1108, 1130 

iterated, 1115–1117, 1131 local, 1111 decision theory, 1059, 1068 decision tree, 1085, 1096–1097 strategy, 1087 decision variable, 1017, 1089 decision-making situation, 1061 declarative representation, 1, 1133 deep belief networks, 1000 degree, 34 bounded, 992 density estimation, 699, 784 density function, 27–31 conditional, 31 joint, 29 dependency network, 96, 822, 823 descendant, 36 detailed balance, 515, 546 deterministic separation, 160 digamma function, 907 directed acyclic graph, see DAG Dirichlet distribution, see BDe prior, 738, 746–750 mixture, 779 posterior, 738 sampling, 900 variational update, 906–907 Dirichlet process, 929–930, 941–942 discretization, 606 discriminative training, 709, 950, 997 distance measure, 1140–1143 distance metric, 1140, 1143 distribution, 16 Bernoulli, 20 conditional, 22 cumulative, 28 empirical, 703 Gamma, 765, 780, 900 Gaussian, see Gaussian, 720 joint, 3, 21 Laplacian, 959 marginal, 21 mixture, see Gaussian, mixture, 484, 713, 875, 915 multinomial, 20, 720 normal-Gamma, 751 Poisson, 283 positive, 25, 116 posterior, 3 

prior, 47 support, 494 uniform, 28 duality, 957, 1171–1172 convex, 470 dynamic Bayesian network, 202–205, 837 fully persistent, 658 parameter estimation, 781 structure learning, 846 dynamic programming, 292–296, 337, 356, 371, 482, 596, 1149 dynamical system, see ﬁlter continuous time Bayesian network, 242 Dynamic Bayesian network, see dynamic Bayesian network hidden Markov model, see hidden Markov model linear, 211 Markovian, 201semi-Markov, 202, 243 semi-Markovian, 243 stationary, 202 switching linear, 212, 684 E-step, 872, 874, 907 variational, 896 edge covered, 78, 100 covering, 78 directed, 34 ﬁll, 307, 340 inter-time-slice, 204 intra-time-slice, 204 reversal, 78, 99, 545, 673 spurious, 173 undirected, 34 EKF, see Kalman ﬁlter, extended EM, 535, 907 accelerated, 892 approximate inference, 893–897 Bayesian network, 868–897 computational complexity, 891 table-CPD, 872–874 belief propagation, 897 clustering, 875–877 convergence, 887 practice, 885–887, 890–892 theory, 874–875, 877–884 dynamic Bayesian network, 937 

exponential family, 874 hard assignment, 876, 884–885, 889, 937 incremental, 892, 938 initialization, 889–890 local maxima, 886, 888–890 log-linear model, 955–956 MAP, 898, 940 noisy-or, 936 overﬁtting, 891 single family, 937 tree-CPD, 936 variational, 895–897 empirical distribution Gaussian, 722 endogenous variable, 1027 energy function, 124 canonical, 129 restricted, 592 submodular, 590, 595 truncation, 602 energy functional, 385, 450, 881–882, 905, 914, 940 convex, 416, 962 energy term, 385 entropy term, 385 factored, 386–387, 411 optimization, 411–414 generalized, 414–428 Gibbs distribution, 458 optimization, 459–468 temperature-weighted, 582–585 energy minimization, 553, 599 entanglement, 656–660 entropy, 477, 1138–1142 Bayesian network, 271 conditional, 1139 convex, 417 exponential family, 270 factored, 386, 964 Gaussian, 270 joint, 1139 Markov network, 270 relative, 1141 conditional, 1142 weighted approximate, 415 EP, see expectation propagation equivalent sample size, 740 error absolute, 290, 544 

relative, 291, 491, 544 estimator, 1145 Bayesian, see Bayesian estimation consistent, 769 MAP, see MAP estimation maximum likelihood, see maximum likelihood estimation representation independence, 752–754 unbiased, 1145 variance, 495 event, 15 measurable, 16 evidence, 26 evidence retraction, 339 expectation linearity of, 32 random variable, 31 expectation maximization, see EM expectation propagation, 430, 441, 444, 664 and belief propagation, 482 convergence point, 447 Gaussian message passing, 621 mixture, 621–626, 686–688 nonlinear, 630, 637–642 message passing, see message passing, expectation propagation expectation step, see E-step Expectimax, 1087 expert system, 67 expert systems, 13 explaining away, see reasoning, intercausal, 55, 196 exponential family, 261, 442, 874, 879 Bayesian network, 268–269 Bernoulli, 265 composition, 266 CPD, 267 EM, 874 factor, 266 Gaussian, 263 invertible, 263, 278, 283 linear, 264, 757 linear Gaussian, 267 multinomial, 265 parameter estimation, 732 exponential time, 1148 factor, 5, 104, 296 

division, 365 expected utility, 1108 generalized, 342, 1130 joint, 1103–1107, 1130, 1131 log-space, 360 marginalization, 297, 360, 378 maximization, 555 nonnegative, 104 operations, 358–361 stride, 358 product, 107, 359 reduction, 111, 303 scope, 104 set, 432 marginalization, 432 product, 432 factor graph, 123, 154, 418 factorization, 50 bayesian network, 62 factor graph, 123 Markov network, 109 faithful, 72, 786 faithfulness assumption, 1042 family score, 805 feature indicator, 125 linear dependence, 132 log-linear model, 125 features, 50 ﬁltering, 652 assumed density, 664 bootstrap, 668 particle, 667–674, 680 collapsed, 674, 693, 694 posterior, 671 Rao-Blackwellized, 674 recursive, 654 state-observation model, 653–654 ﬁxed point equations, 482 ﬁxed-point, 402 equations, 390, 412, 424, 447, 451, 458, 479 forest, 38 forward pass, 654 forward sampling, 488–492, 541 convergence bounds, 490–491 estimator, 490 sample size, 490, 544 forward-backward algorithm, 337, 655 

free energy, 385 Bethe, 414 frequentist interpretation, 16 function concave, 41 convex, 41 game theory, 1130 Gamma distribution, see distribution, Gamma Gamma function, 735, 798 Gaussian, 28, 1144 Bayesian network, 251–254, 1084 belief propagation, 612–614 clique tree, 611–612 covariance matrix, 247 exponential family, 264 independencies, 250–251, 258 information matrix, 248 linearization, 631–637, 650 incremental, 640 mean vector, 247 mixture, 190, 616 collapsing, 620–621, 624–626, 685–688pruning, 685 MRF, 254–257 diagonally dominant, 255 pairwise normalizable, 256, 614 walk-summable, 648 multivariate, 247–251 normalizable, 622–624, 639 standard, 28, 248 Gaussian processes, 778 general pseudo-Bayes, 685, 687 generalization, 704–708, 784 generalized linear model, 178 generative training, 709 genetic inheritance, 57–60 GES algorithm, 821 Gibbs distribution, 108 parameter iz ation, see Markov network, parameter iz ation reduced, 111 Gibbs sampling, 505–507, 512–515, 547 block Gibbs, 513 collapsed, 531, 549, 550, 1056 incomplete data, 901–904, 929, 940 continuous state, 644 incomplete data, 899–904 Markov chain, 512 

regularity, 514 stationary distribution, 512, 546 goodness of ﬁt, 708, 839 GPB, see general pseudo-Bayes gradient, 1162 ascent, 863, 1163–1166 Bayesian network, 867–868 conjugate, 1166 convergence, 887 L-BFGS, 950, 991 line search, 1164 Bayesian network, 863–866, 936–937 log-likelihood, 864 chain rule, 864 Gaussian, 937 hidden variable, 937 log-linear model, 948 partition function, 947–948 unstable, 962 grafting, 992 graph acyclic, 37 chordal, 38, 155, 374 connected, 36 directed, 34 moralized Bayesian network, 134 chain graph, 148 singly connected, 38 skeleton, 77 triangulated, 38 undirected, 34 undirected version, 34 graph cut, 588 ground Bayesian network, 217, 221, 224 Gibbs distribution, 229 random variable, 215 guard, 223 Hammersley-Cliford theorem, 116, 1077 Hessian, 1163 Bayesian network incomplete data, 909 log-likelihood, 950 Markov network, 983 partition function, 947 hidden Markov model, 146, 203, 208, 952 coupled, 148, 204 

duration, 244 factorial, 204, 482 hierarchical, 210, 244 mixed memory, 244 phylogenetic, 206, 483 segment, 244 hidden variable, 65, 713, 849, 925–932 cardinality, 928–930 model selection, 928 hierarchical, 931 information, 926–928 overlapping, 931 partition, 929 hierarchical Bayes, 765, 779 HMM, see hidden Markov model Hoefding bound, 490, 771, 1145 holdout testing, 705–708, 795 Hugin, 377 algorithm, see message passing, belief update hybrid network, 186 hyperbolic tangent, 403 hyperparameter, 958 Beta, 735 Dirichlet prior, 738 hierarchical distribution, 765 hypothesis space, 702, 712, 718, 785 hypothesis testing, 787–790 decision rule, 788 deviance, 788 multiple hypotheses, 790, 843 null hypothesis, 787 p-value, 789, 843 I-equivalence, 76, 784, 815 class, 76, 815, 821 I-map, 60 Markov network construction, 120–122 minimal, 79, 786 construction, 79–81 I-projection, 274, 282, 383 Gaussian, 274 ICI, see independence, causal ICU-Alarm, 749, 796, 802, 820, 830, 885 identiﬁability, 702, 861 Bayesian network structure, 784, 841 hidden variable, 861 incomplete data, 860–862 intervention query, 1055 

local, 862 identity resolution, see correspondence, 165, 532 IID, 698, 1144 image denoising, 112 image registration, 532 image segmentation, 113, 478 immorality, 78 potential, 86 importance sampling, 494–505, 545, 547, 966, 1004 adaptive, 542 annealed, 543, 548 backward, 505 Bayesian network, 498–505 collapsed, 527–530 normalized, 496–498, 503, 545 bias, 497 estimator, 497 variance, 497 sample size efective, 498 sequential, 667–672 variance, 671 unnormalized, 494–496, 502 bias, 495 estimator, 495 variance, 495 incremental update, 369–370 indegree, 34, 804 bounded, 85, 786, 787, 811, 814, 826, 841–842 independence, 23–25 causal, 182, 196, 197 symmetric, 183 conditional continuous, 31 events, 24 random variables, 24 context-speciﬁc, 162, 171–175, 196, 1127 events, 23 marginal, 24 persistent, 657 properties, 24–25, 154 contraction, 25 decomposition, 25 intersection, 25 strong union, 154 symmetry, 24 transitivity, 154 weak union, 25 

test, 783, 786–790, 843, 848 independence test Markov network, 979–981 independencies Bayesian network, 56–57 global, 72 local, 57 chain graph global, 151 local, 150 pairwise, 150 distribution, 60 Gaussian, see Gaussian, independencies inclusion, 94 Markov network, 117–120 global, 115 local, 118, 120–122, 979 pairwise, 118, 120–122, 154, 979 indicator function, 32 induced width, 310 inference, 5 inferential loss, 1080 inﬂuence diagram, 93, 1089–1090 expected utility, 1093–1094 limited memory, 1093 reduction, 1120, 1132 inﬂuence graph, 658 information edge, 1090 irrelevant, 1119–1121 information form, 248 information state, 1091 insurance premium, 1066 interface, 464 interface variable, 202 intervention, 1092, 1112 ideal, 1010 query, 1010, 1015 identiﬁability, 1017–1026, 1031–1034 simpliﬁcation, 1018–1026, 1055 Ising model, 126, 127 iterated conditional modes, 599 iterative proportional ﬁtting, 998 iterative proportional scaling, 998, 1002 $\mathrm{I}_{\mathcal{X}}$ -equivalence, 1049 Jensen inequality, 41 join tree, see clique tree junction tree, see clique tree 

Kalman ﬁlter, 211, 259, 676–684 extended, 212, 631, 678 information form, 677 observation update, 677 state transition update, 676 unscented, 635, 678 kernel density, 730 KL-divergence, see entropy, relative knowledge discovery, 701, 783 knowledge-based model construction, 241, 242, 651 label bias problem, 953 Lagrange multipliers, 388, 868, 1168–1172 language model, 209 Laplace’s correction, 735 latent Dirichlet allocation, 769 latent variable, 1012 latent variable network, 1048 Lauritzen’s algorithm, 626 Lauritzen-Spiegelhalter algorithm, see message passing, belief update leaf, 38 leak probability, 176 lifted inference, 689 likelihood, 699 Bayesian network, 723–726 conditional, 701, 725, 950 decomposability, 723–726, 857 global, 725, 859 local, 726, 859 shared parameters, 755 function, 719, 721 incomplete data, 856–860 computational complexity, 860 local, 725 log-likelihood, 699 log-linear model, 944–949 incomplete data, 954–955 likelihood score, 805 likelihood weighting, 493–494, 541 data dependent, 502 expected sample size, 502 DBN, 665–667 estimator, 493, 500 normalized, 503, 504 ratio, 502, 504 

likelihood, marginal, see marginal likelihood linear program, 579 integer, 577 optimization variables, 577 relaxation, 576, 579 local consistency polytope, 412, 477, 580, 964 local maximum, 1156 local probability model, 53 log-likelihood, see likelihood, 699, 719 expected, 699, 878–881 log-linear model, 125, 155, 946 shared parameters, 228, 965, 1002 log-odds, 179 logical variable, 213 logit, see sigmoid loop, 38 loopy belief propagation, see belief propagation, loopy loss function, 699 0/1 loss, 701 Hamming loss, 701, 978 log-loss, 699 lottery, 1059, 1060 compound, 1062 preference, 1060 lower bound, 386, 412, 469–473, 897 variational, see variational, lower bound M-projection, 274, 277–283, 383, 433, 443, 620, 621, 624, 632, 774, 1170–1171 Bayesian network, 284 chain network, 280, 284 exponential family, 278 factor set, 433–436 Gaussian, 274, 279, 283 M-step, 873, 874, 907 MAP, see query, marginal MAP, 26, 574 assignment, 534, 537, 1155 computational complexity, 551–552 integer program, 577–579 k-best, 559, 601–603, 977, 1005 linear program, 579–581marginal, 27, 554, 559–561, 595 MAP estimation, 751, 753, 898, 983 Beta, 754 log-linear model, 958–961, 984–985 block $\mathrm{L_{1}}$ prior, 984 Gaussian prior, see regularization, $L_{2}$ hyperbolic prior, 1003 

$\mathrm{L_{1}}$ prior, 988–992 Laplacian prior, see regularization, $L_{1}$ margin-based estimation, 976–978 marginal independence, see independence marginal likelihood, 738, 744, 795–799, 826 approximation, 909–916 BIC, 911–912, 915 candidate, 913–915 Cheeseman-Stutz, 912–913, 915 Laplace, 909–911, 915 variational, 914 marginal MAP, see MAP, marginal, 685 computational complexity, 552, 560–561 marginal polytope, 411, 477, 580 marginalization, see factor, marginalization strong, 627 weak, 621–630 Markov assumption dynamical system, 201 Markov blanket, 512 Bayesian network, 135, 155 distribution, 121 undirected graph, 118, 980 Markov chain, 507 conductance, 519 ergodic, 510 homogeneous, 507 kernel, 511 mixing, 515, 519–520, 543, 831, 832 empirical, 522–523 multi-kernel, 511, 546 periodic, 510 reducible, 510, 546regular, 510, 546 reversible, 515 temperature, 524 transition model, 507 Markov chain Monte carlo, see MCMC Markov decision process, 1129 Markov inequality, 40 Markov model, see hidden Markov model Markov network, 5, 103–133 decomposition, 155 pairwise, 110, 404, 478 parameter iz ation, 106–109 canonical, 129–132, 154 redundancy, 132–133, 948 tree, 195 reduced, 111 

utility, 1076 Markov random ﬁeld, see Markov network, 105 labeling, 127, 547 metric, 128, 588–595 semimetric, 128, 588–595 max-calibrated, 563, 574 Max-Clique Problem, 1152 max-margin, 1005 max-marginal, 553, 562, 563 decoding, 553, 556–559, 565–567 pseudo, see pseudo-max-marginal ratio optimality, 566 unambiguous, 553 max-product, 552, 582 max-sum, 553, 577, 1117 max-sum-product, 559 maximization step, see M-step maximum entropy, 956–958 approximate, 964–965 distribution, 1169 expectation constraints, 956 maximum entropy Markov model, 952 maximum expected utility, see MEU maximum likelihood estimation, 719, 722 Bayesian network, 723–732 conditional random ﬁeld, 950–953 consistent, 949, 1002 Gaussian, 722, 778 incomplete data computational complexity, 887 linear Gaussian, 728–730 log-linear model, 949–950 dual, 956–958, 1002 using belief propagation, 963–965 using MCMC, 966–967 multinomial, 722 plate models, 757–760 shared parameters, 756–761 table-CPD, 725 maximum spanning tree, 374 MCMC, see Markov chain, 507, 644, 673, 966,975, 1159 burn-in time, 519 collapsed, 531–532, 831 estimator, 521 variance, 521–522 Gibbs sampling, see Gibbs sampling Metropolis-Hastings, see Metropolis-Hastings network structures, 829–831 

reversible jump, 935 sampling, 508, 520–523 autocovariance, 521, 522 variable ordering, 831–832 mean ﬁeld, 449–456, 895, 906 algorithm, 454–456 cluster, 467 convergence point, 451–453 energy, 449–450 mean prediction, 740 medical diagnosis, 51, 67–68, 177, 183, 197, 1124 message decoding, 393 turbocode, 395 message passing belief-update, 364–368 CLG network, 624–626 max-product, 563 clique tree, 351–352 DBN, 654–655 expectation propagation belief-update, 440–442 exponential family, 442–445 Gaussian, 641 sum-product, 437–439 max-product, 563, 603 counting numbers, 573 order-constrained, 623, 639 region graph, 425–428, 480–481 sum-product, 352, 368, 397, 413 generalized, 418 sum-product-divide, 365–368 meta-network, 742, 858, 899 global decomposition, 743 local decomposition, 746 shared parameters, 763 metric, 127 Metropolis-Hastings, 516–518, 542, 547, 832, 942, 1159 acceptance probability, 517 collapsed incomplete data, 940 continuous state, 644 random walk, 645, 903 MEU principle, 1061 strategy decision rule, 1107, 1108 decision tree, 1098–1100 inﬂuence diagram, 1094, 1115, 1131 

value, 1094, 1119 micromort, 1070, 1081 min-ﬁll, 314 weighted, 314 min-neighbors, 314 min-weight, 314 minimax risk, 1083 minimum description length, 802 missing at random, 854, 936 missing completely at random, 853 MLE, see maximum likelihood estimation model dimension, 801, 983 model selection, 785, 978 module network, 846 moment matching, 278, 949 Monte Carlo localization, see ﬁlter, particle, see robot, localization, 680, 691 moral graph, 135 MPE, see query, MAP MRF, see Markov random ﬁeldmulti conditional training, 1004 multinet, 170, 195 mutilated network, 499, 1014 interventional, 1014–1017, 1044 proposal distribution, 499–500, 530 mutual information, 789, 792, 848, 1140 chain rule, 41 conditional, 41 naive Bayes, 49, 727 Bernoulli, 767 clustering, 875, 877, 915 multinomial, 767 tree augmented, 842 naive Markov, 144, 197, 710 natural bounds, 1033 negative deﬁnite, 1163 neighbor, 34 network polynomial, 304, 339, 378 noise parameter, 176 noisy-or model, see CPD, noisy-or normal-Gamma distribution, 780 NP-hardness, 1150–1153 CSI-separation, 196 elimination ordering, 310 inference approximate, 291–292 exact, 288–290 MAP, 551 

polytree CLG, 617 reduction, 1152 structure learning directed, 811, 841 undirected, 1000 triangulation, 313 numerical integration, 633 exact monomials, 634–637 precision, 635 Gaussian quadrature, 633–634 precision, 633 integration rule, 633, 634 precision, 633 object, 213 object skeleton, 214, 229 object uncertainty, 233 object-oriented Bayesian networks, 192 objective function, 702, 718, 1154 concave over the constraints, 417 observability model, 851 variable, 851 observation model, 207 observed variable, 24, 71, 114, 142 optimization constrained, 381, 1167–1171 optimization problem, 1154 outcome, 1061, 1090 anchor, 1064 atomic, 22 space, 15 canonical, 22 overﬁtting, 704–708, 726, 769, 794, 801, 886 P-map, see perfect map PAC-bound, 709, 770 Bayesian network, 773–776 log-linear model, 991, 1000–1001 multinomial, 771–773 parameter sharing, see shared parameters space, 720 parameter distribution Bernoulli, see Beta distribution conjugate prior, 739 Gaussian, see normal Gamma distribution multinomial, see Dirichlet distribution 

prior, 733 parameter independence, 799, 834, 857 global, 742, 805–806, 837 local, 747 parameter modularity, 805 CPD-tree, 835 parameter posterior, 734, 738 parameter prior, see parameter distribution, prior, 738 Bayesian network, 748, 805–806 conjugate, 737 log-linear model conjugate, 961 L 1 , 959 L 2 , 958 parameters, 46, 720 independent, 46, 259, 801 incomplete data, 912 legal, 262 natural, 263 function, 262 space, 264 space, 262 parametric family, 261, 720 parametric model, 720 parent, 34 partial ancestral graph, 1049–1051 partial correlation coefcient, 259 partially directed acyclic graph, see PDAG, 148 particle, 487 collapsed, 487, 526, 543, 674 data completion, 903–904, 940 parameter, 901–903 deterministic, 536–540, 675 deterministic search, 549 weighted, 493 particle ﬁltering smoothing, 692 partition function, 105, 108, 262, 543 approximate, 966 convex, 947 lower bound, 386, 470 upper bound, 1004–1005 Pascal’s wager, 1082 path, 36 active, 114 Pathﬁnder, 67 PDAG, 37, 843 boundary, 34, 149 

class, 87, 786, 821, 1042 PDF, see probability density function peeling, 337 perfect map, 81, 787 construction, 83–92 persistence edge, 204, 658 variable, 204 piecewise training, 1003, 1004 plate, 217 intersection, 218 nested, 218 plate model, 216–222, 837 text, 767 plateau, 1156 point estimate, 737 polynomial time, 1148 polytree, 38, 313, 340, 552, 617 positive deﬁnite, 248 positive semi-deﬁnite, 248 posterior, 26 potential edge, 110 node, 110 Potts model, 127 prediction, 652 preference independence, 1071–1072 prenatal diagnosis, 1079, 1094 prequential analysis, 796 prior, 19 improper, 740 probabilistic context-free grammar, 243 probabilistic ﬁnite-state automaton, 209 probabilistic relational model, 223, 837 parameter estimation, 781 probability distribution, see distribution probability query, 26, 287 approximate, 290–292 computational complexity, 288–292 lower bound, 537 reasoning, 54–55 causal, 54 evidential, 55 intercausal, 55 probability theory, 2 probably approximately correct, 709 projection, see M-projection; I-projection proposal distribution, 644, 1160 importance sampling, 494, 498, 528–530, 542 

MCMC, see Metropolis Hastings, 516 protein structure prediction, 968–969 pseudo-counts, 740 pseudo-marginal, 412, 580 pseudo-max-marginal, 562, 568 decoding, 568–572 pseudo-moment matching, 963, 1004 pseudo likelihood, 970–974 consistent, 972 generalized, 973 QALY, 1070 quadratic program, 976 qualitative probabilistic networks, 94 query variable, 26 random variable, 3, 20–23 Rao-Blackwell iz ation, see collapsed sampling rationality human, 1067–1068 postulates, 1062–1064, 1084 recall edge, 1092 imperfect, 1093, 1109, 1116, 1119 perfect, 1092, 1098, 1131 record matching, see correspondence redundant feature, 133 parameter iz ation, 263 reference class, 17 region graph, 419–428, 572 belief propagation, see belief propagtion, region graph calibrated, 421 construction, 421–423 saturated, 422 regularization, 705, 751 block- $L_{1}$ , 984 $\mathrm{L_{1}}$ , 959, 984 L 2 , 958, 984 log-linear model, 958–961 rejection sampling, 491, 643 relation, 213 relational Markov network, 229, 1002 relational skeleton, 224 relational uncertainty, 225, 233 relative entropy, 771 Bayesian network, 273 exponential family, 272 

relevance graph, 1114–1115, 1131 renormalization, 287, 339 re parameter iz ation, 574, 868 max-product clique tree, 564–565 cluster graph, 568 counting numbers, 574 sum-product clique tree, 362 cluster graph, 399 response variable, 1028–1030, 1035 constraints, 1031–1033 risk, 700, 1066 averse, 1066 empirical, 700 excess, 709, 774 neutral, 1067 seeking, 1067 RoboSoccer, 1117 robot localization, 187, 678–684 mapping, 681, 892–893, 938 SLAM, 681, 694 $\mathcal{R P}$ , 1153 rule, 166 product, 330 reduced, 172 scope, 166 split, 331 sum, 330 running intersection property, see clique tree, running intersection property s-reachable, 1112–1114, 1131 Saint Petersburg paradox, 1065 sample complexity, 709 sample size, 501 search, 675 assignment, 536–540, 595–597 beam, 595, 675, 685, 693, 1158 branch-and-bound, 595, 603, 604, 1160–1161 hill-climbing ﬁrst-ascent, 815, 1155 greedy, 1155 local, 595, 812, 814, 985, 1154–1160 operators, 596, 1154 random restart, 1159 randomization, 1158–1160 space, 595, 812, 1154 

state, 1154 systematic, 595 tabu, 596, 816, 1156 selection bias, 1013 selector variable, 165 semimetric, 128 sensitivity analysis, 67, 95, 305, 339 separation, 115 completeness, 116–117 CA-independence, 1077 soundness, 115–116 sepset, see clique tree, sepset, see cluster graph, sepset sequence labeling, 952 shared parameters, 754, 780–781 global, 755–760 local, 760–761 $\#\mathcal{P}$ , 1153 shrinkage, 243, 764 sigmoid, 145, 179 similarity network, 95, 171 Simpson’s paradox, 1015–1016, 1021 simulated annealing, 524, 1159 smoothing, 652 computational complexity, 692 particle, 692 spanning forest, see spanning tree spanning tree maximum weight, 809, 1146, 1148 speech recognition, 209, 675 standard deviation, 33 standard gamble, 1069 state-observation model, 207 stationary distribution, 509–511 stationary point, 1162 stereo reconstruction, 113, 593 stick-breaking prior, 930 Stirling’s approximation, 843 strategic relevance, 1110–1115 strategy, 1087 complete, 1091 MEU, see MEU strategy structural uncertainty, 232 structure discovery, 825 conﬁdence estimation, 825 network features, 825, 827–828 structure learning constraint-based, 785–790, 1042 Markov network, 979–981, 1005 

score-based, 785, 790–824 undirected model, 981–995 convergence, 990 global maximum, 989 hypothesis space, 981–982 L 1 prior, 988–992 structure modularity, 804 structure score Bayesian, 794–807, 843, 983–984 decomposable, 799–801, 805 BIC, 802, 843, 911, 983 consistent, 803, 822 decomposability, 808, 818–820, 917–919, 986 decomposable, 805 equivalence, 808 Laplace approximation, 983 likelihood, 791–794, 982–983 decomposable, 792 MAP, 984–985 L 1 , 984, 988–995 score equivalence, 807, 821, 844 structure prior, 804 tree-CPD, 834 template model decomposable, 837 tree-CPD decomposable, 834 structure search, 807–824, 1155 computational complexity, 809, 811, 814–815, 818–820 delta score, 818, 917 hidden variable initialization, 932 I-equivalence classes, 821–824 incomplete data, 917–925 heuristics, 919–920 structural EM, 920–925, 932, 941 local maximum, 815–818 operators, 812–814, 845 edge addition, 812 edge deletion, 812 edge reversal, 812, 813 reinsertion, 847 ordering space, 848 parent constraints, 845 plateau, 815 template model, 837 tree-CPD, 835–836 delta-score, 846 

operators, 835 trees, 808–809 undirected model, 985–995 computational complexity, 987 delta-score, 987, 992–995 gain heuristic, 993–995, 1005 gradient heuristic, 992 local maximum, 988 variable ordering, 809–811 structured variational, 448–469, 895 algorithm, 459–468, 482 convergence point, 458, 482 update, 460–468, 482 subgraph complete, 35 induced, 35 subjective interpretation, 17 subutility function, 1071, 1073, 1117 sufcient statistics, 721 aggregate, 756 Bernoulli, 265 collection, 819–820 expected, 278, 871–874, 880 belief propagation, 962 conditional random ﬁeld, 951 log-linear model, 949 MAP assignment, 967–968 MCMC, 966–967, 1004 function, 262 Gaussian, 263, 721 interventional data, 1044–1046, 1056 log-linear model, 947 multinomial, 265, 721 sum-max-sum rule, 1098 sum-product, 299, 582, 611 message passing, see message passing, sum-product support vector machine, 999 survey propagation, 601 Swendson-Wang algorithm, 547 system state, 200 t-node, 1085 table-CPD, see CPD, table target distribution, 494 target tracking, 678–684 target variable, 142 Taylor series, 631 temperature, 582 

temperature parameter, 126, 524, 1160 template variable instantiated, see ground random variable template model dependency graph, 227, 245 factor, 203, 216 instantiated, 216 feature, 228 lifted inference, 689 parent, 221, 223 structure learning, 837–838, 846 variable, 200, 213 template variable, see attribute temporal ordering, 1092, 1097, 1131 test set, 705 time slice, 201time trade-of, 1069 topological ordering, 36, 62, 1146 trail, 36 active, 71 minimal, 100 training set, 705, 720 trajectory, 200 transition model dynamical system, 202 state-observation model, 207 tree, 38, 808 tree re parameter iz ation, see belief propagation, tree re parameter iz ation tree-CPD, see CPD, tree-CPD, 834, 936 structure learning, 834–836, 845 tree-width, 310 bounded, 982, 1000 triangle inequality, 1140 triangulation, 139, 313, 374 troubleshooting, 166, 1027, 1037, 1055, 1124, 1132 truncated norm, 128, 603 TRW, see belief propagation, tree-reweighted uncertainty, 2 unrolled Bayesian network, 204 unscented transformation, 634 upward closure, 35, 136 utility, 1060 additive independence, 1074–1075 CA-independence, 1075–1078, 1084 expected, 1060, 1061, 1064, 1087, 1093 GA-independence, 1078–1079, 1084 

independence, 1072–1073, 1081 variable, 1090 utility function, 1061 curve, 1065–1067 decomposition, 1073 additive, 1073–1080, 1117 multilinear, 1073 multiplicative, 1073 distribution, 1084 elicitation, 1069, 1080–1081 factorization, 1076 human life, 1069–1070 indiference point, 1069 money, 1065–1066 v-structure, 71 validation set, 708, 891 value of control, 1132 value of information, 1121–1125, 1132 myopic, 1125, 1126 perfect, 1122 variable elimination, 299, 372, 1099 and conditioning, 319–322, 340 causal independence, 325–329 chordal graph, 310–313 cliques, 308 computational complexity, 305–310, 336 context-speciﬁc independence, 329–334expected utility, 1100–1107 factor semantics, 301, 338 generalized, 342–343, 1103–1107, 1130, 1131 induced graph, 306–310 max-product, 556 traceback, 558 max-sum-product, 559–561 traceback, 561, 601 ordering, 299–301, 310 computational complexity, 310 constrained, 561, 596, 629, 1100, 1109 heuristics, 310–315, 340 maximum cardinality, 312, 340 rule-based CPDs, 329–334, 341 sum-product, 299 variational, 470–473, 483 with evidence, 303 variable ordering, 79–81, 809, 826 variance, 33 variational Bayesian network, 483 

lower bound, 469–472, 484 method, 386, 469–473 mixture distribution, 484 parameter, 470 sigmoid, 483 variable elimination, 470, 472–473 variational Bayes, 904–908 variational distance, 1143 variational, Markov network, see Gibbs variational visual-analog scale, 1069 Viterbi algorithm, 598, 675 Viterbi training, 967 

witness, 85 