# Abstract
哈希学习是一个广泛应用于大规模图像检索的基础技术，大多数现存的哈希方法通过对二进制约束的连续松弛解决离散优化问题，这常常导致较大的量化误差，最终导致次优的二进制码。近年来，一些离散哈希方法被提出。但是，这些方法不是完全忽视了一些有用的约束(具体地说，即哈希比特的平衡和不相关性)，就是仅仅将这些约束转为正则项，以使得优化更简单，但也会导致更不准确。在这片文章中，我们提出一个新颖的监督哈希方法，称为强约束离散哈希(SCDH)，该方克服了之间提到的限制。它可以在保持之前所提到的约束下，为训练集的所有样本学习到其二进制码的同时，获得一个对不可见样本的哈希函数。即便SCDH的模型相对复杂，我们也可以对它所有的优化子问题找到闭式解，因而设计一个有效的、可以快速收敛的算法。另外，我们对SCDH延伸出核化的版本SCDH_K。我们在三个大型基准测试数据集上的实验表明，SCDH和SCDH_K不仅相较于当前最先进的基准模型达到了更高的MAP分数，并且它们的训练过程也相对于其他有监督方法更快

**Index Trems**——哈希学习，图像检索，离散优化
# 1 Introduction
在如今大数据的时代，有越来越多的数据需要被自动地存储、索引和处理。哈希学习，作为一项可以将数据表示为紧凑哈希码以实现经济存储和高效计算的富有前景的技术，吸引了大量研究者的注意[1]-[3],[5]-[8],[34]。为了促进近似最近邻搜索，二进制码应该尝试保持数据集中每对样本之间的语义相似度。注重于在学习到的二进制汉明空间中保持成对相似度的哈希方法已经在一系列应用中展现了令人印象深刻的结果，尤其是在大规模图像检索中。然而，如何进一步提升这种方法的效率性和有效性如见仍然是一个重要且富有挑战的研究问题。

一般来说，目前有两类哈希学习的方法：无监督的和有监督的，后者的效果常常优于前者(由于对标签信息的利用)但训练速度更慢(由于需要更复杂的优化算法)

无监督方法中，Spectral Hashing(SH)[9]首先用预定义的核函数为未标记的数据构建了成对相似度矩阵，然后通过谱分解解决语义哈希问题，该方法在数据集较大时较为低效，Self-Taught Hashing(STH)[13]，通过利用松弛的SH方法从未标记的训练文档学习紧凑二进制码，然后训练支持向量机分类器为测试文档预测二进制码，和SH相似，耗时的谱分解方法使得该方法在面对大规模现实应用时不可行，Hashing with Graphs(AGH)[10]，将利用锚点图，将成对相似度矩阵转化低秩的邻接矩阵，这使得在大规模图像集上对应的优化问题是计算可行的。但是，该方法生成的二进制码在图像检索上的表现对锚点的选择较为敏感。Scalable Graph Hashing(SGH)[12]利用特征转化以高效近似整个成对相似度矩阵，并且提出了顺序按位学习算法，但该按位优化算法面对大型数据集且哈希码长度较长时较为缓慢。上述的所有无监督哈希学习方法不是对二进制约束进行了连续松弛，就是仅采取了一个离散约束以简化对应的优化问题，因此留下了很大的提升空间。Discrete Graph Hashing(DGH)[11]可以认为是AGH的延伸，该方法将基于图的哈希转化为复杂的离散优化框架。即便DGH如我们的方法利用了强约束，但由于它的优化算法的限制，它的表现常常差于我们的方法，就如之后的实验部分所展示。

Semi-Supervised Hashing(SSH)[14]，利用了数据项的类别标签来推导它们之间的语义相似度/不相似度，然后从中学习二进制码。它的基于特征分解的方法在可用的标记数据量不是特别大时十分快速。Minimal Loss Hashing(MLH)[15]，在结构化支持向量机框架中对数据项中的语义关系进行建模。Supervised Hashing with Kernels(KSH)[16]采用了基于核的哈希函数以学习可以表征复杂非线性数据的二进制码。Fast Supervised Hashing via Decision Trees(FastH)[18]利用了非线性函数相较于线性函数的优势，使用强化决策树以生成更优的二进制码。上述所有的有监督哈希学习方法，以及其他相似的工作[19]-[22],[24],[25]，面临同样的问题：当有标签的数据集较大时，对应的成对相似度矩阵将过大因此这些方法将运行地十分缓慢并且有时难以在一个合理的时间内完成。Column Sampling Based Discrete Supervised Hashing(COSDISH)[26]是一个快速的算法，该算法使用随机部分有标签的数据，可以在几秒内为包含数百万张图片的数据集学习二进制码。但是在该方法中，监督信息没有被完全利用，这限制了该方法的有效性。Fast Scalable Supervised Hashing(FSSH)[44]在它的离散优化算法中结合了成对的和按点的监督信号，在图像检索中表现优秀，但它对成对相似度矩阵的构造是空间效率低下和耗时的。注意这类有监督哈希方法常常要忽略或丢弃一些有用的约束(例如哈希比特的平衡性和不相关性)以让对应的优化问题更易于解决。虽然最近出现了Discrete Proximal Linearized Minimization(DPLM)[46]和Binary Deep Nerual Network(BDNN)[52]，这两个方法都开始将平衡和不相关约束纳入考虑，但它们只是简单将这些约束转化为目标函数的一部分，因此使得优化更简单但更不准确。

目前同样存在一些基于成对相似度的深度哈希学习方法[21],[40]-[43]。这些深度学习方法可以取得有竞争性的表现，但它们都需要大量的训练数据和计算资源(GPUs或TPUs)，这使得它们相对昂贵。本文中，我们聚焦于快速且浅的哈希学习模型，这类模型更经济并且对大多数大规模现实应用更实用。如何减少基于深度学习的哈希方法是一个开放的研究问题，我们将此留为今后的工作。

为了释放有监督哈希方法的全部潜力，我们提出了一个新颖的方法，名为“强约束离散哈希(SCDH)”，它的主要特点总结为以下几点。
- SCDH是一个具有复杂约束的有监督离散哈希方法，约束不仅要求哈希模型生成二进制码，并且要求哈希比特之间的平衡性和不相关性。虽然平衡和不相关约束已经被表明对于无监督的哈希学习方法是及其重要的，它们在现存的有监督的哈希学习方法中往往是缺失的，因为这些限制会加剧离散优化的难度
- 为了解决SCDH的棘手的离散优化问题，我们引入一个辅助变量，并将原问题分解为若干个子问题，每个子问题都有闭式解。这使得学习算法可以在少数轮跌倒中收敛(通常少于10)。进一步，我们将SCDH拓展至一个核化的版本，称为SCDH_K，它可以为复杂的数据集学习非线性的检索函数
- 在三个大规模图像数据集的大量实验表明我们提出的方法优于目前最先进的方法，我们的方法具有更高的检索准确率同时更少的时间开销，例如，在具有18万多张图片的NUS-WIDE数据集上，SCDH/SCDH_K可以在商用PC上在几分钟内训练完毕，同时有优越的表现。
# 2 Related Work
从优化的角度出发，哈希学习技术的发展可以大致分为如下三个阶段
阶段一(在谱松弛下哈希)：据我们所知，谱哈希(SH)[9]方法大约是第一个针对哈希学习任务，提出平衡和不相关约束，以及显然的二进制约束方法。平衡约束要求每个哈希比特有百分之五十的几率激活，而不相关约束要求哈希比特之间是不相关的。然而，这样的问题构想是一个NP难的混合整数优化问题。为了克服障碍，SH选择在学习哈希函数时，松弛二进制约束为连续的。相似地，自教学哈希(STH)[13]，半监督哈希(SSH)[14]，图哈希(AGH)[10]和有监督核哈希(KSH)[16]都属于谱哈希方法的一族，即松弛了二进制约束。这样的连续松弛技巧可以极大的减小优化的难度，但解是次优的，即通过阈值处理连续码以得到的二进制码很可能会劣于保持二进制约束直接优化得到的二进制码[17],[52]。因此，为了避免这样的负面影响，我们提出的SCDH/SCDH_K保持了二进制约束而不是将其松弛。

阶段二(仅在二进制约束下离散哈希)：为了让离散哈希问题是可解的，有监督离散哈希(SDH)[17]虽然并没有使用连续松弛，但是丢弃了平衡和不相关约束(之前提到的基于谱的哈希方法则采用了平衡和不相关约束)，并且因此提出了“离散循环坐标下降”(DCC)算法，快速有监督离散哈希(FSDH)[45]在SDH的基础上采用了可交换回归技巧以得到高效二进制码的闭式解。快速可放缩有监督哈希(FSSH)[44]和FSDH的主要区别在于同时对按点的和成对的标签信息的利用，该方法相较于仅利用按点的监督信息的FSDH方法，可以取得更好的检索效果。基于列采样的离散哈希(COSDISH)[26]，通过在迭代学习过程中对列随机采样以实现对大规模图像数据集的二进制哈希。总的来说，这种类型的方法，这类方法可以通过离散优化算法避免连续约束以直接生成二进制码。但是，它们都忽视了哈希比特所需要的平衡和不相关性质，因此会损害哈希的有效性。相较于这些方法，我们的SCDH/SCDH_K既可以直接生成二进制码，同时也可以尝试满足平衡和不相关约束。

阶段三(在多个约束下的离散哈希)：如SH[9]中说明，平衡和不相关约束可以帮助最大化二进制码的紧凑型。最近，离散近端线性化最小算法(DPLM)[46]和二进制深度神经网络(BDNN)[52]被提出，将所有的约束(二进制，平衡和不相关)纳入考虑以达到强约束下的离散哈希。然而，这些方法实际做的是将平衡和不相关性质从约束移动到目标函数内，即不是将它们视为约束项而是视为正则项。尽管这是一个用于近似解决难优化问题的通用的技巧，它通常需要对应的算法进行更多轮的迭代以收敛。相比较下，我们的SCDH/SCDH_K在保持平衡性和不相关性为约束的同时，尝试找到该强约束优化问题的闭式解。可以得到闭式解使得我们的算法相较于先前提到的迭代式算法速度更快。
# 3 Problem Statement
令$\mathcal D = \{(\symbfit x_i, \symbfit l_i)\}_{i=1}^N$表示图像集，其中$\symbfit x_i\in \mathbb R^M$表示第$i$张图片对应的$M$维度向量，且$\symbfit l_i \in \{0,1\}^C$为对应的标签向量，即如果图片$\symbfit x_i$属于第$c$个类别($c\in \{1, 2, \cdots, C\}$)，则$\symbfit l_i$的第$c$个元素为1，否则为0。$N$和$C$分别是数据集中图片的数量和类别的数量。和在参考文献[39]中一样，$\symbfit x_i$和$\symbfit x_j$($i,j = 1, 2,\cdots, N$)之间的相似度的计算如下：
公式(1)
如果我们进一步设置：
公式(2)
则成对相似度矩阵$\symbfit S = (s_{ij})_{N\times N}$可以通过以下公式从标签信息中推导出：
公式(3)
其中每个元素$s_{ij}$的范围都是$[-1,+1]$，我们的目的在于学习到一系列可以在汉明空间保持基于标签的成对相似度的哈希函数。具体地说，即$K$个哈希函数$\symbfit H(\cdot) = [h_1(\cdot),h_2(\cdot),\cdots,h_K(\cdot)]^T$将每个图像$\symbfit x_i$嵌入为$K$比特二进制码，即$\symbfit b_i = \symbfit H(\symbfit x_i)\in\{-1,+1\}^K$，那么整个数据集就可以被转化为$\symbfit B = [\symbfit b_1,\symbfit b_2,\cdots, \symbfit b_N]^T\in\{-1,+1\}^{N\times K}$。
原则上，如果$\symbfit x_i$和$\symbfit x_j$共享更多的类别标签，则它们相对应的二进制码$\symbfit b_i$和$\symbfit b_j$之间的汉明距离应该更小。本文中所用到的数学标记总结与表格1。
# 4 Proposed Method
在本部分，我们将详细描述SCDH，一个新颖的有监督离散哈希方法。在其中的联合学习框架中，二进制码和哈希函数可以同时得到。

A. 相似度保持
给定一对图像$(\symbfit x_i,\symbfit x_j)$，其中的每一个图像都被编码为$\{-1,+1\}^K$空间中的$K$比特向量，它们之间的点积值(范围应在$[-K,+K]$)理想下应该和图像之间的语义相似度$s_{ij}$成正比。因此，我们让二进制码通过以下优化保持成对相似度：
公式(4)
其中约束$\symbfit B^T\symbfit 1_N = \symbfit 0_K$要求哈希比特是平衡的(即每一位都有百分之五十的几率被激活)，并且约束$\symbfit B^T\symbfit B = N\cdot \symbfit I_K$要求哈希比特之间是不相关的。平衡和不相关这两个约束，是已知可以促进紧凑二进制码的生成的[9],[11]。
公式(4)中的损失函数在保持成对相似度中的有监督哈希方法中是很常见的，但其中存在两个计算挑战：(1)如何高效构建$N\times N$的成对相似度矩阵$S$。(2)如何高效解决该强约束下的离散优化问题。为了解决第一个挑战，如公式(3)所示，我们通过低秩的矩阵$\symbfit G$(通常$C\ll N$)表示$\symbfit S$，这可以极大地减少存储开销并且同时大幅度加速后续哈希过程的计算。考虑到第二个挑战，大多数现存的哈希方法(例如SH[9]和STH[13])将离散约束$\symbfit B\in\{-1,+1\}^{N\times K}$松弛为连续的$\symbfit B\in \mathbb R^{N\times K}$，这简化了优化过程，但同时损害了检索表现。但是我们的方法可以在辅助变量的帮助下保持离散约束，就如下文所解释。

B. 联合学习
令$\symbfit X = [\symbfit x_1, \symbfit x_2, \cdots, \symbfit x_N]^T$。对于快速的图像检索，我们使用线性的哈希函数$\symbfit P \in \mathbb R^{M\times K}$以生成二进制码：
公式(5)
通过将公式(4)展开为以下形式，哈希函数$\symbfit P$可以和二进制码$\symbfit B$同时学习到：
公式(6)
其中$\lambda$是一个正的参数，用以衡量二进制码和哈希函数的相对重要性，而$\beta$是一个非负的平滑因子以避免过拟合。
取符号函数$sgn(\cdot)$是不可微的，这使得优化问题(6)难以直接解决。因此，我们将$sgn(\symbfit X\symbfit P)$替换为$\symbfit X\symbfit P$。即我们要求$\symbfit X\symbfit P$的每个元素本身而不是它的符号和$\symbfit B$的对应元素(只能是$+1$或$-1$)尽可能接近。另外，为了让离散优化问题更简单，我们引入一个辅助变量$\symbfit Z$作为$\symbfit B$的别名(即$\symbfit Z = \symbfit B$)，并且将公式(6)重写为：
公式(7)

C. 完整的优化问题
最终，我们进一步丢弃约束$\symbfit Z = \symbfit B$，并且将$\symbfit Z$的元素松弛为实值的连续变量以近似$\symbfit B$。换句话说，$\symbfit B$和$\symbfit Z$不再要求严格相等，但它们之间要相互近似。因此，将上述所有条件纳入考虑得到的总优化函数可以写为公式(7)，如下所示：
公式(7)
其中额外参数$\alpha$用于控制$\symbfit Z$对$\symbfit B$的近似程度。
在上述的联合学习框架下，训练数据对应的二进制码和训练集外样本的哈希函数(即测试样本，新的查询样本)可以被同时得到。给定一系列训练集外样本$\symbfit X_{oos}$，我们可以使用哈希函数将他们编码为二进制码：
公式(9)
这实质上只是一个线性变换，因此可以被非常高效地计算。

D. 核化
如KLSH[35],[36]，KSH[16]和FastH[18]中所提到，非线性哈希函数，归功于它们可以拟合数据中复杂模式的能力，常常表现由于线性哈希函数。SCDH也可以通过核函数延伸为非线性哈希函数。给定一个非线性映射$\Phi:\symbfit x \in \mathbb R^M \to \Phi(\symbfit x)\in \mathbb R^D$($D$可以是无限)，整个图像数据集可以被映射为$\Phi(\symbfit X) = [\Phi(\symbfit x_1),\Phi(\symbfit x_2),\cdots, \Phi(x_N)]^T \in \mathbb R^{N\times D}$。让我们从图像数据集中随机选取$Q$个锚点(即一个图像集的一个子集)，将它们标识为$\symbfit y_1, \symbfit y_2,\cdots, \symbfit y_Q$，那么我们可以视$\Phi(\symbfit y_1),\Phi(\symbfit y_2),\cdots, \Phi(\symbfit y_Q)$为一系列可以用于表示$\mathbb R^D$空间中任意向量的基向量。这是一个处理大数据的常用技巧，并且在实际中常常效果不错。因此，我们有：
公式(10)
其中$\symbfit A \in \mathbb R^{Q\times K}$，同样，公式(5)可以被延伸为
公式(11)
令$\mathcal K: \mathbb R^D \times \mathbb R^D \to \mathbb R$表示非线性映射$\Phi$对应的核函数并且令$\mathcal K_Q=(\Phi(\symbfit x_i)^T\Phi(\symbfit y_j))_{N\times Q}$\表示核矩阵，和公式(8)相似，核化的SCDH算法写为：
公式(12)
我们简称其为SCDH_K。
在选择了核函数$\mathcal K$并且矩阵$A$也被学习到后，一个训练集外样本$\symbfit x_{oos}$可以被编码为：
公式(13)
上式中包含的向量-矩阵乘法在计算上是十分高效的，因为我们常常有$Q \ll N$
