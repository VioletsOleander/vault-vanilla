# The Model Hub

## What is the Model Hub?
The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with theÂ [`huggingface_hub`Â client library](https://huggingface.co/docs/huggingface_hub/index), with ğŸ¤—Â [`Transformers`](https://huggingface.co/docs/transformers/index)Â for fine-tuning and other usages or with any of the overÂ [15 integrated libraries](https://huggingface.co/docs/hub/models-libraries). You can even leverage theÂ [Serverless Inference API](https://huggingface.co/docs/hub/models-inference)Â orÂ [Inference Endpoints](https://huggingface.co/docs/inference-endpoints). to use models in production settings.

# Model Cards

## What are Model Cards?
Model cards are files that accompany the models and provide handy information. Under the hood, model cards are simple Markdown files with additional metadata. Model cards are essential for discoverability, reproducibility, and sharing! You can find a model card as theÂ `README.md`Â file in any model repo.

The model card should describe:

- the model
- its intended uses & potential limitations, including biases and ethical considerations as detailed inÂ [Mitchell, 2018](https://arxiv.org/abs/1810.03993)
- the training params and experimental info (you can embed or link to an experiment tracking platform for reference)
- which datasets were used to train your model
- the modelâ€™s evaluation results

> model card å³ model ä»“åº“çš„ `README.md` æ–‡ä»¶
> model card åº”è¯¥æè¿°ï¼š
> æ¨¡å‹æœ¬èº«
> ç”¨é€”ã€æ½œåœ¨çš„é™åˆ¶ (åŒ…æ‹¬åç½®å’Œé“å¾·ä¸Šçš„è€ƒè™‘)
> è®­ç»ƒå‚æ•°å’Œè¯•éªŒä¿¡æ¯
> ä½¿ç”¨çš„è®­ç»ƒæ•°æ®é›†
> æ¨¡å‹çš„è¯„ä¼°ç»“æœ

The model card template is availableÂ [here](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md).

How to fill out each section of the model card is described inÂ [the Annotated Model Card](https://huggingface.co/docs/hub/model-card-annotated).

Model Cards on the Hub have two key parts, with overlapping information:

- [Metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata)
- [Text descriptions](https://huggingface.co/docs/hub/model-cards#model-card-text)

> model card çš„ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ä¸ºï¼šå…ƒæ•°æ®ã€æ–‡æœ¬æè¿°

## Model card metadata
A model repo will render itsÂ `README.md`Â as a model card. The model card is aÂ [Markdown](https://en.wikipedia.org/wiki/Markdown)Â file, with aÂ [YAML](https://en.wikipedia.org/wiki/YAML)Â section at the top that contains metadata about the model.
> model card ä¸º markdown æ–‡ä»¶ï¼Œå®ƒçš„é¡¶éƒ¨åº”è¯¥æœ‰ yaml éƒ¨åˆ†æè¿° model çš„å…ƒæ•°æ®

The metadata you add to the model card supports discovery and easier use of your model. For example:

- Allowing users to filter models atÂ [https://huggingface.co/models](https://huggingface.co/models).
- Displaying the modelâ€™s license.
- Adding datasets to the metadata will add a message readingÂ `Datasets used to train:`Â to your model page and link the relevant datasets, if theyâ€™re available on the Hub.

> model card çš„å…ƒæ•°æ®ç”¨äºå¸®åŠ©ç”¨æˆ·æŸ¥æ‰¾å’Œä½¿ç”¨æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
> ç”¨äºè¿‡æ»¤å¼æœç´¢ç‰¹å®šç±»å‹çš„æ¨¡å‹
> å±•ç¤ºæ¨¡å‹çš„è®¸å¯è¯
> å¦‚æœå…ƒæ•°æ®ä¸­çš„æ•°æ®é›†å¯ä»¥åœ¨ huggingface hub ä¸­å­˜åœ¨ï¼Œmodel card ä¸­ä¼šç›´æ¥åŒ…å«æŒ‡å‘è¯¥æ•°æ®é›†çš„ URL

Dataset, metric, and language identifiers are those listed on theÂ [Datasets](https://huggingface.co/datasets),Â [Metrics](https://huggingface.co/metrics)Â andÂ [Languages](https://huggingface.co/languages)Â pages.

### Adding metadata to your model card
There are a few different ways to add metadata to your model card including:

- Using the metadata UI
- Directly editing the YAML section of theÂ `README.md`Â file
- Via theÂ [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub)Â Python library, see theÂ [docs](https://huggingface.co/docs/huggingface_hub/guides/model-cards#update-metadata)Â for more details.

Many libraries withÂ [Hub integration](https://huggingface.co/docs/hub/models-libraries)Â will automatically add metadata to the model card when you upload a model.
> è®¸å¤šé›†æˆäº† huggingface hub çš„åº“ä¼šè‡ªåŠ¨åœ¨æˆ‘ä»¬ä¸Šä¼ æ¨¡å‹æ—¶æ·»åŠ å…ƒæ•°æ®

#### Using the metadata UI
You can add metadata to your model card using the metadata UI. To access the metadata UI, go to the model page and click on theÂ `Edit model card`Â button in the top right corner of the model card. This will open an editor showing the model cardÂ `README.md`Â file, as well as a UI for editing the metadata.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/metadata-ui-editor.png)

This UI will allow you to add key metadata to your model card and many of the fields will autocomplete based on the information you provide. Using the UI is the easiest way to add metadata to your model card, but it doesnâ€™t support all of the metadata fields. If you want to add metadata that isnâ€™t supported by the UI, you can edit the YAML section of theÂ `README.md`Â file directly.

#### Editing the YAML section of the README.md file
You can also directly edit the YAML section of theÂ `README.md`Â file. If the model card doesnâ€™t already have a YAML section, you can add one by adding threeÂ `---`Â at the top of the file, then include all of the relevant metadata, and close the section with another group ofÂ `---`Â like the example below:

```
---
language: 
  - "List of ISO 639-1 code for your language"
  - lang1
  - lang2
thumbnail: "url to a thumbnail used in social sharing"
tags:
- tag1
- tag2
license: "any valid license identifier"
datasets:
- dataset1
- dataset2
metrics:
- metric1
- metric2
base_model: "base model Hub identifier"
---
```

You can find the detailed model card metadata specificationÂ [here](https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1).

### Specifying a library
You can specify the supported libraries in the model card metadata section. Find more about our supported librariesÂ [here](https://huggingface.co/docs/hub/models-libraries). The library will be specified in the following order of priority:

1. SpecifyingÂ `library_name`Â in the model card (recommended if your model is not aÂ `transformers`Â model). This information can be added via the metadata UI or directly in the model card YAML section:

```
library_name: flair
```

2. Having a tag with the name of a library that is supported

```
tags:
- flair
```

> model card çš„å…ƒæ•°æ®åŒºåŸŸå¯ä»¥ç”¨äºæŒ‡å®šæ¨¡å‹ä¸­æ”¯æŒçš„åº“ï¼ŒæŒ‡å®šçš„æ–¹å¼æœ‰ï¼š 1. `library_name` (å¦‚æœæ¨¡å‹ä¸æ˜¯ `transformers` æ¨¡å‹ï¼Œéƒ½æ¨èä½¿ç”¨è¯¥æ–¹å¼) 2. åœ¨ `tags` ä¸­æŒ‡å®šæ”¯æŒåº“å

If itâ€™s not specified, the Hub will try to automatically detect the library type. However, this approach is discouraged, and repo creators should use the explicitÂ `library_name`Â as much as possible.

1. By looking into the presence of files such asÂ `*.nemo`Â orÂ `*.mlmodel`, the Hub can determine if a model is from NeMo or CoreML.
2. In the past, if nothing was detected and there was aÂ `config.json`Â file, it was assumed the library wasÂ `transformers`. For model repos created after August 2024, this is not the case anymore â€“Â so you need toÂ `library_name: transformers`Â explicitly.

> å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œhuggingface hub ä¼šè‡ªåŠ¨æ£€æµ‹åº“çš„ç±»å‹
> æ¨èå°½é‡æ˜¾å¼ç”¨ `library_name` ï¼ŒåŒ…æ‹¬ `transformers` æ¨¡å‹ (`libraray_name: transformers`)

### Specifying a base model
If your model is a fine-tune, an adapter, or a quantized version of a base model, you can specify the base model in the model card metadata section. This information can also be used to indicate if your model is a merge of multiple existing models. Hence, theÂ `base_model`Â field can either be a single model ID, or a list of one or more base_models (specified by their Hub identifiers).
> å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹æ˜¯åŸºç¡€æ¨¡å‹çš„å¾®è°ƒ/é‡åŒ–ï¼Œæˆ–è€…èåˆäº†å¤šä¸ªæ¨¡å‹ï¼Œå¯ä»¥åœ¨ `base_model` ä¸­æŒ‡å®šåŸºç¡€æ¨¡å‹

```
base_model: HuggingFaceH4/zephyr-7b-beta
```

This metadata will be used to display the base model on the model page. Users can also use this information to filter models by base model or find models that are derived from a specific base model:

For a fine-tuned model:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/base-model-ui.png)

For an adapter (LoRA, PEFT, etc):

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/base_model_adapter.png)

For a quantized version of another model:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/base_model_quantized.png)

For a merge of two or more models:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/base_model_merge.png)

In the merge case, you specify a list of two or more base_models:

```
base_model:
- Endevor/InfinityRP-v1-7B
- l3utterfly/mistral-7b-v0.1-layla-v4
```

The Hub will infer the type of relationship from the current model to the base model (`"adapter", "merge", "quantized", "finetune"`) but you can also set it explicitly if needed:Â `base_model_relation: quantized`Â for instance.
> huggingface hub ä¼šè‡ªåŠ¨æ¨ç†æœ¬æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹çš„å…³ç³» (`adapter, merge, quantized, finetune` )ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨ `base_model_relation` ä¸­æŒ‡å®š

### Specifying a new version
If a new version of your model is available in the Hub, you can specify it in aÂ `new_version`Â field.
> `new_version` æŒ‡å®šæ¨¡å‹çš„æ–°ç‰ˆæœ¬

For example, onÂ `l3utterfly/mistral-7b-v0.1-layla-v3`:

```
new_version: l3utterfly/mistral-7b-v0.1-layla-v4
```

This metadata will be used to display a link to the latest version of a model on the model page. If the model linked inÂ `new_version`Â also has aÂ `new_version`Â field, the very latest version will always be displayed.
> å¦‚æœ `new_version` æŒ‡å‘çš„ä¹Ÿæœ‰ `new_version` ï¼Œå°±å±•ç¤ºæœ€æ–°çš„

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_version.png)

### Specifying a dataset
You can specify the datasets used to train your model in the model card metadata section. The datasets will be displayed on the model page and users will be able to filter models by dataset. You should use the Hub dataset identifier, which is the same as the datasetâ€™s repo name as the identifier:

```
datasets:
- imdb
- HuggingFaceH4/no_robots
```

### Specifying a task ( pipeline_tag )
You can specify theÂ `pipeline_tag`Â in the model card metadata. TheÂ `pipeline_tag`Â indicates the type of task the model is intended for. This tag will be displayed on the model page and users can filter models on the Hub by task. This tag is also used to determine whichÂ [widget](https://huggingface.co/docs/hub/models-widgets#enabling-a-widget)Â to use for the model and which APIs to use under the hood.
> `pipeline_tag` æŒ‡å®šæ¨¡å‹ç”¨äºå“ªç§ç±»å‹çš„ä»»åŠ¡ï¼Œè¯¥ tag ä¹Ÿä¼šè¢«ç”¨äºå†³å®šå“ªäº› widget å’Œ API ä¼šè¢«ç”¨äºæ¨¡å‹

ForÂ `transformers`Â models, the pipeline tag is automatically inferred from the modelâ€™sÂ `config.json`Â file but you can override it in the model card metadata if required. Editing this field in the metadata UI will ensure that the pipeline tag is valid. Some other libraries with Hub integration will also automatically add the pipeline tag to the model card metadata.
> `transformers` æ¨¡å‹çš„ `pipeline_tag` è‡ªåŠ¨ä»æ¨¡å‹çš„ `cofig.json` æ–‡ä»¶ä¸­æ¨å¯¼ï¼Œæ¨å¯¼çš„ç»“æœå¯ä»¥æ‰‹åŠ¨è¦†ç›–

### Specifying a license
You can specify the license in the model card metadata section. The license will be displayed on the model page and users will be able to filter models by license. Using the metadata UI, you will see a dropdown of the most common licenses.

If required, you can also specify a custom license by addingÂ `other`Â as the license value and specifying the name and a link to the license in the metadata.

```
# Example from https://huggingface.co/coqui/XTTS-v1
---
license: other
license_name: coqui-public-model-license
license_link: https://coqui.ai/cpml
---
```

If the license is not available via a URL you can link to a LICENSE stored in the model repo.

### Evaluation Results
You can specify yourÂ **modelâ€™s evaluation results**Â in a structured way in the model card metadata. Results are parsed by the Hub and displayed in a widget on the model page. Here is an example on how it looks like for theÂ [bigcode/starcoder](https://huggingface.co/bigcode/starcoder)Â model:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/eval-results-v2.png)

The metadata spec was based on Papers with codeâ€™sÂ [model-index specification](https://github.com/paperswithcode/model-index). This allow us to directly index the results into Papers with codeâ€™s leaderboards when appropriate. You can also link the source from where the eval results has been computed.

Here is a partial example to describeÂ [01-ai/Yi-34B](https://huggingface.co/01-ai/Yi-34B)â€™s score on the ARC benchmark. The result comes from theÂ [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)Â which is defined as theÂ `source`:

```
---
model-index:
  - name: Yi-34B
    results:
      - task:
          type: text-generation
        dataset:
          name: ai2_arc
          type: ai2_arc
        metrics:
          - name: AI2 Reasoning Challenge (25-Shot)
            type: AI2 Reasoning Challenge (25-Shot)
            value: 64.59
        source:
          name: Open LLM Leaderboard
          url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
---
```

For more details on how to format this data, check out theÂ [Model Card specifications](https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1).

### CO2 Emissions
The model card is also a great place to show information about the CO2Â impact of your model. Visit ourÂ [guide on tracking and reporting CO2Â emissions](https://huggingface.co/docs/hub/model-cards-co2)Â to learn more.

### Linking a Paper
If the model card includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the model tags with the formatÂ `arxiv:<PAPER ID>`. Clicking on the tag will let you:

- Visit the Paper page
- Filter for other models on the Hub that cite the same paper.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-arxiv.png)

Read more about Paper pagesÂ [here](https://huggingface.co/docs/hub/paper-pages).

## Model Card text
Details on how to fill out a human-readable model card without Hub-specific metadata (so that it may be printed out, cut+pasted, etc.) is available in theÂ [Annotated Model Card](https://huggingface.co/docs/hub/model-card-annotated).

## FAQ

### How are model tags determined?
Each model page lists all the modelâ€™s tags in the page header, below the model name. These are primarily computed from the model card metadata, although some are added automatically, as described inÂ [Enabling a Widget](https://huggingface.co/docs/hub/models-widgets#enabling-a-widget).

### Can I add custom tags to my model?
Yes, you can add custom tags to your model by adding them to theÂ `tags`Â field in the model card metadata. The metadata UI will suggest some popular tags, but you can add any tag you want. For example, you could indicate that your model is focused on finance by adding aÂ `finance`Â tag.

### How can I indicate that my model is not suitable for all audiences
You can add aÂ `not-for-all-audience`Â tag to your model card metadata. When this tag is present, a message will be displayed on the model page indicating that the model is not for all audiences. Users can click through this message to view the model card.

### Can I write LaTeX in my model card?
Yes! The Hub uses theÂ [KaTeX](https://katex.org/)Â math typesetting library to render math formulas server-side before parsing the Markdown.

You have to use the following delimiters:

- `$$ ... $$`Â for display mode
- `\\(...\\)`Â for inline mode (no space between the slashes and the parenthesis).
