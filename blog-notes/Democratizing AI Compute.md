# 1 DeepSeek's Impact on AI
Site: https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai
Date: 30 January 2025

DeepSeekâ€™s recent breakthrough has upended assumptions about AIâ€™s compute demands, showing thatÂ **better hardware utilization**Â can dramatically reduce the need for expensive GPUs.
>  DeepSeek çš„æœ€è¿‘çªç ´é¢ è¦†äº†å…³äº AI è®¡ç®—éœ€æ±‚çš„å‡è®¾ï¼Œè¡¨æ˜äº†æ›´å¥½çš„ç¡¬ä»¶åˆ©ç”¨ç‡å¯ä»¥æ˜¾è‘—å‡å°‘å¯¹æ˜‚è´µ GPU çš„éœ€æ±‚

For years, leading AI companies have insisted thatÂ **only those with**Â [**vast compute resources**](https://www.bbc.com/news/articles/cy4m84d2xz2o)Â can drive cutting-edge research, reinforcing the idea that it is â€œ[hopeless to catch up](https://www.youtube.com/watch?v=EtMsG2UtMUU)â€ unless you have billions of dollars to spend on infrastructure. But DeepSeekâ€™s success tells a different story:Â **novel ideas can unlock efficiency breakthroughs to accelerate AI**, and smaller, highly focused teams toÂ **challenge industry giantsâ€“** and even level the playing field.
>  å¤šå¹´æ¥ï¼Œé¢†å…ˆçš„ AI å…¬å¸ä¸€ç›´åšä¿¡åªæœ‰æ‹¥æœ‰å¤§é‡è®¡ç®—èµ„æºçš„å…¬å¸å¯ä»¥æ¨åŠ¨å‰æ²¿ç ”ç©¶
>  ä½† DeepSeek çš„æˆåŠŸåˆ™è¯´æ˜: åˆ›æ–°çš„æƒ³æ³•å¯ä»¥å¸¦æ¥æ•ˆç‡çš„çªç ´ï¼Œå¹¶ä¸”å°å‹è€Œä¸“æ³¨çš„å›¢é˜Ÿä¹Ÿèƒ½æŒ‘æˆ˜è¡Œä¸šå·¨å¤´ï¼Œç”šè‡³å®ç°å…¬å¹³ç«äº‰

We believe DeepSeekâ€™s efficiency breakthrough signals aÂ **coming surge in demand**Â for AI applications. If AI is to continue advancing, we mustÂ **drive down the Total Cost of Ownership (TCO)**â€“by expanding access to alternative hardware, maximizing efficiency on existing systems, and accelerating software innovation. Otherwise, we risk a future where AIâ€™s benefits areÂ **bottlenecked**â€“either byÂ **hardware shortages**Â or by developers struggling to effectively utilize the diverse hardware that is available.
>  æˆ‘ä»¬è®¤ä¸º DeepSeek åœ¨æ•ˆç‡ä¸Šçš„çªç ´é¢„ç¤ºç€å¯¹ AI åº”ç”¨çš„éœ€æ±‚å°†å¤§å¹…å¢é•¿
>  å¦‚æœ AI è¦è¿›æ­¥ï¼Œæˆ‘ä»¬éœ€è¦é™ä½æ€»ä½“æ‹¥æœ‰æˆæœ¬ â€”â€” é€šè¿‡æ‰©å¤§å¯¹æ›¿ä»£ç¡¬ä»¶çš„è®¿é—®ã€æœ€å¤§åŒ–ç°æœ‰ç³»ç»Ÿçš„æ•ˆç‡ï¼ŒåŠ é€Ÿè½¯ä»¶åˆ›æ–°
>  å¦åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥é¢ä¸´æœªæ¥ AI è¢«ç“¶é¢ˆé™åˆ¶ï¼Œè¦ä¹ˆæ˜¯ç¡¬ä»¶çŸ­ç¼ºï¼Œè¦ä¹ˆæ˜¯å¼€å‘è€…éš¾ä»¥åˆ©ç”¨å¤šæ ·åŒ–çš„ç¡¬ä»¶

This isnâ€™t just an abstract problemâ€“it's a challenge Iâ€™ve spent my career working to solve.

## My passion for compute + developer efficiency
I've spent the past 25 years working to unlock computing power for the world. I founded and led the development ofÂ [LLVM](https://en.wikipedia.org/wiki/LLVM), a compiler technology that opened CPUs to new applications of compiler technology. Today, LLVM is the foundation for performance-oriented programming languages like C++, Rust, Swift and more. It powers nearly all iOS and Android apps, as well as the infrastructure behind major internet services from Google and Meta.

This work paved the way for several key innovations I led at Apple, including the creation ofÂ [OpenCL](https://en.wikipedia.org/wiki/OpenCL), an early accelerator framework now widely adopted across the industry, the rebuild of Appleâ€™s CPU and GPU software stack using LLVM, and the development of theÂ [Swift programming language](https://en.wikipedia.org/wiki/Swift_\(programming_language\)). These experiences reinforced my belief in the power of shared infrastructure, the importance of co-designing hardware and software, and how intuitive, developer-friendly tools unlock the full potential of advanced hardware.

## Falling in love with AI
In 2017, I became fascinated by AIâ€™s potential and joined Google to lead software development for the TPU platform. At the time, the hardware was ready, but the software wasnâ€™t functional. Over the next two and a half years, through intense team effort, we launchedÂ [TPUs in Google Cloud](https://cloud.google.com/tpu), scaled them to ExaFLOPS of compute, and built a research platform that enabled breakthroughs likeÂ [_Attention Is All You Need_](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need)Â andÂ [BERT](https://en.wikipedia.org/wiki/BERT_\(language_model\)).

Yet, this journey revealed deeper troubles in AI software. Despite TPUs' success, they remain only semi-compatible with AI frameworks like PyTorchâ€“an issue Google overcomes with vast economic and research resources. A common customer question was,Â **â€œCan TPUs run arbitrary AI models out of the box?â€**Â The hard truth?Â **Noâ€“because we didnâ€™t have CUDA, the de facto standard for AI development.**
>  å°½ç®¡ TPU å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä¸ PyTorch ç­‰ AI æ¡†æ¶çš„å…¼å®¹æ€§ä»ç„¶æœ‰é™ï¼ŒGoogle ç”¨äº†å¤§é‡çš„ç»æµå’Œç ”ç©¶èµ„æºè§£å†³äº†è¿™ä¸€é—®é¢˜
>  ä¸€ä¸ªå¸¸è§çš„å®¢æˆ·é—®é¢˜æ˜¯: TPU èƒ½å¦å¼€ç®±å³ç”¨åœ°è¿è¡Œä»»æ„ AI æ¨¡å‹
>  ç­”æ¡ˆæ˜¯ä¸èƒ½ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ CUDA â€”â€” AI å¼€å‘çš„äº‹å®æ ‡å‡†

Iâ€™m not one to shy away from tackling major industry problems: my recent work has been the creation of next-generation technologies to scale into this new era of hardware and accelerators. This includes the MLIR compiler framework (widely adopted now for AI compilers across the industry) and the Modular team has spent the last 3 years building something specialâ€“but weâ€™ll share more about that later, when the time is right.
>  ç¬”è€…æœ€è¿‘çš„å·¥ä½œå°±æ˜¯å¼€å‘ä¸‹ä¸€ä»£æŠ€æœ¯ï¼Œä»¥é€‚åº”è¿™ä¸ªæ–°çš„ç¡¬ä»¶å’ŒåŠ é€Ÿå™¨æ—¶ä»£

## How do GPUs and next-generation compute move forward?
Because of my background and relationships across the industry, Iâ€™m often asked about the future of compute. Today, countless groups are innovating in hardware (fueled in part by NVIDIAâ€™s soaring market cap), while many software teams are adopting MLIR to enable new architectures. At the same time, senior leaders are questioning whyâ€“despite massive investmentsâ€“the AI software problem remains unsolved. The challenge isnâ€™t a lack of motivation or resources. So why does the industry feel stuck?
>  å¦‚ä»Šï¼Œæ— æ•°å›¢é˜Ÿæ­£åœ¨ç¡¬ä»¶é¢†åŸŸè¿›è¡Œåˆ›æ–°ï¼ŒåŒæ—¶è®¸å¤šè½¯ä»¶å›¢é˜Ÿæ­£åœ¨é‡‡ç”¨ MLIR åœ¨æ”¯æŒæ–°çš„æ¶æ„
>  äºæ­¤åŒæ—¶ï¼Œé¢†å¯¼è€…ä¹Ÿåœ¨è´¨ç–‘ä¸ºä»€ä¹ˆæŠ•å…¥å·¨å¤§ï¼ŒAI è½¯ä»¶é—®é¢˜ä»ç„¶æ²¡æœ‰è§£å†³ï¼Œå¦‚æœé—®é¢˜ä¸æ˜¯ç¼ºä¹èµ„æºå’ŒåŠ¨åŠ›ï¼Œä¸ºä»€ä¹ˆæ•´ä¸ªè¡Œä¸šä¼šé™·å…¥åœæ»

I donâ€™t believe weÂ _are_Â stuck. But we do face difficult, foundational problems.
>  ç¬”è€…ä¸è®¤ä¸ºæˆ‘ä»¬è¢«å›°ä½äº†ï¼Œä½†æˆ‘ä»¬ç¡®å®é¢ä¸´ä¸€äº›å›°éš¾è€ŒåŸºç¡€æ€§çš„é—®é¢˜

To move forward, we need to better understand the underlying industry dynamics. Compute is a deeply technical field, evolving rapidly, and filled with jargon, codenames, and press releases designed to make every new product sound revolutionary. Many people try to cut through the noise toÂ _see the_Â [_forest for the trees_](https://en.wiktionary.org/wiki/see_the_forest_for_the_trees), but to truly understand where weâ€™re going, we need to examine theÂ _roots_â€”the fundamental building blocks that hold everything together.
>  ä¸ºäº†ç†è§£æˆ‘ä»¬å‰è¿›çš„æ–¹å‘ï¼Œæˆ‘ä»¬éœ€è¦å®¡è§†æ ¹æº â€”â€” é‚£äº›æ”¯æ’‘ä¸€åˆ‡çš„åŸºæœ¬æ„å»ºæ¨¡å—

This post is the first in a multipart series where weâ€™ll help answer these critical questions in a straightforward, accessible way:

- ğŸ§ What exactly is CUDA?
- ğŸ¯ Why has CUDA been so successful?
- âš–ï¸ Is CUDA any good?
- â“ Why do other hardware makers struggle to provide comparable AI software?
- âš¡ Why havenâ€™t existing technologies like Triton or OneAPI or OpenCL solved this?
- ğŸš€ How can we as an industry move forward?

>  è¿™ä¸ªç³»åˆ—å°†å›ç­”ä»¥ä¸‹å…³é”®é—®é¢˜:
>  - CUDA æ˜¯ä»€ä¹ˆ
>  - ä¸ºä»€ä¹ˆ CUDA è¿™ä¹ˆæˆåŠŸ
>  - CUDA çœŸçš„å¥½å—
>  - ä¸ºä»€ä¹ˆå…¶ä»–ç¡¬ä»¶å‚å•†éš¾ä»¥æä¾›å¯æ¯”çš„ AI è½¯ä»¶
>  - ä¸ºä»€ä¹ˆç°æœ‰çš„æŠ€æœ¯å¦‚ Triton, OneAPI, OpenCL æœªèƒ½è§£å†³è¿™ä¸ªé—®é¢˜
>  - æˆ‘ä»¬æ•´ä¸ªè¡Œä¸šè¯¥å¦‚ä½•å‘å‰æ¨è¿›

I hope this series sparks meaningful discussions and raises the level of understanding around these complex issues. The rapid advancements in AI â€”like DeepSeekâ€™s recent breakthroughsâ€“remind us that software and algorithmic innovation are still driving forces. A deep understanding of low-level hardware continues to unlock "10x" breakthroughs.
>  AI çš„å¿«é€Ÿè¿›æ­¥ï¼Œæ¯”å¦‚ DeepSeek æœ€è¿‘çš„çªç ´ï¼Œæé†’æˆ‘ä»¬è½¯ä»¶å’Œç®—æ³•çš„åˆ›æ–°è®©ç„¶æ˜¯æ¨åŠ¨å‘å±•çš„æ ¸å¿ƒåŠ›é‡ï¼ŒåŒæ—¶å¯¹åº•å±‚ç¡¬ä»¶çš„æ·±å…¥ç†è§£ä»ç„¶èƒ½å¤Ÿå¸¦æ¥ â€œ10 å€â€ çº§åˆ«çš„çªç ´

AI is advancing at an unprecedented paceâ€“butÂ **thereâ€™s still so much left to unlock**. Together we can break it down, challenge assumptions, and push the industry forward.Â **Letâ€™s dive in!**

-Chris

# 2 What exactly is â€œCUDAâ€? 
Site: https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda
Date: 5 February 2025

It seems likeÂ **everyone**Â has started talking aboutÂ [CUDA](https://en.wikipedia.org/wiki/CUDA)Â in the last year: Itâ€™s theÂ **backbone of deep learning,**Â the reasonÂ **novel hardware struggles to compete,**Â and the core ofÂ **NVIDIAâ€™s moat**Â andÂ **soaring market cap.**Â With DeepSeek, we got a startling revelation: itsÂ **breakthrough was made possible by â€œbypassingâ€ CUDA**,Â [going directly to the PTX layer](https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead) â€¦ but what does this actually mean? It feels like everyone wants to break past the lock-in, but we have toÂ **understand what weâ€™re up against**Â before we can formulate a plan.
>  è¿‡å»ä¸€å¹´é‡Œï¼Œæ‰€æœ‰äººéƒ½å¼€å§‹è®¨è®º CUDA: å®ƒæ˜¯ DL çš„åŸºçŸ³ï¼Œæ˜¯æ–°ç¡¬ä»¶éš¾ä»¥ç«äº‰çš„åŸå› ï¼Œä¹Ÿæ˜¯ NVIDIA æŠ¤åŸæ²³å’Œå¸‚å€¼é£™å‡çš„æ ¸å¿ƒ
>  DeepSeek çš„å¯ç¤ºæ˜¯: å®ƒçš„çªç ´æ˜¯é€šè¿‡ â€œç»•è¿‡â€ CUDA å®ç°çš„ï¼Œç›´æ¥è¿›å…¥ PTX å±‚
>  ä½†è¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿåœ¨åˆ¶å®šè®¡åˆ’ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æˆ‘ä»¬é¢å¯¹çš„æ˜¯ä»€ä¹ˆ

CUDAâ€™s dominance in AI is undeniableâ€”butÂ **most people donâ€™t fully understand what CUDA actually is**. Some think itâ€™s a programming language. Others call it a framework. Many assume itâ€™s justÂ **â€œthat thing NVIDIA uses to make GPUs faster.â€**Â None of these are entirely wrongâ€”and manyÂ [brilliant people are trying to explain this](https://x.com/IanCutress/status/1884374138787357068) â€”but none capture theÂ **full scope of â€œThe CUDA Platform.â€**
>  CUDA åœ¨ AI é¢†åŸŸçš„ä¸»å¯¼åœ°ä½æ— å¯äº‰è®®ï¼Œä½†å¤§å¤šæ•°äººå¹¶ä¸å®Œå…¨ç†è§£ CUDA åˆ°åº•æ˜¯ä»€ä¹ˆ

CUDA is not just one thing. Itâ€™s aÂ **huge, layered Platform**â€”a collection of technologies, software libraries, and low-level optimizations that together form aÂ **massive parallel computing ecosystem**. It includes:

- **A low-level parallel programming model**Â that allows developers to harness the raw power of GPUs with a C++-like syntax.
- **A complex set of libraries and frameworks**â€”middleware that powers crucial vertical use cases like AI (e.g., cuDNN for PyTorch and TensorFlow).
- **A suite of high-level solutions**Â like TensorRT-LLM and Triton, which enable AI workloads (e.g., LLM serving) without requiring deep CUDA expertise.

â€¦and thatâ€™s just scratching the surface.

>  CUDA ä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œè€Œæ˜¯ä¸€ä¸ªåºå¤§ä¸”åˆ†å±‚çš„å¹³å° â€”â€” ç”±ä¸€ç³»åˆ—æŠ€æœ¯ã€è½¯ä»¶åº“å’Œåº•å±‚ä¼˜åŒ–ç»„æˆï¼Œå…±åŒæ„æˆäº†ä¸€ä¸ªåºå¤§çš„å¹¶è¡Œè®¡ç®—ç”Ÿæ€ç³»ç»Ÿï¼Œå®ƒåŒ…æ‹¬:
>  - ä¸€ç§åº•å±‚å¹¶è¡Œç¼–ç¨‹æ¨¡å‹ï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨ç±»ä¼¼ C++ çš„è¯­æ³•ä¸º GPU ç¼–å†™ kernel
>  - ä¸€å¥—å¤æ‚çš„åº“å’Œæ¡†æ¶ â€”â€” å³ä¸­é—´ä»¶ï¼Œç”¨äºæ”¯æŒå…³é”®çš„å‚ç›´åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ç”¨äº PyTorch, TensorFlow çš„ cuDNN
>  - ä¸€ç³»åˆ—é«˜çº§è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ TensorRT-LLM å’Œ Tritonï¼Œä½¿å¾— AI workload å¯ä»¥åœ¨ä¸éœ€è¦æ·±å…¥ CUDA çš„æƒ…å†µä¸‹å®ç°
>  è¿™äº›ä»…ä»…æ˜¯å†°å±±ä¸€è§’

>  ä¸ªäººæŠŠ CUDA çœ‹ä½œä¸€ä¸ªå¯¹ NVIDIA GPU çš„ç¼–ç¨‹æ¥å£ä»¥åŠæ”¯æŒè¿™ä¸ªæ¥å£çš„ä¸€æ•´å¥—å·¥å…·é“¾ï¼Œå¤§è‡´åŒ…æ‹¬:
>  - ç¼–è¯‘å™¨ NVCC å’Œ IR PTX
>  - åº“ cuDNN, cuBLAS, cuFFT
>  - è°ƒè¯•å™¨ cuda-GDB å’Œæ€§èƒ½åˆ†æå™¨ NSight
>  è¿™ä¸ªç¼–ç¨‹æ¥å£å¯¹ç¨‹åºå‘˜æä¾›çš„ç¼–ç¨‹æŠ½è±¡æ˜¯ SIMTï¼Œè¿™ä¸ªç»Ÿä¸€çš„ç¼–ç¨‹æ¨¡å‹å°† GPU å†…éƒ¨çš„ä¸Šåƒä¸ªæ ¸å¿ƒæ•´åˆèµ·æ¥ï¼Œåœ¨ GPU æ¶æ„ä¸Šè¿›è¡Œè®¡ç®—ä»»åŠ¡ï¼Œå› æ­¤ CUDA å«åšè®¡ç®—ç»Ÿä¸€çš„è®¾å¤‡æ¶æ„ (è™½ç„¶å®é™…ä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªè®¾å¤‡æ¶æ„ï¼Œæ˜¯åˆ©ç”¨è®¾å¤‡æ¶æ„çš„å·¥å…·)
>  å½“ç„¶å®é™…ä¸ŠåŸºäº CUDA ä¹‹ä¸Šçš„ä¸œè¥¿ä¹Ÿå±äº CUDA ç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†

In this article, weâ€™ll break down theÂ **key layers of the CUDA Platform**, explore itsÂ **historical evolution**, and explainÂ **why itâ€™s so integral to AI computing today**. This sets the stage for the next part in our series, where weâ€™ll dive intoÂ **why CUDA has been so successful.**Â Hint: it has a lot more to do with market incentives than it does the technology itself.
>  CUDA çš„æˆåŠŸæ›´å¤šå’Œå¸‚åœºæ¿€åŠ±æœ‰å…³ï¼Œè€Œä¸ä»…ä»…æ˜¯æŠ€æœ¯æœ¬èº«

Letâ€™s dive in. ğŸš€

## The Road to CUDA: From Graphics to General-Purpose Compute
Before GPUs became the powerhouses of AI and scientific computing, they wereÂ **graphics processorsâ€”specialized processors for rendering images**. Early GPUsÂ **hardwired**Â image rendering into silicon, meaning that every step of rendering (transformations, lighting, rasterization) was fixed. While efficient for graphics, these chips wereÂ **inflexible**â€”they couldnâ€™t be repurposed for other types of computation.
>  AI ä¹‹å‰ï¼ŒGPU æ˜¯ä¸“ç”¨äºå›¾åƒæ¸²æŸ“çš„å¤„ç†å™¨
>  æ—©æœŸ GPU å°†å›¾åƒæ¸²æŸ“ç¡¬ç¼–ç åˆ°ç¡…èŠ¯ç‰‡ä¸­ï¼Œè¿™æ„å‘³ç€æ¸²æŸ“çš„æ¯ä¸€æ­¥ (å˜æ¢ã€å…‰ç…§ã€å…‰æ …åŒ–) éƒ½æ˜¯å›ºå®šçš„
>  è¿™æ ·çš„å›¾ç‰‡å¤„ç†æ•ˆç‡é«˜ï¼Œä½†èŠ¯ç‰‡ç¼ºä¹çµæ´»æ€§

Everything changed inÂ **2001**Â when NVIDIA introduced theÂ **GeForce3**, the first GPU withÂ **programmable shaders**. This was aÂ **seismic shift**Â in computing:

- ğŸ¨Â **Before:**Â Fixed-function GPUs could only apply pre-defined effects.
- ğŸ–¥ï¸Â **After:**Â Developers couldÂ **write their own shader programs**, unlockingÂ **programmable graphics pipelines**.

This advancement came withÂ **Shader Model 1.0**, allowing developers to writeÂ **small, GPU-executed programs**Â for vertex and pixel processing. NVIDIA sawÂ **where the future was heading:**Â instead of just improving graphics performance, GPUs could becomeÂ **programmable parallel compute engines**.

>  2001 å¹´ NVIDIA æ¨å‡ºäº†é¦–æ¬¾å…·æœ‰å¯ç¼–ç¨‹ç€è‰²å™¨çš„ GPU GeForece3ï¼Œè¿™æ˜¯è®¡ç®—é¢†åŸŸçš„é‡å¤§å˜é©:
>  - ä¹‹å‰: å›ºå®šåŠŸèƒ½çš„ GPU åªèƒ½åº”ç”¨é¢„å®šä¹‰çš„æ•ˆæœ
>  - ä¹‹å: å¼€å‘è€…å¯ä»¥ç¼–å†™è‡ªå·±çš„ç€è‰²å™¨ç¨‹åº
>  è¿™ä¸€è¿›æ­¥å¸¦æ¥äº† Shader Model 1.0ï¼Œå…è®¸å¼€å‘è€…ç¼–å†™å°å‹çš„ã€å¯ä»¥åœ¨ GPU ä¸Šæ‰§è¡Œçš„ç¨‹åºï¼Œç”¨äºé¡¶ç‚¹å’Œåƒç´ å¤„ç†
>  NVIDIA çœ‹åˆ°äº†æœªæ¥çš„å‘å±•æ–¹å‘: é™¤äº†æå‡å›¾å½¢æ€§èƒ½ä¹‹å¤–ï¼ŒGPU å¯ä»¥æˆä¸º**å¯ç¼–ç¨‹çš„å¹¶è¡Œè®¡ç®—å¼•æ“**

At the same time, it didnâ€™t take long for researchers to ask:

> â€œğŸ¤”Â _If GPUs can run small programs for graphics, could we use them for non-graphics tasks?â€_

One of the first serious attempts at this was theÂ [**BrookGPU project**](http://graphics.stanford.edu/projects/brookgpu/)Â at Stanford. Brook introduced a programming model that letÂ **CPUs offload compute tasks to the GPU**â€”a key idea thatÂ [set the stage for CUDA](https://www.nvidia.com/content/GTC/documents/1001_GTC09.pdf).

This move wasÂ **strategic and transformative**. Instead of treating compute as aÂ **side experiment**, NVIDIAÂ **made it a first-class priority**, embedding CUDA deeply intoÂ **its hardware, software, and developer ecosystem**.

>  äºæ­¤åŒæ—¶ï¼Œç ”ç©¶äººå‘˜å¾ˆå¿«å¼€å§‹å¥½å¥‡ GPU æ˜¯å¦å¯ä»¥ç”¨äºéå›¾å½¢ä»»åŠ¡
>  å¯¹è¿™ä¸€é—®é¢˜æœ€æ—©çš„å°è¯•æ˜¯ BrookGPU é¡¹ç›®ï¼ŒBrook å¼•å…¥äº†ä¸€ä¸ªç¼–ç¨‹æ¨¡å‹ï¼Œä½¿å¾— **CPU èƒ½å¤Ÿå°†è®¡ç®—ä»»åŠ¡å¸è½½åˆ° GPU** â€”â€” è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„æ¦‚å¿µï¼Œä¸º CUDA çš„å‡ºç°å¥ å®šäº†åŸºç¡€

## The CUDA Parallel Programming Model
InÂ **2006**, NVIDIA launchedÂ **CUDA (â€Compute Unified Device Architectureâ€)**â€”the firstÂ **general-purpose programming platform for GPUs**. The CUDA programming model is made up of two different things: the â€œCUDA programming languageâ€, and the â€œNVIDIA Driverâ€.
>  NVIDIA åœ¨ 2006 å¹´å‘å¸ƒ CUDA â€”â€” ç¬¬ä¸€ä¸ªé¢å‘ GPU çš„é€šç”¨ç›®çš„ç¼–ç¨‹å¹³å°
>  CUDA ç¼–ç¨‹æ¨¡å‹åŒ…å«ä¸¤ä¸ªä¸œè¥¿: CUDA è¯­è¨€ã€NVIDIA é©±åŠ¨å™¨

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3a98e7decaced44f9e7fd_DC-D01.png)

CUDA is a Layered Stack Requiring Deep Integration from Driver to Kernel

The CUDA language is derived from C++, with enhancements to directly expose low-level features of the GPUâ€”e.g. its ideas of â€œGPU threadsâ€ and memory. A programmer can use this language to define a â€œCUDA Kernelâ€â€”an independent calculation that runs on the GPU. A very simple example is:
>  CUDA è¯­è¨€åŸºäº C++ï¼Œæ·»åŠ äº†èƒ½å¤Ÿæš´éœ² GPU ä½çº§ç‰¹æ€§çš„è¯­æ³• â€”â€” ä¾‹å¦‚ GPU çº¿ç¨‹çš„æ€æƒ³ã€GPU memory çš„æ€æƒ³
>  ç¨‹åºå‘˜ç”¨ CUDA è¯­è¨€å®šä¹‰ CUDA kernel â€”â€” åœ¨ GPU ä¸Šè¿è¡Œçš„ç‹¬ç«‹è®¡ç®—å•å…ƒ

```cpp
__global__ void addVectors(float *a, float *b, float *c, int n) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
```

CUDA kernels allow programmers to define a custom computation that accesses local resources (like memory) and using the GPUs as very fast parallel compute units. This language is translated (â€compiledâ€) down to â€œPTXâ€, which is an assembly language that is the lowest level supported interface to NVIDIA GPUs.
>  CUDA è¯­è¨€ä¸ºå°† GPU ä½œä¸ºéå¸¸å¿«é€Ÿçš„å¹¶è¡Œè®¡ç®—å•å…ƒæä¾›äº†æ¥å£
>  CUDA ç¨‹åºä¼šè¢«ç¼–è¯‘ä¸º PTXï¼Œæ˜¯ NVIDIA GPU æ”¯æŒçš„æœ€ä½çº§åˆ«æ¥å£ (SASS å°±ä¸ç®—æ¥å£äº†ï¼Œå› ä¸ºå®ƒä¸å¯¹å¤–æš´éœ²)

But how does a programÂ **actually execute code on a GPU?**Â Thatâ€™s where theÂ **NVIDIA Driver**Â comes in. It acts as theÂ **bridge**Â between the CPU and the GPU, handling memory allocation, data transfers, and kernel execution. A simple example is:
>  ç¼–è¯‘å¥½çš„ç¨‹åºå¦‚ä½•åœ¨ GPU ä¸Šæ‰§è¡Œå‘¢ï¼Ÿ
>  è¿™æ˜¯ NVIDIA é©±åŠ¨å™¨çš„ä»»åŠ¡ï¼Œå®ƒæ˜¯ CPU å’Œ GPU ä¹‹é—´çš„æ¡¥æ¢ï¼Œå¤„ç†å†…å­˜åˆ†é…ã€æ•°æ®ä¼ è¾“ã€kernel æ‰§è¡Œ (CPU é€šè¿‡ NVIDIA é©±åŠ¨å’Œ GPU äº¤äº’)

>  ä¹Ÿå°±æ˜¯è¯´ï¼Œé€šå¸¸æˆ‘ä»¬ç”¨çš„ CUDA API ä¾‹å¦‚ `cudaMalloc, cudaMemcpy` éƒ½å±äº CUDA Runtime APIï¼Œå®ƒå®é™…ä¸Šæ˜¯ CUDA Driver API ä¸Šçš„ä¸€å±‚å°è£…
>  é©±åŠ¨ç¨‹åºæä¾›çš„å°±æ˜¯ CUDA Driver APIï¼Œé©±åŠ¨æ‰æ˜¯çœŸæ­£ç›´æ¥ä¸ GPU ç¡¬ä»¶é€šä¿¡çš„ä¸»ä½“
>  å½“ç„¶å¿…è¦æƒ…å†µä¸‹ï¼Œç¨‹åºå‘˜æ˜¯å¯ä»¥ç›´æ¥è°ƒç”¨ CUDA Driver API çš„

>  æ— è®ºæ˜¯ CUDA Runtime API è¿˜æ˜¯ CUDA Driver APIï¼Œéƒ½å±äºæºç å±‚æ¬¡ï¼Œæœ€ç»ˆéƒ½ä¼šè¢«ç¼–è¯‘
>  CUDA ä»£ç è¢«ç¼–è¯‘åï¼Œä¼šå…ˆç”Ÿæˆ PTX IRï¼Œæ³¨æ„ PTX æ˜¯ä¸€ä¸ªç‹¬ç«‹äº GPU ç¡¬ä»¶æ¶æ„çš„è¯­è¨€ï¼Œå› æ­¤ CUDA -> PTX åªæ˜¯ NVCC å‰ç«¯ç¼–è¯‘çš„æµç¨‹
>  é©±åŠ¨ç¨‹åºæ¥æ”¶ PTX IRï¼Œå¹¶ä½¿ç”¨å†…ç½®çš„å³æ—¶ç¼–è¯‘å™¨ï¼Œå°† PTX IR è¿›ä¸€æ­¥ç¼–è¯‘ä¸ºç‰¹å®šäº GPU æ¶æ„çš„ SASS æœºå™¨ç 
>  GPU ç¡¬ä»¶æ¥æ”¶åˆ°é©±åŠ¨ç¼–å¥½çš„ SASSï¼Œè¿›è€Œæ‰§è¡Œ

```mermaid
flowchart LR
  A[CUDA Source Program]-- PTX -->B[NVIDIA DRIVER]-- SASS -->C[NVIDIA GPU]
```

```cpp
cudaMalloc(&d_A, size);
cudaMalloc(&d_B, size);
cudaMalloc(&d_C, size);

cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);

int threadsPerBlock = 256;
// Compute the ceiling of N / threadsPerBlock
int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
addVectors<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);

cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
```

Note that all of this is very low levelâ€”full of fiddly details (e.g. pointers and â€œmagic numbersâ€). If you get something wrong, youâ€™re most often informed of this by a difficult to understand crash. Furthermore, CUDA exposes a lot of details that are specific to NVIDIA hardwareâ€”things like the â€œnumber of threads in a warpâ€ (which we won't explore here).

Despite the challenges, these components enabled an entire generation of hardcore programmers to get access to the huge muscle that a GPU can apply to numeric problems. For example, theÂ [**AlexNET**](https://en.wikipedia.org/wiki/AlexNet)Â **ignited modern deep learning in 2012**. It was made possible by custom CUDA kernels for AI operations like convolution, activations, pooling and normalization and the horsepower a GPU can provide.
>  è¿™äº›ç»„ä»¶ä½¿å¾—ç¨‹åºå‘˜å¯ä»¥åˆ©ç”¨ GPU çš„ç®—åŠ›ï¼ŒAlexNet ç‚¹ç‡ƒäº†ç°ä»£æ·±åº¦å­¦ä¹ ï¼Œå®ƒå¾—ç›Šäºä¸º AI è¿ç®—ä¾‹å¦‚å·ç§¯ã€æ± åŒ–ã€è§„èŒƒåŒ–å®šåˆ¶çš„ CUDA kernel ä»¥åŠ GPU æä¾›çš„ç®—åŠ›

While the CUDA language and driver areÂ **what most people typically think of**Â when they hear â€œCUDA,â€ this is far from the whole enchiladaâ€”itâ€™s just theÂ **filling inside**. Over time,Â **the CUDA Platform**Â grew to include much more, and as it did, the meaning of the original acronym fell away from being a useful way to describe CUDA.
>  è™½ç„¶ CUDA è¯­è¨€å’Œé©±åŠ¨ç¨‹åºæ˜¯å¤§å¤šæ•°äººå¬åˆ° â€œCUDAâ€ æ—¶æƒ³åˆ°çš„å†…å®¹ï¼Œä½†è¿™è¿œä¸æ˜¯å…¨éƒ¨
>  éšç€å…¶å‘å±•ï¼ŒCUDA å¹³å°é€æ¸åŒ…å«äº†æ›´å¤šå†…å®¹ï¼Œä¸”åŸå§‹ç¼©å†™ CUDA ä¹Ÿä¸èƒ½å‡†ç¡®æè¿°æ•´ä¸ª CUDA

## High-Level CUDA Libraries: Making GPU Programming More Accessible
The CUDA programming model opened the door toÂ **general-purpose GPU computing and is powerful**, but it brings two challenges:

1. CUDA isÂ **difficult to use**, and even worse...
2. CUDA doesnâ€™t help withÂ **performance portability**

Most kernels written for generation N will â€œkeep workingâ€ on generation N+1, but often the performance is quite badâ€”far from the peak of what N+1 generation can deliver, even though GPUs are all about performance. This makes CUDA aÂ **strong tool for expert engineers**, but aÂ **steep learning curve for most developers.**Â But is also means that significant rewrites are required every time a new generation of GPU comes out (e.g. Blackwell is now emerging).

>  CUDA ç¼–ç¨‹æ¨¡å‹æ‰“å¼€äº†é€šç”¨ç›®çš„ GPU è®¡ç®—çš„å¤§é—¨ï¼Œä½†ä¹Ÿå¸¦æ¥äº†ä¸¤ä¸ªæŒ‘æˆ˜:
>  1. CUDA éš¾å†™
>  2. CUDA ä¸èƒ½å¸®åŠ©å®ç°æ€§èƒ½çš„å¯ç§»æ¤æ€§
>  å¤§å¤šæ•°ä¸ºç¬¬ N ä»£ç¼–å†™çš„ kernel åœ¨ N+1 ä»£ GPU ä¸Šä»ç„¶å¯ä»¥è¿è¡Œï¼Œä½†æ€§èƒ½å¾€å¾€è¿œä½äº N+1 ä»£çš„å³°å€¼æ€§èƒ½ï¼Œè¿™æ„å‘³ç€ï¼Œæ¯å½“æ–°ä¸€ä»£ GPU å‡ºç°æ—¶ï¼Œéƒ½éœ€è¦è¿›è¡Œå¤§é‡çš„é‡å†™å·¥ä½œ

As NVIDIA grew it wanted GPUs to be useful to people who were domain experts in their own problem spaces, but werenâ€™t themselves GPU experts. NVIDIAâ€™s solution to this problem was to start building rich and complicatedÂ **closed-source, high-level libraries**Â that abstract away low-level CUDA details. These include:

- **cuDNN**Â (2014) â€“ Accelerates deep learning (e.g., convolutions, activation functions).
- **cuBLAS**Â â€“ Optimized linear algebra routines.
- **cuFFT**Â â€“ Fast Fourier Transforms (FFT) on GPUs.
- â€¦ andÂ [many others](https://developer.nvidia.com/gpu-accelerated-libraries).

>  ä¸ºäº†è®©éä¸“å®¶ä¹Ÿèƒ½ä½¿ç”¨ CUDA, NVIDIA æ„å»ºäº†ä¸°å¯Œä¸”å¤æ‚çš„é—­æºé«˜çº§åº“ï¼ŒæŠ½è±¡äº†åº•å±‚ CUDA çš„ç»†èŠ‚ï¼Œå…¶ä¸­åŒ…æ‹¬:
>  - cuDNN â€”â€” åŠ é€Ÿæ·±åº¦å­¦ä¹ ï¼Œä¾‹å¦‚å·ç§¯ï¼Œæ¿€æ´»å‡½æ•°
>  - cuBLAS â€”â€” åŠ é€Ÿçº¿æ€§ä»£æ•°è¿ç®—
>  - cuFFTâ€”â€” GPU ä¸Šè¿›è¡Œ FFT

With these libraries, developers couldÂ **tap into CUDAâ€™s power without needing to write custom GPU code**, with NVIDIA taking on the burden ofÂ **rewriting these for every generation of hardware**. This was a big investment from NVIDIA,Â **but it worked**.
>  NVIDIA æ‰¿æ‹…äº†ä¸ºæ¯ä¸€ä»£ç¡¬ä»¶é‡å†™è¿™äº›åº“çš„è´Ÿæ‹…ï¼Œä¸Šå±‚çš„å¼€å‘è€…åˆ™æ— éœ€ç¼–å†™è‡ªå®šä¹‰çš„ GPU ä»£ç å°±èƒ½åˆ©ç”¨ CUDA
>  è¿™å¯¹ NVIDIA æ˜¯ä¸€é¡¹å·¨å¤§çš„æŠ•èµ„ï¼Œä½†æ•ˆæœå¾ˆå¥½

TheÂ **cuDNN library**Â is especially important in this storyâ€”it paved the way for Googleâ€™sÂ **TensorFlow**Â (2015) and Metaâ€™sÂ **PyTorch**Â (2016), enabling deep learning frameworks to take off. While there were earlier AI frameworks, these were the first frameworks to truly scaleâ€”modern AI frameworks now haveÂ **_thousands_**Â of these CUDA kernels and each is very difficult to write. As AI research exploded, NVIDIA aggressively pushed to expand these libraries to cover the important new use-cases.
>  cuDNN åº“åœ¨è¿™ä¸ªæ•…äº‹ä¸­å°¤ä¸ºé‡è¦ï¼Œå®ƒä¸º Google çš„ TensorFlow å’Œ Meta çš„ PyTorch é“ºå¹³äº†é“è·¯ï¼Œä½¿å¾— DL æ¡†æ¶å¯ä»¥è¿…é€Ÿå‘å±•
>  å¦‚ä»Šçš„ DL æ¡†æ¶ä¸­æœ‰æ•°åƒä¸ªè¿™æ ·çš„ CUDA kernel, NVIDIA ä¹Ÿç§¯ææ¨åŠ¨è¿™äº›åº“ï¼Œä»¥è¦†ç›–è¶Šæ¥è¶Šå¤šçš„åœºæ™¯

![Image depicting a layered stack with AI Model Developers at the top, represented by a laptop icon with a sparkle. Below is a cloud labeled PyTorch Ecosystem, resting above a red block labeled PyTorch. Underneath are three more layers: a green block for CUDA Libraries, another green block for CUDA Language, and a blue block at the bottom labeled NVIDIA Driver. The structure highlights the deep dependency chain required to support PyTorch within the CUDA framework.](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a37749c851d98f258c8673_DC-D03.png)

PyTorch on CUDA is Built on Multiple Layers of Dependencies

NVIDIAâ€™s investment into theseÂ **powerful GPU libraries**Â enabled the world to focus on building high-level AI frameworks like PyTorch and developer ecosystems like HuggingFace. Their next step was to make entireÂ **solutions**Â that could be usedÂ **out of the box**â€”without needing to understand the CUDA programming model at all.
>  NVIDIA å¯¹è¿™äº› GPU åº“çš„æŠ•èµ„ä½¿å¾—å…¨çƒå¼€å‘è€…å¯ä»¥ä¸“æ³¨äºæ„å»ºåƒ PyTorch è¿™æ ·çš„é«˜çº§æ¡†æ¶ï¼Œä»¥åŠ HuggingFace è¿™æ ·çš„å¼€å‘ç”Ÿæ€
>  å®ƒä»¬çš„ä¸‹ä¸€æ­¥æ˜¯æ‰“é€ å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆå¼€ç®±å³ç”¨ â€”â€” å®Œå…¨ä¸éœ€è¦ç†è§£ CUDA ç¼–ç¨‹æ¨¡å‹

## Fully vertical solutions to ease the rapid growth of AI and GenAI
The AI boom went far beyond research labsâ€”**AI is now everywhere**. FromÂ **image generation**Â toÂ **chatbots**, fromÂ **scientific discovery**Â toÂ **code assistants**,Â **Generative AI (GenAI) has exploded across industries**, bringing a flood of new applications and developers into the field.

At the same time,Â **a new wave of AI developers emerged, with very different needs.**Â In the early days, deep learning requiredÂ **highly specialized engineers**Â who understood CUDA, HPC, and low-level GPU programming. Now, a new breed of developerâ€”often calledÂ **AI engineers**â€”is building and deploying AI models without needing to touch low-level GPU code.
>  å¦‚ä»Šçš„ AI å·¥ç¨‹å¸ˆåœ¨æ„å»ºå’Œéƒ¨ç½² AI æ¨¡å‹æ—¶ï¼Œä¸éœ€è¦æ¥è§¦åº•å±‚çš„ GPU ä»£ç 

To meet this demand, NVIDIA went beyond just providing librariesâ€”it now offersÂ **turnkey solutions**Â that abstract awayÂ **everything**Â under the hood. Instead of requiringÂ **deep CUDA expertise**, these frameworks allow AI developers toÂ **optimize and deploy models with minimal effort**.

- **Triton Serving**Â â€“ A high-performance serving system for AI models, allowing teams to efficiently run inference across multiple GPUs and CPUs.
- **TensorRT**Â â€“ A deep learning inference optimizer thatÂ **automatically tunes models**Â to run efficiently on NVIDIA hardware.
- **TensorRT-LLM**Â â€“ An even more specialized solution, built forÂ **large language model (LLM) inference at scale**.
- â€¦ plus many (many) other things.

>  è¿™æ˜¯å› ä¸ºï¼ŒNVIDIA ä¸å†ä»…ä»…æä¾›åº“å·¥å…· â€”â€” å®ƒç°åœ¨æä¾›ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œå°†æ‰€æœ‰åº•å±‚ç»†èŠ‚éƒ½æŠ½è±¡æ‰äº†ï¼Œå¼€å‘è€…ä¸å†éœ€è¦ CUDA çŸ¥è¯†ï¼ŒAI å¼€å‘è€…å¯ä»¥ä»¥æœ€å°çš„åŠªåŠ›ä¼˜åŒ–å’Œéƒ¨ç½²æ¨¡å‹
>  - Triton Serving â€”â€” ä¸€ä¸ªé«˜æ€§èƒ½çš„ AI æ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œå…è®¸å›¢é˜Ÿåœ¨å¤šä¸ª GPUs ä¸Šå’Œ CPUs ä¸Šé«˜æ•ˆåœ°è¿è¡Œæ¨ç†
>  - TensorRT â€”â€” æ·±åº¦å­¦ä¹ æ¨ç†ä¼˜åŒ–å™¨ï¼Œè‡ªåŠ¨è°ƒæ•´æ¨¡å‹ä½¿å¾—èƒ½åœ¨ NVIDIA ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œ
>  - TensorRT-LLM â€”â€” ä¸“é—¨ä¸º LLM å¤§è§„æ¨¡æ¨ç†è€Œè®¾è®¡çš„ä¼˜åŒ–å™¨

![Image showing a vertical stack with AI Engineers at the top, represented by a laptop icon with a sparkle. Below are four layers: a green block labeled TensorRT-LLM, followed by CUDA Libraries, then CUDA Language, and finally a blue block at the bottom labeled NVIDIA Driver. The layered structure highlights the multiple dependencies required for AI development within the CUDA ecosystem.](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a376784d69c7a77a206398_DC-D02.png)

Several Layers Exist Between NVIDIA Drivers and TensorRT-LLM

These toolsÂ **completely shield AI engineers from CUDAâ€™s low-level complexity**, letting themÂ **focus on AI models and applications, not hardware details**. These systems provide significant leverage which has enabled the horizontal scale of AI applications.
>  è¿™äº›å·¥å…·å®Œå…¨å±è”½äº† AI å·¥ç¨‹å¸ˆå¯¹ CUDA åº•å±‚å¤æ‚æ€§çš„æ¥è§¦ï¼Œä½¿å¾—ä»–ä»¬å¯ä»¥ä¸“æ³¨äº AI æ¨¡å‹å’Œåº”ç”¨ï¼Œè€Œä¸æ˜¯ç¡¬ä»¶ç»†èŠ‚

## The â€œCUDA Platformâ€ as a whole
CUDA is often thought of as aÂ **programming model**, aÂ **set of libraries**, or even justÂ **"that thing NVIDIA GPUs run AI on."**Â But in reality,Â **CUDA is much more than that**â€”it is aÂ **unifying brand, a truly vast collection of software, and a highly tuned ecosystem**, all deeply integrated with NVIDIAâ€™s hardware. For this reason, the term â€œCUDAâ€ is ambiguousâ€”we prefer the term â€œThe CUDA Platformâ€ to clarify that weâ€™re talking about something closer in spirit to the Java ecosystem, or even an operating system, than merely a programming language and runtime library.
>  CUDA é€šå¸¸è¢«çœ‹ä½œä¸€ä¸ªç¼–ç¨‹æ¨¡å‹ï¼Œä¸€ç»„åº“
>  äº‹å®ä¸Šï¼ŒCUDA æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å“ç‰Œï¼Œä¸€ä¸ªåºå¤§çš„è½¯ä»¶é›†åˆå’Œä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä¸ NVIDIA çš„ç¡¬ä»¶æ·±åº¦é›†æˆ
>  å› æ­¤ â€œCUDA å¹³å°â€ æ›´åŠ å‡†ç¡®ï¼Œå› ä¸ºæˆ‘ä»¬è®¨è®ºçš„ä¸ä»…ä»…æ˜¯ç¼–ç¨‹æ¨¡å‹å’Œè¿è¡Œæ—¶åº“

![Image showing a layered stack of the CUDA ecosystem. At the top are icons for AI GPU Kernel Developers, AI Model Developers, and AI Engineers, with clouds for CUDA Kernels and PyTorch Ecosystem. Below are PyTorch, TensorRT-LLM, CUDA Libraries, CUDA Language, and the foundational NVIDIA Driver, highlighting CUDAâ€™s complex dependencies.](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3781e1402741652bac713_DC-D04.png)

CUDAâ€™s Expanding Complexity: A Multi-Layered Ecosystem Spanning Drivers, Languages, Libraries, and Frameworks

At its core, the CUDA Platform consists of:

- **A massive codebase**Â â€“ Decades of optimized GPU software, spanning everything from matrix operations to AI inference.
- **A vast ecosystem of tools & libraries**Â â€“ FromÂ **cuDNN for deep learning**Â toÂ **TensorRT for inference**, CUDA covers anÂ **enormous range of workloads**.
- **Hardware-tuned performance**Â â€“ Every CUDA release is deeply optimized forÂ **NVIDIAâ€™s latest GPU architectures**, ensuring top-tier efficiency.
- **Proprietary and opaque**Â â€“ When developers interact with CUDAâ€™sÂ **library APIs**, much of what happens under the hood isÂ **closed-source and deeply tied to NVIDIAâ€™s ecosystem**.

>  æœ¬è´¨ä¸Šï¼ŒCUDA å¹³å°åŒ…å«äº†:
>  - åºå¤§çš„ä»£ç åº“ â€”â€” æ•°åå¹´æ¥é’ˆå¯¹ GPU çš„ä¼˜åŒ–è½¯ä»¶ï¼Œæ¶µç›–äº†ä»çŸ©é˜µè¿ç®—åˆ° AI æ¨ç†çš„å„ç§ä»»åŠ¡
>  - ä¸°å¯Œçš„å·¥å…·å’Œåº“ç”Ÿæ€ç³»ç»Ÿ â€”â€” ä»ç”¨äº DL çš„ cuDNN åˆ°ç”¨äºæ¨ç†çš„ TensorRT, CUDA è¦†ç›–äº†å¹¿æ³›çš„ workloads
>  - é’ˆå¯¹ç¡¬ä»¶çš„é«˜æ€§èƒ½ä¼˜åŒ– â€”â€” æ¯æ¬¡ CUDA å‘å¸ƒéƒ½ä¼šæ·±åº¦ä¼˜åŒ–ä»¥é€‚é… NVIDIA çš„æœ€æ–° GPU æ¶æ„ï¼Œç¡®ä¿é¡¶çº§çš„æ•ˆç‡
>  - ä¸“æœ‰ä¸”ä¸é€æ˜ â€”â€” å¼€å‘è€…ä½¿ç”¨ CUDA çš„åº“ API æ—¶ï¼Œå¾ˆå¤šåº•å±‚æ“ä½œæ—¶é—­æºçš„ï¼Œå¹¶ä¸”ä¸ NVIDIA ç”Ÿæ€ç³»ç»Ÿæ·±åº¦ç»‘å®š

CUDA is a powerful but sprawling set of technologiesâ€”**an entire software platform that sits at the foundation of modern GPU computing**, even going beyond AI specifically.
>  CUDA æ˜¯ä¸€ä¸ªå®Œæ•´çš„è½¯ä»¶å¹³å°ï¼Œæ„æˆäº†ç°ä»£ GPU è®¡ç®—çš„åŸºç¡€

Now that we know what â€œCUDAâ€ is, we need to understand how it got to be so successful. Hereâ€™s a hint: CUDAâ€™s success isnâ€™t really aboutÂ **performance**â€”itâ€™s aboutÂ **strategy, ecosystem, and momentum**. In the next post, weâ€™ll explore what enabled NVIDIAâ€™s CUDA software to shape and entrench the modern AI era.
>  CUDA çš„æˆåŠŸå¹¶ä¸çœŸæ­£åœ¨äºæ€§èƒ½ï¼Œè€Œæ˜¯å…³äºç­–ç•¥ã€ç”Ÿæ€å’ŒåŠ¿å¤´

See you next time. ğŸš€

-Chris

# 3 How did CUDA succeed? 
Site: https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed
Date: 12 Feb 2025

If we as an ecosystem hope to make progress, we need to understand howÂ **the CUDA software empire**Â became so dominant. On paper, alternatives existâ€”AMDâ€™s ROCm, Intelâ€™s oneAPI, SYCL-based frameworksâ€”but in practice, CUDA remains theÂ **undisputed king of GPU compute**.
>  CUDA å­˜åœ¨è®¸å¤šæ›¿ä»£æ–¹æ¡ˆ: AMD ROCm, Intel oneAPI, SYCL-based æ¡†æ¶
>  ä½† CUDA ä»ç„¶æ˜¯ GPU è®¡ç®—é¢†åŸŸçš„ç‹è€…

**How did this happen?**

The answer isnâ€™t just aboutÂ **technical excellence**â€”though that plays a role. CUDA is a developer platform built throughÂ **brilliant execution, deep strategic investment, continuity, ecosystem lock-in,**Â and, of course, a littleÂ **bit of luck**.
>  CUDA æ˜¯é€šè¿‡å“è¶Šçš„æ‰§è¡ŒåŠ›ã€æ·±å…¥çš„æˆ˜ç•¥æŠ•èµ„ã€æŒç»­æ€§ã€ç”Ÿæ€ç³»ç»Ÿçš„ç»‘å®šè€Œæ„å»ºå‡ºçš„å¼€å‘è€…å¹³å°

This post breaks downÂ **why CUDA has been so successful**, exploring the layers of NVIDIAâ€™s strategyâ€”from its early bets on generalizing parallel compute to the tight coupling of AI frameworks likeÂ [PyTorch](https://pytorch.org/)Â andÂ [TensorFlow](http://tensorflow.org/). Ultimately, CUDAâ€™s dominance is not just a triumph of software but aÂ **masterclass in long-term platform thinking**.
>  æœ¬æ–‡æ¢è®¨ NVIDIA æˆ˜ç•¥çš„å„ä¸ªå±‚é¢ â€”â€” ä»æ—©æœŸå¯¹å¹¶è¡Œè®¡ç®—é€šç”¨åŒ–çš„æŠ¼æ³¨ï¼Œåˆ°ä¸ PyTorch, TensorFlow çš„ç´§å¯†é›†æˆ
>  CUDA çš„æˆåŠŸæ›´åƒæ˜¯é•¿æœŸå¹³å°æ€ç»´çš„å…¸èŒƒè¯¾

Letâ€™s dive in. ğŸš€

## The Early Growth of CUDA
A key challenge of building a compute platform is attracting developers to learn and invest in it, and it isÂ **hard to gain momentum**Â if you can only target niche hardware. InÂ [a great â€œAcquiredâ€ podcast](https://www.acquired.fm/episodes/jensen-huang), Jensen Huang shares that a key early NVIDIA strategy was to keep their GPUs compatible across generations. This enabled NVIDIA to leverage its install base of already widespreadÂ **gaming GPUs**, which were sold for running DirectX-based PC games. Furthermore, it enabled developers to learn CUDA on low-priced desktop PCs and scale into more powerful hardware that commanded high prices.
>  æ„å»ºè®¡ç®—å¹³å°çš„å…³é”®æŒ‘æˆ˜æ˜¯å¸å¼•å¼€å‘è€…å­¦ä¹ å¹¶æŠ•èµ„ï¼Œå¦‚æœåªèƒ½é’ˆå¯¹å°ä¼—ç¡¬ä»¶ï¼Œå¾ˆéš¾ç§¯ç´¯åŠ¿å¤´
>  NVIDIA æ—©æœŸçš„ä¸€ä¸ªå…³é”®ç­–ç•¥æ˜¯ä¿æŒ GPU åœ¨ä¸åŒä»£é™…ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œä½¿å¾— NVIDIA èƒ½å¤Ÿåˆ©ç”¨å®ƒå·²ç»å¹¿æ³›éƒ¨ç½²çš„æ¸¸æˆ GPU å¸‚åœº
>  æ¸¸æˆ GPU ä¸»è¦ç”¨äºè¿è¡ŒåŸºäº DirectX çš„ PC æ¸¸æˆ

![Chart depicting NVIDIA's earnings segmented by Auto, Data Center, Gaming, OEM & IP, and Professional Visualization](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e611ec063bd8978baf0_DCP3-Diagram01.png)

This might seem obvious now, but at the time it was a bold bet: instead of creating separate product lines optimized for different use-cases (laptops, desktops, IoT, datacenter, etc.), NVIDIA built aÂ **single contiguous GPU product line.**Â This meant accepting trade-offsâ€”such as power or cost inefficienciesâ€”but in return, it created aÂ **unified ecosystem**Â where every developerâ€™s investment in CUDA could scale seamlessly from gaming GPUs to high-performance datacenter accelerators. This strategy is quite analogous to how Apple maintains and drives its iPhone product line forward.
>  åœ¨å½“æ—¶çœ‹æ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªèµŒæ³¨: NVIDIA æ²¡æœ‰ä¸ºä¸“é—¨çš„ä½¿ç”¨åœºæ™¯ (ä¾‹å¦‚ç¬”è®°æœ¬ã€å°å¼æœºã€ç‰©è”ç½‘ã€æ•°æ®ä¸­å¿ƒç­‰) åˆ›é€ æ–°çš„äº§å“çº¿ï¼Œè€Œæ˜¯æ‰“é€ äº†å•ç‹¬ä¸€æ¡è¿ç»­çš„ GPU èŒ¶å“çº¿
>  è¿™æ„å‘³ç€æ¥æ”¶ä¸€äº›æƒè¡¡ â€”â€” æ¯”å¦‚åŠŸè€—æˆ–æˆæœ¬ä¸Šçš„ä½æ•ˆç‡ â€”â€” ä½†å›æŠ¥æ˜¯æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä½¿å¾—æ¯ä¸ªå¼€å‘è€…å¯¹ CUDA çš„æŠ•å…¥éƒ½èƒ½æ— ç¼åœ°ä»æ¸¸æˆ GPU æ‰©å±•åˆ°é«˜æ€§èƒ½æ•°æ®ä¸­å¿ƒåŠ é€Ÿå™¨

The benefits of this approach were twofold:

1. **Lowering Barriers to Entry**Â â€“ Developers could learn CUDA using the GPUs they already had, making it easy to experiment and adopt.
2. **Creating a Network Effect**Â â€“ As more developers started using CUDA, more software and libraries were created, making the platform even more valuable.

>  è¿™ä¸ªæ–¹æ³•çš„å¥½å¤„æœ‰ä¸¤ä¸ª:
>  1. é™ä½å…¥é—¨é—¨æ§›: å¼€å‘è€…ä½¿ç”¨å·²æœ‰çš„ CUDA çŸ¥è¯†ï¼Œä¸éœ€è¦æ–°å­¦ä¹ 
>  2. å½¢æˆç½‘ç»œæ•ˆåº”: éšç€æ›´å¤šå¼€å‘è€…åŠ å…¥ï¼Œæ›´å¤šè½¯ä»¶å’Œåº“è¢«åˆ›é€ ï¼Œå¹³å°æ›´æœ‰ä»·å€¼

This early install base allowed CUDA to grow beyond gaming intoÂ **scientific computing, finance, AI, and high-performance computing (HPC)**. Once CUDA gained traction in these fields, its advantages over alternatives became clear:Â **NVIDIAâ€™s continued investment ensured that CUDA was always at the cutting edge of GPU performance**, while competitors struggled to build a comparable ecosystem.
>  ä¸€æ—¦ CUDA åœ¨å…¶ä»–é¢†åŸŸè·å¾—è®¤å¯ï¼Œå®ƒç›¸å¯¹äºå…¶ä»–æ–¹æ¡ˆçš„ä¼˜åŠ¿å°±æ˜¾è€Œæ˜“è§äº†: NVIDIA æŒç»­çš„æŠ•èµ„ç¡®ä¿äº† CUDA å§‹ç»ˆå¤„äº GPU æ€§èƒ½çš„æœ€å‰æ²¿ï¼Œè€Œç«äº‰å¯¹æ‰‹åˆ™éš¾ä»¥æ„å»ºç±»ä¼¼çš„ç”Ÿæ€ç³»ç»Ÿ

## Catching and Riding the Wave of AI Software
CUDAâ€™s dominance was cemented with theÂ **explosion of deep learning**. In 2012,Â [**AlexNet**](https://en.wikipedia.org/wiki/AlexNet), the neural network thatÂ **kickstarted the modern AI revolution**, was trained using two NVIDIA GeForce GTX 580 GPUs. This breakthrough not only demonstrated thatÂ **GPUs were faster at deep learning**â€”it proved they were essential for AI progress and led toÂ **CUDAâ€™s rapid adoption as the default compute backend**Â for deep learning.
>  AlexNet åœ¨ä¸¤å— NVIDIA GeForce GTX 580 ä¸Šè®­ç»ƒå¾—åˆ°ï¼Œè¿™è¯æ˜äº† GPU åœ¨æ·±åº¦å­¦ä¹ ä¸­è®­ç»ƒæ›´å¿«ï¼Œä½¿å¾— CUDA å¾ˆå¿«æˆä¸ºæ·±åº¦å­¦ä¹ çš„**é»˜è®¤åç«¯**

As deep learning frameworks emergedâ€”most notablyÂ **TensorFlow**Â (Google, 2015) andÂ **PyTorch**Â (Meta, 2016)â€”NVIDIAÂ **seized the opportunity**Â and invested heavily in optimizing itsÂ **High-Level CUDA Libraries**Â to ensure these frameworks ran as efficiently as possible on its hardware. Rather than leavingÂ **AI framework teams**Â to handleÂ **low-level CUDA performance tuning**Â themselves, NVIDIA took on the burden by aggressively refiningÂ **cuDNN**Â andÂ **TensorRT**Â as weÂ [discussed in Part 2](https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda).
>  éšç€æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å…´èµ·ï¼ŒNVIDIA æŠ•èµ„å¹¶æŠ“ä½äº†æœºä¼šï¼Œå¤§åŠ›ä¼˜åŒ–é«˜çº§ CUDA åº“ï¼Œç¡®ä¿è¿™äº›æ¡†æ¶å¯ä»¥å°½å¯èƒ½åœ¨å…¶ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œ
>  NVIDIA æ²¡æœ‰è®© AI æ¡†æ¶å›¢é˜Ÿè‡ªè¡Œå¤„ç†åº•å±‚ CUDA æ€§èƒ½è°ƒä¼˜ï¼Œè€Œæ˜¯**ä¸»åŠ¨æ‰¿æ‹…èµ·è¿™ä¸€è´£ä»»**ï¼Œç§¯ææ”¹è¿› cuDNN å’Œ TensorRT

This move not only madeÂ **PyTorch and TensorFlow significantly faster**Â on NVIDIA GPUsâ€”it also allowed NVIDIA toÂ **tightly integrate its hardware and software**Â (a process known as â€œ[hardware/software co-design](https://towardsdatascience.com/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2/)â€) because it reduced coordination with Google and Meta. Each major new generation of hardware would come out with aÂ **new version of CUDA**Â thatÂ **exploited the new capabilities**Â of the hardware. The AI community, eager for speed and efficiency, was more than willing toÂ **delegate this responsibility to NVIDIA**â€”which directly led to these frameworks beingÂ **tied to NVIDIA hardware**.
>  è¿™ä¸€ä¸¾åŠ¨ä¸ä»…è®© PyTorch å’Œ TensorFlow åœ¨ NVIDIA GPU ä¸Šæ˜¾è‘—æé€Ÿï¼Œä¹Ÿè®© NVIDIA èƒ½å¤Ÿç´§å¯†åœ°ç»“åˆå…¶è½¯ä»¶å’Œç¡¬ä»¶ï¼Œå› ä¸ºå‡å°‘äº†ä¸ Google å’Œ Meta çš„åè°ƒå·¥ä½œ
>  æ¯ä¸€ä»£æ–°çš„ç¡¬ä»¶å‘å¸ƒæ—¶ï¼Œéƒ½ä¼šé…å¥—æ¨å‡ºä¸€ä¸ªèƒ½å¤Ÿå……åˆ†åˆ©ç”¨æ–°ç¡¬ä»¶åŠŸèƒ½çš„ CUDA
>  AI ç¤¾åŒºæ¸´æœ›é€Ÿåº¦å’Œæ•ˆç‡ï¼Œæ•…æ„¿æ„å°†è¿™ä¸€è´£ä»»äº¤ç»™ NVIDIA â€”â€” è¿™ç›´æ¥å¯¼è‡´äº†è¿™äº›æ¡†æ¶å’Œ NVIDIA ç¡¬ä»¶æ·±åº¦ç»‘å®š

![Circular diagram depicting the inter-relationship of New AI Research Techniques, Expanded CUDA Libraries, and New Hardware Feature](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e730cf4b743d9ef1dd3_DCP3-Diagram02.png)

But why did Google and Meta let this happen? The reality is thatÂ **Google and Meta**Â werenâ€™t singularly focused on building a broad AI hardware ecosystemâ€”they were focused on using AI toÂ **drive revenue, improve their products, and unlock new research**. Their top engineers prioritizedÂ **high-impact internal projects**Â to move internal company metrics. For example, these companiesÂ [**decided to**Â **build**](https://thechipletter.substack.com/p/googles-first-tensor-processing-unit)Â their ownÂ [**proprietary TPU chips**](https://cloud.google.com/transform/ai-specialized-chips-tpu-history-gen-ai) â€”pouring their effort into optimizing for their ownÂ [first-party hardware.](https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/)Â It made sense toÂ **give the reins to NVIDIA**Â for GPUs.
>  ä¸ºä»€ä¹ˆ Google, Meta å…è®¸è¿™ç§æƒ…å†µå‘ç”Ÿå‘¢ï¼Œç°å®æ˜¯ Google å’Œ Meta å¹¶æ²¡æœ‰ä¸“æ³¨äºæ„å»ºä¸€ä¸ªå¹¿æ³›çš„ **AI ç¡¬ä»¶ç”Ÿæ€ç³»ç»Ÿ** â€”â€” ä»–ä»¬å…³æ³¨çš„æ˜¯åˆ©ç”¨ AI æ¥æ¨åŠ¨æ”¶å…¥ã€æå‡äº§å“å’Œè§£é”æ–°ç ”ç©¶
>  å®ƒä»¬çš„é¡¶å°–å·¥ç¨‹å¸ˆä¼˜å…ˆè€ƒè™‘çš„æ˜¯é«˜å½±å“åŠ›çš„å†…éƒ¨é¡¹ç›®ï¼Œä»¥æå‡å…¬å¸å†…éƒ¨çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼Œè¿™äº›å…¬å¸å†³å®šè‡ªå·±å¼€å‘ä¸“æœ‰çš„ TPU èŠ¯ç‰‡ï¼Œå¹¶å°†ç»å†æŠ•å…¥åˆ°ä¼˜åŒ–è‡ªå·±çš„**ç¬¬ä¸€æ–¹ç¡¬ä»¶ä¸Š**
>  è¿™åœ¨å½“æ—¶æ˜¯åˆç†çš„ï¼Œå³è®© NVIDIA è´Ÿè´£ GPU çš„å‘å±•

Makers of alternative hardware faced anÂ **uphill battle**â€”trying toÂ **replicate the vast, ever-expanding NVIDIA CUDA library ecosystem**Â without the same level of consolidated hardware focus. Rival hardware vendors werenâ€™t just strugglingâ€”they wereÂ **trapped in an endless cycle**, always chasing the next AI advancement on NVIDIA hardware. This impacted Google and Metaâ€™sÂ **in-house chip projects**Â as well, which led to numerous projects, including XLA and PyTorch 2. We can dive into these deeper in subsequent articles, butÂ [despite some hopes](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/), we can see today that nothing has enabled hardware innovators to match the capabilities of the CUDA platform.
>  å…¶ä»–ç¡¬ä»¶æ›¿ä»£å‚å•†åˆ™é¢ä¸´è‰°éš¾çš„æŒ‘æˆ˜ â€”â€” è¯•å›¾å¤åˆ¶ NVIDIA é‚£æ ·åºå¤§ä¸”ä¸æ–­æ‹“å±•çš„ CUDA åº“ç”Ÿæ€ç³»ç»Ÿï¼Œå´ç¼ºä¹åŒæ ·é›†ä¸­åŒ–çš„ç¡¬ä»¶æŠ•å…¥
>  ç«äº‰çš„ç¡¬ä»¶å‚å•†ä¸ä»…ä¸¾æ­¥ç»´è‰°ï¼Œè¿˜é™·å…¥äº†æ— å°½çš„å¾ªç¯ï¼Œä¸€ç›´åœ¨è¿½é€ NVIDIA ç¡¬ä»¶çš„ä¸‹ä¸€ä¸ª AI è¿›å±•
>  è¿™ä¹Ÿå½±å“äº† Google å’Œ Meta çš„è‡ªç ”èŠ¯ç‰‡é¡¹ç›®ï¼Œå¯¼è‡´äº†è®¸å¤šé¡¹ç›®ï¼Œä¾‹å¦‚ XLA å’Œ PyTorch 2
>  æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¦‚ä»Šæ²¡æœ‰ä»»ä½•ç¡¬ä»¶å¹³å°åˆ›æ–°è€…èƒ½å¤Ÿè¾¾åˆ° CUDA å¹³å°çš„èƒ½åŠ›

With each generation of its hardware,Â **NVIDIA widened the gap**. Then suddenly, in late 2022, ChatGPT exploded onto the scene, and with it,Â **GenAI and GPU compute went mainstream**.
>  éšç€æ¯ä¸€ä»£ç¡¬ä»¶çš„é€€å‡ºï¼ŒNVIDIA çš„ä¼˜åŠ¿ä¸æ–­æ‰©å¤§ï¼Œç›´åˆ° 2022 å¹´åº•ï¼ŒChatGPT å‡ºä¸–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯ç”Ÿæˆå¼ AI å’Œ GPU è®¡ç®—æ­£å¼è¿›å…¥ä¸»æµ

## Capitalizing on the Generative AI Surge
Almost overnight,Â **demand for AI compute**Â skyrocketedâ€”it became the foundation forÂ **billion-dollar industries**, consumer applications, and competitive corporate strategy.Â **Big tech**Â and venture capital firms pouredÂ [**billions**Â into AI research startups](https://techcrunch.com/2025/01/03/generative-ai-funding-reached-new-heights-in-2024/)Â andÂ [CapEx buildouts](https://www.thestreet.com/investing/nvidia-first-in-line-to-reap-gains-from-massive-big-tech-spending-surge) â€”money that ultimately funneled straight to NVIDIA, the only player capable of meeting theÂ **exploding demand for compute**.
>  ä¸€å¤œä¹‹é—´ï¼Œå¯¹ AI è®¡ç®—çš„éœ€æ±‚é£™å‡ â€”â€” å®ƒæˆä¸ºäº†æ•°åäº¿ç¾å…ƒçš„äº§ä¸šã€æ¶ˆè´¹è€…åº”ç”¨å’Œä¼ä¸šç«äº‰ç­–ç•¥çš„åŸºç¡€
>  ç§‘æŠ€å·¨å¤´å’Œé£é™©æŠ•èµ„å…¬å¸çº·çº·å‘ AI åˆåˆ›å…¬å¸æŠ•å…¥æ•°åäº¿ç¾å…ƒ â€”â€” è¿™äº›èµ„é‡‘æœ€ç»ˆæµå‘ NVIDIAï¼Œå› ä¸ºå®ƒæ˜¯å”¯ä¸€èƒ½å¤Ÿæ»¡è¶³**è®¡ç®—éœ€æ±‚æ¿€å¢**çš„ç©å®¶

As demand for AI compute surged, companies faced a stark reality:Â **training and deploying GenAI models is**Â [**incredibly expensive**](https://epoch.ai/blog/how-much-does-it-cost-to-train-frontier-ai-models). Every efficiency gainâ€”no matter how smallâ€”translated into massive savings at scale. WithÂ **NVIDIAâ€™s hardware already entrenched in data centers**, AI companies faced a serious choice:Â **optimize for CUDA or fall behind**. Almost overnight, the industry pivoted to writingÂ **CUDA-specific code**. The result? AI breakthroughs are no longer driven purely by models and algorithmsâ€”they nowÂ **hinge on the ability to extract every last drop of efficiency**Â fromÂ **CUDA-optimized code**.
>  éšç€è®¡ç®—éœ€æ±‚æ¿€å¢ï¼Œä¼ä¸šé¢ä¸´ä¸€ä¸ªä¸¥å³»çš„ç°å®: è®­ç»ƒå’Œéƒ¨ç½² GenAI éå¸¸æ˜‚è´µï¼Œæ¯ä¸€æ¬¡**æ•ˆç‡çš„æå‡**ï¼Œæ— è®ºå¤šä¹ˆå¾®å°ï¼Œåœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶éƒ½ä¼šå¸¦æ¥å·¨å¤§çš„æˆæœ¬èŠ‚çº¦
>  ç”±äº NVIDIA çš„ç¡¬ä»¶å·²ç»æ·±å…¥äº†æ•°æ®ä¸­å¿ƒï¼ŒAI å…¬å¸ä¸å¾—ä¸é¢ä¸´é€‰æ‹©: ä¼˜åŒ– CUDA æˆ–è½åäºäºº
>  ä¸€å¤œä¹‹é—´ï¼Œæ•´ä¸ªè¡Œä¸šè½¬å‘ç¼–å†™ CUDA ä»£ç ï¼Œç»“æœæ˜¯ï¼ŒAI çªç ´ä¸ä»…ä»…å†ä¾èµ–äºæ¨¡å‹å’Œç®—æ³•ï¼Œå®ƒä»¬ç°åœ¨å–å†³äºä» CUDA ä¼˜åŒ–ä»£ç ä¸­æ¦¨å–æ¯ä¸€ä»½æ•ˆç‡çš„èƒ½åŠ›

![Diagram depicting the architecture of FlashAttention-3, delineated by Stored in HBM vs. Computed in SRAM](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e86663492a6adec57a3_DCP3-Diagram04.png)

TakeÂ [**FlashAttention-3**](https://pytorch.org/blog/flashattention-3/), for example: this cutting-edge optimization slashed theÂ **cost of running transformer models**â€”but it was built exclusively forÂ **Hopper GPUs**, reinforcingÂ **NVIDIAâ€™s lock-in**Â by ensuring theÂ **best performance**Â was only available on its latest hardware.Â **Continuous research innovations**Â followed the same trajectory, for example whenÂ [**DeepSeek went directly to PTX assembly**](https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead), gainingÂ [full control over the hardware](https://medium.com/@amin32846/unlock-warp-level-performance-deepseeks-practical-techniques-for-specialized-gpu-tasks-a6cf0c68a178)Â at theÂ **lowest possible level**. With the newÂ [NVIDIA Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing)Â architecture on the horizon, we can look forward to the industryÂ **rewriting everything from scratch again**.

## The Reinforcing Cycles That Power CUDAâ€™s Grip
This system is accelerating andÂ **self-reinforcing**.Â **Generative AI has become a runaway force**, driving an insatiable demand for compute, andÂ **NVIDIA holds all the cards**. The biggestÂ **install base**Â ensures thatÂ **most AI research**Â happens inÂ **CUDA**, which in turnÂ **drives investment**Â into optimizing NVIDIAâ€™s platform.
>  GenAI æ˜¯ä¸€ä¸ªä¸å¯é˜»æŒ¡çš„åŠ›é‡ï¼Œæ¨åŠ¨ç€å¯¹è®¡ç®—èµ„æºçš„æ— å°½éœ€æ±‚ï¼Œè€Œ NVIDIA æ‹¥æœ‰å…¨éƒ¨ä¸»åŠ¨æƒ
>  æœ€å¤§çš„ç”¨æˆ·åŸºç¡€ç¡®ä¿å¤§å¤šæ•°ç ”ç©¶éƒ½åœ¨ CUDA ä¸Šè¿›è¡Œï¼Œè¿›è€Œæ¨åŠ¨äº†å¯¹ NVIDIA å¹³å°ä¼˜åŒ–çš„æŠ•èµ„

![Expansion of the earlier circular diagram, this time overlaying the inter-relationship of New Hardware Features, Datacenter CapEx Race, CUDA Specific Algorithms, and New AI Research Techniques](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e997da80c137d438a5e_DCP3-Diagram03.png)

Every new generation of NVIDIA hardware bringsÂ **new features and new efficiencies**, but it also demandsÂ **new software rewrites, new optimizations, and deeper reliance on NVIDIAâ€™s stack**. The future seems inevitable: a world where CUDAâ€™s grip on AI compute only tightens.
>  æ¯ä¸€ä»£æ–°çš„ NVIDIA ç¡¬ä»¶éƒ½å¸¦æ¥æ–°çš„ç‰¹æ€§å’Œæ•ˆç‡ï¼Œä¹Ÿè¦æ±‚äº†æ–°çš„è½¯ä»¶é‡å†™ã€æ–°çš„ä¼˜åŒ–å’Œå¯¹ NVIDIA æ ˆçš„æ›´æ·±åº¦ä¾èµ–
>  æœªæ¥ä¼¼ä¹åªæ˜¯ CUDA åœ¨ AI è®¡ç®—ä¸­çš„å½±å“åŠ›è¶Šæ¥è¶Šå¼ºçš„ä¸–ç•Œ

#### Except CUDAÂ isn't perfect.
The same forces thatÂ **entrench**Â CUDAâ€™s dominance are also becoming a bottleneckâ€”technical challenges, inefficiencies, andÂ **barriers to broader innovation**. Does this dominance actually serve theÂ **AI research community**? Is CUDAÂ **good for developers**, or justÂ **good for NVIDIA**?
>  ä½†è®© CUDA ç¨³å›ºçš„åŠ›é‡ä¹Ÿé€æ¸æˆä¸ºç“¶é¢ˆ: CUDA çš„ä¸»å¯¼åœ°ä½çœŸçš„æœ‰åˆ©äº AI ç ”ç©¶ç¤¾åŒºå—
>  CUDA å¯¹äºå¼€å‘è€…æ˜¯å¥½çš„ï¼Œè¿˜æ˜¯ä»…ä»…å¯¹äº NVIDIA æ˜¯å¥½çš„

Letâ€™s take a step back: We looked atÂ [**what CUDA is**](https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda)Â and why it is so successful, butÂ **is it actually good?**Â Weâ€™ll explore this in Part 4â€”stay tuned and let us know if you find this series useful, or have suggestions/requests! ğŸš€

-Chris

# 4 CUDA is the incumbent, but is it any good? 
Site: https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good
Date: 20 Feb 2025

Answering the question of whether CUDA is â€œgoodâ€ is much trickier than it sounds. Are we talking about its raw performance? Its feature set? Perhaps its broader implications in the world of AI development? Whether CUDA is â€œgoodâ€ depends onÂ **_who you ask_**Â andÂ **_what they need_**. 
>  å…³äº CUDA æ˜¯å¦ â€œå¥½â€ è¿™ä¸ªé—®é¢˜æ¯”å¬èµ·æ¥è¦å¤æ‚å¾—å¤š
>  æˆ‘ä»¬æ˜¯åœ¨å…³å¿ƒå®ƒçš„åŸå§‹æ€§èƒ½ï¼Ÿå®ƒçš„åŠŸèƒ½é›†ï¼Ÿè¿˜æ˜¯å®ƒåœ¨ AI å¼€å‘é¢†åŸŸæ›´å¹¿æ³›çš„å½±å“ï¼Ÿ
>  CUDA æ˜¯å¦ â€œå¥½â€ï¼Œå–å†³äºé—®çš„æ˜¯è°ä»¥åŠå®ƒä»¬éœ€è¦ä»€ä¹ˆ

In this post, weâ€™ll evaluate CUDA from the perspective of the people who use it day-in and day-outâ€”those who work in the GenAI ecosystem:

1. ForÂ **AI engineers who build on top of CUDA**, itâ€™s an essential tool, but one that comes with versioning headaches, opaque driver behavior, and deep platform dependence.
2. For AI engineersÂ **who write GPU code for NVIDIA hardware**, CUDA offers powerful optimization but only by accepting the pain necessary to achieve top performance.
3. For those who want theirÂ **AI workloads to run on GPUâ€™s from multiple vendors**, CUDA is more an obstacle than a solution.
4. Then thereâ€™sÂ **NVIDIA itself**â€”the company that has built its fortune around CUDA, driving massive profits and reinforcing their dominance over AI compute.

>  åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»æ¯å¤©ä½¿ç”¨ CUDA çš„äººçš„è§’åº¦ â€”â€” é‚£äº›åœ¨ GenAI ç”Ÿæ€ç³»ç»Ÿä¸­å·¥ä½œçš„äºº â€”â€” è¯„ä¼° CUDA 
>  1. å¯¹äºåœ¨ CUDA ä¹‹ä¸Šæ„å»º AI çš„å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æ˜¯å¿…è¦çš„å·¥å…·ï¼Œä½†ä¹Ÿä¼´éšç€ç‰ˆæœ¬ç®¡ç†çš„éº»çƒ¦ã€é©±åŠ¨è¡Œä¸ºçš„ä¸é€æ˜æ€§ï¼Œä»¥åŠå¯¹å¹³å°çš„æ·±åº¦ä¾èµ–
>  2. å¯¹äºä¸º NVIDIA ç¡¬ä»¶ç¼–å†™ CUDA çš„ AI å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æä¾›äº†å¼ºå¤§çš„ä¼˜åŒ–èƒ½åŠ›
>  3. å¯¹äºå¸Œæœ› AI workloads åœ¨å¤šä¸ªä¾›åº”å•†çš„ GPU ä¸Šè¿è¡Œçš„å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æ›´åƒæ˜¯ä¸€ä¸ªéšœç¢è€Œä¸æ˜¯è§£å†³æ–¹æ¡ˆ
>  4. å¯¹äº NVIDIA æœ¬èº«ï¼ŒCUDA å¸¦æ¥äº†å·¨é¢åˆ©æ¶¦ï¼Œå¹¶ä¸”å·©å›ºäº†å®ƒåœ¨ AI è®¡ç®—é¢†åŸŸçš„ä¸»å¯¼åœ°ä½

So, is CUDA â€œgood?â€ Letâ€™s dive into each perspective to find out! ğŸ¤¿

## AI Engineers
Many engineers today are building applications on top ofÂ **AI frameworks**â€”agentic libraries likeÂ [LlamaIndex](https://www.llamaindex.ai/),Â [LangChain](https://www.langchain.com/), andÂ [AutoGen](https://github.com/microsoft/autogen?tab=readme-ov-file) â€”without needing to dive deep into the underlying hardware details. For these engineers, CUDA is aÂ **powerful ally**. Its maturity and dominance in the industry bring significant advantages: most AI libraries are designed to work seamlessly with NVIDIA hardware, and the collective focus on a single platform fosters industry-wide collaboration.
>  å¤§å¤šæ•°å·¥ç¨‹å¸ˆå¦‚ä»Šéƒ½åœ¨ AI æ¡†æ¶ï¼Œä¾‹å¦‚ä»£ç†åº“å¦‚ LlamaIndex, LangChain, AutoGenï¼Œä¸Šæ„å»ºåº”ç”¨ï¼Œä¸éœ€è¦æ·±åº¦ç¡¬ä»¶ç»†èŠ‚
>  å¯¹äºè¿™äº›å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç›Ÿå‹: å¤§å¤šæ•° AI åº“éƒ½è®¾è®¡ä¸ NVIDIA ç¡¬ä»¶æ— ç¼é…åˆï¼Œå¯¹å•ä¸ªå¹³å°çš„é›†ä½“å…³æ³¨ä¹Ÿä¿ƒè¿›äº†æ•´ä¸ªè¡Œä¸šçš„åä½œ

However, CUDAâ€™s dominance comes with its ownÂ **set of persistent challenges**. One of the biggest hurdles is the complexity of managing different CUDA versions, which can be a nightmare. This frustration is the subject of numerous memes:
>  ä½† CUDA çš„ä¸»å¯¼åœ°ä½ä¹Ÿä¼´éšç€ä¸€ç³»åˆ—æŒ‘æˆ˜ï¼Œå…¶ä¸­æœ€å¤§çš„éšœç¢ä¹‹ä¸€å°±æ˜¯ç®¡ç†ä¸åŒ CUDA ç‰ˆæœ¬çš„å¤æ‚æ€§

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b63359eb680c24fd16370c_havent-related-to-a-meme-this-hard-in-a-minute-v0-8il1imrhpnde1.webp)

Credit:Â [x.com/ordax](https://x.com/ordax/status/1876624589993976295)

This isnâ€™tÂ _just_Â a memeâ€”itâ€™s a real, lived experience for many engineers. These AI practitioners constantly need to ensure compatibility between the CUDA toolkit, NVIDIA drivers, and AI frameworks. Mismatches can cause frustrating build failures or runtime errors, as countless developers have experienced firsthand:
>  AI ä»ä¸šè€…éœ€è¦ä¸æ–­ç¡®ä¿ CUDA å·¥å…·åŒ…ã€NVIDIA é©±åŠ¨ã€AI æ¡†æ¶ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œç‰ˆæœ¬ä¸åŒ¹é…å°±ä¼šå¯¼è‡´æ„å»ºå¤±è´¥

> "I failed to build the system with the latest NVIDIA PyTorch docker image. The reason is PyTorch installed by pip is built with CUDA 11.7 while the container uses CUDA 12.1." ([github.com](https://github.com/vllm-project/vllm/issues/129?utm_source=chatgpt.com))

or:

> "Navigating Nvidia GPU drivers and CUDA development software can be challenging. Upgrading CUDA versions or updating the Linux system may lead to issues such as GPU driver corruption." ([dev.to](https://dev.to/moseo/solving-the-version-conflicts-between-the-nvidia-driver-and-cuda-toolkit-2n2?utm_source=chatgpt.com))

Sadly, such headaches are not uncommon. Fixing them often requires deep expertise and time-consuming troubleshooting. NVIDIA's reliance on opaque tools and convoluted setup processes deters newcomers and slows down innovation.
>  å¤„ç†è¿™äº›é—®é¢˜é€šå¸¸éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œæ’æŸ¥å·¥ä½œ
>  NVIDIA å¯¹ä¸é€æ˜å·¥å…·å’Œå¤æ‚è®¾ç½®æµç¨‹çš„ä¾èµ–è®©æ–°æ‰‹éš¾ä»¥ç†Ÿç»ƒ

In response to these challenges, NVIDIA has historically moved up the stack to solve individual point-solutions rather than fixing the fundamental problem: the CUDA layer itself. For example, it recently introducedÂ **NIM**Â (NVIDIA Inference Microservices), a suite of containerized microservices aimed at simplifying AI model deployment. While this might streamline one use-case, NIM also abstracts away underlying operations, increasing lock-in and limiting access to the low-level optimization and innovation key to CUDA's value proposition.
>  ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒNVIDIA å€¾å‘äºåœ¨æŠ€æœ¯æ ˆçš„ä¸Šå±‚æä¾›ç‚¹å¯¹ç‚¹çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯ä»æ ¹æœ¬ä¸Šä¿®å¤ CUDA æœ¬èº«çš„é—®é¢˜
>  NVIDIA æœ€è¿‘æå‡ºäº† NVIDIA æ¨ç†å¾®æœåŠ¡ï¼Œä¸€ä¸ªå®¹å™¨åŒ–çš„å¾®æœåŠ¡å¥—ä»¶ï¼Œä»¥ç®€åŒ– AI æ¨¡å‹çš„éƒ¨ç½²

While AI engineers building on top of CUDA face challenges with compatibility and deployment, those working closer to the metalâ€”**AI model developers and performance engineers**â€”grapple with an entirely different set of trade-offs.
>  åŸºäº CUDA æ„å»ºçš„ AI å·¥ç¨‹å¸ˆåœ¨å…¼å®¹æ€§å’Œéƒ¨ç½²æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒAI æ¨¡å‹å¼€å‘è€…å’Œæ€§èƒ½å·¥ç¨‹å¸ˆåˆ™éœ€è¦é¢å¯¹ä¸åŒçš„æƒè¡¡

## AI Model Developers and Performance Engineers
For researchers and engineers pushing the limits of AI models, CUDA is simultaneously an essential tool and aÂ **frustrating limitation**. For them, CUDA isnâ€™t an API; itâ€™s the foundation for every performance-critical operation they write. These are engineers working at the lowest levels of optimization, writing custom CUDA kernels, tuning memory access patterns, and squeezing every last bit of performance from NVIDIA hardware. The scale and cost of GenAI demand it. But does CUDA empower them, or does it limit their ability to innovate?
>  å¯¹äºæ¨åŠ¨ AI æé™çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆï¼ŒCUDA æ˜¯å¿…é¡»çš„å·¥å…·ï¼Œä¹Ÿæ˜¯é™åˆ¶
>  ä»–ä»¬äº†è§£ CUDAï¼Œä½¿ç”¨ CUDA ç¼–å†™é«˜æ€§èƒ½ kernelï¼Œè°ƒä¼˜å†…å­˜è®¿é—®æ¨¡å¼ï¼Œæ¦¨å– NVIDIA ç¡¬ä»¶çš„æ€§èƒ½

Despite its dominance, CUDA isÂ **showing its age**. It was designed inÂ **2007**, long before deep learningâ€”let alone GenAI. Since then, GPUs have evolved dramatically, withÂ **Tensor Cores**Â and sparsity features becoming central to AI acceleration. CUDAâ€™s early contribution was to make GPU programming easy, butÂ **it hasnâ€™t evolved with modern GPU features necessary for transformers and GenAI performance.**Â This forces engineers toÂ **work around its limitations**Â just to get the performance their workloads demand.
>  CUDA æœ€åˆè®¾è®¡ä¸ 2007 å¹´ï¼Œæ—©äºæ·±åº¦å­¦ä¹ ä»¥åŠ GenAI
>  ä¹‹åï¼ŒGPU ç»å†äº†å·¨å¤§çš„å‘å±•ï¼ŒTensor core å’Œç¨€ç–ç‰¹æ€§å·²ç»ç§°ä¸º AI åŠ é€Ÿçš„æ ¸å¿ƒï¼ŒCUDA çš„æ—©æœŸè´¡çŒ®è®© GPU ç¼–ç¨‹å®¹æ˜“ï¼Œä½†æ²¡æœ‰éšç€é’ˆå¯¹ transformers å’Œ GenAI æ€§èƒ½æ‰€éœ€çš„ç°ä»£ GPU ç‰¹æ€§å‘å±•
>  è¿™è¿«ä½¿å·¥ç¨‹å¸ˆç»•è¿‡ä»–ä»¬çš„é™åˆ¶ï¼Œæ‰èƒ½æ»¡è¶³å…¶ worload å¯¹æ€§èƒ½çš„éœ€æ±‚

###### **CUDA doesnâ€™t do everything modern GPUs can do**
Cutting-edge techniques likeÂ [**FlashAttention-3**](https://pytorch.org/blog/flashattention-3/)Â ([example code](https://github.com/Dao-AILab/flash-attention/blob/a09abcd32d3cae4d83b313446e887f38d02b799f/hopper/copy_sm90_bulk_reduce.hpp#L22)) andÂ [**DeepSeek**](https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai)**â€™s**Â innovations require developers to drop below CUDA intoÂ **PTX**â€”NVIDIAâ€™s lower-level assembly language. PTX is only partially documented, constantly shifting between hardware generations, and effectively a black box for developers.
>  æœ€è¿‘çš„åˆ›æ–°è¦æ±‚å¼€å‘è€…æ·±å…¥ PTX å±‚ï¼ŒPTX æ–‡æ¡£ä¸å…¨ï¼Œä¸”åœ¨ä¸åŒç¡¬ä»¶ä»£é™…ä¹‹é—´é¢‘ç¹å˜åŒ–

More problematic,Â **PTX is even more locked to NVIDIA than CUDA**, and its usability is even worse. However, for teams chasing cutting-edge performance,Â **thereâ€™s no alternative**â€”theyâ€™re forced toÂ **bypass CUDA**Â and endure significant pain.
>  å¹¶ä¸”ï¼ŒPTX å¯¹äº NVIDIA å¹³å°ä¾èµ–æ€§æ›´é«˜

###### **Tensor Cores: Required for performance, but hidden behind black magic**
Today, the bulk of an AI modelâ€™s FLOPs come from â€œ[**Tensor Cores**](https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/)**â€**, not traditional CUDA cores. However, programming Tensor Cores directly is no small feat. While NVIDIA provides some abstractions (like cuBLAS and CUTLASS), getting the most out of GPUs still requiresÂ **arcane knowledge**, trial-and-error testing, and often,Â [reverse engineering undocumented behavior](https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead). Â With each new GPU generation, Tensor Cores change, yet theÂ [**documentation is dated**](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)**.**Â This leaves engineers with limited resources to fully unlock the hardwareâ€™s potential.
>  å¦‚ä»Šå¤§å¤šæ•° AI æ¨¡å‹é€šè¿‡ Tensor cores è®¡ç®—ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ CUDA cores
>  ä½†ç›´æ¥ç¼–ç¨‹ Tensor cores ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹ï¼ŒNVIDIA æä¾›äº†æŠ½è±¡ä¾‹å¦‚ cuBLAS, CUTLASSï¼Œä½†è¦å……åˆ†å‘æŒ¥ GPU æ€§èƒ½ä»ç„¶éœ€è¦æ›´å¤šçŸ¥è¯†ï¼Œæœ‰æ—¶ç”šè‡³éœ€è¦é€†å‘å·¥ç¨‹æœªå…¬å¼€çš„è¡Œä¸º
>  éšç€æ¯ä¸€ä»£ GPU æ¨å‡ºï¼ŒTensor cores ä¹Ÿåœ¨ä¸æ–­å˜åŒ–ï¼Œä½†æ–‡æ¡£å¾€å¾€æ»åï¼Œè¿™ä½¿å¾—å·¥ç¨‹å¸ˆä¸ä¾¿äºå……åˆ†åˆ©ç”¨ç¡¬ä»¶æ½œåŠ›

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b63502b1d4eb5730861f5a_Turing-Tensor-Core-New-Diag-White-Background.jpg)

Credit:Â NVIDIA

###### **AI is Python, but CUDA is C++**
Another major limitation is that writingÂ **CUDA**Â [**fundamentally requires using C++**](https://docs.nvidia.com/cuda/cuda-c-programming-guide/), while modern AI development is overwhelmingly done inÂ **Python**. Engineers working on AI models and performance in PyTorch donâ€™t want to switch back and forth between Python and C++â€”the two languages haveÂ **very different mindsets**. This mismatchÂ **slows down iteration**, creates unnecessary friction, and forces AI engineers to think about low-level performance details when they should be focusing on model improvements. Additionally, CUDA's reliance onÂ [**C++ templates**](https://github.com/NVIDIA/cutlass)Â leads toÂ [painfully slow compile times](https://developer.nvidia.com/blog/reducing-application-build-times-using-cuda-c-compilation-aids/)Â and often incomprehensible error messages.
>  å¦ä¸€ä¸ªé™åˆ¶æ˜¯è¯­è¨€
>  AI å·¥ç¨‹å¸ˆè¦æ±‚æ€§èƒ½ï¼Œå°±éœ€è¦ä½¿ç”¨ C++ï¼Œè€Œ C++ å’Œ Python çš„ç¼–ç¨‹æ€æƒ³å®Œå…¨ä¸åŒ
>  æ­¤å¤–ï¼ŒCUDA ä¾èµ–äº C++æ¨¡æ¿ï¼Œä¼šéªŒè¯é™ä½ç¼–è¯‘æ—¶é—´ï¼Œå¹¶ä¸”é”™è¯¯ä¿¡æ¯ä¸å¥½è¯»

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b6355eb56de6aa5924290c_compiling.png)

Credit:Â [XKCD](https://xkcd.com/303/)

These are the challenges you face if youâ€™re happy to develop specifically for NVIDIA hardware. ButÂ **what if you care about more than just NVIDIA?**
>  è¿™äº›æ˜¯æˆ‘ä»¬å¯¹ NVIDIA ç¡¬ä»¶å¼€å‘éœ€è¦é¢å¯¹çš„å›°éš¾ï¼Œé‚£ä¹ˆå¯¹å…¶ä»–ç¡¬ä»¶å¼€å‘çš„å›°éš¾å‘¢ï¼Ÿ

## Engineers and Researchers Building Portable Software
Not everyone is happy to build software locked to NVIDIAâ€™s hardware, and the challenges are clear. CUDAÂ **doesnâ€™t run on hardware from other vendors**Â (like theÂ [supercomputer in our pockets](https://www.visualcapitalist.com/the-supercomputer-in-your-pocket/)), and no alternatives provide the full performance and capabilities CUDA provides on NVIDIA hardware. This forces developers to write their AI code multiple times, for multiple platforms.
>  CUDA æ— æ³•åœ¨å…¶ä»–ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œå¹¶ä¸”ç›®å‰ä¹Ÿæ²¡æœ‰æ›¿ä»£æ–¹æ¡ˆèƒ½åœ¨å…¶ä»–ç¡¬ä»¶ä¸Šæä¾› NVIDIA + CUDA çš„æ€§èƒ½ï¼Œè¿™éœ€è¦å¼€å‘è€…ä¸ºä¸åŒçš„å¹³å°å¤šæ¬¡ç¼–å†™ AI ä»£ç 

In practice, many cross-platform AI efforts struggle. Early versions of TensorFlow and PyTorch had OpenCL backends, but they lagged far behind the CUDA backend in both features and speed, leading most users to stick with NVIDIA. Maintaining multiple code pathsâ€”CUDA for NVIDIA, something else for other platformsâ€”is costly, and as AI rapidly progresses, only large organizations have resources for such efforts.
>  å®é™…ä¸Šï¼Œè®¸å¤šè·¨å¹³å° AI é¡¹ç›®éƒ½é¢ä¸´å›°éš¾ï¼Œæ—©æœŸç‰ˆæœ¬çš„ TensorFlow å’Œ PyTorch æœ‰ OpenCL åç«¯ï¼Œä½†ä»–ä»¬åœ¨åŠŸèƒ½å’Œé€Ÿåº¦ä¸Šè¿œè¿œè½åäº CUDA åç«¯ï¼Œå¯¼è‡´å¤§å¤šæ•°ç”¨æˆ·ä»ç„¶é€‰æ‹© CUDA
>  åŒæ—¶ç»´æŠ¤é’ˆå¯¹å¤šæ¡çš„ä»£ç è·¯å¾„è¿‡äºæ˜‚è´µï¼Œåªæœ‰å¤§å‹ç»„ç»‡æ‰æœ‰è¿™æ ·çš„èµ„æº

The bifurcation CUDA causes creates aÂ **self-reinforcing cycle**: since NVIDIA has the largest user base and the most powerful hardware, most developers target CUDA first, and hope that others will eventually catch up. This further solidifies CUDAâ€™s dominance as the default platform for AI.
>  è¿™æ ·çš„åˆ†åŒ–ä½¿å¾— CUDA æ‹¥æœ‰ä¸€ä¸ªè‡ªæˆ‘å¼ºåŒ–çš„å¾ªç¯: å¼€å‘è€…é¦–å…ˆé’ˆå¯¹ CUDA è¿›è¡Œå¼€å‘ï¼Œå…¶ä»–çš„å‚å•†åªèƒ½è‡ªè¡Œè·Ÿä¸Š

ğŸ‘‰ Weâ€™ll explore alternatives like OpenCL, TritonLang, and MLIR compilers in our next post, and come to understand why these options havenâ€™t made a dent in CUDA's dominance.
>  æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« è®¨è®ºæ›¿ä»£æ–¹æ¡ˆï¼Œä¾‹å¦‚ OpenCL, TritonLang, MLIRï¼Œå¹¶ç†è§£ä¸ºä»€ä¹ˆè¿™äº›é€‰é¡¹è‡³ä»Šæœªèƒ½æ’¼åŠ¨ CUDA çš„åœ°ä½

## Is CUDA Good for NVIDIA Itself?
Of course, theÂ **answer is yes:**Â the â€œCUDA moatâ€ enables aÂ **winner-takes-most**Â scenario. By 2023, NVIDIA heldÂ [**~98% of the data-center GPU market share**](https://www.datacenterdynamics.com/en/news/nvidia-gpu-shipments-totaled-376m-in-2023-equating-to-a-98-market-share-report/#:~:text=As%20reported%20by%20HPCwire%2C%20the,company%20in%20the%20year%20prior), cementing its dominance in the AI space. As we've discussed inÂ [previous posts](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed), CUDA serves as theÂ **bridge between NVIDIAâ€™s past and future products**, driving the adoption of new architectures like Blackwell and maintaining NVIDIA's leadership in AI compute.
>  2023 å¹´ï¼ŒNVIDIA æ‹¥æœ‰æ•°æ®ä¸­å¿ƒ GPU 98% çš„å¸‚åœºä»½é¢
>  CUDA æ˜¯è¿æ¥ NVIDIA è¿‡å»å’Œæœªæ¥äº§å“çš„æ¡¥æ¢

However,Â **legendary hardware experts**Â likeÂ [Jim Keller](https://en.wikipedia.org/wiki/Jim_Keller_\(engineer\))Â argue that "[**CUDAâ€™s a swamp, not a moat**](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too),â€ making analogies to the X86 architecture that bogged Intel down.
>  ä½†ä¸“å®¶è®¤ä¸º CUDA æ˜¯æ²¼æ³½ï¼Œä¸æ˜¯æŠ¤åŸæ²³ï¼Œå°±åƒ X86 æ›¾ç»è®© Intel é™·å…¥å›°å¢ƒ

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b65da3cd06c4c299e576af__c73e3185-8a45-4a4f-9d79-62d789cc7fe3.jpeg)

"[CUDA's a swamp, not a moat](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too)," arguesÂ [Jim Keller](https://en.wikipedia.org/wiki/Jim_Keller_\(engineer\))

How could CUDA be a problem for NVIDIA? There are several challenges.

###### **CUDA's usability impacts NVIDIA the most**
Jensen Huang famously claims that NVIDIA employsÂ [more software engineers than hardware engineers](https://www.wsj.com/tech/ai/ai-nvidia-apple-amd-jensen-huang-software-bb581f5a), with a significant portion dedicated to writing CUDA. But theÂ **usability and scalability**Â challenges within CUDA slow down innovation, forcing NVIDIA to aggressively hire engineers to fire-fight these issues.
>  CUDA å†…ç”Ÿçš„å¯ç”¨æ€§å’Œå¯æ‹“å±•æ€§é˜»ç¢äº†åˆ›æ–°ï¼Œè¿«ä½¿ NVIDIA ä¸å¾—ä¸å¤§é‡æ‹›è˜å·¥ç¨‹å¸ˆè§£å†³è¿™äº›é—®é¢˜

###### **CUDAâ€™s heft slows new hardware rollout**
CUDA doesnâ€™t provideÂ **performance portability**Â across NVIDIAâ€™s own hardware generations, and the sheer scale of its libraries is a double-edged sword. When launching a new GPU generation like Blackwell, NVIDIA faces a choice: rewrite CUDA or release hardware that doesnâ€™t fully unleash the new architectureâ€™s performance. This explains whyÂ [performance is suboptimal at launch](https://www.forbes.com/sites/karlfreund/2023/09/08/nvidia-adds-new-software-that-can-double-h100-inference-performance/)Â of each new generation. SuchÂ **expansion**Â of CUDAâ€™s surface area is costly and time-consuming.
>  CUDA ä¸èƒ½åœ¨ NVIDIA è‡ªå·±çš„ç¡¬ä»¶ä»£é™…ä¸Šæä¾›**æ€§èƒ½å¯ç§»æ¤æ€§**ï¼Œå…¶åºå¤§çš„åº“è§„æ¨¡ä¹Ÿæ˜¯åŒåˆƒå‰‘
>  å½“æ¨å‡ºæ–°çš„æ¶æ„æ—¶ï¼ŒNVIDIA é¢ä¸´ä¸€ä¸ªé€‰æ‹©ï¼Œè¦ä¹ˆé‡å†™ CUDAï¼Œè¦ä¹ˆå‘å¸ƒæ— æ³•å‘æŒ¥æ–°æ¶æ„æ€§èƒ½çš„ç¡¬ä»¶
>  è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ¯ä¸€ä»£æ–° GPU åœ¨åˆšå‘å¸ƒæ—¶æ€§èƒ½éƒ½æ¬ ä½³

###### **The Innovatorâ€™s Dilemma**
NVIDIAâ€™s commitment to backward compatibilityâ€”one of CUDAâ€™s early selling pointsâ€”has now become â€œ**technical debtâ€**Â that hinders their own ability to innovate rapidly. While maintaining support for older generations of GPUs is essential for their developer base, itÂ **forces NVIDIA to prioritize stability over revolutionary changes**. This long-term support costs time, resources, and could limit their flexibility moving forward.
>  NVIDIA å¯¹å‘åå…¼å®¹æ€§çš„æ‰¿è¯ºå¦‚ä»Šå·²ç»æˆä¸ºäº†æŠ€æœ¯å€ºåŠ¡ï¼Œé˜»ç¢äº†è‡ªèº«çš„åˆ›æ–°
>  ç»´æŒå¯¹æ—§ç‰ˆ GPU çš„æ”¯æŒè¿«ä½¿ NVIDIA æ›´åŠ é‡è§†ç¨³å®šæ€§ï¼Œè€Œéé©å‘½æ€§çš„å˜åŒ–

Though NVIDIA has promised developers continuity, Blackwell couldn't achieve its performance goals withoutÂ [breaking compatibility with Hopper PTX](https://docs.nvidia.com/cuda/blackwell-compatibility-guide/#application-compatibility-on-blackwell-architecture) â€”now someÂ [Hopper PTX operations](https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-multiply-and-accumulate-instruction-wgmma-mma-async)Â donâ€™t work on Blackwell. This means advanced developers who have bypassed CUDA in favor of PTX may find themselves rewriting their code for the next-generation hardware.
>  Blackwell æ¶æ„å·²ç»ä¸å¾—ä¸æ‰“ç ´äº†ä¸ Hopper PTX çš„å…¼å®¹æ€§ â€”â€” ç°åœ¨**ä¸€äº› Hopper PTX æ“ä½œæ— æ³•åœ¨ Blackwell ä¸Šæ— æ³•æ­£å¸¸è¿è¡Œ** (ç ´åäº†å‘åå…¼å®¹)

Despite these challenges,Â **NVIDIAâ€™s strong execution in software**Â and its early strategic decisions have positioned them well for future growth. With the rise of GenAI and a growing ecosystem built on CUDA, NVIDIA is poised to remain at the forefront of AI compute and has rapidly grown into one of theÂ [most valuable companies in the world](https://www.washingtonpost.com/business/2024/11/05/nvidia-tops-apple/).

## Where Are the Alternatives to CUDA?
In conclusion, CUDA remains both a blessing and a burden, depending on which side of the ecosystem youâ€™re on. ItsÂ [massive success](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed)Â drove NVIDIAâ€™s dominance, but its complexity, technical debt, and vendor lock-in present significant challenges for developers and the future of AI compute.

With AI hardware evolving rapidly, a natural question emerges:Â **Where are the alternatives to CUDA?**Â Why hasnâ€™t another approach solved these issues already? In Part 5, weâ€™ll explore the most prominent alternatives, examining the technical and strategic problems that prevent them from breaking through the CUDA moat. ğŸš€
>  ä¸‹ä¸€ç« å°†æ¢è®¨ CUDA çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ†æä»–ä»¬çªç ´ CUDA æŠ¤åŸæ²³çš„æŠ€æœ¯å’Œæˆ˜ç•¥é—®é¢˜

â€“Chris

# 5 What about OpenCL and CUDA C++ alternatives?
Site: https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives
Date: 5 March 2025

**GenAI may be new, but GPUs arenâ€™t!**Â Over the years, many have tried to create portable GPU programming models using C++, from OpenCL to SYCL to OneAPI and beyond. These were the most plausible CUDA alternatives that aimed to democratize AI compute, but you may have never heard of them - because they failed to be relevant for AI.
>  å¤šå¹´æ¥ï¼Œè®¸å¤šäººå°è¯•ç”¨ C++ åˆ›å»ºå¯ç§»æ¤çš„ GPU ç¼–ç¨‹æ¨¡å‹ï¼Œä» OpenCL åˆ° SYCL åˆ° OneAPI
>  è¿™äº›æ˜¯ CUDA æœ€æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨è®© AI è®¡ç®—æ›´åŠ æ™®åŠï¼Œä½†å®ƒä»¬æœªèƒ½åœ¨ AI é¢†åŸŸä¿æŒç›¸å…³æ€§

These projects have all contributed meaningfully to compute, but if we are serious about unlocking AI compute for the future, we must critically examine the mistakes that held them backâ€”not just celebrate the wins. At a high level, the problems stem from the challenges of "[open coopetition](https://en.wikipedia.org/wiki/Open_coopetition)"â€”where industry players both collaborate and competeâ€”as well as specific management missteps along the way.
>  è¿™äº›é¡¹ç›®éƒ½å¯¹è®¡ç®—é¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œæˆ‘ä»¬éœ€è¦æ‰¹åˆ¤æ€§åœ°å®¡è§†å®ƒä»¬å‘å±•çš„é”™è¯¯
>  ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œè¿™äº›é—®é¢˜æºäº â€œå¼€æ”¾åˆä½œç«äº‰â€ çš„æŒ‘æˆ˜ â€”â€” è¡Œä¸šå‚ä¸è€…å³åˆä½œåˆç«äº‰ â€”â€” ä»¥åŠä¸€äº›å‘å±•è¿‡ç¨‹ä¸­çš„å…·ä½“ç®¡ç†å¤±è¯¯

Letâ€™s dive in. ğŸš€

## CUDA C++ Alternatives: OpenCL, SYCL, and More
There are many projects that aimed to unlock GPU programming, but the one I know best isÂ [**OpenCL**](https://en.wikipedia.org/wiki/OpenCL). Like CUDA, OpenCL aimed to give programmers a C++-like experience for writing code that ran on the GPU. Â The history is personal: in 2008, I was one of the lead engineers implementing OpenCL at Apple (it was the first production use of theÂ [Clang compiler](https://en.wikipedia.org/wiki/Clang)Â I was building). After weÂ [shipped it,](https://en.wikipedia.org/wiki/OpenCL#History)Â we made the pivotal decision to contribute it to theÂ [Khronos Group](https://www.khronos.org/opencl/)Â so it could get adopted and standardized across the industry.
>  OpenCL ç›®æ ‡æ˜¯ç»™ç¨‹åºå‘˜ç±»ä¼¼ç¼–å†™ C++ çš„ä½“éªŒæ¥ç¼–å†™è¿è¡Œåœ¨ GPU ä¸Šçš„ä»£ç ï¼Œç±»ä¼¼äº CUDA
>  OpenCL æœ€åˆç”± Clang ç¼–è¯‘å™¨æ”¯æŒï¼Œåæ¥å®ƒè¢«è´¡çŒ®ç»™äº† Khronos Group

That decision led to broad industry adoption of OpenCL (seeÂ [the logos](https://www.khronos.org/opencl/)), particularly in mobile and embedded devices. Today, it remains hugely successful, powering GPU compute on platforms like Android, as well as in specialized applications such as DSPs. Unlike CUDA, OpenCL was designed for portability from the outset, aiming to support heterogeneous compute across CPUs, GPUs, and other accelerators. OpenCL also inspired other systems like SyCL, Vulkan, SPIR-V, oneAPI, WebCL and many others.
>  OpenCL åœ¨è¿™ä¹‹åè¢«è¡Œä¸šå¹¿æ³›é‡‡ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼è®¾å¤‡ä¸­ï¼Œå¦‚ä»Šï¼Œå®ƒä»ç„¶éå¸¸æˆåŠŸï¼Œä¸º Android å¹³å°ä¸Šçš„ GPU è®¡ç®—æä¾›æ”¯æŒ
>  ä¸ CUDA ä¸åŒï¼ŒOpenCL è®¾è®¡ä¹‹åˆå°±å…³æ³¨å¯ç§»æ¤æ€§ï¼Œæ—¨åœ¨æ”¯æŒè·¨ CPU, GPU å’Œå…¶ä»–åŠ é€Ÿå™¨çš„å¼‚æ„è®¡ç®—
>  OpenCL è¿˜å¯å‘äº†å…¶ä»–ç³»ç»Ÿï¼Œä¾‹å¦‚ SyCL, Vulkan, SPIR-V, oneAPI, WebCL ç­‰

However, despite its technical strengths and broad adoption,Â [**OpenCL never became the dominant AI compute platform**](https://github.com/tensorflow/tensorflow/issues/22#issuecomment-155145957). There are several major reasons for this: the inherent tensions of open coopetition, technical problems that flowed from that, the evolving requirements of AI, and NVIDIAâ€™s unified strategy with TensorFlow and PyTorch.
>  ä½† OpenCL æœªèƒ½æˆä¸ºä¸»å¯¼çš„ AI è®¡ç®—å¹³å°ï¼ŒåŸå› æœ‰: å¼€æ”¾åˆä½œç«äº‰å›ºæœ‰çš„çŸ›ç›¾ã€AI è¦æ±‚çš„ä¸æ–­æ¼”å˜ã€NVIDIA åœ¨ TensorFlow å’Œ PyTorch ä¸Šçš„ç»Ÿä¸€ç­–ç•¥

### **â€œ**[**Coopetition**](https://en.wikipedia.org/wiki/Open_coopetition)**â€ at Committee Speed**
In 2008, Apple was a small player in the PC space, and thought that industry standardization would enable it to reach more developers. Â However, while OpenCL did gain broad adoption among hardware makers, its evolution quickly ran into a major obstacle: the speed of committee-driven development. For Apple, this slow-moving, consensus-driven process was a dealbreaker: we wanted to move the platform rapidly, add new features (e.g. add C++ templates), and express the differentiation of the Apple platform. Â We faced a stark reality - the downside of a committee standard is that things suddenly moved at committee consensus speedâ€¦ which felt glacial.
>  2008 å¹´ï¼ŒApple åœ¨ PC é¢†åŸŸè¿˜æ˜¯ä¸€ä¸ªå°ç©å®¶ï¼Œå®ƒè®¤ä¸ºè¡Œä¸šæ ‡å‡†åŒ–èƒ½å¤Ÿå¸å¼•æ›´å¤šå¼€å‘è€…ï¼Œç„¶è€Œå°½ç®¡ OpenCL åœ¨ç¡¬ä»¶å‚å•†ä¸­è·å¾—äº†å¹¿æ³›é‡‡ç”¨ï¼Œå®ƒçš„æ¼”è¿›å¾ˆå¿«é‡åˆ°äº†ä¸€ä¸ªé‡å¤§éšœç¢: å§”å‘˜ä¼šçš„å¼€å‘é€Ÿåº¦
>  è¿™ç§ç¼“æ…¢çš„ã€åŸºäºå…±è¯†çš„æµç¨‹å¯¹äº Apple æ˜¯ä¸å¯æ¥å—çš„ï¼ŒApple å¸Œæœ›å¿«é€Ÿæ¨åŠ¨å¹³å°å‘å±•ï¼Œæ·»åŠ æ–°ç‰¹æ€§ (ä¾‹å¦‚ C++ æ¨¡æ¿)

Hardware vendors recognized the long-term benefits of a unified software ecosystem, but in the short term, they were fierce competitors. This led to subtle but significant problems: instead of telling the committee about the hardware features youâ€™re working on (giving a competitor a head start), participants would keep innovations secret until after the hardware shipped, and only discuss it after these features became commoditized (using vendor-specific extensions instead).
>  ç¡¬ä»¶å‚å•†è®¤è¯†åˆ°ç»Ÿä¸€è½¯ä»¶ç”Ÿæ€çš„é•¿æœŸå¥½å¤„ï¼Œä½†åœ¨çŸ­æœŸå†…ï¼Œå®ƒä»¬ä»ç„¶æ˜¯æ¿€çƒˆçš„ç«äº‰å¯¹æ‰‹
>  è¿™å¯¼è‡´äº†ä¸€äº›å¾®å¦™ä¸”é‡è¦çš„é—®é¢˜: ä¸å…¶å‘å§”å‘˜ä¼šæŠ¥å‘Šä½ æ­£åœ¨å¼€å‘çš„ç¡¬ä»¶ç‰¹æ€§ï¼Œç«äº‰è€…æ›´å€¾å‘äºåœ¨ç¡¬ä»¶å‘å¸ƒåå†å…¬å¼€è¿™äº›åˆ›æ–°ï¼Œå¹¶åœ¨è¿™äº›åŠŸèƒ½å˜å¾—é€šç”¨åŒ–ä¹‹åæ‰è¿›è¡Œè®¨è®º

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c864b4945e50e3855205b4_Coopetition.jpg)

Coopetition: "cooperation" amongst competitors

This became a huge problem for Apple, a company that wanted to move fast in secret to make a big splash with product launches. Â As such, Apple decided to abandon OpenCL: it introduced Metal instead, never brought OpenCL to iOS, and deprecated it out of macOS later. Other companies stuck with OpenCL, but these structural challenges continued to limit its ability to evolve at the pace of cutting-edge AI and GPU innovation.
>  Apple æ˜¯ä¸€å®¶å¸Œæœ›åœ¨ä¿å¯†æƒ…å†µå¿«é€Ÿæ¨è¿›ï¼Œå¹¶ä¸”åœ¨å‘å¸ƒæ—¶é€ æˆå·¨å¤§å½±å“çš„å…¬å¸ï¼Œå› æ­¤ Apple å†³å®šæ”¾å¼ƒ OpenCL: å®ƒæ¨å‡ºäº† Metal ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œä¸”ä»æœªå°† OpenCL å¼•å…¥ iOSï¼Œå¹¶åœ¨åæ¥ä» macOS å¼ƒç”¨äº† OpenCL
>  å…¶ä»–å…¬å¸ç»§ç»­ä½¿ç”¨ OpenCLï¼Œä½†è¿™äº›æ¥å—æ€§æŒ‘æˆ˜é™åˆ¶äº†å®ƒè·Ÿä¸Šæœ€å‰æ²¿çš„ AI å’Œ GPU å‘å±•æ­¥ä¼çš„èƒ½åŠ›

### **Technical Problems with OpenCL**
While Apple boldly decided to contribute the OpenCL standard to Kronos, it wasnâ€™t all-in: it contributed OpenCL as a technical specificationâ€”but without a full reference implementation. Though parts of the compiler front-end (Clang) was open source, there was no shared OpenCL runtime, forcing vendors to develop their own custom forks and complete the compiler. Â Each vendor had to maintain its own implementation (a â€forkâ€), and without a shared, evolving reference, OpenCL became a patchwork of vendor-specific forks and extensions. This fragmentation ultimately weakened its portabilityâ€”the very thing it was designed to enable.
>  Apple å°† OpenCL è´¡çŒ®ç»™ Kronos æ—¶ï¼Œä»…è´¡çŒ®äº†æŠ€æœ¯è§„èŒƒï¼Œä½†æ²¡æœ‰æä¾›å®Œæ•´çš„å‚è€ƒå®ç°
>  è™½ç„¶ç¼–è¯‘å™¨å‰ç«¯ (Clang) æ˜¯å¼€æºçš„ï¼Œä½†æ²¡æœ‰å…±äº«çš„ OpenCL runtimeï¼Œè¿™è¿«ä½¿å„å‚å•†è‡ªè¡Œå¼€å‘è‡ªå·±çš„ fork æ¥å®Œæˆç¼–è¯‘å™¨
>  ç”±äºæ¯ä¸ªå‚å•†éƒ½ç»´æŠ¤è‡ªå·±çš„ forkï¼Œæ²¡æœ‰ä¸€ä¸ªå…±åŒçš„ï¼ŒæŒç»­æ¼”è¿›çš„å‚è€ƒå®ç°ï¼Œå¯¼è‡´ OpenCL ç¢ç‰‡åŒ–ï¼Œåè€Œå‰Šå¼±äº†å®ƒçš„å¯ç§»æ¤æ€§

Furthermore, because vendors held back differentiated features or isolated them into vendor-specific extensions, which exploded in number and fragmented OpenCL (and the derivatives), eroding its ability to be a unifying vendor-agnostic platform. Â These problems were exacerbated by weaknesses in OpenCLâ€™s compatibility and conformance tests. On top of that, it inherited all theÂ [â€œC++ problemsâ€ that we discussed before](https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good/#pythoncuda).
>  æ­¤å¤–ï¼Œç”±äºå‚å•†ä¿ç•™äº†å·®å¼‚åŒ–åŠŸèƒ½ï¼Œæˆ–è€…å°†åŠŸèƒ½éš”ç¦»åˆ°äº†ç‰¹å®šäºå‚å•†çš„æ‹“å±•ä¸­ï¼Œè¿™äº›æ‹“å±•æ•°é‡æ¿€å¢ï¼Œå¯¼è‡´ OpenCL ä¸¥é‡ç¢ç‰‡åŒ–ï¼Œå‰Šå¼±äº†å®ƒä½œä¸ºç»Ÿä¸€çš„ã€æ— å‚å•†ä¾èµ–å¹³å°çš„èƒ½åŠ›

Developers want stable, well-supported toolsâ€”but OpenCLâ€™s fragmentation, weak conformance tests, and inconsistent vendor support made it an exercise in frustration. One developer summed it up by saying thatÂ [**using OpenCL is â€œabout as comfortable as hugging a cactusâ€**](https://futhark-lang.org/blog/2024-07-17-opencl-cuda-hip.html#:~:text=it%20is%20because%20OpenCL%20has,comfortable%20as%20hugging%20a%20cactus)! Ouch.
>  å¼€å‘è€…æƒ³è¦é€‚åˆç”¨ç¨³å®šä¸”å¾—åˆ°è‰¯å¥½æ”¯æŒçš„å·¥å…·ï¼Œä½† OpenCL çš„ç¢ç‰‡åŒ–ã€è–„å¼±çš„ä¸€è‡´æ€§æµ‹è¯•ä»¥åŠå‚å•†æ”¯æŒçš„ä¸ä¸€è‡´ï¼Œä½¿å¾—å…¶å¼€å‘ä½“éªŒå¾ˆå·®

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c864ea4be5af3fe6970d7a_Cactus.jpeg)

One developer described using OpenCL asÂ ["about as comfortable as hugging a cactus."](https://futhark-lang.org/blog/2024-07-17-opencl-cuda-hip.html#:~:text=it%20is%20because%20OpenCL%20has,comfortable%20as%20hugging%20a%20cactus)

While OpenCL was struggling with fragmentation and slow committee-driven evolution, AI was rapidly advancingâ€”both in software frameworks and hardware capabilities. This created an even bigger gap between what OpenCL offered and what modern AI workloads needed.
>  OpenCL è¿˜åœ¨ä¸ºç¢ç‰‡åŒ–å’Œç¼“æ…¢çš„å§”å‘˜ä¼šé©±åŠ¨å‘å±•æŒ£æ‰æ—¶ï¼ŒAI æ­£åœ¨å¿«é€Ÿè¿›æ­¥ â€”â€” è½¯ä»¶æ¡†æ¶å’Œç¡¬ä»¶èƒ½åŠ›éƒ½æ˜¯å¦‚æ­¤
>  OpenCL æä¾›çš„å’Œç°ä»£ AI workload æƒ³è¦çš„å­˜åœ¨äº†æ›´å¤§çš„å·®å¼‚

## The Evolving Needs of AI Research and AI GPU Hardware
The introduction of TensorFlow and PyTorch kicked off a revolution in AI research - powered by improved infrastructure and massive influx of BigCo funding. This posed a major challenge for OpenCL. While it enabled GPU compute, it lacked the high-level AI libraries and optimizations necessary for training and inference at scale. Unlike CUDA, it had no built-in support for key operations like matrix multiplication, Flash Attention, or datacenter-scale training.
>  TensorFlow å’Œ PyTorch çš„å¼•å…¥å‘èµ·äº† AI ç ”ç©¶çš„é©å‘½
>  OpenCL è™½ç„¶æ”¯æŒ GPU è®¡ç®—ï¼Œä½†**ç¼ºä¹é«˜çº§çš„ AI åº“å’Œé’ˆå¯¹è§„æ¨¡åŒ–è®­ç»ƒå’Œæ¨ç†çš„ä¼˜åŒ–**ï¼Œå®ƒæ²¡æœ‰å¯¹å…³é”®è¿ç®—ä¾‹å¦‚çŸ©é˜µä¹˜ã€FlashAttention, æ•°æ®åº“è§„æ¨¡çš„è®­ç»ƒçš„å†…å»ºæ”¯æŒ

Cross-industry efforts to expand TensorFlow andÂ [PyTorch](https://github.com/pytorch/pytorch/issues/488)Â to use OpenCL quickly ran into fundamental roadblocks ([despite being obvious and with incredible demand](https://github.com/tensorflow/tensorflow/issues/22)). The developers who kept hugging the cactus soon discovered a harsh reality: portability to new hardware is meaningless if you canâ€™t unlock its full performance. Without a way to express portable hardware-specific enhancementsâ€”and with coopetition crushing collaborationâ€”progress stalled.
>  è¯•å›¾å°† TensorFlow å’Œ PyTorch æ‹“å±•ä»¥æ”¯æŒ OpenCL çš„åŠªåŠ›å¾ˆå¿«é‡åˆ°äº†æ ¹æœ¬æ€§éšœç¢
>  åšæŒä½¿ç”¨ OpenCL çš„å¼€å‘è€…å‘ç°äº†: å¦‚æœæ— æ³•å……åˆ†å‘æŒ¥ç¡¬ä»¶çš„æ€§èƒ½ï¼Œé‚£ä¹ˆå¯ç§»æ¤æ€§ä¹Ÿå°±æ¯«æ— æ„ä¹‰ï¼Œåœ¨æ²¡æœ‰ä¸€ç§å¯ä»¥è¡¨è¾¾å¯ç§»æ¤çš„ç¡¬ä»¶ç‰¹å®šçš„ä¼˜åŒ–ï¼Œå¹¶ä¸”åˆä½œç«äº‰é¢å‹åˆ¶äº†åä½œçš„æƒ…å†µä¸‹ï¼Œè¿›å±•åœæ»ä¸å‰

One glaring example? OpenCLÂ _still_Â doesnâ€™t provide standardizedÂ [support for Tensor Cores](https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good/#tensorcores) â€”the specialized hardware units that power efficient matrix multiplications in modern GPUs and AI accelerators. This means that using OpenCL often means a 5x to 10x slowdown in performance compared to using CUDA or other fragmented vendor native software. Â For GenAI, where compute costs are already astronomical,Â **a 5x to 10x slowdown isnâ€™t just inconvenientâ€”itâ€™s a complete dealbreaker**.
>  ä¸€ä¸ªæœ€æ˜æ˜¾çš„ä¾‹å­æ˜¯: OpenCL ä»ç„¶æ— æ³•æä¾›å¯¹ Tensor Cores çš„æ ‡å‡†åŒ–æ”¯æŒ
>  è¿™æ„å‘³ç€ä½¿ç”¨ OpenCL ä¼šæ¯”ä½¿ç”¨ CUDA æ…¢ 5-10 å€

### **NVIDIAâ€™s Strategic Approach with TensorFlow and PyTorch**
While OpenCL struggled under the weight of fragmented governance, NVIDIA took a radically different approachâ€”one that was tightly controlled, highly strategic, and ruthlessly effective, as weÂ [discussed earlier](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed). It actively co-designed CUDAâ€™s high-level libraries alongside TensorFlow and PyTorch, ensuring they always ran best on NVIDIA hardware. Since these frameworks were natively built on CUDA, NVIDIA had a massive head startâ€”and it doubled down by optimizing performance out of the box.
>  OpenCL åœ¨ç¢ç‰‡åŒ–çš„æ²»ç†ä¸‹ä¸¾æ­¥ç»´è‰°ï¼ŒNVIDIA åˆ™é‡‡å–ç›¸åçš„ç­–ç•¥ â€”â€” é«˜åº¦æ§åˆ¶çš„æ–¹æ³•
>  NVIDIA ç§¯æå’Œ TensorFlow ä»¥åŠ PyTorch ä¸€èµ·è®¾è®¡ CUDA é«˜å±‚åº“ï¼Œç¡®ä¿å®ƒä»¬å§‹ç»ˆåœ¨ NVIDIA ç¡¬ä»¶ä¸Šè¿è¡Œå¾—æœ€å¥½ï¼Œç”±äºè¿™äº›æ¡†æ¶æ˜¯åŸç”Ÿåœ°åŸºäº CUDA çš„ï¼ŒNVIDIA è·å¾—äº†å·¨å¤§çš„å…ˆå‘ä¼˜åŠ¿ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡ä¼˜åŒ–æ€§èƒ½æ¥å·©å›ºè¿™ä¸€ä¼˜åŠ¿

NVIDIA maintained a token OpenCL implementationâ€”but it was strategically hobbled (e.g., not being able to use TensorCores)â€”ensuring that a CUDA implementation would always be necessary. NVIDIAâ€™s continued and rising dominance in the industry put it on the path to ensure that the CUDA implementations would always be the most heavily invested in. Over time, OpenCL support faded, then vanishedâ€”while CUDA cemented its position as the undisputed standard.
>  NVIDIA ç»´æŒäº†ä¸€ä¸ªæœ‰é™çš„ OpenCL å®ç°ï¼Œä½†å®ƒè¢«æˆ˜ç•¥æ€§åœ°å‰Šå¼±äº† (ä¾‹å¦‚æ— æ³•ä½¿ç”¨ TensorCores)ï¼Œä»è€Œç¡®ä¿ CUDA çš„å®ç°å§‹ç»ˆæ˜¯å¿…è¦çš„

## What Can We Learn From These C++ GPU Projects?
The history above is well understood by those of us who lived through it, but the real value comes from learning from the past. Based on this, I believe successful systems must:

- ProvideÂ **a reference implementation**, not just a paper specification and â€œcompatibilityâ€ tests. A working, adoptable, and scalable implementation should define compatibilityâ€”not a PDF.
- HaveÂ **strong leadership and vision**Â driven by whoever maintains the reference implementation.
- Run withÂ **top performance on the industry leaderâ€™s hardware**â€”otherwise, it will always be a second-class alternative, not something that can unify the industry.
- **Evolve rapidly**Â to meet changing requirements, because AI research isnâ€™t stagnant, and AI hardware innovation is still accelerating.
- **Cultivate developer love**, by providing great usability, tools and fast compile times. Â Also, â€œC++ likeâ€ isnâ€™t exactly a selling point in AI!
- **Build an open community**, because without widespread adoption, technical prowess doesnâ€™t matter.
- **Avoid fragmentation**â€”a standard that splinters into incompatible forks canâ€™t provide an effective unification layer for software developers.

>  åŸºäºæ­¤ï¼Œç¬”è€…è®¤ä¸ºæˆåŠŸçš„ç³»ç»Ÿå¿…é¡»å…·å¤‡ä»¥ä¸‹å‡ ç‚¹:
>  - æä¾›ä¸€ä¸ªå‚è€ƒå®ç°ï¼Œè€Œä¸ä»…ä»…æ˜¯ç™½çš®ä¹¦è§„æ ¼å’Œå…¼å®¹æ€§æµ‹è¯•ï¼Œå…¼å®¹æ€§åº”è¯¥ç”±ä¸€ä¸ªå¯ç”¨çš„ã€å¯æ‹“å±•çš„å®ç°å®šä¹‰ï¼Œè€Œä¸æ˜¯ PDF
>  - æ‹¥æœ‰å¼ºçš„é¢†å¯¼å’Œæ„¿æ™¯ï¼Œç”±ç»´æŠ¤å‚è€ƒå®ç°çš„äººé©±åŠ¨
>  - åœ¨è¡Œä¸šé¢†å…ˆçš„ç¡¬ä»¶ä¸Šæ‹¥æœ‰é¡¶çº§çš„æ€§èƒ½ï¼Œå¦åˆ™æ°¸è¿œåªèƒ½æ˜¯ä¸€ä¸ªæ¬¡ç­‰çš„æ›¿ä»£æ–¹æ¡ˆ
>  - å¿«é€Ÿæ¼”è¿›ï¼Œä»¥æ»¡è¶³ä¸æ–­å˜åŒ–çš„éœ€æ±‚
>  - åŸ¹å…»å¼€å‘è€…çš„æƒ…æ„Ÿè®¤åŒï¼Œé€šè¿‡æä¾›è‰¯å¥½çš„æ˜“ç”¨æ€§ã€å·¥å…·å’Œå¿«é€Ÿç¼–è¯‘æ—¶é—´
>  - æ„å»ºä¸€ä¸ªå¼€æ”¾çš„ç¤¾åŒºï¼Œå› ä¸ºæ²¡æœ‰è¢«å¹¿æ³›é‡‡ç”¨ï¼ŒæŠ€æœ¯å®åŠ›ä¹Ÿæ— ä»è°ˆèµ·
>  - é¿å…ç¢ç‰‡åŒ–ï¼Œä¸€ä¸ªåˆ†è£‚æˆä¸å…¼å®¹åˆ†æ”¯çš„æ ‡å‡†ï¼Œæ— æ³•ä¸ºè½¯ä»¶å¼€å‘è€…æä¾›æœ‰æ•ˆçš„ç»Ÿä¸€å±‚

These are the fundamental reasons why I donâ€™t believe that committee efforts like OpenCL can ever succeed. Itâ€™s also why Iâ€™m even more skeptical of projects likeÂ [Intelâ€™s OneAPI](https://oneapi.io/)Â (nowÂ [UXL Foundation](https://uxlfoundation.org/)) that areÂ _notionally_Â open, but in practice, controlled by a single hardware vendor competing with all the others.
>  è¿™äº›æ˜¯ç¬”è€…è®¤ä¸ºåƒ OpenCL è¿™æ ·çš„å§”å‘˜ä¼šé¡¹ç›®ä¸èƒ½å¤ŸæˆåŠŸçš„æ ¹æœ¬åŸå› ï¼Œåƒ Intel çš„ OneAPI é¡¹ç›®ï¼Œè¡¨é¢ä¸Šæ˜¯å¼€æ”¾çš„ï¼Œä½†å®é™…ä¸Šç”±ä¸€å®¶ç¡¬ä»¶å‚å•†æ§åˆ¶ï¼Œå¹¶ä¸å…¶ä»–æ‰€æœ‰å‚å•†ç«äº‰

## What About AI Compilers?
At the same time that C++ approaches failed to unify AI compute for hardware makers, the AI industry faced a bigger challengeâ€”even using CUDA on NVIDIA hardware. How can we scale AI compute if humans have to write all the code manually? There are too many chips, too many AI algorithms, and too many workload permutations to optimize by hand.

As AIâ€™s dominance grew, it inevitably attracted interest from systems developers and compiler engineersâ€”including myself. In the next post, weâ€™ll dive into widely known â€œAI compilerâ€ stacks like TVM, OpenXLA, and MLIRâ€”examining what worked, what didnâ€™t, and what lessons we can take forward. Unfortunately, the lessons are not wildly different than the ones above:

> History may not repeat itself, but it does rhyme. - Mark Twain

See you next timeâ€”until then, may the FLOPS be with you! ğŸ‘¨â€ğŸ’»

-Chris

# 6 What about TVM, XLA, and AI compilers?
Site: https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers
Date: 12 March 2025

In the early days of AI hardware, writing high-performance GPU code was a manageableâ€”if tediousâ€”task. Engineers could handcraft CUDA kernels in C++ for the key operations they needed, and NVIDIA couldÂ [build these into libraries like cuDNN](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#wave)Â to drive their lock-in. But as deep learning advanced, this approach completely broke down.

Neural networks grew bigger, architectures became more sophisticated, and researchers demanded ever-faster iteration cycles. The number ofÂ [unique operators in frameworks like PyTorch](https://dev-discuss.pytorch.org/t/where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know/373)Â explodedâ€”now numbering in the thousands. Manually writing and optimizing each one for every new hardware target? Impossible.
>  éšç€ç¥ç»ç½‘ç»œå˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œæ¶æ„ä¹Ÿå˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œåƒ PyTorch è¿™æ ·çš„æ¡†æ¶ä¸­çš„ç‹¬ç‰¹ç®—å­æ•°é‡å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œç°åœ¨å·²ç»è¾¾åˆ°æ•°åƒä¸ª
>  ä¸ºæ¯ä¸ªæ–°ç¡¬ä»¶ç›®æ ‡æ‰‹åŠ¨ç¼–å†™å’Œä¼˜åŒ–æ¯ä¸€ä¸ªç®—å­æ˜¯ä¸å¯èƒ½çš„ä»»åŠ¡

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67d080d52d4f5fd7df68a17c_PyTorch_Operator_Count.jpg)

PyTorch operator count by version ([source](https://dev-discuss.pytorch.org/t/where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know/373))

This challenge forced a fundamental shift: instead of writing kernels by hand, what if we had a compiler that couldÂ _generate_Â them automatically? AI compilers emerged to solve this exact problem, marking a transformation from human-crafted CUDA to machine-generated, hardware-optimized compute.
>  è¿™ä¸€æŒ‘æˆ˜è¿«ä½¿äººä»¬è¿›è¡Œæ ¹æœ¬æ€§çš„è½¬å˜: ä¸å…¶æ‰‹åŠ¨ç¼–å†™ kernelï¼Œæˆ‘ä»¬èƒ½å¦**è®©ç¼–è¯‘å™¨è‡ªåŠ¨ç”Ÿæˆå®ƒä»¬**ï¼Ÿ
>  AI ç¼–è¯‘å™¨åº”è¿è€Œç”Ÿï¼Œä»¥è§£å†³è¿™ä¸ªå…·ä½“çš„é—®é¢˜ï¼Œæ ‡å¿—ç€ä»äººå·¥ç¼–å†™çš„ CUDA åˆ°æœºå™¨ç”Ÿæˆçš„ã€ç¡¬ä»¶ä¼˜åŒ–çš„è®¡ç®—çš„è½¬å˜

But as history has shown, building a successful compiler stack isnâ€™t just a technical challengeâ€”itâ€™s a battle over ecosystems, fragmentation, and control. So what worked? What didnâ€™t? And what can we learn from projects like TVM and OpenXLA?
>  ä½†æ­£å¦‚å†å²æ‰€è¡¨æ˜çš„é‚£æ ·ï¼Œæ„å»ºä¸€ä¸ªæˆåŠŸçš„ç¼–è¯‘å™¨æ ˆä¸ä»…ä»…æ˜¯æŠ€æœ¯ä¸Šçš„æŒ‘æˆ˜ â€”â€” å®ƒæ˜¯ä¸€åœºå…³äºç”Ÿæ€ç³»ç»Ÿã€ç¢ç‰‡åŒ–å’Œæ§åˆ¶æƒçš„æ–—äº‰
>  é‚£ä¹ˆï¼Œå“ªäº›åšæ³•æœ‰æ•ˆï¼Œå“ªäº›å¤±è´¥äº†ï¼Œæˆ‘ä»¬åˆèƒ½ä» TVM å’Œ OpenXLA ç­‰é¡¹ç›®ä¸­å¸å–å“ªäº›æ•™è®­

Letâ€™s dive in. ğŸš€

## What is an â€œAI Compilerâ€?
At its core, an AI compiler is a system that takes high-level operationsâ€”like those in PyTorch or TensorFlowâ€”and automatically transforms them into highly efficient GPU code. One of the most fundamental optimizations it performs is called â€œ**kernel fusion**.**â€**Â To see why this matters, letâ€™s consider a simple example:Â [multiplying two matrices](https://en.wikipedia.org/wiki/Matrix_multiplication)Â (â€matmulâ€) and then applying a ReLU (Rectified Linear Unit)Â [activation function](https://en.wikipedia.org/wiki/Activation_function). These are simple but important operations that occur in common neural networks.
>  ä»æ ¹æœ¬ä¸Šè¯´ï¼ŒAI ç¼–è¯‘å™¨æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œå®ƒå°†é«˜çº§æ“ä½œ â€”â€” ä¾‹å¦‚ PyTorch æˆ– TensorFlow ä¸­çš„æ“ä½œ â€”â€” è‡ªåŠ¨è½¬æ¢ä¸ºé«˜æ•ˆçš„ GPU ä»£ç 
>  å®ƒæ‰§è¡Œçš„æœ€åŸºæœ¬çš„ä¼˜åŒ–ä¹‹ä¸€ä¸º kernel fusion
>  ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­: å¯¹ä¸¤ä¸ªçŸ©é˜µè¿›è¡Œç›¸ä¹˜ï¼Œç„¶ååº”ç”¨ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°

### **NaÃ¯ve approach: Two separate kernels**
The most straightforward (but inefficient) way to do this is to perform matrix multiplication first, store the result in memory, then load it again to apply ReLU.
>  æœ€ç®€å•çš„æ–¹å¼æ˜¯å…ˆæ‰§è¡Œ GEMMï¼Œå°†ç»“æœå­˜å…¥å†…å­˜ï¼Œç„¶åå† loadï¼Œåº”ç”¨ ReLU

```python
# NaÃ¯ve matmul implementation for clarity.
def matmul(A, B):
    # Initialize result matrix to zero.
    C = [[0] * N for _ in range(N)]
    for i in range(N):
        for j in range(N):
            sum = 0
            for k in range(N):
                # Matmul sums the dot product of rows and columns.
                sum += A[i][k] * B[k][j]
            C[i][j] = sum # store one output value
    return C

# ReLU clamp negatives to zero with the "max" function.
def relu(C):
    # Allocate result array.
    result = [[0] * N for _ in range(N)]
    for i in range(N):
        for j in range(N):
            # This loads from memory, does a trivial max(0, x) operation,
            # then stores the result.
            result[i][j] = max(0, C[i][j])
    return result

C = matmul(A, B) # Compute matrix multiplication first
D = relu(C)      # Then apply ReLU separately.
```

These operations are extremely familiar to engineers that might write a CUDA kernel (though remember thatÂ [CUDA uses unwieldy C++ syntax!](https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good#pythoncuda)), and there are many tricks used for efficient implementation.

While the above approach is simple and modular, executing operations like this isÂ **extremely slow**Â because it writes the entire matrixÂ `C`Â to memory afterÂ `matmul()`, then reads it back again inÂ `relu()`. This memory traffic dominates performance, especially on GPUs, where memory access is more expensive than local compute.
>  è¿™æ ·çš„æ–¹æ³•ç®€å•ä¸”æ¨¡å—åŒ–ï¼Œä½†å®é™…ä¸Šæå…¶ç¼“æ…¢ï¼Œå› ä¸ºåœ¨ `matmul()` ä¹‹åï¼Œæ•´ä¸ªçŸ©é˜µ `C` éƒ½ä¼šè¢«å†™å…¥å†…å­˜ï¼Œç„¶ååœ¨ `relu()` ä¸­å†æ¬¡è¯»å–å›æ¥
>  è¿™ç§å†…å­˜è®¿é—®æµé‡ä¼šä¸¥é‡å½±å“æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ GPU ä¸Šï¼Œå› ä¸ºå†…å­˜è®¿é—®æˆæœ¬è¿œé«˜äºæœ¬åœ°è®¡ç®—

### **Fused kernel: One pass, no extra memory traffic**
The solution for this is simple: we can â€œ**fuseâ€**Â these two operations into a single kernel, eliminating redundant memory access. Instead of storing C afterÂ `matmul()`, we applyÂ `relu()`Â _immediately_Â inside the same loop:
>  è¦æ¶ˆé™¤è¿™ç§å†—ä½™çš„å†…å­˜æ¬è¿ä¹Ÿå¾ˆç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸¤ä¸ªç®—å­èåˆä¸ºä¸€ä¸ª kernel
>  ä¾‹å¦‚ï¼Œå¯¹äºä¸Šè¿°ä»£ç ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨ç›¸åŒçš„å¾ªç¯ä¸­åº”ç”¨ `relu()`

```python
# Fused kernel: Matrix multiplication + ReLU in one pass
def fused_matmul_relu(A, B):
    # Initialize result matrix to zero.
    C = [[0] * N for _ in range(N)]
    for i in range(N):
        for j in range(N):
            sum = 0
            for k in range(N):
                sum += A[i][k] * B[k][j]  # Compute matmul
                
            # Apply ReLU in the same loop!
            C[i][j] = max(0, sum)
    return C  # Only one read/write cycle

# Compute in a single pass, no extra memory.
C = fused_matmul_relu(A, B)
```

While the benefit of this transformation varies by hardware and matrix size, the results can be profound: sometimes 2x better performance! Why is this the case? By fusing the operations:

âœ… WeÂ **eliminate an extra memory write/read**, reducing pressure on memory bandwidth.

âœ… WeÂ **keep data in registers or shared memory**, avoiding slow global memory access.

âœ… WeÂ **reduce memory usage and allocation/deallocation overhead**, since the intermediate buffer has been removed.

>  è¿™æ ·ç®€å•çš„ä¼˜åŒ–æœ‰æ—¶å¯ä»¥å¸¦æ¥ä¸¤å€çš„æ€§èƒ½æå‡ï¼Œé€šè¿‡èåˆç®—å­:
>  æˆ‘ä»¬æ¶ˆé™¤äº†é¢å¤–çš„å†…å­˜è¯»å†™ï¼Œå‡å°‘äº†å†…å­˜å¸¦å®½çš„å‹åŠ›
>  æˆ‘ä»¬å°†æ•°æ®ä¿å­˜åœ¨å¯„å­˜å™¨æˆ–å…±äº«å†…å­˜ï¼Œé¿å…ç¼“æ…¢çš„å…¨å±€å†…å­˜è®¿é—®
>  æˆ‘ä»¬å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œåˆ†é…/é‡Šæ”¾å¼€é”€ï¼Œå› ä¸ºä¸éœ€è¦ä¸­é—´ç¼“å†²åŒº

This is the simplest example of kernel fusion: There are many more powerful transformations, and AI kernel engineers have always pushed the limits of optimization ([learn more](https://horace.io/brrr_intro.html)). With GenAI driving up compute demand, these optimizations are more critical than ever.

### Great performance, but an exponential complexity explosion!
While these sorts of optimizations can be extremely exciting and fun to implement for those who are chasing low cost and state of the art performance, there is a hidden truth:Â **this approach doesnâ€™t scale**.
>  ä½†è¿™ç±»ä¼˜åŒ–çš„ä¸€ä¸ªéšè—çš„çœŸç›¸æ˜¯: è¿™ç§æ–¹æ³•æ— æ³•æ‹“å±•

Modern machine learning toolkits include hundreds of different â€œoperationsâ€ like matmul, convolution, add, subtract, divide, etc., as well as dozens ofÂ [activation functions](https://en.wikipedia.org/wiki/Activation_function)Â beyond ReLU. Each neural network needs them to be combined in different ways: this causes an explosion in the number of permutations that need to be implemented (hundreds of operations x hundreds of operations = too many to count). NVIDIAâ€™s libraries like cuDNN provide a fixed list of options to choose from, without generality to new research.
>  ç°åœ¨æœºå™¨å­¦ä¹ å·¥å…·åŒ…åŒ…å«äº†æ•°ç™¾ç§ä¸åŒçš„ operationsï¼Œä¾‹å¦‚ matmul, convolution, add, substract, divideï¼Œä»¥åŠæ•°åç§æ¿€æ´»å‡½æ•°
>  æ¯ç§ç¥ç»ç½‘ç»œéœ€è¦ä»¥ä¸åŒçš„æ–¹å¼ç»„åˆè¿™äº› operationsï¼Œè¿™ä¼šå¯¼è‡´éœ€è¦å®ç°çš„ç»„åˆæ–¹å¼å‘ˆæŒ‡æ•°çº§å¢é•¿
>  NVIDIA çš„åº“ä¾‹å¦‚ cuDNN æä¾›çš„é€‰é¡¹æ˜¯å›ºå®šçš„ï¼Œæ— æ³•é€‚åº”æ–°çš„ç ”ç©¶éœ€æ±‚

Furthermore, there are other axes of explosion as well: weâ€™ve seen an explosion of new numerics datatypes (e.g. â€œfloat8â€), and of course, there is also an explosion of the kind of hardware that AI should support.
>  æ­¤å¤–ï¼Œçˆ†ç‚¸å¼å¢é•¿è¿˜ä½“ç°åœ¨å…¶ä»–æ–¹é¢: æ–°çš„æ•°æ®ç±»å‹ã€AI æ‰€éœ€æ”¯æŒçš„ç¡¬ä»¶ç±»å‹

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67d1a39bfe6f96be5eb1b802_DCP6-Axis.png)

Just three dimensions of complexity

## Early AI compilers: TVM
There are many AI compilers, but one of the earliest and most successful is TVM - the â€œ[Tensor Virtual Machine](https://tvm.apache.org/)â€. This system took models from TensorFlow/PyTorch and optimized them for diverse hardware, i.e. by applying kernel fusion automatically. This project started at the University of Washington byÂ [Tianqi Chen](https://tqchen.com/)Â andÂ [Professor Luis Ceze](https://www.cs.washington.edu/people/faculty/luis-ceze/)Â in about 2016, and delivered a number of innovative results and performance wins described inÂ [the 2018 paper](https://arxiv.org/abs/1802.04799)Â that outlines the TVM architecture. It was open sourced and incorporated into the Apache project.
>  æœ€æ—©çš„ä¸”æœ€æˆåŠŸçš„ AI ç¼–è¯‘å™¨ä¹‹ä¸€æ˜¯ TVM
>  è¿™ä¸ªç³»ç»Ÿä» TensorFlow/PyTorch ç§è·å–æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹å„ç§ç¡¬ä»¶è¿›è¡Œä¼˜åŒ–ï¼Œå³é€šè¿‡è‡ªåŠ¨åº”ç”¨ kernel fusion æ¥æå‡æ€§èƒ½

Across its journey, TVM has been adopted by hardware makers (including public contributions from companies like ARM, Qualcomm, Facebook, Intel, and many others) across embedded, DSP, and many other applications. TVM's core contributors later founded OctoAI,Â [which NVIDIA acquired in late 2024](https://www.forbes.com/sites/janakirammsv/2024/09/30/nvidia-acquires-octoai-to-dominate-enterprise-generative-ai-solutions/) â€”giving it control over many of the original TVM developers and, potentially, the project's future.
>  åœ¨å‘å±•è¿‡ç¨‹ç§ï¼ŒTVM è¢«è®¸å¤šç¡¬ä»¶å‚å•†é‡‡ç”¨ï¼Œåº”ç”¨äºåµŒå…¥å¼è®¾å¤‡ã€æ•°å­—ä¿¡å·å¤„ç†å™¨ä»¥åŠå…¶ä»–å¤šç§åº”ç”¨åœºæ™¯
>  TVM çš„æ ¸å¿ƒè´¡çŒ®è€…åæ¥åˆ›ç«‹çš„ OctoAIï¼Œè¯¥å…¬å¸åæ¥è¢« NVIDIA æ”¶è´­ï¼Œè¿™ä½¿å¾— NVIDIA æŒæ¡äº† TVM çš„è®¸å¤šåŸå§‹å¼€å‘è€…ï¼Œå¹¶å¯èƒ½æŒæ§äº†è¯¥é¡¹ç›®æœªæ¥çš„å‘å±•æ–¹å‘


![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67d1b7a1fdbc1f76d43f8a15_NNVM_Compiler_Stack_Diagram.png)

[Source:Â Apache TVM](https://tvm.apache.org/2017/10/06/nnvm-compiler-announcement)

TVM is an important step for the AI compiler industry, but what can we learn from it? Here are my key takeaways.Â **_Disclaimer_**: although TVM was a user of LLVM and I had great interest in it, I was never directly involved. This is my perspective as an outsider.
>  ç¬”è€…å…³äº TVM çš„è§‚ç‚¹å¦‚ä¸‹

###### **Wasnâ€™t able to deliver performance on modern hardware**
TVM struggled to deliver peak performance on modern AI hardware, particularly as GPUs evolved toward TensorCores and other specialized acceleration. It added support over time but was often late and failed to fully unlock performance. As such, it suffered from one ofÂ [the same problems as OpenCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives#evolvingneeds): You canâ€™t deliver performance if you canâ€™t unlock the hardware.
>  TVM åœ¨ç°ä»£ç¡¬ä»¶ä¸Šéš¾ä»¥å®ç°æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ GPU å‘ç€ TensorCores å’Œå…¶ä»–ä¸“ç”¨åŠ é€Ÿæ–¹å‘å‘å±•æ—¶
>  è™½ç„¶å®ƒéšç€æ—¶é—´æ¨ç§»å¢åŠ äº†å¯¹è¿™äº›ç¡¬ä»¶çš„æ”¯æŒï¼Œä½†é€šå¸¸éƒ½æ¯”è¾ƒæ»åï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†é‡Šæ”¾æ€§èƒ½
>  å› æ­¤ï¼Œå®ƒä¹Ÿé¢ä¸´äº†ä¸ OpenCL ç›¸åŒçš„é—®é¢˜ä¹‹ä¸€: æ— æ³•è§£é”ç¡¬ä»¶çš„æœ€ä¼˜æ€§èƒ½

###### **Fragmentation driven by conflicting commercial interests**
Unlike OpenCL, TVM wasn't just a specificationâ€”it was anÂ **actual implementation**. This made it far more useful out of the box and attracted hardware vendors. But fragmentation still reared its head: vendors forked the code, made incompatible changes, and struggled to stay in sync, slowing progress. This led to friction executing architectural changes (because downstream vendors complained about their forks being broken), which slowed development.
>  å’Œ OpenCL ä¸åŒï¼ŒTVM ä¸ä»…ä»…æ˜¯ä¸€ä¸ªè§„èŒƒï¼Œå®ƒæ˜¯ä¸€ä¸ªå®é™…çš„å®ç°
>  ä½†ç¢ç‰‡åŒ–é—®é¢˜ä»ç„¶å‡ºç°: å‚å•†ä»¬ fork ä»£ç ï¼Œåšå‡ºä¸å…¼å®¹çš„æ›´æ”¹ï¼Œå¹¶éš¾ä»¥ä¿æŒåŒæ­¥ï¼Œä»è€Œå‡ç¼“äº†è¿›å±•
>  è¿™å¯¼è‡´äº†åœ¨æ‰§è¡Œæ¶æ„å˜æ›´æ—¶äº§ç”Ÿæ‘©æ“¦ (å› ä¸ºä¸‹æ¸¸å‚å•†æŠ±æ€¨å®ƒä»¬çš„ fork ç‰ˆæœ¬è¢«ç ´å)ï¼Œè¿›è€Œå‡ç¼“äº†å¼€å‘é€Ÿåº¦

###### **Agility is required to keep up with rapid AI advances**
A final challenge is that TVM was quite early, but the pace of AI innovation around it was rapid. TensorFlow and PyTorch rapidly evolved due to backing by huge companies like Google, Meta, andÂ [NVIDIA](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#genaisurge), improving their performance and changing the baselines that TVM compared against. The final nail in the coffin, though, was GenAI, which changed the game. TVM was designed for â€œTradAIâ€: a set of relatively simple operators that needed fusion, but GenAI has large and complex algorithms deeply integrated with the hardwareâ€” [things like FlashAttention3.](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#genaisurge)Â TVMÂ fell progressively behind as the industry evolved.
>  ä¸€ä¸ªæœ€ç»ˆçš„æŒ‘æˆ˜æ˜¯: TVM é¢ä¸–è¾ƒæ—©ï¼Œä½†å›´ç»• AI æŠ€æœ¯åˆ›æ–°å´éå¸¸å¿«
>  ç”±äºæœ‰ Google, Meta, NVIDIA ç­‰å¤§å…¬å¸çš„æ”¯æŒï¼ŒTensorFlow å’Œ PyTorch è¿…é€Ÿæ¼”è¿›ï¼Œæå‡å®ƒä»¬çš„æ€§èƒ½å¹¶æ”¹å˜äº† TVM éœ€è¦å¯¹æ¯”çš„ baseline
>  ç„¶è€Œï¼ŒçœŸæ­£è®© TVM èµ°å‘è¡°è´¥çš„æ˜¯ GenAI çš„å…´èµ·ï¼ŒTVM æ˜¯é’ˆå¯¹ä¼ ç»Ÿ AI è®¾è®¡çš„: å³ä¸€ç»„ç›¸å¯¹ç®€å•çš„ operators è¿›è¡Œ fusionï¼Œè€Œ GenAI æ¶‰åŠçš„æ˜¯ä¸ç¡¬ä»¶æ·±åº¦é›†æˆçš„å¤§è§„æ¨¡å¤æ‚ç®—æ³•ï¼Œä¾‹å¦‚ FlashAttention3
>  éšç€è¡Œä¸šçš„å‘å±•ï¼ŒTVM é€æ¸è½å

Less strategically important (but still material), TVM also has technical problems, e.g. really slow compile times due to excessive auto-tuning. All of these together contributed toÂ [project activity slowing](https://github.com/apache/tvm/graphs/contributors).
>  æ­¤å¤–ï¼ŒTVM ä¹Ÿæœ‰æŠ€æœ¯é—®é¢˜ï¼Œä¾‹å¦‚è¿‡åº¦è‡ªåŠ¨è°ƒä¼˜å¸¦æ¥çš„ç¼“æ…¢ç¼–è¯‘
>  æ‰€æœ‰è¿™äº›é—®é¢˜å¸¦æ¥äº†é¡¹ç›®æ´»åŠ¨çš„æ”¾ç¼“

Today, NVIDIA now employs many of its original leaders, leaving its future uncertain. Meanwhile, Google pursued its own vision with OpenXLA...

## The XLA compiler from Google: Two different systems under one name
Unlike TVM, which started as an academic project, XLA was built within Googleâ€”one of the most advanced AI companies, with deep pockets and a vested interest in AI hardware. Google developed XLA to replace CUDA for its (now successful)Â [TPU hardware](https://cloud.google.com/tpu/docs/intro-to-tpu), ensuring tight integration and peak performance for its own AI workloads. I joined Google Brain in 2017 to help scale TPUs (and XLA) from an experimental project into the world's second-most successful AI accelerator (behind NVIDIA).
>  TVM ä»¥å­¦æœ¯é¡¹ç›®å¯åŠ¨ï¼ŒXLA åˆ™åœ¨ Google å†…æ„å»º
>  Google å¼€å‘äº† XLA æ¥ä¸ºå®ƒçš„ TPU ç¡¬ä»¶æ›¿ä»£ CUDAï¼Œç¡®ä¿å…¶ AI workloads èƒ½å¤Ÿå®ç°ç´§å¯†é›†æˆå’Œæœ€ä½³æ€§èƒ½
>  ç¬”è€…å¸®åŠ© Google å°† TPU, XLA ä»ä¸€ä¸ªè¯•éªŒæ€§é¡¹ç›®æ‹“å±•åˆ°ä¸–ç•Œä¸Šç¬¬äºŒæˆåŠŸçš„ AI åŠ é€Ÿå™¨

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67d1a82617d9ef77df33fea4_TPU.jpg)

Google TPU ([source](https://blog.google/products/google-cloud/google-cloud-offer-tpus-machine-learning/))

Google had hundreds of engineers working on XLA (depending on how you count), and it evolved rapidly. Google added CPU and GPU support, and eventually formed the OpenXLA foundation. XLA is used as the AI compiler foundation for several important hardware projects, includingÂ [AWS Inferentia/Trainium](https://opensource.googleblog.com/2024/12/a-robust-open-ecosystem-accelerating-ai-infrastructure.html)Â among others.
>  Google æœ‰æ•°ç™¾åå·¥ç¨‹å¸ˆå‚ä¸ XLAï¼Œå¹¶ä¸”å®ƒå‘å±•å¾—éå¸¸è¿…é€Ÿ
>  Google é€æ¸å¢åŠ äº†å¯¹ CPU å’Œ GPU çš„æ”¯æŒï¼Œå¹¶æœ€ç»ˆæˆç«‹äº† OpenXLA åŸºé‡‘ä¼šï¼ŒXLA è¢«ç”¨ä½œå¤šä¸ªé‡è¦ç¡¬ä»¶é¡¹ç›®çš„åŸºç¡€ AI ç¼–è¯‘å™¨

Beyond code generation, one of the biggest achievements and contributions of XLA is its ability to handleÂ [large scale machine learning models](https://jax-ml.github.io/scaling-book/). At extreme scale, the ability to train with many thousands of chips becomes essential. Today, the largest practical models are starting to require advanced techniques to partition them across multiple machinesâ€”XLA developed clean and simple approaches that enable this.
>  é™¤äº†ä»£ç ç”Ÿæˆå¤–ï¼ŒXLA æœ€å¤§çš„æˆå°±å’Œè´¡çŒ®ä¹‹ä¸€æ˜¯å®ƒå¤„ç†å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›
>  åœ¨æç«¯çš„è§„æ¨¡ä¸‹ï¼Œä»¥æ•°åƒå°èŠ¯ç‰‡è®­ç»ƒå˜å¾—ååˆ†é‡è¦ï¼Œå¦‚ä»Šï¼Œæœ€å¤§çš„æ¨¡å‹å¼€å§‹éœ€æ±‚å…ˆè¿›çš„æŠ€æœ¯å°†å®ƒä»¬åˆ’åˆ†åˆ°å¤šå°æœºå™¨ä¸Š â€”â€” XLA å¼€å‘äº†ç®€å•ä¸”æ¸…æ™°çš„æ–¹æ¡ˆæ¥å®ç°è¿™ä¸€ç‚¹

Given all this investment, why donâ€™t leading projects like PyTorch and vLLM run GPUs with XLA? The answer is that XLA is two different projects with a conflated brand, incentive structure challenges for their engineers, governance struggles, and technical problems that make it impractical.
>  é‰´äºå¦‚æ­¤å¤šçš„æŠ•å…¥ï¼Œä¸ºä»€ä¹ˆåƒ PyTorch å’Œ vLLM è¿™æ ·çš„é¢†å…ˆé¡¹ç›®ä¸ä½¿ç”¨ XLA æ¥è¿è¡Œåœ¨ GPU ä¸Šï¼Ÿ
>  ç­”æ¡ˆæ˜¯ XLA å®é™…ä¸Šæ˜¯ä¸¤ä¸ªä¸åŒçš„é¡¹ç›®ï¼Œä½†å“ç‰Œè¢«æ··æ·†äº†ï¼Œå·¥ç¨‹å¸ˆé¢ä¸´æ¿€åŠ±ç»“æ„ä¸Šçš„æŒ‘æˆ˜ã€æ²»ç†ä¸Šçš„å›°éš¾ï¼Œä»¥åŠä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œä½¿å¾—å®ƒåœ¨å®è·µä¸­éš¾ä»¥ä½¿ç”¨

###### **Google uses XLA-TPU, but OpenXLA is for everyone else**
The most important thing to understand is that XLA exists in two forms: 1) the internal, closed source XLA-TPU compiler that powers Googleâ€™s AI infrastructure, and 2) OpenXLA, the public project for CPUs and GPUs. These two share some code (â€œ[StableHLO](https://openxla.org/stablehlo)â€) but the vast majority of the code (and corresponding engineering effort) in XLA is Google TPU specificâ€”closed and proprietary, and not used on CPUs or GPUs. XLA on GPU today typically calls into standard CUDA libraries to get performance. ğŸ¤·
>  è¦ç†è§£çš„æœ€é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼ŒXLA æœ‰ä¸¤ç§å½¢å¼:
>  1. å†…éƒ¨çš„ã€é—­æºçš„ XLA-TPU ç¼–è¯‘å™¨
>  2. OpenXLAï¼Œé¢å‘ CPU å’Œ GPU çš„å¼€æºé¡¹ç›®
>  è¿™ä¸¤è€…å…±äº«ä¸€äº›ä»£ç  (StableHLO)ï¼Œä½† XLA ä¸­ç»å¤§å¤šæ•°ä»£ç ä»¥åŠå·¥ç¨‹ä¸Šçš„åŠªåŠ›éƒ½æ˜¯é’ˆå¯¹ Google TPU çš„ï¼Œè¿™äº›ä»£ç æ˜¯é—­æºçš„ï¼Œä¸ä¼šç”¨äº CPU æˆ– GPU
>  å¦‚ä»Š GPU ä¸Šçš„ XLA é€šå¸¸ä¼šä½¿ç”¨æ ‡å‡†çš„ CUDA åº“æ¥è·å¾—æ€§èƒ½

This leads to significant incentive structure problemsâ€”Google engineers might want to build a great general-purpose AI compiler, but their paychecks are tied to making TPUs go brrr. Leadership has little incentive to optimize XLA for GPUs or alternative hardwareâ€”itâ€™s all about keeping TPUs competitive. In my experience, XLA has never prioritized a design change that benefits other chips if it risks TPU performance.
>  è¿™å¯¼è‡´äº†ä¸¥é‡çš„æ¿€åŠ±ç»“æ„é—®é¢˜ â€”â€” Google å·¥ç¨‹å¸ˆå¯èƒ½å¸Œæœ›æ‰“é€ ä¸€ä¸ªä¼˜ç§€çš„é€šç”¨ç›®çš„ AI ç¼–è¯‘å™¨ï¼Œä½†ä»–ä»¬çš„è–ªèµ„ä»…å’Œè®© TPU é«˜é€Ÿè¿è¡ŒæŒ‚é’©
>  ç®¡ç†å±‚æ²¡æœ‰åŠ¨åŠ›å»ä¼˜åŒ–é™¤äº† TPU ä»¥å¤–çš„å…¶ä»–ç¡¬ä»¶ï¼Œè¿™ä½¿å¾— TPU èƒ½å¤Ÿä¿æŒç«äº‰åŠ›
>  XLA ä»æœªè€ƒè™‘è¿‡é‚£äº›å¯¹å…¶ä»–èŠ¯ç‰‡æœ‰ç›Šä½†å¯èƒ½å½±å“ TPU æ€§èƒ½çš„è®¾è®¡å˜æ›´

The result? A compiler that works great for TPUs but falls short elsewhere.
>  ç»“æœå°±æ˜¯ï¼Œè¿™ä¸ªç¼–è¯‘å™¨åœ¨ TPU ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¶ä»–åœ°æ–¹å´æ˜¾å¾—ä¸è¶³

###### **Governance of OpenXLA**
XLA was released early as an open source but explicitly Google-controlled project. Googleâ€™s early leadership in AI with TensorFlow got it adopted by other teams around the industry. In March 2023, the project was renamed to OpenXLA with anÂ [announcement about independence](https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html).

Despite this rebranding, Google still controls OpenXLA (seen in itsÂ [governance structure](https://openxla.org/stablehlo/governance)), and doesnâ€™t seem to be investing: there areÂ [declining community contributions](https://github.com/openxla/community/graphs/contributors), and the OpenXLAÂ [X account](https://x.com/openxla)Â has been inactive since 2023.
>  å°½ç®¡è¿›è¡Œäº†å“ç‰Œé‡å¡‘ï¼Œä½† Google ä»ç„¶æ§åˆ¶ç€ OpenXLAï¼Œå¹¶ä¸”ä¼¼ä¹æ²¡æœ‰æŠ•å…¥å¤ªå¤šèµ„æº: ç¤¾åŒºè´¡çŒ®é‡æ­£åœ¨å‡å°‘ï¼Œä¸” OpenXLA çš„ X è´¦å·è‡ª 2023 å¹´ä»¥æ¥å°±ä¸€ç›´å¤„äºä¸æ´»è·ƒçŠ¶æ€

###### **Technical challenges with XLA**
Like TVM, XLA was designed around a fixed set of predefined operators ([StableHLO](https://openxla.org/stablehlo)). This approach worked well for traditional AI models like ResNet-50 in 2017, but struggles with modern GenAI workloads, which require more flexibility in datatypes, custom kernels, and hardware-specific optimizations. This is a critical problem today, when modern GenAI algorithms require innovation in datatypes (see the chart below), or as DeepSeek showed us,Â [at the hardware level](https://github.com/deepseek-ai/DeepGEMM)Â and inÂ [novel communication strategies](https://github.com/deepseek-ai/DeepEP).
>  ä¸ TVM ä¸€æ ·ï¼ŒXLA æœ€åˆä¹Ÿæ˜¯å›´ç»•ä¸€ç»„é¢„å®šä¹‰çš„ operators (StableHLO) è®¾è®¡çš„ï¼Œè¿™ç§æ–¹æ³•åœ¨ RestNet-50 è¿™æ ·çš„ä¼ ç»Ÿ AI æ¨¡å‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç°ä»£ç”Ÿæˆå¼ AI çš„å·¥ä½œè´Ÿè½½ä¸Šå´æ˜¾å¾—åŠ›ä¸ä»å¿ƒï¼Œå› ä¸ºç”Ÿæˆå¼ AI çš„å·¥ä½œè´Ÿè½½è¦æ±‚åœ¨æ•°æ®ç±»å‹ã€è‡ªå®šä¹‰ kernel å’Œç¡¬ä»¶ç‰¹å®šä¼˜åŒ–æ–¹é¢å…·å¤‡æ›´é«˜çš„çµæ´»æ€§
>  å¦‚ä»Šï¼Œè¿™å·²ç»æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜: ç°ä»£ GenAI ç®—æ³•è¦æ±‚åœ¨æ•°æ®ç±»å‹ä¸Šè¿›è¡Œåˆ›æ–°ï¼Œæˆ–è€…å¦‚ DeepSeek æ‰€å±•ç¤ºçš„é‚£æ ·ï¼Œåœ¨ç¡¬ä»¶å±‚é¢å’Œæ–°å‹é€šä¿¡ç­–ç•¥ä¸Šå®ç°çªç ´

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67d1a29994becf9ea18e3a9e_Quantization_vLLM.png)

Datatypes supported in vLLM 0.7 by hardware type ([source](https://docs.vllm.ai/en/stable/features/quantization/supported_hardware.html))

As a consequence, XLA (like TVM) suffers from being left behind by GenAI: today much of the critical workloads are written in experimentalÂ [systems like Pallas](https://docs.jax.dev/en/latest/pallas/index.html)Â that bypass the XLA compiler, even on TPUs. The core reason is that in its efforts to simplify AI compilation, XLA abstracted away too much of the hardware. This worked for early AI models, but GenAI demands fine-grained control over acceleratorsâ€”something XLA simply wasnâ€™t built to provide. And so, just like TVM, itâ€™s being left behind.
>  ç»“æœå°±æ˜¯: XLA å’Œ TVM ä¸€æ ·ï¼Œä¹Ÿå› ä¸º GenAI çš„å‘å±•è€Œè¢«ç”©åœ¨åé¢: å¦‚ä»Šï¼Œè®¸å¤šå…³é”®çš„ workload éƒ½ä½¿ç”¨è¯•éªŒæ€§ç³»ç»Ÿä¾‹å¦‚ Pallas ç¼–å†™ï¼Œè¿™äº›ç³»ç»Ÿç»•è¿‡äº† XLA ç¼–è¯‘å™¨ï¼Œç”šè‡³åœ¨ TPU ä¸Šä¹Ÿæ˜¯å¦‚æ­¤
>  æ ¹æœ¬åŸå› åœ¨äºï¼Œä¸ºäº†ç®€åŒ– AI ç¼–è¯‘ï¼ŒXLA æŠ½è±¡æ‰äº†å¤ªå¤šç¡¬ä»¶ç»†èŠ‚ï¼Œè¿™åœ¨æ—©æœŸçš„ AI æ¨¡å‹ä¸­æ˜¯å¯è¡Œçš„ï¼Œä½† GenAI è¦æ±‚å¯¹åŠ é€Ÿå™¨è¿›è¡Œç»†ç²’åº¦çš„æ§åˆ¶ï¼Œè€Œè¿™æ­£æ˜¯ XLA æ ¹æœ¬æ²¡æœ‰è®¾è®¡æ¥æä¾›çš„

## Lessons learned from TVM and XLA
I take pride in the technical accomplishments we proved in XLA-TPU: XLA supported many generational research breakthroughs, including the invention of the transformer, countless model architectures, and research and product scaling that isnâ€™t seen anywhere else. It is clearly the most successful non-NVIDIA training and inference hardware that exists, and powers Googleâ€™s (many) leading AI products and technologies. Though I know less about it, I have a lot of respect for TVMâ€™s contribution to compiler research, autotuning and powering many early AI systems.
>  XLA-TPU æ”¯æŒäº†è®¸å¤šä»£çš„ç ”ç©¶çªç ´ï¼ŒåŒ…æ‹¬ transformer ç­‰æ— æ•°æ¨¡å‹æ¶æ„ï¼Œæ¯«æ— ç–‘é—®ï¼Œå®ƒæ˜¯å½“å‰æœ€æˆåŠŸçš„é NVIDIA è®­ç»ƒå’Œæ¨ç†ç¡¬ä»¶

That said, there is a lot to learn from both projects together. Going down theÂ [list of lessons learned from OpenCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives/#lessons):
>  ä» XLA å’Œ TVM ä¸­å­¦ä¹ åˆ°çš„æ•™è®­å¯ä»¥å’Œ OpenCL ä¸­å­¦ä¹ åˆ°çš„æ•™è®­æ¯”è¾ƒ:

- **â€œProvide a reference implementationâ€:**Â They both provide a useful implementation, not just a technical specification like OpenCL. ğŸ‘
>  æä¾›ä¸€ä¸ªå‚è€ƒå®ç°: äºŒè€…éƒ½æä¾›äº†æœ‰ç”¨çš„å®ç°ï¼Œè€Œä¸æ˜¯ OpenCL ä¸€æ ·çš„æŠ€æœ¯è§„èŒƒ

- â€œ**Have**Â **strong leadership and visionâ€:**Â They have defined leadership teams and a vision behind them ğŸ‘. However, OpenXLAâ€™s vision isnâ€™t aligned with hardware teams that want to adopt it. And like many Google projects, itsÂ [long-term prospects are uncertain](https://killedbygoogle.com/), making it risky to depend on. ğŸ‘
>  äºŒè€…éƒ½æœ‰æ˜ç¡®çš„é¢†å¯¼å›¢é˜Ÿå’ŒèƒŒåçš„ç†å¿µï¼Œç„¶è€Œ OpenXLA çš„æ„¿æ™¯ä¸å¸Œæœ›é‡‡ç”¨å®ƒçš„ç¡¬ä»¶å›¢é˜Ÿä¸ä¸€è‡´ï¼Œå¹¶ä¸”ï¼Œåƒè®¸å¤š Google é¡¹ç›®ä¸€æ ·ï¼Œå…¶é•¿æœŸå‰æ™¯ä¸ç¡®å®š

- **â€œRun with top performance on the industry leaderâ€™s hardwareâ€**: Neither XLA nor TVM could fully unlock NVIDIA GPUs without calling into CUDA libraries, and thus it is unclear whether they are â€œgoodâ€ on other AI accelerators without similar libraries to call into. ğŸ‘ XLA on TPUs does show the power of TPU hardware and its greater scalability than NVIDIA hardware. ğŸ‘
>  XLA å’Œ TVM éƒ½æ— æ³•åœ¨ä¸è°ƒç”¨ CUDA åº“çš„æƒ…å†µä¸‹è§£é” NVIDIA GPU çš„èƒ½åŠ›ï¼Œå› æ­¤ä¸æ¸…æ¥šä»–ä»¬åœ¨æ²¡æœ‰ç±»ä¼¼åº“æ”¯æŒçš„å…¶ä»– AI åŠ é€Ÿå™¨ä¸Šæ˜¯å¦è¡¨ç°è‰¯å¥½
>  ä¸è¿‡ XLA åœ¨ TPU ä¸Šå±•ç¤ºäº† TPU ç¡¬ä»¶çš„å¼ºå¤§æ€§èƒ½

- **â€œEvolve rapidlyâ€:**Â Both projects were built for traditional deep learning, but GenAI shattered their assumptions. The shift to massive models, complex memory hierarchies, and novel attention mechanisms required a new level of hardware-software co-design that they werenâ€™t equipped to handle. ğŸ‘ This ultimately made both projects a lot less interesting to folks who might want to use them on modern hardware that is expected to support GenAI. ğŸ‘ğŸ‘
>  å‘å¤§å‹æ¨¡å‹ã€å¤æ‚å†…å­˜ç»“æ„å’Œæ–° attention æœºåˆ¶çš„è½¬å˜éœ€è¦æ–°çš„è½¯ç¡¬ä»¶ååŒæ°´å¹³ï¼ŒTVM å’Œ XLA éƒ½æ²¡æœ‰åšå¥½å‡†å¤‡

- **â€œCultivate developer loveâ€:**Â In its strong spot, XLA provided a simple and clean model that people could understand, one that led to the rise of the JAX framework among others. ğŸ‘ğŸ‘ TVM had cool technology but wasnâ€™t a joy to use with long compile times and incompatibility with popular AI models. ğŸ‘
>  XLA ä¸ºäººä»¬æä¾›äº†ä¸€ä¸ªç®€å•æ¸…æ™°çš„æ¨¡å‹ï¼Œäººä»¬å¯ä»¥ç†è§£
>  TVM æœ‰é…·ç‚«çš„æŠ€æœ¯ï¼Œä½†ç¼–è¯‘æ—¶é—´é•¿ï¼Œä¸”å’Œæµè¡Œçš„ AI æ¨¡å‹ä¸å…¼å®¹

- **â€œBuild an open communityâ€:**Â TVM built an open community, and OpenXLA aimed to. Both benefited from industry adoption as a result. ğŸ‘
- **â€œAvoid fragmentationâ€:**Â Neither project didâ€“TVM was widely forked and changed downstream, and XLA never accepted support for non-CPU/GPU hardware in its tree; all supported hardware was downstream. ğŸ‘
>  ä¸¤ä¸ªé¡¹ç›®éƒ½æ²¡æœ‰åšåˆ°é¿å…ç¢ç‰‡åŒ–
>  TVM è¢«å¹¿æ³› fork å’Œä¿®æ”¹ï¼ŒXLA ä»æœªåœ¨å…¶ä»£ç æ ‘ç§æ¥æ”¶å¯¹é CPU/GPU ç¡¬ä»¶çš„æ”¯æŒï¼Œæ‰€æœ‰å…¶ä»–æ”¯æŒçš„ç¡¬ä»¶éƒ½æ˜¯ä¸‹æ¸¸çš„

## The pros and cons of AI compiler technology
_First-generation_Â AI frameworks like TensorFlow and PyTorch 1.0 relied heavily on hand-written CUDA kernels, which couldnâ€™t scale to rapidly evolving AI workloads. TVM and XLA, asÂ _second-generation_Â approaches, tackled this problem with automated compilation. However, in doing so, they sacrificed key strengths of the first generation: extensibility for custom algorithms, fine-grained control over hardware, and dynamic executionâ€”features that turned out to be critical for GenAI.
>  ç¬¬ä¸€ä»£ AI æ¡†æ¶ä¾‹å¦‚ TensorFlow å’Œ PyTorch 1.0 ä¸¥é‡ä¾èµ–æ‰‹å†™çš„ CUDA kernelsï¼Œè€Œæ‰‹å†™çš„ CUDA kernels æ— æ³•éšç€å¿«é€Ÿå˜åŒ–çš„ AI workload æ¼”å˜
>  TVM å’Œ XLA ä½œä¸ºç¬¬äºŒä»£æŠ€æœ¯ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç¼–è¯‘æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ä¹Ÿç‰ºç‰²äº†ç¬¬ä¸€ä»£çš„å…³é”®ä¼˜åŠ¿: å¯¹è‡ªå®šä¹‰ç®—å­çš„å¯æ‹“å±•æ€§æ”¯æŒã€å¯¹ç¡¬ä»¶çš„ç»†ç²’åº¦æ§åˆ¶ä»¥åŠåŠ¨æ€æ‰§è¡Œï¼Œè¿™äº›ç‰¹æ€§è¢«è¯æ˜å¯¹ GenAI è‡³å…³é‡è¦

Beyond what we learned from OpenCL, we can also add a few wishlist items:
>  é™¤äº†ä» OpenCL å­¦ä¹ åˆ°çš„æ•™è®­ä»¥å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ·»åŠ ä¸€äº›æœŸæœ›:

- **Enable full programmability**: We canâ€™t democratize AI if we hide the power of any given chip from the developer.Â If you spend $100M on a cluster of one specific kind of GPU, youâ€™ll want to unlock the full power of that silicon without being limited to a simplified interface.
>  å®ç°å®Œå…¨çš„å¯ç¼–ç¨‹æ€§: å¦‚æœæˆ‘ä»¬å¯¹å¼€å‘è€…éšè—äº†ä»»ä½•èŠ¯ç‰‡çš„å…¨éƒ¨èƒ½åŠ›ï¼Œæˆ‘ä»¬å°±æ— æ³•å®ç° AI çš„æ°‘ä¸»åŒ–

- **Provide leverage over AI complexity**: The major benefit of AI compilers is that it allows one to scale into the exponential complexity of AI (operators, datatypes, etc) without having to manually write a ton of code. This is essential to unlock next generation research.
>  æä¾›åº”å¯¹ AI å¤æ‚æ€§çš„æœºåˆ¶: AI ç¼–è¯‘å™¨çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒå…è®¸äººä»¬è½»æ¾åº”å¯¹ AI çš„æŒ‡æ•°çº§å¤æ‚æ€§ (operators, datatypes)ï¼Œè€Œæ— éœ€æ‰‹åŠ¨ç¼–å†™å¤§é‡ä»£ç 

- **Enable large scale applications**: The transformative capability of XLA is the ability to easily scale to multiple accelerators and nodes. This capability is required to support the largest and most innovative models with ease. This is something that CUDA never really cracked.
>  æ”¯æŒå¤§è§„æ¨¡åº”ç”¨: XLA çš„å˜é©æ€§èƒ½åŠ›åœ¨äºèƒ½å¤Ÿè½»æ¾æ‹“å±•åˆ°å¤šä¸ªåŠ é€Ÿå™¨å’ŒèŠ‚ç‚¹ï¼Œè€Œè¿™ä¸ªèƒ½åŠ›æ˜¯ CUDA ä»æœªè§£å†³çš„

Despite the wins and losses of these AI compilers, neither could fully unlock GPU performance or democratize AI compute. Instead, they reinforced silos: XLA remained TPU-centric, while TVM splintered into incompatible vendor-specific forks. They failed in the exact way CUDA alternatives were supposed to succeed!
>  å°½ç®¡è¿™äº› AI ç¼–è¯‘å™¨æœ‰æˆåŠŸå’Œå¤±è´¥ä¹‹å¤„ï¼Œä½†å®ƒä»¬éƒ½æ— æ³•å®Œå…¨é‡Šæ”¾ GPU çš„æ€§èƒ½ï¼Œä¹Ÿæ— æ³•å®ç° AI è®¡ç®—çš„æ°‘ä¸»åŒ–
>  ç›¸åï¼Œå®ƒä»¬åŠ å‰§äº†å­¤å²›æ•ˆåº”: XLA ä»ç„¶ä»¥ TPU ä¸ºä¸­å¿ƒï¼Œè€Œ TVM åˆ™åˆ†è£‚æˆä¸å…¼å®¹çš„ä¾›åº”å•† forks
>  å®ƒä»¬æ°æ°æ˜¯ä»¥ CUDA æ›¿ä»£æ–¹æ³•æœ¬åº”æˆåŠŸçš„æ–¹å¼å¤±è´¥çš„

## Maybe the Triton â€œlanguageâ€ will save us?
But while these compilers struggled,Â **a different approach was taking shape.**Â Instead of trying to replace CUDA, it aimed toÂ **embrace GPU programmingâ€”while making it more programmable.**
>  åœ¨ AI ç¼–è¯‘å™¨é‡åˆ°äº†å›°éš¾çš„åŒæ—¶ï¼Œä¸€ç§ä¸åŒçš„æ–¹æ³•ä¹Ÿåœ¨å½¢æˆ: å®ƒä»¬æ²¡æœ‰è¯•å›¾å–ä»£ CUDAï¼Œè€Œæ˜¯æ—¨åœ¨æ‹¥æŠ± GPU ç¼–ç¨‹ï¼ŒåŒæ—¶ä½¿å…¶æ›´åŠ å¯ç¼–ç¨‹

EnterÂ **Triton and the new wave of Python eDSLs**â€”an attempt to bridge the gap between CUDAâ€™s raw power and Pythonâ€™s ease of use. In the next post, weâ€™ll dive into these frameworks to see what they got right, where they fell short, and whether they finallyÂ **broke free from the mistakes of the past**.
>  Triton å’Œæ–°ä¸€ä»£çš„ Python eDSL (embedded DSL) å°è¯•å¼¥åˆ CUDA çš„æ€§èƒ½ä¸ Python çš„æ˜“ç”¨æ€§ä¹‹é—´çš„å·®è·

Of course, you already know the answer. TheÂ **CUDA Empire still reigns supreme**. But why? And more importantlyâ€”**what can we do about it?**

> Those who cannot remember the past are condemned to repeat it.  
> â€”George Santayana

Perhaps one day, compiler technology willÂ **alleviate our suffering without taking away our power**. Until next time, ğŸš€

**â€”Chris**

# 7 What about Triton and Python eDSLs?
Site: https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls
Date: 26 March 2025

## Whatâ€™s an Embedded Domain Specific Language (eDSL)?
[Domain Specific Languages](https://en.wikipedia.org/wiki/Domain-specific_language)Â are used when a specific domain has a unique way to express things that makes developers more productiveâ€”perhaps the most well known are HTML, SQL, andÂ [regular expressions](https://en.wikipedia.org/wiki/Regular_expression). Â An â€œ**eDSLâ€**Â is a DSL that re-uses an existing language's syntaxâ€”but changes how the code works with compiler techniques. eDSLs power many systems, from distributed computing (PySpark) to deep learning frameworks (TensorFlow, PyTorch) to GPU programming (Triton).
>  é¢†åŸŸç‰¹å®šè¯­è¨€ç”¨äºç‰¹å®šçš„é¢†åŸŸæœ‰ä¸€ç§è®©å¼€å‘è€…æ›´åŠ å…·æœ‰ç”Ÿäº§åŠ›çš„ç‹¬ç‰¹è¡¨ç¤ºæ–¹å¼ï¼Œä¾‹å¦‚ HTML, SQL, æ­£åˆ™è¡¨è¾¾å¼
>  eDSL åˆ™æ˜¯å¤ç”¨äº†ç°å­˜è¯­è¨€è¯­æ³•çš„ DSLï¼Œä½†é€šè¿‡ç¼–è¯‘å™¨æŠ€æœ¯ä¿®æ”¹äº†ä»£ç çš„è¯­ä¹‰
>  eDSL çš„ä¾‹å­åŒ…æ‹¬åˆ†å¸ƒå¼è®¡ç®—ï¼Œæ·±åº¦å­¦ä¹ æ¡†æ¶å’Œ GPU ç¼–ç¨‹

For example,Â **PySpark**Â lets users express data transformations in Python, but constructs an optimized execution plan that runs efficiently across a cluster. Similarly,Â **TensorFlow's**Â `tf.function`Â andÂ **PyTorch's**Â `torch.fx`Â convert Python-like code into optimized computation graphs. These eDSLs abstract away low-level details, making it easier to write efficient code without expertise in distributed systems, GPU programming, or compiler design.
>  ä¾‹å¦‚ PySpark è®©ç”¨æˆ·ä»¥ Python æ–¹å¼è¡¨ç¤ºæ•°æ®è½¬æ¢ï¼Œä½†å®é™…ä¼šæ„é€ åœ¨é›†ç¾¤ä¸Šé«˜æ•ˆè¿è¡Œçš„ä¼˜åŒ–çš„æ‰§è¡Œæ–¹æ¡ˆ
>  ä¾‹å¦‚ TensorFlow çš„ `tf.function` å’Œ PyTorch çš„ `torch.fx` å°†ç±» Python ä»£ç è½¬åŒ–ä¸ºä¼˜åŒ–çš„è®¡ç®—å›¾
>  eDSL æŠ½è±¡æ‰åº•å±‚ç»†èŠ‚ï¼Œä½¿å¾—ä¸éœ€è¦ä¸“å®¶çŸ¥è¯†ä¹Ÿå¯ä»¥å†™é«˜æ•ˆä»£ç 

### How does an eDSL work?
eDSLs work their magic by capturing Python code before it runs and transforming it into a form they can process. They typically leverageÂ **decorators**, a Python feature that intercepts functions before they run. When you applyÂ `@triton.jit`, Python hands the function to Triton rather than executing it directly.
>  eDSL çš„å·¥ä½œæ–¹å¼æ˜¯åœ¨ Python ä»£ç è¿è¡Œä¹‹å‰å¯¹å…¶è¿›è¡Œæ•è·ï¼Œç„¶åè½¬åŒ–ä¸ºå¦ä¸€ç§å¯ä»¥å¤„ç†çš„å½¢å¼
>  å®ƒä»¬é€šå¸¸åˆ©ç”¨è£…é¥°å™¨å®ç°ï¼Œè£…é¥°å™¨æ˜¯ä¸€ä¸ª Python ç‰¹æ€§ï¼Œä¼šåœ¨å‡½æ•°è¿è¡Œä¹‹å‰å¯¹å…¶è¿›è¡Œæ‹¦æˆª
>  å½“æˆ‘ä»¬ä½¿ç”¨ `@trition.jit`ï¼ŒPython å°±ä¼šå°†å‡½æ•°äº¤ç»™ Triton è€Œä¸æ˜¯ç›´æ¥æ‰§è¡Œå®ƒ

Here's a simple Triton example:

```python
@triton.jit
def kernel(x_ptr, y_ptr, BLOCK_SIZE: tl.constexpr):
  offs = tl.arange(0, BLOCK_SIZE)
  x = tl.load(x_ptr + offs)
  tl.store(y_ptr + offs, x)
```

When Triton receives this code, it parses the function into anÂ **Abstract Syntax Tree (AST)**Â that represents the function's structure, including operations and data dependencies. This representation allows Triton to analyze patterns, apply optimizations, and generate efficient GPU code that performs the same operations.
>  ä¾‹å¦‚ Triton æ¥å—åˆ°ä¸Šè¿°ä»£ç åï¼Œå®ƒå°†å‡½æ•°è§£æä¸ºè¡¨ç¤ºè¯¥å‡½æ•°ç»“æ„çš„æŠ½è±¡è¯­æ³•æ ‘ï¼Œæ•°ä¸­åŒ…æ‹¬äº† operations å’Œæ•°æ®ä¾èµ–
>  æŠ½è±¡è¯­æ³•æ ‘çš„è¡¨ç¤ºå…è®¸ Triton åˆ†æ patternsï¼Œåº”ç”¨ä¼˜åŒ–ï¼Œå¹¶ç”Ÿæˆæ‰§è¡Œç›¸åŒè¿ç®—çš„é«˜æ•ˆ GPU ä»£ç 

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67e2b49a362ad18a723c0297_GPUCode.jpg)

By leveraging Python's existing syntax and tooling, eDSL creators can focus on building compiler logic rather than designing an entirely new language with its own parser, syntax, and toolchain.
>  eDSL é€šè¿‡å¤ç”¨ Python çš„ç°å­˜è¯­æ³•å’Œå·¥å…·ï¼Œä»¥ä¸“æ³¨æ„å»ºç¼–è¯‘å™¨é€»è¾‘è€Œä¸æ˜¯è®¾è®¡ä¸€æ•´ä¸ªæ–°çš„å¸¦æœ‰è‡ªå·±çš„ parser, syntax, toolchain çš„è¯­è¨€

### The advantage of eDSLs
eDSLs provide huge advantages forÂ **those building a domain-specific compiler**: by embedding the language inside Python, developers can focus on compiler logic instead of reinventing an entire programming language. Designing new syntax, writing parsers, and building IDE tooling is a massive effortâ€”by leveraging Python's existing syntax and AST tools, eDSL creators skip all of that and get straight to solving the problem at hand.
>  eDSL ä¸ºæ„å»ºé¢†åŸŸç‰¹å®šçš„ç¼–è¯‘å™¨çš„å¼€å‘è€…å¸¦æ¥äº†å¾ˆå¤§ä¼˜åŠ¿ï¼Œå› ä¸ºåªéœ€è¦ä¸“æ³¨ç¼–è¯‘å™¨é€»è¾‘ï¼Œä¸éœ€è¦é‡æ–°å‘æ˜æ•´ä¸ªç¼–ç¨‹è¯­è¨€

**Users of the eDSL**Â benefit too: Python eDSLs let developers stay in familiar territory. They get to use the same Python IDEs, autocompletion, debugging tools, package managers (e.g.Â `pip`Â andÂ `conda`), and ecosystem of libraries. Instead of learning a completely new language like CUDA C++, they write code in Pythonâ€”and the eDSL guides execution under the hood.

However, this convenience comes with significant tradeoffs that can frustrate developers who expect eDSLs to behave like regular Python code.

>  eDSL çš„ç”¨æˆ·ä¹ŸåŒæ ·å—ç›Šï¼Œå¯ä»¥ç•™åœ¨ç†Ÿæ‚‰çš„ç¯å¢ƒä¸­ï¼Œä½†è¿™ç§ä¾¿åˆ©ä¹Ÿä¼´éšç€ä¸€äº›æ˜¾è‘—çš„æƒè¡¡

### The challenges with eDSLs
Of course, thereâ€™s no free lunch. eDSLs come with trade-offs, and some can be deeply frustrating.

###### **It looks like Python, but it isn't Python**
This is the most confusing part of eDSLs. While the codeÂ _looks_Â like regular Python, it doesnâ€™tÂ _behave_Â like Python in crucial ways:

>  eDSL çœ‹èµ·æ¥åƒ Pythonï¼Œä½†å®é™…ä¸æ˜¯ Python

```python
# Regular Python: This works as expected
def works():
  kv = dict((i, i * i) for i in range(5))
  return sum(kv.values())

# Python eDSL: The same code fails
@numba.njit()
def fails():
  # Generator expressions aren't supported
  kv = dict((i, i * i) for i in range(5))
  # Built-in function sum isn't implemented
  return sum(kv.values())
```

Why? Because an eDSL isnâ€™tÂ _executing_Â Pythonâ€”itâ€™s capturing and transforming the function into something else. It decides what constructs to support, and many everyday Python features (like dynamic lists, exception handling, or recursion) may simply not work. This can lead toÂ _silent failures_Â orÂ _cryptic errors_Â when something youâ€™d expect to work in Python suddenly doesnâ€™t.
>  ç›¸åŒçš„ Python è¯­æ³•ï¼Œåœ¨ eDSL æ‰§è¡Œä¸‹ï¼Œç»“æœå’Œ Python è§£é‡Šå™¨çš„æ‰§è¡Œç»“æœå®Œå…¨ä¸åŒ
>  åŸå› åœ¨äº eDSL ä¸æ˜¯åœ¨æ‰§è¡Œ Pythonï¼Œå®ƒæ˜¯æ•è·äº†å‡½æ•°ï¼Œå¹¶ä¸”å°†å®ƒè½¬åŒ–ä¸ºäº†å…¶ä»–ä¸œè¥¿
>  eDSL å†³å®šäº†è¦æ”¯æŒé‚£äº›æ„é€ ï¼Œå¹¶ä¸”å¾ˆå¤š Python ç‰¹æ€§ (ä¾‹å¦‚åŠ¨æ€ list, é”™è¯¯å¤„ç†, é€’å½’) æˆ–è®¸éƒ½æ— æ³•å·¥ä½œï¼Œè¿™å¯èƒ½å¯¼è‡´é™é»˜å¤±è´¥æˆ–éš¾ä»¥ç†è§£çš„é”™è¯¯

###### **Errors and Tooling Limitations**
Debugging eDSL code can be aÂ **nightmare**. When your code fails, you often donâ€™t get the friendly Python error messages youâ€™re used to. Instead, youâ€™re staring at an opaque stack trace from deep inside of some compiler internals, with little clue what went wrong. Worse, standard tools like Python debuggers often donâ€™t work at all, forcing you to rely on whatever debugging facilities the eDSL provides (if any). Further, while eDSLs exist within Python, they cannot use Python libraries directly.

>  å¯¹ eDSL ä»£ç è¿›è¡Œ debug éå¸¸å›°éš¾ï¼Œå½“å‡ºç°é”™è¯¯æ—¶ï¼Œæˆ‘ä»¬å¾€å¾€ä¸ä¼šå¾—åˆ° Python çš„é”™è¯¯æ¶ˆæ¯ï¼Œè€Œæ˜¯æ¥è‡ªç¼–è¯‘å™¨å†…éƒ¨çš„ stack trace
>  æ­¤å¤–ï¼Œæ ‡å‡†çš„å·¥å…·ä¾‹å¦‚ Python debuggers å¯èƒ½æ ¹æœ¬æ— æ³•ä½¿ç”¨ï¼Œæ•…æˆ‘ä»¬éœ€è¦ä½¿ç”¨ eDSL çš„ debugger
>  æ­¤å¤–ï¼Œå°½ç®¡ eDSL å­˜åœ¨äº Python ä¸­ï¼Œä½†å®ƒä»¬æ— æ³•ç›´æ¥ä½¿ç”¨ Python åº“

###### **Limited Expressiveness**
eDSLs work by piggybacking on Pythonâ€™s syntax, which means theyÂ **canâ€™t**Â introduce new syntax that might be useful for their domain. A language like CUDA C++ can add custom keywords, new constructs, or domain-specific optimizations, while an eDSL is locked into a sublanguage of Python, which limits what it can express cleanly.
>  eDSL é€šè¿‡ä¾èµ– Python çš„è¯­æ³•æ¥å·¥ä½œï¼Œè¿™æ„å‘³ç€å®ƒä»¬æ— æ³•å¼•å…¥å¯¹äºå…¶é¢†åŸŸæœ‰ç”¨çš„æ–°è¯­æ³•ç»“æ„ï¼Œè¿™é™åˆ¶äº†å®ƒèƒ½å¤Ÿæ¸…æ™°è¡¨è¾¾çš„å†…å®¹

Ultimately, theÂ _quality_Â of a specific eDSL determines how painful these trade-offs feel. A well-implemented eDSL can provide a smooth experience, while a poorly designed one can be a frustrating minefield of broken expectations. So does an eDSL likeÂ **Triton**Â get it right? And how does it compare to CUDA?
>  ç‰¹å®š eDSL çš„è´¨é‡å†³å®šäº†è¿™äº›æƒè¡¡å¸¦æ¥çš„ç—›è‹¦ç¨‹åº¦

## Triton: OpenAI's Python eDSL for GPU Programming
Triton began as a research project fromÂ [Philippe Tillet](https://scholar.google.com/citations?user=SQfo7UgAAAAJ&hl=fr)Â at Harvard University,Â [first published in 2019](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf)Â afterÂ [years working on OpenCL](https://youtu.be/WnBG7je7tO4?si=_M9jWBO4m0XR2R-e&t=70)Â (see myÂ [earlier post on OpenCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives)). The project gained significant momentum when Tillet joined OpenAI, and when PyTorch 2 decided to embrace it.
>  Triton æœ€åˆä½œä¸ºç ”ç©¶é¡¹ç›®å‘èµ·ï¼ŒPyTorch2 å†³å®šæ‹¥æŠ± Triton ä¹‹åï¼Œè¿™ä¸ªé¡¹ç›®çš„å‘å±•è¢«è¿›ä¸€æ­¥æ¨åŠ¨

Unlike general-purpose AI compilers, TritonÂ **focuses on accessibility for Python developers**Â while still allowing for deep optimization. It strikes a balance betweenÂ **high-level simplicity and low-level control**â€”giving developers just enough flexibility to fine-tune performance without drowning in CUDAâ€™s complexity.
>  å’Œé€šç”¨çš„ AI ç¼–è¯‘å™¨ä¸åŒï¼ŒTriton ä¸“æ³¨äºé¢å‘ Python å¼€å‘è€…çš„æ˜“ç”¨æ€§ï¼ŒåŒæ—¶ä»ç„¶æ”¯æŒæ·±åº¦ä¼˜åŒ–
>  Triton åœ¨é«˜å±‚ç®€æ´æ€§å’Œåº•å±‚æ§åˆ¶ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œç»™å¼€å‘è€…è¶³å¤Ÿçš„çµæ´»æ€§æ¥è°ƒèŠ‚æ€§èƒ½ï¼Œè€Œä¸éœ€è¦æ·±å…¥ CUDA çš„å¤æ‚æ€§

Letâ€™s explore what makes Triton so useful.

### Block-centric programming model
Traditional GPU programming forces developers to think in terms ofÂ **individual threads**, managing synchronization and complex indexing by hand. Triton simplifies this by operating at theÂ **block level**â€”where GPUs naturally perform their workâ€”eliminating unnecessary low-level coordination:
>  ä¼ ç»Ÿçš„ GPU ç¼–ç¨‹å¼ºè¿«å¼€å‘è€…ä»¥å•ç‹¬çº¿ç¨‹çš„æ–¹å¼æ€è€ƒï¼Œæ‰‹åŠ¨ç®¡ç†åŒæ­¥å’Œå¤æ‚çš„ç´¢å¼•
>  Triton åœ¨ block level å·¥ä½œï¼Œæ¶ˆé™¤äº†ä¸å¿…è¦çš„ä½çº§åè°ƒ

```python
@triton.jit
def simplified_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    # One line gets us our block position
    block_start = tl.program_id(0) * BLOCK_SIZE
    # Create indexes for the entire block at once
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Process a whole block of data in one operation
    data = tl.load(input_ptr + offsets, mask=offsets < n_elements)
    # No need to worry about thread synchronization
```

This model abstracts away thread management and simplifies basic indexing, but it also makes itÂ **much easier to leverage TensorCores**â€”the specialized hardware responsible for most of a GPUâ€™s FLOPS:
>  è¿™ä¸ªæ¨¡å‹æŠ½è±¡æ‰äº†çº¿ç¨‹ç®¡ç†ï¼Œå¹¶ä¸”ç®€åŒ–äº†åŸºç¡€çš„ç´¢å¼•ï¼Œä¹Ÿä½¿å¾—æˆ‘ä»¬æ›´å®¹æ˜“åˆ©ç”¨ TensorCores

```python
# This simple dot product automatically uses TensorCores when available
result = tl.dot(matrix_a, matrix_b)
```

What would requireÂ **dozens of lines of complex CUDA code**Â becomes a single function call, while still achieving high performance. Triton handles the data layout transformations and hardware-specific optimizations automatically.
>  Triton è‡ªåŠ¨å¤„ç†äº† data layout è½¬æ¢å’Œç¡¬ä»¶ç‰¹å®šçš„ä¼˜åŒ–ï¼Œä½¿å¾—å‡ åè¡Œ CUDA ä»£ç å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå‡½æ•°è°ƒç”¨è§£å†³

### Simplified optimizations
One of CUDA's most frustrating aspects is managing complex index calculations for multi-dimensional data. Triton dramatically simplifies this:

```python
# Simple indexing with broadcast semantics
row_indices = tl.arange(0, BLOCK_M)[:, None]
col_indices = tl.arange(0, BLOCK_N)[None, :]
```

These array manipulations feel similar to NumPy but compile to efficient GPU code with no runtime overhead.

>  CUDA æœ€éº»çƒ¦çš„ä¸€ç‚¹å°±æ˜¯å¤„ç†å¯¹å¤šç»´æ•°æ®çš„å¤æ‚ç´¢å¼•è®¡ç®—ï¼ŒTriton æ˜¾è‘—ç®€åŒ–äº†è¿™ä¸€ç‚¹
>  ä¸Šè¿°ä»£ç çš„æ•°ç»„å¤„ç†æ–¹å¼ç±»ä¼¼äº NumPyï¼Œä½†æ˜¯ä¼šç¼–è¯‘ä¸ºé«˜æ•ˆçš„ GPU ä»£ç ï¼Œä¸”æ²¡æœ‰è¿è¡Œæ—¶å¼€é”€

Triton also includes compiler-driven optimizationsâ€”like vectorizationâ€”and enables simplified double buffering and software pipelining, which overlap memory transfers with computation. In CUDA, these techniques require deep GPU expertise; in Triton, theyâ€™re exposed in a way thatÂ **non-experts can actually use**. For a deeper dive, OpenAI providesÂ [detailed tutorials](https://triton-lang.org/main/getting-started/tutorials/).
>  Triton ä¹ŸåŒ…å«äº†ç”±ç¼–è¯‘å™¨é©±åŠ¨çš„ä¼˜åŒ–åŠŸèƒ½ â€”â€” ä¾‹å¦‚å‘é‡åŒ– â€”â€” å¹¶ä¸”æ”¯æŒç®€åŒ–ç‰ˆçš„ doubler buffer å’Œè½¯ä»¶æµæ°´çº¿ï¼Œå°†å†…å­˜ä¼ è¾“å’Œè®¡ç®—é‡å è¿›è¡Œ

**Triton makes GPU programming far more accessible, but that accessibility comes with tradeoffs.**Â Letâ€™s take a look at some of the key challenges.
>  Triton è®© GPU ç¼–ç¨‹å˜å¾—æ›´åŠ æ˜“ç”¨ï¼Œä½†è¿™ç§æ˜“ç”¨æ€§ä¹Ÿä¼´éšç€ä¸€äº›æƒè¡¡

### Where Triton Falls Short
Triton is widely used and very successful for some cases (e.g. researchers working on training frontier models and specialty use cases). However, it isnâ€™t widely adopted for all applications: in particular, itâ€™s not useful for AI inference use-cases, which require maximum efficiency. Â Furthermore,Â [despite predictions years ago by industry leaders](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/), Triton has not united the ecosystem or challenged CUDA's dominance. Letâ€™s dig in to understand the additional challenges Triton faces on top of the general limitations of all eDSLs (described earlier).
>  Triton åœ¨æŸäº›åœºæ™¯ä¸‹è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å¹¶ä¸æ˜¯é€‚ç”¨äºæ‰€æœ‰åº”æœ‰ï¼Œå°¤å…¶æ˜¯å¯¹äºéœ€è¦æœ€å¤§æ•ˆç‡çš„ AI æ¨ç†åœºæ™¯
>  æ­¤å¤–ï¼ŒTriton å¹¶æœªç»Ÿä¸€æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿï¼Œä¹Ÿæœªèƒ½æŒ‘æˆ˜ CUDA çš„ä¸»å¯¼åœ°ä½

###### **Significant GPU Performance/TCO Loss (compared to CUDA C++)**
**â€**TritonÂ **trades performance for productivity**Â [(as explained by its creator)](https://youtu.be/WnBG7je7tO4?si=VwRalE1KOPX0k3eo&t=1186). While this makes it easier to write GPU code, it also prevents Triton from achieving peak efficiency. The amount varies, but it is common to lose 20% onÂ **NVIDIAâ€™s H100**â€”which dominates AI compute today.
>  Triton ä»¥æå‡å¼€å‘æ•ˆç‡è€Œæ¢å–äº†æ€§èƒ½
>  Triton ä½¿å¾—ç¼–å†™ GPU ä»£ç æ›´åŠ å®¹æ˜“ï¼Œä½†ä¹Ÿé™åˆ¶äº†å®ƒè¾¾åˆ°æœ€é«˜æ•ˆç‡çš„èƒ½åŠ›ï¼Œå…·ä½“çš„æ€§èƒ½æŸå¤±å› æƒ…å†µè€Œå¼‚ï¼Œåœ¨ H100 ä¸Šé€šå¸¸ä¼šæŸå¤±è¶Š 20%

The problem?Â **Compilers canâ€™t optimize as well as a skilled CUDA developer, particularly for todayâ€™s advanced GPUs.**Â In my decades of building compilers, Iâ€™ve never seen the myth of a â€œ[**sufficiently smart compiler**](https://wiki.c2.com/?SufficientlySmartCompiler)â€ actually work out! This is why leading AI labs, including DeepSeek,Â **still rely on CUDA instead of Triton**Â for demanding workloads: a 20% difference is untenable in GenAI: at scale it is the difference between a $1B cloud bill and an $800M one!
>  é—®é¢˜åœ¨äºç¼–è¯‘å™¨æ— æ³•åƒç»éªŒä¸°å¯Œçš„å¼€å‘è€…é‚£æ ·è¿›è¡Œä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯å¯¹äºå½“å‰å…ˆè¿›çš„ GPU
>  â€œè¶³å¤Ÿæ™ºèƒ½çš„ç¼–è¯‘å™¨â€ ä»æœªçœŸæ­£å®ç°ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆé¢†å…ˆçš„ AI å®éªŒå®¤ï¼ŒåŒ…æ‹¬ DeepSeekï¼Œä»ç„¶åœ¨å¤„ç†é«˜è´Ÿè½½ä»»åŠ¡æ—¶ä¾èµ– CUDA è€Œä¸æ˜¯ Triton

###### **Governance: OpenAIâ€™s Control and Focus**
Triton is open source, but OpenAIÂ **owns its roadmap**. Thatâ€™s problematic because OpenAI competes directly with other frontier model labs, raising the question:Â **will it prioritize the needs of the broader AI community, or just its own?**
>  Triton æ˜¯å¼€æºçš„ï¼Œä½† OpenAI æŒæ§ç€å®ƒçš„å¼€å‘è·¯çº¿å›¾
>  å› ä¸º OpenAI ä¸å…¶ä»–å‰æ²¿æ¨¡å‹å®éªŒå®¤ç›´æ¥ç«äº‰ï¼Œè¿™å°±å¼•å‘äº†ä¸€ä¸ªé—®é¢˜: å®ƒä¼šå…ˆè€ƒè™‘ AI ç¤¾åŒºçš„éœ€æ±‚è¿˜æ˜¯è‡ªå·±çš„åˆ©ç›Š

Many engineers have shared frustration about how difficult it is toÂ **contribute enhancements to Triton**, particularly when changes donâ€™t align with OpenAIâ€™s internal priorities. One recurring complaint is thatÂ **support for alternative hardware lags far behind**â€”because OpenAI has little incentive to optimize for accelerators it doesnâ€™t use. Tritonâ€™s leadership admits that â€œ[support for new users is virtually nonexistent](https://youtu.be/o3DrHb-mVLM?si=9cp9syo9S8tKwqQ0&t=880)â€, and they donâ€™t have bandwidth to keep up with community needs.
>  è®¸å¤šå·¥ç¨‹å¸ˆéƒ½è¡¨ç¤ºå‘ Triton è´¡çŒ®éå¸¸å›°éš¾ï¼Œå°¤å…¶æ˜¯æ›´æ”¹ä¸ OpenAI çš„å†…éƒ¨ä¼˜å…ˆçº§ä¸ä¸€è‡´æ—¶
>  ä¸€ä¸ªå¸¸è§çš„æŠ±æ€¨æ˜¯: å¯¹å…¶ä»–ç¡¬ä»¶çš„æ”¯æŒè¿œè¿œè½åï¼Œå› ä¸º OpenAI æ²¡æœ‰åŠ¨åŠ›å»ä¼˜åŒ–å®ƒä»¬è‡ªå·±å¹¶ä¸ä½¿ç”¨çš„åŠ é€Ÿå™¨

###### **Poor Tooling and Debugger Support**
CUDA's complexity is offset byÂ **a mature ecosystem**Â of toolsâ€”Nsight Compute, profiler APIs, and memory debuggersâ€”that give developers deep insights into performance bottlenecks. Triton doesn't work with these tools. eDSLs by design are supposed to abstract out the details. As a result, when issues arise, developers cannot determine what the source of the issue was, they are often leftÂ **_guessing what the compiler did_**. This lack of observability makes performance debugging in Triton more challenging than in CUDA, despite its simpler programming model.
>  CUDA çš„å¤æ‚æ€§è¢«å…¶æˆç†Ÿçš„å·¥å…·ç”Ÿæ€ç³»ç»Ÿæ‰€æŠµæ¶ˆ â€”â€” Nsight Compute, profiler API, memory debugger
>  è€Œ Triton æ— æ³•ä¸è¿™äº›å·¥å…·å…¼å®¹
>  eDSL åœ¨è®¾è®¡ä¸Šæ˜¯æŠ½è±¡æ‰åº•å±‚ç»†èŠ‚çš„ï¼Œå› æ­¤å½“å‡ºç°é—®é¢˜æ—¶ï¼Œå¼€å‘è€…å¾€å¾€æ— æ³•ç¡®å®šé—®é¢˜çš„æ ¹æºï¼Œè€Œæ˜¯åªèƒ½**çŒœæµ‹ç¼–è¯‘å™¨åšäº†ä»€ä¹ˆ**
>  è¿™ä½¿å¾—åœ¨ Triton ä¸­è¿›è¡Œæ€§èƒ½è°ƒè¯•æ¯”åœ¨ CUDA ä¸­æ›´åŠ å›°éš¾ï¼Œå°½ç®¡ Triton çš„ç¼–ç¨‹æ¨¡å‹æ›´ä¸ºç®€å•

###### **GPU Portability Without Performance Portability or Generality**
GPU code written in Triton can run â€œpretty fastâ€ if written for one specific GPU, but that code wonâ€™t go fast on different kinds of GPUâ€™sâ€”even across NVIDIA hardware. For example, Triton code optimized forÂ **A100**Â often performs poorly onÂ **H100**Â because newer architectures requires different code structures even to get to 80% performanceâ€”Triton doesnâ€™t abstract things like pipelining and async memory transfers.
>  ä½¿ç”¨ Triton é’ˆå¯¹ç‰¹å®š GPU ç¼–å†™çš„ä»£ç é€šå¸¸å¯ä»¥è¿è¡Œå¾—ç›¸å½“å¿«ï¼Œä½†è¿™ç§ä»£ç åœ¨ä¸åŒç±»å‹çš„ GPU ä¸Šï¼Œç”šè‡³æ˜¯ NVIDIA çš„ä¸åŒç¡¬ä»¶ä¹‹é—´çš„æ€§èƒ½åˆ™æ— æ³•ä¿æŒ
>  ä¾‹å¦‚ï¼Œé’ˆå¯¹ A100 ä¼˜åŒ–çš„ Triton ä»£ç åœ¨ H100 ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå³ä¾¿æ˜¯è¾¾åˆ° 80% çš„æ€§èƒ½ï¼Œæ–°æ¶æ„ä¹Ÿéœ€è¦ä¸åŒçš„ä»£ç ç»“æ„ï¼Œè€Œ Triton å¹¶æ²¡æœ‰æŠ½è±¡å‡ºåƒæµæ°´çº¿å’Œå¼‚æ­¥å†…å­˜ä¼ è¾“è¿™æ ·çš„ç»†èŠ‚

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67e59c944e857865efd4a94d_MemeFry.jpg)

Triton kernels need to be rewritten for new generations of NVIDIA hardware to unlock their performance.

Moving toÂ **AMD GPUs**Â is even worse. While Triton technically supports AMD hardware, performance and feature parityÂ **lag far behind NVIDIA**, making cross-vendor portability impractical. The situation becomes catastrophic forÂ **non-GPU AI accelerators**Â (e.g., TPUs, Groq chips, or Cerebras wafers). These architectures donâ€™t follow theÂ **SIMT execution model**Â that Triton assumes, leading toÂ **severely degraded performance,**Â or requiring so many workarounds that the approach becomes counterproductive.
>  AMD GPU çš„æƒ…å†µæ›´åŠ ç³Ÿç³•ï¼Œè™½ç„¶ Triton æŠ€æœ¯ä¸Šæ”¯æŒ AMD ç¡¬ä»¶ï¼Œä½†æ€§èƒ½è¿œè¿œä½äº NVIDIA
>  è€Œé GPU çš„ AI åŠ é€Ÿå™¨ä¸éµå¾ª Triton å‡è®¾çš„ SIMT æ‰§è¡Œæ¨¡å‹ï¼Œæ€§èƒ½ä¼šéå¸¸å·®

Ultimately, the promise ofÂ **"write once, run anywhere"**Â typically translates to:Â **"Write once, run anywhereâ€”but with significantly degraded performance on alternate platforms."**

## How does Triton stack up?
In ourÂ [last](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers)Â twoÂ [posts](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives), we started building a wishlist for AI programming systems. Â Measuring against that, Triton has several big strengths and several challenges as well:
>  æˆ‘ä»¬åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œä¸º AI ç¼–ç¨‹ç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªæ„¿æœ›æ¸…å•
>  æ ¹æ®è¿™ä¸ªæ ‡å‡†ï¼ŒTriton æœ‰è¯¸å¤šä¼˜ç‚¹ï¼Œä½†ä¹Ÿé¢ä¸´ä¸€äº›æŒ‘æˆ˜

- **"Provide a reference implementation"**: Triton provides a full implementation, not just a specification, with practical examples and tutorials. ğŸ‘
>  Triton æä¾›äº†å®Œæ•´çš„å®ç°

- **"Have strong leadership and vision"**: Triton has defined leadership under OpenAI, but priorities align with OpenAI's needs rather than the broader community. Long-term governance remains a concern, especially for competing AI labs. ğŸ‘ğŸ‘
>  Triton åœ¨ OpenAI çš„é¢†å¯¼ä¸‹ï¼Œæ•…ä¹Ÿæ˜æ˜¾åå‘ OpenAI çš„éœ€æ±‚ï¼Œè€Œéæ•´ä¸ªç¤¾åŒº

- **"Run with top performance on the industry leader's hardware"**: Triton runs well on NVIDIA hardware but typically with a ~20% performance gap compared to optimized CUDA. It struggles with the newest hardware features like FP8 and TMA. ğŸ‘
>  Triton åœ¨ NVIDIA ç¡¬ä»¶ä¸Šè¿è¡Œè‰¯å¥½ï¼Œä½†ä¸ä¼˜åŒ–åçš„ CUDA å­˜åœ¨ 20% çš„æ€§èƒ½å·®è·ï¼Œåœ¨å¤„ç†æœ€æ–°çš„ç¡¬ä»¶ç‰¹æ€§ä¾‹å¦‚ FP8 å’Œ TMA æ—¶ä¹Ÿæ˜¾å¾—åƒåŠ›

- **"Evolve rapidly"**: Triton has adapted to some GenAI requirements but lags in supporting cutting-edge hardware features. Evolution speed depends on OpenAI's internal priorities rather than industry needs. ğŸ‘
>  Triton å·²ç»é€‚åº”äº†ä¸€äº› GenAI çš„éœ€æ±‚ï¼Œä½†åœ¨æ”¯æŒæœ€å‰æ²¿çš„ç¡¬ä»¶ç‰¹æ€§æ–¹é¢ä»ç„¶è½åï¼Œæ¼”è¿›é€Ÿåº¦å–å†³äº OpenAI çš„å†…éƒ¨ä¼˜å…ˆçº§è€Œä¸æ˜¯è¡Œä¸šéœ€æ±‚

- **"Cultivate developer love"**: Triton provides a clean, Python-based programming model that many developers find intuitive and productive. Its integration with PyTorch 2.0 has expanded its reach. ğŸ‘ğŸ‘ğŸ‘

- **"Build an open community"**: While open source, Triton's community is limited by OpenAI's control over the roadmap. Contributions from outside organizations face significant barriers. ğŸ‘

- **"Avoid fragmentation"**: Triton itself is unified targeting NVIDIA GPUs ğŸ‘, but it is widely Â fragmented by other hardware vendors whose versions have different limitations and tradeoffs. ğŸ‘
>  Triton æœ¬èº«æ˜¯ç»Ÿä¸€çš„ï¼Œä¸“æ³¨äº NVIDIA GPUï¼Œä½†å®ƒè¢«å…¶ä»–ç¡¬ä»¶å‚å•†çš„ç‰ˆæœ¬æ‰€ç¢ç‰‡åŒ–ï¼Œè¿™äº›ç‰ˆæœ¬æœ‰ä¸åŒçš„é™åˆ¶å’Œæƒè¡¡

- **"Enable full programmability"**: Triton provides good programmability for standard operations ğŸ‘ but can't access/control all hardware features, particularly the newest accelerator capabilities. ğŸ‘
>  Triton å¯¹æ ‡å‡†æ“ä½œæä¾›äº†è‰¯å¥½çš„å¯ç¼–ç¨‹æ€§ï¼Œä½†æ— æ³•è®¿é—®æˆ–æ§åˆ¶æ‰€æœ‰ç¡¬ä»¶åŠŸèƒ½ï¼Œå°¤å…¶æ˜¯æœ€æ–°çš„åŠ é€Ÿå™¨èƒ½åŠ›

- **"Provide leverage over AI complexity"**: Triton handles common patterns efficiently and it simplifies development ğŸ‘. Â It doesnâ€™t support automatic fusion to solve the exponential complexity problem. ğŸ‘
>  Triton èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¸¸è§æ¨¡å¼ï¼Œä½†ä¸æ”¯æŒè‡ªåŠ¨èåˆæ¥è§£å†³æŒ‡æ•°çº§å¤æ‚çš„é—®é¢˜

- **"Enable large scale applications"**: Triton focuses on single-device kernels and lacks built-in support for multi-GPU or multi-node scaling, but has great integration into PyTorch which takes care of that. ğŸ‘
>  Triton ä¸“æ³¨äºå•è®¾å¤‡ kernelï¼Œç¼ºä¹å¯¹å¤š GPU æˆ–å¤šèŠ‚ç‚¹æ‹“å±•çš„å†…ç½®æ”¯æŒï¼Œä½†ä¸ PyTorch çš„æ·±åº¦é›†æˆå¯ä»¥è§£å†³è¿™äº›é—®é¢˜

Overall, it is clear that Triton is an extremely valuable part of the AI development ecosystem, particularly when targeting NVIDIA GPUs. Â That said, while Triton is the most well known eDSL due to its integration with PyTorch, other projectsâ€”likeÂ **Pallas, CUTLASS Python, and cuTile**â€”are exploring different trade-offs between productivity, performance, and hardware support. Each of these alternatives builds on similar ideas but takes a unique approach to tackling GPU programmability.
>  å°½ç®¡ Triton å› ä¸ºä¸ PyTorch çš„é›†æˆè€Œæœ€ä¸ºçŸ¥åï¼Œä½†å…¶ä»–é¡¹ç›®ï¼Œä¾‹å¦‚ Pallas, CUTLASS Pyhon, cuTile ä¹Ÿåœ¨æ¢ç´¢æƒè¡¡
>  è¿™äº›æ›¿ä»£æ–¹æ¡ˆéƒ½åŸºäºç±»ä¼¼çš„ç†å¿µï¼Œä½†å„è‡ªé‡‡ç”¨äº†ç‹¬ç‰¹çš„è·¯å¾„æ¥åº”å¯¹ GPU å¯ç¼–ç¨‹æ€§çš„é—®é¢˜

## Other Python eDSLs: Pallas,Â **CUTLASS**Â Python, cuTile, etc.
Python eDSLs arenâ€™t about delivering the best possible performanceâ€”theyâ€™re about making it easier for compiler developers to bring something to market. As a result,Â **there are a lot of them**â€”Triton is just the most well-known. Here are some I get asked about.Â _(Disclaimer: I havenâ€™t worked directly with these.)_
>  Python eDSL å¹¶ä¸æ˜¯ä¸ºäº†å®ç°æœ€ä½³æ€§èƒ½è€Œè®¾è®¡ï¼Œå®ƒä»¬çš„ç›®æ ‡æ˜¯è®©ç¼–è¯‘å™¨å¼€å‘è€…æ›´å®¹æ˜“å°†æŸä¸ªäº§å“æ¨å‘å¸‚åœºï¼Œå› æ­¤ï¼Œè¿™ç±»å·¥å…·æœ‰å¾ˆå¤šï¼ŒTriton åªæ˜¯å…¶ä¸­æœ€çŸ¥åçš„ä¸€ä¸ª

### Google Pallas
[Google Pallas](https://docs.jax.dev/en/latest/pallas/index.html)Â is a subproject of JAX, designed to enable custom opsâ€”particularly for TPUs. It takes heavy inspiration from Triton butÂ **exposes more low-level compiler details**Â rather than offering a high-level, user-friendly API.
>  Google Pallas æ˜¯ JAX çš„ä¸€ä¸ªå­é¡¹ç›®ï¼Œæ—¨åœ¨æ”¯æŒè‡ªå®šä¹‰æ“ä½œ (å°¤å…¶æ˜¯é’ˆå¯¹ TPU)ï¼Œå®ƒæ·±å— Triton å¯å‘ï¼Œä½†æ˜¯æš´éœ²äº†æ›´å¤šçš„åº•å±‚ç¼–è¯‘å™¨ç»†èŠ‚ï¼Œè€Œä¸æ˜¯æä¾›ä¸€ä¸ªå‹å¥½çš„é«˜å±‚æ¬¡ API

From an outsiderâ€™s perspective, Pallas appearsÂ **powerful but difficult to use**, requiring deep knowledge of TPU hardware and compiler internals. Its own documentation highlightsÂ [numerous footguns](https://docs.jax.dev/en/latest/pallas/design/async_note.html#why-doesnt-this-work), making it clear that this is a tool for experts with low-level knowledge. As a result, adoption outside Google has been limited.
>  Pallas å®é™…ä¸Šæ˜¯é¢å¯¹å…·æœ‰åº•å±‚çŸ¥è¯†ä¸“å®¶çš„å·¥å…·ï¼Œå› æ­¤åœ¨ Google ä¹‹å¤–çš„ä½¿ç”¨ç‡ä¸€ç›´æ¯”è¾ƒæœ‰é™

### CUTLASS Python and cuTile
AtÂ **GTC 2025**, NVIDIA announced two new Python eDSLs:Â **CUTLASS Python**Â andÂ **cuTile**. Neither are available for download yet, but here are some initial impressions:
>  NVIDIA åœ¨ GTC 2025 ä¸Šå®£å¸ƒäº†ä¸¤æ¬¾æ–°çš„ Python eDSL: CUTLASS Python å’Œ cuTile

- **CUTLASS Python**Â â€“ Presented inÂ [this GTC talk](https://www.nvidia.com/gtc/session-catalog/?tab.catalogallsessionstab=16566177511100015Kus#/session/1738891305735001ygGc), it looks heavily inspired by Google Pallas. It exposesÂ **low-level compiler details**Â and requires deep hardware knowledge, without the tooling or debuggers that CUDA developers rely on. Itâ€™s launching on Blackwell first, and I doubt NVIDIA will open-source it or support other hardware vendors. Iâ€™m also curious to see how well Pythonâ€™s lack of static typing works for writing low-level systems code like this.
>  CUTLASS Python çœ‹èµ·æ¥æ·±å— Google Pallas çš„å¯å‘ï¼Œå®ƒæš´éœ²äº†æ›´å¤šåº•å±‚ç¼–è¯‘å™¨ç»†èŠ‚ï¼Œå¹¶è¦æ±‚å¯¹ç¡¬ä»¶æœ‰æ·±å…¥ç†è§£ï¼Œä½†ä¹Ÿæ²¡æœ‰ CUDA å¼€å‘è€…æ‰€ä¾èµ–çš„å·¥å…·é“¾å’Œ debugger
>  å®ƒå°†é¦–å…ˆåœ¨ Blackwell ä¸Šå‘å¸ƒ

- **cuTile**Â â€“ This was widely reshared on XÂ [(example)](https://x.com/blelbach/status/1902113767066103949), but aside from a few slides, nothing is known about the availability dates or the technical details. It appears to be positioned as a proprietary Triton alternative. NVIDIAÂ admitsÂ [cuTile is approximately 15% slower than TRT-LLM](https://x.com/blelbach/status/1905707348506918967). Given NVIDIAâ€™s focus on peak performance, itâ€™s unclear if it will use cuTile to build its own CUDA libraries. If it ships,Â **real adoption inside NVIDIA will be the true test**.
>  cuTile çœ‹èµ·æ¥è¢«å®šä½ä¸ºä¸€ç§ä¸“æœ‰çš„ Triton æ›¿ä»£æ–¹æ¡ˆï¼ŒNVIDIA æ‰¿è®¤ cuTile çš„æ€§èƒ½å¤§çº¦æ¯” TRT-LLM æ…¢ 15%

These eDSLs are just part of NVIDIAâ€™s sprawling Python GPU ecosystem. AtÂ **GTC 2025**,Â [NVIDIA said](https://www.nvidia.com/gtc/session-catalog/?tab.catalogallsessionstab=16566177511100015Kus&search=what%27s%20new%20in%20cuda#/session/1726614035480001yvEQ),Â _â€œThere is no one toolâ€”_**_you are going to pick_**Â _the right tool for the job.â€_Â NVIDIA even had a session calledÂ [â€œ1,001 Ways to Write CUDA Kernels in Pythonâ€](https://www.nvidia.com/gtc/session-catalog/?tab.catalogallsessionstab=16566177511100015Kus#/session/1727175449007001EIKh)â€”just the thought of having to pick the right path sounds like a nightmare.

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67e1a27b05acf3e338d1bbe2_Screenshot%202025-03-22%20at%2010.44.07%E2%80%AFAM.png)

According to NVIDIA, "there's no single tool that's optimal for all applications." (Source: NVIDIAÂ GTC 2025,Â [CUDA: New Features and Beyond](https://www.nvidia.com/en-us/on-demand/session/gtc24-s62400/))

As a developer, I donâ€™t think that dozens of options with subtle tradeoffs helps me. We needÂ **fewer tools that work better**â€”not an ever-growing list of tradeoffs. NVIDIA isÂ **fragmenting its own developer ecosystem**.
>  ç¬”è€…ä½œä¸ºå¼€å‘è€…ï¼Œè®¤ä¸ºæˆ‘ä»¬éœ€è¦çš„æ˜¯æ›´å°‘ä½†æ›´ä¼˜ç§€çš„å·¥å…·ï¼Œè€Œä¸æ˜¯ä¸æ–­å¢é•¿çš„æƒè¡¡åˆ—è¡¨ï¼ŒNVIDIA æ­£åœ¨ç ´åè‡ªå·±çš„å¼€å‘è€…ç”Ÿæ€ç³»ç»Ÿ

## MLIR: A Unified Future for AI Compilers?
As I worked to scale Google TPUs in 2017 and 2018, a pattern emerged:Â **first-generation AI frameworks like TensorFlow and PyTorch lacked scalability, while the second generation AI compilers like**Â [**XLA**](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers)Â **sacrificed flexibility**. To break this cycle, I led the team to build a newÂ [**MLIR compiler framework**](https://en.wikipedia.org/wiki/MLIR_\(software\)) â€”a modular, extensible compiler framework designed to support AIâ€™s rapidly evolving hardware landscape.

Did it succeed? MLIR drove industry-wide breakthroughsâ€”Python DSLs likeÂ **Triton, cuTile, and others**Â were built on top of it, redefining GPU programming. But likeÂ [**TVM and XLA**](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers)Â before it, MLIR facesÂ **governance challenges, fragmentation, and competing corporate interests**. The vision of a trulyÂ **unified**Â AI compiler stack still seems just out of reach, caught in the same power struggles that have shaped the industry for decades.
>  MLIR å¸¦æ¥äº†è¡Œä¸šæ€§çš„çªç ´ï¼Œåƒ Triton, cuTile å’Œå…¶ä»– Python DSL éƒ½å»ºç«‹åœ¨å®ƒä¹‹ä¸Šï¼Œé‡æ–°å®šä¹‰äº† GPU ç¼–ç¨‹
>  ä½†å’Œ TVM ä»¥åŠ XLA ä¸€æ ·ï¼ŒMLIR ä¹Ÿé¢ä¸´ç€æ²»ç†æŒ‘æˆ˜ã€ç¢ç‰‡åŒ–ä»¥åŠä¼ä¸šé—´çš„ç«äº‰åˆ©ç›Š
>  ä¸€ä¸ªçœŸæ­£ç»Ÿä¸€çš„çš„ AI ç¼–è¯‘å™¨æ ˆçš„æ„¿æ™¯ä¼¼ä¹é¥ä¸å¯åŠï¼Œè¢«å›°åœ¨å‡ åå¹´æ¥å¡‘é€ æ•´ä¸ªè¡Œä¸šçš„æƒåŠ›æ–—äº‰ä¸­

Fragmentation seems inevitable, and resistance isÂ ~~cuTile~~Â futile. Can a unifying compiler technologyÂ **actually**Â help Democratize AI Compute?

**Tune in next time**â€”weâ€™ll dive into MLIR:Â **the good, the badâ€¦ and the organizational dynamics.**

â€”Chris

# 8 What about the MLIR compiler infrastructure?
Site: https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure
Date: 8 April 2025

By 2018, AI software had a system fragmentation problem. TensorFlow, PyTorch, JAX, Glow, ONNX, TensorFlow-Lite, XLA, TVMâ€”the list kept growing, and each framework invented its own tangled web of â€œAI graphsâ€ with different â€œops.â€Â The ecosystem wasÂ **splintering into silos**, each racing to optimize for different hardware while reinventing the same ideas with subtle variations. Complexity was exploding, and something had to give.
>  åˆ° 2018 å¹´ï¼ŒAI è½¯ä»¶å·²ç»å‡ºç°äº†ç³»ç»Ÿç¢ç‰‡åŒ–çš„é—®é¢˜: TensorFlow, PyTorch, JAX, Glow, ONNX, TensorFlow-Lite, XLA, TVM â€”â€” åˆ—è¡¨ä¸æ–­å¢é•¿ï¼Œæ¯ä¸ªæ¡†æ¶éƒ½å‘æ˜äº†è‡ªå·±çš„ AI å›¾å’Œä¸åŒçš„ opsï¼Œç”Ÿæ€ç³»ç»Ÿæ­£åœ¨åˆ†è£‚ä¸ºä¸€ä¸ªä¸ªå­¤å²›ï¼Œæ¯ä¸ªéƒ½ç«ç›¸ä¼˜åŒ–ä¸åŒçš„ç¡¬ä»¶ï¼ŒåŒæ—¶ä»¥ç»†å¾®çš„å·®å¼‚é‡å¤ç€ç›¸åŒçš„æƒ³æ³•ï¼Œå¤æ‚æ€§æ­£åœ¨çˆ†ç‚¸å¼å¢é•¿

At the time, I was helping scale Googleâ€™s TPUs (and several other internal ASICs) in support of TensorFlow, and it was clear we couldnâ€™t keep reinventing compiler infrastructure from scratch for every project. We needed a better foundation. Fortunately, I had years of experience building LLVMâ€”and Jeff Dean as my manager. Jeff,Â [a legendary engineer](https://en.wikipedia.org/wiki/Jeff_Dean)Â and a compiler PhD himself, saw the same problem.

In a 1:1 conversation, Jeff said something like:

> â€œHey Chris, I agree we have a compiler problem. Why donâ€™t you go build a new compiler to unify this mess?â€

And so,Â [MLIR](https://en.wikipedia.org/wiki/MLIR_\(software\))Â was bornâ€”aÂ **modular, extensible compiler infrastructure**Â designed to bring order to the chaos. It brought forth a foundation that could scale across hardware platforms, software frameworks, and the rapidly evolving needs of machine learning. It aimed to unify these systems, and provide a technology platform that could harmonize compute from many different hardware makers.
>  MLIR æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‹“å±•çš„ç¼–è¯‘å™¨åŸºç¡€è®¾æ–½ï¼Œæ—¨åœ¨ä¸ºæ··ä¹±å¸¦æ¥ç§©åº
>  å®ƒæä¾›äº†ä¸€ä¸ªå¯ä»¥è·¨ç¡¬ä»¶å¹³å°ã€è½¯ä»¶æ¡†æ¶ä»¥åŠæœºå™¨å­¦ä¹ å¿«é€Ÿå˜åŒ–çš„éœ€æ±‚è¿›è¡Œæ‹“å±•çš„åŸºç¡€
>  å®ƒçš„ç›®æ ‡æ˜¯ç»Ÿä¸€è¿™äº›ç³»ç»Ÿï¼Œå¹¶æä¾›ä¸€ä¸ªæŠ€æœ¯å¹³å°ï¼Œèƒ½å¤Ÿåè°ƒæ¥è‡ªä¸åŒç¡¬ä»¶å‚å•†çš„è®¡ç®—èµ„æº

But unification isÂ _hard_. What started as a technical project quickly turned into a battleground:Â **open-source governance**,Â **corporate rivalries**, andÂ **competing visions**Â all collided. What could have been a straightforward engineering win became something much more complicated.

Today, MLIR is embedded in nearly every major AI stackâ€”includingÂ [**CUDA**](https://x.com/JokerEph/status/1902758983116657112)â€”yet it still hasnâ€™t delivered on the dream of democratized AI compute.

This is the story of MLIR: how it started, what it changed, and the power struggles along the way.

## MLIR, the Origin Story
Modern AI systems rely on complex graphs of operationsâ€”matrix multiplications, convolutions, attention mechanisms, and moreâ€”all strung together into computational pipelines. To optimize and transform these efficiently requires a solid compiler foundation, as discussed inÂ [part 6](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers).
>  ç°ä»£ AI ç³»ç»Ÿä¾èµ–äºå„ç§æ“ä½œç»„æˆçš„å¤æ‚è®¡ç®—å›¾ - çŸ©é˜µä¹˜ã€å·ç§¯ã€æ³¨æ„åŠ›ç­‰
>  è¿™äº›æ“ä½œè¢«ä¸²è”ä¸ºè®¡ç®—æµæ°´çº¿
>  ä¸ºäº†ä¼˜åŒ–å’Œè½¬æ¢è¿™äº›è®¡ç®—ï¼Œéœ€è¦ä¸€ä¸ªåšå®çš„ç¼–è¯‘å™¨åŸºç¡€

But in 2018, most AI frameworks were reinventing compiler technologyâ€”and often doing it poorly. Basic techniques likeÂ [**Static Single Assignment (SSA)**](https://en.wikipedia.org/wiki/Static_single-assignment_form)Â wereÂ [missing from many](https://www.tensorflow.org/guide/graph_optimization). Each framework had its own ad-hoc graph system, bolted together with hacks that didnâ€™t scale. The result was a fragmented, inefficient ecosystem, riddled with duplication.
>  2018 å¹´ï¼Œå¤§å¤šæ•° AI æ¡†æ¶éƒ½åœ¨é‡æ–°å‘æ˜ç¼–è¯‘å™¨æŠ€æœ¯ï¼Œå¹¶ä¸”å¾€å¾€åšå¾—ä¸å¥½ï¼Œåƒ SSA è¿™æ ·çš„åŸºæœ¬æŠ€æœ¯åœ¨è®¸å¤šæ¡†æ¶ä¸­éƒ½ç¼ºå¤±äº†
>  æ¯ä¸ªæ¡†æ¶éƒ½æœ‰è‡ªå·±çš„ä¸´æ—¶å›¾ç³»ç»Ÿï¼Œå°†ä¸€äº›æ— æ³•æ‹“å±•çš„æŠ€å·§æ‹¼å‡‘åœ¨ä¸€èµ·ï¼Œç»“æœæ˜¯ä¸€ä¸ªç¢ç‰‡åŒ–ã€ä½æ•ˆçš„ç”Ÿæ€ç³»ç»Ÿï¼Œå……æ»¡äº†å†—ä½™

I knew we needed a better approach, so I pulled four like-minded folks into a small room at Google. We spent days white-boarding, sketching out what a modern, scalable compiler infrastructure for AI might look like. Our central question:Â **Could we build a unified representation that could support every AI framework, every hardware backend, and every kind of optimizationâ€”from algebraic simplification to**Â [**polyhedral analysis**](https://en.wikipedia.org/wiki/Frameworks_supporting_the_polyhedral_model)**?**
>  MLIR çš„æ ¸å¿ƒé—®é¢˜æ˜¯: æ˜¯å¦å¯ä»¥æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„**è¡¨ç¤ºæ–¹å¼**ï¼Œèƒ½å¤Ÿæ”¯æŒæ¯ä¸ª AI æ¡†æ¶ï¼Œæ¯ä¸€ç§ç¡¬ä»¶åç«¯ï¼Œä»¥åŠä»ä»£æ•°ç®€åŒ–åˆ°å¤šé¢ä½“åˆ†æçš„æ¯ä¸€ç§ä¼˜åŒ–

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67f6a4f6fa55c1977bc3ef2d_6B932DBF-B757-4307-BE55-CA50E01194A5.png)

Circa 2018: Yours truly and four colleagues gather in front of a whiteboard to brainstorm a next-generation compiler

The breakthrough idea we created is now known asÂ **MLIR dialects**â€”a way to cleanly separateÂ **domain-specific**Â concerns from theÂ **core infrastructure**Â of a compiler. Rather than forcing every user to adopt a rigid, one-size-fits-all intermediate representation (likeÂ [LLVM](https://llvm.org/pubs/2002-12-LattnerMSThesis.html)Â and other compilers), MLIR would let compiler engineers define their own representationsâ€”custom ops, types, and semanticsâ€”tailored to their domain.
>  è¿™ä¸ªæƒ³æ³•åœ¨ç°åœ¨ç§°ä¸º MLIR æ–¹è¨€ - ä¸€ç§å°†é¢†åŸŸç‰¹å®šé—®é¢˜ä¸ç¼–è¯‘å™¨çš„æ ¸å¿ƒåŸºç¡€è®¾ç½®åˆ†ç¦»çš„æ–¹æ³•
>  ä¸å¼ºåˆ¶æ‰€æœ‰ç”¨æˆ·é‡‡ç”¨ä¸€ä¸ªå›ºå®šçš„ã€é€šç”¨çš„ä¸­é—´è¡¨ç¤ºï¼Œä¾‹å¦‚ LLVM ä¸åŒï¼ŒMLIR å…è®¸ç¼–è¯‘å™¨å·¥ç¨‹å¸ˆæ ¹æ®è‡ªå·±çš„é¢†åŸŸéœ€æ±‚å®šä¹‰è‡ªå·±çš„è¡¨ç¤ºæ–¹å¼ - è‡ªå®šä¹‰ ops, types, semantics

> _Aside: Iâ€™m not diving deep on how MLIR works in this post. If youâ€™re curious, check out the_Â [_original technical keynote_](https://llvm.org/devmtg/2019-04/talks.html#Keynote_1)Â _or one of the_Â [_many tutorials online_](https://lowlevelbits.org/compiling-ruby-part-3/)_._

At the time, this was a radical departure from how most compilers were built. Traditional infrastructures were monolithicâ€”forcing all frontends and passes into a single, rigid model. But MLIR embracedÂ **heterogeneity**Â from day one. It let multiple levels of abstraction coexist, transform, and interoperate seamlessly.
>  è¿™ä¸å¤§å¤šæ•°ç¼–è¯‘å™¨çš„æ„å»ºæ–¹å¼å½¢æˆäº†å·¨å¤§å·®å¼‚ï¼Œä¼ ç»Ÿçš„ç¼–è¯‘å™¨åŸºç¡€è®¾ç½®æ˜¯ç»Ÿä¸€åŒ–çš„ - å¼ºè¿«æ‰€æœ‰å‰ç«¯å’Œ passes é€‚åº”ä¸€ä¸ªå›ºå®šçš„æ¨¡å‹
>  MLIR ä»è®¾è®¡ä¸Šæ‹¥æŠ±äº†å¤šæ ·æ€§ï¼Œå®ƒå…è®¸å¤šä¸ªæŠ½è±¡å±‚æ¬¡å…±å­˜ã€è½¬æ¢ï¼Œå¹¶æ— ç¼åœ°ç›¸äº’åä½œ

That modularity was the key. Instead of reimplementing the same infrastructure over and over, MLIR gave developers a shared foundationâ€”whether they were working with TensorFlow graphs, PyTorch IR, or custom TPU ops. It made it possible to build specialized compilers without starting from scratch, and it enabledÂ **true composability**Â across the AI compiler stack.
>  è¿™ç§æ¨¡å—åŒ–æ˜¯å…³é”®æ‰€åœ¨ï¼ŒMLIR ä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªå…±äº«çš„åŸºç¡€ - æ— è®ºä»–ä»¬æ˜¯åœ¨å¤„ç† TensorFlow å›¾, PyTorch IR, è‡ªå®šä¹‰ TPU ops
>  å®ƒä½¿å¾—ä¸éœ€è¦ä»é›¶å¼€å§‹æ„å»ºä¸“ç”¨ç¼–è¯‘å™¨ï¼Œå¹¶å®ç°äº† AI ç¼–è¯‘å™¨æ ˆä¸­çœŸæ­£çš„å¯ç»„åˆæ€§

MLIR wasnâ€™t just another compiler: It was a framework for buildingÂ **many**Â compilers.
>  MLIR ä¸æ˜¯å¦ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œè€Œæ˜¯ä¸€ä¸ªæ„å»ºæ›´å¤šç¼–è¯‘å™¨çš„æ¡†æ¶

## MLIR Growth Within Google and Beyond
MLIR began as a research project insideÂ [**Google Brain**](https://research.google.com/teams/brain/)Â as a focused team trying to rethink how AI compilers should work. My team was heads-down on the fundamentals: designing the IR, implementing transformations, and validating that the core ideas actually worked. Meanwhile, Googleâ€™s open culture and MLIRâ€™s modular design made it easy for others to pick it up and experiment. Before long, MLIR took on a life of its own.

Across Google, teams working onÂ **custom ASICs**Â saw the potential. MLIR gave them a structured way to express and optimize hardware-specific operations.Â **Application-focused teams**Â started using it forÂ **mobile AI**, and theÂ **TensorFlow team**Â brought MLIR intoÂ **TensorFlow Lite**. Even individual researchers, fascinated by MLIRâ€™s flexibility, began using it to prototype novel compiler techniques.
>  MLIR ä¸º ASIC æä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹å¼æ¥è¡¨è¾¾å’Œä¼˜åŒ–ä¸ç¡¬ä»¶ç›¸å…³çš„æ“ä½œ
>  TensorFlow å›¢é˜Ÿå°† MLIR å¼•å…¥äº† TensorFlow Lite

What followed was aÂ **mini-explosion**Â of use cases. Every new application brought fresh feedback, often while we were still deep in iteration mode. Crucially, this validated our dialect-first approachâ€”proving that MLIR could scale across wildly different domains, from edge devices to datacenter accelerators. Eventually, we reached a tipping point: MLIR was becoming a critical piece of infrastructure across many projects.
>  ä»¥æ–¹è¨€ä¸ºæ ¸å¿ƒçš„æ–¹æ³•ä½¿å¾— MLIR å¯ä»¥åœ¨ä»è¾¹ç¼˜è®¾å¤‡åˆ°æ•°æ®ä¸­å¿ƒåŠ é€Ÿå™¨ç­‰å¤šä¸ªä¸åŒé¢†åŸŸä¸­æ‹“å±•ï¼ŒMLIR æ­£æˆä¸ºè®¸å¤šé¡¹ç›®ä¸­ä¸å¯æˆ–ç¼ºçš„åŸºç¡€æ¶æ„

Many of us wanted MLIR to reach its full potentialâ€”to go beyond Googleâ€™s first-party use cases.

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67f527662cb4f9e15dc06b6d_image.jpeg)

Above: a well-known meme within the MLIR community (Credit: Mehdi Amini)

So we took the leap: weÂ **open-sourced MLIR**Â andÂ [contributed it to theÂ **LLVM Foundation**](https://blog.google/technology/ai/mlir-accelerating-ai-open-source-infrastructure/), making it available for the entire industry. To support adoption, we organized regular â€œ[open design meetings](https://mlir.llvm.org/talks/#open-design-meeting-presentations),â€ where external contributors could participate in MLIRâ€™s evolution and benefit from the engineering investment behind it. This open collaboration helped catalyze MLIRâ€™s global momentum, especially among compiler developers hungry for a modern infrastructure.

**With this as fuel, MLIR took off:**Â It is now theÂ [foundation for many major AI projects](https://mlir.llvm.org/users/):Â [OpenXLA](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers),Â [Triton](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls), and even parts of CUDA itself. Itâ€™s also powering compilers in quantum computing, hardware design ([via CIRCT](https://circt.llvm.org/)), andÂ [many other domains](https://mlir.llvm.org/users/). Companies around the worldâ€”from scrappy startups to hyperscalersâ€”started building their next-generation compilers using MLIR. Much of MLIRâ€™s early growth and success wasÂ **directly attributable to Googleâ€™s leadership and open approach**â€”something I think the industry still under-appreciates.
>  MLIR å·²ç»æˆä¸ºè®¸å¤š AI é¡¹ç›®çš„åŸºç¡€ï¼Œä¾‹å¦‚ OpenXLA, Triton, ä»¥åŠ CUDA çš„éƒ¨åˆ†ç»„ä»¶
>  å…¨çƒå„åœ°çš„å…¬å¸å¼€å§‹ä½¿ç”¨ MLIR æ„å»ºä¸‹ä¸€ä»£ç¼–è¯‘å™¨ï¼ŒMLIR æ—©æœŸçš„å¢é•¿å’ŒæˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäº Google çš„é¢†å¯¼ä½œç”¨å’Œå¼€æ”¾æ€åº¦

Yet for all that success, the grand vision remained out of reach. The ecosystem is still fractured. CUDA still reigns supreme. The dream of truly democratized AI compute remains just thatâ€”a dream.
>  ä½†ç”Ÿæ€ç³»ç»Ÿä»ç„¶ç¢ç‰‡åŒ–ï¼ŒCUDA ä»ç„¶å æ®ä¸»å¯¼åœ°ä½

So what happened? Why did MLIR succeedÂ _technically_, but fail to break the CUDA lock-in?

To understand that, we need to talk about theÂ **politics, power struggles, and compromises**Â that shaped MLIRâ€™s evolution.

## The Race to Claim an End-to-end AI Solution
From the outset, MLIR was conceived asÂ **general-purpose compiler infrastructure**â€”a framework designed to allow forÂ **domain-specific compilers**. The goal was flexibility and modularityâ€”MLIR was never just about Machine Learning. In fact, the â€œMLâ€ in MLIR stood forÂ [_everything but Machine Learning_](https://www.youtube.com/watch?si=PV49fAovBS3pkKo8&t=352&v=qzljG6DKgic&feature=youtu.be)Â (yep, compiler jokes are nerdy!). However, the AI community was hungry for something more. The AI world wanted anÂ **end-to-end compiler**â€”something that could map TensorFlow or PyTorch models cleanly to a broad range of hardware.
>  MLIR æœ€åˆè¢«è®¾è®¡ä¸ºä¸€ä¸ªé€šç”¨çš„ç¼–è¯‘å™¨åŸºç¡€è®¾æ–½ - ä¸€ä¸ªæ—¨åœ¨æ”¯æŒ**é¢†åŸŸä¸“ç”¨ç¼–è¯‘å™¨**çš„æ¡†æ¶
>  MLIR çš„ç›®çš„æ˜¯å®ç°çµæ´»åŒ–å’Œæ¨¡å—åŒ–ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸ºäº†æœºå™¨å­¦ä¹ 
>  ç„¶è€Œï¼ŒAI ç¤¾åŒºæ¸´æœ›æ›´å¼ºå¤§çš„ç¼–è¯‘å™¨ï¼ŒAI é¢†åŸŸéœ€è¦ä¸€ä¸ª**ç«¯åˆ°ç«¯çš„**ç¼–è¯‘å™¨ - å°† TensorFlow æˆ–è€… PyTorch æ¨¡å‹å¹²å‡€åœ°æ˜ å°„åˆ°å„ç§ç¡¬ä»¶å¹³å°ä¸Š

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67f6a522747a174c82c63a69_6EB9E592-117A-41BB-95C7-33BC7CA0CAA8.png)

The race was on to build the first end-to-end MLIR-based AI solution

As MLIR gained traction, teams inside and outside Google began racing to build anÂ **end-to-end AI solution**Â on top of it. Other projectsâ€”likeÂ [OpenXLA](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers),Â [TritonLang](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls)Â and many othersâ€”adopted MLIR as an implementation detail to strengthen their own stacks. This raised a question: Everyone wanted to be the next-gen AI stackâ€”so who would get there first?
>  éšç€ MLIR å½±å“åŠ›æ‰©å¤§ï¼ŒGoogle å†…å¤–çš„å›¢é˜Ÿå¼€å§‹åŸºäº MLIR ç«ç›¸æ„å»ºç«¯åˆ°ç«¯çš„ AI è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ OpenXLA, TritonLang
>  ä»–ä»¬å°† MLIR ä½œä¸ºå…¶å®ç°ç»†èŠ‚ï¼Œä»¥å¢å¼ºè‡ªèº«çš„æŠ€æœ¯æ ˆ

The race was on. Years later, we know the unfortunate answer:Â **nobody**.
>  é—®é¢˜æ˜¯ï¼Œæ¯ä¸€ä¸ªé¡¹ç›®éƒ½æƒ³è¦æˆä¸ºä¸‹ä¸€ä»£ AI stackï¼Œä½†æ²¡æœ‰ä¸€ä¸ªæˆåŠŸ

##### **MLIRâ€™s AI Dialect Explosion**
Contributing MLIR to theÂ [LLVM Foundation](https://foundation.llvm.org/)Â supercharged adoption. It gave companies a shared foundationâ€”and compiler engineers a chance to prove serious impact inside their organizations. The LLVM Foundation helps with oversight and legal matters, but doesnâ€™t intervene in technical design. For that, the community is left to self-organize.
>  LLVM åŸºé‡‘ä¼šè´Ÿè´£ç›‘ç£å’Œæ³•å¾‹äº‹åŠ¡ï¼Œä¸ä¼šå¹²é¢„æŠ€æœ¯è®¾è®¡ï¼Œå› æ­¤æŠ€æœ¯ä¸Šçš„å†³ç­–ç”±ç¤¾åŒºè‡ªè¡Œç»„ç»‡

Engineers across the industry, led by Google, started contributingÂ **AI-specific dialects**â€”includingÂ [**arith, linalg, and tensor**](https://mlir.llvm.org/docs/Dialects/) â€”providing some bits and pieces useful for building a modern AI compiler stack. It started with Google research teams who had early access to MLIRâ€”but the precedent was set: many â€œpotentially usefulâ€ contributions were upstreamed, withÂ [limited governance](https://mlir.llvm.org/getting_started/DeveloperGuide/#guidelines-on-contributing-a-new-dialect-or-important-components)Â that allowed project leaders to say â€œnoâ€ in a principled way.
>  è¡Œä¸šå†…çš„å·¥ç¨‹å¸ˆï¼Œå°¤å…¶æ˜¯ Googleï¼Œå¼€å§‹è´¡çŒ®é’ˆå¯¹ AI çš„æ–¹è¨€ï¼ŒåŒ…æ‹¬äº† `arith, linalg, tensor` - ä¸ºæ„å»ºç°ä»£ AI ç¼–è¯‘å™¨æ ˆæä¾›äº†éƒ¨åˆ†æœ‰ç”¨çš„ç»„ä»¶
>  è®¸å¤š â€œå¯èƒ½æœ‰ç”¨â€ çš„è´¡çŒ®è¢«åˆå¹¶åˆ°ä¸»åˆ†æ”¯ä¸­ï¼Œä¸”æœ‰é™çš„æ²»ç†æœºåˆ¶å…è®¸é¡¹ç›®è´Ÿè´£äººä»¥åˆç†çš„æ–¹å¼æ‹’ç»æŸäº›æè®®

Unfortunately, this explosion happened very early in MLIRâ€™s design, and many design decisions in these dialects werenâ€™t ideal for the evolving requirements of GenAI. For example, much of this early work was directed towards improving TensorFlow and building OpenXLA, so these dialects werenâ€™t designed with first-class PyTorch and GenAI support (as we discussedÂ [earlier in this series](https://www.modular.com/ai-resources/mac)).
>  è¿™æ ·çš„å¿«é€Ÿæ‹“å±•å‘ç”Ÿåœ¨ MLIR çš„æ—©æœŸé˜¶æ®µï¼Œè¿™äº›æ–¹è¨€ä¸­çš„è®¸å¤šè®¾è®¡å†³ç­–å¹¶ä¸é€‚åˆ GenAI ä¸æ–­å˜åŒ–çš„éœ€æ±‚
>  ä¾‹å¦‚ï¼Œæ—©æœŸå¾ˆå¤šå·¥ä½œéƒ½æ˜¯é’ˆå¯¹ä¸ºäº†æ”¹è¿› TensorFlow å’Œæ„å»º OpenXLAï¼Œå› æ­¤è¿™äº›æ–¹è¨€å¹¶æ²¡æœ‰ä¸“é—¨è®¾è®¡ç”¨äºæ”¯æŒ PyTorch å’Œ GenAI

While many of these efforts hit their original goals, the world changed around them.

## Competitive â€œCoopetitionâ€ Strikes Back
For a variety of reasons, almost all of the early MLIR developers (including myself) moved on from Google, with many of them ending up at hardware companies. Â This spread of MLIR knowledge was a positive outcomeâ€”it meant that the technology would grow far and wideâ€”but it also brought new challenges.

The problem? MLIRâ€™s success scattered its core developers across the industry. FormerÂ **allies and colleagues**â€”now at competing companiesâ€”began building proprietary AI stacks on top of shared MLIR dialects. What began as open collaboration soon collided with commercial competition. With a lack of central coordination,Â **communication between these teams broke down**. Competing priorities created tension, and the once-unified vision for MLIR began to splinter.
>  MLIR çš„æ ¸å¿ƒå¼€å‘è€…ç°åœ¨åˆ†æ•£åˆ°äº†æ•´ä¸ªè¡Œä¸šï¼ŒåŸæœ¬çš„å¼€æ”¾åˆä½œä¸å•†ä¸šç«äº‰å‘ç”Ÿäº†å†²çª

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67f6a964522a7c4881e1a1b2_4A67E5F5-1758-4AB7-BB8D-EA23AD89F0E1.png)

MLIR's identity crisis: Machine learning solution or compiler framework?

MLIR now faces is an identity crisis: Is it a general-purpose compiler framework for any domainâ€”or an AI solution? Today, it remains unmatched asÂ **general-purpose, reusable infrastructure**, powering everything from hardware design to quantum computing. On the other hand, the built-in AI-related dialects are contested and incompleteâ€”but still critical to many open and proprietary downstream stacks.
>  MLIR å¦‚ä»Šé¢ä¸´èº«ä»½å±æœº: å®ƒæ˜¯ä¸€ä¸ªé€‚ç”¨äºä»»ä½•é¢†åŸŸçš„é€šç”¨ç¼–è¯‘å™¨æ¡†æ¶ï¼Œè¿˜æ˜¯ä¸€ç§ AI è§£å†³æ–¹æ¡ˆï¼Ÿ

It started to feelÂ **a lot like**Â [**OpenCL all over again**](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives): no reference stack, competing hardware vendors, and aÂ _very polite_Â battlefieldâ€”just like the old Khronos committee.
>  è¿™å¼€å§‹è®©äººæ„Ÿè§‰åƒåˆä¸€æ¬¡çš„ OpenCL: æ²¡æœ‰å‚è€ƒæ ˆã€ç«äº‰çš„ç¡¬ä»¶ä¾›åº”å•†ã€ä»¥åŠä¸€ä¸ªéå¸¸ç¤¼è²Œçš„æˆ˜åœºï¼Œå°±åƒè¿‡å»çš„ Khronos å§”å‘˜ä¼šä¸€æ ·

##### **A New Hope: Improved MLIR Governance**
The tensions have simmered for yearsâ€”and they're deeply felt across the broader LLVM and MLIR communities.
>  è¿™ç§ç´§å¼ çš„å…³ç³»å·²ç»æŒç»­äº†å¤šå¹´

Fortunately,Â **thereâ€™s a new hope**: LLVM is a meritocratic community with a long track record of aligning engineersâ€”even when their companies are at war in the market. The MLIR community is filled with amazing engineers who have poured years of their hearts and souls into improving the project to work through these challenges, and progress is now happening!

MLIR now has a newÂ [Area Team](https://discourse.llvm.org/t/llvm-area-team-election-results/84601)Â to help guide its evolution, along with aÂ [new organizational structure and charter](https://discourse.llvm.org/t/mlir-organization-charter/84118)Â andÂ [governance group](https://mlir.llvm.org/governance/). The charter defines separate area groups: MLIR Core (the domain-independent infrastructure), and the dialects (like the machine learning-specific pieces). Â I am extremely thankful to everyone who is spending time to improve MLIR and work through these issuesâ€”such work has a profound impact on everyone building into the ecosystem as well as the downstream users.
>  MLIR ç°åœ¨æœ‰äº†ä¸€ä¸ªæ–°çš„ Area Team æ¥æŒ‡å¯¼å…¶è¿›å±•ï¼ŒåŒæ—¶è¿˜æœ‰ä¸€å¥—æ–°çš„ç»„ç»‡ç»“æ„å’Œç« ç¨‹ä»¥åŠæ²»ç†å°ç»„
>  ç« ç¨‹å®šä¹‰äº†ä¸åŒé¢†åŸŸçš„å°ç»„: MLIR Core (é¢†åŸŸæ— å…³çš„åŸºç¡€æ¶æ„)ã€å„ç§æ–¹æ¡ˆ (ä¾‹å¦‚ç‰¹å®šäºæœºå™¨å­¦ä¹ çš„éƒ¨åˆ†)

If I could have one wish, it would be for â€MLIRâ€ to unambiguously refer to the domain-independent compiler infrastructure, and for these dialects to get a new, different name (perhaps â€œTensorIRâ€?). This would reduce confusion aboutÂ **what â€œMLIRâ€ actually is**!
>  ç¬”è€…å¸Œæœ› MLIR èƒ½å¤Ÿæ˜ç¡®åœ°æŒ‡ä»£ä¸é¢†åŸŸæ— å…³çš„ç¼–è¯‘å™¨åŸºç¡€è®¾æ–½ï¼Œè€Œæ–¹è¨€åˆ™è·å¾—å…·ä½“çš„åç§°

## Lessons learned from MLIR
The biggest lesson I learned from MLIR is howÂ **scaling too early**â€”before the core foundations are fully settledâ€”can cause lasting problems. The early explosion of interest and contribution was exciting, but it also meant that many design decisions were made in parallel, without clear guidance or alignment. We got â€œmany things fastâ€ at the expense of getting â€œsomething great at each level,â€ and then fell prey toÂ [Hyrum's Law](https://peterm.hashnode.dev/hyrums-law).
>  MLIR æœ€å¤§çš„æ•™è®­å°±æ˜¯è¿‡æ—©åœ°æ‹“å±• - åœ¨æ ¸å¿ƒåŸºç¡€å°šæœªå®Œå…¨ç¡®å®šä¹‹å‰ - ä¼šå¯¼è‡´é•¿æœŸçš„é—®é¢˜
>  æ—©æœŸçš„è´¡çŒ®å’Œå…´è¶£æ¿€å¢æ„å‘³ç€è®¸å¤šè®¾è®¡å†³ç­–æ˜¯åœ¨ç¼ºä¹æ˜ç¡®æŒ‡å¯¼å’Œåè°ƒçš„æƒ…å†µä¸‹å¹¶è¡Œåšå‡ºçš„
>  æˆ‘ä»¬ä»¥ç‰ºç‰² â€œæ¯ä¸ªå±‚é¢éƒ½åšåˆ°æè‡´â€ ä¸ºä»£ä»·ï¼Œå¿«é€Ÿå®ç°äº† â€œå¾ˆå¤šä¸œè¥¿â€ï¼Œéšåé™·å…¥äº† Hyrum å®šå¾‹çš„å›°å¢ƒ

This also reinforced aÂ **management lesson**Â Iâ€™ve learned in other places: when you have too many smart engineers running ahead in different directions, itâ€™s hard to steer the ship laterâ€”even if the ship is made of beautifully designed IR. In this case, while I remain influential in the LLVM/MLIR community, I learned that influence is no match for the paycheck from an employer, which guides a contributor to get their work into the tree so they can move on to the next bug fix or project.
>  è¿™ä¹Ÿæ˜¯ä¸€ä¸ªç®¡ç†ä¸Šçš„æ•™è®­: å½“å¤ªå¤šèªæ˜çš„å·¥ç¨‹å¸ˆæœç€ä¸åŒæ–¹å‘å‰è¿›æ—¶ï¼ŒåæœŸå¾ˆéš¾å†æŒæ§æ–¹å‘

Another lesson is aboutÂ **infrastructure with ambition**. My goal for MLIR was to unify compiler implementationsâ€”and it succeeded beyond my hopes. But I also encouraged and catalyzed others to aim beyond that, fueled by a shared optimism that community-led projects could move the world forward. That didnâ€™t work out, and it reinforced a lesson of mine seen across other industry-impactful projects Iâ€™ve helped buildâ€”LLVM, Clang, Swift, and â€œMLIR Core.â€ I learned, more than ever, that small teams are best at aligning on a vision of success and driving it forward. Only once a projectâ€™s identity is firmly established does it make sense to scale it to a broader community.
>  å¦ä¸€ä¸ªæ•™è®­æ˜¯å…³äºæœ‰æŠ±è´Ÿçš„åŸºç¡€æ¶æ„ï¼Œå°å›¢é˜Ÿæ›´é€‚åˆåœ¨æˆåŠŸæ„¿æ™¯ä¸Šè¾¾æˆä¸€è‡´å¹¶æ¨åŠ¨å…¶å‰è¿›ï¼Œåªæœ‰åœ¨é¡¹ç›®èº«ä»½å¾—åˆ°ç‰¢å›ºç¡®ç«‹ä¹‹åï¼Œæ‰å€¼å¾—å°†å…¶æ‹“å±•åˆ°æ›´å¹¿æ³›çš„ç¤¾åŒº

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67f6a56760994aeaf994beb1_494D2B28-76E8-4196-A239-E1A724E339D0.png)

MLIR has many dialects, but many are contested or incomplete.

As with the tradition of my lastÂ [three](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls)Â [blog](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers)Â [posts](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives), Iâ€™ll try to evaluate the MLIR AI dialects against the wishlist of features for a next-generation AI solution. Â Hereâ€™s my best take:

- **â€œProvide a reference implementationâ€:**Â While MLIR is excellent for general-purpose compiler infrastructure, it does not include an end-to-end solution that can be used directly for AI workloads, just useful building blocks with â€œsome assembly requiredâ€. ğŸ‘
- â€œ**Have**Â **strong leadership and visionâ€:**Â MLIR AI dialects lacked clear leadership early on, with contributions often driven by individuals or different teams, resulting in fragmented direction and confusion over its core identity. While strong leadership is emerging, it remains unresolved. ğŸ‘
- **â€œRun with top performance on the industry leaderâ€™s hardwareâ€**: WhileÂ _MLIR Core_Â provides a strong foundation for optimization, Iâ€™m not aware of any downstream implementations built on theÂ _MLIR AI Dialects_Â that match CUDAâ€™s performance for GenAI LLMs on NVIDIA GPUs (includingÂ [Triton](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls)Â orÂ [cuTile](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls)Â that leave 15-20% performance on the table). ğŸ‘
>  è™½ç„¶ MLIR Core ä¸ºä¼˜åŒ–æä¾›äº†åšå®çš„åŸºç¡€ï¼Œä½†å°šæœªæœ‰ä»»ä½•åŸºäº MLIR AI æ–¹è¨€çš„å®ç°èƒ½å¤Ÿåœ¨ NVIDIA GPU ä¸Šè¾¾åˆ° CUDA çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ Triton, cuTile

- **â€œEvolve rapidlyâ€:**Â MLIRâ€™s pace of evolution has been impressive, with contributions flooding in from a broad community. The flexibility of its design has allowed for rapid adaptation to new use cases and domains. ğŸ‘
- **â€œCultivate developer loveâ€:**Â MLIR has certainly won the hearts of compiler engineers and system researchers, offering a flexible and powerful toolkit for building custom compilers. ğŸ‘ Â However, AI developers, especially those in the machine learning community, have found the learning curve steep and the integration with existing ML frameworks to be less seamless. ğŸ‘
- **â€œBuild an open communityâ€:**Â MLIR has built a truly open and thriving community. Regular design meetings, open contributions, and cross-company collaboration have helped it gain broad adoption and investment from many industry players.ğŸ‘ğŸ‘
- **â€œAvoid fragmentationâ€:**Â This is where MLIR has struggled the most. The early explosion of dialects and contributions, combined with a lack of strong central governance, led to fragmentation in downstream systems. The vision for a unified approach to AI compilation was difficult to maintain as competing projects moved in different directions.ğŸ‘ğŸ‘ğŸ‘
>  MLIR æœ€æŒ£æ‰çš„åœ°æ–¹å°±æ˜¯ç¢ç‰‡åŒ–ï¼Œæ—©æœŸæ–¹è¨€å’Œè´¡çŒ®çš„çˆ†ç‚¸å¼å¢é•¿ï¼ŒåŠ ä¸Šç¼ºä¹å¼ºæœ‰åŠ›çš„ä¸­å¤®æ²»ç†ï¼Œå¯¼è‡´äº†ä¸‹æ¸¸ç³»ç»Ÿçš„ç¢ç‰‡åŒ–

Ultimately, as we discussed before, this is aÂ **wildly unfair way to measure â€œMLIR coreâ€**Â as a compiler building toolkitâ€”MLIR is widely used byÂ [dozens of systems](https://mlir.llvm.org/users/)Â and has certainly succeeded in its original mission. The success of MLIRâ€™s AI dialects is best measured through its impact on the countless downstream AI implementations that it gets utilized inâ€”Iâ€™m just not sure how to do that.

## Why do HW companies struggle to build AI software?
At this point in the series, a pattern has emerged: whether itâ€™sÂ [OpenCL/OneAPI](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives),Â [TVM/XLA](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers), MLIR, or some other well-meaning acronym, weâ€™ve seen powerful attempts to build unifying AI infrastructureâ€”but none have delivered a solution that developersÂ _love_. Projects fragment, promises fade, and users of alternate hardware are left with tools that donâ€™t â€œjust workâ€.
>  ç›®å‰ä¸ºæ­¢ï¼Œè¿™ä¸ªç³»åˆ—ä¸­å·²ç»å‡ºç°äº†ä¸€ä¸ªæ¨¡å¼: æ— è®ºæ˜¯ OpenCL/OneAPI, TVM/XLA, MLIRï¼Œæˆ‘ä»¬éƒ½çœ‹åˆ°äº†æ„å»ºç»Ÿä¸€ AI åŸºç¡€è®¾æ–½çš„æœ‰åˆ©å°è¯•ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªçœŸæ­£è®©å¼€å‘è€…å–œçˆ±
>  é¡¹ç›®é€æ¸ç¢ç‰‡åŒ–ï¼Œæ‰¿è¯ºé€æ¸æ¶ˆå¤±ï¼Œä½¿ç”¨å…¶ä»–ç¡¬ä»¶çš„ç”¨æˆ·æœ€ç»ˆåªèƒ½å¾—åˆ°é‚£äº› â€œæ— æ³•ç›´æ¥ä½¿ç”¨â€ çš„å·¥å…·

The hard truth is this:Â **only one company has ever truly figured this out, and thatâ€™s NVIDIA**. CUDAÂ [isnâ€™t just infrastructureâ€”itâ€™s a strategy](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed), backed by tight vertical integration, application engineers on the ground, and a relentless focus on real-world performance. Itâ€™sÂ [not open and itâ€™s not pretty](https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good) â€”but it works great for NVIDIA, even ifÂ [the innovatorâ€™s dilemma](https://en.wikipedia.org/wiki/The_Innovator%27s_Dilemma)Â is alive and well in Santa Clara.
>  ç°å®æ˜¯çœŸæ­£è§£å†³è¿™ä¸ªé—®é¢˜çš„å…¬å¸åªæœ‰ NVIDIA, CUDA ä¸ä»…ä»…æ˜¯åŸºç¡€è®¾æ–½ï¼Œæ›´æ˜¯ä¸€ç§æˆ˜ç•¥ï¼Œä¾æ‰˜äºç´§å¯†çš„å‚ç›´æ•´åˆã€ç°åœºçš„åº”ç”¨å·¥ç¨‹å¸ˆï¼Œä»¥åŠå¯¹å®é™…æ€§èƒ½çš„ä¸æ‡ˆè¿½æ±‚

_So, why canâ€™t other hardware companies pull this off?_Â Why do the industryâ€™s smartest people, backed by billions in funding, keep producing software no oneÂ _wants_Â to use? When youâ€™re competing against an entrenched, vertically integrated leader, the deck is stacked against youâ€”and the incentives of the industry and the organizations within it shape the outcome:

> â€œShow me the incentive and I'll show you the outcome.â€  
> â€“Â CharlieÂ Munger

Weâ€™ll dive deeper into that next timeâ€”and until then, let no dialect go uncanonicalized! ğŸ› 

-Chris

# 9 Why do HW companies struggle to build AI software?
Site: https://www.modular.com/blog/democratizing-ai-compute-part-9-why-do-hw-companies-struggle-to-build-ai-software
Date: 22 April, 2025

From the launch of ChatGPT in 2023, GenAI reshaped the tech industryâ€”but GPUs didnâ€™t suddenly appear overnight. Hardware companies have spent billions on AI chips for over a decade. Dozens of architectures. Countless engineering hours. And yetâ€”**still**â€”NVIDIA dominates.
>  è‡ª ChatGPT çš„å‘å¸ƒå¼€å§‹ï¼ŒGenAI é‡å¡‘äº†ç§‘æŠ€è¡Œä¸š
>  ç¡¬ä»¶å…¬å¸å·²ç»åœ¨ AI èŠ¯ç‰‡ä¸ŠæŒç»­æŠ•å…¥äº†æ•°åäº¿ç¾å…ƒï¼ŒæŒç»­äº†åå¤šå¹´ï¼Œæ•°åç§æ¶æ„ï¼Œæ— æ•°çš„å·¥ç¨‹æ—¶é—´ï¼Œç„¶è€Œ NVIDIA ä»ç„¶å æ®ä¸»å¯¼åœ°ä½

**Why?**
Because CUDA is more than an SDK. Itâ€™s a fortress of developer experienceÂ [designed to lock you inâ€”and a business strategy](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed)Â engineered to keep competitors perpetually two years behind. Itâ€™sÂ [not beloved](https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good). Itâ€™s not elegant. But it works, andÂ [nothing else comes close](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed).
>  åŸå› åœ¨äº CUDA ä¸ä»…ä»…æ˜¯ä¸€ä¸ª SDKï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼€å‘è€…ä½“éªŒçš„å ¡å’ï¼Œæ—¨åœ¨è®©ä½ é™·å…¥å…¶ä¸­ï¼Œå¹¶ä¸”æ˜¯ä¸€ç§å•†ä¸šç­–ç•¥ï¼Œæ—¨åœ¨è®©ç«äº‰å¯¹æ‰‹å§‹ç»ˆè½åä¸¤å¹´
>  å®ƒä¸ä¼˜é›…ï¼Œä½†å®ƒæœ‰æ•ˆï¼Œè€Œä¸”æ²¡æœ‰å…¶ä»–ä¸œè¥¿èƒ½æ¥è¿‘å®ƒ

Weâ€™ve spent this series tracing the rise and fall of hopeful alternativesâ€” [OpenCL and SyCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives),Â [TVM and XLA](https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure),Â [Triton](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls),Â [MLIR](https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure), and others. The pattern is clear: bold technical ambitions, early excitement, and eventual fragmentation. Meanwhile, the CUDA moat grows deeper.
>  æˆ‘ä»¬å·²ç»è®¨è®ºäº†æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆçš„å…´è¡° - OpenCL, SyCL, TVM, XLA, Triton, MLIR ç­‰ç­‰
>  æ¨¡å¼å¾ˆæ˜æ˜¾: å¤§èƒ†çš„æŠ€æœ¯é›„å¿ƒã€æ—©æœŸçš„äººæƒ…ã€ä»¥åŠæœ€ç»ˆçš„ç¢ç‰‡åŒ–

The trillion-dollar question that keeps hardware leaders awake at night is:Â **Given the massive opportunityâ€”and developers desperate for alternativesâ€”why can't we break free?**

The answer isnâ€™t incompetence. Hardware companies are filled with brilliant engineers and seasoned execs. The problem is structural: misaligned incentives, conflicting priorities, and an underestimation of just how much software investment is required to play in this arena. You donâ€™t just need a chip. You need a platform. And building a platform means making hard, unpopular, long-term betsâ€”without the guarantee that anyone will care.
>  æ— æ³•å‡ºç°æ›¿ä»£æ–¹æ¡ˆçš„åŸå› æ˜¯ç»“æ„æ€§çš„: æ¿€åŠ±æªæ–½ä¸ä¸€è‡´ã€ä¼˜å…ˆçº§å†²çªã€ä»¥åŠä½ä¼°äº†åœ¨è¿™ä¸ªé¢†åŸŸç«äº‰æ‰€éœ€çš„è½¯ä»¶æŠ•èµ„
>  ä½ ä¸ä»…ä»…éœ€è¦ä¸€å—èŠ¯ç‰‡ï¼Œä½ éœ€è¦ä¸€ä¸ªå¹³å°ï¼Œè€Œæ„å»ºä¸€ä¸ªå¹³å°æ„å‘³ç€åšå‡ºè‰°éš¾çš„ã€ä¸å—æ¬¢è¿ã€é•¿æœŸçš„èµŒæ³¨ - è€Œä¸”æ²¡æœ‰ä»»ä½•ä¿è¯ä¼šæœ‰äººåœ¨æ„

In this post, we'll reveal the invisible matrix of constraints that hardware companies operate withinâ€”a system that makes building competitive AI software nearly impossible by design.
>  æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ­ç¤ºç¡¬ä»¶å…¬å¸æ‰€å¤„çš„éšå½¢çº¦æŸä½“ç³» - è¿™æ˜¯ä¸€ä¸ªä½¿å¾—æ„å»ºæœ‰ç«äº‰åŠ›çš„ AI è½¯ä»¶å‡ ä¹ä¸å¯èƒ½çš„ç³»ç»Ÿ

## My career in HW / SW co-design
I live and breathe innovative hardware. I readÂ _SemiAnalysis_,Â _EE Times_,Â _Ars Technica_â€”anything I can get my hands on about the chips, stacks, and systems shaping the future. Over decades, Iâ€™ve fallen in love with the intricate dance of hardware/software co-design: when it works, itâ€™s magic. When it doesnâ€™tâ€¦ well, thatâ€™s what this whole series is about.

A few of my learnings:

- My first real job in tech was atÂ **Intel**, helping optimize launch titles forÂ [the Pentium MMX](https://en.wikipedia.org/wiki/Pentium_\(original\)#P55C) â€”the first PC processor with SIMD instructions. There I learned the crucial lesson: without optimized software, a revolutionary silicon speedboat wonâ€™t get up to speed. Â That early taste of hardware/software interplay stuck with me.
>  ç¬”è€…çš„ç¬¬ä¸€ä»½çœŸæ­£çš„å·¥ä½œæ˜¯åœ¨ Intelï¼Œå¸®åŠ©ä¼˜åŒ–å¥”è…¾ MMX å¤„ç†å™¨çš„é¦–å‘ï¼Œè¿™æ˜¯é¦–æ¬¾å…·å¤‡ SIMD æŒ‡ä»¤çš„ PC å¤„ç†å™¨
>  åœ¨é‚£é‡Œå­¦ä¹ åˆ°çš„ä¸€ä¸ªå…³é”®æ•™è®­æ˜¯: **å¦‚æœæ²¡æœ‰ç»è¿‡ä¼˜åŒ–çš„è½¯ä»¶ï¼Œå³ä¾¿æ˜¯æœ€å…·é©å‘½æ€§çš„èŠ¯ç‰‡ä¹Ÿæ— æ³•å‘æŒ¥å…¶æ½œåŠ›**ï¼Œè¿™ç§æ—©æœŸå¯¹è½¯ç¡¬ä»¶åä½œçš„ç†è§£ä¸€ç›´ä¼´éšç€ç¬”è€…

- AtÂ **Apple**, I built the compiler infrastructure enabling a transition to in-house silicon. Apple taught me that true hardware/software integration requires extraordinary organizational disciplineâ€”it succeeded because instead of settling for a compromise, the teams shared a unified vision that no business unit can override.
>  åœ¨ Appleï¼Œç¬”è€…æ„å»ºäº†ç¼–è¯‘å™¨åŸºç¡€è®¾æ–½ï¼Œä»¥æ”¯æŒå‘è‡ªç ”èŠ¯ç‰‡çš„è¿‡æ¸¡
>  ç¬”è€…è®¤è¯†åˆ°çœŸæ­£çš„è½¯ç¡¬ä»¶æ•´åˆéœ€è¦ä»¥éå‡¡çš„ç»„ç»‡çºªå¾‹æ€§ - å®ƒä¹‹æ‰€ä»¥æˆåŠŸï¼Œæ˜¯å› ä¸ºå›¢é˜Ÿæ²¡æœ‰é€‰æ‹©å¦¥åï¼Œè€Œæ˜¯å…±äº«äº†ä¸€ä¸ªç»Ÿä¸€çš„æ„¿æ™¯ï¼Œæ²¡æœ‰ä»»ä½•ä¸šåŠ¡éƒ¨é—¨å¯ä»¥å‡Œé©¾äºè¿™ä¸ªæ„¿æ™¯ä¹‹ä¸Š

- AtÂ **Google**, I scaled the TPU software stack alongside the hardware and AIÂ research teams. With seemingly unlimited resources and tight HW/SW co-design, we used workload knowledge to deliver the power of specialized silicon â€” an incredible custom AI racing yacht.
- AtÂ **SiFive**, I switched perspectives entirelyâ€”leading engineering at a hardware company taught me the hard truths about hardware business models and organizational values.

Across all these experiences, one thing became clear:Â **software and hardware teams speak different languages**, move at different speeds, and measure success in different ways. Â But there's something deeper at workâ€”I came to see an invisible matrix of constraints that shapes how hardware companies approach software, and explain why software teams struggle with AI software in particular.
>  åœ¨æ‰€æœ‰è¿™äº›ç»éªŒä¸­ï¼Œæœ‰ä¸€ä»¶äº‹å˜å¾—è¶Šæ¥è¶Šæ¸…æ™°: è½¯ä»¶å’Œç¡¬ä»¶å›¢é˜Ÿä½¿ç”¨ä¸åŒçš„è¯­è¨€ï¼Œä»¥ä¸åŒçš„é€Ÿåº¦å‰è¿›ï¼Œä»¥ä¸åŒçš„æ–¹å¼è¡¡é‡æˆåŠŸ
>  è¿˜æœ‰æ›´æ·±å±‚æ¬¡çš„ä¸œè¥¿åœ¨èµ·ä½œç”¨ - ç¬”è€…é€æ¸æ„è¯†åˆ°è¿™æ˜¯ä¸€ç§éšå½¢çš„çº¦æŸä½“ç³»ï¼Œå®ƒå½±å“ç€ç¡¬ä»¶å…¬å¸å¦‚ä½•å¯¹å¾…è½¯ä»¶ï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆè½¯ä»¶å›¢é˜Ÿä¼šåœ¨ AI è½¯ä»¶ä¸ŠæŒ£æ‰

Before we go further, let's step into the mindset of a hardware executiveâ€”where the matrix of constraints begins to reveal itself.
>  åœ¨ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè¿›å…¥ç¡¬ä»¶é«˜ç®¡çš„æ€ç»´æ¨¡å¼ - åœ¨è¿™é‡Œï¼Œè¿™ä¸ªçº¦æŸä½“ç³»å¼€å§‹æ˜¾ç°å‡ºæ¥

## How AI hardware companies think
Thereâ€™s no shortage of brilliant minds in hardware companies. The problem isnâ€™t IQâ€”itâ€™s worldview.
>  ç¡¬ä»¶å…¬å¸å¹¶ä¸ç¼ºä¹èªæ˜çš„äººæ‰ï¼Œé—®é¢˜ä¸åœ¨äºæ™ºå•†ï¼Œè€Œåœ¨äºä¸–ç•Œè§‚

The architectural ingredients for AI chips are well understood by now: systolic arrays, TensorCores, mixed-precision compute, exotic memory hierarchies. Building chips remains brutally hard, but it's no longer the bottleneck for scalable success. The real challenge is getting anyone toÂ _use_Â your siliconâ€”and that means software.
>  å¦‚ä»Šï¼ŒAI èŠ¯ç‰‡çš„è¦ç´ å·²ç»å¹¿ä¸ºäººçŸ¥: é˜µåˆ—ç»“æ„ï¼ŒTensorCoresï¼Œæ··åˆç²¾åº¦è®¡ç®—ï¼Œå¤æ‚çš„å†…å­˜å±‚æ¬¡ç»“æ„
>  åˆ¶é€ èŠ¯ç‰‡ä»ç„¶éå¸¸å›°éš¾ï¼Œä½†å·²ç»ä¸å†æ˜¯å®ç°å¯æ‹“å±•æˆåŠŸçš„ä¸»è¦ç“¶é¢ˆ
>  çœŸæ­£çš„æŒ‘æˆ˜æ˜¯è®©ä»»ä½•äººä½¿ç”¨ä½ çš„èŠ¯ç‰‡ - è€Œè¿™æ„å‘³ç€ä½¿ç”¨è½¯ä»¶

GenAI workloads evolve at breakneck speed. Hardware companies need to design for what developers will needÂ **two years from now**, not just what's hot today. But they're stuck in a mental model that doesn't match realityâ€”trying toÂ **race in open waters**Â with a culture designed for land.
>  GenAI çš„ workload ä»¥æƒŠäººçš„é€Ÿåº¦æ¼”å˜ï¼Œç¡¬ä»¶å…¬å¸éœ€è¦ä¸ºå¼€å‘è€…ä¸¤å¹´åçš„éœ€æ±‚è¿›è¡Œè®¾è®¡ï¼Œè€Œä¸ä»…ä»…æ˜¯åº”å¯¹å½“ä¸‹çš„çƒ­é—¨æŠ€æœ¯
>  ä½†ä»–ä»¬å´å›°åœ¨ä¸€ç§ä¸ç°å®ä¸ç¬¦çš„æ€ç»´æ¨¡å¼ä¸­ - å°±åƒä½¿ç”¨ä¸ºé™†åœ°è®¾è®¡çš„æ–‡åŒ–å»åœ¨å¼€æ”¾æ°´åŸŸä¸­ç«é€Ÿ

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ffc6013fce349d79592fc7_LLVM.jpg)

Fun Fact: LLVM'sÂ mascot is a wyvern, sort of like a dragon with no claws in front.

**In the CPU era, software was simpler**: build a backend for LLVM and your chip inherited an ecosystemâ€”Linux, browsers, compiled applications all worked. AI has no such luxury. There's no central compiler or OS. You're building for a chaotic, fast-moving stackâ€”PyTorch, vLLM, todayâ€™s agent framework of the weekâ€”while your customers are using NVIDIA's tools. You're expected to make it allÂ _feel native_, toÂ _just work_, for AI engineers who neither understand your chip nor want to.
>  åœ¨ CPU æ—¶ä»£ï¼Œè½¯ä»¶è¦ç®€å•å¾—å¤š: åªè¦**ä¸º LLVM æ„å»ºä¸€ä¸ªåç«¯**ï¼Œä½ çš„èŠ¯ç‰‡å°±èƒ½ç»§æ‰¿ä¸€æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿ - Linux, æµè§ˆå™¨, ç¼–è¯‘åçš„åº”ç”¨ç¨‹åºéƒ½èƒ½æ­£å¸¸è¿è¡Œ
>  ä½† AI æ²¡æœ‰è¿™æ ·çš„ä¾¿åˆ©ï¼Œæ²¡æœ‰ä¸­å¿ƒçš„ç¼–è¯‘å™¨æˆ– OS
>  æˆ‘ä»¬æ˜¯åœ¨ä¸ºä¸€ä¸ªæ··ä¹±ä¸”å¿«é€Ÿæ¼”å˜çš„å †æ ˆä¸Šå¼€å‘ - PyTorch, vLLM - è€Œæˆ‘ä»¬çš„å®¢æˆ·æ­£åœ¨ä½¿ç”¨ NVIDIA çš„å·¥å…·
>  æˆ‘ä»¬è¢«æœŸæœ›è®©ä¸€åˆ‡çœ‹èµ·æ¥åŸç”Ÿå¹¶ä¸”èƒ½å¤Ÿå·¥ä½œï¼Œå› ä¸º AI å·¥ç¨‹å¸ˆæ—¢ä¸äº†è§£ä½ çš„èŠ¯ç‰‡ï¼Œä¹Ÿä¸æ„¿æ„å»äº†è§£

Despite this,Â **the chip is still the product**â€”and the P&L makes that crystal clear. Software, docs, tooling, community? Treated like overhead. This is the first constraint of the matrix:Â **hardware companies are structurally incapable of seeing a software ecosystem as a standalone product**. Execs optimize for capex, BOM cost, and tapeout timelines. Software gets some budget, but itâ€™s never enoughâ€”especially as AI software demands scale up. The result is aÂ **demo-driven**Â culture: launch the chip, write a few kernels, run some benchmarks, and build a flashy keynote that proves your FLOPS are real.
>  å°½ç®¡å¦‚æ­¤ï¼ŒèŠ¯ç‰‡ä»ç„¶æ˜¯äº§å“ - åˆ©æ¶¦å’ŒæŸå¤±è¡¨æ¸…æ¥šåœ°è¯´æ˜äº†è¿™ä¸€ç‚¹
>  è½¯ä»¶ã€æ–‡æ¡£ã€å·¥å…·ã€ç¤¾åŒºï¼Ÿè¿™äº›éƒ½è§†ä¸ºæˆæœ¬
>  è¿™æ˜¯çŸ©é˜µä¸­çš„ç¬¬ä¸€ä¸ªçº¦æŸ: ç¡¬ä»¶å…¬å¸åœ¨ç»“æ„ä¸Šæ— æ³•å°†è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿè§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„äº§å“
>  é«˜ç®¡ä»¬å…³æ³¨çš„æ˜¯èµ„æœ¬æ”¯å‡ºï¼ŒBOM æˆæœ¬å’Œæµç‰‡æ—¶é—´è¡¨ï¼Œè½¯ä»¶è·å¾—äº†ä¸€äº›é¢„ç®—ï¼Œä½†æ°¸è¿œä¸å¤Ÿ - å°¤å…¶æ˜¯ AI è½¯ä»¶éœ€è¦å¤§è§„æ¨¡æ‰©å±•çš„æƒ…å†µä¸‹
>  ç»“æœå°±æ˜¯ä¸€ç§æ¼”ç¤ºé©±åŠ¨çš„æ–‡åŒ–: å‘å¸ƒèŠ¯ç‰‡ï¼Œå†™å‡ ä¸ª kernelï¼Œè·‘å‡ ä¸ªæµ‹è¯•ï¼Œç„¶ååšä¸€ä¸ªç‚«é…·çš„å‘å¸ƒä¼šï¼Œè¯æ˜ä½ çš„ FLOPS ç¡®å®æœ‰æ•ˆ

The result is painfully familiar: a technically impressive chip with software no one wants to use. The software team promises improvement next cycle. But they said that last time too. This isn't about individual failureâ€”it's about systemic misalignment of incentives and resources in an industry structured around silicon, not ecosystems.
>  ç»“æœæ˜¯ä»¤äººç—›è‹¦çš„ç†Ÿæ‚‰: ä¸€æ¬¾æŠ€æœ¯ä¸Šè®©äººå°è±¡æ·±åˆ»ä½†æ²¡äººæƒ³ç”¨çš„èŠ¯ç‰‡
>  è½¯ä»¶å›¢é˜Ÿæ‰¿è¯ºä¸‹ä¸ªå‘¨æœŸä¼šæ”¹è¿›ï¼Œä½†ä»–ä»¬ä¸Šæ¬¡ä¹Ÿè¿™ä¹ˆè¯´ï¼Œè¿™ä¸æ˜¯ä¸ªäººå¤±è´¥çš„é—®é¢˜ï¼Œè€Œæ˜¯**æ•´ä¸ªè¡Œä¸šå›´ç»•ç¡…ç‰‡è€Œéç”Ÿæ€ç³»ç»Ÿçš„æ¿€åŠ±å’Œèµ„æºç³»ç»Ÿæ€§é”™é…çš„ç»“æœ**

## Why is GenAI software so hard and expensive to build?
Building GenAI software isnâ€™t just hardâ€”itâ€™s a treadmill pointed uphill, on a mountain thatâ€™s constantly shifting beneath your feet. Itâ€™s less an engineering challenge than a perfect storm of fragmentation, evolving research, and brutal expectationsâ€”each components of the matrix.
>  æ„å»º GenAI è½¯ä»¶ä¸ä»…ä»…å›°éš¾ - å®ƒå°±åƒåœ¨ä¸€ä¸ªå‘ä¸Šå€¾æ–œçš„è·‘æ­¥æœºä¸Šå¥”è·‘ï¼Œè€Œä½ è„šä¸‹çš„å±±å³°å´åœ¨ä¸æ–­ç§»åŠ¨
>  è¿™ä¸å…¶è¯´æ˜¯ä¸€ä¸ªå·¥ç¨‹æŒ‘æˆ˜ï¼Œä¸å¦‚è¯´æ˜¯ä¸€åœºç”±ç¢ç‰‡åŒ–ã€ä¸æ–­æ¼”è¿›çš„ç ”ç©¶å’Œä¸¥è‹›æœŸæœ›ç»„æˆçš„é£æš´ - è¿™äº›éƒ½æ˜¯çŸ©é˜µä¸­çš„ç»„æˆéƒ¨åˆ†

##### **ğŸƒThe treadmill of fragmented AI research innovation**
AI workloads arenâ€™t staticâ€”theyâ€™re a constantly mutating zoo. One week itâ€™s Transformers; the next itâ€™s diffusion, MoEs, or LLM agents. Then comes a new quantization trick, a better optimizer, or some obscure operator that a research team insists must run at max performanceÂ _right now_.
>  AI workload ä¸æ˜¯é™æ€çš„ - ä»–ä»¬æ˜¯ä¸æ–­å˜åŒ–çš„
>  è¿™ä¸€å‘¨æ˜¯ Transformersï¼Œä¸‹ä¸€å‘¨æ˜¯ diffusion, MoEs, LLM agentsï¼Œç„¶ååˆä¼šå‡ºç°ä¸€ç§æ–°çš„é‡åŒ–æŠ€å·§ã€æ›´å¥½çš„ä¼˜åŒ–å™¨

It is well known that youÂ **must innovate in hardware to differentiate**, but often forgotten that every hardware innovation multiplies your software burden against a moving target of use cases. Â Each hardware innovation demands that software engineers deeply understand itâ€”while also understanding the rapidly moving AI research and how to connect the two together.
>  ä¼—æ‰€å‘¨çŸ¥ï¼Œä½ å¿…é¡»åœ¨ç¡¬ä»¶ä¸Šè¿›è¡Œåˆ›æ–°ä»¥**å®ç°å·®å¼‚åŒ–**ï¼Œä½†å¸¸å¸¸è¢«é—å¿˜çš„æ˜¯ï¼Œæ¯ä¸€æ¬¡ç¡¬ä»¶åˆ›æ–°éƒ½ä¼šä½¿ä½ çš„è½¯ä»¶è´Ÿæ‹…ä¹˜ä»¥ä¸€ä¸ªä¸æ–­å˜åŒ–çš„ä½¿ç”¨åœºæ™¯ç›®æ ‡
>  æ¯é¡¹ç¡¬ä»¶åˆ›æ–°éƒ½**è¦æ±‚è½¯ä»¶å·¥ç¨‹å¸ˆæ·±å…¥ç†è§£å®ƒ - åŒæ—¶è¿˜è¦ç†è§£å¿«é€Ÿæ¼”è¿›çš„ AI ç ”ç©¶ï¼Œå¹¶å°†ä¸¤è€…ç»“åˆèµ·æ¥**

The result? Youâ€™re not building a â€œstackâ€â€”youâ€™re building aÂ **cross product**Â of models Ã— quantization formats Ã— batch sizes Ã— inference/training Ã— cloud/edge Ã— framework-of-the-week.
>  ç»“æœå°±æ˜¯ä½ ä¸æ˜¯åœ¨æ„å»ºä¸€ä¸ªæ ˆï¼Œè€Œæ˜¯åœ¨æ„å»ºä¸€ä¸ª***æ¨¡å‹ x é‡åŒ–æ ¼å¼ x æ‰¹æ¬¡å¤§å° x æ¨ç†/è®­ç»ƒ x å…¬æœ‰äº‘/è¾¹ç¼˜è®¡ç®— x æ¯å‘¨çƒ­é—¨æ¡†æ¶çš„äº¤å‰ä¹˜ç§¯*** 

It's combinatorially explosive, which is why no one but NVIDIA can keep up. You end up with ecosystem maps that look like this:
>  è¿™æ˜¯ç»„åˆçˆ†ç‚¸çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåªæœ‰ NVIDIA èƒ½å¤Ÿè·Ÿä¸ŠèŠ‚å¥ï¼Œæœ€ç»ˆï¼Œä½ ä¼šå¾—åˆ°è¿™æ ·çš„ç”Ÿæ€ç³»ç»Ÿåœ°å›¾:

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67fe6dc0761d451881406e3c_Screenshot%202025-04-12%20at%209.58.14%E2%80%AFPM.png)

Compatibility matrix highlighting the complexity of vLLM. Source:Â [vLLM](https://docs.vllm.ai/en/stable/features/quantization/supported_hardware.html)

##### **ğŸŒ You're competing with an industry, not just CUDA**
The real problem isn't just CUDAâ€”it's thatÂ **the entire AI ecosystem writes software for NVIDIA hardware**. Every framework, paper, and library is tuned for their latest TensorCores. Every optimization is implemented there first. This is the compounding loop explored inÂ [Part 3: CUDA is a software gravity](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#cycles)Â well that bends the industryâ€™s efforts toward NVIDIAâ€™s hardware.
>  çœŸæ­£çš„é—®é¢˜ä¸ä»…ä»…æ˜¯ CUDA - è€Œæ˜¯æ•´ä¸ª AI ç”Ÿæ€ç³»ç»Ÿéƒ½åœ¨ä¸º NVIDIA ç¡¬ä»¶ç¼–å†™è½¯ä»¶
>  æ¯ä¸€ä¸ªæ–°æ¡†æ¶ã€è®ºæ–‡å’Œåº“éƒ½é’ˆå¯¹ TensorCores è®¾è®¡ï¼Œæ¯ä¸€ç§ä¼˜åŒ–éƒ½æ˜¯é¦–å…ˆåœ¨ NVIDIA ä¸Šå®ç°çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨ â€œCUDA æ˜¯ä¸€ç§è½¯ä»¶å¼•åŠ›â€ æ¢è®¨è¿‡çš„å¤åˆå¾ªç¯ï¼Œå®ƒå¾ˆå¥½çš„å°†æ•´ä¸ªè¡Œä¸šåŠªåŠ›çš„æ–¹å‘å¼¯æ›²å‘ NVIDIA çš„ç¡¬ä»¶

For alternative hardware, compatibility isn't enoughâ€”you have toÂ **outcompete**Â a global open-source army optimizing for NVIDIA's chips. First you have to â€œrunâ€ the workload, but then it has to beÂ _better_Â than the HW+SW combo theyâ€™re already using.
>  å¯¹äºæ›¿ä»£æ€§ç¡¬ä»¶æ¥è¯´ï¼Œå…¼å®¹æ€§æ˜¯ä¸å¤Ÿçš„ - ä½ å¿…é¡»å‡»è´¥ä¸€æ”¯å…¨çƒèŒƒå›´å†…çš„ï¼Œä¸º NVIDIA èŠ¯ç‰‡è¿›è¡Œä¼˜åŒ–çš„å¼€æºå›¢é˜Ÿ
>  é¦–å…ˆï¼Œä½ å¿…é¡»è®© workload è·‘èµ·æ¥ï¼Œä½†ä¹‹åä½ è¿˜è¦è®©ä»–ä»¬æ¯”ç›®å‰ä½¿ç”¨çš„ç¡¬ä»¶ + è½¯ä»¶ç»„åˆæ›´å¥½

##### **ğŸ¥Š The software team is always outnumbered**
No matter how many software engineers you have, itâ€™s never enough to get ahead of the juggernaut - no matter how brilliant and committed, theyâ€™re just totally outmatched. Their inboxes are full of customer escalations, internal feature requests, and desperate pleas for benchmarks. They're fighting fires instead of building tools to prevent future fires, and theyâ€™re exhausted. Each major success just makes it clear how much more there is left to be done.
>  æ— è®ºä½ æœ‰å¤šå°‘è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œéƒ½æ— æ³•è·Ÿä¸Š NVIDIA çš„èŠ‚å¥ - ä¸ç®¡ä»–ä»¬å¤šä¹ˆèªæ˜å’ŒæŠ•å…¥ï¼Œä»–ä»¬å§‹ç»ˆå¤„äºåŠ£åŠ¿
>  ä»–ä»¬çš„é‚®ç®±é‡Œéƒ½æ˜¯å®¢æˆ·çš„ç´§æ€¥é—®é¢˜ã€å†…éƒ¨çš„åŠŸèƒ½è¯·æ±‚ã€ä»¥åŠè¿«åˆ‡çš„åŸºå‡†æµ‹è¯•éœ€æ±‚
>  ä»–ä»¬åªèƒ½ç–²äºæ•‘ç«ï¼Œè€Œä¸æ˜¯æ„å»ºé¢„é˜²æœªæ¥é—®é¢˜çš„å·¥å…·
>  æ¯ä¸€æ¬¡çš„é‡å¤§æˆåŠŸåªèƒ½è®©ä»–ä»¬æ›´åŠ æ¸…æ¥šåœ°äººæ—¶ä»£è¿˜æœ‰å¤šå°‘å·¥ä½œå°šæœªå®Œæˆ

They have many ideasâ€”they want to invest in infrastructure, build long-term abstractions, define the companyâ€™s software philosophy. But they canâ€™t, because they canâ€™t stop working on the current-gen chip long enough to prepare for the next one. Â Meanwhile, â€¦
>  ä»–ä»¬æœ‰å¾ˆå¤šæƒ³æ³• - ä»–ä»¬å¸Œæœ›æŠ•èµ„åŸºç¡€è®¾æ–½ï¼Œæ„å»ºé•¿æœŸçš„æŠ½è±¡å±‚ï¼Œå®šä¹‰å…¬å¸çš„è½¯ä»¶å“²å­¦
>  ä½†ä»–ä»¬åšä¸åˆ°ï¼Œä»–ä»¬åœ¨å½“å‰èŠ¯ç‰‡ä¸Šçš„ç ”å‘å¿™å¾—ä¸å¯å¼€äº¤ï¼Œè…¾ä¸å‡ºæ—¶é—´æ¥ä¸ºä¸‹ä¸€ä»£äº§å“åšå‡†å¤‡

##### **ğŸ’°The business always â€œchases the whaleâ€**
When a massive account shows up with cash and specific requirements, the business says yes. Those customers have leverage, and chasing them always makes short-term sense.

But thereâ€™s a high cost: Every whale you reel in pulls the team further away from building a scalable platform. Thereâ€™s no time to invest in aÂ **scalable torso-and-tail strategy**Â that might unlock dozens of smaller customers later. Instead of becoming a product company, your software team is forced to operate like a consulting shop.
>  æ¯å¸å¼•ä¸€ä¸ªå¤§å®¢æˆ·ï¼Œå›¢é˜Ÿå°±ç¦»æ„å»ºå¯æ‹“å±•å¹³å°çš„ç›®æ ‡æ›´è¿œï¼Œæ²¡æœ‰æ—¶é—´å»æŠ•èµ„ä¸€ä¸ªå¯æ‹“å±•çš„ â€œèº¯å¹²å’Œå°¾éƒ¨â€ ç­–ç•¥ï¼Œå³ä¾¿è¿™ç§ç­–ç•¥å¯èƒ½ä¼šåœ¨æœªæ¥è§£é”æ•°åä¸ªå°å‹å®¢æˆ·
>  ç»“æœæ˜¯ï¼Œä½ çš„è½¯ä»¶å›¢é˜Ÿè¢«è¿«åƒå’¨è¯¢å…¬å¸ä¸€æ ·è¿è¥ï¼Œè€Œä¸æ˜¯æˆä¸ºä¸€å®¶äº§å“å…¬å¸

It starts innocently, but soon your engineers implement hacks, forks, half-integrations that make one thing fast but break five others. Eventually, your software stack becomes a haunted forest of tech debt and tribal knowledge. Itâ€™s impossible to debug, painful to extend, and barely documentedâ€”who had time to write docs? And what do we do when the engineer who understood it just left?
>  æœ€å¼€å§‹çœ‹èµ·æ¥æ— å®³ï¼Œä½†æ˜¯å¾ˆå¿«ï¼Œå·¥ç¨‹å¸ˆå°±ä¼šå®æ–½å„ç§ hack æ‰‹æ®µï¼Œè®©æŸé¡¹åŠŸèƒ½å˜å¾—å¾ˆå¿«ï¼Œå´ç ´åäº†å…¶ä»–çš„åŠŸèƒ½
>  æœ€ç»ˆï¼Œè½¯ä»¶æ ˆä¼šå˜æˆä¸€ä¸ªå……æ»¡æŠ€æœ¯å€ºåŠ¡å’Œéƒ¨è½çŸ¥è¯†çš„ â€œé¬¼æ£®æ—â€ï¼Œå®ƒéš¾ä»¥è°ƒè¯•ï¼Œæ‹“å±•èµ·æ¥ç—›è‹¦ä¸å ªï¼Œå‡ ä¹æ²¡æœ‰ä»»ä½•æ–‡æ¡£ - è°è¿˜æœ‰æ—¶é—´å†™æ–‡æ¡£å‘¢ï¼Ÿ
>  è€Œå½“å”¯ä¸€ä¸€ä¸ªç†è§£å®ƒçš„å·¥ç¨‹å¸ˆç¦»å¼€åï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿ

## Challenges getting ahead in the hardware regatta
These aren't isolated problemsâ€”they're the universal reality of building GenAI software. The race isn't a sprintâ€”it's aÂ [regatta](https://en.wikipedia.org/wiki/Boat_racing): chaotic, unpredictable, and shaped as much by weather as by engineering. Everyone's crossing the same sea, but in radically different boats.
>  è¿™äº›ä¸æ˜¯å­¤ç«‹çš„é—®é¢˜ - ä»–ä»¬æ˜¯ç°åœ¨æ„å»º GenAI è½¯ä»¶çš„æ™®éç°çŠ¶

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ffc6f8e0264ecef87e0bf6_Boat.jpg)

##### **ğŸš¤ Speedboats: Startups aim for benchmarks, not generality or usability**
Startups are in survival mode. Their goal is to prove the silicon works, that it goes fast, and that someoneâ€”anyoneâ€”might buy it. That means picking a few benchmark workloads and making them fly, using whatever hacks or contortions it takes. Generality and usability donâ€™t matterâ€”The only thing that matters is showing that the chip is real and competitiveÂ _today_. Youâ€™re not building a software stack. Youâ€™re building a pitch deck.
>  åˆåˆ›å…¬å¸æ­£å¤„äºç”Ÿå­˜æ¨¡å¼ï¼Œä»–ä»¬çš„ç›®æ ‡æ˜¯è¯æ˜èŠ¯ç‰‡æœ‰æ•ˆã€è¿è¡Œé€Ÿåº¦å—ï¼Œå¹¶ä¸”æœ‰äºº - ä»»ä½•äºº - å¯èƒ½ä¼šè´­ä¹°å®ƒä»¬
>  è¿™æ„å‘³ç€é€‰æ‹©å‡ ä¸ª benchmark workloads å¹¶è®©å®ƒä»¬å¿«é€Ÿè¿è¡Œï¼Œä¸ç®¡ä½¿ç”¨ä»€ä¹ˆæŠ€å·§æˆ–æ‰‹æ®µ
>  é€šç”¨æ€§å’Œæ˜“ç”¨æ€§å¹¶ä¸é‡è¦ - å”¯ä¸€é‡è¦çš„æ˜¯åœ¨ä»Šå¤©è¯æ˜èŠ¯ç‰‡æ˜¯çœŸå®çš„å¹¶ä¸”å…·æœ‰ç«äº‰åŠ›çš„
>  ä½ ä¸æ˜¯åœ¨æ„å»ºä¸€ä¸ªè½¯ä»¶æ ˆï¼Œè€Œæ˜¯åˆ¶ä½œä¸€ä»½æ¼”ç¤ºæ–‡ç¨¿

##### **â›µ Custom Racing Yachts: Single-chip companies build vertical stacks**
TheÂ [Mag7](https://en.wikipedia.org/wiki/Big_Tech#Magnificent_Seven)Â and advanced startups take a different tack. They buildÂ [TPU racing yachts](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers#xla)Â to win specific races with custom designs. They can be fast and beautifulâ€”but only with their trained crew, their instruction manual, and often their own models. Because these chips leave GPU assumptions behind, they must build bespoke software stacks from scratch.

They own the entire stack because they have to. The result? More fragmentation for AI engineers. Betting on one of these chips means theoretical FLOPS at a discountâ€”but sacrificing momentum from the NVIDIA ecosystem. The most promising strategy for these companies is locking in a few large customers: frontier labs or sovereign clouds hungry for FLOPS without the NVIDIA tax.

##### **ğŸ›³ï¸ Ocean Liners: Giants struggle with legacy and scale**
Then come the giants: Intel, AMD, Apple, Qualcommâ€”companies with decades of silicon experience and sprawling portfolios: CPUs, GPUs, NPUs, even FPGAs. Theyâ€™ve shipped billions of units. But that scale brings a problem: divided software teams stretched across too many codebases, too many priorities. Their customers canâ€™t keep track of all the software and versionsâ€”where to start?
>  å·¨å¤´ä»¬æ‹¥æœ‰æ•°åå¹´çš„èŠ¯ç‰‡ç»éªŒï¼Œäº§å“ç»„åˆåºå¤§
>  å®ƒä»¬å·²ç»äº¤ä»˜äº†æ•°åäº¿ä¸ªå•å…ƒï¼Œä½†è¿™ç§è§„æ¨¡ä¹Ÿå¸¦æ¥äº†é—®é¢˜: è½¯ä»¶å›¢é˜Ÿè¢«åˆ†æ•£åœ¨å¤ªå¤šä»£ç åº“å’Œä¼˜å…ˆçº§ä¹‹é—´ï¼Œå®¢æˆ·æ— æ³•è·Ÿä¸Šæ‰€æœ‰çš„è½¯ä»¶ç‰ˆæœ¬å’Œç»†èŠ‚

One tempting approach is to just embrace CUDA with a translator. It gets you â€œcompatibility,â€ but never great performance. Modern CUDA kernels are written for Hopperâ€™s TensorCores, TMA, and memory hierarchy. Translating them to your architecture wonâ€™t make your hardware shine.
>  ä¸€ç§æœ‰å¸å¼•åŠ›çš„æ–¹æ³•æ˜¯ç›´æ¥é‡‡ç”¨ CUDAï¼Œå¹¶é…ä¸Šä¸€ä¸ªç¿»è¯‘å™¨
>  è¿™èƒ½å¸¦æ¥å…¼å®¹æ€§ï¼Œä½†æ°¸è¿œæ— æ³•å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œç°ä»£çš„ CUDA kernel æ˜¯ä¸º Hopper TensorCores, TMV å’Œå†…å­˜å±‚æ¬¡ç»“æ„è®¾è®¡çš„ï¼Œå°†å®ƒä»¬ç¿»è¯‘åˆ°ä½ çš„æ¶æ„ä¸Šï¼Œä¸ä¼šè®©ä½ çš„ç¡¬ä»¶è¡¨ç°å‡ºè‰²

Sadly, the best-case outcome at this scale isÂ **OneAPI from Intel**â€”open, portable, and community-governed, but lacking momentum or soul. It hasnâ€™t gained traction in GenAI forÂ [the same reasons OpenCL didnâ€™t](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives#evolving-needs): it was designed for a previous generation of GPU workload, and AI moved too fast for it to keep up. Being open only helps if you also keep up.
>  åœ¨è¿™ç§è§„æ¨¡ä¸‹ï¼Œæœ€å¥½çš„ç»“æœæ˜¯ Intel OneAPI - å¼€å‘ï¼Œå¯ç§»æ¤ï¼Œæœ‰ç¤¾åŒºç®¡ç†ï¼Œä½†ç¼ºä¹åŠ¨åŠ›æˆ–çµé­‚
>  å®ƒåœ¨ GenAI æ—¶ä»£æ²¡æœ‰è·å¾—æ™®åŠï¼ŒåŸå› å’Œ OpenCL å½“å¹´ä¸€æ ·: å®ƒæœ€åˆæ˜¯ä¸ºä¸Šä¸€ä»£ GPU workload è®¾è®¡çš„ï¼Œè€Œ AI çš„å‘å±•é€Ÿåº¦å¤ªå¿«ï¼Œå®ƒè·Ÿä¸ä¸Šï¼Œåªæœ‰å¼€æ”¾æ˜¯ä¸å¤Ÿçš„ï¼Œè¿˜éœ€è¦è·Ÿä¸Šæ—¶ä»£çš„è„šæ­¥

##### **ğŸš¢ NVIDIA: The carrier that commands the race**
NVIDIA is the aircraft carrier in the lead: colossal, coordinated, and surrounded by supply ships, fighter jets, and satellite comms. While others struggle to build software for one chip, NVIDIA launches torpedos at anyone who might get ahead. While others optimize for a benchmark, theÂ _world_Â optimizes for NVIDIA. The weather changes to match their runway.
>  å½“å…¶ä»–äººåœ¨åŠªåŠ›ä¸ºä¸€æ¬¾èŠ¯ç‰‡å¼€å‘è½¯ä»¶æ—¶ï¼ŒNVIDIA å´å‘å¯èƒ½è¶…è¿‡å®ƒçš„ä»»ä½•äººå‘èµ·æ”»å‡»
>  å½“å…¶ä»–äººåœ¨ä¼˜åŒ– benchmark æ—¶ï¼Œæ•´ä¸ªä¸–ç•Œä¸º NVIDIA ä¼˜åŒ–

If youâ€™re in the regatta, youâ€™re sailing into their wake. The question isnâ€™t whether youâ€™re making progressâ€”itâ€™s whether the gap is closing or getting wider.

## Breaking out of the matrix
At this point in â€œDemocratizing AI Computeâ€, weâ€™ve mapped the landscape. CUDA isn't dominant by accidentâ€”itâ€™s the result ofÂ [relentless investment, platform control, and market feedback loops](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#cycles)Â that others simply canâ€™t replicate. Billions have been poured into alternatives: vertically-integrated stacks from Mag7 companies, open platforms from industry giants, and innovative approaches from hungry startups. None have cracked it.
>  æˆ‘ä»¬å·²ç»æç»˜äº†æ•´ä¸ªæ ¼å±€ï¼ŒCUDA å¹¶éå¶ç„¶å æ®ä¸»å¯¼åœ°ä½ - å®ƒæ˜¯æŒç»­çš„æŠ•å…¥ã€å¹³å°æ§åˆ¶ã€å¸‚åœºåé¦ˆå¾ªç¯çš„ç»“æœï¼Œè€Œè¿™æ˜¯å…¶ä»–å…¬å¸æ— æ³•å¤åˆ¶çš„
>  æ•°åäº¿ç¾å…ƒè¢«æŠ•å…¥åˆ°æ›¿ä»£æ–¹æ¡ˆä¸­ï¼Œä½†ä»æœªæœ‰äººæˆåŠŸ

But weâ€™re no longer lost in the fog. We canÂ [see the matrix](https://en.wikipedia.org/wiki/The_Matrix)Â now: how these dynamics work, where the traps lie, why even the most brilliant software teams can't get ahead at hardware companies. The question is no longerÂ _why_Â weâ€™re stuck: Itâ€™s whether we can break free.

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6807b373523be178387eebe7_spoon-lg.gif)

> **Child:**Â "Do not try and bend the spoon. That's impossible. Instead... only try to realize the truth."

> **Neo:**Â "What truth?"

> **Child:**Â "**There is no spoon.**Â Then you'll see that it is not the spoon that bends, it is only yourself."

If we want to Democratize AI Compute, someone has to challenge the assumptions weâ€™ve all been working within. The path forward isn't incremental improvementâ€”it's changing the rules of the game entirely.
>  å¦‚æœæˆ‘ä»¬æƒ³è¦æ°‘ä¸»åŒ– AI è®¡ç®—ï¼Œéœ€è¦æœ‰äººæŒ‘æˆ˜æˆ‘ä»¬ä¸€ç›´ä»¥æ¥éµå¾ªçš„å‡è®¾
>  å‰è¿›çš„é“è·¯ä¸æ˜¯æ¸è¿›å¼çš„æ”¹å˜ - éœ€è¦å½»åº•æ”¹å˜æ¸¸æˆè§„åˆ™

Let's explore that together in part 10.

-Chris

# 10 Modularâ€™s bet to break out of the Matrix 
Site: https://www.modular.com/blog/modulars-bet-to-break-out-of-the-matrix-democratizing-ai-compute-part-10
Date: 8 May, 2025

Over the course of this series, weâ€™ve seen just how hard it is to break free from the matrix of constraints imposed by the status quo. Everyone wants a solutionâ€”developers, startups, hardware vendorsâ€”but nothing sticks. Promising efforts flame out. Clever hacks donâ€™t scale. The pace of GenAI accelerates, whileÂ **Mooreâ€™s Law**Â fades and the stack only gets heavier.
>  æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ï¼Œè¦æ‘†è„±ç”±ç°çŠ¶æ–½åŠ çš„é™åˆ¶çŸ©é˜µæœ‰å¤šéš¾ï¼Œæ¯ä¸ªäººéƒ½æƒ³è¦ä¸€ä¸ªè§£å†³æ–¹æ¡ˆ - å¼€å‘è€…ã€åˆåˆ›å…¬å¸ã€ç¡¬ä»¶ä¾›åº”å•† - ä½†ä¼¼ä¹ä»€ä¹ˆéƒ½æ— æ³•æŒä¹…

While AI has unbounded optimism and hype, it also has problems - the purpose of this series is to shine a light on the complexities and challenges of AI infrastructure. Â Itâ€™s with this experience, plenty of scars, and a bit of bull-headedness that we finally said: enough. If we want a different outcome, we have to try a different approach.

Thatâ€™s why Tim and I started Modular. Not because CUDA is evilâ€”it isnâ€™t. CUDA earned its place with brilliant engineering and relentless execution. The truth is, most people are frustrated with CUDAÂ **because it won,**Â **the stakes are so high, and they yearn for something better.**

After two decades, the cracks are showing. CUDAâ€”and the cathedral of software built atop itâ€”have grown brittle. Complexity compounds. Innovation slows. What started as an accelerator is now a constraint. The real problem isnâ€™t CUDA itself:Â [**itâ€™s the complexity it drove into the AI software stack**](https://www.youtube.com/watch?v=pdJQ8iVTwj8&t=3884s) â€”a weight we all carry.
>  ç»å† 20 å¹´çš„æ³•é˜µï¼Œè£‚ç—•å·²ç»æ˜¾ç°ï¼ŒCUDA - ä»¥åŠå»ºç«‹åœ¨å®ƒä¹‹ä¸Šçš„åºå¤§è½¯ä»¶ä½“ç³» - æ­£åœ¨å˜å¾—è„†å¼±
>  å¤æ‚æ€§ä¸æ–­ç´¯ç§¯ï¼Œåˆ›æ–°é€Ÿåº¦æ”¾ç¼“ï¼Œæœ€åˆä½œä¸ºåŠ é€Ÿå™¨ CUDA å¦‚ä»Šå´æˆä¸ºäº†é™åˆ¶å› ç´ 
>  çœŸæ­£çš„é—®é¢˜ä¸æ˜¯ CUDA æœ¬èº«ï¼Œè€Œæ˜¯å®ƒæ¨åŠ¨ AI è½¯ä»¶æ ˆæ‰€ç§¯ç´¯çš„å¤æ‚æ€§

If we want a different future, we canâ€™t just rail against the one weâ€™ve got. We mustÂ **build something better, together**. Something that doesnâ€™t just copy CUDA, but goes beyond itâ€”solving the root problems it grew up around. Something simpler, more flexible, and more empowering for every AI developer.

The problem is thatÂ **this isnâ€™t an incremental step**. It takesÂ _years_Â of development from a large and focused team of experts to move the needle. Â Even if you can attract the experts, how do youÂ **get them to work together**Â and avoid them getting dragged into the firefight of the dayâ€¦ forÂ _years_Â at a time? Â This post explains how we started Modularâ€”and why we believe itâ€™s possible to break through the matrix of constraints and build a better foundation for AI.

Letâ€™s see just how deep the rabbit hole goes. ğŸ‡ğŸ•³ï¸

## What does â€œDemocratizing AI Computeâ€ mean to me?
When we talk about democratizing AI compute, we donâ€™t just mean â€œrun it on more devices.â€ We mean rethinkingÂ **_who_Â gets to buildÂ _what_â€”andÂ _how_**. It means removing the gatekeepers, lowering the barriers, and leveling the playing field for developers, hardware vendors, and researchers alike.
>  å½“æˆ‘ä»¬è®¨è®ºæ°‘ä¸»åŒ– AI è®¡ç®—æ—¶ï¼Œæˆ‘ä»¬ä»…ä»…æ˜¯æŒ‡ â€œåœ¨æ›´å¤šè®¾å¤‡ä¸Šè¿è¡Œâ€ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯è°èƒ½å¤Ÿæ„å»ºä»€ä¹ˆï¼Œä»¥åŠå¦‚ä½•æ„å»º
>  è¿™æ„å‘³ç€æ¶ˆé™¤ä¸­é—´äººï¼Œé™ä½é—¨æ§›ï¼Œä¸ºå¼€å‘è€…ã€ç¡¬ä»¶å‚å•†å’Œç ”ç©¶äººå‘˜åˆ›é€ ä¸€ä¸ªæ›´åŠ å…¬å¹³çš„ç«äº‰ç¯å¢ƒ

Back in 2021, I gave anÂ [industry keynote at a prominent academic conference](https://docs.google.com/presentation/d/1ZMtzT6nmfvNOlIaHRzdaXpFeaAklcT7DvfGjhgpzcxk/edit?slide=id.p#slide=id.p), laying out a vision for a unifying software layer that could finally bring the field together. I hoped someone would pick up the torch and build it. People were intrigued. Conversations sparked. But no one made it to the finish line.

So we asked a different question:Â **What if we designed the stack for AI developers first?**Â What if performance engineering wasnâ€™t the exclusive domain of chip vendors and compiler gurus? What if these tools wereÂ _programmable_,Â _composable_, andÂ _understandable_â€”so thatÂ _anyone_Â could build with them? I think we'd get more "[DeepSeek moments](https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai)" with innovation coming even faster from more innovators, helping the entire world.
>  å¦‚æœæˆ‘ä»¬é¦–å…ˆä¸º AI å¼€å‘è€…è®¾è®¡æ•´ä¸ªè½¯ä»¶æ ˆä¼šæ€ä¹ˆæ ·ï¼Ÿ
>  å¦‚æœæ€§èƒ½å·¥ç¨‹ä¸å†æ˜¯èŠ¯ç‰‡å‚å•†å’Œç¼–è¯‘å™¨ä¸“å®¶çš„ä¸“å±é¢†åŸŸå‘¢ï¼Ÿ
>  å¦‚æœè¿™äº›å·¥å…·æ˜¯å¯ç¼–ç¨‹ã€å¯ç»„åˆã€å¯ç†è§£çš„ - é‚£ä¹ˆä»»ä½•äººéƒ½èƒ½ä½¿ç”¨å®ƒä»¬è¿›è¡Œå¼€å‘å‘¢ï¼Ÿ

Iâ€™ve seen this kind of transformation before. In 2010, the iPhone was an incredible technical platformâ€”but Objective-Câ€™s complexity was gatekeeping app development to experts.Â [Swift](https://www.swift.org/)Â changed that. It unlocked a wave of creativity, empowering an order of magnitude more developers to build great apps. Today, CUDA and other AI infrastructure face the same problem. The tools are powerful, but the complexity is crushing.
>  ç¬”è€…æ›¾ç»è§è¿‡è¿™ä¸ªè½¬å˜ï¼Œåœ¨ 2010 å¹´ Objective-C çš„å¤æ‚æ€§è®© IOS åº”ç”¨å¼€å‘ä»…é™äºä¸“å®¶ï¼ŒSwift æ”¹å˜äº†è¿™ä¸€åˆ‡ï¼Œé‡Šæ”¾äº†ä¸€æ³¢åˆ›é€ åŠ›
>  ä»Šå¤©ï¼ŒCUDA å’Œå…¶ä»– AI Infra é¢ä¸´ç€åŒæ ·çš„é—®é¢˜ï¼Œè¿™äº›å·¥å…·åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¤æ‚æ€§å´è®©äººéš¾ä»¥æ‰¿å—

##### So:Â **how do we break past that?**
I believe the answer lies in the intersection ofÂ **usability, portability, and performance**. After working on highly specialized stacks for TPUs and other accelerators, I saw both the upside of vertical integrationâ€”and the downside of brittle systems that canâ€™t evolve fast enough in a rapidly changing landscape.
>  è¦æ‰“ç ´è¿™ä¸€ç‚¹ï¼Œç­”æ¡ˆåœ¨äºå¯ç”¨æ€§ã€å¯ç§»æ¤æ€§å’Œæ€§èƒ½çš„äº¤æ±‡ç‚¹
>  åœ¨ä¸º TPU å¼€å‘é«˜åº¦ä¸“ç”¨åŒ–çš„è½¯ä»¶æ ˆæ—¶ï¼Œç¬”è€…çœ‹åˆ°äº†å‚ç›´æ•´åˆçš„ä¼˜åŠ¿ï¼Œä½†ä¹Ÿçœ‹åˆ°äº†å…¶å¼Šç«¯ - ç³»ç»Ÿè¿‡äºè„†å¼±ï¼Œåœ¨å¿«é€Ÿå˜åŒ–çš„ç¯å¢ƒä¸­éš¾ä»¥å³ä½¿æ¼”è¿›

That experience defined our metrics for successâ€”[the scorecard weâ€™ve been building](https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure)Â throughout this series:

- Does it serve developers?
- Does it unlock full hardware performance?
- Does it enable innovationÂ _above_Â andÂ _below_Â the stack?
- Does it scale across use cases and chip types?
- Can you actually use it in production?

We need something inspired by theÂ [design of LLVM](https://aosabook.org/en/v1/llvm.html) â€”but reimagined for the modern era of AI. A system where hardware makers can plug in their chips, express what makes them great, and still own their performance. A system where AI software developers can build at the frontierâ€”_without_Â reinventing the stack every time.
>  æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå— LLVM è®¾è®¡å¯å‘çš„ç³»ç»Ÿï¼Œä½¿å¾—ç¡¬ä»¶å‚å•†å¯ä»¥æ’å…¥å®ƒä»¬çš„èŠ¯ç‰‡ï¼Œè¡¨è¾¾å®ƒä»¬çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼ŒåŒæ—¶ä»ç„¶æŒæ§è‡ªå·±çš„æ€§èƒ½ï¼Œä½¿å¾— AI è½¯ä»¶å¼€å‘è€…å¯ä»¥åœ¨å‰æ²¿è¿›è¡Œå¼€å‘ - æ— éœ€æ¯æ¬¡éƒ½è¦é‡æ–°å‘æ˜æ•´ä¸ªè½¯ä»¶æ ˆ

Thatâ€™s what â€œ**Democratizing AI Compute**â€ means to us. Not just more devices. Not just lower cost. But a fundamentally open, modern foundationâ€”one that unlocks progress forÂ _everyone,_Â not just the trillion-dollar incumbents.

## How do we tackle an industry-scale problem?
Thereâ€™s just one small challenge: building aÂ [high-performance AI stack for aÂ _single_Â chip is already hard](https://www.modular.com/blog/democratizing-ai-compute-part-9-why-do-hw-companies-struggle-to-build-ai-software). Solving it atÂ **industry scale**â€”across devices, vendors, and workloadsâ€”is an order of magnitude harder.
>  ä½†ä¸ºå•ä¸ªèŠ¯ç‰‡æ„å»ºé«˜æ€§èƒ½ AI stack å·²ç»å¾ˆéš¾äº†ï¼Œè€Œåœ¨æ•´ä¸ªè¡Œä¸šè§„æ¨¡ä¸Šè§£å†³è¿™ä¸ªé—®é¢˜ - è¦†ç›–å„ç§è®¾å¤‡ã€å‚å•†å’Œ workloads - éš¾åº¦è¦å†å¤§ä¸€ä¸ªæ•°é‡çº§

This isnâ€™t Clayton Christensenâ€™sÂ [_Innovatorâ€™s Dilemma_](https://en.wikipedia.org/wiki/The_Innovator%27s_Dilemma), where incumbents stumble because they ignore disruption. This is theÂ _opposite_Â problem:Â **everyone sees the challenge. Everyone is trying to solve it.**Â And yetâ€”despite smart people, serious funding, and real effortâ€”most attempts stall out.

Letâ€™s be honest: a lot of folks today believe the systemÂ _canâ€™t_Â be changed. Not because they love it, but because theyâ€™ve watched team after team tryâ€”and fail. Meanwhile, the world keeps moving. GenAI explodes. Mooreâ€™s Law slows. The stack grows more brittle and complex. More chips are announced, butÂ **CUDA remains the gravitational center of it all**. So why does nothing stick? Why do smart people with serious funding at the biggest companies keep hitting the same wall?
>  ä»Šå¤©æœ‰å¾ˆå¤šäººè®¤ä¸ºè¿™ä¸ªç³»ç»Ÿæ— æ³•è¢«æ”¹å˜ï¼Œæ˜¯å› ä¸ºä»–ä»¬çœ‹åˆ°äº†ä¸€æ”¯åˆä¸€æ”¯çš„å›¢é˜Ÿå°è¯• - ç„¶åå¤±è´¥
>  äºæ­¤åŒæ—¶ï¼Œä¸–ç•Œä»åœ¨å‰è¿›ï¼ŒGenAI çˆ†å‘ï¼Œæ‘©å°”å®šå¾‹å¤±æ•ˆï¼Œè½¯ä»¶æ ˆå˜å¾—è¶Šæ¥è¶Šè„†å¼±å’Œå¤æ‚ï¼Œè¶Šæ¥è¶Šå¤šçš„èŠ¯ç‰‡è¢«å®£å¸ƒæ¨å‡ºï¼Œä½† CUDA ä»ç„¶æ˜¯ä¸€åˆ‡çš„ä¸­å¿ƒ

Iâ€™ve been through this before. Iâ€™ve seenâ€”and helped solveâ€”industry-scale problems like this. In my experience, when transformation keeps failing, it's not usually for lack of talent or funding. It's becauseÂ **those projects arenâ€™t solving the whole problem**. Instead of disruption theory, we need to understand whyÂ **new solutions**Â fail to stick.
>  å½“è½¬å‹ä¸æ–­å¤±è´¥æ—¶ï¼Œé€šå¸¸ä¸æ˜¯å› ä¸ºç¼ºä¹äººæ‰å’Œèµ„é‡‘ï¼Œè€Œæ˜¯å› ä¸ºè¿™äº›é¡¹ç›®æ²¡æœ‰è§£å†³æ•´ä¸ªé—®é¢˜
>  æˆ‘ä»¬éœ€è¦ç†è§£ä¸ºä»€ä¹ˆæ–°çš„è§£å†³æ–¹æ¡ˆæ— æ³•æŒç»­ä¸‹å»

For that, Iâ€™ve come to value a different lens: theÂ [**Lippitt-Knoster Model for Managing Complex Change**](https://sergiocaredda.eu/organisation/models-the-lippitt-knoster-model-for-managing-complex-change). It outlines six things every successful transformation needs:

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/681b51c3393c4b635c96bbe3_w%3D3840%2Cquality%3D90%2Cfit%3Dscale-down.png)

The Lippitt-Knoster Model for Managing Complex Change. Image credit:Â [Sergio Caredda](https://sergiocaredda.eu/organisation/models-the-lippitt-knoster-model-for-managing-complex-change)

**Vision, Consensus, Skills, Incentives, Resources, and Action Plan.**

If any one of them is missing, change failsâ€”and it fails in aÂ _predictable_Â way.

- âŒ Weak vision â†’Â **Confusion**Â ğŸ˜µâ€ğŸ’«
- âš”ï¸ Weak consensus â†’Â **Conflict & Resistance**Â ğŸ™…
- ğŸ§  Inadequate skillset â†’Â **Stress & Anxiety**Â ğŸ˜¬
- ğŸ’¸ Misaligned incentives â†’Â **Drag & Delay**Â ğŸŒ
- ğŸª« Insufficient resources â†’Â **Fatigue & Frustration**Â ğŸ˜¤
- ğŸŒ€ No clear plan â†’Â **False starts & Chaos**Â ğŸ¤¯

>  Lippit-Knoster å¤æ‚å˜é©ç®¡ç†æ¨¡å‹åˆ—å‡ºäº†æˆåŠŸè½¬å‹æ‰€éœ€è¦çš„å…­ä¸ªè¦ç´ :
>  æ„¿æ™¯ã€å…±è¯†ã€æŠ€èƒ½ã€æ¿€åŠ±ã€èµ„æºã€è¡ŒåŠ¨æ–¹æ¡ˆ
>  å¦‚æœä»»æ„ä¸€è€…ç¼ºå¤±ï¼Œå˜é©å°±ä¼šå¤±è´¥ï¼Œå¹¶ä¸”æ˜¯ä»¥ä¸€ç§å¯é¢„æµ‹çš„æ–¹å¼å¤±è´¥:
>  - ç¼ºä¹æ„¿æ™¯ -> æ··ä¹±
>  - ç¼ºä¹å…±è¯† -> å†²çªå’ŒæŠµè§¦
>  - ç¼ºä¹æŠ€èƒ½ -> å‹åŠ›ä¸ç„¦è™‘
>  - æ¿€åŠ±ä¸ä¸€è‡´ -> æ‹–å»¶å’Œå»¶è¿Ÿ
>  - èµ„æºä¸è¶³ -> ç–²æƒ«ä¸æŒ«è´¥
>  - æ²¡æœ‰æ˜ç¡®è®¡åˆ’ -> é”™è¯¯çš„èµ·æ­¥å’Œæ··ä¹±

Weâ€™ve seen all of this in previous blog posts:Â [OpenCL & SYCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives),Â [TVM &Â XLA](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers),Â [Triton](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls), and evenÂ [MLIR](https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure). The patterns are realâ€”and the failures werenâ€™t technical, they wereÂ **systemic**.
>  ä¹‹å‰è®¨è®ºçš„å¤±è´¥ä¸æ˜¯æŠ€æœ¯ä¸Šçš„ï¼Œè€Œæ˜¯ç³»ç»Ÿä¸Šçš„

So if we want to break the cycle, we canâ€™t just build great tech. We have to solve theÂ _whole equation_. Thatâ€™s the bar we set at Modularâ€”not just to write a better point solution or design a slicker API, but to alignÂ **vision, capability, and momentum**Â across the ecosystem.
>  å¦‚æœæˆ‘ä»¬æƒ³è¦æ‰“ç ´å¾ªç¯ï¼Œä¸èƒ½ä»…ä»…åªæ„å»ºæŠ€æœ¯ï¼Œè€Œæ˜¯éœ€è¦è§£å†³æ•´ä¸ªé—®é¢˜
>  è¿™å°±æ˜¯ Modular è®¾å®šçš„æ ‡å‡† - ä¸ä»…ä»…æ˜¯ç¼–å†™ä¸€ä¸ªæ›´å¥½çš„è§£å†³æ–¹æ¡ˆæˆ–è®¾è®¡ä¸€ä¸ªæ›´ç¾è§‚çš„ APIï¼Œè€Œæ˜¯åœ¨æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿä¸­åè°ƒæ„¿æ™¯ã€èƒ½åŠ›å’ŒåŠ¿å¤´

Because thatâ€™s what it takes for real change to stickâ€”and thatâ€™s exactly what we set out to do.

## How we set up Modular to maximize odds of success
Once we understood the full complexity of the problemâ€”and the long history of failed attemptsâ€”we knew we had to build Modular differently from day one. That meant engineering great software, yesâ€”but also designing a team, a structure, and a mission that could sustain progress where so many others had stalled.
>  åœ¨æˆ‘ä»¬ç†è§£äº†é—®é¢˜çš„å¤æ‚æ€§ä¹‹åï¼Œæˆ‘ä»¬å°±çŸ¥é“ä»ç¬¬ä¸€å¤©èµ·å°±éœ€è¦ä»¥ä¸åŒçš„æ–¹å¼æ„å»º Modular
>  è¿™æ„å‘³ç€ä¸ä»…è¦æ‰“é€ ä¼˜ç§€çš„è½¯ä»¶ï¼Œè¿˜è¦è®¾è®¡ä¸€ä¸ªèƒ½å¤ŸæŒç»­æ¨åŠ¨è¿›å±•çš„å›¢é˜Ÿã€ç»“æ„å’Œä½¿å‘½

**We started with a clear vision**: to makeÂ [AI compute accessible, performant, and programmable](https://www.modular.com/blog/the-future-of-ai-depends-on-modularity) â€”for everyone. Not just for billion-dollar chipmakers or compiler wizards. For researchers, developers, startups, and hardware builders. That meantÂ [rethinking and rebuilding theÂ _entire_Â stack, not just optimizing one layer](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform). We needed a system that could scale across use cases, not a point solutionÂ [destined to be thrown away](https://petewarden.com/2023/10/15/the-unstoppable-rise-of-disposable-ml-frameworks/)Â when AI shifts again.
>  æˆ‘ä»¬ä»ä¸€ä¸ªæ¸…æ™°çš„æ„¿æ™¯å¼€å§‹: è®© AI è®¡ç®—å¯è®¿é—®ã€é«˜æ€§èƒ½ã€å¯ç¼–ç¨‹ - å¯¹äºæ¯ä¸ªäººæ¥è¯´éƒ½æ˜¯å¦‚æ­¤
>  è¿™æ„å‘³ç€è¦é‡æ–°æ€è€ƒå’Œæ„å»ºæ•´ä¸ªæ ˆï¼Œè€Œä¸æ˜¯ä»…ä»…ä¼˜åŒ–ä¸€å±‚ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè·¨å„ç§ç”¨ä¾‹æ‹“å±•çš„ç³»ç»Ÿï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåœ¨ AI å˜åŒ–æ—¶è¢«æŠ›å¼ƒçš„ä¸€ä¸ª point solution

**We**Â [**assembled a team**](https://www.modular.com/company/careers)Â **that had lived the pain**. Folks who helped build CUDA, TPUs, MLIR, TensorFlow, PyTorch, and many other software systems. We werenâ€™t armchair criticsâ€”we wrote the code, built the infra, and lived the failures. That gave us a deep understanding of both the technical and human sides of the problemâ€”and a shared sense of unfinished business.

But having great people isnâ€™t enough. To take on an industry-scale challenge, we had toÂ **empower them**Â with the rightÂ [environment and values](https://www.modular.com/company/culture). We focused early onÂ **leadership, culture, and product excellence**, because weâ€™d seen how quickly misaligned incentives can derail even great technology. We made space to â€œbuild things rightâ€ because so little in AI actually is.

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/681b614edc8ff3f82f1ec728_66a76980a81895c66b0ab3b5_mojo-wireframe%20\(1\).jpg)

**We are independent and focused on AI infrastructure**â€”because we knew we couldnâ€™t truly serve the ecosystem if we were secretly trying to sell chips, cloud services, foundation models, or autonomous vehicles. Our incentive had to be aligned with the long-term success of AI software itselfâ€”not just one narrow application. Weâ€™re not building a chip. Or a cloud. Or a foundation model. Weâ€™re building theÂ _neutral ground_â€”the infrastructure others can build on. An enabler, not a competitor.
>  æˆ‘ä»¬ç‹¬ç«‹ä¸”ä¸“æ³¨äº AI Infraï¼Œæˆ‘ä»¬çš„æ¿€åŠ±éœ€è¦å’Œ AI è½¯ä»¶é•¿æœŸä¿æŒä¸€è‡´ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªç‹­çª„çš„é¢†åŸŸ
>  æˆ‘ä»¬ä¸æ˜¯åœ¨æ„å»ºèŠ¯ç‰‡ã€äº‘æœåŠ¡ã€åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨æ„å»ºä¸­ç«‹çš„å¹³å° - å…¶ä»–äººéƒ½å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¼€å‘ï¼Œæˆ‘ä»¬æ˜¯æ¨åŠ¨è€…ï¼Œè€Œä¸æ˜¯ç«äº‰å¯¹æ‰‹

**We also needed scale.**Â This is a huge vision, and requires not just talent and alignment, but serious resources to pay for it. We were fortunate to raise enough funding to launch this mission. Even more importantly, we were backed by investors likeÂ [Dave Munichiello at GV](https://www.gv.com/team/dave-munichiello)Â and theÂ [team at General Catalyst](https://www.generalcatalyst.com/team)â€”people who brought not only deep technical understanding, but long time horizons and conviction about what success could mean for the entire field.

All of this was just the starting point. With the fundamentals in placeâ€”clear vision, the right people, aligned incentives, and enough runwayâ€”we could finally begin building. But there was still one enormous problem:Â **there was no shared direction in the industry.**Â No common foundation. No unifying plan. Just a tangle of competing tools, brittle abstractions, and hardware racing ahead of the software meant to support it. We had many ideasâ€”but no illusions. Real progress meant solving what the industry had failed to crack for over a decade:Â **a massive open research problem**, with no guaranteed answers.

## How to tackle a massive open research problem
AI isnâ€™t a sleepy industry, and the pace of system-building isnâ€™t calm either. Itâ€™s aÂ [hardware regatta in a turbulent sea](https://www.modular.com/blog/democratizing-ai-compute-part-9-why-do-hw-companies-struggle-to-build-ai-software)Â ğŸŒŠ.

Everyoneâ€™s racingâ€”the startup speedboats ğŸš¤, the focused yachts â›µ, the megacorp ocean liners ğŸ›³ï¸, and of course, NVIDIAâ€™s aircraft carrier ğŸš¢. Theyâ€™re all jockeying for positionâ€”building chips and stacks, launching foundation models and platforms, locking down APIs while chasing the next GenAI breakthrough. And while they collide, the sea is littered with wreckage: churn, complexity, fragmentationâ€¦ and a graveyard of half-built stacks.

**We took a different path. We got out of the water and took to the air. âœˆï¸**

Instead of entering the same race and dodging torpedoes, we made space for deep research. We stepped back, recharted the map, and spentÂ _years_Â quietly working on problems others had circled for a decade but never solved. And yes, some people told us we were crazy.

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/681b53253bba9267940c2075_success-straight-768x542.jpg)

(This popular meme is actually fromÂ [This is a Book](https://www.amazon.com/gp/product/0446539694?camp=1789&creative=390957&creativeASIN=0446539694)Â by Demetri Martin)

> ğŸ§ª Taking years for fundamental R&D sounds slowâ€¦ until you realize everyone else has been stuck for a decade.

While others chased accelerators and point solutions,Â [we proved generality on CPUs](https://www.modular.com/blog/the-worlds-fastest-unified-matrix-multiplication)â€”because if it works on CPUs, it can work anywhere. While the world narrowed toward vertical silos, we doubled down on programmability and flexibility. Because the only way to crack a grand challenge isnâ€™t just to race fasterâ€”itâ€™s to build something fundamentally new.

We also stayed deliberately closedâ€”not because we donâ€™t know open ecosystems, but becauseÂ **consensus kills research**. Sometimes, you need space toÂ _figure things out_Â before inviting the world in. I learned this the hard way withÂ [OpenCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives)Â andÂ [MLIR](https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure): everyone has opinions, especially in infrastructure, and too many inputs and constraints too early just slows you down.

We took flack for that. But letâ€™s be clear:

> We're not here to win points on Twitter. Weâ€™re willing to do the hard thing in order to make fundamental progress.

## Scaling into this deliberately: one step at a time
With space to do the fundamental work, we tackled the hard problems head-onâ€”and scaled deliberately, one milestone at a time. First, we had to prove that a new approach to code generation could actually work. Then came syntax, usability, performance, and ecosystem fit.

As we built the platform, we were our own first users. We hit the bugs, ran into the limitations, struggled through the early painâ€”and used that pain to guide our priorities. That kept us honest.

No proxy metrics. No vague abstractions. Just one question:

> Can real engineers build real systems, faster, with this?

>  æ²¡æœ‰ä»£ç†çš„æŒ‡æ ‡ï¼Œä¹Ÿæ²¡æœ‰æ¨¡ç³Šçš„æŠ½è±¡æ¦‚å¿µï¼Œåªæœ‰ä¸€ä¸ªé—®é¢˜:
>  çœŸæ­£çš„å·¥ç¨‹å¸ˆèƒ½å¦ä½¿ç”¨è¿™ä¸ªå·¥å…·æ›´å¿«åœ°æ„å»ºçœŸæ­£çš„ç³»ç»Ÿï¼Ÿ

We kept raising the bar. First, it was PyTorch, TorchScript, and ONNX. Then TensorRT-LLM, vLLM, and the bleeding edge of GenAI workloads. And when we finally got to H100 earlier this yearâ€”with a tiny team and no vendor hand-holdingâ€”we brought it up from scratch, tuned it ourselves, and got real models running in under two months.

> Most teams donâ€™t even have their kernel compiler booting in two months. We were already running production-grade models at performance matching the rest of the world. This was on the most popular hardware that had been tuned byÂ _the entire world_Â for years at this point.

Thatâ€™s the kind of pressure that forges breakthroughs. Because in this space, if youâ€™re not catching up from behind while the bar keeps moving, youâ€™re not even in the race. Â Getting here took over three years of methodical, closed development. But from the very beginning, we werenâ€™t building just for ourselves. We always knew this had to scale beyond us.

Weâ€™re not here to build everythingâ€”weâ€™re here to build theÂ **foundation**. A foundation thatâ€™s fast, flexible, and open. One that can scale with the industry, adapt to new use cases, and help everyone go faster. But that only works if it's open so the whole community can participate.
>  æˆ‘ä»¬ä¸æ˜¯è¦æ‰“é€ ä¸€åˆ‡ -æˆ‘ä»¬æ˜¯è¦å»ºé€ åŸºç¡€ï¼Œä¸€ä¸ªå¿«é€Ÿã€çµæ´»ã€å¼€æ”¾çš„åŸºç¡€
>  ä¸€ä¸ªå¯ä»¥éšç€è¡Œä¸šä¸€èµ·æ‹“å±•ã€é€‚åº”æ–°ç”¨ä¾‹ã€å¸®åŠ©æ¯ä¸ªäººæé€Ÿçš„åŸºç¡€

## Modular is now Open!
After more than three years of heads-down R&D, weâ€™re officially out of the labâ€”and into the wild. Modular is now in full execution mode: shipping major releases every 6â€“8 weeks, and developer builds nearly every night. The platform is working. The stack is coming together. The APIs are starting to settle.
>  ç»è¿‡ä¸‰å¹´çš„ç ”å‘åï¼ŒModular å·²ç»è¿›å…¥äº†æ‰§è¡Œæ¨¡å¼: æ¯ 6-8 å‘¨å‘å¸ƒä¸€ä¸ªé‡å¤§ç‰ˆæœ¬ï¼Œå¹¶ä¸”æ¯å¤©éƒ½æœ‰å¼€å‘è€…æ„å»ºç‰ˆæœ¬
>  å¹³å°å·²ç»æ­£å¸¸è¿è¡Œï¼Œæ•´ä¸ªæŠ€æœ¯æ ˆæ­£åœ¨é€æ­¥æ•´åˆï¼ŒAPI ä¹Ÿè¶‹äºç¨³å®š

This means itâ€™s time to open the doorsâ€”and see whatÂ _you_Â can build with it.

[Weâ€™ve just open-sourced over half a million lines](https://www.modular.com/blog/modular-platform-25-3-450k-lines-of-open-source-code-and-pip-install-modular)Â of high-performance GPU primitivesâ€”optimized, portable, and ready to run across multiple architectures. Alongside that, weâ€™ve released serving infrastructure, models, and more. You can run it all for free.

**This isnâ€™t a teaser. This is real software, running real GenAI workloads, built to move at real-world Â speed.**

##### **Our goal is simple: finally, truly, Democratize AI Compute.**
Weâ€™re not just here to â€œcatch up to CUDA.â€ CUDA launched the AI revolutionâ€”but itâ€™s time for the next step. Weâ€™re building a better way to programÂ _all_Â acceleratorsâ€”even NVIDIAâ€™s.
>  æˆ‘ä»¬ä¸æ˜¯ä¸ºäº†èµ¶ä¸Š CUDAï¼Œæˆ‘ä»¬æ­£åœ¨æ‰“é€ ä¸€ç§æ›´å¥½çš„æ–¹å¼æ¥**ç¼–ç¨‹æ‰€æœ‰åŠ é€Ÿå™¨** - åŒ…æ‹¬ NVIDIA çš„

Because while NVIDIA makes incredible hardware, it faces the same challenges as everyone else: fragmentation, usability, and the fast moving nature of AI. Thatâ€™s the problem weâ€™ve signed up to solveâ€”with somethingÂ _portable_,Â _programmable_, andÂ _powerful_Â enough to serve the entire AI community.
>  æˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜å°±æ˜¯æ‰“é€ ä¸€ç§å¯ç§»æ¤ã€å¯ç¼–ç¨‹å¹¶ä¸”å¼ºå¤§åˆ°è¶³ä»¥æœåŠ¡æ•´ä¸ª AI ç¤¾åŒºçš„è§£å†³æ–¹æ¡ˆ

Letâ€™s end the gatekeeping. Letâ€™s stop pretending GPU programming is just for compiler wizards or billion-dollar chip companies. Itâ€™s time to open up the frontierâ€”to make AI compute usable and accessible for everyone. Just like Swift opened up iOS development, this is about unlocking the next wave of developer innovation.

> â€œThe best way to predict the future is to invent it.â€ -Alan Kay

Next time, weâ€™ll dig into how it all comes togetherâ€”starting with a run-through of Modular's product line-up.

Until thenâ€”stay above the waves, keep your compass steady, and chart your own path. âœˆï¸ğŸŒŠ

- Chris

# 11 How is Modular Democratizing AI Compute? 
Site: https://www.modular.com/blog/how-is-modular-democratizing-ai-compute
Date: 20 June 2025

Given time, budget, and expertise from a team of veteransÂ [whoâ€™ve built this stack before](https://www.modular.com/blog/modulars-bet-to-break-out-of-the-matrix-democratizing-ai-compute-part-10), Modular set out to solve one of the defining challenges of our era:Â **how to Democratize AI Compute**. But what does thatÂ _really_Â meanâ€”and how does it all add up?

This post is your end-to-end guide. Weâ€™ll walk through the technology, the architecture, and the underlying philosophyâ€”before diving deeper into each layer in future posts.

At the heart of it is a singular idea: to democratize AI compute,Â **we need to unify the scattered stars of AI**:

- Unify developersâ€”across backgrounds and skill levels.
- Unify low-level softwareâ€”across frameworks and runtimes.
- Unify hardware makersâ€”across vendors, devices, and use cases.
- Unify an industry of competing interestsâ€”who have grown a chaotic software stack that consolidated around one dominant vendor.

>  è¦æ°‘ä¸»åŒ– AI è®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦å°†åˆ†æ•£çš„ AI æ˜Ÿè¾°æ±‡èšæˆä¸€ä¸ªæ•´ä½“:
>  - æ±‡èšå¼€å‘è€… - æ— è®ºä»–ä»¬çš„èƒŒæ™¯å’ŒæŠ€èƒ½æ°´å¹³å¦‚ä½•
>  - æ±‡èšåº•å±‚è½¯ä»¶ - è·¨è¶Šä¸åŒçš„æ¡†æ¶å’Œè¿è¡Œæ—¶ç¯å¢ƒ
>  - æ±‡èšç¡¬ä»¶åˆ¶é€ å•† - è·¨è¶Šä¸åŒçš„ä¾›åº”å•†ã€è®¾å¤‡ã€ä½¿ç”¨åœºæ™¯
>  - æ±‡èšä¸€ä¸ªåˆ©ç›Šç«äº‰çš„è¡Œä¸š - å®ƒå·²ç»å½¢æˆäº†ä¸€ä¸ªæ··ä¹±çš„è½¯ä»¶æ ˆï¼Œå¹¶å›´ç»•ä¸€ä¸ªä¸»å¯¼ä¾›åº”å•†é›†ä¸­

For too long, the AI software landscape has been a disconnected starfieldâ€”brilliant points of innovation, but hard to navigate, harder to connect, and spreading further apart every year. Modular is building theÂ **infrastructure to turn that starfield into a constellation**: a coherent system that helps developers chart their path, unites the stars, and unlocks the full potential of AI.
>  Modular æ„åœ¨ä¸ºè¿™äº›ä¸œè¥¿æ„å»ºä¸€ä¸ªåŸºç¡€è®¾æ–½ï¼Œå°†å®ƒä»¬æ±‡èš: ä¸€ä¸ªè¿è´¯çš„ç³»ç»Ÿï¼Œå¸®åŠ©å¼€å‘è€…è§„åˆ’è·¯å¾„ï¼Œå¹¶é‡Šæ”¾ AI çš„å…¨éƒ¨æ½œåŠ›

Success in AI isnâ€™t just about how powerful your hardware is, itâ€™s about how many people canÂ _use_Â it. That means lowering barriers, opening access, andÂ **building software tools that people love to use**â€”not just to run benchmarks.

## ğŸŒŒ The Worldâ€™s First Unified AI Constellation
Democratizing AI compute is about removing the invisibleÂ [dark matter](https://en.wikipedia.org/wiki/Dark_matter)Â that divides the landscape. Today, the stars of AI are scattered across vendor boundaries, siloed software stacks, and outdated abstractions. We all want higher throughput and lower latency and TCO, but AIÂ developers & deployers are forced to choose: a â€œsafe bet for todayâ€ or owning your destiny with portability and generality in the future.

At Modular, we believe thereâ€™s a better way. One that doesnâ€™t ask developers to compromise:Â **weâ€™re building toward a unified constellation.**

Our goal is to expose theÂ _full power_Â of modern hardwareâ€”NVIDIAâ€™s Tensor Cores, AMDâ€™s matrix units, Appleâ€™s advanced unified memory architectureâ€”not by hiding their complexity, but by building a system that understands it. One that lets developers scale effortlessly across clients, datacenters, and edge devicesâ€”without getting lost in a maze of incompatible compilers and fragmented runtimes.
>  æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é‡Šæ”¾ç°ä»£ç¡¬ä»¶çš„å…¨éƒ¨æ½œåŠ› - NVIDIA Tensor Cores, AMD matrix units, Apple çš„å…ˆè¿›ç»Ÿä¸€å†…å­˜æ¶æ„ - ä¸æ˜¯é€šè¿‡éšè—ä»–ä»¬çš„å¤æ‚æ€§ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿç†è§£è¿™äº›å¤æ‚æ€§çš„ç³»ç»Ÿ
>  ä¸€ä¸ªè®©å¼€å‘è€…åœ¨å®¢æˆ·ç«¯ã€æ•°æ®ä¸­å¿ƒã€è¾¹ç¼˜è®¾å¤‡æ— ç¼æ‹“å±•çš„ç³»ç»Ÿ - è€Œæ— éœ€é™·å…¥ä¸å…¼å®¹çš„ç¼–è¯‘å™¨å’Œç¢ç‰‡åŒ–çš„è¿è¡Œæ—¶ç¯å¢ƒä¸­

Itâ€™s time to move beyond legacy architecturesâ€”like OpenCL and CUDAâ€”designed in a pre-GenAI era. CUDA launched the AI revolution, and theÂ [industry owes it a great deal](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed). But the future requires something more:Â **a software stack built for GenAI from the ground up**, designed for todayâ€™s workloads, todayâ€™s developers, and todayâ€™s hardware and scale.
>  æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»é›¶å¼€å§‹ä¸º GenAI æ‰“é€ çš„è½¯ä»¶æ ˆï¼Œé’ˆå¯¹å¦‚ä»Šçš„ workloadsã€å¦‚ä»Šçš„å¼€å‘è€…ã€å¦‚ä»Šçš„ç¡¬ä»¶å’Œè§„æ¨¡è®¾è®¡

This constellationÂ [canâ€™t be unified by any single hardware vendor](https://www.modular.com/blog/democratizing-ai-compute-part-9-why-do-hw-companies-struggle-to-build-ai-software): vendors build great software forÂ _their_Â chipsâ€”but the starry night sky is much broader. It spans NVIDIA, AMD, Intel, Apple, Qualcomm, and others inÂ [the hardware regatta â›µ](https://www.modular.com/blog/democratizing-ai-compute-part-9-why-do-hw-companies-struggle-to-build-ai-software), along withÂ [a](https://www.cerebras.net/)Â [wave](https://www.etched.com/)Â [of](https://groq.com/)Â [new](https://www.sifive.com/)Â [stars](https://tenstorrent.com/)Â [rising](https://www.graphcore.ai/)Â [across](https://mythic.ai/)Â the AI hardware frontier. We think the industry must link arms and build together instead of fragmenting the galaxy further.

At Modular, we measure success with a simple but ambitious goal:

> We want a unified, programmable system (one small binary!) that can scale across architectures from multiple vendorsâ€”while providing industry-leading performance on the most widely used GPUs (and CPUs).

>  Modularï¼Œæˆ‘ä»¬è¡¡é‡æˆåŠŸçš„ç›®æ ‡æ˜¯:
>  æœ‰ä¸€ä¸ªç»Ÿä¸€çš„å¯ç¼–ç¨‹çš„ç³»ç»Ÿ (ä¸€ä¸ªå°å°çš„äºŒè¿›åˆ¶æ–‡ä»¶)ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå‚å•†çš„æ¶æ„ä¸Šæ‹“å±• - åŒæ—¶åœ¨æœ€å¹¿æ³›ä½¿ç”¨çš„ GPU ä¸Šæä¾›ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½

Thatâ€™s what a unified constellation means: Not uniformityâ€”but a coherent, collaborative, and collective momentum. A system that celebrates hardware diversity while empowering developers with a common mapâ€”one they can use to build, explore, and reach further than ever before.

## ğŸª A Galactic Map for AI Compute
The AI universe is vastâ€”and itâ€™s rare to find two developers who work on exactly the same thing. Some operate near the core, close to the metal. Others orbit further out: building models, deploying inference pipelines, or managing massive GPU fleets. The landscape is fragmentedâ€”but it doesnâ€™t have to be.

We designed the Modular Platform to unify this space with a novel, layered architecture: a system thatâ€™s powerful when used as a whole, but modular enough to plug into your existing tools like PyTorch, vLLM, and CUDA. Whether you're writing kernels, consolidating your inference platform, or scaling your infrastructure,Â **Modular meets you where you areâ€”and lights the path to where you're going.**
>  Modular å¹³å°ä½¿ç”¨ä¸€ä¸ªå±‚æ¬¡åŒ–çš„æ¶æ„æ¥ç»Ÿä¸€ AI çš„å„å±‚å¼€å‘: ä¸€ä¸ªæ•´ä½“ä½¿ç”¨æ—¶åŠŸèƒ½å¼ºå¤§çš„ç³»ç»Ÿï¼ŒåŒæ—¶åˆè¶³å¤Ÿæ¨¡å—åŒ–ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„å·¥å…·ä¸­ï¼Œä¾‹å¦‚ PyTorch, vLLM, CUDAï¼Œæ— è®ºä½ æ˜¯ç¼–å†™ kernelï¼Œæ•´åˆæ¨ç†å¹³å°ï¼Œè¿˜æ˜¯æ‹“å±•åŸºç¡€è®¾æ–½

Letâ€™s dig into how the layers stack up

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/68556282c5a792c6052e9a63_Group%2021.png)

The central star of the solar system is the hardware, withÂ **Mojo**Â closely orbiting it, whileÂ **MAX**Â Â is a gas giant with a deep atmosphere. At the edges, we see the system is wrapped by a spiral arm of thisÂ **Mammoth**Â cluster.

### MojoğŸ”¥: A Programming Language for Heterogenous GenAI Compute
[**Mojo**](https://www.modular.com/mojo)Â is a new language for a GenAI era, designed to solve the language fragmentation problem in AI. Developers love Mojo because it provides the speed and capability of C++, Rust, and CUDA but with familiar and easy-to-learn Python syntax that AI developers demand.
>  Mojo æ˜¯ä¸€é—¨é¢å‘ GenAI çš„æ–°è¯­è¨€ï¼Œæ—¨åœ¨è§£å†³ AI ä¸­çš„è¯­è¨€ç¢ç‰‡åŒ–é—®é¢˜
>  Mojo æä¾›äº† C++, Rust, CUDA çš„é€Ÿåº¦å’ŒåŠŸèƒ½ï¼ŒåŒæ—¶æ‹¥æœ‰ AI å¼€å‘è€…æ‰€æœŸæœ›ä¸”æ˜“äºå­¦ä¹ çš„ Python è¯­æ³•

Mojo seamlessly integrates into existing workflowsâ€”Mojo files live side-by-side with Python modules with no bindings or extra build toolsâ€”while unlocking modern hardware: CPUs, GPUs, and custom accelerators. It offers developers great flexibility and usability, whether itâ€™s crafting advanced GPU kernels like FlashAttention, leveraging Tensor Cores and TMAs, or implementing AI-specific optimizations with low-level control.
>  Mojo å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„å·¥ä½œæµä¸­ - Mojo æ–‡ä»¶å¯ä»¥ä¸ Python modules å…±å­˜ï¼Œæ— éœ€ç»‘å®šæˆ–é¢å¤–çš„æ„å»ºå·¥å…· - åŒæ—¶è§£é”ç°ä»£ç¡¬ä»¶: CPUs, GPUs å’Œè‡ªå®šä¹‰åŠ é€Ÿå™¨
>  æ— è®ºæ˜¯åˆ©ç”¨ Tensor Cores, TMA ç¼–å†™é«˜çº§ GPU kernel ä¾‹å¦‚ FlashAttentionï¼Œæˆ–è€…é€šè¿‡ä½çº§æ§åˆ¶å®ç° AI ç‰¹å®šçš„ä¼˜åŒ–ï¼ŒMojo éƒ½ä¸ºå¼€å‘è€…æä¾›äº†çµæ´»æ€§å’Œæ˜“ç”¨æ€§

> Mojo is like the inner planets of a solar systemâ€”close to the heat, close to the metal. This is where performance lives and FLOPS go brrrr.

Though Modular is focused on AI, we believe Mojo's ability to accelerateÂ _existing_Â Python code opens up high-performance GPU programming toÂ **millions more developers**, across domains. We aspire for Mojo to be the â€œbest way to extend Python codeâ€ for developers in all domains.
>  æˆ‘ä»¬ç›¸ä¿¡ Mojo åŠ é€Ÿç°å­˜ä»£ç çš„èƒ½åŠ›ï¼Œå°†é«˜æ€§èƒ½ GPU ç¼–ç¨‹å‘æ•°ç™¾ä¸‡æ›´å¤šå¼€å‘è€…å¼€æ”¾ï¼Œè¦†ç›–å„ä¸ªé¢†åŸŸ
>  æˆ‘ä»¬å¸Œæœ› Mojo æˆä¸ºæ‹“å±• Python ä»£ç çš„æœ€ä½³æ–¹å¼

### MAXÂ **ğŸ‘©â€ğŸš€**: The Modeling and Serving Layer
Orbiting Mojo isÂ [**MAX**](https://www.modular.com/max) â€”a unified, production-grade GenAI serving framework that answers the natural follow-up to Mojoâ€™s portability: â€œWhy not just build in PyTorch?â€ MAX goes where PyTorch stops, packaging state-of-the-art inference into a slim 1 GB container that cold-starts fast.
> å›´ç»• Mojo çš„æ˜¯ MAX - ä¸€ä¸ªç»Ÿä¸€çš„ã€é€‚ç”¨äºç”Ÿäº§çš„ GenAI æœåŠ¡æ¡†æ¶ï¼Œå®ƒå›ç­”äº† Mojo å¯ç§»æ¤æ€§ä¹‹åçš„ä¸€ä¸ªè‡ªç„¶é—®é¢˜: ä¸ºä»€ä¹ˆä¸åœ¨ PyTorch ä¸­æ„å»ºï¼Ÿ
> MAX è¶…è¶Šäº† PyTorch çš„é™åˆ¶ï¼Œå°† SOTA çš„æ¨ç†èƒ½åŠ›æ‰“åŒ…åˆ°ä¸€ä¸ªä»… 1GB çš„è½»é‡å®¹å™¨ä¸­ï¼Œå®ç°å¿«é€Ÿå†·å¯åŠ¨

GenAI is about far more than a forward pass. Modern pipelines juggle KV-cache lifecycles, paged attention, speculative decoding, and hardware-aware scheduling. MAX folds all of that complexity into a familiar, PyTorch-like Python API, so you write dynamic graphs while it delivers predictable, fleet-wide performance.
>  GenAI è¿œä¸æ­¢ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œç°ä»£çš„æµæ°´çº¿éœ€è¦å¤„ç† KVCache å£°æ˜å‘¨æœŸï¼Œpaged-attentionï¼Œæ¨æµ‹è§£ç å’Œç¡¬ä»¶æ„ŸçŸ¥çš„è°ƒåº¦
>  MAX å°†æ‰€æœ‰çš„å¤æ‚æ€§å°è£…åœ¨ä¸€ä¸ªç†Ÿæ‚‰ä¸”ç±»ä¼¼äº PyTorch çš„ Python API ä¸­ï¼Œè®©æˆ‘ä»¬åœ¨å¯ä»¥ç¼–å†™åŠ¨æ€å›¾çš„åŒæ—¶ï¼Œè·å¾—å¯é¢„æµ‹çš„ã€å…¨é›†ç¾¤èŒƒå›´çš„é«˜æ€§èƒ½è¡¨ç°

> Picture MAX as the massive gas giant in your GenAI solar system. Compute is the central star, and MAXâ€™s deep â€œatmosphereâ€ of KV-cache handling, paged attention, and speculative decoding provides the gravitational heft that keeps individual AI apps in orderly orbit while letting new models or hardware drift in without turbulence.

Built for use in heterogeneous clusters, a single MAX binary extracts peak throughput from todayâ€™s H200â€™s, B200â€™s and MI325â€™s, growing into tomorrowâ€™s MI355â€™s and B300â€™s, and even mixed CPU/GPU footprints. Aggressive batching and memory optimizations drive the highest tokens-per-dollar, while the elimination of surprise recompiles and kernel swaps keeps latency steady under spiky loadsâ€”turning research notebooks into production-ready GenAI services without sacrificing speed, flexibility, or hardware choice.
>  MAX ä¸“ä¸ºå¼‚æ„é›†ç¾¤è®¾è®¡ï¼Œä¸€ä¸ªå•ç‹¬çš„ MAX äºŒè¿›åˆ¶æ–‡ä»¶å¯ä»¥å……åˆ†å‘æŒ¥ H200, B200, MI325 çš„å³°å€¼ååé‡ï¼Œå¹¶èƒ½å¤Ÿæ‹“å±•æœªæ¥çš„ç¡¬ä»¶ï¼Œç”šè‡³æ”¯æŒæ··åˆ CPU/GPU é…ç½®

### Mammoth ğŸ¦£: GPU Cluster Management for the GenAI Age
[**Mammoth**](https://www.modular.com/mammoth)Â is aÂ **Kubernetes-native**Â platform that turns fixed GPU footprintsâ€”on-prem or in the cloudâ€”into an elastic, high-performance inference fabric.
>  Mammoth æ˜¯ä¸€ä¸ªåŸç”Ÿ K8S å¹³å°ï¼Œå°†å›ºå®šçš„ GPU éƒ¨ç½² - åœ¨äº‘ç«¯æˆ–æœ¬åœ° - è½¬åŒ–ä¸ºå¼¹æ€§çš„é«˜æ€§èƒ½æ¨ç†ç½‘ç»œ

GenAI has pushed optimizations higher up the stack: modern transformer models split their pre-fill and decode stages acrossÂ _many_Â GPUs, shattering two old cloud assumptions. First, workloads are no longer statelessâ€”chatbots and agents need to preserve conversational context. Second, GPUs canâ€™t be spun up on demand; theyâ€™re capacity-constrained assets tied to multi-year commits, so every TFLOP has to count.
>  GenAI å°†ä¼˜åŒ–å‘å †æ ˆçš„æ›´é«˜å±‚æ¨åŠ¨: ç°ä»£çš„ transformer å°† pre-fill å’Œ decode é˜¶æ®µåˆ†å¸ƒåœ¨å¤šä¸ª GPU ä¸Šï¼Œæ‰“ç ´äº†ä¸¤ä¸ªä¼ ç»Ÿçš„äº‘å‡è®¾
>  ç¬¬ä¸€ä¸ªï¼Œworkloads ä¸å†æ˜¯æ— çŠ¶æ€çš„ - chatbots, agents éœ€è¦ä¿ç•™å¯¹è¯ä¸Šä¸‹æ–‡
>  ç¬¬äºŒä¸ªï¼ŒGPUs ä¸èƒ½æŒ‰éœ€å¯åŠ¨ï¼Œå®ƒä»¬æ˜¯å—å¤šå¹´æ‰¿è¯ºçº¦æŸçš„èµ„æºï¼Œå› æ­¤æ¯ TFLOPS éƒ½å¿…é¡»è¢«å……åˆ†åˆ©ç”¨

BecauseÂ **Kubernetes is already the control plane enterprises trust**, Mammoth simply drops into existing clusters and layers on the capabilities teams are missing:

- **MAX-aware orchestration**Â lets Mammoth coordinate with MAX for just-in-time autoscaling, intelligent placement of pre-fill and decode nodes, and fast checkpoint streaming.
- **Dynamic, multi-hardware scheduling**Â treats a cluster of accelerators from multiple vendors as one resource pool, bin-packing workloads onto the best silicon in real time.
- **A unified declarative ops model**Â exposes one API for on-prem and cloud clusters, so platform teams can ditch bespoke schedulers and hand-rolled scripts.

>  å› ä¸º K8S å·²ç»æ˜¯ä¼ä¸šæ‰€ä¿¡ä»»çš„æ§åˆ¶å¹³å°ï¼ŒMammoth åªéœ€éƒ¨ç½²åˆ°ç°æœ‰çš„é›†ç¾¤ä¸­ï¼Œå¹¶å åŠ å›¢é˜Ÿç¼ºå¤±çš„åŠŸèƒ½:
>  - MAX å‹å¥½çš„ç¼–æ’: Mammoth å’Œ MAX åä½œï¼Œå®ç°è‡ªåŠ¨æŒ‰éœ€æ‹“å±•ã€prefill å’Œ decode nodes çš„æ™ºèƒ½æ”¾ç½®ä»¥åŠå¿«é€Ÿ checkpoint ä¼ è¾“
>  - åŠ¨æ€ã€å¤šç¡¬ä»¶è°ƒåº¦: å°†æ¥è‡ªå¤šä¸ªä¾›åº”å•†çš„åŠ é€Ÿå™¨é›†ç¾¤è§†ä½œä¸€ä¸ªç»Ÿä¸€çš„èµ„æºæ± ï¼Œå®æ—¶å°† workloads åˆ†é…åˆ°æœ€ä½³çš„èŠ¯ç‰‡ä¸Š
>  - ç»Ÿä¸€çš„å£°æ˜å¼è¿è¥æ¨¡å‹: ä¸ºæœ¬åœ°å’Œäº‘é›†ç¾¤æš´éœ²ç»Ÿä¸€çš„ API

The result is aÂ **simple, scalable orchestration layer**Â that lets CIOs embrace heterogeneous hardware without vendor lock-inâ€”while developers stay entirely inside the Kubernetes workflows they already know.
>  è¿™æ ·å¾—åˆ°çš„å°±æ˜¯ä¸€ä¸ªç®€å•ã€å¯æ‹“å±•çš„ç¼–æ’å±‚ï¼Œè®© CIO å¯ä»¥é‡‡ç”¨å¼‚æ„ç¡¬ä»¶è€Œæ— éœ€é”å®šå‚å•†ï¼ŒåŒæ—¶å¼€å‘è€…åœ¨ä»–ä»¬ç†Ÿæ‚‰çš„ K8S å·¥ä½œæµä¸­å·¥ä½œ

> Mammoth is like the spiral arm of the galaxyâ€”an overarching gravitational framework that organizes many solar systems at once. Mammothâ€™s scheduling gravity aligns each solar system into smooth, predictable rotation, making room for new â€œstarsâ€ or â€œplanetsâ€ (hardware and workloads) without ever destabilizing the galactic whole.

While each of these layersâ€”Mojo, MAX, Mammothâ€”can stand on its own, together they form aÂ **coherent galactic map**Â for GenAI compute: scalable, reliable, and portable across hardware and time.

### ğŸ’  High Performance Models and Kernels
The Modular Platform is more than a CUDA-replacementâ€”itâ€™s a launchpad that meets two very different personas right where they work:

- **AI engineers & MLOps teams want production-ready assets.**Â We ship complete, open-source model pipelines pre-tuned for speed and packaged in a ~1 GB container that run unchanged on CPUs and NVIDIA or AMD GPUs.
- **AI researchers & kernel hackers crave low-level control.**Â Our GitHub repo atÂ `modular/modular`Â exposes hand-optimized GPU kernelsâ€”FlashAttention, paged attention, KV-cache orchestration, speculative decodingâ€”written in Mojo so you can tweak internals or invent entirely new operators without rewriting the stack.

>  Modular å¹³å°èƒ½å¤Ÿæ»¡è¶³ä¸¤ç§éå¸¸ä¸åŒçš„è§’è‰²çš„å·¥ä½œ:
>  - å¯¹äº AI å·¥ç¨‹å¸ˆå’Œ MLOps å›¢é˜Ÿï¼Œæˆ‘ä»¬æä¾›å®Œæ•´ã€å¼€æºçš„æ¨¡å‹æµæ°´çº¿ï¼Œæ‰“åŒ…åœ¨ä¸€ä¸ª 1GB çš„å®¹å™¨ä¸­ï¼Œè¿™äº›å®¹å™¨åœ¨ CPUs å’Œ NVIDIA, AMD GPUs ä¸Šæ— éœ€ä¿®æ”¹å°±èƒ½è¿è¡Œ
>  - å¯¹äº AI ç ”ç©¶äººå‘˜å’Œ kernel ç¼–å†™è€…ï¼Œæˆ‘ä»¬æä¾›äº† Mojo ç¼–å†™çš„ï¼Œæ‰‹å·¥ä¼˜åŒ–çš„ GPUI kernels

Because every model and kernel sits on a common runtime, you can start fast with proven building blocks and dive deep only when you need to. The result is the largest coherent library of portable, open-source AI components anywhereâ€”powerful enough for enterprise teams that just want to ship, yet modular enough for researchers pushing the frontier.
>  å› ä¸ºæ‰€æœ‰çš„æ¨¡å‹å’Œ kernel éƒ½åœ¨å…±åŒçš„è¿è¡Œæ—¶ä¸Šæ„å»ºï¼Œæ•…å¯ä»¥å€Ÿç”¨å…¶ä»–çš„æ„å»ºå¥½çš„ blocks å¿«é€Ÿå¯åŠ¨

> Picture these model pipelines as comets that soar around of the solar systemâ€”the content that gives the infrastructure meaning.

Open source remains the bedrock of AI progress; a unified ecosystem ensures you can start with something powerful and go further than ever beforeâ€”whether that means shipping a feature on Monday or publishing a paper on Friday.

### ğŸ›ï¸ An Expanding Hardware Constellation
Truly democratizing AI compute requires the ability to scale into far more hardware than any team could individually supportâ€”it requires an industry coalition and experts in the hardware to drive best-possible support for their silicon.

> Hardware diversity should be the foundation of the modern AI universe, not a problem. More choice and specialized solutions will drive more progress and products into the world.

The Modular stack was specifically designed to scale into a wide range of different accelerators, giving hardware innovators control over their performance and capabilities. Â Now that Modular can prove portability across multiple industry standard GPUs from leaders like NVIDIA and AMD, we would like to open up our technology platform to far more hardware partners.
>  Modular stack ä¸“é—¨è®¾è®¡ç”¨äºæ‹“å±•åˆ°ä¸åŒçš„åŠ é€Ÿè®¾å¤‡ï¼ŒModular ç›®å‰å·²ç»å¯ä»¥è¯æ˜è·¨å¤šä¸ªå·¥ä¸šæ ‡å‡† GPU ä¸Šçš„å¯ç§»æ¤æ€§

We donâ€™t have all the details figured out yet though! If you are part of a hardware company and interested to learn more,Â [please get in touch](https://www.modular.com/company/democratize-hardware)Â and weâ€™ll reach out at the right time. If you are an AI developer and would like expanded support for new hardware, please ask that hardware team to reach out to us!

## ğŸ“‹Â The Mission Checklist
A new AI platform canâ€™t just beÂ _clever_Â orÂ _well-intentioned_â€”it has toÂ _ship_Â andÂ **_work_**. Modular's work will never be done, but we can now show real progress on every dimension we believe is critical to Democratizing AI Compute.

Hereâ€™s how we judge the Modular Platform against the scorecard weâ€™ve used in this series to evaluateÂ [other](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls)Â [systems](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers):

- ğŸš¤**â›µğŸ›³ï¸ğŸš¢ Enable portability across hardware from multiple vendors:**Â Compute is already diverse with many participants, and Modular has demonstrated the ability to scale from CPUs to NVIDIA and to AMD,Â **all from a single unified binary**â€”an industry first. âœ… Modularâ€™s stack is designed to support ASICâ€™s and more exotic systems, but still needs to prove that. âš ï¸
>  é€šè¿‡å•ä¸ªç»Ÿä¸€çš„äºŒè¿›åˆ¶æ–‡ä»¶å®ç°è·¨å¤šä¸ªä¾›åº”å•†çš„å¯ç§»æ¤æ€§ - è¿™æ˜¯è¡Œä¸šå†…çš„é¦–åˆ›
>  Modular stack åœ¨è®¾è®¡ä¸Šå°±æ”¯æŒ ASIC å’Œæ›´å¤æ‚çš„ç³»ç»Ÿ

- ğŸš€Â **Run with top performance on the industry leaderâ€™s hardware:**Â NVIDIA makes great hardware, has the most widely deployed datacenter footprint, and is the most widely used by enterprises. Modular delivers peak performance on NVIDIAâ€™s powerful Hopper and Blackwell architectures, not just alternative hardware. âœ…
>  Modular åœ¨ NVIDIA Hopper, Blackwell ä¸Šå®ç°äº†å³°å€¼æ€§èƒ½ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›¿ä»£ç¡¬ä»¶

- ğŸ”§Â **Provide a full reference implementation:**Â Modular ships a complete, production-grade stack that you can download today: a language, a framework, a runtime, and a Kubernetes-scale system. This isnâ€™t a whitepaper or committee specâ€”itâ€™s real software you can run in production. âœ…
>  Modular æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ç”Ÿäº§çº§ stack: è¯­è¨€ã€æ¡†æ¶ã€è¿è¡Œæ—¶ã€K8S-scale ç³»ç»Ÿ

- âš¡Â **Evolve rapidly:**Â AI moves fastâ€”we move faster. ModularÂ [ships major updates every 6â€“8 weeks](https://docs.modular.com/max/changelog), and weâ€™ve brought up complex platforms like H200 and AMD MI325X in record time. This velocity is only possible because ofÂ [three years of deep tech investment](https://www.modular.com/blog/modulars-bet-to-break-out-of-the-matrix-democratizing-ai-compute-part-10). âœ…
>  æ¯ 6-8 å‘¨å°±å‘å¸ƒé‡å¤§æ›´æ–°

- ğŸ’»Â **Cultivate developer love:**Â We build for developersâ€”clean APIs, deep control, and tools that scale from hobby projects to HPC. Weâ€™re opening more of the stack every month, and weâ€™re engaging directly through forums, Discord, hackathons, and events. âœ…
>  å¹²å‡€çš„ APIï¼Œå¯¹ç¡¬ä»¶çš„æ·±å…¥æ§åˆ¶ï¼Œä»¥åŠä»ä¸ªäººé¡¹ç›®æ‹“å±•åˆ° HPC çš„å·¥å…·

- ğŸŒÂ **Build an open community:**Â [Modular is vastly open source](https://github.com/modular/modular/): hundreds of thousands of lines of high-performance models, kernels, and serving infrastructure. This is the largest portable and open AI GPU stack available today. âœ…
- ğŸ§©Â **Avoid fragmentation across implementations:**Â We embrace opennessâ€”but anchor it in aÂ **single, stable release**Â process. This gives the ecosystem confidence, avoids version nightmares, and provides a reliable foundation that runs across CPUs and GPUs alike. âœ…
>  æˆ‘ä»¬é”šå®šåœ¨ä¸€ä¸ªå•ä¸€çš„ã€ç¨³å®šçš„å‘å¸ƒæµç¨‹ä¸Šï¼ŒåŒæ—¶æä¾›äº†ä¸€ä¸ªåœ¨ GPU å’Œ CPU ä¸Šè¿è¡Œçš„åº•å±‚åŸºç¡€

- ğŸ› ï¸Â **Enable full programmability:**Â No black boxes. Mojo gives you deep control, from low-level GPU kernels to high-level orchestration, all with Pythonic clarity. Modular layers work togetherâ€”but remain programmable and composable on their own. âœ…
>  æ²¡æœ‰é»‘ç›’ï¼Œæä¾›äº†å®Œå…¨çš„å¯ç¼–ç¨‹æ€§ï¼Œä»ä½å±‚ GPU kernels åˆ°é«˜å±‚ç¼–æ’ï¼ŒåŒæ—¶ä¿æŒ Python å¼çš„æ¸…æ™°åº¦

- ğŸ¦¾Â **Provide leverage over AI complexity:**Â Todayâ€™s challenge isnâ€™t just FLOPSâ€”itâ€™sÂ _complexity at scale_. Modular brings the â€œbest ofâ€ in GenAI systems together into one place: compiler, language, and cluster orchestration. âœ…
>  Modular å°† GenAI ç³»ç»Ÿçš„æœ€ä½³å®è·µé›†ä¸­åˆ°ä¸€ä¸ªåœ°æ–¹: ç¼–è¯‘å™¨ã€è¯­è¨€ã€é›†ç¾¤ç¼–æ’

- ğŸ—ï¸Â **Enable large-scale applications:**Â Modular isnâ€™t just for benchmarksâ€”itâ€™s for production. Stateful workloads, intelligent scheduling, and resource orchestration are first-class citizens. âœ…
- ğŸ§­Â **Have strong leadership and vision:**Â Weâ€™ll let our track record speak for itself. Modular is setting an ambitious course and shipping major milestones. The path ahead is long, and weâ€™re committed to charging into it. âœ…

Each goal is ambitious on its own. Together, they define what a true successor to CUDA must deliver. Modular is well on its wayâ€”but we donâ€™t support all the worldâ€™s hardware and we know that heterogeneous compute has a future far beyond AI.

Democratizing AI compute is a galactic-scale missionâ€”far too ambitious forÂ **any one company alone**. We as an industry need to continue to come together to solve this problem as a consortium.

### Stay tuned for MojoğŸ”¥: Tackling xPU Programmability
This post laid out the big pictureâ€”a galactic map ğŸ—ºï¸ of Modularâ€™s architecture and mission. But to understand how it all works, we have to start at the core.

In the next post, weâ€™ll descend from the star clusters back toward the inner planets withÂ **Mojo**: the foundation of Modularâ€™s stack, and our boldest bet. Itâ€™s a new kind of programming languageâ€”designed to give developers deep, precise control over modern hardware, without giving up the clarity and flexibility of Python. Itâ€™s where performance meets programmability, where the hardware burns hot, truly where the magic begins.

> _â€œThe future is already here â€” itâ€™s just not evenly distributed.â€_Â â€” William Gibson

Until then, may your GPU fleets chart safe paths through the star systemsâ€”without falling into the black hole of complexity.

-Chris
