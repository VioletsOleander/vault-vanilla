# 1 DeepSeek's Impact on AI
Site: https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai
Date: 30 January 2025

DeepSeekâ€™s recent breakthrough has upended assumptions about AIâ€™s compute demands, showing thatÂ **better hardware utilization**Â can dramatically reduce the need for expensive GPUs.
>  DeepSeek çš„æœ€è¿‘çªç ´é¢ è¦†äº†å…³äº AI è®¡ç®—éœ€æ±‚çš„å‡è®¾ï¼Œè¡¨æ˜äº†æ›´å¥½çš„ç¡¬ä»¶åˆ©ç”¨ç‡å¯ä»¥æ˜¾è‘—å‡å°‘å¯¹æ˜‚è´µ GPU çš„éœ€æ±‚

For years, leading AI companies have insisted thatÂ **only those with**Â [**vast compute resources**](https://www.bbc.com/news/articles/cy4m84d2xz2o)Â can drive cutting-edge research, reinforcing the idea that it is â€œ[hopeless to catch up](https://www.youtube.com/watch?v=EtMsG2UtMUU)â€ unless you have billions of dollars to spend on infrastructure. But DeepSeekâ€™s success tells a different story:Â **novel ideas can unlock efficiency breakthroughs to accelerate AI**, and smaller, highly focused teams toÂ **challenge industry giantsâ€“** and even level the playing field.
>  å¤šå¹´æ¥ï¼Œé¢†å…ˆçš„ AI å…¬å¸ä¸€ç›´åšä¿¡åªæœ‰æ‹¥æœ‰å¤§é‡è®¡ç®—èµ„æºçš„å…¬å¸å¯ä»¥æ¨åŠ¨å‰æ²¿ç ”ç©¶
>  ä½† DeepSeek çš„æˆåŠŸåˆ™è¯´æ˜: åˆ›æ–°çš„æƒ³æ³•å¯ä»¥å¸¦æ¥æ•ˆç‡çš„çªç ´ï¼Œå¹¶ä¸”å°å‹è€Œä¸“æ³¨çš„å›¢é˜Ÿä¹Ÿèƒ½æŒ‘æˆ˜è¡Œä¸šå·¨å¤´ï¼Œç”šè‡³å®ç°å…¬å¹³ç«äº‰

We believe DeepSeekâ€™s efficiency breakthrough signals aÂ **coming surge in demand**Â for AI applications. If AI is to continue advancing, we mustÂ **drive down the Total Cost of Ownership (TCO)**â€“by expanding access to alternative hardware, maximizing efficiency on existing systems, and accelerating software innovation. Otherwise, we risk a future where AIâ€™s benefits areÂ **bottlenecked**â€“either byÂ **hardware shortages**Â or by developers struggling to effectively utilize the diverse hardware that is available.
>  æˆ‘ä»¬è®¤ä¸º DeepSeek åœ¨æ•ˆç‡ä¸Šçš„çªç ´é¢„ç¤ºç€å¯¹ AI åº”ç”¨çš„éœ€æ±‚å°†å¤§å¹…å¢é•¿
>  å¦‚æœ AI è¦è¿›æ­¥ï¼Œæˆ‘ä»¬éœ€è¦é™ä½æ€»ä½“æ‹¥æœ‰æˆæœ¬ â€”â€” é€šè¿‡æ‰©å¤§å¯¹æ›¿ä»£ç¡¬ä»¶çš„è®¿é—®ã€æœ€å¤§åŒ–ç°æœ‰ç³»ç»Ÿçš„æ•ˆç‡ï¼ŒåŠ é€Ÿè½¯ä»¶åˆ›æ–°
>  å¦åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥é¢ä¸´æœªæ¥ AI è¢«ç“¶é¢ˆé™åˆ¶ï¼Œè¦ä¹ˆæ˜¯ç¡¬ä»¶çŸ­ç¼ºï¼Œè¦ä¹ˆæ˜¯å¼€å‘è€…éš¾ä»¥åˆ©ç”¨å¤šæ ·åŒ–çš„ç¡¬ä»¶

This isnâ€™t just an abstract problemâ€“it's a challenge Iâ€™ve spent my career working to solve.

## My passion for compute + developer efficiency
I've spent the past 25 years working to unlock computing power for the world. I founded and led the development ofÂ [LLVM](https://en.wikipedia.org/wiki/LLVM), a compiler technology that opened CPUs to new applications of compiler technology. Today, LLVM is the foundation for performance-oriented programming languages like C++, Rust, Swift and more. It powers nearly all iOS and Android apps, as well as the infrastructure behind major internet services from Google and Meta.

This work paved the way for several key innovations I led at Apple, including the creation ofÂ [OpenCL](https://en.wikipedia.org/wiki/OpenCL), an early accelerator framework now widely adopted across the industry, the rebuild of Appleâ€™s CPU and GPU software stack using LLVM, and the development of theÂ [Swift programming language](https://en.wikipedia.org/wiki/Swift_\(programming_language\)). These experiences reinforced my belief in the power of shared infrastructure, the importance of co-designing hardware and software, and how intuitive, developer-friendly tools unlock the full potential of advanced hardware.

## Falling in love with AI
In 2017, I became fascinated by AIâ€™s potential and joined Google to lead software development for the TPU platform. At the time, the hardware was ready, but the software wasnâ€™t functional. Over the next two and a half years, through intense team effort, we launchedÂ [TPUs in Google Cloud](https://cloud.google.com/tpu), scaled them to ExaFLOPS of compute, and built a research platform that enabled breakthroughs likeÂ [_Attention Is All You Need_](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need)Â andÂ [BERT](https://en.wikipedia.org/wiki/BERT_\(language_model\)).

Yet, this journey revealed deeper troubles in AI software. Despite TPUs' success, they remain only semi-compatible with AI frameworks like PyTorchâ€“an issue Google overcomes with vast economic and research resources. A common customer question was,Â **â€œCan TPUs run arbitrary AI models out of the box?â€**Â The hard truth?Â **Noâ€“because we didnâ€™t have CUDA, the de facto standard for AI development.**
>  å°½ç®¡ TPU å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä¸ PyTorch ç­‰ AI æ¡†æ¶çš„å…¼å®¹æ€§ä»ç„¶æœ‰é™ï¼ŒGoogle ç”¨äº†å¤§é‡çš„ç»æµå’Œç ”ç©¶èµ„æºè§£å†³äº†è¿™ä¸€é—®é¢˜
>  ä¸€ä¸ªå¸¸è§çš„å®¢æˆ·é—®é¢˜æ˜¯: TPU èƒ½å¦å¼€ç®±å³ç”¨åœ°è¿è¡Œä»»æ„ AI æ¨¡å‹
>  ç­”æ¡ˆæ˜¯ä¸èƒ½ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ CUDA â€”â€” AI å¼€å‘çš„äº‹å®æ ‡å‡†

Iâ€™m not one to shy away from tackling major industry problems: my recent work has been the creation of next-generation technologies to scale into this new era of hardware and accelerators. This includes the MLIR compiler framework (widely adopted now for AI compilers across the industry) and the Modular team has spent the last 3 years building something specialâ€“but weâ€™ll share more about that later, when the time is right.
>  ç¬”è€…æœ€è¿‘çš„å·¥ä½œå°±æ˜¯å¼€å‘ä¸‹ä¸€ä»£æŠ€æœ¯ï¼Œä»¥é€‚åº”è¿™ä¸ªæ–°çš„ç¡¬ä»¶å’ŒåŠ é€Ÿå™¨æ—¶ä»£

## How do GPUs and next-generation compute move forward?
Because of my background and relationships across the industry, Iâ€™m often asked about the future of compute. Today, countless groups are innovating in hardware (fueled in part by NVIDIAâ€™s soaring market cap), while many software teams are adopting MLIR to enable new architectures. At the same time, senior leaders are questioning whyâ€“despite massive investmentsâ€“the AI software problem remains unsolved. The challenge isnâ€™t a lack of motivation or resources. So why does the industry feel stuck?
>  å¦‚ä»Šï¼Œæ— æ•°å›¢é˜Ÿæ­£åœ¨ç¡¬ä»¶é¢†åŸŸè¿›è¡Œåˆ›æ–°ï¼ŒåŒæ—¶è®¸å¤šè½¯ä»¶å›¢é˜Ÿæ­£åœ¨é‡‡ç”¨ MLIR åœ¨æ”¯æŒæ–°çš„æ¶æ„
>  äºæ­¤åŒæ—¶ï¼Œé¢†å¯¼è€…ä¹Ÿåœ¨è´¨ç–‘ä¸ºä»€ä¹ˆæŠ•å…¥å·¨å¤§ï¼ŒAI è½¯ä»¶é—®é¢˜ä»ç„¶æ²¡æœ‰è§£å†³ï¼Œå¦‚æœé—®é¢˜ä¸æ˜¯ç¼ºä¹èµ„æºå’ŒåŠ¨åŠ›ï¼Œä¸ºä»€ä¹ˆæ•´ä¸ªè¡Œä¸šä¼šé™·å…¥åœæ»

I donâ€™t believe weÂ _are_Â stuck. But we do face difficult, foundational problems.
>  ç¬”è€…ä¸è®¤ä¸ºæˆ‘ä»¬è¢«å›°ä½äº†ï¼Œä½†æˆ‘ä»¬ç¡®å®é¢ä¸´ä¸€äº›å›°éš¾è€ŒåŸºç¡€æ€§çš„é—®é¢˜

To move forward, we need to better understand the underlying industry dynamics. Compute is a deeply technical field, evolving rapidly, and filled with jargon, codenames, and press releases designed to make every new product sound revolutionary. Many people try to cut through the noise toÂ _see the_Â [_forest for the trees_](https://en.wiktionary.org/wiki/see_the_forest_for_the_trees), but to truly understand where weâ€™re going, we need to examine theÂ _roots_â€”the fundamental building blocks that hold everything together.
>  ä¸ºäº†ç†è§£æˆ‘ä»¬å‰è¿›çš„æ–¹å‘ï¼Œæˆ‘ä»¬éœ€è¦å®¡è§†æ ¹æº â€”â€” é‚£äº›æ”¯æ’‘ä¸€åˆ‡çš„åŸºæœ¬æ„å»ºæ¨¡å—

This post is the first in a multipart series where weâ€™ll help answer these critical questions in a straightforward, accessible way:

- ğŸ§ What exactly is CUDA?
- ğŸ¯ Why has CUDA been so successful?
- âš–ï¸ Is CUDA any good?
- â“ Why do other hardware makers struggle to provide comparable AI software?
- âš¡ Why havenâ€™t existing technologies like Triton or OneAPI or OpenCL solved this?
- ğŸš€ How can we as an industry move forward?

>  è¿™ä¸ªç³»åˆ—å°†å›ç­”ä»¥ä¸‹å…³é”®é—®é¢˜:
>  - CUDA æ˜¯ä»€ä¹ˆ
>  - ä¸ºä»€ä¹ˆ CUDA è¿™ä¹ˆæˆåŠŸ
>  - CUDA çœŸçš„å¥½å—
>  - ä¸ºä»€ä¹ˆå…¶ä»–ç¡¬ä»¶å‚å•†éš¾ä»¥æä¾›å¯æ¯”çš„ AI è½¯ä»¶
>  - ä¸ºä»€ä¹ˆç°æœ‰çš„æŠ€æœ¯å¦‚ Triton, OneAPI, OpenCL æœªèƒ½è§£å†³è¿™ä¸ªé—®é¢˜
>  - æˆ‘ä»¬æ•´ä¸ªè¡Œä¸šè¯¥å¦‚ä½•å‘å‰æ¨è¿›

I hope this series sparks meaningful discussions and raises the level of understanding around these complex issues. The rapid advancements in AI â€”like DeepSeekâ€™s recent breakthroughsâ€“remind us that software and algorithmic innovation are still driving forces. A deep understanding of low-level hardware continues to unlock "10x" breakthroughs.
>  AI çš„å¿«é€Ÿè¿›æ­¥ï¼Œæ¯”å¦‚ DeepSeek æœ€è¿‘çš„çªç ´ï¼Œæé†’æˆ‘ä»¬è½¯ä»¶å’Œç®—æ³•çš„åˆ›æ–°è®©ç„¶æ˜¯æ¨åŠ¨å‘å±•çš„æ ¸å¿ƒåŠ›é‡ï¼ŒåŒæ—¶å¯¹åº•å±‚ç¡¬ä»¶çš„æ·±å…¥ç†è§£ä»ç„¶èƒ½å¤Ÿå¸¦æ¥ â€œ10 å€â€ çº§åˆ«çš„çªç ´

AI is advancing at an unprecedented paceâ€“butÂ **thereâ€™s still so much left to unlock**. Together we can break it down, challenge assumptions, and push the industry forward.Â **Letâ€™s dive in!**

-Chris

# 2 What exactly is â€œCUDAâ€? 
Site: https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda
Date: 5 February 2025

It seems likeÂ **everyone**Â has started talking aboutÂ [CUDA](https://en.wikipedia.org/wiki/CUDA)Â in the last year: Itâ€™s theÂ **backbone of deep learning,**Â the reasonÂ **novel hardware struggles to compete,**Â and the core ofÂ **NVIDIAâ€™s moat**Â andÂ **soaring market cap.**Â With DeepSeek, we got a startling revelation: itsÂ **breakthrough was made possible by â€œbypassingâ€ CUDA**,Â [going directly to the PTX layer](https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead) â€¦ but what does this actually mean? It feels like everyone wants to break past the lock-in, but we have toÂ **understand what weâ€™re up against**Â before we can formulate a plan.
>  è¿‡å»ä¸€å¹´é‡Œï¼Œæ‰€æœ‰äººéƒ½å¼€å§‹è®¨è®º CUDA: å®ƒæ˜¯ DL çš„åŸºçŸ³ï¼Œæ˜¯æ–°ç¡¬ä»¶éš¾ä»¥ç«äº‰çš„åŸå› ï¼Œä¹Ÿæ˜¯ NVIDIA æŠ¤åŸæ²³å’Œå¸‚å€¼é£™å‡çš„æ ¸å¿ƒ
>  DeepSeek çš„å¯ç¤ºæ˜¯: å®ƒçš„çªç ´æ˜¯é€šè¿‡ â€œç»•è¿‡â€ CUDA å®ç°çš„ï¼Œç›´æ¥è¿›å…¥ PTX å±‚
>  ä½†è¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿåœ¨åˆ¶å®šè®¡åˆ’ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æˆ‘ä»¬é¢å¯¹çš„æ˜¯ä»€ä¹ˆ

CUDAâ€™s dominance in AI is undeniableâ€”butÂ **most people donâ€™t fully understand what CUDA actually is**. Some think itâ€™s a programming language. Others call it a framework. Many assume itâ€™s justÂ **â€œthat thing NVIDIA uses to make GPUs faster.â€**Â None of these are entirely wrongâ€”and manyÂ [brilliant people are trying to explain this](https://x.com/IanCutress/status/1884374138787357068) â€”but none capture theÂ **full scope of â€œThe CUDA Platform.â€**
>  CUDA åœ¨ AI é¢†åŸŸçš„ä¸»å¯¼åœ°ä½æ— å¯äº‰è®®ï¼Œä½†å¤§å¤šæ•°äººå¹¶ä¸å®Œå…¨ç†è§£ CUDA åˆ°åº•æ˜¯ä»€ä¹ˆ

CUDA is not just one thing. Itâ€™s aÂ **huge, layered Platform**â€”a collection of technologies, software libraries, and low-level optimizations that together form aÂ **massive parallel computing ecosystem**. It includes:

- **A low-level parallel programming model**Â that allows developers to harness the raw power of GPUs with a C++-like syntax.
- **A complex set of libraries and frameworks**â€”middleware that powers crucial vertical use cases like AI (e.g., cuDNN for PyTorch and TensorFlow).
- **A suite of high-level solutions**Â like TensorRT-LLM and Triton, which enable AI workloads (e.g., LLM serving) without requiring deep CUDA expertise.

â€¦and thatâ€™s just scratching the surface.

>  CUDA ä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œè€Œæ˜¯ä¸€ä¸ªåºå¤§ä¸”åˆ†å±‚çš„å¹³å° â€”â€” ç”±ä¸€ç³»åˆ—æŠ€æœ¯ã€è½¯ä»¶åº“å’Œåº•å±‚ä¼˜åŒ–ç»„æˆï¼Œå…±åŒæ„æˆäº†ä¸€ä¸ªåºå¤§çš„å¹¶è¡Œè®¡ç®—ç”Ÿæ€ç³»ç»Ÿï¼Œå®ƒåŒ…æ‹¬:
>  - ä¸€ç§åº•å±‚å¹¶è¡Œç¼–ç¨‹æ¨¡å‹ï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨ç±»ä¼¼ C++ çš„è¯­æ³•ä¸º GPU ç¼–å†™ kernel
>  - ä¸€å¥—å¤æ‚çš„åº“å’Œæ¡†æ¶ â€”â€” å³ä¸­é—´ä»¶ï¼Œç”¨äºæ”¯æŒå…³é”®çš„å‚ç›´åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ç”¨äº PyTorch, TensorFlow çš„ cuDNN
>  - ä¸€ç³»åˆ—é«˜çº§è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ TensorRT-LLM å’Œ Tritonï¼Œä½¿å¾— AI workload å¯ä»¥åœ¨ä¸éœ€è¦æ·±å…¥ CUDA çš„æƒ…å†µä¸‹å®ç°
>  è¿™äº›ä»…ä»…æ˜¯å†°å±±ä¸€è§’

>  ä¸ªäººæŠŠ CUDA çœ‹ä½œä¸€ä¸ªå¯¹ NVIDIA GPU çš„ç¼–ç¨‹æ¥å£ä»¥åŠæ”¯æŒè¿™ä¸ªæ¥å£çš„ä¸€æ•´å¥—å·¥å…·é“¾ï¼Œå¤§è‡´åŒ…æ‹¬:
>  - ç¼–è¯‘å™¨ NVCC å’Œ IR PTX
>  - åº“ cuDNN, cuBLAS, cuFFT
>  - è°ƒè¯•å™¨ cuda-GDB å’Œæ€§èƒ½åˆ†æå™¨ NSight
>  è¿™ä¸ªç¼–ç¨‹æ¥å£å¯¹ç¨‹åºå‘˜æä¾›çš„ç¼–ç¨‹æŠ½è±¡æ˜¯ SIMTï¼Œè¿™ä¸ªç»Ÿä¸€çš„ç¼–ç¨‹æ¨¡å‹å°† GPU å†…éƒ¨çš„ä¸Šåƒä¸ªæ ¸å¿ƒæ•´åˆèµ·æ¥ï¼Œåœ¨ GPU æ¶æ„ä¸Šè¿›è¡Œè®¡ç®—ä»»åŠ¡ï¼Œå› æ­¤ CUDA å«åšè®¡ç®—ç»Ÿä¸€çš„è®¾å¤‡æ¶æ„ (è™½ç„¶å®é™…ä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªè®¾å¤‡æ¶æ„ï¼Œæ˜¯åˆ©ç”¨è®¾å¤‡æ¶æ„çš„å·¥å…·)
>  å½“ç„¶å®é™…ä¸ŠåŸºäº CUDA ä¹‹ä¸Šçš„ä¸œè¥¿ä¹Ÿå±äº CUDA ç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†

In this article, weâ€™ll break down theÂ **key layers of the CUDA Platform**, explore itsÂ **historical evolution**, and explainÂ **why itâ€™s so integral to AI computing today**. This sets the stage for the next part in our series, where weâ€™ll dive intoÂ **why CUDA has been so successful.**Â Hint: it has a lot more to do with market incentives than it does the technology itself.
>  CUDA çš„æˆåŠŸæ›´å¤šå’Œå¸‚åœºæ¿€åŠ±æœ‰å…³ï¼Œè€Œä¸ä»…ä»…æ˜¯æŠ€æœ¯æœ¬èº«

Letâ€™s dive in. ğŸš€

## The Road to CUDA: From Graphics to General-Purpose Compute
Before GPUs became the powerhouses of AI and scientific computing, they wereÂ **graphics processorsâ€”specialized processors for rendering images**. Early GPUsÂ **hardwired**Â image rendering into silicon, meaning that every step of rendering (transformations, lighting, rasterization) was fixed. While efficient for graphics, these chips wereÂ **inflexible**â€”they couldnâ€™t be repurposed for other types of computation.
>  AI ä¹‹å‰ï¼ŒGPU æ˜¯ä¸“ç”¨äºå›¾åƒæ¸²æŸ“çš„å¤„ç†å™¨
>  æ—©æœŸ GPU å°†å›¾åƒæ¸²æŸ“ç¡¬ç¼–ç åˆ°ç¡…èŠ¯ç‰‡ä¸­ï¼Œè¿™æ„å‘³ç€æ¸²æŸ“çš„æ¯ä¸€æ­¥ (å˜æ¢ã€å…‰ç…§ã€å…‰æ …åŒ–) éƒ½æ˜¯å›ºå®šçš„
>  è¿™æ ·çš„å›¾ç‰‡å¤„ç†æ•ˆç‡é«˜ï¼Œä½†èŠ¯ç‰‡ç¼ºä¹çµæ´»æ€§

Everything changed inÂ **2001**Â when NVIDIA introduced theÂ **GeForce3**, the first GPU withÂ **programmable shaders**. This was aÂ **seismic shift**Â in computing:

- ğŸ¨Â **Before:**Â Fixed-function GPUs could only apply pre-defined effects.
- ğŸ–¥ï¸Â **After:**Â Developers couldÂ **write their own shader programs**, unlockingÂ **programmable graphics pipelines**.

This advancement came withÂ **Shader Model 1.0**, allowing developers to writeÂ **small, GPU-executed programs**Â for vertex and pixel processing. NVIDIA sawÂ **where the future was heading:**Â instead of just improving graphics performance, GPUs could becomeÂ **programmable parallel compute engines**.

>  2001 å¹´ NVIDIA æ¨å‡ºäº†é¦–æ¬¾å…·æœ‰å¯ç¼–ç¨‹ç€è‰²å™¨çš„ GPU GeForece3ï¼Œè¿™æ˜¯è®¡ç®—é¢†åŸŸçš„é‡å¤§å˜é©:
>  - ä¹‹å‰: å›ºå®šåŠŸèƒ½çš„ GPU åªèƒ½åº”ç”¨é¢„å®šä¹‰çš„æ•ˆæœ
>  - ä¹‹å: å¼€å‘è€…å¯ä»¥ç¼–å†™è‡ªå·±çš„ç€è‰²å™¨ç¨‹åº
>  è¿™ä¸€è¿›æ­¥å¸¦æ¥äº† Shader Model 1.0ï¼Œå…è®¸å¼€å‘è€…ç¼–å†™å°å‹çš„ã€å¯ä»¥åœ¨ GPU ä¸Šæ‰§è¡Œçš„ç¨‹åºï¼Œç”¨äºé¡¶ç‚¹å’Œåƒç´ å¤„ç†
>  NVIDIA çœ‹åˆ°äº†æœªæ¥çš„å‘å±•æ–¹å‘: é™¤äº†æå‡å›¾å½¢æ€§èƒ½ä¹‹å¤–ï¼ŒGPU å¯ä»¥æˆä¸º**å¯ç¼–ç¨‹çš„å¹¶è¡Œè®¡ç®—å¼•æ“**

At the same time, it didnâ€™t take long for researchers to ask:

> â€œğŸ¤”Â _If GPUs can run small programs for graphics, could we use them for non-graphics tasks?â€_

One of the first serious attempts at this was theÂ [**BrookGPU project**](http://graphics.stanford.edu/projects/brookgpu/)Â at Stanford. Brook introduced a programming model that letÂ **CPUs offload compute tasks to the GPU**â€”a key idea thatÂ [set the stage for CUDA](https://www.nvidia.com/content/GTC/documents/1001_GTC09.pdf).

This move wasÂ **strategic and transformative**. Instead of treating compute as aÂ **side experiment**, NVIDIAÂ **made it a first-class priority**, embedding CUDA deeply intoÂ **its hardware, software, and developer ecosystem**.

>  äºæ­¤åŒæ—¶ï¼Œç ”ç©¶äººå‘˜å¾ˆå¿«å¼€å§‹å¥½å¥‡ GPU æ˜¯å¦å¯ä»¥ç”¨äºéå›¾å½¢ä»»åŠ¡
>  å¯¹è¿™ä¸€é—®é¢˜æœ€æ—©çš„å°è¯•æ˜¯ BrookGPU é¡¹ç›®ï¼ŒBrook å¼•å…¥äº†ä¸€ä¸ªç¼–ç¨‹æ¨¡å‹ï¼Œä½¿å¾— **CPU èƒ½å¤Ÿå°†è®¡ç®—ä»»åŠ¡å¸è½½åˆ° GPU** â€”â€” è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„æ¦‚å¿µï¼Œä¸º CUDA çš„å‡ºç°å¥ å®šäº†åŸºç¡€

## The CUDA Parallel Programming Model
InÂ **2006**, NVIDIA launchedÂ **CUDA (â€Compute Unified Device Architectureâ€)**â€”the firstÂ **general-purpose programming platform for GPUs**. The CUDA programming model is made up of two different things: the â€œCUDA programming languageâ€, and the â€œNVIDIA Driverâ€.
>  NVIDIA åœ¨ 2006 å¹´å‘å¸ƒ CUDA â€”â€” ç¬¬ä¸€ä¸ªé¢å‘ GPU çš„é€šç”¨ç›®çš„ç¼–ç¨‹å¹³å°
>  CUDA ç¼–ç¨‹æ¨¡å‹åŒ…å«ä¸¤ä¸ªä¸œè¥¿: CUDA è¯­è¨€ã€NVIDIA é©±åŠ¨å™¨

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3a98e7decaced44f9e7fd_DC-D01.png)

CUDA is a Layered Stack Requiring Deep Integration from Driver to Kernel

The CUDA language is derived from C++, with enhancements to directly expose low-level features of the GPUâ€”e.g. its ideas of â€œGPU threadsâ€ and memory. A programmer can use this language to define a â€œCUDA Kernelâ€â€”an independent calculation that runs on the GPU. A very simple example is:
>  CUDA è¯­è¨€åŸºäº C++ï¼Œæ·»åŠ äº†èƒ½å¤Ÿæš´éœ² GPU ä½çº§ç‰¹æ€§çš„è¯­æ³• â€”â€” ä¾‹å¦‚ GPU çº¿ç¨‹çš„æ€æƒ³ã€GPU memory çš„æ€æƒ³
>  ç¨‹åºå‘˜ç”¨ CUDA è¯­è¨€å®šä¹‰ CUDA kernel â€”â€” åœ¨ GPU ä¸Šè¿è¡Œçš„ç‹¬ç«‹è®¡ç®—å•å…ƒ

```cpp
__global__ void addVectors(float *a, float *b, float *c, int n) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
```

CUDA kernels allow programmers to define a custom computation that accesses local resources (like memory) and using the GPUs as very fast parallel compute units. This language is translated (â€compiledâ€) down to â€œPTXâ€, which is an assembly language that is the lowest level supported interface to NVIDIA GPUs.
>  CUDA è¯­è¨€ä¸ºå°† GPU ä½œä¸ºéå¸¸å¿«é€Ÿçš„å¹¶è¡Œè®¡ç®—å•å…ƒæä¾›äº†æ¥å£
>  CUDA ç¨‹åºä¼šè¢«ç¼–è¯‘ä¸º PTXï¼Œæ˜¯ NVIDIA GPU æ”¯æŒçš„æœ€ä½çº§åˆ«æ¥å£ (SASS å°±ä¸ç®—æ¥å£äº†ï¼Œå› ä¸ºå®ƒä¸å¯¹å¤–æš´éœ²)

But how does a programÂ **actually execute code on a GPU?**Â Thatâ€™s where theÂ **NVIDIA Driver**Â comes in. It acts as theÂ **bridge**Â between the CPU and the GPU, handling memory allocation, data transfers, and kernel execution. A simple example is:
>  ç¼–è¯‘å¥½çš„ç¨‹åºå¦‚ä½•åœ¨ GPU ä¸Šæ‰§è¡Œå‘¢ï¼Ÿ
>  è¿™æ˜¯ NVIDIA é©±åŠ¨å™¨çš„ä»»åŠ¡ï¼Œå®ƒæ˜¯ CPU å’Œ GPU ä¹‹é—´çš„æ¡¥æ¢ï¼Œå¤„ç†å†…å­˜åˆ†é…ã€æ•°æ®ä¼ è¾“ã€kernel æ‰§è¡Œ (CPU é€šè¿‡ NVIDIA é©±åŠ¨å’Œ GPU äº¤äº’)

>  ä¹Ÿå°±æ˜¯è¯´ï¼Œé€šå¸¸æˆ‘ä»¬ç”¨çš„ CUDA API ä¾‹å¦‚ `cudaMalloc, cudaMemcpy` éƒ½å±äº CUDA Runtime APIï¼Œå®ƒå®é™…ä¸Šæ˜¯ CUDA Driver API ä¸Šçš„ä¸€å±‚å°è£…
>  é©±åŠ¨ç¨‹åºæä¾›çš„å°±æ˜¯ CUDA Driver APIï¼Œé©±åŠ¨æ‰æ˜¯çœŸæ­£ç›´æ¥ä¸ GPU ç¡¬ä»¶é€šä¿¡çš„ä¸»ä½“
>  å½“ç„¶å¿…è¦æƒ…å†µä¸‹ï¼Œç¨‹åºå‘˜æ˜¯å¯ä»¥ç›´æ¥è°ƒç”¨ CUDA Driver API çš„

>  æ— è®ºæ˜¯ CUDA Runtime API è¿˜æ˜¯ CUDA Driver APIï¼Œéƒ½å±äºæºç å±‚æ¬¡ï¼Œæœ€ç»ˆéƒ½ä¼šè¢«ç¼–è¯‘
>  CUDA ä»£ç è¢«ç¼–è¯‘åï¼Œä¼šå…ˆç”Ÿæˆ PTX IRï¼Œæ³¨æ„ PTX æ˜¯ä¸€ä¸ªç‹¬ç«‹äº GPU ç¡¬ä»¶æ¶æ„çš„è¯­è¨€ï¼Œå› æ­¤ CUDA -> PTX åªæ˜¯ NVCC å‰ç«¯ç¼–è¯‘çš„æµç¨‹
>  é©±åŠ¨ç¨‹åºæ¥æ”¶ PTX IRï¼Œå¹¶ä½¿ç”¨å†…ç½®çš„å³æ—¶ç¼–è¯‘å™¨ï¼Œå°† PTX IR è¿›ä¸€æ­¥ç¼–è¯‘ä¸ºç‰¹å®šäº GPU æ¶æ„çš„ SASS æœºå™¨ç 
>  GPU ç¡¬ä»¶æ¥æ”¶åˆ°é©±åŠ¨ç¼–å¥½çš„ SASSï¼Œè¿›è€Œæ‰§è¡Œ

```mermaid
flowchart LR
  A[CUDA Source Program]-- PTX -->B[NVIDIA DRIVER]-- SASS -->C[NVIDIA GPU]
```

```cpp
cudaMalloc(&d_A, size);
cudaMalloc(&d_B, size);
cudaMalloc(&d_C, size);

cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);

int threadsPerBlock = 256;
// Compute the ceiling of N / threadsPerBlock
int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
addVectors<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);

cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
```

Note that all of this is very low levelâ€”full of fiddly details (e.g. pointers and â€œmagic numbersâ€). If you get something wrong, youâ€™re most often informed of this by a difficult to understand crash. Furthermore, CUDA exposes a lot of details that are specific to NVIDIA hardwareâ€”things like the â€œnumber of threads in a warpâ€ (which we won't explore here).

Despite the challenges, these components enabled an entire generation of hardcore programmers to get access to the huge muscle that a GPU can apply to numeric problems. For example, theÂ [**AlexNET**](https://en.wikipedia.org/wiki/AlexNet)Â **ignited modern deep learning in 2012**. It was made possible by custom CUDA kernels for AI operations like convolution, activations, pooling and normalization and the horsepower a GPU can provide.
>  è¿™äº›ç»„ä»¶ä½¿å¾—ç¨‹åºå‘˜å¯ä»¥åˆ©ç”¨ GPU çš„ç®—åŠ›ï¼ŒAlexNet ç‚¹ç‡ƒäº†ç°ä»£æ·±åº¦å­¦ä¹ ï¼Œå®ƒå¾—ç›Šäºä¸º AI è¿ç®—ä¾‹å¦‚å·ç§¯ã€æ± åŒ–ã€è§„èŒƒåŒ–å®šåˆ¶çš„ CUDA kernel ä»¥åŠ GPU æä¾›çš„ç®—åŠ›

While the CUDA language and driver areÂ **what most people typically think of**Â when they hear â€œCUDA,â€ this is far from the whole enchiladaâ€”itâ€™s just theÂ **filling inside**. Over time,Â **the CUDA Platform**Â grew to include much more, and as it did, the meaning of the original acronym fell away from being a useful way to describe CUDA.
>  è™½ç„¶ CUDA è¯­è¨€å’Œé©±åŠ¨ç¨‹åºæ˜¯å¤§å¤šæ•°äººå¬åˆ° â€œCUDAâ€ æ—¶æƒ³åˆ°çš„å†…å®¹ï¼Œä½†è¿™è¿œä¸æ˜¯å…¨éƒ¨
>  éšç€å…¶å‘å±•ï¼ŒCUDA å¹³å°é€æ¸åŒ…å«äº†æ›´å¤šå†…å®¹ï¼Œä¸”åŸå§‹ç¼©å†™ CUDA ä¹Ÿä¸èƒ½å‡†ç¡®æè¿°æ•´ä¸ª CUDA

## High-Level CUDA Libraries: Making GPU Programming More Accessible
The CUDA programming model opened the door toÂ **general-purpose GPU computing and is powerful**, but it brings two challenges:

1. CUDA isÂ **difficult to use**, and even worse...
2. CUDA doesnâ€™t help withÂ **performance portability**

Most kernels written for generation N will â€œkeep workingâ€ on generation N+1, but often the performance is quite badâ€”far from the peak of what N+1 generation can deliver, even though GPUs are all about performance. This makes CUDA aÂ **strong tool for expert engineers**, but aÂ **steep learning curve for most developers.**Â But is also means that significant rewrites are required every time a new generation of GPU comes out (e.g. Blackwell is now emerging).

>  CUDA ç¼–ç¨‹æ¨¡å‹æ‰“å¼€äº†é€šç”¨ç›®çš„ GPU è®¡ç®—çš„å¤§é—¨ï¼Œä½†ä¹Ÿå¸¦æ¥äº†ä¸¤ä¸ªæŒ‘æˆ˜:
>  1. CUDA éš¾å†™
>  2. CUDA ä¸èƒ½å¸®åŠ©å®ç°æ€§èƒ½çš„å¯ç§»æ¤æ€§
>  å¤§å¤šæ•°ä¸ºç¬¬ N ä»£ç¼–å†™çš„ kernel åœ¨ N+1 ä»£ GPU ä¸Šä»ç„¶å¯ä»¥è¿è¡Œï¼Œä½†æ€§èƒ½å¾€å¾€è¿œä½äº N+1 ä»£çš„å³°å€¼æ€§èƒ½ï¼Œè¿™æ„å‘³ç€ï¼Œæ¯å½“æ–°ä¸€ä»£ GPU å‡ºç°æ—¶ï¼Œéƒ½éœ€è¦è¿›è¡Œå¤§é‡çš„é‡å†™å·¥ä½œ

As NVIDIA grew it wanted GPUs to be useful to people who were domain experts in their own problem spaces, but werenâ€™t themselves GPU experts. NVIDIAâ€™s solution to this problem was to start building rich and complicatedÂ **closed-source, high-level libraries**Â that abstract away low-level CUDA details. These include:

- **cuDNN**Â (2014) â€“ Accelerates deep learning (e.g., convolutions, activation functions).
- **cuBLAS**Â â€“ Optimized linear algebra routines.
- **cuFFT**Â â€“ Fast Fourier Transforms (FFT) on GPUs.
- â€¦ andÂ [many others](https://developer.nvidia.com/gpu-accelerated-libraries).

>  ä¸ºäº†è®©éä¸“å®¶ä¹Ÿèƒ½ä½¿ç”¨ CUDA, NVIDIA æ„å»ºäº†ä¸°å¯Œä¸”å¤æ‚çš„é—­æºé«˜çº§åº“ï¼ŒæŠ½è±¡äº†åº•å±‚ CUDA çš„ç»†èŠ‚ï¼Œå…¶ä¸­åŒ…æ‹¬:
>  - cuDNN â€”â€” åŠ é€Ÿæ·±åº¦å­¦ä¹ ï¼Œä¾‹å¦‚å·ç§¯ï¼Œæ¿€æ´»å‡½æ•°
>  - cuBLAS â€”â€” åŠ é€Ÿçº¿æ€§ä»£æ•°è¿ç®—
>  - cuFFTâ€”â€” GPU ä¸Šè¿›è¡Œ FFT

With these libraries, developers couldÂ **tap into CUDAâ€™s power without needing to write custom GPU code**, with NVIDIA taking on the burden ofÂ **rewriting these for every generation of hardware**. This was a big investment from NVIDIA,Â **but it worked**.
>  NVIDIA æ‰¿æ‹…äº†ä¸ºæ¯ä¸€ä»£ç¡¬ä»¶é‡å†™è¿™äº›åº“çš„è´Ÿæ‹…ï¼Œä¸Šå±‚çš„å¼€å‘è€…åˆ™æ— éœ€ç¼–å†™è‡ªå®šä¹‰çš„ GPU ä»£ç å°±èƒ½åˆ©ç”¨ CUDA
>  è¿™å¯¹ NVIDIA æ˜¯ä¸€é¡¹å·¨å¤§çš„æŠ•èµ„ï¼Œä½†æ•ˆæœå¾ˆå¥½

TheÂ **cuDNN library**Â is especially important in this storyâ€”it paved the way for Googleâ€™sÂ **TensorFlow**Â (2015) and Metaâ€™sÂ **PyTorch**Â (2016), enabling deep learning frameworks to take off. While there were earlier AI frameworks, these were the first frameworks to truly scaleâ€”modern AI frameworks now haveÂ **_thousands_**Â of these CUDA kernels and each is very difficult to write. As AI research exploded, NVIDIA aggressively pushed to expand these libraries to cover the important new use-cases.
>  cuDNN åº“åœ¨è¿™ä¸ªæ•…äº‹ä¸­å°¤ä¸ºé‡è¦ï¼Œå®ƒä¸º Google çš„ TensorFlow å’Œ Meta çš„ PyTorch é“ºå¹³äº†é“è·¯ï¼Œä½¿å¾— DL æ¡†æ¶å¯ä»¥è¿…é€Ÿå‘å±•
>  å¦‚ä»Šçš„ DL æ¡†æ¶ä¸­æœ‰æ•°åƒä¸ªè¿™æ ·çš„ CUDA kernel, NVIDIA ä¹Ÿç§¯ææ¨åŠ¨è¿™äº›åº“ï¼Œä»¥è¦†ç›–è¶Šæ¥è¶Šå¤šçš„åœºæ™¯

![Image depicting a layered stack with AI Model Developers at the top, represented by a laptop icon with a sparkle. Below is a cloud labeled PyTorch Ecosystem, resting above a red block labeled PyTorch. Underneath are three more layers: a green block for CUDA Libraries, another green block for CUDA Language, and a blue block at the bottom labeled NVIDIA Driver. The structure highlights the deep dependency chain required to support PyTorch within the CUDA framework.](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a37749c851d98f258c8673_DC-D03.png)

PyTorch on CUDA is Built on Multiple Layers of Dependencies

NVIDIAâ€™s investment into theseÂ **powerful GPU libraries**Â enabled the world to focus on building high-level AI frameworks like PyTorch and developer ecosystems like HuggingFace. Their next step was to make entireÂ **solutions**Â that could be usedÂ **out of the box**â€”without needing to understand the CUDA programming model at all.
>  NVIDIA å¯¹è¿™äº› GPU åº“çš„æŠ•èµ„ä½¿å¾—å…¨çƒå¼€å‘è€…å¯ä»¥ä¸“æ³¨äºæ„å»ºåƒ PyTorch è¿™æ ·çš„é«˜çº§æ¡†æ¶ï¼Œä»¥åŠ HuggingFace è¿™æ ·çš„å¼€å‘ç”Ÿæ€
>  å®ƒä»¬çš„ä¸‹ä¸€æ­¥æ˜¯æ‰“é€ å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆå¼€ç®±å³ç”¨ â€”â€” å®Œå…¨ä¸éœ€è¦ç†è§£ CUDA ç¼–ç¨‹æ¨¡å‹

## Fully vertical solutions to ease the rapid growth of AI and GenAI
The AI boom went far beyond research labsâ€”**AI is now everywhere**. FromÂ **image generation**Â toÂ **chatbots**, fromÂ **scientific discovery**Â toÂ **code assistants**,Â **Generative AI (GenAI) has exploded across industries**, bringing a flood of new applications and developers into the field.

At the same time,Â **a new wave of AI developers emerged, with very different needs.**Â In the early days, deep learning requiredÂ **highly specialized engineers**Â who understood CUDA, HPC, and low-level GPU programming. Now, a new breed of developerâ€”often calledÂ **AI engineers**â€”is building and deploying AI models without needing to touch low-level GPU code.
>  å¦‚ä»Šçš„ AI å·¥ç¨‹å¸ˆåœ¨æ„å»ºå’Œéƒ¨ç½² AI æ¨¡å‹æ—¶ï¼Œä¸éœ€è¦æ¥è§¦åº•å±‚çš„ GPU ä»£ç 

To meet this demand, NVIDIA went beyond just providing librariesâ€”it now offersÂ **turnkey solutions**Â that abstract awayÂ **everything**Â under the hood. Instead of requiringÂ **deep CUDA expertise**, these frameworks allow AI developers toÂ **optimize and deploy models with minimal effort**.

- **Triton Serving**Â â€“ A high-performance serving system for AI models, allowing teams to efficiently run inference across multiple GPUs and CPUs.
- **TensorRT**Â â€“ A deep learning inference optimizer thatÂ **automatically tunes models**Â to run efficiently on NVIDIA hardware.
- **TensorRT-LLM**Â â€“ An even more specialized solution, built forÂ **large language model (LLM) inference at scale**.
- â€¦ plus many (many) other things.

>  è¿™æ˜¯å› ä¸ºï¼ŒNVIDIA ä¸å†ä»…ä»…æä¾›åº“å·¥å…· â€”â€” å®ƒç°åœ¨æä¾›ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œå°†æ‰€æœ‰åº•å±‚ç»†èŠ‚éƒ½æŠ½è±¡æ‰äº†ï¼Œå¼€å‘è€…ä¸å†éœ€è¦ CUDA çŸ¥è¯†ï¼ŒAI å¼€å‘è€…å¯ä»¥ä»¥æœ€å°çš„åŠªåŠ›ä¼˜åŒ–å’Œéƒ¨ç½²æ¨¡å‹
>  - Triton Serving â€”â€” ä¸€ä¸ªé«˜æ€§èƒ½çš„ AI æ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œå…è®¸å›¢é˜Ÿåœ¨å¤šä¸ª GPUs ä¸Šå’Œ CPUs ä¸Šé«˜æ•ˆåœ°è¿è¡Œæ¨ç†
>  - TensorRT â€”â€” æ·±åº¦å­¦ä¹ æ¨ç†ä¼˜åŒ–å™¨ï¼Œè‡ªåŠ¨è°ƒæ•´æ¨¡å‹ä½¿å¾—èƒ½åœ¨ NVIDIA ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œ
>  - TensorRT-LLM â€”â€” ä¸“é—¨ä¸º LLM å¤§è§„æ¨¡æ¨ç†è€Œè®¾è®¡çš„ä¼˜åŒ–å™¨

![Image showing a vertical stack with AI Engineers at the top, represented by a laptop icon with a sparkle. Below are four layers: a green block labeled TensorRT-LLM, followed by CUDA Libraries, then CUDA Language, and finally a blue block at the bottom labeled NVIDIA Driver. The layered structure highlights the multiple dependencies required for AI development within the CUDA ecosystem.](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a376784d69c7a77a206398_DC-D02.png)

Several Layers Exist Between NVIDIA Drivers and TensorRT-LLM

These toolsÂ **completely shield AI engineers from CUDAâ€™s low-level complexity**, letting themÂ **focus on AI models and applications, not hardware details**. These systems provide significant leverage which has enabled the horizontal scale of AI applications.
>  è¿™äº›å·¥å…·å®Œå…¨å±è”½äº† AI å·¥ç¨‹å¸ˆå¯¹ CUDA åº•å±‚å¤æ‚æ€§çš„æ¥è§¦ï¼Œä½¿å¾—ä»–ä»¬å¯ä»¥ä¸“æ³¨äº AI æ¨¡å‹å’Œåº”ç”¨ï¼Œè€Œä¸æ˜¯ç¡¬ä»¶ç»†èŠ‚

## The â€œCUDA Platformâ€ as a whole
CUDA is often thought of as aÂ **programming model**, aÂ **set of libraries**, or even justÂ **"that thing NVIDIA GPUs run AI on."**Â But in reality,Â **CUDA is much more than that**â€”it is aÂ **unifying brand, a truly vast collection of software, and a highly tuned ecosystem**, all deeply integrated with NVIDIAâ€™s hardware. For this reason, the term â€œCUDAâ€ is ambiguousâ€”we prefer the term â€œThe CUDA Platformâ€ to clarify that weâ€™re talking about something closer in spirit to the Java ecosystem, or even an operating system, than merely a programming language and runtime library.
>  CUDA é€šå¸¸è¢«çœ‹ä½œä¸€ä¸ªç¼–ç¨‹æ¨¡å‹ï¼Œä¸€ç»„åº“
>  äº‹å®ä¸Šï¼ŒCUDA æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å“ç‰Œï¼Œä¸€ä¸ªåºå¤§çš„è½¯ä»¶é›†åˆå’Œä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä¸ NVIDIA çš„ç¡¬ä»¶æ·±åº¦é›†æˆ
>  å› æ­¤ â€œCUDA å¹³å°â€ æ›´åŠ å‡†ç¡®ï¼Œå› ä¸ºæˆ‘ä»¬è®¨è®ºçš„ä¸ä»…ä»…æ˜¯ç¼–ç¨‹æ¨¡å‹å’Œè¿è¡Œæ—¶åº“

![Image showing a layered stack of the CUDA ecosystem. At the top are icons for AI GPU Kernel Developers, AI Model Developers, and AI Engineers, with clouds for CUDA Kernels and PyTorch Ecosystem. Below are PyTorch, TensorRT-LLM, CUDA Libraries, CUDA Language, and the foundational NVIDIA Driver, highlighting CUDAâ€™s complex dependencies.](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3781e1402741652bac713_DC-D04.png)

CUDAâ€™s Expanding Complexity: A Multi-Layered Ecosystem Spanning Drivers, Languages, Libraries, and Frameworks

At its core, the CUDA Platform consists of:

- **A massive codebase**Â â€“ Decades of optimized GPU software, spanning everything from matrix operations to AI inference.
- **A vast ecosystem of tools & libraries**Â â€“ FromÂ **cuDNN for deep learning**Â toÂ **TensorRT for inference**, CUDA covers anÂ **enormous range of workloads**.
- **Hardware-tuned performance**Â â€“ Every CUDA release is deeply optimized forÂ **NVIDIAâ€™s latest GPU architectures**, ensuring top-tier efficiency.
- **Proprietary and opaque**Â â€“ When developers interact with CUDAâ€™sÂ **library APIs**, much of what happens under the hood isÂ **closed-source and deeply tied to NVIDIAâ€™s ecosystem**.

>  æœ¬è´¨ä¸Šï¼ŒCUDA å¹³å°åŒ…å«äº†:
>  - åºå¤§çš„ä»£ç åº“ â€”â€” æ•°åå¹´æ¥é’ˆå¯¹ GPU çš„ä¼˜åŒ–è½¯ä»¶ï¼Œæ¶µç›–äº†ä»çŸ©é˜µè¿ç®—åˆ° AI æ¨ç†çš„å„ç§ä»»åŠ¡
>  - ä¸°å¯Œçš„å·¥å…·å’Œåº“ç”Ÿæ€ç³»ç»Ÿ â€”â€” ä»ç”¨äº DL çš„ cuDNN åˆ°ç”¨äºæ¨ç†çš„ TensorRT, CUDA è¦†ç›–äº†å¹¿æ³›çš„ workloads
>  - é’ˆå¯¹ç¡¬ä»¶çš„é«˜æ€§èƒ½ä¼˜åŒ– â€”â€” æ¯æ¬¡ CUDA å‘å¸ƒéƒ½ä¼šæ·±åº¦ä¼˜åŒ–ä»¥é€‚é… NVIDIA çš„æœ€æ–° GPU æ¶æ„ï¼Œç¡®ä¿é¡¶çº§çš„æ•ˆç‡
>  - ä¸“æœ‰ä¸”ä¸é€æ˜ â€”â€” å¼€å‘è€…ä½¿ç”¨ CUDA çš„åº“ API æ—¶ï¼Œå¾ˆå¤šåº•å±‚æ“ä½œæ—¶é—­æºçš„ï¼Œå¹¶ä¸”ä¸ NVIDIA ç”Ÿæ€ç³»ç»Ÿæ·±åº¦ç»‘å®š

CUDA is a powerful but sprawling set of technologiesâ€”**an entire software platform that sits at the foundation of modern GPU computing**, even going beyond AI specifically.
>  CUDA æ˜¯ä¸€ä¸ªå®Œæ•´çš„è½¯ä»¶å¹³å°ï¼Œæ„æˆäº†ç°ä»£ GPU è®¡ç®—çš„åŸºç¡€

Now that we know what â€œCUDAâ€ is, we need to understand how it got to be so successful. Hereâ€™s a hint: CUDAâ€™s success isnâ€™t really aboutÂ **performance**â€”itâ€™s aboutÂ **strategy, ecosystem, and momentum**. In the next post, weâ€™ll explore what enabled NVIDIAâ€™s CUDA software to shape and entrench the modern AI era.
>  CUDA çš„æˆåŠŸå¹¶ä¸çœŸæ­£åœ¨äºæ€§èƒ½ï¼Œè€Œæ˜¯å…³äºç­–ç•¥ã€ç”Ÿæ€å’ŒåŠ¿å¤´

See you next time. ğŸš€

-Chris

# 3 How did CUDA succeed? 
Site: https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed
Date: 12 Feb 2025

If we as an ecosystem hope to make progress, we need to understand howÂ **the CUDA software empire**Â became so dominant. On paper, alternatives existâ€”AMDâ€™s ROCm, Intelâ€™s oneAPI, SYCL-based frameworksâ€”but in practice, CUDA remains theÂ **undisputed king of GPU compute**.
>  CUDA å­˜åœ¨è®¸å¤šæ›¿ä»£æ–¹æ¡ˆ: AMD ROCm, Intel oneAPI, SYCL-based æ¡†æ¶
>  ä½† CUDA ä»ç„¶æ˜¯ GPU è®¡ç®—é¢†åŸŸçš„ç‹è€…

**How did this happen?**

The answer isnâ€™t just aboutÂ **technical excellence**â€”though that plays a role. CUDA is a developer platform built throughÂ **brilliant execution, deep strategic investment, continuity, ecosystem lock-in,**Â and, of course, a littleÂ **bit of luck**.
>  CUDA æ˜¯é€šè¿‡å“è¶Šçš„æ‰§è¡ŒåŠ›ã€æ·±å…¥çš„æˆ˜ç•¥æŠ•èµ„ã€æŒç»­æ€§ã€ç”Ÿæ€ç³»ç»Ÿçš„ç»‘å®šè€Œæ„å»ºå‡ºçš„å¼€å‘è€…å¹³å°

This post breaks downÂ **why CUDA has been so successful**, exploring the layers of NVIDIAâ€™s strategyâ€”from its early bets on generalizing parallel compute to the tight coupling of AI frameworks likeÂ [PyTorch](https://pytorch.org/)Â andÂ [TensorFlow](http://tensorflow.org/). Ultimately, CUDAâ€™s dominance is not just a triumph of software but aÂ **masterclass in long-term platform thinking**.
>  æœ¬æ–‡æ¢è®¨ NVIDIA æˆ˜ç•¥çš„å„ä¸ªå±‚é¢ â€”â€” ä»æ—©æœŸå¯¹å¹¶è¡Œè®¡ç®—é€šç”¨åŒ–çš„æŠ¼æ³¨ï¼Œåˆ°ä¸ PyTorch, TensorFlow çš„ç´§å¯†é›†æˆ
>  CUDA çš„æˆåŠŸæ›´åƒæ˜¯é•¿æœŸå¹³å°æ€ç»´çš„å…¸èŒƒè¯¾

Letâ€™s dive in. ğŸš€

## The Early Growth of CUDA
A key challenge of building a compute platform is attracting developers to learn and invest in it, and it isÂ **hard to gain momentum**Â if you can only target niche hardware. InÂ [a great â€œAcquiredâ€ podcast](https://www.acquired.fm/episodes/jensen-huang), Jensen Huang shares that a key early NVIDIA strategy was to keep their GPUs compatible across generations. This enabled NVIDIA to leverage its install base of already widespreadÂ **gaming GPUs**, which were sold for running DirectX-based PC games. Furthermore, it enabled developers to learn CUDA on low-priced desktop PCs and scale into more powerful hardware that commanded high prices.
>  æ„å»ºè®¡ç®—å¹³å°çš„å…³é”®æŒ‘æˆ˜æ˜¯å¸å¼•å¼€å‘è€…å­¦ä¹ å¹¶æŠ•èµ„ï¼Œå¦‚æœåªèƒ½é’ˆå¯¹å°ä¼—ç¡¬ä»¶ï¼Œå¾ˆéš¾ç§¯ç´¯åŠ¿å¤´
>  NVIDIA æ—©æœŸçš„ä¸€ä¸ªå…³é”®ç­–ç•¥æ˜¯ä¿æŒ GPU åœ¨ä¸åŒä»£é™…ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œä½¿å¾— NVIDIA èƒ½å¤Ÿåˆ©ç”¨å®ƒå·²ç»å¹¿æ³›éƒ¨ç½²çš„æ¸¸æˆ GPU å¸‚åœº
>  æ¸¸æˆ GPU ä¸»è¦ç”¨äºè¿è¡ŒåŸºäº DirectX çš„ PC æ¸¸æˆ

![Chart depicting NVIDIA's earnings segmented by Auto, Data Center, Gaming, OEM & IP, and Professional Visualization](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e611ec063bd8978baf0_DCP3-Diagram01.png)

This might seem obvious now, but at the time it was a bold bet: instead of creating separate product lines optimized for different use-cases (laptops, desktops, IoT, datacenter, etc.), NVIDIA built aÂ **single contiguous GPU product line.**Â This meant accepting trade-offsâ€”such as power or cost inefficienciesâ€”but in return, it created aÂ **unified ecosystem**Â where every developerâ€™s investment in CUDA could scale seamlessly from gaming GPUs to high-performance datacenter accelerators. This strategy is quite analogous to how Apple maintains and drives its iPhone product line forward.
>  åœ¨å½“æ—¶çœ‹æ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªèµŒæ³¨: NVIDIA æ²¡æœ‰ä¸ºä¸“é—¨çš„ä½¿ç”¨åœºæ™¯ (ä¾‹å¦‚ç¬”è®°æœ¬ã€å°å¼æœºã€ç‰©è”ç½‘ã€æ•°æ®ä¸­å¿ƒç­‰) åˆ›é€ æ–°çš„äº§å“çº¿ï¼Œè€Œæ˜¯æ‰“é€ äº†å•ç‹¬ä¸€æ¡è¿ç»­çš„ GPU èŒ¶å“çº¿
>  è¿™æ„å‘³ç€æ¥æ”¶ä¸€äº›æƒè¡¡ â€”â€” æ¯”å¦‚åŠŸè€—æˆ–æˆæœ¬ä¸Šçš„ä½æ•ˆç‡ â€”â€” ä½†å›æŠ¥æ˜¯æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä½¿å¾—æ¯ä¸ªå¼€å‘è€…å¯¹ CUDA çš„æŠ•å…¥éƒ½èƒ½æ— ç¼åœ°ä»æ¸¸æˆ GPU æ‰©å±•åˆ°é«˜æ€§èƒ½æ•°æ®ä¸­å¿ƒåŠ é€Ÿå™¨

The benefits of this approach were twofold:

1. **Lowering Barriers to Entry**Â â€“ Developers could learn CUDA using the GPUs they already had, making it easy to experiment and adopt.
2. **Creating a Network Effect**Â â€“ As more developers started using CUDA, more software and libraries were created, making the platform even more valuable.

>  è¿™ä¸ªæ–¹æ³•çš„å¥½å¤„æœ‰ä¸¤ä¸ª:
>  1. é™ä½å…¥é—¨é—¨æ§›: å¼€å‘è€…ä½¿ç”¨å·²æœ‰çš„ CUDA çŸ¥è¯†ï¼Œä¸éœ€è¦æ–°å­¦ä¹ 
>  2. å½¢æˆç½‘ç»œæ•ˆåº”: éšç€æ›´å¤šå¼€å‘è€…åŠ å…¥ï¼Œæ›´å¤šè½¯ä»¶å’Œåº“è¢«åˆ›é€ ï¼Œå¹³å°æ›´æœ‰ä»·å€¼

This early install base allowed CUDA to grow beyond gaming intoÂ **scientific computing, finance, AI, and high-performance computing (HPC)**. Once CUDA gained traction in these fields, its advantages over alternatives became clear:Â **NVIDIAâ€™s continued investment ensured that CUDA was always at the cutting edge of GPU performance**, while competitors struggled to build a comparable ecosystem.
>  ä¸€æ—¦ CUDA åœ¨å…¶ä»–é¢†åŸŸè·å¾—è®¤å¯ï¼Œå®ƒç›¸å¯¹äºå…¶ä»–æ–¹æ¡ˆçš„ä¼˜åŠ¿å°±æ˜¾è€Œæ˜“è§äº†: NVIDIA æŒç»­çš„æŠ•èµ„ç¡®ä¿äº† CUDA å§‹ç»ˆå¤„äº GPU æ€§èƒ½çš„æœ€å‰æ²¿ï¼Œè€Œç«äº‰å¯¹æ‰‹åˆ™éš¾ä»¥æ„å»ºç±»ä¼¼çš„ç”Ÿæ€ç³»ç»Ÿ

## Catching and Riding the Wave of AI Software
CUDAâ€™s dominance was cemented with theÂ **explosion of deep learning**. In 2012,Â [**AlexNet**](https://en.wikipedia.org/wiki/AlexNet), the neural network thatÂ **kickstarted the modern AI revolution**, was trained using two NVIDIA GeForce GTX 580 GPUs. This breakthrough not only demonstrated thatÂ **GPUs were faster at deep learning**â€”it proved they were essential for AI progress and led toÂ **CUDAâ€™s rapid adoption as the default compute backend**Â for deep learning.
>  AlexNet åœ¨ä¸¤å— NVIDIA GeForce GTX 580 ä¸Šè®­ç»ƒå¾—åˆ°ï¼Œè¿™è¯æ˜äº† GPU åœ¨æ·±åº¦å­¦ä¹ ä¸­è®­ç»ƒæ›´å¿«ï¼Œä½¿å¾— CUDA å¾ˆå¿«æˆä¸ºæ·±åº¦å­¦ä¹ çš„**é»˜è®¤åç«¯**

As deep learning frameworks emergedâ€”most notablyÂ **TensorFlow**Â (Google, 2015) andÂ **PyTorch**Â (Meta, 2016)â€”NVIDIAÂ **seized the opportunity**Â and invested heavily in optimizing itsÂ **High-Level CUDA Libraries**Â to ensure these frameworks ran as efficiently as possible on its hardware. Rather than leavingÂ **AI framework teams**Â to handleÂ **low-level CUDA performance tuning**Â themselves, NVIDIA took on the burden by aggressively refiningÂ **cuDNN**Â andÂ **TensorRT**Â as weÂ [discussed in Part 2](https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda).
>  éšç€æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å…´èµ·ï¼ŒNVIDIA æŠ•èµ„å¹¶æŠ“ä½äº†æœºä¼šï¼Œå¤§åŠ›ä¼˜åŒ–é«˜çº§ CUDA åº“ï¼Œç¡®ä¿è¿™äº›æ¡†æ¶å¯ä»¥å°½å¯èƒ½åœ¨å…¶ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œ
>  NVIDIA æ²¡æœ‰è®© AI æ¡†æ¶å›¢é˜Ÿè‡ªè¡Œå¤„ç†åº•å±‚ CUDA æ€§èƒ½è°ƒä¼˜ï¼Œè€Œæ˜¯**ä¸»åŠ¨æ‰¿æ‹…èµ·è¿™ä¸€è´£ä»»**ï¼Œç§¯ææ”¹è¿› cuDNN å’Œ TensorRT

This move not only madeÂ **PyTorch and TensorFlow significantly faster**Â on NVIDIA GPUsâ€”it also allowed NVIDIA toÂ **tightly integrate its hardware and software**Â (a process known as â€œ[hardware/software co-design](https://towardsdatascience.com/how-to-co-design-software-hardware-architecture-for-ai-ml-in-a-new-era-b296f2842fe2/)â€) because it reduced coordination with Google and Meta. Each major new generation of hardware would come out with aÂ **new version of CUDA**Â thatÂ **exploited the new capabilities**Â of the hardware. The AI community, eager for speed and efficiency, was more than willing toÂ **delegate this responsibility to NVIDIA**â€”which directly led to these frameworks beingÂ **tied to NVIDIA hardware**.
>  è¿™ä¸€ä¸¾åŠ¨ä¸ä»…è®© PyTorch å’Œ TensorFlow åœ¨ NVIDIA GPU ä¸Šæ˜¾è‘—æé€Ÿï¼Œä¹Ÿè®© NVIDIA èƒ½å¤Ÿç´§å¯†åœ°ç»“åˆå…¶è½¯ä»¶å’Œç¡¬ä»¶ï¼Œå› ä¸ºå‡å°‘äº†ä¸ Google å’Œ Meta çš„åè°ƒå·¥ä½œ
>  æ¯ä¸€ä»£æ–°çš„ç¡¬ä»¶å‘å¸ƒæ—¶ï¼Œéƒ½ä¼šé…å¥—æ¨å‡ºä¸€ä¸ªèƒ½å¤Ÿå……åˆ†åˆ©ç”¨æ–°ç¡¬ä»¶åŠŸèƒ½çš„ CUDA
>  AI ç¤¾åŒºæ¸´æœ›é€Ÿåº¦å’Œæ•ˆç‡ï¼Œæ•…æ„¿æ„å°†è¿™ä¸€è´£ä»»äº¤ç»™ NVIDIA â€”â€” è¿™ç›´æ¥å¯¼è‡´äº†è¿™äº›æ¡†æ¶å’Œ NVIDIA ç¡¬ä»¶æ·±åº¦ç»‘å®š

![Circular diagram depicting the inter-relationship of New AI Research Techniques, Expanded CUDA Libraries, and New Hardware Feature](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e730cf4b743d9ef1dd3_DCP3-Diagram02.png)

But why did Google and Meta let this happen? The reality is thatÂ **Google and Meta**Â werenâ€™t singularly focused on building a broad AI hardware ecosystemâ€”they were focused on using AI toÂ **drive revenue, improve their products, and unlock new research**. Their top engineers prioritizedÂ **high-impact internal projects**Â to move internal company metrics. For example, these companiesÂ [**decided to**Â **build**](https://thechipletter.substack.com/p/googles-first-tensor-processing-unit)Â their ownÂ [**proprietary TPU chips**](https://cloud.google.com/transform/ai-specialized-chips-tpu-history-gen-ai) â€”pouring their effort into optimizing for their ownÂ [first-party hardware.](https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/)Â It made sense toÂ **give the reins to NVIDIA**Â for GPUs.
>  ä¸ºä»€ä¹ˆ Google, Meta å…è®¸è¿™ç§æƒ…å†µå‘ç”Ÿå‘¢ï¼Œç°å®æ˜¯ Google å’Œ Meta å¹¶æ²¡æœ‰ä¸“æ³¨äºæ„å»ºä¸€ä¸ªå¹¿æ³›çš„ **AI ç¡¬ä»¶ç”Ÿæ€ç³»ç»Ÿ** â€”â€” ä»–ä»¬å…³æ³¨çš„æ˜¯åˆ©ç”¨ AI æ¥æ¨åŠ¨æ”¶å…¥ã€æå‡äº§å“å’Œè§£é”æ–°ç ”ç©¶
>  å®ƒä»¬çš„é¡¶å°–å·¥ç¨‹å¸ˆä¼˜å…ˆè€ƒè™‘çš„æ˜¯é«˜å½±å“åŠ›çš„å†…éƒ¨é¡¹ç›®ï¼Œä»¥æå‡å…¬å¸å†…éƒ¨çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼Œè¿™äº›å…¬å¸å†³å®šè‡ªå·±å¼€å‘ä¸“æœ‰çš„ TPU èŠ¯ç‰‡ï¼Œå¹¶å°†ç»å†æŠ•å…¥åˆ°ä¼˜åŒ–è‡ªå·±çš„**ç¬¬ä¸€æ–¹ç¡¬ä»¶ä¸Š**
>  è¿™åœ¨å½“æ—¶æ˜¯åˆç†çš„ï¼Œå³è®© NVIDIA è´Ÿè´£ GPU çš„å‘å±•

Makers of alternative hardware faced anÂ **uphill battle**â€”trying toÂ **replicate the vast, ever-expanding NVIDIA CUDA library ecosystem**Â without the same level of consolidated hardware focus. Rival hardware vendors werenâ€™t just strugglingâ€”they wereÂ **trapped in an endless cycle**, always chasing the next AI advancement on NVIDIA hardware. This impacted Google and Metaâ€™sÂ **in-house chip projects**Â as well, which led to numerous projects, including XLA and PyTorch 2. We can dive into these deeper in subsequent articles, butÂ [despite some hopes](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/), we can see today that nothing has enabled hardware innovators to match the capabilities of the CUDA platform.
>  å…¶ä»–ç¡¬ä»¶æ›¿ä»£å‚å•†åˆ™é¢ä¸´è‰°éš¾çš„æŒ‘æˆ˜ â€”â€” è¯•å›¾å¤åˆ¶ NVIDIA é‚£æ ·åºå¤§ä¸”ä¸æ–­æ‹“å±•çš„ CUDA åº“ç”Ÿæ€ç³»ç»Ÿï¼Œå´ç¼ºä¹åŒæ ·é›†ä¸­åŒ–çš„ç¡¬ä»¶æŠ•å…¥
>  ç«äº‰çš„ç¡¬ä»¶å‚å•†ä¸ä»…ä¸¾æ­¥ç»´è‰°ï¼Œè¿˜é™·å…¥äº†æ— å°½çš„å¾ªç¯ï¼Œä¸€ç›´åœ¨è¿½é€ NVIDIA ç¡¬ä»¶çš„ä¸‹ä¸€ä¸ª AI è¿›å±•
>  è¿™ä¹Ÿå½±å“äº† Google å’Œ Meta çš„è‡ªç ”èŠ¯ç‰‡é¡¹ç›®ï¼Œå¯¼è‡´äº†è®¸å¤šé¡¹ç›®ï¼Œä¾‹å¦‚ XLA å’Œ PyTorch 2
>  æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¦‚ä»Šæ²¡æœ‰ä»»ä½•ç¡¬ä»¶å¹³å°åˆ›æ–°è€…èƒ½å¤Ÿè¾¾åˆ° CUDA å¹³å°çš„èƒ½åŠ›

With each generation of its hardware,Â **NVIDIA widened the gap**. Then suddenly, in late 2022, ChatGPT exploded onto the scene, and with it,Â **GenAI and GPU compute went mainstream**.
>  éšç€æ¯ä¸€ä»£ç¡¬ä»¶çš„é€€å‡ºï¼ŒNVIDIA çš„ä¼˜åŠ¿ä¸æ–­æ‰©å¤§ï¼Œç›´åˆ° 2022 å¹´åº•ï¼ŒChatGPT å‡ºä¸–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯ç”Ÿæˆå¼ AI å’Œ GPU è®¡ç®—æ­£å¼è¿›å…¥ä¸»æµ

## Capitalizing on the Generative AI Surge
Almost overnight,Â **demand for AI compute**Â skyrocketedâ€”it became the foundation forÂ **billion-dollar industries**, consumer applications, and competitive corporate strategy.Â **Big tech**Â and venture capital firms pouredÂ [**billions**Â into AI research startups](https://techcrunch.com/2025/01/03/generative-ai-funding-reached-new-heights-in-2024/)Â andÂ [CapEx buildouts](https://www.thestreet.com/investing/nvidia-first-in-line-to-reap-gains-from-massive-big-tech-spending-surge) â€”money that ultimately funneled straight to NVIDIA, the only player capable of meeting theÂ **exploding demand for compute**.
>  ä¸€å¤œä¹‹é—´ï¼Œå¯¹ AI è®¡ç®—çš„éœ€æ±‚é£™å‡ â€”â€” å®ƒæˆä¸ºäº†æ•°åäº¿ç¾å…ƒçš„äº§ä¸šã€æ¶ˆè´¹è€…åº”ç”¨å’Œä¼ä¸šç«äº‰ç­–ç•¥çš„åŸºç¡€
>  ç§‘æŠ€å·¨å¤´å’Œé£é™©æŠ•èµ„å…¬å¸çº·çº·å‘ AI åˆåˆ›å…¬å¸æŠ•å…¥æ•°åäº¿ç¾å…ƒ â€”â€” è¿™äº›èµ„é‡‘æœ€ç»ˆæµå‘ NVIDIAï¼Œå› ä¸ºå®ƒæ˜¯å”¯ä¸€èƒ½å¤Ÿæ»¡è¶³**è®¡ç®—éœ€æ±‚æ¿€å¢**çš„ç©å®¶

As demand for AI compute surged, companies faced a stark reality:Â **training and deploying GenAI models is**Â [**incredibly expensive**](https://epoch.ai/blog/how-much-does-it-cost-to-train-frontier-ai-models). Every efficiency gainâ€”no matter how smallâ€”translated into massive savings at scale. WithÂ **NVIDIAâ€™s hardware already entrenched in data centers**, AI companies faced a serious choice:Â **optimize for CUDA or fall behind**. Almost overnight, the industry pivoted to writingÂ **CUDA-specific code**. The result? AI breakthroughs are no longer driven purely by models and algorithmsâ€”they nowÂ **hinge on the ability to extract every last drop of efficiency**Â fromÂ **CUDA-optimized code**.
>  éšç€è®¡ç®—éœ€æ±‚æ¿€å¢ï¼Œä¼ä¸šé¢ä¸´ä¸€ä¸ªä¸¥å³»çš„ç°å®: è®­ç»ƒå’Œéƒ¨ç½² GenAI éå¸¸æ˜‚è´µï¼Œæ¯ä¸€æ¬¡**æ•ˆç‡çš„æå‡**ï¼Œæ— è®ºå¤šä¹ˆå¾®å°ï¼Œåœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶éƒ½ä¼šå¸¦æ¥å·¨å¤§çš„æˆæœ¬èŠ‚çº¦
>  ç”±äº NVIDIA çš„ç¡¬ä»¶å·²ç»æ·±å…¥äº†æ•°æ®ä¸­å¿ƒï¼ŒAI å…¬å¸ä¸å¾—ä¸é¢ä¸´é€‰æ‹©: ä¼˜åŒ– CUDA æˆ–è½åäºäºº
>  ä¸€å¤œä¹‹é—´ï¼Œæ•´ä¸ªè¡Œä¸šè½¬å‘ç¼–å†™ CUDA ä»£ç ï¼Œç»“æœæ˜¯ï¼ŒAI çªç ´ä¸ä»…ä»…å†ä¾èµ–äºæ¨¡å‹å’Œç®—æ³•ï¼Œå®ƒä»¬ç°åœ¨å–å†³äºä» CUDA ä¼˜åŒ–ä»£ç ä¸­æ¦¨å–æ¯ä¸€ä»½æ•ˆç‡çš„èƒ½åŠ›

![Diagram depicting the architecture of FlashAttention-3, delineated by Stored in HBM vs. Computed in SRAM](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e86663492a6adec57a3_DCP3-Diagram04.png)

TakeÂ [**FlashAttention-3**](https://pytorch.org/blog/flashattention-3/), for example: this cutting-edge optimization slashed theÂ **cost of running transformer models**â€”but it was built exclusively forÂ **Hopper GPUs**, reinforcingÂ **NVIDIAâ€™s lock-in**Â by ensuring theÂ **best performance**Â was only available on its latest hardware.Â **Continuous research innovations**Â followed the same trajectory, for example whenÂ [**DeepSeek went directly to PTX assembly**](https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead), gainingÂ [full control over the hardware](https://medium.com/@amin32846/unlock-warp-level-performance-deepseeks-practical-techniques-for-specialized-gpu-tasks-a6cf0c68a178)Â at theÂ **lowest possible level**. With the newÂ [NVIDIA Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing)Â architecture on the horizon, we can look forward to the industryÂ **rewriting everything from scratch again**.

## The Reinforcing Cycles That Power CUDAâ€™s Grip
This system is accelerating andÂ **self-reinforcing**.Â **Generative AI has become a runaway force**, driving an insatiable demand for compute, andÂ **NVIDIA holds all the cards**. The biggestÂ **install base**Â ensures thatÂ **most AI research**Â happens inÂ **CUDA**, which in turnÂ **drives investment**Â into optimizing NVIDIAâ€™s platform.
>  GenAI æ˜¯ä¸€ä¸ªä¸å¯é˜»æŒ¡çš„åŠ›é‡ï¼Œæ¨åŠ¨ç€å¯¹è®¡ç®—èµ„æºçš„æ— å°½éœ€æ±‚ï¼Œè€Œ NVIDIA æ‹¥æœ‰å…¨éƒ¨ä¸»åŠ¨æƒ
>  æœ€å¤§çš„ç”¨æˆ·åŸºç¡€ç¡®ä¿å¤§å¤šæ•°ç ”ç©¶éƒ½åœ¨ CUDA ä¸Šè¿›è¡Œï¼Œè¿›è€Œæ¨åŠ¨äº†å¯¹ NVIDIA å¹³å°ä¼˜åŒ–çš„æŠ•èµ„

![Expansion of the earlier circular diagram, this time overlaying the inter-relationship of New Hardware Features, Datacenter CapEx Race, CUDA Specific Algorithms, and New AI Research Techniques](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67ab8e997da80c137d438a5e_DCP3-Diagram03.png)

Every new generation of NVIDIA hardware bringsÂ **new features and new efficiencies**, but it also demandsÂ **new software rewrites, new optimizations, and deeper reliance on NVIDIAâ€™s stack**. The future seems inevitable: a world where CUDAâ€™s grip on AI compute only tightens.
>  æ¯ä¸€ä»£æ–°çš„ NVIDIA ç¡¬ä»¶éƒ½å¸¦æ¥æ–°çš„ç‰¹æ€§å’Œæ•ˆç‡ï¼Œä¹Ÿè¦æ±‚äº†æ–°çš„è½¯ä»¶é‡å†™ã€æ–°çš„ä¼˜åŒ–å’Œå¯¹ NVIDIA æ ˆçš„æ›´æ·±åº¦ä¾èµ–
>  æœªæ¥ä¼¼ä¹åªæ˜¯ CUDA åœ¨ AI è®¡ç®—ä¸­çš„å½±å“åŠ›è¶Šæ¥è¶Šå¼ºçš„ä¸–ç•Œ

#### Except CUDAÂ isn't perfect.
The same forces thatÂ **entrench**Â CUDAâ€™s dominance are also becoming a bottleneckâ€”technical challenges, inefficiencies, andÂ **barriers to broader innovation**. Does this dominance actually serve theÂ **AI research community**? Is CUDAÂ **good for developers**, or justÂ **good for NVIDIA**?
>  ä½†è®© CUDA ç¨³å›ºçš„åŠ›é‡ä¹Ÿé€æ¸æˆä¸ºç“¶é¢ˆ: CUDA çš„ä¸»å¯¼åœ°ä½çœŸçš„æœ‰åˆ©äº AI ç ”ç©¶ç¤¾åŒºå—
>  CUDA å¯¹äºå¼€å‘è€…æ˜¯å¥½çš„ï¼Œè¿˜æ˜¯ä»…ä»…å¯¹äº NVIDIA æ˜¯å¥½çš„

Letâ€™s take a step back: We looked atÂ [**what CUDA is**](https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda)Â and why it is so successful, butÂ **is it actually good?**Â Weâ€™ll explore this in Part 4â€”stay tuned and let us know if you find this series useful, or have suggestions/requests! ğŸš€

-Chris

# 4 CUDA is the incumbent, but is it any good? 
Site: https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good
Date: 20 Feb 2025

Answering the question of whether CUDA is â€œgoodâ€ is much trickier than it sounds. Are we talking about its raw performance? Its feature set? Perhaps its broader implications in the world of AI development? Whether CUDA is â€œgoodâ€ depends onÂ **_who you ask_**Â andÂ **_what they need_**. 
>  å…³äº CUDA æ˜¯å¦ â€œå¥½â€ è¿™ä¸ªé—®é¢˜æ¯”å¬èµ·æ¥è¦å¤æ‚å¾—å¤š
>  æˆ‘ä»¬æ˜¯åœ¨å…³å¿ƒå®ƒçš„åŸå§‹æ€§èƒ½ï¼Ÿå®ƒçš„åŠŸèƒ½é›†ï¼Ÿè¿˜æ˜¯å®ƒåœ¨ AI å¼€å‘é¢†åŸŸæ›´å¹¿æ³›çš„å½±å“ï¼Ÿ
>  CUDA æ˜¯å¦ â€œå¥½â€ï¼Œå–å†³äºé—®çš„æ˜¯è°ä»¥åŠå®ƒä»¬éœ€è¦ä»€ä¹ˆ

In this post, weâ€™ll evaluate CUDA from the perspective of the people who use it day-in and day-outâ€”those who work in the GenAI ecosystem:

1. ForÂ **AI engineers who build on top of CUDA**, itâ€™s an essential tool, but one that comes with versioning headaches, opaque driver behavior, and deep platform dependence.
2. For AI engineersÂ **who write GPU code for NVIDIA hardware**, CUDA offers powerful optimization but only by accepting the pain necessary to achieve top performance.
3. For those who want theirÂ **AI workloads to run on GPUâ€™s from multiple vendors**, CUDA is more an obstacle than a solution.
4. Then thereâ€™sÂ **NVIDIA itself**â€”the company that has built its fortune around CUDA, driving massive profits and reinforcing their dominance over AI compute.

>  åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»æ¯å¤©ä½¿ç”¨ CUDA çš„äººçš„è§’åº¦ â€”â€” é‚£äº›åœ¨ GenAI ç”Ÿæ€ç³»ç»Ÿä¸­å·¥ä½œçš„äºº â€”â€” è¯„ä¼° CUDA 
>  1. å¯¹äºåœ¨ CUDA ä¹‹ä¸Šæ„å»º AI çš„å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æ˜¯å¿…è¦çš„å·¥å…·ï¼Œä½†ä¹Ÿä¼´éšç€ç‰ˆæœ¬ç®¡ç†çš„éº»çƒ¦ã€é©±åŠ¨è¡Œä¸ºçš„ä¸é€æ˜æ€§ï¼Œä»¥åŠå¯¹å¹³å°çš„æ·±åº¦ä¾èµ–
>  2. å¯¹äºä¸º NVIDIA ç¡¬ä»¶ç¼–å†™ CUDA çš„ AI å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æä¾›äº†å¼ºå¤§çš„ä¼˜åŒ–èƒ½åŠ›
>  3. å¯¹äºå¸Œæœ› AI workloads åœ¨å¤šä¸ªä¾›åº”å•†çš„ GPU ä¸Šè¿è¡Œçš„å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æ›´åƒæ˜¯ä¸€ä¸ªéšœç¢è€Œä¸æ˜¯è§£å†³æ–¹æ¡ˆ
>  4. å¯¹äº NVIDIA æœ¬èº«ï¼ŒCUDA å¸¦æ¥äº†å·¨é¢åˆ©æ¶¦ï¼Œå¹¶ä¸”å·©å›ºäº†å®ƒåœ¨ AI è®¡ç®—é¢†åŸŸçš„ä¸»å¯¼åœ°ä½

So, is CUDA â€œgood?â€ Letâ€™s dive into each perspective to find out! ğŸ¤¿

## AI Engineers
Many engineers today are building applications on top ofÂ **AI frameworks**â€”agentic libraries likeÂ [LlamaIndex](https://www.llamaindex.ai/),Â [LangChain](https://www.langchain.com/), andÂ [AutoGen](https://github.com/microsoft/autogen?tab=readme-ov-file) â€”without needing to dive deep into the underlying hardware details. For these engineers, CUDA is aÂ **powerful ally**. Its maturity and dominance in the industry bring significant advantages: most AI libraries are designed to work seamlessly with NVIDIA hardware, and the collective focus on a single platform fosters industry-wide collaboration.
>  å¤§å¤šæ•°å·¥ç¨‹å¸ˆå¦‚ä»Šéƒ½åœ¨ AI æ¡†æ¶ï¼Œä¾‹å¦‚ä»£ç†åº“å¦‚ LlamaIndex, LangChain, AutoGenï¼Œä¸Šæ„å»ºåº”ç”¨ï¼Œä¸éœ€è¦æ·±åº¦ç¡¬ä»¶ç»†èŠ‚
>  å¯¹äºè¿™äº›å·¥ç¨‹å¸ˆæ¥è¯´ï¼ŒCUDA æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç›Ÿå‹: å¤§å¤šæ•° AI åº“éƒ½è®¾è®¡ä¸ NVIDIA ç¡¬ä»¶æ— ç¼é…åˆï¼Œå¯¹å•ä¸ªå¹³å°çš„é›†ä½“å…³æ³¨ä¹Ÿä¿ƒè¿›äº†æ•´ä¸ªè¡Œä¸šçš„åä½œ

However, CUDAâ€™s dominance comes with its ownÂ **set of persistent challenges**. One of the biggest hurdles is the complexity of managing different CUDA versions, which can be a nightmare. This frustration is the subject of numerous memes:
>  ä½† CUDA çš„ä¸»å¯¼åœ°ä½ä¹Ÿä¼´éšç€ä¸€ç³»åˆ—æŒ‘æˆ˜ï¼Œå…¶ä¸­æœ€å¤§çš„éšœç¢ä¹‹ä¸€å°±æ˜¯ç®¡ç†ä¸åŒ CUDA ç‰ˆæœ¬çš„å¤æ‚æ€§

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b63359eb680c24fd16370c_havent-related-to-a-meme-this-hard-in-a-minute-v0-8il1imrhpnde1.webp)

Credit:Â [x.com/ordax](https://x.com/ordax/status/1876624589993976295)

This isnâ€™tÂ _just_Â a memeâ€”itâ€™s a real, lived experience for many engineers. These AI practitioners constantly need to ensure compatibility between the CUDA toolkit, NVIDIA drivers, and AI frameworks. Mismatches can cause frustrating build failures or runtime errors, as countless developers have experienced firsthand:
>  AI ä»ä¸šè€…éœ€è¦ä¸æ–­ç¡®ä¿ CUDA å·¥å…·åŒ…ã€NVIDIA é©±åŠ¨ã€AI æ¡†æ¶ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œç‰ˆæœ¬ä¸åŒ¹é…å°±ä¼šå¯¼è‡´æ„å»ºå¤±è´¥

> "I failed to build the system with the latest NVIDIA PyTorch docker image. The reason is PyTorch installed by pip is built with CUDA 11.7 while the container uses CUDA 12.1." ([github.com](https://github.com/vllm-project/vllm/issues/129?utm_source=chatgpt.com))

or:

> "Navigating Nvidia GPU drivers and CUDA development software can be challenging. Upgrading CUDA versions or updating the Linux system may lead to issues such as GPU driver corruption." ([dev.to](https://dev.to/moseo/solving-the-version-conflicts-between-the-nvidia-driver-and-cuda-toolkit-2n2?utm_source=chatgpt.com))

Sadly, such headaches are not uncommon. Fixing them often requires deep expertise and time-consuming troubleshooting. NVIDIA's reliance on opaque tools and convoluted setup processes deters newcomers and slows down innovation.
>  å¤„ç†è¿™äº›é—®é¢˜é€šå¸¸éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œæ’æŸ¥å·¥ä½œ
>  NVIDIA å¯¹ä¸é€æ˜å·¥å…·å’Œå¤æ‚è®¾ç½®æµç¨‹çš„ä¾èµ–è®©æ–°æ‰‹éš¾ä»¥ç†Ÿç»ƒ

In response to these challenges, NVIDIA has historically moved up the stack to solve individual point-solutions rather than fixing the fundamental problem: the CUDA layer itself. For example, it recently introducedÂ **NIM**Â (NVIDIA Inference Microservices), a suite of containerized microservices aimed at simplifying AI model deployment. While this might streamline one use-case, NIM also abstracts away underlying operations, increasing lock-in and limiting access to the low-level optimization and innovation key to CUDA's value proposition.
>  ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒNVIDIA å€¾å‘äºåœ¨æŠ€æœ¯æ ˆçš„ä¸Šå±‚æä¾›ç‚¹å¯¹ç‚¹çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯ä»æ ¹æœ¬ä¸Šä¿®å¤ CUDA æœ¬èº«çš„é—®é¢˜
>  NVIDIA æœ€è¿‘æå‡ºäº† NVIDIA æ¨ç†å¾®æœåŠ¡ï¼Œä¸€ä¸ªå®¹å™¨åŒ–çš„å¾®æœåŠ¡å¥—ä»¶ï¼Œä»¥ç®€åŒ– AI æ¨¡å‹çš„éƒ¨ç½²

While AI engineers building on top of CUDA face challenges with compatibility and deployment, those working closer to the metalâ€”**AI model developers and performance engineers**â€”grapple with an entirely different set of trade-offs.
>  åŸºäº CUDA æ„å»ºçš„ AI å·¥ç¨‹å¸ˆåœ¨å…¼å®¹æ€§å’Œéƒ¨ç½²æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒAI æ¨¡å‹å¼€å‘è€…å’Œæ€§èƒ½å·¥ç¨‹å¸ˆåˆ™éœ€è¦é¢å¯¹ä¸åŒçš„æƒè¡¡

## AI Model Developers and Performance Engineers
For researchers and engineers pushing the limits of AI models, CUDA is simultaneously an essential tool and aÂ **frustrating limitation**. For them, CUDA isnâ€™t an API; itâ€™s the foundation for every performance-critical operation they write. These are engineers working at the lowest levels of optimization, writing custom CUDA kernels, tuning memory access patterns, and squeezing every last bit of performance from NVIDIA hardware. The scale and cost of GenAI demand it. But does CUDA empower them, or does it limit their ability to innovate?
>  å¯¹äºæ¨åŠ¨ AI æé™çš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆï¼ŒCUDA æ˜¯å¿…é¡»çš„å·¥å…·ï¼Œä¹Ÿæ˜¯é™åˆ¶
>  ä»–ä»¬äº†è§£ CUDAï¼Œä½¿ç”¨ CUDA ç¼–å†™é«˜æ€§èƒ½ kernelï¼Œè°ƒä¼˜å†…å­˜è®¿é—®æ¨¡å¼ï¼Œæ¦¨å– NVIDIA ç¡¬ä»¶çš„æ€§èƒ½

Despite its dominance, CUDA isÂ **showing its age**. It was designed inÂ **2007**, long before deep learningâ€”let alone GenAI. Since then, GPUs have evolved dramatically, withÂ **Tensor Cores**Â and sparsity features becoming central to AI acceleration. CUDAâ€™s early contribution was to make GPU programming easy, butÂ **it hasnâ€™t evolved with modern GPU features necessary for transformers and GenAI performance.**Â This forces engineers toÂ **work around its limitations**Â just to get the performance their workloads demand.
>  CUDA æœ€åˆè®¾è®¡ä¸ 2007 å¹´ï¼Œæ—©äºæ·±åº¦å­¦ä¹ ä»¥åŠ GenAI
>  ä¹‹åï¼ŒGPU ç»å†äº†å·¨å¤§çš„å‘å±•ï¼ŒTensor core å’Œç¨€ç–ç‰¹æ€§å·²ç»ç§°ä¸º AI åŠ é€Ÿçš„æ ¸å¿ƒï¼ŒCUDA çš„æ—©æœŸè´¡çŒ®è®© GPU ç¼–ç¨‹å®¹æ˜“ï¼Œä½†æ²¡æœ‰éšç€é’ˆå¯¹ transformers å’Œ GenAI æ€§èƒ½æ‰€éœ€çš„ç°ä»£ GPU ç‰¹æ€§å‘å±•
>  è¿™è¿«ä½¿å·¥ç¨‹å¸ˆç»•è¿‡ä»–ä»¬çš„é™åˆ¶ï¼Œæ‰èƒ½æ»¡è¶³å…¶ worload å¯¹æ€§èƒ½çš„éœ€æ±‚

###### **CUDA doesnâ€™t do everything modern GPUs can do**
Cutting-edge techniques likeÂ [**FlashAttention-3**](https://pytorch.org/blog/flashattention-3/)Â ([example code](https://github.com/Dao-AILab/flash-attention/blob/a09abcd32d3cae4d83b313446e887f38d02b799f/hopper/copy_sm90_bulk_reduce.hpp#L22)) andÂ [**DeepSeek**](https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai)**â€™s**Â innovations require developers to drop below CUDA intoÂ **PTX**â€”NVIDIAâ€™s lower-level assembly language. PTX is only partially documented, constantly shifting between hardware generations, and effectively a black box for developers.
>  æœ€è¿‘çš„åˆ›æ–°è¦æ±‚å¼€å‘è€…æ·±å…¥ PTX å±‚ï¼ŒPTX æ–‡æ¡£ä¸å…¨ï¼Œä¸”åœ¨ä¸åŒç¡¬ä»¶ä»£é™…ä¹‹é—´é¢‘ç¹å˜åŒ–

More problematic,Â **PTX is even more locked to NVIDIA than CUDA**, and its usability is even worse. However, for teams chasing cutting-edge performance,Â **thereâ€™s no alternative**â€”theyâ€™re forced toÂ **bypass CUDA**Â and endure significant pain.
>  å¹¶ä¸”ï¼ŒPTX å¯¹äº NVIDIA å¹³å°ä¾èµ–æ€§æ›´é«˜

###### **Tensor Cores: Required for performance, but hidden behind black magic**
Today, the bulk of an AI modelâ€™s FLOPs come from â€œ[**Tensor Cores**](https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/)**â€**, not traditional CUDA cores. However, programming Tensor Cores directly is no small feat. While NVIDIA provides some abstractions (like cuBLAS and CUTLASS), getting the most out of GPUs still requiresÂ **arcane knowledge**, trial-and-error testing, and often,Â [reverse engineering undocumented behavior](https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead). Â With each new GPU generation, Tensor Cores change, yet theÂ [**documentation is dated**](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)**.**Â This leaves engineers with limited resources to fully unlock the hardwareâ€™s potential.
>  å¦‚ä»Šå¤§å¤šæ•° AI æ¨¡å‹é€šè¿‡ Tensor cores è®¡ç®—ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ CUDA cores
>  ä½†ç›´æ¥ç¼–ç¨‹ Tensor cores ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹ï¼ŒNVIDIA æä¾›äº†æŠ½è±¡ä¾‹å¦‚ cuBLAS, CUTLASSï¼Œä½†è¦å……åˆ†å‘æŒ¥ GPU æ€§èƒ½ä»ç„¶éœ€è¦æ›´å¤šçŸ¥è¯†ï¼Œæœ‰æ—¶ç”šè‡³éœ€è¦é€†å‘å·¥ç¨‹æœªå…¬å¼€çš„è¡Œä¸º
>  éšç€æ¯ä¸€ä»£ GPU æ¨å‡ºï¼ŒTensor cores ä¹Ÿåœ¨ä¸æ–­å˜åŒ–ï¼Œä½†æ–‡æ¡£å¾€å¾€æ»åï¼Œè¿™ä½¿å¾—å·¥ç¨‹å¸ˆä¸ä¾¿äºå……åˆ†åˆ©ç”¨ç¡¬ä»¶æ½œåŠ›

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b63502b1d4eb5730861f5a_Turing-Tensor-Core-New-Diag-White-Background.jpg)

Credit:Â NVIDIA

###### **AI is Python, but CUDA is C++**
Another major limitation is that writingÂ **CUDA**Â [**fundamentally requires using C++**](https://docs.nvidia.com/cuda/cuda-c-programming-guide/), while modern AI development is overwhelmingly done inÂ **Python**. Engineers working on AI models and performance in PyTorch donâ€™t want to switch back and forth between Python and C++â€”the two languages haveÂ **very different mindsets**. This mismatchÂ **slows down iteration**, creates unnecessary friction, and forces AI engineers to think about low-level performance details when they should be focusing on model improvements. Additionally, CUDA's reliance onÂ [**C++ templates**](https://github.com/NVIDIA/cutlass)Â leads toÂ [painfully slow compile times](https://developer.nvidia.com/blog/reducing-application-build-times-using-cuda-c-compilation-aids/)Â and often incomprehensible error messages.
>  å¦ä¸€ä¸ªé™åˆ¶æ˜¯è¯­è¨€
>  AI å·¥ç¨‹å¸ˆè¦æ±‚æ€§èƒ½ï¼Œå°±éœ€è¦ä½¿ç”¨ C++ï¼Œè€Œ C++ å’Œ Python çš„ç¼–ç¨‹æ€æƒ³å®Œå…¨ä¸åŒ
>  æ­¤å¤–ï¼ŒCUDA ä¾èµ–äº C++æ¨¡æ¿ï¼Œä¼šéªŒè¯é™ä½ç¼–è¯‘æ—¶é—´ï¼Œå¹¶ä¸”é”™è¯¯ä¿¡æ¯ä¸å¥½è¯»

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b6355eb56de6aa5924290c_compiling.png)

Credit:Â [XKCD](https://xkcd.com/303/)

These are the challenges you face if youâ€™re happy to develop specifically for NVIDIA hardware. ButÂ **what if you care about more than just NVIDIA?**
>  è¿™äº›æ˜¯æˆ‘ä»¬å¯¹ NVIDIA ç¡¬ä»¶å¼€å‘éœ€è¦é¢å¯¹çš„å›°éš¾ï¼Œé‚£ä¹ˆå¯¹å…¶ä»–ç¡¬ä»¶å¼€å‘çš„å›°éš¾å‘¢ï¼Ÿ

## Engineers and Researchers Building Portable Software
Not everyone is happy to build software locked to NVIDIAâ€™s hardware, and the challenges are clear. CUDAÂ **doesnâ€™t run on hardware from other vendors**Â (like theÂ [supercomputer in our pockets](https://www.visualcapitalist.com/the-supercomputer-in-your-pocket/)), and no alternatives provide the full performance and capabilities CUDA provides on NVIDIA hardware. This forces developers to write their AI code multiple times, for multiple platforms.
>  CUDA æ— æ³•åœ¨å…¶ä»–ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œå¹¶ä¸”ç›®å‰ä¹Ÿæ²¡æœ‰æ›¿ä»£æ–¹æ¡ˆèƒ½åœ¨å…¶ä»–ç¡¬ä»¶ä¸Šæä¾› NVIDIA + CUDA çš„æ€§èƒ½ï¼Œè¿™éœ€è¦å¼€å‘è€…ä¸ºä¸åŒçš„å¹³å°å¤šæ¬¡ç¼–å†™ AI ä»£ç 

In practice, many cross-platform AI efforts struggle. Early versions of TensorFlow and PyTorch had OpenCL backends, but they lagged far behind the CUDA backend in both features and speed, leading most users to stick with NVIDIA. Maintaining multiple code pathsâ€”CUDA for NVIDIA, something else for other platformsâ€”is costly, and as AI rapidly progresses, only large organizations have resources for such efforts.
>  å®é™…ä¸Šï¼Œè®¸å¤šè·¨å¹³å° AI é¡¹ç›®éƒ½é¢ä¸´å›°éš¾ï¼Œæ—©æœŸç‰ˆæœ¬çš„ TensorFlow å’Œ PyTorch æœ‰ OpenCL åç«¯ï¼Œä½†ä»–ä»¬åœ¨åŠŸèƒ½å’Œé€Ÿåº¦ä¸Šè¿œè¿œè½åäº CUDA åç«¯ï¼Œå¯¼è‡´å¤§å¤šæ•°ç”¨æˆ·ä»ç„¶é€‰æ‹© CUDA
>  åŒæ—¶ç»´æŠ¤é’ˆå¯¹å¤šæ¡çš„ä»£ç è·¯å¾„è¿‡äºæ˜‚è´µï¼Œåªæœ‰å¤§å‹ç»„ç»‡æ‰æœ‰è¿™æ ·çš„èµ„æº

The bifurcation CUDA causes creates aÂ **self-reinforcing cycle**: since NVIDIA has the largest user base and the most powerful hardware, most developers target CUDA first, and hope that others will eventually catch up. This further solidifies CUDAâ€™s dominance as the default platform for AI.
>  è¿™æ ·çš„åˆ†åŒ–ä½¿å¾— CUDA æ‹¥æœ‰ä¸€ä¸ªè‡ªæˆ‘å¼ºåŒ–çš„å¾ªç¯: å¼€å‘è€…é¦–å…ˆé’ˆå¯¹ CUDA è¿›è¡Œå¼€å‘ï¼Œå…¶ä»–çš„å‚å•†åªèƒ½è‡ªè¡Œè·Ÿä¸Š

ğŸ‘‰ Weâ€™ll explore alternatives like OpenCL, TritonLang, and MLIR compilers in our next post, and come to understand why these options havenâ€™t made a dent in CUDA's dominance.
>  æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« è®¨è®ºæ›¿ä»£æ–¹æ¡ˆï¼Œä¾‹å¦‚ OpenCL, TritonLang, MLIRï¼Œå¹¶ç†è§£ä¸ºä»€ä¹ˆè¿™äº›é€‰é¡¹è‡³ä»Šæœªèƒ½æ’¼åŠ¨ CUDA çš„åœ°ä½

## Is CUDA Good for NVIDIA Itself?
Of course, theÂ **answer is yes:**Â the â€œCUDA moatâ€ enables aÂ **winner-takes-most**Â scenario. By 2023, NVIDIA heldÂ [**~98% of the data-center GPU market share**](https://www.datacenterdynamics.com/en/news/nvidia-gpu-shipments-totaled-376m-in-2023-equating-to-a-98-market-share-report/#:~:text=As%20reported%20by%20HPCwire%2C%20the,company%20in%20the%20year%20prior), cementing its dominance in the AI space. As we've discussed inÂ [previous posts](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed), CUDA serves as theÂ **bridge between NVIDIAâ€™s past and future products**, driving the adoption of new architectures like Blackwell and maintaining NVIDIA's leadership in AI compute.
>  2023 å¹´ï¼ŒNVIDIA æ‹¥æœ‰æ•°æ®ä¸­å¿ƒ GPU 98% çš„å¸‚åœºä»½é¢
>  CUDA æ˜¯è¿æ¥ NVIDIA è¿‡å»å’Œæœªæ¥äº§å“çš„æ¡¥æ¢

However,Â **legendary hardware experts**Â likeÂ [Jim Keller](https://en.wikipedia.org/wiki/Jim_Keller_\(engineer\))Â argue that "[**CUDAâ€™s a swamp, not a moat**](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too),â€ making analogies to the X86 architecture that bogged Intel down.
>  ä½†ä¸“å®¶è®¤ä¸º CUDA æ˜¯æ²¼æ³½ï¼Œä¸æ˜¯æŠ¤åŸæ²³ï¼Œå°±åƒ X86 æ›¾ç»è®© Intel é™·å…¥å›°å¢ƒ

![](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67b65da3cd06c4c299e576af__c73e3185-8a45-4a4f-9d79-62d789cc7fe3.jpeg)

"[CUDA's a swamp, not a moat](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too)," arguesÂ [Jim Keller](https://en.wikipedia.org/wiki/Jim_Keller_\(engineer\))

How could CUDA be a problem for NVIDIA? There are several challenges.

###### **CUDA's usability impacts NVIDIA the most**
Jensen Huang famously claims that NVIDIA employsÂ [more software engineers than hardware engineers](https://www.wsj.com/tech/ai/ai-nvidia-apple-amd-jensen-huang-software-bb581f5a), with a significant portion dedicated to writing CUDA. But theÂ **usability and scalability**Â challenges within CUDA slow down innovation, forcing NVIDIA to aggressively hire engineers to fire-fight these issues.
>  CUDA å†…ç”Ÿçš„å¯ç”¨æ€§å’Œå¯æ‹“å±•æ€§é˜»ç¢äº†åˆ›æ–°ï¼Œè¿«ä½¿ NVIDIA ä¸å¾—ä¸å¤§é‡æ‹›è˜å·¥ç¨‹å¸ˆè§£å†³è¿™äº›é—®é¢˜

###### **CUDAâ€™s heft slows new hardware rollout**
CUDA doesnâ€™t provideÂ **performance portability**Â across NVIDIAâ€™s own hardware generations, and the sheer scale of its libraries is a double-edged sword. When launching a new GPU generation like Blackwell, NVIDIA faces a choice: rewrite CUDA or release hardware that doesnâ€™t fully unleash the new architectureâ€™s performance. This explains whyÂ [performance is suboptimal at launch](https://www.forbes.com/sites/karlfreund/2023/09/08/nvidia-adds-new-software-that-can-double-h100-inference-performance/)Â of each new generation. SuchÂ **expansion**Â of CUDAâ€™s surface area is costly and time-consuming.
>  CUDA ä¸èƒ½åœ¨ NVIDIA è‡ªå·±çš„ç¡¬ä»¶ä»£é™…ä¸Šæä¾›**æ€§èƒ½å¯ç§»æ¤æ€§**ï¼Œå…¶åºå¤§çš„åº“è§„æ¨¡ä¹Ÿæ˜¯åŒåˆƒå‰‘
>  å½“æ¨å‡ºæ–°çš„æ¶æ„æ—¶ï¼ŒNVIDIA é¢ä¸´ä¸€ä¸ªé€‰æ‹©ï¼Œè¦ä¹ˆé‡å†™ CUDAï¼Œè¦ä¹ˆå‘å¸ƒæ— æ³•å‘æŒ¥æ–°æ¶æ„æ€§èƒ½çš„ç¡¬ä»¶
>  è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ¯ä¸€ä»£æ–° GPU åœ¨åˆšå‘å¸ƒæ—¶æ€§èƒ½éƒ½æ¬ ä½³

###### **The Innovatorâ€™s Dilemma**
NVIDIAâ€™s commitment to backward compatibilityâ€”one of CUDAâ€™s early selling pointsâ€”has now become â€œ**technical debtâ€**Â that hinders their own ability to innovate rapidly. While maintaining support for older generations of GPUs is essential for their developer base, itÂ **forces NVIDIA to prioritize stability over revolutionary changes**. This long-term support costs time, resources, and could limit their flexibility moving forward.
>  NVIDIA å¯¹å‘åå…¼å®¹æ€§çš„æ‰¿è¯ºå¦‚ä»Šå·²ç»æˆä¸ºäº†æŠ€æœ¯å€ºåŠ¡ï¼Œé˜»ç¢äº†è‡ªèº«çš„åˆ›æ–°
>  ç»´æŒå¯¹æ—§ç‰ˆ GPU çš„æ”¯æŒè¿«ä½¿ NVIDIA æ›´åŠ é‡è§†ç¨³å®šæ€§ï¼Œè€Œéé©å‘½æ€§çš„å˜åŒ–

Though NVIDIA has promised developers continuity, Blackwell couldn't achieve its performance goals withoutÂ [breaking compatibility with Hopper PTX](https://docs.nvidia.com/cuda/blackwell-compatibility-guide/#application-compatibility-on-blackwell-architecture) â€”now someÂ [Hopper PTX operations](https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-multiply-and-accumulate-instruction-wgmma-mma-async)Â donâ€™t work on Blackwell. This means advanced developers who have bypassed CUDA in favor of PTX may find themselves rewriting their code for the next-generation hardware.
>  Blackwell æ¶æ„å·²ç»ä¸å¾—ä¸æ‰“ç ´äº†ä¸ Hopper PTX çš„å…¼å®¹æ€§ â€”â€” ç°åœ¨**ä¸€äº› Hopper PTX æ“ä½œæ— æ³•åœ¨ Blackwell ä¸Šæ— æ³•æ­£å¸¸è¿è¡Œ** (ç ´åäº†å‘åå…¼å®¹)

Despite these challenges,Â **NVIDIAâ€™s strong execution in software**Â and its early strategic decisions have positioned them well for future growth. With the rise of GenAI and a growing ecosystem built on CUDA, NVIDIA is poised to remain at the forefront of AI compute and has rapidly grown into one of theÂ [most valuable companies in the world](https://www.washingtonpost.com/business/2024/11/05/nvidia-tops-apple/).

## Where Are the Alternatives to CUDA?
In conclusion, CUDA remains both a blessing and a burden, depending on which side of the ecosystem youâ€™re on. ItsÂ [massive success](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed)Â drove NVIDIAâ€™s dominance, but its complexity, technical debt, and vendor lock-in present significant challenges for developers and the future of AI compute.

With AI hardware evolving rapidly, a natural question emerges:Â **Where are the alternatives to CUDA?**Â Why hasnâ€™t another approach solved these issues already? In Part 5, weâ€™ll explore the most prominent alternatives, examining the technical and strategic problems that prevent them from breaking through the CUDA moat. ğŸš€
>  ä¸‹ä¸€ç« å°†æ¢è®¨ CUDA çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ†æä»–ä»¬çªç ´ CUDA æŠ¤åŸæ²³çš„æŠ€æœ¯å’Œæˆ˜ç•¥é—®é¢˜

â€“Chris
