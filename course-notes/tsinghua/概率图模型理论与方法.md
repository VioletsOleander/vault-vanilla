>古槿
# 1 Introduction to Probabilistic Graphical Models
>2024.9.9

用图模型求解概率问题，基于概率在不确定的世界中做出决策

**概率论**
现代概率论：
- 频数论 (基于随机试验)
    将做试验的次数和事件发生的次数的比值作为概率
- 贝叶斯
    指定先验，通过观测更新先验
    $P(X|Y) = \frac {P(Y|X)P(X)}{P(Y)} = \frac {P(Y|X)P(X)}{\sum_X P(Y|X)P(X)}$
    $Y$: 观测/果
    $X$: 原因/因
    $P(X)$ : 先验
    贝叶斯公式将执果索因的条件概率写为仅仅和执因索果的条件概率和原因的先验概率有关的形式 

条件独立：$X\perp Y | Z: P(XYZ) = P(X|Z)P(Y|Z)P(Z)$

链式法则：每一次仅决策一个变量 (分治)，逐个将变量拆分到条件

**图论**
path：路径
trail: 同一条路径，双向可达
tree：单个节点仅有一个父节点
polytree：可以有多个父节点，但没有三角形
chordal graph：图中最大的形状就是三角形，没有多边形
clique：子图，子图内所有节点两两相连

贝叶斯网络：有向无环图，节点表示变量，边表示依赖
Markov 网络：无向图

概率图模型：
- 表示：建模
- 推断：已知参数下，计算概率
- 学习：根据数据，学习参数

朴素贝叶斯：只要类别标签已知，所有的观测变量都独立

# 2 Baysian Network: Representation
> 2024.9.16

I-map: 确保在图上 ($\mathcal G$ 空间) 做的所有操作在 $P$ 空间都是成立的

证明 $\mathcal G$ 是否是 $P$ 的 perfect map 是一个 NP hard 问题
证明 $P$ 是否存在 perfect map 是一个 NP hard 问题

# 3 Local Probabilistic Models
> 2024.9.21

概率图模型的直观思想：
    有向无环图（节点表示变量）
    用图定义变量之间的独立性

概率图模型的形式化思想：
    定义公理：$\mathcal G$ 中的独立性定义为 $(X_i \perp Nondesc (X_i) \mid Pa (X_i))$
    由公理推导 factorization 定理：
    - 充分条件： $\mathcal G$ is a I-Map of $P$ $\rightarrow$ $P (X_1,\dots, X_n) = \prod_{i=1}^n P (X_1\mid Pa (X_i))$ ($P$ 满足 $\mathcal G$ 中的所有独立性，故根据 $\mathcal G$ 分解 $P$)
    - 必要条件： $P (X_1,\dots, X_n) = \prod_{i=1}^n P (X_1\mid Pa (X_i))$ $\rightarrow$  $\mathcal G$ is a I-Map of $P$ (若 $P$ 可以根据 $\mathcal G$ 分解，则 $\mathcal G$ 是 $P$ 的 I-Map，即 $\mathcal G$ 不包含 $P$ 中没有的独立性)

必要条件主要用于结构学习

v-structure：
    子节点被观察到后，父节点发生关联

Noisy-or:
    子节点的值不需要知道全部的父节点的值才能决定，每一个父节点都有各自一定的概率对子节点产生相同的影响
    例如：只要任意一个父节点（以一定概率 $f_i$）发生了 failure，此时子节点就一定会 failure，$P (F = False) = \prod_i (1-f_i)$
    再考虑 leak probability $f_0$，就写为 $P (F = False) = (1-f_0)\prod_i (1-f_i)$

BN2O:
    朴素贝叶斯和 Noisy-or 的拓展
    多个结果节点（子节点）
    朴素贝叶斯：已知某个子节点的全部父节点，该子节点和其余所有子节点条件独立
    Noisy-or：每个子节点和其所有父节点之间构成了 Noisy-or 模型

Generalized linear model:
    多个 cause 的效果乘上权重（权重可为负），线性叠加，得到总效果（分数）
    包括：Logistic CPD, Linear Gaussian 等

Logistic CPD:
    Sigmoid 函数将效果/分数映射到 $[0,1]$ （概率空间）
    由此定义的 CPD 称为 Logistic CPD，即 $P (Y = y^1 \mid X_1,\dots, X_k) = \text{sigmoid} (w_0 + \sum_{i=1}^k w_iX_i$)
    Logistic CPD odds:

$$
\begin{align}
O &= \frac {P(Y=y^1\mid X_1,\dots,X_k)}{P(Y = y^0\mid X_1,\dots, X_k)} \\
&=\frac {e^s / (1+e^s)}{1/(1+e^s)}\\
&=e^s
\end{align}
$$
    For binary $X_i$, when $X_j$ change from $0$ to $1$
$$
\Delta O = e^{w_j}
$$


Linear Gaussian:
    $(Y\mid X_1,\dots ,X_k) \sim \mathcal N (w_0 + \sum_i w_i X_i, \sigma^2)$

如果神经元激活是按照概率 $P (Y = y^1 \mid X_1,\dots , X_k)$，则神经网络也可以视作概率图模型

Pooling:
    抗噪声
    max pooling - 平移稳定性
    median pooling - 过滤 outlier 噪声
    mean pooling - 过滤高斯噪声

hierarchical model: 决定了变量分布的参数同样服从于某个需要学习的分布

Three steps for representation:
    Define node/variables
    Consider edges/dependencies
    Choose local CPDs

# 4 Dynamic Models
> 2024.9.28

Markov/Memoryless property:
    assumption: 
    $P (X) = \prod_{t=1}^n P (X (t)\mid X (t-1))$ and $\forall i, j, P (X (i)\mid X (i-1)) = P (X (j)\mid X (j-1))$

HMM:
    Local structure 1: Hiddent states (can not be observed) are linked as Markov chain $P (S^T \mid S^{t < T}) =  P (S^T\mid S^{T-1})$
    Local structure 2: Observable variables are only determined by the hiddent state $P (O^T \mid S^{t\le T})  = P (O^T \mid S^T)$

The key is designing the hidden variables.

State-Obseration models:
    State-State Transimission Matrix $T_{(n, n)}$: $S^t = TS^{t-1}$ 
    State-Observation Emission Matrix $E_{(m, n)}$: $O^t = ES^t$

Calculations in HMM:
    1.  $P (X\mid \theta)$: given the model and the observation sequence, infer the probability
    2.  $\arg\max_{Y}P (X, Y\mid \theta)$: given the model and the observation sequence, infer the hidden labels of the sequence
    3.  $\arg\max_{\theta}P (X\mid \theta)$: learn parameters from the observation sequence

**Question 1**: Infer $P (X\mid \theta)$
Define:

$$
\begin{align}
\theta &= \begin{cases}
T = \{t_{i,j},\quad i,j = 1,\dots N\},\\
E = \{e_{i,j},\quad i = 1,\dots N,j=1,\dots ,K\},\\
\pi = \{\pi_i,\quad i = 1,\dots N\}
\end{cases}\\\\
X &= \{x_t,\quad t=1,\dots T\}
\end{align}
$$

$\theta$ is the parameterization of HMM, $X$ is the observation sequence.

Basic Idea:
$P (X) = \sum_Y P (X, Y)$

by factorization theorem:

$$\begin{align}
P (X, Y) &= P (Y_1) P (X_1\mid Y_1) P (Y_2\mid Y_1) P (X_2\mid Y_2)\cdots P(Y_T\mid Y_{T-1})P(X_T\mid Y_T)\\
&=P(X_T\mid Y_T)P(Y_T\mid Y_{T-1})P(X_{T-1}\mid Y_{T-1})P(Y_{T-1}\mid Y_{T-2})\cdots P(Y_1)
\end{align}$$

the probability can be solved by eliminating $Y_i$ iteratively

Forward algorithm:
$\alpha_t (i) = P (x_1,\dots, x_t, y_t = i\mid \theta)$ (the probability that the Markov chain has forwarded into time $t$ with state $y_t = i$ and observation $x_1,\dots, x_t$ )
- Initialization: $\alpha_1 (i) = \pi_i e_{i, x_1}$ 
    (the probability that $y_1 = i$ and observed $x_1$)
- Induction: $\alpha_{t+1}(i) = [\sum_{j=1}^N\alpha_t (j) t_{j, i}]e_{j, x_{t+1}}$
    (the probability that the Makov chain has forwarded into time $t+1$ and observe $x_{t+1}$)
- Termination: $P (X\mid \theta) = \sum_{i=1}^N\alpha_T (i)$

Backward algorithm:
$\beta_t (i) = P (x_{t+1},\dots, x_T\mid y_t = i, \theta)$ (the probability that the future observation will be $x_{t+1},\dots, x_T$, given state $y_t = i$ )
- Initialization: $\beta_{T+1}(i) = 1$ 
    (the probability of a non-event, given the fact that the chain is finished)
- Induction: $\beta_t (i) =\sum_{j=1}^N t_{i, j}e_{j, x_{t+1}}\beta_{t+1}(j)$
    $$
\begin{align}
\beta_t (i) &= P (\mathbf x_{t+1: T} \mid y_t = i)\\
&= \sum_j P (y_{t+1} = j, \mathbf x_{t+1: T}\mid y_t = i)\\
&=\sum_j P (y_{t+1} = j, \mathbf x_{t+1: T}\mid y_t = i) \\
&=\sum_j P ( \mathbf x_{t+1: T}\mid y_t = i,y_{t+1} = j)P(y_{t+1} = j\mid y_t = i) \\
&=\sum_j t_{ij}P ( \mathbf x_{t+1: T}\mid y_{t+1} = j) \\
&=\sum_j t_{ij}P(x_{t+1},\mathbf x_{t+2:T}\mid y_{t+1} = j)\\
&=\sum_j t_{ij}P(x_{t+1}\mid y_{t+1} = j) P(\mathbf x_{t+1:T}\mid y_{t+1} = j)\\
&=\sum_j t_{ij}e_{j, x_{t+1}}\beta_{t+1}(j)
\end{align}
    $$

- Termination: $P (X\mid \theta) = \beta_0(i) = \sum_{j=1}^N \pi_j e_{j, x_1}\beta_1(j)$

**Question 2**: Infer $\arg\max_Y P (X,Y\mid \theta)$
Question 1 在前向递推的时候，实际上每一个 $t$ 上都对所有 $Y_t = i$ 的情况做了求和
如果需要递推 Question 2，此时我们在前向递推的时候，只需要记录最大的路径概率即可 (最大概率的路径是满足贪心的，类似于 Dijstra 算法的思想，但注意全局最优状态序列 $Y$ 的递推不一定是满足贪心的)

也就是说，在递推时，对于状态 $y_t = i$，我们记录了从开始到 $y_t = i$ 的最可能路径，而不是所有路径，因此在 $t$ 时刻，我们仅仅记录了 $|Val (y_t)|$ 条路径，注意每条路径的状态序列都不同
之后，对于下一个时刻，我们对于 $\forall i,y_{t+1} = i$，都从 $|Val (y_t)|$ 条路径中选出最可能的一条，最后在 $t+1$ 时刻，我们同样得到 $|Val (y_{t+1})|$ 条路径
不到最后的时刻 $T$，我们无法确定哪条路径是全局最优，因此也无法在中间时刻确定一个部分的状态序列，但一旦确定了某条全局最优的路径，整个状态序列也随之确定

因此，我们在前向推导时记录概率，后向推导时计算状态

Viterbi algorithm:
The probability at time 1 with state $y_1 =i$ and observation $x_1$ :
$\delta_{1, i} = \pi_i e_{i, x_1}$

The probability at time 2 with state $y_2 = i$ and observation $x_1, x_2$ , given the most probable path:
$\delta_{2, i} = e_{i, x_2}\max_{y_1 = 1,\dots, N}(\pi_{y_1}e_{y_1, x_1}\times t_{y_1, i}) =e_{i, x_2}\max_{y_1 = 1,\dots, N}(\delta_{1,y_1}\times t_{y_1,i})$
The most likely previous state on the most probable path to $y_2 = i$:
$\phi_2 (i) = \arg\max_{y_1 = 1,\dots, N}(\delta_{1, y_1}\times t_{y_1, i})$ 

The probability at time $t$ with state $y_t = i$ and observation $x_1,\dots, x_t$, given the most probable path:
$\delta_{t, i} = e_{i, t}\max_{y_{t-1} = 1,\dots, N}(\delta_{t-1, y_{t-1}}\times t_{y_{t-1}, i})$
The most likely previous state on the most probable path to $y_t = i$:
$\phi_t (i) = \arg\max_{y_{t-1}=1,\dots, N}(\delta_{t-1, y_{t-1}}\times t_{y_{t-1},i})$

The probability at time $T$ with state $y_T = i$ and observation $x_1,\dots, x_T$, given the most probable path:
$\delta_{T, i} = e_{i, x_T}\max_{y_{T-1 = 1,\dots, N}}(\delta_{T-1, y_{T-1}}\times t_{y_{T-1}, i})$
The most likely previous state on the most probable path to $y_T = i$:
$\phi_T (i) = \arg\max_{y_{T-1}=1,\dots, N}(\delta_{T-1, y_{T-1}}\times t_{y_{T-1},i})$

$y_T^* = \arg\max_{y_T = 1,\dots, N}(\delta_{T, y_T})$

After $y_t^*$ is inferred, trace back to get $y_{t-1}^*$:
$y^*_{t-1} = \phi_t (y_t^*) = \arg\max_{y_{t-1} = 1,\dots, N}(\delta_{t-1, y_t^*}\times t_{y_{t-1}, y_t^*})$

**Question 3**: Learn $\arg\max_{\theta} P (X\mid \theta)$
参数 $\theta$ 实际上都和隐状态 $Y$ 有关，因此在未知状态的情况下是无法做出推导的
故简单的思路就是对状态进行猜测，迭代优化参数

General startegy:
- set an initial value of the parameters
- do the inference of hidden states
- re-estimate and re-learn the paramters
- repeat the process until convergence

Baum-Welch algorithm:
- Define an intermediate variables for inference:
    $\xi_t (i, j) = P (y_t = i, y_{t+1} = j \mid X, \theta)$
    (given observation and parameter, the probability of $y_t = i$ and $y_{t+1} = j$)
- Use forward and backward algorithm to calculate probability:
    $\theta^0 = \{T^0, E^0, \pi^0\}$
    $\alpha_t(i) = P (\mathbf x_{1: t}, y_t = i\mid \theta^0)$
    $\beta_t (i) = P (\mathbf x_{t+1:T}\mid y_t = i, \theta^0)$

    $$
\begin{align}
\xi_t(i,j) &= P(y_t = i, y_{t+1} = j \mid \mathbf x_{1:T})\\
&=\frac {P(y_t = i, y_{t+1} = j , \mathbf x_{1:T})}{P(\mathbf x_{1:T})}\\
&=\frac {P(y_t = i, y_{t+1} = j , \mathbf x_{1:T})}{\sum_{i,j}P(y_t = i, y_{t+1}=j,\mathbf x_{1:T})}\\
&= \frac {\alpha_t(i)t_{i,j}e_{j,x_{t+1}}\beta_{t+1}(j)}{\sum_{i=1}^Y\sum_{j=1}^Y\alpha_t(i)t_{i,j}\beta_{t+1}(j)e_{j,x_{t+1}}}\\
\end{align}
$$

    (Firstly forward to $t$ with state $y_{t} = i$ and observation $\mathbf x_{1:t}$, then transition to $t+1$ with state $y_{t+1} = j$, and emit observation $x_{t+1}$，then forward to $T$ with observation $\mathbf x_{t+2:T}$)

- Calculate:
    The probability of $y_t = i$:
    $\gamma_t (i) = \sum_{j=1}^Y\xi_t (i, j)$ (which corresponds to $\pi_i$)
    The expected times for state stayed in $Y = i$:
    $\sum_{t=0}^{T-1}\gamma_t (i)$
    The expected times for state transition $i-j$:
    $\sum_{t=0}^{T-1}\xi_t (i, j)$
- Re-estimate all paramters:
    $$
\begin{align}
t_{i,j} &= \frac {\sum_{t=0}^{T-1}\xi_t(i,j)}{\sum_{t=0}^{T-1}\gamma_t(i)}\\
e_{i,x} &=\frac {\sum_{t=0}^T\mathbf I(x_t = x)\gamma_t(i)}{\sum_{t=0}^{T-1}\gamma_t(i)}
\end{align}
    $$

- Repeat above steps until convergence:
    $|\log P (X\mid \theta) - \log P (X\mid \theta^0)| < \epsilon$

Generate simulated data using HMMs:
- Find the initial probability for hidden states
- MCMC
    MCMC are class of algorithms based on constructing a Markov chain whose equilibrium distribution is our desired distribution. Therefore, the state of the chain after a large number of steps is then used as a sample from our desired distribution
    A good Markov chain's stationary distribution can be reached quickly starting from an arbitrary point.

Max entropy Markov Models:
    termed MEMM or CMM (conditional Markov Models)
    MEMM is a discriminative model which extends the standard maximum entropy model by assuming the unknown value to be learned are connected in a Markov chain rather than conditionally independent of each other ($Y^i$ 之间有联系，而不是互相条件独立)
    MEMM cares about $P (Y\mid X)$, standard Markov Model cares about $P (Y, X)$, that's the different

Hopfield Network:
    the predecessor of RNN

Markov blanket: 一个变量 $S$ 的 Markov blanket 指一个变量集合，满足给定该集合内的变量为条件，$S$ 和网络中所有其他变量条件独立

# 5 Markov Networks
Markov Networks (Intuitive)
考虑连接成网格状的四个分子 $A, B, C, D$，每个分子的状态由它周围的分子状态决定
物理上称该结构为分子场，分子场的状态由所有分子的状态组成，随着时间变化，场的状态不断变化，我们希望研究这个场的状态服从哪一类分布

直观上，我们希望该网格状的场中存在独立性 $(A\perp D\mid B, C)$ 和  $(B\perp C\mid A, D)$
可以证明不存在有向无环的贝叶斯网络是以上独立性的 Perfect Map，因此我们需要引入无向图，也就是 Markov 网络

Markov 网络编码的全局独立性假设 (Global Markov assumptions) 的形式为 
$\mathcal I (\mathcal H) = \{(\pmb X \perp \pmb Y \mid \pmb Z) : \text{sep}_{\mathcal H}(\pmb X; \pmb Y\mid \pmb Z)\}$，也就是给定一个节点的 Markov blanket (所有邻居节点)，该节点和网络中其他剩余节点条件独立

Compare with Bayesian Networks
带有多边形结构的 Markov 网络表达的独立性无法由贝叶斯网络完美表示，带有 v-structure (explaining away) 结构的贝叶斯网络表达的独立性无法由 Markov 网络完美表示

Markov Network Factors
Markov 网络中，我们在一组变量上定义因子 (factor)，因子是函数，将某个变量集合 $\pmb D$ 的赋值映射到某个实数，即 $f: Val(\pmb D) \mapsto \mathbb R^+$ ，要求 $\pmb D$ 中的变量应该直接地相互依赖
因子的取值表示了特定变量赋值中各个变量的特定赋值之间的亲和度，亲和度越高，说明该特定变量赋值的出现可能性就越高，因子的取值应该就越高
因子的取值要求是正数

条件概率分布也符合 factor 的定义，可以看作是 factor 的特例 (Factors generalize the notion of CPDs: Every CPD is a factor)

Gibbs Distribution
Markov 网络定义的 Gibbs 分布由两部分组成：因子乘积和划分函数
因子乘积即 Markov 网络中所有因子的乘积，该乘积定义了一个 Markov 网络上未归一化的分布
划分函数是归一化因子，它将为归一化的分布归一化，可以理解为系统的总能量
Gibbs 分布等于未归一化的分布除以划分函数

Cliques: Closed Sets on the Graph
Markov 网络 $\{V, E\}$ 满足 $X_i-X_j\in E$ 当且仅当 $P(x_i\mid \pmb x \backslash x_i)\ne P(x_i\mid \pmb x\backslash(x_i, x_j))$
也就是在根据概率空间定义图空间时，如果变量 $X_i, X_j$ 要相连，则在概率空间中，给定 $X_j$ 会让 $X_i$ 的条件概率分布发生变化，也就是 $X_j$ 需要对 $X_i$ 的分布直接产生影响

定义 $X_i$ 的邻域 $N(X_i)$ 为在图空间所有和 $X_i$ 直接相连的节点 $X_j$ 组成的集合

变量集合 $C\subseteq V$ 是一个团当且仅当对于任意 $X\in C$，$C\subseteq \{X, N(X)\}$，这意味着团是一个闭集，它能够影响的范围只有它自己和其邻域，或者说一组节点构成的团一定处于其中任意一个节点的邻域中

团是 Markov 网络的基本局部结构

Factorization of Markov Networks
HC 定理联系了图空间和概率空间，它可以视为无向图的分解定理
I-map to Factorization
    给定无向图 $\mathcal H$，如果 $\mathcal H$ 是 $P$ 的 I-map，则 $P$ 可以分解为 $P (\pmb X) = \frac 1 Z\prod \pi_i[\pmb D_i]$，其中 $\pmb D_i$ 是 $\mathcal H$ 中的团
Factorization to I-map
    给定无向图 $\mathcal H$ ，如果 $P$ 可以分解为 $P (\pmb X) = \frac 1 Z \prod \pi_i[\pmb D_i]$，则 $\mathcal H$ 是 $P$ 的 I-map

分解于 $\mathcal H$ 的分布 $P$ 就称为 $\mathcal H$ 上的 Gibbs 分布

在 Markov 网络的分布表示中，并不是所有的团都是必须的，所必须的团都是极大团

Maximal clique: 
一个极大团中的多个子团对应的 factor 的乘积可以由该极大团对应的 factor 表示
例如，考虑极大团 $\{A, B, C\}$，其中有子团 $\{A, B\}, \{B, C\}$，我们可以定义一个 factor $\pi[A, B, C]$ 来直接表示 $\pi_1[A, B, C]\pi_2[A, B]\pi_3[B, C]$

因此，Markov 网络的分布表示中，factor 对应的变量集合 $\pmb D$ 必须在图中构成一个极大团，团保证了 $\pmb D$ 中的变量对都是直接相互依赖的，同时每个极大团必须对应一个 factor 的变量结合 $\pmb D$ (极大团是最低要求)，这保证了由 factors 定义的 Gibbs 分布足以表示 Markov 网络中的所有独立性

Example: Pairwise Markov Networks
图 $\mathcal H$ 上的成对 Markov 网络包含：
    一组定义于单个节点的节点因子/势能函数，记作 $\{\pi[X_i],i=1,\dots, n\}$
    一组定义于边的边因子/势能函数，记作 $\{\pi[X_i, X_j], X_i, X_j\in\mathcal H\}$
    Gibbs 分布写为 $P = \frac 1 Z \prod_i \pi[X_i] \prod_{(i,j)} \pi[X_i, X_j]$

能量势能函数一般习惯使用指数形式，故对原表示进行指对数转化：
$\pi[\pmb D] = \exp (-\epsilon (\pmb D))$，其中 $\epsilon (\pmb D) = -\ln [\pi[\pmb D]]$
在对数表示下，Gibbs 分布重写为：

$$
\begin{align}
P (\pmb X) &= \frac 1 Z \prod_i \left\{\exp(-\epsilon (\pmb D_i))\right\}\\
&=\frac 1 Z \exp\left\{\sum_{i}-\epsilon(\pmb D_i)\right\}\\
&=\frac 1 Z \exp\left\{-\sum_{i}\epsilon(\pmb D_i)\right\}
\end{align}$$

$\pi[\pmb D]$ 越高，势能函数 $\epsilon (\pmb D) = -\ln \pi[\pmb D]$ 越低

负号来自于所有的自由能都是负值

MN Representation Revisited
Besag 进一步推进了 HC 定理，他为 HC 定理给出了另一种构造性证明，并且为 HC 定理中，无向图的 Gibbs 分布的展开形式中的各个 factor 给出了确定的形式

对于无向图 $\mathcal H$，定义 $Q$ 函数：

$$
Q(x\mid \text{NB}(x)) = \ln\left[\frac {P(x\mid \text{NB(x)})}{P(x=0\mid \text{NB(x)})}\right]
$$
并且

$$
Q(\pmb x) = \ln \left[\frac {P(\pmb x)}{P(\pmb x= \pmb 0)}\right]
$$

容易知道

$$
\begin{align}
P(\pmb x) &=  \exp\{\ln P(\pmb x)\}\\
&=\frac {\exp\{\ln P(\pmb x)\} \cdot \exp\{-\ln P(\pmb 0)\} }{\exp\{-\ln P(\pmb 0)\}}\\
&=\frac {\exp\left\{\ln \frac {P(\pmb x)}{P(\pmb 0)}\right\}}{\frac {1}{P(\pmb 0)}}\\
&=\frac {\exp\left\{Q(\pmb x)\right\}}{\frac {\sum_{\pmb x}P(\pmb x)}{P(\pmb 0)}}\\
&=\frac {\exp\left\{Q(\pmb x)\right\}}{\sum_{\pmb x}\frac {P(\pmb x)}{P(\pmb 0)}}\\
&=\frac {\exp\left\{Q(\pmb x)\right\}}{\sum_{\pmb x}\exp\left\{\ln\frac {P(\pmb x)}{P(\pmb 0)}\right\}}\\
&=\frac {\exp\left\{Q(\pmb x)\right\}}{\sum_{\pmb x}\exp\left\{Q(\pmb x)\right\}}\\
\end{align}
$$

$\mathcal H$ 是正分布 $P$ 的 I-map 等价于 $Q$ 函数 $Q(\pmb x) = \ln \frac {P(\pmb x)}{P(\pmb x = \pmb 0)}$ 存在唯一的展开：

$$
Q(\pmb x) = \sum_i x_i \psi_i(x_i) + \sum_{i,j}x_ix_j\psi_{i,j}(x_i,x_j) + \cdots +  x_1x_2\dots x_n \psi_{1,2\dots ,n}(x_1,x_2,\dots, x_n)
$$

其中 $\psi_{s} \ne 0$ 当且仅当所有的 $v\in s$ 在 $\mathcal H$ 中构成一个团

因此，给定无向图 $\mathcal H$，我们可以根据枚举图中的团，根据 $Q(\pmb x)$ 的形式，写出在图上分解的 Gibbs 分布 $P(\pmb x) = \frac 1 Z\exp\{Q(\pmb x)\}$，其中 $Z = \sum_{\pmb x }\exp \{Q(\pmb x)\}$，分布 $P$ 将满足 $\mathcal I (P) \subseteq \mathcal I (\mathcal H)$，即分布 $P$ 编码了图中的独立性

An example with 3 binary variables:
$\pmb x$ 的 $Q$ 函数写为：
$Q (\pmb x) = \ln\left[ \frac {P (\pmb x)}{P(\pmb x = \pmb 0)}\right] = \sum_{i=1,2,3} \alpha_i x_i + \sum_{i<j = 1, 2, 3}\alpha_{i,j}x_ix_j + \alpha_{1, 2, 3}x_1x_2x_3$

$x_1$ 的 $Q$ 函数写为：

$$\begin{align}
Q (x_1\mid x_2, x_3)
&= \ln\left[\frac {P(x_1\mid x_2, x_3)}{P(x_1 = 0\mid x_2, x_3)}\right]\\ 
&=\ln\left[\frac {P(x_1, x_2, x_3)}{P(x_1 = 0 ,x_2, x_3)}\right]\\
&=\ln\left[\frac {P(x_1, x_2, x_3)/P(0,0,0)}{P(x_1 = 0 ,x_2, x_3)/P(0,0,0)}\right]\\
&=\ln\left[\frac {P(\pmb x)}{P(\pmb x = \pmb 0)}\right]- \ln\left[\frac {P(x_1 = 0, x_2, x_3)}{P(0,0,0)}\right]\\
&=Q(x_1, x_2, x_3) - Q(x_1 = 0, x_2, x_3)\\
&=\alpha_1x_1 + \alpha_{1,2}x_1x_2 + \alpha_{1, 3}x_1x_3 + \alpha_{1,2,3}x_1x_2x_3
\end{align}$$

图 $\mathcal H$ 中编码了 $(X_1 \perp X_2 \mid X_3)$ $\Longleftrightarrow$ $Q (x_1\mid x_2, x_3)$ 展开式中关于 $x_3$ 的项应该是零，即 $\alpha_{1, 3}, \alpha_{1, 2, 3} = 0$

对于仅由二元变量构成的系统，可以将所有的势能函数 $\psi ()$ 替换为单个参数而不失一般性，因为 $\psi ()$ 仅在全部相关变量都取为 1 时才可以为 $Q$ 函数做出贡献，此时 Gibbs 分布可以写为：

$$
\begin{align}
P(X) & = \frac 1 Z \exp(-U)\\
U&=-\sum_i\left(\beta_j\prod_{x_j \in C_i}x_j\right)\\
Z&= \sum_X\exp(-U)
\end{align}
$$

Notes about HC theorem by Besag:
- ground state: 当某个变量 $X_i$ 处于 ground state (等于零)，所有和它相关的势能都变为零 ($x_i = 0$ 乘以任何数都得到零)
- $Q$ 函数描述了相对于 ground state 的相对概率
- $Q$ 函数连接了由图编码的独立性和由分布编码的独立性，它们是等价的
- 定义好 Markov 网络中每个团的势能函数 $\psi ()$，就可以写出 Gibbs 分布 (log-linear format)

Ising Model
Ising 模型的能量函数表示为 $\xi = -\sum_{i<j}w_{i, j}x_ix_j - \sum_{i}u_i x_i$，故其表示的概率分布记为 $P(\xi) = \exp(-\xi)/Z$

Restricted Boltzmann Machines
限制性玻尔兹曼机要求所有的交互发生在层与层之间，因此层内节点的交互必须是间接的

RBM 的能量函数表示为 $\xi = -\sum_{i, j} w_{i, j}b_ic_j - \sum_i v_i b_i - \sum_{i}h_ic_i$，

Gaussian Random Field
任何一个多元高斯分布的 PDF 的形式都可以写为 $p(x) \propto \exp(-\frac 1 2x^TJx + (J\mu)^T x)$
因此任何一个多元高斯分布都可以由一个 Markov 网络表示，我们称为高斯随机场，其能量函数就是 $-(-\frac 1 2 x^TJx + (J\mu)^Tx)$

能量函数可以划分为两项 $-\frac 1 2 J_{ii}x_ix_i + h_i x_i$ 和 $-J_{ij}x_ix_j$，分别表示单个节点和成对节点的交互，因此高斯随机场实际是一个成对 Markov 网络

唯一的要求是信息矩阵 $J$ 需要是正定的

以上结论可以推广到任意指数族分布

Summary: Markov Networks
Markov 网络的因子分解定理称为 HC 定理，该定理称，对于任意 Markov 网络 $\{\mathcal H, P\}$，当且仅当 $\mathcal H$ 是 $P$ 的 I-map，正分布 $P$ 可以按照 $\mathcal H$ 中的团上的因子分解为因子积

Besag 为 HC 定理提供了构造性证明，并且给出了一种构建每个团上的因子的一种方法 (构造为 $\exp\{x_ix_j \psi_{ij}(x_i, x_j)\}$)

# 6 Examples for Advanced PGM Representation
表示：将 $P$ 映射到 $\mathcal H$ 或 $\mathcal G$
    贝叶斯网络：有向无环图
    Markov 网络：无向图
    动态/序列模型：有环图（实际上按时间轴展开也得到的是有向无环图）
    连续分布的概率图模型：高斯图模型

Markov 网络
    基本结构 (local structure)：团，每个团对应于一个正的局部因子 (local factor)
    联合概率分布：传统的 Gibbs 分布表示、Log-linear 表示
    Markov blanket：所有邻居节点
(Log-linear 表示：
$P(\pmb x) = \frac 1 Z e^{-U (\pmb x)}$
$U (\pmb x) =  - \sum_{C_i}\left[\psi_i(C_i)\prod_{x_j \in C_i}x_j\right]$ 
仅适用于离散分布)

Bayesian 网络
    基本结构：父节点 -> 子节点，对应于一个 CPD
    联合概率分布：所有局部 CPD 的乘积
    Markov blanket：所有父节点、所有子节点、所有子节点的所有父节点

“表示”三步走：
    1. 定义随机变量
    2. 绘制图模型拓扑结构
    3. 确定局部概率模型

**Model Conditional Information**
Example: 
POS (part of speech) identification 词性标注
早期使用 HMM，将 POS 建模为隐状态，将词建模为观测
HMM 的缺陷：
在 HMM 中，我们要建模给定 POS，它生成不同的词的概率，故观测的维度太高了（等于词的数量）
HMM 将问题建模为一个生成式问题，建模的是联合概率 $P (X, Y)$，而我们实际仅需要生成模型，并不需要知道给定 POS，生成某个词的概率

Generative Model:
    建模所有变量的联合概率
    缺点：困难

Discriminative Model:
    仅建模条件概率 $P(Y\mid X = x)$（其中 $X$ 是观测变量，$Y$ 是状态变量），
    目的是推理状态变量 $Y$（例如标签）
    优点：简单
    缺点：决策边界上的 outlier 会较大程度影响决策边界；无法生成新数据
    (you don't really konw how the things work, unless you can make one
    you don't really konw what you are working, unless you can make anybody understand)

Example:
Max Entropy Markov Model (MEMM)
判别模型，该模型利用 Markov 链拓展了最大熵分类器，假定了隐变量通过 Markov 链相连，而不是互相条件独立

但是该模型效果不理想被抛弃了

Example:
Generalized Conditional BNs
对 BN 中的全部节点都添加一个父节点，称为条件节点，该节点永远被观测到
因为该节点永远被观测到，故对图的实际分解并没有影响，在原来的每个 CPD 的条件中加上该节点即可，同时所有 CPD 乘起来的联合分布也是在给定该条件时的条件分布

Conditional MNs (CRFs)
对 MN 中的全部节点都添加一个邻居节点，该节点永远被观测到，故也不会影响 MN 中的节点的 Markov blanket


Example Recall: POS
考虑表示以下特性（这些特性是语言本身的特点）：
- 如果词在句子开始，POS 为 noun 的概率上升
- 如果词不在句子开始，且首字母大写，POS 为 noun 的概率上升
- 如果上一个词是 vt，该词是 noun 的概率上升

考虑使用 CRF 模型解决 POS 问题：
其中条件节点 $\pmb X = X_1, \dots, X_n$ 表示观测到的词序列
状态节点 $Y_1, \dots, Y_n$ 以 Markov 链两两相连，且都与条件节点 $\pmb X$ 相连

CRF 中的 feature function 定义为 indicator function，该 indicator function 是关于一个状态和一个观测子序列的 indicator，例如 $f_k (X, Y_i) = 1$ 仅当 $Y_i = noun$ 且当前观测 $X$ 是首字母大写的时候成立
此时，每个 feature function 都编码了 Markov 链中的一个或两个相邻节点以及观测 $\pmb X$ 的一个子序列

注意这些 feature function 的设计都是直接根据语言特性和经验人为设计得到的

**Deep structures**
Shallow models:
    why shallow?
    用于训练判别式模型的数据量较少，因此需要减少过拟合的结构风险 ( Reduce structure risk for overfitting )，也就是模型不能太复杂

Deep models:
    why deep?
    有大量数据可以用于训练生成式模型，模型复杂度应该与数据复杂度匹配

# 7 Inference as Optimization: Cluster Graph & Belief Propagation
Inference: 
给定观测 $\pmb E = \pmb e$，推断后验概率分布 $P(\pmb Y \mid \pmb E = \pmb e)$，或推断 $\arg\max_{\pmb y} P(\pmb Y = \pmb y \mid \pmb E = \pmb e)$ (极大后验推断/MAP)
MAP 推断不需要求出完整的分布，仅需要进行一个点估计

例子：在链式的贝叶斯网络中求 $e$ 的边际分布 $P(e)$

$$
\begin{align}
P(e) &= \sum_d\sum_c\sum_b\sum_a P(a,b,c,d,e)\\
&= \sum_d\sum_c\sum_b\sum_a P(a)P(b\mid a)P(c\mid b)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_c\sum_b\left(\sum_a P(a)P(b\mid a)\right)P(c\mid b)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_c\sum_bp(b)P(c\mid b)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_c\left(\sum_bp(b)P(c\mid b)\right)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_cp(c)P(d\mid c)P(e\mid d)\\
&=\sum_d\left(\sum_cp(c)P(d\mid c)\right)P(e\mid d)\\
&=\sum_dp(d)P(e\mid d)\\
&=p(e)
\end{align}
$$

此即消元法的思想

从消元法的思想可以进一步理解 HMM 的前向推理算法，其本质也是沿着链路不断消元，并将消元得到的信息向后传递的过程
VE: Forward Algorithm (compute $P(X\mid \theta)$)
Initialization: $\alpha_1(i) = \pi_i e_{i, x_1}$
Induction: $\alpha_{t+1}(i) = \left(\sum_{j=1}^N \alpha_t(j) t_{j, i}\right) e_{i, x_{t+1}}$
Termination: $P(X\mid \theta) = \sum_{i=1}^N \alpha_T(i)$

($\alpha_t(i) = P(x_1, \dots, x_t, y_t = i\mid \theta)$)

进一步，考虑在任意的贝叶斯网络中利用消元法进行推断
VE: General Bayesian Network
We want to compute $P(D)$, thus we need to eliminate $V, S, X, T, L, A, B$

The factorized joint distribution is:
$P(V )P(S)P(T \mid V )P(L \mid  S)P(B \mid  S)P(A \mid T, L)P(X \mid  A)P(D \mid  A, B)$

我们将 CPDs 都视作因子 (定义在一组变量上的函数)，将其重写为：
$\phi(V )\phi(S)\phi(T, V )\phi(L,S)\phi(B,S)\phi(A , T, L)\phi(X ,  A)\phi(D ,  A, B)$

我们按照图由上到下地对联合分布进行消元
首先消去 $V$，计算 $\sum_V \phi(V)\phi(T, V) = \tau_1(T)$
然后消去 $S$，计算 $\sum_S \phi(S)\phi(B, S)\phi(L, S) = \tau_2(L, B)$
然后消去 $X$，计算 $\sum_X \phi(X, A) = \tau_3(A)$
然后消去 $T$，计算 $\sum_{T}\tau_1(T)\phi(A, T, L) = \tau_4(A, L)$
然后消去 $L$，计算 $\sum_{L}\tau_4(A, L)\tau_2(B, L) = \tau_5(A, B)$
然后消去 $A$，计算 $\sum_A\tau_5(A, B)\tau_3(A)\phi(D, A, B) = \tau_6(B, D)$
最后消去 $B$，计算 $\sum_B \tau_6(B, D) = \tau_7(D) = P(D)$

按照这样的顺序消元的优势就在于每一次求和消元时的因子涉及到的变量都是最少的，注意求和消元的开销是和因子涉及变量数量成指数关系的
如果按照任意的顺序消元，例如先消去 $A$，则就需要计算 $\sum_A\phi(A, T, L)\phi(X, A)\phi(D, A, B)$，这涉及在一个作用域包含过多变量的因子中求和消去一个变量，开销会过大


Dealing with Evidence
假设给定观测 $V = v, S = s, D = d$，需要计算 $P(L, V = v, S = s, D = d)$
给定这些观测，我们写出此时的联合分布为：
$\phi (v )\phi (s)\phi (T, V )\phi (L, s)\phi (B, s)\phi (A , T, L)\phi (X ,  A)\phi (d ,  A, B)$

此时一些因子发生了简化/退化，例如 $\phi(V)$ 退化为常数 $\phi(v)$，$\phi(D, A, B)$ 退化为仅关于 $A, B$ 的因子 $\phi(d, A, B) = \tilde \phi(A, B)$

根据观测简化了相应因子后，我们执行和之前一样的消元法即可

Induced Graph in VE
在消元过程中出现的因子所涉及的变量应该互相连接
例如，在一开始将 CPDs 转化为因子时，$P(A\mid T, L)$ 转化为了 $\phi(A, T, L)$，因此原图中 $T, L$ 应该相连，也就是做了 Moralization
以及，消元时计算出了 $\tau_2(L, B)$ ，因此原图中的 $L, B$ 也应该相连

这就会得到 Induced Graph，VE 的导出图实际上就是对原图执行了两类操作，一类是 Moralization，对应于将 CPDs 转化为因子时的情况，一类是三角化，对应于根据新出现的因子连接节点的情况 (因为消元法一次消去一个节点，故对于四边形的结构，消去其中一个顶点一定会得到关于另外两个顶点的因子，故需要将二者相连，这就是三角化的过程，对于多边形也是如此，因此导出图中不会有多边形)

因此 Induced Graph 是 Moralized 且 Chordal

VE Disadvantages:
每次推理需要遍历整个图，效率较低

实际上消元时的一些中间结果是可以复用的

Re-Thinking VE Process:
考虑到 Induce Graph 的基本结构单元是 clique (maximal)，故可以将 maximal clique 上的边际分布作为中间结果存下来

将消元的过程视作消息传递
- 首先将初始消息 (local CPDs) 赋值给相关的 clique
- 然后消元法的过程就是每个 clique 消去自己独有的变量之后，将消息传递给相邻的 clique

Clique Tree: A Concrete Example
Clique Tree 的每个节点都对应于原图中的一个**极大** clique，我们将 local CPDs 分配给和它相关的 clique (也称为 factor)
(clique tree 也是对原来概率分布分解形式的一个等价表示，我们在原图中进行 moralization 和 triangulation/chordalization，得到 Induced Graph，然后根据 Induced Graph 中的 cliques 构造 clique tree)

Clique Tree 有两大重要特性：
- Tree and family perserving
    因子 (factors) $\phi_i$ 定义在 clique 上
    边 (edges) 定义在两个相连的 clique 的分离集 (共享的变量) $S_{i, j}$ 上 
- Running Intersecatoin property
    任意变量 $X$ 仅出现在 clique tree 中唯一的一个子图上，或者说一个变量在 clique tree 中一定是连通的，不会出现在两个分离的子图上 (否则在这样的图结构中进行推理会出现问题，无法确定要相信哪个子图的结果)

Message Passing: Sum Product on Clique Tree
目标为 $P(J)$
首先将每个簇/团的初始因子设定为和它关联的因子的乘积
$C_1$: 消去变量 $C$，发送消息 $\delta_{1\rightarrow 2}(D)$ 给 $C_2$
$C_2$: 消去变量 $D$，发送消息 $\delta_{2\rightarrow 3}(G, I)$ 给 $C_3$
$C_3$: 消去变量 $I$，发送消息 $\delta_{3\rightarrow 5}(G, S)$ 给 $C_5$
$C_4$: 消去变量 $H$，发送消息 $\delta_{4\rightarrow 5}(G, J)$ 给 $C_5$
$C_5$: 求和消去 $G, S, L$，得到 $P(J)$

最后汇总信息的团就是根团，我们从叶子团开始，逐步向内传递信息

Clique Tree 是原来分布的一个等价表示，在 Clique Tree 上执行 Sum-Product 消息传递等价于在原来分布执行消元法

Clique Tree Calibration
Clique Tree 的问题在于每次进行推理仍然需要执行完整的消息传递，要对树中的每个团再执行求和消元和信息传递操作
这实际上没有利用 Clique Tree 的结构特性，我们希望预先得到每个 Clique 上的边际分布，之后在进行推理仅需要关注一个 Clique 即可 

在 Clique Tree Calibration 过程分为 upward pass 和 downward pass
upward pass 中，我们选择任意一个团作为根团，从叶子节点向根团发送消息，完成 upward pass 后，我们实际上得到了根团上的边际分布
downward pass 中，我们从根团开始，向叶子节点传播消息 (注意朝向每个叶子节点的消息计算是不同的，计算时不会乘上从该叶子方向进来的消息)，完成 downward pass 后，我们等价于知道了每个团要计算其边际所需要的消息，因此实际得到了其余所有团的边际分布
(downward pass 本质上是复用了计算好了的消息)

因此校准算法仅用两次 pass 就计算出了全部 clique 的边际分布

Calibrated Clique Tree as Distribution
校准的 clique tree 除了存储了各个 clique 的边际分布，也可以视作整体未规范化的联合分布 $\tilde P_\Phi$ 的另一种表示，$\tilde P_\Phi$ 等于全部 cliques 的边际分布的乘积除去全部分离集上边际分布的乘积

To Random-Message Passing
在 Sum-Product 消息传递算法中，一个 clique 只有在 ready 状态，即收到了所有需要的输入消息之后，才可以发出消息

考虑让一个因子在不是 ready 状态也能向邻居发出消息

Message Passing: Belief Update
将从 $C_i$ 到 $C_j$ 发出的消息 $\delta_{i\rightarrow j}$ 视作将 $C_i$ 的信念 $\beta_i$ 乘上所有的消息，再除去 $\delta_{j\rightarrow i}$ ，再求和消去 $C_i - S_{i, j}$ 得到的结果

在信念更新算法中，每个 clique 直接维护自己的信念，每条边维护上一次发送的消息，即分离集上的信念 $\mu_{i, j}$
算法迭代中，每次都随机选取一条边，无论对应的 clique 是否是 ready 状态，都发送一条信息
算法的停止条件是发送信息不再会带来更新，即信息经过计算都得到的是 1，此时所有的 clique 都被校准了

Belief Update 算法主要涉及到近似推断
在精确推断中，Belief Update 算法和 Sum Product 算法等价

Answering Queries Outside a Clique
在 Clique Tree 中，找到包含所有 query 变量的最小的子树
通过 $\prod \beta_i / \prod \mu_{i, j}$ 计算子树上的边际分布 ($\beta_i$ 即子树涉及的 cliques 的信念，$\mu_{i, j}$ 即子树涉及的 sepsets 的信念)
在该边际分布上求和消去其余变量

Ansewering Queries with Increments
当引入一个新的观测 $Z = z$ 时，我们引入一个示性函数 $\mathbf I(Z = z)$ 作为新的因子
如果 query 变量 $X$ 和 $Z$ 在同一个 clique，则将该 clique 的信念和示性函数相乘，得到的就是 $Z = z$ 状态下的边际分布，此时求和消去其他变量即可

如果 $X$ 和 $Z$ 不在同一个 clique，我们将示性函数和某个包含 $Z$ 的 clique 的因子相乘，然后沿着该 clique 向包含 $X$ 的 clique 的路径传播消息，将 $\mathbf I(Z = z)$ 的信息传播到包含 $X$ 的 clique 中，之后求和消去其他变量即可

Clique Tree 的优势在于
- 其结构比原来的 BN/MN 更简单，推断更高效
- 其信念更新算法容易推广到近似推断

Constructing Clique Trees
我们想要构建具有族保持性质和运行相交性质的簇树，但这实际上是一个 NP-hard 问题

From Clique Tree to Loopy Cluster Graph
不考虑做 moralization 和 triangulation，也就是不考虑一定在 clique tree 上做精确推断，考虑直接在簇图上做近似推断 (簇图不一定是一颗树，是带环的)

环状的簇图无法定义根团，因此无法用类似 Sum-Product 的算法，故直接将 Belief Update 算法推广到簇图上，得到的就是 Belief Propagation 算法
该算法在簇图上没有理论保证，如果最后收敛了，就将结果视作对 clique 的边际的近似

loopy cluster graph 不一定要求是一颗树，但也要求满足运行相交性质，即某个变量 $X$ 仅能出现在一个子图中，不能出现在两个分离的子图中

设计 cluster graph 的一个方法是设计 bethe cluster graph，该设计可以保证运行相交性质

BP 算法不能保证收敛，也不能保证收敛后得到的信念是真实的边际分布

# 8 Inference as Optimization: Structured Variational Inference
变分法的基本思想：要在 $P(X)$ 中推理，考虑找到一个可以近似 $P(X)$ 的分布 $Q(X)$，在 $Q(X)$ 中进行推理，其中 $Q(X)$ 一般限制为一族具有简单形式的分布

一般使用 KL 散度 $D_{KL}(Q||P)$ 来衡量二者之间的差异

$$
D_{KL}(Q||P)  = E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X)}\right]
$$

优化的目标是找到最小化二者的 KL 散度的分布 $Q$，即 $\min_Q D_{KL}(Q||P)$

KL 散度不对称，且不满足三角不等式，因此严格来说不是一种距离度量


结构变分法：基于具有简单结构的图定义 $Q(X)$
例如，基于一个没有边的图定义 $Q(X)$，即平均场算法


关于 KL 散度的不对称性：
Reverse KL (I-projection): $D_{KL}(Q||P)$
Forward KL (M-projection): $D_{KL}(P||Q)$

Reverse KL 中，最优的 $Q(X)$ 不必要覆盖 $P(X)$ 的全部取值，只需要拟合好 $P(X)$ 的一个峰，就可以让 $D_{KL}(Q||P)$ 很小
Forward KL 中，最优的 $Q(X)$ 必须要覆盖 $P(X)$ 的全部取值，否则 $D_{KL}(P||Q)$ 会在 $Q$ 没有覆盖到且 $P > 0$ 的地方无穷大

一般使用 Reverse KL，因为对于 $Q$ 求期望显然比对于 $P$ 求期望容易


目标分布是条件分布的情况：
考虑目标分布为 $P(X\mid Z = z)$

$$
\begin{align}
D_{KL}(Q||P) &= E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X\mid z)}\right]\\
 &= E_{X\sim Q}\left[\ln \frac {Q(X)P(z)}{P(X,z)} \right]\\
 &= E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)} + \ln P(z) \right]\\
 &= E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)}\right] + \ln P(z) \\\\
 \ln P(z) - D_{KL}(Q||P) &=-E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)}\right]\\
&=E_{X\sim Q}\left[\ln P(X,z)\right] + E_{X\sim Q} \left[-\ln Q(X)\right]\\
&=E_{X\sim Q}\left[\ln P(X,z)\right] + H(Q)\\
\end{align}
$$

我们的目标是找到最小化 $D_{KL}(Q||P)$ 的 $Q$，等价于找到最大化 $\ln P(z) - D_{KL}(Q||P)$ 的 $Q$，$\ln P(z) - D_{KL}(Q||P)$ 可以记作 $L(Q)$

等式 $L(Q)=  E_{X\sim Q}\left[\ln P(X, z)\right] + H(Q)$ 的 RHS 被称为能量泛函，它是关于分布 $Q$ 的泛函，因此我们的目标就是找到最大化能量泛函的分布 $Q$ (或者说找到最小化 $J(Q) = -L(Q)$ 的分布 $Q$)

考虑能量泛函中的两项：
第一项 $E_{X\sim Q}\left[\ln P (X, z)\right]$ 衡量了 $Q$ 和 $P(X, z)$ 之间的关系，显然这一项在 $Q(X) = P(X \mid z)$ 时最大

第二项 $H(Q)$ 衡量了 $Q$ 本身的熵，显然这一项在 $Q$ 是均匀分布 (diffuse) 时最大

显然第二项相对于第一项倾向于正则项，用于避免过拟合

因为 KL 散度非负，因此 $L(Q) = \ln P(z) - D_{KL}(Q||P) \le \ln P(z)$
故 $L(Q) = E_{X\sim Q}\left[\ln P(X, z)\right] + H(Q)$ 实际上是 $\ln P(z)$ 的一个下界，或者说，能量泛函的上界是 $\ln P(z)$，当且仅当 $D_{KL}(Q || P)$ 为零时，界取到

故我们也可以将能量泛函 $L(Q)$ 称为 Evidence Lower Bound Observation (ELBO)，表示它是观测的概率的下界，我们的目标是找到一个 $Q$ 最大化 ELBO

对于 $Q$ 的形式的选择，最简单的例子是认为所有的变量在 $Q$ 中都是相互独立的，也就是和 $Q$ 相关的图没有任何边，这就是平均场推理
对于更复杂的情况，我们可以在 $Q$ 中设计特定的结构，也就是结构变分推理
更进一步，我们不直接设计 $Q$ 的结构，直接用神经网络表示


Mean Field Variational Inference
当假定 $q(x)$ 的图表示中没有任何边时，我们实际上假设了联合分布中所有涉及到的变量都是相互独立的，因此联合分布就是各个变量边际分布的乘积，即

$$
q(x;\theta) = \prod_i q_i(x_i)
$$

我们将 $q(x)$ 的分解形式代入能量泛函中，得到

$$
\begin{align}
E_{X\sim q}\left[\ln p(X,z)\right] &= \sum_x q(x) \ln p(x,z)\\
&=\sum_x \left(\left[\prod_i q_i(x_i)\right] \ln p(x,z)\right)
\\\\
H(q) &=E_{X\sim q}\left[-\ln q(X)\right]\\
&=\sum_x q(x)\left[-\ln q(x)\right]\\
&=\sum_x \left(\left[\prod_i q_i(x_i)\right]\left[-\ln\prod_i q_i(x_i) \right]\right)\\
&=\sum_x\left( \left[\prod_i q_i(x_i)\right]\left[-\sum_i\ln q_i(x_i) \right]\right)
\end{align}
$$

因此，能量泛函为

$$
\begin{align}
L(q) &= E_{X\sim q}\left[\ln p(X, z)\right] + H(q)\\
&=\sum_x \left(\left[\prod_i q_i(x_i)\right]\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)
\end{align}
$$

考虑单个变量的边际分布 $q_j$，假定所有其他变量的边际分布 $q_{-j}$ 都固定，将能量泛函写为关于 $q_j$ 的形式

$$
\begin{align}
L(q_j) &= \sum_x \left(\left[\prod_i q_i(x_i)\right]\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j}\sum_{x_{-j}}\left(\left[\prod_i q_i(x_i)\right]\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j}\sum_{x_{-j}}\left(q_j(x_j)\prod_{i\ne j} q_i(x_i)\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j} q_j(x_j)\left(\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j} q_j(x_j)\left(\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x,z) - \prod_{i\ne j}q_i(x_i)\sum_k \ln q_k(x_k)\right)\\
&=\sum_{x_j} q_j(x_j)\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x,z) - \sum_{x_j}q_j(x_j)\sum_{x_{-j}}\prod_{i\ne j}q_i(x_i)\sum_k \ln q_k(x_k)\\
&=\sum_{x_j} q_j(x_j)\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x,z) - \sum_{x_j}q_j(x_j)\sum_{x_{-j}}\prod_{i\ne j}q_i(x_i) \left[\sum_{k\ne j}\ln q_k(x_k) + \ln q_j(x_j)\right]\\
&=\sum_{x_j}q_j(x_j)\ln f_j(x_j) - \sum_{x_j} q_j(x_j) \ln q_j(x_j) + const
\end{align}
$$

其中 $\ln f_j(x_j) = \sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x, z) = E_{-q_j}\left[\ln p(x, z)\right]$

因此，我们进一步将 $L(q_j)$ 写为

$$
\begin{align}
L(q_j) &= \sum_{x_j} q_j(x_j)\ln\left[\frac {f_j(x_j)}{q_j(x_j)}\right] + const\\
&= \sum_{x_j} q_j(x_j)\ln\left[\frac {\exp \left\{E_{-q_j}\left[\ln p(x,z)\right]\right\}}{q_j(x_j)}\right] + const\\
\end{align}
$$

我们考虑进行迭代式优化，先仅优化 $q_j$，目标是最大化 $L(q_j)$
观察发现 $L(q_j)$ 的形式和 KL 散度的形式完全一致，因此要最大化 $L(q_j)$，我们希望

$$
q_j(x_j) \propto \exp\left\{E_{-q_j}[\ln p(x,z)]\right\}
$$

An Example for Ising Model
考虑一个 Ising Model 的例子，在该模型中，因变量 $x_i$ 都是二元变量，Evidence $z_i$ 总是被观测到

我们需要求一个近似分布 $q(x)$，我们使用平均场模型，假设所有隐变量相互独立，并应用上述的推导过程，我们可以知道每次迭代优化要求解的 $q_j(x_j)$ 需要正比于 
$\exp\left\{E_{-q_j}[\ln p (x, z)]\right\}$，因此我们实际要计算的目标就是 $E_{-q_j}[\ln p (x, z)]$

我们的目标分布是 $p(x\mid z) \propto p(x) p(z\mid x) = p(x,z)$，在 Ising Model 中，我们有

$$
\begin{align}
p(x) &= \frac 1 Z e^{\sum_{i,j}w_{ij}x_ix_j} \\
&= \frac 1 Z e^{-E_0(x)}\\
\\
p(z\mid x) &= \prod_i p(z_i \mid x_i)\\
&=e^{\sum_i \ln p(z_i \mid x_i)}\\
&= e^{\sum_i L_i(x_i)}
\end{align}
$$

其中 $p(x)$ 没有考虑一阶项 (Ising Model 不考虑一阶项)
其中 $p(z\mid x)$ 的分解形式是因为 Ising Model 中每个 $z_i$ 在给定 $x_i$ 时和所有其他 $z_j$ 条件独立

因此 

$$
\begin{align}
\ln p(x, z) &= \ln p(x) p(z\mid x)\\
&=\ln \frac 1 Z e^{\sum_{ij}w_{ij}x_ix_j}\cdot e^{\sum_i L_i(x_i)}\\
&=\sum_{ij} w_{ij}x_ix_j + \sum_i L_i(x_i) + const
\end{align}
$$

接下来，我们需要考虑为 $\ln p(x, z)$ 相对于 $q$ 求期望，即计算 $E_{-q_j}[\ln p (x, z)]$

我们使用的平均场模型假设了 $q$ 中的独立性，但我们尚未假设 $q$ 的具体形式
考虑到所涉及的变量都是二值变量，我们可以将 $q_i$ 的形式假设为二项分布，因此 $q$ 就是多个二项分布的乘积，即

$$
\begin{align}
q(x;\theta) & = \prod_i q_i(x_i ; \theta_i)\\
&\text{where}\ \  q_i(x_i;\theta_i) = \begin{cases}
\theta_i& \text{if}\ x_i = 1\\
1-\theta_i& \text{if}\ x_i = -1
\end{cases}
\end{align}
$$

其中的各个 $\theta_i$ 就是需要优化求解的参数，在迭代式优化中，我们每次迭代优化一个 $\theta_i$

我们开始考虑 $E_{-q_j}[\ln p (x, z)]$

$$
\begin{align}
E_{-q_j}[\ln p (x, z)] &=E_{-q_j}\left[\sum_{kl} w_{kl}x_kx_l + \sum_k L_k(x_k) + const\right]\\
&=E_{-q_j}\left[\sum_{kl} w_{kl}x_kx_l + \sum_k L_k(x_k) \right] + const\\
&=E_{-q_j}\left[\sum_{k\in neighbor(j)} w_{kj}x_kx_j + \sum_k L_k(x_k) \right] + const\\
&=E_{-q_j}\left[\sum_{k\in neighbor(j)} w_{kj}x_kx_j + L_j(x_j) \right] + const\\
&=E_{-q_j}\left[\sum_{k\in neighbor(j)} w_{kj}x_kx_j\right] +E_{-q_j}\left[ L_j(x_j) \right] + const\\
&=x_j \left(\sum_{k\in neighbor(j)} w_{kj}E_{-q_j}\left[x_k\right]\right) + L_j(x_j) + const\\
&=x_j \left(\sum_{k\in neighbor(j)} w_{kj}(2\theta_k - 1)\right) + L_j(x_j) + const\\
&=x_j \sum_{i\in neighbor(j)} [w_{ij}(2\theta_i - 1)] + L_j(x_j) + const\\
\end{align}
$$

其中第三个等号来源于在迭代优化中除了 $x_j$ 以外其他 $x_k$ 的值认为是已知常量，故 $E_{-q_i}\left[\sum_{k\ne j, l\ne j} w_{kl}x_kx_l\right]$ 与 $x_j$ 无关，归为常量
第四个等号也类似，与 $x_j$ 无关的都直接归为常量

从这个结果可以发现只有在 $p$ 中和 $x_j$ 有直接关系的节点才会对 $q_j$ 的优化有影响

我们令 $m_j = \sum_{i\in neighbor (j)} [w_{ij}(2\theta_i - 1)]$，因此 $E_{-q_j}[\ln p(x, z)] = x_jm_j + L_j(x_j) + const$，故我们希望 

$$q_j(x_j) \propto \exp\left\{E_{-q_j}[\ln p(x, z)]\right\} = \exp\{x_jm_j + L_j(x_j) + const\}$$

不妨设 $q_j (x_j)  = t\cdot \exp\left\{E_{-q_j}[\ln p (x, z)]\right\} = \exp\{x_jm_j + L_j (x_j) + const\}$，其中 $t$ 为任意非零常数，则

$$
\begin{cases}
\theta_j = t\cdot  \exp\{m_j + L_j(1) + const\} & x_j = 1\\
1-\theta_j = t \cdot \exp\{-m_j + L_j(-1) + const\} &x_j= -1
\end{cases}
$$

容易求得

$$
\theta_j = \frac {\theta_j}{1-\theta_j + \theta_j} = \frac {\exp\{m_j + L_j(1) \}}{\exp(-m_j + L_j(-1)) + \exp\{m_j + L_j(1)\}} 
$$

其中 $L_j(x_j) = \ln p(z_j \mid x_j), m_j = \sum_{i\in neighbor (j)} [w_{ij}(2\theta_i - 1)]$

对于 General Structured Variational Inference，推导的过程是类似的，差异仅在于对 $q$ 的分解表示，这依赖于我们具体假设的结构

变分法的缺点：简单的 $Q$ 族可能难以捕获 $P$ 中的复杂模式；复杂的 $Q$ 族则难以优化，容易过拟合

进一步拓展，我们可以不显式规定 $Q$ 的具体形式，而是认为 $Q$ 是某个简单分布 $Z$ (例如高斯) 经过一个泛函转化而来，即 $Q = f(Z)$，其本质思想仍然是用 $Q$ 拟合 $P$ 的变分思想，但该形式显然具有更多的灵活性
我们用神经网络拟合泛函 $f(\cdot)$，显然，网络越复杂，$Q$ 的搜索空间就越大，相较于限制 $Q$ 到某一具体的形式，该方法显然具有更大的灵活性，也不需要我们做出各种显式的假设，但使用这类方法也需要注意过拟合的问题

# 9 Monte Carlo Approximate Inference
Monte Carlo Approximation
基本思想：从分布 $P(\pmb X)$ 中生成样本 $\xi[1], \dots, \xi[M]$，利用平均值估计期望值 $E_p[f(\pmb X)] \approx \frac 1 M \sum_{m=1}^M f(\xi[m])$

该策略能成立的理由来自于频率学派对概率的定义：概率就是频率的极限值

Monte Carlo 近似方法的关键在于采样，或者说如何高效从后验分布 $P(\pmb Y\mid \pmb E = \pmb e)$ 中生成样本


Forward Sampling
利用贝叶斯网络的结构实现从 $P(\pmb X)$ 中生成样本
根据拓扑结构，依次采样


Forward Sampling with Rejection
进一步，前向采样得到的样本满足它们是服从 $P(\pmb X)$ 的独立同分布样本，但在给定观测/证据 $\pmb E = \pmb e$，我们希望从后验分布 $P(\pmb Y \mid \pmb E = \pmb e)$ 中采样，也就是得到的样本是服从 $P(\pmb Y\mid \pmb E = \pmb e)$ 的独立同分布的样本

注意到后验分布 $P(\pmb Y \mid \pmb E =\pmb e) \propto P(\pmb Y, \pmb e)$，因此样本是服从 $P(\pmb Y, \pmb e)$ 的独立同分布样本也满足我们的要求

一个思路就是前向采样 + 拒绝采样，我们利用前向采样从 $P(\pmb X)$ 中得到大量样本，筛选出其中满足 $\pmb E = \pmb e$ 的样本，也就是拒绝 $\pmb E \ne \pmb e$ 的样本
这样留下来的样本，我们认为它们就是服从 $P(\pmb Y , \pmb e)$ 的独立同分布样本，我们用这些样本进行相对于 $P(\pmb Y, \pmb e)$ 的 Monte Carlo 估计

该方法的问题在于如果 $P(\pmb E = \pmb e)$ 的概率很小，生成的样本中满足 $\pmb E = \pmb e$ 的数量往往会很少，拒绝的样本数量会太多，大部分生成的样本都被浪费了

Likelihood Weighting
重新回顾前向采样 + 拒绝采样的过程，我们首先做前向采样，根据 $P(\pmb X)$ 的分解形式 $P(\pmb X) = \prod_i P(X_i\mid \text{Pa}_{X_i})$，随着拓扑结构逐个采样 $X_i$ 的值，最后得到样本
如果该样本不满足 $\pmb E = \pmb e$，则拒绝该样本，否则留下该样本

考虑前向采样过程中对观测变量 $E_j \in \pmb E$ 的采样过程，我们从 CPD $P(E_j\mid \text{Pa}_{E_j})$ 随机采样获得 $E_j$ 的采样值 $e'_j$ 
一个样本不满足 $\pmb E = \pmb e$，意味着在某个或者多个 $E_j$ 的采样过程中，$E_j$ 没有采样得到观测值 $e_j$，也就是 $e'_j \ne e_j$
对于一个样本，在前向采样过程中，其 $E_j$ 采样得到 $e_j$ 值的可能性为 $P(E_j = e_j\mid \text{Pa}_{E_j})$，一个样本要被接受，则其所有的 $E_j$ 都需要采样到正确值，这件事发生的概率是 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$，也就是每个样本都有 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ 的概率被留下

注意，对于每一个样本，因为在采样时对于观测变量 $E_j$ 的父变量 $\text{Pa}_{E_j}$ 的采样值不同，因此 $E_j$ 的 CPD 就不同，故 $P(E_j = e_j \mid \text{Pa}_{E_j})$ 的值在不同的样本之间是不同的
例如，$P(E_j = e_j\mid \text{Pa}_{E_j})$ 在 $\text{Pa}_{E_j}$ 取某特定值 $\alpha$ 的概率会为 $0.8$，取另一特定值 $\beta$ 时的概率则仅为 $0.2$
这一相关性会在我们的前向采样 + 拒绝采样中留下的样本中观察到，例如留下的 $10000$ 个样本中 $\text{Pa}_{E_j}$ 取 $\alpha$ 的占了全部的 80%，$\text{Pa}_{E_j}$ 取 $\beta$ 占了全部的 20%

我们想要改进拒绝采样方法，故我们考虑在采样时通过将 $\pmb E$ 的采样值直接设定为 $\pmb e$ 来让每个样本都直接留下，这样留下的样本中将无法体现这种相关性，因为无论 $\text{Pa}_{E_j}$ 取 $\alpha$ 还是 $\beta$，在统计上，这些样本中的 $P(E_j = e_j \mid \text{Pa}_{E_j})$ 的概率都为 1
为了在这样留下来的样本中体现这种相关性，或者说分布性质，我们应该根据每个样本的 $\text{Pa}_{E_j}$ 值为每个样本加权，权重就是 $P(E_j = e_j \mid \text{Pa}_{E_j})$
此时，在这些留下的样本中考虑权重进行统计，我们仍然能得到 $\text{Pa}_{E_j}$ 取 $\alpha$ 的样本出现的次数比 $\text{Pa}_{E_j}$ 取 $\beta$ 的样本的出现次数约为 80% : 20%

因此，我们在前向采样过程中，直接将观测 $\pmb E$ 设定为 $\pmb e$，使得每个样本都留下，但需要为它乘上它留下的概率 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ ，表示该样本实际仅有 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ 的权重，或者说这个样本不能算作一个样本，而是 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ 个样本


Importance Sampling
从 $Q$ 中采样而不是从 $P$ 中采样
$Q$ 需要满足 $P(x) > 0 \Rightarrow Q(x) > 0$，换句话说，$Q$ 需要是 $P$ 的支撑集，$Q$ 的空间需要覆盖 $P$ 的空间，不允许出现 $P$ 有定义但 $Q$ 没有定义的点

使用重要性采样的一个原因是原分布 $P$ 较复杂，不容易从中采样，这主要针对的是无向图定义的分布，因为无向图定义的分布涉及到了划分函数 $Z$，会使得实际的采样过程变得复杂；对于有向图，大部分情况我们可以使用之前介绍的前向采样和似然加权方法执行采样，一般不需要用到重要性采样
另一个原因时可以用一个 $Q$ 对应多个 $P$，例如 $Q$ 为简单的均匀分布，我们仅编写从均匀分布中进行采样的程序，对于不同的 $P$，我们都从 $Q$ 中采样，然后用重要性采样技术对样本进行加权，模拟从 $P$ 中采样

Unnormalized Importance Sampling
如果 $Q$ 满足 $P(x) > 0 \Rightarrow Q(x) > 0$，我们可以得到以下等价关系

$$
\begin{align}
&E_{P(\pmb X)}[f(\pmb X)]\\
=&\sum_{\pmb x\in Val(\pmb X)}P(\pmb x)f(\pmb x)\\
=&\sum_{\pmb x\in Val(\pmb X)}Q(\pmb x)f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}\\
=&\sum_{\pmb x\in Val(\pmb X)}Q(\pmb x)\left(f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}\right)\\
=&E_{Q(\pmb X)}\left[f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}\right]
\end{align}
$$

因此我们将 $f(\pmb X)$ 相对于 $P$ 的期望转化为了 $f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}$ 相对于 $Q$ 的期望
可以将 $\frac {P(\pmb X)}{Q(\pmb X)}$ 视为权重，对于从 $Q$ 从采样得到的一个样本 $\pmb x$，如果 $P$ 在此处的概率 $P(\pmb x)$ 大于 $Q(\pmb x)$，则该样本的权重相应升高，反之则相应降低

我们从 $Q$ 中采样 $M$ 个样本后，用均值估计期望 $E_{Q(\pmb X)}\left[f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}\right] = E_{P(\pmb X)}[f(\pmb X)]$，即

$$
E_{P(\pmb X)}[f(\pmb X)]\approx \frac 1 M \sum_{m=1}^Mf(\pmb x[m])\frac {P(\pmb x[m])}{Q(\pmb x[m])}
$$

该估计器的方差随着样本数量 $M$ 上升而下降
该估计器的方差也随着 $Q$ 和 $P$ 之间的相似程度而变化，$Q=P$ 时的估计器是方差最小的

Normalized Importance Sampling
未规范化的重要性采样假定分布 $P$ 是已知的
但实践中我们往往仅知道 $P$ 的未规范化的形式 $P'$，$P$ 和 $P'$ 之间相差一个规范化常数 $P = P'/\alpha$ (显然我们可以通过 $\alpha = \sum_{\pmb x}P'(\pmb x)$ 求出规范化常数，但实践中完全遍历概率空间的所有取值往往不可行)

例如，实践中我们往往仅知道先验分布 $P(\pmb X, \pmb e)$，而我们计算关于后验分布 $P(\pmb X \mid \pmb e)$ 的期望，显然先验分布和后验分布之间相差一个规范化常数 $\alpha = P(\pmb e)$，即 $P(\pmb X\mid \pmb e) = P(\pmb X, \pmb e)/P(\pmb e)$

因此，为了得到后验分布，我们需要先得到规范化常数 $\alpha$，我们避免直接计算的大量开销，考虑使用采样的方式对其进行估计
容易知道

$$\alpha = \sum_{\pmb x}P'(\pmb x) = \sum_{\pmb x}P'(\pmb x)f(\pmb x) = E_{P'(\pmb X)}[f(\pmb X)]$$

其中 $f(\cdot)$ 为恒等函数 $f\equiv 1$，因此 $\alpha$ 可以视作恒等函数 $f$ 相对于 $P'$ 的期望值

这样的说法实际上不严谨，因为 $P'$ 作为未规范化的分布，一般并不是一个合法的分布 (如果 $P'$ 合法，则 $\alpha=1$)，但无论如何，我们可以将 $\alpha$ 进一步写为关于 $Q$ 的期望的形式，$Q$ 是合法的采样分布

$$
\begin{align}
\alpha &= \sum_{\pmb x\in Val(\pmb X)}P'(\pmb x)\\
&=\sum_{\pmb x\in Val(\pmb X)}Q(\pmb x)\frac {P'(\pmb x)}{Q(\pmb x)}\\
&=E_{Q(\pmb x)}\left[\frac {P'(\pmb X)}{Q(\pmb X)}\right]
\end{align}
$$

那么我们就可以用重要性采样对该期望值 (也就是 $\alpha$ 的值) 进行估计，故得到

$$
\alpha \approx \frac 1 M\sum_{m=1}^M f(\pmb x[m])\frac {P'(\pmb x[m])}{Q(\pmb x[m])}
$$

除了仅仅估计 $\alpha$ 以外，我们可以在仅知道先验分布的情况下直接考虑用重要性采样估计相对于后验分布的期望值
例如，假设我们需要估计 $E_{P(\pmb X)}[f(\pmb X)]$，我们可以推导出

$$
\begin{align}
E_{P(\pmb X)}[f(\pmb X)]
&=\sum_{\pmb x\in Val(\pmb X)}P(\pmb x)f(\pmb x)\\
&=\sum_{\pmb x\in Val(\pmb X)}\frac {P'(\pmb x)}{\alpha}f(\pmb x)\\
&=\frac 1 \alpha\sum_{\pmb x\in Val(\pmb X)} {P'(\pmb x)}f(\pmb x)\\
&=\frac 1 \alpha\sum_{\pmb x\in Val(\pmb X)} Q(\pmb x)f(\pmb x)\frac {P'(\pmb x)}{Q(\pmb x)}\\
&=\frac 1 \alpha E_{Q(\pmb X)}\left[f(\pmb  X)\frac {P'(\pmb X)}{Q(\pmb X)}\right]\\
&=\frac {E_{Q(\pmb X)}\left[f(\pmb X)\frac {P'(\pmb X)}{Q(\pmb X)}\right]}{E_{Q(\pmb X)}\left[\frac {P'(\pmb X)}{Q(\pmb X)}\right]}
\end{align}
$$

显然目标期望是两个相对于 $Q$ 的期望的商，我们可以通过采样估计分母和分子两个期望，然后将其商作为对目标期望的估计

给定 $Q$ 中的 $M$ 个样本，我们可以用这些样本同时估计分母和分子，即

$$
\begin{align}
E_{P(\pmb X)}[f(\pmb X)]&\approx\frac {\frac 1 M\sum_{m=1}^M f(\pmb x[m])\frac {P'(\pmb x[m])}{Q(\pmb x[m])}}{\frac 1 M\sum_{m=1}^M \frac {P'(\pmb x[m])}{Q(\pmb x[m])}}\\
&=\frac {\sum_{m=1}^M f(\pmb x[m])P'(\pmb x[m])/Q(\pmb x[m])}{\sum_{m=1}^MP'(\pmb x[m])/Q(\pmb x[m])}
\end{align}
$$

Importance Sampling for BNs
我们了解了重要性采样在数学上的形式，接下来考虑如何在贝叶斯网络中应用它

对于一个贝叶斯网络 $\mathcal G$，它定义了分布 $P$，给定 evidence $\pmb E = \pmb e$，我们想要计算某个函数相对于后验分布 $P(\pmb X \mid \pmb e)$ 的期望

根据之前的推导，我们令后验分布 $P(\pmb X \mid \pmb e)$ 为目标分布 $P(\pmb X)$，令先验 $P(\pmb X, \pmb e)$ 为目标分布未规范化的形式 $\tilde P(\pmb X)$，则如果使用重要性采样方法，关于 $P(\pmb X \mid \pmb e)$ 的期望可以用以下式子估计

$$
E_{P(\pmb X)}[f(\pmb X)] \approx \frac {\sum_{m=1}^M f(\pmb x[m])P'(\pmb x[m])/Q(\pmb x[m])}{\sum_{m=1}^MP'(\pmb x[m])/Q(\pmb x[m])}
$$

注意先验分布 $\tilde P(\pmb X) = P(\pmb X, \pmb e)$ 存在以下分解形式

$$
\begin{align}
P(\pmb X, \pmb e)
&=\prod_{X_i\in \pmb E}P(X_i =\pmb e\langle X_i\rangle\mid \text{Pa}_{X_i})\prod_{X_j \not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})\\
&=\prod_{X_i\in \pmb E}P(X_i =e_i\mid \text{Pa}_{X_i})\prod_{X_j \not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})
\end{align}
$$

其中 $e_i$ 表示 $\pmb e\langle X_i\rangle$

我们接着考虑采样分布 $Q$ 的选择
我们将 $\pmb E$ 中的变量和其父节点的连接边全部去除，构造其残缺化的网络 $\mathcal G_{\pmb E = \pmb e}$，其中 $\pmb E$ 中的变量的 CPD 都设定为确定的 CPD，即取到观测值的概率为 1，取其他值的概率为零
我们在 $\mathcal G_{\pmb E = \pmb e}$ 中进行采样，或者说，我们将 $\mathcal G_{\pmb E = \pmb e}$ 定义的分布作为采样分布 $Q$

容易知道 $Q$ 存在以下分解形式

$$
\begin{align}
Q(\pmb X) &= \prod_{X_i\in \mathcal X}P(X_i\mid \text{Pa}_{X_i})\\
&=\prod_{X_i\in \pmb E}P(X_i\mid \text{Pa}_{X_i})\prod_{X_j\not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})\\
&=\prod_{X_i\in \pmb E}\mathbf 1\{X_i=\pmb e\langle X_i\rangle\}\prod_{X_j\not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})\\
&=\prod_{X_i\in \pmb E}\mathbf 1\{X_i=e_i\}\prod_{X_j\not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})
\end{align}
$$

其中第三个等号是因为 $\pmb E$ 中的变量 $X_i$ 的 CPD 都被修改为了确定性分布，而 $\pmb E$ 以外的变量的 CPD 保持不变

接着，假设我们从 $Q$ 中采样了一个样本 $\pmb x$，我们考虑它的重要性权重

$$
\begin{align}
\frac {P'(\pmb x)}{Q(\pmb x)}&=\frac {\prod_{X_i\in \pmb E }P(x_i = e_i\mid \text{Pa}_{X_i})\prod_{X_j\not\in \pmb E}P( x_j\mid \text{Pa}_{X_j})}{\prod_{X_i\in \pmb E}\mathbf 1\{x_i=e_i\}\prod_{X_j\not\in \pmb E}P(x_j\mid \text{Pa}_{X_j})}\\
&=\frac {\prod_{X_i\in \pmb E }P(x_i = e_i\mid \text{Pa}_{X_i})\prod_{X_j\not\in \pmb E}P( x_j\mid \text{Pa}_{X_j})}{\prod_{X_j\not\in \pmb E}P(x_j\mid \text{Pa}_{X_j})}\\
&=\prod_{X_i\in \pmb E }P(x_i = e_i\mid \text{Pa}_{X_i})
\end{align}
$$

其中 $x_i$ 表示 $\pmb x\langle X_i\rangle$，第二个等号是因为从 $Q$ 中采样得到的样本 $\pmb x$ 显然一定满足 $x_i = e_i$

因此，可以发现从残缺化网络执行重要性采样得到的每个样本的重要性权重和似然加权算法中为每个样本赋予的权重是一样的，并且，考察流程发现，似然加权算法的采样流程等价于从残缺化网络中进行采样 (对于观察变量，一定采样到观察值，对于其他变量，则按照其 CPD 正常采样)
因此，从残缺化网络进行重要性采样实际上等价于直接执行似然加权采样算法，反过来说，似然加权采样算法等价于重要性采样方法 (定义一个更容易采样的分布 $Q$，从 $Q$ 中采样，对样本进行赋予重要性权重)


Limitations of Likelihood Weighting and Importance Sampling
似然加权采样方法定义在有向图的前向采样算法之上，因此要应用在无向图时，需要将 MN 先转化为 BN，这会引入很多额外的边，因此会很低效

重要性采样方法中，如果原分布较为复杂，则采样分布 $Q$ 的选择就比较困难，如果 $Q$ 太复杂，则采样本身会低效，如果 $Q$ 太简单，则 $Q$ 和 $P$ 不够相似，收敛效率很低


Boltzmann Distribution
一个分子当前的状态在概率上是由它的上一时刻的状态决定的 (Markov 性质)
对于一个独立且规范 (isolated regular) 的系统，当它达到稳定 (steady) 状态后，系统的状态服从 Boltzmann (Gibbs) 分布
即 $P(X = s) \propto e^{-\frac {E_s}{KT}}$，其中 $X$ 为表示系统状态的随机变量，$s$ 是某个状态，$E_s$ 是该状态的自由能 (自由能都是负值)，根据该式，我们可以知道，状态 $s$ 的自由能越低，状态出现的概率就越高

这一规律来自于物理学中对粒子系统的统计规律，物理试验发现处于稳态的粒子系统的状态服从这一规律
在历史上，物理学家根据数据的历史观测值提出了这一假设，并进行了大量试验验证该假设，试验现象都符合该假设，因此认为该假设无法推翻，进而是成立的
(自然科学的发展就是假设驱动的，即提出假设，设计试验证伪，若无法证伪，则认为假设成立)

这一理论启发我们设计一个 Markov chain，我们将样本视作粒子，将样本的赋值视作其状态，样本 (粒子) 从一个初始赋值 (状态) 开始，沿着 Markov chain 不断演化 (变化状态)，最终样本达到稳态，其赋值的分布服从稳态分布

我们将 Markov chain 的稳态分布设定为我们希望样本的赋值服从的理想分布，也就是 $P$
那么，对于一个样本，我们从一个初始的赋值开始，让该样本的赋值逐渐随着 Markov chain 变化，最终达到稳态后，我们就可以认为该赋值服从稳态分布 $P$，也就是说达到稳态后的赋值就可以被视作从 $P$ 中采样得到的赋值

因此，我们通过对一个样本的赋值在 Markov chain 的不断演化，最终得到了该样本从 $P$ 中采样得到的一次赋值，进而可以直接简单认为该样本是从 $P$ 中采样得到的

该思路和之前的采样方式不同，沿着 Markov chain，每个新样本 (赋值) 都依赖于之前的样本 (赋值)，且各个样本 (赋值) 服从的分布都不同，而之前则是在一个固定的分布中进行独立同分布采样，样本之间相互独立

Markov Chain
一个 Markov Chain 包含了
- 一个状态空间 $Val(\pmb X)$
- 一个转移模型 $\mathcal T(\pmb x \rightarrow \pmb x')$，定义了从状态 $\pmb x$ 转移到 $\pmb x'$ 的概率

根据转移模型，我们可以从 $t$ 时刻的状态分布推导出 $t+1$ 时刻的状态分布

$$
P^{(t+1)}(\pmb X^{(t+1)}= \pmb x') = \sum_{\pmb x \in Val(\pmb X)}P^{(t)}(\pmb X^{(t)} = \pmb x)\mathcal T(\pmb x\rightarrow \pmb x')
$$

显然该式就定义了状态分布是如何沿着 Markov chain 演化的

一个 Markov chain 的稳态分布记作 $\pi(\pmb X)$，达到稳态后，在定义上，分布应该不会再随着 Markov chain 演化/变化，因此 $\pi(\pmb X)$ 应该满足

$$
\pi(\pmb X = \pmb x') = \sum_{\pmb x\in Val(\pmb X)}\pi(\pmb X = \pmb x)\mathcal T(\pmb x \rightarrow \pmb x')
$$

对于一个 Markov chain，如果存在一个数 $k$，使得对于其状态空间的任意一对状态 $\pmb x, \pmb x'\in Val(\pmb X)$，通过 $k$ 步从 $\pmb x$ 到 $\pmb x'$ 的概率大于零，则称该 Markov chain 是 regular 的
一个有限状态的 Markov chain 具有唯一的稳态分布当且仅当它是 regular 的

我们的目标就是设计一个具有唯一的稳态分布 $P(\pmb X\mid \pmb e)$ 的 Markov chain，用该 Markov chain 进行样本演化，实现模拟从 $P(\pmb X\mid \pmb e)$ 进行采样

这个思路就是 Markov Chain Monte Carlo (MCMC) 方法

Markov Chain Monte Carlo
基于前一个粒子，(根据后验分布) 生成当前粒子

为了使得稳态分布是唯一的，我们需要让我们的 Markov chain 为 regular
使得 Markov chain 为 regular 的一个充分条件就是该 Markov chain 满足各态遍历性，也就是其状态空间的所有状态都可以由空间中的任意其他状态在有限步数内转移到

在概率图模型中，以 Markov 网络举例，我们将 Markov 网络定义的 Gibbs 分布作为 Markov chain 的稳态分布，则该 Markov chain 的各态遍历性等价于在图中所有节点都是互通的 (所有的节点在单个子图内连通，而不是多个子图)，并且分布 $P_\Phi$ 中所有的因子/团势能都严格大于零 (所有团势能都严格为正意味着在 Gibbs 抽样的过程中，对于任何配置 $c$ (即网络中所有变量的取值组合)，其联合概率 $P_\Phi(c)$ 也是严格为正的，这就意味着在 Gibbs 链的转移矩阵中，从一个状态转移到另一个状态的概率总是大于零，因为没有哪个状态会被赋予零概率，根据定义，由于从任何一个状态到任何一个其他状态的概率都不为零，因此转移矩阵的任意次数的幂都将包含正值，也就是说，Gibbs 抽样马尔可夫链是规则的 )

Gibbs Sampling
Gibbs Sampling 是一类 MCMC 方法，其 Markov chain 的描述如下
状态空间为 $Val(\pmb X)$ 中和证据 $\pmb e$ 一致的部分，或者说对 $\pmb X$ 的和证据 $\pmb e$ 一致的赋值，其中 $\pmb X = \{X_i\}_{i=1,\dots, n}$
转移模型通过我们所需要的稳态分布 (后验分布) 定义，即

$$
\mathcal T(\pmb X^{(t)} = \pmb x \rightarrow \pmb X^{(t+1)} = \pmb x') = P(\pmb X^{(t+1)} =\pmb x'\mid \pmb X^{(t)} = \pmb x,\pmb e)
$$

也就是 $\pmb x$ 转移到 $\pmb x'$ 的概率定义为 $\pmb x'$ 条件于 $\pmb x, \pmb e$ 的后验概率，或者说 $\pmb X^{(t)}$ 转移到 $\pmb X^{(t+1)}$ 的概率是条件概率分布 $P(\pmb X^{(t+1)}\mid \pmb X^{(t)}, \pmb e)$
其中 $P(\pmb X \mid \pmb e)$ 是我们想要的稳态分布

注意以上写法实际上是一种简写，完整的一次转移 $\mathcal T (\pmb x \rightarrow \pmb x')$ 实际上是由多个连续的 kernel $\mathcal T_i(i=1,\dots, k)$ 组成的，其中每个 $\mathcal T_i$ 仅改变变量 $x_i$ 的状态，保持其他变量的状态不变，其定义为

$$
\mathcal T_i((\pmb x_{-i}, x_i)\rightarrow(\pmb x_{-i}, x'_i)) = P(x_i'\mid \pmb x_{-i}, \pmb e)
$$

因此 $\mathcal T$ 实际上的定义应该展开为

$$
\begin{align}
\mathcal T(\pmb x\rightarrow \pmb x') &=\mathcal T_1((\pmb x_{-1}, x_1)\rightarrow (\pmb x_{-1},x_1')) \mathcal T_2((\pmb x_{-\{1,2\}},x_1',x_2)\rightarrow  (\pmb x_{-\{1,2\}},x_1',x_2'))\cdots\\
&=\prod_{i=1}^k \mathcal T_i((\pmb x_{-\{1,\dots,i-1\}},x'_1,\cdots,x'_{i-1},x_i)\rightarrow (\pmb x_{-\{1,\dots,i-1\}},x_1',\cdots,x_{i-1}',x_i'))\\
&=\prod_{i=1}^k P(x_i' \mid x_1',\cdots,x_{i-1}',\pmb x_{-\{1,\dots, i-1\}}, \pmb e)\\
&=\prod_{i=1}^k P(x_i' \mid x_1',\cdots,x_{i-1}',x_{i+1},\cdots, x_k, \pmb e)\\
\end{align}
$$

完成所有 kernel $\mathcal T_i$ 的转移即完成 $\mathcal T$ 的一次转移

我们需要证明，这样定义的 Markov chain 的稳态分布确实是 $P(\pmb X\mid \pmb e)$，并且是唯一的，那么我们就可以确认在这样定义的 Markov chain 进行 MCMC 过程最终会得到服从 $P(\pmb X\mid \pmb e)$ 的样本

为此，我们考察

$$
\begin{align}
&\sum_{\pmb x \in Val(\pmb X)} P(\pmb X = \pmb x\mid \pmb e)\mathcal T(\pmb x \rightarrow \pmb x')\\
=&\frac 1 {P(\pmb e)}\sum_{\pmb x \in Val(\pmb X)} P( \pmb x, \pmb e)\mathcal T(\pmb x \rightarrow \pmb x')\\
=&\frac 1 {P(\pmb e)}\sum_{x_k}\cdots \sum_{x_1}P(\pmb x, \pmb e) \mathcal T(\pmb x \rightarrow \pmb x')\\
=&\frac 1 {P(\pmb e)}\sum_{x_k}\cdots \sum_{x_1}P(\pmb x, \pmb e) \prod_{i=1}^k P(x_i' \mid x_1',\cdots,x_{i-1}',x_{i+1},\cdots, x_k, \pmb e)\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[\sum_{x_2}\left[\sum_{x_1} P(x_1,x_2,\cdots, x_k,\pmb e) P(x_1'\mid x_2,\cdots ,x_k,\pmb e)\right]P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[\sum_{x_2}\left[ P(x_2,\cdots, x_k,\pmb e) P(x_1'\mid x_2,\cdots ,x_k,\pmb e)\right]P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[\sum_{x_2} P(x_1', x_2,\cdots ,x_k,\pmb e)P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[ P(x_1',x_3,\cdots ,x_k,\pmb e)P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[ P(x_2', x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\cdots\\
=&\frac 1 {P(\pmb e)}P(x'_k, \cdots, x'_1, \pmb e)\\
=&\frac {1}{P(\pmb e)}P(\pmb x', \pmb e)\\
=&P(\pmb x' \mid \pmb e)
\end{align}
$$

因此，分布 $P(\pmb X \mid \pmb e)$ 经过 $\mathcal T$ 转移以后，仍然是 $P(\pmb X \mid \pmb e)$，故该分布就是该 Markov chain 的稳态分布

确定了 $P(\pmb X \mid \pmb e)$ 是该 Markov chain 的稳态分布后，我们考察该 Markov chain 是否具有唯一的稳态分布
Markov chain 具有唯一稳态分布的一个充分条件是各态遍历性，即状态空间内任意两个状态都在有限步数内互相可达，而 $P_\Phi$ 中所有团势能都严格大于零可以保证这一条件成立
因此对于 Bayesian 网络，要求就是所有的 CPDs 都严格为正；对于 Markov 网络，要求就是所有 clique potential 严格为正

我们进一步考虑每一个 kernel $\mathcal T_i$ 的表示，我们知道 $\mathcal T_i$ 写为 
(不考虑证据，考虑证据时情况是类似的；
这里我们将 $x_j'$ 写为 $x_j^{(t+1)}$，将 $x_j$ 写为 $x_j^{(t)}$)

$$
\begin{align}
\mathcal T_i(x_i^{(t+1)}) &= P(x_i^{(t+1)} \mid x_1^{(t+1)},\cdots,x_{i-1}^{(t+1)},x_{i+1}^{(t)},\cdots, x_k^{(t)})\\
\end{align}
$$

这是关于变量 $X_i$ 的后验概率分布，其中 $P = P_\Phi$ 是 Markov 网络定义的 Gibbs 分布
我们可以利用 Markov 网络编码的独立性，简化这一后验分布，容易知道，给定变量 $X_i$ 的 Markov blanket，变量 $X_i$ 和网络中所有其他变量条件独立，因此，$\mathcal T_i(X_i)$ 可以进一步简化为

$$
\mathcal T_i(X_i) = P(X_i\mid \text{MB}(X_i))
$$

因此，$\mathcal T_i(X_i = x_i^{(t+1)}) = \mathcal T_i(x_i^{(t+1)})$ 写为

$$
\mathcal T_i(x_i^{(t+1)})  = P(x_i^{(t+1)} \mid X_j = x_j^{T})
$$

其中 $X_j \in \text{MB}(X_i)$，当 $j > i$ 时，$T = t$，当 $j < i$ 时，$T = t-1$

注意 Gibbs 对于所有的分布都适用，也就是目标稳态分布 $P$ 可以是任意分布，我们在这里是利用 $P$ 的图表示对采样过程进行了简化 (利用 $P$ 中的独立性简化条件概率分布)

Gibbs Sampling for BNs
我们考虑对 BN 进行 Gibbs 采样，在 Markov chain 的一步转化中，我们考虑化简后验分布 $P(X_i \mid X_1, \cdots, X_{i-1}, X_{i+1}, \cdots, X_n)$
将 $X_i$ 的子节点记作 $\pmb Y = \{Y_1, \cdots, Y_k\}$，用 $X_i$ 的 Markov blanket  (给定一个节点的 Markov blanket，它和网络中所有其余节点条件独立) 将该分布化简为

$$
\begin{align}
P(X_i \mid X_1, \cdots, X_{i-1}, X_{i+1},\cdots, X_n) &= P(X_i\mid \text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y} -\{X_i\})\\
&=\frac {P(X_i, \text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y}-\{X_i\})}
{P(\text{Pa}_{X_i},\pmb Y, \text{Pa}_{\pmb Y}- \{X_i\})}\\
&\approx P(X_i, \text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y}-\{X_i\})\\
&=P(\text{Pa}_{X_i})P(X_i,\pmb Y, \text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})\\
&=P(\text{Pa}_{X_i})P(\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})P(X_i,\pmb Y\mid \text{Pa}_{\pmb Y}-\{X_i\},\text{Pa}_{X_i})\\
&=P(\text{Pa}_{X_i})P(\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})P(X_i\mid \text{Pa}_{\pmb Y}-\{X_i\},\text{Pa}_{X_i})P(\pmb Y\mid \text{Pa}_{X_i}, \text{Pa}_{\pmb Y}- \{X_i\},X_i)\\
&=P(\text{Pa}_{X_i})P(\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})P(X_i\mid \text{Pa}_{X_i})P(\pmb Y\mid \text{Pa}_{\pmb Y}- \{X_i\},X_i)\\
&\approx P(X_i\mid \text{Pa}_{X_i})P(\pmb Y\mid\text{Pa}_{\pmb Y})
\end{align}
$$

其中第三行的 $\approx$ 是因为 $P(\text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y}- \{X_i\})$ 中各个变量的取值在此时都是固定的，与 $X_i$ 的取值无关，因此视为常数
其中最后一行的 $\approx$ 是也是同理， $P (\text{Pa}_{X_i}) P (\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})$ 其中各个变量的取值此时都是固定的，与 $X_i$ 的取值无关，因此视为常数

因此，在对 $X_i$ 进行采样时，我们直接定义 $\tilde P(X_i) = P(X_i \mid \text{Pa}_{X_i}) P(\pmb Y \mid \text{Pa}_{\pmb Y})$，我们为每个 $X_i$ 的取值 $x_i$ 计算 $\tilde P(X_i = x_i)$，最后通过归一化得到 $X_i$ 此时真实的后验分布

$$
P(X_i) = \frac {\tilde P(X_i)}{\sum_{x_i} \tilde P(X_i = x_i)}
$$

然后根据 $P(X_i)$ 进行采样即可

Gibbs Sampling for MNs
推导类似，见 [[book-notes/Probabilistic Graphical Models-Principles and Techniques|Probabilistic Graphical Models-Principles and Techniques]] 中 eq (12.23)

Gibbs Sampling for HMM
Initialization：
根据先验生成标签序列，即对于所有的 $1\le t \le T$，根据 $\pi$ ，生成 $Y_t = y_i$

Sampling：
不断地随机重新生成 $Y_t$

$$
\begin{align}
&P(y_t \mid Y, X, \theta)\\
=&P(y_t \mid y_{t-1}, y_{t+1}, x_t, \theta)\\
=&\frac {P(y_t, y_{t-1}, y_{t+1}, x_t, \theta)}{P(y_{t-1},  y_{t+1}, x_t, \theta)}\\
=&\frac {P(y_t,y_{t-1},y_{t+1},x_t,\theta)}{P(y_{t+1}, x_t \mid y_{t-1}, \theta)P(y_{t-1}, \theta)}\\
=&\frac {P(y_t,y_{t+1},x_t\mid y_{t-1},\theta)}{P(y_{t+1},x_t\mid y_{t-1},\theta)}\\
=&\frac {P(y_{t+1},x_t\mid y_t, y_{t-1}, \theta)P(y_t \mid y_{t-1}, \theta)}{P(y_{t+1},x_t\mid y_{t-1}, \theta)}\\
=&\frac {P(y_{t+1},x_t\mid y_t, y_{t-1}, \theta)}{P(y_{t+1},x_t\mid y_{t-1}, \theta)}P(y_t \mid y_{t-1}, \theta)\\
\propto& P(y_{t+1}\mid y_t,\theta)P(x_t\mid y_t, \theta)P(y_t\mid y_{t-1}, \theta)\\
=&t_{y_t, y_{t+1}}e_{y_t, e_t}t_{y_{t-1},y_t}
\end{align}
$$

Estimating:
使用样本的估计值更新 HMM 模型的 trainsition 和 emission 概率

$$
\begin{align}
t_{i\rightarrow j}&=\frac {\sum_{t=1}^T \mathbf 1(Y^{t+1} = j, Y^t = i)}{\sum_{t=1}^T\mathbf 1(Y^t = i)}\\
e_{i\rightarrow k}&=\frac {\sum_{t=1}^T\mathbf 1(Y^t = i, X^t = k)}{\sum_{t=1}^T\mathbf 1(Y^t = i)}
\end{align}
$$

Metropolis-Hasting Algorithm
Gibbs 采样在设计 Markov chain 的 transition kernel 时，是依据目标分布 $P$ 设计的，在生成 Markov chain 的下一个样本时，需要依据基于 $P$ 的后验分布进行采样

MH 算法希望从任意分布生成下一个样本，也就是 transition kernel 可以是任意分布 $Q$ 而不是某个特定分布，并且通过定义接受概率使得即便以任意分布作为 transition kernel，也可以使得 Markov chain 的稳态概率为 $P$

MH 将 Markov chain 的 transition kernel 设计为如下形式

$$
\begin{align}
\mathcal T(\pmb x\rightarrow \pmb x')&=\mathcal T^Q(\pmb x \rightarrow \pmb x')\mathcal A(\pmb x \rightarrow \pmb x')\quad \pmb x\ne \pmb x'\\
\mathcal T(\pmb x \rightarrow \pmb x) &=\mathcal T^Q(\pmb x\rightarrow \pmb x)\sum_{\pmb x' \ne \pmb x}\mathcal T^Q(\pmb x\rightarrow \pmb x')(1-\mathcal A(\pmb x\rightarrow \pmb x'))
\end{align}
$$

直观上说，在执行 transition 时，假设当前状态为 $\pmb x$，下一个状态为 $\pmb x'$，考虑 $P(\pmb x')/ P(\pmb x)$ 和 $Q(\pmb x \rightarrow \pmb x')/Q(\pmb x' \rightarrow \pmb x)$ ，前者表示在 $P$ 中 $\pmb x'$ 相较于 $\pmb x$ 的出现概率的比例，后者表示在 $Q$ 中从 $\pmb x \rightarrow \pmb x'$ 的转移概率相较于 $\pmb x'\rightarrow \pmb x$ 的转移概率的比例
如果前者大于后者，说明在 $P$ 中 $\pmb x$ 到 $\pmb x'$ 的转移倾向比在 $Q$ 中更强烈，此时接受概率 $\mathcal A(\pmb x\rightarrow \pmb x')$ 就会比较高 (最大为 1)，让 $\pmb x$ 尽快转移
如果后者大于前者，说明在 $Q$ 中 $\pmb x$ 到 $\pmb x'$ 的转移倾向比在 $P$ 中更强烈，此时接受概率 $\mathcal A(\pmb x \rightarrow \pmb x')$ 就会比较低，使得 $\pmb x$ 更大可能保留在原状态，以补足这一差异 
因此转移分布 $Q$ 和接受概率 $\mathcal A$ 一起定义的 kernel 也可以最终收敛到稳态分布 $P$

接受概率可以由转移分布 $\mathcal T^Q$ 和稳态分布 $\pi(\pmb x)$ 推导得到，根据细致平衡条件，$\mathcal T(\pmb x\rightarrow \pmb x') = \mathcal T^Q(\pmb x\rightarrow \pmb x')\mathcal A(\pmb x\rightarrow \pmb x')$ 应该满足

$$
\pi(\pmb x)\mathcal T^Q(\pmb x\rightarrow \pmb x')\mathcal A(\pmb x\rightarrow \pmb x') = \pi(\pmb x')\mathcal T^Q(\pmb x'\rightarrow \pmb x)\mathcal A(\pmb x'\rightarrow \pmb x)
$$

故

$$
\frac {\mathcal A(\pmb x\rightarrow \pmb x')}{\mathcal A(\pmb x' \rightarrow \pmb x)} = \frac {\pi(\pmb x')\mathcal T^Q(\pmb x'\rightarrow \pmb x)}{\pi(\pmb x)\mathcal T^Q(\pmb x\rightarrow \pmb x')}
$$

因此可以将接受概率定义为如下形式，以满足上述方程

$$
\mathcal A(\pmb x\rightarrow \pmb x') = \min\left[1, \frac {\pi (\pmb x')\mathcal T^Q(\pmb x' \rightarrow \pmb x)}{\pi(\pmb x)\mathcal T^Q(\pmb x\rightarrow \pmb x')}\right]
$$

MH Algorithm for Continuous Probability
在目标分布和提案分布是连续分布的情况下，以上的推导也成立，我们要做的就是将概率替换为概率密度函数

连续情况下，常用的提案分布是多元高斯分布，即我们令 $\mathcal T(\pmb x \rightarrow \pmb x') = \mathcal N(\pmb x';\pmb x, \Sigma)$
我们的目标分布为 $\pi(\pmb x) = p(\pmb x)$，则接受概率定义为

$$
\mathcal A(\pmb x\rightarrow\pmb  x') = \min\left[1, \frac {\pi(\pmb x')\mathcal T(\pmb x' \rightarrow \pmb x)}{\pi(\pmb x)\mathcal T(\pmb x\rightarrow \pmb x')}\right]
$$

因为多元高斯分布是各项同性的，故 $\mathcal N(\pmb x; \pmb x', \Sigma) = \mathcal N(\pmb x'; \pmb x, \Sigma)$，因此接受概率可以简化为

$$
\mathcal A(\pmb x\rightarrow \pmb x') = \min\left[1, \frac {\pi(\pmb x')}{\pi(\pmb x)}\right] = \min\left[1, \frac {p(\pmb x')}{p(\pmb x)}\right]
$$

A Drawback of MCMC Algorithm
Inefficient trainsitions between different states
有时即便我们的分布满足各态遍历性，但有些状态出现的概率非常低，此时我们的 Markov Chain 往往不容易出现这些状态，链上的状态往往聚集在一个互相较为相通的状态子集，也就是一个峰上，具体是哪一个峰就和我们的初始状态相关，不同的初始状态显然会带来差异较大的状态集合
显然，在这种情况下我们采样得到的状态实际上是高度相关的，它们主要都来自于原分布的一个峰，不能视作从完整的原分布中采样，此时用这些样本估计相对于原分布的期望往往是偏差较大的

Simulated Annealing for Ergodicity
在多数应用情况下，目标分布往往采用指数族表示，即 $\tilde P \propto \exp(-U(\phi))$
那么如果 $-U(\phi)$ 有多个锐利的峰，采样过程就有可能被困在某个局部，我们掉入这个局部之后，就几乎采样不到另一个峰的值

为了避免陷入局部，我们希望在采样最开始的时候就尽量转移到尽可能多的不同状态，获得一点全局性质

因此我们希望将峰进行压缩，故我们引入了温度参数 $t$，此时目标分布仍满足 $\tilde P\propto \exp(-\frac 1 t U(\phi))$，但注意此时的 Markov Chain 的稳态分布显然不会是目标分布
在最开始的时候，温度参数较大，此时 $\exp(-\frac 1 tU(\phi))$ 的形状较为平滑，此时的采样就较为容易地在多个峰之间转移，以保证 Markov Chain 可以得到各态遍历，随着时间增长，我们再缓慢地将温度参数降到 1
当然，如果我们仅有一条 Markov Chain，最后得到的样本大概率还是集中在一个峰，但如果我们有多条 Markov Chain，则我们就不需要担心初始值的设计，因为这些 Markov Chain 在一开始温度较高的时候会各自随机转移到不同的状态，随着之后温度下降，它们就各自收敛到不同的峰，最后我们将各个 Markov Chain 的部分样本都收集起来，作为总体的样本，就较为平均

模拟退火的思路来自于炼钢，在炼钢时，不同的金属在低温下的属性大不相同，因此我们在开始的时候先给出较大的温度，使得金属的属性可以进行变化，之后缓慢地降温，最后得到具有理想属性的金属

(Hybrid) Hamiltonian Monte Carlo
Hamiltonian 算子
一个动力学系统 (不考虑热力学) 包含了两部分能量，一部分为势能 (potential energy)，另一部分是动能 (kinetic energy)，势能 + 动能 = 总能量，系统的总能量守恒

例如，一个处在光滑斜坡上的小球，它的高度决定了它的势能，将它推下来，它的势能将转化为动能，动能体现在它的速度上

Main idea of HMC algorithm
系统的总势能就是目标分布的势能 (Gibbs 分布的能量函数)
我们为系统的变量集合人为定义动能

根据能量守恒，势能小的地方动能大，势能大的地方动能小

在采样过程中，当样本处在概率较大的峰中，它就容易被困在这个局部，此时我们希望它有较大的动能，以脱离这个峰；当样本处在概率较小的地方时，我们希望它的动能较小，因为这些地方的精细变化会对我们的结果产生较大影响，我们希望这个地方有较为充足的样本，以便更清楚地刻画这个地方

Hamiltonian
在量子物理学中，Hamiltonian 是一个对应于系统总能量的算子

$$
\mathcal H(s, \phi) = E(s) + K(\phi) =E(s) + \frac 1 2\phi^2
$$

其中 $s$ 表示系统当前的状态，$E(s)$ 表示该状态的势能；$\phi$ 表示系统的速度，$K(\phi)$ 表示系统的动能，我们假设系统质量为 1，故动能直接写为 $\frac 1 2 \phi^2$

HMC 中，我们通过模拟物理系统来建模样本，我们将样本视作一个微粒系统，其中每个变量表示系统中的一个微粒，假设微粒的质量都为 1，则我们有

$$
\mathcal H(s, \phi) = E(s) + K(\phi) = E(s) + \frac 1 2\sum_i \phi_i^2 
$$

其中我们将 $E(s)$ 认为是目标分布 $P$ 的势能，也就是其势能函数 $U(s)$

显然，当当前状态 $s$ 的势能越大，其概率就越小 ($P(s)\propto \exp(-U(s))$)，根据能量守恒，此时其动量较小，因此移动得较慢
而当当前状态的势能较小时，其概率就越大 (能量越低，系统状态越稳定，因此其出现的概率就大)，此时其动量较大，因此移动得较快

Hamiltonian equation 告诉我们

$$
\begin{align}
\frac {ds_i}{dt} &= \frac {\partial \mathcal H}{\partial \phi_i} = \phi_i\\
\frac {d\phi_i}{dt}&=-\frac {\partial \mathcal H}{\partial s_i} = -\frac {\partial E}{\partial s_i}
\end{align}
$$

HMC 算法将时间步分为多个时间片，每一步时间步上的更新执行的是多步更新，即采用 Leap-Frog 算法

Leap-Frog Algorithm: Inner loop
$\phi_i(t + \epsilon/2) = \phi_i(t) - \frac {\epsilon}2 \frac {\partial }{\partial s_i}E(s(t))$
$s_i(t + \epsilon) = s_i(t) + \epsilon \phi_i(t + \epsilon /2)$
$\phi_i(t + \epsilon) = \phi_i(t + \epsilon/2) - \frac {\epsilon}{2}\frac {\partial }{\partial s_i}E(s(t + \epsilon))$

HMC 的状态转移步骤为
从单元高斯分布中采样得到一个新的速度
执行 $n$ 步的 Leap-Frog，得到新的状态 $x'$
根据 MH 算法执行一次接受/拒绝采样 (使用 MH 算法是因为 Leap-Frog 定义的 Markov Chain 的稳态分布不是我们期待的稳态分布，也就是说可以将 leap-Frog 视作提案分布的 Markov Transition，故我们需要进行 MH 校正)

MH 的接受概率定义为

$$
\mathcal A(\pmb x\rightarrow \pmb x') = \min\left[1, \frac {\exp(-\mathcal H(s', \phi'))}{\exp(-\mathcal H(s, \phi))}\right]
$$

Langevin Monte Carlo
Langevin 动力学描述的是进行布朗运动的系统的动态，也就是受一个随机的力的系统的动态
它和 Hamiltonian 的差异在于它考虑了加速度 (二阶项)，而 Hamiltonian 仅考虑了一阶项

上述 MCMC 算法的变体的思想都是先通过一些技巧期望尽量达到状态之间的各态遍历，随后再用 MH 算法将 Markov Chain 的稳态分布修正回去

MCMC in Practice
实践中，需要等待 Markov Chain 的 burn-in time 结束才能接受样本，也就是等待 Markov Chain 进入稳态
但注意 burn-in time 不好确定，也没有理论保证
同时注意同一条 Markov Chain 上得到的样本是互相关联的，因此前面的方法都是需要应用于多条链，只有单条链总是无法避免地进入局部

Sampling Strategy
因此，收集 Markov Chain 的样本时，我们的策略有
Strategy 1
运行 $M$ 条链，各自的初始状态不同，每条链走 $N$ 步，最后取每条链的最后一个状态作为样本，这样就得到 $M$ 个 IID 样本

Strategy 2
只运行一条链，等待 burn-in 后，取 $M$ 个样本

Hybrid Strategy
运行多条链，从每条链采样部分样本

MAP Inference
概率推断 (Probability Inference)的目的是计算分布 $P(\pmb Y)$ 或 $P(\pmb Y \mid \pmb e)$

极大后验推断 (MAP) 的目的是求出能最大化后验的点估计，即
$MAP(\pmb Y \mid \pmb e) = \arg\max_{y\in Val(\pmb Y)}P(\pmb y, \pmb e)$

# 10 Parameter Learning with Complete Data
对于概率模型的学习，我们有两个视角
Learning as optimization - 找到能最大化给定目标函数的概率模型
Learning as probabilistic inference - 将参数视作随机变量，基于观测到的数据推断它们的后验概率

Learning as optimization
基于优化的学习中，对于概率模型，我们主要用的是以下两种方法
Maximum likelihood - $\pmb \theta^* = \max_{\pmb \theta}P(x[1], x[2], \dots, x[M]\mid \pmb \theta)$
Maximum a posterior - $\pmb \theta^* = \max_{\pmb \theta} P(\pmb \theta \mid x[1], x[2], \dots, x[M])$

基于优化的学习关键在于基于不同的准则定义损失或似然函数，然后对其进行优化

Learning as probabilistic inference
基于概率推断的学习中，我们在概率框架下学习
基于概率推断的学习主要有两种分类
Parameter Learning: $\{x[m]\}_{m=1\sim M}\mid \mathcal G \rightarrow P(\theta \mid \mathcal D)$
Structure Learning: $\{x[m]\}_{m=1\sim M} \rightarrow P(\mathcal G, \theta\mid \mathcal D)$

其中参数学习是在已知图结构下学习参数，结构学习则是同时学习结构和参数

在建模时，我们也有两种选择
Generative models - 学习联合分布 $\tilde P(Y, X\mid \theta)$
Discriminative models - 学习条件分布 $P(Y\mid X = x,\theta)$

Learning Basics: IID Samples
在学习中，我们总是假设样本是 IID (independent and identical distributed)
IID 意味着样本之间的生成是互不关联的

Learning Basics: Avoid Overfitting
模型的复杂度远大于训练数据的复杂度时，模型可以很容易学习到零的经验误差，即过拟合
为了提高泛化能力，减少过拟合，一个重要的方法就是对模型的复杂度进行惩罚，即添加正则化项

在理论上，提高泛化的方法有 Bayesian Learning, Statistical Learning Theory: structural risk, Sparsity, Low-rank 
其中稀疏性和低秩的理论都是压缩感知

在经验上，提高泛化的方法有交叉验证 (留一交叉验证、N 折交叉验证)、0.632 bootstrapping ($0.632 = 1-e^{-1}$)

0.632 Bootstrapping
对于已经有的 $M$ 个样本 $x[1], \dots, x[M]$，我们从中进行有放回采样，随机采样出另外 $M$ 个样本作为训练集，原来 $M$ 个样本中没有被采样得到的样本就作为测试集样本
平均来说，原来的样本中会有 $1-e^{-1} = 0.632$ 的样本会被采集到训练集，剩余的 $0.368$ 样本会被作为测试集
我们在训练集上学习模型，在测试集上测试模型

重复以上的步骤多次，我们检查训练的 performance 和测试的 performance 的差异，以及各自的方差，如果二者差异不大，说明模型没有过拟合

IID Assumption for Learning
IID 假设即要求给定参数时，各个样本之间条件独立，即
$P(x[1], \dots, x[M]\mid \theta) = P(x[1]\mid \theta)\cdots P(x[m]\mid \theta)$

Maximum Likelihood Parameter Estimation
似然直接描述了给定特定参数的情况下，数据出现的概率，进而间接描述了给定数据的情况下，我们对于特定参数取值的信心或者概率，即
$\ell(\theta: \mathcal D) = P(\mathcal D \mid \theta)\propto \tilde P(\mathcal D\mid \theta)$
我们常用对数似然
$\ell(\theta: \mathcal D) \propto \log \tilde P(\mathcal D\mid \theta) = \log\prod_i \tilde P(x[i]\mid \theta) = \sum_i \log \tilde P(x[i]\mid \theta)$

MLE 的目标是能找到对于训练集能最大化其似然的参数 $\theta^*$

Gradient-Based Methods
当似然函数较为复杂，难以直接求得参数的闭式解时，我们会考虑梯度上升/下降法来对参数进行优化，即
$\theta^{(k+1)}  = \theta^{(k)} + \lambda^{(k)}\nabla \ell(\theta; x)$

如果似然函数为凸函数，则梯度上升法一定会收敛到最优解

在多数情况下，每个 $\theta_i$ 之间是相互独立的，因此在优化时，我们可以采取坐标上升法，每次计算关于 $\theta_i$ 的偏导，仅优化 $\theta_i$

Stochastic Gradient Ascent
朴素的梯度上升法中，参数参照它完整的似然函数的梯度进行更新，计算完整的似然函数需要涉及到完整的训练集，也就是需要综合考虑全部样本的似然

因为这样的开销过大，故随机梯度上升每次仅考虑一个样本的似然，即
$\theta^{(k+1)}  = \theta^{(k)} + \lambda^{(k)} \nabla l_{m_k}(\theta, x[m_k])$

随机梯度上升和朴素梯度上升的差异在于：朴素梯度上升每一步考虑的都是最优的更新方向，因此收敛的步数一定更少，但是每一步的计算量大；随机梯度上色后给你每一步不是最优的更新方向，因此收敛的步数会多，但是每一步的计算量大大减少

对于凸的目标函数，随机梯度上升也一定会收敛

随机梯度上升中，每一次 epoch 中样本的顺序是随机的
为了避免取到局部极小，也需要使用启发式方法，例如模拟退火 (学习率一开始高，随着时间逐渐减小) 或遗传算法

Bayesian Parameter Estimation
贝叶斯模型用于预测时，我们有

$$
\begin{align}
&P(X^{n+1}\mid X^1, \dots, X^n) \\
=&\frac {P(X^1, \dots, X^n, X^{n+1})}{P(X^1, \dots, X^n)} \\
=&\frac {1}{P(X^1, \dots, X^n)}P(X^1, \dots, X^n, X^{n+1}) \\
=&\frac {1}{P(X^1, \dots, X^n)}\int P(X^1, \dots, X^n, X^{n+1},\theta) d\theta\\
=&\frac {1}{P(X^1, \dots, X^n)}\int P(X^1, \dots, X^n, X^{n+1}\mid \theta)P(\theta) d\theta\\
=& \frac {1}{P(X^1, \dots, X^n)}\int P(X^{n+1}\mid \theta)P(X^1, \dots, X^n\mid \theta)P(\theta)d\theta
\end{align}
$$

Bayesian 参数估计中，参数的后验概率可以写为

$$
\begin{align}
P(\theta \mid X) &= \frac {P(X, \theta)}{P(X)}\\
&= \frac {P(X, \theta)}{\int P(X, \theta)d\theta}\\
&= \frac {P(X\mid \theta)P(\theta)}{\int P(X\mid \theta)P(\theta)d\theta}
\end{align}
$$

可以看到后验概率正比于先验和似然的乘积

没有先验信息时，常用的先验是 $[0,1]$ 之间的均匀分布

Priors: Beta Distribution
对于二项式来说，常用的先验是 Beta 分布

$$
\theta \sim Beta(\alpha_1, \alpha_0)\quad\text{if}\ p(\theta) = \gamma\theta^{\alpha_1 - 1}(1-\theta)^{\alpha_0 - 1}
$$

其中 $\gamma = \Gamma(\alpha_1 + \alpha _0)/\Gamma(\alpha_1)\Gamma(\alpha_0)$，是规范化常数
伽马函数对于整数定义为 $\Gamma(n) = (n-1)!$

Beta 分布和服从二项式分布随机变量的似然共轭，也就是说，当似然函数 $P(X\mid \theta)$ 的形式为二项式分布似然的形式 $\theta^{M[1]}(1-\theta)^{M[0]}$，且先验为 Beta 分布 $p(\theta) = \gamma \theta^{\alpha_1 - 1}(1-\theta)^{\alpha_0 - 1}$，则此时可以计算得到后验分布也是 Beta 分布，其超参数为 $M[1] + \alpha_1 , M[0] + \alpha_0$，即 $P(\theta \mid X) = Beta(M[1] + \alpha_1, M[0] + \alpha_0)$

当先验为 Beta 分布，在预测时，我们可以计算得到 

$$
\begin{align}
&P(x[m+1] = 1\mid X) \\
=&\int P(x[m+1] = 1, \theta\mid X)d\theta\\
=& \int P(x[m+1]\mid \theta ,X)P(\theta \mid X)d\theta\\
=& \int P(x[m+1]\mid \theta )P(\theta \mid X)d\theta\\
\end{align}
$$

其中 

$$
\begin{align}
P(\theta \mid X)&\propto P(X\mid \theta) P(\theta) \\
&= \theta^{M[1]}(1-\theta)^{M[0]}\gamma\theta^{\alpha_1-1}(1-\theta)^{\alpha_0 -1}\\
&\propto \theta^{M[1]}(1-\theta)^{M[0]}\theta^{\alpha_1-1}(1-\theta)^{\alpha_0 -1}\\
&= \theta^{M[1]+\alpha_1-1}(1-\theta)^{M[0]+\alpha_0-1}
\end{align}$$

故经过计算可以得到

$$
\begin{align}
&P(x[m+1] = 1\mid X) \\
=&\int \theta P(\theta \mid X)d\theta\\
=& \frac {M[1] + \alpha_1}{M+\alpha}\\
\end{align}$$

因此使用均匀分布做先验时，等价于使用 $Beta(1, 1)$ 作为先验，此时
$P(x[m+1] + 1\mid X) = (M[1] + 1)/(M+2)$
这被称为拉普拉斯修正

Priors: Latent Dirichlet Distribution
Beta 仅适用于二项式分布，带有一个独立参数，二项式分布的参数满足 $\theta_1 + \theta_2 = 1$
Dirichlet 推广到多项式分布，带有 $k-1$ 个独立参数，多项式分布的参数满足 $\sum \theta_k = 1$

多项式分布的似然函数的形式为 $\ell(\theta:\mathcal D) = \prod_k \theta_k^{M[k]}$

Dirichlet 分布的形式为

$$
\theta \sim Dirichlet (\alpha_k) \quad \text{if}\ P(\theta)\propto \prod_k\theta_k^{\alpha_k - 1}
$$

Dirichlet 分布和多项式分布共轭，故当先验为 Dirichlet 时，后验也将是 Dirichlet，其超参数被修正为 $\alpha_k^* = M[k] + \alpha_k$

故经过计算，此时的预测为 $P(x[M+1] = k \mid X) = (M[k] + \alpha_k)/(M + \alpha)$

我们令 $\theta^k$ 为样本取值为 $k$ 在先验中的期望，因此有 $\theta_k' = \alpha_k/\alpha$
我们将预测重写为

$$
\frac {M[k] + \alpha_k}{M+\alpha} = \frac {\alpha}{M+\alpha}\theta_k'+\frac {M}{M+\alpha}\frac {M[k]}{M}
$$

因此预测可以写为先验估计和 MLE 估计的加权平均

The likelihood of Markov Network
我们考虑 Markov Network 的参数学习问题，仅考虑离散的变量
根据 HC 定理，我们写出 MN 表示的分布为 $P(X) = \frac 1 Z \exp\{\sum_i \theta_i f_i(C_i)\}$
其中 $f_i$ 为团 $C_i$ 上的特征，$\theta_i$ 为参数

我们同样可以用 MLE 方法优化 MN 的参数，因此需要考虑计算似然

进而对于实例为 $x[m]$ 的数据集 $\mathcal D$，其对数似然写为
$\ell(\pmb \theta: \mathcal D) = \sum_m\sum_i \theta_i f_i(x_i[m]) - M\ln Z =\sum_i\theta_i\sum_m f_i(x_i[m]) - M\ln Z$ 
其中 $M$ 为数据集大小

进而平均对数似然为
$\frac 1 M \ell(\pmb \theta: \mathcal D) = \sum_i\theta_i\frac 1 M\sum_m f_i(x_i[m]) - \ln Z=\sum_i \theta_i E_{\mathcal D}[f_i(x_i)] - \ln Z$

我们知道 $Z$ 为划分函数，$Z = \sum_{c_i}\exp\{\sum_i \theta_if_i(c_i)\}$
显然 $Z$ 也和参数 $\pmb \theta$ 有关，我们在学习时需要考虑到 $Z$，同时我们发现 $Z$ 涉及了全部的因子，我们此时难以将整个似然函数按照各个因子解耦

The Convexity of Partition Function
我们考虑划分函数的凸性
我们计算划分函数相对于参数的 Hessian 矩阵，如果它半正定，则说明划分函数相对于参数是凸的

The First Derivative of $\ln Z(\pmb \theta)$

$$
\begin{align}
\frac {\partial }{\partial \theta_i}\ln Z(\pmb \theta) 
&=\frac {1}{Z(\pmb \theta)}\frac {\partial}{\partial \theta_i}\sum_{\xi}\exp\left\{\sum_j \theta_j f_j(\xi)\right\}\\
&=\frac {1}{Z(\pmb \theta)}\sum_{\xi}\frac {\partial}{\partial \theta_i}\exp\left\{\sum_j \theta_j f_j(\xi)\right\}\\
&=\frac {1}{Z(\pmb \theta)}\sum_{\xi}f_i(\xi)\exp\left\{\sum_j \theta_j f_j(\xi)\right\}\\
&=\sum_{\xi}f_i(\xi)\frac {\exp\left\{\sum_j \theta_j f_j(\xi)\right\}}{Z(\pmb \theta)}\\
&=\sum_{\xi}f_i(\xi)P(\xi)\\
&=E_{\pmb \theta}[f_i]
\end{align}
$$

发现 $\ln Z(\pmb \theta)$ 相对于特征 $f_i$ 的系数 $\theta_i$ 的一阶导数等于特征 $f_i$ 相对于参数 $\pmb \theta$ 定义的分布 $P(\xi : \pmb \theta)$ 的期望

The Secondary Derivative of $\ln Z(\pmb \theta)$

$$
\begin{align}
\frac {\partial^2}{\partial \theta_j\partial \theta_i} \ln Z(\pmb \theta) 
&=\frac {\partial }{\partial \theta_j} \left[\frac {\partial }{\partial \theta_i}\ln Z(\pmb \theta)\right]\\
&=\frac {\partial }{\partial \theta_j}\left[\frac {1}{Z(\pmb \theta)}\sum_{\xi}f_i(\xi)\exp\left\{\sum_k \theta_kf_k(\xi)\right\}\right]\\
&=-\frac {1}{Z(\pmb \theta)^2}\left(\frac {\partial}{\partial \theta_j}Z(\pmb \theta)\right)\sum_{\xi}f_i(\xi)\exp\left\{\sum_k \theta_kf_k(\xi)\right\}\\
&\quad +\frac {1}{Z(\pmb \theta)}\sum_{\xi}f_i(\xi)f_j(\xi)\exp\left\{\sum_k \theta_kf_k(\xi)\right\}\\
&=-\frac {1}{Z(\pmb \theta)^2}Z(\pmb \theta)E_{\pmb \theta}[f_j]\sum_{\xi}f_i(\xi)\tilde P(\xi: \pmb \theta)\\
&\quad +\frac {1}{Z(\pmb \theta)}\sum_{\xi}f_i(\xi)f_j(\xi)\tilde P(\xi : \pmb \theta)\\
&=-E_{\pmb \theta}[f_j]\sum_{\xi}f_i(\xi) P(\xi: \pmb \theta)+\sum_{\xi}f_i(\xi)f_j(\xi) P(\xi : \pmb \theta)\\
&=E_{\pmb \theta}[f_if_j] - E_{\pmb \theta}[f_i]E_{\pmb \theta}[f_j]\\
&=Cov_{\pmb \theta}[f_i,f_j]
\end{align}
$$

可以发现 $\ln Z(\pmb \theta)$ 相对于参数 $\theta_i, \theta_j$ 的二阶导数等于，特征 $f_i, f_j$ 在分布 $P(\xi:\pmb \theta)$ 中的协方差

因为协方差矩阵总是半正定，故 $\ln Z(\pmb \theta)$ 是关于 $\pmb \theta$ 的凸函数

我们进而考察平均对数似然函数的第一项 $\sum_i \theta_i E_{\mathcal D}[f_i(x_i)]$，容易知道它是关于 $\pmb \theta$ 的线性函数
(一个关于向量的线性函数 $f$ 满足: 
1. 加法性 $f(u + v) = f(u) + f(v)$ 
2. 齐次性 $f(cv) = cf(v)$，其中 $c$ 为标量
)

一个关于 $\pmb \theta$ 的线性函数加上一个关于 $\pmb \theta$ 的凸函数的和是关于 $\pmb \theta$ 的凸函数，因此 MN 的 (平均) 对数似然函数就是关于 $\pmb \theta$ 的凸函数
故我们可以使用梯度上升法优化平均对数似然，它的凸性保证它存在最大值点

Recall: Gradient Ascent
我们根据 $\pmb \theta^{(k+1)} = \pmb \theta^{(k)}  +\lambda^{(k)}\nabla\ell(\pmb \theta: \mathcal D)$ 来执行梯度上升更新，$\ell (\pmb \theta:\mathcal D)$ 的凸性保证梯度上升可以收敛到全局最优

同时，注意一般情况下 $\pmb \theta$ 中的 $\theta_i$ 之间是相互独立的，因此我们可以使用坐标上升法，每次仅根据 $\ell (\pmb \theta:\mathcal D)$ 相对于 $\theta_i$ 的偏导更新 $\theta_i$

Stochastic Gradient Ascent
IID 数据集的对数似然的计算为 $\ell (\pmb \theta : \mathcal D ) = \sum_{m=1}^M \ell (\pmb \theta: \pmb x[m])$
即所有数据样本的似然求和

显然每一次梯度更新都需要计算该似然的开销太大

故随机梯度上升方法一次仅基于一个数据样本的似然计算梯度，即
$\pmb \theta^{(k+1)} = \pmb \theta^{(k)}  +\lambda^{(k)}\nabla\ell (\pmb \theta: \pmb x[m])$
其中每次 epoch 中样本的顺序随机
我们重复多次 epoch 直到收敛

SGA 实际上仅仅是策略上的考虑，我们也可以折中用 minibatch，在目标函数为凸的条件下，它们都可以收敛

Calculation on the Partition Function
我们之前证明了对数似然函数相对于 $\pmb \theta$ 的凸性，这使得我们可以用 SGA 对其进行优化

我们开始考虑如何计算对数似然函数相对于 $\pmb \theta$ 的梯度
在使用坐标上升法时，我们考虑对数似然函数相对于 $\theta_i$ 的偏导数，容易知道

$$
\begin{align}
\frac 1 M \ell(\pmb \theta: \mathcal D) &= \sum_i \theta_i E_{\mathcal D}[f_i(x_i)] - \ln Z(\pmb \theta)\\
\frac 1 M\frac {\partial \ell(\pmb \theta: \mathcal D)}{\partial \theta_i}&= E_{\mathcal D}[f_i] - E_{\pmb\theta}[f_i]
\end{align}
$$

可以发现该偏导数涉及到两个期望值，一个是 $f_i$ 相对于数据集的期望，一个是 $f_i$ 相对于当前参数 $\pmb \theta$ 定义的分布的期望

显然第二个期望的计算需要我们遍历所有可能取值，不可能实现

Solution: Approximate Calculations by MC
因此一种方法是使用采样方法对其进行近似
我们从 $P(\xi: \pmb \theta)$ 中进行采样，然后用均值估计期望

对于 BN 和 MN 的 MLE，可以发现 MN 由于划分函数的存在，其似然的形式和梯度的形式都变得更加复杂

# 11 Learning with Incomplete Data
数据不完全的情况
- 存在不可被观察的变量 (隐变量)
    隐变量可以是从我们视角出发观测不到的变量 (从其他人视角出发或许可以观测)，也可以是本身就不存在于真实世界，是我们概念上定义的变量
- 部分变量的值丢失以及数据存在离群点 (一般会丢弃离群点)
    例如传感器出错了

涉及隐变量的模型有
HMM: 其状态变量无法观测
Mixture Models: 从多个模型产生的混合数据中推理出某个数据点的来源模型
Latent Linear Model: 降维

Learning Gaussian Mixture Models
考虑对 GMM 模型的学习

我们有数据 $\mathcal D = \{\pmb y[1], \dots, \pmb y[M]\}$，考虑进行 MLE，即找到 $\arg\max_{\pmb \theta}p(\mathcal D \mid \pmb \theta)$

我们假设每个观测数据 $Y$ 都来自于某个高斯模型，我们用隐变量 $X$ 作为标签变量，它决定了 $Y$ 来自于哪一个高斯模型，显然 $X$ 是 $Y$ 的父节点

此时，我们的学习任务存在两个未知：隐变量取值未知、参数未知

考虑如果每个 $\pmb y[m]$ 对应的隐变量取值 $x[m]$ 都已知，则我们有完整数据 $\mathcal D_C\{(x[i], \pmb y[i])\}_{i=1,\dots, M}$ 则该问题就是在完整数据上的学习问题

此时关于 $X$ 的分布 (多项式分布) 的 MLE 估计为

$$
\pi_k^* = \frac {M[x=k]}{M}
$$

关于 $Y$ 的分布 (高斯分布) 的 MLE 估计为

$$
\begin{align}
\pmb \mu_k^* &= \frac {1}{M[x=k]}\sum_m \pmb y[m] \mid_{x[m] = k}\\
\pmb \Sigma_k^* &=\frac {1}{M[x=k]}\sum_m (\pmb y[m]-\pmb \mu_k^*)(\pmb y[m]-\pmb \mu_k^*)^T\mid_{x[m] = k}
\end{align}
$$

考虑如果参数已知，则我们可以借助参数为每个数据实例 $\pmb y[m]$ 推理出其标签变量的后验概率

$$
\begin{align}
Q(x=k) &= P(x = k \mid y,  \theta)\\
&=\frac {P(x=k, y\mid \theta)}{P(y\mid \theta)}\\
&=\frac {P(x=k, y\mid \theta)}{\sum_{k=1}^K P(x=k,y\mid \theta)}\\
&=\frac {P(y\mid x=k, \theta)P(x=k\mid\theta)}{\sum_{k=1}^K P(y\mid x=k,\theta)P(x=k\mid \theta)}\\
\end{align}
$$

因此

$$
\begin{align}
Q(x[m] = k)&=\frac {\pi_k N_k(y[m])}{\sum_{k=1}^K \pi_k N_k(y[m])}
\end{align}
$$

其中

$$
N_k(y) = \frac {1}{\sqrt {|2\pi|\Sigma}}e^{-\frac 1 2(y-\mu_k)\Sigma^{-1}(y-\mu_k)}
$$

现在面临的问题在于同时出现了数据不全 (隐变量) 和参数未知两个问题

EM 算法迭代式地解决这两个子问题，它从某个初始参数开始，迭代式地执行以下步骤
- 为每个数据点推断 $X$ 的后验概率 $P(X\mid y[m], \theta)$，将 $X$ 的后验认为是 $X$ 的先验
- 进行 MLE 估计，公式如下

$$
\begin{align}
\arg\max_{\theta} \ln P(\mathcal D_{obs}\mid \theta) &= \arg\max_{\theta}\sum_{m} \ln P(y[m]\mid \theta)\\
&=\arg\max_{\theta} \sum_m\ln \sum_{x} P(y[m],x \mid \theta)\\
&=\arg\max_{\theta}\sum_m\ln \sum_x P(y[m]\mid x, \theta)P(x\mid \theta)
\end{align}
$$

可以发现，对于一个数据实例，因为 $x[m]$ 缺失，我们无法计算似然 $P(y[m], x[m]\mid \theta)$，而 EM 算法实际上使用了似然的期望值 $\sum_x P(y[m], x\mid \theta)$ 替代了似然 $P(y[m], x[m]\mid \theta)$，该期望值是基于当前 $X$ 的后验分布而计算

直观上理解，就类似于用一个期望值将 $x[m]$ 填充了，使得数据完整，故而可以执行 MLE

此时，关于 $X$ 的分布的参数的 MLE 为

$$
\pi_k^{(t+1)} = \frac 1 M \sum_{m=1}^M Q^{(t)}(x[m] = k)
$$ 
和完整数据情况下的更新公式 

$$\pi_k^* = \frac 1 M \sum_{m=1}^M \mathbf 1[x[m] = k] = \frac {M[x=k]} M$$

进行比较，不难发现，直观上看，我们将 $x[m]$ 等于 $k$ 的概率当作了零点几个样本，对其进行了计数，故关于 $X$ 的多项式分布的 MLE 参数更新仍然可以视作是频率计算

此时，关于各个高斯成分的参数的 MLE 为

$$
\begin{align}
\pmb \mu_k^{(t+1)} &= f(\pmb y, Q^{(t)}) = \frac {\sum_{m=1}^M Q^{(t)}(x[m] = k)\pmb y[m]}{\sum_{m=1}^M Q^{(t)}(x[m] = k)} \\
\pmb \Sigma_k^{(t+1)} &= f(\pmb y,Q^{(t)}, \pmb\mu_k^{(t+1)})\\
&=\frac {\sum_{m=1}^M Q^{(t)}(x[m] = k)(\pmb y[m]-\pmb \mu_k^{(t+1)})(\pmb y[m] - \pmb \mu_k^{(t+1)})^T}{\sum_{m=1}^M Q^{(t)}(x[m] = k)}
\end{align}
$$ 
可以发现该更新公式仍然符合直觉，即每个数据实例基于它属于第 $k$ 个高斯成分的概率作为权重而对该高斯分布的参数更新做出贡献

在参数更新之后，我们基于新的参数，更新后验概率得到 $Q^{(t+1)}(x[m] = k)$
如此循环直到收敛即可

可以看到，对于 GMM，我们可以使用 EM 算法进行参数学习，我们从初始参数开始，推断出隐变量的后验分布，然后基于该后验分布进行 MLE 参数更新，如此迭代
可以看到，对于 GMM，对于隐变量的后验分布的推断和 MLE 更新，我们都有闭式的解

但该迭代过程不能确保收敛，同时也不能确保收敛后得到的估计是无偏的
目前没有能高效地同时优化参数空间和变量空间，且有理论收敛性保证的算法

Expectation Maximization
基本思路：
对于带有未观测变量的似然函数，我们可以基于其后验将似然函数中的这些变量求和消去，然后用消去这些变量的似然函数做 MLE 估计

实现：
E-step: 给定参数，为未观测变量做推理，得到其后验概率分布
M-step: 利用推理得到的后验，边际化消去隐藏变量，基于得到的边际似然来学习
迭代式执行推理 + 学习

EM 算法也存在变体，上面介绍的传统 EM 中，我们在 M-step 优化的目标似然是

$$
\begin{align}
P(\mathcal D_{obs}\mid \theta) &= \int P(\mathcal D_{obs}, X_{miss}\mid \theta) dX_{miss}\\
&=\int P(\mathcal D_{obs}\mid X_{miss}, \theta)P(X_{miss}\mid \theta)dX_{miss}
\end{align}
$$

其中 $P(X_{miss}\mid \theta)$ 是由 E-step 得到的后验 $P(X_{miss}\mid \mathcal D_{obs}, \theta)$ 代替

我们也可以不必考虑对 $X_{miss}$ 的所有可能进行积分，而是之间简单选择使得 $P(X_{miss}\mid \mathcal D_{obs}, \theta)$ 最大的 $\hat X_{miss}$ 作为填充值，则此时在 M-step 优化的目标似然是

$$
P(\mathcal D_{obs}\mid \theta) \approx P(\mathcal D_{obs}, \hat X_{miss}\mid \theta)
$$

这样优化较为激进，考虑的空间更小，但收敛速度会更快

除此以外，我们也可以不选择 MAP，而是根据后验分布进行一次采样，随机确定应该选择哪个值来填充缺失变量

EM 算法的思想可以进一步泛化，应用到任意的同时涉及参数空间和变量空间的优化问题
我们的优化目标可以不仅限于似然函数，而是定义任意的分数函数，例如似然函数 + 正则项

在优化时，我们的迭代仍然采用：
设定参数 -> 进行推理 -> 重新估计参数(学习) -> 重新进行推理 ...
这样进行，但是每一步的推理和学习都可以考虑不同的推理、学习方法

例如，在推理时，如之前的介绍，可以边际化整个分布，也可以进行 MAP，又或者采样一个值；在学习时，可以不使用 MLE，而是使用贝叶斯学习

Baum-Welch Algorithm: Revisited
BW 算法的目标是计算 $\arg\max_{\theta}P(X\mid \theta)$，其 general strategy 如下
1. 为参数设定初始值
2. 对隐变量执行推理
3. 重新估计参数
4. 重复以上两步直到收敛

显然 BW 算法就是 EM 算法在 HMM 模型上的应用

Generalization
Inference
- MCMC
- Variational Inference
- Belief Propagation

Learning
- MLE, MAP
- Gradient ascent/decent
- Hierarchical Bayesian

General Principles
现今大多数学习方法都可以用 EM 的框架来解释
1. 我们首先定义一个同时涉及了参数和隐变量/缺失值的目标/评分函数
2. 我们从初始参数开始，进行 EM 迭代
    Inference 步骤中，可以使用机器学习方法，也可以直接使用神经网络，推断隐变量的分布，或者仅推断 MAP 值
    Learning 步骤中，目标可以是在隐变量的所有可能取值上最大化观察的边际似然，也就是找到能最大化积分 $\int P(D_{obs}, X_{miss}\mid \theta) dX_{miss}$ 的参数 $\theta$，也可以最大化隐变量取 MAP 值下的数据集似然，即找到能最大化 $P(D_{obs}, \hat X_{miss} \mid \theta)$ 的参数 $\theta$
3. 最后直到 Learning 步骤的目标/评分函数收敛，或者 Inference 步骤的后验概率收敛 (对于 Bayesian 模型)，我们停止迭代

# 12 Structure Learning in Bayesian Networks
Constraint-Based Structure Learning
基于约束的结构学习的目标是找到最能够捕获数据中独立性的网络结构，或者说找到数据的极小 I-map

则首要的一个问题就是如何测定数据中的独立性
测定数据中两个变量 $X, Y$ 的独立性有两种计算方法

$$
\begin{align}
d_{\chi^2}(\mathcal D) &= \sum_{x,y}\frac {(M[x,y]-M\hat P(x)\hat P(y))^2}{M\hat P(x)\hat P(y)}\\
d_I(\mathcal D) &= \frac 1 M \sum_{x,y} M[x,y]\log \frac {M[x,y]}{M[x]M[y]}
\end{align}
$$

其一是计算卡方统计量，直观上看，$X, Y$ 在经验分布中的独立性越强，卡方统计量越接近于 0
其二是计算 $X, Y$ 在经验分布 $\hat P$ 中的互信息

卡方统计量可以进一步用于计算卡方分布的尾分布，并计算 pvalue 如下

$$
pvalue =  1 - F_{\chi^2}(d_{\chi^2}(\mathcal D))
$$

实践中，pvalue 的阈值一般为 0.01 或 0.05
该卡方分布的自由度应该是数据集 $X, Y$ 的不同赋值出现的总数减一，即 $[x, y] - 1$
另外，根据中心极限定理，如果 $M[x, y]$ 很大，该统计量可以直接认为服从正态分布

该独立性检验可以拓展到更复杂的结构 $X\leftarrow Z \rightarrow Y$，即检验条件独立性

$$
d_{\chi^2}(\mathcal D) = \sum_{x,y,z}\frac {(M[x,y,z]- M\hat P(x\mid z)\hat P(y\mid z)\hat P(z))^2}{M\hat P(x\mid z)\hat P(y\mid z)\hat P(z)}
$$

因此，我们穷举所有的变量组合，进行独立性检验，判断数据是否支持特定的结构，然后根据 Build-PDAG 算法 (Textbook Algorithm3.5) 构建贝叶斯网络

Model Selection for Structure Learning
基于模型选择的结构学习和参数学习的框架类似，我们定义一个模型结构分布或评分函数，根据评分函数从预定义的结构空间中选择出最优的结构

此时结构学习任务被转化为一个优化任务或者模型选择任务

Likelihood Structure Score
我们将结构也视作随机变量，此时数据集的似然函数将拥有两个超参数，一个是结构空间 $\pmb {\mathcal G}$，一个是参数空间 $\Theta$，即似然函数为

$$
P(\mathcal D \mid \mathcal G, \theta_{\mathcal G})
$$

注意具体的参数空间 $\Theta$ 实际上由结构空间 $\pmb {\mathcal G}$ 中具体的结构 $\mathcal G$ 决定

对于结构空间 $\pmb {\mathcal G}$ 中每个确定的结构 $\mathcal G$，我们都可以从数据 $\mathcal D$ 中推断出它的极大似然参数 $\hat \theta_{\mathcal G}$，进而，我们为结构 $\mathcal G$ 定义其极大似然结构分数为

$$
score_{ML}(\mathcal G: \mathcal D) = \max_{\theta}P(\mathcal D\mid \mathcal G, \theta_{\mathcal G}) = P(\mathcal D\mid \mathcal G, \hat \theta_{\mathcal G})
$$

也就是在采用极大似然参数 $\hat \theta_{\mathcal G}$ 的情况下，数据集的似然

此时结构本身的极大似然估计值就是

$$
\mathcal G^* = \arg\max_{\mathcal G} P(\mathcal D\mid \mathcal G, \hat \theta_{\mathcal G})
$$

极大似然结构分数 $score_{ML}(\mathcal G: \mathcal D)$ 的问题在于，该分数总是偏好更加复杂的结构，也就是复杂结构的极大似然分数可以保证不低于高于简单结构的极大似然分数，因此全连通结构可以保证达到极大似然结构分数的上界
(直观上看，越复杂的结构表示能力越强，越能完美拟合数据集，使得数据集似然更大)

Bayesian Score
我们考虑 $\mathcal G$ 的后验概率

$$
P(\mathcal G \mid \mathcal D) = \frac {P(\mathcal D\mid \mathcal G)P(\mathcal G)}{P(\mathcal D)}
$$

因此我们将贝叶斯分数定义如下

$$
score_B(\mathcal G: \mathcal D) = \log P(\mathcal D\mid \mathcal G) + \log P(\mathcal G)
$$

其中第二项是结构 $\mathcal G$ 的先验概率
第一项是数据集在 $\mathcal G$ 定义的参数空间上边际化得到的边际分布，即

$$
\begin{align}
P(\mathcal D \mid \mathcal G) &= \int P(\mathcal D, \theta_{\mathcal G}\mid \mathcal G)d\theta_{\mathcal G} \\
&= \int P(\mathcal D \mid \theta_{\mathcal G}, \mathcal G)P(\theta_{\mathcal G}\mid \mathcal G)d\theta_{\mathcal G}
\end{align}
$$

第一项称为贝叶斯结构分数

贝叶斯结构分数 + 先验就是贝叶斯分数

贝叶斯分数考虑了参数空间内参数的所有可能性，考虑的是期望值，而极大似然分数考虑的是点估计

Bayesian Structure Score: Parameter Averaging Controls Overfitting
考虑贝叶斯结构分数 $\log P(\mathcal D \mid \mathcal G)$

根据链式法则，我们可以将该后验概率按照数据集分解如下

$$
P(\mathcal D \mid \mathcal G) = \prod_{m=1}^M P(x[m]\mid x[1], \dots, x[m-1], \mathcal G)
$$

其中 $P^{[m]} = P(x[m]\mid x[1], \dots, x[m-1], \mathcal G)$ 可以视作基于前面 $m-1$ 个样本对第 $m$ 个样本的预测

这也就是一个顺序预测问题 (prequential prediction)，考虑到参数，顺序预测问题实际上可以写为

$$
\begin{align}
P^{[m + 1]} &= P(x[m + 1]\mid x[1\sim m], \mathcal G)\\
&=\int P(x[m+1], \theta^{[m]}\mid x[1\sim m], \mathcal G)d\theta^{[m]}\\
&=\int P(x[m+1]\mid \theta^{[m]}, x[1\sim m], \mathcal G)P(\theta^{[m]}\mid x[1\sim m], \mathcal G)d\theta^{[m]}\\
&=\int P(x[m+1]\mid \theta^{[m]}, \mathcal G)P(\theta^{[m]}\mid x[1\sim m],\mathcal G)d\theta^{[m]}
\end{align}
$$

我们知道，给定结构 $\mathcal G$，在参数先验 $P(\theta_{\mathcal G}\mid \mathcal G)$ 和数据集似然 $P(\mathcal D\mid \theta_{\mathcal G}, \mathcal G)$ 共轭的前提下，每一次的预测概率 $P^{[m]} = P(x[m]\mid x[1\sim m-1], \mathcal G)$ 仍服从和参数先验形式相同的分布，差异仅在于基于观测到的数据对先验超参数做出修正

因此，此时 $P(\mathcal D\mid \mathcal G)$ 可以通过顺序地迭代数据集进行计算

容易知道，在实践中，$P(\mathcal D\mid \mathcal G)$ 往往比 $P(\mathcal D\mid \mathcal G, \hat \theta_{\mathcal G})$ 要低，不妨说后者是前者的上界
直观上，可以认为后者直接收集了全部数据集的信息，得到了最优的参数，然后计算似然，而前者则逐个收集数据集的样本，从先验开始，利用收集到的信息逐渐更新参数，利用当前参数计算当前样本的似然，最后求和得到总体的似然

因此，贝叶斯结构分数的计算实际上考虑了参数先验的影响，并且直观上可以认为是在利用观察数据顺序地测试结构的泛化性，故贝叶斯结构分数的过拟合风险会显然低于极大似然分数

Bayesian Structure Score for BNs
在 BN 中，如果参数先验满足全局独立性，且各个 CPD 的参数满足局部独立性，则贝叶斯结构分数可以按照贝叶斯网络结构和 CPD 条目分解，如下所示

$$
\begin{align}
P(\mathcal D\mid \mathcal G) &= \prod_{m}P(\pmb x[m]\mid \mathcal G)\\
&=\prod_m \int_{\Theta}P(\pmb x[m], \theta\mid \mathcal G)d\theta\\
&=\prod_m \int_{\Theta}P(\pmb x[m]\mid \theta,\mathcal G)P(\theta\mid \mathcal G)d\theta\\
&=\prod_m \int_{\Theta}P(\pmb x[m]\mid \theta,\mathcal G)P(\theta\mid \mathcal G)d\theta\\
&=\prod_m \int_{\Theta} \prod_{i}P(x_i[m]\mid \pmb U_i, \theta, \mathcal G )P(\theta\mid \mathcal G)d\theta\\
&=\prod_m \int_{\Theta} \prod_{i}P(x_i[m]\mid \pmb U_i, \theta_{x_i}, \mathcal G )P(\theta\mid \mathcal G)d\theta\\
&=\prod_m \int_{\Theta} \prod_{i}P(x_i[m]\mid \pmb U_i, \theta_{x_i}, \mathcal G )\prod_{i}P(\theta_{x_i}\mid \mathcal G)d\theta\\
&=\prod_m \int_{\Theta} \prod_{i}P(x_i[m]\mid \pmb U_i, \theta_{x_i}, \mathcal G )P(\theta_{x_i}\mid \mathcal G)d\theta\\
&=\prod_m \prod_i\int_{\Theta_{X_i}} P(x_i[m]\mid \pmb U_i, \theta_{x_i}, \mathcal G )P(\theta_{x_i}\mid \mathcal G)d\theta_{x_i}\\
&=\prod_m \prod_i\int_{\Theta_{X_i}} \prod_{\pmb u_i}P(x_i[m]\mid \pmb u_i, \theta_{x_i}, \mathcal G )P(\theta_{x_i}\mid \mathcal G)d\theta_{x_i}\\
&=\prod_m \prod_i\int_{\Theta_{X_i}} \prod_{\pmb u_i}P(x_i[m]\mid \pmb u_i, \theta_{x_i\mid \pmb u_i}, \mathcal G )P(\theta_{x_i\mid \pmb u_i}\mid \mathcal G)d\theta_{x_i}\\
&=\prod_m \prod_i \prod_{\pmb u_i}\int_{\Theta_{X_i}\mid \pmb u_i} P(x_i[m]\mid \pmb u_i, \theta_{x_i\mid \pmb u_i}, \mathcal G )P(\theta_{x_i\mid \pmb u_i}\mid \mathcal G)d\theta_{x_i\mid \pmb u_i}\\
&= \prod_i \prod_{\pmb u_i}\int_{\Theta_{X_i}\mid \pmb u_i} \prod_{m:\pmb U_i[m] = \pmb u_i}P(x_i[m]\mid \pmb u_i, \theta_{x_i\mid \pmb u_i}, \mathcal G )P(\theta_{x_i\mid \pmb u_i}\mid \mathcal G)d\theta_{x_i\mid \pmb u_i}\\
\end{align}
$$

如果参数先验和数据集似然共轭，其中的积分项就可以替换为闭式项

Priors for Bayesian Structure Score
在参数满足全局独立性和局部独立性的情况下，后验概率 $P(\mathcal D\mid \mathcal G)$ 可以按照变量和 CPD 分解，因此贝叶斯结构分数可以分解为

$$
\begin{align}
score_{BS}(\mathcal G:\mathcal D)  
=\log P(\mathcal D\mid \mathcal G) 
= \sum_i FamScore(X_i\mid \text{Pa}_i^{\mathcal G}:\mathcal D)
\end{align}
$$

其中每个 $FamScore$ 仅和变量 $X_i$ 有关

此时每个局部的 $FamScore$ 的优化是独立于其他局部分数的，而全局的最优等价于每个局部的最优的结合
因此，我们可以为每个变量 $X_i$ 进行局部搜索，找到最大化局部分数的父变量 $\text{Pa}_{i}$，最后组合得到全局最优结构 $\mathcal G^*$

Bayesian Information Criterion
可以证明，当参数先验是 Dirichlet 先验时，在 $M$ 足够大的条件下，贝叶斯结构分数等价于

$$
\begin{align}
score_{BS}(\mathcal G: \mathcal D) &= \log P(\mathcal D\mid \mathcal G)\\
&=\ell(\hat \theta_{\mathcal G}:\mathcal G) - \frac {\log M}{2}\dim[\mathcal G] + O(1)
\end{align}
$$

其中 $\dim[\mathcal G]$ 是图模型中独立参数的个数，可以看到，贝叶斯结构分数对结构的复杂性做出了惩罚

去掉常数项 $O(1)$，得到的就是贝叶斯信息准则

$$
score_{BIC}(\mathcal G: \mathcal D) = \ell(\hat \theta_{\mathcal G}: \mathcal D) - \frac {\log M}{2}\dim[\mathcal G]
$$

BIC for BNs
BIC 分数中，第一项是极大似然结构分数，我们可以将该分数分解，得到

$$
score_{BIC}(\mathcal G:\mathcal D) = M\sum_{i=1}^n I_{\hat P}(X_i, \text{Pa}_{X_i}) - M\sum_{i=1}^n \hat H_{\hat P}(X_i) - \frac {\log M}{2}\dim[\mathcal G]
$$

其中第一项是每个局部结构 (变量 $X_i$ 和其父变量) 的在经验分布中的互信息，第二项是每个变量在经验分布中的熵

注意到第二项和结构是无关的，因此在结构选择中可以忽略，故找到最优结构就等价于最大化互信息的同时惩罚结构复杂度

一个评分函数，如果满足：
- perfect-map $\mathcal G^*$ 会最大化该评分函数
- 所有不是 $\mathcal G^*$ 的 I-equivalence 的结构 $\mathcal G$ 的分数都会严格低于 $\mathcal G^*$ 
称该评分函数是一致的

可以证明 BIC 分数是一致的
因此常常直接使用 BIC 分数而不是贝叶斯分数

General Structure Priors
考虑贝叶斯分数的第二项 $\log P(\mathcal G)$

结构先验的重要性随着数据量的增加而减少

常用的先验可以是均匀分布，或者对图中边的数量进行惩罚，即 $P(\mathcal G) \propto c^{|\mathcal G|}, 0 < c < 1$

Search for Optimal Structure
给定
- 训练集 $\mathcal D$
- 评分函数 (例如 Bayesian 分数，BIC 分数)
- 搜索空间 (可能结构的集合 $\pmb {\mathcal G}$)

我们的目标是找到最大化评分函数的结构 $\mathcal G^*$

如果评分函数可分解，我们可以将全局搜索问题分解为多个局部搜索问题

结构的搜索和连续优化不同，结构的搜索本质上是离散优化/整数优化问题，而整数优化问题基本上都是 NP-hard 问题
而对于 $k$ 个变量，其可能的结构数量为 $2^{O(k^2)}$

此时我们有两种选择：限制搜索空间大小、使用启发式搜索算法

Learning Tree-Structured Networks
我们仅考虑树结构，以限制搜索空间的大小
树结构中，每个节点最多只有一个父节点

搜索从空结构 $\mathcal G_{\emptyset}$ 开始，此时的评分为

$$
score(\mathcal G_{\emptyset}: \mathcal D) = \sum_i FamScore(X_i:\mathcal D)
$$

这是因为空结构中每个节点没有父节点，因此局部的 $FamScore$ 就仅涉及单个变量

向空结构添加一条边 $X_j \to X_i$，此时评分的增量为

$$
\Delta_{j\to i} = FamScore(X_i\mid X_j:\mathcal D) - FamScore(X_i:\mathcal D)
$$

因为树结构中每个节点最多只有一个父节点，故我们可以比较所有的节点，选出具有最大增量的节点 $X_j$ 作为 $X_i$ 的父节点，以此就确定了 $X_i$ 的局部结构

注意增量值集合 $\{\Delta_{j\to i}\ \text{for all}\ i,j\}$ 完全可以提前计算好，在比较时直接取计算好的值即可，增量值可以看作对于有向边 $j\to i$ 的权重

添加了 $k$ 条边后，我们得到 $\mathcal G$，此时评分的总增量就为

$$
\Delta(\mathcal G) = \sum_{(X_j \to X_j)\in \mathcal G}\Delta_{j\to i}
$$

因此，我们完全可以计算每种可能结构的评分增量，最优的结构就是增量最大的结构，即

$$
\mathcal G^* = \arg\max_{\mathcal G} \Delta(\mathcal G)
$$

该结构搜索问题实际上可以视作在全连接的结构中搜索一个最大权重的生成树，该算法的复杂度为 $O(n^2M + n^2\log n)$，其中 $n$ 为变量/节点数量，$M$ 为样本数量

General Structure Search
考虑进一步拓展结构，让每个变量最多有 $d$ 个父节点
此时如果 $d \ge 2$，结构搜索问题就是 NP-hard 问题

General Search with MCMC
对于结构搜索可以使用 MCMC 框架，我们将结构视作随机变量，此时我们不仅仅需要的是最优的/最大后验概率的结构，而是想要一个关于结构的后验分布

任意的两个结构，可以在如下定义的三个操作下，在有限步数内可达
- 添加边
- 删除边
- 逆转边

这三个操作就构成了 transition
Markov 链上的状态就是结构

在转移时，我们需要计算给定数据和当前结构的条件下，下一个结构的分布，即 $P(\mathcal G' \mid \mathcal G, \mathcal D)$

因为该后验概率在结构空间难以计算，因此我们不考虑采用 Gibbs 采样，而是采用 MH 采样，转移概率的计算如下：

$$
\begin{align}
\mathcal A(x\rightarrow x') = \min\left[1,\frac {\pi(x')\mathcal T^Q(x' \to x)}{\pi(x)\mathcal T^Q(x\to x')}\right]
\end{align}
$$

MH 采样仅需要知道稳态分布即可，不需要显式计算转移概率，转移概率可以自己设定

在实践中，对于转移概率 $\mathcal T^Q$，我们一般定义为首先根据预定义的概率 $P(Action)$ 任意选定一个操作，然后再通过转移概率 $P(Transition)$ 选择根据该操作应该转移到的下一个图

具体地说，在每一步中：
我们首先计算

$$
\frac {\pi(x')}{\pi(x)} = \frac {P(\mathcal G'\mid \mathcal D)}{P(\mathcal G\mid \mathcal D)} = \frac {P(\mathcal D\mid \mathcal G')P(\mathcal G')}{P(\mathcal D\mid \mathcal G)P(\mathcal G)}
$$

如果我们使用均匀的结构先验，我们可以得到

$$
\frac {\pi(x')}{\pi(x)}  = \frac {P(\mathcal D\mid \mathcal G')P(\mathcal G')}{P(\mathcal D\mid \mathcal G)P(\mathcal G)} = \frac {P(\mathcal D\mid \mathcal G')}{P(\mathcal D\mid \mathcal G)} = \frac {\exp\{score_{BIC}(\mathcal G':\mathcal D)\}}{\exp\{score_{BIC}(\mathcal G:\mathcal D)\}}
$$

其中 $score_{BIC}(\mathcal G: \mathcal D) = \ell(\hat \theta_{\mathcal G}:\mathcal D) - \frac {\log M}{2}\dim[\mathcal G]$

因为结构的搜索空间较大，如果要进一步提高 MCMC 的效率，可以考虑 collapsed particles
例如，我们可以不直接采样结构本身，而是只采样变量加入结构的顺序，也就是用 MCMC 建模不同顺序之间的 transition，变量加入结构的顺序定下后，我们可以再另外考虑如何连接这些变量，即在给定变量顺序下搜索最优结构

当 MCMC 稳定之后，我们可以根据样本估计结构中某个观察到的有向边的概率是多少
例如，我们记录样本中边 $X\to Y$ 和边 $X\leftarrow Y$ 的频率，作为概率 $P(X\to Y), P(X\leftarrow Y)$，来评估这两个边的相对重要性

Bayesian Model Averaging
在贝叶斯估计中，做预测时，我们需要计算在所有可能参数上的期望概率，即

$$
P(X^{n+1}\mid X^1,\dots, X^n) = \frac {1}{P(X^1, \dots, X^n)}\int P(X^{n+1}\mid \theta)P(X^1, \dots, X^n\mid \theta)P(\theta)d\theta
$$ 
进一步，如果我们将结构也视作随机变量，贝叶斯预测时，我们可以考虑所有可能的结构来做预测

$$
P(X^{n+1}\mid X^1,\dots, X^n) = \sum_{\mathcal G}P(X^{n+1}\mid X^1, \dots,X^n, \mathcal G)P(\mathcal G\mid X^1, \dots, X^n)
$$

要计算这一预测，我们需要计算所有结构的后验  $P(\mathcal G\mid X^1, \dots, X^n)$，这一后验的计算就可以使用 MCMC 方法计算

Bayesian Networks in Practice
Structure Scores
- BIC: Bayesian Information Criterion
    $score_{BIC}(\mathcal G: \mathcal D) = \ell (\hat \theta:\mathcal D)- \frac {\log n}{2}k$
- AIC: Akaike Information Criterion
    $score_{AIC}(\mathcal G: \mathcal D) = \ell(\hat \theta: \mathcal D)- k - \frac {k(k+1)}{n-k-1}$
- MDL: Minimum description length

Structure Priors
- Uniform prior
- Sparse prior: $P(\mathcal G) \propto c^{|\mathcal G|}, 0<c<1$
    稀疏先验偏好更简单的结构

实践中，结构学习也存在局部最优问题，我们一般需要运行多次取最优

Discussion: How about hidden variables?
考虑加入隐变量的目的：减少所需的参数，减少依赖性，这等价于减少图中的边的数量

# 13 PGM and Beyond


