>古槿
# 1 Introduction to Probabilistic Graphical Models
>2024.9.9

用图模型求解概率问题，基于概率在不确定的世界中做出决策

**概率论**
现代概率论：
- 频数论 (基于随机试验)
    将做试验的次数和事件发生的次数的比值作为概率
- 贝叶斯
    指定先验，通过观测更新先验
    $P(X|Y) = \frac {P(Y|X)P(X)}{P(Y)} = \frac {P(Y|X)P(X)}{\sum_X P(Y|X)P(X)}$
    $Y$: 观测/果
    $X$: 原因/因
    $P(X)$ : 先验
    贝叶斯公式将执果索因的条件概率写为仅仅和执因索果的条件概率和原因的先验概率有关的形式 

条件独立：$X\perp Y | Z: P(XYZ) = P(X|Z)P(Y|Z)P(Z)$

链式法则：每一次仅决策一个变量 (分治)，逐个将变量拆分到条件

**图论**
path：路径
trail: 同一条路径，双向可达
tree：单个节点仅有一个父节点
polytree：可以有多个父节点，但没有三角形
chordal graph：图中最大的形状就是三角形，没有多边形
clique：子图，子图内所有节点两两相连

贝叶斯网络：有向无环图，节点表示变量，边表示依赖
Markov 网络：无向图

概率图模型：
- 表示：建模
- 推断：已知参数下，计算概率
- 学习：根据数据，学习参数

朴素贝叶斯：只要类别标签已知，所有的观测变量都独立

# 2 Baysian Network: Representation
> 2024.9.16

I-map: 确保在图上 ($\mathcal G$ 空间) 做的所有操作在 $P$ 空间都是成立的

证明 $\mathcal G$ 是否是 $P$ 的 perfect map 是一个 NP hard 问题
证明 $P$ 是否存在 perfect map 是一个 NP hard 问题

# 3 Local Probabilistic Models
> 2024.9.21

概率图模型的直观思想：
    有向无环图（节点表示变量）
    用图定义变量之间的独立性

概率图模型的形式化思想：
    定义公理：$\mathcal G$ 中的独立性定义为 $(X_i \perp Nondesc (X_i) \mid Pa (X_i))$
    由公理推导 factorization 定理：
    - 充分条件： $\mathcal G$ is a I-Map of $P$ $\rightarrow$ $P (X_1,\dots, X_n) = \prod_{i=1}^n P (X_1\mid Pa (X_i))$ ($P$ 满足 $\mathcal G$ 中的所有独立性，故根据 $\mathcal G$ 分解 $P$)
    - 必要条件： $P (X_1,\dots, X_n) = \prod_{i=1}^n P (X_1\mid Pa (X_i))$ $\rightarrow$  $\mathcal G$ is a I-Map of $P$ (若 $P$ 可以根据 $\mathcal G$ 分解，则 $\mathcal G$ 是 $P$ 的 I-Map，即 $\mathcal G$ 不包含 $P$ 中没有的独立性)

必要条件主要用于结构学习

v-structure：
    子节点被观察到后，父节点发生关联

Noisy-or:
    子节点的值不需要知道全部的父节点的值才能决定，每一个父节点都有各自一定的概率对子节点产生相同的影响
    例如：只要任意一个父节点（以一定概率 $f_i$）发生了 failure，此时子节点就一定会 failure，$P (F = False) = \prod_i (1-f_i)$
    再考虑 leak probability $f_0$，就写为 $P (F = False) = (1-f_0)\prod_i (1-f_i)$

BN2O:
    朴素贝叶斯和 Noisy-or 的拓展
    多个结果节点（子节点）
    朴素贝叶斯：已知某个子节点的全部父节点，该子节点和其余所有子节点条件独立
    Noisy-or：每个子节点和其所有父节点之间构成了 Noisy-or 模型

Generalized linear model:
    多个 cause 的效果乘上权重（权重可为负），线性叠加，得到总效果（分数）
    包括：Logistic CPD, Linear Gaussian 等

Logistic CPD:
    Sigmoid 函数将效果/分数映射到 $[0,1]$ （概率空间）
    由此定义的 CPD 称为 Logistic CPD，即 $P (Y = y^1 \mid X_1,\dots, X_k) = \text{sigmoid} (w_0 + \sum_{i=1}^k w_iX_i$)
    Logistic CPD odds:

$$
\begin{align}
O &= \frac {P(Y=y^1\mid X_1,\dots,X_k)}{P(Y = y^0\mid X_1,\dots, X_k)} \\
&=\frac {e^s / (1+e^s)}{1/(1+e^s)}\\
&=e^s
\end{align}
$$
    For binary $X_i$, when $X_j$ change from $0$ to $1$
$$
\Delta O = e^{w_j}
$$


Linear Gaussian:
    $(Y\mid X_1,\dots ,X_k) \sim \mathcal N (w_0 + \sum_i w_i X_i, \sigma^2)$

如果神经元激活是按照概率 $P (Y = y^1 \mid X_1,\dots , X_k)$，则神经网络也可以视作概率图模型

Pooling:
    抗噪声
    max pooling - 平移稳定性
    median pooling - 过滤 outlier 噪声
    mean pooling - 过滤高斯噪声

hierarchical model: 决定了变量分布的参数同样服从于某个需要学习的分布

Three steps for representation:
    Define node/variables
    Consider edges/dependencies
    Choose local CPDs

# 4 Dynamic Models
> 2024.9.28

Markov/Memoryless property:
    assumption: 
    $P (X) = \prod_{t=1}^n P (X (t)\mid X (t-1))$ and $\forall i, j, P (X (i)\mid X (i-1)) = P (X (j)\mid X (j-1))$

HMM:
    Local structure 1: Hiddent states (can not be observed) are linked as Markov chain $P (S^T \mid S^{t < T}) =  P (S^T\mid S^{T-1})$
    Local structure 2: Observable variables are only determined by the hiddent state $P (O^T \mid S^{t\le T})  = P (O^T \mid S^T)$

The key is designing the hidden variables.

State-Obseration models:
    State-State Transimission Matrix $T_{(n, n)}$: $S^t = TS^{t-1}$ 
    State-Observation Emission Matrix $E_{(m, n)}$: $O^t = ES^t$

Calculations in HMM:
    1.  $P (X\mid \theta)$: given the model and the observation sequence, infer the probability
    2.  $\arg\max_{Y}P (X, Y\mid \theta)$: given the model and the observation sequence, infer the hidden labels of the sequence
    3.  $\arg\max_{\theta}P (X\mid \theta)$: learn parameters from the observation sequence

**Question 1**: Infer $P (X\mid \theta)$
Define:

$$
\begin{align}
\theta &= \begin{cases}
T = \{t_{i,j},\quad i,j = 1,\dots N\},\\
E = \{e_{i,j},\quad i = 1,\dots N,j=1,\dots ,K\},\\
\pi = \{\pi_i,\quad i = 1,\dots N\}
\end{cases}\\\\
X &= \{x_t,\quad t=1,\dots T\}
\end{align}
$$

$\theta$ is the parameterization of HMM, $X$ is the observation sequence.

Basic Idea:
$P (X) = \sum_Y P (X, Y)$

by factorization theorem:

$$\begin{align}
P (X, Y) &= P (Y_1) P (X_1\mid Y_1) P (Y_2\mid Y_1) P (X_2\mid Y_2)\cdots P(Y_T\mid Y_{T-1})P(X_T\mid Y_T)\\
&=P(X_T\mid Y_T)P(Y_T\mid Y_{T-1})P(X_{T-1}\mid Y_{T-1})P(Y_{T-1}\mid Y_{T-2})\cdots P(Y_1)
\end{align}$$

the probability can be solved by eliminating $Y_i$ iteratively

Forward algorithm:
$\alpha_t (i) = P (x_1,\dots, x_t, y_t = i\mid \theta)$ (the probability that the Markov chain has forwarded into time $t$ with state $y_t = i$ and observation $x_1,\dots, x_t$ )
- Initialization: $\alpha_1 (i) = \pi_i e_{i, x_1}$ 
    (the probability that $y_1 = i$ and observed $x_1$)
- Induction: $\alpha_{t+1}(i) = [\sum_{j=1}^N\alpha_t (j) t_{j, i}]e_{j, x_{t+1}}$
    (the probability that the Makov chain has forwarded into time $t+1$ and observe $x_{t+1}$)
- Termination: $P (X\mid \theta) = \sum_{i=1}^N\alpha_T (i)$

Backward algorithm:
$\beta_t (i) = P (x_{t+1},\dots, x_T\mid y_t = i, \theta)$ (the probability that the future observation will be $x_{t+1},\dots, x_T$, given state $y_t = i$ )
- Initialization: $\beta_{T+1}(i) = 1$ 
    (the probability of a non-event, given the fact that the chain is finished)
- Induction: $\beta_t (i) =\sum_{j=1}^N t_{i, j}e_{j, x_{t+1}}\beta_{t+1}(j)$
    $$
\begin{align}
\beta_t (i) &= P (\mathbf x_{t+1: T} \mid y_t = i)\\
&= \sum_j P (y_{t+1} = j, \mathbf x_{t+1: T}\mid y_t = i)\\
&=\sum_j P (y_{t+1} = j, \mathbf x_{t+1: T}\mid y_t = i) \\
&=\sum_j P ( \mathbf x_{t+1: T}\mid y_t = i,y_{t+1} = j)P(y_{t+1} = j\mid y_t = i) \\
&=\sum_j t_{ij}P ( \mathbf x_{t+1: T}\mid y_{t+1} = j) \\
&=\sum_j t_{ij}P(x_{t+1},\mathbf x_{t+2:T}\mid y_{t+1} = j)\\
&=\sum_j t_{ij}P(x_{t+1}\mid y_{t+1} = j) P(\mathbf x_{t+1:T}\mid y_{t+1} = j)\\
&=\sum_j t_{ij}e_{j, x_{t+1}}\beta_{t+1}(j)
\end{align}
    $$

- Termination: $P (X\mid \theta) = \beta_0(i) = \sum_{j=1}^N \pi_j e_{j, x_1}\beta_1(j)$

**Question 2**: Infer $\arg\max_Y P (X,Y\mid \theta)$
Question 1 在前向递推的时候，实际上每一个 $t$ 上都对所有 $Y_t = i$ 的情况做了求和
如果需要递推 Question 2，此时我们在前向递推的时候，只需要记录最大的路径概率即可 (最大概率的路径是满足贪心的，类似于 Dijstra 算法的思想，但注意全局最优状态序列 $Y$ 的递推不一定是满足贪心的)

也就是说，在递推时，对于状态 $y_t = i$，我们记录了从开始到 $y_t = i$ 的最可能路径，而不是所有路径，因此在 $t$ 时刻，我们仅仅记录了 $|Val (y_t)|$ 条路径，注意每条路径的状态序列都不同
之后，对于下一个时刻，我们对于 $\forall i,y_{t+1} = i$，都从 $|Val (y_t)|$ 条路径中选出最可能的一条，最后在 $t+1$ 时刻，我们同样得到 $|Val (y_{t+1})|$ 条路径
不到最后的时刻 $T$，我们无法确定哪条路径是全局最优，因此也无法在中间时刻确定一个部分的状态序列，但一旦确定了某条全局最优的路径，整个状态序列也随之确定

因此，我们在前向推导时记录概率，后向推导时计算状态

Viterbi algorithm:
The probability at time 1 with state $y_1 =i$ and observation $x_1$ :
$\delta_{1, i} = \pi_i e_{i, x_1}$

The probability at time 2 with state $y_2 = i$ and observation $x_1, x_2$ , given the most probable path:
$\delta_{2, i} = e_{i, x_2}\max_{y_1 = 1,\dots, N}(\pi_{y_1}e_{y_1, x_1}\times t_{y_1, i}) =e_{i, x_2}\max_{y_1 = 1,\dots, N}(\delta_{1,y_1}\times t_{y_1,i})$
The most likely previous state on the most probable path to $y_2 = i$:
$\phi_2 (i) = \arg\max_{y_1 = 1,\dots, N}(\delta_{1, y_1}\times t_{y_1, i})$ 

The probability at time $t$ with state $y_t = i$ and observation $x_1,\dots, x_t$, given the most probable path:
$\delta_{t, i} = e_{i, t}\max_{y_{t-1} = 1,\dots, N}(\delta_{t-1, y_{t-1}}\times t_{y_{t-1}, i})$
The most likely previous state on the most probable path to $y_t = i$:
$\phi_t (i) = \arg\max_{y_{t-1}=1,\dots, N}(\delta_{t-1, y_{t-1}}\times t_{y_{t-1},i})$

The probability at time $T$ with state $y_T = i$ and observation $x_1,\dots, x_T$, given the most probable path:
$\delta_{T, i} = e_{i, x_T}\max_{y_{T-1 = 1,\dots, N}}(\delta_{T-1, y_{T-1}}\times t_{y_{T-1}, i})$
The most likely previous state on the most probable path to $y_T = i$:
$\phi_T (i) = \arg\max_{y_{T-1}=1,\dots, N}(\delta_{T-1, y_{T-1}}\times t_{y_{T-1},i})$

$y_T^* = \arg\max_{y_T = 1,\dots, N}(\delta_{T, y_T})$

After $y_t^*$ is inferred, trace back to get $y_{t-1}^*$:
$y^*_{t-1} = \phi_t (y_t^*) = \arg\max_{y_{t-1} = 1,\dots, N}(\delta_{t-1, y_t^*}\times t_{y_{t-1}, y_t^*})$

**Question 3**: Learn $\arg\max_{\theta} P (X\mid \theta)$
参数 $\theta$ 实际上都和隐状态 $Y$ 有关，因此在未知状态的情况下是无法做出推导的
故简单的思路就是对状态进行猜测，迭代优化参数

General startegy:
- set an initial value of the parameters
- do the inference of hidden states
- re-estimate and re-learn the paramters
- repeat the process until convergence

Baum-Welch algorithm:
- Define an intermediate variables for inference:
    $\xi_t (i, j) = P (y_t = i, y_{t+1} = j \mid X, \theta)$
    (given observation and parameter, the probability of $y_t = i$ and $y_{t+1} = j$)
- Use forward and backward algorithm to calculate probability:
    $\theta^0 = \{T^0, E^0, \pi^0\}$
    $\alpha_t(i) = P (\mathbf x_{1: t}, y_t = i\mid \theta^0)$
    $\beta_t (i) = P (\mathbf x_{t+1:T}\mid y_t = i, \theta^0)$

    $$
\begin{align}
\xi_t(i,j) &= P(y_t = i, y_{t+1} = j \mid \mathbf x_{1:T})\\
&=\frac {P(y_t = i, y_{t+1} = j , \mathbf x_{1:T})}{P(\mathbf x_{1:T})}\\
&=\frac {P(y_t = i, y_{t+1} = j , \mathbf x_{1:T})}{\sum_{i,j}P(y_t = i, y_{t+1}=j,\mathbf x_{1:T})}\\
&= \frac {\alpha_t(i)t_{i,j}e_{j,x_{t+1}}\beta_{t+1}(j)}{\sum_{i=1}^Y\sum_{j=1}^Y\alpha_t(i)t_{i,j}\beta_{t+1}(j)e_{j,x_{t+1}}}\\
\end{align}
$$

    (Firstly forward to $t$ with state $y_{t} = i$ and observation $\mathbf x_{1:t}$, then transition to $t+1$ with state $y_{t+1} = j$, and emit observation $x_{t+1}$，then forward to $T$ with observation $\mathbf x_{t+2:T}$)

- Calculate:
    The probability of $y_t = i$:
    $\gamma_t (i) = \sum_{j=1}^Y\xi_t (i, j)$ (which corresponds to $\pi_i$)
    The expected times for state stayed in $Y = i$:
    $\sum_{t=0}^{T-1}\gamma_t (i)$
    The expected times for state transition $i-j$:
    $\sum_{t=0}^{T-1}\xi_t (i, j)$
- Re-estimate all paramters:
    $$
\begin{align}
t_{i,j} &= \frac {\sum_{t=0}^{T-1}\xi_t(i,j)}{\sum_{t=0}^{T-1}\gamma_t(i)}\\
e_{i,x} &=\frac {\sum_{t=0}^T\mathbf I(x_t = x)\gamma_t(i)}{\sum_{t=0}^{T-1}\gamma_t(i)}
\end{align}
    $$

- Repeat above steps until convergence:
    $|\log P (X\mid \theta) - \log P (X\mid \theta^0)| < \epsilon$

Generate simulated data using HMMs:
- Find the initial probability for hidden states
- MCMC
    MCMC are class of algorithms based on constructing a Markov chain whose equilibrium distribution is our desired distribution. Therefore, the state of the chain after a large number of steps is then used as a sample from our desired distribution
    A good Markov chain's stationary distribution can be reached quickly starting from an arbitary point.

Max entropy Markov Models:
    termed MEMM or CMM (conditional Markov Models)
    MEMM is a discriminative model which extends the standard maximum entropy model by assuming the unkown value to be learned are connected in a Markov chain rather than conditionally independent of each other ($Y^i$ 之间有联系，而不是互相条件独立)
    MEMM cares about $P (Y\mid X)$, standard Markov Model cares about $P (Y, X)$, that's the differenct

Hopfield Network:
    the predecessor of RNN

Markov blanket: 一个变量 $S$ 的 Markov blanket 指一个变量集合，满足给定该集合内的变量为条件，$S$ 和网络中所有其他变量条件独立

# 5 Markov Networks
Markov 网络编码的全局独立性假设 (Global Markov assumptions)： 
$\mathcal I (\mathcal H) = \{(\pmb X \perp \pmb Y \mid \pmb Z) : \text{sep}_{\mathcal H}(\pmb X; \pmb Y\mid \pmb Z)\}$

Factor: 因子是函数，将某个变量集合 $\pmb D$ 的赋值映射到某个实数，即
$f: Val(\pmb D) \mapsto \mathbb R^+$；要求 $\pmb D$ 中的变量应该直接地相互依赖

显然，条件概率分布也符合 factor 的定义，故可以看作是 factor 的特例 
(Factors generalize the notion of CPDs: Every CPD is a factor)

Markov 网络中，factor 对应的变量集合 $\pmb D$ 必须在图中构成一个团，如果不是团，则 $\pmb D$ 中就会存在没有直接相互依赖的变量对
团是 Markov 网络的基本局部结构

Maximal clique
    一个极大团中的多个子团对应的 factor 的乘积可以由该极大团对应的 factor 表示
    例如，考虑极大团 $\{A, B, C\}$，其中有子团 $\{A, B\}, \{B, C\}$，我们可以定义一个 factor $\pi[A, B, C]$ 来直接表示 $\pi_1[A, B, C]\pi_2[A, B]\pi_3[B, C]$

I-map to factorization
    给定无向图 $\mathcal H$，如果 $\mathcal H$ 是 $P$ 的 I-map，则 $P$ 可以分解为 $P (\pmb X) = \frac 1 Z\prod \pi_i[\pmb D_i]$，其中 $\pmb D_i$ 是 $\mathcal H$ 中的团

factorization to I-map
    给定无向图 $\mathcal H$ ，如果 $P$ 可以分解为 $P (\pmb X) = \frac 1 Z \prod \pi_i[\pmb D_i]$，则 $\mathcal H$ 是 $P$ 的 I-map

$Z$ 被称为分区函数，可以理解为系统的总能量，用于 normalize

分解于 $\mathcal H$ 的分布 $P$ 就称为 $\mathcal H$ 上的 Gibbs 分布

Example: Pairwise Markov Networks
图 $\mathcal H$ 上的成对 Markov 网络包含：
    一组定义于单个节点的节点因子/势能函数，记作 $\{\pi[X_i],i=1,\dots, n\}$
    一组定义于边的边因子/势能函数，记作 $\{\pi[X_i, X_j], X_i, X_j\in\mathcal H\}$
    Gibbs 分布写为 $P = \frac 1 Z \prod_i \pi[X_i] \prod_{(i,j)} \pi[X_i, X_j]$

能量势能函数一般习惯使用指数形式，故对原表示进行指对数转化：
$\pi[\pmb D] = \exp (-\epsilon (\pmb D))$，其中 $\epsilon (\pmb D) = -\ln [\pi[\pmb D]]$
在对数表示下，Gibbs 分布重写为：

$$
\begin{align}
P (\pmb X) &= \frac 1 Z \prod_i \left\{\exp(-\epsilon (\pmb D_i))\right\}\\
&=\frac 1 Z \exp\left\{\sum_{i}-\epsilon(\pmb D_i)\right\}\\
&=\frac 1 Z \exp\left\{-\sum_{i}\epsilon(\pmb D_i)\right\}
\end{align}$$

$\pi[\pmb D]$ 越高，势能函数 $\epsilon (\pmb D) = -\ln \pi[\pmb D]$ 越低

HC 定理
对于无向图 $\mathcal H$，定义 $Q$ 函数：

$$
Q(x\mid \text{NB}(x)) = \ln\left[\frac {P(x\mid \text{NB(x)})}{P(x=0\mid \text{NB(x)})}\right]
$$

 $\mathcal H$ 是正分布 $P$ 的 I-map 等价于 $Q$ 函数存在唯一的展开：

$$
Q(\pmb x) = \sum_i x_i \psi_i(x_i) + \sum_{i,j}x_ix_j\psi_{i,j}(x_i,x_j) + \cdots +  x_1x_2\dots x_n \psi_{1,2\dots ,n}(x_1,x_2,\dots, x_n)
$$

其中 $\psi_{s} \ne 0$ 当且仅当所有的 $v\in s$ 在 $\mathcal H$ 中构成一个团

Notes about HC theorem:
- ground state: 当某个变量 $X_i$ 处于 ground state (等于零)，所有和它相关的势能都变为零 ($x_i = 0$ 乘以任何数都得到零)
- $Q$ 函数描述了相对于 ground state 的相对概率
- $Q$ 函数连接了由图编码的独立性和由分布编码的独立性，它们是等价的
- 定义好 Markov 网络中每个团的势能函数 $\psi ()$，就可以写出 Gibbs 分布 (log-linear format)

HC 定理允许我们看图说话，给定无向图 $\mathcal H$，我们可以根据枚举图中的团写出在图上分解的 Gibbs 分布 $P$，分布 $P$ 满足 $\mathcal I (P) \subseteq \mathcal I (\mathcal H)$

An example with 3 binary variables:
$\pmb x$ 的 $Q$ 函数写为：
$Q (\pmb x) = \ln\left[ \frac {P (\pmb x)}{P(\pmb x = \pmb 0)}\right] = \sum_{i=1,2,3} \alpha_i x_i + \sum_{i<j = 1, 2, 3}\alpha_{i,j}x_ix_j + \alpha_{1, 2, 3}x_1x_2x_3$

$x_1$ 的 $Q$ 函数写为：

$$\begin{align}
Q (x_1\mid x_2, x_3)
&= \ln\left[\frac {P(x_1\mid x_2, x_3)}{P(x_1 = 0\mid x_2, x_3)}\right]\\ 
&=\ln\left[\frac {P(x_1, x_2, x_3)}{P(x_1 = 0 ,x_2, x_3)}\right]\\
&=\ln\left[\frac {P(x_1, x_2, x_3)/P(0,0,0)}{P(x_1 = 0 ,x_2, x_3)/P(0,0,0)}\right]\\
&=\ln\left[\frac {P(\pmb x)}{P(\pmb x = \pmb 0)}\right]- \ln\left[\frac {P(x_1 = 0, x_2, x_3)}{P(0,0,0)}\right]\\
&=Q(x_1, x_2, x_3) - Q(x_1 = 0, x_2, x_3)\\
&=\alpha_1x_1 + \alpha_{1,2}x_1x_2 + \alpha_{1, 3}x_1x_3 + \alpha_{1,2,3}x_1x_2x_3
\end{align}$$

图 $\mathcal H$ 中编码了 $(X_1 \perp X_2 \mid X_3)$ $\Longleftrightarrow$ $Q (x_1\mid x_2, x_3)$ 展开式中关于 $x_3$ 的项应该是零，即 $\alpha_{1, 3}, \alpha_{1, 2, 3} = 0$

对于仅由二元变量构成的系统，可以将所有的势能函数 $\psi ()$ 替换为单个参数而不失一般性，因为 $\psi ()$ 仅在全部相关变量都取为 1 时才可以为 $Q$ 函数做出贡献，此时 Gibbs 分布可以写为：

$$
\begin{align}
P(X) & = \frac 1 Z \exp(-U)\\
U&=-\sum_i\left(\beta_j\prod_{x_j \in C_i}x_j\right)\\
Z&= \sum_X\exp(-U)
\end{align}
$$

# 6 Examples for Advanced PGM Representation
表示：将 $P$ 映射到 $\mathcal H$ 或 $\mathcal G$
    贝叶斯网络：有向无环图
    Markov 网络：无向图
    动态/序列模型：有环图（实际上按时间轴展开也得到的是有向无环图）
    连续分布的概率图模型：高斯图模型

Markov 网络
    基本结构 (local structure)：团，每个团对应于一个正的局部因子 (local factor)
    联合概率分布：传统的 Gibbs 分布表示、Log-linear 表示
    Markov blanket：所有邻居节点
(Log-linear 表示：
$P(\pmb x) = \frac 1 Z e^{-U (\pmb x)}$
$U (\pmb x) =  - \sum_{C_i}\left[\psi_i(C_i)\prod_{x_j \in C_i}x_j\right]$ 
仅适用于离散分布)

Bayesian 网络
    基本结构：父节点 -> 子节点，对应于一个 CPD
    联合概率分布：所有局部 CPD 的乘积
    Markov blanket：所有父节点、所有子节点、所有子节点的所有父节点

“表示”三步走：
    1. 定义随机变量
    2. 绘制图模型拓扑结构
    3. 确定局部概率模型

**Model Conditional Information**
Example: 
POS (part of speech) identification 词性标注
早期使用 HMM，将 POS 建模为隐状态，将词建模为观测
HMM 的缺陷：
在 HMM 中，我们要建模给定 POS，它生成不同的词的概率，故观测的维度太高了（等于词的数量）
HMM 将问题建模为一个生成式问题，建模的是联合概率 $P (X, Y)$，而我们实际仅需要生成模型，并不需要知道给定 POS，生成某个词的概率

Generative Model:
    建模所有变量的联合概率
    缺点：困难

Discriminative Model:
    仅建模条件概率 $P(Y\mid X = x)$（其中 $X$ 是观测变量，$Y$ 是状态变量），
    目的是推理状态变量 $Y$（例如标签）
    优点：简单
    缺点：决策边界上的 outlier 会较大程度影响决策边界；无法生成新数据
    (you don't really konw how the things work, unless you can make one
    you don't really konw what you are working, unless you can make anybody understand)

Example:
Max Entropy Markov Model (MEMM)
判别模型，该模型利用 Markov 链拓展了最大熵分类器，假定了隐变量通过 Markov 链相连，而不是互相条件独立

但是该模型效果不理想被抛弃了

Example:
Generalized Conditional BNs
对 BN 中的全部节点都添加一个父节点，称为条件节点，该节点永远被观测到
因为该节点永远被观测到，故对图的实际分解并没有影响，在原来的每个 CPD 的条件中加上该节点即可，同时所有 CPD 乘起来的联合分布也是在给定该条件时的条件分布

Conditional MNs (CRFs)
对 MN 中的全部节点都添加一个邻居节点，该节点永远被观测到，故也不会影响 MN 中的节点的 Markov blanket


Example Recall: POS
考虑表示以下特性（这些特性是语言本身的特点）：
- 如果词在句子开始，POS 为 noun 的概率上升
- 如果词不在句子开始，且首字母大写，POS 为 noun 的概率上升
- 如果上一个词是 vt，该词是 noun 的概率上升

考虑使用 CRF 模型解决 POS 问题：
其中条件节点 $\pmb X = X_1, \dots, X_n$ 表示观测到的词序列
状态节点 $Y_1, \dots, Y_n$ 以 Markov 链两两相连，且都与条件节点 $\pmb X$ 相连

CRF 中的 feature function 定义为 indicator function，该 indicator function 是关于一个状态和一个观测子序列的 indicator，例如 $f_k (X, Y_i) = 1$ 仅当 $Y_i = noun$ 且当前观测 $X$ 是首字母大写的时候成立
此时，每个 feature function 都编码了 Markov 链中的一个或两个相邻节点以及观测 $\pmb X$ 的一个子序列

注意这些 feature function 的设计都是直接根据语言特性和经验人为设计得到的

**Deep structures**
Shallow models:
    why shallow?
    用于训练判别式模型的数据量较少，因此需要减少过拟合的结构风险 ( Reduce structure risk for overfitting )，也就是模型不能太复杂

Deep models:
    why deep?
    有大量数据可以用于训练生成式模型，模型复杂度应该与数据复杂度匹配

# 7 Inference as Optimization: Cluster Graph & Belief Propagation
Inference: 
给定观测 $\pmb E = \pmb e$，推断后验概率分布 $P(\pmb Y \mid \pmb E = \pmb e)$，或推断 $\arg\max_{\pmb y} P(\pmb Y = \pmb y \mid \pmb E = \pmb e)$ (极大后验推断/MAP)
MAP 推断不需要求出完整的分布，仅需要进行一个点估计

例子：在链式的贝叶斯网络中求 $e$ 的边际分布 $P(e)$

$$
\begin{align}
P(e) &= \sum_d\sum_c\sum_b\sum_a P(a,b,c,d,e)\\
&= \sum_d\sum_c\sum_b\sum_a P(a)P(b\mid a)P(c\mid b)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_c\sum_b\left(\sum_a P(a)P(b\mid a)\right)P(c\mid b)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_c\sum_bp(b)P(c\mid b)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_c\left(\sum_bp(b)P(c\mid b)\right)P(d\mid c)P(e\mid d)\\
&=\sum_d\sum_cp(c)P(d\mid c)P(e\mid d)\\
&=\sum_d\left(\sum_cp(c)P(d\mid c)\right)P(e\mid d)\\
&=\sum_dp(d)P(e\mid d)\\
&=p(e)
\end{align}
$$

此即消元法的思想

从消元法的思想可以进一步理解 HMM 的前向推理算法，其本质也是沿着链路不断消元，并将消元得到的信息向后传递的过程
VE: Forward Algorithm (compute $P(X\mid \theta)$)
Initialization: $\alpha_1(i) = \pi_i e_{i, x_1}$
Induction: $\alpha_{t+1}(i) = \left(\sum_{j=1}^N \alpha_t(j) t_{j, i}\right) e_{i, x_{t+1}}$
Termination: $P(X\mid \theta) = \sum_{i=1}^N \alpha_T(i)$

($\alpha_t(i) = P(x_1, \dots, x_t, y_t = i\mid \theta)$)

进一步，考虑在任意的贝叶斯网络中利用消元法进行推断
VE: General Bayesian Network
We want to compute $P(D)$, thus we need to eliminate $V, S, X, T, L, A, B$

The factorized joint distribution is:
$P(V )P(S)P(T \mid V )P(L \mid  S)P(B \mid  S)P(A \mid T, L)P(X \mid  A)P(D \mid  A, B)$

我们将 CPDs 都视作因子 (定义在一组变量上的函数)，将其重写为：
$\phi(V )\phi(S)\phi(T, V )\phi(L,S)\phi(B,S)\phi(A , T, L)\phi(X ,  A)\phi(D ,  A, B)$

我们按照图由上到下地对联合分布进行消元
首先消去 $V$，计算 $\sum_V \phi(V)\phi(T, V) = \tau_1(T)$
然后消去 $S$，计算 $\sum_S \phi(S)\phi(B, S)\phi(L, S) = \tau_2(L, B)$
然后消去 $X$，计算 $\sum_X \phi(X, A) = \tau_3(A)$
然后消去 $T$，计算 $\sum_{T}\tau_1(T)\phi(A, T, L) = \tau_4(A, L)$
然后消去 $L$，计算 $\sum_{L}\tau_4(A, L)\tau_2(B, L) = \tau_5(A, B)$
然后消去 $A$，计算 $\sum_A\tau_5(A, B)\tau_3(A)\phi(D, A, B) = \tau_6(B, D)$
最后消去 $B$，计算 $\sum_B \tau_6(B, D) = \tau_7(D) = P(D)$

按照这样的顺序消元的优势就在于每一次求和消元时的因子涉及到的变量都是最少的，注意求和消元的开销是和因子涉及变量数量成指数关系的
如果按照任意的顺序消元，例如先消去 $A$，则就需要计算 $\sum_A\phi(A, T, L)\phi(X, A)\phi(D, A, B)$，这涉及在一个作用域包含过多变量的因子中求和消去一个变量，开销会过大


Dealing with Evidence
假设给定观测 $V = v, S = s, D = d$，需要计算 $P(L, V = v, S = s, D = d)$
给定这些观测，我们写出此时的联合分布为：
$\phi (v )\phi (s)\phi (T, V )\phi (L, s)\phi (B, s)\phi (A , T, L)\phi (X ,  A)\phi (d ,  A, B)$

此时一些因子发生了简化/退化，例如 $\phi(V)$ 退化为常数 $\phi(v)$，$\phi(D, A, B)$ 退化为仅关于 $A, B$ 的因子 $\phi(d, A, B) = \tilde \phi(A, B)$

根据观测简化了相应因子后，我们执行和之前一样的消元法即可

Induced Graph in VE
在消元过程中出现的因子所涉及的变量应该互相连接
例如，在一开始将 CPDs 转化为因子时，$P(A\mid T, L)$ 转化为了 $\phi(A, T, L)$，因此原图中 $T, L$ 应该相连，也就是做了 Moralization
以及，消元时计算出了 $\tau_2(L, B)$ ，因此原图中的 $L, B$ 也应该相连

这就会得到 Induced Graph，VE 的导出图实际上就是对原图执行了两类操作，一类是 Moralization，对应于将 CPDs 转化为因子时的情况，一类是三角化，对应于根据新出现的因子连接节点的情况 (因为消元法一次消去一个节点，故对于四边形的结构，消去其中一个顶点一定会得到关于另外两个顶点的因子，故需要将二者相连，这就是三角化的过程，对于多边形也是如此，因此导出图中不会有多边形)

因此 Induced Graph 是 Moralized 且 Chordal

VE Disadvantages:
每次推理需要遍历整个图，效率较低

实际上消元时的一些中间结果是可以复用的

Re-Thinking VE Process:
考虑到 Induce Graph 的基本结构单元是 clique (maximal)，故可以将 maximal clique 上的边际分布作为中间结果存下来

将消元的过程视作消息传递
- 首先将初始消息 (local CPDs) 赋值给相关的 clique
- 然后消元法的过程就是每个 clique 消去自己独有的变量之后，将消息传递给相邻的 clique

Clique Tree: A Concrete Example
Clique Tree 的每个节点都对应于原图中的一个**极大** clique，我们将 local CPDs 分配给和它相关的 clique (也称为 factor)
(clique tree 也是对原来概率分布分解形式的一个等价表示，我们在原图中进行 moralization 和 triangulation/chordalization，得到 Induced Graph，然后根据 Induced Graph 中的 cliques 构造 clique tree)

Clique Tree 有两大重要特性：
- Tree and family perserving
    因子 (factors) $\phi_i$ 定义在 clique 上
    边 (edges) 定义在两个相连的 clique 的分离集 (共享的变量) $S_{i, j}$ 上 
- Running Intersecatoin property
    任意变量 $X$ 仅出现在 clique tree 中唯一的一个子图上，或者说一个变量在 clique tree 中一定是连通的，不会出现在两个分离的子图上 (否则在这样的图结构中进行推理会出现问题，无法确定要相信哪个子图的结果)

Message Passing: Sum Product on Clique Tree
目标为 $P(J)$
首先将每个簇/团的初始因子设定为和它关联的因子的乘积
$C_1$: 消去变量 $C$，发送消息 $\delta_{1\rightarrow 2}(D)$ 给 $C_2$
$C_2$: 消去变量 $D$，发送消息 $\delta_{2\rightarrow 3}(G, I)$ 给 $C_3$
$C_3$: 消去变量 $I$，发送消息 $\delta_{3\rightarrow 5}(G, S)$ 给 $C_5$
$C_4$: 消去变量 $H$，发送消息 $\delta_{4\rightarrow 5}(G, J)$ 给 $C_5$
$C_5$: 求和消去 $G, S, L$，得到 $P(J)$

最后汇总信息的团就是根团，我们从叶子团开始，逐步向内传递信息

Clique Tree 是原来分布的一个等价表示，在 Clique Tree 上执行 Sum-Product 消息传递等价于在原来分布执行消元法

Clique Tree Calibration
Clique Tree 的问题在于每次进行推理仍然需要执行完整的消息传递，要对树中的每个团再执行求和消元和信息传递操作
这实际上没有利用 Clique Tree 的结构特性，我们希望预先得到每个 Clique 上的边际分布，之后在进行推理仅需要关注一个 Clique 即可 

在 Clique Tree Calibration 过程分为 upward pass 和 downward pass
upward pass 中，我们选择任意一个团作为根团，从叶子节点向根团发送消息，完成 upward pass 后，我们实际上得到了根团上的边际分布
downward pass 中，我们从根团开始，向叶子节点传播消息 (注意朝向每个叶子节点的消息计算是不同的，计算时不会乘上从该叶子方向进来的消息)，完成 downward pass 后，我们等价于知道了每个团要计算其边际所需要的消息，因此实际得到了其余所有团的边际分布
(downward pass 本质上是复用了计算好了的消息)

因此校准算法仅用两次 pass 就计算出了全部 clique 的边际分布

Calibrated Clique Tree as Distribution
校准的 clique tree 除了存储了各个 clique 的边际分布，也可以视作整体未规范化的联合分布 $\tilde P_\Phi$ 的另一种表示，$\tilde P_\Phi$ 等于全部 cliques 的边际分布的乘积除去全部分离集上边际分布的乘积

To Random-Message Passing
在 Sum-Product 消息传递算法中，一个 clique 只有在 ready 状态，即收到了所有需要的输入消息之后，才可以发出消息

考虑让一个因子在不是 ready 状态也能向邻居发出消息

Message Passing: Belief Update
将从 $C_i$ 到 $C_j$ 发出的消息 $\delta_{i\rightarrow j}$ 视作将 $C_i$ 的信念 $\beta_i$ 乘上所有的消息，再除去 $\delta_{j\rightarrow i}$ ，再求和消去 $C_i - S_{i, j}$ 得到的结果

在信念更新算法中，每个 clique 直接维护自己的信念，每条边维护上一次发送的消息，即分离集上的信念 $\mu_{i, j}$
算法迭代中，每次都随机选取一条边，无论对应的 clique 是否是 ready 状态，都发送一条信息
算法的停止条件是发送信息不再会带来更新，即信息经过计算都得到的是 1，此时所有的 clique 都被校准了

Belief Update 算法主要涉及到近似推断
在精确推断中，Belief Update 算法和 Sum Product 算法等价

Answering Queries Outside a Clique
在 Clique Tree 中，找到包含所有 query 变量的最小的子树
通过 $\prod \beta_i / \prod \mu_{i, j}$ 计算子树上的边际分布 ($\beta_i$ 即子树涉及的 cliques 的信念，$\mu_{i, j}$ 即子树涉及的 sepsets 的信念)
在该边际分布上求和消去其余变量

Ansewering Queries with Increments
当引入一个新的观测 $Z = z$ 时，我们引入一个示性函数 $\mathbf I(Z = z)$ 作为新的因子
如果 query 变量 $X$ 和 $Z$ 在同一个 clique，则将该 clique 的信念和示性函数相乘，得到的就是 $Z = z$ 状态下的边际分布，此时求和消去其他变量即可

如果 $X$ 和 $Z$ 不在同一个 clique，我们将示性函数和某个包含 $Z$ 的 clique 的因子相乘，然后沿着该 clique 向包含 $X$ 的 clique 的路径传播消息，将 $\mathbf I(Z = z)$ 的信息传播到包含 $X$ 的 clique 中，之后求和消去其他变量即可

Clique Tree 的优势在于
- 其结构比原来的 BN/MN 更简单，推断更高效
- 其信念更新算法容易推广到近似推断

Constructing Clique Trees
我们想要构建具有族保持性质和运行相交性质的簇树，但这实际上是一个 NP-hard 问题

From Clique Tree to Loopy Cluster Graph
不考虑做 moralization 和 triangulation，也就是不考虑一定在 clique tree 上做精确推断，考虑直接在簇图上做近似推断 (簇图不一定是一颗树，是带环的)

环状的簇图无法定义根团，因此无法用类似 Sum-Product 的算法，故直接将 Belief Update 算法推广到簇图上，得到的就是 Belief Propagation 算法
该算法在簇图上没有理论保证，如果最后收敛了，就将结果视作对 clique 的边际的近似

loopy cluster graph 不一定要求是一颗树，但也要求满足运行相交性质，即某个变量 $X$ 仅能出现在一个子图中，不能出现在两个分离的子图中

设计 cluster graph 的一个方法是设计 bethe cluster graph，该设计可以保证运行相交性质

BP 算法不能保证收敛，也不能保证收敛后得到的信念是真实的边际分布

# 8 Inference as Optimization: Structured Variational Inference
变分法的基本思想：要在 $P(X)$ 中推理，考虑找到一个可以近似 $P(X)$ 的分布 $Q(X)$，在 $Q(X)$ 中进行推理，其中 $Q(X)$ 一般限制为一族具有简单形式的分布

一般使用 KL 散度 $D_{KL}(Q||P)$ 来衡量二者之间的差异

$$
D_{KL}(Q||P)  = E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X)}\right]
$$

优化的目标是找到最小化二者的 KL 散度的分布 $Q$，即 $\min_Q D_{KL}(Q||P)$

KL 散度不对称，且不满足三角不等式，因此严格来说不是一种距离度量


结构变分法：基于具有简单结构的图定义 $Q(X)$
例如，基于一个没有边的图定义 $Q(X)$，即平均场算法


关于 KL 散度的不对称性：
Reverse KL (I-projection): $D_{KL}(Q||P)$
Forward KL (M-projection): $D_{KL}(P||Q)$

Reverse KL 中，最优的 $Q(X)$ 不必要覆盖 $P(X)$ 的全部取值，只需要拟合好 $P(X)$ 的一个峰，就可以让 $D_{KL}(Q||P)$ 很小
Forward KL 中，最优的 $Q(X)$ 必须要覆盖 $P(X)$ 的全部取值，否则 $D_{KL}(P||Q)$ 会在 $Q$ 没有覆盖到且 $P > 0$ 的地方无穷大

一般使用 Reverse KL，因为对于 $Q$ 求期望显然比对于 $P$ 求期望容易


目标分布是条件分布的情况：
考虑目标分布为 $P(X\mid Z = z)$

$$
\begin{align}
D_{KL}(Q||P) &= E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X\mid z)}\right]\\
 &= E_{X\sim Q}\left[\ln \frac {Q(X)P(z)}{P(X,z)} \right]\\
 &= E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)} + \ln P(z) \right]\\
 &= E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)}\right] + \ln P(z) \\\\
 \ln P(z) - D_{KL}(Q||P) &=-E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)}\right]\\
&=-E_{X\sim Q}\left[\ln \frac {Q(X)}{P(X,z)}\right]\\
&=E_{X\sim Q}\left[\ln P(X,z)\right] + E_{X\sim Q} \left[-\ln Q(X)\right]\\
&=E_{X\sim Q}\left[\ln P(X,z)\right] + H(Q)\\
\end{align}
$$

我们的目标是找到最小化 $D_{KL}(Q||P)$ 的 $Q$，等价于找到最大化 $\ln P(z) - D_{KL}(Q||P)$ 的 $Q$，$\ln P(z) - D_{KL}(Q||P)$ 可以记作 $L(Q)$

等式 $L(Q)=  E_{X\sim Q}\left[\ln P(X, z)\right] + H(Q)$ 的 RHS 被称为能量泛函，它是关于分布 $Q$ 的泛函，因此我们的目标就是找到最大化能量泛函的分布 $Q$ (或者说找到最小化 $J(Q) = -L(Q)$ 的分布 $Q$)

考虑能量泛函中的两项：
第一项 $E_{X\sim Q}\left[\ln P (X, z)\right]$ 衡量了 $Q$ 和 $P(X, z)$ 之间的关系，显然这一项在 $Q(X) = P(X \mid z)$ 时最大

第二项 $H(Q)$ 衡量了 $Q$ 本身的熵，显然这一项在 $Q$ 是均匀分布 (diffuse) 时最大

显然第二项相对于第一项倾向于正则项，用于避免过拟合

因为 KL 散度非负，因此 $L(Q) = \ln P(z) - D_{KL}(Q||P) \le \ln P(z)$
故 $L(Q) = E_{X\sim Q}\left[\ln P(X, z)\right] + H(Q)$ 实际上是 $\ln P(z)$ 的一个下界，或者说，能量泛函的上界是 $\ln P(z)$，当且仅当 $D_{KL}(Q || P)$ 为零时，界取到

故我们也可以将能量泛函 $L(Q)$ 称为 Evidence Lower Bound Observation (ELBO)，表示它是观测的概率的下界，我们的目标是找到一个 $Q$ 最大化 ELBO

对于 $Q$ 的形式的选择，最简单的例子是认为所有的变量在 $Q$ 中都是相互独立的，也就是和 $Q$ 相关的图没有任何边，这就是平均场推理
对于更复杂的情况，我们可以在 $Q$ 中设计特定的结构，也就是结构变分推理
更进一步，我们不直接设计 $Q$ 的结构，直接用神经网络表示


Mean Field Variational Inference
当假定 $q(x)$ 的图表示中没有任何边时，我们实际上假设了联合分布中所有涉及到的变量都是相互独立的，因此联合分布就是各个变量边际分布的乘积，即

$$
q(x;\theta) = \prod_i q_i(x_i)
$$

我们将 $q(x)$ 的分解形式代入能量泛函中，得到

$$
\begin{align}
E_{X\sim q}\left[\ln p(X,z)\right] &= \sum_x q(x) \ln p(x,z)\\
&=\sum_x \left(\left[\prod_i q_i(x_i)\right] \ln p(x,z)\right)
\\\\
H(q) &=E_{X\sim q}\left[-\ln q(X)\right]\\
&=\sum_x q(x)\left[-\ln q(x)\right]\\
&=\sum_x \left(\left[\prod_i q_i(x_i)\right]\left[-\ln\prod_i q_i(x_i) \right]\right)\\
&=\sum_x\left( \left[\prod_i q_i(x_i)\right]\left[-\sum_i\ln q_i(x_i) \right]\right)
\end{align}
$$

因此，能量泛函为

$$
\begin{align}
L(q) &= E_{X\sim q}\left[\ln p(X, z)\right] + H(q)\\
&=\sum_x \left(\left[\prod_i q_i(x_i)\right]\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)
\end{align}
$$

考虑单个变量的边际分布 $q_j$，假定所有其他变量的边际分布 $q_{-j}$ 都固定，将能量泛函写为关于 $q_j$ 的形式

$$
\begin{align}
L(q_j) &= \sum_x \left(\left[\prod_i q_i(x_i)\right]\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j}\sum_{x_{-j}}\left(\left[\prod_i q_i(x_i)\right]\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j}\sum_{x_{-j}}\left(q_j(x_j)\prod_{i\ne j} q_i(x_i)\left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j} q_j(x_j)\left(\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \left[\ln p(x,z) - \sum_k \ln q_k(x_k)\right]\right)\\
&=\sum_{x_j} q_j(x_j)\left(\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x,z) - \prod_{i\ne j}q_i(x_i)\sum_k \ln q_k(x_k)\right)\\
&=\sum_{x_j} q_j(x_j)\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x,z) - \sum_{x_j}q_j(x_j)\sum_{x_{-j}}\prod_{i\ne j}q_i(x_i)\sum_k \ln q_k(x_k)\\
&=\sum_{x_j} q_j(x_j)\sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x,z) - \sum_{x_j}q_j(x_j)\sum_{x_{-j}}\prod_{i\ne j}q_i(x_i) \left[\sum_{k\ne j}\ln q_k(x_k) + \ln q_j(x_j)\right]\\
&=\sum_{x_j}q_j(x_j)\ln f_j(x_j) - \sum_{x_j} q_j(x_j) \ln q_j(x_j) + const
\end{align}
$$

其中 $\ln f_j(x_j) = \sum_{x_{-j}} \prod_{i\ne j} q_i(x_i) \ln p(x, z) = E_{-q_j}\left[\ln p(x, z)\right]$

因此，我们进一步将 $L(q_j)$ 写为

$$
\begin{align}
L(q_j) &= \sum_{x_j} q_j(x_j)\ln\left[\frac {f_j(x_j)}{q_j(x_j)}\right] + const\\
&= \sum_{x_j} q_j(x_j)\ln\left[\frac {\exp \left\{E_{-q_j}\left[\ln p(x,z)\right]\right\}}{q_j(x_j)}\right] + const\\
\end{align}
$$

我们考虑进行迭代式优化，先仅优化 $q_j$，目标是最大化 $L(q_j)$
观察发现 $L(q_j)$ 的形式和 KL 散度的形式完全一致，因此要最大化 $L(q_j)$，我们希望

$$
q_j(x_j) \propto \exp\left\{E_{-q_j}[\ln p(x,z)]\right\}
$$

An Example for Ising Model
考虑一个 Ising Model 的例子，在该模型中，因变量 $x_i$ 都是二元变量，Evidence $z_i$ 总是被观测到

我们需要求一个近似分布 $q(x)$，我们使用平均场模型，假设所有隐变量相互独立，并应用上述的推导过程，我们可以知道每次迭代优化要求解的 $q_j(x_j)$ 需要正比于 
$\exp\left\{E_{-q_j}[\ln p (x, z)]\right\}$，因此我们实际要计算的目标就是 $E_{-q_j}[\ln p (x, z)]$

我们的目标分布是 $p(x\mid z) \propto p(x) p(z\mid x) = p(x,z)$，在 Ising Model 中，我们有

$$
\begin{align}
p(x) &= \frac 1 Z e^{\sum_{i,j}w_{ij}x_ix_j} \\
&= \frac 1 Z e^{-E_0(x)}\\
\\
p(z\mid x) &= \prod_i p(z_i \mid x_i)\\
&=e^{\sum_i \ln p(z_i \mid x_i)}\\
&= e^{\sum_i L_i(x_i)}
\end{align}
$$

其中 $p(x)$ 没有考虑一阶项 (Ising Model 不考虑一阶项)
其中 $p(z\mid x)$ 的分解形式是因为 Ising Model 中每个 $z_i$ 在给定 $x_i$ 时和所有其他 $z_j$ 条件独立

因此 

$$
\begin{align}
\ln p(x, z) &= \ln p(x) p(z\mid x)\\
&=\ln \frac 1 Z e^{\sum_{ij}w_{ij}x_ix_j}\cdot e^{\sum_i L_i(x_i)}\\
&=\sum_{ij} w_{ij}x_ix_j + \sum_i L_i(x_i) + const
\end{align}
$$

接下来，我们需要考虑为 $\ln p(x, z)$ 相对于 $q$ 求期望，即计算 $E_{-q_j}[\ln p (x, z)]$

我们使用的平均场模型假设了 $q$ 中的独立性，但我们尚未假设 $q$ 的具体形式
考虑到所涉及的变量都是二值变量，我们可以将 $q_i$ 的形式假设为二项分布，因此 $q$ 就是多个二项分布的乘积，即

$$
\begin{align}
q(x;\theta) & = \prod_i q_i(x_i ; \theta_i)\\
&\text{where}\ \  q_i(x_i;\theta_i) = \begin{cases}
\theta_i& \text{if}\ x_i = 1\\
1-\theta_i& \text{if}\ x_i = -1
\end{cases}
\end{align}
$$

其中的各个 $\theta_i$ 就是需要优化求解的参数，在迭代式优化中，我们每次迭代优化一个 $\theta_i$

我们开始考虑 $E_{-q_j}[\ln p (x, z)]$

$$
\begin{align}
E_{-q_j}[\ln p (x, z)] &=E_{-q_j}\left[\sum_{kl} w_{kl}x_kx_l + \sum_k L_k(x_k) + const\right]\\
&=E_{-q_j}\left[\sum_{kl} w_{kl}x_kx_l + \sum_k L_k(x_k) \right] + const\\
&=E_{-q_j}\left[\sum_{k\in neighbor(j)} w_{kj}x_kx_j + \sum_k L_k(x_k) \right] + const\\
&=E_{-q_j}\left[\sum_{k\in neighbor(j)} w_{kj}x_kx_j + L_j(x_j) \right] + const\\
&=E_{-q_j}\left[\sum_{k\in neighbor(j)} w_{kj}x_kx_j\right] +E_{-q_j}\left[ L_j(x_j) \right] + const\\
&=x_j \left(\sum_{k\in neighbor(j)} w_{kj}E_{-q_j}\left[x_k\right]\right) + L_j(x_j) + const\\
&=x_j \left(\sum_{k\in neighbor(j)} w_{kj}(2\theta_k - 1)\right) + L_j(x_j) + const\\
&=x_j \sum_{i\in neighbor(j)} [w_{ij}(2\theta_i - 1)] + L_j(x_j) + const\\
\end{align}
$$

其中第三个等号来源于在迭代优化中除了 $x_j$ 以外其他 $x_k$ 的值认为是已知常量，故 $E_{-q_i}\left[\sum_{k\ne j, l\ne j} w_{kl}x_kx_l\right]$ 与 $x_j$ 无关，归为常量
第四个等号也类似，与 $x_j$ 无关的都直接归为常量

从这个结果可以发现只有在 $p$ 中和 $x_j$ 有直接关系的节点才会对 $q_j$ 的优化有影响

我们令 $m_j = \sum_{i\in neighbor (j)} [w_{ij}(2\theta_i - 1)]$，因此 $E_{-q_j}[\ln p(x, z)] = x_jm_j + L_j(x_j) + const$，故我们希望 

$$q_j(x_j) \propto \exp\left\{E_{-q_j}[\ln p(x, z)]\right\} = \exp\{x_jm_j + L_j(x_j) + const\}$$

不妨设 $q_j (x_j)  = t\cdot \exp\left\{E_{-q_j}[\ln p (x, z)]\right\} = \exp\{x_jm_j + L_j (x_j) + const\}$，其中 $t$ 为任意非零常数，则

$$
\begin{cases}
\theta_j = t\cdot  \exp\{m_j + L_j(1) + const\} & x_j = 1\\
1-\theta_j = t \cdot \exp\{-m_j + L_j(-1) + const\} &x_j= -1
\end{cases}
$$

容易求得

$$
\theta_j = \frac {\theta_j}{1-\theta_j + \theta_j} = \frac {\exp\{m_j + L_j(1) \}}{\exp(-m_j + L_j(-1)) + \exp\{m_j + L_j(1)\}} 
$$

其中 $L_j(x_j) = \ln p(z_j \mid x_j), m_j = \sum_{i\in neighbor (j)} [w_{ij}(2\theta_i - 1)]$

对于 General Structured Variational Inference，推导的过程是类似的，差异仅在于对 $q$ 的分解表示，这依赖于我们具体假设的结构

变分法的缺点：简单的 $Q$ 族可能难以捕获 $P$ 中的复杂模式；复杂的 $Q$ 族则难以优化，容易过拟合

进一步拓展，我们可以不显式规定 $Q$ 的具体形式，而是认为 $Q$ 是某个简单分布 $Z$ (例如高斯) 经过一个泛函转化而来，即 $Q = f(Z)$，其本质思想仍然是用 $Q$ 拟合 $P$ 的变分思想，但该形式显然具有更多的灵活性
我们用神经网络拟合泛函 $f(\cdot)$，显然，网络越复杂，$Q$ 的搜索空间就越大，相较于限制 $Q$ 到某一具体的形式，该方法显然具有更大的灵活性，也不需要我们做出各种显式的假设，但使用这类方法也需要注意过拟合的问题

# 9 Monte Carlo Approximate Inference
Monte Carlo Approximation
基本思想：从分布 $P(\pmb X)$ 中生成样本 $\xi[1], \dots, \xi[M]$，利用平均值估计期望值 $E_p[f(\pmb X)] \approx \frac 1 M \sum_{m=1}^M f(\xi[m])$

该策略能成立的理由来自于频率学派对概率的定义：概率就是频率的极限值

Monte Carlo 近似方法的关键在于采样，或者说如何高效从后验分布 $P(\pmb Y\mid \pmb E = \pmb e)$ 中生成样本


Forward Sampling
利用贝叶斯网络的结构实现从 $P(\pmb X)$ 中生成样本
根据拓扑结构，依次采样


Forward Sampling with Rejection
进一步，前向采样得到的样本满足它们是服从 $P(\pmb X)$ 的独立同分布样本，但在给定观测/证据 $\pmb E = \pmb e$，我们希望从后验分布 $P(\pmb Y \mid \pmb E = \pmb e)$ 中采样，也就是得到的样本是服从 $P(\pmb Y\mid \pmb E = \pmb e)$ 的独立同分布的样本

注意到后验分布 $P(\pmb Y \mid \pmb E =\pmb e) \propto P(\pmb Y, \pmb e)$，因此样本是服从 $P(\pmb Y, \pmb e)$ 的独立同分布样本也满足我们的要求

一个思路就是前向采样 + 拒绝采样，我们利用前向采样从 $P(\pmb X)$ 中得到大量样本，筛选出其中满足 $\pmb E = \pmb e$ 的样本，也就是拒绝 $\pmb E \ne \pmb e$ 的样本
这样留下来的样本，我们认为它们就是服从 $P(\pmb Y , \pmb e)$ 的独立同分布样本，我们用这些样本进行相对于 $P(\pmb Y, \pmb e)$ 的 Monte Carlo 估计

该方法的问题在于如果 $P(\pmb E = \pmb e)$ 的概率很小，生成的样本中满足 $\pmb E = \pmb e$ 的数量往往会很少，拒绝的样本数量会太多，大部分生成的样本都被浪费了

Likelihood Weighting
重新回顾前向采样 + 拒绝采样的过程，我们首先做前向采样，根据 $P(\pmb X)$ 的分解形式 $P(\pmb X) = \prod_i P(X_i\mid \text{Pa}_{X_i})$，随着拓扑结构逐个采样 $X_i$ 的值，最后得到样本
如果该样本不满足 $\pmb E = \pmb e$，则拒绝该样本，否则留下该样本

考虑前向采样过程中对观测变量 $E_j \in \pmb E$ 的采样过程，我们从 CPD $P(E_j\mid \text{Pa}_{E_j})$ 随机采样获得 $E_j$ 的采样值 $e'_j$ 
一个样本不满足 $\pmb E = \pmb e$，意味着在某个或者多个 $E_j$ 的采样过程中，$E_j$ 没有采样得到观测值 $e_j$，也就是 $e'_j \ne e_j$
对于一个样本，在前向采样过程中，其 $E_j$ 采样得到 $e_j$ 值的可能性为 $P(E_j = e_j\mid \text{Pa}_{E_j})$，一个样本要被接受，则其所有的 $E_j$ 都需要采样到正确值，这件事发生的概率是 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$，也就是每个样本都有 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ 的概率被留下

注意，对于每一个样本，因为在采样时对于观测变量 $E_j$ 的父变量 $\text{Pa}_{E_j}$ 的采样值不同，因此 $E_j$ 的 CPD 就不同，故 $P(E_j = e_j \mid \text{Pa}_{E_j})$ 的值在不同的样本之间是不同的
例如，$P(E_j = e_j\mid \text{Pa}_{E_j})$ 在 $\text{Pa}_{E_j}$ 取某特定值 $\alpha$ 的概率会为 $0.8$，取另一特定值 $\beta$ 时的概率则仅为 $0.2$
这一相关性会在我们的前向采样 + 拒绝采样中留下的样本中观察到，例如留下的 $10000$ 个样本中 $\text{Pa}_{E_j}$ 取 $\alpha$ 的占了全部的 80%，$\text{Pa}_{E_j}$ 取 $\beta$ 占了全部的 20%

我们想要改进拒绝采样方法，故我们考虑在采样时通过将 $\pmb E$ 的采样值直接设定为 $\pmb e$ 来每个样本都直接留下，这样留下的样本中将无法体现这种相关性，因为无论 $\text{Pa}_{E_j}$ 取 $\alpha$ 还是 $\beta$，在统计上，这些样本中的 $P(E_j = e_j \mid \text{Pa}_{E_j})$ 的概率都为 1
为了在这样留下来的样本中体现这种相关性，或者说分布性质，我们应该根据每个样本的 $\text{Pa}_{E_j}$ 值为每个样本加权，权重就是 $P(E_j = e_j \mid \text{Pa}_{E_j})$
此时，在这些留下的样本中考虑权重进行统计，我们仍然能得到 $\text{Pa}_{E_j}$ 取 $\alpha$ 的样本出现的次数比 $\text{Pa}_{E_j}$ 取 $\beta$ 的样本的出现次数约为 80% : 20%

因此，我们在前向采样过程中，直接将观测 $\pmb E$ 设定为 $\pmb e$，使得每个样本都留下，但需要为它乘上它留下的概率 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ ，表示该样本实际仅有 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ 的权重，或者说这个样本不能算作一个样本，而是 $\prod_j P(E_j = e_j \mid \text{Pa}_{E_j})$ 个样本


Importance Sampling
从 $Q$ 中采样而不是从 $P$ 中采样
$Q$ 需要满足 $P(x) > 0 \Rightarrow Q(x) > 0$，换句话说，$Q$ 需要是 $P$ 的支撑集，$Q$ 的空间需要覆盖 $P$ 的空间，不允许出现 $P$ 有定义但 $Q$ 没有定义的点

使用重要性采样的一个原因是原分布 $P$ 较复杂，不容易从中采样，这主要针对的是无向图定义的分布，因为无向图定义的分布涉及到了划分函数 $Z$，会使得实际的采样过程变得复杂；对于有向图，大部分情况我们可以使用之前介绍的前向采样和似然加权方法执行采样，一般不需要用到重要性采样
另一个原因时可以用一个 $Q$ 对应多个 $P$，例如 $Q$ 为简单的均匀分布，我们仅编写从均匀分布中进行采样的程序，对于不同的 $P$，我们都从 $Q$ 中采样，然后用重要性采样技术对样本进行加权，模拟从 $P$ 中采样

Unnormalized Importance Sampling
如果 $Q$ 满足 $P(x) > 0 \Rightarrow Q(x) > 0$，我们可以得到以下等价关系

$$
\begin{align}
&E_{P(\pmb X)}[f(\pmb X)]\\
=&\sum_{\pmb x\in Val(\pmb X)}P(\pmb x)f(\pmb x)\\
=&\sum_{\pmb x\in Val(\pmb X)}Q(\pmb x)f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}\\
=&\sum_{\pmb x\in Val(\pmb X)}Q(\pmb x)\left(f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}\right)\\
=&E_{Q(\pmb X)}\left[f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}\right]
\end{align}
$$

因此我们将 $f(\pmb X)$ 相对于 $P$ 的期望转化为了 $f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}$ 相对于 $Q$ 的期望
可以将 $\frac {P(\pmb X)}{Q(\pmb X)}$ 视为权重，对于从 $Q$ 从采样得到的一个样本 $\pmb x$，如果 $P$ 在此处的概率 $P(\pmb x)$ 大于 $Q(\pmb x)$，则该样本的权重相应升高，反之则相应降低

我们从 $Q$ 中采样 $M$ 个样本后，用均值估计期望 $E_{Q(\pmb X)}\left[f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}\right] = E_{P(\pmb X)}[f(\pmb X)]$，即

$$
E_{P(\pmb X)}[f(\pmb X)]\approx \frac 1 M \sum_{m=1}^Mf(\pmb x[m])\frac {P(\pmb x[m])}{Q(\pmb x[m])}
$$

该估计器的方差随着样本数量 $M$ 上升而下降
该估计器的方差也随着 $Q$ 和 $P$ 之间的相似程度而变化，$Q=P$ 时的估计器是方差最小的

Normalized Importance Sampling
未规范化的重要性采样假定分布 $P$ 是已知的
但实践中我们往往仅知道 $P$ 的未规范化的形式 $P'$，$P$ 和 $P'$ 之间相差一个规范化常数 $P = P'/\alpha$ (显然我们可以通过 $\alpha = \sum_{\pmb x}P'(\pmb x)$ 求出规范化常数，但实践中完全遍历概率空间的所有取值往往不可行)

例如，实践中我们往往仅知道先验分布 $P(\pmb X, \pmb e)$，而我们计算关于后验分布 $P(\pmb X \mid \pmb e)$ 的期望，显然先验分布和后验分布之间相差一个规范化常数 $\alpha = P(\pmb e)$，即 $P(\pmb X\mid \pmb e) = P(\pmb X, \pmb e)/P(\pmb e)$

因此，为了得到后验分布，我们需要先得到规范化常数 $\alpha$，我们避免直接计算的大量开销，考虑使用采样的方式对其进行估计
容易知道

$$\alpha = \sum_{\pmb x}P'(\pmb x) = \sum_{\pmb x}P'(\pmb x)f(\pmb x) = E_{P'(\pmb X)}[f(\pmb X)]$$

其中 $f(\cdot)$ 为恒等函数 $f\equiv 1$，因此 $\alpha$ 可以视作恒等函数 $f$ 相对于 $P'$ 的期望值

这样的说法实际上不严谨，因为 $P'$ 作为未规范化的分布，一般并不是一个合法的分布 (如果 $P'$ 合法，则 $\alpha=1$)，但无论如何，我们可以将 $\alpha$ 进一步写为关于 $Q$ 的期望的形式，$Q$ 是合法的采样分布

$$
\begin{align}
\alpha &= \sum_{\pmb x\in Val(\pmb X)}P'(\pmb x)\\
&=\sum_{\pmb x\in Val(\pmb X)}Q(\pmb x)\frac {P'(\pmb x)}{Q(\pmb x)}\\
&=E_{Q(\pmb x)}\left[\frac {P'(\pmb X)}{Q(\pmb X)}\right]
\end{align}
$$

那么我们就可以用重要性采样对该期望值 (也就是 $\alpha$ 的值) 进行估计，故得到

$$
\alpha \approx \frac 1 M\sum_{m=1}^M f(\pmb x[m])\frac {P'(\pmb x[m])}{Q(\pmb x[m])}
$$

除了仅仅估计 $\alpha$ 以外，我们可以在仅知道先验分布的情况下直接考虑用重要性采样估计相对于后验分布的期望值
例如，假设我们需要估计 $E_{P(\pmb X)}[f(\pmb X)]$，我们可以推导出

$$
\begin{align}
E_{P(\pmb X)}[f(\pmb X)]
&=\sum_{\pmb x\in Val(\pmb X)}P(\pmb x)f(\pmb x)\\
&=\sum_{\pmb x\in Val(\pmb X)}\frac {P'(\pmb x)}{\alpha}f(\pmb x)\\
&=\frac 1 \alpha\sum_{\pmb x\in Val(\pmb X)} {P'(\pmb x)}f(\pmb x)\\
&=\frac 1 \alpha\sum_{\pmb x\in Val(\pmb X)} Q(\pmb x)f(\pmb x)\frac {P'(\pmb x)}{Q(\pmb x)}\\
&=\frac 1 \alpha E_{Q(\pmb X)}\left[f(\pmb  X)\frac {P'(\pmb X)}{Q(\pmb X)}\right]\\
&=\frac {E_{Q(\pmb X)}\left[f(\pmb X)\frac {P'(\pmb X)}{Q(\pmb X)}\right]}{E_{Q(\pmb X)}\left[\frac {P'(\pmb X)}{Q(\pmb X)}\right]}
\end{align}
$$

显然目标期望是两个相对于 $Q$ 的期望的商，我们可以通过采样估计分母和分子两个期望，然后将其商作为对目标期望的估计

给定 $Q$ 中的 $M$ 个样本，我们可以用这些样本同时估计分母和分子，即

$$
\begin{align}
E_{P(\pmb X)}[f(\pmb X)]&\approx\frac {\frac 1 M\sum_{m=1}^M f(\pmb x[m])\frac {P'(\pmb x[m])}{Q(\pmb x[m])}}{\frac 1 M\sum_{m=1}^M \frac {P'(\pmb x[m])}{Q(\pmb x[m])}}\\
&=\frac {\sum_{m=1}^M f(\pmb x[m])P'(\pmb x[m])/Q(\pmb x[m])}{\sum_{m=1}^MP'(\pmb x[m])/Q(\pmb x[m])}
\end{align}
$$

Importance Sampling for BNs
我们了解了重要性采样在数学上的形式，接下来考虑如何在贝叶斯网络中应用它

对于一个贝叶斯网络 $\mathcal G$，它定义了分布 $P$，给定 evidence $\pmb E = \pmb e$，我们想要计算某个函数相对于后验分布 $P(\pmb X \mid \pmb e)$ 的期望

根据之前的推导，我们令后验分布 $P(\pmb X \mid \pmb e)$ 为目标分布 $P(\pmb X)$，令先验 $P(\pmb X, \pmb e)$ 为目标分布未规范化的形式 $\tilde P(\pmb X)$，则如果使用重要性采样方法，关于 $P(\pmb X \mid \pmb e)$ 的期望可以用以下式子估计

$$
E_{P(\pmb X)}[f(\pmb X)] \approx \frac {\sum_{m=1}^M f(\pmb x[m])P'(\pmb x[m])/Q(\pmb x[m])}{\sum_{m=1}^MP'(\pmb x[m])/Q(\pmb x[m])}
$$

注意先验分布 $\tilde P(\pmb X) = P(\pmb X, \pmb e)$ 存在以下分解形式

$$
\begin{align}
P(\pmb X, \pmb e)
&=\prod_{X_i\in \pmb E}P(X_i =\pmb e\langle X_i\rangle\mid \text{Pa}_{X_i})\prod_{X_j \not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})\\
&=\prod_{X_i\in \pmb E}P(X_i =e_i\mid \text{Pa}_{X_i})\prod_{X_j \not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})
\end{align}
$$

其中 $e_i$ 表示 $\pmb e\langle X_i\rangle$

我们接着考虑采样分布 $Q$ 的选择
我们将 $\pmb E$ 中的变量和其父节点的连接边全部去除，构造其残缺化的网络 $\mathcal G_{\pmb E = \pmb e}$，其中 $\pmb E$ 中的变量的 CPD 都设定为确定的 CPD，即取到观测值的概率为 1，取其他值的概率为零
我们在 $\mathcal G_{\pmb E = \pmb e}$ 中进行采样，或者说，我们将 $\mathcal G_{\pmb E = \pmb e}$ 定义的分布作为采样分布 $Q$

容易知道 $Q$ 存在以下分解形式

$$
\begin{align}
Q(\pmb X) &= \prod_{X_i\in \mathcal X}P(X_i\mid \text{Pa}_{X_i})\\
&=\prod_{X_i\in \pmb E}P(X_i\mid \text{Pa}_{X_i})\prod_{X_j\not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})\\
&=\prod_{X_i\in \pmb E}\mathbf 1\{X_i=\pmb e\langle X_i\rangle\}\prod_{X_j\not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})\\
&=\prod_{X_i\in \pmb E}\mathbf 1\{X_i=e_i\}\prod_{X_j\not\in \pmb E}P(X_j\mid \text{Pa}_{X_j})
\end{align}
$$

其中第三个等号是因为 $\pmb E$ 中的变量 $X_i$ 的 CPD 都被修改为了确定性分布，而 $\pmb E$ 以外的变量的 CPD 保持不变

接着，假设我们从 $Q$ 中采样了一个样本 $\pmb x$，我们考虑它的重要性权重

$$
\begin{align}
\frac {P'(\pmb x)}{Q(\pmb x)}&=\frac {\prod_{X_i\in \pmb E }P(x_i = e_i\mid \text{Pa}_{X_i})\prod_{X_j\not\in \pmb E}P( x_j\mid \text{Pa}_{X_j})}{\prod_{X_i\in \pmb E}\mathbf 1\{x_i=e_i\}\prod_{X_j\not\in \pmb E}P(x_j\mid \text{Pa}_{X_j})}\\
&=\frac {\prod_{X_i\in \pmb E }P(x_i = e_i\mid \text{Pa}_{X_i})\prod_{X_j\not\in \pmb E}P( x_j\mid \text{Pa}_{X_j})}{\prod_{X_j\not\in \pmb E}P(x_j\mid \text{Pa}_{X_j})}\\
&=\prod_{X_i\in \pmb E }P(x_i = e_i\mid \text{Pa}_{X_i})
\end{align}
$$

其中 $x_i$ 表示 $\pmb x\langle X_i\rangle$，第二个等号是因为从 $Q$ 中采样得到的样本 $\pmb x$ 显然一定满足 $x_i = e_i$

因此，可以发现从残缺化网络执行重要性采样得到的每个样本的重要性权重和似然加权算法中为每个样本赋予的权重是一样的，并且，考察流程发现，似然加权算法的采样流程等价于从残缺化网络中进行采样 (对于观察变量，一定采样到观察值，对于其他变量，则按照其 CPD 正常采样)
因此，从残缺化网络进行重要性采样实际上等价于直接执行似然加权采样算法，反过来说，似然加权采样算法等价于重要性采样方法 (定义一个更容易采样的分布 $Q$，从 $Q$ 中采样，对样本进行赋予重要性权重)


Limitations of Likelihood Weighting and Importance Sampling
似然加权采样方法定义在有向图的前向采样算法之上，因此要应用在无向图时，需要将 MN 先转化为 BN，这会引入很多额外的边，因此会很低效

重要性采样方法中，如果原分布较为复杂，则采样分布 $Q$ 的选择就比较困难，如果 $Q$ 太复杂，则采样本身会低效，如果 $Q$ 太简单，则 $Q$ 和 $P$ 不够相似，收敛效率很低


Boltzmann Distribution
一个分子当前的状态在概率上是由它的上一时刻的状态决定的 (Markov 性质)
对于一个独立且规范 (isolated regular) 的系统，当它达到稳定 (steady) 状态后，系统的状态服从 Boltzmann (Gibbs) 分布
即 $P(X = s) \propto e^{-\frac {E_s}{KT}}$，其中 $X$ 为表示系统状态的随机变量，$s$ 是某个状态，$E_s$ 是该状态的自由能 (自由能都是负值)，根据该式，我们可以知道，状态 $s$ 的自由能越低，状态出现的概率就越高

这一规律来自于物理学中对粒子系统的统计规律，物理试验发现处于稳态的粒子系统的状态服从这一规律
在历史上，物理学家根据数据的历史观测值提出了这一假设，并进行了大量试验验证该假设，试验现象都符合该假设，因此认为该假设无法推翻，进而是成立的
(自然科学的发展就是假设驱动的，即提出假设，设计试验证伪，若无法证伪，则认为假设成立)

这一理论启发我们设计一个 Markov chain，我们将样本视作粒子，将样本的赋值视作其状态，样本 (粒子) 从一个初始赋值 (状态) 开始，沿着 Markov chain 不断演化 (变化状态)，最终样本达到稳态，其赋值的分布服从稳态分布

我们将 Markov chain 的稳态分布设定为我们希望样本的赋值服从的理想分布，也就是 $P$
那么，对于一个样本，我们从一个初始的赋值开始，让该样本的赋值逐渐随着 Markov chain 变化，最终达到稳态后，我们就可以认为该赋值服从稳态分布 $P$，也就是说达到稳态后的赋值就可以被视作从 $P$ 中采样得到的赋值

因此，我们通过对一个样本的赋值在 Markov chain 的不断演化，最终得到了该样本从 $P$ 中采样得到的一次赋值，进而可以直接简单认为该样本是从 $P$ 中采样得到的

该思路和之前的采样方式不同，沿着 Markov chain，每个新样本 (赋值) 都依赖于之前的样本 (赋值)，且各个样本 (赋值) 服从的分布都不同，而之前则是在一个固定的分布中进行独立同分布采样，样本之间相互独立

Markov Chain
一个 Markov Chain 包含了
- 一个状态空间 $Val(\pmb X)$
- 一个转移模型 $\mathcal T(\pmb x \rightarrow \pmb x')$，定义了从状态 $\pmb x$ 转移到 $\pmb x'$ 的概率

根据转移模型，我们可以从 $t$ 时刻的状态分布推导出 $t+1$ 时刻的状态分布

$$
P^{(t+1)}(\pmb X^{(t+1)}= \pmb x') = \sum_{\pmb x \in Val(\pmb X)}P^{(t)}(\pmb X^{(t)} = \pmb x)\mathcal T(\pmb x\rightarrow \pmb x')
$$

显然该式就定义了状态分布是如何沿着 Markov chain 演化的

一个 Markov chain 的稳态分布记作 $\pi(\pmb X)$，达到稳态后，在定义上，分布应该不会再随着 Markov chain 演化/变化，因此 $\pi(\pmb X)$ 应该满足

$$
\pi(\pmb X = \pmb x') = \sum_{\pmb x\in Val(\pmb X)}\pi(\pmb X = \pmb x)\mathcal T(\pmb x \rightarrow \pmb x')
$$

对于一个 Markov chain，如果存在一个数 $k$，使得对于其状态空间的任意一对状态 $\pmb x, \pmb x'\in Val(\pmb X)$，通过 $k$ 步从 $\pmb x$ 到 $\pmb x'$ 的概率大于零，则称该 Markov chain 是 regular 的
一个有限状态的 Markov chain 具有唯一的稳态分布当且仅当它是 regular 的

我们的目标就是设计一个具有唯一的稳态分布 $P(\pmb X\mid \pmb e)$ 的 Markov chain，用该 Markov chain 进行样本演化，实现模拟从 $P(\pmb X\mid \pmb e)$ 进行采样

这个思路就是 Markov Chain Monte Carlo (MCMC) 方法

Markov Chain Monte Carlo
基于前一个粒子，(根据后验分布) 生成当前粒子

为了使得稳态分布是唯一的，我们需要让我们的 Markov chain 为 regular
使得 Markov chain 为 regular 的一个充分条件就是该 Markov chain 满足各态遍历性，也就是其状态空间的所有状态都可以由空间中的任意其他状态在有限步数内转移到

在概率图模型中，以 Markov 网络举例，我们将 Markov 网络定义的 Gibbs 分布作为 Markov chain 的稳态分布，则该 Markov chain 的各态遍历性等价于在图中所有节点都是互通的 (所有的节点在单个子图内连通，而不是多个子图)，并且分布 $P_\Phi$ 中所有的因子/团势能都严格大于零 (所有团势能都严格为正意味着在 Gibbs 抽样的过程中，对于任何配置 $c$ (即网络中所有变量的取值组合)，其联合概率 $P_\Phi(c)$ 也是严格为正的，这就意味着在 Gibbs 链的转移矩阵中，从一个状态转移到另一个状态的概率总是大于零，因为没有哪个状态会被赋予零概率，根据定义，由于从任何一个状态到任何一个其他状态的概率都不为零，因此转移矩阵的任意次数的幂都将包含正值，也就是说，Gibbs 抽样马尔可夫链是规则的 )

Gibbs Sampling
Gibbs Sampling 是一类 MCMC 方法，其 Markov chain 的描述如下
状态空间为 $Val(\pmb X)$ 中和证据 $\pmb e$ 一致的部分，或者说对 $\pmb X$ 的和证据 $\pmb e$ 一致的赋值，其中 $\pmb X = \{X_i\}_{i=1,\dots, n}$
转移模型通过我们所需要的稳态分布 (后验分布) 定义，即

$$
\mathcal T(\pmb X^{(t)} = \pmb x \rightarrow \pmb X^{(t+1)} = \pmb x') = P(\pmb X^{(t+1)} =\pmb x'\mid \pmb X^{(t)} = \pmb x,\pmb e)
$$

也就是 $\pmb x$ 转移到 $\pmb x'$ 的概率定义为 $\pmb x'$ 条件于 $\pmb x, \pmb e$ 的后验概率，或者说 $\pmb X^{(t)}$ 转移到 $\pmb X^{(t+1)}$ 的概率是条件概率分布 $P(\pmb X^{(t+1)}\mid \pmb X^{(t)}, \pmb e)$
其中 $P(\pmb X \mid \pmb e)$ 是我们想要的稳态分布

注意以上写法实际上是一种简写，完整的一次转移 $\mathcal T (\pmb x \rightarrow \pmb x')$ 实际上是由多个连续的 kernel $\mathcal T_i(i=1,\dots, k)$ 组成的，其中每个 $\mathcal T_i$ 仅改变变量 $x_i$ 的状态，保持其他变量的状态不变，其定义为

$$
\mathcal T_i((\pmb x_{-i}, x_i)\rightarrow(\pmb x_{-i}, x'_i)) = P(x_i'\mid \pmb x_{-i}, \pmb e)
$$

因此 $\mathcal T$ 实际上的定义应该展开为

$$
\begin{align}
\mathcal T(\pmb x\rightarrow \pmb x') &=\mathcal T_1((\pmb x_{-1}, x_1)\rightarrow (\pmb x_{-1},x_1')) \mathcal T_2((\pmb x_{-\{1,2\}},x_1',x_2)\rightarrow  (\pmb x_{-\{1,2\}},x_1',x_2'))\cdots\\
&=\prod_{i=1}^k \mathcal T_i((\pmb x_{-\{1,\dots,i-1\}},x'_1,\cdots,x'_{i-1},x_i)\rightarrow (\pmb x_{-\{1,\dots,i-1\}},x_1',\cdots,x_{i-1}',x_i'))\\
&=\prod_{i=1}^k P(x_i' \mid x_1',\cdots,x_{i-1}',\pmb x_{-\{1,\dots, i-1\}}, \pmb e)\\
&=\prod_{i=1}^k P(x_i' \mid x_1',\cdots,x_{i-1}',x_{i+1},\cdots, x_k, \pmb e)\\
\end{align}
$$

完成所有 kernel $\mathcal T_i$ 的转移即完成 $\mathcal T$ 的一次转移

我们需要证明，这样定义的 Markov chain 的稳态分布确实是 $P(\pmb X\mid \pmb e)$，并且是唯一的，那么我们就可以确认在这样定义的 Markov chain 进行 MCMC 过程最终会得到服从 $P(\pmb X\mid \pmb e)$ 的样本

为此，我们考察

$$
\begin{align}
&\sum_{\pmb x \in Val(\pmb X)} P(\pmb X = \pmb x\mid \pmb e)\mathcal T(\pmb x \rightarrow \pmb x')\\
=&\frac 1 {P(\pmb e)}\sum_{\pmb x \in Val(\pmb X)} P( \pmb x, \pmb e)\mathcal T(\pmb x \rightarrow \pmb x')\\
=&\frac 1 {P(\pmb e)}\sum_{x_k}\cdots \sum_{x_1}P(\pmb x, \pmb e) \mathcal T(\pmb x \rightarrow \pmb x')\\
=&\frac 1 {P(\pmb e)}\sum_{x_k}\cdots \sum_{x_1}P(\pmb x, \pmb e) \prod_{i=1}^k P(x_i' \mid x_1',\cdots,x_{i-1}',x_{i+1},\cdots, x_k, \pmb e)\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[\sum_{x_2}\left[\sum_{x_1} P(x_1,x_2,\cdots, x_k,\pmb e) P(x_1'\mid x_2,\cdots ,x_k,\pmb e)\right]P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[\sum_{x_2}\left[ P(x_2,\cdots, x_k,\pmb e) P(x_1'\mid x_2,\cdots ,x_k,\pmb e)\right]P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[\sum_{x_2} P(x_1', x_2,\cdots ,x_k,\pmb e)P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[ P(x_1',x_3,\cdots ,x_k,\pmb e)P(x_2'\mid x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\frac 1 {P(\pmb e)} \sum_{x_k}\cdots \left[ P(x_2', x_1',x_3,\cdots, x_k, \pmb e)\right]\cdots\\
=&\cdots\\
=&\frac 1 {P(\pmb e)}P(x'_k, \cdots, x'_1, \pmb e)\\
=&\frac {1}{P(\pmb e)}P(\pmb x', \pmb e)\\
=&P(\pmb x' \mid \pmb e)
\end{align}
$$

因此，分布 $P(\pmb X \mid \pmb e)$ 经过 $\mathcal T$ 转移以后，仍然是 $P(\pmb X \mid \pmb e)$，故该分布就是该 Markov chain 的稳态分布

确定了 $P(\pmb X \mid \pmb e)$ 是该 Markov chain 的稳态分布后，我们考察该 Markov chain 是否具有唯一的稳态分布
Markov chain 具有唯一稳态分布的一个充分条件是各态遍历性，即状态空间内任意两个状态都在有限步数内互相可达，而 $P_\Phi$ 中所有团势能都严格大于零可以保证这一条件成立
因此对于 Bayesian 网络，要求就是所有的 CPDs 都严格为正；对于 Markov 网络，要求就是所有 clique potential 严格为正

我们进一步考虑每一个 kernel $\mathcal T_i$ 的表示，我们知道 $\mathcal T_i$ 写为 
(不考虑证据，考虑证据时情况是类似的；
这里我们将 $x_j'$ 写为 $x_j^{(t+1)}$，将 $x_j$ 写为 $x_j^{(t)}$)

$$
\begin{align}
\mathcal T_i(x_i^{(t+1)}) &= P(x_i^{(t+1)} \mid x_1^{(t+1)},\cdots,x_{i-1}^{(t+1)},x_{i+1}^{(t)},\cdots, x_k^{(t)})\\
\end{align}
$$

这是关于变量 $X_i$ 的后验概率分布，其中 $P = P_\Phi$ 是 Markov 网络定义的 Gibbs 分布
我们可以利用 Markov 网络编码的独立性，简化这一后验分布，容易知道，给定变量 $X_i$ 的 Markov blanket，变量 $X_i$ 和网络中所有其他变量条件独立，因此，$\mathcal T_i(X_i)$ 可以进一步简化为

$$
\mathcal T_i(X_i) = P(X_i\mid \text{MB}(X_i))
$$

因此，$\mathcal T_i(X_i = x_i^{(t+1)}) = \mathcal T_i(x_i^{(t+1)})$ 写为

$$
\mathcal T_i(x_i^{(t+1)})  = P(x_i^{(t+1)} \mid X_j = x_j^{T})
$$

其中 $X_j \in \text{MB}(X_i)$，当 $j > i$ 时，$T = t$，当 $j < i$ 时，$T = t-1$

注意 Gibbs 对于所有的分布都适用，也就是目标稳态分布 $P$ 可以是任意分布，我们在这里是利用 $P$ 的图表示对采样过程进行了简化 (利用 $P$ 中的独立性简化条件概率分布)

Gibbs Sampling for BNs
我们考虑对 BN 进行 Gibbs 采样，在 Markov chain 的一步转化中，我们考虑化简后验分布 $P(X_i \mid X_1, \cdots, X_{i-1}, X_{i+1}, \cdots, X_n)$
将 $X_i$ 的子节点记作 $\pmb Y = \{Y_1, \cdots, Y_k\}$，用 $X_i$ 的 Markov blanket  (给定一个节点的 Markov blanket，它和网络中所有其余节点条件独立) 将该分布化简为

$$
\begin{align}
P(X_i \mid X_1, \cdots, X_{i-1}, X_{i+1},\cdots, X_n) &= P(X_i\mid \text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y} -\{X_i\})\\
&=\frac {P(X_i, \text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y}-\{X_i\})}
{P(\text{Pa}_{X_i},\pmb Y, \text{Pa}_{\pmb Y}- \{X_i\})}\\
&\approx P(X_i, \text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y}-\{X_i\})\\
&=P(\text{Pa}_{X_i})P(X_i,\pmb Y, \text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})\\
&=P(\text{Pa}_{X_i})P(\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})P(X_i,\pmb Y\mid \text{Pa}_{\pmb Y}-\{X_i\},\text{Pa}_{X_i})\\
&=P(\text{Pa}_{X_i})P(\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})P(X_i\mid \text{Pa}_{\pmb Y}-\{X_i\},\text{Pa}_{X_i})P(\pmb Y\mid \text{Pa}_{X_i}, \text{Pa}_{\pmb Y}- \{X_i\},X_i)\\
&=P(\text{Pa}_{X_i})P(\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})P(X_i\mid \text{Pa}_{X_i})P(\pmb Y\mid \text{Pa}_{\pmb Y}- \{X_i\},X_i)\\
&\approx P(X_i\mid \text{Pa}_{X_i})P(\pmb Y\mid\text{Pa}_{\pmb Y})
\end{align}
$$

其中第三行的 $\approx$ 是因为 $P(\text{Pa}_{X_i}, \pmb Y, \text{Pa}_{\pmb Y}- \{X_i\})$ 中各个变量的取值在此时都是固定的，与 $X_i$ 的取值无关，因此视为常数
其中最后一行的 $\approx$ 是也是同理， $P (\text{Pa}_{X_i}) P (\text{Pa}_{\pmb Y} - \{X_i\} \mid \text{Pa}_{X_i})$ 其中各个变量的取值此时都是固定的，与 $X_i$ 的取值无关，因此视为常数

因此，在对 $X_i$ 进行采样时，我们直接定义 $\tilde P(X_i) = P(X_i \mid \text{Pa}_{X_i}) P(\pmb Y \mid \text{Pa}_{\pmb Y})$，我们为每个 $X_i$ 的取值 $x_i$ 计算 $\tilde P(X_i = x_i)$，最后通过归一化得到 $X_i$ 此时真实的后验分布

$$
P(X_i) = \frac {\tilde P(X_i)}{\sum_{x_i} \tilde P(X_i = x_i)}
$$

然后根据 $P(X_i)$ 进行采样即可

Gibbs Sampling for MNs
推导类似，见 [[book-notes/Probabilistic Graphical Models-Principles and Techniques|Probabilistic Graphical Models-Principles and Techniques]] 中 eq (12.23)


Metropolis-Hasting Algorithm
Gibbs 采样在设计 Markov chain 的 transition kernel 时，是依据目标分布 $P$ 设计的，在生成 Markov chain 的下一个样本时，需要依据基于 $P$ 的后验分布进行采样

MH 算法希望从任意分布生成下一个样本，也就是 transition kernel 可以是任意分布 $Q$ 而不是某个特定分布，并且通过定义接受概率使得即便以任意分布作为 transition kernel，也可以使得 Markov chain 的稳态概率为 $P$

MH 将 Markov chain 的 transition kernel 设计为如下形式

$$
\begin{align}
\mathcal T(\pmb x\rightarrow \pmb x')&=\mathcal T^Q(\pmb x \rightarrow \pmb x')\mathcal A(\pmb x \rightarrow \pmb x')\quad \pmb x\ne \pmb x'\\
\mathcal T(\pmb x \rightarrow \pmb x) &=\mathcal T^Q(\pmb x\rightarrow \pmb x)\sum_{\pmb x' \ne \pmb x}\mathcal T^Q(\pmb x\rightarrow \pmb x')(1-\mathcal A(\pmb x\rightarrow \pmb x'))
\end{align}
$$

直观上说，在执行 transition 时，假设当前状态为 $\pmb x$，下一个状态为 $\pmb x'$，考虑 $P(\pmb x')/ P(\pmb x)$ 和 $Q(\pmb x \rightarrow \pmb x')/Q(\pmb x' \rightarrow \pmb x)$ ，前者表示在 $P$ 中 $\pmb x'$ 相较于 $\pmb x$ 的出现概率的比例，后者表示在 $Q$ 中从 $\pmb x \rightarrow \pmb x'$ 的转移概率相较于 $\pmb x'\rightarrow \pmb x$ 的转移概率的比例
如果前者大于后者，说明在 $P$ 中 $\pmb x$ 到 $\pmb x'$ 的转移倾向比在 $Q$ 中更强烈，此时接受概率 $\mathcal A(\pmb x\rightarrow \pmb x')$ 就会比较高 (最大为 1)，让 $\pmb x$ 尽快转移
如果后者大于前者，说明在 $Q$ 中 $\pmb x$ 到 $\pmb x'$ 的转移倾向比在 $P$ 中更强烈，此时接受概率 $\mathcal A(\pmb x \rightarrow \pmb x')$ 就会比较低，使得 $\pmb x$ 更大可能保留在原状态，以补足这一差异 
因此转移分布 $Q$ 和接受概率 $\mathcal A$ 一起定义的 kernel 也可以最终收敛到稳态分布 $P$

接受概率可以由转移分布 $\mathcal T^Q$ 和稳态分布 $\pi(\pmb x)$ 推导得到，根据细致平衡条件，$\mathcal T(\pmb x\rightarrow \pmb x') = \mathcal T^Q(\pmb x\rightarrow \pmb x')\mathcal A(\pmb x\rightarrow \pmb x')$ 应该满足

$$
\pi(\pmb x)\mathcal T^Q(\pmb x\rightarrow \pmb x')\mathcal A(\pmb x\rightarrow \pmb x') = \pi(\pmb x')\mathcal T^Q(\pmb x'\rightarrow \pmb x)\mathcal A(\pmb x'\rightarrow \pmb x)
$$
故

$$
\frac {\mathcal A(\pmb x\rightarrow \pmb x')}{\mathcal A(\pmb x' \rightarrow \pmb x)} = \frac {\pi(\pmb x')\mathcal T^Q(\pmb x'\rightarrow \pmb x)}{\pi(\pmb x)\mathcal T^Q(\pmb x\rightarrow \pmb x')}
$$

因此可以将接受概率定义为如下形式，以满足上述方程

$$
\mathcal A(\pmb x\rightarrow \pmb x') = \min\left[1, \frac {\pi (\pmb x')\mathcal T^Q(\pmb x' \rightarrow \pmb x)}{\pi(\pmb x)\mathcal T^Q(\pmb x\rightarrow \pmb x')}\right]
$$


