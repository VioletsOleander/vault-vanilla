>古槿
# 1 Introduction to Probabilistic Graphical Models
>2024.9.9

用图模型求解概率问题，基于概率在不确定的世界中做出决策

**概率论**
现代概率论：
- 频数论 (基于随机试验)
    将做试验的次数和事件发生的次数的比值作为概率
- 贝叶斯
    指定先验，通过观测更新先验
    $P(X|Y) = \frac {P(Y|X)P(X)}{P(Y)} = \frac {P(Y|X)P(X)}{\sum_X P(Y|X)P(X)}$
    $Y$: 观测/果
    $X$: 原因/因
    $P(X)$ : 先验
    贝叶斯公式将执果索因的条件概率写为仅仅和执因索果的条件概率和原因的先验概率有关的形式 

条件独立：$X\perp Y | Z: P(XYZ) = P(X|Z)P(Y|Z)P(Z)$

链式法则：每一次仅决策一个变量 (分治)，逐个将变量拆分到条件

**图论**
path：路径
trail: 同一条路径，双向可达
tree：单个节点仅有一个父节点
polytree：可以有多个父节点，但没有三角形
chordal graph：图中最大的形状就是三角形，没有多边形
clique：子图，子图内所有节点两两相连

贝叶斯网络：有向无环图，节点表示变量，边表示依赖
Markov 网络：无向图

概率图模型：
- 表示：建模
- 推断：已知参数下，计算概率
- 学习：根据数据，学习参数

朴素贝叶斯：只要类别标签已知，所有的观测变量都独立
# 2 Baysian Network: Representation
> 2024.9.16

I-map: 确保在图上 ($\mathcal G$ 空间) 做的所有操作在 $P$ 空间都是成立的

证明 $\mathcal G$ 是否是 $P$ 的 perfect map 是一个 NP hard 问题
证明 $P$ 是否存在 perfect map 是一个 NP hard 问题
# 3 Local Probabilistic Models
> 2024.9.21

概率图模型的直观思想：
    有向无环图（节点表示变量）
    用图定义变量之间的独立性

概率图模型的形式化思想：
    定义公理：$\mathcal G$ 中的独立性定义为 $(X_i \perp Nondesc (X_i) \mid Pa (X_i))$
    由公理推导 factorization 定理：
    - 充分条件： $\mathcal G$ is a I-Map of $P$ $\rightarrow$ $P (X_1,\dots, X_n) = \prod_{i=1}^n P (X_1\mid Pa (X_i))$ ($P$ 满足 $\mathcal G$ 中的所有独立性，故根据 $\mathcal G$ 分解 $P$)
    - 必要条件： $P (X_1,\dots, X_n) = \prod_{i=1}^n P (X_1\mid Pa (X_i))$ $\rightarrow$  $\mathcal G$ is a I-Map of $P$ (若 $P$ 可以根据 $\mathcal G$ 分解，则 $\mathcal G$ 是 $P$ 的 I-Map，即 $\mathcal G$ 不包含 $P$ 中没有的独立性)

必要条件主要用于结构学习

v-structure：
    子节点被观察到后，父节点发生关联

Noisy-or:
    子节点的值不需要知道全部的父节点的值才能决定，每一个父节点都有各自一定的概率对子节点产生相同的影响
    例如：只要任意一个父节点（以一定概率 $f_i$）发生了 failure，此时子节点就一定会 failure，$P (F = False) = \prod_i (1-f_i)$
    再考虑 leak probability $f_0$，就写为 $P (F = False) = (1-f_0)\prod_i (1-f_i)$

BN2O:
    朴素贝叶斯和 Noisy-or 的拓展
    多个结果节点（子节点）
    朴素贝叶斯：已知某个子节点的全部父节点，该子节点和其余所有子节点条件独立
    Noisy-or：每个子节点和其所有父节点之间构成了 Noisy-or 模型

Generalized linear model:
    多个 cause 的效果乘上权重（权重可为负），线性叠加，得到总效果（分数）
    包括：Logistic CPD, Linear Gaussian 等

Logistic CPD:
    Sigmoid 函数将效果/分数映射到 $[0,1]$ （概率空间）
    由此定义的 CPD 称为 Logistic CPD，即 $P (Y = y^1 \mid X_1,\dots, X_k) = \text{sigmoid} (w_0 + \sum_{i=1}^k w_iX_i$)
    Logistic CPD odds:

$$
\begin{align}
O &= \frac {P(Y=y^1\mid X_1,\dots,X_k)}{P(Y = y^0\mid X_1,\dots, X_k)} \\
&=\frac {e^s / (1+e^s)}{1/(1+e^s)}\\
&=e^s
\end{align}
$$
    For binary $X_i$, when $X_j$ change from $0$ to $1$
$$
\Delta O = e^{w_j}
$$


Linear Gaussian:
    $(Y\mid X_1,\dots ,X_k) \sim \mathcal N (w_0 + \sum_i w_i X_i, \sigma^2)$

如果神经元激活是按照概率 $P (Y = y^1 \mid X_1,\dots , X_k)$，则神经网络也可以视作概率图模型

Pooling:
    抗噪声
    max pooling - 平移稳定性
    median pooling - 过滤 outlier 噪声
    mean pooling - 过滤高斯噪声

hierarchical model: 决定了变量分布的参数同样服从于某个需要学习的分布

Three steps for representation:
    Define node/variables
    Consider edges/dependencies
    Choose local CPDs

# 4 Dynamic Models
> 2024.9.28

Markov/Memoryless property:
    assumption: 
    $P (X) = \prod_{t=1}^n P (X (t)\mid X (t-1))$ and $\forall i, j, P (X (i)\mid X (i-1)) = P (X (j)\mid X (j-1))$

HMM:
    Local structure 1: Hiddent states (can not be observed) are linked as Markov chain $P (S^T \mid S^{t < T}) =  P (S^T\mid S^{T-1})$
    Local structure 2: Observable variables are only determined by the hiddent state $P (O^T \mid S^{t\le T})  = P (O^T \mid S^T)$

The key is designing the hidden variables.

State-Obseration models:
    State-State Transimission Matrix $T_{(n, n)}$: $S^t = TS^{t-1}$ 
    State-Observation Emission Matrix $E_{(m, n)}$: $O^t = ES^t$

Calculations in HMM:
    1.  $P (X\mid \theta)$: given the model and the observation sequence, infer the probability
    2.  $\arg\max_{Y}P (X, Y\mid \theta)$: given the model and the observation sequence, infer the hidden labels of the sequence
    3.  $\arg\max_{\theta}P (X\mid \theta)$: learn parameters from the observation sequence

**Question 1**: Infer $P (X\mid \theta)$
Define:

$$
\begin{align}
\theta &= \begin{cases}
T = \{t_{i,j},\quad i,j = 1,\dots N\},\\
E = \{e_{i,j},\quad i = 1,\dots N,j=1,\dots ,K\},\\
\pi = \{\pi_i,\quad i = 1,\dots N\}
\end{cases}\\\\
X &= \{x_t,\quad t=1,\dots T\}
\end{align}
$$

$\theta$ is the parameterization of HMM, $X$ is the observation sequence.

Basic Idea:
$P (X) = \sum_Y P (X, Y)$

by factorization theorem:

$$\begin{align}
P (X, Y) &= P (Y_1) P (X_1\mid Y_1) P (Y_2\mid Y_1) P (X_2\mid Y_2)\cdots P(Y_T\mid Y_{T-1})P(X_T\mid Y_T)\\
&=P(X_T\mid Y_T)P(Y_T\mid Y_{T-1})P(X_{T-1}\mid Y_{T-1})P(Y_{T-1}\mid Y_{T-2})\cdots P(Y_1)
\end{align}$$

the probability can be solved by eliminating $Y_i$ iteratively

Forward algorithm:
$\alpha_t (i) = P (x_1,\dots, x_t, y_t = i\mid \theta)$ (the probability that the Markov chain has forwarded into time $t$ with state $y_t = i$ and observation $x_1,\dots, x_t$ )
- Initialization: $\alpha_1 (i) = \pi_i e_{i, x_1}$ 
    (the probability that $y_1 = i$ and observed $x_1$)
- Induction: $\alpha_{t+1}(i) = [\sum_{j=1}^N\alpha_t (j) t_{j, i}]e_{j, x_{t+1}}$
    (the probability that the Makov chain has forwarded into time $t+1$ and observe $x_{t+1}$)
- Termination: $P (X\mid \theta) = \sum_{i=1}^N\alpha_T (i)$

Backward algorithm:
$\beta_t (i) = P (x_{t+1},\dots, x_T\mid y_t = i, \theta)$ (the probability that the future observation will be $x_{t+1},\dots, x_T$, given state $y_t = i$ )
- Initialization: $\beta_{T+1}(i) = 1$ 
    (the probability of a non-event, given the fact that the chain is finished)
- Induction: $\beta_t (i) =\sum_{j=1}^N t_{i, j}e_{j, x_{t+1}}\beta_{t+1}(j)$
    $$
\begin{align}
\beta_t (i) &= P (\mathbf x_{t+1: T} \mid y_t = i)\\
&= \sum_j P (y_{t+1} = j, \mathbf x_{t+1: T}\mid y_t = i)\\
&=\sum_j P (y_{t+1} = j, \mathbf x_{t+1: T}\mid y_t = i) \\
&=\sum_j P ( \mathbf x_{t+1: T}\mid y_t = i,y_{t+1} = j)P(y_{t+1} = j\mid y_t = i) \\
&=\sum_j t_{ij}P ( \mathbf x_{t+1: T}\mid y_{t+1} = j) \\
&=\sum_j t_{ij}P(x_{t+1},\mathbf x_{t+2:T}\mid y_{t+1} = j)\\
&=\sum_j t_{ij}P(x_{t+1}\mid y_{t+1} = j) P(\mathbf x_{t+1:T}\mid y_{t+1} = j)\\
&=\sum_j t_{ij}e_{j, x_{t+1}}\beta_{t+1}(j)
\end{align}
    $$

- Termination: $P (X\mid \theta) = \beta_0(i) = \sum_{j=1}^N \pi_j e_{j, x_1}\beta_1(j)$

**Question 2**: Infer $\arg\max_Y P (X,Y\mid \theta)$
Question 1 在前向递推的时候，实际上每一个 $t$ 上都对所有 $Y_t = i$ 的情况做了求和
如果需要递推 Question 2，此时我们在前向递推的时候，只需要记录最大的路径概率即可 (最大概率的路径是满足贪心的，类似于 Dijstra 算法的思想，但注意全局最优状态序列 $Y$ 的递推不一定是满足贪心的)

也就是说，在递推时，对于状态 $y_t = i$，我们记录了从开始到 $y_t = i$ 的最可能路径，而不是所有路径，因此在 $t$ 时刻，我们仅仅记录了 $|Val (y_t)|$ 条路径，注意每条路径的状态序列都不同
之后，对于下一个时刻，我们对于 $\forall i,y_{t+1} = i$，都从 $|Val (y_t)|$ 条路径中选出最可能的一条，最后在 $t+1$ 时刻，我们同样得到 $|Val (y_{t+1})|$ 条路径
不到最后的时刻 $T$，我们无法确定哪条路径是全局最优，因此也无法在中间时刻确定一个部分的状态序列，但一旦确定了某条全局最优的路径，整个状态序列也随之确定

因此，我们在前向推导时记录概率，后向推导时计算状态

Viterbi algorithm:
The probability at time 1 with state $y_1 =i$ and observation $x_1$ :
$\delta_{1, i} = \pi_i e_{i, x_1}$

The probability at time 2 with state $y_2 = i$ and observation $x_1, x_2$ , given the most probable path:
$\delta_{2, i} = e_{i, x_2}\max_{y_1 = 1,\dots, N}(\pi_{y_1}e_{y_1, x_1}\times t_{y_1, i}) =e_{i, x_2}\max_{y_1 = 1,\dots, N}(\delta_{1,y_1}\times t_{y_1,i})$
The most likely previous state on the most probable path to $y_2 = i$:
$\phi_2 (i) = \arg\max_{y_1 = 1,\dots, N}(\delta_{1, y_1}\times t_{y_1, i})$ 

The probability at time $t$ with state $y_t = i$ and observation $x_1,\dots, x_t$, given the most probable path:
$\delta_{t, i} = e_{i, t}\max_{y_{t-1} = 1,\dots, N}(\delta_{t-1, y_{t-1}}\times t_{y_{t-1}, i})$
The most likely previous state on the most probable path to $y_t = i$:
$\phi_t (i) = \arg\max_{y_{t-1}=1,\dots, N}(\delta_{t-1, y_{t-1}}\times t_{y_{t-1},i})$

The probability at time $T$ with state $y_T = i$ and observation $x_1,\dots, x_T$, given the most probable path:
$\delta_{T, i} = e_{i, x_T}\max_{y_{T-1 = 1,\dots, N}}(\delta_{T-1, y_{T-1}}\times t_{y_{T-1}, i})$
The most likely previous state on the most probable path to $y_T = i$:
$\phi_T (i) = \arg\max_{y_{T-1}=1,\dots, N}(\delta_{T-1, y_{T-1}}\times t_{y_{T-1},i})$

$y_T^* = \arg\max_{y_T = 1,\dots, N}(\delta_{T, y_T})$

After $y_t^*$ is inferred, trace back to get $y_{t-1}^*$:
$y^*_{t-1} = \phi_t (y_t^*) = \arg\max_{y_{t-1} = 1,\dots, N}(\delta_{t-1, y_t^*}\times t_{y_{t-1}, y_t^*})$

**Question 3**: Learn $\arg\max_{\theta} P (X\mid \theta)$
参数 $\theta$ 实际上都和隐状态 $Y$ 有关，因此在未知状态的情况下是无法做出推导的
故简单的思路就是对状态进行猜测，迭代优化参数

General startegy:
- set an initial value of the parameters
- do the inference of hidden states
- re-estimate and re-learn the paramters
- repeat the process until convergence

Baum-Welch algorithm:
- Define an intermediate variables for inference:
    $\xi_t (i, j) = P (y_t = i, y_{t+1} = j \mid X, \theta)$
    (given observation and parameter, the probability of $y_t = i$ and $y_{t+1} = j$)
- Use forward and backward algorithm to calculate probability:
    $\theta^0 = \{T^0, E^0, \pi^0\}$
    $\alpha_t(i) = P (\mathbf x_{1: t}, y_t = i\mid \theta^0)$
    $\beta_t (i) = P (\mathbf x_{t+1:T}\mid y_t = i, \theta^0)$

    $$
\begin{align}
\xi_t(i,j) &= P(y_t = i, y_{t+1} = j \mid \mathbf x_{1:T})\\
&=\frac {P(y_t = i, y_{t+1} = j , \mathbf x_{1:T})}{P(\mathbf x_{1:T})}\\
&=\frac {P(y_t = i, y_{t+1} = j , \mathbf x_{1:T})}{\sum_{i,j}P(y_t = i, y_{t+1}=j,\mathbf x_{1:T})}\\
&= \frac {\alpha_t(i)t_{i,j}e_{j,x_{t+1}}\beta_{t+1}(j)}{\sum_{i=1}^Y\sum_{j=1}^Y\alpha_t(i)t_{i,j}\beta_{t+1}(j)e_{j,x_{t+1}}}\\
\end{align}
$$

    (Firstly forward to $t$ with state $y_{t} = i$ and observation $\mathbf x_{1:t}$, then transition to $t+1$ with state $y_{t+1} = j$, and emit observation $x_{t+1}$，then forward to $T$ with observation $\mathbf x_{t+2:T}$)

- Calculate:
    The probability of $y_t = i$:
    $\gamma_t (i) = \sum_{j=1}^Y\xi_t (i, j)$ (which corresponds to $\pi_i$)
    The expected times for state stayed in $Y = i$:
    $\sum_{t=0}^{T-1}\gamma_t (i)$
    The expected times for state transition $i-j$:
    $\sum_{t=0}^{T-1}\xi_t (i, j)$
- Re-estimate all paramters:
    $$
\begin{align}
t_{i,j} &= \frac {\sum_{t=0}^{T-1}\xi_t(i,j)}{\sum_{t=0}^{T-1}\gamma_t(i)}\\
e_{i,x} &=\frac {\sum_{t=0}^T\mathbf I(x_t = x)\gamma_t(i)}{\sum_{t=0}^{T-1}\gamma_t(i)}
\end{align}
    $$

- Repeat above steps until convergence:
    $|\log P (X\mid \theta) - \log P (X\mid \theta^0)| < \epsilon$

Generate simulated data using HMMs:
- Find the initial probability for hidden states
- MCMC
    MCMC are class of algorithms based on constructing a Markov chain whose equilibrium distribution is our desired distribution. Therefore, the state of the chain after a large number of steps is then used as a sample from our desired distribution
    A good Markov chain's stationary distribution can be reached quickly starting from an arbitary point.

Max entropy Markov Models:
    termed MEMM or CMM (conditional Markov Models)
    MEMM is a discriminative model which extends the standard maximum entropy model by assuming the unkown value to be learned are connected in a Markov chain rather than conditionally independent of each other ($Y^i$ 之间有联系，而不是互相条件独立)
    MEMM cares about $P (Y\mid X)$, standard Markov Model cares about $P (Y, X)$, that's the differenct

Hopfield Network:
    the predecessor of RNN

Markov blanket: 一个变量 $S$ 的 Markov blanket 指一个变量集合，满足给定该集合内的变量为条件，$S$ 和网络中所有其他变量条件独立

# 5 Markov Networks
Markov 网络编码的全局独立性假设 (Global Markov assumptions)： 
$\mathcal I (\mathcal H) = \{(\pmb X \perp \pmb Y \mid \pmb Z) : \text{sep}_{\mathcal H}(\pmb X; \pmb Y\mid \pmb Z)\}$

Factor: 因子是函数，将某个变量集合 $\pmb D$ 的赋值映射到某个实数，即
$f: Val(\pmb D) \mapsto \mathbb R^+$；要求 $\pmb D$ 中的变量应该直接地相互依赖

显然，条件概率分布也符合 factor 的定义，故可以看作是 factor 的特例 
(Factors generalize the notion of CPDs: Every CPD is a factor)

Markov 网络中，factor 对应的变量集合 $\pmb D$ 必须在图中构成一个团，如果不是团，则 $\pmb D$ 中就会存在没有直接相互依赖的变量对
团是 Markov 网络的基本局部结构

Maximal clique
    一个极大团中的多个子团对应的 factor 的乘积可以由该极大团对应的 factor 表示
    例如，考虑极大团 $\{A, B, C\}$，其中有子团 $\{A, B\}, \{B, C\}$，我们可以定义一个 factor $\pi[A, B, C]$ 来直接表示 $\pi_1[A, B, C]\pi_2[A, B]\pi_3[B, C]$

I-map to factorization
    给定无向图 $\mathcal H$，如果 $\mathcal H$ 是 $P$ 的 I-map，则 $P$ 可以分解为 $P (\pmb X) = \frac 1 Z\prod \pi_i[\pmb D_i]$，其中 $\pmb D_i$ 是 $\mathcal H$ 中的团

factorization to I-map
    给定无向图 $\mathcal H$ ，如果 $P$ 可以分解为 $P (\pmb X) = \frac 1 Z \prod \pi_i[\pmb D_i]$，则 $\mathcal H$ 是 $P$ 的 I-map

$Z$ 被称为分区函数，可以理解为系统的总能量，用于 normalize

分解于 $\mathcal H$ 的分布 $P$ 就称为 $\mathcal H$ 上的 Gibbs 分布

Example: Pairwise Markov Networks
图 $\mathcal H$ 上的成对 Markov 网络包含：
    一组定义于单个节点的节点因子/势能函数，记作 $\{\pi[X_i],i=1,\dots, n\}$
    一组定义于边的边因子/势能函数，记作 $\{\pi[X_i, X_j], X_i, X_j\in\mathcal H\}$
    Gibbs 分布写为 $P = \frac 1 Z \prod_i \pi[X_i] \prod_{(i,j)} \pi[X_i, X_j]$

能量势能函数一般习惯使用指数形式，故对原表示进行指对数转化：
$\pi[\pmb D] = \exp (-\epsilon (\pmb D))$，其中 $\epsilon (\pmb D) = -\ln [\pi[\pmb D]]$
在对数表示下，Gibbs 分布重写为：

$$
\begin{align}
P (\pmb X) &= \frac 1 Z \prod_i \left\{\exp(-\epsilon (\pmb D_i))\right\}\\
&=\frac 1 Z \exp\left\{\sum_{i}-\epsilon(\pmb D_i)\right\}\\
&=\frac 1 Z \exp\left\{-\sum_{i}\epsilon(\pmb D_i)\right\}
\end{align}$$

$\pi[\pmb D]$ 越高，势能函数 $\epsilon (\pmb D) = -\ln \pi[\pmb D]$ 越低

HC 定理
对于无向图 $\mathcal H$，定义 $Q$ 函数：

$$
Q(x\mid \text{NB}(x)) = \ln\left[\frac {P(x\mid \text{NB(x)})}{P(x=0\mid \text{NB(x)})}\right]
$$

 $\mathcal H$ 是正分布 $P$ 的 I-map 等价于 $Q$ 函数存在唯一的展开：

$$
Q(\pmb x) = \sum_i x_i \psi_i(x_i) + \sum_{i,j}x_ix_j\psi_{i,j}(x_i,x_j) + \cdots +  x_1x_2\dots x_n \psi_{1,2\dots ,n}(x_1,x_2,\dots, x_n)
$$

其中 $\psi_{s} \ne 0$ 当且仅当所有的 $v\in s$ 在 $\mathcal H$ 中构成一个团

Notes about HC theorem:
- ground state: 当某个变量 $X_i$ 处于 ground state (等于零)，所有和它相关的势能都变为零 ($x_i = 0$ 乘以任何数都得到零)
- $Q$ 函数描述了相对于 ground state 的相对概率
- $Q$ 函数连接了由图编码的独立性和由分布编码的独立性，它们是等价的
- 定义好 Markov 网络中每个团的势能函数 $\psi ()$，就可以写出 Gibbs 分布 (log-linear format)

HC 定理允许我们看图说话，给定无向图 $\mathcal H$，我们可以根据枚举图中的团写出在图上分解的 Gibbs 分布 $P$，分布 $P$ 满足 $\mathcal I (P) \subseteq \mathcal I (\mathcal H)$

An example with 3 binary varaibles:
$\pmb x$ 的 $Q$ 函数写为：
$Q (\pmb x) = \ln\left[ \frac {P (\pmb x)}{P(\pmb x = \pmb 0)}\right] = \sum_{i=1,2,3} \alpha_i x_i + \sum_{i<j = 1, 2, 3}\alpha_{i,j}x_ix_j + \alpha_{1, 2, 3}x_1x_2x_3$

$x_1$ 的 $Q$ 函数写为：

$$\begin{align}
Q (x_1\mid x_2, x_3)
&= \ln\left[\frac {P(x_1\mid x_2, x_3)}{P(x_1 = 0\mid x_2, x_3)}\right]\\ 
&=\ln\left[\frac {P(x_1, x_2, x_3)}{P(x_1 = 0 ,x_2, x_3)}\right]\\
&=\ln\left[\frac {P(x_1, x_2, x_3)/P(0,0,0)}{P(x_1 = 0 ,x_2, x_3)/P(0,0,0)}\right]\\
&=\ln\left[\frac {P(\pmb x)}{P(\pmb x = \pmb 0)}\right]- \ln\left[\frac {P(x_1 = 0, x_2, x_3)}{P(0,0,0)}\right]\\
&=Q(x_1, x_2, x_3) - Q(x_1 = 0, x_2, x_3)\\
&=\alpha_1x_1 + \alpha_{1,2}x_1x_2 + \alpha_{1, 3}x_1x_3 + \alpha_{1,2,3}x_1x_2x_3
\end{align}$$

图 $\mathcal H$ 中编码了 $(X_1 \perp X_2 \mid X_3)$ $\Longleftrightarrow$ $Q (x_1\mid x_2, x_3)$ 展开式中关于 $x_3$ 的项应该是零，即 $\alpha_{1, 3}, \alpha_{1, 2, 3} = 0$

对于仅由二元变量构成的系统，可以将所有的势能函数 $\psi ()$ 替换为单个参数而不失一般性，因为 $\psi ()$ 仅在全部相关变量都取为 1 时才可以为 $Q$ 函数做出贡献，此时 Gibbs 分布可以写为：

$$
\begin{align}
P(X) & = \frac 1 Z \exp(-U)\\
U&=-\sum_i\left(\beta_j\prod_{x_j \in C_i}x_j\right)\\
Z&= \sum_X\exp(-U)
\end{align}
$$

# 6 Examples for Advanced PGM Representation
表示：将 $P$ 映射到 $\mathcal H$ 或 $\mathcal G$
    贝叶斯网络：有向无环图
    Markov 网络：无向图
    动态/序列模型：有环图（实际上按时间轴展开也得到的是有向无环图）
    连续分布的概率图模型：高斯图模型

Markov 网络
    基本结构(local structure)：团，每个团对应于一个正的局部因子(local factor)
    联合概率分布：传统的 Gibbs 分布表示、Log-linear 表示
    Markov blanket：所有邻居节点
(Log-linear 表示：
$P(\pmb x) = \frac 1 Z e^{-U (\pmb x)}$
$U (\pmb x) =  - \sum_{C_i}\left[\psi_i(C_i)\prod_{x_j \in C_i}x_j\right]$ 
仅适用于离散分布)

Bayesian 网络
    基本结构：父节点 -> 子节点，对应于一个 CPD
    联合概率分布：所有局部 CPD 的乘积
    Markov blanket：所有父节点、所有子节点、所有子节点的所有父节点

“表示”三步走：
    1. 定义随机变量
    2. 绘制图模型拓扑结构
    3. 确定局部概率模型

**Model Conditional Information**
Example: 
POS (part of speech) identification 词性标注
早期使用 HMM，将 POS 建模为隐状态，将词建模为观测
HMM 的缺陷：
在 HMM 中，我们要建模给定 POS，它生成不同的词的概率，故观测的维度太高了（等于词的数量）
HMM 将问题建模为一个生成式问题，建模的是联合概率 $P (X, Y)$，而我们实际仅需要生成模型，并不需要知道给定 POS，生成某个词的概率

Generative Model:
    建模所有变量的联合概率
    缺点：困难

Discriminative Model:
    仅建模条件概率 $P(Y\mid X = x)$（其中 $X$ 是观测变量，$Y$ 是状态变量），
    目的是推理状态变量 $Y$（例如标签）
    优点：简单
    缺点：决策边界上的 outlier 会较大程度影响决策边界；无法生成新数据
    (you don't really konw how the things work, unless you can make one
    you don't really konw what you are working, unless you can make anybody understand)

Example:
Max Entropy Markov Model (MEMM)
判别模型，该模型利用 Markov 链拓展了最大熵分类器，假定了隐变量通过 Markov 链相连，而不是互相条件独立

但是该模型效果不理想被抛弃了

Example:
Generalized Conditional BNs
对 BN 中的全部节点都添加一个父节点，称为条件节点，该节点永远被观测到
因为该节点永远被观测到，故对图的实际分解并没有影响，在原来的每个 CPD 的条件中加上该节点即可，同时所有 CPD 乘起来的联合分布也是在给定该条件时的条件分布

Conditional MNs (CRFs)
对 MN 中的全部节点都添加一个邻居节点，该节点永远被观测到，故也不会影响 MN 中的节点的 Markov blanket


Example Recall: POS
考虑表示以下特性（这些特性是语言本身的特点）：
- 如果词在句子开始，POS 为 noun 的概率上升
- 如果词不在句子开始，且首字母大写，POS 为 noun 的概率上升
- 如果上一个词是 vt，该词是 noun 的概率上升

考虑使用 CRF 模型解决 POS 问题：
其中条件节点 $\pmb X = X_1, \dots, X_n$ 表示观测到的词序列
状态节点 $Y_1, \dots, Y_n$ 以 Markov 链两两相连，且都与条件节点 $\pmb X$ 相连

CRF 中的 feature function 定义为 indicator function，该 indicator function 是关于一个状态和一个观测子序列的 indicator，例如 $f_k (X, Y_i) = 1$ 仅当 $Y_i = noun$ 且当前观测 $X$ 是首字母大写的时候成立
此时，每个 feature function 都编码了 Markov 链中的一个或两个相邻节点以及观测 $\pmb X$ 的一个子序列

注意这些 feature function 的设计都是直接根据语言特性和经验人为设计得到的

**Deep structures**
Shallow models:
    why shallow?
    用于训练判别式模型的数据量较少，因此需要减少过拟合的结构风险 ( Reduce structure risk for overfitting )，也就是模型不能太复杂

Deep models:
    why deep?
    有大量数据可以用于训练生成式模型，模型复杂度应该与数据复杂度匹配