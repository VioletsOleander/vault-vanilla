# 1 导论
第一节课的内容：
1. MPI + X (CUDA/SASA/OpenMP/HIP 等并行编程框架) 
    MPI 执行进程级并行
    X 执行线程级并行
2. HPC is important.

# 2 MPI 并行程序设计入门
## 并行编程简介
并行计算机可以
- 按照指令和数据的并行方式分类，分为 SIMD/SPMD/MIMD/MPMD 等
- 按照存储的组织方式分类，分为共享内存、分布式内存、分布式共享内存 (共享内存和分布式内存的结合)

并行编程模型包括：
- 数据并行：对不同的数据执行相同的操作
    OpenMP, OpenACC
    消息传递由编译器实现
    性能和效果依赖于编译器
    适用于 SIMD/SPMD，即单指令/单程序下的并行
    适用于共享内存
- 消息传递：对不同的数据执行不同的操作，用消息传递进行同步
    消息传递由程序员实现
    性能和效果依赖于程序员
    适用于 SIMD/SPMD/MIMD/MPMD 全部
    适用于分布式内存、分布式共享内存

## 并行算法
优化加速比 Speedup 定义为优化前后性能的提升比值，通常用执行时间的比值表示，即

$$
Speedup = \frac {T_{\text{before}}}{T_{\text{after}}}
$$

优化效率定义为优化加速比和理论最优加速比 $S$ 的比值，即

$$
E = \frac {Speedup}{S}
$$

对于一个既有串行成分又有并行成分的程序，它的理论最优加速比可以用 Amdahl 定律计算，即

$$
S = \frac {1}{\frac {f_{\text{parellel}}}{P} + (1-f_{\text{parellel}})}
$$

其中 $P$ 表示并行的粒度，$f_{\text{parallel}}$ 表示程序中可并行的部分在串行执行时占用的计算时间的比率，或者可以粗略理解为并行程序所占的比率

Amdahl 定律指出，在并行粒度 $P$ 相同的情况下，尽可能多地并行化代码可以提高理论最优加速比

实践中，一般并行的粒度越小，越有可能开发更多的并行度，但并行的粒度越小，通信次数和通信量就相对增多，会带来更多的开销，这仍是一个 trade-off 问题

并行算法设计的一大重要原则就是增大计算时间相对于通信时间的比重，甚至**以计算换通信**
原因在于通信开销往往要远大于计算开销，因此需要尽可能降低通信次数 (为此并行粒度不能太小，以及需要尽可能合并可以合并的通信)，重叠计算和通信等等

## MPI 简介
MPI (Message Passing Interface) 是一种标准，不特指某个具体实现，它是一个消息传递编程模型，最终目的是服务于进程间通信这一目标

MPI 以独立于语言的形式定义其接口，对 Fortran, C, C++ 的绑定基于该定义实现，该定义不包含任何专用于某个操作系统、制造商、硬件的特性

MPI 程序的结构和其他程序的差异仅在于 MPI 环境，MPI 程序中，所有的 MPI 并行代码应该在 MPI 环境中，程序员需要在适合的地方初始化 MPI 环境和退出 MPI 环境

所有包含 MPI 的程序都需要 `#include` MPI 文件头，即 `#include <mpi.h>`

## 7 个基本 MPI 函数

```
MPI_INIT 进入 MPI 环境并初始化
MPI_FINALIZE 离开 MPI 环境
MPI_COMM_SIZE 返回当前通信域的进程数量
MPI_COMM_RANK 返回当前进程在通信域中的 ID
MPI_SEND 发送消息
MPI_RECV 接受消息
MPI_WTIME 计时函数
```

所有 MPI 名字都以 `MPI_` 前缀开头

MPI 接口的参数有三种类型：
- IN (输入)：接口的调用者传递给接口的参数，接口只能使用该参数，不得修改
- OUT (输出)：接口返回给其调用者的结果参数，该参数的初始值对接口没有意义
- INOUT (输入输出)：调用者将该参数传递给接口，接口修改该参数后返回给到用着，该参数的初始值和返回值都有意义

消息传递与消息接收：

```c
int MPI_Send(void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)
int MPI_Recv(void* buf, int ocunt, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status status)
```

前三个参数 (起始地址、消息长度、消息类型) 构成**消息缓冲** (message buffer)，后三个参数 (接受/发送进程、消息标签、通信域) 构成**消息信封** (message envelope)

MPI 的消息传递包括三个阶段：
- 消息装配：从缓冲区取数据，添加必要信息构造为消息
- 消息传递
- 消息拆卸：从消息中取出数据，存入缓冲区

三个阶段都需要类型匹配，装配时，缓冲区中变量类型需要和 `MPI_Send` 中指定的变量类型匹配，传递时，`MPI_Send` 和 `MPI_Recv` 指定的变量类型需要匹配，拆卸时，缓冲区变量类型需要和 `MPI_Recv` 指定的类型匹配

MPI 的消息类型/数据类型包括：
- 预定义数据类型：预定义数据类型会和语言的特定类型相对应
- 派生数据类型：用于处理包含了多种类型数据以及地址空间不连续的数据的消息

消息标签用于帮助接收者区分来自同一发送者的连续的消息，确保 `MPI_Send` 发出的消息由确定的 `MPI_Recv` 接受

接收者从 `MPI_Any_source` 接受 `MPI_Any_tag` 的消息时，消息状态中的信息可以用于确定消息的具体来源和标签

MPI 最基本的并行程序设计模式：主从模式、对等模式
基于这两个模式，可以设计任意复杂的 MPI 程序

# 3 MPI 进阶与深入
## MPI 的组通信
MPI 的通信：
- 点对点通信
- 组通信 (Collective Communications): 一个进程组中的所有进程都参加的全局通信操作

广播 `MPI_BCAST`：一个进程将相同的消息发送给通信域内所有其他进程
`MPI_BCAST` 的调用对于发送进程是发送消息，对于接受进程是接受消息，其他的组通信函数也是同理

收集 `MPI_GATHER`：一个进程从通信域内所有其他进程 (包括自己) 接受消息
收到的消息按照进程标识 rank 进行拼接，然后放入接受进程的缓冲区
`MPI_GATHER` 只能从每个进程收集同样多的数据，`MPI_GATHERV` 用于从不同进程收集不同大小的数据，`MPI_GATHERV` 还允许数据放在接受进程缓冲区的指定位置

散播 `MPI_SCATTER`: 执行和 GATHER 相反的操作，一个进程向其他进程发送不同的消息
SCATTERV 执行和 GATHERV 相反的操作

全局收集 `MPI_Allgather` : 相当于每个进程都执行一次 `MPI_GATHER` ，每个进程拿到所有进程的数据

全局交换 `MPI_Alltoall` : 相当于每个进程执行一次 `MPI_Scatter`

MPI 提供归约和扫描两种计算
归约 `MPI_Reduce`
组规约 `MPI_Allreduce` : 组内每个进程都执行一次归约操作，即每个进程都会有归约结果

扫描 `MPI_Scan` : 可以看作一种特殊的归约，每个进程都仅对排在它前面的进程执行归约操作

所有的 MPI 组通信操作都具有如下的特点:
- 通信域中的所有进程必须调用组通信函数。如果只有通信域中的一部分成员调用了组通信函数而其它没有调用，则是错误的。
- MPI_Barrier 以外，每个组通信函数使用类似于点对点通信中的标准、阻塞的通信模式。也就是说，一个进程一旦结束了它所参与的组操作就从组函数中返回，但是并不保证其它进程执行该组函数已经完成。
- 一个组通信操作是不是同步操作取决于实现。MPI 要求用户负责保证他的代码无论实现是否同步都必须是正确的

通信模式：
标准通信模式
对要发送的数据是否进行缓存由 MPI 决定

缓存通信模式
用户控制缓存区，用户保证发送消息前一定有缓存区可用

同步通信模式
通信模式的开始不依赖于接受进程相应的接受操作是否已经开始，但同步发送必须等到相应的接受进程开始才可以正确返回

就绪通信模式
只有当接收进程的接收操作已经启动时，才可以在发送进程启动发送操作

## 非阻塞通信 MPI
阻塞通信：调用返回结果之前，当前进程悬挂
非阻塞通信：不等待返回结果

标准非阻塞发送 `MPI_ISEND`：其调用的返回仅意味着该消息可以被发送，而不是成功发送
标准非阻塞接受 `MPI_IRECV`：其调用的返回仅意味着符合要求的消息可以被接受，而不是接受到了消息

参数 `request` 为非阻塞通信对象，描述了通信状态，`MPI_WAIT` 以 `request` 为参数，直到它对应的通信完成才返回，同时释放 `request`

## MPI 并行程序设计的两种基本模式
对等模式、主从模式

对等模式实例：jacobi 算法
局部性较好的算法都可以取得较高的并行性
halo 区：将数据按块分割 (给每个进程)，各块之内可以独立并行计算，各块之间相邻的元素则需要通信取得，因此各块需要额外分配一个 halo 区存储从邻居块通信得来的数据

在迭代之前，每个进程都需要从相邻进程取得数据块，同时也需要向相邻进程提供数据块
MPI 提供了捆绑发送和接受操作 `MPI_Sendrecv` ，可以在一条 MPI 语句中同时实现向其它进程的数据发送和从其它进程接收数据操作
`MPI_SENDRECV` 可以有效避免单独书写发送和接受操作时由于次序错误导致的死锁，因为系统会优化 `MPI_SENDRECV` 内的通信次序


对于左侧和右侧的边界块，不容易将各自的发送和接收操作合并到一个调用之中，因此在程序中，对边界块仍编写单独的发送和接收语句，所以 MPI 引入虚拟进程 `MPI_PROC_NULL` 把边界块和内部块统一看待，这样全部通信都用 `MPI_SENDRECV` 语句实现。

虚拟进程是不存在的假想进程。在 MPI 中的主要作用是充当真实进程通信的目的或源，引入虚拟进程的目的是为了在某些情况下编写通信语句的方便。当一个真实进程向一个虚拟进程发送数据或从一个虚拟进程接收数据时该真实进程会立即正确返回，如同执行了一个空操作

主从模式实例：矩阵向量乘

## 并行程序优化
串行程序优化
（1）调用高性能库
（2）选择适当的编译器优化选项

比较通用的 gcc 优化选项有“-O”、“-O0”、“-O1” “-O2”、“-O3” 等
- “-O0” 表示不做优化
- “-O1”、“-O2”、“-O3” 等表示不同级别的优化，优化级别越高，生成的代码的性能可能会越好，但采用过高级别的优化会大大降低编译速度，并且可能导致错误的运行结果。
- 通常，“-O2” 的优化被认为是安全的，它可以保证程序运行的正确性。对于一般程序的编译而言，使用优化选项“-O2” 或“-O3” 就可以了。

（3）注意嵌套循环的顺序

提高 cache 使用效率的一个简单原则是尽量改善**数据访问的空间局部性和时间局部性**
- 空间局部性指访问了一个地址后，应该接着访问它的邻居
- 时间局部性则指对同一地址的多次访问应该在尽可能相邻的时间内完成

（4）数据分块

当处理大数组时，对数组、循环进行适当分块有助于同时改善访存的时间和空间局部性
数据分块是一项比较复杂的优化技术，好的分块方式与分块参数的确定需要对代码及 cache 结构进行非常细致的分析或通过大量的实验才能得到

（5）循环展开
循环展开除了能够改善数据访问的时间和空间局部性外，还由于增加了每步循环中的指令与运算的数目，亦有助于 CPU 多个运算部件的充分利用

并行程序优化：
（1）减少通信开销

减少通信时间的途径主要有四个：
- 减少通信量：尽量将通信局限在相邻的子区域之间，避免整个数据场的全局通信
- 提高通信粒度：减少通信次数，即尽可能将可以一起传递的数据合并起来一次传递
- 提高通信中的并发度: 即不同结点对间同时进行通信
- 使用高速互联网络

（2）全局通信尽量利用高效聚合通信算法 (调 MPI 库)

但使用标准库函数的一个缺点是整个通信过程被封装起来，无法在通信的同时进行计算工作，此时，可以自行编制相应通信代码，将其与计算过程结合起来，以达到最佳的效果

（3）挖掘算法的并行度，减少 CPU 空闲等待
一些具有数据相关性的计算过程会导致并行运行的部分进程空闲等待。在这种情况下，可以考虑改变算法来**消除数据相关性**。

某些情况下数据相关性的消除是以增加计算量做为代价的。当算法在某个空间方向具有相关性时，应该考虑充分挖掘其他空间方向以及时间上的并行度

（4）负载平衡

必要时使用动态负载平衡技术，即根据各进程计算完成的情况动态地分配或调整各进程的计算任务。

动态调整负载时要考虑负载调整的开销及由于负载不平衡而引起的空闲等待对性能的影响，寻找最优负载调整方案。

（5）通信、计算的重叠: 非阻塞通信

（6）通过引入重复计算来减少通信

# 4 AI Computing System
现阶段的智能计算系统通常是集成 CPU 和智能芯片的异构系统，软件上通常包括一套面向开发者的智能计算编程环境（包括编程框架和编程语言）

第一代智能计算系统： 1980 年代，面向符号主义智能处理的专用计算机（Prolog 机， LISP 机）
第二代智能计算系统： 2010 年代，面向连接主义智能处理的专用计算机（深度学习计算机）
第三代智能计算系统：未来强人工智能/通用人工智能的载体

摩尔定律在 21 世纪发展放缓，通用 CPU 性能增长停滞，专用智能计算系统的性能优势越来越大

深度学习框架：将深度学习算法中的基本操作封装成一系列组件，帮助研究人员更简单的实现已有算法，或设计新的算法。这一系列深度学习组件，即构成一套深度学习框架

异构编程模型：在特定加速设备（CPU、 GPU、 ASIC 等）上进行程序设计和优化的编程方法和编程语言

举例：
- 开放标准：HiP， OpenCL， oneAPI
- 通用语言：CUDA（Nv）， Ascend-C（华为昇腾）， SDAA-C（神威 AI）

深度学习算法的实现人员异构编程语言语言将神经网络的基本操作实现为在加速器上运行算子，供编程框架调用

优化：
- 硬件化：计算和访存模式的适配
- 算法优化：降低存储和计算量
- 软硬件协同：面向硬件的算法优化

# 5 AI + X: Interdisciplinary Areas
Foundation Models: converting big data into a unified serialized format (全部转为 tokens 模式进行计算)

LLM 的组合泛化：对已有知识的融合创新，例如同时印象派和抽象派，把各种知识融在一起

AI4SCI 目前的方向： 
- 用 AI 计算 (DL 拟合近似) 替代传统的科学数值 (密集型) 计算
    例如，用 AI 近似计算 Hessian 矩阵
- (目前尚在探索阶段) 让 AI 读文章，做研究，发现新科学规律

这里的 SCI 也包括了自然科学和社会科学

AI 善于捕捉相关性，因为是概率建模，但相关性 != 因果性，因此基于数据建模可能在逻辑推断中存在问题
Machine learning is inductive: supervised learning of neural networks is essentially function fitting of training data distribution (例如训练数据中仅包含白天鹅，模型就不太可能认识到黑天鹅的存在)

目前让 AI 直接发现新的物理规律、数学规律不太可能，但在实验科学中找规律还是有可能的，例如开普勒第三定律的发现 (开普勒在大量试验数据的基础上直接总结出开普勒第三定律)

SCI4AI：现在的 AI 科研和试验物理学有点像，都是大量试验，然后总结，例如 Scaling law 公式的总结

AlphaFold: pair representation，用已知的蛋白质结构推未知的蛋白质结构
蛋白质预测并不在意总体的结构正确与否，只是希望能够更多地发现靶点 (局部结构)，帮助药物研发

托卡马克: 在核聚变的模拟场景中，训练出控制策略，控制各个时间点的系统参数，在过去，这个过程是人完成的

LLM 知识不欠缺，但行动欠缺，没有相应经验，无法根据知识自我行动

目前来说，通用模型在特定领域比专用模型弱

# 6 GPU 并行程序设计
## 众核处理器概述
CPU 发展面临的问题
- 并行度有限
- 处理器-访存差异增大: 内存延时，内存带宽
- 功耗效率过低，无更多发展空间

**并行度有限**
CPU 处理器的并行度:
进程级并行 - MPI
线程级并行 - OpenMP
数据并行 - SIMD
指令级并行 - Instruction Level Parallelism

指令级并行是由硬件开发处理的，这层并行对于软件是透明的

一般数据级别并行在存在数据依赖时就不行，但此时仍存在指令级别并行的可能性

**处理器-访存差异增大**
缓解方法:
缓存 cache
数据预取 prefetching - 通过动态分析程序的访存模式，预测和选取未来数据访问，以降低访存时延 (memory stall) (注意，prefetching 的预测不可靠时，可能反而降低性能)

**功耗效率过低，发展无更大空间**
CPU 发展面临的问题:
更多资源用于控制电路
更多资源用于缓存数据
较少资源用于实际数据
芯片资源效率低
能量效率低

## GPU 并行程序设计
NVIDIA CUDA:
将 NVIDIA 的 GPU 进行抽象
细粒度多线程计算平台
为编程人员提供编程接口和工具
驱动 + 工具包 + SDK

GPU 集群的潜在问题: 访问 GPU 和访问远程节点类似，访问延时与带宽均在同一数量级
解决方法: 通过算法设计，让 CPU 和 GPU 协同计算；新型总线技术，例如 NVLink, OpenCAPI

CUDA APIs:
higher-level API called the CUDA runtime API
- `myKernel<<<grid_size>>>(args)`
- `Memcpy()`
- etc

low-level API called the CUDA driver API
- `cuModuleLoad, cuParamSetv, cuLaunchGrid` etc

Low abstraction lightweight GPU programming toolkits:
- CUDA C
- OpenCL
- OpenACC

In CUDA:
A kernel is executed as a grid of thread blocks
- Different kernels can have different grid/block configuration

Blocks of threads are transparently assigned to SMs
- A block of threads executes on one SM & does not migrate
- Threads from the same block have access to a shared memory and their execution can be synchronized
- Several blocks can reside concurrently on one SM, the specific number is resource dependent. 
- Each block can execute in any order relative to other blocks.

32 Threads = 1 Warp
- A warp (of threads) executes one common instruction at a time

The Host/device synchronization is implicit. There is an implicit barrier across all threads at kernel return

CUDA C:
CUDA C extends standard C as follows
- Function type qualifiers to specify whether a function executes on the host or on the device
- Variable type qualifiers to specify the memory location on the device
- A new directive to specify how a kernel is executed on the device
- Four built-in variables that specify the grid and block dimensions and the block and thread indices
- Built-in vector types derived from basic integer and float type

Built-in vector types:
Vector types derived from basic integer and float types
They are all structures, like this:

```c
typedef struct { 
    float x,y,z,w;
} float4;
```

They all come with a constructor function in the form  `make_<type name>`, e.g.,

```c
int2 make_int2(int x, int y);
```

Build-in variables:

```c
dim3 gridDim
unit3 blockID
dim3 blockDim
unit3 threadIdx
int warpSize
```

It is not allowed to take addresses of any of the built-in variables
It is not allowed to assign values to any of the built-in variables

Variable type qualifiers:

```c
__device__ int globalvar
__device__ __shared__ int sharedvar
__device__ __constant__ int constantvar
```

Function type qualifiers:

```c
__device__ float DeviceFunc()
__global__ void KernelFunc()
__host__ float HostFunc()
```

-  `__device__` and `__global__` functions do not support recursion, cannot declare static variables inside their body, cannot have a variable number of arguments
- `__device__` functions cannot have their address taken
- `__host__` and `__device__` qualifiers can be used together, in which case the function is compiled for both
- `__global__` and `__host__` qualifiers cannot be used together
- `__global__` function must have void return type, its execution configuration must be specified, and the call is asynchronous

Intrinsic Functions:
Supported on the device only
Start with `__`, as in `__sinf(x)`
End with
-  `_rn` (round-to-nearest-even rounding mode)
-  `_rz` (round-towards-zero rounding mode)
-  `_ru` (round-up rounding mode)
-  `_rd` (round-down rounding mode)
- as in `__fadd_rn(x,y)`;

There are mathematical (`__log10f(x)`), type conversion (`__int2float_rn(x)`), type casting (`__int_as_float(x)`), and bit manipulation (`__ffs(x)`) functions

Device Management:

```c
cudaGetDeviceCount()
cudaGetDeviceProperties()
cudaSetDevice()
cudaGetDevice()
cudaChooseDevice()
```

Error Handling:
All CUDA runtime API functions return an error code. The runtime maintains an error variable for each host thread that is overwritten by the error code every time an error concurs

```c
cudaGetLastError()
cudaGetErrorString()
```

## 性能优化
memory: shared memory, constant memory

thread and blocks:
The number of blocks in a grid should be larger than the number of multiprocessors
- Each multiprocessor should have at least one block to execute
- Desirable to have multiple active blocks per multiprocessor to avoid entire multiprocessor waiting on `__syncthreads()`
- Summary: thousands of grid blocks should be launched

The number of threads per block should be selected to maximize the occupancy
- 1024 maximum threads per thread block (depends on card)
- Occupancy also depends on the register usage as well
- Threads per block should be a multiple of warp size
- A minimum of 64 threads per block is desirable, but only if there are multiple concurrent blocks per SM

## 常用工具
As part of NVIDIA driver
- nvidia-smi (NVIDIA System Management Interface program)

As part of NVIDIA CUDA SDK
- deviceQuery
- bandwidthTest

# 7 OMP 与超算基础
## OpenMP
传统并行编程模型
- 数据并行: OpenMP (Open Multi-Processing), OpenACC
- 消息传递: MPI

OpenMP 并行适用于共享内存架构，采用 fork-join 并行执行模式
fork-join: fork 指主线程创造一个并行线程组, join 指并行线程组完成并行区域的语句时，它们同步、终止，仅留下主线程

OpenMP 并行由编译器主导，核心是对并行区的操作
并行区在结束处存在隐式同步

OpenMP 通过对已有语言的标注拓展来实现并行
OpenMP 代码中，包含编译制导的制导符，在 C/C++ 中以预处理宏指令的形式存在
并行制导符会被串行编译器忽略，故对于串行执行没有影响

并行制导符的形式为

```
#pragma omp <directive> [clause[[,] clause]...]
```

其中 `#pragma omp` 为制导指令前缀，所有的 OpenMP 语句都需要该前缀
`directive` 为制导指令，前缀和子句之间必须有一个正确的制导指令
`clause` 表示子句，子句可以无序
末尾换行符表示制导指令的结束

制导指令 `directive` 分为四大类

(1) 并行区控制: `parallel`
该指令表示开始并行执行后面的代码块，这段代码将被多个线程并行执行
并行区内的每个并行线程都有唯一的 ID (0 线程为主线程) ，在执行过程中线程个数不变，并行线程在该块最后进行隐式同步

可选的从句有:
- `if (scalar-expression)`：判断条件
- `private (list)`：指定变量为线程局部存储
- `shared (list)`：指定变量为所有线程共享
- `firstprivate (list)`： 线程局部存储变量，其初值是进入并行区之前的值
- `default (shared | none)`：默认变量属性
- `copyin (list)`： 让 threadprivate 的变量的值和主线程的值相同
- `reduction (operator: list)`：归约
- `num_threads (integer-expression)`： 设置线程数量

(2) 任务分担类:
-  `do/for`: 将循环任务分配给多线程执行，由用户保证循环之间没有数据依赖
- `sections`: `sections` 中的各个 section 并行执行, `sections` 与 `sections` 串行，隐式同步
- `parallel for/parallel sections`
- `single`: 仅单线程执行修饰代码，隐式同步

(3) 同步控制类:
- `critical` : 保证只有一个线程进入
- `flush`
- `barrier`
- `atomic`
- `master`

MPI .vs. OpenMP
- 库扩展 vs 语言扩展
- 进程并行 vs 线程并行
- 分布（不同地址空间）vs 共享内存（相同地址空间）
- 编写新的程序 vs 扩展已有程序（逐渐增加并行的比重）
- 程序编写: 难 vs 容易
- 程序性能: 高 vs 依赖并行的结构（规则并行性能高）
- 扩展性: 高 vs 不如 MPI
- 通信语句: 需要 vs 不需要（共享变量）


进程-线程并行: 节点间 MPI, 节点内 OpenMP

