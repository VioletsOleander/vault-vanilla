# 1 Introduction
Two key words:
1. Abstraction
2. Tradeoff

Tradeoff between throughput and latency
latency for a single user, throughput for the whole system
E.g. large batch size will benefit the throughput but not the latency, the serving system focus on throughput because this brings more money

Different upper system use similar infrastructures (store, compute, coordinate)

Learn how to debug in distributed systems in the labs

Vertical: scale-up
Horizontal: scale-out (multiple machines)

Strong consistency can only be defined if we have a globally synchronized clock time steps that assign a time step to each operation. This is very expensive.

Internet generally provides eventual consistency.

# 2 MapReduce
OS Level: PaaS
App Level: SaaS

Distributed systems concerns:
Scalability
Fault-Tolerance -> Correctness
Load Balance -> Performance
Consistency

Socket Program: server part, client part. Socket program is the insight of MPI.
MPI: `MPI_Send`, `MPI_Recv` , Scatter/Gather/Broadcast
'Scatter' scatters different values to many nodes, 'Broadcast' broadcasts the same values to many nodes.

Shuffle: collect intermediate key/value pairs with the same keys
We can use hash partition: hash(key)%N, to map pairs with the same key to the same partition.
In the same partition, we use sort, to further arrange the keys in finer grain. Note that a partition corresponds to a reduce task. The sorting should be done by the reduce worker locally.

MapReduce provides strong scalability, because the user only concerns about `map` and `reduce` functions, and not concerns about distribution. We can easily increase the cores for executing the MapReduce job without modifying `map` and `reduce` .
MapReduce provides strong load-balance. For `map` , if number of `map` is much larger than the number of nodes, the master just schedule `map` to a node when it is available. The situation for `reduce` is similar.
MapReduce provides strong fault-tolerance. For `map`, when a worker failed, the master just re-schedule its job to another node, as long as the document for the job to process remains unbroken. Therefore it fundamentally relies on the reliability of the underlying file system. The GFS provides reliability by replication. For `reduce` , it is similar, we should guarantee the intermediate key/value pairs intact. The intermediate key/value pairs should also be stored in the distributed file system , it is similar, we should guarantee the intermediate key/value pairs intact. The intermediate key/value pairs should also be stored in the distributed file system.

# 3 The Google File System
In RPC, the local just pass arguments to the remote, the remote executes the function and returns the results.
RPC hides the difficulties and complexity of commutating between different machines, making the remote invocation similar to local invocation.

Exactly once = At least once + duplication detection (memory based)

Heartbeat message: client tell the server it still alive

A distributed system is built upon RPC

# 4,5 Raft
A Turing machine is a state machine, therefore every computer program can be modeled as a state machine.

Raft is designed to directly reach consensus on multiple log entries.

The leader takes full responsibility (very similar to the primary in GFS)

Term id is similar to the proposal id in Paxos (monotonically increase)
Term id servers as the logical time, which is to used to compare who is older, who is newer. (physical time may be different for different perceivers, therefore physical time is not usable here)

The idea of Raft: if we have only one leader (proposer), the protocol can be simplified.

Availability: Muti-Paxos' availability is stronger than Raft, because Raft will be unavailable in the leader selection process. (just a better degree of high availability, both is actually high available)

Performance: leader's IOPS is the upper bound, multi-Paxos: IOPS is aggerated (multi-Paxos have potentially larger IOPS, but must be implemented very carefully)

Raft trade those for understandability.

`ApendEntries` will be periodically issued by the leader to establish authority, if a follower does not receive it for a certain time, it will start an election. 

Raft guarantees logically one term will only one leader (physically it may happens that there multiple leaders).
The older leader will issue `AppendEntires` with older term id, and the request will be denied.
Note that as long as there is a new leader, majority of voters will know now it is the new term. Therefore, the older leader will not achieve majority in the new term. (but still may be recognized by minority)
That is the guarantee brought be the Raft's leader election.

In GFS, the primary decides the index of append, In Raft, the log index to append is decided by leader. Similar again.

Older term can not prohibit other lag followers become the new leader.

# 6 ZooKeeper
Review for Raft:
linearizability: concurrent submitted operations should be executed in series (sequential) and atomically

It is desired that concurrent execution can still show a linearizability property (though not actually linear), which is considered a good consistency (we get both consistency and performance)

The consistency client want is exactly execute once, the consistency Raft want is consensus, we use duplication to achieve consensus based on consistency.

ZooKeeper:
Implement every distributed service using RSM will be time consuming. Therefore we define a higher-level abstraction of RSM.

ZooKeeper also want to scale the performance for read operations. In Raft, the performance is limited by the single leader.

Zab only supports consensus for writing operation, while Raft support any operations.

zxid is similar to the log index in Raft.

In Raft, only leader can response the client's reading. In ZooKeeper, followers can response it.

key-value store vs file system:
in kv store: there is no partial write, only full write
in file system: tree structure, not flat structure
ZooKeeper combines those two, the simplicity makes ZooKeeper able to guarantee consistency

又要有性能，又要有 consensus, ZooKeeper 就牺牲了数据结构的复杂性，只允许简单的 get/put

ephemeral is like heartbeat, sequential flag is like unique uid (uuid), or to say, we can use those two interface to implement those mechanism

like in GFS, in ZooKeeper, the version will be incremented after each modification

the `sync` operation internally is a `write` operation which writes zero data.

# 7 Chain Replication
Availability: Paxos > Raft > Primary/Backup

In PB, any server's failure will lead to service outage
In Raft, master's failure will lead to service outage
In Paxos, as long as the majority is alive, the service continues.

Tolerance: PB > Raft = Paxos

In PB, $N-1$ of $N$ servers' failure can be tolerated
In Raft, Paxos, the majority should be alive.

Why respond query from tail: because every information in tail is guaranteed to be committed.
