# 1 Introduction
Two key words:
1. Abstraction
2. Tradeoff

Tradeoff between throughput and latency
latency for a single user, throughput for the whole system
E.g. large batch size will benefit the throughput but not the latency, the serving system focus on throughput because this brings more money

Different upper system use similar infrastructures (store, compute, coordinate)

Learn how to debug in distributed systems in the labs

Vertical: scale-up
Horizontal: scale-out (multiple machines)

Strong consistency can only be defined if we have a globally synchronized clock time steps that assign a time step to each operation. This is very expensive.

Internet generally provides eventual consistency.

# 2 MapReduce
OS Level: PaaS
App Level: SaaS

Distributed systems concerns:
Scalability
Fault-Tolerance -> Correctness
Load Balance -> Performance
Consistency

Socket Program: server part, client part. Socket program is the insight of MPI.
MPI: `MPI_Send`, `MPI_Recv` , Scatter/Gather/Broadcast
'Scatter' scatters different values to many nodes, 'Broadcast' broadcasts the same values to many nodes.

Shuffle: collect intermediate key/value pairs with the same keys
We can use hash partition: hash(key)%N, to map pairs with the same key to the same partition.
In the same partition, we use sort, to further arrange the keys in finer grain. Note that a partition corresponds to a reduce task. The sorting should be done by the reduce worker locally.

MapReduce provides strong scalability, because the user only concerns about `map` and `reduce` functions, and not concerns about distribution. We can easily increase the cores for executing the MapReduce job without modifying `map` and `reduce` .
MapReduce provides strong load-balance. For `map` , if number of `map` is much larger than the number of nodes, the master just schedule `map` to a node when it is available. The situation for `reduce` is similar.
MapReduce provides strong fault-tolerance. For `map`, when a worker failed, the master just re-schedule its job to another node, as long as the document for the job to process remains unbroken. Therefore it fundamentally relies on the reliability of the underlying file system. The GFS provides reliability by replication. For `reduce` , it is similar, we should guarantee the intermediate key/value pairs intact. The intermediate key/value pairs should also be stored in the distributed file system , it is similar, we should guarantee the intermediate key/value pairs intact. The intermediate key/value pairs should also be stored in the distributed file system.

# 3 The Google File System
In RPC, the local just pass arguments to the remote, the remote executes the function and returns the results.
RPC hides the difficulties and complexity of commutating between different machines, making the remote invocation similar to local invocation.

Exactly once = At least once + duplication detection (memory based)

Heartbeat message: client tell the server it still alive

A distributed system is built upon RPC


