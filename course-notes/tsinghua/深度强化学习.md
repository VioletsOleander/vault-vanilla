# Lecture 1
RL vs Supervised Learning
- No supervision, only reward signal
- Reward is delayed, not immediate
- Data is not i.i.d . Data is sequentially collected. The actions will matter the data we collect.

## MDP Basics
MDP:
A set of states $\mathcal S$
A set of observations $\mathcal O$
A set of actions $\mathcal A$
A transition function $\mathcal T(s, a, s')$ , i.e. $P(s'\mid s, a)$ , also called the model/dynamics
A reward function $R(s, a, s')$
An initial state distribution
A terminal state

Markov property means in transition, the action outcomes only depend on the current state, no previous states needed.

Partially Observable MDP (POMDP):
Fully observable:
state is the same as observation (例如围棋)
Partially observable:
only part of the states are observed

状态集合 $\mathcal S$ 和观察集合 $\mathcal O$ 在简单的情况下相等，但在复杂实例下 (现实世界) 不同，例如环境的状态包括了 360 度下所有人的动作，而具身智能体的观察视角则是有限的。

Agent 的定义:
Sutton: Only the intelligent part is agent, other things like muscles and skeletons are just controllable environment.

An other idea: some part of the environment is part of the agent, for example when we use the notebook for noting, the notebook partially server as our intelligence (for memory, for thinking), then it is part of the agent.

Reward hypothesis:
All goals are described by scalar reward
It is not true in real life. In real life, the reward is multidimensional. For example, dating a girl will increase your happiness, and fail an exam will make you upset. When you failed an exam, dating a girl now may still not simply make you happy.

## Value Iteration
Initialization: $V_0(s)\leftarrow 0$

Update:

$$
\begin{align}
V_{k+1}(s) := &\max_a\mathbb E_{s'\sim P(s'\mid s, a)}[R(s,a,s') + \gamma V_k(s')\mid s, a]\\
=&\max_a\sum_{s'}P(s'\mid s, a)[R(s, a, s') + \gamma V_k(s')]\\
\end{align}
$$

Convergence of policy and value function:
For policy, only the rank matters, the value's actual value does not matter so much. Therefore, the policy converges faster than the values.

Complexity for each state's update: $O(|\mathcal S||\mathcal A|)$
Complexity for the whole update: $O(|\mathcal S|^2 |\mathcal A|)$

## Policy Iteration
Sometimes policy iteration converges much faster than value iteration.

### Policy evaluation
Initialization: $V_0^\pi(s) \leftarrow 0$

Update:

$$
\begin{align}
V_{k+1}^\pi(s)&\leftarrow \mathbb E_{s'\sim P(s\mid s, \pi(s))}[R(s, \pi(s),s') + \gamma V_{k}^\pi(s')]\\
&=\sum_{s'} P(s'\mid s, \pi(s))[R(s, \pi(s),s') + \gamma V_{k}^\pi(s')]
\end{align}
$$

Complexity: $O(|\mathcal S|^2)$ (the action should be sampled first, therefore not accounted in complexity)

### Policy Improvement

$$
\begin{align}
\pi_{k+1}(s) &= \arg\max_{a} Q_{k+1}(s, a)\\
&=\arg\max_a \mathbb E_{s'\sim P(s'\mid s, a)}[R(s, a, s') + \gamma V_{k+1}(s')]\\
&=\arg\max_a\sum_{s'}P(s'\mid s, a)[R(s, a,s') + \gamma V_{k+1}(s')]
\end{align}
$$

## MDP to Reinforcement Learning


# Lecture 2
## Model-free Estimation: Monte-Carlo Learning
Direct estimation:

$$
G_t  = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^{T-1}r_{T}
$$

use the empirical mean of $G_t$ to approximate $V_\pi(s) = \mathbb E_\pi[G_t \mid S_t = s]$

MC estimation can be updated online using the incremental mean formula:

$$
\begin{align}
\mu_k &= \frac {1}{k}\sum_{i=1}^k x_i\\
&=\frac 1 k(x_k + \sum_{i=1}^{k-1}x_i)\\
&=\frac 1 k(x_k +(k-1)\mu_{k-1})\\
&=\mu_{k-1} + \frac 1 k(x_k - \mu_{k-1})
\end{align}
$$

## Model-free Estimation: Temporal Difference Learning
TD(0)

$$
V(s)\leftarrow V(s) + \alpha[(R(s, a, s')+ \gamma V(s')) - V(s)]
$$

TD target: $R(s, a, s') + \gamma V(s')$
TD error: $\delta = R(s, a, s') + \gamma V(s') - V(s)$ 

# Lecture 3
Ideas to improve DQN:
(1) Double DQN to overcome overestimation

(2) Prioritized experience replay to give more informative samples (the samples which brings more TD error) larger weight

(3) Dueling network is an improvement of DQN architecture: the better the action $a$ is, the larger the advantage $A(s, a) = Q(s, a) - V(s)$ will be ($V(s)$ measures an average level of action choice)
Example: The value stream learns to pay attention to the road (the average level), and the advantage stream learns to pay attention to critical events

(4) Use n-step return (n-step TD target): some sort of trade-off between variance and bias

For DQN investigation, refers to the 'Rainbow' paper from DeepMind

Distributional DQN makes the value function to give a distribution of value instead of a hard-coded value

# Lecture 4
In DQN, we first learn value function, and then get the policy. (Implicit policy: the policy is induced by Q)
We may want to directly learn the policy, that is, we will directly parametrize the policy. (Remember that the policy generally converge faster than the value function)

The objective $J(\pmb \theta)$ is calculated based on scalar rewards from the environment. Though the action decision is given by $\pmb \theta$, the objective itself is not differentiable with respect to $\pmb \theta$ (as our target is to maximize the objective, this is different from previous TD learning that our target is to make the prediction delta close the the reward, therefore turns the undifferentiable reward differentiable)

Therefore, to solve this problem, there are two methods:
(1) Use the differentiable simulator
(2) Expand the expectation, the expectation is actually differential wrt to $\pmb \theta$.
(Well, I think the objective is just differentiable essentially in the very first beginning, all we need is just some math tricks)



