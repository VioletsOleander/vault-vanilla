# Lecture 1
RL vs Supervised Learning
- No supervision, only reward signal
- Reward is delayed, not immediate
- Data is not i.i.d . Data is sequentially collected. The actions will matter the data we collect.

状态集合 $\mathcal S$ 和观察集合 $\mathcal O$ 在简单的情况下相等，但在复杂实例下 (现实世界) 不同，例如环境的状态包括了 360 度下所有人的动作，而具神智能体的观察视角则是有限的。

Sutton: Only the intelligent part is agent, other things like muscles and skeletons are just controllable environment.

An other idea: some part of the environment is part of the agent, for example when we use the notebook for noting, the notebook partially server as our intelligence (for memory, for thinking), then it is part of the agent.

Reward hypothesis:
All goals are described by scalar reward
It is not true in real life. In real life, the reward is multidimensional. For example, dating a girl will increase your happiness, and fail an exam will make you upset. When you failed an exam, dating a girl now may still not simply make you happy.

For policy, only the rank matters, the value's actual value does not matter so much. Therefore, the policy converges faster than the values.