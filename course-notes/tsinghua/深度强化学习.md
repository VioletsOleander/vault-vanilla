# Lecture 1
RL vs Supervised Learning
- No supervision, only reward signal
- Reward is delayed, not immediate
- Data is not i.i.d . Data is sequentially collected. The actions will matter the data we collect.

## MDP Basics
MDP:
A set of states $\mathcal S$
A set of observations $\mathcal O$
A set of actions $\mathcal A$
A transition function $\mathcal T(s, a, s')$ , i.e. $P(s'\mid s, a)$ , also called the model/dynamics
A reward function $R(s, a, s')$
An initial state distribution
A terminal state

Markov property means in transition, the action outcomes only depend on the current state, no previous states needed.

Partially Observable MDP (POMDP):
Fully observable:
state is the same as observation (例如围棋)
Partially observable:
only part of the states are observed

状态集合 $\mathcal S$ 和观察集合 $\mathcal O$ 在简单的情况下相等，但在复杂实例下 (现实世界) 不同，例如环境的状态包括了 360 度下所有人的动作，而具身智能体的观察视角则是有限的。

Agent 的定义:
Sutton: Only the intelligent part is agent, other things like muscles and skeletons are just controllable environment.

An other idea: some part of the environment is part of the agent, for example when we use the notebook for noting, the notebook partially server as our intelligence (for memory, for thinking), then it is part of the agent.

Reward hypothesis:
All goals are described by scalar reward
It is not true in real life. In real life, the reward is multidimensional. For example, dating a girl will increase your happiness, and fail an exam will make you upset. When you failed an exam, dating a girl now may still not simply make you happy.

## Value Iteration
Initialization: $V_0(s)\leftarrow 0$

Update:

$$
\begin{align}
V_{k+1}(s) := &\max_a\mathbb E_{s'\sim P(s'\mid s, a)}[R(s,a,s') + \gamma V_k(s')\mid s, a]\\
=&\max_a\sum_{s'}P(s'\mid s, a)[R(s, a, s') + \gamma V_k(s')]\\
\end{align}
$$

Convergence of policy and value function:
For policy, only the rank matters, the value's actual value does not matter so much. Therefore, the policy converges faster than the values.

Complexity for each state's update: $O(|\mathcal S||\mathcal A|)$
Complexity for the whole update: $O(|\mathcal S|^2 |\mathcal A|)$

## Policy Iteration
Sometimes policy iteration converges much faster than value iteration.

### Policy evaluation
Initialization: $V_0^\pi(s) \leftarrow 0$

Update:

$$
\begin{align}
V_{k+1}^\pi(s)&\leftarrow \mathbb E_{s'\sim P(s\mid s, \pi(s))}[R(s, \pi(s),s') + \gamma V_{k}^\pi(s')]\\
&=\sum_{s'} P(s'\mid s, \pi(s))[R(s, \pi(s),s') + \gamma V_{k}^\pi(s')]
\end{align}
$$

Complexity: $O(|\mathcal S|^2)$ (the action should be sampled first, therefore not accounted in complexity)

### Policy Improvement

$$
\begin{align}
\pi_{k+1}(s) &= \arg\max_{a} Q_{k+1}(s, a)\\
&=\arg\max_a \mathbb E_{s'\sim P(s'\mid s, a)}[R(s, a, s') + \gamma V_{k+1}(s')]\\
&=\arg\max_a\sum_{s'}P(s'\mid s, a)[R(s, a,s') + \gamma V_{k+1}(s')]
\end{align}
$$

## MDP to Reinforcement Learning


# Lecture 2
## Model-free Estimation: Monte-Carlo Learning
Direct estimation:

$$
G_t  = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^{T-1}r_{T}
$$

use the empirical mean of $G_t$ to approximate $V_\pi(s) = \mathbb E_\pi[G_t \mid S_t = s]$

MC estimation can be updated online using the incremental mean formula:

$$
\begin{align}
\mu_k &= \frac {1}{k}\sum_{i=1}^k x_i\\
&=\frac 1 k(x_k + \sum_{i=1}^{k-1}x_i)\\
&=\frac 1 k(x_k +(k-1)\mu_{k-1})\\
&=\mu_{k-1} + \frac 1 k(x_k - \mu_{k-1})
\end{align}
$$

## Model-free Estimation: Temporal Difference Learning
TD(0)

$$
V(s)\leftarrow V(s) + \alpha[(R(s, a, s')+ \gamma V(s')) - V(s)]
$$

TD target: $R(s, a, s') + \gamma V(s')$
TD error: $\delta = R(s, a, s') + \gamma V(s') - V(s)$ 

# Lecture 3
## From Estimation to Policy
**Q-Learning:**

$$
Q(s, a) \leftarrow Q(s,a) + \alpha\underbrace{(R(s) + \gamma\max_{a'}Q(s',a') - Q(s,a))}_{\text{One step TD error}}
$$

**Q-Learning Convergence:**

Theorem: Q-learning converges to the optimal Q-value function in the limit with probability 1, if
- MDP states and actions are finite
- Every state-action pair is visited infinitely often
- Exploration is enabled
- Robins-Monro Condition: $\sum_{t=0}^\infty \alpha_t(s, a) = \infty, \sum_{t=0}^{\infty}\alpha_t^2(s,a) <\infty$

Proof:
Theorem: The random process $\{\Delta_t\}$ taking values in $\mathbb R^n$ and defines as 

$$
\Delta_{t+1}(x) = (1-\alpha_t(x))\Delta_t(x) + \alpha_t(x)F_t(x)
$$

converges to zero with probability 1 under the following assumptions:
- $0\le \alpha_t\le 1,\sum_{t=0}^\infty \alpha_t = \infty, \sum_{t=0}^\infty \alpha_t^2 < \infty$
- $\|\mathbb E[F_t\mid \mathcal F_t]\|_\gamma \le \gamma\|\Delta_t\|_W$, with $\gamma < 1$
- $\mathrm{Var}[F_t\mid \mathcal F_t] \le C(1 + \|\Delta_t\|_W^2)$, for $C > 0$

(Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6(6):1185–1201, 1994)

Let $\Delta_t(x, a) = Q_t(x, a) - Q^*(x, a)$, then

$$
\begin{align}
\Delta_{t+1}(x,a) &=Q_{t+1}(x, a) - Q^*(x,a)\\
&=Q(x,a) + \alpha_t(x,a){(R(s) + \gamma\max_{a'}Q(x',a') - Q(x,a))} - Q^*(x,a)\\
&=(1-\alpha_t(x,a))Q(x,a) + \alpha_t(R(x)+ \gamma\max_a'Q(x',a')) - Q^*(x,a)\\
&=(1-\alpha_t(x,a))(Q(x,a) - Q^*(x,a)) + \alpha_t(R(x)+ \gamma\max_a'Q(x',a') - Q^*(x,a))\\
&=(1-\alpha_t(x,a))\Delta_{t}(x,a) + \alpha_t(R(x)+ \gamma\max_a'Q(x',a') - Q^*(x,a))\\
&=(1-\alpha_t(x,a))\Delta_{t}(x,a) + \alpha_t(x,a)F_t(x)\\
\end{align}
$$

**SARSA:**

$$
Q(s,a)\leftarrow Q(s,a) + \alpha\underbrace{(R(s) + \gamma Q(s',a') -Q(s,a))}_{\text{One step TD error}}
$$

On-policy: the next action should be sampled by current policy

## Function Approximation
To scale up: instead of calculating $v_\pi(s)$ directly for each state, we directly approximate function $v_\pi(\cdot): \mathcal S \mapsto \mathbb R$ (also $q(\cdot, \cdot): \mathcal S\times \mathcal A \mapsto \mathbb R$)

## Deep Q Learning
DL + RL: RL defines the objective (value function), DL learns the hierarchical feature representations and approximate the objective in a parameterized way.

Q values oscillates a lot:
(1) Correlated data distribution, the NNs might overfit -> inefficient data usage
(2) Overestimation problem
(3) Unstable gradients if reward are too large

Solutions:
For (1): Experience replay to increase data utilization
For (2): Target Q network
For (3): Control the reward range to limit the impact of any one update. DQN clips reward to \[-1, 1\] to prevents too large Q values and ensure gradients are well-conditioned.

Ideas to improve DQN:
(1) Double DQN to overcome overestimation

(2) Prioritized experience replay to give more informative samples (the samples which brings more TD error) larger weight

(3) Dueling network is an improvement of DQN architecture: the better the action $a$ is, the larger the advantage $A(s, a) = Q(s, a) - V(s)$ will be ($V(s)$ measures an average level of action choice)
Example: The value stream learns to pay attention to the road (the average level), and the advantage stream learns to pay attention to critical events

(4) Use n-step return (n-step TD target): some sort of trade-off between variance and bias

For DQN investigation, refers to the 'Rainbow' paper from DeepMind

Distributional DQN makes the value function to give a distribution of value instead of a hard-coded value

# Lecture 4
## Policy Gradient
In DQN, we first learn value function, and then get the policy. (Implicit policy: the policy is induced by Q)
We may want to directly learn the policy, that is, we will directly parametrize the policy. (Remember that the policy generally converge faster than the value function)

Therefore, to solve this problem, there are two methods:
(1) Use the differentiable simulator
(2) Expand the expectation, the expectation is actually differential wrt to $\pmb \theta$.
(Well, I think the objective is just differentiable essentially in the very first beginning, all we need is just some math tricks)

**Policy Objective Functions**
To measure the quality of the policy $\pi_{\pmb\theta}$, we can use the average reward per time step:

$$
\begin{align}
J_{\text{average Reward}}(\pmb\theta) &=\mathbb E_{S,A}[R(S,A)]\\
&=\sum_{s}d(s)\sum_{a}\pi_{\pmb\theta}(s,a)R(s,a)
\end{align}
$$

where $d(s)$ is the stationary distribution of the Markov Chain

Further, we can use the return instead of the immediate reward, that is, we can substitute $R(s, a)$ with the return $G(s, a)$. By doing this, we can further substitute $\sum_{a}\pi_{\pmb\theta}(s, a) G(s, a)$ with the value function $V_{\pi_{\pmb\theta}}(s)$. In this way, the objective function is

$$
\begin{align}
J(\pmb\theta) &= \mathbb E_S[V_{\pi_{\pmb\theta}}(S)]\\
&=\sum_s d(s) V_{\pi_{\pmb \theta}}(s)
\end{align}
$$

The objective $J(\pmb \theta)$ is calculated based on scalar rewards from the environment. Though the action decision is given by $\pmb \theta$, the objective itself is not differentiable with respect to $\pmb \theta$ (as our target is to maximize the objective, this is different from previous TD learning that our target is to make the prediction delta close the the reward, therefore turns the undifferentiable reward differentiable)

**Policy Gradient**
We first consider the policy gradient when using immediate reward in the objective function:

$$
\begin{align}
\frac {\partial J(\pmb\theta)}{\partial \pmb\theta} &= \sum_sd(s)\frac {\partial \pi_{\pmb\theta}(s,a)R(s,a)}{\partial \pmb\theta}\\
&=\sum_sd(s)\nabla_{\pmb\theta}\pi_{\pmb\theta}(s,a)R(s,a)
\end{align}
$$

Given $s$:

$$
\begin{align}
\nabla_{\pmb\theta}\mathbb E_{a\sim p(\cdot\mid s;\pmb\theta)}[R(s,a)]&=
\nabla_{\pmb\theta}\pi_{\pmb\theta}(s, a) R (s, a)\\
&=\sum_a R(s,a)\nabla_{\pmb\theta}\pi_{\pmb\theta}(s,a)\\
&=\sum_a R(s,a)\nabla_{\pmb\theta}p(a\mid s;\pmb\theta)\\
&=\sum_a R(s,a)p(a\mid s;\pmb\theta)\frac {\nabla_{\pmb\theta}p(a\mid s;\pmb\theta)}{p(a\mid s;\pmb\theta)}\\
&=\sum_a R(s,a)p(a\mid s;\pmb\theta)\nabla_{\pmb\theta}\ln p(a\mid s;\pmb\theta)\\
&=\mathbb E_{a\sim p(\cdot\mid s;\pmb\theta)}[R(s,a)\nabla_{\pmb\theta}\ln p(a\mid s;\pmb\theta)]
\end{align}
$$

By this derivation, we move the nabla operator into the expectation.

Next, consider the policy gradient when using the return (or the reward of the trajectory) in the objective function.

We denote the return $G(s, a)$ as $R(\tau)$, where $\tau$ represents the whole trajectory. The objective function now is

$$
\begin{align}
J(\pmb\theta) &= \mathbb E_S[R(\tau)]\\
&=\sum_s d(s) p(\tau\mid s;\pmb\theta)R(\tau)
\end{align}
$$

We can incorporate the initial state into the trajectory as well. Now the objective function is

$$
J(\pmb\theta) = \mathbb E_{\tau\sim p(\cdot;\pmb\theta)}[R(\tau)]
$$

where $p(\tau ;\pmb\theta)$ is

$$
p(\tau;\pmb\theta) = \underbrace{d(s_0)}_{\text{initial state distribution}}\cdot \prod_{t=0}^{T-1}[\underbrace{\pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta)}_{\text{policy}}\cdot \underbrace{p(s_{t+1},r_t\mid s_t,a_t)}_{\text{transition}}]
$$

The gradient is

$$
\begin{align}
\nabla_{\pmb\theta}J(\pmb\theta) &= \nabla_{\pmb\theta}\mathbb E_{\tau \sim p(\cdot;\pmb\theta)}[R(\tau)]\\
&=\mathbb E_{\tau\sim p(\cdot;\pmb\theta)}[R(\tau)\nabla_{\pmb\theta}\ln p(\tau;\pmb\theta)]
\end{align}
$$

where

$$
\begin{align}
\nabla_{\pmb\theta}\ln p(\tau;\pmb\theta) &= \nabla_{\pmb\theta}\ln \left[d(s_0)\cdot \prod_{t=0}^{T-1}\pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta) \cdot p(s_{t+1},r_t\mid s_t,a_t)\right]\\
&=\nabla_{\pmb\theta}\left[\ln d(s_0) + \sum_{t=0}^{T-1}\ln \pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta) + \sum_{t=0}^{T-1}\ln p(s_{t+1,r_t\mid s_t,a_t})\right]\\
&=\nabla_{\pmb\theta}\sum_{t=0}^{T-1}\ln \pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta)\\
&=\sum_{t=0}^{T-1}\nabla_{\pmb\theta}\ln \pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta)\\
\end{align}
$$

Therefore

$$
\begin{align}
\nabla_{\pmb\theta}J(\pmb\theta) &= \nabla_{\pmb\theta}\mathbb E_{\tau \sim p(\cdot;\pmb\theta)}[R(\tau)]\\
&=\mathbb E_{\tau\sim p(\cdot;\pmb\theta)}[R(\tau)\nabla_{\pmb\theta}\ln p(\tau;\pmb\theta)]\\
&=\mathbb E_{\tau \sim p(\cdot;\pmb\theta)}[R(\tau)\nabla_{\pmb\theta}\sum_{t=0}^{T-1}\ln \pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta)]
\end{align}
$$

**REINFORCE**
1. Sample trajectory $\tau$
2. Use MC to approximate $\nabla_{\pmb\theta}J(\pmb\theta)$
3. Update $\pmb \theta$

**REINFORCE with baseline**
Notice that given $s$:

$$
\begin{align}
\mathbb E_{a\sim\pi_{\pmb\theta}(a\mid s;\pmb\theta)}[\nabla_{\pmb\theta}B(s)\ln\pi_{\pmb\theta}(a\mid s;\pmb\theta)]&=B(s)\mathbb E_{a\sim \pi_{\pmb\theta}}[\nabla_{\pmb\theta}\ln \pi_{\pmb\theta}(a\mid s;\pmb\theta)]\\
&=B(s)\mathbb E_{a\sim \pi_{\pmb\theta}}[\frac {\nabla_{\pmb\theta}\pi_{\pmb\theta}(a\mid s;\pmb\theta)}{\pi_{\pmb\theta}(a\mid s;\pmb\theta)}]\\
&=B(s)\sum_a\nabla_{\pmb\theta}\pi_{\pmb\theta}(a\mid s;\pmb\theta)\\
&=B(s)\nabla_{\pmb\theta}\sum_a \pi_{\pmb\theta}(a\mid s;\pmb\theta)\\
&=B(s)\cdot\nabla_{\pmb\theta}  1\\
&=0
\end{align}
$$

Therefore, any function $B(s)$ can be used as baseline without affecting the policy gradient's accuracy 

$$
\begin{align}
\nabla_{\pmb\theta}J(\pmb\theta) &= \nabla_{\pmb\theta}\mathbb E_{\tau \sim p(\cdot;\pmb\theta)}[R(\tau)]\\
&=\mathbb E_{\tau\sim p(\cdot;\pmb\theta)}[R(\tau)\nabla_{\pmb\theta}\ln p(\tau;\pmb\theta)]\\
&=\mathbb E_{\tau \sim p(\cdot;\pmb\theta)}[R(\tau)\nabla_{\pmb\theta}\sum_{t=0}^{T-1}\ln \pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta)]\\
&=\mathbb E_{\tau \sim p(\cdot;\pmb\theta)}[(R(\tau)-B(s))\nabla_{\pmb\theta}\sum_{t=0}^{T-1}\ln \pi_{\pmb\theta}(a_t\mid s_t;\pmb\theta)]\\
\end{align}
$$

Paper 'The Role of Baselines in Policy Gradient Optimization' shows that the primary effect of baseline is to reduce the aggressiveness of the updates rather than their variance.

How to parameterize the policy $\pi_{\pmb\theta}$?
(1) Neural Networks
(2) Softmax
Softmax weights actions using linear combination of features $\phi(s, a)^T\pmb\theta$, and the probability of each action is proportional to exponentiated weight:

$$
\pi_{\pmb\theta}(s, a) \propto e^{\phi(s,a)^T\pmb\theta}
$$

The score function is:

$$
\nabla_{\pmb\theta}\ln\pi_{\pmb\theta}(s,a) = \phi(s, a) ) -\mathbb E_{\pi_{\pmb\theta}}[\phi(s,\cdot)]
$$

(3) Gaussian Policy
Mean is a linear combination of state features $\mu(s) = \phi(s)^T\pmb\theta$
Variance may be fixed to $\sigma^2$ or be parameterized
Policy is a Gaussian $a \sim \mathcal N(\mu(s), \sigma^2)$
The score function is 

$$
\nabla_{\pmb\theta}\ln\pi_{\pmb\theta}(s,a) = \frac {(a-\mu(s))\phi(s)}{\sigma^2}
$$

# Lecture 5
## Policy Gradient
Baseline remove some kind of randomness of the transitions, improving focus on certain action, and center the return to zero mean.

## Off-Policy Policy Gradient
Importance sampling is a Monte Carlo method for evaluating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. （Wikipedia）

off-policy PG objective (using sample from another policy $\tilde p(\tau)$)

$$
J(\theta) = \mathbb E_{\tau \sim \tilde p(\tau)}[\frac {p_\theta(\tau)}{\tilde p(\tau)}r(\tau)]
$$

where

$$
\frac{p_\theta(\tau)}{\tilde p(\tau)} = \frac {p(s_0)\prod_{t=0}^T\pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)}{p(s_0)\prod_{t=0}^T\tilde \pi(a_t\mid s_t)p(s_{t+1}\mid s_t, a_t)}=\frac {\prod_{t=0}^T\pi_\theta(a_t\mid s_t)}{\prod_{t=0}^T \tilde \pi(a_t\mid s_t)}
$$

off-policy Policy Gradient:

$$
\begin{align}
\nabla_{\theta_{now}}J(\theta_{now}) &= \nabla_{\theta_{now}}\mathbb E_{\tau \sim p_{\theta_{old}}(\tau)}[\frac {p_{\theta_{now}}(\tau)}{p_{\theta_{old}}(\tau)}r(\tau)]\\
 &= \mathbb E_{\tau \sim p_{\theta_{old}}(\tau)}[\frac {p_{\theta_{now}}(\tau)}{p_{\theta_{old}}(\tau)}\nabla_{\theta_{now}}\ln \pi_{\theta_{now}}(\tau)r(\tau)]\\
 &= \mathbb E_{\tau \sim p_{\theta_{old}}(\tau)}[\frac {p_{\theta_{now}}(\tau)}{p_{\theta_{old}}(\tau)}(\sum_{t=0}^T\nabla_{\theta_{now}}\ln \pi_{\theta_{now}}(a_t\mid s_t))r(\tau)]\\
 &= \mathbb E_{\tau \sim p_{\theta_{old}}(\tau)}\left[\left(\prod_{t=0}^T \frac {\pi_{\theta_{now}}(a_t\mid s_t)}{\pi_{\theta_{old}}(a_t\mid s_t)}\right)\left(\sum_{t=0}^T\nabla_{\theta_{now}}\ln \pi_{\theta_{now}}(a_t\mid s_t)\right)r(\tau)\right]\\
 &= \mathbb E_{\tau \sim p_{\theta_{old}}(\tau)}\left[\left(\prod_{t=0}^T \frac {\pi_{\theta_{now}}(a_t\mid s_t)}{\pi_{\theta_{old}}(a_t\mid s_t)}\right)\left(\sum_{t=0}^T\nabla_{\theta_{now}}\ln \pi_{\theta_{now}}(a_t\mid s_t)\right)\left(\sum_{t=0}^T r(s_t,a_t)\right)\right]\\
 &= \mathbb E_{\tau \sim p_{\theta_{old}}(\tau)}\left[\left(\prod_{t=0}^T \frac {\pi_{\theta_{now}}(a_t\mid s_t)}{\pi_{\theta_{old}}(a_t\mid s_t)}\right)\left(\sum_{t=0}^T\nabla_{\theta_{now}}\ln \pi_{\theta_{now}}(a_t\mid s_t)\right)\left(\sum_{t=0}^T r(s_t,a_t)\right)\right]\\
\end{align}
$$

## Actor-Critic Algorithms
Critic: the baseline $V_\pi(s)$

Make DQN work for continuous action:
solution 1: sample and optimize
sample $N$ action from some distribution, and select the best one:

$$
\max_a Q(s, a)\approx\max\{Q(s,a_1), \dots, Q(s, a_N)\}
$$

solution 2: better Q function structure

$$
Q_\theta(s, a) = -\frac 1 2(a- \mu_\theta(s))^T P_\theta(s)(a- \mu_\theta (s)) + V_\theta(s)
$$

then

$$
\arg\max_{a}Q_\theta(s,a) = \mu_\theta(s)\quad {\text{then}}\quad \max Q_\theta(s,a) = V_\theta(s)
$$

solution 3: gradient ascent
using gradient to search the best action, but it is inefficient

solution 4: learn an approximate maximizer
basic idea: train an NN $\mu_\theta(s)$ that $\mu_\theta(s) = \arg\max_a Q(s, a)$

this idea leads to DDPG

DDPG have problems like: 
- overestimation
- The critics might be unstable
- The critic weirdly prefers some action but not their neighbor actions, which leads to strange landscape

solving it leads to Twin Delayed DDPG (TD3):

solution 1: clipped double Q-learning to address overestimation
we have two Qs, and choose the $min$ as my Q:

Why not double-Q?
In continuous condition, get optimal A from Q will require optimization which might be unstable and time-consuming

solution 2: delayed policy updates to address unstable critics
basic idea: lower the frequency of actor updates (or increase the frequency of critic updates)

solution 3: target policy smoothing to address strange landscape
basic idea: add noise to the actions to smooth the value
adding noise can help avoiding edging condition

$$
a_{\text{TD3}}(s) =\text{clip}(\mu_\theta(s) + \text{clip}(\epsilon, -c, c), a_{low}, a_{high})
$$

(both clipping the noise and the action added with noise)
