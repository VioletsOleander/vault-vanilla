# 1 What is Artificial Intelligence
> 2024.9.10 董超
## 1.1 The Definition of Artificial Intelligence
AI is the algorithm to solve complicated tasks
- complicated: ill-posed; probability (没有解析解、输出的是概率)

AI needs to realize complex objectives
- complex: complex 是一个维度上的，complicated 是多个维度上的
- objective: AI 不会自己生成目标，但会生成子目标，AI 的危险来自于自己生成的子目标

AI has the ability of learning from big data
- 人类的经验代代相传，是局部最优，机器可以探索全局最优

AI connects subjective perception and recognition
- 主观互通
## 1.2 What is Artificial General Intelligence
我们关注 AI 的能力，而不是过程；关注潜力，不关心部署
## 1.3 What is Artificial Super Intelligence
ASI can have objectives itself
# 2 Learning from Examples
> 2024.9.24 张超

用分类表示回归：
    我们需要的（回归）输出是32bit 浮点数，因此考虑到每一个 bit ，32bit 的浮点数就一共有 $2^{32}$ 个目标类别，或者将需要的（回归）输出看作 $4\times 8$ byte，进而一共有 $4\times 2^8$ 个目标类别
    为什么这么做：有时分类模型的效果会比回归好

线性分类器实现多类别分类：
    One vs the rest
    One vs one
    这两种方法都存在未定义的空间
    为此，可以考虑 Several vs the rest，取决于我们对问题的设计

Sigmoid 函数近似阶跃函数：
    阶跃函数不可导
    Sigmoid 函数 $y = 1/(1 + \exp (-a/\tau))$，在 $\tau$ 接近 $0$ 时无限趋近于阶跃函数，因此可以用 Sigmoid 函数 + 较小的 $\tau$ 来近似阶跃函数 ("soft" step function)

区分式 vs 生成式：
    生成式模型：建模输入或输出的分布，例如 $p (\vec x), p (\vec x \mid C_k)$
    区分式模型：直接建模后验概率 $p (C_k\mid \vec x)$，或者建模和决策相关的函数（例如边界）
    生成式模型可以生成新数据，但是需要训练数据多
    相同数据下，区分式模型效果比区分式好，但不了解潜在的特征依赖 
    GPT 实际上是用区分式模型进行生成（建模 $p(C_k\mid \vec x)$） 
    “生成式”也可以认为是表示输入和输出之间的弱（非唯一）联系，也就是其中存在随机性和不确定性，也就是“幻觉”
    实际上，“生成式”依赖于“幻觉”，但也受“幻觉“困扰

损失函数：
    熵：度量随机变量的不确定性
    交叉熵：没有最小值
    KL 散度：有最小值
    MSE：$\mathcal L = \sum_n (t_n-y_n)^2$
    MAE：$\mathcal L = \sum_{n} | t_n - y_n|$
# 3 Deep Learning and AI Computing
> 2024.9.29
## 3.1 Deep Learning
感知机：
    加权求和+非线性激活： $\hat y = g (\sum_{i=1}^m w_ix_i$)
    添加偏置项：$\hat y = g (w_0 + \sum_{i=1}^m w_i x_i)$

激活函数：
    例如：sigmoid, tanh, relu
    relu 简单快速，唯一的缺陷在于在零点处不可导
    目的：引入非线性

单输出感知机->多输出感知机
单层感知机->深层神经网络

损失：
    Empirical Loss = Empirical risk = Cost function = Objective function

训练：
    寻找最小化损失的参数

优化：
    梯度下降

学习率：
    按照梯度走的步长
    学习率调度：随着训练进行，学习率逐渐减小
    （线性调度：下降速度平缓；Cosine 调度：下降速度逐渐变快）

梯度的计算：
    反向传播
    minibatch：随机梯度下降
    视觉网络一般需要走90-100个 epoch，大模型1-2个 epoch 即可
    理论上，随机梯度下降和梯度下降的效率实际上是相当的（收敛性的上界一样），实践中，随机梯度下降的收敛比梯度下降快得多
    经验上，batchsize 扩大 n 倍，lr 也可以扩大 n 倍，收敛效率一致（直观上理解即样本越多，梯度估计约准确，故学习率可以更大）
## 3.2 AI Computing
AlexNet - 61M parameters - 262PFLOPS - GTX 580
GPT-3 - 175B parameters - 323ZFLOPS

AI Trends:
    2012年以前-机器学习：
    参数规模： $O (10^3)$，数据集规模： $O (10^3)$，硬件：CPU，代表模型：SVM
    2012年-2022年-深度学习：
    参数规模： $O (10^6)$，数据集规模： $O (10^5\sim 10^6)$，硬件：GPUs $O(\sim 10^3$)，代表模型：ResNet
    2022年以后-基石模型：
    参数规模： $O (10^{9\sim})$，数据集规模： $O (10^{11}\sim 10^{12})$，硬件：GPUs $O(10^4\sim 10^5)$，代表模型：GPT

从2002-2021年，模型、数据集规模随着年份指数上升
而训练的规模 = 模型规模×数据集规模

AI Platform：
    部署平台
    AI 框架
    后端：编译器、优化器、库
    硬件架构

Nvidia Grace Hopper:
    Grace: CPU
    Hopper: GPU
    内置了 Transformer Engine
    第四代 NVLink
    第三代 NVSwitch
    Nvidia Confidential Computing （隐私计算）
    第二代 MIG（虚拟化）
    DPX Instructions


GPU Communication:
    RDMA：GPU直接访问远端，不需要经过 CPU，但仍然会通过一次 CPU 总线
    GPUDirect RDMA：不通过 CPU 总线，通过 NVLink 实现

Nvidia DGX-2：
    高度集成的计算节点
    包括了16个 V100-SXM3 GPUs

Pytorch:
    Autograd
    Dispatcher: 根据 python 代码（要执行的运算）以及底层硬件信息（例如 CUDA、CPU 等），调用合适的 kernel

`torch.compile`:
    封装了：
    TorchDynamo-读取 Python Byte Code 得到图结构
    TorchInductor-基于 Dynamo 获取的 define-by-run IR 进行编译优化和 codegen，得到子图
    AOTAutograd
    PrimTorch-Stable Primitive operators

TorchDynamo：
    a Python level JIT compiler
    rewrites Python bytecode to extract sequences of PyTorch operations into an FX Graph
    FX Graph is then compiled by a customizable backend（自定义的后端编译器就可以进行算子融合）

算子融合：
    例如 MoE 的 gating 算子涉及到多个小操作，融合调用会比一个一个顺序调用减少不少开销

TorchInductor：
    it works as a JIT compiler for Pytorch models that transforms them into more optimized code
    例如将算子展开，然后交给 Triton 生成高性能代码

Triton：
    OpenAI 开发
    是基于 Python 进行 CUDA 编程的编程语言

Python code -> \[Dynamo and Inductor\]-> Triton code -> Triton IR -> MLIR

Pytorch ops (2000+) -> \[归类\]-> ATen ops (750+) -> \[归类\] -> Prim ops (250+)

JAX：
    仅 JAX 支持 TPU
    JAX 的后端是 XLA（和 Tensorflow 一样）
    JAX+XLA 类似于 Torch+Triton

XLA 自己就可以做图融合，Torch 则需要转为 FX Graph，然后交给外部编译器做

并行训练：
    数据并行：复制模型为多份，每一份模型计算不同数据，最后聚合梯度
    模型并行：模型切开，分块算矩阵乘，子矩阵求和时需要通信，因此计算中需要不断通信

数据并行的梯度通信：
    Parameter Server：数据给 Serever，Server 负责聚合计算，然后分发
    该方法适用于稀疏网络，例如推荐引擎（一个样本仅激活部分参数，故可以不用传全部数据给 Server，但 Transformer 这类模型不是稀疏的，一个样本会激活所有参数）
    All Reduce：GPU 直接互联，使用 Ring 算法一起计算

模型并行：
    Tensor Model Parallel：竖着切
    Pipeline Parallel：横着切，为了减少 worker 之间的等待，采用流水线

# 4 Speech and Audio Processing
> 2024.10.8 张超

## 4.1 Speech Processing Tasks
### 4.1.1 Speech/Audio perception and understanding
**Automatic Speech Recognition (ASR)** 
    语音转文本 (speech-to-text/STT)
    其子任务：keyword spotting (KWS), wake-up work recognition, phoneme recogintion, syllable recognition, word recognition

ASR 按词袋大小划分为：
    数字识别
    命令识别
    大词袋连续语音识别 (LVCSR)
    端到端识别 (无词袋)
    上下文语音识别

ASR 基本公式：

$$
\begin{align}
W^* &= \arg\max_W P(W\mid O)\\
&= \arg\max_W \frac {P(O\mid W)P(W)}{P(O)}\\
&= \arg\max_W P(O\mid W)P(W)
\end{align}
$$

其中 $W^*$ 是输出假设，也就是输出的单词序列，通过 MAP 搜索（最大化后验概率）得到，该过程也称为解码 （decoding）
$O$ 是输入语音序列
$\arg\max_W P (O\mid W) P (W)$ 中，$P (O\mid W)$ 是声音模型（acoustic model），$P (W)$ 是语言模型（language model）

ASR 的代表模型是 OpenAI Whisper，其基本架构就是 Transformer
    Whiper-Large-v3：
    训练数据是 1M 小时的语音（100年=0.875M 小时）
    支持97个语言的 ASR
    支持发音级别的端点检测（说话片段的开始和结束）和自动语音翻译
    最大的版本参数量为1.55B

**Automatic Speech Translation (AST)**
    speech-to-text-to-speech or speech-to-speech

AST 的代表模型是 SeamlessM4T 和 Seamless Streaming，后者是前者的微调版本，用于支持流式的输入语音
Seamless M4T 支持 speech-to-speech 和 speech-to-text 的翻译，支持101中语言的语音输入，支持36种语言的文本输出、96种语言的语音输出

**Speech Enhancement**
    语音强化包括去噪、回声消除等

**Speech Seperation**
    将包含多个说话者的混合语音分离为多个流

**Speaker Recognition**

**Speaker Diarisatoin**
    说话人日志，在长的语音流中确认谁在什么时候说话，及其说话内容

**Audio/Sound Event Detection**
    检测和分类音频事件，例如 A man/woman is singling/dancing/crying

**Automatic Emotion Recognition (AER)**
    识别说话人的情感

语音-语言大模型：SALMONN (Speech Audio Langague Music Open NN)

语音-视频-语言大模型：Vide-SALMONN 2

### 4.1.3 Speech/Audio/Videio generation
Text-to-Speech synthesis (TTS)
    基于文本生成语音
    zero-shot TTS：通过一个人的3秒的语音样本生成其语音

Text-to-Sing (TTSing)
    代表模型：Diffsinger

Speech Editing
    向原语音中添加指定的话，使其听不出来差别

Voice Conversion/Cloning
    AI 孙燕姿

Text-to-Audio
    代表模型：AudioLDM2 - Generating audio events (soud effects), music and speech based on input text and audio prompts

Music Generation
    Suno

Music Editing
    代表模型：StableAudioControl

Audio Driven Portrait Animation Generation
    代表模型：EchoMimic

(End-to-end) Speech-Conversational AI
    代表模型：GPT-4o

# 5 自然语言处理
> 2024.10.15 张超

## 5.1 自然语言处理概览
自然语言特点：固有歧义性、无限性（递归性）、长距离依赖、以离散符号作为基本表示单元（与连续的数字表示不同）

最早：基于规则的方法
过渡：基于统计的方法（源语言和目标语言语法结构一一对应）
目前：基于深度学习的方法（Input --> Encoder --> Representation --> Decoder --> Output）

## 5.2 基于深度学习的自然语言处理
NLP 的基本问题：
- 分类：输入为语言，输出为标签
    例如文本分类、情感分析
- 匹配：为两个输入的匹配度输出分数
    例如搜索、问答、单轮对话
- 翻译：输入为源语言序列，输出为目标语言序列
    例如机器翻译、语音翻译、手写识别、单轮对话
- 结构预测：输入为语言序列，输出为该序列的语法分析树
    例如命名实体识别（e.g. “清华大学”, “建华楼”）、词性标注、语法/语义分析
    很长一段时间内，人们认为用树结构清晰表示语言要优于深度网络中的向量表示
- Markov 决策过程：序列的状态转移
    例如多轮对话，现在多轮对话已经是 GPT 统治了

词表示：如何用计算的方法定义词的含义
- One-hot 表示（很适合做语法分析树，故很长一段时间被认可）
- 实值向量表示

一个词的含义应该由伴随它出现的词定义，即上下文，显然 One-hot 做不到这一点

语言模型（LM）：判断任意给定字符串是一个合法句子的概率
    N-gram LM 即建模时每个词仅和它前 $n-1$ 个词有关系
    例如在 4-gram LM 中，给定前三个词作为输入，第四个词的概率建模为 $p (w_t \mid w_{t-1}, w_{t-2}, w_{t-3})$
    N-gram LM 的训练任务就是在大量的 N-gram 语料将 $P (w_t \mid context)$ 与 $w_t$ 在语料库中的 N-gram 词频匹配
    训练结束后，提取最后一个隐藏层输出作为词表示（Word Embedding）
    该方法提取出的词向量中，上下文相近的词其词向量会更接近

word2vec：
    CBOW：用上下文预测当前词
    Skip-Gram： 用当前词预测上下文
    优点：快
    缺点：没有词序信息

文本卷积：
    速度中等，可以部分利用词序信息

RNN:
    $h_t = f (W_h h_{t-1} + W_x x + b)$
    特点：理论上无限长、强时序（利用全部词序信息）
    缺点：慢、梯度爆炸/消失
    梯度消失举例：RNN 的激活函数一般用 Sigmoid 函数 $\sigma$，因此序列号高的状态 $h_t$ 就是经过一系列 Sigmoid 嵌套计算得到的。Sigmoid 函数的导数为 $\sigma' (x) = \sigma (x)(1-\sigma (x))$，因为 $0 < \sigma (x) < 1$，因此容易直到 $\sigma' (x)$ 在 $\sigma (x) = 0.5$ 时取到最大值为 $\frac 1 4$，并且在 $\sigma (x)$ 接近 0 或 1 时取值接近于 0，因此梯度在反向传播时会一直乘上小于1的数，逐渐消失，导致网络的较浅层根本训练不到。

LSTM:
    RNN 的问题：梯度爆炸/消失
    $i_t = \sigma (W^{(i)}x_t + U^{(i)}h_{t-1})$
    $f_t = \sigma (W^{(f)}x_t + U^{(f)}h_{t-1})$
    $o_t = \sigma (W^{(o)}x_t + U^{(o)}h_{t-1})$
    ($\sigma$ 是指 Sigmoid 函数，它将输入向量每一维都压缩到 $(0,1)$ 之间，这就类似于电路中的门控)
    $\tilde c_t = \tanh(W^{(c)}x_t + U^{(c)}h_{t-1})$
    $c_t = f_t \circ c_{t-1} + i_t \circ \tilde c_t$ 
    ($c_t$ 为 cell，表示记忆存储向量，该公式的意思是当前时刻的记忆向量等于之前的记忆向量忘掉一部分加上一部分新输入的记忆向量
    $f_t$ 表示遗忘门，计算需要遗忘多少之前的记忆
    $i_t$ 表示输入门，计算新输入的记忆占多少比例
    这就类似于记忆的容量是有限的，为了输入新的，需要遗忘一部分旧的)
    $h_t = o_t \circ \tanh (c_t)$
    (隐层状态等于 $c_t$ 经过 $\tanh$ 激活函数再和输出门结果 $o_t$ 按元素乘)
    LSTM 中，每个门都可以视作一个 RNN

GRU：
    $z_t = \sigma (W^{(z)}x_t + U^{(z)}h_{t-1})$
    $r_t = \sigma (W^{(r)}x_t + U^{(r)}h_{t-1})$
    $\tilde h_t = \tanh (Wx_t + r_t \circ Uh_{t-1})$
    $h_t = z_t \circ h_{t-1} + (1- z_t) \circ \tilde h_t$

RNN 典型结构：
每个时间步输出一个标签：
用于序列标注

Encoder-Decoder 结构：
Encoder-将输入序列所有信息压缩为一个向量
Decoder-将向量解码，解码到 EOS 停止
编码器常用 Elman RNN，即接受一个序列输入的 RNN，每个时间步的输入是上个时间步的输出 (隐藏层向量) + 对应的序列中的输入 (one hot token)，该结构适合编码一个序列。
解码器常用 Jordan RNN，即接受单个向量输入的 RNN，每个时间步的输入是上一个时间步的输出 (one hot token) + 输入向量，该结构适合自回归生成。

Bidirecitonal-RNN：
每个时刻的状态 = 正序的状态和逆序的状态拼接
其问题是网络中每个时间步的状态都包含了序列的所有信息，但没有具体差别，因此每个时间步的状态都是相似的，仅在顺序上有差异

Bidirectional-RNN + Attention：
每个时间步的状态来自于整个序列所有时间步输入的加权组合，提高了各个时间步状态的差异性

Attention is all you need：
不需要 RNN，Attention 就够了

预训练模型代表性工作：
word2vec: 直接从分布式表示学习词向量，问题是词向量是与上下文无关的
ELMo: Encoder 和 Decoder 都是双向 LSTM，先做预训练
BERT: LSTM 换 Transformer，BERT 开始，NLP 任务开始统一，预训练的范式开始流行

# 6 计算机视觉
> 2024.10.22 代季峰

# 7 多模态智能
> 2024.10.29 王佳琦

## 7.1 文本图像多模态模型
多模态任务：
- 判别式任务：图文检索、零样本分类
- 生成式任务：图像描述、VQA、视觉定位、文生图

多模态模型的发展：
    判别式多模态模型：CLIP/SigLIP
    早期生成式多模态模型：Coca/SimVLM/BLIP
    多模态大模型 (基于 LLM 的多模态模型)：Flamingo/BLIP2/LLaVA

**判别式多模态模型-CLIP** 
训练过程：
    数据准备：大量图像-文本对
    编码：使用图像编码器 (例如 ViT) 和文本编码器 (Bert) 将图像和文本转化为高维向量表示，并将该表示投影到公共空间
    对比损失：用对比学习训练编码器和投影矩阵，训练目标是将正确匹配图像-文本对推向相似的向量空间，不正确匹配的推向远离的空间
推理过程：
    编码：将图像和文本通过训练好的编码器转化为向量表示，并投影到公共空间
    计算相似度：计算二者之间的余弦相似度

伪代码：

```python
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW of Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of images embed to joint embed
# W_t[d_t, d_e] - learned proj of text embed to joint embed
# t - leraned temperature parameter

# extract feature representation of each modality
I_f = image_encoder(I) # [n, d_i]
I_t = text_encoder(T) # [n, d_t]

# joint multimodal embedding [n, d_e]
I_e = l2_normalize(np.dot(I_f, W_i), axis = 1)
T_e = l2_normalize(np.dot(I_t, W_t), axis = 1)

# scaled pairwise cosine similarities [n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)

# symmetric loss function
labels = np.arange(n)
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t) / 2
```

视觉编码器-ViT
输入处理：
    图像分块：划分 patch
    线性映射：将 patch 拉平为一个向量，然后投影到一个固定维度的 embedding 向量
位置编码：
    加上 positional embedding
Transformer 编码：
    MHA：捕获全局上下文信息
    MLP：进一步提特征
MLP 分类头：
    将最终 `[CLS]` token 的 embedding 投影，计算概率，做分类

文本编码器-Bert vs GPT
架构：
    Bert：基于 Encoder，双向编码器，token 注意到两边的上下文，用于理解任务
    GPT：基于 Decoder，单向解码器，token 注意到前面的上下文，用于生成任务
    Bert 适合编码，GPT 适合解码
训练方式：
    Bert：Masked Language Model，随机 mask 输入中的词，预测这些被 mask 的词；Next Sequence Prediction，判断两个句子是否连贯
    GPT：Auto-regressive，根据之前的词预测下一个词，主要关注生成连贯的文本

注意力计算：$Z = \text{softmax} \left(QK^\top/\sqrt d_k\right) V$，其中除以 $\sqrt d_k$ 是为了防止 $\text{softmax}$ 的输入过大导致梯度消失，使得训练更稳定
($y = \text{softmax }(x)$，则 $dy/dx = y - y^2$，如果某个 $x_i$ 过大，会导致 $y_i$ 接近于1，因此梯度 $dy_i/dx_i$ 就会接近于0，而其他的 $y_j$ 则会接近于0，因此梯度 $dy_j / dx_j$ 也会接近于0，本质是 softmax 函数在面临较大的值的时候 $y$ 的值接近饱和， $y$ 相对于 $x$ 的变化率太低了)

**早期生成式多模态模型**
Image Encoder + Text Decoder: VirTex/SimVLM
Image Encoder + Text Encoder + TextDecoder: Coca

SimVLM:
结构：图像 token 和文本 token 一起经过 Transformer Encoder，然后再经过 Transfomer Decoder，自回归解码输出文本 token
预训练：数据为图像 + 描述文本，进行生成式预训练，Prefix Language Modeling (给定图和关于图的自然语言问题，自回归预测下一个 token/答案)

CoCa:
融合判别式 VLM (CLIP) 和生成式 VLM (SimVLM) 的功能
    判别式任务：零样本分类、图文检索
    生成式任务：图像描述、VQA
    两个损失一起训练：图文对比学习损失 (constrastive loss)、生成式学习损失 (captioning loss)

Coca 的性能在多个任务上全包围了之前的 SOTA 模型

伪代码：

```python
# image, text.ids, text.labels, text.mask: paired {image, text} data
# con_query: 1 query token for constrastive embedding
# cap_query: N query tokens for captioning embedding
# cls_token_id: a special cls_token_id in vocabulary

def attentional_pooling(features, query):
    out = multihead_attention(img_feature, con_quary)
    return layer_norm(out)

img_feature = vit_encoder(image) # [batch, seq_len, dim]
con_feature = attentional_pooling(img_feature, con_quary) # [batch, 1, dim]
cap_feature = attentional_pooling(img_feature, cap_query) # [batch, N, dim]

ids = concat(text.ids, cls_token_id)
mask = concat(text.mask, zeros_like(cls_token_id)) # unpad cls_token_id
txt_embs = embedding_lookup(ids)
unimodal_out = lm_transformers(txt_embs, mask, cross_attn=None)
multimodal_out = lm_transformers(unimodal_out[:, :-1, :], mask, cross_attn=cap_feature)
cls_token_feature = layer_norm(unimodal_out)[:, -1:, :] # [batch, 1, dim]

con_loss = constrastive_loss(con_feature, cls_token_feature)
cap_loss = softmax_cross_entropy_loss(multimodal_out, labels=txt.labels, mask=txt.masks)
```

**多模态大模型**
Modality Encoder 多模态编码器
    CLIP：编码连续特征空间，保留更多细节；已完成文本对齐训练，降低 LLM 对 image token 对齐的难度
    SigLIP
    VQ-VAE，VQ-GAN：编码离散特征空间的 embedding；可以作为 Next Token Prediction 的训练目标
Connector 跨模态连接器
    MLP
    Q-Former
    X-Attn
LLM
    Flan-T5
    LLama
    Vicuna
Generator
    Token Decoder：VQ-GAN，Encodec
    生成模型：Stable Diffusion

VQ-VAE/VQ-GAN 流程：
将图像使用编码器编码为 $\hat z$
从 Codebook 中查询离 $\hat z$ 最近的离散 token，其 embedding 为 $z_q$
将 $z_q$ 使用解码器解码为图像
计算 loss，希望解码后的图像和原图像尽量相近 
(计算每个像素的差异，然后平均，这样做的问题在于可能计算的 loss 很低，但是视觉效果上看两张图还是差异明显)
VQ-GAN 的改进就是最后添加了一个判别模型，将目标从最小化每个像素点的差异改为最小化判别模型判别结果的差异

LLaVA-1.5：
使用 MLP 作为 Connector
vision encoder 是 CLIP ViT-L，图像经过 vision encoder 之后还要经过 MLP，将 700 多维的图像特征映射到 1000 多维的 token embedding
图像 embedding 和文本 embedding 一起喂给 Vicuna v1.5 13B

两阶段训练：
    第一阶段为多模态对齐，训练数据是图和文本描述，固定 LLM
    第二阶段为多模态指令微调，训练数据是图文 QA，全模型放开

Flamingo：
使用 Resampler 作为 Connector，Resampler 的输入是 Image Feature 和 Query Embeds，输出是 Query Embeds
Resampler 的思路是将压缩图像 token embedding 的数量，一般一个视频的每一帧都会抽取一个 token embedding，导致它们数量会过多，因此 Q-former 的思路是用一组数量较少的 learnable latent query 作为这些 token embedding 的压缩代表
这些 query 会和 token embedding 做 cross attention，经过 MLP，作为图像 embedding 的代表传递给之后的模型，而原始的 token embedding 则仅用于做 cross attention，不会向上传递

QWen-VL：
使用 Resampler 作为 Connector
三阶段训练：
    阶段一为多模态对齐，使用图文描述数据训练，固定 LLM (保持 LLM 行性能，因为一开始图像的 query embed 是完全没有意义的，不能让 LLM 根据没有意义的这些 embed 来更新自己的参数，而是让图像 encoder fit/对齐到 LLM 所想要的东西上，使得语言模型可以理解)
    阶段二为多模态指令微调1，使用图文 QA 、图像描述、视觉定位、OCR、纯文本等数据 (纯文本是为了让 LLM 保持文本性能，因为我们在训练时都是图 + 文的数据，这容易导致最后得到的 LLM 只能在图 + 文的情况下有良好表现，纯文本能力反而大幅下降，而我们需要模型即使在没有图的时候也可以正常聊天)，全模型放开，侧重刷 benchmark
    阶段三为多模态指令微调2，使用图文 QA 数据，固定 ViT (保持 ViT 性能)，侧重对话体验

BLIP2：
使用 Q-Former 作为 Connector
Q-Former 也就是层数变多了的 Resampler

Q-Former 的层数变多了，因此也更加难学，BLIP2采用的学习方法和 Coca 类似，分为两个阶段：
    阶段一为 vision-language 表征学习，目标是对齐视觉文本空间，有三个预训练任务，包括 Image-Text 对比学习 (拉近图像的 learned query 和文本的 embedding 之间的距离)、Image-to-Text 生成 (基于图像的 learned query 自回归生成图像的文本描述)、Image-Text 匹配 (基于图像的 learned query 和文本描述的 embedding 二分类，匹配为 yes，不匹配为 no)；阶段一可以训练好 ViT 和 QFormer
    阶段二为 vision to language 生成式学习，使用图像问答数据

InstructBLIP：
Q-Former 不仅仅接受 learned query，还接受文本指令的 embedding，这些 embedding 对模型注入了 Instruction 的先验，让 Q-Former 关注和指令相关的视觉内容
Q-Former 输出的 learned query‘s embedding 和 Intruction 的 embedding 会一起喂给后面的 LLM

局限性：如果有一张图，需要问多个问题，则需要每个问题的 Instruction 都和图像过一遍 Q-Former，使得计算量增大，且麻烦
因此最近的工作还是主要仅在后面的 LLM 中才把指令放进去，Q-Former 就仅关注图像即可

InternVL：
使用 Q-LLaMA 作为 Connector
将 InstructBLIP scale up，多个任务 SOTA

小结：Q-Former 的优势是压缩了视觉 token 的数量，劣势是增加了参数量

Flamingo：
使用 X-Attn 作为 Connector

X-Attn 的思路是让文本的 token 和视觉 token 做 cross attention，使得文本 token 融合了视觉信息，最后喂给 LLM 的仅有文本 token
cross attention 之后的文本 token embedding 在和 cross attention 之前的文本 token embedding 加和的时候会先乘上一个 $\alpha$ 系数，该系数随着训练过程逐渐增大，使得视觉信息可以以较平缓的方式融合入文本信息，让训练更稳定
这可以在不增加 LLM 需要处理的 token 数量的情况下融合图片信息 (而不是把视觉 token 拼接到文本 token 之前)，同时可以保持 LLM 原有的参数固定，仅需要在其中添加新的 cross attention 层，进行额外的信息注入

多张图和多张文本一起喂入的时候，文本只会和它对应的图进行 cross attention，避免视觉信息混淆
Resampler 也是由 Flamingo 提出

保持 LLM 原有参数不变，注入额外信息的另一种方案：LoRA
LoRA添加参数矩阵 $A (d\times r), B (r \times d)$，使得 $W^{new} = W^{old} + \Delta W$，其中 $\Delta W = AB$

$A$ 进行随机初始化，$B$ 进行零初始化，因此在训练一开始 LoRA 块不会影响输出，之后新训练的参数对模型的影响会逐渐增大，这和 Flamingo 的 $\alpha$ 系数的思想类似

LoRA 的参数量少，容易训练，同时可以在较好的保持原模型性能的同时也不容易过拟合 (如果放开全模型，且新的数据量不够，将很容易过拟合)

**支持任意分辨率的多模态大模型**
使用预训练的 vision encoder 的问题在模型 (例如 CLIP) 一般都预训练于固定分辨率的图像 (224x224, 336x336, 448x448)，因此仅支持固定分辨率
在之前的工作中，需要将大的图 resize 到固定分辨率，再传给 vision encoder，这会导致丢失很多图中细节 (例如一张很大的图都是字)

让多模态大模型理解任意分辨率图像的思路包括：
    不要 vision encoder，直接把图像切块转 token 喂给 LLM，和文本 token 一起训
    图像切块，分块过 vision encoder，再拼起来
    训练一个支持任意分辨率的 vision encoder

Fuyu:
不使用 vision encoder，直接将图像的 patch embeding 输入 LLM ，使用特殊 token '\n' 表示换行 (保留原图像的一部分布局信息)
其问题在于一开始的 image patch 和 text token 之间的距离过大，视觉特征和文本特征无法对齐，image patch embedding 根本不存在 LLM 能理解的语义，导致 LLM 容易学傻掉
因此需要很大的训练代价才可以训练好

InternLM-XComposer2-4KHD:
使用 336x336 分辨率的 CLIP 作为 vision encoder，分两路编码图像
    全局分路：将图像 resize，直接编码，相邻的4个 token 在 channel 维度合并 (concat)，以减少 token 数量，降低计算代价，合并时使用 '\n' token 表示换行
    局部分路：图像划分为 336x336的多个 patch，各自编码，相邻的4个 token 在 channel 维度合并 (concat)，以减少 token 数量，降低计算代价，合并时使用 '\n' token 表示换行
    最后全局分路和局部分路得到的 token embeding 在 channel 维度合并，中间添加 token 'sp' 表示区分，得到图像的最终表示 token embedding

多任务 SOTA，并且发现对于需要细节理解的评测 (特别是 OCR 相关)，增加分辨率会大幅提升性能

Qwen-VL2:
训练了一个支持任意分辨率的 vision encoder

## 7.2 视频、音频多模态模型
## 7.3 其他多模态模型
## 7.4 生成式模型和图像生成


# 8 Agent & Embodied AI
> 2024.11.5 王泰










