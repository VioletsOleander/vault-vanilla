# 1 What is Artificial Intelligence
> 2024.9.10 董超
## 1.1 The Definition of Artificial Intelligence
AI is the algorithm to solve complicated tasks
- complicated: ill-posed; probability (没有解析解、输出的是概率)

AI needs to realize complex objectives
- complex: complex 是一个维度上的，complicated 是多个维度上的
- objective: AI 不会自己生成目标，但会生成子目标，AI 的危险来自于自己生成的子目标

AI has the ability of learning from big data
- 人类的经验代代相传，是局部最优，机器可以探索全局最优

AI connects subjective perception and recognition
- 主观互通
## 1.2 What is Artificial General Intelligence
我们关注 AI 的能力，而不是过程；关注潜力，不关心部署
## 1.3 What is Artificial Super Intelligence
ASI can have objectives itself
# 2 Learning from Examples
> 2024.9.24 张超

用分类表示回归：
    我们需要的（回归）输出是32bit 浮点数，因此考虑到每一个 bit ，32bit 的浮点数就一共有 $2^{32}$ 个目标类别，或者将需要的（回归）输出看作 $4\times 8$ byte，进而一共有 $4\times 2^8$ 个目标类别
    为什么这么做：有时分类模型的效果会比回归好

线性分类器实现多类别分类：
    One vs the rest
    One vs one
    这两种方法都存在未定义的空间
    为此，可以考虑 Several vs the rest，取决于我们对问题的设计

Sigmoid 函数近似阶跃函数：
    阶跃函数不可导
    Sigmoid 函数 $y = 1/(1 + \exp (-a/\tau))$，在 $\tau$ 接近 $0$ 时无限趋近于阶跃函数，因此可以用 Sigmoid 函数 + 较小的 $\tau$ 来近似阶跃函数 ("soft" step function)

区分式 vs 生成式：
    生成式模型：建模输入或输出的分布，例如 $p (\vec x), p (\vec x \mid C_k)$
    区分式模型：直接建模后验概率 $p (C_k\mid \vec x)$，或者建模和决策相关的函数（例如边界）
    生成式模型可以生成新数据，但是需要训练数据多
    相同数据下，区分式模型效果比区分式好，但不了解潜在的特征依赖 
    GPT 实际上是用区分式模型进行生成（建模 $p(C_k\mid \vec x)$） 
    “生成式”也可以认为是表示输入和输出之间的弱（非唯一）联系，也就是其中存在随机性和不确定性，也就是“幻觉”
    实际上，“生成式”依赖于“幻觉”，但也受“幻觉“困扰

损失函数：
    熵：度量随机变量的不确定性
    交叉熵：没有最小值
    KL 散度：有最小值
    MSE：$\mathcal L = \sum_n (t_n-y_n)^2$
    MAE：$\mathcal L = \sum_{n} | t_n - y_n|$
# 3 Deep Learning and AI Computing
> 2024.9.29
## 3.1 Deep Learning
感知机：
    加权求和+非线性激活： $\hat y = g (\sum_{i=1}^m w_ix_i$)
    添加偏置项：$\hat y = g (w_0 + \sum_{i=1}^m w_i x_i)$

激活函数：
    例如：sigmoid, tanh, relu
    relu 简单快速，唯一的缺陷在于在零点处不可导
    目的：引入非线性

单输出感知机->多输出感知机
单层感知机->深层神经网络

损失：
    Empirical Loss = Empirical risk = Cost function = Objective function

训练：
    寻找最小化损失的参数

优化：
    梯度下降

学习率：
    按照梯度走的步长
    学习率调度：随着训练进行，学习率逐渐减小
    （线性调度：下降速度平缓；Cosine 调度：下降速度逐渐变快）

梯度的计算：
    反向传播
    minibatch：随机梯度下降
    视觉网络一般需要走90-100个 epoch，大模型1-2个 epoch 即可
    理论上，随机梯度下降和梯度下降的效率实际上是相当的（收敛性的上界一样），实践中，随机梯度下降的收敛比梯度下降快得多
    经验上，batchsize 扩大 n 倍，lr 也可以扩大 n 倍，收敛效率一致（直观上理解即样本越多，梯度估计约准确，故学习率可以更大）
## 3.2 AI Computing
AlexNet - 61M parameters - 262PFLOPS - GTX 580
GPT-3 - 175B parameters - 323ZFLOPS

AI Trends:
    2012年以前-机器学习：
    参数规模： $O (10^3)$，数据集规模： $O (10^3)$，硬件：CPU，代表模型：SVM
    2012年-2022年-深度学习：
    参数规模： $O (10^6)$，数据集规模： $O (10^5\sim 10^6)$，硬件：GPUs $O(\sim 10^3$)，代表模型：ResNet
    2022年以后-基石模型：
    参数规模： $O (10^{9\sim})$，数据集规模： $O (10^{11}\sim 10^{12})$，硬件：GPUs $O(10^4\sim 10^5)$，代表模型：GPT

从2002-2021年，模型、数据集规模随着年份指数上升
而训练的规模 = 模型规模×数据集规模

AI Platform：
    部署平台
    AI 框架
    后端：编译器、优化器、库
    硬件架构

Nvidia Grace Hopper:
    Grace: CPU
    Hopper: GPU
    内置了 Transformer Engine
    第四代 NVLink
    第三代 NVSwitch
    Nvidia Confidential Computing （隐私计算）
    第二代 MIG（虚拟化）
    DPX Instructions


GPU Communication:
    RDMA：GPU直接访问远端，不需要经过 CPU，但仍然会通过一次 CPU 总线
    GPUDirect RDMA：不通过 CPU 总线，通过 NVLink 实现

Nvidia DGX-2：
    高度集成的计算节点
    包括了16个 V100-SXM3 GPUs

Pytorch:
    Autograd
    Dispatcher: 根据 python 代码（要执行的运算）以及底层硬件信息（例如 CUDA、CPU 等），调用合适的 kernel

`torch.compile`:
    封装了：
    TorchDynamo-读取 Python Byte Code 得到图结构
    TorchInductor-基于 Dynamo 获取的 define-by-run IR 进行编译优化和 codegen，得到子图
    AOTAutograd
    PrimTorch-Stable Primitive operators

TorchDynamo：
    a Python level JIT compiler
    rewrites Python bytecode to extract sequences of PyTorch operations into an FX Graph
    FX Graph is then compiled by a customizable backend（自定义的后端编译器就可以进行算子融合）

算子融合：
    例如 MoE 的 gating 算子涉及到多个小操作，融合调用会比一个一个顺序调用减少不少开销

TorchInductor：
    it works as a JIT compiler for Pytorch models that transforms them into more optimized code
    例如将算子展开，然后交给 Triton 生成高性能代码

Triton：
    OpenAI 开发
    是基于 Python 进行 CUDA 编程的编程语言

Python code -> \[Dynamo and Inductor\]-> Triton code -> Triton IR -> MLIR

Pytorch ops (2000+) -> \[归类\]-> ATen ops (750+) -> \[归类\] -> Prim ops (250+)

JAX：
    仅 JAX 支持 TPU
    JAX 的后端是 XLA（和 Tensorflow 一样）
    JAX+XLA 类似于 Torch+Triton

XLA 自己就可以做图融合，Torch 则需要转为 FX Graph，然后交给外部编译器做

并行训练：
    数据并行：复制模型为多份，每一份模型计算不同数据，最后聚合梯度
    模型并行：模型切开，分块算矩阵乘，子矩阵求和时需要通信，因此计算中需要不断通信

数据并行的梯度通信：
    Parameter Server：数据给 Serever，Server 负责聚合计算，然后分发
    该方法适用于稀疏网络，例如推荐引擎（一个样本仅激活部分参数，故可以不用传全部数据给 Server，但 Transformer 这类模型不是稀疏的，一个样本会激活所有参数）
    All Reduce：GPU 直接互联，使用 Ring 算法一起计算

模型并行：
    Tensor Model Parallel：竖着切
    Pipeline Parallel：横着切，为了减少 worker 之间的等待，采用流水线

# 4 Speech and Audio Processing
> 2024.10.8 张超

## 4.1 Speech Processing Tasks
### 4.1.1 Speech/Audio perception and understanding
**Automatic Speech Recognition (ASR)** 
    语音转文本 (speech-to-text/STT)
    其子任务：keyword spotting (KWS), wake-up work recognition, phoneme recogintion, syllable recognition, word recognition

ASR 按词袋大小划分为：
    数字识别
    命令识别
    大词袋连续语音识别 (LVCSR)
    端到端识别 (无词袋)
    上下文语音识别

ASR 基本公式：

$$
\begin{align}
W^* &= \arg\max_W P(W\mid O)\\
&= \arg\max_W \frac {P(O\mid W)P(W)}{P(O)}\\
&= \arg\max_W P(O\mid W)P(W)
\end{align}
$$

其中 $W^*$ 是输出假设，也就是输出的单词序列，通过 MAP 搜索（最大化后验概率）得到，该过程也称为解码 （decoding）
$O$ 是输入语音序列
$\arg\max_W P (O\mid W) P (W)$ 中，$P (O\mid W)$ 是声音模型（acoustic model），$P (W)$ 是语言模型（language model）

ASR 的代表模型是 OpenAI Whisper，其基本架构就是 Transformer
    Whiper-Large-v3：
    训练数据是 1M 小时的语音（100年=0.875M 小时）
    支持97个语言的 ASR
    支持发音级别的端点检测（说话片段的开始和结束）和自动语音翻译
    最大的版本参数量为1.55B

**Automatic Speech Translation (AST)**
    speech-to-text-to-speech or speech-to-speech

AST 的代表模型是 SeamlessM4T 和 Seamless Streaming，后者是前者的微调版本，用于支持流式的输入语音
Seamless M4T 支持 speech-to-speech 和 speech-to-text 的翻译，支持101中语言的语音输入，支持36种语言的文本输出、96种语言的语音输出

**Speech Enhancement**
    语音强化包括去噪、回声消除等

**Speech Seperation**
    将包含多个说话者的混合语音分离为多个流

**Speaker Recognition**

**Speaker Diarisatoin**
    说话人日志，在长的语音流中确认谁在什么时候说话，及其说话内容

**Audio/Sound Event Detection**
    检测和分类音频事件，例如 A man/woman is singling/dancing/crying

**Automatic Emotion Recognition (AER)**
    识别说话人的情感

语音-语言大模型：SALMONN (Speech Audio Langague Music Open NN)

语音-视频-语言大模型：Vide-SALMONN 2

### 4.1.3 Speech/Audio/Videio generation
Text-to-Speech synthesis (TTS)
    基于文本生成语音
    zero-shot TTS：通过一个人的3秒的语音样本生成其语音

Text-to-Sing (TTSing)
    代表模型：Diffsinger

Speech Editing
    向原语音中添加指定的话，使其听不出来差别

Voice Conversion/Cloning
    AI 孙燕姿

Text-to-Audio
    代表模型：AudioLDM2 - Generating audio events (soud effects), music and speech based on input text and audio prompts

Music Generation
    Suno

Music Editing
    代表模型：StableAudioControl

Audio Driven Portrait Animation Generation
    代表模型：EchoMimic

(End-to-end) Speech-Conversational AI
    代表模型：GPT-4o

# 5 自然语言处理
> 2024.10.15 张超

## 5.1 自然语言处理概览
自然语言特点：固有歧义性、无限性（递归性）、长距离依赖、以离散符号作为基本表示单元（与连续的数字表示不同）

最早：基于规则的方法
过渡：基于统计的方法（源语言和目标语言语法结构一一对应）
目前：基于深度学习的方法（Input --> Encoder --> Representation --> Decoder --> Output）

## 5.2 基于深度学习的自然语言处理
NLP 的基本问题：
- 分类：输入为语言，输出为标签
    例如文本分类、情感分析
- 匹配：为两个输入的匹配度输出分数
    例如搜索、问答、单轮对话
- 翻译：输入为源语言序列，输出为目标语言序列
    例如机器翻译、语音翻译、手写识别、单轮对话
- 结构预测：输入为语言序列，输出为该序列的语法分析树
    例如命名实体识别（e.g. “清华大学”, “建华楼”）、词性标注、语法/语义分析
    很长一段时间内，人们认为用树结构清晰表示语言要优于深度网络中的向量表示
- Markov 决策过程：序列的状态转移
    例如多轮对话，现在多轮对话已经是 GPT 统治了

词表示：如何用计算的方法定义词的含义
- One-hot 表示（很适合做语法分析树，故很长一段时间被认可）
- 实值向量表示

一个词的含义应该由伴随它出现的词定义，即上下文，显然 One-hot 做不到这一点

语言模型（LM）：判断任意给定字符串是一个合法句子的概率
    N-gram LM 即建模时每个词仅和它前 $n-1$ 个词有关系
    例如在 4-gram LM 中，给定前三个词作为输入，第四个词的概率建模为 $p (w_t \mid w_{t-1}, w_{t-2}, w_{t-3})$
    N-gram LM 的训练任务就是在大量的 N-gram 语料将 $P (w_t \mid context)$ 与 $w_t$ 在语料库中的 N-gram 词频匹配
    训练结束后，提取最后一个隐藏层输出作为词表示（Word Embedding）
    该方法提取出的词向量中，上下文相近的词其词向量会更接近

word2vec：
    CBOW：用上下文预测当前词
    Skip-Gram： 用当前词预测上下文
    优点：快
    缺点：没有词序信息

文本卷积：
    速度中等，可以部分利用词序信息

RNN:
    $h_t = f (W_h h_{t-1} + W_x x + b)$
    特点：理论上无限长、强时序（利用全部词序信息）
    缺点：慢

LSTM:
    

# 6 计算机视觉
> 2024.10.22 代季峰









