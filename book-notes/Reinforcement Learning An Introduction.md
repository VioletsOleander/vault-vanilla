# 3 Finite Markov Decision Processes 
In this chapter we introduce the problem that we try to solve in the rest of the book. For us, this problem defines the field of reinforcement learning: any method that is suited to solving this problem we consider to be a reinforcement learning method. 
Our objective in this chapter is to describe the reinforcement learning problem in a broad sense. We try to convey the wide range of possible applications that can be framed as reinforcement learning tasks. We also describe mathematically idealized forms of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem’s mathematical structure, such as value functions and Bellman equations. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability. In this chapter we introduce this tension and discuss some of the trade-offs and challenges that it implies. 

## 3.1 The Agent–Environment Interface 
The reinforcement learning problem is meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to those actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent tries to maximize over time. A complete specification of an environment defines a task, one instance of the reinforcement learning problem. 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/676517caabf785249e0b9ec8a44c3b55f5606549c89604f62e7d5edddc5a6c3a.jpg) 
Figure 3.1: The agent–environment interaction in reinforcement learning. 
More specifically, the agent and environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,\ldots$ .2 At each time step $t$ , the agent receives some representation of the environment’s state, $S_{t}\in\mathcal{S}$ , where S is the set of possible states, and on that basis selects an action, $A_{t}\in\mathcal{A}(S_{t})$ , where $\mathcal{A}(S_{t})$ is the set of actions available in state $S_{t}$ . One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1}\in$ $\mathcal{R}\subset\mathbb{R}$ , and finds itself in a new state, $S_{t+1}$ .3 Figure 3.1 diagrams the agent– environment interaction. 
At each time step, the agent implements a mapping from states to probabilities of selecting each possible action. This mapping is called the agent’s policy and is denoted $\pi_{t}$ , where $\pi_{t}(a|s)$ is the probability that $A_{t}=a$ if $S_{t}=s$ . Reinforcement learning methods specify how the agent changes its policy as a result of its experience. The agent’s goal, roughly speaking, is to maximize the total amount of reward it receives over the long run. 
This framework is abstract and flexible and can be applied to many different problems in many different ways. For example, the time steps need not refer to fixed intervals of real time; they can refer to arbitrary successive stages of decision-making and acting. The actions can be low-level controls, such as the voltages applied to the motors of a robot arm, or high-level decisions, such as whether or not to have lunch or to go to graduate school. Similarly, the states can take a wide variety of forms. They can be completely determined by low-level sensations, such as direct sensor readings, or they can be more high-level and abstract, such as symbolic descriptions of objects in a room. Some of what makes up a state could be based on memory of past sensations or even be entirely mental or subjective. For example, an agent could be in the state of not being sure where an object is, or of having just been surprised in some clearly defined sense. Similarly, some actions might be totally mental or computational. For example, some actions might control what an agent chooses to think about, or where it focuses its attention. In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them. 
In particular, the boundary between agent and environment is not often the same as the physical boundary of a robot’s or animal’s body. Usually, the boundary is drawn closer to the agent than that. For example, the motors and mechanical linkages of a robot and its sensing hardware should usually be considered parts of the environment rather than parts of the agent. Similarly, if we apply the framework to a person or animal, the muscles, skeleton, and sensory organs should be considered part of the environment. Rewards, too, presumably are computed inside the physical bodies of natural and artificial learning systems, but are considered external to the agent. 
The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment. We do not assume that everything in the environment is unknown to the agent. For example, the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. But we always consider the reward computation to be external to the agent because it defines the task facing the agent and thus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may know everything about how its environment works and still face a difficult reinforcement learning task, just as we may know exactly how a puzzle like Rubik’s cube works, but still be unable to solve it. The agent– environment boundary represents the limit of the agent’s absolute control, not of its knowledge. 
The agent–environment boundary can be located at different places for different purposes. In a complicated robot, many different agents may be operating at once, each with its own boundary. For example, one agent may make high-level decisions which form part of the states faced by a lower-level agent that implements the high-level decisions. In practice, the agent–environment boundary is determined once one has selected particular states, actions, and rewards, and thus has identified a specific decision-making task of interest. 
The reinforcement learning framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent’s goal (the rewards). This framework may not be sufficient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable. 
Of course, the particular states and actions vary greatly from task to task, and how they are represented can strongly affect performance. In reinforcement learning, as in other kinds of learning, such representational choices are at present more art than science. In this book we offer some advice and examples regarding good ways of representing states and actions, but our primary focus is on general principles for learning how to behave once the representations have been selected. 
Example 3.1: Bioreactor Suppose reinforcement learning is being applied to determine moment-by-moment temperatures and stirring rates for a bioreactor (a large vat of nutrients and bacteria used to produce useful chemicals). The actions in such an application might be target temperatures and target stirring rates that are passed to lower-level control systems that, in turn, directly activate heating elements and motors to attain the targets. The states are likely to be thermocouple and other sensory readings, perhaps filtered and delayed, plus symbolic inputs representing the ingredients in the vat and the target chemical. The rewards might be moment-by-moment measures of the rate at which the useful chemical is produced by the bioreactor. Notice that here each state is a list, or vector, of sensor readings and symbolic inputs, and each action is a vector consisting of a target temperature and a stirring rate. It is typical of reinforcement learning tasks to have states and actions with such structured representations. Rewards, on the other hand, are always single numbers. 
Example 3.2: Pick-and-Place Robot Consider using reinforcement learning to control the motion of a robot arm in a repetitive pick-and-place task. If we want to learn movements that are fast and smooth, the learning agent will have to control the motors directly and have low-latency information about the current positions and velocities of the mechanical linkages. The actions in this case might be the voltages applied to each motor at each joint, and the states might be the latest readings of joint angles and velocities. The reward might be $+1$ for each object successfully picked up and placed. To encourage smooth movements, on each time step a small, negative reward can be given as a function of the moment-to-moment “jerkiness” of the motion. 
Example 3.3: Recycling Robot A mobile robot has the job of collecting empty soda cans in an office environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery. The robot’s control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the current charge level of the battery. This agent has to decide whether the robot should (1) actively search for a can for a certain period of time, (2) remain stationary and wait for someone to bring it a can, or (3) head back to its home base to recharge its battery. This decision has to be made either periodically or whenever certain events occur, such as finding an empty can. The agent therefore has three actions, and its state is determined by the state of the battery. The rewards might be zero most of the time, but then become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. In this example, the reinforcement learning agent is not the entire robot. The states it monitors describe conditions within the robot itself, not conditions of the robot’s external environment. The agent’s environment therefore includes the rest of the robot, which might contain other complex decision-making systems, as well as the robot’s external environment.  

## 3.2 Goals and Rewards 
In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special reward signal passing from the environment to the agent. At each time step, the reward is a simple number, $R_{t}\in\mathbb R$ . Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis: 
That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). 
The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning. 
Although formulating goals in terms of reward signals might at first appear limiting, in practice it has proved to be flexible and widely applicable. The best way to see this is to consider examples of how it has been, or could be, used. For example, to make a robot learn to walk, researchers have provided reward on each time step proportional to the robot’s forward motion. In making a robot learn how to escape from a maze, the reward is often $-1$ for every time step that passes prior to escape; this encourages the agent to escape as quickly as possible. To make a robot learn to find and collect empty soda cans for recycling, one might give it a reward of zero most of the time, and then a reward of $+1$ for each can collected. One might also want to give the robot negative rewards when it bumps into things or when somebody yells at it. For an agent to learn to play checkers or chess, the natural rewards are $+1$ for winning, $-1$ for losing, and 0 for drawing and for all nonterminal positions. 
You can see what is happening in all of these examples. The agent always learns to maximize its reward. If we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up truly indicate what we want accomplished. In particular, the reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.4 For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such taking its opponent’s pieces or gaining control of the center of the board. If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal. For example, it might find a way to take the opponent’s pieces even at the cost of losing the game. The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved. 
Newcomers to reinforcement learning are sometimes surprised that the rewards—which define of the goal of learning—are computed in the environment rather than in the agent. Certainly most ultimate goals for animals are recognized by computations occurring inside their bodies, for example, by sensors for recognizing food, hunger, pain, and pleasure. Nevertheless, as we discussed in the previous section, one can redraw the agent–environment interface in such a way that these parts of the body are considered to be outside of the agent (and thus part of the agent’s environment). For example, if the goal concerns a robot’s internal energy reservoirs, then these are considered to be part of the environment; if the goal concerns the positions of the robot’s limbs, then these too are considered to be part of the environment—that is, the agent’s boundary is drawn at the interface between the limbs and their control systems. These things are considered internal to the robot but external to the learning agent. For our purposes, it is convenient to place the boundary of the learning agent not at the limit of its physical body, but at the limit of 
its control. 
The reason we do this is that the agent’s ultimate goal should be something over which it has imperfect control: it should not be able, for example, to simply decree that the reward has been received in the same way that it might arbitrarily change its actions. Therefore, we place the reward source outside of the agent. This does not preclude the agent from defining for itself a kind of internal reward, or a sequence of internal rewards. Indeed, this is exactly what many reinforcement learning methods do. 

## 3.3 Returns 
So far we have discussed the objective of learning informally. We have said that the agent’s goal is to maximize the cumulative reward it receives in the long run. How might this be defined formally? If the sequence of rewards received after time step $t$ is denoted $R_{t+1},R_{t+2},R_{t+3},...$ , then what precise aspect of this sequence do we wish to maximize? In general, we seek to maximize the expected return, where the return $G_{t}$ is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards: 

$$
G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\cdot\cdot\cdot+R_{T},
$$ 
where $T$ is a final time step. This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent– environment interaction breaks naturally into subsequences, which we call episodes,5 such as plays of a game, trips through a maze, or any sort of repeated interactions. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Tasks with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted S, from the set of all states plus the terminal state, denoted $\mathcal{S}^{+}$ . 
On the other hand, in many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate a continual process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation (3.1) is problematic for continuing tasks because the final time step would be $T=\infty$ , and the return, which is what we are trying to maximize, could itself easily be infinite. (For example, suppose the agent receives a reward of $+1$ at each time step.) Thus, in this book we usually use a definition of return that is slightly more complex conceptually but much simpler mathematically. 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/6d463a9c341f2fa798a155f1216e02f0eff9446cb432003076828d01f351f57b.jpg) 
Figure 3.2: The pole-balancing task. 
The additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_{t}$ to maximize the expected discounted return: 
$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\cdot\cdot\cdot=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1},
$$ 
where $\gamma$ is a parameter, $0\leq\gamma\leq1$ , called the discount rate. 
The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. If $\gamma<1$ , the infinite sum has a finite value as long as the reward sequence $\{R_{k}\}$ is bounded. If $\gamma=0$ , the agent is “myopic” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose $A_{t}$ so as to maximize only $R_{t+1}$ . If each of the agent’s actions happened to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize (3.2) by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return may actually be reduced. As $\gamma$ approaches 1, the objective takes future rewards into account more strongly: the agent becomes more farsighted. 
Example 3.4: Pole-Balancing Figure 3.2 shows a task that served as an early illustration of reinforcement learning. The objective here is to apply forces to a cart moving along a track so as to keep a pole hinged to the cart from falling over. A failure is said to occur if the pole falls past a given angle from vertical or if the cart runs off the track. The pole is reset to vertical after each failure. This task could be treated as episodic, where the natural episodes are the repeated attempts to balance the pole. The reward in this case could be $+1$ for every time step on which failure did not occur, so that the return at each time would be the number of steps until failure. Alternatively, we could treat pole-balancing as a continuing task, using discounting. In this case the reward would be $-1$ on each failure and zero at all other times. The return at each time would then be related to $-\gamma^{K}$ , where $K$ is the number of time steps before failure. In either case, the return is maximized by keeping the pole balanced for as long as possible. 

## 3.4 Unified Notation for Episodic and Continuing Tasks 
In the preceding section we described two kinds of reinforcement learning tasks, one in which the agent–environment interaction naturally breaks down into a sequence of separate episodes (episodic tasks), and one in which it does not (continuing tasks). The former case is mathematically easier because each action affects only the finite number of rewards subsequently received during the episode. In this book we consider sometimes one kind of problem and sometimes the other, but often both. It is therefore useful to establish one notation that enables us to talk precisely about both cases simultaneously. 
To be precise about episodic tasks requires some additional notation. Rather than one long sequence of time steps, we need to consider a series of episodes, each of which consists of a finite sequence of time steps. We number the time steps of each episode starting anew from zero. Therefore, we have to refer not just to $S_{t}$ , the state representation at time $t$ , but to $S_{t,i}$ , the state representation at time $t$ of episode $i$ (and similarly for $A_{t,i}$ , $R_{t,i}$ , $\boldsymbol{\pi}_{t,i}$ , $T_{i}$ , etc.). However, it turns out that, when we discuss episodic tasks we will almost never have to distinguish between different episodes. We will almost always be considering a particular single episode, or stating something that is true for all episodes. Accordingly, in practice we will almost always abuse notation slightly by dropping the explicit reference to episode number. That is, we will write $S_{t}$ to refer to $S_{t,i}$ , and so on. 
We need one other convention to obtain a single notation that covers both episodic and continuing tasks. We have defined the return as a sum over a finite number of terms in one case (3.1) and as a sum over an infinite number of terms in the other (3.2). These can be unified by considering episode termination to be the entering of a special absorbing state that transitions only to itself and that generates only rewards of zero. For example, consider the state transition 
diagram 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/6894782f0790589b5ad37302228b879381bb6c96779b17f923f4a53da790c8ef.jpg) 
Here the solid square represents the special absorbing state corresponding to the end of an episode. Starting from $S_{0}$ , we get the reward sequence $+1,+1,+1,0,0,0,\ldots$ . Summing these, we get the same return whether we sum over the first $T$ rewards (here $T=3$ ) or over the full infinite sequence. This remains true even if we introduce discounting. Thus, we can define the return, in general, according to (3.2), using the convention of omitting episode numbers when they are not needed, and including the possibility that $\gamma=1$ if the sum remains defined (e.g., because all episodes terminate). Alternatively, we can also write the return as 
$$
G_{t}=\sum_{k=0}^{T-t-1}\gamma^{k}R_{t+k+1},
$$ 
including the possibility that $T=\infty$ or $\gamma=1$ (but not both $^6$ ). We use these conventions throughout the rest of the book to simplify notation and to express the close parallels between episodic and continuing tasks. 
## 3.5 The Markov Property\*
In the reinforcement learning framework, the agent makes its decisions as a function of a signal from the environment called the environment’s state. In this section we discuss what is required of the state signal, and what kind of information we should and should not expect it to provide. In particular, we formally define a property of environments and their state signals that is of particular interest, called the Markov property. 
In this book, by “the state” we mean whatever information is available to the agent. We assume that the state is given by some preprocessing system that is nominally part of the environment. We do not address the issues of constructing, changing, or learning the state signal in this book. We take this approach not because we consider state representation to be unimportant, but in order to focus fully on the decision-making issues. In other words, our main concern is not with designing the state signal, but with deciding what action to take as a function of whatever state signal is available. 
Certainly the state signal should include immediate sensations such as sensory measurements, but it can contain much more than that. State representations can be highly processed versions of original sensations, or they can be complex structures built up over time from the sequence of sensations. For example, we can move our eyes over a scene, with only a tiny spot corresponding to the fovea visible in detail at any one time, yet build up a rich and detailed representation of a scene. Or, more obviously, we can look at an object, then look away, and know that it is still there. We can hear the word “yes” and consider ourselves to be in totally different states depending on the question that came before and which is no longer audible. At a more mundane level, a control system can measure position at two different times to produce a state representation including information about velocity. In all of these cases the state is constructed and maintained on the basis of immediate sensations together with the previous state or some other memory of past sensations. In this book, we do not explore how that is done, but certainly it can be and has been done. There is no reason to restrict the state representation to immediate sensations; in typical applications we should expect the state representation to be able to inform the agent of more than that. 
On the other hand, the state signal should not be expected to inform the agent of everything about the environment, or even everything that would be useful to it in making decisions. If the agent is playing blackjack, we should not expect it to know what the next card in the deck is. If the agent is answering the phone, we should not expect it to know in advance who the caller is. If the agent is a paramedic called to a road accident, we should not expect it to know immediately the internal injuries of an unconscious victim. In all of these cases there is hidden state information in the environment, and that information would be useful if the agent knew it, but the agent cannot know it because it has never received any relevant sensations. In short, we don’t fault an agent for not knowing something that matters, but only for having known something and then forgotten it! 
What we would like, ideally, is a state signal that summarizes past sensations compactly, yet in such a way that all relevant information is retained. This normally requires more than the immediate sensations, but never more than the complete history of all past sensations. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property (we define this formally below). For example, a checkers position—the current configuration of all the pieces on the board—would serve as a Markov state because it summarizes everything important about the complete sequence of positions that led to it. Much of the information about the sequence is lost, but all that really matters for the future of the game is retained. Similarly, the current position and velocity of a cannonball is all that matters for its future flight. It doesn’t matter how that position and velocity came about. This is sometimes also referred to as an “independence of path” property because all that matters is in the current state signal; its meaning is independent of the “path,” or history, of signals that have led up to it. 
We now formally define the Markov property for the reinforcement learning problem. To keep the mathematics simple, we assume here that there are a finite number of states and reward values. This enables us to work in terms of sums and probabilities rather than integrals and probability densities, but the argument can easily be extended to include continuous states and rewards. Consider how a general environment might respond at time $t+1$ to the action taken at time $t$ . In the most general, causal case this response may depend on everything that has happened earlier. In this case the dynamics can be defined only by specifying the complete probability distribution: 
$$
\operatorname*{Pr}\{R_{t+1}=r,S_{t+1}=s^{\prime}\mid S_{0},A_{0},R_{1},\ldots,S_{t-1},A_{t-1},R_{t},S_{t},A_{t}\},
$$ 
for all $r,s^{\prime}$ , and all possible values of the past events: $S_{0}$ , $A_{0}$ , $R_{1}$ , ..., $S_{t-1}$ , $A_{t-1}$ , $R_{t}$ , $S_{t}$ , $A_{t}$ . If the state signal has the Markov property, on the other hand, then the environment’s response at $t+1$ depends only on the state and action representations at $t$ , in which case the environment’s dynamics can be defined by specifying only 
$$
p(s^{\prime},r|s,a)=\operatorname*{Pr}\{R_{t+1}=r,S_{t+1}=s^{\prime}\mid S_{t},A_{t}\},
$$ 
for all $r$ , $s^{\prime}$ , $S_{t}$ , and $A_{t}$ . In other words, a state signal has the Markov property, and is a Markov state, if and only if (3.5) is equal to (3.4) for all $s^{\prime}$ , $r$ , and histories, $S_{0}$ , $A_{0}$ , $R_{1}$ , ..., $S_{t-1}$ , $A_{t-1}$ , $R_{t}$ , $S_{t}$ , $A_{t}$ . In this case, the environment and task as a whole are also said to have the Markov property. 
If an environment has the Markov property, then its one-step dynamics (3.5) enable us to predict the next state and expected next reward given the current state and action. One can show that, by iterating this equation, one can predict all future states and expected rewards from knowledge only of the current state as well as would be possible given the complete history up to the current time. It also follows that Markov states provide the best possible basis for choosing actions. That is, the best policy for choosing actions as a function of a Markov state is just as good as the best policy for choosing actions as a function of complete histories. 
Even when the state signal is non-Markov, it is still appropriate to think of the state in reinforcement learning as an approximation to a Markov state. 
In particular, we always want the state to be a good basis for predicting future rewards and for selecting actions. In cases in which a model of the environment is learned (see Chapter 8), we also want the state to be a good basis for predicting subsequent states. Markov states provide an unsurpassed basis for doing all of these things. To the extent that the state approaches the ability of Markov states in these ways, one will obtain better performance from reinforcement learning systems. For all of these reasons, it is useful to think of the state at each time step as an approximation to a Markov state, although one should remember that it may not fully satisfy the Markov property. 
The Markov property is important in reinforcement learning because decisions and values are assumed to be a function only of the current state. In order for these to be effective and informative, the state representation must be informative. All of the theory presented in this book assumes Markov state signals. This means that not all the theory strictly applies to cases in which the Markov property does not strictly apply. However, the theory developed for the Markov case still helps us to understand the behavior of the algorithms, and the algorithms can be successfully applied to many tasks with states that are not strictly Markov. A full understanding of the theory of the Markov case is an essential foundation for extending it to the more complex and realistic non-Markov case. Finally, we note that the assumption of Markov state representations is not unique to reinforcement learning but is also present in most if not all other approaches to artificial intelligence. 
Example 3.5: Pole-Balancing State In the pole-balancing task introduced earlier, a state signal would be Markov if it specified exactly, or made it possible to reconstruct exactly, the position and velocity of the cart along the track, the angle between the cart and the pole, and the rate at which this angle is changing (the angular velocity). In an idealized cart–pole system, this information would be sufficient to exactly predict the future behavior of the cart and pole, given the actions taken by the controller. In practice, however, it is never possible to know this information exactly because any real sensor would introduce some distortion and delay in its measurements. Furthermore, in any real cart–pole system there are always other effects, such as the bending of the pole, the temperatures of the wheel and pole bearings, and various forms of backlash, that slightly affect the behavior of the system. These factors would cause violations of the Markov property if the state signal were only the positions and velocities of the cart and the pole. 
However, often the positions and velocities serve quite well as states. Some early studies of learning to solve the pole-balancing task used a coarse state signal that divided cart positions into three regions: right, left, and middle (and similar rough quantizations of the other three intrinsic state variables). This distinctly non-Markov state was sufficient to allow the task to be solved easily by reinforcement learning methods. In fact, this coarse representation may have facilitated rapid learning by forcing the learning agent to ignore fine distinctions that would not have been useful in solving the task. ■ 
Example 3.6: Draw Poker In draw poker, each player is dealt a hand of five cards. There is a round of betting, in which each player exchanges some of his cards for new ones, and then there is a final round of betting. At each round, each player must match or exceed the highest bets of the other players, or else drop out (fold). After the second round of betting, the player with the best hand who has not folded is the winner and collects all the bets. 
The state signal in draw poker is different for each player. Each player knows the cards in his own hand, but can only guess at those in the other players’ hands. A common mistake is to think that a Markov state signal should include the contents of all the players’ hands and the cards remaining in the deck. In a fair game, however, we assume that the players are in principle unable to determine these things from their past observations. If a player did know them, then she could predict some future events (such as the cards one could exchange for) better than by remembering all past observations. 
In addition to knowledge of one’s own cards, the state in draw poker should include the bets and the numbers of cards drawn by the other players. For example, if one of the other players drew three new cards, you may suspect he retained a pair and adjust your guess of the strength of his hand accordingly. The players’ bets also influence your assessment of their hands. In fact, much of your past history with these particular players is part of the Markov state. Does Ellen like to bluff, or does she play conservatively? Does her face or demeanor provide clues to the strength of her hand? How does Joe’s play change when it is late at night, or when he has already won a lot of money? 

Although everything ever observed about the other players may have an effect on the probabilities that they are holding various kinds of hands, in practice this is far too much to remember and analyze, and most of it will have no clear effect on one’s predictions and decisions. Very good poker players are adept at remembering just the key clues, and at sizing up new players quickly, but no one remembers everything that is relevant. As a result, the state representations people use to make their poker decisions are undoubtedly nonMarkov, and the decisions themselves are presumably imperfect. Nevertheless, people still make very good decisions in such tasks. We conclude that the inability to have access to a perfect Markov state representation is probably not a severe problem for a reinforcement learning agent. ■ 

## 3.6 Markov Decision Processes 
A reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP). Finite MDPs are particularly important to the theory of reinforcement learning. We treat them extensively throughout this book; they are all you need to understand 90% of modern reinforcement learning. 
>  满足 Markov 性质的 RL 任务称为 MDP，如果状态和动作空间有限，就称为有限 MDP

A particular finite MDP is defined by its state and action sets and by the one-step dynamics of the environment. Given any state and action $s$ and $a$ , the probability of each possible pair of next state and reward, $s^{\prime},r$ , is denoted 

$$
p(s^{\prime},r|s,a)=\operatorname*{Pr}\{S_{t+1}=s^{\prime},R_{t+1}=r\mid S_{t}=s,A_{t}=a\}.\tag{3.6}
$$ 
These quantities completely specify the dynamics of a finite MDP. Most of the theory we present in the rest of this book implicitly assumes the environment is a finite MDP. 

>  一个有限 Markov 决策过程的定义包括其状态集、动作集和环境的一步动态
>  环境的一步动态指给定状态和动作 $s, a$，下一个状态和回报 $s', r$ 二者的联合条件分布，如 3.6 所示

Given the dynamics as specified by (3.6), one can compute anything else one might want to know about the environment, such as the expected rewards for state–action pairs, 
>  给定 3.6 的环境动态，可以计算任意关于环境的信息

$$
r(s,a)=\mathbb{E}[R_{t+1}\mid S_{t}=s,A_{t}=a]=\sum_{r\in\mathcal{R}}r\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime},r|s,a),\tag{3.7}
$$

>  例如给定状态-动作对，期望的回报如上 (先计算回报的边际分布，然后计算回报的期望)

the state-transition probabilities, 

$$
p(s^{\prime}|s,a)=\operatorname*{Pr}\{S_{t+1}=s^{\prime}\mid S_{t}=s,A_{t}=a\}=\sum_{r\in\mathcal{R}}p(s^{\prime},r|s,a),\tag{3.8}
$$

>  状态转移概率如上 (下一个状态 $S'$ 的边际分布)

and the expected rewards for state–action–next-state triples, 

$$
r(s,a,s^{\prime})=\mathbb{E}[R_{t+1}\mid S_{t}=s,A_{t}=a,S_{t+1}=s^{\prime}]=\frac{\sum_{r\in\mathcal{R}}r p(s^{\prime},r|s,a)}{p(s^{\prime}|s,a)}.\tag{3.9}
$$ 
>  状态-动作-下一个状态三元组的期望回报如上 (先计算回报的边际条件分布，然后计算期望)

In the first edition of this book, the dynamics were expressed exclusively in terms of the latter two quantities, which were denote $\mathcal{P}_{s s^{\prime}}^{a}$ and $\mathcal{R}_{s s^{\prime}}^{a}$ respectively. One weakness of that notation is that it still did not fully characterize the dynamics of the rewards, giving only their expectations. Another weakness is the excess of subscripts and superscripts. In this edition we will predominantly use the explicit notation of (3.6), while sometimes referring directly to the transition probabilities (3.8). 

Example 3.7: Recycling Robot MDP The recycling robot (Example 3.3) can be turned into a simple example of an MDP by simplifying it and providing some more details. (Our aim is to produce a simple example, not a particularly realistic one.) Recall that the agent makes a decision at times determined by external events (or by other parts of the robot’s control system). At each such time the robot decides whether it should (1) actively search for a can, (2) remain stationary and wait for someone to bring it a can, or (3) go back to home base to recharge its battery. Suppose the environment works as follows. The best way to find cans is to actively search for them, but this runs down the robot’s battery, whereas waiting does not. Whenever the robot is searching, the possibility exists that its battery will become depleted. In this case the robot must shut down and wait to be rescued (producing a low reward). 

The agent makes its decisions solely as a function of the energy level of the battery. It can distinguish two levels, high and low, so that the state set is $\mathcal{S}=\{\mathtt{h i g h},\mathtt{l o w}\}$ . Let us call the possible decisions—the agent’s actions— wait, search, and recharge. When the energy level is high, recharging would always be foolish, so we do not include it in the action set for this state. The agent’s action sets are 

$$
\begin{array}{r c l}{{\mathcal{A}(\mathrm{high})}}&{{=}}&{{\{\mathrm{search},\mathrm{wait}\}}}\ {{\mathcal{A}(\mathrm{1ow})}}&{{=}}&{{\{\mathrm{search},\mathrm{wait},\mathrm{recharge}\}.}}\end{array}
$$ 
If the energy level is high, then a period of active search can always be completed without risk of depleting the battery. A period of searching that begins with a high energy level leaves the energy level high with probability $\alpha$ and reduces it to low with probability $1-\alpha$ . On the other hand, a period of searching undertaken when the energy level is low leaves it low with probability $\beta$ and depletes the battery with probability $1-\beta$ . In the latter case, the robot must be rescued, and the battery is then recharged back to high. Each can collected by the robot counts as a unit reward, whereas a reward of $^{-3}$ results whenever the robot has to be rescued. Let $r_{\tt s e a r c h}$ and $r_{\tt w a i t}$ , with $r_{\tt s e a r c h}>r_{\tt w a i t}$ , respectively denote the expected number of cans the robot will collect (and hence the expected reward) while searching and while waiting. Finally, to keep things simple, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted. This system is then a finite MDP, and we can write down the transition probabilities and the expected rewards, as in Table 3.1. 

A transition graph is a useful way to summarize the dynamics of a finite MDP. Figure 3.3 shows the transition graph for the recycling robot example. There are two kinds of nodes: state nodes and action nodes. There is a state node for each possible state (a large open circle labeled by the name of the state), and an action node for each state–action pair (a small solid circle labeled 
<html><body><table><tr><td>S</td><td>s'</td><td>a</td><td>p(s'ls,a)</td><td>r(s,a, s)</td></tr><tr><td>high</td><td>high</td><td>search</td><td></td><td>rsearch</td></tr><tr><td>high</td><td>low</td><td>search</td><td>1-Q</td><td>rsearch</td></tr><tr><td>low</td><td>high</td><td>search</td><td>1-β</td><td>-3</td></tr><tr><td>low</td><td>low</td><td>search</td><td>β</td><td>Tsearch</td></tr><tr><td>high</td><td>high</td><td>wait</td><td>1</td><td>rwait</td></tr><tr><td>high</td><td>low</td><td>wait</td><td>0</td><td>rwait</td></tr><tr><td>low</td><td>high</td><td>wait</td><td>0</td><td>rwait</td></tr><tr><td>low</td><td>low</td><td>wait</td><td>1</td><td>rwait</td></tr><tr><td>low</td><td>high</td><td>recharge</td><td>1</td><td>0</td></tr><tr><td>low</td><td>low</td><td>recharge</td><td>0</td><td>0.</td></tr></table></body></html> 
Table 3.1: Transition probabilities and expected rewards for the finite MDP of the recycling robot example. There is a row for each possible combination of current state, $s$ , next state, $s^{\prime}$ , and action possible in the current state, $a\in\mathcal{A}(s)$ . 
by the name of the action and connected by a line to the state node). Starting in state $s$ and taking action $a$ moves you along the line from state node $s$ to action node $(s,a)$ . Then the environment responds with a transition to the next state’s node via one of the arrows leaving action node $(s,a)$ . Each arrow corresponds to a triple $(s,s^{\prime},a)$ ，where $s^{\prime}$ is the next state, and we label the arrow with the transition probability, $p(s^{\prime}|s,a)$ , and the expected reward for that transition, $r(s,a,s^{\prime})$ . Note that the transition probabilities labeling the arrows leaving an action node always sum to 1. ■ 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/73fa2d3755140c75137d981971bbb5b951b99dbe7501d02401822ab555b1b60b.jpg) 
Figure 3.3: Transition graph for the recycling robot example 

## 3.7 Value Functions 
Almost all reinforcement learning algorithms involve estimating value functions—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular policies. 
>  几乎所有 RL 算法都涉及估计价值函数，价值函数是关于状态/状态-动作对的函数，评估了智能体在给定状态/在给定状态并执行给定动作的“优异”程度
>  “优异”程度由未来的期望奖励定义，即期望回报
>  智能体在未来期望收到的奖励取决于它执行的动作，故价值函数是相对于特定策略而定义

Recall that a policy, $\pi$ , is a mapping from each state, $s\in\mathcal{S}$ , and action, $a\in$ ${\mathcal{A}}(s)$ , to the probability $\pi({a}|{s})$ of taking action $a$ when in state $s$ . Informally, the value of a state $s$ under a policy $\pi$ , denoted $v_{\pi}(s)$ , is the expected return when starting in $s$ and following $\pi$ thereafter. 
>  策略 $\pi$ 是从状态 $s\in \mathcal S$ 和动作 $a\in \mathcal A(s)$ 到概率 $\pi(a\mid s)$ 的映射，表示了在状态 $s$ 下执行动作 $a$ 的概率
>  状态 $s$ 在策略 $\pi$ 下的价值记作 $v_\pi(s)$，定义为从状态 $s$ 开始，后续执行策略 $\pi$ 所能收到的期望回报

For MDPs, we can define $v_{\pi}(s)$ formally as 

$$
v_\pi(s) = \mathbb E_\pi\left[G_t \mid S_t= s\right] = \mathbb E_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t + k +1}\mid S_t =s \right]\tag{3.10}
$$

where $\mathbb{E}_{\pi}[\cdot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$ , and $t$ is any time step. Note that the value of the terminal state, if any, is always zero. We call the function $v_{\pi}$ the state-value function for policy $\pi$ . 

>  对于 MDP，策略 $\pi$ 的状态价值函数 $v_\pi$ 的形式定义如上
>  终止状态的状态价值函数为零

Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$ , denoted $q_{\pi}(s,a)$ , as the expected return starting from $s$ , taking the action $a$ , and thereafter following policy $\pi$ : 

$$
q_\pi(s, a) = \mathbb E_\pi\left[G_t\mid S_t =s, A_t = a\right] = \mathbb E_\pi\left[\sum_{k=0}^\infty R_{t+k+1}\mid S_t = s, A_t = a\right]\tag{3.11}
$$

We call $q_{\pi}$ the action-value function for policy $\pi$ . 

>  在策略 $\pi$ ，在状态 $s$ 执行动作 $a$ 的价值记作 $q_\pi(s, a)$，定义为从状态 $s$ 开始，执行动作 $a$，然后遵循策略 $\pi$ 所能得到的期望回报
>  $q_\pi(s, a)$ 称为策略 $\pi$ 的动作价值函数

The value functions $v_{\pi}$ and $q_{\pi}$ can be estimated from experience. For example, if an agent follows policy $\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state’s value, $v_{\pi}(s)$ , as the number of times that state is encountered approaches infinity. If separate averages are kept for each action taken in a state, then these averages will similarly converge to the action values, $q_{\pi}(s,a)$ . 
>  价值函数 $v_\pi, q_\pi$ 都可以从实践中估计
>  例如智能体遵循策略 $\pi$，在实践中记录每次遇到状态 $s$ 得到的实际回报，计算平均值，随着次数增加，该均值会收敛到该状态的真实价值 $v_\pi(s)$
>  同样，智能体在实践中记录每个状态下执行每个动作得到的实际回报的均值，均值最后会收敛到真实值 $q_\pi(s, a)$

We call estimation methods of this kind Monte Carlo methods because they involve averaging over many random samples of actual returns. These kinds of methods are presented in Chapter 5. Of course, if there are very many states, then it may not be practical to keep separate averages for each state individually. Instead, the agent would have to maintain $v_{\pi}$ and $q_{\pi}$ as parameterized functions and adjust the parameters to better match the observed returns. This can also produce accurate estimates, although much depends on the nature of the parameterized function approximator (Chapter 9). 
>  这类估计方法就是 Monte Carlo 方法，它使用均值估计期望
>  为每个状态和每个状态-动作对分别维护均值过于昂贵，智能体可以用参数化的函数表示 $v_\pi, q_\pi$，通过调节参数以匹配观察，这虽然依赖于参数化函数估计器的性质，但也可以产生正确的估计

A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$ , the following consistency condition holds between the value of $s$ and the value of its possible successor states: 
>  价值函数的一个基本性质是满足特定的递归关系
>  对于任意的策略 $\pi$ 和任意的状态 $s$， $s$ 和它可能的后续状态满足以下一致条件

$$
\begin{align}
v_\pi(s)&=\mathbb E_\pi\left[G_t \mid S_t = s\right]\\
&=\mathbb E_\pi\left[\sum_{k=0}^\infty\gamma^{k}R_{t+k+1}\mid S_t = s\right]\\
&=\mathbb E_\pi \left[R_{t+1} + \gamma\cdot\sum_{k=0}^\infty\gamma^k R_{t+k+2}\mid S_t = s\right]\\
&=\sum_{a}\pi(a\mid s)\sum_{s'}\sum_r p(s', r\mid s, a)\left[r + \gamma\mathbb E_{\pi}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+2}\mid S_{t+1} = s' \right]\right]\\
&=\sum_a\pi(a\mid s)\sum_{s', r}p(s',r\mid s, a)[r + \gamma v_\pi(s')]\qquad (3.12)
\end{align}
$$

>  其中第四个等号将对于 $\pi$ 的期望按序拆分，首先根据当前状态选择一个可能动作，然后根据动作-状态对和环境动态选择可能的后续状态-奖励对，此时 $R_{t+1}$ 已经确定为 $r$，后续则对 $R_{t+2}$ 之后的奖励，基于状态 $s'$ 按照 $\pi$ 取期望
>  因此状态价值和后续的状态价值构成了以上递归关系

where it is implicit that the actions, $a$ , are taken from the set ${\mathcal{A}}(s)$ , the next states, $s^{\prime}$ , are taken from the set $\mathcal S$ (or from $\mathcal{S}^{+}$ in the case of an episodic problem), and the rewards, $r$ , are taken from the set $\mathcal{R}$ . 

Note also how in the last equation we have merged the two sums, one over all the values of $s^{\prime}$ and the other over all values of $r$ , into one sum over all possible values of both. We will use this kind of merged sum often to simplify formulas. 

Note how the final expression can be read very easily as an expected value. It is really a sum over all values of the three variables, $a$ , $s^{\prime}$ , and $r$ . For each triple, we compute its probability, $\pi(a|s)p(s^{\prime},r|s,a)$ , weight the quantity in brackets by that probability, then sum over all possibilities to get an expected value. 
>  最后的表达式可以容易地解读为一个期望值，它实际上是对三个变量 $a, s', r$ 的所有取值的求和，对于每个三元组，我们计算其概率 $\pi(a\mid s) p(s', r\mid s, a)$，使用概率对 $r + \gamma v_\pi(s')$ 加权，然后求加权平均和得到期望

Equation (3.12) is the Bellman equation for $v_{\pi}$ . It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from one state to its possible successor states, as suggested by Figure 3.4a. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state $s$ , the root node at the top, the agent could take any of some set of actions—three are shown in Figure 3.4a. From each of these, the environment could respond with one of several next states, $s^{\prime}$ , along with a reward, $r$ . The Bellman equation (3.12) averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. 
>  Eq 3.12 是状态价值函数 $v_\pi$ 的 Bellman 方程，该方程表示了当前状态的价值和其后继状态的价值之间的关系
>  Bellman 方程遍历所有的可能性 (给定 $s$，可能发生的动作 $a$，以及后续可能的状态和奖励) 用概率作为权重求加权平均和。该方程说明了起始状态的价值必须等于下一个状态的期望 (折扣) 价值加上对应的期望奖励

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/651d493c3b2c94886e64b88c82b45730a0f68d34b2d6a793a212e548141a086e.jpg) 

Figure 3.4: Backup diagrams for (a) $v_{\pi}$ and (b) $q_{\pi}$ . 

The value function $v_{\pi}$ is the unique solution to its Bellman equation. We show in subsequent chapters how this Bellman equation forms the basis of a number of ways to compute, approximate, and learn $v_{\pi}$ . 
>  价值函数 $v_\pi$ 是其 Bellman 方程的唯一解

We call diagrams like those shown in Figure 3.4 backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs). We use backup diagrams throughout the book to provide graphical summaries of the algorithms we discuss. (Note that unlike transition graphs, the state nodes of backup diagrams do not necessarily represent distinct states; for example, a state might be its own successor. We also omit explicit arrowheads because time always flows downward in a backup diagram.) 
>  如 Figure 3.4 的图称为备份图，这类图描绘了形成 RL 学习方法基础的更新或备份操作，这些操作将价值信息从后继状态/状态-动作对传递回先导状态/状态-动作对
>  注意与转移图不同，备份图中的节点不一定表示不同的状态，因为一个状态的后继状态也可能是它自己

Example 3.8: Gridworld Figure 3.5a uses a rectangular grid to illustrate value functions for a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of $-1$ . Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a reward of $+10$ and take the agent to $\mathrm{A}^{\prime}$ . From state B, all actions yield a reward of $+5$ and take the agent to $\mathrm{B^{\prime}}$ . 

Suppose the agent selects all four actions with equal probability in all states. Figure 3.5b shows the value function, $v_{\pi}$ , for this policy, for the discounted reward case with $\gamma=0.9$ . This value function was computed by solving the system of equations (3.12). Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy. State A is the best state to be in under this policy, but its expected return is less than 10, its immediate reward, because from A the agent is taken to $\mathrm{A}^{\prime}$ , from which it is likely to run into the edge of the grid. State B, on the other hand, is valued more than 5, its immediate reward, because from B the agent is taken to $\mathrm{B^{\prime}}$ , which has a positive value. From $\mathrm{B^{\prime}}$ the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto A or B. 

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/d2a736ee309cc3ea7eea418bf42f988a546c36028d2a401689187c0f4e80637b.jpg) 

Figure 3.5: Grid example: (a) exceptional reward dynamics; (b) state-value function for the equiprobable random policy. 

Example 3.9: Golf To formulate playing a hole of golf as a reinforcement learning task, we count a penalty (negative reward) of $-1$ for each stroke until we hit the ball into the hole. The state is the location of the ball. The value of a state is the negative of the number of strokes to the hole from that location. Our actions are how we aim and swing at the ball, of course, and which club we select. Let us take the former as given and consider just the choice of club, which we assume is either a putter or a driver. The upper part of Figure 3.6 shows a possible state-value function, $v_{\mathrm{putt}}(s)$ , for the policy that always uses the putter. The terminal state in-the-hole has a value of 0. From anywhere on the green we assume we can make a putt; these states have value $-1$ . Off the green we cannot reach the hole by putting, and the value is greater. If we can reach the green from a state by putting, then that state must have value one less than the green’s value, that is, $-2$ . For simplicity, let us assume we can putt very precisely and deterministically, but with a limited range. This gives us the sharp contour line labeled $-2$ in the figure; all locations between that line and the green require exactly two strokes to complete the hole. Similarly, any location within putting range of the $-2$ contour line must have a value of $^{-3}$ , and so on to get all the contour lines shown in the figure. Putting doesn’t get us out of sand traps, so they have a value of $-\infty$ . Overall, it takes us six strokes to get from the tee to the hole by putting. 

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/2f79d9d17a82f91bb1a7e4a2d59b816fbb775b7dbbdf3fc3dca4e9253cc88ed2.jpg) 

Figure 3.6: A golf example: the state-value function for putting (above) and the optimal action-value function for using the driver (below). 

## 3.8 Optimal Value Functions 
Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$ if its expected return is greater than or equal to that of $\pi^{\prime}$ for all states. In other words, $\pi\geq\pi^{\prime}$ if and only if $v_{\pi}(s)\geq v_{\pi^{\prime}}(s)$ for all $s\in\mathcal{S}$ . There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{*}$ . They share the same state-value function, called the optimal state-value function, denoted $v_{*}$ , and defined as 

$$
v_{*}(s)=\operatorname*{max}_{\pi}v_{\pi}(s),\tag{3.13}
$$ 
for all $s\in\mathcal{S}$ . 

>  求解一个 RL 问题大致意味着找到一个可以在长期执行下获得大量奖励的策略
>  对于有限 MDP，可以确切地定义一个最优策略：价值函数定义了策略之间的一个偏序关系，对于所有的状态，如果一个策略 $\pi$ 的价值不小于另一个策略 $\pi'$ 的价值，就称策略 $\pi$ 相对于 $\pi'$ 是更优的或者至少是等价的，即 $\pi \ge \pi'$ 当且仅当 $v_\pi(s)\ge v_{\pi'}(s)\quad \forall s \in \mathcal S$
>  有限 MDP 中，总是存在至少一个策略，它大于等于所有其他策略，该策略即最优策略，最优策略也不一定仅有一个，我们记所有最优策略为 $\pi_*$
>  所有最优策略在价值函数层面等价，或者说它们共享相同的状态价值函数，最优策略的状态价值函数记作 $v_*(s)$，定义如上

Optimal policies also share the same optimal action-value function, denoted $q_{*}$ , and defined as 

$$
q_{*}(s,a)=\operatorname*{max}_{\pi}q_{\pi}(s,a),\tag{3.14}
$$ 
for all $s\in\mathcal{S}$ and $a\in\mathcal{A}(s)$ . For the state–action pair $(s,a)$ , this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. 

>  最优策略同样共享相同的动作价值函数，记作 $q_*$，定义如上
>  该价值函数给出了在状态 $s$ 执行动作 $a$，之后遵循一个最优策略所能得到的期望回报

Thus, we can write $q_{*}$ in terms of $v_{*}$ as follows: 

$$
\begin{align}
q_{*}(s,a)=\mathbb{E}[R_{t+1}+\gamma v_{*}(S_{t+1})\mid S_{t}=s,A_{t}=a].\tag{3.15}
\end{align}
$$ 
>  因此，最优动作价值函数和下一个状态的最优状态价值函数存在如上的关系 (这里的求期望实际上就是在对后继状态求期望)

Example 3.10: Optimal Value Functions for Golf The lower part of Figure 3.6 shows the contours of a possible optimal action-value function $q_{*}(s,\mathtt{d r i v e r})$ . These are the values of each state if we first play a stroke with the driver and afterward select either the driver or the putter, whichever is better. The driver enables us to hit the ball farther, but with less accuracy. We can reach the hole in one shot using the driver only if we are already very close; thus the $-1$ contour for $q_{*}(s,\mathtt{d r i v e r})$ covers only a small portion of the green. If we have two strokes, however, then we can reach the hole from much farther away, as shown by the $-2$ contour. In this case we don’t have to drive all the way to within the small $-1$ contour, but only to anywhere on the green; from there we can use the putter. The optimal action-value function gives the values after committing to a particular first action, in this case, to the driver, but afterward using whichever actions are best. The $^{-3}$ contour is still farther out and includes the starting tee. From the tee, the best sequence of actions is two drives and one putt, sinking the ball in three strokes. 

Because $v_{*}$ is the value function for a policy, it must satisfy the self-consistency condition given by the Bellman equation for state values (3.12). Because it is the optimal value function, however, $v_{*}$ ’s consistency condition can be written in a special form without reference to any specific policy. This is the Bellman equation for $v_{*}$ , or the Bellman optimality equation. 
>  最优状态价值函数 $v_*$ 也是某个最优策略的价值函数，故显然也满足 Bellman 方程给出的自一致条件
>  $v_*$ 的一致性条件可以进一步写为不参照任意特定策略的特殊形式，即 $v_*$ 的 Bellman 方程，我们称为 Bellman 最优方程

Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state: 

$$
\begin{align}
v_*(s) &= \max_{a\in \mathcal A(s)}q_{\pi_*}(s, a)\\
&=\max_a \mathbb E_{\pi^*}[G_t\mid S_t = s, A_t = a]\\
&=\max_a \mathbb E_{\pi^*}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1}\mid S_t = s, A_t = a\right]\\
&=\max_a \mathbb E_{\pi^*}\left[R_{t+1} + \gamma\cdot\sum_{k=0}^\infty \gamma^{k} R_{t+k+2}\mid S_t = s, A_t = a\right]\\
&=\max_a \mathbb E\left[R_{t+1} + \gamma v_*(S_{t+1})\mid S_t = s, A_t = a\right]\tag{3.16}\\
&=\max_a \sum_{s',r}p(s',r'\mid s, a)[r + \gamma v_*(s')]\tag{3.17}
\end{align}
$$

The last two equations are two forms of the Bellman optimality equation for $v_{*}$ . 

>  直观上，Bellman 最优方程表明了最优策略下的状态价值一定等于从该状态采取最佳行动的期望回报

The Bellman optimality equation for $q_{*}$ is 

$$
\begin{align}
q_*(s, a) 
&=\mathbb E_{\pi^*}\left[G_t\mid S_t = s, A_t = a\right]\\
&=\mathbb E_{\pi^*}\left[\sum_{k=0}^\infty \gamma^kR_{t+k+1}\mid S_t = s, A_t = a\right]\\
&=\mathbb E_{\pi^*}\left[R_{t+1} + \gamma\cdot \sum_{k=0}^\infty\gamma^k R_{t+k+2}\mid S_t = s, A_t = a\right]\\
&=\mathbb E_{}\left[R_{t+1} + \gamma v_*(S_{t+1})\mid S_t = s, A_t = a\right]\\
&= \mathbb E\left[R_{t+1} + \gamma\max_{a'} q_*(S_{t+1}, a')\mid S_t = s, A_t = a\right]\\
&=\sum_{s',r}p(s',r\mid s, a)[r + \gamma \max_{a'}q_*(s', a')]
\end{align}
$$

>  最优动作价值函数 $q_*$ 的 Bellman 最优方程如上

The backup diagrams in Figure 3.7 show graphically the spans of future states and actions considered in the Bellman optimality equations for $v_{*}$ and $q_{*}$ . These are the same as the backup diagrams for $v_{\pi}$ and $q_{\pi}$ except that arcs have been added at the agent’s choice points to represent that the maximum over that choice is taken rather than the expected value given some policy. Figure 3.7a graphically represents the Bellman optimality equation (3.17). 
>  Figure 3.7 展示了 Bellman 最优方程所考虑的未来状态和动作，图形和之前是相同的，差异在于为智能体的选择点添加了弧线，以表示智能体仅选择达到最大值的动作，而不是计算期望值

For finite MDPs, the Bellman optimality equation (3.17) has a unique solution independent of the policy. The Bellman optimality equation is actually a system of equations, one for each state, so if there are $N$ states, then there are $N$ equations in $N$ unknowns. If the dynamics of the environment are known $(p(s^{\prime},r|s,a))$ , then in principle one can solve this system of equations for $v_{*}$ using any one of a variety of methods for solving systems of nonlinear equations. One can solve a related set of equations for $q_{*}$ . 
>  对于有限 Markov 决策过程，Bellman 最优方程有独立于策略的唯一解
>  Bellman 最优方程本质是一个方程组，每个状态贡献一个方程，如果有 $N$ 个状态，就有 $N$ 个未知数的 $N$ 个方程，如果环境动态 $p(s', r\mid s, a)$ 已知，则原则上可以求解该方程组以得到 $v_*$， $q_*$ 同理

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/b3b0bf249c33bdbaa61d946b8261f51a2cc318c1da400a346e20c36791969953.jpg) 


Figure 3.7: Backup diagrams for (a) $v_{*}$ and (b) $q_{*}$ 

Once one has $v_{*}$ , it is relatively easy to determine an optimal policy. For each state $s$ , there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. Any policy that assigns nonzero probability only to these actions is an optimal policy. 
>  解出最优状态价值函数 $v_*$ 之后，就容易决定最优策略：对于每个状态 $s$，会存在一组动作使得 Bellman 最优方程 $\max_a \mathbb E\left[R_{t+1} + \gamma v_*(S_{t+1})\mid S_t = s, A_t = a\right]$ 中的期望取到最大，任意对这组动作赋予非零概率值，对其余动作赋予零概率值的策略就都是最优策略

You can think of this as a one-step search. If you have the optimal value function, $v_{*}$ , then the actions that appear best after a one-step search will be optimal actions. Another way of saying this is that any policy that is greedy with respect to the optimal evaluation function $v_{*}$ is an optimal policy. 
>  这个过程可以视作一步搜索，已知最优状态价值函数 $v_*$ 之后，在一步搜索之后最优的动作就是最优策略会执行的动作
>  换句话说，任意相对于最优状态价值函数 $v_*$ 是贪心的策略都是最优策略

The term greedy is used in computer science to describe any search or decision procedure that selects alternatives based only on local or immediate considerations, without considering the possibility that such a selection may prevent future access to even better alternatives. Consequently, it describes policies that select actions based only on their short-term consequences. The beauty of $v_{*}$ is that if one uses it to evaluate the short-term consequences of actions—specifically, the one-step consequences—then a greedy policy is actually optimal in the long-term sense in which we are interested because $v_{*}$ already takes into account the reward consequences of all possible future behavior. By means of $v_{*}$ , the optimal expected long-term return is turned into a quantity that is locally and immediately available for each state. Hence, a one-step-ahead search yields the long-term optimal actions. 
>  最优状态状态价值函数 $v_*$ 已经考虑了所有未来可能行为的奖励后果，因此基于 $v_*$ 进行贪心决策 (基于一步搜索的结果进行决策) 就能得到长期上最优的行动。$v_*$ 将最优的长期回报转化为可以局部地基于每个状态立即获得的量

Having $q_{*}$ makes choosing optimal actions still easier. With $q_{*}$ , the agent does not even have to do a one-step-ahead search: for any state $s$ , it can simply find any action that maximizes $q_{*}(s,a)$ . The action-value function effectively caches the results of all one-step-ahead searches. It provides the optimal expected long-term return as a value that is locally and immediately available for each state–action pair. Hence, at the cost of representing a function of state–action pairs, instead of just of states, the optimal action-value function allows optimal actions to be selected without having to know anything about possible successor states and their values, that is, without having to know anything about the environment’s dynamics. 
>  解出 $q_*$ 之后，仍然可以容易选择最优行动，对于任意状态 $s$ ，智能体只需要找到能够最大化 $q_*(s, a)$ 的动作
>  动作价值函数有效存储了所有一步搜索 (即执行一次动作) 的结果，提供了局部地基于每个状态-动作对就可以知道的最优期望长期回报值
>  因此基于最优动作价值函数可以在不需要知道可能的后继状态和其价值的情况下进行判断，也就是不需要知道环境动态

Example 3.11: Bellman Optimality Equations for the Recycling Robot Using (3.17), we can explicitly give the Bellman optimality equation for the recycling robot example. To make things more compact, we abbreviate the states high and low, and the actions search, wait, and recharge respectively by h, l, s, w, and re. Since there are only two states, the Bellman optimality equation consists of two equations. The equation for $v_{*}(\mathtt{h})$ can be written as follows: 

$$
\begin{array}{r l}{\mathrm{~\psi_*(h)~}=}&{\operatorname*{max}\left\{\begin{array}{l l}{p(\mathrm{h}|\mathbf{h},\mathbf{s})[r(\mathbf{h},\mathbf{s},\mathbf{h})+\gamma v_{*}(\mathbf{h})]+p(\mathrm{\mathbb{1}}|\mathbf{h},\mathbf{s})[r(\mathbf{h},\mathbf{s},\mathbf{1})+\gamma v_{*}(\mathbf{1})],}\ {p(\mathrm{h}|\mathbf{h},\mathbf{v})[r(\mathbf{h},\mathbf{v},\mathbf{h})+\gamma v_{*}(\mathbf{h})]+p(\mathrm{\mathbb{1}}|\mathbf{h},\mathbf{v})[r(\mathbf{h},\mathbf{w},\mathbf{1})+\gamma v_{*}(\mathbf{1})]}\end{array}\right\}}\ {=}&{\operatorname*{max}\left\{\begin{array}{l l}{\alpha[r_{\mathbf{s}}+\gamma v_{*}(\mathbf{h})]+(1-\alpha)[r_{\mathbf{s}}+\gamma v_{*}(\mathbf{1})],}\ {1[r_{\mathbf{s}}+\gamma v_{*}(\mathbf{h})]+0[r_{\mathbf{s}}+\gamma v_{*}(\mathbf{1})]}\end{array}\right\}}\ {=}&{\operatorname*{max}\left\{\begin{array}{l l}{r_{\mathbf{s}}+\gamma[\alpha v_{*}(\mathbf{h})+(1-\alpha)v_{*}(\mathbf{1})],}\ {r_{\mathbf{u}}+\gamma v_{*}(\mathbf{h})}\end{array}\right\}.}\end{array}
$$ 
Following the same procedure for $v_{*}(1)$ yields the equation 

$$
v_{*}(1)=\operatorname*{max}\left\{\begin{array}{l}{\beta r_{\mathrm{s}}-3(1-\beta)+\gamma[(1-\beta)v_{*}(\mathrm{h})+\beta v_{*}(1)]}\ {r_{\mathrm{u}}+\gamma v_{*}(1),}\ {\gamma v_{*}(\mathrm{h})}\end{array}\right\}.
$$ 
For any choice of $r_{\mathrm{s}}$ , $r_{\mathrm{{w}}}$ , $\alpha$ , $\beta$ , and $\gamma$ , with $0\leq\gamma<1$ , $0\leq\alpha,\beta\leq1$ , there is exactly one pair of numbers, $v_{*}(\mathtt{h})$ and $v_{*}(1)$ , that simultaneously satisfy these two nonlinear equations. 

Example 3.12: Solving the Gridworld Suppose we solve the Bellman equation for $v_{*}$ for the simple grid task introduced in Example 3.8 and shown again in Figure 3.8a. Recall that state A is followed by a reward of $+10$ and transition to state $\mathrm{A}^{\prime}$ , while state B is followed by a reward of $+5$ and transition to state $\mathrm{B^{\prime}}$ . Figure 3.8b shows the optimal value function, and Figure 3.8c shows the corresponding optimal policies. Where there are multiple arrows in a cell, any of the corresponding actions is optimal. 

Explicitly solving the Bellman optimality equation provides one route to finding an optimal policy, and thus to solving the reinforcement learning problem. However, this solution is rarely directly useful. It is akin to an exhaustive search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirabilities in terms of expected rewards. This solution relies on at least three assumptions that are rarely true in practice: (1) we accurately know the dynamics of the environment; (2) we have enough computational resources to complete the computation of the solution; and (3) the Markov property. For the kinds of tasks in which we are interested, one is generally not able to implement this solution exactly because various combinations of these assumptions are violated. For example, although the first and third assumptions present no problems for the game of backgammon, the second is a major impediment. Since the game has about $10^{20}$ states, it would take thousands of years on today’s fastest computers to solve the Bellman equation for $v_{*}$ , and the same is true for finding $q_{*}$ . In reinforcement learning one typically has to settle for approximate solutions. 
>  获取最优策略的一种方式就是显式求解 Bellman 最优方程，但 Bellman 最优方程的求解要求遍历所有的可能性，且依赖于实践中较少成立的三个假设
>  1. 确切知道环境动态
>  2. 具有足够计算资源求解
>  3. 满足 Markov 性质
>  状态集合的大小往往是指数级别，因此确切求解 Bellman 最优方程往往不现实

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/70982c51cdbfc121197ae7db3f3f084106132d14fae4c14289a72c075209a8ab.jpg) 

Figure 3.8: Optimal solutions to the gridworld example. 

Many different decision-making methods can be viewed as ways of approximately solving the Bellman optimality equation. For example, heuristic search methods can be viewed as expanding the right-hand side of (3.17) several times, up to some depth, forming a “tree” of possibilities, and then using a heuristic evaluation function to approximate $v_{*}$ at the “leaf” nodes. (Heuristic search methods such as A∗ are almost always based on the episodic case.) The methods of dynamic programming can be related even more closely to the Bellman optimality equation. Many reinforcement learning methods can be clearly understood as approximately solving the Bellman optimality equation, using actual experienced transitions in place of knowledge of the expected transitions. We consider a variety of such methods in the following chapters. 
>  许多决策过程可以视作近似求解 Bellman 最优方程，例如启发式搜索可以认为是多次展开 Eq 3.17 的 RHS，构成一个树，在叶子节点近似 $v_*$，又例如动态规划，以及许多 RL 方法

## 3.9 Optimality and Approximation 
We have defined optimal value functions and optimal policies. Clearly, an agent that learns an optimal policy has done very well, but in practice this rarely happens. For the kinds of tasks in which we are interested, optimal policies can be generated only with extreme computational cost. A well-defined notion of optimality organizes the approach to learning we describe in this book and provides a way to understand the theoretical properties of various learning algorithms, but it is an ideal that agents can only approximate to varying degrees. As we discussed above, even if we have a complete and accurate model of the environment’s dynamics, it is usually not possible to simply compute an optimal policy by solving the Bellman optimality equation. For example, board games such as chess are a tiny fraction of human experience, yet large, custom-designed computers still cannot compute the optimal moves. A critical aspect of the problem facing the agent is always the computational power available to it, in particular, the amount of computation it can perform in a single time step. 
The memory available is also an important constraint. A large amount of memory is often required to build up approximations of value functions, policies, and models. In tasks with small, finite state sets, it is possible to form these approximations using arrays or tables with one entry for each state (or state–action pair). This we call the tabular case, and the corresponding methods we call tabular methods. In many cases of practical interest, however, there are far more states than could possibly be entries in a table. In these cases the functions must be approximated, using some sort of more compact parameterized function representation. 
Our framing of the reinforcement learning problem forces us to settle for approximations. However, it also presents us with some unique opportunities for achieving useful approximations. For example, in approximating optimal behavior, there may be many states that the agent faces with such a low probability that selecting suboptimal actions for them has little impact on the amount of reward the agent receives. Tesauro’s backgammon player, for example, plays with exceptional skill even though it might make very bad decisions on board configurations that never occur in games against experts. In fact, it is possible that TD-Gammon makes bad decisions for a large fraction of the game’s state set. The on-line nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states. This is one key property that distinguishes reinforcement learning from other approaches to approximately solving MDPs. 

## 3.10 Summary 
Let us summarize the elements of the reinforcement learning problem that we have presented in this chapter. Reinforcement learning is about learning from interaction how to behave in order to achieve a goal. The reinforcement learning agent and its environment interact over a sequence of discrete time steps. The specification of their interface defines a particular task: the actions are the choices made by the agent; the states are the basis for making the choices; and the rewards are the basis for evaluating the choices. Everything inside the agent is completely known and controllable by the agent; everything outside is incompletely controllable but may or may not be completely known. A policy is a stochastic rule by which the agent selects actions as a function of states. The agent’s objective is to maximize the amount of reward it receives over time. 
The return is the function of future rewards that the agent seeks to maximize. It has several different definitions depending upon the nature of the task and whether one wishes to discount delayed reward. The undiscounted formulation is appropriate for episodic tasks, in which the agent–environment interaction breaks naturally into episodes; the discounted formulation is appropriate for continuing tasks, in which the interaction does not naturally break into episodes but continues without limit. 
An environment satisfies the Markov property if its state signal compactly summarizes the past without degrading the ability to predict the future. This is rarely exactly true, but often nearly so; the state signal should be chosen or constructed so that the Markov property holds as nearly as possible. In this book we assume that this has already been done and focus on the decisionmaking problem: how to decide what to do as a function of whatever state signal is available. If the Markov property does hold, then the environment is called a Markov decision process (MDP). A finite $M D P$ is an MDP with finite state and action sets. Most of the current theory of reinforcement learning is restricted to finite MDPs, but the methods and ideas apply more generally. 
A policy’s value functions assign to each state, or state–action pair, the expected return from that state, or state–action pair, given that the agent uses the policy. The optimal value functions assign to each state, or state–action pair, the largest expected return achievable by any policy. A policy whose value functions are optimal is an optimal policy. Whereas the optimal value functions for states and state–action pairs are unique for a given MDP, there can be many optimal policies. Any policy that is greedy with respect to the optimal value functions must be an optimal policy. The Bellman optimality equations are special consistency condition that the optimal value functions must satisfy and that can, in principle, be solved for the optimal value functions, from which an optimal policy can be determined with relative ease. 
A reinforcement learning problem can be posed in a variety of different ways depending on assumptions about the level of knowledge initially available to the agent. In problems of complete knowledge, the agent has a complete and accurate model of the environment’s dynamics. If the environment is an MDP, then such a model consists of the one-step transition probabilities and expected rewards for all states and their allowable actions. In problems of incomplete knowledge, a complete and perfect model of the environment is not available. 
Even if the agent has a complete and accurate environment model, the agent is typically unable to perform enough computation per time step to fully use it. The memory available is also an important constraint. Memory may be required to build up accurate approximations of value functions, policies, and models. In most cases of practical interest there are far more states than could possibly be entries in a table, and approximations must be made. 
A well-defined notion of optimality organizes the approach to learning we describe in this book and provides a way to understand the theoretical properties of various learning algorithms, but it is an ideal that reinforcement learning agents can only approximate to varying degrees. In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be approximated in some way. 

# 4 Dynamic Programming 
The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the methods presented in the rest of this book. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment. 
>  动态规划指在环境被完美建模为 MDP 问题下用于计算最优策略的一类算法
>  经典的动态规划算法需要完美的模型，并且计算开销较大，因此应用有限，但具有理论价值
>  之后介绍的方法都可以视作一更少的计算或者不假设环境具有完美模型的情况下，尝试达到和 DP 相同的效果

Starting with this chapter, we usually assume that the environment is a finite MDP. That is, we assume that its state, action, and reward sets, $\mathcal S$, ${\mathcal{A}}(s)$ , and $\mathcal{R}$ , for $s\in\mathcal{S}$ , are finite, and that its dynamics are given by a set of probabilities $p(s^{\prime},r|s,a)$ , for all $s\in\mathcal S$ , $a\in\mathcal A(s)$ , $r\in\mathcal R$ , and $s^{\prime}\in\mathcal{S}^{+}$ ( $\mathcal{S}^{+}$ is $\mathcal S$ plus a terminal state if the problem is episodic). Although DP ideas can be applied to problems with continuous state and action spaces, exact solutions are possible only in special cases. A common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply finite-state DP methods. The methods we explore in Chapter 9 are applicable to continuous problems and are a significant extension of that approach. 
>  我们假设环境是有限 MDP，即其状态集合、动作集合、奖励集合 $\mathcal S, \mathcal A(s)\ \text{for}\ s\in \mathcal S, \mathcal R$ 都是有限集合。MDP 的动态由一组概率 $p(s', r\mid s, a)\ \text{for all}\ a\in \mathcal S, a\in \mathcal A(s), r\in \mathcal R, s' \in \mathcal S^+$ ($\mathcal S^+$ 为 $\mathcal S$ 再加上一个终止状态)
>  DP 一般情况下仅限于有限 MDP，对于连续的状态和动作空间，一般会进行量化近似

The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In this chapter we show how DP can be used to compute the value functions defined in Chapter 3. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions, $v_{*}$ or $q_{*}$ , which satisfy the Bellman optimality equations: 
>  DP 的关键思想是用价值函数来组织和结构化对策略的搜索，即我们用 DP 计算最优价值函数，根据最优价值函数定义策略

>  最状态优价值函数和最优动作价值函数对于所有的 $s\in \mathcal S, a\in \mathcal A(s), s' \in \mathcal S^+$ 满足 Bellman 最优方程，如下所示：

$$
\begin{align}
v_*(s)&=\max_a \mathbb E\left[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t = s, A_t = a\right]\\
&=\max_a\sum_{s',r}p(s',r\mid s, a)[r + \gamma v_*(s')]\tag{4.1}
\end{align}
$$

or 

$$
\begin{align}
q_*(s, a)&= \mathbb E\left[R_{t+1} + \gamma \max_a'q_*(S_{t+1}, a')\mid S_t = s, A_t = a\right]\\
&=\sum_{s',r}p(s',r\mid s, a)[r + \gamma\max_{a'}q_*(s',a')]\tag{4.2}
\end{align}
$$

for all $s\in\mathcal S$ , $a\in\mathcal{A}(s)$ , and $s^{\prime}\in\mathcal{S}^{+}$ . 

As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions. 
>  DP 算法将 Bellman 方程转化为赋值，即转化为改进所需的价值函数 (的近似) 的更新规则

## 4.1 Policy Evaluation 
First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$ . This is called policy evaluation in the DP literature. We also refer to it as the prediction problem. Recall from Chapter 3 that, for all $s\in\mathcal{S}$ , 

$$
\begin{align}
v_\pi(s) &=\mathbb E_\pi[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\mid S_t = s]\\
&=\mathbb E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})\mid S_t = s]\tag{4.3}\\
&=\sum_a\pi(a\mid s)\sum_{s',r}p(s',r\mid s, a)[r + \gamma v_\pi(s')]\tag{4.4}
\end{align}
$$

where $\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$ , and the expectations are subscripted by $\pi$ to indicate that they are conditional on $\pi$ being followed. The existence and uniqueness of $v_{\pi}$ are guaranteed as long as either $\gamma<1$ or eventual termination is guaranteed from all states under the policy $\pi$ . 
>  考虑为任意策略 $\pi$ 计算其状态价值函数 $v_\pi$，DP 中称为策略评估
>  我们知道 $v_\pi$ 满足 Eq 4.3 和 Eq 4.4 描述的 Bellman 方程，只要折扣因子 $\gamma < 1$ 或者在策略 $\pi$ 下，从任意状态开始都可以达到终止状态，$v_\pi$ 的存在性和唯一性就可以保证

If the environment’s dynamics are completely known, then (4.4) is a system of $|\mathcal S|$ simultaneous linear equations in $|\mathcal S|$ unknowns (the $v_{\pi}(s)$ , $s\in\mathcal{S}$ ). In principle, its solution is a straightforward, if tedious, computation. For our purposes, iterative solution methods are most suitable. Consider a sequence of approximate value functions $v_{0},v_{1},v_{2},\ldots.$ , each mapping $\mathcal{S}^{+}$ to $\mathbb{R}$ . The initial approximation, $v_{0}$ , is chosen arbitrarily (except that the terminal state, if any, must be given value $0$ ), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ (3.12) as an update rule: 

$$
\begin{align}
v_{k+1}(s)&=\mathbb E[R_{t+1} + \gamma v_k(S_{t+1})\mid S_t = a]\\
&=\sum_a\pi(a\mid s)\sum_{s',r}p(s',r\mid s, a)[r + \gamma v_k(s')]\tag{4.5}
\end{align}
$$

for all $s\in\mathcal{S}$ . Clearly, $v_{k}=v_{\pi}$ is a fixed point for this update rule because the Bellman equation for $v_{\pi}$ assures us of equality in this case. Indeed, the sequence $\{v_{k}\}$ can be shown in general to converge to $v_{\pi}$ as $k\rightarrow\infty$ under the same conditions that guarantee the existence of $v_{\pi}$ . This algorithm is called iterative policy evaluation. 

>  如果环境动态完全已知，则 (4.4) 就是包含了 $|\mathcal S|$ 个未知数 (这些未知数是 $v_\pi(s), s\in \mathcal S$) 和 $|\mathcal S|$ 个线性方程的方程组
>  原则上，求解改线性方程组较为繁琐但直接，而迭代解法是较为合适的解法

To produce each successive approximation, $v_{k+1}$ from $v_{k}$ , iterative policy evaluation applies the same operation to each state $s$ : it replaces the old value of $s$ with a new value obtained from the old values of the successor states of $s$ , and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. We call this kind of operation a full backup. Each iteration of iterative policy evaluation backs up the value of every state once to produce the new approximate value function $v_{k+1}$ . There are several different kinds of full backups, depending on whether a state (as here) or a state–action pair is being backed up, and depending on the precise way the estimated values of the successor states are combined. All the backups done in DP algorithms are called full backups because they are based on all possible next states rather than on a sample next state. The nature of a backup can be expressed in an equation, as above, or in a backup diagram like those introduced in Chapter 3. For example, Figure 3.4a is the backup diagram corresponding to the full backup used in iterative policy evaluation. 

To write a sequential computer program to implement iterative policy evaluation, as given by (4.5), you would have to use two arrays, one for the old values, $v_{k}(s)$ , and one for the new values, $v_{k+1}(s)$ . This way, the new values can be computed one by one from the old values without the old values being changed. Of course it is easier to use one array and update the values “in place,” that is, with each new backed-up value immediately overwriting the old one. Then, depending on the order in which the states are backed up, sometimes new values are used instead of old ones on the right-hand side of (4.5). This slightly different algorithm also converges to $v_{\pi}$ ; in fact, it usually converges faster than the two-array version, as you might expect, since it uses new data as soon as they are available. We think of the backups as being done in a sweep through the state space. For the in-place algorithm, the order in which states are backed up during the sweep has a significant influence on the rate of convergence. We usually have the in-place version in mind when we think of DP algorithms. 

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/208c5870260790cbf9bca7dc6c715f4c4b7c3dfd157f032f5e63f0850b44ec8c.jpg) 

Figure 4.1: Iterative policy evaluation. 

Another implementation point concerns the termination of the algorithm. Formally, iterative policy evaluation converges only in the limit, but in practice it must be halted short of this. A typical stopping condition for iterative policy evaluation is to test the quantity max $s\in\mathcal{S}$ $|v_{k+1}(s)-v_{k}(s)|$ after each sweep and stop when it is sufficiently small. Figure 4.1 gives a complete algorithm for iterative policy evaluation with this stopping criterion. 

Example 4.1 Consider the $4\times4$ gridworld shown below. 

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/300aea38dffe36886d8d2e0f8392fc2ee231a9bad468e7e673a7b5e4608630a4.jpg) 

The nonterminal states are $\mathcal{S}=\{1,2,\ldots,14\}$ . There are four actions possible in each state, ${\mathcal{A}}={\mathfrak{f}}\mathfrak{u}\mathfrak{p}$ , down, right, left}, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged. Thus, for instance, $p(6|5,\mathtt{r i g h t})=1$ , $p(10|5,\mathrm{right})=0$ , and $p(7|7,\mathtt{r i g h t})=1$ . This is an undiscounted, episodic task. The reward is $-1$ on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus $r(s,a,s^{\prime})=-1$ for all states $s,s^{\prime}$ and actions $a$ . Suppose the agent follows the equiprobable random policy (all actions equally likely). The left side of Figure 4.2 shows the sequence of value functions $\{v_{k}\}$ computed by iterative policy evaluation. The final estimate is in fact $v_{\pi}$ , which in this case gives for each state the negation of the expected number of steps from that state until 

![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/f2d3f8357a59914825a4820d30e3c09de9e7bfc3935df37fa238caadf7a2f7bf.jpg) 

Figure 4.2: Convergence of iterative policy evaluation on a small gridworld. The left column is the sequence of approximations of the state-value function for the random policy (all actions equal). The right column is the sequence of greedy policies corresponding to the value function estimates (arrows are shown for all actions achieving the maximum). The last policy is guaranteed only to be an improvement over the random policy, but in this case it, and all policies after the third iteration, are optimal. 

termination. 

## 4.2 Policy Improvement 
Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function $v_{\pi}$ for an arbitrary deterministic policy $\pi$ . For some state $s$ we would like to know whether or not we should change the policy to deterministically choose an action $a\neq\pi(s)$ . We know how good it is to follow the current policy from $s$ —that is $v_{\pi}(s)$ —but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting $a$ in $s$ and thereafter following the existing policy, $\pi$ . The value of this way of behaving is 
$$
\begin{array}{r c l}{{q_{\pi}(s,a)}}&{{=}}&{{\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=a]}}\ {{}}&{{=}}&{{\displaystyle\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_{\pi}(s^{\prime})\Big].}}\end{array}
$$ 
The key criterion is whether this is greater than or less than $v_{\pi}(s)$ . If it is greater—that is, if it is better to select $a$ once in $s$ and thereafter follow $\pi$ than it would be to follow $\pi$ all the time—then one would expect it to be better still to select $a$ every time $s$ is encountered, and that the new policy would in fact be a better one overall. 
That this is true is a special case of a general result called the policy improvement theorem. Let $\pi$ and $\pi^{\prime}$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$ , 
$$
q_{\pi}(s,\pi^{\prime}(s))\geq v_{\pi}(s).
$$ 
Then the policy $\pi^{\prime}$ must be as good as, or better than, $\pi$ . That is, it must obtain greater or equal expected return from all states $s\in\mathcal{S}$ : 
$$
v_{\pi^{\prime}}(s)\geq v_{\pi}(s).
$$ 
Moreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8) at at least one state. This result applies in particular to the two policies that we considered in the previous paragraph, an original deterministic policy, $\pi$ , and a changed policy, $\pi^{\prime}$ , that is identical to $\pi$ except that $\pi^{\prime}(s)=a\neq\pi(s)$ . Obviously, (4.7) holds at all states other than $s$ . Thus, if $q_{\pi}(s,a)>v_{\pi}(s)$ , then the changed policy is indeed better than $\pi$ . 
The idea behind the proof of the policy improvement theorem is easy to understand. Starting from (4.7), we keep expanding the $q_{\pi}$ side and reapplying 
(4.7) until we get $v_{\pi^{\prime}}(s)$ : 
$$
\begin{array}{r l}{v_{\pi}(s)}&{\le q_{\pi}(s,\pi^{\prime}(s))}\ {}&{=~\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma v_{\pi}(S_{t+1})~|~S_{t}=s]}\ {}&{\le~\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma q_{\pi}(S_{t+1},\pi^{\prime}(S_{t+1}))~|~S_{t}=s]}\ {}&{=~\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma\mathbb{E}_{\pi^{\prime}}[R_{t+2}+\gamma v_{\pi}(S_{t+2})]~|~S_{t}=s]}\ {}&{=~\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}v_{\pi}(S_{t+2})~|~S_{t}=s]}\ {}&{\le~\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}v_{\pi}(S_{t+3})~|~S_{t}=s]}\ {}&{\vdots}\ {}&{\le~\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}+\cdots~|~S_{t}=s]}\ {}&{=~v_{\pi^{\prime}}(s).}\end{array}
$$ 
So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state to a particular action. It is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to $q_{\pi}(s,a)$ . In other words, to consider the new greedy policy, $\pi^{\prime}$ , given by 
$$
\begin{array}{r c l}{{\pi^{\prime}(s)}}&{{=}}&{{\arg\operatorname*{max}q_{\pi}(s,a)}}\ {{}}&{{=}}&{{\arg\operatorname*{max}\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=a]}}\ {{}}&{{=}}&{{\arg\operatorname*{max}\displaystyle\sum_{a}p(s^{\prime},r|s,a)\Big[r+\gamma v_{\pi}(s^{\prime})\Big],}}\end{array}
$$ 
where $\mathrm{arg}\operatorname*{max}_{a}$ denotes the value of $a$ at which the expression that follows is maximized (with ties broken arbitrarily). The greedy policy takes the action that looks best in the short term—after one step of lookahead—according to $v_{\pi}$ . By construction, the greedy policy meets the conditions of the policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement. 
Suppose the new greedy policy, $\pi^{\prime}$ , is as good as, but not better than, the old policy $\pi$ . Then $v_{\pi}=v_{\pi^{\prime}}$ , and from (4.9) it follows that for all $s\in\mathcal{S}$ : 
$$
\begin{array}{r c l}{\displaystyle v_{\pi^{\prime}}(s)}&{=}&{\displaystyle\operatorname*{max}_{a}\mathbb{E}[R_{t+1}+\gamma v_{\pi^{\prime}}(S_{t+1})\mid S_{t}=s,A_{t}=a]}\ &{=}&{\displaystyle\operatorname*{max}_{a}\sum_{s^{\prime},r}p(s^{\prime},r\vert s,a)\Big[r+\gamma v_{\pi^{\prime}}(s^{\prime})\Big].}\end{array}
$$ 
But this is the same as the Bellman optimality equation (4.1), and therefore, $v_{\pi^{\prime}}$ must be $v_{*}$ , and both $\pi$ and $\pi^{\prime}$ must be optimal policies. Policy improvement thus must give us a strictly better policy except when the original policy is already optimal. 
So far in this section we have considered the special case of deterministic policies. In the general case, a stochastic policy $\pi$ specifies probabilities, $\pi(\boldsymbol{a}|\boldsymbol{s})$ , for taking each action, $a$ , in each state, $s$ . We will not go through the details, but in fact all the ideas of this section extend easily to stochastic policies. In particular, the policy improvement theorem carries through as stated for the stochastic case, under the natural definition: 
$$
q_{\pi}(s,\pi^{\prime}(s))=\sum_{a}\pi^{\prime}(a|s)q_{\pi}(s,a).
$$ 
In addition, if there are ties in policy improvement steps such as (4.9)—that is, if there are several actions at which the maximum is achieved—then in the stochastic case we need not select a single action from among them. Instead, each maximizing action can be given a portion of the probability of being selected in the new greedy policy. Any apportioning scheme is allowed as long as all submaximal actions are given zero probability. 
The last row of Figure 4.2 shows an example of policy improvement for stochastic policies. Here the original policy, $\pi$ , is the equiprobable random policy, and the new policy, $\pi^{\prime}$ , is greedy with respect to $v_{\pi}$ . The value function $v_{\pi}$ is shown in the bottom-left diagram and the set of possible $\pi^{\prime}$ is shown in the bottom-right diagram. The states with multiple arrows in the $\pi^{\prime}$ diagram are those in which several actions achieve the maximum in (4.9); any apportionment of probability among these actions is permitted. The value function of any such policy, $v_{\pi^{\prime}}(s)$ , can be seen by inspection to be either $-1$ , $-2$ , or $^{-3}$ at all states, $s\in\mathcal{S}$ , whereas $v_{\pi}(s)$ is at most $-14$ . Thus, $v_{\pi^{\prime}}(s)\geq v_{\pi}(s)$ , for all $s\in\mathcal S$ , illustrating policy improvement. Although in this case the new policy $\pi^{\prime}$ happens to be optimal, in general only an improvement is guaranteed. 
## 4.3 Policy Iteration 
Once a policy, $\pi$ , has been improved using $v_{\pi}$ to yield a better policy, $\pi^{\prime}$ , we can then compute $v_{\pi^{\prime}}$ and improve it again to yield an even better $\pi^{\prime\prime}$ . We can thus obtain a sequence of monotonically improving policies and value functions: 
$$
\pi_{0}\stackrel{\mathrm{\tiny~E}}{\longrightarrow}v_{\pi_{0}}\stackrel{\mathrm{\tiny~I}}{\longrightarrow}\pi_{1}\stackrel{\mathrm{\tiny~E}}{\longrightarrow}v_{\pi_{1}}\stackrel{\mathrm{\tiny~I}}{\longrightarrow}\pi_{2}\stackrel{\mathrm{\tiny~E}}{\longrightarrow}\cdot\cdot\cdot\stackrel{\mathrm{\tiny~I}}{\longrightarrow}\pi_{*}\stackrel{\mathrm{\tiny~E}}{\longrightarrow}v_{*},
$$ 
where $\xrightarrow{\textrm{E}}$ denotes a policy evaluation and $\xrightarrow{\mathrm{~I~}}$ denotes a policy improvement. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations. 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/1267a494db94feb9dd11f824b3c14be757f29f81edfbf4d283a38589bdddcb31.jpg) 
Figure 4.3: Policy iteration (using iterative policy evaluation) for $v_{*}$ . This algorithm has a subtle bug, in that it may never terminate if the policy continually switches between two or more policies that are equally good. The bug can be fixed by adding additional flags, but it makes the pseudocode so ugly that it is not worth it. :-) 
This way of finding an optimal policy is called policy iteration. A complete algorithm is given in Figure 4.3. Note that each policy evaluation, itself an iterative computation, is started with the value function for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value function changes little from one policy to the next). 
Policy iteration often converges in surprisingly few iterations. This is illustrated by the example in Figure 4.2. The bottom-left diagram shows the value function for the equiprobable random policy, and the bottom-right diagram shows a greedy policy for this value function. The policy improvement theorem assures us that these policies are better than the original random policy. In this case, however, these policies are not just better, but optimal, proceeding to the terminal states in the minimum number of steps. In this example, policy iteration would find the optimal policy after just one iteration. 
Example 4.2: Jack’s Car Rental Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited $\$10$ by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of $\$2$ per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\textstyle{\frac{\lambda^{n}}{n!}}e^{-\lambda}$ , where $\lambda$ is the expected number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\gamma=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight. Figure 4.4 shows the sequence of policies found by policy iteration starting from the policy that never moves any cars. 
## 4.4 Value Iteration 
One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. Must we wait for exact convergence, or can we stop short of that? The example in Figure 4.2 certainly suggests that it may be possible to truncate policy evaluation. In that example, policy evaluation iterations beyond the first three have no effect on the corresponding greedy policy. 
In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called value iteration. It can be written as a particularly simple backup operation that combines the policy improvement and truncated policy evaluation steps: 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/047a1d185674055acd4d84601e4a2b12c3f9af7726b32f424df9dccfb211e0b7.jpg) 
Figure 4.4: The sequence of policies found by policy iteration on Jack’s car rental problem, and the final state-value function. The first five diagrams show, for each number of cars at each location at the end of the day, the number of cars to be moved from the first location to the second (negative numbers indicate transfers from the second location to the first). Each successive policy is a strict improvement over the previous policy, and the last policy is optimal. 
$$
\begin{array}{r c l}{{\displaystyle v_{k+1}(s)}}&{{=}}&{{\displaystyle\operatorname*{max}_{a}\mathbb{E}[R_{t+1}+\gamma v_{k}(S_{t+1})\mid S_{t}=s,A_{t}=a]}}\ {{}}&{{=}}&{{\displaystyle\operatorname*{max}_{a}\sum_{s^{\prime},r}p(s^{\prime},r\mid s,a)\Big[r+\gamma v_{k}(s^{\prime})\Big],}}\end{array}
$$ 
for all $s\in\mathcal{S}$ . For arbitrary $v_{0}$ , the sequence $\{v_{k}\}$ can be shown to converge to $v_{*}$ under the same conditions that guarantee the existence of $v_{*}$ . 
Another way of understanding value iteration is by reference to the Bellman optimality equation (4.1). Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule. Also note how the value iteration backup is identical to the policy evaluation backup (4.5) except that it requires the maximum to be taken over all actions. Another way of seeing this close relationship is to compare the backup diagrams for these algorithms: Figure 3.4a shows the backup diagram for policy evaluation and Figure 3.7a shows the backup diagram for value iteration. These two are the natural backup operations for computing $v_{\pi}$ and $v_{*}$ . 
Finally, let us consider how value iteration terminates. Like policy evaluation, value iteration formally requires an infinite number of iterations to converge exactly to $v_{*}$ . In practice, we stop once the value function changes by only a small amount in a sweep. Figure 4.5 gives a complete value iteration algorithm with this kind of termination condition. 
Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation backups and some of which use value iteration backups. Since the max operation in (4.10) is the only difference between these backups, this just means that the max operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs. 
Example 4.3: Gambler’s Problem A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of $\$100$ , or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, $s\in\{1,2,\ldots,99\}$ and the actions are stakes, $a\in\{0,1,\ldots,\operatorname*{min}(s,100-s)\}$ . The reward is zero on all transitions except those on which the gambler reaches his goal, when it is $+1$ . The state-value function then gives the probability of winning from each state. A policy is a mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_{h}$ denote the probability of the coin coming up heads. If $p_{h}$ is known, then the entire problem is known and it can be solved, for instance, by value iteration. Figure 4.6 shows the change in the value function over successive sweeps of value iteration, and the final policy found, for the case of $p_{h}=0.4$ . This policy is optimal, but not unique. In fact, there is a whole family of optimal policies, all corresponding to ties for the argmax action selection with respect to the optimal value function. Can you guess what the entire family looks like? ■ 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/6fce07fdd317278d43251337471be073a3aad6e43a28c6d58415899b2892bcb7.jpg) 
Figure 4.5: Value iteration. 
## 4.5 Asynchronous Dynamic Programming 
A major drawback to the DP methods that we have discussed so far is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very large, then even a single sweep can be prohibitively expensive. For example, the game of backgammon has over $10^{20}$ states. Even if we could perform the value iteration backup on a million states per second, it would take over a thousand years to complete a single sweep. 
Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. These algorithms back up the values of states in any order whatsoever, using whatever values of other states happen to be available. The values of some states may be backed up several times before the values of others are backed up once. To converge correctly, however, an asynchronous algorithm must continue to backup the values of all the states: it can’t ignore any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in selecting states to which backup operations are applied. 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/e1b8ccfe1c2958b76f442927c3265826bd4e6502ce5e9cb16cb16164091410a5.jpg) 
Figure 4.6: The solution to the gambler’s problem for $p_{h}=0.4$ . The upper graph shows the value function found by successive sweeps of value iteration. The lower graph shows the final policy. 
For example, one version of asynchronous value iteration backs up the value, in place, of only one state, $s_{k}$ , on each step, $k$ , using the value iteration backup (4.10). If $0\leq\gamma<1$ , asymptotic convergence to $v_{*}$ is guaranteed given only that all states occur in the sequence $\{s_{k}\}$ an infinite number of times (the sequence could even be stochastic). (In the undiscounted episodic case, it is possible that there are some orderings of backups that do not result in convergence, but it is relatively easy to avoid these.) Similarly, it is possible to intermix policy evaluation and value iteration backups to produce a kind of asynchronous truncated policy iteration. Although the details of this and other more unusual DP algorithms are beyond the scope of this book, it is clear that a few different backups form building blocks that can be used flexibly in a wide variety of sweepless DP algorithms. 
Of course, avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an algorithm does not need to get locked into any hopelessly long sweep before it can make progress improving a policy. We can try to take advantage of this flexibility by selecting the states to which we apply backups so as to improve the algorithm’s rate of progress. We can try to order the backups to let value information propagate from state to state in an efficient way. Some states may not need their values backed up as often as others. We might even try to skip backing up some states entirely if they are not relevant to optimal behavior. Some ideas for doing this are discussed in Chapter 8. 
Asynchronous algorithms also make it easier to intermix computation with real-time interaction. To solve a given MDP, we can run an iterative DP algorithm at the same time that an agent is actually experiencing the MDP. The agent’s experience can be used to determine the states to which the DP algorithm applies its backups. At the same time, the latest value and policy information from the DP algorithm can guide the agent’s decision-making. For example, we can apply backups to states as the agent visits them. This makes it possible to focus the DP algorithm’s backups onto parts of the state set that are most relevant to the agent. This kind of focusing is a repeated theme in reinforcement learning. 
## 4.6 Generalized Policy Iteration 
Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement). In policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary. In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement. In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even finer grain. In some cases a single state is updated in one process before returning to the other. As long as both processes continue to update all states, the ultimate result is typically the same—convergence to the optimal value function and an optimal policy. 
We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy. This overall schema for GPI is illustrated in Figure 4.7. 
It is easy to see that if both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation (4.1) holds, and thus that the policy and the value function are optimal. 
The evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically makes the value function incorrect for the changed policy, and making the value function consistent with the policy typically causes that policy no longer to be greedy. In the long run, however, these two processes interact to find a single joint solution: the optimal value function and an optimal policy. 
One might also think of the interaction between the evaluation and improvement processes in GPI in terms of two constraints or goals—for example, as two lines in two-dimensional space: 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/4e89034c6f9aa89953587a4d766b70f850e719e91dc8ef09671ee055937ce40e.jpg) 
Figure 4.7: Generalized policy iteration: Value and policy functions interact until they are optimal and thus consistent with each other. 
![](https://cdn-mineru.openxlab.org.cn/extract/1f83486c-03b4-4bfd-9cdf-2c61c53bbf89/6658a5d9925f65aa919223b8bd8781a6151e38e68c06a53664b2680257aae27c.jpg) 
Although the real geometry is much more complicated than this, the diagram suggests what happens in the real case. Each process drives the value function or policy toward one of the lines representing a solution to one of the two goals. The goals interact because the two lines are not orthogonal. Driving directly toward one goal causes some movement away from the other goal. Inevitably, however, the joint process is brought closer to the overall goal of optimality. The arrows in this diagram correspond to the behavior of policy iteration in that each takes the system all the way to achieving one of the two goals completely. In GPI one could also take smaller, incomplete steps toward each goal. In either case, the two processes together achieve the overall goal of optimality even though neither is attempting to achieve it directly. 
## 4.7 Efficiency of Dynamic Programming 
DP may not be practical for very large problems, but compared with other methods for solving MDPs, DP methods are actually quite efficient. If we ignore a few technical details, then the (worst case) time DP methods take to find an optimal policy is polynomial in the number of states and actions. If $n$ and $m$ denote the number of states and actions, this means that a DP method takes a number of computational operations that is less than some polynomial function of $n$ and $m$ . A DP method is guaranteed to find an optimal policy in polynomial time even though the total number of (deterministic) policies is $m^{n}$ . In this sense, DP is exponentially faster than any direct search in policy space could be, because direct search would have to exhaustively examine each policy to provide the same guarantee. Linear programming methods can also be used to solve MDPs, and in some cases their worst-case convergence guarantees are better than those of DP methods. But linear programming methods become impractical at a much smaller number of states than do DP methods (by a factor of about 100). For the largest problems, only DP methods are feasible. 
DP is sometimes thought to be of limited applicability because of the curse of dimensionality (Bellman, 1957a), the fact that the number of states often grows exponentially with the number of state variables. Large state sets do create difficulties, but these are inherent difficulties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming. 
In practice, DP methods can be used with today’s computers to solve MDPs with millions of states. Both policy iteration and value iteration are widely used, and it is not clear which, if either, is better in general. In practice, these methods usually converge much faster than their theoretical worst-case run times, particularly if they are started with good initial value functions or policies. 
On problems with large state spaces, asynchronous DP methods are often preferred. To complete even one sweep of a synchronous method requires computation and memory for every state. For some problems, even this much memory and computation is impractical, yet the problem is still potentially solvable because only a relatively few states occur along optimal solution trajectories. Asynchronous methods and other variations of GPI can be applied in such cases and may find good or optimal policies much faster than synchronous 
methods can. 
## 4.8 Summary 
In this chapter we have become familiar with the basic ideas and algorithms of dynamic programming as they relate to solving finite MDPs. Policy evaluation refers to the (typically) iterative computation of the value functions for a given policy. Policy improvement refers to the computation of an improved policy given the value function for that policy. Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods. Either of these can be used to reliably compute optimal policies and value functions for finite MDPs given complete knowledge of the MDP. 
Classical DP methods operate in sweeps through the state set, performing a full backup operation on each state. Each backup updates the value of one state based on the values of all possible successor states and their probabilities of occurring. Full backups are closely related to Bellman equations: they are little more than these equations turned into assignment statements. When the backups no longer result in any changes in value, convergence has occurred to values that satisfy the corresponding Bellman equation. Just as there are four primary value functions ( $v_{\pi}$ , $v_{*}$ , $q_{\pi}$ , and $q_{*}$ ), there are four corresponding Bellman equations and four corresponding full backups. An intuitive view of the operation of backups is given by backup diagrams. 
Insight into DP methods and, in fact, into almost all reinforcement learning methods, can be gained by viewing them as generalized policy iteration (GPI). GPI is the general idea of two interacting processes revolving around an approximate policy and an approximate value function. One process takes the policy as given and performs some form of policy evaluation, changing the value function to be more like the true value function for the policy. The other process takes the value function as given and performs some form of policy improvement, changing the policy to make it better, assuming that the value function is its value function. Although each process changes the basis for the other, overall they work together to find a joint solution: a policy and value function that are unchanged by either process and, consequently, are optimal. In some cases, GPI can be proved to converge, most notably for the classical DP methods that we have presented in this chapter. In other cases convergence has not been proved, but still the idea of GPI improves our understanding of the methods. 
It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous $D P$ methods are in-place iterative methods that back up states in an arbitrary order, perhaps stochastically determined and using out-of-date information. Many of these methods can be viewed as fine-grained forms of GPI. 
Finally, we note one last special property of DP methods. All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea bootstrapping. Many reinforcement learning methods perform bootstrapping, even those that do not require, as DP requires, a complete and accurate model of the environment. In the next chapter we explore reinforcement learning methods that do not require a model and do not bootstrap. In the chapter after that we explore methods that do not require a model but do bootstrap. These key features and properties are separable, yet can be mixed in interesting combinations. 
