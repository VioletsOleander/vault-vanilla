# Part 0
# CH1 Introduction
## 1.1 Mathmatical optimization
一个数学优化问题，或者说优化问题，有如下形式
$$\begin{align}
&minimize\quad f_0(x)\\
&subject\ to\quad f_i(x)\le b_i,i=1,\dots,m
\end{align}\tag{1.1}$$
向量$x = (x_1,\dots,x_m)$为问题的优化变量，函数$f_0:\mathbf R^n \to \mathbf R$为目标函数，函数$f_i:\mathbf R^n \to \mathbf R,i=1,\dots,m$为约束函数，当然常量$b_i,i=1,\dots,m$也是约束的一部分，称为约束的界

如果一个向量$x^*$满足在所有符合约束的向量中，它的目标函数值$f_0(x^*)$最小，则称它为原问题的一个解/最优解

如果问题1.1的目标函数和约束函数都是线性函数，即对所有的$x,y \in \mathbf R^n$和所有的$\alpha, \beta \in \mathbf R$，满足

$$f_i(\alpha x+\beta y)=\alpha f_i(x) + \beta f_i(y)\tag{1.2}$$

则称问题1.1是一个线性规划问题，该优化问题是线性的
否则，该优化问题是非线性的，那这就是一个非线性规划问题

本书讨论凸优化问题，即目标函数和约束函数都是凸函数的优化问题，即对所有的$x,y \in \mathbf R^n$和所有的$\alpha, \beta \in \mathbf R,\alpha + \beta = 1,\alpha \ge 0,\beta \ge 0$，满足

$$f_i(\alpha x+\beta y)\le\alpha f_i(x) + \beta f_i(y)\tag{1.3}$$

显然函数的凸性质比函数的线性性质更宽泛，条件不再是等式而是不等式，且不等式只需要对特定范围的$\alpha, \beta$成立即可
因此线性优化问题一定属于凸优化问题
### 1.1.1 Applications
### 1.1.2 Solving optimization problems
如果每个约束函数仅依赖于较少的变量，则问题是稀疏的(sparse)

对于一部分的优化问题类型，我们已经有在问题规模很大时也可以高效可靠地求解的算法，例如最小二乘问题和线性规划问题，以及凸优化问题
## 1.2 Least-squares and linear programming
两个最广泛运用的凸优化问题的子集问题是最小二乘问题和线性规划问题
### 1.2.1 Least-squares problems
最小二乘问题是没有约束(即 $m=0$)，且目标函数的形式是形式为 $(a_i^Tx-b_i)$ 的项的平方和的问题

$$minimize\quad f_0(x) = \|Ax-b\|_2^2=\sum_{i=1}^k(a_i^Tx-b_i)^2\tag{1.4}$$

其中$A \in \mathbf R^{k\times n}$($k\ge n$)，$a_i^T$为$A$的行，向量$x\in \mathbf R^n$是优化变量

**Solving least-squares problems**
对问题1.4的求解可以简化为求解一系列线性方程(linear equations)

$$(A^TA)x = A^Tb$$

因此解析解为$x = (A^TA)^{-1}A^Tb$
最小二乘问题可以在近似正比于$n^2k$的时间内解决

解最小二乘问题的算法已经足够可靠，可以用于嵌入式系统中

有些时候，通过利用系数矩阵$A$的一些特殊结构，例如稀疏性，我们可以在更少时间内解更大规模的最小二乘问题

求解最小二乘问题已经可以称为是一个成熟的技术(technology)

**Using least-squares**
最小二乘问题也有一系列统计解释，例如对向量$x$的最大似然估计

我们只要确定目标函数是一个二次函数，且相关的二次型是半正定的，就可以确认一个优化问题是最小二乘问题

一些方法也被提出用于提高最小二乘问题的灵活性
例如加权最小二乘问题中，目标函数为

$$\sum_{i=1}^kw_i(a_i^Tx-b_i)^2$$

其中 $w_1,\dots, w_k$ 为正数，这些权重用于反映项的相对重要性，在统计上，该问题对应各项的方差不同的情况
这个问题可以被转换为标准最小二乘问题然后求解

带正则项的最小二乘问题中，目标函数包含了额外的一个项

$$\sum_{i=1}^k(a_i^x-b_i)^2+\rho \sum_{i=1}^n x_i^2$$

其中$\rho > 0$，额外的正则项惩罚大的$x_i$项，参数$\rho$用于权衡两项，在统计上，该问题对应$x$从属于一个先验分布的情况
这个问题可以被转换为标准最小二乘问题然后求解
### 1.2.2 Linear programming
线性规划问题中，目标函数和所有的约束函数都是线性的
$$\begin{align}
&minimize\quad c^Tx\\
&subject\ to\quad a_i^Tx\le b_i,i=1,\dots,m
\end{align}\tag{1.5}$$
其中向量$c,a_1,\dots,a_m \in \mathbf R^n$，标量$b_1,\dots,b_m\in \mathbf R$为问题参数

**Solving linear programs**
不像最小二乘问题，线性规划问题并没有简单的解析公式，但仍存在有效的方法，例如Dantzig's simplex方法以及interior point方法
不像最小二乘问题，我们无法给出解一个线性规划问题需要的算术操作数，但内点方法可以针对给定的准确率，给出需要的操作数的严格的界(rigorous bound)

解线性规划问题已经可以说是一项成熟的技术，线性规划求解程序(solver)也广泛用于嵌入式

**Using linear programming**
形式1.5是线性规划的标准形式

考虑切比雪夫近似问题
$$minimize\quad \max_{i=1,\dots,k}|a_i^Tx - b_i|\tag{1.6}$$
$x\in \mathbf R^n$为变量，$a_1,\dots,a_k\in \mathbf R^n,b_1,\dots,b_k \in \mathbf R$为参数
该目标函数和最小二乘有些类似，都是对项$a_i^Tx - b_i$的规模的衡量，最小二乘中，我们使用平方和，而切比雪夫近似中，我们使用绝对值的最大值
另一个区别在于式1.6是不可微的

问题1.6可以通过解以下线性规划问题得到
$$\begin{align}
&minimize&t\\
&subject\ to& a_i^Tx-t\le b_i,i=1,\dots,k\\
&&-a_i^Tx-t\le -b_i,i=1,\dots,k
\end{align}\tag{1.7}$$
变量$x\in \mathbf R^n,t\in\mathbf R$
## 1.3 Convex optimization
凸优化问题的形式为
$$\begin{align}
&minimize\quad f_0(x)\\
&subject\ to\quad f_i(x)\le b_i,i=1,\dots,m
\end{align}\tag{1.8}$$
其中函数$f_1,\dots,f_m: \mathbf R^n \rightarrow \mathbf R$是凸的，即对所有的$x,y \in \mathbf R^n$和所有的$\alpha, \beta \in \mathbf R,\alpha + \beta = 1,\alpha \ge 0,\beta \ge 0$，满足
$$f_i(\alpha x+\beta y)\le\alpha f_i(x) + \beta f_i(y)$$
问题1.4和问题1.5都是问题1.8的特殊形式
### 1.3.1 Solving convex optimization problems
凸优化问题通常没有解析形式的公式，但也存在非常有效的求解算法，内点方法(interior-point methods)在实际中效果很好，并且在一些情况下被证明在运算数量不超过一个有关问题维度的多项式(a polynominal of the problem dimensions)的时候可以将问题解到指定的准确度

内点方法常常可以在10到100的迭代步数中解决问题1.8，忽视问题的特殊结构(例如稀疏性)，每一步需要$O(max\{n^3,n^2m,F\})$的运算数量，其中$F$是计算目标和约束函数$f_0,\dots, f_m$的第一阶导数和第二阶导数的开销

目前还不能说解广义的凸优化问题是一项成熟的技术，对于一些凸优化问题的子集，例如二阶锥(second-order cone)规划和几何(geometric)规划，可以说内点方法已经接近一项技术
### 1.3.2 Using convex optimization
可以说，如果我们能将一个实际问题建构(formulate)成一个凸优化问题，那我们就已经解决了原问题

因此本学科的技巧和挑战就在于用凸优化认识并建构问题
## 1.4 Nonlinear optimization
非线性优化/非线性规划用于描述目标函数或约束函数是非线性的，且并不确定是否是凸函数的优化问题
对于广义的非线性优化问题1.1，没有有效的解法
### 1.4.1 Local optimization
局部优化中，我们放弃寻找全局最优解，只寻找局部最优解，即在该点周围的可行的点内，它将目标函数最小化

局部优化方法快速且广泛应用，只要求目标函数和约束函数的可微性

局部优化方法需要猜测起始点，起始点的选择可以极大影响结果值，局部优化方法难以确定全局解和局部解的差距，局部优化方法常常对算法参数敏感

局部优化方法更像技巧而非技术(more art than technology)

局部优化方法仅要求函数的可微性，因此建构局部优化问题很容易，技巧在于求解，而凸优化则反之，技巧在于建构凸优化问题
### 1.4.2 Global optimization
全局优化中，我们需要寻找问题1.1的全局最优解，以效率为代价，全局优化算法的最坏情况复杂度相对问题大小$n,m$成比例增长

全局优化算法主要用于变量数较少的问题，此时问题的计算时间不重要，找到真正全局最优解的价值较高
### 1.4.3 Role of convex optimization in nonconvex problems
**Initialization for local optimization**
为了解决一个非凸优化问题，我们需要先建立一个对近似原问题的凸优化问题，此时我们不需要猜测初始点就可以解该近似问题，用该近似问题的解作为原问题的起始点

**Convex heuristics for nonconvex optimization**
凸优化是数个用于解决非凸优化问题的启发式算法的基础
例如找到一个满足一些约束的稀疏向量(即只有少量非零项的向量)，这是一个难的组合问题(combinatorial problem)，但存在基于凸优化的启发式算法，可以找到相对稀疏的解
又例如随机化算法(randomized algorithm)，该算法通过从一个概率分布中抽取一定数量的候选(candidates)，然后选择最优的一个作为原非凸问题的近似解，而假设我们已经参数化(parametrized)(例如均值和方差)了一族(family)我们会从中抽取候选的概率分布，那么问题在于哪个分布会给我们目标的最小期望值(smallest expetced value of the objective)，而这个问题就常常是凸优化问题

**Bounds for global optimization**
许多全局优化方法需要一个可以便捷计算的非凸问题最优解的下界，两个解决该问题的算法都基于凸优化，一个是松弛(relaxation)，即每个非凸约束都被替换为一个更松弛(looser)，但凸的约束，另一个是拉格朗日松弛(Lagrangian relaxation)，需要求解拉格朗日对偶问题，这是一个凸优化问题
## 1.5 Outline
## 1.6 Notation
# Part 1 Theory
# CH2 Convex sets
## 2.1 Affine and convex sets
### 2.1.1 Lines and line segments
$x_1 \ne x_2$是$\mathbf R^n$中的两个点，形式为如下所示的点
$$y = \theta x_1 + (1-\theta)x_2$$
其中$\theta \in \mathbf R$，构成了穿过$x_1, x_2$的一条线(line)，当参数值$\theta = 0$，对应$y = x_2$，当参数值$\theta = 1$，对应$y = x_1$，当$\theta$的值在$0$到$1$之间，对应的点形成了$x_1,x_2$之间的线段(line segment)
$y$也可以写成以下形式
$$y = x_2 + \theta(x_1-x_2)$$
因此可以从另一个角度解释，$y$是基点(base point)$x_2$(对应$\theta = 0$)以及方向(direction)$x_1-x_2$(从$x_2$指向$x_1$)的和，其中方向由参数$\theta$放缩，因此，$\theta$给出了$x_2$到$y$的距离是$x_2$到$x_1$的距离的比例，当$\theta$从$0$增长到$1$，$y$从$x_2$移动到$x_1$，当$\theta > 1$，$y$沿直线越过了$x_1$
### 2.1.2 Affine sets
一个集合 $C\subseteq \mathbf R^n$ 是仿射集(affine)，当仅当穿过 $C$ 中任意两个不同的点之间的线在 $C$ 中，即对任意 $x_1, x_2 \in C, \theta \in \mathbf R$，我们都有 $\theta x_1 + (1-\theta)x_2\in C$，
换句话说，$C$包含了$C$中任意两点的线性组合(条件是线性组合的系数和为$1$)

这可以被推广至多于两点的情况，我们称形式为 $\theta_1 x_1 + \cdots + \theta_k x_k$，其中 $\theta_1 + \cdots + \theta_k = 1$ 的一个点为点 $x_1,\dots, x_k$ 的仿射组合(affine combination)，因此，仿射集也可以定义为包含了集合中任意两点的仿射组合的集合

并且，基于此我们可以推出一个仿射集包含了集合中的点的所有仿射组合，即：
若 $C$ 是一个仿射集，$x_1,\dots, x_k \in C$，且 $\theta_1 + \dots + \theta_k = 1$，则点 $\theta_1x_1 + \cdots + \theta_kx_k$ 仍属于集合 $C$
(
推导：
已知 $x_1,\dots, x_k \in C$，$C$ 是一个仿射集
对于任意 $\theta_{1}\in \mathbf R$，有 $\theta_1 x_1 + (1-\theta_1) x_2 \in C$
对于任意 $\theta_{2}\in \mathbf R$ ，有 $\theta_2 x_2 + (1-\theta_2) x_3 \in C$
则对于任意 $\theta_1,\theta_{2}\in \mathbf R$，有 $\theta_1 x_1 + (1-\theta_1)(\theta_2 x_2 + (1-\theta_2) x_3) \in C$，即
$\theta_1 x_1 + (1-\theta_1)(\theta_2) x_2 + (1-\theta_1)(1-\theta_2) x_3 \in C$

由此依次构造，我们可以得到：
对于任意 $\theta_1,\theta_2,\dots,\theta_k \in \mathbf R$，有
$\theta_1 x_1 + (1-\theta_1)\theta_2 x_2 + (1-\theta_1)(1-\theta_2)\theta_3 x_3 + \cdots + (1-\theta_1)\cdots (1-\theta_{k-1}) x_k \in C$
且满足 $\alpha_1 + \cdots + \alpha_k = 1$
由于 $\theta_1,\dots,\theta_k$ 的任意性，显然该序列构成了所有的 $x_1,\dots, x_k$ 的仿射组合，而它们都属于集合 $C$，证毕
)

若$C$是仿射集且$x_0\in C$，则集合
$$V = C-x_0=\{x-x_0|x\in C\}$$
是一个子空间(subspace)，即该集合对加法和标量乘法(scalar multiplication)封闭

要证明，我们假设有$v_1, v_2 \in V$，且$\alpha, \beta \in \mathbf R$，那么我们显然有$v_1 + x_0 \in C$和$v_2 + x_0 \in C$，那么根据$C$是仿射集的性质，就有
$$\alpha v_1 + \beta v_2 + x_0 = \alpha(v_1 + x_0) + \beta (v_2 + x_0)+(1-\alpha-\beta)x_0\in C$$
那么就有$\alpha v_1 + \beta v_2 \in C - x_0 = V$

因此仿射集$C$也可以表示成
$$C = V + x_0=\{v+x_0|v\in V\}$$
即一个子空间加上一个偏移(offset)
而和$C$有关的子空间$V$并不受对$x_0$的选择而决定，因此$x_0$可以选为$C$中的任意一点
我们定义仿射集$C$的维度(dimension)为子空间$V = C - x_0$的维度，其中$x_0$是$C$中的任意一点

**例2.1** 线性方程组的解集
一个线性方程组的解集$C = \{x | Ax = b\}$，其中$A\in \mathbf R^{m\times n},b\in \mathbf R^m$，是一个仿射集

要证明，我们设$x_1, x_2\in C$，即$Ax_1 = b, Ax_2 = b$，则对任意$\theta$，有
$$A(\theta x_1 + (1-\theta)x_2) = \theta Ax_1 + (1-\theta)Ax_2 = b$$
即仿射组合$\theta x_1 + (1-\theta)x_2$仍属于$C$
与该仿射集相关联的子空间是$A$的零空间

这个事实相反过来也是对的，即任意一个仿射集都可以表示成一个线性方程组的解集

某个集合$C\subseteq \mathbf R^{n}$中的所有仿射组合称为$C$的仿射包(affine hull)，记作$\mathbf {aff}\ C$
$$\mathbf {aff}\ C = \{\theta_1x_1+\cdots+\theta_kx_k|x_1,\dots,x_k\in C,\theta_1+\cdots+\theta_k=1\}$$
$C$的仿射闭包即最小的包含$C$的仿射集，即如果$S$是任意仿射集，且$C\subseteq S$，那么一定有$\mathbf {aff}\ C \subseteq S$
### 2.1.3 Affine dimension and relative interior
我们定义一个集合的仿射维度(affine dimension)为它的仿射闭包的维度，注意仿射维度并不总是和其他对维度(dimension)的定义一致，例如，考虑$\mathbf R^2$中的单位圆$\{x\in \mathbf R^2| x_1^2 + x_2^2 = 1\}$，它的仿射闭包是$\mathbf R^2$，因此它的仿射维度是$2$，但在多数的维度定义中，$\mathbf R^2$中的单位圆的维度是$1$

若集合 $C\subseteq \mathbf R^n$ 的仿射维度少于 $n$,，意味着该集合的仿射包 $\mathbf {aff}\ C \ne \mathbf R^n$

我们定义一个集合$C$的相对内部(relative interior)，记为$\mathbf {relint}\ C$，作为它相对于自己的仿射包$\mathbf {aff}\ C$的内部
$$\mathbf {relint}\ C=\{x\in C| B(x,r)\cap \mathbf {aff}\ C\subseteq C\ \text{for some}\ r > 0\}$$
其中$B(x,r) = \{y\ |\ \|y-x\|\le r\}$，即以点$x$为中心，$r$为半径的球体，两点之间的距离度量是范数$\|\cdot\|$(可以是任意范数，所有的范数定义了相同的相对内部)

我们继而定义集合$C$的相对边界(relative boundary)为$\mathbf {cl}\ C \backslash \mathbf {relint}\ C$，其中$\mathbf {cl}\ C$是$C$的闭包(closure)

**例2.2** 考虑$\mathbf R^3$中$(x_1,x_2)$平面上的一个正方形，定义为
$$C = \{x\in\mathbf R^3|-1\le x_1\le1,-1\le x_2 \le 1,x_3=0\}$$
它的仿射包是$(x_1,x_2)$平面，即$\mathbf {aff}\ C = \{x\in \mathbf R^3 | x_3 = 0\}$，$C$的内部是空集，但它的相对内部是
$$\mathbf {relint}\ C=\{x\in\mathbf R^3| -1 < x_1 < 1, -1 < x_2 < 1, x_3 = 0\}$$
$C$在$\mathbf R^3$中的边界(boundary)是它自己，它的相对边界是一个线框轮廓线(wire-frame outline)
$$\{x\in\mathbf R^3|\max\{|x_1|,|x_2|\}=1,x_3=0\}$$

>闭包(closure)
>一个[拓扑空间](https://zh.wikipedia.org/wiki/%E6%8B%93%E6%89%91%E7%A9%BA%E9%97%B4 "拓扑空间")里，子集$S$的闭包是由$S$的所有点及$S$的[极限点](https://zh.wikipedia.org/wiki/%E6%A5%B5%E9%99%90%E9%BB%9E "极限点")所组成的一个
>集合，直观上来说即为所有“靠近”$S$的点所组成的集合

> 内部(interior)
> 拓扑空间内子集合$S$的“内部”定义为：所有$S$的开子集的并集，直观上可以想成“不在$S$的边界上”的$S$的点组成的集合，$S$的内部中的点称为$S$的内点
> 一个集合的外部(exterior)是它补集的内部，等同于它闭包的补集，它包含既不在集合内，也不在边界上的点
> 一个子集的内部、边界和外部一同将整个空间分为三块(或者更少，因为这三者有可能是空集)，内部和外部总是开的，而边界总是闭的，没有内部的集合叫做边缘集(boundary set)

> 边界(boundary)
> 拓扑空间$X$的子集$S$的边界是从$S$和从$S$的[外部](https://zh.wikipedia.org/wiki/%E5%A4%96%E9%83%A8 "外部")都可以接近的点的集合，更严格的说，它是属于$S$的闭包但不是$S$的[内点](https://zh.wikipedia.org/wiki/%E5%86%85%E9%83%A8 "内部")的所有点的集合
### 2.1.4 Convex sets
对于一个集合$C$，如果集合$C$中的任意两点之间的线段也落在集合$C$中，则该集合是凸的(convex)，即对于任意$x_1,x_2\in C$，和任意$\theta$，满足$0\le \theta \le 1$，我们有
$$\theta x_1 +(1-\theta)x_2\in C$$
粗略地说，如果一个集合是凸的，则集合中的任意点都可以被集合内的其他所有点看到，沿着它们之间的是一条畅通无阻(unobstructed)的直线，在那里畅通无阻意思即落在集合中

显然所有的仿射集都是凸集，仿射集包含了集合中两个点之间的整条直线，自然也包括了线段
下图展示了$\mathbf R^2$中的凸集和非凸集
![[Convex Optimization-Fig2.2.png]]

我们称形式为$\theta_1 x_1 + \cdots + \theta_k x_k$，其中$\theta_1 + \cdots + \theta_k = 1$且$\theta_i \ge 0,i=1,\dots,k$的一个点为点$x_1,\dots, x_k$的凸组合(convex combination)，因此，凸集也可以定义为定义为包含了集合中任意两点的凸组合的集合
并且，基于此我们可以推出一个凸集包含了集合中的点的所有凸组合，即：
若$C$是一个凸集，$x_1,\dots, x_k \in C$，且$\theta_1 + \dots + \theta_k = 1$，$\theta_i\ge 0,i=1,\dots,k$，则点$\theta_1x_1 + \cdots + \theta_kx_k$仍属于集合$C$

一个形式为凸结合的点可以认为是多个点的混合(mixture)或加权平均(weighted average)，而$\theta_i$就是点$x_i$在混合中所占的比例

集合$C$的凸包(convex hull)，记为$\mathbf {conv}\ C$，即$C$中点的所有凸组合构成的集合
$$\mathbf {conv}\ C=\{\theta_1x_1+\cdots+\theta_k x_k|x_i\in C,\theta_i\ge0,i=1,\dots,k,\theta_1+\cdots+\theta_k=1\}$$
集合$C$的凸包$\mathbf {conv}\ C$一定是凸集，且是包含$C$的最小的凸集，换句话说，如果$B$是任意包含$C$的凸集，则有$\mathbf {conv}\ C\subseteq B$
下图展示了$\mathbf R^2$中的凸包
![[Convex Optimization-Fig2.3.png]]

凸组合的概念可以推广到无穷和、积分，以及最一般形式的概率分布
假设$\theta_1,\theta_2,\dots$满足
$$\theta_i\ge0,i=1,2,\dots,\quad\sum_{i=1}^{\infty}\theta_i=1$$
且$x_1,x_2,\dots \in C$，其中$C\subseteq \mathbf R^n$是凸集，则如果级数收敛，我们有
$$\sum_{i=1}^{\infty}\theta_ix_i\in C$$
更一般地说，假设$p:\mathbf R^n \rightarrow \mathbf R$对所有$x\in C$满足$p(x)\ge 0$，且$\int_C p(x)\ dx = 1$，其中$C\subseteq \mathbf R^n$是凸集，则如果积分存在，我们有
$$\int_Cp(x)x\ dx\in C$$

最一般地说，假设$C\subseteq \mathbf R^n$是凸集，且$x$是随机向量，满足事件$x\in C$的概率是$1$，则有$\mathbf E\ x\in C$，实际上其他的形式都是该形式的特例，例如假设随机变量$x$只有两种可能取值$x_1,x_2$，且$\mathbf {prob}(x=x_1) = \theta$，$\mathbf {prob}(x=x_2) = 1-\theta$，其中$0\le \theta \le 1$，则我们显然有$\mathbf {E}\ x = \theta x_1 + (1-\theta)x_2 \in C$，即两个点的简单凸组合
### 2.1.5 Cones
对于一个集合$C$，如果对于所有$x\in C$且$\theta \ge 0$，我们有$\theta x \in C$，则称该集合是一个锥(cone)或非负其次的(nonnegative homogeneous)
一个集合是凸锥(convex cone)当且仅当它即是锥也是凸的，也就意味着对于任意$x_1,x_2\in C$和$\theta_1,\theta_2\ge 0$，我们有
$$\theta_1x_1+\theta_2x_2\in C$$
这种形式的点可以几何地描述为形成二维的饼状切片，顶点为$0$，边缘通过$x_1$和$x_2$
![[Convex Optimization-Fig2.4.png]]
形式为$\theta_1 x_1 + \cdots + \theta_k x_k$，其中$\theta_1,\dots,\theta_k \ge 0$的点可以称为是点$x_1,\dots,x_k$的锥组合(conic combination)或非负线性组合(nonnegative linear combination)
若$x_i$含于一个凸锥$C$，则$x_i$的所有锥组合都位于$C$，反过来说，一个集合是凸锥当且仅当它包含了集合中点的所有锥组合
类似仿射组合和凸组合，锥组合也可以推广到无穷和和积分

集合$C$的锥包(conic hull)，即$C$中点的所有锥组合构成的集合
$$\{\theta_1x_1+\cdots+\theta_k x_k|x_i\in C,\theta_i\ge 0,i=1,\dots,k\}$$
$C$的锥包也是包含集合$C$的最小的凸锥
![[Convex Optimization-Fig2.5.png]]
## 2.2 Some important examples
本节介绍一些将会在之后常见的凸集
- 空集$\emptyset$、任意的单个点(即单元素集合)$\{x_0\}$，以及整个空间$\mathbf R^n$都是$\mathbf R^n$的仿射子集(因此也是凸集)
- 任意的线都是仿射集，若它穿过零点，则是一个子空间(subspace)(因此也是一个凸锥)
- 线段是凸集，但不是仿射集(除非它缩小到一个点)
- 一条射线(ray)，即形式为$\{x_0 + \theta v\ |\ \theta \ge 0\}$(其中$v\ne 0$)的点集，是凸集，但不是仿射集，如果基点$x_0$是$0$，则是一个凸锥
- 任意子空间都是仿射集，且也是凸锥(因此也是凸集)
### 2.2.1 Hyperplanes and halfspaces
一个超平面是形式为$$\{x\ |\ a^Tx=b\}$$的集合，其中$a\in \mathbf R^n, a\ne 0$且$b\in \mathbf R$，从解析上看(analytically)，它是一个关于$x$的分量的非平凡的线性方程组的解集(the solution set of a nontrivial linear equation among the components of x)(因此是一个仿射集)，从几何上看(geometrically)，超平面$\{x\ | \ a^Tx = b\}$可以解释为和给定向量$a$具有常数内积的点的集合，或者认为是一个法向量(normal vector)为$a$，相对于原点的偏移量是$b\in \mathbf R$的超平面

从几何解释出发，可以将超平面写成以下形式
$$\{x\ |\ a^T(x-x_0)=0\}$$
其中$x_0$就是超平面上的任意一点(即满足$a^Tx = b$的任一点)，我们将这种表示继而写为
$$\{x\ |\ a^T(x-x_0)=0\}=x_0+a^{\perp}$$
其中$a^{\perp}$表示$a$的正交补(orthogonal eomplement)(即所有与$a$正交的向量构成的集合)
$$a^{\perp}=\{v\ |\ a^Tv=0\}$$
这种形式的表示说明了超平面由一个偏移$x_0$加上所有与法向量$a$正交的向量构成，例如图2.6所示
![[Convex Optimization-Fig2.6.png]]

一个超平面将$\mathbf R^n$分割成两个半空间(halfspaces)，一个(闭的/closed)半空间是形式如下的集合
$$\{x\ |\ a^Tx \le b\}\tag{2.1}$$
其中$a\ne 0$，即一个非平凡的线性不等式组的解集，半空间是凸的，但不是仿射的，如图2.7所示
![[Convex Optimization-Fig2.7.png]]

半空间(2.1)也可以被表示为
$$\{x\ |\ a^T(x-x_0)\le 0\}\tag{2.2}$$
其中$x_0$是在相关超平面上的任意一点，即满足$a^Tx=b$的点，表示(2.2)表明了半空间的一个简单的几何解释：半空间是由向量$x_0$加上任何可以与外向法向量(outward normal vector)$a$呈钝角(或直角)的向量组成的，如图2.8所示
![[Convex Optimization-Fig2.8.png]]

半空间(2.1)的边界就是超平面$\{x\ |\ a^Tx= b\}$，集合$\{x\ |\ a^Tx < b\}$，即半空间$\{x\ |\ a^Tx \le b\}$的内部，称为开半空间(open halfspace)
### 2.2.2 Euclidean balls and ellipsoids
A (Euclidean) ball (or just ball) in $\mathbf{R}^{n}$ has the form 
> $\mathbf R^n$ 中的欧式球/球

$$
B(x_{c},r) = \{x\mid||x-x_c||_{2}\le r\} = \{x \mid (x-x_c)^{T}(x-x_{c})\le r^2\}
$$

where $r\,>\,0$ , and $||\cdot||_{2}$ denotes the Euclidean norm, i.e. , $\|\boldsymbol{u}\|_{2}=(\boldsymbol{u}^{T}\boldsymbol{u})^{1/2}$ . The vector $x_{c}$ is the center of the ball and the scalar $r$ is its radius; $B({x}_{c},r)$ consists of all points within a distance $r$ of the center $x_{c}$ . 
> 其中 $r>0$，$\|\cdot\|_2$ 表示欧式范数
> 向量 $x_c$ 为球心，标量 $r$ 为半径
> 球包含了所有到达中心 $x_c$ 的距离小于等于 $r$ 的点

Another common representation for the Euclidean ball is 
> 球的另一种常用表示

$$
B(x_{c},r)=\{x_{c}+r u\mid\|u\|_{2}\leq1\}.
$$ 
Euclidean ball is a convex set: if $\|x_{1}\,-\,x_{c}\|_{2}\;\leq\;r$ , $\|x_{2}\,-\,x_{c}\|_{2}\;\leq\;r$ , and $0\leq\theta\leq1$, then 

$$
\begin{array}{l c l}{\|\theta x_{1}+(1-\theta)x_{2}-x_{c}\|_{2}}&{=}&{\|\theta(x_{1}-x_{c})+(1-\theta)(x_{2}-x_{c})\|_{2}}\\ &{\leq}&{\theta\|x_{1}-x_{c}\|_{2}+(1-\theta)\|x_{2}-x_{c}\|_{2}}\\ &{\leq}&{r.}\end{array}
$$ 

(Here we use the homogeneity property and triangle inequality for $\lVert\cdot\rVert_{2}$ ; see §A.1.2 .) 
> 这里利用了欧式范数/二范数的齐次性: $\|\alpha \mathbf x\|_{2} = |\alpha|\|\mathbf x\|_2$，其中 $\alpha$ 是标量
> 以及欧式范数/二范数的三角不等式: $\|\mathbf a + \mathbf b\|_{2}\le \|\mathbf a\|_{2}+ \|\mathbf b\|_{2}$，三角不等式可以简单理解为从一个点出发到达另一个点的最短距离是直线距离，而不是绕道经过第三个点的距离 (三角不等式的证明思路即根据二范数的定义将它平方展开)

![[Convex Optimization-Fig2.9.png]]

A related family of convex sets is the ellipsoids , which have the form 
> 椭球体也是凸集

$$
{\mathcal{E}}=\{x\mid(x-x_{c})^{T}P^{-1}(x-x_{c})\leq1\},\tag{2.3}
$$ 
where $P=P^{T}\succ0$ , i.e. , $P$ is symmetric and positive definite. 
> 其中矩阵 $P$ 是对称的且正定的

The vector $x_{c}\in\mathbf{R}^{n}$ is the center of the ellipsoid. The matrix $P$ determines how far the ellipsoid extends in every direction from $x_{c}$ ; the lengths of the semi-axes of $\mathcal{E}$ are given by $\sqrt{\lambda_{i}}$ , where $\lambda_{i}$ are the eigenvalues of $P$.
> 向量 $x_{c}\in \mathbf R^{n}$ 是椭球的中心
> 矩阵 $P$ 决定了椭球在各个方向从中心 $x_c$ 延伸出去的距离
> 椭球的半轴的长度是 $\sqrt \lambda_i$ 其中 $\lambda_i$ 是矩阵 $P$ 的特征值

A ball is an ellipsoid with $P=r^{2}I$ . Figure 2.9 shows an ellipsoid in $\mathbf{R}^{2}$ . 
> 当 $P = r^2I$ 时，椭球就是球

Another common representation of an ellipsoid is 

$$
{\mathcal{E}}=\{x_{c}+A u\mid\|u\|_{2}\leq1\},\tag{2.4}
$$ 
where $A$ is square and nonsingular. 
> 其中 $A$ 是方阵且非奇异 (可逆)

In this representation we can assume without loss of generality that $A$ is symmetric and positive definite. By taking $A=P^{1/2}$ , this representation gives the ellipsoid defined in ( 2.3 ). 

When the matrix $A$ in ( 2.4 ) is symmetric positive semidefinite but singular, the set in ( 2.4 ) is called a degenerate ellipsoid ; its affine dimension is equal to the rank of $A$ . Degenerate ellipsoids are also convex. 
> 如果矩阵 $A$ 是对称半正定但是奇异，则集合 2.4 称为退化的椭球体，它的仿射维度等于矩阵 $A$ 的秩
> 退化的椭球体也是凸集
### 2.2.3 Norm balls and norm cones 
Suppose $\lVert\cdot\rVert$ is any norm on $\mathbf{R}^{n}$ (see § A.1.2 ). 

From the general properties of norms it can be shown that a norm ball of radius $r$ and center $x_{c}$ , given by $\{x\mid\|x\!-\!x_{c}\|\leq r\}$ , is convex. 
> 将二范数延伸，$\mathbf R^n$ 中任意范数定义的球体都是凸的

The norm cone associated with the norm $||\cdot||$ is the set 
> 范数锥：满足 $x$ 的范数不大于 $t$ 
$$
C=\{(x,t)\mid\|x\|\leq t\}\subseteq\mathbf{R}^{n+1}.
$$ 
It is (as the name suggests) a convex cone. 


**Example 2.3**
The second-order cone is the norm cone for the Euclidean norm, i.e. , 
> 二阶锥即使用欧式范数/二范数的锥

$$
\begin{array}{r c l}{C}&{=}&{\{(x,t)\in\mathbf{R}^{n+1}\mid\|x\|_{2}\leq t\}}\\ &{=}&{\left\{\left[\begin{array}{l}{x}\\ {t}\end{array}\right]\mid\left[\begin{array}{l}{x}\\ {t}\end{array}\right]^{T}\left[\begin{array}{l l}{I}&{0}\\ {0}&{-1}\end{array}\right]\left[\begin{array}{l}{x}\\ {t}\end{array}\right]\leq0,\ t\geq0\right\}.}\end{array}
$$ 
The second-order cone is also known by several other names. It is called the quadratic cone , since it is defined by a quadratic inequality. It is also called the Lorentz cone or ice-cream cone . 
> 二阶锥可以称为二次锥、洛伦茨锥、ice-cream 锥

Figure 2.10 shows the second-order cone in $\mathbf{R}^{3}$ . 

![[Convex Optimization-Fig2.10.png]]

### 2.2.4 Polyhedra 
A polyhedron is defined as the solution set of a finite number of linear equalities and inequalities: 
> 多面体定义为有限数量的线性等式和不等式的解集

$$
\begin{array}{r}{\mathcal{P}=\{x\mid a_{j}^{T}x\leq b_{j},\ j=1,\ldots,m,\ c_{j}^{T}x=d_{j},\ j=1,\ldots,p\}.}\end{array}\tag{2.5}
$$ 
A polyhedron is thus the intersection of a finite number of halfspaces and hyper-planes. 
> 多面体因此是有限数量的半空间和超平面的交集

Affine sets ( e.g. , subspaces, hyperplanes, lines), rays, line segments, and halfspaces are all polyhedra. It is easily shown that polyhedra are convex sets.
> 仿射集、射线、线段、半空间都是多面体
> 多面体是凸集

A bounded polyhedron is sometimes called a polytope , but some authors use the opposite convention ( i.e. , polytope for any set of the form ( 2.5 ), and polyhedron when it is bounded). 
> 有界的多面体称为多胞形

Figure 2.11 shows an example of a polyhedron defined as the intersection of five halfspaces. 


![[Convex Optimization-Fig2.11.png]]

It will be convenient to use the compact notation 

$$
{\mathcal{P}}=\{x\mid A x\preceq b,\ C x=d\}\tag{2.6}
$$ 
for ( 2.5 ), where 

$$
A=\left[\begin{array}{c}{a_{1}^{T}}\\ {\vdots}\\ {a_{m}^{T}}\end{array}\right],\qquad C=\left[\begin{array}{c}{c_{1}^{T}}\\ {\vdots}\\ {c_{p}^{T}}\end{array}\right],
$$ 

and the symbol $\preceq$ denotes vector inequality or componentwise inequality in $\mathbf{R}^{m}$ : $u\preceq v$ means $u_{i}\leq v_{i}$ for $i=1,\ldots,m$ . 
> $\preceq$ 表示向量间的逐元素比较


**Example 2.4** 
The nonnegative orthant (非负象限) is the set of points with nonnegative components, i.e. , 
> 非负象限即具有非负元素的点的集合

$$
\mathbf{R}_{+}^{n}=\{x\in\mathbf{R}^{n}\mid x_{i}\geq0,\ i=1,.\,.\,,n\}=\{x\in\mathbf{R}^{n}\mid x\succeq0\}.
$$ 
(Here $\mathbf{R}_{+}$ denotes the set of nonnegative numbers: $\mathbf{R}_{+}=\{x\in\mathbf{R}\mid x\geq0\}$ .) The nonnegative orthant is a polyhedron and a cone (and therefore called a polyhedral cone ). 
> 非负象限是多面体也是锥，称为多面锥

##### Simplexes 
Simplexes are another important family of polyhedra. 
Suppose the $k+1$ points $v_{0},.\,.\,.\,,v_{k}\ \in\ \mathbf{R}^{n}$ are affinely independent , which means $v_{1}\,-\,v_{0},.\,.\,.\,,v_{k}\,-\,v_{0}$ are linearly independent. 
> 当 $v_{0},\dots, v_{k}$ 是仿射无关的时， $v_1-v_{0},\dots, v_{k} - v_{0}$ 是线性无关的

The simplex determined by them is given by 

$$
C={\bf c o n v}\{v_{0},.\,.\,.\,,v_{k}\}=\{\theta_{0}v_{0}+\cdot\cdot\cdot+\theta_{k}v_{k}\mid\theta\succeq0,\ {\bf1}^{T}\theta=1\},\tag{2.7}
$$ 
where $\mathbf 1$ denotes the vector with all entries one. The affine dimension of this simplex is $k$ , so it is sometimes referred to as a $k$ -dimensional simplex in $\mathbf{R}^{n}$ .
> $k$ 维单纯形是包含了 $k$ 个仿射无关的点的点集的凸包，$k$ 维单纯形的仿射维度是 $k$


>线性无关 (Linear Independence)
>线性无关是指一组向量中没有一个向量可以表示为其余向量的线性组合
>具体来说，如果有一组线性无关的向量 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$，则对于所有的标量 $c_1, c_2, \ldots, c_n$，如果

 $$
 c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n = \mathbf{0}
 $$

> 就有 $c_1 = c_2 = \cdots = c_n = 0$，即除了全为零的情况外，没有任何一组非零系数可以使这些向量的线性组合为零向量
> 因此，线性无关向量组 $\mathbf v_{1},\dots ,\mathbf v_n$ 中的任何一个向量都不能表示为其余向量的线性组合
> 线性无关的向量组可以形成一个向量空间的基，这意味着它们可以唯一地表示该空间中的每一个向量。

> 仿射无关 (Affine Independence)
> 仿射无关是指一组点不在同一超平面上，在数学中，若一组点 $\mathbf{p}_1, \mathbf{p}_2, \ldots, \mathbf{p}_{n+1}$ 仿射无关，则说明它们中没有一个点可以表示为其余点的仿射组合
> 具体来说，对于满足 $a_1 + a_2 + \cdots + a_{n+1} = 1$ 的所有标量 $a_1, a_2, \ldots, a_{n+1}$，如果

$$a_1 \mathbf{p}_1 + a_2 \mathbf{p}_2 + \cdots + a_{n+1} \mathbf{p}_{n+1} = \mathbf{p}_i$$

> 就有 $a_1 = a_2 = \cdots = a_{i-1} = 0, a_i = 1, a_{i+1} = \cdots = a_{n+1} = 0$，即除了其中一个系数为1且其余系数为0的情况外，没有任何一组非零系数可以使这些点的仿射组合等于其中一个点
> 仿射无关的点可以确定一个仿射空间中的一个仿射子空间，例如，三个仿射无关的点可以确定一个平面（在三维空间中）

> 区别
>
> 1. **定义域**：
> - 线性无关涉及向量空间中的向量
> - 仿射无关涉及点集，这些点可以视为向量空间中的向量加上一个公共的平移向量
>
> 2. **组合类型**：
> - 线性无关考虑的是线性组合，即系数之和不必为1
> - 仿射无关考虑的是仿射组合，即系数之和必须为1
>
> 3. **几何意义**：
 >- 线性无关的向量可以形成一个向量空间的基
 >- 仿射无关的点可以形成一个仿射空间的基
>
> 4. **数量要求**：
> - 线性无关的向量可以任意数量，但仿射无关的点至少需要比所在空间的维数多一个（例如，在二维空间中至少需要三个点）


**Example 2.5** Some common simplexes. 
A 1-dimensional simplex is a line segment; a 2-dimensional simplex is a triangle (including its interior); and a 3-dimensional simplex is a tetrahedron. 

The *unit simplex* is the $n$ -dimensional simplex determined by the zero vector and the unit vectors, i.e. , 0 , $e_{1},.\,.\,.\,,e_{n}\,\in\,\mathbf{R}^{n}$ . It can be expressed as the set of vectors that satisfy 
> $n$ 维单元单纯形由零向量和 $n$ 个单位向量决定

$$
x\succeq0,\qquad\mathbf{1}^{T}x\leq1.
$$ 
The *probability simplex* is the $(n-1)$ -dimensional simplex determined by the unit vectors $e_{1},.\,.\,.\,,e_{n}\in\mathbf{R}^{n}$ . It is the set of vectors that satisfy 
> $n-1$ 维概率单纯形有 $n$ 个单位向量决定

$$
x\succeq0,\qquad\mathbf{1}^{T}x=1.
$$ 

Vectors in the probability simplex correspond to probability distributions on a set with $n$ elements, with $x_{i}$ interpreted as the probability of the i th element. 
> 概率单纯形中的向量对应于一个具有 $n$ 个元素的概率分布


To describe the simplex ( 2.7 ) as a polyhedron, i.e. , in the form ( 2.6 ), we proceed as follows. 
By definition, $x\in C$ if and only if $x=\theta_{0}v_{0}+\theta_{1}v_{1}+\cdot\cdot\cdot+\theta_{k}v_{k}$ for some $\theta\succeq0$ with $\mathbf{1}^{T}\boldsymbol{\theta}=1$ . 
> $x$ 是 $k$ 个仿射无关的点的凸组合

Equivalently, if we define $y=(\theta_{1},.\,.\,.\,,\theta_{k})$ and 

$$
B=\left[\begin{array}{l l l}{v_{1}-v_{0}}&{\cdot\cdot\cdot}&{v_{k}-v_{0}}\end{array}\right]\in\mathbf{R}^{n\times k},
$$ 
we can say that $x\in C$ if and only if 

$$
x=v_{0}+B y\tag{2.8}
$$ 
for some $y\,\succeq\,0$ with $\mathbf{1}^{T}y\ \leq\ 1$ . 
> 和之前完全等价的定义

Now we note that affine independence of the points $v_{0},\ldots,v_{k}$ implies that the matrix $B$ has rank $k$ . 
> 点之间仿射无关表明了 $B$ 列满秩

Therefore there exists a nonsingular matrix $A=\left(A_{1},A_{2}\right)\in\mathbf{R}^{n\times n}$ such that 

$$
A B=\left[\begin{array}{l}{A_{1}}\\ {A_{2}}\end{array}\right]B=\left[\begin{array}{l}{I}\\ {0}\end{array}\right].
$$ 
Multiplying ( 2.8 ) on the left with $A$ , we obtain 

$$
A_{1}x=A_{1}v_{0}+y,\qquad A_{2}x=A_{2}v_{0}.
$$ 
From this we see that $x\,\in\,C$ if and only if $A_{2}x~=~A_{2}v_{0}$ , an vector $y~=$ $A_{1}x-A_{1}v_{0}$ satisfies $y\succeq0$ and $\mathbf{1}^{T}y\leq1$ . 

In other words we have $x\in C$ if and only if 

$$
A_{2}x=A_{2}v_{0},\qquad A_{1}x\succeq A_{1}v_{0},\qquad\mathbf{1}^{T}A_{1}x\leq1+\mathbf{1}^{T}A_{1}v_{0},
$$ 
which is a set of linear equalities and inequalities in $x$ , and so describes a polyhedron. 
#### Convex hull description of polyhedra 
The convex hull of the finite set $\{v_{1},\ldots,v_{k}\}$ is 

$$
\mathbf{conv}\{v_{1},\dots,v_{k}\}=\{\theta_{1}v_{1}+\cdot\cdot\cdot+\theta_{k}v_{k}\mid\theta\succeq0,~\mathbf{1}^{T}\theta=1\}.
$$ 
This set is a polyhedron, and bounded, but (except in special cases, e.g. , a simplex) it is not simple to express it in the form ( 2.5 ), i.e. , by a set of linear equalities and inequalities. 
> 一个有限集合的凸包是多面体，并且有界，但是一般不容易将它表示为 (2.5) 的形式，即线性等式和不等式的集合

A generalization of this convex hull description is 

$$
\{\theta_{1}v_{1}+\cdot\cdot\cdot+\theta_{k}v_{k}\mid\theta_{1}+\cdot\cdot\cdot+\theta_{m}=1,~\theta_{i}\geq0,~i=1\} \tag{2.9}
$$ 
where $m\leq k$ . Here we consider nonnegative linear combinations of $v_{i}$ , but only the first $m$ coefficients are required to sum to one. 
> 考虑的仍然是 $v_i$ 的非负线性组合，但是仅要求前 $m$ 个系数的和是 1

Alternatively, we can interpret ( 2.9 ) as the convex hull of the points $v_{1},\dots,v_{m}$ , plus the conic hull of the points $v_{m+1},.\cdot\cdot,v_{k}$ . The set ( 2.9 ) defines a polyhedron, and conversely, every polyhedron can be represented in this form (although we will not show this). 
> 可以将式 (2.9) 解释为点 $v_1,\dots,v_m$ 的凸包加上点 $v_{m+1}, \dots, v_k$ 的锥包
> 集合 (2.9) 定义了一个多面体，并且每个多面体都可以写成集合 (2.9) 的形式

The question of how a polyhedron is represented is subtle, and has very im- portant practical consequences. As a simple example consider the unit ball in the $\ell_{\infty}$ -norm in $\mathbf{R}^{n}$ , 

$$
C=\{x\mid|x_{i}|\leq1,\ i=1,.\,.\,.\,,n\}.
$$ 
The set $C$ can be described in the form ( 2.5 ) with $2n$ linear inequalities $\pm e_{i}^{T}x\leq1$ , where $e_{i}$ is the $i$ th unit vector. 
To describe it in the convex hull form ( 2.9 ) requires at least $2^{n}$ points: 

$$
C=\mathbf{conv}\{v_{1},.\,.\,.\,,v_{2^{n}}\},
$$ 
where $v_{1},\dots,v_{2^{n}}$ are the $2^{n}$ vectors all of whose components are $1$ or $-1$ . Thus the size of the two descriptions diﬀers greatly, for large $n$ . 
### 2.2.5 The positive semidefinite cone 
We use the notation $\mathbf{S}^{n}$ to denote the set of symmetric $n\times n$ matrices, 

$$
\mathbf{S}^{n}=\{X\in\mathbf{R}^{n\times n}\mid X=X^{T}\},
$$ 
which is a vector space with dimension $n(n+1)/2$ . 
> 所有 $n\times n$ 的对称矩阵组成的集合

We use the notation $\mathbf{S}_{+}^{n}$ to denote the set of symmetric positive semidefinite matrices: 

$$
\mathbf{S}_{+}^{n}=\{X\in\mathbf{S}^{n}\mid X\succeq0\},
$$ 
and the notation $\mathbf{S}_{++}^{n}$ to denote the set of symmetric positive definite matrices: 

$$
\mathbf{S}_{++}^{n}=\{X\in\mathbf{S}^{n}\mid X\succ0\}.
$$ 
(This notation is meant to be analogous to $\mathbf{R}_{+}$ , which denotes the nonnegative reals, and $\mathbf{R}_{++}$ , which denotes the positive reals.) 



The set $\mathbf{S}_{+}^{n}$ is a convex cone: if $\theta_{1},\theta_{2}\geq0$ and $A$ , $B\in\mathbf{S}_{+}^{n}$ , then $\theta_{1}A\!+\!\theta_{2}B\in\mathbf{S}_{+}^{n}$ . This can be seen directly from the definition of positive semidefiniteness: 
for any $x\in\mathbf{R}^{n}$ , we have 

$$
x^{T}(\theta_{1}A+\theta_{2}B)x=\theta_{1}x^{T}A x+\theta_{2}x^{T}B x\geq0,
$$ 
if $A\succeq0$ , $B\succeq0$ and $\theta_{1}$ , $\theta_{2}\geq0$ . 
> 所有 $n\times n$ 的对称半正定矩阵组成的集合 $\mathbf S_+^n$ 是一个凸锥，即半正定锥


**Example 2.6** Positive semidefinite cone in $\mathbf{S}^{2}$ . 
We have 

$$
X={\left[\begin{array}{l l}{x}&{y}\\ {y}&{z}\end{array}\right]}\in\mathbf{S}_{+}^{2}\quad\Longleftrightarrow\quad x\geq0,\quad z\geq0,\quad x z\geq y^{2}.
$$

The boundary of this cone is shown in figure 2.12 , plotted in $\mathbf{R}^{3}$ as $(x,y,z)$ .

![[Convex Optimization-Fig2.12.png]]

## 2.3 Operations that preserve convexity 
In this section we describe some operations that preserve convexity of sets, or allow us to construct convex sets from others. 
These operations, together with the simple examples described in § 2.2 , form a calculus of convex sets that is useful for determining or establishing convexity of sets. 
> 本节介绍保持集合的凸性质的运算，以及允许我们从其他集合构造凸集的运算
### 2.3.1 Intersection 
Convexity is preserved under intersection: 
if $S_{1}$ and $S_{2}$ are convex, then $S_{1}\cap S_{2}$ is convex. 
This property extends to the intersection of an infinite number of sets: 
if $S_{\alpha}$ is convex for every $\alpha\in\mathcal A$ , then $\left.\bigcap_{\alpha\in\mathcal{A}}S_{\alpha}\right.$ is convex. (Subspaces, affine sets, and convex cones are also closed under arbitrary intersections.) 
> 两个集合之间的交集运算保持凸性质，并且可以拓展到无限数量的集合

As a simple example, a polyhedron is the intersection of halfspaces and hyperplanes (which are convex), and therefore is convex. 

**Example 2.7** The positive semidefinite cone $\mathbf{S}_{+}^{n}$ can be expressed as 

$$
\bigcap_{z\neq0}\{X\in\mathbf{S}^{n}\mid z^{T}X z\geq0\}.
$$ 
For each $z\neq0$ , $z^{T}X z$ is a (not identically zero) linear function of $X$ , so the sets 

$$
\{X\in\mathbf{S}^{n}\mid z^{T}X z\geq0\}
$$ 
are, in fact, halfspaces in $\mathbf{S}^{n}$ . 
Thus the positive semidefinite cone is the intersection of an infinite number of halfspaces, and so is convex. 
> 半正定锥是无限个子空间的交集，因此是凸的


**Example 2.8** We consider the set 

$$
S=\{x\in\mathbf{R}^{m}\mid|p(t)|\leq1{\mathrm{~for~}}|t|\leq\pi/3\},\tag{2.10}
$$ 
where $\begin{array}{r}{p(t)=\sum_{k=1}^{m}x_{k}\cos k t}\end{array}$ . 

The set $S$ can be expressed as the intersection of an infinite number of *slabs* : $S=\bigcap_{|t|\le\pi/3}\,S_{t}$ , where 

$$
S_{t}=\{x\mid-1\leq\left(\cos t,.\,.\,,\cos m t\right)^{T}x\leq1\},
$$ 
and so is convex. The definition and the set are illustrated in figures 2.13 and 2.14 , for $m=2$ . 

![[Convex Optimization-Fig2.13.png]]

In the examples above we establish convexity of a set by expressing it as a (possibly infinite) intersection of halfspaces. 
> 我们在上两个例子中都将一个凸集表示为了无限个子空间的交集

We will see in $\S2.5.1$ that a converse holds: every closed convex set $S$ is a (usually infinite) intersection of halfspaces. In fact, a closed convex set $S$ is the intersection of all halfspaces that contain it: 

$$
S=\bigcap\;\{{\mathcal{H}}\mid{\mathcal{H}}{\mathrm{~halfspace,~}}S\subseteq{\mathcal{H}}\}.
$$
> 一个封闭的凸集是所有包含了它的子空间的交集
### 2.3.2 Affine functions 
Recall that a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ is affine if it is a sum of a linear function and a constant, i.e. , if it has the form $f(x)=A x+b\,$ , where $A\in\mathbf{R}^{m\times n}$ and b $b\in\mathbf{R}^{m}$ . 
> 仿射函数可以写为一个线性函数加上一个常数，即形式为 $f (x) = Ax + b$

Suppose $S\subseteq\mathbf{R}^{n}$ is convex and $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ is an affine function. Then the image of S under $f$ , 

$$
f(S)=\{f(x)\mid x\in S\},
$$ 
is convex. 
> 一个凸集在一个仿射函数下的像是凸的

(
推导：
已知集合 $S\subseteq \mathbf R^n$  是凸集，函数 $f: \mathbf R^n \rightarrow \mathbf R^m$ 是仿射函数，其形式可以写为 $f (x) = Ax + b$
集合 $S$ 在函数 $f$ 下的像定义为 $f (S) = \{f (x)\mid x\in S\}$

对于任意的 $x_1, x_2\in S$，对于任意的 $\theta\in \mathbf R,0\le \theta \le 1$，我们知道 $\theta x_{1}+ (1-\theta)x_{2}\in S$
同时，我们知道 $f (x_1), f (x_2)\in S$
考虑 $\theta f (x_ 1) + (1-\theta)  f (x_2)$，容易知道

$$
\begin{align}
\theta f(x_1)+ (1-\theta)f(x_2) &= \theta Ax_1 + \theta b + (1-\theta)Ax_2 + (1-\theta)b\\
&=\theta Ax_1 + (1-\theta)Ax_2 + b\\
&=A(\theta x_1  +(1-\theta) x_2) + b\\
&\in f(S)
\end{align}
$$

因此 $f (S)$ 也包含了集合中任意元素的凸组合，是凸集
因此知道仿射函数保持了集合的凸性质
)

Similarly, if $f:\mathbf{R}^{k}\rightarrow\mathbf{R}^{n}$ is an affine function, the inverse image of $S$ under $f$ , 

$$
f^{-1}(S)=\{x\mid f(x)\in S\},
$$ 
is convex. 
> 一个凸集在一个仿射函数的逆下的像/在一个仿射函数下的逆像是凸的

Two simple examples a scaling and translation .
If $S\subseteq\mathbf{R}^{n}$ is convex, $\alpha\in\mathbf{R}$ , and $a\in\mathbf{R}^{n}$ , then the sets $\alpha S$ and $S+a$ are convex, where 

$$
\alpha S=\{\alpha x\mid x\in S\},\qquad S+a=\{x+a\mid x\in S\}
$$

> 缩放：集合乘上常数
> 转化：集合加上常数
> 二者都是仿射函数，因此凸集经过缩放和转化还是凸集

The projection of a convex set onto some of its coordinates is convex: 
if $S\subseteq$ ${\bf R}^{m}\times{\bf R}^{n}$ is convex, then 

$$
T=\{x_{1}\in\mathbf{R}^{m}\mid(x_{1},x_{2})\in S{\mathrm{~for~some~}}x_{2}\in\mathbf{R}^{n}\}
$$ 
is convex. 
> 凸集到坐标轴的投影也是凸的

The sum of two sets is defined as 

$$
S_{1}+S_{2}=\{x+y\mid x\in S_{1},\ y\in S_{2}\}.
$$ 
If $S_{1}$ and $S_{2}$ are convex, then $S_{1}+S_{2}$ is convex. To see this, if $S_{1}$ and $S_{2}$ are convex, then so is the direct or Cartesian product 

$$
S_{1}\times S_{2}=\{(x_{1},x_{2})\mid x_{1}\in S_{1},\ x_{2}\in S_{2}\}.
$$ 
The image of this set under the linear function $f(x_{1},x_{2})\,=\,x_{1}+x_{2}$ is the sum $S_{1}+S_{2}$ . 
> 两个凸集的和也是凸的

We can also consider the partial sum of $S_{1},\ S_{2}\in\mathbf{R}^{n}\times\mathbf{R}^{m}$ , defined as 

$$
S=\{(x,y_{1}+y_{2})\mid(x,y_{1})\in S_{1},\ (x,y_{2})\in S_{2}\},
$$ 
where $x\in\mathbf{R}^{n}$ $y_{i}\in\mathbf{R}^{m}$ . For $m=0$ , the partial sum gives the intersection of $S_{1}$ and $S_{2}$ ; for $n = 0$, it is set addition. 
Partial sums of convex sets are convex (see exercise 2.16 ). 
> 两个凸集的部分和也是凸的


**Example 2.9** Polyhedron. 
The polyhedron $\{x\mid A x\preceq b,\;C x=d\}$ can be expressed as the inverse image of the Cartesian product of the nonnegative orthant and the origin under the affine function $f(x)=(b-A x,d-C x)$ : 
> $x$ 在仿射函数 $f(x)$ 下的像属于非负象限和原点的笛卡尔积
> 非负象限和原点的笛卡尔积在仿射函数 $f(x)$ 下的逆像就是 $x$ 

$$
\{x\mid A x\preceq b,\ C x=d\}=\{x\mid f(x)\in\mathbf{R}_{+}^{m}\times\{0\}\}.
$$ 


**Example 2.10** Solution set of linear matrix inequality. The condition 

$$
A(x)=x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n}\preceq B,\tag{2.11}
$$ 
where $B$ , $A_{i}\in\mathbf{S}^{m}$ , is called a linear matrix inequality (LMI) in $x$ . (Note the similarity to an ordinary linear inequality, 

$$
a^{T}x=x_{1}a_{1}+\cdot\cdot\cdot+x_{n}a_{n}\leq b,
$$ 
with $b,\ a_{i}\in\mathbf{R}$ .) 

The solution set of a linear matrix inequality, $\{x\mid A(x)\preceq B\}$ , is convex. Indeed, it is the inverse image of the positive semidefinite cone under the affine function $f:\mathbf{R}^{n}\rightarrow\mathbf{S}^{m}$ given by $f(x)=B-A(x)$ . 
> 线性矩阵不等式的解集是凸的
> 解集 $x$ 在仿射函数 $f(x) = B - A(x)$ 下的像是一个半正定锥
> 半正定锥在仿射函数 $f(x) = B - A(x)$ 下的逆像就是解集 $x$


**Example 2.11** Hyperbolic cone. The set 

$$
\{{x}\ |\ {x}^{T}{P}{x}\leq({c}^{T}{x})^{2},\ {c}^{T}{x}\geq0\}
$$ 
where $P\in\mathbf{S}_{+}^{n}$ and $c\in\mathbf{R}^{n}$ , is convex, since it is the inverse image of the second-order cone, 

$$
\{(z,t)\mid z^{T}z\leq t^{2},\ t\geq0\},
$$ 
under the affine function $f(x)=(P^{1/2}x,c^{T}x)$ . 
> $x$ 在仿射函数 $f(x) = (P^{\frac{1}{2}}x, c^Tx)$ 下的像是二阶锥 $\{(z,t)\mid z^{T}z\leq t^{2},\ t\geq0\}$
> 二阶锥在仿射函数 $f(x) = (P^{\frac{1}{2}}x, c^Tx)$ 下的逆像就是双曲锥 $x$


**Example 2.12** Ellipsoid. The ellipsoid 

$$
{\mathcal{E}}=\{x\mid(x-x_{c})^{T}P^{-1}(x-x_{c})\leq1\},
$$ 
where $P\,\in\,\mathbf{S}_{++}^{n}$ the unit Euclidean ball $\{u\ |\ \|u\|_{2}\,\leq\,1\}$ under the affine mapping $f(u)=P^{1/2}u+x_{c}$ . (It is also the inverse image of the unit ball under the affine mapping $g(x)=P^{-1/2}(x-x_{c})$ .) 
> 椭球体是单位欧式球在仿射函数 $f(u) = P^{\frac{1}{2}}u+ x_c$ 下的像
> 椭球体在仿射函数 $g(x) = P^{-1/2}(x-x_c)$ 下的像也是单位欧式球
### 2.3.3 Linear-fractional and perspective functions 
In this section we explore a class of functions, called linear-fractional , that is more general than affine but still preserves convexity. 
> 本节介绍线性分式函数，该函数比仿射函数更广泛，但该函数仍保持凸性质
#### The perspective function
We define the perspective function $P:\mathbf{R}^{n+1}\rightarrow\mathbf{R}^{n}$ , with domain $\mathbf{dom}\  P=\mathbf{R}^{n}\times \mathbf{R}_{++}$ , as $P(z,t)=z/t$ . (Here $\mathbf{R}_{++}$ denotes the set of positive numbers: ${\bf R}_{++}\,= \{x\in\mathbf{R}\mid x>0\}$ .) 
The perspective function scales or normalizes vectors so the last component is one, and then drops the last component.
> 透视函数的定义域包括两部分，第一部分是一个 $n$ 维向量，第二部分是一个正数
> 透视函数根据正数 $t$ 对向量 $[z,t]$ 进行放缩/规范化，使得其最后一个成分变为 1，然后丢弃最后一个成分

**Remark 2.1** We can interpret the perspective function as the action of a pin-hole camera . A pin-hole camera (in $\mathbf{R}^{3}$ ) consists of an opaque horizontal plane $x_{3}\,=\,0$ , with a single pin-hole at the origin, through which light can pass, and a horizontal image plane $x_{3}=-1$ . An object at $x$ , above the camera ( i.e. , with $x_{3}>0$ ), forms an image at the point $-(x_{1}/x_{3},x_{2}/x_{3},1)$ on the image plane. Dropping the last component of the image point (since it is always $^{-1}$ ), the image of a point at $x$ appears at $y=-(x_{1}/x_{3},x_{2}/x_{3})=-P(x)$ on the image plane. This is illustrated in figure 2.15 . 

If $C\subseteq\mathbf{dom}\,P$ is convex, then its image 

$$
P(C)=\{P(x)\mid x\in C\}
$$ 
is convex. 
> 透视函数保持集合的凸性质

This result is certainly intuitive: a convex object, viewed through a pin-hole camera, yields a convex image. 
To establish this fact we show that line segments are mapped to line segments under the perspective function. (This too makes sense: a line segment, viewed through a pin-hole camera, yields a line segment image.) 

Suppose that $x=({\tilde{x}},x_{n+1}),\ y=({\tilde{y}},y_{n+1})\in\mathbf{R}^{n+1}$ with $x_{n+1}>0$ , $y_{n+1}>0$ . Then for $0\leq\theta\leq1$ , 

$$
P(\theta x+(1-\theta)y)=\frac{\theta\tilde{x}+(1-\theta)\tilde{y}}{\theta x_{n+1}+(1-\theta)y_{n+1}}=\mu P(x)+(1-\mu)P(y),
$$ 
where 

$$
\mu=\frac{\theta x_{n+1}}{\theta x_{n+1}+(1-\theta)y_{n+1}}\in[0,1].
$$ 
This correspondence between $\theta$ and $\mu$  is monotonic: as $\theta$ varies between $0$ and $1$ $\mu$ (which sweeps out the line segment $[x,y],$ ), varies between $0$ and $1$ (which sweeps $\mu$ out the line segment $[P(x),P(y)])$ . 
This shows that $P([x,y])=[P(x),P(y)]$ . 
> 这里证明了 $\mathbf{dom}P$ 中的点 $x, y$ 之间的一个点 $\theta x + (1-\theta) y, 0\le \theta \le 1$，经过 $P$ 映射之后的像点可以表示为 $P(\mathbf{dom}P)$ 中点 $P (x), P (y)$ 之间的一个点 $\mu P (x) + (1-\mu) P (y)$
> 这说明了 $\mathbf{dom}P$ 中的线段 $[x, y]$ 和 $P(\mathbf{dom}P)$ 中的线段 $[P (x), P (y)]$ 是一一对应的

Now suppose $C$ is convex with $C\subseteq\mathbf{dom}\,P$ ( i.e. , $x_{n+1}>0$ for all $x\in C$ ), and $x,\ y\in C$ . To establish convexity of $P(C)$ we need to show that the line segment $[P(x),P(y)]$ is in $P(C)$ . But this line segment is the image of the line segment $[x,y]$ under $P$ , and so lies in $P(C)$ . 
> 如果存在一个凸集 $C \subseteq \mathbf{dom}P$ 
> 对于 $x, y \in C$，根据凸集的性质我们有 $\theta x + (1-\theta) y \in C$
> 容易知道 $P (x), P (y), P(\theta x + (1-\theta)y) \in P (C)$，且对于满足 $0\le \mu \le 1$ 的 $\mu$ ，根据前面的推导可以知道 $\mu P (x) + (1-\mu) P (y)$ 一定对应于一个 $P (\theta x + (1-\theta) y), 0\le \theta \le 1$，因此 $\mu P (x) + (1-\mu) P (y) \in C$，也就是 $P$ 保持了集合 $C$ 的凸性质

The inverse image of a convex set under the perspective function is also convex: if $C\subseteq\mathbf{R}^{n}$ is convex, then 

$$
P^{-1}(C)=\{(x,t)\in\mathbf{R}^{n+1}\mid x/t\in C,\;t>0\}
$$ 
is convex. 
> $P^{-1}: \mathbf R^n \to \mathbf R^{n+1}$ 也是保凸的

To show this, suppose $(x,t)\in P^{-1}(C)$ , $(y,s)\in P^{-1}(C)$ , and $0\leq\theta\leq1$ . We need to show that 

$$
\theta(x,t)+(1-\theta)(y,s)\in P^{-1}(C),
$$ 
i.e. , that 

$$
\frac{\theta x+(1-\theta)y}{\theta t+(1-\theta)s}\in C
$$ 
> 要证明对于满足 $0\le \theta \le 1$ 的 $\theta$，有 $\theta (x, t) + (1-\theta)(y, s) \in P^{-1}(C)$，也就是 $(\theta x + (1-\theta) y , \theta t + (1-\theta) s)\in P^{-1}(C)$，也就是$\frac{\theta x+(1-\theta) y}{\theta t+(1-\theta) s}\in C$

$(\theta t+(1-\theta)s>0$ is obvious). This follows from 

$$
{\frac{\theta x+(1-\theta)y}{\theta t+(1-\theta)s}}=\mu(x/t)+(1-\mu)(y/s),
$$ 
where 

$$
\mu=\frac{\theta t}{\theta t+(1-\theta)s}\in[0,1].
$$

#### Linear-fractional functions 
A linear-fractional function is formed by composing the perspective function with an affine function. Suppose $g:\mathbf{R}^{n}\to\mathbf{R}^{m+1}$ is affine, i.e. , 

$$
g(x)=\left[\begin{array}{c c}{{A}}\\ {{c^{T}}}\end{array}\right]x+\left[\begin{array}{c c}{{b}}\\ {{d}}\end{array}\right],\tag{2.12}
$$ 
where $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , $c\in\mathbf{R}^{n}$ , and $d\in\mathbf{R}$ . 

The function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ given by $f=P\circ g$ , i.e. , 

$$
f(x)=(A x+b)/(c^{T}x+d),\qquad\mathbf{dom}\,f=\{x\mid c^{T}x+d>0\},\tag{2.13}
$$ 
is called a linear-fractional (or projective ) function. If $c=0$ and $d>0$ , the domain of $f$ is $\mathbf{R}^{n}$ , and $f$ is an affine function. So we can think of affine and linear functions as special cases of linear-fractional functions.
> 线性分式/投影函数是一个透视函数（降一维）和一个仿射函数的结合（升一维）

**Remark 2.2** Projective interpretation. 
It is often convenient to represent a linear- fractional function as a matrix 

$$
Q=\left[\begin{array}{c c}{{A}}&{{b}}\\ {{c^{T}}}&{{d}}\end{array}\right]\in\mathbf{R}^{\left(m+1\right)\times\left(n+1\right)}\tag{2.14}
$$ 
that acts on (multiplies) points of form $(x,1)$ , which yields $(A x+b,c^{T}x+d)$ . This result is then scaled or normalized so that its last component is one, which yields $(f(x),1)$ . 

This representation can be interpreted geometrically by associating $\mathbf{R}^{n}$ with a set of rays in $\mathbf{R}^{n+1}$ as follows. With each point $z$ in $\mathbf{R}^{n}$ we associate the (open) ray $\mathcal{P}(z)=\{t(z,1)\ |\ t>0\}$ in $\mathbf{R}^{n+1}$ . The last component of this ray takes on positive values. Conversely any ray in $\mathbf{R}^{n+1}$ , with base at the origin and last component akes on positive values, can be wri en as ${\mathcal P}(v)=\{t(v,1)\ |\ t\geq0\}$ for some $v\,\in\mathbf{R}^{n}$  . This (projective) correspondence P between R and the halfspace of rays with positive last component is one-to-one and onto. 
> 对于 $\mathbf R^n$ 中的每一个点 $z$，我们将其关联到 $\mathbf R^{n+1}$ 中的一条射线 $\mathcal P (z) = \{t (z, 1)\mid t > 0\}$
> 对于 $\mathbf R^{n+1}$ 中的每一条基点为原点且最后一个成分是正数的射线, 我们将它写为 $\mathcal P (v) = \{t (v, 1)\mid t > 0\}\text{ for some } v$，因此关联到了 $\mathbf R^n$ 中的一个点

The linear-fractional function ( 2.13 ) can be expressed as 

$$
\boldsymbol{f}(\boldsymbol{x})=\boldsymbol{\mathcal{P}}^{-1}(\boldsymbol{Q}\boldsymbol{\mathcal{P}}(\boldsymbol{x})).
$$ 
, we start with $x\,\in\,\mathbf{dom}\,f$ , i.e. , $c^{T}x+d>0$ . We then form the ray $\mathcal{P}(x)$ in $\mathbf{R}^{n+1}$ . The linear transformation with matrix $Q$ acts on this ray to produce another ray $Q\mathcal{P}(x)$ . Since $x\in\mathbf{dom}\ f$ , the last component of this ray assumes positive values. Finally we take the inverse projective transformation to recover $f(x)$ . 
> (2.13) 中的线性分式表示为先将 $\mathbf R^n$ 中的一点转化为 $\mathbf R^{n+1}$ 中的一条射线，然后对射线进行线性变换，然后再映射回 $\mathbf R^n$ 中的另一点

Like the perspective function, linear-fractional functions preserve convexity. If $C$ is convex and lies in the domain of $f$ ( i.e. , $c^{T}x+d>0$ for $x\in C$ ), then its image $f(C)$ is convex. 
This follows immediately from results above: the image of C under the affine mapping ( 2.12 ) is convex, and the image of the resulting set under the perspective function $P$ , which yields $f(C)$ , is convex. 
Similarly, if $C\subseteq\mathbf{R}^{m}$ is convex, then the inverse image $f^{-1}(C)$ is convex. 
> 线性分式函数是透视函数和仿射函数的结合，因此它是保凸的，同理它的反函数也是保凸的

**Example 2.13** Conditional probabilities. 
Suppose $u$ and $v$ are random variables that take on values in $\{1,\ldots,n\}$ and $\{1,\cdot\cdot\cdot,m\}$ , respectively, and let $p_{i j}$ denote $\mathbf{prob}(u\,=\,i,v\,=\,j)$ ). Then the conditional probability $f_{i j}\,=\,\mathbf{prob}(u\,=\,i|v\,=\,j)$ ) is given by 

$$
f_{i j}=\frac{p_{i j}}{\sum_{k=1}^{n}p_{k j}}.
$$ 
Thus $f$ is obtained by a linear-fractional mapping from $p$ . 

It follows that if $C$ is a convex set of joint probabilities for $(u,v)$ , then the associated set of conditional probabilities of $u$ given $v$ is also convex. 

Figure 2.16 shows a set $C\,\subseteq\,\mathbf{R}^{2}$ , and its image under the linear-fractional function 

$$
f(x)={\frac{1}{x_{1}+x_{2}+1}}x,\qquad\operatorname{dom}f=\{(x_{1},x_{2})\mid x_{1}+x_{2}+1>0\}.
$$ 
## 2.4 Generalized inequalities 
### 2.4.1 Proper cones and generalized inequalities 
A cone $K\subseteq\mathbf{R}^{n}$ is called a proper cone if it satisfies the following: 

- $K$ is convex. 
- $K$ is closed. 
- $K$ is solid , which means it has nonempty interior. 
- $K$ is pointed , which means that it contains no line (or equivalently, $x\in$ $K,\ -x\in K\implies x=0,$ 

> 如果一个锥满足：
> 凸
> 闭，包含了所有的极限点（没有“小孔”或“缺口”）
> 实心，即一定有内点（占据了一定的空间体积）
> 尖锐，不包含任何完整的直线
> 则它是正常锥

A proper cone $K$ can be used to define a generalized inequality , which is a partial ordering on $\mathbf{R}^{n}$ that has many of the properties of the standard ordering on $\mathbf{R}$ . We associate with the proper cone $K$ the partial ordering on $\mathbf{R}^{n}$ defined by 

$$
x\ \preceq_K  y\iff y-x\in K.
$$ 
We also write $x\succeq_{K}y$ for $y\preceq_{K}x$ . 
> 正常锥用于定义广义不等式
> 广义不等式是一个 $\mathbf R^n$ 中的偏序关系，其许多性质和 $\mathbf R$ 中的标准顺序关系一致
> 偏序：定义当 $y - x \in K$，$x \preceq_K y$


Similarly, we define an associated strict partial ordering by 

$$
x\prec_{K}y\iff y-x\in\mathbf{int}\,K,
$$ 
and write $x\ \succ_{K}\ y$ for $y\ \prec K\ x$ . 
> 严格偏序：定义当 $y-x \in \mathbf{int}K, x\prec y$

(To distinguish the generalized inequality $\preceq_{K}$ from the strict generalized inequality, we sometimes refer to $\preceq_{K}$ as the nonstrict generalized inequality.) 

When ${\cal K}\,=\,{\bf R}_{+}$ , the partial ordering $\preceq_{K}$ is the usual ordering $\leq$ on $\mathbf{R}$ , a d the strict partial ordering $\prec_{K}$ is the same as the usual strict ordering $<$ on R . So generalized inequalities include as a special case ordinary (nonstrict and strict) inequality in $\mathbf{R}$ . 

Example 2.14 Nonnegative orthant and componentwise inequality. 
The nonnegative orthant $K=\mathbf{R}_{+}^{n}$ is a proper cone. The associated generalized inequality $\preceq_{K}$ corresponds to componentwise inequality between vectors: $x\ {\stackrel{\rightharpoonup}{-}}\,K\ \ y$ means that $x_{i}\,\leq\,y_{i}$ , $i\,=\,1,\dots,n$ . The associated strict inequality corresponds to componentwise strict inequality: $x\prec K\ y$ means that $x_{i}<y_{i}$ , $i=1,\dots,n$ . 

The nonstrict and strict partial orderings associated with the nonnegative orthant arise so frequently that we drop the subscript $\mathbf{R}_{+}^{n}$ ; it is understood when the symbol $\preceq$ or $\prec$ appears between vectors. 
> 非负象限 $\mathbf R_+^n$ 对应的广义不等式对应于的是向量之间的 compontentwise 不等式
> 当 $\preceq,\prec$ 出现在向量之间时，它们就意味着 $\preceq_{\mathbf R_+^n}$ 和 $\prec_{\mathbf R_+^n}$

Example 2.15 Positive semidefinite cone and matrix inequality. The positive semidefinite cone Sn + is a proper cone in Sn. The associated generalized inequality ≼K is the usual matrix inequality: X ≼K Y means Y - X is positive semidefinite. The interior of Sn + (in Sn) consists of the positive definite matrices, so the strict generalized inequality also agrees with the usual strict inequality between symmetric matrices: X ≺K Y means Y - X is positive definite.

Here, too, the partial ordering arises so frequently that we drop the subscript: for symmetric matrices we write simply $X\ \preceq\ Y$ or $X\ \prec\ Y$ . It is understood that the generalized inequalities are with respect to the positive semidefinite cone. 
> 半正定锥 $\mathbf S_+^n$ 对应的广义不等式 $X\preceq Y$ 表示 $Y-X$ 是半正定的，$X\prec Y$ 表示 $Y-X$ 是正定的

Example 2.16 Cone of polynomials nonnegative on $[0,1]$ . 
Let $K$ be defined as 

$$
K=\{c\in\mathbf{R}^{n}\mid c_{1}+c_{2}t+\cdot\cdot\cdot+c_{n}t^{n-1}\geq0{\mathrm{~for~}}t\in[0,1]\},\tag{2.15}
$$ 
i.e. , $K$ is the cone of (coefficients of) polynominals of degree $n-1$ that are nonnegative on the interval $[0,1]$ . It can be shown that K is a proper cone; its interior is the set of coefficients of polynomials that are positive on the interval $[0,1]$ . 
> 定义 $K$ 为在区间 $[0,1]$ 保持非负的 $n-1$ 为多项式的系数向量集合
> $K$ 是一个正常锥
> 它的内点是在区间 $[0,1]$ 保持正的 $n-1$ 为多项式的系数向量集合

Two vectors $c,\ d\in\mathbf{R}^{n}$ satisfy $c\preceq_{K}d$ if and only if 

$$
c_{1}+c_{2}t+\cdot\cdot\cdot+c_{n}t^{n-1}\leq d_{1}+d_{2}t+\cdot\cdot\cdot+d_{n}t^{n-1}
$$ 
for all $t\in[0,1]$ . 
#### Properties of generalized inequalities 
A generalized inequality $\preceq_{K}$ satisfies many properties, such as 

• $\preceq_{K}$ is preserved under addition : if $x\preceq_{K}y$ and $u\preceq_{K}v$ , then $x+u\preceq_{K}y+v$ . • $\preceq_{K}$ is transitive : if $x\preceq_{K}y$ and $y\preceq_{K}z$ then $x\preceq_{K}z$ . 
• $\preceq_{K}$ is preserved under nonnegative scaling : if $x\ \preceq_{K}\ y$ and $\alpha\ \geq\ 0$ then $\alpha x\preceq_{K}$ αy . 
• $\preceq_{K}$ is reﬂexive: $x\preceq_{K}x$ . 
• $\preceq_{K}$ is antisymmetric: if $x\preceq_{K}y$ and $y\preceq_{K}x$ , then $x=y$ . 
• $\preceq_{K}$ served imits: if $x_{i}\preceq_{K}$ $y_{i}$ for $i=1,\ 2,\ldots.$ , $x_{i}\rightarrow x$ and $y_{i}\rightarrow y$ as i , then x $x\preceq_{K}y$ . 

> 广义不等式的性质：
> 不等关系在加法下保持
> 不等关系可传递
> 不等关系在非负放缩下保持
> 不等关系自反
> 不等关系反对称
> 不等关系在极限下保持

The corresponding strict generalized inequality $\prec_{K}$ satisfies, for example, 

• if $x\prec K\,\,\mathcal{Y}$ then $x\preceq_{K}y$ . • if $x\prec_{K}y$ and $u\preceq_{K}v$ then $x+u\prec_{K}y+v$ . 
• if $x\prec_{K}y$ and $\alpha>0$ then $\alpha x\prec_{K}\alpha y$ . 
•$x\not\prec x$ . if $x\prec_{K}y$ , then for $u$ and $v$ small enough, $x+u\prec_{K}y+v$ . 

These properties are inherited from the definitions of $\preceq_{K}$ and $\prec_{K}$ , and the prop- erties of proper cones; see exercise 2.30 . 

### 2.4.2 Minimum and minimal elements 
The notation of generalized ine ality ( i .e.$,\,\,\preceq_{K},\,\,\prec_{K}$ )is meant to suggest the analogy to ordinary inequality on $\mathbf{R}\left(i.e.,\leq,<\right)$ . While many properties of ordinary inequality do hold for generalized inequalities, some important ones do not. The most obvious diﬀerence is that $\leq$ on $\mathbf{R}$ is a linear ordering : any two points are comparable , meaning either $x\ \leq\ y$ or $y\ \leq\ x$ . This property does not hold for other generalized inequalities. One implication is that concepts like minimum and maximum are more complicated in the context of generalized inequalities. We brieﬂy discuss this in this section. 
> 广义不等式下的最大和最小关系要更为复杂

We say that $x\in S$ is the minimum element of $S$ (with respect to the general- ized inequality $\preceq_{K}$ ) if for every $y\in S$ we have $x\preceq_{K}y$ . We define the maximum element of a set S , with respect to a generalized inequality, in a similar way. If a set has a minimum (maximum) element, then it is unique. A related concept is minimal element . We say that $x\in S$ is a minimal element of $S$ (with respect to the generalized inequality $\preceq_{K}$ ) if $y\in S$ , $y\preceq_{K}x$ only if $y\,=\,x$ . We define maxi- mal element in a similar way. A set can have many diﬀerent minimal (maximal) elements. 
> 如果对于 $S$ 中任意元素 $y$，满足 $x \preceq y$，则 $x \in S$ 是 $S$ 中的最小元，同理可以定义最大元
> 最小元和最大元是唯一的，它们表示是所有元素中最小/最大的
> 极小元和极大元则不唯一，如果对于 $S$ 中任意元素 $y$，满足当 $y \preceq x$ 时，一定有 $y = x$，则 $x$ 是 $S$ 的极小元
> 极小元和极大元表示没有比他们更小/更大的元素，但可以有和他们同等级别的元素

We can describe minimum and minimal elements using simple set notation. A point $x\in S$ is the minimum element of $S$ if and only if 

$$
S\subseteq x+K.
$$ 
Here $x+K$ denotes all the points that are comparable to $x$ and greater than or equal to $x$ (according to $\preceq_{K}$ ). A point $x\in S$ is a minimal element if and only if 

$$
(x-K)\cap S=\{x\}.
$$ 
Here $x-K$ denotes all the points that are comparable to $x$ and less than or equal to $x$ (according to $\preceq_{K}$ ); the only point in common wit $S$ is $x$ . 

For $K=\mathbf{R}_{+}$ , which induces the usual ordering on R , the concepts of minimal and minimum are the same, and agree with the usual definition of the minimum element of a set. 

Example 2.17 
Consider the cone $\mathbf{R}_{+}^{2}$ , which induces componentwise inequality in $\scriptstyle\mathbf{R}^{2}$ . 
Here we can give some simple geometric descriptions of minimal and minimum nts. The inequality $x\preceq y$ means $y$ is above and to the right of $x$ . o say that $x\in S$ ∈ is the minimum element of a set S means that all other points of S lie above and to the right. To say that $x$ is a minimal element of a set $S$ means that no other point of $S$ lies to the left and below $x$ . This is illustrated in figure 2.17 . 

![[Convex Optimization-Fig2.17.png]]

Example 2.18 Minimum and minimal elements of a set of symmetric matrices. 
We associate with each $A\in\mathbf{S}_{++}^{n}$ an ellipsoid centered at the origin, given by 

$$
{\mathcal{E}}_{A}=\{x\mid x^{T}A^{-1}x\leq1\}.
$$ 
We have $A\preceq B$ if and only if $\mathcal{E}_{A}\subseteq\mathcal{E}_{B}$ . 

Let $v_{1},.\,.\,.\,,v_{k}\in\mathbf{R}^{n}$ be given and define 

$$
{\cal S}=\{P\in{\bf S}_{++}^{n}\mid v_{i}^{T}P^{-1}v_{i}\le1,\;i=1,.\,.\,,k\},
$$

which corresponds to the set of ellipsoids that contain the points $v_{1},\dots,v_{k}$ . The set $S$ does not have a minimum element: for any ellipsoid that contains the points $v_{1},\dots,v_{k}$ we can find another one that contains the points, and is not comparable to it. An ellipsoid is minimal if it contains the points, but no smaller ellipsoid does. Figure 2.18 shows an example in $\mathbf{R}^{2}$ with $k=2$ . 
## 2.5 Separating and supporting hyperplanes 
### 2.5.1 Separating hyperplane theorem 
In this section we describe an idea that will be important later: 
the use of hyperplanes or affine functions to separate convex sets that do not intersect. 
> 本节描述使用超平面或仿射函数来分离凸集，使分离后的结果互不相交

The basic result is the separating hyperplane theorem : Suppose $C$ and $D$ are nonempty disjoint c sets, i.e. , $C\cap D=\emptyset$ n there exist $a\ne0$ and $b$ such that $a^{T}x\leq b$ for all x $x\in C$ ∈ and $a^{T}x\geq b$ for all x $x\in D$ ∈ . other words, the affine function a $a^{T}x-b$ − is nonpositive on C and nonnegative n D . he hyperplane $\{x\mid a^{T}x=b\}$ is alled a separating hyperplane for the sets C and D , or is said to separate the sets C and $D$ . This is illustrated in figure 2.19 . 
> 分离超平面定理：任意两个不相交的凸集都存在一个分离超平面将二者分离

![[Convex Optimization-Fig2.19.png]]

#### Proof of separating hyperplane theorem 
Here we consider a special case, and leave the extension of the proof to the general case as an exercise (exercise 2.22 ). 

We assume that the (Euclidean) distance between $C$ and $D$ , defined as 

$$
\mathbf{dist}(C,D)=\operatorname*{inf}\{\|u-v\|_{2}\ |\ u\in C,\ v\in D\},
$$ 
is positive, and that there exist points $c\in C$ and $d\in D$ that achieve the minimum distan e, i.e. $||c-d||_{2}=\mathbf{dist}(C,D)$ . (These conditions are satisfied, for example, when C and D are closed and one set is bounded.) 
> 定义两个集合之间的欧式距离为它们各自的点之间的欧式距离的最小值
> 假设就是 $c, d$ 两点

Define 

$$
a=d-c,\qquad b={\frac{\|d\|_{2}^{2}-\|c\|_{2}^{2}}{2}}.
$$ 
We will show that the affine function 

$$
f(x)=a^{T}x-b=(d-c)^{T}(x-(1/2)(d+c))
$$ 
is nonpositive on $C$ nonnegative on $D$ , i.e. , that the hyperplane $\{x\mid a^{T}x=b\}$ separates C and D . 
> 直接根据 $c, d$ 构造出了超平面 $\{x \mid a^T x = b\}$
> 我们接下来要证明集合 $C, D$ 各在超平面中的一边

This hyperplane is perpendicular to the line segment between $c$ and $d$ , and passes through its midpoint, as shown in figure 2.20 . 

![[Convex Optimization-Fig2.20.png]]

We first show that $f$ is nonnegative on $D$ . The proof that $f$ is nonpositive on $C$ is similar (or follows by swapping $C$ and $D$ and considering $-f$ ). 

Suppose there were a point $u\in D$ for which 

$$
f(u)=(d-c)^{T}(u-(1/2)(d+c))<0.\tag{2.16}
$$ 
We can express $f(u)$ as 

$$
f(u)=(d-c)^{T}(u-d+(1/2)(d-c))=(d-c)^{T}(u-d)+(1/2)\|d-c\|_{2}^{2}.
$$ 
We see that ( 2.16 ) implies $(d-c)^{T}(u-d)<0$ . Now we observe that 

$$
\frac{d}{d t}\|d+t(u-d)-c\|_{2}^{2}\bigg|_{t=0}=2(d-c)^{T}(u-d)<0,
$$ 
so for some small $t>0$ , with $t\leq1$ , we have 

$$
||d+t(u-d)-c||_{2}<||d-c||_{2},
$$ 
i.e. , the point $d+t(u-d)$ is closer to $c$ than $d$ is. Since $D$ is convex and contains d and $u$ , we have $d+t(u-d)\in D$ But this is impossible, since d is assumed to be the point in D that is closest to C . 
> 反证法，假设 $u\in D$ 在超平面的另一边，推导出此时 $D$ 中存在比 $d$ 离 $c$ 更近的点，因此矛盾

Example 2.19 Separation of an affine and a convex set. 
Suppose $C$ is convex and $D$ is affine, i.e. , $D=\{F u+g\mid u\in\mathbf{R}^{m}\}$ , where $F\in\mathbf{R}^{n\times m}$ . Suppose $C$ and $D$ are disjoint, so by the separating hyperplane theorem there are $a\ne0$ and b such that $a^{T}x\leq b$ for all $x\in C$ and $a^{T}x\geq b$ for all $x\in D$ . 

Now $a^{T}x\,\geq\,b$ for all $x\in D$ ns $a^{T}F u\geq b-a^{T}g$ for all $u\,\in\,\mathbf{R}^{m}$ . linear function is bounded below on R $\mathbf{R}^{m}$ m only when it is zero, so we conclude a $\boldsymbol{a}^{T}\boldsymbol{F}=0$ = 0 (and hence, $b\leq a^{T}g$ ). 

Thus we conclude that there exists $a\ne0$ such that $F^{T}a=0$ and $a^{T}x\leq a^{T}g$ for all $x\in C$ . 
#### Strict separation 
The separating hyperplane we constructed above satisfies the stronger condition that $a^{T}x\;<\;b$ for all $x\,\in\,C$ and $a^{T}x\;>\;b$ for all $x\,\in\,D$ . This is called strict separation of the sets C and D . 
Simple examples show that in general, disjoint convex sets need not be strictly separable by a hyperplane (even when the sets are closed; see exercise 2.23 ). In many special cases, however, strict separation can be established. 
> 如果构造出的分离超平面满足 $a^Tx < b$ for all $x \in C$ and $a^T x > b$ for all $x \in D$，则称这个超平面严格分离的集合 $C$ 和 $D$

Example 2.20 Strict separation of a point and a closed convex set . 
Let $C$ be a closed conve set and $x_{0}\notin C$ . Then there exists a hyperplane that strictly separates $x_{0}$ from C . 
> 对于一个闭的凸集 $C$ 和一个点 $x_0 \notin C$，存在一个超平面将点 $x_0$ 从 $C$ 中严格分离

To see this, note that the two sets $C$ and $B(x_{0},\epsilon)$ do not intersect for some $\epsilon>0$ . By the separating hyperplane theorem, there exist $a\ne0$ and $b$ such that $a^{T}x\leq b$ for $x\in C$ and $a^{T}x\geq b$ for $x\in B(x_{0},\epsilon)$ . 

Using $B(x_{0},\epsilon)=\{x_{0}+u\mid\|u\|_{2}\leq\epsilon\}$ , the second condition can be expressed as 

$$
a^{T}(x_{0}+u)\geq b\;\;{\mathrm{for~all~}}\;\;\|u\|_{2}\leq\epsilon.
$$ 
The $u$ that minimizes the lefthand side is $u=-\epsilon a/||a||_{2}$ ; using this value we have 

$$
a^{T}x_{0}-\epsilon\|a\|_{2}\geq b.
$$ 
Therefore the affine function 

$$
f(x)=a^{T}x-b-\epsilon\|a\|_{2}/2
$$ 
is negative on $C$ and positive at . $x_{\mathrm{0}}$ 

As an immediate consequence we can establish a fact that we already mentioned above: a closed convex set is the intersection of all halfspaces that contain it. Indeed, let $C$ be closed and convex, and let $S$ be the intersection of all halfspaces containing $C$ . Obviously $x\in C\Rightarrow x\in S$ . To show the converse, suppose there exists $x\in S$ , $x\notin C$ . By the strict separation result there exi s a hyperplane that strictly separates $x$ from C , i.e. , there is a halfspace containing C but not $x$ . In other words, $x\notin S$ . 
> 这个结论可以推导出：一个闭的凸集是所有包含它的超平面的交集
> 证明：令 $C$ 是一个闭的凸集，令 $S$ 是所有包含 $C$ 的超平面的交集，
> 如果 $x\in C$，显然有 $x\in S$
> 如果 $x\in S$，假设 $x\notin C$，则根据容易知道存在一个超平面严格分割 $x$ 和 $C$，也就是存在一个包含了 $C$ 但不包含 $x$ 的超平面，即 $x\notin S$ ，矛盾
#### Converse separating hyperplane theorems 
The converse of the separating hyperplane theorem ( i.e. , existence of a separating hyperplane implies that $C$ and $D$ do not intersect) is not true, unless one imposes additional constraints on $C$ or $D$ , even beyond convexity. As a simple counterexmple, consider $C=D=\{0\}\subseteq\mathbf{R}$ . Here the hyperplane $x=0$ separates $C$ and D . 
> 分离超平面定理反过来（ $C$ 和 $D$ 之间存在一个分离超平面，则说明 $C$ 和 $D$ 不相交）则不一定对

By adding conditions on $C$ and $D$ various converse separation theorems can be derived. As a very simple example, suppose $C$ and $D$ are convex sets, with $C$ open, and there exists an affine function $f$ that is nonpositive on $C$ and nonnegative on $D$ . Then $C$ and $D$ are disjoint. (To see this we first note that $f$ must be negative on $C$ ; for if $f$ were zero at a point of $C$ then $f$ would take on positive values near the point, which is a contradiction. But then $C$ and $D$ must be disjoint since $f$ is negative on $C$ and nonnegative on $D$ .) 
> 如果 $C$ 和 $D$ 是两个凸集，并且 $C$ 开放，以及存在一个仿射函数 $f$，在 $C$ 上非正，在 $D$ 上非负，则如果 $C, D$ 之间存在分离超平面，二者不相交

Putting this converse together with the separating hyperplane theorem, we have the following result: any two convex sets $C$ and $D$ , at least one of which is open, are disjoint if and only if there exists a separating hyperplane. 
> 任意两个凸集，其中至少一个是开的，当且仅当二者之间存在分离超平面，二者不相交

Example 2.21 Theorem of alternatives for strict linear inequalities. We derive the necessary and sufficient conditions for solvability of a system of strict linear inequalities

$$
A x\prec b.\tag{2.17}
$$ 
These inequalities are infeasible if and only if the (convex) sets 

$$
C=\{b-A x\mid x\in\mathbf{R}^{n}\},\qquad D=\mathbf{R}_{++}^{m}=\{y\in\mathbf{R}^{m}\mid y\succ0\}
$$ 
do not intersect. 
> 我们推导严格线性不等式 $Ax \prec b$ 的可解条件：
> 当且仅当凸集 $C = \{ b - Ax \mid x \in \mathbf R^n\}, D = \mathbf R_{++}^m = \{y \in \mathbf R^m \mid y \succ 0\}$ 不相交时，该不等式不可解

The set $D$ is open; $C$ is an affine set. Hence by the result above, $C$ and $D$ are disjoint if and only if there exists a separating hyperplane, i.e. , a nonzero $\lambda\in\mathbf{R}^{m}$ and $\mu\in\mathbf{R}$ such that $\lambda^{T}y\leq\mu$ on $C$ and $\lambda^{T}y\geq\mu$ on $D$ . 

Each of these conditions can be simplified. 
The first means $\lambda^{T}(b-A x)\leq\mu$ for all $x$ . This implies (as in example 2.19 ) that $A^{T}\lambda=0$ and $\lambda^{T}b\leq\mu$ . 
The second inequality means $\lambda^{T}y\geq\mu$ for all $y\succ0$ . This implies $\mu\leq0$ and $\lambda\succeq0$ , $\lambda\neq0$ . 
> $D$ 是开集，$C$ 是仿射集，则可以知道当且仅当二者之间存在分离超平面，二者不相交

Putting it all together, we find that the set of strict inequalities ( 2.17 ) is infeasible if and only if there exists $\lambda\in\mathbf{R}^{m}$ such that 

$$
\begin{array}{r}{\boldsymbol{\lambda}\neq\boldsymbol{0},\qquad\boldsymbol{\lambda}\succeq\boldsymbol{0},\qquad\boldsymbol{A}^{T}\boldsymbol{\lambda}=\boldsymbol{0},\qquad\boldsymbol{\lambda}^{T}\boldsymbol{b}\leq\boldsymbol{0}.}\end{array}\tag{2.18}
$$ 
This is also a system of linear inequalities and linear equations in the variable $\lambda\in\mathbf{R}^{m}$ . We say that ( 2.17 ) and ( 2.18 ) form a pair of alternatives : for any data A and b , exactly one of them is solvable. 
### 2.5.2 Supporting hyperplanes 
Suppose $C\subseteq\mathbf{R}^{n}$ , and is a point in its boundary $\mathbf{bd}\,C$ , i.e. , $x_{0}$ 

$$
x_{0}\in\mathbf{bd}\,C=\mathbf{cl}\,C\setminus\mathbf{int}\,C.
$$ 
If $a\ne0$ satisfies $a^{T}x\leq a^{T}x_{0}$ for all $x\in C$ , then the hyperplane $\{x\mid a^{T}x=a^{T}x_{0}\}$ is called a supporting hyperplane to C at the point $x_{0}$ . 
> $x_0 \in \mathbf{bd}C$ 是集合 $C$ 的边界上的一点，如果对于所有的 $x\in C$，存在 $a \ne 0$ 满足 $a^T x \le a^T x_0$，则称超平面 $\{x \mid a^T x= a^Tx_0\}$ 是集合 $C$ 在点 $x_0$ 处的支撑超平面

This is equivalent to saying hat the point $x_{0}$ and the set $C$ are separated by the hyperplane $\{x\mid a^{T}x=a^{T}x_{0}\}$ . The geometric interpretation is that the hyperplane $\{x\mid a^{T}x=a^{T}x_{0}\}$ is tangent to C at $x_{0}$ , and the halfspace $\{x\mid a^{T}x\leq a^{T}x_{0}\}$ contains C . This is illustrated in figure 2.21 . 
> 这等价于说点 $x_0$ 和集合 $C$ 被超平面 $\{x \mid a^T x= a^Tx_0\}$ 分离
> 在几何上，集合 $C$ 在点 $x_0$ 上的支撑超平面是和集合 $C$ 相切的，对应的半空间包含了 $C$

![[Convex Optimization-Fig2.21.png]]

A basic result, called the supporting hyperplane theorem , states that for any onempty convex set $C$ , and any $x_{0}\in\mathbf{bd}\,C$ , there exists a supporting hyperplane to C at $x_{0}$ . The supporting hyperplane theorem is readily proved from the separating hyperplane theorem. We distinguish two cases. If the interior of $C$ is nonempty, the result follows immediately by applying the separating hyperplane theorem to the sets $\{\boldsymbol{x}_{0}\}$ and $\operatorname{int}C$ . If the interior of $C$ is empty, then $C$ must lie in an affine set of dimension less than $n$ , and any hyperplane containing that affine set contains $C$ and $x_{0}$ , and is a (trivial) supporting hyperplane. 
> 支撑超平面定理：对于任意的非空凸集 $C$，以及任意在 $C$ 边界上的点 $x_0 \in \mathbf{bd}C$，$C$ 存在一个在 $x_0$ 上的支撑超平面
> 证明：根据分离超平面定理证明
> 如果 C 的内部非空，则直接对集合 $\{x_0\}$ 和 $\text{int}C$ 应用分离超平面定理
> 如果 C 的内部为空，则 C 必须处于一个维度小于 n 的仿射集内，且任意包含了该仿射集和 $x_0$ 的超平面就是支撑超平面

There is also a partial converse of the supporting hyperplane theorem: If a set is closed, has nonempty interior, and has a supporting hyperplane at every point in its boundary, then it is convex. (See exercise 2.27 .) 
## 2.6 Dual cones and generalized inequalities 
### 2.6.1 Dual cones 
Let $K$ be a cone. The set 

$$
K^{*}=\{y\mid x^{T}y\geq0{\mathrm{~for~all~}}x\in K\}\tag{2.19}
$$ 
is called the dual cone of $K$ . 

As the name suggests, $K^{*}$ is a cone, and is always convex, even when the original cone $K$ is not (see exercise 2.31 ). 
> 给定一个锥，由和锥内的所有点的点积都大于等于0的点构成的集合称为 $K$ 的对偶锥，对偶锥一定是凸锥

Geometrically, $y\,\in\,K^{*}$ if and only if $-y$ is the normal of a hyperplane that supports K at the origin. This is illustrated in figure 2.22 . 
> $y\in K^*$ 当且仅当 $-y$ 是在源点处支持 $K$ 的超平面的法向量

![[Convex Optimization-Fig2.22.png]]
Example 2.22 Subspace.
The dual cone of a subspace $V\subseteq\mathbf{R}^{n}$ (which is a cone) is its orthogonal complement $V^{\bot}=\{y\mid v^{T}y=0$ for all $v\in V\}$ . 
> 子空间 $V \subseteq \mathbf R^n$ 的对偶锥是它的正交补 $V^{\perp} = \{y\mid v^Ty=0 \text{ for all }v \in V\}$

Example 2.23 Nonnegative orthant. 
The cone ${\bf R}_{+}^{n}$ is its own dual: 

$$
x^{T}y\geq0{\mathrm{~for~all~}}x\succeq0\iff y\succeq0.
$$ 
We call such a cone self-dual.
> 锥 $\mathbf R_+^n$（非负象限）的对偶锥是它自己，也就是自对偶的

Example 2.24 Positive semidefinite cone. 
On the set of symmetric $n\times n$ matrices $\mathbf{S}^{n}$ , we use the standard inner product $\begin{array}{r}{\mathbf{tr}(X Y)=\sum_{i,j=1}^{n}X_{i j}Y_{i j}}\end{array}$ (see § A.1.1 ). 
The positive semidefinite cone $\mathbf{S}_{+}^{n}$ is self-dual, i.e. , for $X, Y\in\mathbf{S}^{n}$, 

$$
\mathbf{tr}(X Y)\geq0{\mathrm{~for~all~}}X\succeq0\iff Y\succeq0
$$ 
We will establish this fact. 
> 半正定锥是自对偶的

Suppose $Y\notin\mathbf{S}_{+}^{n}$ . Then there exists $q\in\mathbf{R}^{n}$ with 

$$
q^{T}Y q=\mathbf{tr}({q}{q^{T}}Y)<0.
$$ 
Hence the positive semidefinite matrix $X=q q^{T}$ satisfies $\mathbf{tr}(X Y)<0$ ; it follows that $Y\notin(\mathbf{S}_{+}^{n})^{*}$ . 
> 假设 $Y\notin \mathbf S_+^n$，则存在半正定矩阵满足 $tr (XY) < 0$，则 $Y \notin (\mathbf S_+^n)^*$

Now suppose $X$ , $Y\in\mathbf{S}_{+}^{n}$ . We can express $X$ in terms of its eigenvalue decomposition as $\begin{array}{r}{X=\sum_{i=1}^{n}\lambda_{i}q_{i}q_{i}^{T}.}\end{array}$ , where (the eigenvalues) $\lambda_{i}\geq0$ , $i=1,\dots,n$ . Then we have 

$$
\mathbf{tr}(Y X)=\mathbf{tr}\left(Y\sum_{i=1}^{n}\lambda_{i}q_{i}q_{i}^{T}\right)=\sum_{i=1}^{n}\lambda_{i}q_{i}^{T}Y q_{i}\geq0.
$$ 
This shows that $Y\in(\mathbf{S}_{+}^{n})^{*}$ 
> 假设 $X, Y \in \mathbf S_+^n$，我们可以将 $X$ 写为它的特征值分解的形式 $X = \sum_{i=1}^n \lambda_i q_i q_i^T$，其中特征值 $\lambda_i \ge 0$
> 容易知道 $tr (YX) \ge 0$，则 $Y \in (\mathbf S_+^n)^*$

Example 2.25 Dual of a norm cone. 
Let $||\cdot||$ be a norm on $\mathbf{R}^{n}$ . 
The dual of the associated cone $K=\{(x,t)\in\mathbf{R}^{n+1}\mid\|x\|\leq t\}$ is the cone defined by the dual norm, i.e. , 

$$
K^{*}=\{(u,v)\in\mathbf{R}^{n+1}\mid\|u\|_{*}\leq v\},
$$ 
where the dual norm is given by $\|u\|_{*}=\operatorname*{sup}\{u^{T}x\ |\ \|x\|\leq1\}$ (see ( A.1.6 )). 
> 范数锥的对偶锥是它的对偶范数定义的锥

To prove the result we have to show that 

$$
x^{T}u+t v\geq0{\mathrm{~whenever~}}\|x\|\leq t\iff\|u\|*\leq v.
$$ 
Let us start by showing that the righthand condition on $(u,v)$ implies the lefthand condition. Suppose $\lvert|u\rvert|_{*}\leq v$ , and $\|x\|\leq t$ for some $t>0$ . (If $t=0$ , $x$ must be zero, so obviously $u^{T}x+v t\geq0$ .) Applying the definition of the dual norm, and the fact that $\|\!-\!x/t\|\leq1$ , we have 

$$
u^{T}(-x/t)\leq\|u\|_{*}\leq v,
$$ 
and therefore $u^{T}x+v t\geq0$ . 

Next we show that the lefthand condition in ( 2.20 ) implies the righthand condition in ( 2.20 ). Suppose $\|u\|_{*}>v$ , i.e. , that the righthand condition does not h n by the definition of the dual norm, there exists an $x$ with $\|x\|\leq1$ and x $x^{T}u\,>\,v$ . Taking $t=1$ , we have 

$$
u^{T}(-x)+v<0,
$$ 
which contradicts the lefthand condition in ( 2.20 ). 

Dual cones satisfy several properties, such as: 

- $K^{*}$ is closed and convex. 
- $K_{1}\subseteq K_{2}$ implies $K_{2}^{*}\subseteq K_{1}^{*}$ ⊆ . 
- If $K$ has nonempty interior, then $K^{*}$ is pointed. 
- If the closure of $K$ is pointed then $K^{*}$ has nonempty interior. 
- $K^{**}$ is the closure of the convex hull of $K$ . (Hence if $K$ is convex and closed, $K^{**}=K$ .) 

(See exercise 2.31 .) These properties show that if $K$ is a proper cone, then so is its dual $K^{*}$ , and moreover, that $K^{**}=K$ . 
### 2.6.2 Dual generalized inequalities 
Now suppose that the convex cone $K$ is proper, so it induces a generalized inequality $\preceq_{K}$ . Then its dual cone $K^{*}$ is also proper, and therefore induces a generalized inequality. We refer to the generalized inequality $\preceq_{K^{*}}$ as the dual of the generalized inequality $\preceq_{K}$ . 

Some important properties relating a generalized inequality and its dual are: 

• $x\preceq_{K}y$ if and only if $\lambda^{T}x\leq\lambda^{T}y$ for all $\lambda\succeq_{K^{*}}0$ . 
• $x\prec_{K}y$ if and only if $\lambda^{T}x<\lambda^{T}y$ for all $\lambda\succeq_{K^{*}}0$ , $\lambda\neq0$ . 

Since $K=K^{**}$ , the dual generalized inequality associated with $\preceq_{K^{*}}$ is $\preceq_{K}$ , so these properties hold if the generalized inequality and its dual are swapped. As a specific example, we have $\lambda\preceq_{K^{*}}\mu$ if and only if $\lambda^{T}x\leq\mu^{T}x$ for all $x\succeq_{K}$ 0. 

Example 2.26 Theorem of alternatives for linear strict generalized inequalities. Sup- pose $K\subseteq\mathbf{R}^{m}$ is a proper cone. Consider the strict generalized inequality 

$$
A x\prec_{K}b,
$$ 
where $x\in\mathbf{R}^{n}$ 

We will derive a theorem of alternatives for this inequality. Suppose it is infeasible, i.e. , the affine set $\{b-A x\mid x\in\mathbf{R}^{n}\}$ does not intersect the open convex set $\mathrm{int}\,K$ . Then there is a separating hyperplane, i.e. , a nonzero $\lambda\in\mathbf{R}^{m}$ and $\mu\in\mathbf{R}$ such that $\lambda^{T}(b-A x)\leq\mu$ for all $x$ , and $\lambda^{T}y\geq\mu$ for all $y\in\mathbf{int}\,K$ . The first condition implies $A^{T}\lambda=0$ = 0 and $\lambda^{T}b\leq\mu$ . The second condition implies $\lambda^{T}y\geq\mu$ for all $y\in K$ , which can only happen if $\lambda\in K^{*}$ and $\mu\leq0$ . 

Putting it all together we find that if ( 2.21 ) is infeasible, then there exists $\lambda$ such that 

$$
\begin{array}{r}{\lambda\neq0,\qquad\lambda\succeq_{K^{*}}0,\qquad A^{T}\lambda=0,\qquad\lambda^{T}b\leq0.}\end{array}
$$ 
Now we show the converse: if ( 2.22 ) holds, then the inequality system ( 2.21 ) cannot be feasible. Suppose that both inequality systems hold. The $\lambda^{T}(b-A x)>$ $0$ , since $\lambda\ \ne\ 0$ , $\lambda\ \succeq_{K^{*}}\ 0$ , and $b\,-\,A x\,\,\succ_{\,K}\,\,0$ . But using A $A^{T}\lambda\,=\,0$ = 0 we find that $\lambda^{T}(b-A x)=\lambda^{T}b\leq0$ , which is a contradiction. 

Thus, the inequality systems ( 2.21 ) and ( 2.22 ) are alternatives: for any data $A$ , $b$ , exactly one of them is feasible. (This generalizes the alternatives ( 2.17 ), ( 2.18 ) for the special case $K=\mathbf{R}_{+}^{m}$ .) 
### 2.6.3 Minimum and minimal elements via dual inequalities 
We can use dual generalized inequalities to characterize minimum and minimal elements of a (possibly nonconvex) t $S\subseteq\mathbf{R}^{m}$ with respect to the generalized inequality induced by a proper cone K . 
#### Dual characterization of minimum element 
We first consider a characterization of the minimum element: $x$ is the minimum element of $S$ , with respect to the gene ed in ty $\preceq_{K}$ , if and only if for all $\lambda\succ_{K^{*}}0$ , $x$ is the unique minimizer of λ $\lambda^{T}z$ over z $z\in S$ ∈ . Geometrically, this means that for any $\lambda\succ_{K^{*}}0$ , the hyperplane 

$$
\{z\mid\lambda^{T}(z-x)=0\}
$$ 
is a strict supporting hyperplane to $S$ at $x$ . (By strict supporting hyperplane, we mean that the hyperplane intersects $S$ only at the point $x$ .) Note that convexity of the set $S$ is not required. This is illustrated in figure 2.23 . 

To show this result, suppose $x$ is the minimum element of $S$ , i.e. , $x\preceq_{K}z$ for ll $z\in S$ , and let $\lambda\succ_{K^{*}}$ $0$ . Let $z\in S$ , $z\neq x$ . Since $x$ is the minimum element of S , we have $z\mathrm{~-~}x\succeq_{K}0$ . From $\lambda\succ_{K}$ ∗ 0 and $z\mathrm{~-~}x\succeq_{K}0$ , $z-x\neq0$ , we conclude $\lambda^{T}(z\mathrm{~-~}x)\,>\,0$ . Since $z$ is an arb ry element of S , not equal to $x$ , this shows that $x$ the unique minimizer of λ $\lambda^{T}z$ $z\in S$ versely, suppose that for all $\lambda\succ_{K^{*}}0$ , $x$ is the unique minimizer of λ $\lambda^{T}z$ over z $z\in S$ ∈ , but $x$ is not the minimum 
 
Figure 2.23 Dual characterization of minimum element. The point $x$ is the minimum element of the set $S$ with respect to $\mathbf{R}_{+}^{2}$ . This is equivalent to: or every $\lambda\succ0$ , he hyperplane $\{z\mid\lambda^{T}(z-x)=0\}$ trictly supports $S$ at x , i.e. , contains S on one side, and touches it only at x . 

element of $S$ . Then there exists $z\in S$ with $z\not\succeq_{K}x$ . Since $z-x\neq_{K}0$ , there exists $\tilde{\lambda}\succeq_{K^{*}}0$ 0 with $\tilde{\lambda}^{T}(z-x)<0$ − 0. Hence $\lambda^{T}(z-x)<0$ for $\lambda\succ_{K^{*}}0$ in the neig hood of λ . This contradicts the assumption that $x$ is the unique minimizer of λ $\lambda^{T}z$ over $S$ . 
#### Dual characterization of minimal elements 
We now turn to a similar characterization of minimal elements . Here there is a gap between the necessary and sufficient conditions. If $\lambda\succ_{K^{*}}0$ and $x$ minimizes $\lambda^{T}z$ over $z\in S$ , then $x$ is minimal. This is illustrated in figure 4 . 

To show this, suppose th $\lambda\succ_{K^{*}}0$ , and $x$ minimizes λ $\lambda^{T}z$ over $S$ , but $x$ is not minimal, i.e. , there exists a z $z\in S$ ∈ , $z\neq x$ , and $z\preceq_{K}x$ . en $\lambda^{T}(x-z)>0$ , which contradicts our assumption that $x$ is the minimizer of λ $\lambda^{T}z$ over $S$ . 

The converse is in general false: a point $x$ can be minimal in $S$ , but not a minimizer of $\lambda^{T}z$ over $z~\in~S$ , for any $\lambda$ , as shown in figure 2.25 . This figure suggests that convexity plays an important role in the converse, which is correct. Provided the set $S$ is convex, we can say that for any minimal element $x$ there exists a nonzero $\lambda\succeq_{K^{*}}$ $0$ such that $x$ minimizes $\lambda^{T}z$ over $z\in S$ . 

To show this, suppose $x$ is minimal, which means that $((x-K)\setminus\{x\})\cap S=\emptyset$ . pplying the separating hyperplane theorem to the convex sets $(x-K)\setminus\{x\}$ and S , we conclude that is a $\lambda\neq0$ and $\mu$ such that $\lambda^{T}(x-y)\leq\mu$ for al $y\in K$ , and $\lambda^{T}z\geq\mu$ for all z $z\in S$ ∈ . From the first inequality we conclude $\lambda\succeq_{K^{*}}$ 0. Since $x\in S$ and $x\in x-K$ , w ve $\lambda^{T}x=\mu$ , so the second inequality im s that $\mu$ is the minimum value of λ $\lambda^{T}z$ over S . Therefore, $x$ is a minimizer of λ $\lambda^{T}z$ over S , where $\lambda\neq0$ , $\lambda\succeq_{K^{*}}$ 0. 

This converse theorem cannot be strengthened to $\lambda\,\,\succ_{\!\!\!K^{*}}\,\,\,0$ . Examples show that a point $x$ can be a minimal point of a convex set S , but not a minimizer of 

$\lambda^{T}z$ over $z\in S$ for any $\lambda\succ_{K^{*}}$ 0. (See figure 2.26 , left.) Nor is it true that any minimizer of λ $\lambda^{T}z$ over $z\in S$ , with $\lambda\succeq_{K^{*}}0$ , is minimal (see figure 2.26 , right.) 

Example 2.27 Pareto optimal production frontier. We consider a product which requires $n$ resources (such as labor, electricity, natural gas, water) to manufacture. The product can be manufactured or produced in many ways. With each production method, we associate a resource vector $x\,\in\,\mathbf{R}^{n}$ , where $x_{i}$ denotes the amount of resource $i$ consumed by the method to manufacture the product. We assume that $x_{i}\geq$ 0 ( i.e. , resources are consumed by the production methods) and that the resources are valuable (so using less of any resource is preferred). 

The production set $P\;\subseteq\;\mathbf{R}^{n}$ is defined as the set of all resource vectors $x$ that correspond to some production method. 

Production methods with resource vectors that are minimal elements of $P$ , with respect to componentwise inequality, are called Pareto optimal or efficient . The set of minimal elements of $P$ is called the efficient production frontier . 

We can give a simple interpretation of Pareto optimality. We say that one production method, with resource vector $x$ , is better than another, with resource vector $y$ , if $x_{i}\,\leq\,y_{i}$ for all $i$ , and for some $i$ , $x_{i}\,<\,y_{i}$ . In other words, one production method is better than another if it uses no more of each resource than another method, and for at least one resource, actually uses less. This corresponds to $x\preceq y,\,x\neq y$ . Then we can say: A production method is Pareto optimal or efficient if there is no better production method. 

We can find Pareto optimal production methods ( i.e. , minimal resource vectors) by minimizing 

$$
\lambda^{T}x=\lambda_{1}x_{1}+\cdot\cdot\cdot+\lambda_{n}x_{n}
$$ 
over the set $P$ of production vectors, using any $\lambda$ that satisfies $\lambda\succ0$ . 

Here the vector $\lambda$ has a simple interpretation: $\lambda_{i}$ is the price of resource $i$ . By minimizing $\lambda^{T}x$ over $P$ we are finding the overall cheapest production method (for the resource prices $\lambda_{i}$ ). As long as the prices are positive, the resulting production method is guaranteed to be efficient. 

These ideas are illustrated in figure 2.27 

Figure 2.27 The production set $P$ , for a product that requires labor and fuel to produce, is shown shaded. The two dark curves show the efficient production frontier. The points $x_{1}$ , $x_{2}$ and $x_{3}$ are efficient. The points $x_{4}$ and $x_{5}$ are not (since in particular, $x_{2}$ corresponds to a production method that uses no more fuel, and less labor). The point $x_{1}$ is also the minimum cost production method for the price vector $\lambda$ (which is positive). The point $x_{2}$ is efficient, but cannot be found by minimizing the total cost $\lambda^{T}x$ for any price vector $\lambda\succeq0$ . 
# Chapter 3 

# Convex functions 

# 3.1 Basic properties and examples 

# 3.1.1 Definition 

A function $f\;:\;\mathbf{R}^{n}\;\rightarrow\;\mathbf{R}$ is convex if $\mathbf{dom}\,f$ is a convex set and if for all $x$ , $y\in\mathbf{dom}\,f$ , and $\theta$ with $0\leq\theta\leq1$ , we have 

$$
f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y).
$$ 

Geometrically, this inequality means that the line segment between $\left(x,f(x)\right)$ and $(y,f(y))$ , which is the chord from $x$ to $y$ , lies above the graph of $f$ (figure 3.1 ). A fu $f$ is strictly convex if strict inequality holds in ( 3.1 ) whenever $x\neq y$ and 0 $0<\theta<1$ 1. We say $f$ is concave if $-f$ is convex, and strictly concave if $-f$ is strictly convex. 

For an affine function we always have equality in ( 3.1 ), so all affine (and therefore also linear) functions are both convex and concave. Conversely, any function that is convex and concave is affine. 

A function is convex if and only if it is convex when restricted to any line that intersects its domain. In other words $f$ is convex if and only if for all $x\in\mathbf{dom}\,f$ and 

![](images/e26427c65e0b8f416a27fb0bc8bf71af91bad75e228cdd6d541af428e1b4bdd7.jpg) 
Figure 3.1 Graph of a convex function. The chord ( i.e. , line segment) be- tween any two points on the graph lies above the graph. 

all $v$ , the function $g(t)=f(x+t v)$ is convex (on its domain, $\{t\mid x+t v\in\mathbf{dom}\,f\}$ ). This property is very useful, since it allows us to check whether a function is convex by restricting it to a line. 

The analysis of convex functions is a well developed field, which we will not pursue in any depth. One simple result, for example, is that a convex function is continuous on the relative interior of its domain; it can have discontinuities only on its relative boundary. 

# 3.1.2 Extended-value extensions 

It is often convenient to extend a convex function to all of $\mathbf{R}^{n}$ by defining its value to be $\infty$ outside its domain. If $f$ is convex we define its extended-value extension $\ddot{f}:\mathbf{R}^{n}\rightarrow\mathbf{R}\cup\{\infty\}$ → ∪{∞} by 

$$
{\tilde{f}}(x)={\left\{\begin{array}{l l}{f(x)}&{x\in\mathbf{dom}\,f}\\ {\infty}&{x\not\in\mathbf{dom}\,f.}\end{array}\right.}
$$ 

The extension $\ddot{f}$ is defined on all $\mathbf{R}^{n}$ , and takes values in $\mathbf{R}\cup\{\infty\}$ . We can recover the domain of the original function $f$ from the extension $\ddot{f}$ ˜ as $\mathbf{dom}\ f=\{x\ |\ {\dot{f}}(x)<$ $\infty\}$ . 

The extension can simplify notation, since we do not need to explicitly describe the domain, or add the qualifier ‘for all $x\in\mathbf{dom}\,f^{\prime}$ every time we refer to $f(x)$ . Consider, for example, the basic defining inequality ( 3.1 ). In terms of the extension $\tilde{f}$ , we can express it as: for $0<\theta<1$ , 

$$
\tilde{f}(\theta x+(1-\theta)y)\le\theta\tilde{f}(x)+(1-\theta)\tilde{f}(y)
$$ 

for any and . (For $\theta=0$ or $\theta=1$ the inequality always holds.) Of course here we $x$ $y$ must interpret the inequality using extended arithmetic and ordering. For $x$ and $y$ both in $\mathbf{dom}\ f$ , this inequality coincides with ( 3.1 ); if either is outside $\mathbf{dom}\,f$ , then the righthand side is $\infty$ , and the inequality therefore holds. As another ample of this notational device, suppose $f_{1}$ and $f_{2}$ are two convex functions on R $\mathbf{R}^{n}$ . The point $f=f_{1}+f_{2}$ is the fu h domain $\mathbf{dom}\,f=\mathbf{dom}\,f_{1}\cap\mathbf{dom}\,f_{2}$ , with $f(x)=f_{1}(x)+f_{2}(x)$ ) for any $x\in\mathbf{dom}\,f$ ∈ . Using extended-value extensions we can simply say that for any $x$ , $\tilde{f}(x)=\tilde{f}_{1}(x)+\tilde{f}_{2}(x)$ ). In this equation the domain of $f$ has b tica s $\mathbf{dom}\,f=\mathbf{dom}\,f_{1}\,\cap\,\mathbf{dom}\,f_{2}$ , since ${\tilde{f}}(x)=\infty$ ∞ whenever $x\notin\mathbf{dom}\,f_{1}$ ̸∈ or $x\not\in\mathbf{dom}\,f_{2}$ ̸∈ . In this example we are relying on extended arithmetic to automatically define the domain. 

In this book we will use the same symbol to denote a convex function and its extension, whenever there is no harm from the ambiguity. This is the same as assuming that all convex functions are implicitly extended, i.e. , are defined as $\infty$ outside their domains. 

![](images/ac7e00e1777f3e75e6ace78ce5239249181284f66f05f6610c5e1a94b855b3cd.jpg) 
Figure 3.2 If $f$ is convex and diﬀerentiable, then $f(x)\!+\!\nabla f(x)^{T}(y\!-\!x)\le f(y)$ for all $x,\ y\in\mathbf{dom}\,f$ . 

is given by 

$$
{\tilde{I}}_{C}(x)={\left\{\begin{array}{l l}{0}&{x\in C}\\ {\infty}&{x\not\in C.}\end{array}\right.}
$$ 

The convex function $\tilde{I}_{C}$ is called the indicator function of the set $C$ . 

We can play several notational tricks with the indicator function $\tilde{I}_{C}$ . For example the problem of minimizing a function $f$ (defined on all of $\mathbf{R}^{n}$ , say) on the set $C$ is the same as minimizing the function $f+I_{C}$ over all of $\mathbf{R}^{n}$ . Indeed, the function $f+\tilde{I}_{C}$ is (by our convention) $f$ restricted to the set $C$ . 

In a similar way we can extend a concave function by defining it to be $-\infty$ outside its domain. 

# 3.1.3 First-order conditions 

Suppose $f$ is diﬀerentiable ( i.e. , its gradient $\nabla f$ exists at each point in $\mathbf{dom}\ f$ , which is open). Then $f$ is convex if and only if $\mathbf{dom}\ f$ is convex and 

$$
f(y)\geq f(x)+\nabla f(x)^{T}(y-x)
$$ 

holds for all $x,\ y\in\mathbf{dom}\ f$ . This inequality is illustrated in figure 3.2 . 

The affine function of $y$ given by $f(x){+}\nabla f(x)^{T}(y{-}x)$ is, of course, the first-order Taylor approximation of $f$ near $x$ . The inequality ( 3.2 ) states that for a convex function, the first-order Taylor approximation is in fact a global underestimator of the function. Conversely, if the first-order Taylor approximation of a function is always a global underestimator of the function, then the function is convex. 

The inequality ( 3.2 ) shows that from local information about a convex function ( i.e. , its value and derivative at a point) we can derive global information ( i.e. , a global underestimator of it). This is perhaps the most important property of convex functions, and explains some of the remarkable properties of convex functions and convex optimization problems. As one simple example, the inequality ( 3.2 ) shows that if $\nabla f(x)=0$ , then for all $y\in\mathbf{dom}\,f$ , $f(y)\geq f(x)$ , i.e. , $x$ is a global minimizer of the function $f$ . 

Strict convexity can also be characterized by a first-order condition: $f$ is strictly convex if and only if $\mathbf{dom}\ f$ is convex and for $x,\ y\in\mathbf{dom}\,f$ , $x\neq y$ , we have 

$$
f(y)>f(x)+\nabla f(x)^{T}(y-x).
$$ 

For concave functions we have the corresponding characterization: $f$ is concave if and only if $\mathbf{dom}\ f$ is convex and 

$$
f(y)\leq f(x)+\nabla f(x)^{T}(y-x)
$$ 

for all $x,\ y\in\mathbf{dom}\ f$ . 

# Proof of first-order convexity condition 

To prove ( 3.2 ), we first consider the case $n\,=\,1$ : We show that a diﬀerentiable function $f:\mathbf{R}\rightarrow\mathbf{R}$ is convex if and only if 

$$
f(y)\geq f(x)+f^{\prime}(x)(y-x)
$$ 

for all $x$ and $y$ in $\mathbf{dom}\ f$ . 

Assume first that $f$ is convex and $x,\ y\in\mathbf{dom}\ f$ . Since $\mathbf{dom}\ f$ is convex ( i.e. , an interval), we conclude that for all $0<t\le1$ , $x+t(y-x)\,\in\,\mathbf{dom}\,f$ , and by convexity of $f$ , 

$$
f(x+t(y-x))\leq(1-t)f(x)+t f(y).
$$ 

If we divide both sides by $t$ , we obtain 

$$
f(y)\geq f(x)+{\frac{f(x+t(y-x))-f(x)}{t}},
$$ 

and taking the limit as $t\rightarrow0$ yields ( 3.4 ). 

To show sufficiency, assume the function satisfies ( 3.4 ) for all $x$ and $y$ in $\mathbf{dom}\ f$ (which is an interval). Choose any $x\neq y$ , and $0\leq\theta\leq1$ , and let $z=\theta x+(1-\theta)y$ . Applying ( 3.4 ) twice yields 

$$
f(x)\geq f(z)+f^{\prime}(z)(x-z),\qquad f(y)\geq f(z)+f^{\prime}(z)(y-z).
$$ 

Multiplying the first inequality by $\theta$ , the second by $1-\theta$ , and adding them yields 

$$
\theta f(x)+(1-\theta)f(y)\geq f(z),
$$ 

which proves that $f$ is convex. 

Now we can prove the general case, with $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ . Let $x,\ y\in\mathbf{R}^{n}$ and consider $f$ restricted to the line passing through them, i.e. , the function defined by $g(t)=f(t y+(1-t)x)$ , so $g^{\prime}(t)=\nabla f(t y+(1-t)x)^{T}(y-x)$ . 

First assume $f$ is convex, which implies $g$ is convex, so by the argument above we have $g(1)\geq g(0)+g^{\prime}(0)$ , which means 

$$
f(y)\geq f(x)+\nabla f(x)^{T}(y-x).
$$ 

Now assume that this inequality holds for any $x$ and $y$ , so if $\;t y+(1-t)x\in\mathbf{dom}\,f$ and $\tilde{t}y+(1-\tilde{t})x\in\mathbf{dom}\,f$ − ∈ , we have 

$$
f(t y+(1-t)x)\geq f(\tilde{t}y+(1-\tilde{t})x)+\nabla f(\tilde{t}y+(1-\tilde{t})x)^{T}(y-x)(t-\tilde{t}),
$$ 

i.e. , $g(t)\geq g(\dot{t})+g^{\prime}(\dot{t})(t-\dot{t})$ − ). We have seen that this implies that $g$ is convex. 

# 3.1.4 Second-order conditions 

We now assume that $f$ is twice diﬀerentiable, that is, its Hessian or second deriva- tive $\nabla^{2}f$ exists at each point in $\mathbf{dom}\ f$ , which is open. Then $f$ is convex if and only if $\mathbf{dom}\ f$ is convex and its Hessian is positive semidefinite: for all $x\in\mathbf{dom}\,f$ , 

$$
\nabla^{2}f(x)\succeq0.
$$ 

For a function on $\mathbf{R}$ , this reduces to the simple condition $f^{\prime\prime}(x)\geq0$ (and $\mathbf{dom}\ f$ convex, i.e. , an interval), which means that the derivative is nondecreasing. The condition $\nabla^{2}f(x)\succeq0$ can be interpreted geometrically as the requirement that the graph of the function have positive (upward) curvature at $x$ . We leave the proof of the second-order condition as an exercise (exercise 3.8 ). 

Similarly, $f$ is concave if and only if $\mathbf{dom}\ f$ is convex and $\nabla^{2}f(x)\,\preceq\,0$ for all $x\,\in\,\mathbf{dom}\,f$ . Strict convexity can be partially characterized by second-order conditions. If $\nabla^{2}f(x)\succ\ 0$ for all $x\;\in\;\mathbf{dom}\,f$ , then $f$ is strictly convex. The converse, however, is not true: for example, the function $f:\mathbf{R}\rightarrow\mathbf{R}$ given by $f(x)=x^{4}$ is strictly convex but has zero second derivative at $x=0$ . 

Example 3.2 Quadratic functions. Consider the quadratic function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , with dom $f=\mathbf{R}^{n}$ , given by 

$$
f(\boldsymbol{x})=(1/2)\boldsymbol{x}^{T}\boldsymbol{P}\boldsymbol{x}+\boldsymbol{q}^{T}\boldsymbol{x}+\boldsymbol{r},
$$ 

with $P\in\mathbf{S}^{n}$ , $q\in\mathbf{R}^{n}$ , and $r\in\mathbf{R}$ . Since $\nabla^{2}f(x)=P$ for all $x$ , $f$ is convex if and only if $P\succeq0$ (and concave if and only if $P\preceq0$ ). 

For quadratic functions, strict convexity is easily characterized: $f$ is strictly convex if and only if $P\succ0$ (and strictly concave if and only if $P\prec0$ ). 

Remark 3.1 The separate requirement that dom $f$ be convex cannot be dropped from the first- or second-order characterizations of convexity and concavity. For example, the function $f(x)=1/x^{2}$ , with $\mathbf{dom}\ f=\left\{x\in\mathbf{R}\ |\ x\neq0\right\}$ , satisfies $f^{\prime\prime}(x)>0$ for all $x\in\mathbf{dom}\ f$ , but is not a convex function. 

# 3.1.5 Examples 

We have already mentioned that all linear and affine functions are convex (and concave), and have described the convex and concave quadratic functions. In this section we give a few more examples of convex and concave functions. We start with some functions on $\mathbf{R}$ , with variable $x$ . 

• Exponential. $e^{a x}$ is convex on $\mathbf{R}$ , for any $a\in\mathbf{R}$ . • Powers. $x^{u}$ is convex on $\mathbf{R}_{++}$ when $a\geq1$ or $a\leq0$ , and concave for $0\leq a\leq1$ . • Powers of absolute value. $|x|^{p}$ , for $p\geq1$ , is convex on $\mathbf{R}$ . • Logarithm. $\log x$ is concave on $\mathbf{R}_{++}$ . 

![](images/d7590753794cbebcafb1ff1b46211288d7994c470251fb261ef3fa54a6f30fc6.jpg) 
Figure 3.3 Graph of $f(x,y)=x^{2}/y$ . 

• Negative entropy. $x\log x$ (either on $\mathbf{R}_{++}$ , or on $\mathbf{R}_{+}$ , defined as $0$ for $x=0$ ) is convex. 

Convexity or concavity of these examples can be shown by verifying the ba- sic inequality ( 3.1 ), or by checking that the second derivative is nonnegative or nonpositive. For example, with $f(x)=x\log x$ we have 

$$
f^{\prime}(x)=\log x+1,\qquad f^{\prime\prime}(x)=1/x,
$$ 

so that $f^{\prime\prime}(x)\,>\,0$ for $x\,>\,0$ . This shows that the negative entropy function is (strictly) convex. 

We now give a few interesting examples of functions on $\mathbf{R}^{n}$ 

Norms. Every norm on $\mathbf{R}^{n}$ is convex. 

Max function. $f(x)=\operatorname*{max}\{x_{1},.\,.\,.\,,x_{n}\}$ is convex on $\mathbf{R}^{n}$ . 

• Quadratic-over-linear function. The function $f(x,y)=x^{2}/y$ , with 

$$
\mathbf{dom}\ f=\mathbf{R}\times\mathbf{R}_{++}=\{(x,y)\in\mathbf{R}^{2}\ |\ y>0\},
$$ 

is convex (figure 3.3 ). 

• Log-sum-exp. The function $f(x)\,=\,\log\left(e^{x_{1}}+\cdot\cdot\cdot+e^{x_{n}}\right)$ is convex on $\mathbf{R}^{n}$ . This function can be interpreted as a diﬀerentiable (in fact, analytic) approx- imation of the max function, since 

$$
\operatorname*{max}\{x_{1},\ldots,x_{n}\}\leq f(x)\leq\operatorname*{max}\{x_{1},\ldots,x_{n}\}+\log n
$$ 

for all $x$ . (The second inequality is tight when all components of $x$ are equal.) Figure 3.4 shows $f$ for $n=2$ . 

![](images/9c2c7d5884028e72567622a48d6f46a92a2fc470d6c0d1724621944773f174bf.jpg) 
Figure 3.4 Graph of $f(x,y)=\log(e^{x}+e^{y})$ . 

• Geom an. The geometric mean $\begin{array}{r}{f(\boldsymbol{x})\,=\,\left(\prod_{i=1}^{n}x_{i}\right)^{1/n}}\end{array}$ is concave on n dom $f=\mathbf{R}_{++}^{n}$ . • Log-determinant. The function $f(X)\,=\,\log\operatorname*{det}X$ is concave on $\mathbf{dom}\ f\ =$ $\mathbf{S}_{++}^{n}$ . 

Convexity (or concavity) of these examples can be verified in several ways, such as directly verifying the inequality ( 3.1 ), verifying that the Hessian is positive semidefinite, or restricting the function to an arbitrary line and verifying convexity of the resulting function of one variable. 

Norms. If $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is a norm, and $0\leq\theta\leq1$ , then 

$$
f(\theta x+(1-\theta)y)\leq f(\theta x)+f((1-\theta)y)=\theta f(x)+(1-\theta)f(y).
$$ 

The inequality follows from the triangle inequality, and the equality follows from homogeneity of a norm. 

Max function. The function $f(x)=\operatorname*{max}_{i}x_{i}$ satisfies, for $0\leq\theta\leq1$ , 

$$
\begin{array}{r c l}{f(\theta x+(1-\theta)y)}&{=}&{\displaystyle\operatorname*{max}_{i}(\theta x_{i}+(1-\theta)y_{i})}\\ &{\leq}&{\displaystyle\theta\operatorname*{max}_{i}x_{i}+(1-\theta)\operatorname*{max}_{i}y_{i}}\\ &{=}&{\displaystyle\theta f(x)+(1-\theta)f(y).}\end{array}
$$ 

Quadratic-over-linear function. To show that the quadratic-over-linear function $f(x,y)=x^{2}/y$ is convex, we note that (for $y>0$ ), 

$$
\nabla^{2}f(x,y)={\frac{2}{y^{3}}}\left[{\begin{array}{c c}{y^{2}}&{-x y}\\ {-x y}&{x^{2}}\end{array}}\right]={\frac{2}{y^{3}}}\left[{\begin{array}{c}{y}\\ {-x}\end{array}}\right]\left[{\begin{array}{c}{y}\\ {-x}\end{array}}\right]^{T}\succeq0.
$$ 

Log-sum-exp. The Hessian of the log-sum-exp function is 

$$
\nabla^{2}f(x)=\frac{1}{({\bf1}^{T}z)^{2}}\left(({\bf1}^{T}z)\,{\bf d i a g}(z)-z z^{T}\right),
$$ 

where $z=\left(e^{x_{1}},\cdot\cdot\cdot,e^{x_{n}}\right)$ . To verify that $\nabla^{2}f(x)\succeq0$ we must show that for all $v$ , $v^{T}\nabla^{2}f(x)v\geq0$ , i.e. , 

$$
v^{T}\nabla^{2}f(x)v={\frac{1}{(1^{T}z)^{2}}}\left(\left(\sum_{i=1}^{n}z_{i}\right)\left(\sum_{i=1}^{n}v_{i}^{2}z_{i}\right)-\left(\sum_{i=1}^{n}v_{i}z_{i}\right)^{2}\right)\geq0.
$$ 

But this follows from the Cauchy-Schwarz inequality $(a^{T}a)(b^{T}b)\geq(a^{T}b)^{2}$ applied to the vectors with components $a_{i}=v_{i}\sqrt{z_{i}}$ , $b_{i}=\sqrt{z_{i}}$ . 

Geometric mean. In a similar way we can show that the geometric mean $f(x)=$ $\scriptstyle\left(\prod_{i=1}^{n}x_{i}\right)^{1/n}$ is concave on $\mathbf{dom}\,f=\mathbf{R}_{++}^{n}$ . Its Hessian $\nabla^{2}f(x)$ is given by 

$$
\frac{\partial^{2}f(x)}{\partial x_{k}^{2}}=-(n-1)\frac{\left(\prod_{i=1}^{n}x_{i}\right)^{1/n}}{n^{2}x_{k}^{2}},\qquad\frac{\partial^{2}f(x)}{\partial x_{k}\partial x_{l}}=\frac{\left(\prod_{i=1}^{n}x_{i}\right)^{1/n}}{n^{2}x_{k}x_{l}}\quad\mathrm{for~}k\neq l,
$$ 

and can be expressed as 

$$
\nabla^{2}f(x)=-{\frac{\prod_{i=1}^{n}x_{i}^{1/n}}{n^{2}}}\left(n\,\mathbf{diag}(1/x_{1}^{2},\dots,1/x_{n}^{2})-q q^{T}\right)
$$ 

where $q_{i}=1/x_{i}$ . We must show that $\nabla^{2}f(x)\preceq0$ , i.e. , that 

$$
v^{T}\nabla^{2}f(x)v=-{\frac{\prod_{i=1}^{n}x_{i}^{1/n}}{n^{2}}}\left(n\sum_{i=1}^{n}v_{i}^{2}/x_{i}^{2}-\left(\sum_{i=1}^{n}v_{i}/x_{i}\right)^{2}\right)\leq0
$$ 

for all $v$ . Again this follows the Cauchy-Schwarz inequality $(a^{T}a)(b^{T}b)\,\geq$ $(a^{T}b)^{2}$ , applied to the vectors a $a=\mathbf{1}$ 1 and $b_{i}=v_{i}/x_{i}$ . 

Log-determinant. For the function $f(X)=\log\operatorname*{det}X$ , we can verify concavity by considering an arbitrary line, given by $X=Z+t V$ , where $Z,\,\,V\in\mathbf{S}^{n}$ . We define $g(t)=f(Z+t V)$ , and restrict $g$ to the interval of s of t for which $Z+t V\succ0$ . Without loss of generality, we can assume that t = 0 is inside this interval, i.e. , $Z\succ0$ . We have 

$$
\begin{array}{r c l}{{g(t)}}&{{=}}&{{\log\operatorname*{det}(Z+t V)}}\\ {{}}&{{=}}&{{\log\operatorname*{det}(Z^{1/2}(I+t Z^{-1/2}V Z^{-1/2})Z^{1/2})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{i=1}^{n}\log(1+t\lambda_{i})+\log\operatorname*{det}Z}}\end{array}
$$ 

where $\lambda_{1},.\cdot\cdot,\lambda_{n}$ are the eigenvalues of $Z^{-1/2}V Z^{-1/2}$ . Therefore we have 

$$
g^{\prime}(t)=\sum_{i=1}^{n}{\frac{\lambda_{i}}{1+t\lambda_{i}}},\qquad g^{\prime\prime}(t)=-\sum_{i=1}^{n}{\frac{\lambda_{i}^{2}}{(1+t\lambda_{i})^{2}}}.
$$ 

Since $g^{\prime\prime}(t)\le0$ , we conclude that $f$ is concave. 

# 3.1.6 Sublevel sets 

The $\alpha$ -sublevel set of a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is defined as 

$$
C_{\alpha}=\{x\in\mathbf{dom}\ f\ |\ f(x)\leq\alpha\}.
$$ 

Sublevel sets of a convex function are convex, for any value of $\alpha$ . The proof is immediate from the definition of convexity: if $x,\ y\,\in\,C_{\alpha}$ , then $f(x)\,\leq\,\alpha$ and $f(y)\leq\alpha$ , and so $f(\theta x+(1-\theta)y)\leq\alpha$ for $0\leq\theta\leq1$ , and hence $\theta x+(1-\theta)y\in C_{\alpha}$ . 

The converse is not true: a function can have all its sublevel sets convex, but not be a convex function. For example, $f(x)=-e^{x}$ is not convex on $\mathbf{R}$ (indeed, it is strictly concave) but all its sublevel sets are convex. 

If $f$ is concave, then its $\alpha$ -superlevel set , given by $\{x\in\mathbf{dom}\ f\ |\ f(x)\geq\alpha\}$ , is a convex set. The sublevel set property is often a good way to establish convexity of a set, by expressing it as a sublevel set of a convex function, or as the superlevel set of a concave function. 

Example 3.3 The geometric and arithmetic means of $x\in\mathbf{R}_{+}^{n}$ are, respectively, 

$$
G(x)=\left(\prod_{i=1}^{n}x_{i}\right)^{1/n},\qquad A(x)=\frac1n\sum_{i=1}^{n}x_{i},
$$ 

(where we take $0^{1/n}\,=\,0$ in our definition of $G$ ). The arithmetic-geometric mean inequality states that $G(x)\leq A(x)$ . 

Suppose $0\leq\alpha\leq1$ , and consider the set 

$$
\{x\in\mathbf{R}_{+}^{n}\mid G(x)\geq\alpha A(x)\},
$$ 

i.e. , the set of vectors with geometric mean at least as large as a factor $\alpha$ times the arithmetic mean. This set is convex, since it is the 0-superlevel set of the function $\boldsymbol{G}(\boldsymbol{x})-\alpha\boldsymbol{A}(\boldsymbol{x})$ , which is concave. In fact, the set is positively homogeneous, so it is a convex cone. 

# 3.1.7 Epigraph 

The graph of a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is defined as 

$$
\{(x,f(x))\mid x\in\mathbf{dom}\,f\},
$$ 

which is a subset of $\mathbf{R}^{n+1}$ . The epigraph of a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is defined as 

$$
\mathbf{epi}\,f=\{(x,t)\mid x\in\mathbf{dom}\,f,\ f(x)\leq t\},
$$ 

which is a subset of $\mathbf{R}^{n+1}$ . (‘Epi’ means ‘above’ so epigraph means ‘above the graph’.) The definition is illustrated in figure 3.5 . 

The link between convex sets and convex functions is via the epigraph: A function is convex if and only if its epigraph is a convex set. A function is concave if and only if its hypograph , defined as 

$$
\mathbf{hypo}\ f=\{(x,t)\mid t\leq f(x)\},
$$ 

is a convex set. 

![](images/cef8342625285ef9ebe5dc6eb05fd7ea00afcf5aa8149248f9f7d94c76bf27d5.jpg) 
Figure 3.5 Epigraph of a function $f$ , shown shaded. The lower boundary, shown darker, is the graph of $f$ . 

Example 3.4 Matrix fractional function. The function $f:\mathbf{R}^{n}\times\mathbf{S}^{n}\rightarrow\mathbf{R}$ , defined as 

$$
f(x,Y)=x^{T}Y^{-1}x
$$ 

is convex on dom $f=\mathbf{R}^{n}\times\mathbf{S}_{++}^{n}$ . (This generalizes the quadratic-over-linear function $f(x,y)=x^{2}/y$ , with $\mathbf{dom}\ f=\mathbf{R}\times\mathbf{R}_{++}$ .) 

One easy way to establish convexity of $f$ is via its epigraph: 

$$
\begin{array}{r l r}{\mathbf{epi}\,f}&{=}&{\left\{(x,Y,t)\mid Y\succ0,\ x^{T}Y^{-1}x\leq t\right\}}\\ &{=}&{\left\{(x,Y,t)\enspace\left|\begin{array}{l l}{\left[\begin{array}{l l}{Y}&{x}\\ {x^{T}}&{t}\end{array}\right]\succeq0,\ Y\succ0}\end{array}\right.\right\}\,,}\end{array}
$$ 

using the Schur complement condition for positive semidefiniteness of a block matrix (see $\S$ A.5.5 ). The last condition is a linear matrix inequality in $(x,Y,t)$ , and therefore $\mathbf{\epsilon}_{f}$ is convex. 

For the special case $n=1$ , the matrix fractional function reduces to the quadratic- over-linear function $x^{2}/y$ , and the associated LMI representation is 

$$
\left[\begin{array}{c c}{{y}}&{{x}}\\ {{x}}&{{t}}\end{array}\right]\succeq0,~~~~~~y>0
$$ 

(the graph of which is shown in figure 3.3 ). 

Many results for convex functions can be proved (or interpreted) geometrically using epigraphs, and applying results for convex sets. As an example, consider the first-order condition for convexity: 

$$
f(y)\geq f(x)+\nabla f(x)^{T}(y-x),
$$ 

where $f$ is convex and $x,\ y\ \in\ \mathbf{dom}\ f$ . We can interpret this basic inequality geometrically in terms of $\mathbf{\epsilon}_{f}$ . If $(y,t)\in\mathbf{epi}\,f$ , then 

$$
t\geq f(y)\geq f(x)+\nabla f(x)^{T}(y-x).
$$ 

![](images/4025ed15773bc532f7fa5522c9dda2f669f8f6bf6592e1fb2f27c86adeb75917.jpg) 
Figure 3.6 For a diﬀerentiable convex function $f$ , he ector $(\nabla f(x),-1)$ defines a supporting hyperplane to the epigraph of f at x . 

We can express this as: 

$$
(y,t)\in\mathbf{epi}\,f\implies\left[{\begin{array}{c}{\nabla f(x)}\\ {-1}\end{array}}\right]^{T}\left(\left[\begin{array}{c}{y}\\ {t}\end{array}\right]-\left[\begin{array}{c}{x}\\ {f(x)}\end{array}\right]\right)\leq0.
$$ 

This means that the hyperplane defined by $(\nabla f(x),-1)$ supports $\mathbf{\epsilon}_{f}$ at the boundary point $\left(x,f(x)\right)$ ; see figure 3.6 . 

# 3.1.8 Jensen’s inequality and extensions 

The basic inequality ( 3.1 ), i.e. , 

$$
f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y),
$$ 

is sometimes called Jensen’s inequality . It is easily extended to convex combinations of m nts: If $f$ is convex, $x_{1},.\,.\,.\,,x_{k}\,\in\,\mathbf{dom}\,f$ , and $\theta_{1},.\ldots,\theta_{k}\,\geq\,0$ with θ $\theta_{1}+\cdot\cdot\cdot+\theta_{k}=1$ · · · = 1, then 

$$
f{\big(}\theta_{1}x_{1}+\cdot\cdot\cdot+\theta_{k}x_{k}{\big)}\leq\theta_{1}f(x_{1})+\cdot\cdot\cdot+\theta_{k}f(x_{k}).
$$ 

As in the case of convex sets, the inequality extends to infinite sums, integrals, and expected values. For example, if $p(x)\geq0$ on $S\subseteq\mathbf{dom}\,f$ , $\textstyle\int_{S}p(x)\ d x=1$ = 1, then 

$$
f\left(\int_{S}p(x)x\;d x\right)\leq\int_{S}f(x)p(x)\;d x,
$$ 

provided the integrals exist. In the most general case we can take any probability measure with support in $\mathbf{dom}\,f$ . If $x$ is a random variable such that $x\in\mathbf{dom}\,f$ with probability one, and $f$ is convex, then we have 

$$
f(\mathbf{E}\,x)\leq\mathbf{E}\,f(x),
$$ 

provided the expectations exist. We can recover the basic inequality ( 3.1 ) from this general form, by taking the random variable $x$ to have support $\{x_{1},x_{2}\}$ , with $\mathbf{prob}(x=x_{1})=\theta$ , $\mathbf{prob}(x=x_{2})=1-\theta$ . Thus the inequality ( 3.5 ) characterizes convexity: If $f$ is not convex, there is a random variable $x$ , with $x\in\mathbf{dom}\ f$ with probability one, such that $f(\mathbf{E}\,x)>\mathbf{E}\,f(x)$ . 

All of these inequalities are now called Jensen’s inequality , even though the inequality studied by Jensen was the very simple one 

$$
f\left({\frac{x+y}{2}}\right)\leq{\frac{f(x)+f(y)}{2}}.
$$ 

Remark 3.2 We can interpret ( 3 as follows. Suppose $x\in\mathbf{dom}\,f\subseteq\mathbf{R}^{n}$ and $z$ is n any zero mean random vector in R $\mathbf{R}^{n}$ . Then we have 

$$
\mathbf{E}\,f(x+z)\geq f(x).
$$ 

Thus, randomization or dithering ( i.e. , adding a zero mean random vector to the argument) cannot decrease the value of a convex function on average. 

# 3.1.9 Inequalities 

Many famous inequalities can be derived by applying Jensen’s inequality to some appropriate convex function. (Indeed, convexity and Jensen’s inequality can be made the foundation of a theory of inequalities.) As a simple example, consider the arithmetic-geometric mean inequality: 

$$
{\sqrt{a b}}\leq(a+b)/2
$$ 

for $a,b\geq0$ . The function $-\log x$ is convex; Jensen’s inequality with $\theta=1/2$ yields 

$$
-\log\left({\frac{a+b}{2}}\right)\leq{\frac{-\log a-\log b}{2}}.
$$ 

Taking the exponential of both sides yields ( 3.6 ). 

As a less trivial example we prove H¨ older’s inequality: for $p>1$ , $1/p+1/q=1$ , and $x,\ y\in\mathbf{R}^{n}$ , 

$$
\sum_{i=1}^{n}x_{i}y_{i}\leq\left(\sum_{i=1}^{n}|x_{i}|^{p}\right)^{1/p}\left(\sum_{i=1}^{n}|y_{i}|^{q}\right)^{1/q}.
$$ 

By convexity of $-\log x$ , and Jensen’s inequality with general $\theta$ , we obtain the more general arithmetic-geometric mean inequality 

$$
a^{\theta}b^{1-\theta}\leq\theta a+(1-\theta)b,
$$ 

valid for $a,\ b\geq0$ and $0\leq\theta\leq1$ . Applying this with 

$$
a=\frac{|x_{i}|^{p}}{\sum_{j=1}^{n}|x_{j}|^{p}},\qquad b=\frac{|y_{i}|^{q}}{\sum_{j=1}^{n}|y_{j}|^{q}},\qquad\theta=1/p,
$$ 

yields 

$$
\left(\frac{|x_{i}|^{p}}{\sum_{j=1}^{n}|x_{j}|^{p}}\right)^{1/p}\left(\frac{|y_{i}|^{q}}{\sum_{j=1}^{n}|y_{j}|^{q}}\right)^{1/q}\leq\frac{|x_{i}|^{p}}{p\sum_{j=1}^{n}|x_{j}|^{p}}+\frac{|y_{i}|^{q}}{q\sum_{j=1}^{n}|y_{j}|^{q}}.
$$ 

Summing over $i$ then yields H¨ older’s inequality. 

# 3.2 Operations that preserve convexity 

In this section we describe some operations that preserve convexity or concavity of functions, or allow us to construct new convex and concave functions. We start with some simple operations such as addition, scaling, and pointwise supremum, and then describe some more sophisticated operations (some of which include the simple operations as special cases). 

# 3.2.1 Nonnegative weighted sums 

Evidently if $f$ is a convex function and $\alpha\,\geq\,0$ , then the function $\alpha f$ is convex. If $f_{1}$ and $f_{2}$ are both convex functions, then so is their sum $f_{\mathrm{1}}+f_{\mathrm{2}}$ . Combining nonnegative scaling and addition, we see that the set of convex functions is itself a convex cone: a nonnegative weighted sum of convex functions, 

$$
f=w_{1}f_{1}+\cdot\cdot\cdot+w_{m}f_{m},
$$ 

is convex. Similarly, a nonnegative weighted sum of concave functions is concave. A nonnegative, nonzero weighted sum of strictly convex (concave) functions is strictly convex (concave). 

These properties extend to infinite sums and integrals. For example if $f(x,y)$ is convex in $x$ for each $y\in\mathcal A$ , and $w(y)\geq0$ for each $y\in\mathcal A$ , then the function $g$ defined as 

$$
g(x)=\int_{\mathcal{A}}w(y)f(x,y)\ d y
$$ 

is convex in $x$ (provided the integral exists). 

The fact that convexity is preserved under nonnegative scaling and addition is easily verified directly, or can be seen in terms of the associated epigraphs. For example, if $w\geq0$ and $f$ is convex, we have 

$$
\mathbf{epi}(w f)=\left[\begin{array}{l l}{I}&{0}\\ {0}&{w}\end{array}\right]\mathbf{epi}\,f,
$$ 

which is convex because the image of a convex set under a linear mapping is convex. 

# 3.2.2 Composition with an affine mapping 

Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $A\in\mathbf{R}^{n\times m}$ , and $b\in\mathbf{R}^{n}$ . Define $g:\mathbf{R}^{m}\rightarrow\mathbf{R}$ by 

$$
g(x)=f(A x+b),
$$ 

with $\mathbf{dom}\,g=\{x\mid A x+b\in\mathbf{dom}\,f\}$ . Then if $f$ is convex, so is $g$ ; if $f$ is concave, so is $g$ . 

# 3.2.3 Pointwise maximum and supremum 

If $f_{1}$ and $f_{2}$ are convex functions then their pointwise maximum $f$ , defined by 

$$
f(x)=\operatorname*{max}\{f_{1}(x),f_{2}(x)\},
$$ 

$\mathbf{dom}\,f=\mathbf{dom}\,f_{1}\cap\mathbf{dom}\,f_{2}$ , is also convex. This property is easily verified: if $0\leq\theta\leq1$ ≤ ≤ 1 and $x,\ y\in\mathbf{dom}\ f$ ∈ , then 

$$
\begin{array}{r c l}{f(\theta x+(1-\theta)y)}&{=}&{\operatorname*{max}\{f_{1}(\theta x+(1-\theta)y),f_{2}(\theta x+(1-\theta)y)\}}\\ &{\leq}&{\operatorname*{max}\{\theta f_{1}(x)+(1-\theta)f_{1}(y),\theta f_{2}(x)+(1-\theta)f_{2}(y)\}}\\ &{\leq}&{\theta\operatorname*{max}\{f_{1}(x),f_{2}(x)\}+(1-\theta)\operatorname*{max}\{f_{1}(y),f_{2}(y)\}}\\ &{=}&{\theta f(x)+(1-\theta)f(y),}\end{array}
$$ 

which establishes convexity of $f$ . It is easily shown that if $f_{1},\ldots,f_{m}$ are convex, then their pointwise maximum 

$$
f(x)=\operatorname*{max}\{f_{1}(x),.\,.\,.\,,f_{m}(x)\}
$$ 

is also convex. 

Example 3.5 Piecewise-linear functions. The function 

$$
f(x)=\operatorname*{max}\{a_{1}^{T}x+b_{1},.\,.\,.\,,a_{L}^{T}x+b_{L}\}
$$ 

defines a piecewise-linear (or really, affine) function (with $L$ or fewer regions). It is convex since it is the pointwise maximum of affine functions. 

The converse can also be shown: any piecewise-linear convex function with $L$ or fewer regions can be expressed in this form. (See exercise 3.29 .) 

Example 3.6 Sum of $T^{\star}$ largest components. For $x\,\in\,\mathbf{R}^{\,n}$ we denote by $x_{[i]}$ the i th largest component of $x$ , i.e. , 

$$
x_{[1]}\geq x_{[2]}\geq\cdot\cdot\geq x_{[n]}
$$ 

are the components of $_{x}$ sorted in nonincreasing order. Then the function 

$$
f(x)=\sum_{i=1}^{r}x_{[i]},
$$ 

i.e. , the sum of the $r$ largest elements of $x$ , is a convex function. This can be seen by writing it as 

$$
f(x)=\sum_{i=1}^{r}x_{[i]}=\operatorname*{max}\{x_{i_{1}}+\cdot\cdot\cdot+x_{i_{r}}\mid1\leq i_{1}<i_{2}<\cdot\cdot\cdot<i_{r}\leq n\},
$$ 

i.e. , the maximum of all possible sums of $r$ diﬀerent components of $x$ . Since it is the pointwise maximum of $n!/(r!(n-r)!)$ linear functions, it is convex. 

be shown that the function $\begin{array}{r}{\sum_{i=1}^{r}w_{i}x_{[i]}}\end{array}$ ] is convex, provided $w_{1}\geq w_{2}\geq\cdot\cdot\cdot\geq w_{r}\geq0$ ≥ ≥· · · ≥ ≥ 0. (See exercise 3.19 .) 

The pointwise maximum property extends to the pointwise supremum over an infinite set of convex functions. If for each $y\in\mathcal A$ , $f(x,y)$ is convex in $x$ , then the function $g$ , defined as 

$$
g(x)=\operatorname*{sup}_{y\in A}f(x,y)
$$ 

is convex in $x$ . Here the domain of $g$ is 

$$
\mathbf{dom}\,g=\{x\mid(x,y)\in\mathbf{dom}\,f\ \mathrm{for~all}\ y\in\mathcal{A},\ \operatorname*{sup}_{y\in\mathcal{A}}f(x,y)<\infty\}.
$$ 

Similarly, the pointwise infimum of a set of concave functions is a concave function. In terms of epigraphs, the pointwise supremum of functions corresponds to the intersection of epigraphs: with $f$ , $g$ , and $\mathcal{A}$ as defined in ( 3.7 ), we have 

$$
\mathbf{epi}\,g=\bigcap_{y\in\mathcal{A}}\mathbf{epi}\,f(\cdot,y).
$$ 

Thus, the result follows from the fact that the intersection of a family of convex sets is convex. 

Example 3.7 Support function of a set. Let $C\,\subseteq\,\mathbf{R}^{n}$ , with $C\,\neq\,\emptyset$ . The support function $S_{C}$ associated with the set $C$ is defined as 

$$
S_{C}(x)=\operatorname*{sup}\{x^{T}y\mid y\in C\}
$$ 

(and, naturally, $\begin{array}{r}{\mathbf{dom}\,S_{C}=\{x\ |\ \operatorname*{sup}_{y\in C}x^{T}y<\infty\})}\end{array}$ . 

For each $y\in C$ , $x^{T}y$ is a linear function of $x$ , so $S_{C}$ is the pointwise supremum of a family of linear functions, hence convex. 

Example 3.8 Distance to farth st point of $a$ set. Let $C\subseteq\mathbf{R}^{n}$ . The distance (in any norm) to the farthest point of C , 

$$
f(x)=\operatorname*{sup}_{y\in C}\|x-y\|,
$$ 

is convex. To see this, note that for any $y$ , the function $\|x-y\|$ is convex in $x$ . Since $f$ is the pointwise supremum of a family of convex functions (indexed by $y\in C$ ), it is a convex function of $x$ . 

Example 3.9 Least-squares cost as a function of weights. Let $a_{1},\cdot\cdot\cdot,a_{n}\in\mathbf{R}^{m}$ . In a ghted uares proble we minimize the objective func n $\begin{array}{r l r}{\lefteqn{\sum_{i=1}^{n}w_{i}\big(a_{i}^{T}x-}}&{{}}&{}\end{array}$ $b_{i}\big)^{2}$ over x $x\in\mathbf{R}^{m}$ ∈ . We refer to w as weights , and allow negative w (which opens the i possibility that the objective function is unbounded below). 

We define the (optimal) weighted least-squares cost as 

$$
g(w)=\operatorname*{inf}_{x}\sum_{i=1}^{n}w_{i}\big(a_{i}^{T}x-b_{i}\big)^{2},
$$ 

with domain 

$$
\mathbf{dom}\,g=\left\{w\,\left|\,\operatorname*{inf}_{x}\sum_{i=1}^{n}w_{i}(a_{i}^{T}x-b_{i})^{2}>-\infty\right\right\}.
$$ 

Since $g$ is the infimum of a family of linear functions of $w$ (indexed by $x\in\mathbf{R}^{m}$ ), it is a concave function of $w$ . 

We can derive an explicit expression for $g$ , at least on part of its domain. Let $\boldsymbol{W}\,=\,\mathbf{diag}(\boldsymbol{w})$ , the diagonal matrix with elements $w_{1},\dots,w_{n}$ , and let $A\,\in\,\mathbf{R}^{n\times m}$ have rows $a_{i}^{T}$ , so we have 

$$
g(w)=\operatorname*{inf}_{x}(A x-b)^{T}W(A x-b)=\operatorname*{inf}_{x}(x^{T}A^{T}W A x-2b^{T}W A x+b^{T}W b).
$$ 

From this we see that if $A^{T}W A\,\nsupseteq\,0$ , the quadratic function is unbounded below in $x$ , $g(w)~=~-\infty$ , i.e. , $w\ \notin\ \mathbf{dom}\,g$ . We can give a simple expression for $g$ when A $A^{T}W A\,\succ\,0$ ≻ 0 (which defines a strict linear matrix inequality), by analytically minimizing the quadratic function: 

$$
\begin{array}{r c l}{g(w)}&{=}&{b^{T}W b-b^{T}W A(A^{T}W A)^{-1}A^{T}W b}\\ &{=}&{\displaystyle\sum_{i=1}^{n}w_{i}b_{i}^{2}-\sum_{i=1}^{n}w_{i}^{2}b_{i}^{2}a_{i}^{T}\left(\sum_{j=1}^{n}w_{j}a_{j}a_{j}^{T}\right)^{-1}a_{i}.}\end{array}
$$ 

Concavity of $g$ from this expression is not immediately obvious (but does follow, for example, from convexity of the matrix fractional function; see example 3.4 ). 

Example 3.10 Maximum eigenvalue of a symmetric matrix. The function $f(X)=$ $\lambda_{\mathrm{max}}(X)$ , with $\mathbf{dom}\ f=\mathbf{S}^{m}$ , is convex. To see this, we express $f$ as 

$$
f(X)=\operatorname*{sup}\{y^{T}X y\mid\|y\|_{2}=1\},
$$ 

i.e. , as the pointwise supremum of a family of linear functions of $X$ ( i.e. , $y^{T}X y$ ) indexed by $y\in\mathbf{R}^{m}$ . 

Example 3.11 Norm of a matrix. Consider $f(X)\,=\,\|X\|_{2}$ with $\mathbf{dom}\,f\,=\,\mathbf{R}^{p\times q}$ , where $||\cdot||_{2}$ denotes the spectral norm or maximum singular value. Convexity of $f$ follows from 

$$
\begin{array}{r}{f(\boldsymbol{X})=\operatorname*{sup}\{\boldsymbol{u}^{T}\boldsymbol{X}\boldsymbol{v}\mid\|\boldsymbol{u}\|_{2}=1,~\|\boldsymbol{v}\|_{2}=1\},}\end{array}
$$ 

which shows it is the pointwise supremum of a family of linear functions of $X$ 

As a generalization suppose $||\cdot||_{a}$ and $||\cdot||_{b}$ are norms on $\mathbf{R}^{p}$ and $\mathbf{R}^{q}$ , respectively. The induced norm of a matrix $X\in\mathbf{R}^{p\times q}$ is defined as 

$$
\|X\|_{a,b}=\operatorname*{sup}_{v\neq0}{\frac{\|X v\|_{a}}{\|v\|_{b}}}.
$$ 

(This reduces to the spectral norm when both norms are Euclidean.) The induced norm can be expressed as 

$$
\begin{array}{l l l}{\|{\boldsymbol{X}}\|_{a,b}}&{=}&{\operatorname*{sup}\{\|{\boldsymbol{X}}v\|_{a}\mid\|v\|_{b}=1\}}\\ &{=}&{\operatorname*{sup}\{u^{T}{\boldsymbol{X}}v\mid\|u\|_{a*}=1,\ \|v\|_{b}=1\},}\end{array}
$$ 

where $||\cdot||_{a*}$ is the dual norm of $||\cdot||_{a}$ , and we use the fact that 

$$
\|z\|_{a}=\operatorname*{sup}\{u^{T}z\mid\|u\|_{a*}=1\}.
$$ 

Since we have expressed $\|X\|_{a,b}$ as a supremum of linear functions of $X$ , it is a convex function. 

# Representation as pointwise supremum of affine functions 

The examples above illustrate a good method for establishing convexity of a func- tion: by expressing it as the pointwise supremum of a family of affine functions. Except for a technical condition, a converse holds: almost every convex function can be expressed as the pointwise supremum of a family of affine functions. For example, if $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex, with $\mathbf{dom}\,f=\mathbf{R}^{\,n}$ , then we have 

$$
f(x)=\operatorname*{sup}\{g(x)\mid g{\mathrm{~after~}},\ g(z)\leq f(z){\mathrm{~for~all~}}z\}.
$$ 

In other words, $f$ is the pointwise supremum of the set of all affine global under- estimators of it. We give the proof of this result below, and leave the case where d $\mathbf{\sigma}\mathbf{om}\ f\neq\mathbf{R}^{n}$ as an exercise (exercise 3.28 ). 

Suppose $f$ is convex with $\mathbf{dom}\,f=\mathbf{R}^{\,n}$ . The inequality 

$$
f(x)\geq\operatorname*{sup}\{g(x)\mid g{\mathrm{~afterme,~}}g(z)\leq f(z){\mathrm{~for~all~}}z\}
$$ 

is clear, since if $g$ is any affine underestimator of $f$ , we have $g(x)\,\leq\,f(x)$ . To establish equality, we will show that for each $x\in\mathbf{R}^{n}$ , there is an affine function $g$ , which is a global underestimator of $f$ , and satisfies $g(x)=f(x)$ . 

The epigraph of $f$ is, of course, a convex set. Hence we can find a supporting hyperplane to it at $\left(x,f(x)\right)$ , i.e. , $a\in\mathbf{R}^{n}$ and $b\in\mathbf{R}$ with $(a,b)\neq0$ and 

$$
{\left[\begin{array}{l}{a}\\ {b}\end{array}\right]}^{T}{\left[\begin{array}{l}{\ x-z}\\ {f(x)-t}\end{array}\right]}\leq0
$$ 

for all $(z,t)\in\mathbf{epi}\,f$ . This means that 

$$
\boldsymbol{a}^{T}(\boldsymbol{x}-\boldsymbol{z})+\boldsymbol{b}(\boldsymbol{f}(\boldsymbol{x})-\boldsymbol{f}(\boldsymbol{z})-\boldsymbol{s})\leq0
$$ 

for all $z\in\mathbf{dom}\,f=\mathbf{R}^{n}$ and all $s\geq0$ (since $(z,t)\in\mathbf{epi}\,f$ means $t=f(z)+s$ for so $s\geq0$ ). For the inequality ( 3.8 ) to hold for all $s\geq0$ , we m $b\geq0$ . If b = 0 the inequality ( 3.8 ) reduces to $a^{T}(x-z)\leq0$ f $z\in\mathbf{R}^{n}$ ∈ , which implies a = 0 and contradicts $(a,b)\neq0$ . We conclude that b > 0, i.e. , that the supporting hyperplane is not vertical. 

Using the fact that $b>0$ we rewrite ( 3.8 ) for $s=0$ as 

$$
g(z)=f(x)+(a/b)^{T}(x-z)\leq f(z)
$$ 

for all $z$ . The function $g$ is an affine underestimator of $f$ , and satisfies $g(x)=f(x)$ . 

# 3.2.4 Composition 

In this section we examine conditions on $h\,:\,\mathbf{R}^{k}\,\rightarrow\,\mathbf{R}$ and $g:\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}^{k}$ that guarantee convexity or concavity of their composition $f=h\circ g:\mathbf{R}^{n}\to\mathbf{R}$ , defined by 

$$
f(x)=h(g(x)),\qquad\mathbf{dom}\,f=\{x\in\mathbf{dom}\,g\mid g(x)\in\mathbf{dom}\,h\}.
$$ 

# Scalar composition 

We first consider the $k=1$ , so $h:\mathbf{R}\rightarrow\mathbf{R}$ and $g:\mathbf{R}^{n}\rightarrow\mathbf{R}$ . We can restrict ourselves to the case n = 1 (since convexity is determined by the behavior of a function on arbitrary lines that intersect its domain). 

To discover the composition rules, we start by assuming that $h$ and are twice $g$ diﬀerentiable, with $\mathbf{dom}\,g=\mathbf{dom}\,h=\mathbf{R}$ . In this case, convexity of $f$ reduces to $f^{\prime\prime}\geq0$ (meaning, $f^{\prime\prime}(x)\geq0$ for all $x\in\mathbf{R}$ ). 

The second derivative of the composition function $f=h\circ g$ is given by 

$$
f^{\prime\prime}(x)=h^{\prime\prime}(g(x))g^{\prime}(x)^{2}+h^{\prime}(g(x))g^{\prime\prime}(x).
$$ 

Now suppose, for example, that is convex (so $g^{\prime\prime}\,\geq\,0$ ) and $h$ is convex and $g$ nondecreasing (so $h^{\prime\prime}\geq0$ and $h^{\prime}\geq0$ ). It follows from ( 3.9 ) that $f^{\prime\prime}\geq0$ , i.e. , $f$ is convex. In a similar way, the expression ( 3.9 ) gives the results: 

$f$ is convex if $h$ is convex and nondecreasing, and $g$ is convex, $f$ is convex if $h$ is convex and nonincreasing, and $g$ is concave, $f$ is concave if $h$ is concave and nondecreasing, and $g$ is concave, $f$ is concave if $h$ is concave and nonincreasing, and $g$ is convex. 

These statements are valid when the functions and $h$ are twice diﬀerentiable and $g$ have domains that are all of $\mathbf{R}$ . It turns out that very similar composition rules hold in the general case $n>1$ , without assuming di e rent i ability of $h$ and , or $g$ that $\mathbf{dom}\,g=\mathbf{R}^{n}$ and $\mathbf{dom}\,h=\mathbf{R}$ : 

$f$ is convex if $h$ is convex, $\tilde{h}$ is nondecreasing, and is convex, $g$ $f$ is convex if $h$ is convex, h is nonincreasing, and $g$ is concave, $f$ is concave if $h$ is concave, h is nondecreasing, and is concave, $g$ $f$ is concave if $h$ is concave, h is nonincreasing, and is convex. $g$ 

Here $\tilde{h}$ denotes the extended-value extension of the function $h$ , which assigns the value $(\infty)$ $(-\infty)$ to points not in $\mathbf{dom}\,h$ for $h$ convex (concave). The only diﬀerence between these results, and the results in ( 3.10 ), is that we require that the extended- value extension function $\tilde{h}$ be nonincreasing or nondecreasing, on all of $\mathbf{R}$ . 

To understand what this means, suppose $h$ is convex, so h takes on the value $\infty$ outside $\mathbf{dom}\,h$ . To say that $\tilde{h}$ is nondecreasing means that for any $x,\ y\in\mathbf{R}$ , with $x<y$ , we have $\ddot{h}(x)\leq\ddot{h}(y)$ ≤ ). In partic lar, this means that if $y\in\mathbf{dom}\,h$ , then $x\in$ $\mathbf{dom}\,h$ . In other words, the domain of h extends infinitely in the negative direction; it is either $\mathbf{R}$ , or an interv l of the form $(-\infty,a)$ or $(-\infty,a]$ . In a similar way, to say that $h$ is convex and $\tilde{h}$ is nonincreasing means that $h$ is nonincreasing and $\mathbf{dom}\,h$ extends infinitely in the positive direction. This is illustrated in figure 3.7 . 

![](images/697f71b2f8c7e260fd9ff2eb596e8e075a49cf69c7d209b66f3f2a02436f1ed0.jpg) 
Figure 3.7 Left. The function $x^{2}$ , with domain $\mathbf{R}_{+}$ , is convex and nonde- creasing on its domain, but its extended-value extension is not nondecreas- ing. Right. The function $\operatorname*{max}\{x,0\}^{2}$ , with domain $\mathbf{R}$ , is convex, and its extended-value extension is nondecreasing. 

• The funct on $h(x)\;=\;x^{1/2}$ , with $\mathbf{dom}\,h\;=\;\mathbf{R}_{+}$ , is concave and satisfies the condition $\tilde{h}$ nondecreasing. • The funct n $h(x)=x^{3/2}$ , with $\mathbf{dom}\,h=\mathbf{R}_{+}$ , is convex but does not satisfy the condition $\tilde{h}$ nondecreasing. For example, we have $\tilde{h}(-1)=\infty$ , but $\tilde{h}(1)=1$ − ∞ • The function $h(x)=x^{3/2}$ for $x\geq0$ , and $h(x)=0$ for $x<0$ , with $\mathbf{dom}\,h=\mathbf{R}$ , satisfy the condition $\tilde{h}$ is convex and does nondecreasing. 

The composition results ( 3.11 ) can be proved directly, without assuming dif- ferentiability, or using the formula ( 3.9 ). As an example, we will prove the fol- is convex, and $\ddot{h}$ lowing composition theorem: if is convex, $h$ is nondecreasing, $g$ then $f\,=\,h\circ g$ is convex. Assume that $x$ , $y\,\in\,\mathbf{dom}\,f$ , and $0\,\leq\,\theta\,\leq\,1$ . Since $x,\ y\in\mathbf{dom}\ f$ , we have that $x,\ y\in\mathbf{dom}\,g$ and $g(x)$ , $g(y)\in\mathbf{dom}\,h$ . Since $\mathbf{dom}\,g$ is convex, we conclude that $\theta x+(1-\theta)y\in\mathbf{dom}\,g$ , and from convexity of $g$ , we have 

$$
g(\theta x+(1-\theta)y)\leq\theta g(x)+(1-\theta)g(y).
$$ 

Since $g(x),\ g(y)\in\,\mathbf{dom}\,h$ , we con hat $\theta g(x)+(1-\theta)g(y)\,\in\,\mathbf{dom}\,h$ , i.e the righthand side of ( 3.12 ) is in dom . Now we use the assumption that $\ddot{h}$ is nondecreasing, which means that its domain extends infinitely in the negative direction. Since the righthand side of ( 3.12 ) is in $\mathbf{dom}\,h$ , we conclude that the lefthand side, i.e. , $g(\theta x+(1\!-\!\theta)y)\in\mathbf{dom}\,h$ . This means that $\theta x+(1\!-\!\theta)y\in\mathbf{dom}\,f$ . At this point, we have shown that $\mathbf{dom}\ f$ is convex. 

Now using the fact that $\tilde{h}$ is nondecreasing and the inequality ( 3.12 ), we get 

$$
h(g(\theta x+(1-\theta)y))\leq h(\theta g(x)+(1-\theta)g(y)).
$$ 

From convexity of $h$ , we have 

$$
h(\theta g(x)+(1-\theta)g(y))\leq\theta h(g(x))+(1-\theta)h(g(y)).
$$ 

Putting ( 3.13 ) and ( 3.14 ) together, we have 

$$
h(g(\theta x+(1-\theta)y))\leq\theta h(g(x))+(1-\theta)h(g(y)).
$$ 

which proves the composition theorem. 

Example 3.13 Simple composition results. 

• If $g$ is convex then $\exp g(x)$ is convex. • If $g$ is concave and positive, then $\log g(x)$ is concave. • If $g$ is concave and positive, then $1/g(x)$ is convex. • If $g$ is convex and nonnegative and $p\geq1$ , then $g(x)^{p}$ is convex. $\bullet$ If $g$ is convex then $-\log(-g(x))$ is convex on $\{x\mid g(x)<0\}$ . 

Remark 3.3 The requirement that monotonicity hold for the extended-value extension h , and not just the function $h$ , cannot be removed. For example, consider the function $g(x)=x^{2}$ , with $\mathbf{dom}\,g=\mathbf{R}$ , and $h(x)=0$ , with $\mathbf{dom}\,h=[1,2]$ ]. Here $g$ is convex, and $h$ is convex and nondecreasing. But the function $f=h\circ g$ , given by 

$$
f(x)=0,\qquad\mathbf{dom}\,f=[-\sqrt{2},-1]\,\cup\,[1,\sqrt{2}],
$$ 

is not convex, since its domain is not convex. Here, of course, the function $\tilde{h}$ is not nondecreasing. 

# Vector composition 

We now turn to the more complicated case when $k\geq1$ . Suppose 

$$
f(x)=h(g(x))=h(g_{1}(x),.\,.\,.\,,g_{k}(x)),
$$ 

with $h:\mathbf{R}^{k}\rightarrow\mathbf{R}$ $g_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ . Again without loss of generality we can assume $n=$ 1. As in the case k = 1, we start by assuming the functions are twice diﬀerentiable, with $\mathbf{dom}\,g=\mathbf{R}$ and $\mathbf{dom}\,h=\mathbf{R}^{k}$ , in order to discover the composition rules. We have 

$$
f^{\prime\prime}(x)=g^{\prime}(x)^{T}\nabla^{2}h(g(x))g^{\prime}(x)+\nabla h(g(x))^{T}g^{\prime\prime}(x),
$$ 

which is the vector analog of ( 3.9 ). Again the issue is to determine conditions under which $f^{\prime\prime}(x)\,\geq\,0$ for all $x$ (or $f^{\prime\prime}(x)\,\leq\,0$ for all $x$ for concavity). From ( 3.15 ) we can derive many rules, for example: 

$f$ is convex if $h$ is convex, $h$ is nondecreasing in each argument, and $g_{i}$ are convex, $f$ is convex if $h$ is convex, $h$ is nonincreasing in each argument, and $g_{i}$ are concave, $f$ is concave if $h$ is concave, $h$ is nondecreasing in each argument, and $g_{i}$ are concave. 

As in the scalar case, similar composition results hold in general, with $n>1$ , no as- sumption of di e rent i ability of $h$ or , and general domains. For the general results, $g$ the monotonicity condition on $h$ must hold for the extended-value extension h . 

To understand the meaning of the condition that the extended-value exten- sion $h:\mathbf{R}^{k}\rightarrow\mathbf{R}$ is convex, and $\ddot{h}$ h be monotonic, we consider the case where ng, i.e. , whenever $u\,\preceq\,v$ , we h ve $\tilde{h}(u)\,\leq\,\tilde{h}(v)$ ≤ ). This implies that if $v\,\in\,\mathbf{dom}\,h$ ∈ , then so is $u$ : the domain of h must extend infinitely in the $-\mathbf{R}_{+}^{k}$ directions. We can express this compactly as $\mathbf{dom}\,h-\mathbf{R}_{+}^{k}=\mathbf{dom}\,h$ . 

Example 3.14 Vector composition examples. 

• Let $h(z)=z_{[1]}+\cdot\cdot\cdot+z_{[r]}$ , the sum of the $r$ largest components of $z\in\mathbf{R}^{k}$ . Then $h$ is convex and nondecreasing in each argument. Suppose are convex $g_{1},\ldots,g_{k}$ functions on $\mathbf{R}^{n}$ . Then the composition function $f=h\circ g$ , i.e. , the pointwise sum of the $r$ largest $g_{i}$ ’s, is convex. • The func $\begin{array}{r}{h(z)=\log(\sum_{i=1}^{k}e^{z_{i}})}\end{array}$ ) is convex and nondecreasing in each argu- men $\textstyle\log\bigl(\sum_{i=1}^{k}e^{g_{i}}\bigr)$ ) is convex whenever $g_{i}$ are. • For 0 $0~<~p~\leq~1$ ≤ 1, t e function $\begin{array}{l}{h(z)\ =\ (\sum_{i=1}^{k}z_{i}^{p})^{1/p}}\end{array}$ on $\mathbf{R}_{+}^{k}$ is concave, and its extension (which has the value −∞ for $z\ \succeq\ 0$ ) is nondecreasing in each component. So if g are concave and nonnegative, we conclude that $f(x)\,=$ $\textstyle\bigl(\sum_{i=1}^{k}g_{i}(x)^{p}\bigr)^{1/p}$ is concave. • Suppose $p\geq1$ , and $g_{1},\ldots,g_{k}$ are convex and nonnegative. Then the function $\textstyle\bigl(\sum_{i=1}^{k}g_{i}(x)^{p}\bigr)^{1/p}$ is convex. To show this, we consider the function $h:\mathbf{R}^{k}\rightarrow\mathbf{R}$ defined as 

$$
h(z)=\left(\sum_{i=1}^{k}\operatorname*{max}\{z_{i},0\}^{p}\right)^{1/p},
$$ 

with $\mathbf{dom}\,h\,=\,\mathbf{R}^{k}$ , so $h={\ddot{h}}$ . This function is convex, and nondecreasing, so we conclude $h(g(x))$ is a convex function of $x$ . For $z\succeq0$ , we have $h(z)\,=$ $(\sum_{i=1}^{k}z_{i}^{p})^{1/p}$ , so our $\textstyle\bigl(\sum_{i=1}^{k}g_{i}(x)^{p}\bigr)^{1/p}$ is convex. 

• The geometric mean $\begin{array}{r}{h(z)=(\prod_{i=1}^{k}z_{i})^{1/k}}\end{array}$ Q on $\mathbf{R}_{+}^{k}$ is concave and its extension is nondecreasing in each argument. It follows that if $g_{1},\ldots,g_{k}$ are nonnegative concave functions, then so is their geometric mean, $\scriptstyle(\prod_{i=1}^{\kappa}g_{i})^{1/k}$ . 

# 3.2.5 Minimization 

We have seen that the maximum or supremum of an arbitrary family of convex functions is convex. It turns out that some special forms of minimization also yield convex functions. If $f$ is convex in $(x,y)$ , and $C$ is a convex nonempty set, then the function 

$$
g(x)=\operatorname*{inf}_{y\in C}f(x,y)
$$ 

is convex in $x$ , provided $g(x)>-\infty$ for all $x$ . The domain of $g$ is the projection of

 $\mathbf{dom}\ f$ on its $x$ -coordinates, i.e. , 

$$
\mathbf{lom}\,g=\{x\mid(x,y)\in\mathbf{dom}\,f{\mathrm{~for~some~}}y\in C\}.
$$ 

We prove th ing Jensen’ $x_{1},\ x_{2}\in\mathbf{dom}\,g$ . Let $\epsilon>0$ . ere are y $y_{1},\ y_{2}\in C$ ∈ such that f $f(x_{i},y_{i})\leq g(x_{i})+\epsilon$ ≤ for i = 1 , 2. Now let

 $\theta\in[0,1]$ ∈ 1]. We have 

$$
\begin{array}{r c l}{g(\theta x_{1}+(1-\theta)x_{2})}&{=}&{\displaystyle\operatorname*{inf}_{y\in C}f(\theta x_{1}+(1-\theta)x_{2},y)}\\ &{\leq}&{f(\theta x_{1}+(1-\theta)x_{2},\theta y_{1}+(1-\theta)y_{2})}\\ &{\leq}&{\theta f(x_{1},y_{1})+(1-\theta)f(x_{2},y_{2})}\\ &{\leq}&{\theta g(x_{1})+(1-\theta)g(x_{2})+\epsilon.}\end{array}
$$ 

Since this holds for any $\epsilon>0$ , we have 

$$
g(\theta x_{1}+(1-\theta)x_{2})\leq\theta g(x_{1})+(1-\theta)g(x_{2}).
$$ 

The result can also be seen in terms of epigraphs. With $f$ , $g$ , and $C$ defined as in ( 3.16 ), and assuming the infimum over $y\in C$ is attained for each $x$ , we have 

$$
\mathbf{epi}\,g=\{(x,t)\mid(x,y,t)\in\mathbf{epi}\,f{\mathrm{~for~some~}}y\in C\}
$$ 

Thus $\mathbf{epi}\,g$ is convex, since it is the projection of a convex set on some of its components. 

Example 3.15 Schur complement. Suppose the quadratic function 

$$
\boldsymbol{f}(\boldsymbol{x},\boldsymbol{y})=\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x}+2\boldsymbol{x}^{T}\boldsymbol{B}\boldsymbol{y}+\boldsymbol{y}^{T}\boldsymbol{C}\boldsymbol{y},
$$ 

(where $A$ and $C$ are symmetric) is convex in $(x,y)$ , which means 

$$
\left[\begin{array}{c c}{{\boldsymbol{A}}}&{{\boldsymbol{B}}}\\ {{\boldsymbol{B}}^{T}}&{{\boldsymbol{C}}}\end{array}\right]\succeq0.
$$ 

We can express $g(x)=\textstyle\operatorname*{inf}_{y}f(x,y)$ as 

$$
g(x)=x^{T}(A-B C^{\dagger}B^{T})x,
$$ 

where $C^{\dagger}$ is the pseudo-inverse of $C$ (see $\S$ A.5.4 ). By the minimization rule, $g$ is convex, so we conclude that $A-B C^{\dagger}B^{T}\succeq0$ . 

If $C$ is invertible, i.e. , $C\ \succ\ 0$ , then the matrix $A\,-\,B C^{-1}B^{T}$ is called the Schur complement of $C$ in the matrix 

$$
\left[\begin{array}{l l}{\boldsymbol{A}}&{\boldsymbol{B}}\\ {\boldsymbol{B}^{T}}&{\boldsymbol{C}}\end{array}\right]
$$ 

(see A.5.5 ). 

Example 3.16 Distance to a set. The distance of a point $x$ to a set $S\subseteq\mathbf{R}^{n}$ , in the norm $||\cdot||$ , is defined as 

$$
\mathbf{dist}(x,S)=\operatorname*{inf}_{y\in S}||x-y||.
$$ 

The function $\|x\!-\!y\|$ is convex in $(x,y)$ , so if the set $S$ is convex, the distance function ${\bf d i s t}(x,S)$ is a convex function of $x$ . 

Example 3.17 Suppose $h$ is convex. Then the function $g$ defined as 

$$
g(x)=\operatorname*{inf}\left\{h(y)\mid A y=x\right\}
$$ 

is convex. To see this, we define $f$ by 

$$
f(x,y)={\left\{\begin{array}{l l}{h(y)}&{{\mathrm{if~}}A y=x}\\ {\infty}&{{\mathrm{otherwise}},}\end{array}\right.}
$$ 

which is convex in $(x,y)$ . Then $g$ is the minimum of $f$ over $y$ , and hence is convex. (It is not hard to show directly that $g$ is convex.) 

# 3.2.6 Perspective of a function 

If $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , then the perspective of $f$ is the function $g:\mathbf{R}^{n+1}\rightarrow\mathbf{R}$ defined by 

$$
g(x,t)=t f(x/t),
$$ 

with domain 

$$
\mathbf{dom}\,g=\{(x,t)\mid x/t\in\mathbf{dom}\,f,\ t>0\}.
$$ 

The perspective operation preserves convexity: If $f$ is a convex function, then so is its perspective function $g$ . Similarly, if $f$ is concave, then so is $g$ . 

This can be proved several ways, for example, direct verification of the defining inequality (see exercise 3.33 ). We give a short proof here using epigraphs and the perspective mapping on $\mathbf{R}^{n+1}$ described in § 2.3.3 (which will also explain the name ‘perspective’). For $t>0$ we have 

$$
\begin{array}{r l}{(x,t,s)\in\mathbf{epi}\,g}&{\Longleftrightarrow\quad t f(x/t)\leq s}\\ &{\Longleftrightarrow\quad f(x/t)\leq s/t}\\ &{\Longleftrightarrow\quad(x/t,s/t)\in\mathbf{epi}\,f.}\end{array}
$$ 

Therefore $\mathbf{epi}\,g$ is the inverse image of $\mathbf{\epsilon}_{f}$ under the perspective mapping that takes $(u,v,w)$ to $(u,w)/v$ . It follows (see $\S2.3.3)$ that $\mathbf{epi}\,g$ is convex, so the function $g$ is convex. 

Example 3.18 Euclidean norm squared. The perspective of the convex function $f(x)=x^{T}x$ on $\mathbf{R}^{n}$ is 

$$
g(x,t)=t(x/t)^{T}(x/t)=\frac{x^{T}x}{t},
$$ 

which is convex in $(x,t)$ for $t>0$ . 

We can deduce convexity of $g$ using several other methods. First, we can express $g$ as the sum of the quadratic-over-linear functions $x_{i}^{2}/t$ , which were shown to be convex in § 3.1.5 . We can also express $g$ as a special case of the matrix fractional function $x^{T}(t I)^{-1}x$ (see example 3.4 ). 

Example 3.19 Negative logarithm. Consider the convex function $f(x)=-\log x$ on $\mathbf{R}_{++}$ . Its perspective is 

$$
g(x,t)=-t\log(x/t)=t\log(t/x)=t\log t-t\log x,
$$ 

and is convex on $\mathbf{R}_{++}^{2}$ . The function $g$ is called the relative entropy of $t$ and $x$ . For $x=1$ , $g$ reduces to the negative entropy function. 

From convexity of $g$ we can establish convexity or concavity of several interesting related functions. First, the relative entropy of two vectors $u,\ v\in\mathbf{R}_{++}^{n}$ , defined as 

$$
\sum_{i=1}^{n}u_{i}\log\!\left(u_{i}/v_{i}\right)\!,
$$ 

is convex in $(u,v)$ , since it is a sum of relative entropies of $u_{i},\ v_{i}$ . 

A closely related function is the Kullback-Leibler divergence between $u,\ v\in\mathbf{R}_{++}^{n}$ , given by 

$$
D_{\mathrm{k{l}}}(u,v)=\sum_{i=1}^{n}\left(u_{i}\log(u_{i}/v_{i})-u_{i}+v_{i}\right),
$$ 

which is convex, since it is the relative entropy plus a linear function of $(u,v)$ . The Kullback-Leibler divergence satisfies $D_{\mathrm{{kl}}}(u,v)\,\ge\,0$ , and $D_{\mathrm{{kl}}}(u,v)=0$ if and only if $u=v$ , and so can be used as a measure of deviation between two positive vectors; see exercise 3.13 . (Note that the relative entropy and the Kullback-Leibler divergence are the same when $u$ and $v$ are probability vectors, i.e. , satisfy $\mathbf{1}^{T}u=\mathbf{1}^{T}v=1$ .) 

If we take $\boldsymbol{v}_{i}\,=\,\mathbf{1}^{T}\boldsymbol{u}$ in the relative entropy function, we obtain the concave (and homogeneous) function of $u\in\mathbf{R}_{++}^{n}$ given by 

$$
\sum_{i=1}^{n}u_{i}\log(\mathbf{1}^{T}u/u_{i})=(\mathbf{1}^{T}u)\sum_{i=1}^{n}z_{i}\log(1/z_{i}),
$$ 

where $z\,=\,u/(\mathbf{1}^{T}u)$ , which is called the normalized entropy function. The vector $z\,=\,u/\mathbf{1}^{T}u$ is a normalized vector or probability distribution, since its components sum to one; the normalized entropy of $u$ is ${\mathbf{1}}^{T}u$ times the entropy of this normalized distribution. 

Example 3.20 Suppose $f:\mathbf{R}^{m}\rightarrow\mathbf{R}$ is convex, and $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , $c\in\mathbf{R}^{n}$ , and $d\in\mathbf{R}$ . We define 

$$
g(x)=(c^{T}x+d)f\left((A x+b)/(c^{T}x+d)\right),
$$ 

with 

$$
\begin{array}{r}{\mathbf{dom}\,g=\{x\ |\ c^{T}x+d>0,\ (A x+b)/(c^{T}x+d)\in\mathbf{dom}\,f\}.}\end{array}
$$ 

Then $g$ is convex. 

# 3.3 The conjugate function 

In this section we introduce an operation that will play an important role in later chapters. 

![](images/81fa558442899d5a6c3a3cef4125910cc0081044a0130ad96eb07badf6c4590e.jpg) 
Figure 3 function $f\,:\,\mathbf{R}\,\rightarrow\,\mathbf{R}$ , and a value $y\ \in\ \mathbf{R}$ . The conjugate function f $f^{*}(y)$ ) is the maximum gap between the linear function $y x$ and $f(x)$ , as shown by the dashed line in the figure. If $f$ is diﬀerentiable, this occurs at a point $x$ where $f^{\prime}(x)=y$ . 

# 3.3.1 Definition and examples 

Let $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ . The function $f^{*}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , defined as 

$$
f^{*}(y)=\operatorname*{sup}_{x\in\mathbf{dom}_{f}}\left(y^{T}x-f(x)\right),
$$ 

is called the conjugate of the function $f$ . The domain of the conjugate function consists of $y\in\mathbf{R}^{n}$ for which the supremum is finite, i.e. , for which the diﬀerence $y^{T}x-f(x)$ is bounded above on $\mathbf{dom}\ f$ . This definition is illustrated in figure 3.8 . 

We see immediately that $f^{*}$ is a convex function, since it is the pointwise supremum of a family of convex (indeed, affine) functions of $y$ . This is true whether or not $f$ is convex. (Note that when $f$ is convex, the subscript $x\in\mathbf{dom}\,f$ is not necessary since, by convention, $y^{T}x-f(x)=-\infty$ for $x\not\in\mathbf{dom}\,f$ .) 

We start with some simple examples, and then describe some rules for conjugat- ing functions. This allows us to derive an analytical expression for the conjugate of many common convex functions. 

• Affine function. $f(x)=a x+b$ . As a function of $x$ , $y x-a x-b$ is bounded if and only if $y\,=\,a$ , in which case it is constant. Therefore the domain of the conjugate function $f^{*}$ is the singleton $\{a\}$ , and $f^{*}(a)=-b$ . • Negative logarithm. $f(x)=-\log x$ , with dom $f=\mathbf{R}_{++}$ . The function $_{x y+\log x}$ is unbounded above if $y\geq0$ and reaches its maximum at $x=-1/y$ otherwise. Therefore, $\mathbf{dom}\ f^{*}=\left\{y\ |\ y<0\right\}=-\mathbf{R}_{++}$ and $f^{*}(y)=-\log(-y)\!-\!1$ for $y<0$ . • Exponential. $f(x)\,=\,e^{x}$ . $x y-e^{x}$ is unbounded if $y\,<\,0$ . For $y\,>\,0$ , $x y-e^{x}$ reaches its maximum at $x=\log y$ , so we have $f^{*}(y)=y\log y-y$ . For $y=0$ , 

$\begin{array}{r}{f^{*}(y)\,=\,\operatorname*{sup}_{x}-e^{x}\,=\,0}\end{array}$ . ary, $\mathbf{dom}\,f^{*}\,=\,\mathbf{R}_{+}$ and $f^{*}(y)\,=\,y\log y\,-\,y$ (with the interpretation 0 log 0 = 0). 

• Negative entropy. $f(x)\;=\;x\log x$ , with $\mathbf{dom}\,f\,=\,\mathbf{R}_{+}$ (and $f(0)\,=\,0$ ). The function $x y-x\log x$ is above on $\mathbf{R}_{+}$ for all $y$ , hence $\mathbf{dom}\ f^{*}=\mathbf{R}$ . It attains its maximum at x $x=e^{y-1}$ , and substituting we find $f^{*}(y)=e^{y-1}$ . • Inverse. $f(x)=1/x$ on $\mathbf{R}_{++}$ . For $y\,>\,0$ , $y x-1/x$ is unbounded above. For $y\,=\,0$ this function has supremum 0; for $y\,<\,0$ the supremum is attained at $x=(-y)^{-1/2}$ . Therefore we have $f^{*}(y)=-2(-y)^{1/2}$ , with $\mathbf{dom}\,f^{*}=-\mathbf{R}_{+}$ . 

Example 3.22 Strictly convex quadratic function. Consider $f(x)\,=\,{\textstyle\frac{1}{2}}x^{T}Q x$ , with $\mathrm{~\it~Q~}\in{\bf S}_{++}^{n}$ . The function $y^{T}x-\textstyle{\frac{1}{2}}x^{T}Q x$ is bounded above as a function of $x$ for all $y$ . It attains its maximum at $x=Q^{-1}y$ , so 

$$
f^{*}(y)=\frac{1}{2}y^{T}Q^{-1}y.
$$ 

Example 3.23Log-determinant.We consider $f(X)\,=\,\log\operatorname*{det}X^{-1}$ on $\mathbf{S}_{++}^{n}$.Theconjugate function is defined as 

$$
f^{*}(Y)=\operatorname*{sup}_{X\succ0}\left(\mathbf{tr}(Y X)+\log\operatorname*{det}X\right),
$$ 

since $\mathbf{tr}(Y X)$ is the standard inner product on $\mathbf{S}^{\ast}$ . We first show that $\mathbf{tr}(Y X)+$ $\log\operatorname*{det}X$ is unbounded above unless $Y\prec0$ . If $Y\neq0$ , then $Y$ has an eigenvector $v$ , with $\|v\|_{2}=1$ , and eigenvalue $\lambda\geq0$ . Taking $X=I+t v v^{T}$ we find that 

$$
\mathbf{tr}(Y X)+\log\operatorname*{det}X=\mathbf{tr}\,Y+t\lambda+\log\operatorname*{det}(I+t v v^{T})=\mathbf{tr}\,Y+t\lambda+\log(1+t),
$$ 

which is unbounded above as $t\to\infty$ . 

Now consider the case $Y\prec0$ . We can find the maximizing $X$ by setting the gradient with respect to $X$ equal to zero: 

$$
\nabla_{\boldsymbol{X}}\left(\mathbf{tr}(\boldsymbol{Y}\boldsymbol{X})+\log\operatorname*{det}\boldsymbol{X}\right)=\boldsymbol{Y}+\boldsymbol{X}^{-1}=0
$$ 

(see § A.4.1 ), which yields $X=-Y^{-1}$ (which is, indeed, positive definite). Therefore we have 

$$
f^{*}(Y)=\log\operatorname*{det}(-Y)^{-1}-n,
$$ 

with $\mathbf{dom}\,f^{*}=-\mathbf{S}_{++}^{n}$ . 

Example 3.24 Indicator function. Let $I_{S}$ be the indicator function of a (not neces- sarily convex) set $S\subseteq\mathbf{R}^{n}$ , i.e. , $I_{S}(x)=0$ on $\mathbf{dom}\,I_{S}=S$ . Its conjugate is 

$$
I_{S}^{*}(y)=\operatorname*{sup}_{x\in S}y^{T}x,
$$ 

which is the support function of the set $S$ . 

Example 3.25 Log-sum-exp function. To derive the conjugate of the log-sum-exp unction $f(x)\;=\;\log(\sum_{i=1}^{n}e^{x_{i}})$ ), we first determine the values of $y$ for which the maximum over x of $y^{T}x-f(x)$ − ) is attained. By setting the gradient with respect to x equal to zero, we obtain the condition 

$$
y_{i}={\frac{e^{x_{i}}}{\sum_{j=1}^{n}e^{x_{j}}}},\quad i=1,\ldots,n.
$$ 

These equations are solvable for $x$ if and only if $y\succ0$ and $\mathbf{1}^{T}y=1$ . By substituting the pression for $y_{i}$ o $y^{T}x{-}f(x)$ we obtai $\begin{array}{r}{f^{*}(y)=\sum_{i=1}^{n}y_{i}\log y_{i}}\end{array}$ . This expression for $f^{*}$ ∗ is still correct if some components of y are zero, as long as $y\succeq0$ and $\mathbf{1}^{T}y=1$ , and we interpret 0 log 0 as 0. 

In fact the domain of $f^{*}$ is exactly given by $\mathbf{1}^{T}y=1$ , $y\succeq0$ . To show this, suppose that a component of $y$ is negative, say, $y_{k}<0$ . Then we can show that $y^{T}x-f(x)$ is unbounded above by choosing $x_{k}=-t$ , and $x_{i}=0$ , $i\neq k$ , and letting $t$ go to infinity. If $y\succeq0$ but $\mathbf{1}^{T}y\neq1$ , we choose $x=t{\bf1}$ , so that 

$$
y^{T}x-f(x)=t\mathbf{1}^{T}y-t-\log n.
$$ 

If $\mathbf{1}^{T}y>1$ , this grows unboundedly as $t\to\infty$ ; if $\mathbf{1}^{T}y<1$ , it grows unboundedly as $t\to-\infty$ . 

In summary, 

$$
f^{*}(y)={\left\{\begin{array}{l l}{\sum_{i=1}^{n}y_{i}\log y_{i}}&{{\mathrm{if~}}y\succeq0{\mathrm{~and~}}\mathbf{1}^{T}y=1}\\ {\infty}&{{\mathrm{otherwise}}.}\end{array}\right.}
$$ 

In other words, the conjugate of the log-sum-exp function is the negative entropy function, restricted to the probability simplex. 

Example 3.26 Norm. Let $||\cdot||$ be a norm on $\mathbf{R}^{n}$ , with dual norm $||\cdot||*$ . We will show that the conjugate of $f(x)=\left\|x\right\|$ is 

$$
f^{*}(y)={\left\{\begin{array}{l l}{0}&{\|y\|_{*}\leq1}\\ {\infty}&{{\mathrm{otherwise}},}\end{array}\right.}
$$ 

i.e. , the conjugate of a norm is the indicator function of the dual norm unit ball. 

If $\|y\|_{*}>1$ , then by definition of the dual norm, there is a $z\in\mathbf{R}^{n}$ with $\|z\|\leq1$ and $y^{T}z>1$ . Taking $x=t z$ and letting $t\to\infty$ , we have 

$$
\boldsymbol{y}^{T}\boldsymbol{x}-\|\boldsymbol{x}\|=t(\boldsymbol{y}^{T}\boldsymbol{z}-\|\boldsymbol{z}\|)\to\infty,
$$ 

which shows that $f^{*}(y)=\infty$ . Conversely, if $\|y\|_{*}\leq1$ , then ve $y^{T}x\leq\|x\|\|y\|_{*}$ for all $x$ , which implies for all $x$ , $y^{T}x-\|x\|\leq0$ . Therefore x = 0 is the value that maximizes $y^{T}x-\|x\|$ , with maximum value $0$ . 

Example 3.27 Norm squared. Now consider the function $f(x)=(1/2)\|x\|^{2}$ , where $\left\|\cdot\right\|$ is a norm, with dual norm $\big|\big|\cdot\big|\big|_{*}$ . We will show that its conjugate is $f^{*}(y)=(1/2)\lvert|y\rvert|_{*}^{2}$ . From $y^{T}x\leq\|y\|_{*}\|x\|$ , we conclude 

$$
\boldsymbol{y}^{T}\boldsymbol{x}-(1/2)\|\boldsymbol{x}\|^{2}\leq\|\boldsymbol{y}\|_{*}\|\boldsymbol{x}\|-(1/2)\|\boldsymbol{x}\|^{2}
$$ 

for all $x$ . The righthand side is a quadratic function of $\left\|x\right\|$ , which has maximum value $(1/2)\|y\|_{*}^{2}$ . Therefore for all $x$ , we have 

$$
y^{T}x-(1/2)\|x\|^{2}\leq(1/2)\|y\|_{*}^{2},
$$ 

which shows that $f^{*}(y)\leq(1/2)\|y\|_{*}^{2}$ . 

To show the other inequality, let $x$ be any vector with $y^{T}x=\|y\|_{*}\|x\|$ , scaled so that $\|x\|=\|y\|_{*}$ . Then we have, for this $x$ , 

$$
\boldsymbol{y}^{T}\boldsymbol{x}-(1/2)\|\boldsymbol{x}\|^{2}=(1/2)\|\boldsymbol{y}\|_{*}^{2},
$$ 

which shows that $f^{\ast}(y)\geq(1/2)\Vert y\Vert_{\ast}^{2}$ . 

Example 3.28 Revenue and profit functions. We consider a business or enterprise that consumes $n$ resources and produces a product that can be sold. We let $r=\left(r_{1},.\,.\,.\,,r_{n}\right)$ denote the vector of resource quantities consumed, and $S(r)$ denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let denote the price (per unit) of resource $i$ , so the total amount paid for resources $p_{i}$ by the enterprise is $p^{T}r$ . The profit derived by the firm is then $\boldsymbol{S}(\boldsymbol{r})-\boldsymbol{p}^{T}\boldsymbol{r}$ . Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by 

$$
M(p)=\operatorname*{sup}_{r}\left(S(r)-p^{T}r\right).
$$ 

The function $M(p)$ gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express $M$ as 

$$
M(p)=(-S)^{*}(-p).
$$ 

Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed). 

# 3.3.2 Basic properties 

# Fenchel’s inequality 

From the definition of conjugate function, we immediately obtain the inequality 

$$
f(x)+f^{*}(y)\geq x^{T}y
$$ 

for all $x$ , $y$ . This is called Fenchel’s inequality (or Young’s inequality when $f$ is diﬀerentiable). 

For example with $f(x)=(1/2)x^{T}Q x$ , where $Q\in\mathbf{S}_{++}^{n}$ , we obtain the inequality 

$$
x^{T}y\leq(1/2)x^{T}Q x+(1/2)y^{T}Q^{-1}y.
$$ 

# Conjugate of the conjugate 

The examples above, and the name ‘conjugate’, suggest that the conjugate of the conjugate of a convex function is the original function. This is the case provided a technical condition holds: if $f$ is convex, and $f$ is closed ( i.e. , $\mathbf{\epsilon}_{f}$ is a closed set; see § A.3.3 ), then $f^{**}=f$ . For example, if $\mathbf{dom}\,f=\mathbf{R}^{n}$ , then we have $f^{**}=f$ , i.e. , the conjugate of the conjugate of $f$ is $f$ again (see exercise 3.39 ). 

# Diﬀerentiable functions 

The conjugate of a diﬀerentiable function $f$ is also called the Legendre transform of $f$ . (To distinguish the general definition from the diﬀerentiable case, the term Fenchel conjugate is sometimes used instead of conjugate.) 

Suppose $f$ is convex and diﬀerentiable, with $\mathbf{dom}\,f=\mathbf{R}^{n}$ . Any maximizer $x^{*}$ $y^{T}x-f(x)$ satisfies $y=\nabla f(x^{*})$ , and conversely, if $x^{*}$ satisfies $y=\nabla f(x^{*})$ , then $x^{*}$ ∗ maximizes $y^{T}x-f(x)$ . Therefore, if $y=\nabla f(x^{*})$ , we have 

$$
f^{*}(y)=x^{*T}\nabla f(x^{*})-f(x^{*}).
$$ 

This allows us to determine $f^{*}(y)$ for any $y$ for which we can solve the gradient equation $y=\nabla f(z)$ for $\mathcal{Z}$ . 

We can express this another way. Let $z\in\mathbf{R}^{n}$ be arbitrary and define $y=\nabla f(z)$ . Then we have 

$$
f^{*}(y)=z^{T}\nabla f(z)-f(z).
$$ 

# Scaling and composition with affine transformation 

For $a>0$ an $b\in\mathbf{R}$ he conjugate of $g(x)=a f(x)+b$ is $g^{*}(y)=a f^{*}(y/a)-b$ . Suppose A $A\in\mathbf{R}^{n\times n}$ ∈ is nonsingular and b $b\in\mathbf{R}^{n}$ ∈ . Then the conjugate of $g(x)=$ $f(A x+b)$ is 

$$
g^{*}(y)=f^{*}(A^{-T}y)-b^{T}A^{-T}y,
$$ 

with $\mathbf{dom}\,g^{*}=A^{T}\,\mathbf{dom}\,f^{*}$ . 

# Sums of independent functions 

If $f(u,v)=f_{1}(u)+f_{2}(v)$ , where $f_{1}$ and $f_{2}$ are convex functions with conjugates $f_{1}^{*}$ and $f_{2}^{*}$ , respectively, then 

$$
f^{*}(w,z)=f_{1}^{*}(w)+f_{2}^{*}(z).
$$ 

In other words, the conjugate of the sum of independent convex functions is the sum of the conjugates. (‘Independent’ means they are functions of diﬀerent variables.) 

# 3.4 Quasiconvex functions 

# 3.4.1 Definition and examples 

A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is called quasiconvex (or unimodal ) if its domain and all its sublevel sets 

$$
S_{\alpha}=\{x\in\mathbf{dom}\ f\ |\ f(x)\leq\alpha\},
$$ 

for $\alpha\in\mathbf{R}$ , are convex. A function is quasiconcave if $-f$ is quasiconvex, i.e. , every superlevel set $\{x\mid f(x)\geq\alpha\}$ is convex. A function that is both quasiconvex and quasiconcave is called quasilinear . If a function $f$ is quasilinear, then its domain, and every level set $\{x\mid f(x)=\alpha\}$ is convex. 

![](images/0ff9da10e977f33aca55128a69d956d793651fa8d57737da2498e94f3c045ce1.jpg) 
Figure 3.9 A quasiconvex function on $\mathbf{R}$ . For each $\alpha$ , the $\alpha$ -sublevel set $S_{\alpha}$ is convex, i.e. , an interval. The sublevel set $S_{\alpha}$ is the interval $[a,b]$ . The sublevel set $S_{\beta}$ is the interval $(-\infty,c]$ . 

For a function on $\mathbf{R}$ , quasiconvexity requires that each sublevel set be an interval (including, possibly, an infinite interval). An example of a quasiconvex function on $\mathbf{R}$ is shown in figure 3.9 . 

Convex functions have convex sublevel sets, and so are quasiconvex. But simple examples, such as the one shown in figure 3.9 , show that the converse is not true. 

# Example 3.29 Some examples on $\mathbf{R}$ : 

• Logarithm. $\log x$ on $\mathbf{R}_{++}$ is quasiconvex (and quasiconcave, hence quasilinear). • Ceiling function. $\operatorname{ceil}(x)=\operatorname*{inf}\{z\in\mathbf{Z}\mid z\geq x\}$ is quasiconvex (and quasicon- cave). 

These examples show that quasiconvex functions can be concave, or discontinuous. We now give some examples on $\mathbf{R}^{n}$ . 

Example 3.30 Length of a vector. We define the length of $x\,\in\,\mathbf{R}^{\,n}$ as the largest index of a nonzero component, i.e. , 

$$
f(x)=\operatorname*{max}\{i\mid x_{i}\neq0\}.
$$ 

(We define the length of the zero vector to be zero.) This function is quasiconvex on $\mathbf{R}^{n}$ , since its sublevel sets are subspaces: 

$$
f(x)\leq\alpha\iff x_{i}=0{\mathrm{~for~}}i=\lfloor\alpha\rfloor+1,.\,.\,.\,,n.
$$ 

Example 3.31 Consider $f:\mathbf{R}^{2}\rightarrow\mathbf{R}$ , with $\mathbf{dom}\ f=\mathbf{R}_{+}^{2}$ and $f(x_{1},x_{2})=x_{1}x_{2}$ . This function is neither convex nor concave since its Hessian 

$$
\nabla^{2}f(x)={\left[\begin{array}{l l}{0}&{1}\\ {1}&{0}\end{array}\right]}
$$ 

is indefinite; it has one positive and one negative eigenvalue. The function $f$ is quasiconcave, however, since the superlevel sets 

$$
\{x\in\mathbf{R}_{+}^{2}\mid x_{1}x_{2}\geq\alpha\}
$$ 

are convex sets for all $\alpha$ . (Note, however, that $f$ is not quasiconcave on $\scriptstyle\mathbf{R}^{2}$ .) 

Example 3.32 Linear-fractional function. The function 

$$
f(x)={\frac{a^{T}x+b}{c^{T}x+d}},
$$ 

with $\mathbf{dom}\ f=\{x\ |\ c^{T}x+d>0\}$ , is quasiconvex, and quasiconcave, i.e. , quasilinear. Its $\alpha$ -sublevel set is 

$$
\begin{array}{l l l}{{S_{\alpha}}}&{{=}}&{{\{x\mid c^{T}x+d>0,~(a^{T}x+b)/(c^{T}x+d)\leq\alpha\}}}\\ {{}}&{{=}}&{{\{x\mid c^{T}x+d>0,~a^{T}x+b\leq\alpha(c^{T}x+d)\},}}\end{array}
$$ 

which is convex, since it is the intersection of an open halfspace and a closed halfspace. (The same method can be used to show its superlevel sets are convex.) 

Example 3.33 Distance ratio function. Suppose $a,b\in\mathbf{R}^{n}$ , and define 

$$
f(x)={\frac{||x-a||_{2}}{||x-b||_{2}}},
$$ 

i.e. , the ratio of the Euclidean distance to $a$ to the distance to $b$ . Then $f$ is quasiconvex on the halfspace $\{x\mid\|x-a\|_{2}\leq\|x-b\|_{2}\}$ . To see this, we consider the $\alpha$ -sublevel set of $f$ , with $\alpha\leq1$ since $f(x)\leq1$ on the halfspace $\{x\mid\|x-a\|_{2}\leq\|x-b\|_{2}\}$ . This sublevel set is the set of points satisfying 

$$
\|x-a\|_{2}\leq\alpha\|x-b\|_{2}.
$$ 

Squaring both sides, and rearranging terms, we see that this is equivalent to 

$$
\begin{array}{r}{(1-\alpha^{2})x^{T}x-2(a-\alpha^{2}b)^{T}x+a^{T}a-\alpha^{2}b^{T}b\leq0.}\end{array}
$$ 

This describes a convex set (in fact a Euclidean ball) if $\alpha\leq1$ . 

Example 3.34 Internal rate of return. Let $x=\left(x_{0},x_{1},.\,.\,.\,,x_{n}\right)$ denote a cash ﬂow sequence over $n$ periods, where $x_{i}>0$ means a payment to us in period $i$ , and $x_{i}<0$ means a payment by us in period $i$ . We define the present value of a cash ﬂow, with interest rate $r\geq0$ , to be 

$$
\operatorname{PV}(x,r)=\sum_{i=0}^{n}(1+r)^{-i}x_{i}.
$$ 

(The factor $(1+r)^{-i}$ is a discount factor for a payment by or to us in period $i$ .) 

Now we consider cash ﬂows for which $x_{\mathrm{0}}<\;0$ and $x_{0}+x_{1}+\cdot\cdot\cdot+x_{n}\,>\,0$ . This means that we start with an investment of $\left|x_{0}\right|$ in period $0$ , and that the total of the 

remaining cash ﬂow, $x_{1}+\cdot\cdot\cdot+x_{n}$ , (not taking any discount factors into account) exceeds our initial investment. 

For such a cash ﬂow, $\mathrm{PV}(x,0)>0$ and $\mathrm{PV}(x,r)\to x_{0}<0$ as $r\rightarrow\infty$ , so it follows that for at least one $r\,\geq\,0$ , we have $\mathrm{PV}(x,r)\,=\,0$ . We define the internal rate of return of the cash ﬂow as the smallest interest rate $r\geq0$ for which the present value is zero: 

$$
\operatorname{IRR}(x)=\operatorname*{inf}\{r\geq0\mid\operatorname{PV}(x,r)=0\}.
$$ 

Internal rate of return is a quasiconcave function of $x$ (restricted to $x_{\mathrm{0}}<0$ , $x_{1}+\cdot\cdot\cdot+$ $x_{n}>0$ ). To see this, we note that 

$$
\mathrm{IRR}(x)\geq R\iff\mathrm{PV}(x,r)>0\ \mathrm{for}\ 0\leq r<R.
$$ 

The lefthand side defines the $R$ -superlevel set of IRR. The righthand side is the intersection of the sets $\{x\mid\mathrm{PV}(x,r)>0\}$ , indexed by $r$ , over the range $0\leq r<R$ . For each $r$ , $\mathrm{PV}(x,r)>0$ defines an open halfspace, so the righthand side defines a convex set. 

# 3.4.2 Basic properties 

The examples above show that quasiconvexity is a considerable generalization of convexity. Still, many of the properties of convex functions hold, or have analogs, for quasiconvex functions. For example, there is a variation on Jensen’s inequality that characterizes quasiconvexity: A function $f$ is quasiconvex if and only if $\mathbf{dom}\ f$ is convex and for any $x,\ y\in\mathbf{dom}\ f$ and $0\leq\theta\leq1$ , 

$$
f(\theta x+(1-\theta)y)\leq\operatorname*{max}\{f(x),f(y)\},
$$ 

i.e. , the value of the function on a segment does not exceed the maximum of its values at the endpoints. The inequality ( 3.19 ) is sometimes called Jensen’s inequality for quasiconvex functions, and is illustrated in figure 3.10 . 

Example 3.35 Cardinality of a nonnegative vector. The cardinality or size of a vector $x\,\in\,\mathbf{R}^{\,n}$ is the number of nonzero com ents, and denoted $\mathbf{card}(x)$ . The function card is quasiconcave on $\mathbf{R}_{+}^{n}$ (but not R ). This follows immediately from the modified Jensen inequality 

$$
\mathbf{card}(x+y)\geq\operatorname*{min}\{\mathbf{card}(x),\mathbf{card}(y)\},
$$ 

which holds for $x,\ y\succeq0$ . 

Example 3.36 Rank of positive semidefinite matrix. The function $\mathbf{rank}\,X$ is quasi- concave on $\mathbf{S}_{+}^{n}$ . This follows from the modified Jensen inequality ( 3.19 ), 

$$
{\bf r a n k}(X+Y)\geq\operatorname*{min}\{{\bf r a n k}\,X,{\bf r a n k}\,Y\}
$$ 

which holds for $X$ , $Y\,\in\,{\bf S}_{+}^{n}$ . (This can be considered an extension of the previous example, since $\mathbf{rank}(\mathbf{diag}(x))=\mathbf{card}(x)$ for $x\succeq0$ .) 

![](images/cf06f1798b4afd6532e7bbe4a11087af7f9ef823c148452292d293a17694c600.jpg) 
Figure $\mathbf{3.10\textup{A}}$ quasiconvex function on $\mathbf{R}$ . The value of $f$ between $x$ and $y$ is no more than $\operatorname*{max}\{f(x),f(y)\}$ . 

Like convexity, quasiconvexity is characterized by the behavior of a function $f$ on lines: $f$ is quasiconvex if and only if its restriction to any line intersecting its domain is quasiconvex. In particular, quasiconvexity of a function can be verified by restricting it to an arbitrary line, and then checking quasiconvexity of the resulting function on $\mathbf{R}$ . 

# Quasiconvex functions on R 

We can give a simple characterization of quasiconvex functions on $\mathbf{R}$ . We consider continuous functions, since stating the conditions in the general case is cumbersome. A continuous function $f:\mathbf{R}\rightarrow\mathbf{R}$ is quasiconvex if and only if at least one of the following conditions holds: 

• $f$ is nondecreasing 

$f$ is nonincreasing 

• there is a point $c\;\in\;\mathbf{dom}\,f$ such that for $t~\leq~c$ (and $t\,\in\,\mathbf{dom}\,f$ ), $f$ is nonincreasing, and for $t\geq c$ (and $t\in\mathbf{dom}\ f$ ), $f$ is nondecreasing. 

The point $c$ can be chosen as any point which is a global minimizer of $f$ . Figure 3.11 illustrates this. 

# 3.4.3 Diﬀerentiable quasiconvex functions 

# First-order conditions 

Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is diﬀerentiable. Then $f$ is quasiconvex if and only if $\mathbf{dom}\,f$ is convex and for all $x$ , $y\in\mathbf{dom}\ f$ 

$$
f(y)\leq f(x)\Longrightarrow\nabla f(x)^{T}(y-x)\leq0.
$$ 

![](images/38ff735d5727563a7b5569e304ad0ceb99284984b2fd5d87429ee7fbf7a38c1c.jpg) 
Figure 3.11 A quasiconvex function on $\mathbf{R}$ . The function is nonincreasing for $t\leq c$ and nondecreasing for $t\geq c$ . 

![](images/c8236ef8b466cbf8f916d843801030542b4d68a94810be268aecf10d5b894f0e.jpg) 
Figure 3.12 Three level curves of a quasiconvex function $f$ are shown. The vector $\nabla f(x)$ defines a supporting hyperplane to the sublevel set $\{z\mid f(z)\leq$ $f(x)\}$ at $x$ . 

This is the analog of inequality ( 3.2 ), for quasiconvex functions. We leave the proof as an exercise (exercise 3.43 ). 

The condition ( 3.20 ) has a simple geometric interpretation when $\nabla f(x)\neq0$ . It states that $\nabla f(x)$ defines a supporting hyperplane to the sublevel set $\{y\mid f(y)\leq$ $f(x)\}$ , at the point $x$ , as illustrated in figure 3.12 . 

While the first-order condition for convexity ( 3.2 ), and the first-order condition for quasiconvexity ( 3.20 ) are similar, there are some important diﬀerences. For example, if $f$ is convex and $\nabla f(x)=0$ , then $x$ is a global minimizer of $f$ . But this statement is false for quasiconvex functions: it is possible that $\nabla f(x)=0$ , but $x$ is not a global minimizer of $f$ . 

# Second-order conditions 

Now suppose $f$ is twice diﬀerentiable. If $f$ is quasiconvex, then for all $x\in\mathbf{dom}\,f$ , and all $y\in\mathbf{R}^{n}$ , we have 

$$
\boldsymbol{y}^{T}\nabla f(\boldsymbol{x})=\boldsymbol{0}\Longrightarrow\boldsymbol{y}^{T}\nabla^{2}f(\boldsymbol{x})\boldsymbol{y}\geq0.
$$ 

For a quasiconvex function on $\mathbf{R}$ , this reduces to the simple condition 

$$
f^{\prime}(x)=0\Longrightarrow f^{\prime\prime}(x)\geq0,
$$ 

i.e. , at any point with zero slope, the second derivative is nonnegative. For a quasiconvex function on $\mathbf{R}^{n}$ , the interpretation of the condition ( 3.21 ) is a bit more complicated. As in the case $n=1$ , we conclude that whenever $\nabla f(x)=0$ , we must have $\nabla^{2}f(x)\succeq0$ . When $\nabla f(x)\,\ne\,0$ , the condition ( 3.21 ) means that $\nabla^{2}f(x)$ is positive semidefinite on the $(n-1)$ -dimensional subspace $\nabla f(x)^{\perp}$ . This implies that $\nabla^{2}f(x)$ can have at most one negative eigenvalue. 

As a (partial) converse, if $f$ satisfies 

$$
\boldsymbol{y}^{T}\nabla f(\boldsymbol{x})=0\Longrightarrow\boldsymbol{y}^{T}\nabla^{2}f(\boldsymbol{x})\boldsymbol{y}>0
$$ 

for all $x\in\mathbf{dom}\ f$ and all $y\in\mathbf{R}^{n}$ , $y\ne0$ , then $f$ is quasiconvex. This condition is the same as requiring $\nabla^{2}f(x)$ to be positive definite for any point with $\nabla f(x)=0$ , and for all other points, requiring $\nabla^{2}f(x)$ to be positive definite on the $(n-1)$ - dimensional subspace $\nabla f(x)^{\perp}$ . 

# Proof of second-order conditions for quasiconvexity 

By restricting the function to an arbitrary line, it suffices to consider the case in which $f:\mathbf{R}\rightarrow\mathbf{R}$ . 

We first show that if $f:\mathbf{R}\rightarrow\mathbf{R}$ is quasiconvex on an interval $(a,b)$ , then it must satisfy ( 3.21 ), i.e. , if $f^{\prime}(c)=0$ with $c\in(a,b)$ , then we must have $f^{\prime\prime}(c)\geq0$ . If $f^{\prime}(c)=0$ with $c\in(a,b)$ , $f^{\prime\prime}(c)<0$ , then for small positive $\epsilon$ we have $f(c\!-\!\epsilon)<f(c)$ and $f(c+\epsilon)\,<\,f(c)$ . It follows that the sublevel set $\{x\ \vert\ f(x)\ \le\ f(c)\,-\,\epsilon\}$ is disconnected for small positive $\epsilon$ , and therefore not convex, which contradicts our assumption that $f$ is quasiconvex. 

Now we show that if the condition ( 3.22 ) holds, then $f$ is quasiconvex. Assume that ( 3.22 ) holds, i.e. , for each $c\in(a,b)$ with $f^{\prime}(c)=0$ , we have $f^{\prime\prime}(c)>0$ . This means that whenever the function $f^{\prime}$ crosses the value $0$ , it is strictly increasing. Therefore it can cross the value $0$ at most once. If $f^{\prime}$ does not cross the value $0$ at all, then $f$ is either nonincreasing or nondecreasing on $(a,b)$ , and therefore quasiconvex. Otherwise it must cross the value 0 exactly once, say at $c\in(a,b)$ . Since $f^{\prime\prime}(c)>0$ , it follows that $f^{\prime}(t)\leq0$ for $a<t\leq c$ , and $f^{\prime}(t)\geq0$ for $c\leq t<b$ . This shows that $f$ is quasiconvex. 

# 3.4.4 Operations that preserve quasiconvexity 

# Nonnegative weighted maximum 

A nonnegative weighted maximum of quasiconvex functions, i.e. , 

$$
f=\operatorname*{max}\{w_{1}f_{1},.\,.\,.\,,w_{m}f_{m}\},
$$ 

with $w_{i}\,\geq\,0$ and $f_{i}$ quasiconvex, is quasiconvex. The property extends to the general pointwise supremum 

$$
f(x)=\operatorname*{sup}_{y\in C}(w(y)g(x,y))
$$ 

where $w(y)\geq0$ and $g(x,y)$ is quasiconvex in $x$ for each $y$ . This fact can be easily verified: $f(x)\leq\alpha$ if and only if 

$$
w(y)g(x,y)\leq\alpha{\mathrm{~for~all~}}y\in C,
$$ 

i.e. , the $\alpha$ -sublevel set of $f$ is the intersection of the $\alpha$ -sublevel sets of the functions $w(y)g(x,y)$ in the variable $x$ . 

Example 3.37 Generalized eigenvalue. The maximum generalized eigenvalue of a pair of symmetric matrices $(X,Y)$ , with $Y\succ0$ , is defined as 

$$
\lambda_{\operatorname*{max}}(X,Y)=\operatorname*{sup}_{u\neq0}\frac{u^{T}X u}{u^{T}Y u}=\operatorname*{sup}\{\lambda\mid\operatorname*{det}(\lambda Y-X)=0\}.
$$ 

(See § A.5.3 ). This function is quasiconvex on $\mathbf{dom}\,f=\mathbf{S}^{n}\times\mathbf{S}_{++}^{n}$ 

To see this we consider the expression 

$$
\lambda_{\operatorname*{max}}(X,Y)=\operatorname*{sup}_{u\neq0}\frac{u^{T}X u}{u^{T}Y u}.
$$ 

For each $u\ne~0$ , the function $u^{T}X u/u^{T}Y u$ is linear-fractional in $(X,Y)$ , hence a quasiconvex function of $(X,Y)$ . We conclude that $\lambda_{\mathrm{max}}$ is quasiconvex, since it is the supremum of a family of quasiconvex functions. 

# Composition 

If $g:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is quasiconvex and $h:\mathbf{R}\rightarrow\mathbf{R}$ is nondecreasing, then $f=h\circ g$ is quasiconvex. 

The composition of a quasiconvex function with an affine or linear-fractional transformation yields a quasiconvex function. If $f$ is quasiconvex, then $g(x)\,=$ $f(A x+b)$ is quasiconvex, and $\tilde{g}(x)=f((A x+b)/(c^{T}x+d))$ )) is quasiconvex on the set 

$$
\{x\mid c^{T}x+d>0,\ (A x+b)/(c^{T}x+d)\in\mathbf{dom}\,f\}.
$$ 

# Minimization 

If $f(x,y)$ is quasiconvex jointly in $x$ and $y$ and $C$ is a convex set, then the function 

$$
g(x)=\operatorname*{inf}_{y\in C}f(x,y)
$$ 

is quasiconvex. 

To show this, we need to show that $\{x\mid g(x)\leq\alpha\}$ is convex re $\alpha\in\mathbf{R}$ is arbitrary. From the definition of $g$ , $g(x)\leq\alpha$ if and only if for any ǫ > 0 there exists a $y\in C$ with $f(x,y)\leq\alpha+\epsilon$ . Now let $x_{1}$ and $x_{2}$ be two points in the $\alpha$ -sublevel set of $g$ . Then for any ǫ > 0, there exists $y_{1}$ , $y_{2}\in C$ with 

$$
f(x_{1},y_{1})\leq\alpha+\epsilon,\qquad f(x_{2},y_{2})\leq\alpha+\epsilon,
$$ 

and since $f$ is quasiconvex in $x$ and $y$ , we also have 

$$
f{\bigl(}\theta x_{1}+(1-\theta)x_{2},\theta y_{1}+(1-\theta)y_{2}{\bigr)}\leq\alpha+\epsilon,
$$ 

for $0\leq\theta\leq1$ . Hence $g(\theta x_{1}+(1-\theta)x_{2})\leq\alpha$ , which proves that $\{x\mid g(x)\leq\alpha\}$ is convex. 

# 3.4.5 Representation via family of convex functions 

In the sequel, it will be convenient to represent the sublevel sets of a quasiconvex function $f$ (which are convex) via inequalities of convex functions. We seek a family of convex functions $\phi_{t}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , indexed by $t\in\mathbf{R}$ , with 

$$
f(x)\leq t\iff\phi_{t}(x)\leq0,
$$ 

i.e. , the $t$ -sublevel set of the quasiconvex function $f$ is the $0$ -sublevel set of the convex function $\phi_{t}$ . Evidently $\phi_{t}$ must satisfy the property that for all $x\,\in\,\mathbf{R}^{n}$ , $\phi_{t}(x)\,\leq\,0\;\;\Longrightarrow\;\;\phi_{s}(x)\,\leq\,0$ ⇒ ≤ 0 for $s\,\geq\,t$ . This is satisfied if for each $x$ , $\phi_{t}(x)$ is a nonincreasing function of $t$ , i.e. , $\phi_{s}(x)\leq\phi_{t}(x)$ whenever $s\geq t$ . 

To see that such a representation always exists, we can take 

$$
\phi_{t}(x)={\left\{\begin{array}{l l}{0}&{f(x)\leq t}\\ {\infty}&{{\mathrm{otherwise}},}\end{array}\right.}
$$ 

i.e. , $\phi_{t}$ is the indicator function of the $t$ -sublevel of $f$ . Obviously this representation is not unique; for example if the sublevel sets of $f$ are closed, we can take 

$$
\phi_{t}(x)=\mathrm{{dist}}\left(x,\left\{z\mid f(z)\leq t\right\}\right).
$$ 

We are usually interested in a family $\phi_{t}$ with nice properties, such as diﬀerentia- bility. 

Example 3.38 Convex over concave function. Suppose $p$ is a convex function, $q$ is a concave function, with $p(x)\geq0$ an $q(x)>0$ on a convex set $C$ . Then the function $f$ defined by $f(x)=p(x)/q(x)$ , on C , is quasiconvex. 

Here we have 

$$
f(x)\leq t\iff p(x)-t q(x)\leq0,
$$ 

so we can take $\phi_{t}(x)=p(x)-t q(x)$ for $t\geq0$ . For each $t$ , $\phi_{t}$ is convex and for each $x$ , $\phi_{t}(x)$ is decreasing in $t$ . 

# 3.5 Log-concave and log-convex functions 

# 3.5.1 Definition 

A function $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ is logarithmically concave or log-concave if $f(x)\;>\;0$ for all $x\,\in\,\mathbf{dom}\,f$ and $\log f$ is concave. It is said to be logarithmically convex or log-convex if $\log f$ is convex. Thus $f$ is log-convex if and only if $1/f$ is log- concave. It is convenient to allow $f$ to take on the value zero, in which case we take $\log f(x)\,=\,-\infty$ . In this case we say $f$ is log-concave if the extended-value function $\log f$ is concave. 

We can express log-concavity directly, without logarithms: a function $f:\mathbf{R}^{n}\rightarrow$ $\mathbf{R}$ , with convex domain and $f(x)>0$ for all $x\in\mathbf{dom}\,f$ , is log-concave if and only if for all $x,\ y\in\mathbf{dom}\,f$ and $0\leq\theta\leq1$ , we have 

$$
f(\theta x+(1-\theta)y)\geq f(x)^{\theta}f(y)^{1-\theta}.
$$ 

In particular, the value of a log-concave function at the average of two points is at least the geometric mean of the values at the two points. 

From the composition rules we know that $e^{h}$ is convex if $h$ is convex, so a log- convex function is convex. Similarly, a nonnegative concave function is log-concave. It is also clear that a log-convex function is quasiconvex and a log-concave function is quasiconcave, since the logarithm is monotone increasing. 

# Example 3.39 Some simple examples of log-concave and log-convex functions. 

• Affine function. $f(x)=a^{T}x+b$ is log-concave on $\{x\mid a^{T}x+b>0\}$ . • Powers. $f(x)=x^{a}$ , on $\mathbf{R}_{++}$ , is log-convex for $a\leq0$ , and log-concave for $a\geq0$ . • Exponentials. $f(x)=e^{a x}$ is log-convex and log-concave. • The cumulative distribution function of a Gaussian density, $\Phi(x)={\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{x}e^{-u^{2}/2}\;d u,$ is log-concave (see exercise 3.54 ). • Gamma function. The Gamma function, $\Gamma(x)=\int_{0}^{\infty}u^{x-1}e^{-u}~d u,$ is log-convex for $x\geq1$ (see exercise 3.52 ). • Determinant. $\operatorname{det}X$ is log concave on $\mathbf{S}_{++}^{n}$ . • Determinant over trace. $\operatorname*{det}X/\operatorname{tr}X$ is log concave on $\mathbf{S}_{++}^{n}$ (see exercise 3.49 ). 

Example 3.40 Log-concave density functions. Many common probability density functions are log-concave. Two examples are the multivariate normal distribution, 

$$
f(x)={\frac{1}{\sqrt{(2\pi)^{n}\operatorname*{det}{\Sigma}}}}e^{-{\frac{1}{2}}(x-{\bar{x}})^{T}{\Sigma}^{-1}(x-{\bar{x}})}
$$ 

(where $\bar{\boldsymbol{x}}\in\mathbf{R}^{n}$ ∈ and $\Sigma\in\mathbf{S}_{++}^{n}$ ), and the exponential distribution on ${\bf R}_{+}^{n}$ , 

$$
f(x)=\left(\prod_{i=1}^{n}\lambda_{i}\right)e^{-\lambda^{T}x}
$$ 

(where $\lambda\succ0$ ). Another example is the uniform distribution over a convex set $C$ , 

$$
f(x)={\left\{\begin{array}{l l}{1/\alpha}&{x\in C}\\ {0}&{x\not\in C}\end{array}\right.}
$$ 

where $\alpha=\mathbf{vol}(C)$ is the volume (Lebesgue measure) of $C$ . In this case $\log f$ takes on the value $-\infty$ outside $C$ , and $-\log\alpha$ on $C$ , hence is concave. 

As a more exotic example consider the Wishart distribution, defined as follows. Let $x_{1},.\,.\,.\,,x_{p}\,\in\,\mathbf{R}^{n}$ be independent Gaussian random vectors with zero mean and co- variance Σ $\Sigma\in\mathbf{S}^{\pi}$ ∈ , with $p>n$ . The random matrix $\begin{array}{r}{X=\sum_{i=1}^{p}x_{i}x_{i}^{T}}\end{array}$ has the Wishart density 

$$
f(X)=a\left(\operatorname*{det}X\right)^{(p-n-1)/2}e^{-\frac{1}{2}\,\mathbf{tr}(\Sigma^{-1}X)},
$$ 

with $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ , and $a$ is a positive constant. The Wishart density is log-concave, since 

$$
\log f(X)=\log a+{\frac{p-n-1}{2}}\log\operatorname*{det}X-{\frac{1}{2}}\operatorname{\bftr}(\Sigma^{-1}X),
$$ 

which is a concave function of $X$ . 

# 3.5.2 Properties 

# Twice diﬀerentiable log-convex/concave functions 

Suppose $f$ is twice diﬀerentiable, with $\mathbf{dom}\ f$ convex, so 

$$
\nabla^{2}\log f(x)={\frac{1}{f(x)}}\nabla^{2}f(x)-{\frac{1}{f(x)^{2}}}\nabla f(x)\nabla f(x)^{T}.
$$ 

We conclude that $f$ is log-convex if and only if for all $x\in\mathbf{dom}\,f$ , 

$$
f(x)\nabla^{2}f(x)\succeq\nabla f(x)\nabla f(x)^{T},
$$ 

and log-concave if and only if for all $x\in\mathbf{dom}\,f$ , 

$$
f(x)\nabla^{2}f(x)\preceq\nabla f(x)\nabla f(x)^{T}.
$$ 

# Multiplication, addition, and integration 

Log-convexity and log-concavity are closed under multiplication and positive scal- ing. For example, if $f$ and $g$ are log-concave, then so is the pointwise product $h(x)=f(x)g(x)$ , since $\log h(x)=\log f(x)+\log g(x)$ , and $\log f(x)$ and $\log g(x)$ are concave functions of $x$ . 

Simple examples show that the sum of log-concave functions is not, in general, log-concave. Log-convexity, however, is preserved under sums. Let $f$ and $g$ be log- convex functions, i.e. , $F=\log f$ and $G=\log g$ are convex. From the composition rules for convex functions, it follows that 

$$
\log\left(\exp F+\exp G\right)=\log(f+g)
$$ 

is convex. Therefore the sum of two log-convex functions is log-convex. More generally, if $f(x,y)$ is log-convex in $x$ for each $y\in C$ then 

$$
g(x)=\int_{C}f(x,y)\;d y
$$ 

is log-convex. 

Example 3.41 Laplace transform of a nonnegative function and the moment and cumulant generating functions. Suppose $p:\mathbf{R}^{n}\rightarrow\mathbf{R}$ satisfies $p(x)\geq0$ for all $x$ . The Laplace transform of $p$ , 

$$
P(z)=\int p(x)e^{-z^{T}x}\;d x,
$$ 

is log-convex on $\mathbf{R}^{n}$ . (Here $\mathbf{dom}\,P$ is, naturally, $\{z\mid P(z)<\infty\}$ .) 

Now suppose $p$ is a density, i.e. , satisfies $\textstyle\int p(x)\;d x=1$ = 1. The function $M(z)=P(-z)$ is called the moment generating function of the density. It gets its name from the fact that the moments of the density can be found from the derivatives of the moment generating function, evaluated at $z=0$ , e.g. , 

$$
\nabla M(0)={\bf E}\,\boldsymbol{v},\qquad{\nabla}^{2}M(0)={\bf E}\,\boldsymbol{v}\boldsymbol{v}^{T},
$$ 

where $v$ is a random variable with density $p$ . 

The function $\log M(z)$ , which is convex, is called the cumulant generating function for $p$ , since its derivatives give the cumulants of the density. For example, the first and second derivatives of the cumulant generating function, evaluated at zero, are the mean and covariance of the associated random variable: 

$$
\nabla\log M(0)={\bf E}\,\boldsymbol{v},\qquad\nabla^{2}\log M(0)={\bf E}(\boldsymbol{v}-{\bf E}\,\boldsymbol{v})(\boldsymbol{v}-{\bf E}\,\boldsymbol{v})^{T}.
$$ 

# Integration of log-concave functions 

In some special cases log-concavity is preserved by integration. If $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ is log-concave, then 

$$
g(x)=\int f(x,y)\;d y
$$ 

is a log-concave function of $x$ (on $\mathbf{R}^{n}$ ). (The integration here is over $\mathbf{R}^{m}$ .) A proof of this result is not simple; see the references. 

This result has many important consequences, some of which we describe in the rest of this section. It implies, for example, that marginal distributions of log- concave probability densities are log-concave. It also implies that log-concavity is closed under convolution, i.e. , if $f$ and are log-concave on $\mathbf{R}^{n}$ , then so is the $g$ convolution 

$$
(f*g)(x)=\int f(x-y)g(y)\;d y.
$$ 

(To see this, note that $g(y)$ and $f(x\!-\!y)$ are log-concave in $(x,y)$ , hence the product $f(x-y)g(y)$ is; then the integration result applies.) 

Suppose $C\subseteq\mathbf{R}^{n}$ is a convex set and $w$ is a random vector in $\mathbf{R}^{n}$ with log- concave probability density $p$ . Then the function 

$$
f(x)=\mathbf{prob}(x+w\in C)
$$ 

is log-concave in $x$ . To see this, express $f$ as 

$$
f(x)=\int g(x+w)p(w)~d w,
$$ 

where $g$ is defined as 

$$
g(u)={\left\{\begin{array}{l l}{1}&{u\in C}\\ {0}&{u\not\in C,}\end{array}\right.}
$$ 

(which is log-concave) and apply the integration result. 

Example 3.42 The cumulative distribution function of a probability density function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is defined as 

$$
F(x)={\bf p r o b}(w\preceq x)=\int_{-\infty}^{x_{n}}\cdot\cdot\cdot\int_{-\infty}^{x_{1}}f(z)\;d z_{1}\cdot\cdot\cdot d z_{n},
$$ 

where $w$ is a random variable with density $f$ . If $f$ is log-concave, then $F$ is log- concave. We have already encountered a special case: the cumulative distribution function of a Gaussian random variable, 

$$
f(x)={\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{x}e^{-t^{2}/2}\;d t,
$$ 

is log-concave. (See example 3.39 and exercise 3.54 .) 

Example 3.43 Yield function. Let $x\,\in\,\mathbf{R}^{\,n}$ denote the nominal or target value of a set of parameters of a product that is manufactured. Variation in the manufacturing process causes the parameters of the product, when manufactured, to have the value $x+w$ , where $w\,\in\,\mathbf{R}^{\,n}$ is a random vector that represents manufacturing variation, and is usually assumed to have zero mean. The yield of the manufacturing process, as a function of the nominal parameter values, is given by 

$$
Y(x)=\mathbf{prob}(x+w\in S),
$$ 

where $S\subseteq\mathbf{R}^{n}$ denotes the set of acceptable parameter values for the product, i.e. , the product specifications . 

If the density of the manufacturing error $w$ is log-concave (for example, Gaussian) and the set $S$ of product specifications is convex, then the yield function $Y$ is log-concave. This implies that the $\alpha$ - yield region , defined as the set of nominal parameters for which the yield exceeds $\alpha$ , is convex. For example, the 95% yield region 

$$
\{x\mid Y(x)\geq0.95\}=\{x\mid\log Y(x)\geq\log0.95\}
$$ 

is convex, since it is a superlevel set of the concave function $\log{Y}$ . Example 3.44 Volume of polyhedron. Let $A\in\mathbf{R}^{m\times n}$ . Define 

$$
P_{u}=\{x\in\mathbf{R}^{n}\mid A x\preceq u\}.
$$ 

Then its volume $\mathbf{vol}P_{u}$ is a log-concave function of $u$ To prove this, note that the function 

$$
\Psi(x,u)=\left\{\begin{array}{l l}{{1}}&{{A x\preceq u}}\\ {{0}}&{{\mathrm{otherwise},}}\end{array}\right.
$$ 

is log-concave. By the integration result, we conclude that 

$$
\int\Psi(x,u)\;d x=\mathbf{vol}\,P_{u}
$$ 

is log-concave. 

# 3.6 Convexity with respect to generalized inequalities 

We now consider generalizations of the notions of monotonicity and convexity, using generalized inequalities instead of the usual ordering on $\mathbf{R}$ . 

# 3.6.1 Monotonicity with respect to a generalized inequality 

Suppose $K\subseteq\mathbf{R}^{n}$ is a proper ne with associated generalized inequality $\preceq_{K}$ . A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is called K - nondecreasing if 

$$
x\preceq_{K}y\Longrightarrow f(x)\leq f(y),
$$ 

and $K$ - increasing if 

$$
x\preceq_{K}y,\;x\neq y\Longrightarrow f(x)<f(y).
$$ 

We define $K$ - nonincreasing and $K$ - decreasing functions in a similar way. 

Example 3.45 Monotone vector functions. A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is nondecreasing with respect to $\mathbf{R}_{+}^{n}$ if and only if 

$$
x_{1}\leq y_{1},.\,.\,.\,x_{n}\leq y_{n}\implies f(x)\leq f(y)
$$ 

for all $x,\ y$ . This is the same as saying that $f$ , when restricted to any component $x_{i}$ ( i.e. , $x_{i}$ is considered the variable while $x_{j}$ for $j\neq i$ are fixed), is nondecreasing. 

Example 3.46 Matrix monotone functions. A function $f:\mathbf{S}^{n}\rightarrow\mathbf{R}$ is called ma- trix monotone (increasing, decreasing) if it is monotone with respect to the posi- tive semidefinite cone. Some examples of matrix monotone functions of the variable $X\in\mathbf{S}^{n}$ : 

• $\mathbf{tr}(W X)$ , where $W\,\in\,{\bf S}^{n}$ , is matrix nondecreasing if $W\succeq0$ , and matrix in- creasing if $W\succ0$ (it is matrix nonincreasing if $W\preceq0$ , and matrix decreasing if $W\prec0$ ). • $\mathbf{tr}(X^{-1})$ is matrix decreasing on $\mathbf{S}_{++}^{n}$ . • $\operatorname{det}X$ is matrix increasing on $\mathbf{S}_{++}^{n}$ , and matrix nondecreasing on $\mathbf{S}_{+}^{n}$ . 

# Gradient conditions for monotonicity 

Recall that a diﬀerentiable function $f:\mathbf{R}\rightarrow\mathbf{R}$ , with convex ( i.e. , interval) domain, is nondecreasing if and only if $f^{\prime}(x)\;\geq\;0$ for all $x\;\in\;\mathbf{dom}\,f$ , and increasing if $f^{\prime}(x)\;>\;0$ for all $x\;\in\;\mathbf{dom}\,f$ (but the converse is not true). These conditions are readily extended to the case of monotonicity with respect to a generalized inequality. A diﬀerentiable function $f$ , with convex domain, is $K$ -nondecreasing if and only if 

$$
\nabla f(x)\succeq_{K^{*}}0
$$ 

for all $x\,\in\,\mathbf{dom}\,f$ . Note the diﬀerence with the simple scalar case: the gradi- ent must be nonnegative in the dual inequality. For the strict case, we have the following: If 

$$
\nabla f(x)\succ_{K^{*}}0
$$ 

for all $x\in\mathbf{dom}\,f$ , then $f$ is $K$ -increasing. As in the scalar case, the converse is not true. 

Let us prove these first-order conditions for monotonicity. First, assume that $f$ satisfies ( 3.24 ) for all $x$ , but is not $K$ -nondecreasing, i.e. , there exist $x$ , $y$ with $x\preceq_{K}y$ and $f\!\left(y\right)<f\!\left(x\right)$ . By di e rent i ability of $f$ there exists a $t\in[0,1]$ with 

$$
\frac{d}{d t}f(x+t(y-x))=\nabla f(x+t(y-x))^{T}(y-x)<0.
$$ 

Since $y-x\in K$ this means 

$$
\nabla f(x+t(y-x))\notin K^{*},
$$ 

which contradicts our assumption that ( 3.24 ) is satisfied everywhere. In a similar way it can be shown that ( 3.25 ) implies $f$ is $K$ -increasing. 

It is also straightforward to see that it is necessary that ( 3.24 ) hold everywhere. Assume ( 3.24 ) does not hold for $x=z$ . By the definition of dual cone this means there exists a $v\in K$ with 

$$
\nabla f(z)^{T}v<0.
$$ 

Now consider $h(t)=f(z+t v)$ as a function of $t$ . We have $h^{\prime}(0)=\nabla f(z)^{T}v<0$ , and therefore there exists $t>0$ with $h(t)=f(z+t v)<h(0)=f(z)$ , which means $f$ is not $K$ -nondecreasing. 

# 3.6.2 Convexity with respect to a generalized inequality 

Suppose $K\subseteq\mathbf{R}^{m}$ is proper cone with associated generalized inequality $\preceq_{K}$ . We say $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ is K -convex if for all $x$ , $y$ , and $0\leq\theta\leq1$ , 

$$
f{\big(}\theta x+(1-\theta)y{\big)}\ \preceq_{K}\ \theta f(x)+(1-\theta)f(y).
$$ 

The function is strictly $K$ -convex if 

$$
f(\theta x+(1-\theta)y)\;\prec_{K}\;\theta f(x)+(1-\theta)f(y)
$$ 

for all $x\neq y$ and $0<\theta<1$ . These definitions reduce to ordinary convexity and strict convexity when m = 1 (and $K=\mathbf{R}_{+}$ ). 

Example 3.47 Convexity with respect to componentwise inequality . A function $f:$ ${\bf R}^{n}\rightarrow{\bf R}^{m}$ is convex with respect to componentwise inequality ( i.e. , the generalized inequality induced by $\mathbf{R}_{+}^{m}$ ) if and only if for all $x,\ y$ and $0\leq\theta\leq1$ , 

$$
f(\theta x+(1-\theta)y)\preceq\theta f(x)+(1-\theta)f(y),
$$ 

i.e. , each component $f_{i}$ is a convex function. The function $f$ is strictly convex with respect to componentwise inequality if and only if each component $f_{i}$ is strictly con- vex. 

Example 3.48 Matrix convexity . Suppose $f$ is a symmetric matrix valued function, i.e. , $f:\mathbf{R}^{n}\rightarrow\mathbf{S}^{n n}$ . The function $f$ is convex with respect to matrix inequality if 

$$
f(\theta x+(1-\theta)y)\preceq\theta f(x)+(1-\theta)f(y)
$$ 

for any $x$ and $y$ , and for $\theta\,\in\,[0,1]$ . This is sometimes called matrix convexity . An equivalent definition is that the scalar function $z^{T}f(x)z$ is convex for all vectors $z$ . (This is often a good way to prove matrix convexity). A matrix function is strictly matrix convex if 

$$
f(\theta x+(1-\theta)y)\prec\theta f(x)+(1-\theta)f(y)
$$ 

when $x\neq y$ and $0<\theta<1$ , or, equivalently, if $z^{T}f z$ is strictly convex for every $z\neq0$ . Some examples: 

• The function $f(X)~=~X X^{T}$ where $X\,\,\in\,\,\mathbf{R}^{n\times m}$ is matrix convex, since for fixed $z$ the funct n $z^{T}X X^{T}z=\|X^{T}z\|_{2}^{2}$ is a convex quadratic function of (the components of) X . For the same reason, $f(X)=X^{2}$ is matrix convex on $\mathbf{S}^{n}$ . • The function $X^{p}$ is matrix convex on $\mathbf{S}_{++}^{n}$ for $1\leq p\leq2$ or $-1\leq p\leq0$ , and matrix concave for $0\leq p\leq1$ . • The function $f(X)=e^{X}$ is not matrix convex on $\mathbf{S}^{\ast}$ , for $n\geq2$ . 

Many of the results for convex functions have extensions to $K$ -convex functions. As a simple example, a function is $K$ -convex if and only if its restriction to any line in its domain is $K$ -convex. In the rest of this section we list a few results for $K$ -convexity that we will use later; more results are explored in the exercises. 

# Dual characterization of $K$ -convexity 

A function $f$ is $K$ -convex if and only if for every $w\succeq_{K^{*}}$ $0$ , the (real-valued) function $w^{T}f$ is convex (in the ordinary sense); $f$ is strictly K -convex if and only if for every nonzero $w\succeq_{K^{*}}$ 0 the function $w^{T}f$ is strictly convex. (These follow directly from the definitions and properties of dual inequality.) 

# Diﬀerentiable $K$ -convex functions 

A diﬀerentiable function $f$ is $K$ -convex if and only if its domain is convex, and for all $x,\ y\in\mathbf{dom}\,f$ , 

$$
f(y)\succeq_{K}f(x)+D f(x)(y-x).
$$ 

(Here $D f(x)\in\mathbf{R}^{m\times n}$ is the derivative or Jacobian matrix of $f$ at $x$ ; see § A.4.1 .) The function $f$ is strictly $K$ -convex if and only if for all $x,\ y\in\mathbf{dom}\ f$ with $x\neq y$ , 

$$
f(y)\succ_{K}f(x)+D f(x)(y-x).
$$ 

# Composition theorem 

Many of the results on composition can be generalized to $K$ -convexity. For example, if $g:\mathbf{R}^{n}\rightarrow\mathbf{R}^{p}$ i $K$ -convex, $h:\mathbf{R}^{p}\rightarrow\mathbf{R}$ is convex, and h (the extended-value extension of $h$ ) is K -nondecreasing, then $h\circ g$ is convex. This generalizes the fact that a nondecreasing convex function of a convex function is convex. The condition that $\tilde{h}$ h be $K$ -nondecreasing implies that $\mathbf{dom}\,h-K=\mathbf{dom}\,h$ . 

Example 3.49 The quadratic matrix function $g:\mathbf{R}^{m\times n}\rightarrow\mathbf{S}^{n}$ defined by 

$$
g(\boldsymbol{X})=\boldsymbol{X}^{T}\boldsymbol{A}\boldsymbol{X}+\boldsymbol{B}^{T}\boldsymbol{X}+\boldsymbol{X}^{T}\boldsymbol{B}+\boldsymbol{C},
$$ 

where $A\in\mathbf{S}^{m}$ , $B\in\mathbf{R}^{m\times n}$ , and $C\in\mathbf{S}^{n}$ , is convex when $A\succeq0$ . 

The function $h:\mathbf{S}^{n}\rightarrow\mathbf{R}$ defined by $h(Y)=-\log\operatorname*{det}(-Y)$ is convex and increasing on $\mathbf{dom}\,h=-\mathbf{S}_{++}^{n}$ . 

By the composition theorem, we conclude that 

$$
\boldsymbol{f}(\boldsymbol{X})=-\log\operatorname*{det}(-(\boldsymbol{X}^{T}\boldsymbol{A}\boldsymbol{X}+\boldsymbol{B}^{T}\boldsymbol{X}+\boldsymbol{X}^{T}\boldsymbol{B}+\boldsymbol{C}))
$$ 

is convex on 

$$
\begin{array}{r}{\mathbf{dom}\ f=\{X\in\mathbf{R}^{m\times n}\mid X^{T}A X+B^{T}X+X^{T}B+C\prec0\}.}\end{array}
$$ 

This generalizes the fact that 

$$
-\log(-(a x^{2}+b x+c))
$$ 

is convex on 

$$
\{x\in\mathbf{R}\mid a x^{2}+b x+c<0\},
$$ 

provided $a\geq0$ . 

# Bibliography 

The standard reference on convex analysis is Rockafellar [ Roc70 ]. Other books on convex functions are Stoer and Witzgall [ SW70 ], Roberts and Varberg [ RV73 ], Van Tiel [ vT84 ], Hiriart-Urruty and Lemar´ echal [ HUL93 ], Ekeland and T´ emam [ ET99 ], Borwein and Lewis [ BL00 ], Florenzano and Le Van [ FL01 ], Barvinok [ Bar02 ], and Bertsekas, Nedi´ c, and Ozdaglar [ Ber03 ]. Most nonlinear programming texts also include chapters on convex functions (see, for example, Mangasarian [ Man94 ], Bazaraa, Sherali, and Shetty [ BSS93 ], Bertsekas [ Ber99 ], Polyak [ Pol87 ], and Peressini, Sullivan, and Uhl [ PSU88 ]). 

Jensen’s inequality appears in [ Jen06 ]. A general study of inequalities, in which Jensen’s inequality plays a central role, is presented by Hardy, Littlewood, and P´ olya [ HLP52 ], and Beckenbach and Bellman [ BB65 ]. 

The term perspective function is from Hiriart-Urruty and Lemar´ echal [ HUL93 , volume 1, page 100]. For the definitions in example 3.19 (relative entropy and Kullback-Leibler divergence), and the related exercise 3.13 , see Cover and Thomas [ CT91 ]. 

Some important early references on quasiconvex functions (as well as other extensions of convexity) are Nikaidˆ o [ Nik54 ], Mangasarian [ Man94 , chapter 9], Arrow and Enthoven [ AE61 ], Ponstein [ Pon67 ], and Luenberger [ Lue68 ]. For a more comprehensive reference list, we refer to Bazaraa, Sherali, and Shetty [ BSS93 , page 126]. 

Pr´ ekopa [ Pr´ e80 ] gives a survey of log-concave functions. Log-convexity of the Laplace transform is mentioned in Barndorﬀ-Nielsen [ BN78 , § 7]. For a proof of the integration result of log-concave functions, see Pr´ ekopa [ Pr´ e71 , Pr´ e73 ]. 

Generalized inequalities are used extensively in the recent literature on cone programming, starting with Nesterov and Nemirovski [ NN94 , page 156]; see also Ben-Tal and Nemirovski [ BTN01 ] and the references at the end of chapter 4 . Convexity with respect to generalized inequalities also appears in the work of Luenberger [ Lue69 , § 8.2] and Isii [ Isi64 ]. Matrix monotonicity and matrix convexity are attributed to L¨ owner [ L¨ ow34 ], and are discussed in detail by Davis [ Dav63 ], Roberts and Varberg [ RV73 , page 216] and Marshall and Olkin [ MO79 , § 16E]. For the result on convexity and concavity of the function $X^{\nu}$ in example 3.48 , see Bondar [ Bon94 , theorem 16.1]. For a simple example that demonstrates that $e^{X}$ is not matrix convex, see Marshall and Olkin [ MO79 , page 474]. 

# Exercises 

# Definition of convexity 

3.1 Suppose $f:\mathbf{R}\rightarrow\mathbf{R}$ is convex, and $\it{a,b}\in\mathrm{{dom}\,f}$ with $a<b$ . 

(a) Show that 

$$
f(x)\leq{\frac{b-x}{b-a}}f(a)+{\frac{x-a}{b-a}}f(b)
$$ 

for all $x\in[a,b]$ . 

(b) Show that 

$$
{\frac{f(x)-f(a)}{x-a}}\leq{\frac{f(b)-f(a)}{b-a}}\leq{\frac{f(b)-f(x)}{b-x}}
$$ 

for all $x\in(a,b)$ . Draw a sketch that illustrates this inequality. (c) Suppose $f$ is diﬀerentiable. Use the result in (b) to show that 

$$
f^{\prime}(a)\leq{\frac{f(b)-f(a)}{b-a}}\leq f^{\prime}(b).
$$ 

Note that these inequalities also follow from ( 3.2 ): 

$$
f(b)\geq f(a)+f^{\prime}(a)(b-a),\qquad f(a)\geq f(b)+f^{\prime}(b)(a-b).
$$ 

(d) Suppose $f$ is twice diﬀerentiable. Use the result in (c) to show that $f^{\prime\prime}(a)\geq0$ and $f^{\prime\prime}(b)\geq0$ . 

3.2 Level sets of convex, concave, quasiconvex, and quasiconcave functions. Some level sets of a function $f$ are shown below. The curve labeled 1 shows $\{x\mid f(x)=1\}$ , etc. 

![](images/bf6c656ef46e114bf9dc70ae080224538c578fbfb56e846d5845d43f4bac9947.jpg) 

Could $f$ be convex (concave, quasiconvex, quasiconcave)? Explain your answer. Repeat for the level curves shown below. 

![](images/0eacb9e7421b5bf66015168912f6fb3b23780780c305ba85d0877b1192f21b0a.jpg) 

3.3 Inverse of an increasing convex function. Suppose $f:\mathbf{R}\rightarrow\mathbf{R}$ is increasing and convex on its domain $(a,b)$ . Let $g$ denote its inverse, i.e. , the function with domain $(f(a),f(b))$ and $g(f(x))=x$ for $a<x<b$ . What can you say about convexity or concavity of $g$ ? 

3.4 [ RV73 , page 15] Show that a continuous function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex if and only if for every line segment, its average value on the segment is less than or equal to the average of its values at the endpoints of the segment: For every $x,\ y\in\mathbf{R}^{n}$ , 

$$
\int_{0}^{1}f(x+\lambda(y-x))\,d\lambda\leq{\frac{f(x)+f(y)}{2}}.
$$ 

3.5 [ RV73 , page 22] Running average of a convex func on. Suppose $f:\mathbf{R}\rightarrow\mathbf{R}$ is convex, with $\mathbf{R}_{+}\subseteq\mathbf{dom}\,f$ . Show that its running average F , defined as 

$$
F(x)={\frac{1}{x}}\int_{0}^{x}f(t)\;d t,\qquad\operatorname{dom}F=\mathbf{R}_{++},
$$ 

1 is convex. Hint. For each $s$ , $f(s x)$ is convex in $x$ , so $\textstyle\int_{0}^{1}f(s x)\ d s$ is convex. 

3.6 Functions and epigraphs. When is the epigraph of a function a halfspace? When is the epigraph of a function a convex cone? When is the epigraph of a function a polyhedron? 

3.7 uppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex with $\mathbf{dom}\,f=\mathbf{R}^{n}$ , and bounded above on $\mathbf{R}^{n}$ . Show that $f$ is constant. 

3.8 Second-order condition for convexity. Prove that a twice diﬀerentiable function $f$ is convex if and only if its domain is convex and $\nabla^{2}f(x)\succeq0$ for all $x\in\mathbf{dom}\ f$ . Hint. First consider the case $f:\mathbf{R}\rightarrow\mathbf{R}$ . You can use the first-order condition for convexity (which was proved on page 70 ). 

3.9 Second-order or convexity on a $F\,\in\,\mathbf{R}^{n\times m}$ , $\scriptstyle{\hat{x}}\ \in\ \mathbf{R}^{n}$ ∈ n . The restriction of f $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ → to the affine set { $\{F z+\hat{x}\ |\ z\in\mathbf{R}^{m}\}$ | ∈ } is defined as the function $\tilde{f}:\mathbf{R}^{m}\rightarrow\mathbf{R}$ → with 

$$
\tilde{f}(z)=f(F z+\hat{x}),\qquad\operatorname{dom}\tilde{f}=\{z\mid F z+\hat{x}\in\operatorname{dom}f\}.
$$ 

Suppose $f$ is twice diﬀerentiable with a convex domain. (a) Show that $\tilde{f}$ is convex if and only if for all $z\in\mathbf{dom}\,\tilde{f}$ 

$$
\boldsymbol{F}^{T}\boldsymbol{\nabla}^{2}f(F\boldsymbol{z}+\hat{\boldsymbol{x}})\boldsymbol{F}\succeq0.
$$ 

(b) Suppose $A\,\in\,\mathbf{R}^{p\times n}$ is a matrix whose nullspace is equal to the range of $F$ , i.e. , $A F=0$ ank $A=n-\mathbf{rank}\,F$ . Show that $\hat{f}$ is convex if for all $z\in\mathbf{dom}\,\tilde{f}$ there exists a λ $\lambda\in\mathbf{R}$ ∈ such that 

$$
\nabla^{2}f(F z+{\hat{x}})+\lambda A^{T}A\succeq0.
$$ 

Hint. Use the following re ult: If $B\,\in\,\mathbf{S}^{n}$ $A\,\in\,\mathbf{R}^{p\times n}$ , then $x^{T}B x\,\geq\,0$ for all $x\in{\mathcal{N}}(A)$ if there exists a λ such that $B+\lambda A^{T}A\succeq0$ ⪰ 0. 

3.10 An extension of Jensen’s inequality. One interpretation of Jensen’s inequality is that randomization or dithering hurts, i.e. , raises the average value of a convex function: For $f$ convex and $v$ a zero mean random variable, we have $\mathbf{E}\,f(x_{0}+v)\geq f(x_{0})$ This leads to the following conjecture. If $f$ is convex, then the larger the variance of v , the larger $\mathbf{E}\,f(x_{0}+v)$ . 

(a) Give a counterexample that shows that this conjecture is false. Find zero mean random variables $v$ and $w$ , with $\mathbf{var}(v)>\mathbf{var}(w)$ , a convex function $f$ , and a point $x_{0}$ , such that $\mathbf{E}\,f(x_{0}+v)<\mathbf{E}\,f(x_{0}+w)$ . 

(b) The conjecture is true when $v$ and $w$ are scaled versions of each other. Show that $\mathbf{E}\,f(x_{0}+t v)$ is monotone increasing in $t\geq0$ , when $f$ is convex and $v$ is zero mean. 

3.11 Monotone mappings. A function $\psi:\mathbf{R}^{n}\rightarrow\mathbf{R}^{n}$ is called monotone if for all $x,\ y\in\mathbf{dom}\,\psi$ , 

$$
\begin{array}{r}{(\psi(x)-\psi(y))^{T}(x-y)\geq0.}\end{array}
$$ 

(Note that ‘monotone’ as defined here is e as the definition given in § 3.6.1 . Both definitions are wid used.) Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ → is a diﬀerentiable convex function. Show that its gradient ∇ $\nabla f$ is monotone. Is the converse true, i.e. , is every monotone mapping the gradient of a convex function? 

3.12 Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex, $g:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is concave, $\mathbf{dom}\,f\,=\,\mathbf{dom}\,g\,=\,\mathbf{R}^{n}$ , and for all $x$ , $g(x)\,\leq\,f(x)$ . Show that there exists an affine function $h$ such that for all $x$ , $g(x)\,\leq\,h(x)\,\leq\,f(x)$ . In other words, if a concave function $g$ is an underestimator of a convex function f , then we can fit an affine function between f and $g$ . 

3.13 Kullback-Leibler divergence and the information inequality. Let $D_{\mathrm{kl}}$ be the Kullback- Leibler divergence, as defined in ( 3. e information ality : $D_{\mathrm{{kl}}}(u,v)\ge0$ for all $u,\ v\in\mathbf{R}_{++}^{n}$ . Also show that $D_{\mathrm{kl}}(u,v)=0$ ) = 0 if and only if u $u=v$ = v . Hint. The Kullback-Leibler divergence can be expressed as 

$$
D_{\mathrm{kl}}(u,v)=f(u)-f(v)-\nabla f(v)^{T}(u-v),
$$ 

where $\begin{array}{r}{f(v)=\sum_{i=1}^{n}v_{i}\log v_{i}}\end{array}$ is the negative entropy of $v$ . 

3.14 Convex-concave functions and saddle-points. We say the function $f\,:\,\mathbf{R}^{n}\times\mathbf{R}^{m}\,\rightarrow\,\mathbf{R}$ is convex-concave if $f(x,z)$ is a concave function of $z$ , for each fixed $x$ , and a convex function of $x$ , for each fixed $z$ . We also require its domain to have the product form $\mathbf{dom}\,f=A\times B$ , where $A\subseteq\mathbf{R}^{n}$ and $B\subseteq\mathbf{R}^{m}$ are convex. 

(a) Give a second-order condition for a twice diﬀ function $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ to be convex-concave, in terms of its Hessian ∇ $\nabla^{2}f(x,z)$ ). 

(b) Suppose that $f:\mathbf{R}^{n}{\times}\mathbf{R}^{m}\rightarrow\mathbf{R}$ is convex-concave and diﬀerentiable, with $\nabla f(\tilde{x},\tilde{z})=$ 0. Show that the saddle-point property holds: for all $x,z$ , we have 

$$
f({\tilde{x}},z)\leq f({\tilde{x}},{\tilde{z}})\leq f(x,{\tilde{z}}).
$$ 

Show that this implies that $f$ satisfies the strong max-min property : 

$$
\operatorname*{sup}_{z}\ \operatorname*{inf}_{x}\ f(x,z)=\operatorname*{inf}_{x}\ \operatorname*{sup}_{z}\ f(x,z)
$$ 

(and their common value is $f(\tilde{x},\tilde{z})$ )). 

(c) Now suppose that $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ is diﬀerentiable, but not necessarily convex- concave, and the saddle-point property holds at $\tilde{x},\tilde{z}$ : 

$$
f(\tilde{x},z)\leq f(\tilde{x},\tilde{z})\leq f(x,\tilde{z})
$$ 

for all $x,z$ . Show that $\nabla f(\tilde{x},\tilde{z})=0

$ 

# Examples 

3.15 A family of concave utility functions. For $0<\alpha\leq1$ let 

$$
u_{\alpha}(x)=\frac{x^{\alpha}-1}{\alpha},
$$ 

with $\mathbf{dom}\,u_{\alpha}=\mathbf{R}_{+}$ . We also define $u_{0}(x)=\log x$ (with d $\mathbf{\delta}_{\mathbf{om}\,u_{0}}=\mathbf{R}_{++}$ ). 

(a) Show that for $x>0$ , $\begin{array}{r}{u_{0}(x)=\operatorname*{lim}_{\alpha\to0}u_{\alpha}(x)}\end{array}$ . (b) Show that $u_{\alpha}$ are concave, monotone increasing, and all satisfy $u_{\alpha}(1)=0$ . 

These functions are often used in economics to model the benefit or utility of some quantity of goods or money. Concavity of $u_{\alpha}$ means that the marginal utility ( i.e. , the increase in utility obtained for a fixed increase in the goods) decreases as the amount of goods increases. In other words, concavity models the eﬀect of satiation . 

3.16 For each of the following functions determine whether it is convex, concave, quasiconvex, or quasiconcave. 

(a) $f(x)=e^{x}-1$ on $\mathbf{R}$ . (b) $f(x_{1},x_{2})=x_{1}x_{2}$ on $\mathbf{R}_{++}^{2}$ . $f(x_{1},x_{2})=1/(x_{1}x_{2})\ \mathrm{on}\ \mathbf{R}_{++}^{2}.$ (d) $f(x_{1},x_{2})=x_{1}/x_{2}$ on $\mathbf{R}_{++}^{2}$ . (e) $f(x_{1},x_{2})=x_{1}^{2}/x_{2}$ on $\mathbf{R}\times\mathbf{R}_{++}$ . (f) $f(x_{1},x_{2})=x_{1}^{\alpha}\,x_{2}^{1-\alpha}$ , where $0\leq\alpha\leq1$ , on $\mathbf{R}_{++}^{2}$ . 

3.17 Suppose $p<1$ , $p\neq0$ . Show that the function 

$$
f(x)=\left(\sum_{i=1}^{n}x_{i}^{p}\right)^{1/p}
$$ 

with $\mathbf{dom}\,f=\mathbf{R}_{++}^{n}$ des as special cases $\begin{array}{r}{f(x)=(\sum_{i=1}^{n}x_{i}^{1/2})^{2}}\end{array}$ and the harmonic mean f $\begin{array}{r}{f(x)=(\sum_{i=1}^{n}1/x_{i})^{-1}}\end{array}$ P . Hint. Adapt the proofs for the log-sum-exp function and the geometric mean in § 3.1.5 . 

3.18 Adapt the proof of concavity of the log-determinant function in § 3.1.5 to show the follow- ing. 

(a) $f(X)=\mathbf{tr}\left(X^{-1}\right)$  is convex on $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ . (b) f $f(X)=(\operatorname*{det}X)^{1/n}$ is concave on $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ . 

3.19 Nonnegative weighted sums and integrals. 

(a) $\begin{array}{r}{f(x)\,=\,\sum_{i=1}^{r}\alpha_{i}x_{[i]}}\end{array}$ is a convex function o $x$ , where $\alpha_{1}\,\geq\,\alpha_{2}\,\geq\,\cdot\cdot\cdot\,\geq$ $\alpha_{r}\geq0$ ≥ 0, and x $x_{[i]}$ denotes the i th largest component of x . (You can use the fact that $\begin{array}{r}{f(x)=\sum_{i=1}^{k}x_{[i]}}\end{array}$ is convex on $\mathbf{R}^{n}$ .) (b) Let $T(x,\omega)$ ) denote the trigonometric polynomial 

$$
T(x,\omega)=x_{1}+x_{2}\cos\omega+x_{3}\cos2\omega+\cdot\cdot\cdot+x_{n}\cos(n-1)\omega.
$$ 

Show that the function 

$$
f(x)=-\int_{0}^{2\pi}\log T(x,\omega)\;d\omega
$$ 

is convex on $\{x\in\mathbf{R}^{n}\mid T(x,\omega)>0,\ 0\leq\omega\leq2\pi\}$ . 

3.20 Composition with an affine function. Show that the following functions $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are convex. 

(a) $f(x)=\|A x-b\|$ , where $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , and $||\cdot||$ is a norm on $\mathbf{R}^{m}$ . (b) $f(x)=-\left(\operatorname*{det}(A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n})\right)^{1/m}$ , on $\{x\mid A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n}\succ0\}$ , where A $A_{i}\in\mathbf{S}^{m}$ ∈ . (c) $f(X)=\operatorname{\mathbf{tr}}\left(A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n}\right)^{-1}$ , on $\{x\mid A_{0}\!+\!x_{1}A_{1}\!+\!\cdot\cdot\!+\!x_{n}A_{n}\succ0\}$ , where $A_{i}\in\mathbf{S}^{m}$ ∈ . (Use the fact that $\mathbf{tr}(X^{-1})$ ) is convex on $\mathbf{S}_{++}^{m}$ ; see exercise 3.18 .) 

3.21 Pointwise maximum and supremum. Show that the following functions $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are convex. 

(a) $\begin{array}{r}{f(\boldsymbol{x})=\operatorname*{max}_{i=1,\dots,k}\|\boldsymbol{A}^{(i)}\boldsymbol{x}-\boldsymbol{b}^{(i)}\|}\end{array}$ , where $A^{(i)}\in\mathbf{R}^{m\times n}$ , $b^{(i)}\in\mathbf{R}^{m}$ and $||\cdot||$ is a norm on R $\mathbf{R}^{m}$ . (b) $\begin{array}{r}{f(x)=\sum_{i=1}^{r}|x|_{[i]}}\end{array}$ | | on $\mathbf{R}^{n}$ , where $|x|$ denotes t ector w th $|x|_{i}=|x_{i}|$ ( i.e. , $|x|$ is the absolute value of x , componentwise), and | $|x|_{[i]}$ is the i th largest component of $|x|$ | | . In other words, $|x|_{[1]},\,|x|_{[2]},\,.\,.\,.\,,\,|x|_{[n]}$ are the absolute values of the components of x , sorted in nonincreasing order. 

3.22 Composition rules . Show that the following functions are convex. 

(a) $\begin{array}{r}{f(x)=-\log(-\log(\sum_{i=1}^{m}e^{a_{i}^{T}x+b_{i}}))}\end{array}$$\begin{array}{r}{\mathbf{dom}\,f=\{x\ |\ \sum_{i=1}^{m}e^{a_{i}^{T}x+b_{i}}<1\}}\end{array}$}. You can$\log(\sum_{i=1}^{n}e^{y_{i}})$ ) is convex. (b) f $\textstyle f(x,u,v)=-{\sqrt{u v-x^{T}x}}$ − √ − on $\operatorname{dom}f=\{(x,u,v)\ |\ u v\,>x^{T}x,\ u,\ v>0\}$ { | } . Use the fact that $x^{T}x/u$ is convex in $(x,u)$ for $u>0$ , and that $-{\sqrt{x_{1}x_{2}}}$ is convex on $\mathbf{R}_{++}^{2}$ . (c) $f(x,u,v)=-\log(u v-x^{T}x)$ on $\mathbf{dom}\,f=\{(x,u,v)\mid u v>x^{T}x,\ u,\ v>0\}.$ . (d) $f(x,t)=-\big(t^{p}-\|x\|_{p}^{p}\big)^{1/p}$ where $p>1$ and $\mathbf{dom}\,f=\{(x,t)\mid t\geq||x||_{p}\}$ . You can use the fact that $\|x\|_{p}^{p}/u^{p-1}$ is convex in $(x,u)$ for $u\,>\,0$ (see exercise 3.23 ), and that $-x^{1/p}y^{1-1/p}$ is convex on $\mathbf{R}_{+}^{2}$ (see exercise 3.16 ). (e) $f(x,t)=-\log(t^{p}-\|x\|_{p}^{p})$ ) where $p>1$ and $\mathbf{dom}\ f=\{(x,t)\ |\ t>\|x\|_{p}\}$ . You can use the fact that $\|x\|_{p}^{p}/u^{p-1}$ is convex in $(x,u)$ for $u>0$ (see exercise 3.23 ). 

3.23 Perspective of $a$ function. 

(a) Show that for $p>1$ , 

$$
f(x,t)={\frac{|x_{1}|^{p}+\cdot\cdot\cdot+|x_{n}|^{p}}{t^{p-1}}}={\frac{||x||_{p}^{p}}{t^{p-1}}}
$$ 

is convex on $\{(x,t)\mid t>0\}$ . 

(b) Show that 

$$
f(x)={\frac{||A x+b||_{2}^{2}}{c^{T}x+d}}
$$ 

is convex on $\{x\mid c^{T}x+d>0\}$ , where $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , $c\in\mathbf{R}^{n}$ and $d\in\mathbf{R}

$ 

3.24 Some functions on the probability simplex. Let $x$ be a real-valued random variable which s in $\{a_{1},.\,.\,.\,,a_{n}\}$ where $a_{1}\ <\ a_{2}\ <\ \cdot\cdot\cdot\ <\ a_{n}$ , with $\mathbf{prob}(x~=~a_{i})~=~p_{i}$ $i\,=\,1,\dots,n$ . For each of the following functions of $p$ (on the probability simplex { $\{p\in$ ∈ ${\bf R}_{+}^{n}\mid\mathbf{1}^{T}p=1\}$ | } ), determine if the function is convex, concave, quasiconvex, or quasicon- cave. 

(a) ${\bf E}\,x$ . (b) $\mathbf{prob}(x\geq\alpha)$ . (c) $\mathbf{prob}(\alpha\leq x\leq\beta)$ . (d) $\textstyle\sum_{i=1}^{n}p_{i}\log p_{i}$ , the negative entropy of the distribution. (e) var $x=\mathbf{E}(x-\mathbf{E}\,x)^{2}$ . (f) $\mathbf{quantile}(x)=\operatorname*{inf}\{\beta\mid\mathbf{prob}(x\leq\beta)\geq0.25\}$ { | ≤ ≥ } . (g) The cardinality of the smallest set ${\mathcal{A}}\subseteq\{a_{1},\ldots,a_{n}\}$ with probability $\ge90\%$ . (By cardinality we mean the number of elements in A .) (h) The minimum width interval that contains $90\%$ of the probability, i.e. , 

$$
\operatorname*{inf}\left\{\beta-\alpha\mid\mathbf{prob}(\alpha\leq x\leq\beta)\geq0.9\right\}.
$$ 

3.25 Maximum probability distance between distribu $p$ $q\in\mathbf{R}^{n}$ represent two proba- bility distributions on $\{1,\cdot\cdot\cdot,n\}$ (so $p$ , $q\succeq0$ , $\mathbf{1}^{T}p=\mathbf{1}^{T}q=1$ = 1). We define the maximum probability distance $d_{\mathrm{mp}}(p,q)$ between $p$ and $q$ as the maximum diﬀerence in probability assigned by $p$ and $q$ , over all events: 

$$
d_{\mathrm{mp}}(p,q)=\operatorname*{max}\{|\,\mathbf{prob}(p,C)-\mathbf{prob}(q,C)|\ |\ C\subseteq\{1,\ldots,n\}\}.
$$ 

Here prob $(p,C)$ is the probability of $C$ , under the distribution , i.e. , $\mathbf{prob}(p,C)\;=\;$ $p$ $\textstyle\sum_{i\in C}p_{i}$ . 

Find a simple expressio $d_{\mathrm{mp}}$ i $\begin{array}{r}{\|p-q\|_{1}=\sum_{i=1}^{n}|p_{i}-q_{i}|}\end{array}$ − t $d_{\mathrm{mp}}$ is a convex function on R ${\bf R}^{n}\times{\bf R}^{n}$ × . (Its domain is { $\{(p,q)\mid p,\ q\succeq0,\ \mathbf{1}^{T}p=\mathbf{1}^{T}q=1\}$ | ⪰ } , but it has a natural extension to all of R ${\bf R}^{n}\times{\bf R}^{n}$ × .) 

3.26 More functi genvalues. Let $\lambda_{1}(X)\geq\lambda_{2}(X)\geq\cdot\cdot\geq\lambda_{n}(X)$ denote the eigenvalues n of a matrix X $X\in\mathbf{S}^{n}$ ∈ . We have ready seen several functions of the eigenvalues that are convex or concave functions of X . 

• aximum eigenvalue $\lambda_{1}(X)$ is convex (example 3.10 ). The minimum eigenvalue $\lambda_{n}(X)$ ) is concave. • The sum of the eigenvalues (or trace), $\mathrm{~\bf~tr}\,X=\lambda_{1}(X)+\cdot\cdot\cdot+\lambda_{n}(X)$ , is linear. • The sum of the inverses of the eigenvalues (or trace of the inverse), $\mathbf{tr}(X^{-1})\ =$ $\textstyle\sum_{i=1}^{n}1/\lambda_{i}(X)$ ), is convex on $\mathbf{S}_{++}^{n}$ (exercise 3.18 ). • Th ric mean of the eigenvalues, $\begin{array}{r}{(\operatorname*{det}X)^{1/n}\;=\;(\prod_{i=1}^{n}\lambda_{i}(X))^{1/n}}\end{array}$ , and the logarithm of the product of the eigenvalues, log det $\begin{array}{r}{\log\operatorname*{det}X=\sum_{i=1}^{n}\log\lambda_{i}(X)}\end{array}$ P ), are concave on $X\in\mathbf{S}_{++}^{n}$ ∈ (exercise 3.18 and page 74 ). 

In this problem we explore some more functions of eigenvalues, by exploiting variational characterizations. (a) Sum of $k$ largest eigenvalues. Show that $\textstyle\sum_{i=1}^{k}\lambda_{i}(X)$ ) is convex on $\mathbf{S}^{n}$ . Hint. [ HJ85 , page 191] Use the variational characterization 

$$
\sum_{i=1}^{k}\lambda_{i}(X)=\operatorname*{sup}\{\mathbf{tr}(V^{T}X V)\mid V\in\mathbf{R}^{n\times k},\;V^{T}V=I\}.
$$ 

(b) Geometr ean of $k$ smallest eigenvalues. that $(\textstyle\prod_{i=n-k+1}^{n}\lambda_{i}(X))^{1/k}$ is con- cave on $\mathbf{S}_{++}^{n}$ . Hint. [ MO79 , page 513] For X $X\succ0$ ≻ 0, we have 

$$
\left(\prod_{i=n-k+1}^{n}\lambda_{i}(X)\right)^{1/k}={\frac{1}{k}}\operatorname*{inf}\{\mathbf{tr}(V^{T}X V)\mid V\in\mathbf{R}^{n\times k},\,\operatorname*{det}V^{T}V=1\}.
$$ 

(c) Log product of $k$ smallest eigenvalu ow that $\textstyle\sum_{i=n-k+1}^{n}\log\lambda_{i}(X)$ ) is concave on S $\mathbf{S}_{++}^{n}$ . Hint. [ MO79 , page 513] For X $X\succ0$ ≻ 0, 

$$
\prod_{i=n-k+1}^{n}\lambda_{i}(X)=\operatorname*{inf}\left\{\prod_{i=1}^{k}(V^{T}X V)_{i i}~\left|~V\in\mathbf{R}^{n\times k},~V^{T}V=I\right.\right\}.
$$ 

3.27 elements of Cholesky factor. Each $X\in\mathbf{S}_{++}^{n}$ has a unique Cholesky factorization $X=L L^{T}$ , where $L$ is lower triangular, with $L_{i i}>0$ . Show that $L_{i i}$ is a concave function of $X$ (with domain $\mathbf{S}_{++}^{n}$ ). 

Hint. $L_{i i}$ can be expressed as $L_{i i}=(w-z^{T}Y^{-1}z)^{1/2}$ , where 

$$
\left[\begin{array}{l l}{Y}&{z}\\ {z^{T}}&{w}\end{array}\right]
$$ 

is the leading $i\times i$ submatrix of $X$ 

# Operations that preserve convexity 

3.28 Expressing a convex function as the pointwise supremum of $a$ family of affine functions. In this problem we extend the result proved on page 83 to the case where $\mathbf{dom}\ f\neq\mathbf{R}^{n}$ . Let $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ be a convex function. Define ${\tilde{f}}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ as the pointwise supremum of all affine functions that are global underestimators of f : 

$$
{\tilde{f}}(x)=\operatorname*{sup}\{g(x)\mid g{\mathrm{~after~}},\,g(z)\leq f(z){\mathrm{~for~all~}}z\}.
$$ 

(a) Show that $f(x)={\tilde{f}}(x)$ ) for $x\in\operatorname{int}\mathbf{dom}\,f$ . 

(b) Show that $f={\tilde{f}}$ if $f$ is closed ( i.e. , $\mathbf{epi}\,f$ is a closed set; see $\S$ A.3.3 ). 

3.29 Representation of piecewise-linear convex functions. A convex functi $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , with dom $\mathbf{\nabla}\cdot f=\mathbf{R}^{n}$ , is called piecewise-linear if there exists a partition of R as 

$$
\mathbf{R}^{n}=X_{1}\cup X_{2}\cup\cdot\cdot\cup X_{L},
$$ 

$\mathbf{int}\,X_{i}\,\neq\,\emptyset$ $\operatorname{int}X_{i}\,\cap\,\operatorname{int}X_{j}\,=\,\emptyset$ $i\neq j$ , and a family of affine functions $a_{1}^{T}x+b_{1}$ , . . . , $a_{L}^{T}x+b_{L}$ such that $f(x)=a_{i}^{T}x+b_{i}$ for $x\in X_{i}$ . Show that such a function has the form $f(x)=\operatorname*{max}\{a_{1}^{T}x+b_{1},.\,.\,.\,,a_{L}^{T}x+b_{L}\}$ } . 

3.30 Convex hull or envelope of a function . The convex hull or convex envelope of a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is defined as 

$$
g(x)=\operatorname*{inf}\{t\mid(x,t)\in\mathbf{conv}\,\mathbf{epi}\,f\}.
$$ 

Geometrically, the epigraph of $g$ is the convex hull of the epigraph of $f$ . Show that $g$ is the largest convex underestimator of $f$ . In other words, show that if $h$ is convex and satisfies $h(x)\leq f(x)$ for all $x$ , then $h(x)\leq g(x)$ for all $x$ . 

3.31 [ Roc70 , page 35] Largest homogeneous underestimator. Let $f$ be a convex function. Define the function $g$ as 

$$
g(x)=\operatorname*{inf}_{\alpha>0}{\frac{f(\alpha x)}{\alpha}}.
$$ 

(a) Show that $g$ is homogeneous ( $g(t x)=t g(x)$ for all $t\geq0$ ). 

(b) Show that $g$ is the largest homogeneous underestimator of $f$ : If $h$ is homogeneous and $h(x)\leq f(x)$ for all $x$ , then we have $h(x)\leq g(x)$ for all $x$ . (c) Show that $g$ is convex. 

3.32 Products and ratios of convex functions. In general the product or ratio of two convex functions is not convex. However, there are some results that apply to functions on $\mathbf{R}$ . Prove the following. 

(a) If $f$ and $g$ are convex, both nondecreasing (or nonincreasing), and positive functions on an interval, then $f g$ is convex. (b) If $f$ , $g$ are concave, positive, with one nondecreasing and the other nonincreasing, then $f g$ is concave. (c) If $f$ is convex, nondecreasing, and positive, and $g$ is concave, nonincreasing, and positive, then $f/g$ is convex. 

3.33 Direct proof of perspective theorem. Give a direct proof that the perspective function $g$ , as defined in unction $f$ is convex: Show that $\mathbf{dom}\,g$ is a convex set, and that for ( $(x,t),\ (y,s)\in\mathbf{dom}\,g$ ∈ , and $0\leq\theta\leq1$ , we have 

$$
g(\theta x+(1-\theta)y,\theta t+(1-\theta)s)\leq\theta g(x,t)+(1-\theta)g(y,s).
$$ 

3.34 The Minkowski function. The Minkowski function of a convex set $C$ is defined as 

$$
M_{C}(x)=\operatorname*{inf}\{t>0\mid t^{-1}x\in C\}.
$$ 

(a) Draw a picture giving a geometric interpretation of how to find $M_{C}(x)$ . (b) Show that $M_{C}$ is homogeneous, i.e. , $M_{C}(\alpha x)=\alpha M_{C}(x)$ for $\alpha\geq0$ . (c) What is $\mathbf{dom}\,M_{C}$ ? (d) Show that $M_{C}$ is a convex function. (e) Suppose $C$ is also closed, bou d, symmetric (if $x\,\in\,C$ then $-x\,\in\,C$ ), and has nonempty interior. Show that M is a norm. What is the corresponding unit ball? 

3.35 Recall that the support functi of a set $C\subseteq\mathbf{R}^{n}$ is defined as $S_{C}(y)=\operatorname*{sup}\{y^{T}x\mid x\in C\}$ { | ∈ } . On page 81 we showed that S $S_{C}$ is a convex function. 

(a) Show that $S_{B}=S_{\mathbf{conv}\,B}$ . (b) Show that $S_{A+B}=S_{A}+S_{B}$ . (c) Show that $S_{A\cup B}=\operatorname*{max}\{S_{A},S_{B}\}$ . (d) Let $B$ be closed and convex. Show that $A\subseteq B$ if and only if $S_{A}(y)\leq S_{B}(y)$ for all $y$ . 

# Conjugate functions 

3.36 Derive the conjugates of the following functions. 

(a) Max function. $f(x)=\textstyle\operatorname*{max}_{i=1,\ldots,n}x_{i}$ on $\mathbf{R}^{n}$ . (b) Sum of largest el ments. $\begin{array}{r}{f(x)=\sum_{i=1}^{r}x_{[i]}}\end{array}$ on $\mathbf{R}^{n}$ . (c) Piecewise-linear function on R . $f(x)\;=\;\mathrm{max}_{i=1,...,m}(a_{i}x\,+\,b_{i})$ on $\mathbf{R}$ . You can assume that the $a_{i}$ rted in increasing order, i.e. , $a_{1}\leq\cdot\cdot\cdot\leq a_{m}$ , and th none of the functions a $a_{i}x+b_{i}$ i is redundant, i.e. , for each k there is at least one x with $f(x)=a_{k}x+b_{k}$ . (d) Power function. $f(x)=x^{p}$ on $\mathbf{R}_{++}$ , where $p>1$ . Repeat for $p<0$ . (e) Negative geometric mean. $f(x)=-(\prod x_{i})^{1/n}$ on $\mathbf{R}_{++}^{n}$ . (f) Negative generalized logarithm for second-order cone. $f(x,t)=-\log(t^{2}-x^{T}x)$ on $\{(x,t)\in\mathbf{R}^{n}\times\mathbf{R}\mid\|x\|_{2}<t\}$ . 

3.37 Show that the conjugate of $f(X)=\mathbf{tr}(X^{-1})$ with $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ is given by 

$$
f^{*}(Y)=-2\,\mathbf{tr}(-Y)^{1/2},\qquad\mathbf{dom}\,f^{*}=-\mathbf{S}_{+}^{n}.
$$ 

Hint. The gradient of $f$ is $\nabla f(X)=-X^{-2}

$ 

3.38 Young’s inequality Let $f:\mathbf{R}\rightarrow\mathbf{R}$ be an increasing function, with $f(0)=0$ , and let $g$ be its inverse. Define F and G as 

$$
F(x)=\int_{0}^{x}f(a)\,d a,\qquad G(y)=\int_{0}^{y}g(a)\,d a.
$$ 

Show that $F^{\prime}$ and $G$ are conjugates. Give a simple graphical interpretation of Young’s inequality, 

$$
x y\leq F(x)+G(y).
$$ 

3.39 Properties of conjugate functions. 

(a) Conjugate of convex plus affine function. Define $g(x)=f(x)+c^{T}x+d$ , where $f$ is convex. Express $g^{*}$ in terms of $f^{*}$ (and $c,\ d$ ). (b) Conjugate of perspective. Express the conjugate of the perspective of a convex function $f$ in terms of $f^{*}$ . 

(c) Conjugate and minimization. Let $f(x,z)$ be convex in $(x,z)$ and define $g(x)\,=$ $\textstyle\operatorname*{inf}_{z}f(x,z)$ . Express the conjugate $g^{*}$ in terms of $f^{*}$ . As an application, exp s he co jugate of $\begin{array}{r}{g(x)=\operatorname*{inf}_{z}\{h(z)\mid A z+b=x\}}\end{array}$ , where $h$ is convex, in terms of h $h^{*}$ , A , and b . (d) Conjugate of conjugate. Show that the conjugate of the conjugate of a closed convex function is itself: $f\,=\,f^{**}$ if $f$ is closed and convex. (A function is closed if its epigraph is closed; see $\S$ A.3.3 .) Hin . Show that $f^{**}$ is the pointwise supremum of all affine global underestimators of f . Then apply the result of exercise 3.28 . 

3.40 Gradient and Hessian of conjugate functi n. Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex continuously diﬀerentiable. Suppose $y$ and ¯ are related by ${\bar{y}}=\nabla f({\bar{x}})$ ∇ ), and that ∇ $\nabla^{2}f(\bar{x})\succ$ ≻ 0. 

(a) Show that $\nabla f^{*}\left(\Bar{y}\right)=\Bar{x}$ . (b) Show that $\nabla^{2}f^{*}(\bar{y})=\nabla^{2}f(\bar{x})^{-1}

$ 

3.41 Conjugate of negative normalized entropy. Show that the conjugate of the negative nor- malized entropy 

$$
f(x)=\sum_{i=1}^{n}x_{i}\log(x_{i}/1^{T}x),
$$ 

with $\mathbf{\Gamma}\mathbf{dom}\mathbf{\:}f=\mathbf{R}_{++}^{n}$ , is given by 

$$
f^{*}(y)={\left\{\begin{array}{l l}{0}&{\sum_{i=1}^{n}e^{y_{i}}\leq1}\\ {+\infty}&{{\mathrm{otherwise}}.}\end{array}\right.}
$$ 

# Quasiconvex functions 

3.42 Approximation width. Let $f_{0},.\,.\,.\,,\,f_{n}:\mathbf{R}\rightarrow\mathbf{R}$ be given contin ions. ider the problem of approximating f $f_{0}$ as a linear combination of f $f_{1},\ldots,f_{n}$ . For x $x\in\mathbf{R}^{n}$ ∈ , we say that $f\,=\,x_{1}f_{1}\,+\,\cdot\cdot\cdot\,+\,x_{n}f_{n}$ ximates $f_{0}$ with tolerance ǫ > 0 the interval $[0,T]$ if $|f(t)-f_{0}(t)|\leq\epsilon$ for 0 $0\leq t\leq T$ ≤ ≤ . ow we choo e a fixed tolera e ǫ > 0 and define the approximation width as the largest T such that f approximates f $f_{0}$ over the interval $[0,T]$ : 

$$
W(x)=\operatorname*{sup}\{T\mid|x_{1}f_{1}(t)+\cdot\cdot\cdot+x_{n}f_{n}(t)-f_{0}(t)|\leq\epsilon{\mathrm{~for~}}0\leq t\leq T\}.
$$ 

Show that $W$ is quasiconcave. 

3.43 First-order condition for quasiconvexity. Prove the first-order condition for quasiconvexity given in § 3.4.3 : A diﬀerentiable function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , with $\mathbf{dom}\ f$ convex, is quasiconvex if and only if for all $x,y\in\mathbf{dom}\ f$ , 

$$
f(y)\leq f(x)\Longrightarrow\nabla f(x)^{T}(y-x)\leq0.
$$ 

Hint. It suffices to prove the result for a function on $\mathbf{R}$ ; the general result follows by restriction to an arbitrary line. 

3.44 Second-order conditions for quasiconvexity. In this problem we derive alternate repre- sentations of the second-order conditions for quasiconvexity given in § 3.4.3 . Prove the following. 

(a) A point $x\in\mathbf{dom}\ f$ satisfies ( 3.21 ) if there exists a $O$ such that 

$$
\nabla^{2}f(x)+\sigma\nabla f(x)\nabla f(x)^{T}\succeq0.
$$ 

It satisfies ( 3.22 ) for all $y\ne0$ if and only if there exists a $\sigma$ such 

$$
\nabla^{2}f(x)+\sigma\nabla f(x)\nabla f(x)^{T}\succ0.
$$ 

Hint. We can assume without loss of generality that $\nabla^{2}f(x)$ is diagonal. 

(b) A point $x\in\mathbf{dom}\ f$ satisfies ( 3.21 ) if and only if either $\nabla f(x)=0$ and $\nabla^{2}f(x)\succeq0$ , or $\nabla f(x)\neq0$ and the matrix 

$$
H(x)=\left[\begin{array}{c c}{\nabla^{2}f(x)}&{\nabla f(x)}\\ {\nabla f(x)^{T}}&{0}\end{array}\right]
$$ 

has exactly one negative eigenvalue. It satisfies ( 3.22 ) for all $y\ne0$ if and only if $H(x)$ has exactly one nonpositive eigenvalue. 

Hint. You can use the result of part (a). The following result, which follows from the lue interlacing theorem in linear algebra, may also be useful: If $B\in\mathbf{S}^{n}$ and a $a\in\mathbf{R}^{n}$ , then 

$$
\lambda_{n}\left({\left[\begin{array}{l l}{B}&{a}\\ {a^{T}}&{0}\end{array}\right]}\right)\geq\lambda_{n}(B).
$$ 

3.45 Use the first and second-order conditions for qua ven in $93.4.3$ to verify quasiconvexity of the function $f(x)=-x_{1}x_{2}$ , with $\mathbf{dom}\ f=\mathbf{R}_{++}^{2}$ . 

3.46 Quasilinear functions with domain $\mathbf{R}^{n}$ . A function on $\mathbf{R}$ that is quasilinear ( i.e. , qua- siconvex and quasiconcave) is monotone, i.e. , either nondecreasing or nonincreasing. In this problem we consider a generalization of this result to functions on $\mathbf{R}^{n}$ . 

Suppose the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ near and continuous with $\mathbf{dom}\,f=\mathbf{R}^{n}$ that it can be expressed as $f(x)=g(a^{T}x)$ ), where $g:\mathbf{R}\rightarrow\mathbf{R}$ is monotone and a $a\in\mathbf{R}^{n}$ ∈ . In other words, a quasilinear function with domain R must be a monotone function of a linear function. (The converse is also true.) 

# Log-concave and log-convex functions 

3.47 Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is diﬀerentiable, $\mathbf{dom}\,f$ is $f(x)>0$ for all $x\in\mathbf{dom}\ f$ . Show that f is log-concave if and only if for all $x,y\in\mathbf{dom}\ f$ , 

$$
{\frac{f(y)}{f(x)}}\leq\exp\left({\frac{\nabla f(x)^{T}(y-x)}{f(x)}}\right).
$$ 

3.48 Show that if $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ is log-concave and $a\,\geq\,0$ , then the function $g\,=\,f\,-\,a$ is log-concave, where $\mathbf{dom}\,g=\{x\in\mathbf{dom}\,f\ |\ f(x)>a\}$ . 

3.49 Show that the following functions are log-concave. 

(b) Harmonic mean: 

$$
f(x)={\frac{1}{1/x_{1}+\cdot\cdot\cdot+1/x_{n}}},\qquad\operatorname{dom}f=\mathbf{R}_{++}^{n}.
$$ 

(c) Product over sum: 

$$
f(x)={\frac{\prod_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}}},\qquad\mathbf{dom}\,f=\mathbf{R}_{++}^{n}.
$$ 

(d) Determinant over trace: 

$$
f(X)={\frac{\operatorname*{det}X}{\operatorname{tr}X}},\qquad\operatorname{dom}f=\mathbf{S}_{++}^{n}.
$$ 

3.50 Coefficients of a polynomial as a function of the roots. Show that the coefficients of a polynomial with real negative roots are log-concave functions of the roots. In other words, the functions $a_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , defined by the identity 

$$
s^{n}+a_{1}(\lambda)s^{n-1}+\cdot\cdot\cdot+a_{n-1}(\lambda)s+a_{n}(\lambda)=(s-\lambda_{1})(s-\lambda_{2})\cdot\cdot\cdot(s-\lambda_{n}),
$$ 

are log-concave on $-\mathbf{R}_{++}^{n}$ . 

Hint. The function 

$$
S_{k}(x)=\sum_{1\leq i_{1}<i_{2}<\cdots<i_{k}\leq n}x_{i_{1}}x_{i_{2}}\cdot\cdot\cdot x_{i_{k}},
$$ 

with $\mathbf{dom}\,S_{k}\,\in\mathbf{R}_{+}^{n}$ and $1\leq k\leq n$ , is called the $k$ th elementary symmetric function on $\mathbf{R}^{n}$ . It can be shown that $\boldsymbol{S}_{k}^{1/k}$ is concave (see [ ML57 ]). 

3.51 [ BL00 , page 41] Let be a polynomial on $\mathbf{R}$ , with all its roots real. Show that it is $p$ log-concave on any interval on which it is positive. 

3.52 [ MO g-conv of moment functions. Suppose $f:\mathbf{R}\rightarrow\mathbf{R}$ is nonnegative with $\mathbf{R}_{+}\subseteq\mathbf{dom}\,f$ . For x $x\geq0$ ≥ 0 define 

$$
\phi(x)=\int_{0}^{\infty}u^{x}f(u)~d u.
$$ 

Show that $\phi$ is a log-convex function. (If $x$ is a positive integer, and $f$ is a probability density function, then $\phi(x)$ is the $x$ th moment of the distribution.) Use this to show that the Gamma function, 

$$
\Gamma(x)=\int_{0}^{\infty}u^{x-1}e^{-u}\;d u,
$$ 

is log-convex for $x\geq1$ . 

3.53 Suppose and are independent random vectors in $\mathbf{R}^{n}$ , with log-concave probability $x$ $y$ density functions $f$ and $g$ , respectively. Show that the probability density function of the sum $z=x+y$ is log-concave. 

3.54 Log-concavity of Gaussian cumulative distribution function. The cumulative distribution function of a Gaussian random variable, 

$$
f(x)={\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{x}e^{-t^{2}/2}\;d t,
$$ 

is log-concave. This follows from the general result that the convolution of two log-concave functions is log-concave. In this problem we guide you through a simple self-contained proof t at $f$ is log-concave. Recall that $f$ is log-concave if and only if $f^{\prime\prime}(x)f(x)\leq f^{\prime}(x)^{2}$ for all x . 

(a) Verify that $f^{\prime\prime}(x)f(x)\leq f^{\prime}(x)^{2}$ for $x\geq0$ . That leaves us the hard part, which is to show the inequality for x < 0. 

(b) Verify that for any $t$ and $x$ we have $t^{2}/2\geq-x^{2}/2+x t$ . (c) Using part (b) show that $e^{-t^{2}/2}\leq e^{x^{2}/2-x t}$ . Conclude that, for $x<0$ , 

$$
\int_{-\infty}^{x}e^{-t^{2}/2}\;d t\leq e^{x^{2}/2}\int_{-\infty}^{x}e^{-x t}\;d t.
$$ 

(d) Use part (c) to verify that $f^{\prime\prime}(x)f(x)\leq f^{\prime}(x)^{2}$ for $x\leq0$ . 

3.55 Log-concavity of the cumulative distribution function of a log-concave probability density. In this problem we extend the result of exercise 3.54 . Let $g(t)=\exp(-h(t))$ be a diﬀer- entiable log-concave probability density function, and let 

$$
f(x)=\int_{-\infty}^{x}g(t)\,d t=\int_{-\infty}^{x}e^{-h(t)}\,d t
$$ 

be its cumulative distribution. We will show that $f$ is log-concave, i.e. , it satisfies $f^{\prime\prime}(x)f(x)\leq(f^{\prime}(x))^{2}$ for all $x$ . 

(a) th ives of $f$ in terms of the function $h$ . Verify that $f^{\prime\prime}(x)f(x)\leq$ $(f^{\prime}(x))^{2}$ if $h^{\prime}(x)\geq0$ ≥ 0. 

(b) Assume that $h^{\prime}(x)<0$ . Use the inequality 

$$
h(t)\geq h(x)+h^{\prime}(x)(t-x)
$$ 

(which follows from convexity of $h$ ), to show that 

$$
\int_{-\infty}^{x}e^{-h(t)}\,d t\leq{\frac{e^{-h(x)}}{-h^{\prime}(x)}}.
$$ 

Use this inequality to verify that $f^{\prime\prime}(x)f(x)\leq(f^{\prime}(x))^{2}$ if $h^{\prime}(x)<0$ .

 3.56 More log-concave densities. Show that the following densities are log-concave. (a) [ MO79 , page 493] The gamma density , defined by 

$$
f(x)={\frac{\alpha^{\lambda}}{\Gamma(\lambda)}}x^{\lambda-1}e^{-\alpha x},
$$ 

with $\mathbf{dom}\ f=\mathbf{R}_{+}$ . The parameters $\lambda$ and $\alpha$ satisfy $\lambda\geq1$ , $\alpha>0$ . (b) [ MO79 , page 306] The Dirichlet density 

$$
f(x)={\frac{\Gamma(\mathbf{1}^{T}\lambda)}{\Gamma(\lambda_{1})\cdot\cdot\cdot\Gamma(\lambda_{n+1})}}x_{1}^{\lambda_{1}-1}\cdot\cdot\cdot x_{n}^{\lambda_{n}-1}\left(1-\sum_{i=1}^{n}x_{i}\right)^{\lambda_{n+1}-1}
$$ 

with $\mathbf{dom}\ f=\{x\in\mathbf{R}_{++}^{n}\ |\ \mathbf{1}^{T}x<1\}$ | } . The parameter $\lambda$ satisfies $\lambda\succeq{\bf1}$ . 

# Convexity with respect to a generalized inequality 

3.57 Show that the function $f(X)=X^{-1}$ is matrix convex on $\mathbf{S}_{++}^{n}

$ 3.58 Schur complement. Suppose $X\in\mathbf{S}^{\pi}$ partitioned as 

$$
\boldsymbol{X}=\left[\begin{array}{c c}{\boldsymbol{A}}&{\boldsymbol{B}}\\ {\boldsymbol{B}^{T}}&{\boldsymbol{C}}\end{array}\right],
$$ 

where $A\,\in\,\mathbf{S}^{k}$ . The Schur complement of $X$ (with respect to $A$ ) is $S\,=\,C\,-\,B^{T}A^{-1}B$ (see § A.5.5 ). Show that the Schur complement, viewed as a function from S $\mathbf{S}^{n}$ into S $\mathbf{S}^{n-k}$ , is matrix concave on $\mathbf{S}_{++}^{n}$ . 

3.59 Second-order conditions for $K$ -c xity. Let $K\,\subseteq\,\mathbf{R}^{m}$ be a proper convex cone, with ciated generalized inequa y ⪯ $\preceq_{K}$ . Show that a twice diﬀerentiable function $f:\mathbf{R}^{n}\rightarrow$ $\mathbf{R}^{m}$ , with convex domain, is K -convex if and only if for all $x\in\mathbf{dom}\ f$ and all $y\in\mathbf{R}^{n}$ , 

$$
\sum_{i,j=1}^{n}{\frac{\partial^{2}f(x)}{\partial x_{i}\partial x_{j}}}y_{i}y_{j}\succeq_{K}0,
$$ 

i.e. , the second d $K$ -n biline r form. (Here $\partial^{2}f/\partial x_{i}\partial x_{j}\,\in\,\bf{R}^{m}$ , with components $\partial^{2}f_{k}/\partial x_{i}\partial x_{j}$ , for $k=1,\ldots,m$ ; see § A.4.1 .) 

3.60 Sublevel sets and epigraph of $K$ -convex ctions. Let $K\subseteq\mathbf{R}^{m}$ be a prope cone with associated generalized inequal $\preceq_{K}$ ⪯ , and let $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}^{m}$ . For α $\alpha\,\in\,\mathbf{R}^{m}$ ∈ , the $\alpha$ -sublevel set of $f$ (with respect to ⪯ $\preceq_{K}$ ) is defined as 

$$
C_{\alpha}=\{x\in\mathbf{R}^{n}\mid f(x)\preceq_{K}\alpha\}.
$$ 

The epigraph of $f$ , with respect to $\preceq_{K}$ , is defined as the set 

$$
\mathbf{epi}_{K}f=\{(x,t)\in\mathbf{R}^{n+m}\mid f(x)\preceq_{K}t\}.
$$ 

Show the following: 

(a) If $f$ is $K$ -convex, then its sublevel sets $C_{\alpha}$ are convex for all $\alpha$ . (b) $f$ is $K$ -convex if and only if ${\bf e p i}_{K}\,f$ is a convex set. 

# Chapter 4 

# Convex optimization problems 

# 4.1 Optimization problems 

# 4.1.1 Basic terminology 

We use the notation 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p}\end{array}}
$$ 

to describe the problem of finding an $x$ that minimizes $f_{0}(x)$ among all $x$ that satisfy the conditions $f_{i}(x)\leq0$ , $i=1,\ldots,m$ , and $h_{i}(x)=0$ , $i=1,\cdot\cdot\cdot,p$ . We call $x\in\mathbf{R}^{n}$ the optimization variable and the function $f_{0}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ the objective function or cost function . The inequalities $f_{i}(x)\leq0$ are called inequality constraints , and the corresponding functions $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are called the inequality constraint functions . The equations $h_{i}(x)\;=\;0$ are called the equality constraints , and the functions $h_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are the equality constraint functions . If there are no constraints ( i.e. , $m=p=0$ ) we say the problem ( 4.1 ) is unconstrained . 

The set of points for which the objective and all constraint functions are defined, 

$$
\mathcal{D}=\bigcap_{i=0}^{m}\mathbf{dom}\,f_{i}\,\cap\,\bigcap_{i=1}^{p}\mathbf{dom}\,h_{i},
$$ 

is called the domain of the optimization problem ( 4.1 ). A point $x\in\mathcal{D}$ is feasible if it satisfies the constraints $f_{i}(x)\,\leq\,0$ , $i=1,\cdot\cdot\cdot,m$ , and $h_{i}(x)=0,\;i=1,.\,.\,,p$ . The problem ( 4.1 ) is said to be feasible if there exists at least one feasible point, and infeasible otherwise. The set of all feasible points is called the feasible set or the constraint set . 

The optimal value $p^{\star}$ of the problem ( 4.1 ) is defined as 

$$
p^{\star}=\operatorname*{inf}\left\{f_{0}(x)\ |\ f_{i}(x)\leq0,\ i=1,.\,.\,,m,\ h_{i}(x)=0,\ i=1,.\,.\,,p\right\}.
$$ 

We allow $p^{\star}$ to take on the extended values $\pm\infty$ . If the problem is infeasible, we have $p^{\star}=\infty$ (following the standard convention that the infimum of the empty set is ). If there are feasible points with $f_{0}(x_{k})\to-\infty$ as $k\rightarrow\infty$ , then $p^{\star}=-\infty$ , $\infty$ $x_{k}$ and we say the problem ( 4.1 ) is unbounded below . 

# Optimal and locally optimal points 

We say $x^{\star}$ is an optimal point , or solves the problem ( 4.1 ), if $x^{\star}$ is feasible and $f_{0}(x^{\star})=p^{\star}$ . The set of all optimal points is the optimal set , denoted 

$$
X_{\mathrm{opt}}=\{x\mid f_{i}(x)\leq0,\ i=1,\ldots,m,\ h_{i}(x)=0,\ i=1,\ldots,p,\ f_{0}(x)=p^{\star}\}.
$$ 

If there exists an optimal point for the problem ( 4.1 ), we say the optimal value is attained or achieved , and the problem is solvable . If $X_{\mathrm{opt}}$ is empty, we say the optimal value is not attained or not achieved. (This always occurs when the problem is unbounded below.) A feasible point $x$ with $f_{0}(x)\;\leq\;p^{\star}\,+\,\epsilon$ (where

 $\epsilon\,>\,0$ ) is called $\epsilon$ -suboptimal , and the set of all $\epsilon$ -suboptimal points is called the

 $\epsilon$ -suboptimal set for the problem ( 4.1 ). 

We say a feasible point $x$ is locally optimal if there is an $R>0$ such that 

$$
\begin{array}{l}{{f_{0}(x)=\operatorname*{inf}\{f_{0}(z)~|~f_{i}(z)\le0,~i=1,.\,.\,,m,}}\\ {{\qquad h_{i}(z)=0,~i=1,.\,.\,.\,,p,~\|z-x\|_{2}\le R\},}}\end{array}
$$ 

or, in other words, $x$ solves the optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{f_{0}(z)}\\ {{\mathrm{subject~to}}}&{f_{i}(z)\leq0,\quad i=1,.\,.\,,m}\\ &{h_{i}(z)=0,\quad i=1,.\,.\,,p}\\ &{\|z-x\|_{2}\leq R}\end{array}}
$$ 

with variable $z$ . Roughly speaking, this means $x$ minimizes $f_{0}$ over nearby points in the feasible set. The term ‘globally optimal’ is sometimes used for ‘optimal’ to distinguish between ‘locally optimal’ and ‘optimal’. Throughout this book, however, optimal will mean globally optimal. 

If $x$ is feasible and $f_{i}(x)=0$ , we say the $i$ th inequality constraint $f_{i}(x)\leq0$ is active at $x$ . If $f_{i}(x)<0$ , we say the constraint $f_{i}(x)\leq0$ is inactive . (The equality constraints are active at all feasible points.) We say that a constraint is redundant if deleting it does not change the feasible set. 

Example 4.1 We illustrate these definitions with a few simple unconstrained opti- mization problems with variable $x\in\mathbf{R}$ , and $\mathbf{dom}\,f_{0}=\mathbf{R}_{++}$ . 

• $f_{0}(x)=1/x$ : $p^{\star}=0$ , but the optimal value is not achieved. • $f_{0}(x)=-\log x$ : $p^{\star}=-\infty$ , so this problem is unbounded below. • $f_{0}(x)=x\log x$ : $p^{\star}=-1/e$ , achieved at the (unique) optimal point $x^{\star}=1/e$ 

# Feasibility problems 

If the objective function is identically zero, the optimal value is either zero (if the feasible set is nonempty) or $\infty$ (if the feasible set is empty). We call this the feasibility problem , and will sometimes write it as 

$$
{\begin{array}{l r l}{{\mathrm{find}}}&{x}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=0,\quad i=1,.\,.\,,p.}\end{array}}
$$ 

The feasibility problem is thus to determine whether the constraints are consistent, and if so, find a point that satisfies them. 

# 4.1.2 Expressing problems in standard form 

We refer to ( 4.1 ) as an optimization problem in standard form . In the standard form problem we adopt the convention that the righthand side of the inequality and equality constraints are zero. This can always be arranged by subtracting any nonzero righthand side: we represent the equality constraint $g_{i}(x)\;=\;\tilde{g}_{i}(x)$ ), for example, as $h_{i}(x)=0$ , where $h_{i}(x)\,=\,g_{i}(x)\,-\,\tilde{g}_{i}(x)$ ). In a similar way we express inequalities of the form $f_{i}(x)\geq0$ as $-f_{i}(x)\leq0$ . 

Example 4.2 Box constraints. Consider the optimization problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(\boldsymbol{x})}}&{{}}\\ {{\mathrm{subject~to}}}&{{l_{i}\leq x_{i}\leq u_{i},\quad i=1,\ldots,n,}}\end{array}
$$ 

where $x\in\mathbf{R}^{n}$ is the variable. The constraints are called variable bounds (since they give lower and upper bounds for each $x_{i}$ ) or box constraints (since the feasible set is a box). 

We can express this problem in standard form as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad l_{i}-x_{i}\leq0,\quad i=1,\ldots,n}\\ &{x_{i}-u_{i}\leq0,\quad i=1,\ldots,n.}\end{array}}
$$ 

There are $2n$ inequality constraint functions: 

$$
f_{i}(x)=l_{i}-x_{i},\quad i=1,.\,.\,.\,,n,
$$ 

and 

$$
f_{i}(x)=x_{i-n}-u_{i-n},\quad i=n+1,.\,.\,.\,,2n.
$$ 

# Maximization problems 

We concentrate on the minimization problem by convention. We can solve the maximization problem 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=0,\quad i=1,.\,.\,,p}\end{array}}
$$ 

by minimizing the function $-f_{0}$ subject to the constraints. By this correspondence we can define all the terms above for the maximization problem ( 4.2 ). For example the optimal value of ( 4.2 ) is defined as 

$$
^{\star}=\operatorname*{sup}\{f_{0}(x)\ |\ f_{i}(x)\leq0,\ i=1,.\,.\,.\,,m,\ h_{i}(x)
$$ 

and a feasible point $x$ is $\epsilon$ -suboptimal if $f_{0}(x)\,\geq\,p^{\star}\,-\,\epsilon$ . When the maximization problem is considered, the objective is sometimes called the utility or satisfaction level instead of the cost. 

# 4.1.3 Equivalent problems 

In this book we will use the notion of equivalence of optimization problems in an informal way. We call two problems equivalent if from a solution of one, a solution of the other is readily found, and vice versa. (It is possible, but complicated, to give a formal definition of equivalence.) 

As a simple example, consider the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\tilde{f}(x)=\alpha_{0}f_{0}(x)}}\\ {{\mathrm{subject~to}}}&{{\tilde{f}_{i}(x)=\alpha_{i}f_{i}(x)\le0,}}&{{i=1,.\.\,.\,,m}}\\ &{{\tilde{h}_{i}(x)=\beta_{i}h_{i}(x)=0,}}&{{i=1,.\.\,.\,,p,}}\end{array}
$$ 

where $\alpha_{i}>0$ , $i=0,\ldots,m$ , and $\beta_{i}\neq0$ , $i=1,\cdot\cdot\cdot,p$ . This problem is obtained from the standard form problem ( 4.1 ) by scaling the objective and inequality constraint functions by positive constants, and scaling the equality constraint functions by nonzero constants. As a result, the feasible sets of the problem ( 4.3 ) and the original problem ( 4.1 ) are identical. A point $x$ is optimal for the original problem ( 4.1 ) if and only if it is optimal for the scaled problem ( 4.3 ), so we say the two problems are equivalent. The two problems ( 4.1 ) and ( 4.3 ) are not, however, the same (unless $\alpha_{i}$ and $\beta_{i}$ are all equal to one), since the objective and constraint functions diﬀer. 

We now describe some general transformations that yield equivalent problems. 

# Change of variables 

Suppose $\phi:\mathbf{R}^{n}\rightarrow\mathbf{R}^{n}$ is one-to-one, with image overing the problem domain $\mathcal{D}$ , i.e. , $\phi(\mathbf{dom}\,\phi)\supseteq\mathcal{D}$ . We define functions $\tilde{f}_{i}$ and $\tilde{h}_{i}$ as 

$$
\tilde{f}_{i}(z)=f_{i}(\phi(z)),\quad i=0,.\,.\,.\,,m,\qquad\tilde{h}_{i}(z)=h_{i}(\phi(z)),\quad i=1,.\,.\,,p.
$$ 

Now consider the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\tilde{f}_{0}(z)}}\\ {{\mathrm{subject~to}}}&{{\tilde{f}_{i}(z)\leq0,\quad i=1,.\,.\,,m}}\\ &{{\tilde{h}_{i}(z)=0,\quad i=1,.\,.\,,p,}}\end{array}
$$ 

with variable $z$ . We say that the standard form problem ( 4.1 ) and the problem ( 4.4 ) are related by the change of variable or substitution of variable $x=\phi(z)$ . 

The two problems are clearly equivalent: if $x$ solves the problem ( 4.1 ), then $z=\phi^{-1}(x)$ solves the problem ( 4.4 ); if $z$ solves the problem ( 4.4 ), then $x=\phi(z)$ solves the problem ( 4.1 ). 

# Transformation of objective and constraint functions 

Suppose that $\psi_{0}:\mathbf{R}\rightarrow\mathbf{R}$ is monotone increasing, $\psi_{1},.\cdot\cdot,\psi_{m}\,:\,\mathbf{R}\,\rightarrow\,\mathbf{R}$ satisfy $\psi_{i}(u)\leq0$ if and only if $u\leq0$ , and $\psi_{m+1},.\,.\,.\,,\psi_{m+p}:\mathbf{R}\to\mathbf{R}$ satisfy $\psi_{i}(u)=0$ if and only if $u=0$ . We define functions $\tilde{f}_{i}$ and $\tilde{h}_{i}$ as the compositions 

$$
\tilde{f}_{i}(x)=\psi_{i}(f_{i}(x)),\quad i=0,\ldots,m,\qquad\tilde{h}_{i}(x)=\psi_{m+i}(h_{i}(x)),\quad i=1,\ldots,p.
$$ 

Evidently the associated problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\tilde{f}_{0}(x)}}&{{}}\\ {{\mathrm{subject~to}}}&{{\tilde{f}_{i}(x)\leq0,\quad i=1,.\,.\,,m}}&{{}}\\ {{}}&{{\tilde{h}_{i}(x)=0,\quad i=1,.\,.\,,p}}&{{}}\end{array}
$$ 

and the standard form problem ( 4.1 ) are equivalent; indeed, the feasible sets are identical, and the optimal points are identical. (The example ( 4.3 ) above, in which the objective and constraint functions are scaled by appropriate constants, is the special case when all $\psi_{i}$ are linear.) 

Example 4.3 Least-norm and least-norm-squared problems. As a simple example consider the unconstrained Euclidean norm minimization problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2},
$$ 

with variable $x\in\mathbf{R}^{n}$ . Since the norm is always nonnegative, we can just as well solve the problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}=(A x-b)^{T}(A x-b),
$$ 

in which we minimize the square of the Euclidean norm. The problems ( 4.5 ) and ( 4.6 ) are clearly equivalent; the optimal points are the same. The two problems are not the same, however. For example, the objective in ( 4.5 ) is not diﬀerentiable at any $x$ with $A x-b=0$ , whereas the objective in ( 4.6 ) is diﬀerentiable for all $x$ (in fact, quadratic). 

# Slack variables 

One simple transformation is based on the observation that $f_{i}(x)\leq0$ if and only if there is an $s_{i}\geq0$ that satisfies $f_{i}(x)+s_{i}=0$ . Using this transformation we obtain the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad s_{i}\geq0,\quad i=1,\ldots,m}\\ &{f_{i}(x)+s_{i}=0,\quad i=1,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p,}\end{array}}
$$ 

where the variables are $x\in\mathbf{R}^{n}$ and $s\in\mathbf{R}^{m}$ . This problem has $n+m$ variables, $m$ inequality constraints (the nonnegativity constraints on $s_{i}$ ), and $m+p$ equality constraints. The new variable $s_{i}$ is called the slack variable associated with the original inequality constraint $f_{i}(x)\leq0$ . Introducing slack variables replaces each inequality constraint with an equality constraint, and a nonnegativity constraint. 

The problem ( 4.7 ) is equivalent to the original standard form problem ( 4.1 ). Indeed, if $(x,s)$ is feasible for the problem ( 4.7 ), then $x$ is feasible for the original problem, since $s_{i}=-f_{i}(x)\geq0$ . Conversely, if $x$ is feasible for the original problem, then $(x,s)$ is feasible for the problem ( 4.7 ), where we take $s_{i}=-f_{i}(x)$ . Similarly, $x$ is optimal for the original problem ( 4.1 ) if and only if $(x,s)$ is optimal for the problem ( 4.7 ), where $s_{i}=-f_{i}(x)$ . 

# Eliminating equality constraints 

If we can explicitly parametrize all solutions of the equality constraints 

$$
h_{i}(x)=0,\quad i=1,.\,.\,.\,,p,
$$ 

using some parameter $z\;\in\;\mathbf{R}^{k}$ , then we can eliminate the equality constraints from the problem, as follows. Suppose the f $\phi:\mathbf{R}^{k}\,\rightarrow\,\mathbf{R}^{n}$ is such that $x$ satisfies ( 4.8 ) if and only if there is some z $z\,\in\,\mathbf{R}^{k}$ ∈ such that $x\,=\,\phi(z)$ . The optimization problem 

$$
\begin{array}{l r}{\mathrm{minimize}}&{\tilde{f}_{0}(z)=f_{0}(\phi(z))}\\ {\mathrm{subject~to}}&{\tilde{f}_{i}(z)=f_{i}(\phi(z))\leq0,\quad i=1,.\,.\,,m}\end{array}
$$ 

is then equivalent to the original problem ( 4.1 ). This transformed problem has variable $z\,\in\,\mathbf{R}^{k}$ , $m$ inequality constraints, and no equality constraints. If $\mathcal{Z}$ is optimal for the transformed problem, then $x\,=\,\phi(z)$ is optimal for the original problem. Conversely, if $x$ is optimal for the original problem, then (since $x$ is feasible) there is at least one $z$ such that $x=\phi(z)$ . Any such $\mathcal{Z}$ is optimal for the transformed problem. 

# Eliminating linear equality constraints 

The process of eliminating variables can be described more explicitly, and easily carried out numerically, when the equality constraints are all linear, i.e. , have the form $A x=b$ . If $A x=b$ is inconsistent, i.e. , $b\notin\mathcal{R}(A)$ , then the original problem is infeasible. Assuming this is not the case, let $x_{0}$ denote any solution of the equality constraints. Let $F\,\in\,\mathbf{R}^{n\times k}$ be atrix with $\mathcal{R}(F)\,=\,\mathcal{N}(A)$ , s eneral solution of e linear equations Ax $A x=b$ is given by $F z+x_{0}$ , where z $z\in\mathbf{R}^{k}$ ∈ . (We can choose F to be full rank, in which case we have $k=n-\mathbf{rank}\,A$ .) 

Substituting $x=F z+x_{0}$ into the original problem yields the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(F z+x_{0})}}\\ {{\mathrm{subject~to}}}&{{f_{i}(F z+x_{0})\leq0,\quad i=1,\ldots,m,}}\end{array}
$$ 

with variable $z$ , which is equivalent to the original problem, has no equality con- straints, and $\mathbf{rank}\,A$ fewer variables. 

# Introducing equality constraints 

We can also introduce equality constraints and new variables into a problem. In- stead of describing the general case, which is complicated and not very illuminating, we give a typical example that will be useful later. Consider the problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{f_{0}(A_{0}x+b_{0})}\\ {\mathrm{subject~to}}&{f_{i}(A_{i}x+b_{i})\le0,\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=0,\quad i=1,.\,.\,,p,}\end{array}
$$ 

where $x\,\in\,\mathbf{R}^{n}$ , $A_{i}\,\in\,\mathbf{R}^{k_{i}\times n}$ , and $f_{i}:\mathbf{R}^{k_{i}}\rightarrow\mathbf{R}$ . In this problem he objective and constraint functions are given as compositions of the functions f with affine transformations defined by $A_{i}x+b_{i}$ . 

ntrod riables $y_{i}\in\mathbf{R}^{k_{i}}$ , as well as new equality constraints $y_{i}=$ $A_{i}x+b_{i}$ i , for i $i=0,\ldots,m$ , and form the equivalent problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(y_{0})}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(y_{i})\leq0,\quad i=1,\ldots,m}\\ &{y_{i}=A_{i}x+b_{i},\quad i=0,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p.}\end{array}}
$$ 

This problem has $k_{0}+\cdot\cdot\cdot+k_{m}$ new variables, 

$$
y_{0}\in\mathbf{R}^{k_{0}},\quad\ldots,\quad y_{m}\in\mathbf{R}^{k_{m}},
$$ 

and $k_{0}+\cdot\cdot\cdot+k_{m}$ new equality constraints, 

$$
y_{0}=A_{0}x+b_{0},\quad.\,.\,.\,,\quad y_{m}=A_{m}x+b_{m}.
$$ 

The objective and inequality constraints in this problem are independent , i.e. , in- volve diﬀerent optimization variables. 

# Optimizing over some variables 

We always have 

$$
\operatorname*{inf}_{x,y}f(x,y)=\operatorname*{inf}_{x}{\tilde{f}}(x)
$$ 

where $\textstyle{\ddot{f}}(x)=\operatorname*{inf}_{y}f(x,y)$ ). In other words, we can always minimize a function by first minimizing over some of the variables, and then minimizing over the remaining ones. This simple and general principle can be used to transform problems into equivalent forms. The general case is cumbersome to describe and not illuminating, so we describe instead an example. 

ose th $x\,\in\,\mathbf{R}^{\,n}$ is partitioned as $x\,=\,\left(x_{1},x_{2}\right)$ , with $x_{1}\,\in\,\mathbf{R}^{n_{1}}$ , n $x_{2}\in\mathbf{R}^{n_{2}}$ , and n $n_{1}+n_{2}=n$ . We consider the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}\!\left(x_{1},x_{2}\right)}}\\ {{\mathrm{subject~to}}}&{{f_{i}\!\left(x_{1}\right)\le0,\quad i=1,\ldots,m_{1}}}\\ &{{\tilde{f}_{i}\!\left(x_{2}\right)\le0,\quad i=1,\ldots,m_{2},}}\end{array}
$$ 

in which the constraints are independent, in the sense that each constraint function depends on $x_{1}$ or $x_{2}$ . We first minimize over $x_{2}$ . Define the function ${\ddot{f}}_{0}$ of $x_{1}$ by 

$$
\tilde{f}_{0}(x_{1})=\operatorname*{inf}\{f_{0}(x_{1},z)\mid\tilde{f}_{i}(z)\leq0,\ i=1,.\,.\,,m_{2}\}.
$$ 

The problem ( 4.9 ) is then equivalent to 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\tilde{f}_{0}(x_{1})}\\ {\mathrm{subject~to}}&{f_{i}(x_{1})\leq0,\quad i=1,\ldots,m_{1}.}\end{array}
$$ 

Example 4.4 Minimizing a quadratic function with constraints on some variables. Consider a problem with strictly convex quadratic objective, with some of the vari- ables unconstrained: 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\;x_{1}^{T}P_{11}x_{1}+2x_{1}^{T}P_{12}x_{2}+x_{2}^{T}P_{22}x_{2}}\\ &{\mathrm{subject~to}\quad f_{i}(x_{1})\le0,\quad i=1,.\,.\,,m,}\end{array}
$$ 

where $P_{11}$ and $P_{22}$ are symmetric. Here we can analytically minimize over $x_{2}$ : 

$$
\operatorname*{inf}_{x_{2}}\left(x_{1}^{T}P_{11}x_{1}+2x_{1}^{T}P_{12}x_{2}+x_{2}^{T}P_{22}x_{2}\right)=x_{1}^{T}\left(P_{11}-P_{12}P_{22}^{-1}P_{12}^{T}\right)x_{1}
$$ 

(see § A.5.5 ). Therefore the original problem is equivalent to 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\ x_{1}^{T}\left(P_{11}-P_{12}P_{22}^{-1}P_{12}^{T}\right)x_{1}}\\ &{\mathrm{subject~to}\quad f_{i}(x_{1})\leq0,\quad i=1,.\,.\,,m.}\end{array}
$$ 

# Epigraph problem form 

The epigraph form of the standard problem ( 4.1 ) is the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{t}\\ {{\mathrm{subject~to}}\quad f_{0}(x)-t\leq0}\\ &{f_{i}(x)\leq0,\quad i=1,\dots,m}\\ &{h_{i}(x)=0,\quad i=1,\dots,p,}\end{array}}
$$ 

with variables $x\,\in\,\mathbf{R}^{n}$ and $t\in\mathbf{R}$ . We can easily see that it is equivalent to the original problem: $(x,t)$ is optimal for ( 4.11 ) if and only if $x$ is optimal for ( 4.1 ) and $t=f_{0}(x)$ . Note that the objective function of the epigraph form problem is a linear function of the variables $x$ , $t$ . 

The epigraph form problem ( 4.11 ) can be interpreted geometrically as an op- timization problem in the ‘graph space’ $(x,t)$ : we minimize $t$ over the epigraph of $f_{0}$ , subject to the constraints on $x$ . This is illustrated in figure 4.1 . 

# Implicit and explicit constraints 

By a simple trick already mentioned in § 3.1.2 , we can include any of the constraints implicitly in the objective function, by redefining its domain. As an extreme ex- ample, the standard form problem can be expressed as the unconstrained problem 

$$
{\mathrm{minimize}}\quad F(x),
$$ 

where we define the function $F$ as $f_{0}$ , but with domain restricted to the feasible set: 

$$
\mathbf{dom}\,F=\{x\in\mathbf{dom}\,f_{0}\ |\ f_{i}(x)\leq0,\ i=1,\ldots,m,\ h_{i}(x)=0,\ i=1,\ldots,p\},
$$ 

and $F(x)=f_{0}(x)$ for $x\in\mathbf{dom}\,F$ . (Equivalently, we can define $F(x)$ to have value $\infty$ for $x$ not feasible.) The problems ( 4.1 ) and ( 4.12 ) are clearly equivalent: they have the same feasible set, optimal points, and optimal value. 

Of course this transformation is nothing more than a notational trick. Making the constraints implicit has not made the problem any easier to analyze or solve, 

![](images/c6caabd10fe79410bd77e53ff9a9cc4a0411a200c29f55b55644444d8faccbec.jpg) 
Figure 4.1 Geometric interpretation of epigraph form problem, for a prob- lem with no constraints. The problem is to find the point in the epigraph (shown shaded) that minimizes $t$ , i.e. , the ‘lowest’ point in the epigraph. The optimal point is $(x^{\star},t^{\star})$ . 

even though the problem ( 4.12 ) is, at least nominally, unconstrained. In some ways the transformation makes the problem more difficult. Suppose, for example, that the objective $f_{0}$ in the original problem is diﬀerentiable, so in particular its domain is open. The restricted objective function $F$ is probably not diﬀerentiable, since its domain is likely not to be open. 

Conversely, we will encounter problems with implicit constraints, which we can then make explicit. As a simple example, consider the unconstrained problem 

$$
{\mathrm{minimize~}}\quad f(x)
$$ 

where the function $f$ is given by 

$$
f(x)={\left\{\begin{array}{l l}{x^{T}x}&{A x=b}\\ {\infty}&{{\mathrm{otherwise.}}}\end{array}\right.}
$$ 

Thus, the objective function is equal to the quadratic form $x^{T}x$ on the affine set defined by $\mathit{A x}\:=\:b$ , and $\infty$ oﬀ ne set. Since we can clearly restrict our attention to points that satisfy Ax $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ , we say that the problem ( 4.13 ) has an implicit equality constraint $\mathit{A x}\;=\;b$ hidden in the objective. We can make the implicit equality constraint explicit, by forming the equivalent problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad x^{T}x}\\ &{{\mathrm{subject~to}}\quad A x=b.}\end{array}}
$$ 

While the problems ( 4.13 ) and ( 4.14 ) are clearly equivalent, they are not the same. The problem ( 4.13 ) is unconstrained, but its objective function is not diﬀerentiable. The problem ( 4.14 ), however, has an equality constraint, but its objective and constraint functions are diﬀerentiable. 

# 4.1.4 Parameter and oracle problem descriptions 

For a problem in the standard form ( 4.1 ), there is still the question of how the objective and constraint functions are specified. In many cases these functions have some analytical or closed form, i.e. , are given by a formula or expression that involves the variable $x$ as well as some parameters. Suppose, for example, the objective is quadratic, so it has the form $f_{0}(x)=(1/2)x^{T}P x+q^{T}x+r$ . To specify the objective function we give the coefficients (also called problem parameters or problem data ) $P\,\in\,{\bf S}^{n}$ , $q\,\in\,\mathbf{R}^{n}$ , and $r\,\in\,\mathbf{R}$ . We call this a parameter problem description , since the specific problem to be solved ( i.e. , the problem instance) is specified by giving the values of the parameters that appear in the expressions for the objective and constraint functions. 

In other cases the objective and constraint functions are described by oracle models (which are also called black box or subroutine models). In an oracle model, we do not know $f$ explicitly, but can evaluate $f(x)$ (and usually also some deriva- tives) at any $x\in\mathbf{dom}\,f$ . This is referred to as querying the oracle , and is usually associated with some cost, such as time. We are also given some prior information about the function, such as convexity and a bound on its values. As a concrete example of an oracle model, consider an unconstrained problem, in which we are to minimize the function $f$ . The function value $f(x)$ and its gradient $\nabla f(x)$ are evaluated in a subroutine. We can call the subroutine at any $x\in\mathbf{dom}\,f$ , but do not have access to its source code. Calling the subroutine with argument $x$ yields (when the subroutine returns) $f(x)$ and $\nabla f(x)$ . Note that in the oracle model, we never really know the function; we only know the function value (and some derivatives) at the points where we have queried the oracle. (We also know some given prior information about the function, such as di e rent i ability and convexity.) 

In practice the distinction between a parameter and oracle problem description is not so sharp. If we are given a parameter problem description, we can construct an oracle for it, which simply evaluates the required functions and derivatives when queried. Most of the algorithms we study in part III work with an oracle model, but can be made more efficient when they are restricted to solve a specific parametrized family of problems. 

# 4.2 Convex optimization 

# 4.2.1 Convex optimization problems in standard form 

A convex optimization problem is one of the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{a_{i}^{T}x=b_{i},\quad i=1,.\,.\,,p,}\end{array}}
$$ 

where $f_{0},\ldots,f_{m}$ are convex functions. Comparing ( 4.15 ) with the general standard form problem ( 4.1 ), the convex problem has three additional requirements: 

• • the inequality constraint functions must be convex, • the equality constraint functions $h_{i}(x)=a_{i}^{T}x-b_{i}$ − must be affine. 

We immediately note an important property: The feasible set of a convex optimiza- tion problem is convex, since it is the intersection of the domain of the problem 

$$
\mathcal{D}=\bigcap_{i=0}^{m}\mathbf{dom}\,f_{i},
$$ 

which is a convex set, with $m$ (convex) sublevel sets $\{x\mid f_{i}(x)\leq0\}$ and $p$ hyper- planes $\{x\mid a_{i}^{T}x=b_{i}\}$ } . (We an assume without loss of generality that $a_{i}\neq0$ : if $a_{i}\,=\,0$ and b = 0 for some i , then the $\imath$ th equality constraint can be deleted; if $a_{i}=0$ and $b_{i}\neq0$ , the $i$ th equality constraint is inconsistent, and the problem is in- feasible.) Thus, in a convex optimization problem, we minimize a convex objective function over a convex set. 

If $f_{0}$ is quasiconvex instead of convex, we say the problem ( 4.15 ) is a (standard form) quasiconvex optimization problem . Since the sublevel sets of a convex or quasiconvex function are convex, we conclude that for a convex or quasiconvex optimization problem the $\epsilon$ -suboptimal sets are convex. In particular, the optimal set is convex. If the objective is strictly convex, then the optimal set contains at most one point. 

# Concave maximization problems 

With a slight abuse of notation, we will also refer to 

$$
\begin{array}{l r l}{\mathrm{maximize}}&{\ f_{0}(x)}\\ {\mathrm{subject~to}}&{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{a_{i}^{T}x=b_{i},\quad i=1,\ldots,p,}\end{array}
$$ 

as a convex optimization problem if the objective function $f_{0}$ is concave, and the inequality constraint functions $f_{1},\ldots,f_{m}$ are convex. This concave maximization problem is readily solved by minimizing the convex objective function $-f_{0}$ . All of the results, conclusions, and algorithms that we describe for the minimization problem are easily transposed to the maximization case. In a similar way the maximization problem ( 4.16 ) is called quasiconvex if $f_{0}$ is quasiconcave. 

# Abstract form convex optimization problem 

It is important to note a subtlety in our definition of convex optimization problem. Consider the example with $x\in\mathbf{R}^{2}$ , 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{f_{0}(x)=x_{1}^{2}+x_{2}^{2}}\\ {{\mathrm{subject~to}}}&{f_{1}(x)=x_{1}/(1+x_{2}^{2})\leq0}\\ &{h_{1}(x)=(x_{1}+x_{2})^{2}=0,}\end{array}}
$$ 

which is in the standard form ( 4.1 ). This problem is not a convex optimization problem in standard form since the equality constraint function $h_{1}$ is not affine, and 

the inequality constraint function $f_{1}$ is not convex. Nevertheless the feasible set, which is $\{x\mid x_{1}\leq0$ , $x_{1}+x_{2}=0\}$ , is convex. So although in this problem we are minimizing a convex function f $f_{0}$ over a convex set, it is not a convex optimization problem by our definition. 

Of course, the problem is readily reformulated as 

$$
\begin{array}{l l}{\mathrm{minimize}}&{f_{0}(x)=x_{1}^{2}+x_{2}^{2}}\\ {\mathrm{subject~to}}&{\tilde{f}_{1}(x)=x_{1}\leq0}\\ &{\tilde{h}_{1}(x)=x_{1}+x_{2}=0,}\end{array}
$$ 

which is in standard convex optimization form, since $f_{0}$ and $\ddot{f}_{1}$ are convex, and $\ddot{h}_{1}$ is affine. 

Some authors use the term abstract convex optimization problem to describe the (abstract) problem of minimizing a convex function over a convex set. Using this terminology, the problem ( 4.17 ) is an abstract convex optimization problem. We will not use this terminology in this book. For us, a convex optimization problem is not just one of minimizing a convex function over a convex set; it is also required that the feasible set be described specifically by a set of inequalities involving convex functions, and a set of linear equality constraints. The problem ( 4.17 ) is not a convex optimization problem, but the problem ( 4.18 ) is a convex optimization problem. (The two problems are, however, equivalent.) 

Our adoption of the stricter definition of convex optimization problem does not matter much in practice. To solve the abstract problem of minimizing a convex function over a convex set, we need to find a description of the set in terms of convex inequalities and linear equality constraints. As the example above suggests, this is usually straightforward. 

# 4.2.2 Local and global optima 

A fundamental property of convex optimization problems is that any locally optimal point is also (globally) optimal. To see this, suppose that $x$ is locally optimal for a convex optimization problem, i.e. , $x$ is feasible and 

$$
f_{0}(x)=\operatorname*{inf}\{f_{0}(z)\mid z{\mathrm{~feasible}},\ \|z-x\|_{2}\leq R\},
$$ 

for some $R>0$ . Now suppose that $x$ is not globally optimal, i.e. , there is a feasible $y$ such that $f_{0}(y)<f_{0}(x)$ . Evidently $\|y-x\|_{2}>R$ , since otherwise $f_{0}(x)\leq f_{0}(y)$ . Consider the point $z$ given by 

$$
z=(1-\theta)x+\theta y,\qquad\theta={\frac{R}{2||y-x||_{2}}}.
$$ 

Then we have $\|z\,-\,x\|_{2}\,=\,R/2\,<\,R$ , and by convexity of the feasible set, $z$ is feasible. By convexity of $f_{0}$ we have 

$$
f_{0}(z)\leq(1-\theta)f_{0}(x)+\theta f_{0}(y)<f_{0}(x),
$$ 

which contradicts ( 4.19 ). Hence there exists no feasible $y$ with $f_{0}(y)<f_{0}(x)$ , i.e. , $x$ is globally optimal. 

![](images/38c301fcd4790aad83fb777a3cac0eafb1a175eb6f0e028ad7ca0fe3b297f163.jpg) 
Figure 4.2 Geometric interpretation of the optimality condition ( 4.21 ). The feasible set $X$ is shown shaded. Some level curves of $f_{0}$ are shown as dashed lines. The point $x$ is op al: $-\nabla f_{0}(x)$ defines a supporting hyperplane (shown as a solid line) to X at x . 

It is not true that locally optimal points of quasiconvex optimization problems are globally optimal; see 4.2.5 . 

# 4.2.3 An optimality criterion for diﬀerentiable $f_{0}$ 

Suppose that the objective $f_{0}$ in a convex optimization problem is diﬀerentiable, so that for all $x,y\in\mathbf{dom}\,f_{0}$ , 

$$
f_{0}(y)\geq f_{0}(x)+\nabla f_{0}(x)^{T}(y-x)
$$ 

(see § 3.1.3 ). Let $X$ denote the feasible set, i.e. , 

$$
X=\{x\mid f_{i}(x)\leq0,\ i=1,\ldots,m,\ h_{i}(x)=0,\ i=1,\ldots,p\}.
$$ 

Then $x$ is optimal if and only if $x\in X$ and 

$$
\nabla f_{0}(x)^{T}(y-x)\geq0{\mathrm{~for~all~}}y\in X.
$$ 

This optimality criterion can be understood geometrically: If $\nabla f_{0}(x)\neq0$ , it means that $-\nabla f_{0}(x)$ defines a supporting hyperplane to the feasible set at $x$ (see fig- ure 4.2 ). 

# Proof of optimality condition 

First suppose $x\ \in\ X$ and satisfies ( 4.21 ). Then if $y\ \in\ X$ we have, by ( 4.20 ), $f_{0}(y)\geq f_{0}(x)$ . This shows $x$ is an optimal point for ( 4.1 ). 

Conversely, suppose $x$ is optimal, but the condition ( 4.21 ) does not hold, i.e. , for some $y\in X$ we have 

$$
\nabla f_{0}(x)^{T}(y-x)<0.
$$ 

Consider the point $z(t)=t y+(1-t)x$ , where $t\in[0,1]$ is a parameter. Since $z(t)$ is on the line segment between $x$ and $y$ , and the feasible set is convex, $z(t)$ is feasible. We claim that for small positive $t$ we have $f_{0}(z(t))<f_{0}(x)$ , which will prove that $x$ is not optimal. To show this, note that 

$$
\left.\frac{d}{d t}f_{0}(\boldsymbol{z}(t))\right|_{t=0}=\nabla f_{0}(\boldsymbol{x})^{T}(\boldsymbol{y}-\boldsymbol{x})<0,
$$ 

so for small positive $t$ , we have $f_{0}(z(t))<f_{0}(x)$ 

We will pursue the topic of optimality conditions in much more depth in chap- ter 5 , but here we examine a few simple examples. 

# Unconstrained problems 

For an unconstrained problem ( i.e. , $m=p=0$ ), the condition ( 4.21 ) reduces to the well known necessary and sufficient condition 

$$
\nabla f_{0}(x)=0
$$ 

for $x$ to be optimal. While we have already seen this optimality condition, it is useful to see how it follows from ( 4.21 ). Suppose $x$ is optimal, which means here that $x\in\mathbf{dom}\,f_{0}$ , and for all feasible $y$ we have $\nabla f_{0}(x)^{T}(y-x)\geq0$ . Since $f_{0}$ is diﬀerentiable, its domain is (by definition) open, so all $y$ sufficiently close to $x$ are feasible. Let us take $y=x-t\nabla f_{0}(x)$ , where $t\in\mathbf{R}$ is a parameter. For $t$ small and positive, $y$ is feasible, and so 

$$
\nabla f_{0}(x)^{T}(y-x)=-t\|\nabla f_{0}(x)\|_{2}^{2}\geq0,
$$ 

from which we conclude $\nabla f_{0}(x)=0$ . 

There are several possible situations, depending on the number of solutions of ( 4.22 ). If there are no solutions of ( 4.22 ), then there are no optimal points; the optimal value of the problem is not attained. Here we can distinguish between two cases: the problem is unbounded below, or the optimal value is finite, but not attained. On the other hand we can have multiple solutions of the equation ( 4.22 ), in which case each such solution is a minimizer of $f_{0}$ . 

Example 4.5 Unconstrained quadratic optimization. Consider the problem of mini- mizing the quadratic function 

$$
f_{0}(x)=(1/2)x^{T}P x+q^{T}x+r,
$$ 

where $P\in\mathbf{S}_{+}^{n}$ (which makes $f_{0}$ convex). The necessary and sufficient condition for $x$ to be a minimizer of $f_{0}$ is 

$$
\nabla f_{0}(x)=P x+q=0.
$$ 

Several cases can occur, depending on whether this (linear) equation has no solutions, one solution, or many solutions. 

• If $q\not\in{\mathcal{R}}(P)$ , then there is no solution. In this case $f_{0}$ is unbounded below. • If $P\succ0$ (which is the condition for $f_{0}$ to be strictly convex), then there is a unique minimizer, $x^{\star}=-P^{-1}q$ . 

• If $P$ is singular, but $q\in\mathcal{R}(P)$ hen the set of optimal points is he (affine) set $X_{\mathrm{opt}}=-P^{\dagger}q+\mathcal{N}(P)$, where P$P^{\dagger}$ denotes the pseudo-inverse of P (see §A.5.4). 

Example 4.6 Analytic centering. Consider the (unconstrained) problem of minimiz- ing the (convex) function $f_{0}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , defined as 

$$
f_{0}(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),\qquad\operatorname{dom}f_{0}=\{x\mid A x\prec b\},
$$ 

where $a_{1}^{T},\cdot\cdot\cdot,a_{m}^{T}$ are the rows of $A$ . The function $f_{0}$ is diﬀerentiable, so the necessary and sufficient conditions for $_{x}$ to be optimal are 

$$
A x\prec b,\qquad\nabla f_{0}(x)=\sum_{i=1}^{m}\frac{1}{b_{i}-a_{i}^{T}x}a_{i}=0.
$$ 

(The condition $A x\prec b$ is just $x\in\mathbf{dom}\,f_{0}$ .) If $A x\prec b$ is infeasible, then the domain of $f_{0}$ is empty. Assuming $A x\prec b$ is feasible, there are still several possible cases (see exercise 4.2 ): 

• There are no solutions of ( 4.23 ), and hence no optimal points for the problem. This occurs if and only if $f_{0}$ is unbounded below. • There are many solutions of ( 4.23 ). In this case it can be shown that the solutions form an affine set. • There is a unique solution of ( 4.23 ), i.e. , a unique minimizer of $f_{0}$ . This occurs if and only if the open polyhedron $\{x\mid A x\prec b\}$ is nonempty and bounded. 

# Problems with equality constraints only 

Consider the case where there are equality constraints but no inequality constraints, i.e. , 

$$
\begin{array}{l c l}{\mathrm{minimize}}&{f_{0}(\boldsymbol{x})}\\ {\mathrm{subject~to}}&{A\boldsymbol{x}=\boldsymbol{b}.}\end{array}
$$ 

Here the feasible set is affine. We assume that it is nonempty; otherwise the problem is infeasible. The optimality condition for a feasible $x$ is that 

$$
\nabla f_{0}(x)^{T}(y-x)\geq0
$$ 

must hold for all $y$ satisfying $A y=b$ . Since $x$ is feasible, every feasible $y$ has the form $\boldsymbol{y}\,=\,\boldsymbol{x}+\boldsymbol{v}$ for some $v\,\in{\mathcal{N}}(A)$ . The optimality condition can therefore be expressed as: 

$$
\nabla f_{0}(x)^{T}v\geq0{\mathrm{~for~all~}}v\in{\mathcal{N}}(A)
$$ 

If a linear function is nonnegative on a subspace, then it must be zero on the subspace, so it follows that $\nabla f_{0}(x)^{T}v=0$ for all $v\in{\mathcal{N}}(A)$ . In other words, 

$$
\nabla f_{0}(x)\perp{\mathcal{N}}(A).
$$ 

Using the fact that $\mathcal{N}(A)^{\perp}=\mathcal{R}(A^{T})$ , timality condition can be expressed as $\nabla f_{0}(x)\in{\mathcal{R}}(A^{T})$ , i.e. , there exists a ν $\nu\in\mathbf{R}^{p}$ ∈ p such that 

$$
\nabla f_{0}(x)+A^{T}\nu=0.
$$ 

Together with the requirement $A x=b$ ( i.e. , that $x$ is feasible), this is the classical Lagrange multiplier optimality condition, which we will study in greater detail in chapter 5 . 

# Minimization over the nonnegative orthant 

As another example we consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad f_{0}(x)}\\ &{{\mathrm{subject~to}}\quad x\succeq0,}\end{array}}
$$ 

where the only inequality constraints are nonnegativity constraints on the variables. The optimality condition ( 4.21 ) is then 

$$
x\succeq0,\qquad\nabla f_{0}(x)^{T}(y-x)\geq0\mathrm{~for~all~}y\succeq0.
$$ 

The term $\nabla f_{0}(x)^{T}y$ , which is a linear function of $y$ , is unbounded below on $y\succeq0$ , unless we have $\nabla f_{0}(x)\succeq0$ . The condition then reduces to $-\nabla f_{0}(x)^{T}x\geq0$ . But $x\succeq0$ and $\nabla f_{0}(x)\succeq0$ , so we must have $\nabla f_{0}(x)^{T}x=0$ , i.e. , 

$$
\sum_{i=1}^{n}(\nabla f_{0}(x))_{i}x_{i}=0.
$$ 

Now each of the terms in this sum is the product of two nonnegative numbers, so we conclude that each term must be zero, i.e. , $(\nabla f_{0}(x))_{i}\,x_{i}=0$ for $i=1,\cdot\cdot\cdot,n$ . The optimality condition can therefore be expressed as 

$$
x\succeq0,\qquad\nabla f_{0}(x)\succeq0,\qquad x_{i}\left(\nabla f_{0}(x)\right)_{i}=0,\quad i=1,\ldots,n.
$$ 

The last condition is called complementarity , since it means that the sparsity pat- terns ( i.e. , the set of indices corresponding to nonzero components) of the vectors $x$ and $\nabla f_{0}(x)$ are complementary ( i.e. , have empty intersection). We will encounter complementarity conditions again in chapter 5 . 

# 4.2.4 Equivalent convex problems 

It is useful to see which of the transformations described in § 4.1.3 preserve convex- ity. 

# Eliminating equality constraints 

For a convex problem the equality constraints must be linear, i.e. , of the form $A x=b$ . In this case they can be eliminated by finding a particular solution of $x_{0}$ 

$\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ , and a matrix $F$ whose range is the nullspace of $A$ , which results in the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(F z+x_{0})}}\\ {{\mathrm{subject~to}}}&{{f_{i}(F z+x_{0})\leq0,\quad i=1,\ldots,m,}}\end{array}
$$ 

with variable $z$ . Since the composition of a convex function with an affine func- tion is convex, eliminating equality constraints preserves convexity of a problem. Moreover, the process of eliminating equality constraints (and reconstructing the solution of the original problem from the solution of the transformed problem) involves standard linear algebra operations. 

At least in principle, this means we can restrict our attention to convex opti- mization problems which have no equality constraints. In many cases, however, it is better to retain the equality constraints, since eliminating them can make the problem harder to understand and analyze, or ruin the efficiency of an algorithm that solves it. This is true, for example, when the variable $x$ has very large dimen- sion, and eliminating the equality constraints would destroy sparsity or some other useful structure of the problem. 

# Introducing equality constraints 

We can introduce new variables and equality constraints into a convex optimization problem, provided the equality constraints are linear, and the resulting problem will also be convex. For example, if an objective or constraint function has the form $f_{i}(A_{i}x+b_{i})$ , wher $A_{i}\in\mathbf{R}^{k_{i}\times n}$ , we can introduce a new variabl $y_{i}\in\mathbf{R}^{k_{i}}$ place $f_{i}(A_{i}x+b_{i})$ ) with $f_{i}(y_{i})$ ), and add the linear equality constraint y $y_{i}=A_{i}x+b_{i}$ . 

# Slack variables 

By introducing slack variables we have the new constraints $f_{i}(x)+s_{i}=0$ . Since equality constraint functions must be affine in a convex problem, we must have $f_{i}$ affine. In other words: introducing slack variables for linear inequalities preserves convexity of a problem. 

# Epigraph problem form 

The epigraph form of the convex optimization problem ( 4.15 ) is 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{t}\\ {{\mathrm{subject~to}}}&{f_{0}(x)-t\leq0}\\ &{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{a_{i}^{T}x=b_{i},\quad i=1,\ldots,p.}\end{array}}
$$ 

The objective is linear (hence convex) and the new constraint function $f_{0}(x)-t$ is also convex in $(x,t)$ , so the epigraph form problem is convex as well. 

It is sometimes said that a linear objective is universal for convex optimization, since any convex optimization problem is readily transformed to one with linear objective. The epigraph form of a convex problem has several practical uses. By assuming the objective of a convex optimization problem is linear, we can simplify theoretical analysis. It can also simplify algorithm development, since an algo- rithm that solves convex optimization problems with linear objective can, using the transformation above, solve any convex optimization problem (provided it can handle the constraint $f_{0}(x)-t\leq0)$ ). 

# Minimizing over some variables 

Minimizing a convex function over some variables preserves convexity. Therefore, if $f_{0}$ in ( 4.9 ) is jointly convex in $x_{1}$ and $x_{2}$ , and $f_{i}$ , $i\,=\,1,\ldots,m_{1}$ , and $\tilde{f}_{i}$ , $\textit{i}=$ $1,.\cdot\cdot\cdot,m_{2}$ , are convex, then the equivalent problem ( 4.10 ) is convex. 

# 4.2.5 Quasiconvex optimization 

Recall that a quasiconvex optimization problem has the standard form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b,}\end{array}}
$$ 

where the inequality constraint functions $f_{1},\ldots,f_{m}$ are convex, and the objective $f_{0}$ is quasiconvex (instead of convex, as in a convex optimization problem). (Qua- siconvex constraint functions can be replaced with equivalent convex constraint functions, i.e. , constraint functions that are convex and have the same $0$ -sublevel set, as in 3.4.5 .) 

In this section we point out some basic diﬀerences between convex and quasicon- vex optimization problems, and also show how solving a quasiconvex optimization problem can be reduced to solving a sequence of convex optimization problems. 

# Locally optimal solutions and optimality conditions 

The most important diﬀerence between convex and quasiconvex optimization is that a quasiconvex optimization problem can have locally optimal solutions that are not (globally) optimal. This phenomenon can be seen even in the simple case of unconstrained minimization of a quasiconvex function on $\mathbf{R}$ , such as the one shown in figure 4.3 . 

Nevertheless, a variation of the optimality condition ( 4.21 ) given in § 4.2.3 does hold for quasiconvex optimization problems with diﬀerentiable objective function. Let $X$ denote the feasible set for the quasiconvex optimization problem ( 4.24 ). It follows from the first-order condition for quasiconvexity ( 3.20 ) that $x$ is optimal if 

$$
x\in X,\qquad\nabla f_{0}(x)^{T}(y-x)>0{\mathrm{~for~all~}}y\in X\setminus\{x\}.
$$ 

There are two important diﬀerences between this criterion and the analogous one ( 4.21 ) for convex optimization: 

• The condition ( 4.25 ) is only sufficient for optimality; simple examples show that it need not hold for an optimal point. In contrast, the condition ( 4.21 ) is necessary and sufficient for $x$ to solve the convex problem. • The condition ( 4.25 ) requires the gradient of $f_{0}$ to be nonzero, whereas the condition ( 4.21 ) does not. Indeed, when $\nabla f_{0}(x)=0$ in the convex case, the condition ( 4.21 ) is satisfied, and $x$ is optimal. 

![](images/6fd302852f11b0ab226e98f67afad7bc0faaafb1a7cd1a31c05a920c623967d4.jpg) 
Figure 4.3 A quasiconvex function $f$ on $\mathbf{R}$ , with a locally optimal point $x$ that is not globally optimal. This example shows that the simple optimality condition $f^{\prime}(x)=0$ , valid for convex functions, does not hold for quasiconvex functions. 

# Quasiconvex optimization via convex feasibility problems 

One general approach to quasiconvex optimization relies on the representation of the sublevel sets of a quasiconvex function via a family of convex inequalities, as described in § 3.4.5 . Let $\phi_{t}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $t\in\mathbf{R}$ , be a family of convex functions that satisfy 

$$
f_{0}(x)\leq t\iff\phi_{t}(x)\leq0,
$$ 

and also, for each $x$ , $\phi_{t}(x)$ is a nonincreasing function of $t$ , i.e. , $\phi_{s}(x)\,\leq\,\phi_{t}(x)$ whenever $s\geq t$ . 

Let $p^{\star}$ denote the optimal value of the quasiconvex optimization problem ( 4.24 ). If the feasibility problem 

$$
\begin{array}{l r l}{{\mathrm{find}}}&{{x}}\\ {{\mathrm{subject~to}}}&{{\phi_{t}(x)\le0}}\\ &{{f_{i}(x)\le0,\quad i=1,.\,.\,,m}}\\ &{{A x=b,}}\end{array}
$$ 

is feasible, then we have $p^{\star}\leq t$ . Conversely, if the problem ( 4.26 ) is infeasible, then we can conclude $p^{\star}\geq t$ . The problem ( 4.26 ) is a convex feasibility problem, since the inequality constraint functions are all convex, and the equality constraints are linear. Thus, we can check whether the optimal value $p^{\star}$ of a quasiconvex optimization problem is less than or more than a given value $t$ by solving the convex feasibility problem ( 4.26 ). If the convex feasibility problem is feasible then we have $p^{\star}\leq t$ , and any feasible point $x$ is feasible for the quasiconvex problem and satisfies $f_{0}(x)\leq t$ . If the convex feasibility problem is infeasible, then we know that $p^{\star}\geq t$ . 

This observation can be used as the basis of a simple algorithm for solving the quasiconvex optimization problem ( 4.24 ) using bisection , solving a convex feasi- bility problem at each step. We assume that the problem is feasible, and start with an interval $[l,u]$ known to contain the optimal value $p^{\star}$ . We then solve the convex feasibility problem at its midpoint $t=(l+u)/2$ , to determine whether the optimal value is in the lower or upper half of the interval, and update the interval accordingly. This produces a new interval, which also contains the optimal value, but has half the width of the initial interval. This is repeated until the width of the interval is small enough: 

Algorithm 4.1 Bisection method for quasiconvex optimization. 

given $l\le p^{\star}$ , $u\geq p^{\star}$ , tolerance $\epsilon>0$ . 

repeat 1. $t:=(l+u)/2$ . 2. Solve the convex feasibility problem ( 4.26 3. if ( 4.26 ) is feasible, $u:=t$ ; else $\mathbf{\Psi}_{\ell}:=t$ . until $u-l\le\epsilon$ . 

The interval $[l,u]$ is guaranteed to contain $p^{\star}$ , i.e. , we have $l\;\leq\;p^{\star}\;\leq\;u$ at each step. In each iteration the interval is divided in two, i.e. , bisected, so the length of the interval after $k$ iterations is $2^{-k}(u-l)$ , where $u-l$ is the length of the initial interval. It follows that exactly $\lceil\log_{2}((u-l)/\epsilon)\rceil$ iterations are required before the algorithm terminates. Each step involves solving the convex feasibility problem ( 4.26 ). 

# 4.3 Linear optimization problems 

When the objective and constraint functions are all affine, the problem is called a linear program (LP). A general linear program has the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x+d}\\ {{\mathrm{subject~to}}\quad G x\preceq h}\\ &{A x=b,}\end{array}}
$$ 

where $G\,\in\,\mathbf{R}^{m\times n}$ and $A\,\in\,\mathbf{R}^{p\times n}$ . Linear programs are, of course, convex opti- mization problems. 

It is common to omit the constant $d$ in the objective function, since it does not aﬀect the optimal (or feasible) set. Since we can maximize an affine objective $c^{T}x+$ $d$ , by minimizing $-c^{T}x-d$ (which is still convex), we also refer to a maximization problem with affine objective and constraint functions as an LP. 

The geometric interpretation of an LP is illustrated in figure 4.4 . The feasible set of th 4.27 ) is a polyhedron $\mathcal{P}$ ; the proble to m imize the affine function c $c^{T}x+d$ (or, equivalently, the linear function c $c^{T}x$ x ) over . 

# Standard and inequality form linear programs 

Two special cases of the LP ( 4.27 ) are so widely encountered that they have been given separate names. In a standard form $L P$ the only inequalities are componen- 

![](images/7da5a7dfd3c11cb9a94abce9c1c22e5c17bba801608d21aac6891a362f77ef41.jpg) 
Figure 4.4 Geometric interpretation of an . The feasible set $\mathcal{P}$ , which is a polyhedron, is shaded. The objective c $c^{T}x$ x is linear, so its level curves are hyperplanes orthogonal to $c$ (shown as dashed lines). The point $x^{\star}$ is optimal; it is the point in $\mathcal{P}$ as far as possible in the direction $-c$ . 

twise nonnegativity constraints $x\succeq0$ : 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}}&{A x=b}\\ &{x\succeq0.}\end{array}}
$$ 

If the LP has no equality constraints, it is called an inequality form $L P$ , usually written as 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b.}\end{array}}
$$ 

# Converting LPs to standard form 

It is sometimes useful to transform a general LP ( 4.27 ) to one in standard form ( 4.28 ) (for example in order to use an algorithm for standard form LPs). The first step is to introduce slack variables $s_{i}$ for the inequalities, which results in 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ c^{T}x+d}\\ {{\mathrm{subject~to}}}&{G x+s=h}\\ &{A x=b}\\ &{s\succeq0.}\end{array}}
$$ 

The second step is to express the variable $x$ as the diﬀerence of two nonnegative variables $x^{+}$ and $x^{-}$ , i.e. , $x=x^{+}-x^{-}$ , $x^{+}$ , $x^{-}\succeq0$ . This yields the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ c^{T}x^{+}-c^{T}x^{-}+d}\\ {{\mathrm{subject~to}}}&{G x^{+}-G x^{-}+s=h}\\ &{A x^{+}-A x^{-}=b}\\ &{x^{+}\succeq0,\quad x^{-}\succeq0,\quad s\succeq0,}\end{array}}
$$ 

which is an LP in standard form, with variables $x^{+}$ , $x^{-}$ , and $s$ . (For equivalence of this problem and the original one ( 4.27 ), see exercise 4.10 .) 

These techniques for manipulating problems (along with many others we will see in the examples and exercises) can be used to formulate many problems as linear programs. With some abuse of terminology, it is common to refer to a problem that can be formulated as an LP as an LP, even if it does not have the form ( 4.27 ). 

# 4.3.1 Examples 

LPs arise in a vast number of fields and applications; here we give a few typical examples. 

# Diet problem 

A healthy diet contains $m$ diﬀerent nutrients in quantities at least equal to $b_{1},\,.\,.\,,$ $b_{m}$ . We can compose such a diet by choosing nonnegative quantities $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ of $n$ diﬀerent foods. One unit quantity of food $j$ contains an amount $a_{i j}$ of nutrient $i$ , and has a cost of . We want to determine the cheapest diet that satisfies the $c_{j}$ nutritional requirements. This problem can be formulated as the LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ c^{T}x}\\ {{\mathrm{subject~to}}}&{A x\succeq b}\\ &{x\succeq0.}\end{array}}
$$ 

Several variations on this problem can also be formulated as LPs. For example, we can insist on an exact amount of a nutrient in the diet (which gives a linear equality constraint), or we can impose an upper bound on the amount of a nutrient, in addition to the lower bound as above. 

# Chebyshev center of a polyhedron 

We consider the problem of finding the largest Euclidean ball that lies in a poly- hedron described by linear inequalities, 

$$
{\mathcal{P}}=\{x\in\mathbf{R}^{n}\mid a_{i}^{T}x\leq b_{i},\ i=1,\ldots,m\}.
$$ 

(The center of the optimal ball is called the Chebyshev center of the polyhedron; it is the point deepest inside the polyhedron, i.e. , farthest from the boundary; see § 8.5.1 .) We represent the ball as 

$$
{\mathcal{B}}=\{x_{c}+u\mid\|u\|_{2}\leq r\}.
$$ 

The varia les in the problem are the $x_{c}\in\mathbf{R}^{n}$ and the radius $r$ ; we wish to maximize r subject to the constraint . 

We start by considering the simpler constraint that $\mathcal{B}$ lies in one halfspace $a_{i}^{T}x\leq b_{i}$ , i.e. , 

$$
\|u\|_{2}\leq r\implies a_{i}^{T}(x_{c}+u)\leq b_{i}.
$$ 

Since 

$$
\operatorname*{sup}\{a_{i}^{T}u\mid\|u\|_{2}\leq r\}=r\|a_{i}\|_{2}
$$ 

we can write ( 4.30 ) as 

$$
a_{i}^{T}x_{c}+r||a_{i}||_{2}\leq b_{i},
$$ 

which is a linear inequality in $x_{c}$ and $r$ . In other words, the constraint that the ball lies in the halfspace determined by the inequality $a_{i}^{T}x\leq b_{i}$ ≤ can be written as a linear inequality. 

Therefore ${\mathcal{B}}\subseteq{\mathcal{P}}$ if and only if ( 4.31 ) holds for all $i\,=\,1,.\,.\,.\,,m$ . Hence the Chebyshev center can be determined by solving the LP 

$$
\begin{array}{l l}{\mathrm{maximize}}&{r}\\ {\mathrm{subject~to}}&{a_{i}^{T}x_{c}+r\|a_{i}\|_{2}\leq b_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

with variables $r$ and $x_{c}$ . (For more on the Chebyshev center, see 8.5.1 .) 

# Dynamic activity planning 

We consider the problem of choosing, or planning, the activity levels of $n$ activities, or sectors of an economy, over $N$ time period . We let $x_{j}(t)\,\geq\,0$ , $t\,=\,1,.\,.\,.\,,N$ , denote the activity level of sector $j$ , in period t . The activities both consume and produce products or goods in proportion to their activity levels. The amount of good $i$ produced per unit of activity $j$ is given by . Similarly, the amount of good $i$ $a_{i j}$ consumed per unit of activity $j$ is $b_{i j}$ . The total amount of goods produced in period $t$ is given by $A x(t)\,\in\,\mathbf{R}^{m}$ , and the amount of goods consumed is $B x(t)\,\in\,\mathbf{R}^{m}$ . (Although we refer to these products as ‘goods’, they can also include unwanted products such as pollutants.) 

The goods consumed in a period cannot exceed those produced in the previous period: we must have $B x(t+1)\preceq A x(t)$ for $t=1,\cdot\cdot\cdot,N$ . A vector $g_{0}\in\mathbf{R}^{m}$ f initial goods is given, which constrains the first period activity levels: $B x(1)\preceq g_{0}$ ⪯ . The (vectors of) excess goods not consumed by the activities are given by 

$$
\begin{array}{r c l}{s(0)}&{=}&{g_{0}-B x(1)}\\ {s(t)}&{=}&{A x(t)-B x(t+1),\quad t=1,\ldots,N-1}\\ {s(N)}&{=}&{A x(N).}\end{array}
$$ 

The objective is to maximize a discounted total value of excess goods: 

$$
c^{T}s(0)+\gamma c^{T}s(1)+\cdot\cdot\cdot+\gamma^{N}c^{T}s(N),
$$ 

where $c\in\mathbf{R}^{m}$ gives the values of the goods, and $\gamma>0$ is a discount factor. (The value $c_{i}$ is negative if the $\imath$ th product is unwanted, e.g. , a pollutant; $|c_{i}|$ is then the cost of disposal per unit.) 

Putting it all together we arrive at the LP 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\ c^{T}s(0)+\gamma c^{T}s(1)+\cdot\cdot\cdot+\gamma^{N}c^{T}s(N)}\\ {{\mathrm{subject~to}}}&{x(t)\succeq0,\quad t=1,\ldots,N}\\ &{s(t)\succeq0,\quad t=0,\ldots,N}\\ &{s(0)=g_{0}-B x(1)}\\ &{s(t)=A x(t)-B x(t+1),\quad t=1,\ldots,N-1}\\ &{s(N)=A x(N),}\end{array}}
$$ 

with variables $x(1),\ldots,x(N)$ , $s(0),\ldots,s(N)$ . This problem is a standard form LP; the variables $s(t)$ are the slack variables associated with the constraints $B x(t\!+\!1)\preceq$ $A x(t)$ . 

# Chebyshev inequalities 

We consider a probability distribution for a discrete random variable $x$ on a set $\{u_{1},.\,.\,.\,,u_{n}\}\subseteq\mathbf{R}$ with $n$ elements. We describe the distribution of $x$ by a vector $p\in\mathbf{R}^{n}$ , where 

$$
p_{i}=\mathbf{prob}(x=u_{i}),
$$ 

so $p$ satisfies $p\succeq0$ and ${\bf1}^{T}p=1$ . Conversely, if $p$ satisfies $p\succeq0$ and ${\bf1}^{T}p=1$ , then it defines a probability distribution for $x$ . We assume that $u_{i}$ are known and fixed, but the distribution $p$ is not known. 

If $f$ is any function of $x$ , then 

$$
\mathbf{E}\,f=\sum_{i=1}^{n}p_{i}f(u_{i})
$$ 

is a linear function of $p$ . If $S$ is any subset of $\mathbf{R}$ , then 

$$
\mathbf{prob}(x\in S)=\sum_{u_{i}\in S}p_{i}
$$ 

is a linear function of $p$ . 

Although we do not know $p$ , we are given prior knowledge of the following form: We know upper and lower bounds on expected values of some functions of $x$ , and probabilities of some subsets of $\mathbf{R}$ . This prior knowledge can be expressed as linear inequality constraints on $p$ , 

$$
\alpha_{i}\leq a_{i}^{T}p\leq\beta_{i},\quad i=1,.\,.\,.\,,m.
$$ 

The problem is to give lower and upper bounds on $\mathbf{E}\,f_{0}(x)=a_{0}^{T}p$ , where $f_{0}$ is some function of $x$ . 

To find a lower bound we solve the LP 

$$
\begin{array}{l l}{\mathrm{minimize}}&{a_{0}^{T}p}\\ {\mathrm{subject~to}}&{p\succeq0,\quad\mathbf{1}^{T}p=1}\\ &{\alpha_{i}\leq a_{i}^{T}p\leq\beta_{i},\quad i=1,\ldots,m,}\end{array}
$$ 

with variable $p$ . The optimal value of this LP gives the lowest possible value of $\mathbf{E}\,f_{0}(X)$ for any distribution that is consistent with the prior information. More- over, the bound is sharp: the optimal solution gives a distribution that is consistent with the prior information and achieves the lower bound. In a similar way, we can find the best upper bound by maximizing $a_{0}^{T}p$ subject to the same constraints. (We will consider Chebyshev inequalities in more detail in 7.4.1 .) 

# Piecewise-linear minimization 

Consider the (unconstrained) problem of minimizing the piecewise-linear, convex function 

$$
f(x)=\operatorname*{max}_{i=1,\ldots,m}(a_{i}^{T}x+b_{i}).
$$ 

This problem can be transformed to an equivalent LP by first forming the epigraph problem, 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{t}\\ {{\mathrm{subject~to}}}&{\operatorname*{max}_{i=1,\dots,m}(a_{i}^{T}x+b_{i})\leq t,}\end{array}}
$$ 

and then expressing the inequality as a set of $m$ separate inequalities: 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{t}\\ {{\mathrm{subject~to}}}&{a_{i}^{T}x+b_{i}\leq t,\quad i=1,.\,.\,,m.}\end{array}}
$$ 

This is an LP (in inequality form), with variables $x$ and $t$ . 

# 4.3.2 Linear-fractional programming 

The problem of minimizing a ratio of affine functions over a polyhedron is called a linear-fractional program : 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(x)}}\\ {{\mathrm{subject~to}}}&{{G x\preceq h}}\\ {{}}&{{A x=b}}\end{array}
$$ 

where the objective function is given by 

$$
f_{0}(x)=\frac{c^{T}x+d}{e^{T}x+f},\qquad\mathrm{dom}\ f_{0}=\{x\mid e^{T}x+f>0\}.
$$ 

The objective function is quasiconvex (in fact, quasilinear) so linear-fractional pro- grams are quasiconvex optimization problems. 

# Transforming to a linear program 

If the feasible set 

$$
\{x\mid G x\preceq h,\;A x=b,\;e^{T}x+f>0\}
$$ 

is nonempty, the linear-fractional program ( 4.32 ) can be transformed to an equiv- alent linear program 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{c^{T}y+d z}\\ {{\mathrm{subject~to}}\ }&{G y-h z\preceq0}\\ &{A y-b z=0}\\ &{e^{T}y+f z=1}\\ &{z\geq0}\end{array}}
$$ 

with variables $y$ , $\mathcal{Z}$ . 

To show the equivalence, we first note that if $x$ is feasible in ( 4.32 ) then the pair 

$$
y=\frac{x}{e^{T}x+f},\qquad z=\frac{1}{e^{T}x+f}
$$ 

is feasible in ( 4.33 ), with the same objective value $c^{T}y+d z=f_{0}(x)$ . It follows that the optimal value of ( 4.32 ) is greater than or equal to the optimal value of ( 4.33 ). 

Conversely, if $(y,z)$ is feasible in ( 4.33 ), with $z\neq0$ , then $x\,=\,y/z$ is feasible in ( 4.32 ), with the same objective value $f_{0}(x)\,=\,c^{T}y+d z$ . If $(y,z)$ is feasible in ( 4.33 ) with $z\,=\,0$ , and $x_{0}$ is feasible for ( 4.32 ), then $~x\,=\,x_{0}\,+\,t y$ is feasible in ( 4.32 ) for all $t\geq0$ . Moreover, $\begin{array}{r}{\operatorname*{lim}_{t\rightarrow\infty}f_{0}(x_{0}+t y)=c^{T}y+d z}\end{array}$ , so we can find feasible points in ( 4.32 ) with objective values arbitrarily close to the objective value of $(y,z)$ . We conclude that the optimal value of ( 4.32 ) is less than or equal to the optimal value of ( 4.33 ). 

# Generalized linear-fractional programming 

A generalization of the linear-fractional program ( 4.32 ) is the generalized linear- fractional program in which 

$$
f_{0}(x)=\operatorname*{max}_{i=1,\ldots,r}{\frac{c_{i}^{T}x+d_{i}}{e_{i}^{T}x+f_{i}}},\qquad\mathrm{dom}\ f_{0}=\{x\mid e_{i}^{T}x+f_{i}>0,\,i=1,\ldots,r\}.
$$ 

The objective function is the pointwise maximum of $r$ quasiconvex functions, and therefore quasiconvex, so this problem is quasiconvex. When $r=1$ it reduces to the standard linear-fractional program. 

Example 4.7 Von Neumann growth problem. We consider an economy with $n$ sectors, and activity levels $x_{i}>0$ in the current period, and activity levels $x_{i}^{+}>0$ 0 in the next period. (In this problem we only consider one period.) There are $_{m}$ goods which are consumed, and also produced, by the activity: An activity level $x$ consumes goods $B x\,\in\,\mathbf{R}^{m}$ , and produces goods $A x$ . The goods consumed in the next period cannot exceed the goods produced in the current period, i.e. , $B x^{+}\preceq A x$ . The growth rate in sector $i$ , over the period, is given by $x_{i}^{+}/x_{i}$ . 

Von Neumann’s growth problem is to find an activity level vector $x$ that maximizes the minimum growth rate across all sectors of the economy. This problem can be expressed as a generalized linear-fractional problem 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\operatorname*{min}_{i=1,\dots,n}x_{i}^{+}/x_{i}}\\ {{\mathrm{subject~to}}}&{x^{+}\succeq0}\\ &{B x^{+}\preceq A x}\end{array}}
$$ 

with domain $\{(x,x^{+})\mid x\succ0\}$ . Note that this problem is homogeneous in $x$ and $x^{+}$ , so we can replace the implicit constraint $x\succ0$ by the explicit constraint $x\succeq\mathbf{1}$ . 

# 4.4 Quadratic optimization problems 

The convex optimization problem ( 4.15 ) is called a quadratic program (QP) if the objective function is (convex) quadratic, and the constraint functions are affine. A quadratic program can be expressed in the form 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{(1/2)x^{T}P x+q^{T}x+r}\\ {\operatorname{subject\to}}&{G x\preceq h}\\ &{A x=b,}\end{array}}
$$ 

where $P\in\mathbf{S}_{+}^{n}$ , $G\in\mathbf{R}^{m\times n}$ , and $A\in\mathbf{R}^{p\times n}$ . In a quadratic program, we minimize a convex quadratic function over a polyhedron, as illustrated in figure 4.5 . 

If the objective in ( 4.15 ) as well as the inequality constraint functions are (con- vex) quadratic, as in 

$$
\begin{array}{l l}{\mathrm{minimize}}&{(1/2)x^{T}P_{0}x+q_{0}^{T}x+r_{0}}\\ {\mathrm{subject~to}}&{(1/2)x^{T}P_{i}x+q_{i}^{T}x+r_{i}\le0,\quad i=1,\ldots,m}\\ &{A x=b,}\end{array}
$$ 

![](images/315ab6809d797e26c5c7486d05a11ae6073468dfc1f258101fd0e5bf0623fa79.jpg) 
Figure 4.5 Geometric illustration of QP. The feasible set $\mathcal{P}$ , which is a poly- hedron, is shown shaded. The contour lines of the objective function, which is convex quadratic, are shown as dashed curves. The point $x^{\star}$ is optimal. 

where $P_{i}\,\in\,{\bf S}_{+}^{n}$ , $i\,=\,0,1\,.\,.\,.\,,m$ , the problem is called a quadratically constrained quadratic program (QCQP). In a QCQP, we minimize a convex quadratic function over a feasible region that is the intersection of ellipsoids (when $P_{i}\succ0$ ). 

Quadratic programs include linear programs as a special case, by taking $P=0$ in ( 4.34 ). Quadratically constrained quadratic programs include quadratic pro- grams (and therefore also linear programs) as a special case, by taking $P_{i}\,=\,0$ in ( 4.35 ), for $i=1,\ldots,m$ . 

# 4.4.1 Examples 

# Least-squares and regression 

The problem of minimizing the convex quadratic function 

$$
\|A x-b\|_{2}^{2}=x^{T}A^{T}A x-2b^{T}A x+b^{T}b
$$ 

is an (unconstrained) QP. It arises in many fields and has many names, e.g. , re- gression analysis or least-squares approximation . This problem is simple enough to have the well known analytical solution $x=A^{\dagger}b$ , where $A^{\dagger}$ is the pseudo-inverse of $A$ (see A.5.4 ). 

When linear inequality constraints are added, the problem is called constrained regression or constrained least-squares , and there is no longer a simple analytical solution. As an example we can consider regression with lower and upper bounds on the variables, i.e. , 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\|A x-b\|_{2}^{2}}}\\ {{\mathrm{subject~to}}}&{{l_{i}\leq x_{i}\leq u_{i},}}&{{i=1,.\,.\,,n,}}\end{array}
$$ 

which is a QP. (We will study least-squares and regression problems in far more depth in chapters 6 and 7 .) 

# Distance between polyhedra 

The (Euclidean) d ance between the polyhedra ${\mathcal{P}}_{1}=\{x\mid A_{1}x\preceq b_{1}\}$ and $\mathcal{P}_{2}=$ $\{x\mid A_{2}x\preceq b_{2}\}$ in R $\mathbf{R}^{n}$ is defined as 

$$
\mathbf{dist}({\mathcal{P}}_{1},{\mathcal{P}}_{2})=\operatorname*{inf}\{\|x_{1}-x_{2}\|_{2}\mid x_{1}\in{\mathcal{P}}_{1},\ x_{2}\in{\mathcal{P}}_{2}\}.
$$ 

If the polyhedra intersect, the distance is zero. To find the distance between $\mathcal{P}_{1}$ and $\mathcal{P}_{2}$ , we can solve the QP 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|x_{1}-x_{2}\|_{2}^{2}}\\ {\mathrm{subject~to}}&{A_{1}x_{1}\preceq b_{1},\quad A_{2}x_{2}\preceq b_{2},}\end{array}
$$ 

with variables $x_{1}$ $\mathbf{\Sigma}_{1},\ \boldsymbol{x}_{2}\,\in\,\mathbf{R}^{\,n}$ . This problem is infeasible if and only if one of the polyhedra is empty. The optimal value is zero if and only if the polyhedra intersect, in which case the optimal $x_{1}$ and $x_{2}$ are equal (and is a point in the intersection $\mathcal{P}_{1}\cap\mathcal{P}_{2}$ ). Otherwise the optimal $x_{1}$ and $x_{2}$ are the points in $\mathcal{P}_{1}$ and $\mathcal{P}_{2}$ , respectively, that are closest to each other. (We will study geometric problems involving distance in more detail in chapter 8 .) 

# Bounding variance 

We consider again the Chebyshev inequalities example (page 150 ), where the vari- able is an unknown probability distribution given by $p\in\mathbf{R}^{n}$ , about which we have some prior information. The variance of a random variable $f(x)$ is given by 

$$
\mathbf{E}\,f^{2}-(\mathbf{E}\,f)^{2}=\sum_{i=1}^{n}f_{i}^{2}p_{i}-\left(\sum_{i=1}^{n}f_{i}p_{i}\right)^{2},
$$ 

(where $f_{i}=f(u_{i}).$ ), which is a concave quadratic function of $p$ . 

It follows that we can maximize the variance of $f(x)$ , subject to the given prior information, by solving the QP 

$$
\begin{array}{r l}{\mathrm{maximize}}&{\sum_{i=1}^{n}f_{i}^{2}p_{i}-\left(\sum_{i=1}^{n}f_{i}p_{i}\right)^{2}}\\ {\mathrm{subject~to}}&{p\succeq0,\quad\mathbf{1}^{T}p=1}\\ &{\alpha_{i}\leq a_{i}^{T}p\leq\beta_{i},\quad i=1,.\,.\,,m.}\end{array}
$$ 

The optimal value gives the maximum possible variance of $f(x)$ , over all distribu- tions that are consistent with the prior information; the optimal $p$ gives a distri- bution that achieves this maximum variance. 

# Linear program with random cost 

We consider an LP, 

$$
{\begin{array}{r l}{\operatorname{minimize}\ }&{c^{T}x}\\ {{\mathrm{subject~to}}}&{G x\preceq h}\\ &{A x=b,}\end{array}}
$$ 

with variable $x\,\in\,\mathbf{R}^{\,n}$ . We suppose that the cost function (vector) $c\,\in\,\mathbf{R}^{n}$ is random , with mean value $\overline{c}$ and covariance ${\bf E}(c-\overline{{c}})(c-\overline{{c}})^{T}\:=\:\Sigma$ . (We assume for simplicity that the other problem parameters are deterministic.) For a given $x\in\mathbf{R}^{n}$ , the cost $c^{T}x$ is a (scalar) random variable with mean $\mathbf{E}\,c^{T}x\,=\,{\overline{{c}}}^{T}x$ and variance 

$$
\mathbf{var}(c^{T}x)=\mathbf{E}(c^{T}x-\mathbf{E}\,c^{T}x)^{2}=x^{T}\Sigma x.
$$ 

In general there is a trade-oﬀbetween small expected cost and small cost vari- ance. One way to take variance into account is to minimize a linear combination of the expected value and the variance of the cost, i.e. , 

$$
\mathbf{E}\,c^{T}x+\gamma\,\mathbf{var}(c^{T}x),
$$ 

which is called the risk-sensitive cost . The parameter $\gamma\geq0$ is called the risk- aversion parameter , since it sets the relative values of cost variance and expected value. (For $\gamma\,>\,0$ , we are willing to trade oﬀan increase in expected cost for a sufficiently large decrease in cost variance). 

To minimize the risk-sensitive cost we solve the QP 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{{\overline{{c}}}^{T}{\boldsymbol{x}}+\gamma{\boldsymbol{x}}^{T}\Sigma{\boldsymbol{x}}}\\ {\operatorname{subject\to}}&{G{\boldsymbol{x}}\preceq h}\\ &{A{\boldsymbol{x}}=b.}\end{array}}
$$ 

# Markowitz portfolio optimization 

We consider a classical portfolio problem with $n$ assets or stocks held over a period of time. We let $x_{i}$ denote the amount of asset $i$ held throughout the period, with $x_{i}$ in dollars, at the price at the beginning of the period. A normal long position in asset $i$ corresponds to $x_{i}>0$ ; a short position in asset $i$ ( i.e. , the obligation to buy the asset at the end of the period) corresponds to $x_{i}<0$ . We let $p_{i}$ denote the relative price change of asset $i$ over the period, i.e. , its change in price over the period divided by its price at the beginning of the period. The overall return on the portfolio is ${\boldsymbol{r}}\,=\,{\boldsymbol{p}}^{T}{\boldsymbol{x}}$ (given in dollars). The optimization variable is the portfolio vector $x\in\mathbf{R}^{n}$ . 

A wide variety of constraints on the portfolio can be considered. The simplest set of constraints is that $x_{i}\geq0$ ( i.e. , no short positions) and $\mathbf{1}^{T}x=B$ ( i.e. , the total budget to be invested is B , which is often taken to be one). 

We take a stochastic model r price changes: $p\in\mathbf{R}^{n}$ is m vector, with known mean $\overline{{p}}$ and covariance Σ. Therefore with portfolio $x\,\in\,\mathbf{R}^{n}$ , the return $r$ is a (scalar) random variable with mean $\overline{{p}}^{T}x$ and variance x $x^{T}\Sigma{\mathit{x}}$ . The choice of portfolio $x$ involves a trade-oﬀbetween the mean of the return, and its variance. 

The classical portfolio optimization problem, introduced by Markowitz, is the QP 

$$
\begin{array}{l r c l}{\mathrm{minimize}}&{x^{T}\Sigma x}\\ {\mathrm{subject~to}}&{\overline{{p}}^{T}x\geq r_{\mathrm{min}}}\\ &{\mathbf{1}^{T}x=1,\quad x\succeq0,}\end{array}
$$ 

where $x$ , the portfolio, is the variable. Here we find the portfolio that minimizes the return variance (which is associated with the $r i s k$ of the portfolio) subject to achieving a minimum acceptable mean return $r_{\mathrm{min}}$ , and satisfying the portfolio budget and no-shorting constraints. 

Many extensions are possible. One standard extension, for example, is to allow short positions, i.e. , $x_{i}<\,0$ . To do this we introduce variables and , $x_{\mathrm{long}}$ $x_{\mathrm{short}}$ with 

$$
x_{\mathrm{long}}\succeq0,\qquad x_{\mathrm{short}}\succeq0,\qquad x=x_{\mathrm{long}}-x_{\mathrm{short}},\qquad{\mathbf{1}}^{T}x_{\mathrm{short}}\leq\eta{\mathbf{1}}^{T}x_{\mathrm{long}}.
$$ 

The last constraint limits the total short position at the beginning of the period to some fraction $\eta$ of the total long position at the beginning of the period. 

As another extension we can include linear transaction costs in the portfolio optimization problem. Starting from a given initial portfolio $x_{\mathrm{init}}$ we buy and sell assets to achieve the portfolio $x$ , which we then hold over the period as described above. We are charged a transaction fee for buying and selling assets, which is proportional to the amount bought or sold. To handle this, we introduce variables $u_{\mathrm{buy}}$ and $u_{\mathrm{sell}}$ , which determine the amount of each asset we buy and sell before the holding period. We have the constraints 

$$
x=x_{\mathrm{init}}+u_{\mathrm{bary}}-u_{\mathrm{sell}},\qquad u_{\mathrm{bby}}\succeq0,\qquad u_{\mathrm{sell}}\succeq0.
$$ 

We replace the simple budget constraint ${\bf1}^{T}x=1$ with the condition that the initial buying and selling, including transaction fees, involves zero net cash: 

$$
(1-f_{\mathrm{sell}})\mathbf{1}^{T}u_{\mathrm{sell}}=(1+f_{\mathrm{buy}})\mathbf{1}^{T}u_{\mathrm{buy}}
$$ 

Here the lefthand side is the total proceeds from selling assets, less the selling transaction fee, and the righthand side is the total cost, including transaction fee, of buying assets. The constants $f_{\mathrm{buy}}\geq0$ and $f_{\mathrm{sell}}\geq0$ are the transaction fee rates for buying and selling (assumed the same across assets, for simplicity). 

The problem of minimizing return variance, subject to a minimum mean return, and the budget and trading constraints, is a QP with variables $x,~u_{\mathrm{buy}},~u_{\mathrm{sell}}$ . 

# 4.4.2 Second-order cone programming 

A problem that is closely related to quadratic programming is the second-order cone program (SOCP): 

$$
\begin{array}{l r}{\mathrm{minimize}\ }&{f^{T}x}\\ {\mathrm{subject~to}\ }&{\|A_{i}x+b_{i}\|_{2}\le c_{i}^{T}x+d_{i},\quad i=1,.\,.\,,m}\\ &{F x=g,}\end{array}
$$ 

where $x\in\mathbf{R}^{n}$ is the optimization variable, $A_{i}\in\mathbf{R}^{n_{i}\times n}$ , and $F\in\mathbf{R}^{p\times n}$ . We call a constraint of the form 

$$
\|A x+b\|_{2}\leq c^{T}x+d,
$$ 

where $A\in\mathbf{R}^{k\times n}$ , a second-order cone constraint , since it is the same uiring the affine function $(A x+b,c^{T}x+d)$ to lie in the second-order cone in R $\mathbf{R}^{k+1}$ . 

When $c_{i}=0$ , $i=1,\cdot\cdot\cdot,m$ , the SOCP ( 4.36 ) is equivalent to a QCQP (which is obtained by squaring each of the constraints). Similarly, if $A_{i}=0$ , $i=1,\ldots,m$ , then the SOCP ( 4.36 ) reduces to a (general) LP. Second-order cone programs are, however, more general than QCQPs (and of course, LPs). 

# Robust linear programming 

We consider a linear program in inequality form, 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{c^{T}x}}\\ {{\mathrm{subject~to}}}&{{a_{i}^{T}x\leq b_{i},\quad i=1,.\,.\,,m,}}\end{array}
$$ 

in which there is some uncertainty or variation in the parameters $c$ , $a_{i}$ , $b_{i}$ . To simplify the exposition we assume that $c$ and $b_{i}$ are fixed, and that $a_{i}$ are known to lie in given ellipsoids: 

$$
a_{i}\in\mathcal{E}_{i}=\{\overline{{a}}_{i}+P_{i}u\mid\|u\|_{2}\leq1\},
$$ 

$P_{i}\in\mathbf{R}^{n\times n}$ . (If $P_{i}$ is singular we obtain ‘ﬂat’ ellipsoids, of dimension rank $P_{i}$ ; $P_{i}=0$ = 0 means that $a_{i}$ is known perfectly.) 

We will require that the constraints be satisfied for all possible values of the parameters $a_{i}$ , which leads us to the robust linear program 

$$
\begin{array}{l r}{\mathrm{minimize}\ }&{c^{T}x}\\ {\mathrm{subject~to}\ }&{a_{i}^{T}x\leq b_{i}\ \mathrm{for~all}\ a_{i}\in{\mathcal E}_{i},\quad i=1,.\,.\,,m.}\end{array}
$$ 

The robust linear constraint, $a_{i}^{T}x\leq b_{i}$ ≤ for all $a_{i}\in\mathcal{E}_{i}$ , can be expressed as 

$$
\operatorname*{sup}\{a_{i}^{T}x\mid a_{i}\in{\mathcal{E}}_{i}\}\leq b_{i},
$$ 

the lefthand side of which can be expressed as 

$$
\begin{array}{l l l l}{\operatorname*{sup}\{a_{i}^{T}x\mid a_{i}\in{\mathcal E}_{i}\}}&{=}&{\overline{{a}}_{i}^{T}x+\operatorname*{sup}\{u^{T}P_{i}^{T}x\mid\|u\|_{2}\leq1\}}\\ &{=}&{\overline{{a}}_{i}^{T}x+\|P_{i}^{T}x\|_{2}.}\end{array}
$$ 

Thus, the robust linear constraint can be expressed as 

$$
\overline{{a}}_{i}^{T}\boldsymbol{x}+\|P_{i}^{T}\boldsymbol{x}\|_{2}\leq b_{i},
$$ 

which is evidently a second-order cone constraint. Hence the robust LP ( 4.37 ) can be expressed as the SOCP 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{c^{T}x}}&{{}}\\ {{\mathrm{subject~to}}}&{{\overline{{{a}}}_{i}^{T}x+\|P_{i}^{T}x\|_{2}\leq b_{i},\quad i=1,.\,.\,,m.}}\end{array}
$$ 

Note that the additional norm terms act as regularization terms ; they prevent $x$ from being large in directions with considerable uncertainty in the parameters $a_{i}$ . 

# Linear programming with random constraints 

The robust LP described above can also be considered in a statistical framework. Here we suppose that the parameters $a_{i}$ are independent Gaussian random vectors, with mean $\overline{{a}}_{i}$ and covariance $\Sigma_{i}$ . We require that each constraint $a_{i}^{T}x\leq b_{i}$ ≤ should hold with a probability (or confidence) exceeding $\eta$ , where $\eta\geq0.5$ , i.e. , 

$$
\mathbf{prob}(a_{i}^{T}x\leq b_{i})\geq\eta.
$$ 

We will show that this probability constraint can be expressed as a second-order cone constraint. 

Letting $u=a_{i}^{T}x$ , with $\sigma^{2}$ denoting its variance, this constraint can be written as 

$$
\mathbf{prob}\left(\frac{u-\overline{{u}}}{\sigma}\leq\frac{b_{i}-\overline{{u}}}{\sigma}\right)\geq\eta.
$$ 

Since $(u\mathrm{~-~}\overline{{u}})/\sigma$ is a zero mean unit variance Gaussian variable, the probability above is simply $\Phi((b_{i}-\overline{{u}})/\sigma)$ , where 

$$
\Phi(z)={\frac{1}{\sqrt{2\pi}}}\int_{-\infty}^{z}e^{-t^{2}/2}\;d t
$$ 

is the cumulative distribution function of a zero mean unit variance Gaussian ran- dom variable. Thus the probability constraint ( 4.38 ) can be expressed as 

$$
\frac{b_{i}-\overline{{u}}}{\sigma}\geq\Phi^{-1}(\eta),
$$ 

or, equivalently, 

$$
\overline{{u}}+\Phi^{-1}(\eta)\sigma\leq b_{i}.
$$ 

From $\overline{{u}}=\overline{{a}}_{i}^{T}x$ and $\sigma=(x^{T}\Sigma_{i}x)^{1/2}$ we obtain 

$$
\overline{{a}}_{i}^{T}x+\Phi^{-1}(\eta)\|\Sigma_{i}^{1/2}x\|_{2}\leq b_{i}.
$$ 

By our assumption that $\eta\,\geq\,1/2$ , we have $\Phi^{-1}(\eta)\,\geq\,0$ , so this constraint is a second-order cone constraint. 

In summary, the problem 

$$
\begin{array}{l r}{\mathrm{minimize}}&{c^{T}x}\\ {\mathrm{subject~to}}&{\mathbf{prob}(a_{i}^{T}x\leq b_{i})\geq\eta,\quad i=1,.\,.\,,m}\end{array}
$$ 

can be expressed as the SOCP 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{c^{T}x}}&{{}}\\ {{\mathrm{subject~to}}}&{{\overline{{{a}}}_{i}^{T}x+\Phi^{-1}(\eta)\|\Sigma_{i}^{1/2}x\|_{2}\leq b_{i},\quad i=1,\ldots,m.}}\end{array}
$$ 

(We will consider robust convex optimization problems in more depth in chapter 6 . See also exercises 4.13 , 4.28 , and 4.59 .) 

Example 4.8 Portfolio optimization with loss risk constraints. We consider again the classical Markowitz portfolio problem described above (page 155 ). We assume here that the price change vector $p\,\in\,\mathbf{R}^{n}$ is a Gaussian random variable, with mean $\overline{{p}}$ and covariance $\Sigma$ . Therefore the return $r$ is a Gaussian random variable with mean $\overline{{r}}=\overline{{p}}^{I}x$ and variance $\sigma_{r}^{2}=x^{T}\Sigma x$ . 

Consider a loss risk constraint of the form 

$$
\mathbf{prob}(r\leq\alpha)\leq\beta,
$$ 

where $\alpha$ is a given unwanted return level ( e.g. , a large loss) and $\beta$ is a given maximum probability. 

As in the stochastic interpretation of the robust LP given above, we can express this constraint using the cumulative distribution function $\Phi$ of a unit Gaussian random variable. The inequality ( 4.39 ) is equivalent to 

$$
\overline{{p}}^{T}x+\Phi^{-1}(\beta)\,\|\Sigma^{1/2}x\|_{2}\geq\alpha.
$$ 

Provided $\beta\leq1/2$ ( i.e. , $\Phi^{-1}(\beta)\leq0$ ), this loss risk constraint is a second-order cone constraint. (If $\beta>1/2$ , the loss risk constraint becomes nonconvex in $x$ .) 

The problem of maximizing the expected return subject to a bound on the loss risk (with $\beta\leq1/2$ ), can therefore be cast as an SOCP with one second-order cone constraint: 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\overline{{p}}^{T}x}\\ {\mathrm{subject~to}}&{\overline{{p}}^{T}x+\Phi^{-1}(\beta)\,\|\Sigma^{1/2}x\|_{2}\geq\alpha}\\ &{x\succeq0,\quad\mathbf{1}^{T}x=1.}\end{array}
$$ 

There are many extensions of this problem. For example, we can impose several loss risk constraints, i.e. , 

$$
\mathbf{prob}(r\leq\alpha_{i})\leq\beta_{i},\quad i=1,.\,.\,.\,,k,
$$ 

(where $\beta_{i}\leq1/2$ ), which expresses the risks ( $\beta_{i}$ ) we are willing to accept for various levels of loss ( $\cdot^{\alpha_{i}}$ ). 

# Minimal surface 

Consider a diﬀerentiable function $f:\mathbf{R}^{2}\rightarrow\mathbf{R}$ with $\mathbf{dom}\,f=C$ . The surface area of its graph is given by 

$$
A=\int_{C}{\sqrt{1+\|\nabla f(x)\|_{2}^{2}}}\ d x=\int_{C}\|(\nabla f(x),1)\|_{2}\ d x,
$$ 

which is a convex functional of $f$ . The minimal surface problem is to find the function $f$ that minimizes $A$ subject to some constraints, for example, some given values of $f$ on the boundary of $C$ . 

We will approximate this problem by discretizing the function $f$ . Let $C\;=$ $[0,1]\times[0,1]$ , and let $f_{i j}$ denote the value of $f$ at the point $(i/K,j/K)$ , for $i,\ j=$ $0,\ldots,K$ . An approximate expression for the gradient of $f$ at the point $x=$ $(i/K,j/K)$ can be found using forward diﬀerences: 

$$
\nabla f(x)\approx K\left[\begin{array}{l}{f_{i+1,j}-f_{i,j}}\\ {f_{i,j+1}-f_{i,j}}\end{array}\right].
$$ 

Substituting this into the expression for the area of the graph, and approximating the integral as a sum, we obtain an approximation for the area of the graph: 

$$
A\approx A_{\mathrm{disc}}={\frac{1}{K^{2}}}\sum_{i,j=0}^{K-1}\left\|{\left[\begin{array}{c}{K(f_{i+1,j}-f_{i,j})}\\ {K(f_{i,j+1}-f_{i,j})}\\ {1}\end{array}\right]}\right\|_{2}
$$ 

The discretized area approximation $A_{\mathrm{disc}}$ is a convex function of $f_{i j}$ 

We can consider a wide variety of constraints on $f_{i j}$ , such as equality or in- equality constraints on any of its entries (for example, on the boundary values), or on its moments. As an example, we consider the problem of finding the minimal area surface with fixed boundary values on the left and right edges of the square: 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{A_{\mathrm{disc}}}\\ {\operatorname{subject\to}\quad}&{f_{0j}=l_{j},\quad j=0,.\,.\,.\,,K}\\ &{f_{K j}=r_{j},\quad j=0,.\,.\,.\,,K}\end{array}}
$$ 

where $f_{i j}$ , $i,j\,=\,0,.\,.\,.\,,K$ , are the variables, and $\mathrm{\Delta}l_{j},\mathrm{\Delta}r_{j}$ are the given boundary values on the left and right sides of the square. 

We can transform the problem ( 4.40 ) into an SOCP by introducing new vari- ables $t_{i j},\,i,\,\,j=0,\ldots,K-1$ : 

$$
\begin{array}{l}{\mathrm{minimize}\quad\;(1/K^{2})\sum_{i,j=0}^{K-1}t_{i j}}\\ {\mathrm{subject~to}\quad\left\|\left[\begin{array}{c}{K(f_{i+1,j}-f_{i,j})}\\ {K(f_{i,j+1}-f_{i,j})}\\ {1}\end{array}\right]\right\|_{2}\leq t_{i j},\quad i,\ j=0,\dots,K-1}\\ {f_{0j}=l_{j},\quad j=0,\dots,K}\\ {f_{K j}=r_{j},\quad j=0,\dots,K.}\end{array}
$$ 

# 4.5 Geometric programming 

In this section we describe a family of optimization problems that are not convex in their natural form. These problems can, however, be transformed to convex op- timization problems, by a change of variables and a transformation of the objective and constraint functions. 

# 4.5.1 Monomials and posynomials 

A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ with $\mathbf{dom}\,f=\mathbf{R}_{++}^{n}$ , defined as 

$$
f(x)=c x_{1}^{a_{1}}x_{2}^{a_{2}}\cdot\cdot\cdot x_{n}^{a_{n}},
$$ 

where $c>0$ and $a_{i}\in\mathbf{R}$ , is called a monomial function , or simply, a monomial . The exponents $a_{i}$ of a monomial can be any real numbers, including fractional or negative, but the coefficient $c$ can only be positive. (The term ‘monomial’ conﬂicts with the standard definition from algebra, in which the exponents must be non- negative integers, but this should not cause any confusion.) A sum of monomials, $i$ .e. , a function of the form 

$$
f(x)=\sum_{k=1}^{K}c_{k}x_{1}^{a_{1k}}x_{2}^{a_{2k}}\cdot\cdot\cdot x_{n}^{a_{n k}},
$$ 

where $c_{k}>0$ , is called a posynomial function (with $K$ terms), or simply, a posyn- omial . 

Posynomials are closed under addition, multiplication, and nonnegative scal- ing. Monomials are closed under multiplication and division. If a posynomial is multiplied by a monomial, the result is a posynomial; similarly, a posynomial can be divided by a monomial, with the result a posynomial. 

# 4.5.2 Geometric programming 

An optimization problem of the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\leq1,\quad i=1,\ldots,m}\\ &{h_{i}(x)=1,\quad i=1,\ldots,p}\end{array}}
$$ 

where $f_{0},\ldots,f_{m}$ are posynomials and $h_{1},.\,.\,.\,,h_{p}$ are monomials, is called a geomet- ric program (GP). The domain of this problem is $\mathcal{D}=\mathbf{R}_{++}^{n}$ ; the constraint $x\succ0$ is implicit. 

# Extensions of geometric programming 

Several extensions are readily handled. If $f$ is a posynomial and $h$ is a monomial, then the constraint $f(x)\leq h(x)$ can be handled by expressing it as $f(x)/h(x)\leq1$ (since $f/h$ is posynomial). This includes as a special case a constraint of the form $f(x)\leq a$ , where $f$ is posynomial and $a>0$ . In a similar way if $h_{1}$ and $h_{2}$ are both nonzero monomial functions, then we can handle the equality constraint $h_{1}(x)=h_{2}(x)$ by expressing it as $h_{1}(x)/h_{2}(x)=1$ (since $h_{1}/h_{2}$ is monomial). We can maximize a nonzero monomial objective function, by minimizing its inverse (which is also a monomial). 

For example, consider the problem 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{x/y}\\ {{\mathrm{subject~to}}}&{2\leq x\leq3}\\ &{x^{2}+3y/z\leq{\sqrt{y}}}\\ &{x/y=z^{2},}\end{array}}
$$ 

with variables $x,\ y,\ z\ \in\textbf{R}$ (and the implicit constraint $x,\;\;y,\;\;z\;>\;0,$ ). Using the simple transformations described above, we obtain the equivalent standard form GP 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{x^{-1}y}\\ {{\mathrm{subject~to}}}&{2x^{-1}\leq1,\quad(1/3)x\leq1}\\ &{x^{2}y^{-1/2}+3y^{1/2}z^{-1}\leq1}\\ &{x y^{-1}z^{-2}=1.}\end{array}}
$$ 

We will refer to a problem like this one, that is easily transformed to an equiva- lent GP in the standard form ( 4.43 ), also as a GP. (In the same way that we refer to a problem easily transformed to an LP as an LP.) 

# 4.5.3 Geometric program in convex form 

Geometric programs are not (in general) convex optimization problems, but they can be transformed to convex problems by a change of variables and a transforma- tion of the objective and constraint functions. 

We will use the variables defined as $y_{i}=\log x_{i}$ , so $x_{i}=e^{y_{i}}$ . If $f$ is the monomial function of $x$ given in ( 4.41 ), i.e. , 

$$
f(x)=c x_{1}^{a_{1}}x_{2}^{a_{2}}\cdot\cdot\cdot x_{n}^{a_{n}},
$$ 

then 

$$
\begin{array}{l c l}{f(x)}&{=}&{f\bigl(e^{y_{1}},.\,.\,,e^{y_{n}}\bigr)}\\ &{=}&{c\bigl(e^{y_{1}}\bigr)^{a_{1}}\cdot\cdot\cdot\bigl(e^{y_{n}}\bigr)^{a_{n}}}\\ &{=}&{e^{a^{T}y+b},}\end{array}
$$ 

where $b\,=\,\log c$ . The change of variables $y_{i}=\log x_{i}$ turns a monomial function into the exponential of an affine function. 

Similarly, if $f$ is the posynomial given by ( 4.42 ), i.e. , 

$$
f(x)=\sum_{k=1}^{K}c_{k}x_{1}^{a_{1k}}x_{2}^{a_{2k}}\cdot\cdot\cdot x_{n}^{a_{n k}},
$$ 

then 

$$
f(x)=\sum_{k=1}^{K}e^{a_{k}^{T}y+b_{k}},
$$ 

where $a_{k}=(a_{1k},.\,.\,.\,,a_{n k})$ and $b_{k}=\log c_{k}$ . After the change of variables, a posyn- omial becomes a sum of exponentials of affine functions. 

The geometric program ( 4.43 ) can be expressed in terms of the new variable $y$ as 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\sum_{k=1}^{K_{0}}e^{a_{0k}^{T}y+b_{0k}}}\\ {\mathrm{subject~to}}&{\sum_{k=1}^{K_{i}}e^{a_{i k}^{T}y+b_{i k}}\leq1,\quad i=1,\ldots,m}\\ &{e^{g_{i}^{T}y+h_{i}}=1,\quad i=1,\ldots,p,}\end{array}
$$ 

where $\boldsymbol{a}_{i k}\in\mathbf{R}^{n}$ , $i=0,\ldots,m$ , contain the exponents of the posynomial inequality n constraints, and g $g_{i}\,\in\,\mathbf{R}^{\,n}$ ∈ , $i\,=\,1,\dots,p$ , contain the exponents of the monomial equality constraints of the original geometric program. 

Now we transform the objective and constraint functions, by taking the loga- rithm. This results in the problem 

$$
\begin{array}{l l}{\mathrm{minimize~}}&{\tilde{f}_{0}(y)=\log\left(\sum_{k=1}^{K_{0}}e^{a_{0k}^{T}y+b_{0k}}\right)}\\ {\mathrm{subject~to}}&{\tilde{f}_{i}(y)=\log\left(\sum_{k=1}^{K_{i}}e^{a_{i k}^{T}y+b_{i k}}\right)\leq0,\quad i=1,\ldots,m}\\ &{\tilde{h}_{i}(y)=g_{i}^{T}y+h_{i}=0,\quad i=1,\ldots,p.}\end{array}
$$ 

Since the functions $\tilde{f}_{i}$ are convex, and $\ddot{h}_{i}$ are affine, this problem is a convex optimization problem. We refer to it as a geometric program in convex form . To distinguish it from the original geometric program, we refer to ( 4.43 ) as a geometric program in posynomial form . 

Note that the transformation between the posynomial form geometric pro- gram ( 4.43 ) and the convex form geometric program ( 4.44 ) does not involve any computation; the problem data for the two problems are the same. It simply changes the form of the objective and constraint functions. 

If the posynomial objective and constraint functions all have only one term, i.e. , are monomials, then the convex form geometric program ( 4.44 ) reduces to a (general) linear program. We can therefore consider geometric programming to be a generalization, or extension, of linear programming. 

# 4.5.4 Examples 

# Frobenius norm diagonal scaling 

Consider a matrix $M\,\in\,\mathbf{R}^{n\times n}$ , and the associated linear function that $u$ into $y=M u$ . Suppose we scale the coordinates, i.e. , change variables to ˜ $\tilde{u}=D u$ , $\tilde{y}\,=\,D y$ , where $D$ is diagonal, with $D_{i i}\,>\,0$ . In the new coordinates the linear function is given by $\tilde{y}=D M D^{-1}\tilde{u}$ . 

Now suppose we want to choose the scaling in such a way that the resulting matrix, $D M D^{-1}$ , is small. We will use the Frobenius norm (squared) to measure the size of the matrix: 

$$
\begin{array}{l l l}{\displaystyle||D M D^{-1}||_{F}^{2}}&{=}&{\mathbf{tr}\left(\left(D M D^{-1}\right)^{T}\left(D M D^{-1}\right)\right)}\\ &{=}&{\displaystyle\sum_{i,j=1}^{n}\left(D M D^{-1}\right)_{i j}^{2}}\\ &{=}&{\displaystyle\sum_{i,j=1}^{n}M_{i j}^{2}d_{i}^{2}/d_{j}^{2},}\end{array}
$$ 

where $D=\mathbf{diag}(d)$ . Since this is a posynomial in $d$ , the problem of choosing the scaling $d$ to minimize the Frobenius norm is an unconstrained geometric program, 

$$
\begin{array}{r}{\mathrm{minimize}\quad\sum_{i,j=1}^{n}M_{i j}^{2}d_{i}^{2}/d_{j}^{2},}\end{array}
$$ 

with variable $d$ . The only exponents in this geometric program are $0$ , $2$ , and $-2$ . 

# Design of a cantilever beam 

We consider the design of a cantilever beam, which consists of $N$ segments, num- bered from right to left as $1,\cdot\cdot\cdot,N$ , as shown in figure 4.6 . Each segment has unit length and a uniform rectangular cross-section with width $w_{i}$ and height $h_{i}$ . A vertical load (force) $F$ is applied at the right end of the beam. This load causes the beam to deﬂect (downward), and induces stress in each segment of the beam. We assume that the deﬂections are small, and that the material is linearly elastic, with Young’s modulus $E$ . 

![](images/45be0ad36d906fe6935c579dfb9b01e5d54ccde7bd00dad07c440c69de768253.jpg) 
Figure 4.6 Segmented cantilever beam with 4 segments. Each segment has unit length and a rectangular profile. A vertical force $F^{\prime}$ is applied at the right end of the beam. 

The design variables in the problem are the widths and heights $h_{i}$ of the $N$ $w_{i}$ segments. We seek to minimize the total volume of the beam (which is proportional to its weight), 

$$
w_{1}h_{1}+\cdot\cdot\cdot+w_{N}h_{N},
$$ 

subject to some design constraints. We impose upper and lower bounds on width and height of the segments, 

$$
v_{i}\le w_{\operatorname*{max}},\quad h_{\operatorname*{min}}\le h_{i}\le h_{\operatorname*{max}},\quad i=1,\ldots,N
$$ 

as well as the aspect ratios, 

$$
S_{\operatorname*{min}}\leq h_{i}/w_{i}\leq S_{\operatorname*{max}}.
$$ 

In addition, we have a limit on the maximum allowable stress in the material, and on the vertical deﬂection at the end of the beam. 

We first consider the maximum stress constraint. The maximum stress in seg- ment $i$ , which we denote $\sigma_{i}$ , is given by $\sigma_{i}=6i F/(w_{i}h_{i}^{2})$ ). We impose the constraints 

$$
\frac{6i F}{w_{i}h_{i}^{2}}\le\sigma_{\mathrm{max}},\quad i=1,.\,.\,.\,,N,
$$ 

to ensure that the stress does not exceed the maximum allowable value $\sigma_{\mathrm{max}}$ any- where in the beam. 

The last constraint is a limit on the vertical deﬂection at the end of the beam, which we will denote $y_{1}$ : 

$$
y_{1}\leq y_{\mathrm{{max}}}.
$$ 

The deﬂection $y_{1}$ can be found by a recursion that involves the deﬂection and slope of the beam segments: 

$$
v_{i}=12(i-1/2){\frac{F}{E w_{i}h_{i}^{3}}}+v_{i+1},\qquad y_{i}=6(i-1/3){\frac{F}{E w_{i}h_{i}^{3}}}+v_{i+1}+y_{i+1},
$$ 

for $i=N,N-1,.\,.\,.\,,1$ , with starting values $v_{N+1}=y_{N+1}=0$ . In this recursion, $y_{i}$ is the deﬂection at the right end of segment i , and $v_{i}$ is the slope at that point. We can use the recursion ( 4.45 ) to show that these deﬂection and slope quantities are in fact posynomial functions of the variables and $h$ . We first note that $w$ $v_{N+1}$ and $y_{N+1}$ are zero, and therefore posynomials. Now assume that $v_{i+1}$ and $y_{i+1}$ are posynomial functions of $w$ and $h$ . The lefthand equation in ( 4.45 ) shows that $v_{i}$ is the sum of a monomial and a posynomial ( i.e. , $v_{i+1}$ ), and therefore is a posynomial. From the righthand equation in ( 4.45 ), we see that the deﬂection $y_{i}$ is the sum of a monomial and two posynomials ( $v_{i+1}$ and $y_{i+1}$ ), and so is a posynomial. In particular, the deﬂection at the end of the beam, $y_{1}$ , is a posynomial. 

The problem is then 

$$
\begin{array}{r l}{\mathrm{minimize}\:\:}&{\sum_{i=1}^{N}w_{i}h_{i}}\\ {\mathrm{subject~to}\:\:}&{w_{\operatorname*{min}}\leq w_{i}\leq w_{\operatorname*{max}},\quad i=1,\ldots,N}\\ &{h_{\operatorname*{min}}\leq h_{i}\leq h_{\operatorname*{max}},\quad i=1,\ldots,N}\\ &{S_{\operatorname*{min}}\leq h_{i}/w_{i}\leq S_{\operatorname*{max}},\quad i=1,\ldots,N}\\ &{6i F/(w_{i}h_{i}^{2})\leq\sigma_{\operatorname*{max}},\quad i=1,\ldots,N}\\ &{y_{1}\leq y_{\operatorname*{max}},}\end{array}
$$ 

with variables $w$ and $h$ . This is a GP, since the objective is a posynomial, and the constraints can all be expressed as posynomial inequalities. (In fact, the con- straints can be all be expressed as monomial inequalities, with the exception of the deﬂection limit, which is a complicated posynomial inequality.) 

When the number of segments $N$ is large, the number of monomial terms ap- pearing in the posynomial $y_{1}$ grows approximately as $N^{2}$ . Another formulation of this problem, explored in exercise 4.31 , is obtained by introducing $v_{1},\dots,v_{N}$ and $y_{1},\dotsc,y_{N}$ as variables, and including a modified version of the recursion as a set of constraints. This formulation avoids this growth in the number of monomial terms. 

# Minimizing spectral radius via Perron-Frobenius theory 

e the matrix $A\in\mathbf{R}^{n\times n}$ is elementwise nonnegat $A_{i j}\geq0$ for $i,j=$ $1,\dots,n$ , and irreducible, which means that the matrix ( $(I+A)^{n-1}$ is elementwise positive. The Perron-Frobenius theorem states that $A$ has a positive real eigenvalue $\lambda_{\mathrm{sf}}$ equal to its spectral radius, i.e. , the largest magnitude of its eigenvalues. The Perron-Frobenius eigenvalue $\lambda_{\mathrm{sf}}$ determines the asymptotic rate of growth or decay of $A^{k}$ , as $k\rightarrow\infty$ ; in fact, th matrix $((1/\lambda_{\mathrm{sf}}){\cal A})^{k}$ converges. Roughly speaking, this means that as $k\rightarrow\infty$ , A $A^{k}$ grows like $\lambda_{\mathrm{sf}}^{k}$ , if $\lambda_{\mathrm{Pf}}\,>\,1$ , or decays like $\lambda_{\mathrm{sf}}^{k}$ , if $\lambda_{\mathrm{gf}}<1$ . 

A basic result in the theory of nonnegative matrices states that the Perron- Frobenius eigenvalue is given by 

$$
\lambda_{\mathrm{sf}}=\operatorname*{inf}\{\lambda\mid A v\preceq\lambda v{\mathrm{~for~some~}}v\succ0\}
$$ 

(and moreover, that the infimum is achieved). The inequality $A v\ \preceq\ \lambda v$ can be expressed as 

$$
\sum_{j=1}^{n}A_{i j}v_{j}/(\lambda v_{i})\le1,\quad i=1,\ldots,n,
$$ 

which is a set of posynomial inequalities in the variables $A_{i j}$ , , and $\lambda$ . Thus, $v_{i}$ the condition that $\lambda_{\mathrm{Pf}}\,\le\,\lambda$ can be expressed as a set of posynomial inequalities in $A$ , $v$ , and $\lambda$ . This allows us to solve some optimization problems involving the Perron-Frobenius eigenvalue using geometric programming. 

Suppose that the entries of the matrix $A$ are posynomial functions of some $x\,\in\,\mathbf{R}^{k}$ underlying variable hi the in ties ( 4.47 ) are posynomial inequalities in the variables x $x\in\mathbf{R}^{k}$ ∈ , v $v\in\mathbf{R}^{n}$ ∈ , and λ $\lambda\in\mathbf{R}$ ∈ R . We consider the problem of choosing $x$ to minimize the Perron-Frobenius eigenvalue (or spectral radius) of $A$ , possibly subject to posynomial inequalities on $x$ , 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\lambda_{\mathrm{pf}}(A(x))}\\ &{\mathrm{subject~to}\quad f_{i}(x)\le1,\quad i=1,.\,.\,,p,}\end{array}
$$ 

where $f_{i}$ are posynomials. Using the characterization above, we can express this problem as the GP 

$$
\begin{array}{r l}{\mathrm{minimize~}\,}&{\lambda}\\ {\mathrm{subject~to}\,}&{\sum_{j=1}^{n}A_{i j}v_{j}/(\lambda v_{i})\le1,\quad i=1,.\,.\,,n}\\ &{f_{i}(x)\le1,\quad i=1,.\,.\,,p,}\end{array}
$$ 

where the variables are $x$ , $v$ , and $\lambda$ . 

As a specific example, we consider a simple model for the population dynamics for a bacterium, with time or period denoted by $t=0,1,2,\ldots.$ , in hours. The vector $p(t)\in\mathbf{R}_{+}^{4}$ characterizes the population age distribution at period $t$ : $p_{1}(t)$ is the total population between 0 and $1$ hours old; $p_{2}(t)$ is the total population between $1$ and 2 hours old; and so on. We (arbitrarily) assume that no bacteria live more than 4 hours. The population propagates in time as $p(t+1)=A p(t)$ , where 

$$
A=\left[\begin{array}{l l l l}{b_{1}}&{b_{2}}&{b_{3}}&{b_{4}}\\ {s_{1}}&{0}&{0}&{0}\\ {0}&{s_{2}}&{0}&{0}\\ {0}&{0}&{s_{3}}&{0}\end{array}\right].
$$ 

Here $b_{i}$ is the birth rate among bacteria in age group $i$ , and $s_{i}$ is the survival rate from age group $i$ into age group $i+1$ . We assume that $b_{i}\,>\,0$ and $0\,<\,s_{i}\,<\,1$ , which implies that the matrix $A$ is irreducible. 

The Perron-Frobenius eigenvalue of $A$ determines the asymptotic growth or decay rate of the population. If $\lambda_{\mathrm{Pf}}\,<\,1$ , the population converges to zero like $\lambda_{\mathrm{sf}}^{t}$ , and so has a half-life of $-1/\log_{2}\lambda_{\mathrm{sf}}$ hours. If $\lambda_{\mathrm{sf}}>1$ the population grows geometrically like $\lambda_{\mathrm{sf}}^{t}$ , with a doubling time of $1/\log_{2}\lambda_{\mathrm{sf}}$ hours. Minimizing the spectral radius of $A$ corresponds to finding the fastest decay rate, or slowest growth rate, for the population. 

As our underlying variables, on which the matrix $A$ depends, we take and , $c_{1}$ $c_{2}$ the concentrations of two chemicals in the environment that aﬀect the birth and survival rates of the bacteria. We model the birth and survival rates as monomial functions of the two concentrations: 

$$
\begin{array}{r l r}{b_{i}}&{=}&{b_{i}^{\mathrm{non}}(c_{1}/c_{1}^{\mathrm{non}})^{\alpha_{i}}(c_{2}/c_{2}^{\mathrm{non}})^{\beta_{i}},\quad i=1,\ldots,4}\\ {s_{i}}&{=}&{s_{i}^{\mathrm{non}}(c_{1}/c_{1}^{\mathrm{non}})^{\gamma_{i}}(c_{2}/c_{2}^{\mathrm{non}})^{\delta_{i}},\quad i=1,\ldots,3.}\end{array}
$$ 

Here, $b_{i}^{\mathrm{norm}}$ is nominal birth rate, $s_{i}^{\mathrm{norm}}$ is nominal survival rate, and $c_{i}^{\mathrm{{non}}}$ is nominal concentration of chemical $i$ . The constants , $\beta_{i}$ , , and $\delta_{i}$ give the eﬀect on the $\alpha_{i}$ $\gamma_{i}$ 

birth and survival rates due to changes in the concentrations of the chemicals away from the nominal values. For example $\alpha_{2}\:=\:-0.3$ and $\gamma_{1}~=~0.5$ means that an increase in concentration of chemical 1, over the nominal concentration, causes a decrease in the birth rate of bacteria that are between 1 and 2 hours old, and an increase in the survival rate of bacteria from 0 to $1$ hours old. 

We assume that the concentrations $c_{1}$ and $c_{2}$ can be independently increased or decreased (say, within a factor of 2), by administering drugs, and pose the problem of finding the drug mix that maximizes the population decay rate ( i.e. , minimizes $\lambda_{\mathrm{pf}}(A))$ . Using the approach described above, this problem can be posed as the GP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\lambda}\\ {{\mathrm{subject~to}}\quad}&{b_{1}v_{1}+b_{2}v_{2}+b_{3}v_{3}+b_{4}v_{4}\leq\lambda v_{1}}\\ &{s_{1}v_{1}\leq\lambda v_{2}}\\ &{s_{2}v_{2}\leq\lambda v_{3}}\\ &{s_{3}v_{3}\leq\lambda v_{4}}\\ &{1/2\leq c_{i}/c_{i}^{\mathrm{nom}}\leq2,\quad i=1,2}\\ &{b_{i}=b_{i}^{\mathrm{nom}}(c_{1}/c_{1}^{\mathrm{nom}})^{\alpha_{i}}(c_{2}/c_{2}^{\mathrm{nom}})^{\beta_{i}},\quad i=1,\ldots,4}\\ &{s_{i}=s_{i}^{\mathrm{nom}}(c_{1}/c_{1}^{\mathrm{nom}})^{\gamma_{i}}(c_{2}/c_{2}^{\mathrm{nom}})^{\delta_{i}},\quad i=1,\ldots,3,}\end{array}}
$$ 

with variables $b_{i},\ s_{i},\ c_{i},\ v_{i}$ , and $\lambda$ . 

# 4.6 Generalized inequality constraints 

One very useful generalization of the standard form convex optimization prob- lem ( 4.15 ) is obtained by allowing the inequality constraint functions to be vector valued, and using generalized inequalities in the constraints: 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\preccurlyeq_{K_{i}}0,\quad i=1,.\,.\,,m}\\ &{A x=b,}\end{array}}
$$ 

where $f_{0}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $K_{i}\subseteq\mathbf{R}^{k_{i}}$ are proper cones, and $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}^{k_{i}}$ are $K_{i}$ -convex. We refer to this problem as a (standard form) convex optimization problem with generalized inequality constraints . Problem ( 4.15 ) is a special case with $K_{i}=\mathbf{R}_{+}$ , $i=1,\ldots,m$ . 

Many of the results for ordinary convex optimization problems hold for problems with generalized inequalities. Some examples are: 

The feasible set, any sublevel set, and the optimal set are convex. 

• Any point that is locally optimal for the problem ( 4.48 ) is globally optimal. • The optimality condition for diﬀerentiable $f_{0}$ , given in § 4.2.3 , holds without any change. 

We will also see (in chapter 11 ) that convex optimization problems with generalized inequality constraints can often be solved as easily as ordinary convex optimization problems. 

# 4.6.1 Conic form problems 

Among the simplest convex optimization problems with generalized inequalities are the conic form problems (or cone programs ), which have a linear objective and one inequality constraint function, which is affine (and therefore $K$ -convex): 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\ c^{T}x}\\ {{\mathrm{subject~to}}}&{F x+g\ {\preceq}_{K}0}\\ &{A x=b.}\end{array}}
$$ 

When $K$ is the nonnegative orthant, the conic form problem reduces to a linear program. We can view conic form problems as a generalization of linear programs in which componentwise inequality is replaced with a generalized linear inequality. Continuing the analogy to linear programming, we refer to the conic form prob- lem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ c^{T}x}\\ {{\mathrm{subject~to}}}&{x\succeq_{K}0}\\ &{A x=b}\end{array}}
$$ 

as a conic form problem in standard form . Similarly, the problem 

$$
\begin{array}{l c l}{{\mathrm{minimize}}}&{{c^{T}x}}\\ {{\mathrm{subject~to}}}&{{F x+g\preceq_{K}0}}\end{array}
$$ 

is called a conic form problem in inequality form . 

# 4.6.2 Semidefinite programming 

When $K$ is $\mathbf{S}_{+}^{k}$ , the cone of positive semidefinite $k\times k$ matrices, the associated conic form problem is called a semidefinite program (SDP), and has the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{c^{T}x}\\ {{\mathrm{subject~to}}\ }&{x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0}\\ &{A x=b,}\end{array}}
$$ 

where $G,\,\,\,F_{1},.\,.\,.\,,F_{n}\in{\bf S}^{k}$ , and $A\in\mathbf{R}^{p\times n}$ . The inequality here is a linear matrix inequality (see example 2.10 ). 

If the matrices G $;,\;F_{1},.\,.\,.\,,F_{n}$ are all diagonal, then the LMI in ( 4.50 ) is equiva- lent to a set of $n$ linear inequalities, and the SDP ( 4.50 ) reduces to a linear program. 

# Standard and inequality form semidefinite programs 

Following the analogy to LP, a standard form $S D P$ has linear equality constraints, and a (matrix) nonnegativity constraint on the variable $X\in\mathbf{S}^{n}$ : 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\operatorname{\mathbf{tr}}(C X)}\\ &{{\mathrm{subject~to}}\quad\operatorname{\mathbf{tr}}(A_{i}X)=b_{i},\quad i=1,.\,.\,.\,,p}\\ &{\qquad\qquad\qquad\quad X\succeq0,}\end{array}}
$$ 

where $C$ , $A_{1},\.\cdot\cdot,A_{p}\in\mathbf{S}^{n}$ . (Recall th $\begin{array}{r}{\mathbf{tr}(C X)=\sum_{i,j=1}^{n}C_{i j}X_{i j}}\end{array}$ is the form of a general real-valued linear function on S $\mathbf{S}^{n}$ .) This form should be compared to the standard form linear program ( 4.28 ). In LP and SDP standard forms, we minimize a linear function of the variable, subject to $p$ linear equality constraints on the variable, and a nonnegativity constraint on the variable. 

An inequality form $S D P$ , analogous to an inequality form LP ( 4.29 ), has no equality constraints, and one LMI: 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ c^{T}x}\\ {{\mathrm{subject~to}}}&{x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n}\preceq B,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n}$ , and parameters $B$ , $A_{1},\dots,A_{n}\in\mathbf{S}^{k}$ , $c\in\mathbf{R}^{n}$ . 

# Multiple LMIs and linear inequalities 

It is common to refer to a problem with linear objective, linear equality and in- equality constraints, and several LMI constraints, i.e. , 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ {{\mathrm{subject~to}}\quad F^{(i)}(x)=x_{1}F_{1}^{(i)}+\cdot\cdot\cdot+x_{n}F_{n}^{(i)}+G^{(i)}\preceq0,\quad i=1,\cdot\,.\,,K}\\ &{\;G x\preceq h,\qquad A x=b,}\end{array}}
$$ 

as an SDP as well. Such problems are readily transformed to an SDP, by forming a large block diagonal LMI from the individual LMIs and linear inequalities: 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad\mathbf{diag}(G x-h,F^{(1)}(x),\ldots,F^{(K)}(x))\preceq0}\\ &{\quad\quad\quad\quad A x=b.}\end{array}}
$$ 

# 4.6.3 Examples 

# Second-order cone programming 

The SOCP ( 4.36 ) can be expressed as a conic form problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{c^{T}x}\\ {{\mathrm{subject~to}}\ }&{-(A_{i}x+b_{i},c_{i}^{T}x+d_{i})\preceq_{K_{i}}0,\quad i=1,\ldots,m}\\ &{F x=g,}\end{array}}
$$ 

in which 

$$
K_{i}=\{(y,t)\in\mathbf{R}^{n_{i}+1}\mid\|y\|_{2}\leq t\},
$$ 

i.e. , the second-order cone in $\mathbf{R}^{n_{i}+1}$ . This explains the name second-order cone program for the optimization problem ( 4.36 ). 

# Matrix norm minimization 

Let $A(x)=A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n}$ , where $A_{i}\in\mathbf{R}^{p\times q}$ . We consider the uncon- strained problem 

$$
{\mathrm{minimize}}\quad\|A(x)\|_{2},
$$ 

where $||\cdot||_{2}$ denotes the spectral norm (maximum singular value), and $x\in\mathbf{R}^{n}$ is the variable. This is a convex problem since $\|A(x)\|_{2}$ nvex function of $x$ . 

Using the fact that $\|A\|_{2}\,\leq\,s$ if and only if A $A^{T}A\,\preceq\,s^{2}I$ ⪯ (and $s\geq0$ ), we can express the problem in the form 

$$
\begin{array}{l r c l}{\mathrm{minimize}}&{s}\\ {\mathrm{subject~to}}&{A(x)^{T}A(x)\preceq s I,}\end{array}
$$ 

with variables $x$ and $s$ . Since the function $A(x)^{T}A(x)\,-\,s I$ is matrix convex in $(x,s)$ , this is a convex optimization problem with a single $q\times q$ matrix inequality constraint. 

We can also formulate the problem using a single linear matrix inequality of size $(p+q)\times(p+q)$ , using the fact that 

$$
A^{T}A\preceq t^{2}I\ (\mathrm{and~}t\geq0)\iff\left[\begin{array}{l l}{t I}&{A}\\ {A^{T}}&{t I}\end{array}\right]\succeq0.
$$ 

(see § A.5.5 ). This results in the SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ t}\\ &{{\mathrm{subject~to}}\quad{\left[\begin{array}{l l}{\quad t I}&{A(x)}\\ {A(x)^{T}}&{\quad t I}\end{array}\right]}\succeq0}\end{array}}
$$ 

in the variables $x$ and $t$ . 

# Moment problems 

Let $t$ be a random variable in $\mathbf{R}$ . The expected values $\mathbf{E}\,t^{k}$ (assuming they exist) are called the (power) moments of the distribution of $t$ . The following classical results give a characterization of a moment sequence. 

If there is a probability distribution on $\mathbf{R}$ such that $x_{k}=\mathbf{E}\,t^{k}$ , $k=0,\hdots,2n$ , then $x_{0}=1$ and 

$$
H(x_{0},\ldots,x_{2n})=\left[\begin{array}{c c c c c c}{x_{0}}&{x_{1}}&{x_{2}}&{\ldots}&{x_{n-1}}&{x_{n}}\\ {x_{1}}&{x_{2}}&{x_{3}}&{\ldots}&{x_{n}}&{x_{n+1}}\\ {x_{2}}&{x_{3}}&{x_{4}}&{\ldots}&{x_{n+1}}&{x_{n+2}}\\ {\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}\\ {x_{n-1}}&{x_{n}}&{x_{n+1}}&{\ldots}&{x_{2n-2}}&{x_{2n-1}}\\ {x_{n}}&{x_{n+1}}&{x_{n+2}}&{\ldots}&{x_{2n-1}}&{x_{2n}}\end{array}\right]\succeq0.
$$ 

(The matrix $H$ is called the Hankel matrix associated with .) This is $x_{0},\ldots,x_{2n}$ easy to see: Let $x_{i}=\mathbf{E}\,t^{i}$ , $i=0,\ldots,2n$ be the moments of some distribution, and let $y=\left(y_{0},y_{1},.\cdot\cdot y_{n}\right)\in\mathbf{R}^{n+1}$ . Then we have 

$$
y^{T}H(x_{0},\dots,x_{2n})y=\sum_{i,j=0}^{n}y_{i}y_{j}\,{\mathbf E}\,t^{i+j}={\mathbf E}(y_{0}+y_{1}t^{1}+\cdot\cdot\cdot+y_{n}t^{n})^{2}\geq0.
$$ 

The following partial converse is les bvious: If $x_{0}=1$ and $H(x)\succ0$ , then there exists a probability distribution on R such that $x_{i}=\mathbf{E}\,t^{i}$ , $i\,=\,0,.\,.\,.\,,2n$ . (For a proof, see exercise 2.37 .) Now suppose that $x_{0}=1$ , and $H(x)\succeq0$ (but possibly $H(x)\neq0$ ), i.e. , the linear matrix inequality ( 4.52 ) lds, but possibly not strictly. In this case, there is a sequence of distributions on R , whose moments converge to $x$ . In summary: the condition that $x_{0}$ , . . . , $x_{2n}$ be the moments of some distribution on $\mathbf{R}$ (or the limit of the moments of a sequence of distributions) can be expressed as the linear matrix inequality ( 4.52 ) in the variable $x$ , together with the linear equality $x_{0}=1$ . Using this fact, we can cast some interesting problems involving moments as SDPs. 

Suppose $t$ is a random variable on $\mathbf{R}$ . We do not know its distribution, but we do know some bounds on the moments, i.e. , 

$$
\underline{{\mu}}_{k}\leq\mathbf{E}\,t^{k}\leq\overline{{\mu}}_{k},\quad k=1,.\,.\,.\,,2n
$$ 

(which includes, as a special case, knowing exact values of some of the moments). Let $p(t)=c_{0}+c_{1}t+\cdot\cdot\cdot+c_{2n}t^{2n}$ e a given polynomial in $t$ . The expected value of $p(t)$ is linear in the moments E $\mathbf{E}\,t^{i}$ : 

$$
\mathbf{E}\,p(t)=\sum_{i=0}^{2n}c_{i}\,\mathbf{E}\,t^{i}=\sum_{i=0}^{2n}c_{i}x_{i}.
$$ 

We can compute upper and lower bounds for $\mathbf{E}\,p(t)$ , 

$$
\begin{array}{l r l}&{\mathrm{minimize~(maximize)}}&{\mathbf{E}\,p(t)}\\ &{\mathrm{subject~to}}&{\underline{{\mu}}_{k}\le\mathbf{E}\,t^{k}\le\overline{{\mu}}_{k},\quad k=1,.\,.\,.\,,2n,}\end{array}
$$ 

over all probability distributions that satisfy the given moment bounds, by solving the SDP 

$$
{\begin{array}{l l l}{{\mathrm{minimize~(maximize)}}}&{c_{1}x_{1}+\dots+c_{2n}x_{2n}}\\ {{\mathrm{subject~to}}}&{{\underline{{\mu}}}_{k}\leq x_{k}\leq{\overline{{\mu}}}_{k},\quad k=1,\dots,2n}\\ &{H(1,x_{1},\dots,x_{2n})\succeq0}\end{array}}
$$ 

with variables $x_{1}$ , $\cdot\cdot\cdot$ , $x_{2n}$ . This gives bounds on $\mathbf{E}\,p(t)$ , over all probability dis- tributions that satisfy the known moment constraints. The bounds are sharp in the sense that there exists a sequence of distributions, whose moments satisfy the given moment bounds, for which $\mathbf{E}\,p(t)$ converges to the upper and lower bounds found by these SDPs. 

# Bounding portfolio risk with incomplete covariance information 

We consider once again the setup for the classical Markowitz portfolio problem (see page 155 ). We have a portfolio of $n$ assets or stocks, with $x_{i}$ denoting the amount of asset $i$ that is held over some investment period, and denoting the relative $p_{i}$ price change of asset $i$ over the period. The change in total value of the portfolio is $p^{T}x$ . The price change vector $p$ is modeled as a random vector, with mean and covariance 

$$
{\overline{{p}}}=\mathbf{E}\,p,\qquad\Sigma=\mathbf{E}(p-{\overline{{p}}})(p-{\overline{{p}}})^{T}.
$$ 

The change in value of the portfolio is therefore a random variable with mean $\overline{{p}}^{T}x$ and standard deviation $\sigma\,=\,(x^{T}\Sigma x)^{1/2}$ . The risk of a large loss, i.e. , a change in portfolio value that is substantially below its expected value, is directly related to the standard deviation $\sigma$ , and increases with it. For this reason the standard deviation $\sigma$ (or the variance $\sigma^{2}$ ) is used as a measure of the risk associated with the portfolio. 

In the classical portfolio optimization problem, the portfolio $x$ is the optimiza- tion variable, and we minimize the risk subject to a minimum mean return and other constraints. The price change statistics $\overline{{p}}$ and $\Sigma$ are known problem param- eters. In the risk bounding problem considered here, we turn the problem around: we assume the portfolio $x$ is known, but only partial information is available about the covariance matrix $\Sigma$ . We might have, for example, an upper and lower bound on each entry: 

$$
L_{i j}\le\Sigma_{i j}\le U_{i j},\quad i,\ j=1,.\,.\,.\,,n,
$$ 

where $L$ and $U$ are given. We now pose the question: what is the maximum risk for our portfolio, over all covariance matrices consistent with the given bounds? We define the worst-case variance of the portfolio as 

$$
\sigma_{\mathrm{wc}}^{2}=\operatorname*{sup}\{x^{T}\Sigma x\mid L_{i j}\leq\Sigma_{i j}\leq U_{i j},\ i,j=1,.\,.\,,n,\ \Sigma\succeq0\}.
$$ 

We have added the condition $\Sigma\succeq0$ , which the covariance matrix must, of course, satisfy. 

We can find $\sigma_{\mathrm{wc}}$ by solving the SDP 

$$
\begin{array}{l l}{\mathrm{maximize}}&{~x^{T}\Sigma x}\\ {\mathrm{subject~to}}&{L_{i j}\leq\Sigma_{i j}\leq U_{i j},\quad i,~j=1,.\,.\,.\,,n}\\ &{\Sigma\succeq0}\end{array}
$$ 

with variable $\Sigma\,\in\,\mathbf{S}^{n}$ (and problem parameters $x$ , $L$ , and $U$ ). The optimal $\Sigma$ is the worst covariance matrix consistent with our given bounds on the entries, where ‘worst’ means largest risk with the (given) portfolio $x$ . We can easily construct a distribution for $p$ that is consistent with the given bounds, and achieves the worst-case variance, from an optimal $\Sigma$ for the SDP. For example, we can take $p=\overline{{p}}+\Sigma^{1/2}v$ , where $v$ is any random vector with $\mathbf{E}\,v=0$ and ${\bf E}\,v v^{T}=I$ . 

Evidently we can use the same method to determine $\sigma_{\mathrm{wc}}$ for any prior informa- tion about $\Sigma$ that is convex. We list here some examples. 

• Known variance of certain portfolios. We might have equality constraints such as 

$$
u_{k}^{T}\Sigma u_{k}=\sigma_{k}^{2},
$$ 

where $u_{k}$ and $\sigma_{k}$ are given. This corresponds to prior knowledge that certain known portfolios (given by $u_{k}$ ) have known (or very accurately estimated) variance. 

Including eﬀects of estimation error. If the covariance $\Sigma$ is stimated from empirical data, the estimation method will give an estimate Σ, and some in- formation about the reliability of the estimate, such as a confidence ellipsoid. This can be expressed as 

$$
C(\Sigma-\hat{\Sigma})\leq\alpha,
$$ 

where $C$ is a positive definite quadratic form on $\mathbf{S}^{n}$ , and the constant $\alpha$ determines the confidence level. 

Factor models. The covariance might have the form 

$$
\Sigma=F\Sigma_{\mathrm{factor}}F^{T}+D,
$$ 

where $F\,\in\,\mathbf{R}^{n\times k}$ , $\Sigma_{\mathrm{faster}}\,\in\,\mathbf{S}^{k}$ , and $D$ is diagonal. This corresponds to a model of the price changes of the form 

$$
p=F z+d,
$$ 

where $\mathcal{Z}$ is a random variable (the underlying factors that aﬀect the price changes) and $d_{i}$ are independent (additional volatility of each asset price). We assume that the factors are known. Since $\Sigma$ is linearly related to $\Sigma_{\mathrm{factor}}$ and $D$ , we can impose any convex constraint on them (representing prior information) and still compute $\sigma_{\mathrm{wc}}$ using convex optimization. 

• Informati about correlation coefficients. In the simplest case, the diagonal entries of Σ ( i.e. , the volatilities of each asset price) are known, and bounds on correlation coefficients between price changes are known: 

$$
l_{i j}\le\rho_{i j}=\frac{\Sigma_{i j}}{\Sigma_{i i}^{1/2}\Sigma_{j j}^{1/2}}\le u_{i j},\quad i,\ j=1,.\,.\,.\,,n.
$$ 

Since $\Sigma_{i i}$ are known, but $\Sigma_{i j}$ for $i\neq j$ are not, these are linear inequalities. 

# Fastest mixing Markov chain on a graph 

We consider an undirected graph, with nodes $1,\cdot\cdot\cdot,n$ , and a set of edges 

$$
{\mathcal{E}}\subseteq\{1,.\,.\,.\,,n\}\times\{1,.\,.\,.\,,n\}.
$$ 

Here $(i,j)\,\in\,\mathcal{E}$ mea that nodes $i$ and $j$ are connected by an edge. Since the graph is undirected, E is symmetric: $(i,j)\in\mathcal{E}$ if and only if $(j,i)\in\mathcal{E}$ . We allow the possibility of self-loops, i.e. , we can have $(i,i)\in\mathcal{E}$ . 

We define a Markov chain, with state $X(t)\,\in\,\{1,.\,.\,.\,,n\}$ , for $t\in{\bf Z}_{+}$ (the set of nonnegative integers), as follows. With ch edge $(i,j)\;\in\;\mathcal{E}$ we associate a probability $P_{i j}$ , which is the probability that X makes a transition between nodes $i$ and $j$ . State transitions can only occur across edges; we have $P_{i j}=0$ for $(i,j)\notin\mathcal{E}$ . The probabilities associated with the edges must be nonnegative, and for each node, the sum of the probabilities of links connected to the node (including a self-loop, if there is one) must equal one. 

The Markov chain has transition probability matrix 

$$
P_{i j}=\mathbf{prob}(X(t+1)=i\mid X(t)=j),\quad i,j=1,.\,.\,.\,,n.
$$ 

This matrix must satisfy 

$$
P_{i j}\geq0,\quad i,\ j=1,\ldots,n,\qquad{\bf1}^{T}P={\bf1}^{T},\qquad P=P^{T},
$$ 

and also 

$$
P_{i j}=0\quad\mathrm{for}\ (i,j)\not\in\mathcal{E}.
$$ 

Since $P$ is symmetric and ${\bf1}^{T}P\,=\,{\bf1}^{T}$ , we conclude $P\mathbf{1}\,=\,\mathbf{1}$ , so the uniform distribution $(1/n)\mathbf{1}$ is an equilibrium distribution for the Markov chain. Conver- gence of the distribution of $X(t)$ to $(1/n)\mathbf{1}$ is determined by the second largest (in magnitude) eigenvalue of $P$ , i.e. , by $r=\operatorname*{max}\{\lambda_{2},-\lambda_{n}\}$ , where 

$$
1=\lambda_{1}\geq\lambda_{2}\geq\cdot\cdot\cdot\geq\lambda_{n}
$$ 

are the eigenvalues of $P$ . We refer to $r$ as the mixing rate of the Markov chain. If $r=1$ , then the distribution of $X(t)$ need not converge to $(1/n)\mathbf{1}$ (which means the Markov chain does not mix). When $r<1$ , the distribution of $X(t)$ approaches $(1/n)\mathbf{1}$ asymptotically as $r^{t}$ , as $t\,\rightarrow\,\infty$ . Thus, the smaller $r$ is, the faster the Markov chain mixes. 

The fastest mixing Markov chain problem is to find $P$ , subject to the con- straints ( 4.53 ) and ( 4.54 ), that minimizes $r$ . (The problem data is the graph, i.e. , $\mathcal{E}$ .) We will show that this problem can be formulated as an SDP. 

Since the eigenvalue $\lambda_{1}=1$ is associated with the eigenvector 1 , we can express the mixing rate as the norm of the matrix $P$ , restricted to the subspace ${\bf1}^{\perp}$ : $r=$ $||Q P Q||_{2}$ , where $Q=I-(1/n)\mathbf{1}\mathbf{1}^{T}$ is the matrix representing orthogonal projection on 1 ${\bf1}^{\perp}$ . Using the property P $P\mathbf{1}=\mathbf{1}$ , we have 

$$
\begin{array}{l l l}{{r}}&{{=}}&{{\|Q P Q\|_{2}}}\\ {{\ }}&{{=}}&{{\|(I-(1/n){\bf11}^{T})P(I-(1/n){\bf11}^{T})\|_{2}}}\\ {{\ }}&{{=}}&{{\|P-(1/n){\bf11}^{T}\|_{2}.}}\end{array}
$$ 

This shows that the mixing rate $r$ is a convex function of $P$ , so the fastest mixing Markov chain problem can be cast as the convex optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\|P-(1/n)\mathbf{1}\mathbf{1}^{T}\|_{2}}\\ {{\mathrm{subject~to}}\quad P\mathbf{1}=\mathbf{1}}\\ &{P_{i j}\geq0,\quad i,j=1,\ldots,n}\\ &{P_{i j}=0{\mathrm{~for~}}(i,j)\not\in{\mathcal{E}},}\end{array}}
$$ 

with variable $P\,\in\,{\bf S}^{n}$ . We can express the problem as an SDP by introducing a scalar variable $t$ to bound the norm of $P-(1/n)\mathbf{1}\mathbf{1}^{T}$ : 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{t}\\ {{\mathrm{subject~to}}\ }&{-t I\prec P-(1/n)\mathbf{1}\mathbf{1}^{T}\preceq t I}\\ &{P\mathbf{1}=\mathbf{1}}\\ &{P_{i j}\geq0,\quad i,j=1,\ldots,n}\\ &{P_{i j}=0{\mathrm{~for~}}(i,j)\not\in{\mathcal{E}}.}\end{array}}
$$ 

# 4.7 Vector optimization 

# 4.7.1 General and convex vector optimization problems 

In $\S4.6$ we extended the standard form problem ( 4.1 ) to include vector-valued constraint functions. In this section we investigate the meaning of a vector-valued objective function . We denote a general vector optimization problem as 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize~(with~respect~to~}}K)}&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&{f_{i}(x)\leq0,}&{i=1,\ldots,m}\\ &{}&{h_{i}(x)=0,}&{i=1,\ldots,p.}\end{array}}
$$ 

Here $x\in\mathbf{R}^{n}$ is the optimization variable, $K\subseteq\mathbf{R}^{q}$ is a proper cone, $f_{0}:{\mathbf{R}}^{n}\rightarrow{\mathbf{R}}^{q}$ is the objective function, $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are the inequality constraint functions, and $h_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are the equality constraint functions. The only diﬀerence between this problem and the standard optimization problem ( 4.1 ) is that here, the objective function takes values in $\mathbf{R}^{q}$ , and the problem specification includes a proper cone $K$ , which is used to compare objective values. In the context of vector optimization, the standard optimization problem ( 4.1 ) is sometimes called a scalar optimization problem . 

We say the vector optimization problem ( 4.56 ) is a convex vector optimization problem if the objective function $f_{0}$ is $K$ -convex, the inequality constraint functions $f_{1},\ldots,f_{m}$ are convex, and the equality constraint functions $h_{1},.\,.\,.\,,h_{p}$ are affine. (As in the scalar case, we usually express the equality constraints as $A x=b$ , where $A\in\mathbf{R}^{p\times n}$ .) 

What meaning can we give to the vector optimization problem ( 4.56 )? Suppose $x$ and $y$ are two feasible points ( i.e. , they satisfy the constraints). Their associated objective values, $f_{0}(x)$ and $f_{0}(y)$ , are to be compared using the generalized inequal- ity $\preceq_{K}$ . We interpret $f_{0}(x)\preceq_{K}f_{0}(y)$ as meaning that $x$ ‘better than or equal’ in value to $y$ (as judged by the objective $f_{0}$ , with respect to K ). The confusing aspect of vector optimization is that the two objective values $f_{0}(x)$ and $f_{0}(y)$ need not be comparable; we can have neither $f_{0}(x)\preceq_{K}f_{0}(y)$ nor $f_{0}(y)\preceq_{K}f_{0}(x)$ , i.e. , neither is better than the other. This cannot happen in a scalar objective optimization problem. 

# 4.7.2 Optimal points and values 

We first consider a special case, in which the meaning of the vector optimization problem is clear. Consider the set of objective values of feasible points, 

$$
{\mathcal{O}}=\{f_{0}(x)\ |\ \exists x\in{\mathcal{D}},\ f_{i}(x)\leq0,\ i=1,.\,.\,,m,\ h_{i}(x)=0,\ i=1,.\,.\,,p\}\subseteq\mathbf{R}^{q},
$$ 

which is called the set of achievable objective values . If this set has a minimum element (see § 2.4.2 ), i.e. , there is a feasible $x$ such that $f_{0}(x)\ \preceq_{K}\ f_{0}(y)$ for all feasible $y$ , then we say $x$ is optimal for the problem ( 4.56 ), and refer to $f_{0}(x)$ as the optimal value of the problem. (When a vector optimization problem has an optimal value, it is unique.) If $x^{\star}$ is an optimal point, then $f_{0}(x^{\star})$ , the objective at $x^{\star}$ , can be compared to the objective at every other feasible point, and is better than or equal to it. Roughly speaking, $x^{\star}$ is unambiguously a best choice for $x$ , among feasible points. 

A point $x^{\star}$ is optimal if and only if it is feasible and 

$$
\mathcal{O}\subseteq f_{0}(x^{\star})+K
$$ 

![](images/f4f8745a787db69ee2f74593128d2afdac6122f6f6506e85ddd5d0c57e3036a5.jpg) 
Figure 4.7 The set $\mathcal{O}$ of achievab for a vector optimization with objective values in R , with cone $K=\mathbf{R}_{+}^{2}$ , is shown shaded. In this case, the point labeled $f_{0}\mathopen{}\mathclose\bgroup\left(x^{\star}\aftergroup\egroup\right)$ is the optimal value of the problem, and $x^{\star}$ is an optimal point. The objective value $f_{0}\mathopen{}\mathclose\bgroup\left(x^{\star}\aftergroup\egroup\right)$ can be compared to every other achievable value $f_{0}(y)$ , and is better than or equal to $f_{0}(y)$ . (Here, ‘better than or equal to’ means ‘is below and to the left of’.) The lightly shaded region is $f_{0}(x^{\star})\,{+}\,K$ , which is th all $z\in\mathbf{R}^{2}$ corresponding to objective values worse than (or equal to) $f_{0}\mathopen{}\mathclose\bgroup\left(x^{\star}\aftergroup\egroup\right)$ ). 

(see § 2.4.2 ). The set $f_{0}(x^{\star})+K$ can be interpreted as the set of values that are worse than, or equal to, $f_{0}(x^{\star})$ , so the condition ( 4.57 ) states that every achievable value falls in this set. This is illustrated in figure 4.7 . Most vector optimization problems do not have an optimal point and an optimal value, but this does occur in some special cases. 

Example 4.9 Best linear unbiased estimator. Suppose $y=A x+v$ , where $v\in\mathbf{R}^{m}$ is a measurement noise, $y\in\mathbf{R}^{m}$ is a vector of measurement and $x\in\mathbf{R}^{n}$ is a vector to be estimated, given the measurement $y$ . We assume that A has rank $n$ , and that the measurement noise satisfies $\mathbf{E}\,v\,=\,0$ , ${\bf E}\,v v^{T}=I$ , i.e. , its components are zero mean and uncorrelated. 

A l near estim $x$ has t $\widehat{x}=F y$ . The estimator is called unbiased if for all x we have E $\mathbf{E}\,\widehat{x}=x$ , i.e. , if FA $F A=I$ . The error covariance of an unbiased estimator is 

$$
\mathbf{E}(\widehat{x}-x)(\widehat{x}-x)^{T}=\mathbf{E}\,F{v v}^{T}{F}^{T}=F{F}^{T}.
$$ 

Our goal is to find an unbiased estimator that has a ‘small’ error covariance matrix. We can compare error covariances using matrix inequality, i.e. , with respect to $\mathbf{S}_{+}^{n}$ . has the following inter retation: Suppose $\hat{x}_{1}=F_{1}y$ , $\hat{x}_{2}=F_{2}y$ are two unbiased estimators. Then the first estimator is at least as good as the second, $i$ .e. , $F_{1}F_{1}^{T^{\prime}}\preceq$ ⪯ $F_{2}F_{2}^{T}$ , if and only if for all c , 

$$
\mathbf{E}(c^{T}\widehat{x}_{1}-c^{T}x)^{2}\leq\mathbf{E}(c^{T}\widehat{x}_{2}-c^{T}x)^{2}.
$$ 

In other words, for any linear function of $x$ , the estimator $F_{1}$ yields at least as good an estimate as does $F_{2}$ . 

We can express the problem of finding an unbiased estimator for $x$ as the vector optimization problem 

$$
\begin{array}{l l}{\mathrm{minimize~(w.r.t.~}\mathbf{S}_{+}^{n})}&{\mathbf{\mathit{F F}}^{T}}\\ {\mathrm{subject~to}}&{\mathbf{\mathit{F A}}=\mathbf{\mathit{I}},}\end{array}
$$ 

with variable $F\in\mathbf{R}^{n\times m}$ . The objective $F F^{T}$ is convex with respect to $\mathbf{S}_{+}^{n}$ , so the problem ( 4.58 ) is a convex vector optimization problem. An easy way to see this is to observe that $v^{T}F F^{T}v=\|F^{T}v\|_{2}^{2}$ is a convex function of $F$ for any fixed $v$ . 

It is a famous result that the problem ( 4.58 ) has an optimal solution, the least-squares estimator, or pseudo-inverse, 

$$
F^{\star}=A^{\dagger}=(A^{T}A)^{-1}A^{T}.
$$ 

For any $F^{\prime}$ with $F A=I$ , we have $F F^{T}\succeq F^{\star}F^{\star T}$ . The matrix 

$$
F^{\star}F^{\star T}=A^{\dagger}A^{\dagger T}=(A^{T}A)^{-1}
$$ 

is the optimal value of the problem ( 4.58 ). 

# 4.7.3 Pareto optimal points and values 

We now consider the case (which occurs in most vector optimization problems of interest) in which the set of achievable objective values does not have a minimum element, so the problem does not have an optimal point or optimal value. In these cases minimal elements of the set of achievable values play an important role. We say that a feasible point $x$ is Pareto optimal (or efficient ) if $f_{0}(x)$ is a minimal element of the set of achievable values $\mathcal{O}$ . In this case we say that $f_{0}(x)$ is a Pareto optimal value for the vector optimization problem ( 4.56 ). Thus, a point $x$ is Pareto optimal if it is feasible and, for any feasible $y$ , $f_{0}(y)\preceq_{K}f_{0}(x)$ implies $f_{0}(y)=f_{0}(x)$ . In other words: any feasible point $y$ that is better than or equal to $x$ ( i.e. , $f_{0}(y)\preceq_{K}f_{0}(x))$ has exactly the same objective value as $x$ . 

A point $x$ is Pareto optimal if and only if it is feasible and 

$$
(f_{0}(x)-K)\cap\mathcal{O}=\{f_{0}(x)\}
$$ 

(see § 2.4.2 ). The set $f_{0}(x)-K$ can be interpreted as the set of values that are better than or equal to $f_{0}(x)$ , so the condition ( 4.59 ) states that the only achievable value better than or equal to $f_{0}(x)$ is $f_{0}(x)$ itself. This is illustrated in figure 4.8 . 

A vector optimization problem can have many Pareto optimal values (and points). The set of Pareto optimal values, denoted $\mathcal{P}$ , satisfies 

$$
{\mathcal{P}}\subseteq{\mathcal{O}}\cap{\mathbf{bd}}\,{\mathcal{O}},
$$ 

i.e. , every Pareto optimal value is an achievable objective value that lies in the boundary of the set of achievable objective values (see exercise 4.52 ). 

![](images/ad1840656cc12d510184112cbe06a70533c5aa03f5af45a527b2a0dc5acb0955.jpg) 
Figure 4.8 The set $\mathcal{O}$ of a evable values for a vector optimization problem with objective values in R , with cone ${\cal K}\,=\,{\bf R}_{+}^{2}$ , is shown shaded. This problem does not have an optimal point or value, but it does have a set of Pareto optimal points, whose corresponding values are shown as the dark- ened curve on the lower left b dary of $\mathcal{O}$ . The point labeled $f_{0}\big(x^{\mathrm{peo}}\big)$ is a Pareto optimal value, and x $x^{\mathrm{{po}}}$ is a Pareto optimal point. The lightly shaded region is $f_{0}(x^{\mathrm{{pe}}})-K$ , which is the s l $z\in\mathbf{R}^{2}$ corresponding to objective values better than (or equal to) $f_{0}\big(x^{\mathrm{peo}}\big)$ ). 

# 4.7.4 Scalarization 

Scalarization is a standard technique for finding Pareto optimal (or optimal) points for a vector optimization problem, based on the characterization of minimum and minimal points via dual generalized inequalities given in § 2.6.3 . Choose any $\lambda\succ_{K^{*}}$ 0, i.e. , any vector that is positive in the dual generalized inequality. Now consider the scalar optimization problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,\,}&{\lambda^{T}f_{0}(x)}\\ {\mathrm{subject~to}\,\,}&{f_{i}(x)\le0,\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=0,\quad i=1,.\,.\,,p,}\end{array}
$$ 

and let $x$ be an optimal point. Then $x$ is Pareto optimal for the vector optimization problem ( 4.56 ). This follows from the dual inequality characterization of minimal points given in $\S2.6.3$ , and is also easily shown directly. If $x$ were not Pareto optimal, then there is a $y$ that is feasible, satisfies $f_{0}(y)\ \preceq_{K}\ f_{0}(x)$ , and $f_{0}(x)\;\neq\;f_{0}(y)$ . Since $f_{0}(x)\,-\,f_{0}(y)\,\succeq_{K}\,0$ and is nonzero, we have $\lambda^{T}(f_{0}(x)\,-\,f_{0}(y))\,>\,0$ , i.e. , $\lambda^{T}f_{0}(x)\;>\;\lambda^{T}f_{0}(y)$ . This contradicts the assumption that $x$ is optimal for the scalar problem ( 4.60 ). 

Using scalarization, we can find Pareto optimal points for any vector opti- mization problem by solving the ordinary scalar optimization problem ( 4.60 ). The vector $\lambda$ , which is sometimes called the weight vector , must satisfy $\lambda\succ_{K^{*}}0$ . The weight vector is a free parameter; by varying it we obtain (possibly) diﬀerent Pareto optimal solutions of the vector optimization problem ( 4.56 ). This is illustrated in figure 4.9 . The figure also shows an example of a Pareto optimal point that cannot 

![](images/b0b41af769f7f50a704dfce340780d14ca9aea523f14d6f698c41195f8836f71.jpg) 
Figure 4.9 Scalarization. Th $\mathcal{O}$ of achievable values for a vector opti- mization problem with cone $K=\mathbf{R}_{+}^{2}$ . Three Pareto optimal values $f_{0}\!\left(x_{1}\right)$ , $f_{0}\!\left(x_{2}\right)$ , $f_{0}\mathopen{}\mathclose\bgroup\left(x_{3}\aftergroup\egroup\right)$ are shown. The first two values can be obtained by scalar- ization: $f_{0}\!\left(x_{1}\right)$ minimizes $\lambda_{1}^{T}u$ over all $u\,\in\,\mathcal{O}$ and $f_{0}\!\left(x_{2}\right)$ minimizes $\lambda_{2}^{T}u$ , where $\lambda_{1},\lambda_{2}\succ0$ . The value $f_{0}\mathopen{}\mathclose\bgroup\left(x_{3}\aftergroup\egroup\right)$ is Pareto optimal, but cannot be found by scalarization. 

be obtained via scalarization, for any value of the weight vector $\lambda\succ_{K^{*}}0$ . 

The method of scalarization can be interpreted geometrically. A point $x$ is optimal for the scalarized problem, i.e. , minimizes $\lambda^{T}f_{0}$ over the feasible set, if and only if $\lambda^{T}(f_{0}(y)-f_{0}(x))\geq0$ for all feasible $y$ . But this is the same as saying that $\{u\mid\;-\lambda^{T}(u-f_{0}(x))=0\}$ is a supporting hyperplane to the set of achievable objective values at the point $f_{0}(x)$ ; in particular 

$$
\{u\mid\lambda^{T}(u-f_{0}(x))<0\}\cap\mathcal{O}=\emptyset.
$$ 

(See figure 4.9 .) Thus, when we find an optimal point for the scalarized problem, we not only find a Pareto optimal point for the original vector optimization problem; we also find an entire halfspace in $\scriptstyle\mathbf{R}^{q}$ , given by ( 4.61 ), of objective values that cannot be achieved. 

# Scalarization of convex vector optimization problems 

Now suppose the vector optimization problem ( 4.56 ) is convex. Then the scalarized problem ( 4.60 ) is also convex, since $\lambda^{T}f_{0}$ is a (scalar-valued) convex function (by the results in § 3.6 ). This means that we can find Pareto optimal points of a convex vector optimization problem by solving a convex scalar optimization problem. For each choice of the weight vector $\lambda\succ_{K^{*}}0$ we get a (usually diﬀerent) Pareto optimal point. 

For convex vector optimization problems we have a partial converse: For every Pareto optimal point $x^{\mathrm{peo}}$ , there is some nonzero $\lambda\succeq_{K^{*}}0$ such that $x^{\mathrm{peo}}$ is a solution of the scalarized problem ( 4.60 ). So, roughly speaking, for convex problems the method of scalarization yields all Pareto optimal points, as the weight vector $\lambda$ varies over the $K^{*}$ -nonnegative, nonzero values. We have to be careful here, because it is not true that every solution of the scalarized problem, with $\lambda\succeq_{K^{*}}0$ and $\lambda\neq0$ , is a Pareto optimal point for the vector problem. (In contrast, every solution of the scalarized problem with $\lambda\succ_{K^{*}}0$ is Pareto optimal.) 

In some cases we can use this partial converse to find all Pareto optimal points of a convex vector optimization problem. Scalarization with $\lambda\succ_{K^{*}}$ $0$ gives a set of Pareto optimal points (as it would in a nonconvex vector optimization problem as well). To find the remaining Pareto optimal solutions, we have to consider nonzero weight vectors $\lambda$ that satisfy $\lambda\succeq_{K^{*}}$ 0. For each such weight vector, we first identify all solutions of the scalarized problem. Then among these solutions we must check which are, in fact, Pareto optimal for the vector optimization problem. These ‘extreme’ Pareto optimal points can also be found as the limits of the Pareto optimal points obtained from positive weight vectors. 

To establish this partial converse, we consider the set 

$$
{\mathcal{A}}={\mathcal{O}}+K=\{t\in\mathbf{R}^{q}\mid f_{0}(x)\preceq_{K}t{\mathrm{~for~some~feasible~}}x\},
$$ 

which consists of all values that are worse than equal to (with respect to $\preceq_{K}$ ) some achievable objective v ue. While the set O of achievable objective values need not be convex, the t A is convex, when the problem is convex. Moreover, the minimal elements of A are exactly the same as the minimal elements of the set $\mathcal{O}$ of achievable values, i.e. , they are the same as the Pareto optimal values. (See exerci 4.53 .) Now use th results of $\S2.6.3$ to conclude that any minimal element of A minimizes λ $\lambda^{T}z$ over A for some nonzero $\lambda\succeq_{K^{*}}0$ . This means that every Pareto optimal point for the vector optimization problem is optimal for the scalarized problem, for some nonzero weight $\lambda\succeq_{K^{*}}$ 0. 

Example 4.10 Minimal upper bound on a set of matrices. We consider the (convex) vector optimization problem, with respect to the positive semidefinite cone, 

$$
\begin{array}{l l}{\mathrm{minimize~(w.r.t.~}\mathbf{S}_{+}^{n})}&{X}\\ {\mathrm{subject~to~}}&{X\succeq A_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

where $A_{i}\in\mathbf{S}^{n}$ , $i\,=\,1,\,.\,.\,.\,,m$ , are given. The constraints mean that $X$ is an upper bound on the given matrices $A_{1},\ldots,A_{m}$ ; a Pareto optimal solution of ( 4.63 ) is a minimal upper bound on the matrices. 

To find a Pareto optimal point, we apply scalarization: we choose any $W\in\mathbf{S}_{++}^{n}$ and form the problem 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{\operatorname{\mathbf{tr}}(W X)}\\ &{{\mathrm{subject~to}}}&{X\succeq A_{i},\quad i=1,\ldots,m,}\end{array}}
$$ 

which is an SDP. Diﬀerent choices for $W$ will, in general, give diﬀerent minimal solutions. 

The partial converse tells us that if $X$ is Pareto optimal for the vector problem ( 4.63 ) then it is optimal for the SDP ( 4.64 ), for some nonzero weight matrix $W~\succeq~0$ . (In this case, however, not every solution of ( 4.64 ) is Pareto optimal for the vector optimization problem.) 

We can give a simple geometric interpretation for this problem. We associate with each $A\in\mathbf{S}_{++}^{n}$ an ellipsoid centered at the origin, given by 

$$
{\mathcal{E}}_{A}=\{u\ |\ u^{T}A^{-1}u\leq1\},
$$ 

![](images/db1b7b2d3f62abe4368ac8c3886942027a2f1f82b07dfbe01780cc8fa29cf794.jpg) 
Figure 4.10 Geometric interpretation of the problem ( 4.63 ). The three shaded ellipsoids correspond to the data $A_{1}$ , $A_{2}$ , $A_{3}\,\in\,{\bf S}_{++}^{2}$ ; the Pareto optimal points correspond to minimal ellipsoids that contain them. The two ellipsoids, with boundaries labeled $X_{1}$ and $X_{2}$ , show two minimal ellipsoids obtained by solving the SDP ( 4.64 ) for two diﬀerent weight matrices $W_{1}$ and $W_{2}$ . 

so that $A\,\preceq\,B$ if and only if $\mathcal{E}_{A}\;\subseteq\;\mathcal{E}_{B}$ . A Pareto optimal point $X$ for the prob- lem ( 4.63 ) corresponds to a minimal ellipsoid that contains the ellipsoids associated with $A_{1},\ldots,A_{m}$ . An example is shown in figure 4.10 . 

# 4.7.5 Multicriterion optimization 

When a vector optimization problem involves the cone $K\,=\,\mathbf{R}_{+}^{q}$ , it is called a multicriterion or multi-objective optimization problem. The components of $f_{0}$ , say, $F_{1},\dots,F_{q}$ , can be interpreted as $q$ diﬀerent scalar objectives, each of which we would like to minimize. We refer to $F_{i}$ as the i th objective of the problem. A multicriterion optimization problem is convex if $f_{1},\ldots,f_{m}$ are convex, $h_{1},.\,.\,.\,,h_{p}$ are affine, and the objectives $F_{1},\dots,F_{q}$ are convex. 

Since multicriterion problems are vector optimization problems, all of the ma- terial of § 4.7.1 – § 4.7.4 applies. For multicriterion problems, though, we can be a bit more specific in the interpretations. If $x$ is feasible, we can think of $F_{i}(x)$ as its score or value, according to the $i$ th objective. If and are both feasible, $x$ $y$ $F_{i}(x)\leq F_{i}(y)$ means that is at least as good as , according to the $i$ th ob ective; $x$ $y$ $F_{i}(x)<F_{i}(y)$ means that $x$ is better than $y$ , or $x$ beats $y$ , according to the i th ob- jective. If $x$ and $y$ are both feasible, we say that $x$ is better than $y$ , or $x$ dominates $y$ , if $F_{i}(x)\leq F_{i}(y)$ for $i=1,\dots,q$ , and for at least one $j$ , $F_{j}(x)<F_{j}(y)$ . Roughly speaking, $x$ is better than $y$ if $x$ meets or beats $y$ on all objectives, and beats it in at least one objective. 

In a multicriterion problem, an optimal point $x^{\star}$ satisfies 

$$
F_{i}(x^{\star})\leq F_{i}(y),\quad i=1,.\,.\,.\,,q,
$$ 

for every feasible . In other words, $x^{\star}$ is simultaneously optimal for each of the $y$ scalar problems 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{F_{j}(x)}\\ {{\mathrm{subject~to}}\ }&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=0,\quad i=1,.\,.\,,p,}\end{array}}
$$ 

for $j\,=\,1,\cdot\cdot\cdot,q$ . When there is an optimal point, we say that the objectives are noncompeting , since no compromises have to be made among the objectives; each objective is as small as it could be made, even if the others were ignored. 

A Pareto optimal point $x^{\mathrm{{peo}}}$ satisfies the following: if is feasible and $F_{i}(y)\leq$ $y$ $F_{i}(x^{\mathrm{peo}})$ for $i=1,\dots,q$ , then $F_{i}\big(x^{\mathrm{{pe}}}\big)=F_{i}(y)$ , $i=1,\dots,q$ . This can be restated as: a point is Pareto optimal if and only if it is feasible and there is no better feasible point. In particular, if a feasible point is not Pareto optimal, there is at least one other feasible point that is better. In searching for good points, then, we can clearly limit our search to Pareto optimal points. 

# Trade-oﬀanalysis 

Now suppose that $x$ and $y$ are Pareto optimal points with, say, 

$$
\begin{array}{r l r l}&{F_{i}(x)<F_{i}(y),}&&{i\in A}\\ &{F_{i}(x)=F_{i}(y),}&&{i\in B}\\ &{F_{i}(x)>F_{i}(y),}&&{i\in C,}\end{array}
$$ 

where $A\cup B\cup C=\{1,.\,.\,.\,,q\}$ . In other words, $A$ is the set of (indices of) objectives for which $x$ beats $y$ , B is the set of objectives for which the points $x$ and $y$ are tied, and $C$ is the set of objectives for which beats . If $A$ and $C$ are empty, then $y$ $x$ the two points $x$ and $y$ have exactly the same objective values. If this is not the case, then both $A$ and $C$ must be nonempty. In other words, when comparing two Pareto optimal points, they either obtain the same performance ( i.e. , all objectives equal), or, each beats the other in at least one objective. 

In comparing the point $x$ to $y$ , we say that we have traded or traded oﬀ better objective values for $i\in A$ for worse objective values for $i\in C$ . Optimal trade-oﬀ analysis (or just trade-oﬀanalysis) is the study of how much worse we must do in one or more objectives in order to do better in some other objectives, or more generally, the study of what sets of objective values are achievable. 

As an example, consider a bi-criterion ( i.e. , two criterion) problem. Suppose $x$ is a Pareto optimal point, with objectives $F_{1}(x)$ and $F_{2}(x)$ . We might ask how much larger $F_{2}(z)$ would have to be, in order to obtain a feasible point $z$ with $F_{1}(z)\leq F_{1}(x)-a$ , where $a>0$ is some constant. Roughly speaking, we are asking how much we must pay in the second objective to obtain an improvement of $a$ in the first objective. If a large increase in $F_{2}$ must be accepted to realize a small decrease in $F_{1}$ , we say that there is a strong trade-oﬀ between the objectives, near the Pareto optimal value $(F_{1}(x),F_{2}(x))$ . If, on the other hand, a large decrease in $F_{1}$ can be obtained with only a small increase in $F_{2}$ , we say that the trade-oﬀ between the objectives is weak (near the Pareto optimal value $(F_{1}(x),F_{2}(x)))$ . 

We can also consider the case in which we trade worse performance in the first objective for an improvement in the second. Here we find how much smaller $F_{2}(z)$ 

can be made, to obtain a feasible point $z$ with $F_{1}(z)\,\le\,F_{1}(x)+a$ , where $a\,>\,0$ is some constant. In this case we receive a benefit in the second objective, i.e. , a reduction in $F_{2}$ compared to $F_{2}(x)$ . If this benefit is large ( i.e. , by increasing $F_{1}$ a small amount we obtain a large reduction in $F_{2}$ ), we say the objectives exhibit a strong trade-oﬀ. If it is small, we say the objectives trade oﬀweakly (near the Pareto optimal value $(F_{1}(x),F_{2}(x))$ ). 

# Optimal trade-oﬀsurface 

The set of Pareto optimal values for a multicriterion problem is called the optimal trade-oﬀsurface (in general, when $q>2$ ) or the optimal trade-oﬀcurve (when $q=2$ ). (Since it would be foolish to accept any point that is not Pareto optimal, we can restrict our trade-oﬀanalysis to Pareto optimal points.) Trade-oﬀanalysis is also sometimes called exploring the optimal trade-oﬀsurface . (The optimal trade- oﬀsurface is usually, but not always, a surface in the usual sense. If the problem has an optimal point, for example, the optimal trade-oﬀsurface consists of a single point, the optimal value.) 

An optimal trade-oﬀcurve is readily interpreted. An example is shown in figure 4.11 , on page 185 , for a (convex) bi-criterion problem. From this curve we can easily visualize and understand the trade-oﬀs between the two objectives. 

• The endpoint at the right shows the smallest possible value of $F_{2}$ , without any consideration of $F_{1}$ . • The endpoint at the left shows the smallest possible value of $F_{1}$ , without any consideration of $F_{2}$ . • By finding the intersection of the curve with a vertical line at $F_{1}=\alpha$ , we can see how large $F_{2}$ must be to achieve $F_{1}\leq\alpha$ . • By finding the intersection of the curve with a horizontal line at $F_{2}=\beta$ , we can see how large $F_{1}$ must be to achieve $F_{2}\leq\beta$ . • The slope of the optimal trade-oﬀcurve at a point on the curve ( i.e. , a Pareto optimal value) shows the local optimal trade-oﬀbetween the two objectives. Where the slope is steep, small changes in $F_{1}$ are accompanied by large changes in $F_{2}$ . • A point of large curvature is one where small decreases in one objective can only be accomplished by a large increase in the other. This is the prover- bial knee of the trade-oﬀcurve , and in many applications represents a good compromise solution. 

All of these have simple extensions to a trade-oﬀsurface, although visualizing a surface with more than three objectives is difficult. 

# Scalarizing multicriterion problems 

When we scalarize a multicriterion problem by forming the weighted sum objective 

$$
\lambda^{T}f_{0}(x)=\sum_{i=1}^{q}\lambda_{i}F_{i}(x),
$$ 

where $\lambda\succ0$ , we can interpret $\lambda_{i}$ as the weight we attach to the $i$ th objective. The weight $\lambda_{i}$ can be thought of as quantifying our desire to make $F_{i}$ small (or our objection to having $F_{i}$ large). In particular, we should take $\lambda_{i}$ large if we want $F_{i}$ to be small; if we care much less about $F_{i}$ , we can take $\lambda_{i}$ small. We can interpret the ratio $\lambda_{i}/\lambda_{j}$ as the relative weight or relative importance of the $i$ th objective compared to the $j$ th objective. Alternatively, we can think of $\lambda_{i}/\lambda_{j}$ as exchange rate between the two objectives, since in the weighted sum objective a decrease (say) in $F_{i}$ by $\alpha$ is considered the same as an increase in $F_{j}$ in the amount $({\lambda_{i}}/{\lambda_{j}})\alpha$ . 

These interpretations give us some intuition about how to set or change the weights while exploring the optimal trade-oﬀsurface. Suppose, for example, that the weight vector $\lambda\succ0$ yields the Pareto optimal point $x^{\mathrm{{peo}}}$ , with objective values $F_{1}(x^{\mathrm{{pe}}}),.\,.\,.\,,F_{q}(x^{\mathrm{{pe}}})$ . To find a (possibly) new Pareto optimal point which trades oﬀa better $k$ th objective value (say), for (possibly) worse objective values for the other objectives, we form a new weight vector λ with 

$$
\tilde{\lambda}_{k}>\lambda_{k},\qquad\tilde{\lambda}_{j}=\lambda_{j},\quad j\neq k,\quad j=1,.\,.\,.\,,q,
$$ 

i.e. , we increase the weight on the $k$ th objective. This yields a new Pareto optimal point $\tilde{x}^{\mathrm{{peo}}}$ po with $F_{k}\big(\tilde{x}^{\mathrm{{po}}}\big)\,\le\,F_{k}\big(x^{\mathrm{{po}}}\big)$ ≤ ) (and usually, $F_{k}(\tilde{x}^{\mathrm{po}})\,<\,F_{k}(x^{\mathrm{po}}))$ )), i.e. , a new Pareto optimal point with an improved $k$ th objective. 

We can also see that at any point where the optimal trade-oﬀsurface is smooth, $\lambda$ gives the inward normal to the surface at the associated Pareto optimal point. In particular, when we choose a weight vector $\lambda$ and apply scalarization, we obtain a Pareto optimal point where $\lambda$ gives the local trade-oﬀs among objectives. 

In practice, optimal trade-oﬀsurfaces are explored by ad hoc adjustment of the weights, based on the intuitive ideas above. We will see later (in chapter 5 ) that the basic idea of scalarization, i.e. , minimizing a weighted sum of objectives, and then adjusting the weights to obtain a suitable solution, is the essence of duality. 

# 4.7.6 Examples 

# Regularized least-squares 

We are given $A\,\in\,\mathbf{R}^{m\times n}$ and $b\,\in\,\mathbf{R}^{m}$ , and want to choose $x\,\in\,\mathbf{R}^{\,n}$ taking into account two quadratic objectives: 

• $F_{1}(x)\,=\,\|A x\,-\,b\|_{2}^{2}\,=\,x^{T}A^{T}A x\,-\,2b^{T}A x\,+\,b^{T}b$ − is a measure of the misfit between $A x$ and $b$ , $F_{2}(x)=\|x\|_{2}^{2}=x^{T}x$ is a measure of the size of $x$ . 

Our goal is to find $x$ that gives a good fit ( i.e. , small $F_{1}$ ) and that is not large ( i.e. , small $F_{2}$ ). We can formulate this problem as a vector optimization problem with respect to the cone $\mathbf{R}_{+}^{2}$ , i.e. , a bi-criterion problem (with no constraints): 

$$
\mathrm{minimize}\ (\mathrm{w.r.t.}\ \mathbf{R}_{+}^{2})\quad f_{0}(x)=(F_{1}(x),F_{2}(x)).
$$ 

![](images/838b722633e904a56ae898d7c5cb3c55413600a5feacbe79bbb3bcc8cec2e79b.jpg) 
Figure 4.11 Optimal trade-oﬀcurve for a regularized least-squares problem. The shaded set is the set of achievable values $(\|A x-b\|_{2}^{2},\|x\|_{2}^{2})$ ∥ ∥ ). The optimal trade-oﬀcurve, shown darker, is the lower left part of the boundary. 

We can scalarize this problem by taking $\lambda_{\mathrm{1}}\,>\,0$ and $\lambda_{2}\,>\,0$ and minimizing the scalar weighted sum objective 

$$
\begin{array}{l c l}{\lambda^{T}f_{0}(x)}&{=}&{\lambda_{1}F_{1}(x)+\lambda_{2}F_{2}(x)}\\ &{=}&{x^{T}(\lambda_{1}A^{T}A+\lambda_{2}I)x-2\lambda_{1}b^{T}A x+\lambda_{1}b^{T}b,}\end{array}
$$ 

which yields 

$$
x(\mu)=(\lambda_{1}A^{T}A+\lambda_{2}I)^{-1}\lambda_{1}A^{T}b=(A^{T}A+\mu I)^{-1}A^{T}b,
$$ 

where $\mu=\lambda_{2}/\lambda_{1}$ . For any $\mu>0$ , this point is Pareto optimal for the bi-criterion problem. We can interpret $\mu=\lambda_{2}/\lambda_{1}$ as the relative weight we assign $F_{2}$ compared to $F_{1}$ . 

This method produces all Pareto optimal points, except two, associated with the extremes $\mu\,\rightarrow\,\infty$ and $\mu\,\rightarrow\,0$ . In the first case we have the Pareto optimal solution $x=0$ , which would be obtained by scalarization with $\lambda=(0,1)$ . At the $A^{\dagger}b$ $A^{\dagger}$ other extreme we have the Pareto optimal solution , where is the pseudo- inverse of $A$ . This Pareto optimal solution is obtained as the limit of the optimal solution of the scalarized problem as $\mu\to0$ , i.e. , as $\lambda\to(1,0)$ . (We will encounter the regularized least-squares problem again in § 6.3.2 .) 

Figure 4.11 shows the optimal trade-oﬀcurve and the set of achievable values for a regularized least-squares problem with problem data $A\in\mathbf{R}^{100\times10}$ , $b\in\mathbf{R}^{100}$ . (See exercise 4.50 for more discussion.) 

# Risk-return trade-oﬀin portfolio optimization 

The classical Markowitz portfolio optimization problem described on page 155 is naturally expressed as a bi-criterion problem, where the objectives are the negative mean return (since we wish to maximize mean return) and the variance of the return: 

$$
\begin{array}{l l}{\mathrm{minimize~(w.r.t.~}\,\mathbf{R}_{+}^{2})}&{(F_{1}(x),F_{2}(x))=(-\overline{{p}}^{T}x,x^{T}\Sigma x)}\\ {\mathrm{subject~to~}}&{\mathbf{1}^{T}x=1,\quad x\succeq0.}\end{array}
$$ 

In forming the associated scalarized problem, we can (without loss of generality) take $\lambda_{1}=1$ and $\lambda_{2}=\mu>0$ : 

$$
\begin{array}{l r c l}{\mathrm{minimize}}&{-\overline{{p}}^{T}x+\mu x^{T}\Sigma x}\\ {\mathrm{subject~to}}&{\mathbf{1}^{T}x=1,\quad x\succeq0,}\end{array}
$$ 

which is a QP. In this example too, we get all Pareto optimal portfolios except for the two limiting cases corresponding to $\mu\to0$ and $\mu\to\infty$ . Roughly speaking, in the first case we get a maximum mean return, without regard for return variance; in the second case we form a minimum variance return, without regard for mean return. Assuming that $\overline{{p}}_{k}>\overline{{p}}_{i}$ for $i\neq k$ , i.e. , that asset $k$ is the unique asset with maximum mean return, the portfolio allocation $x=e_{k}$ is the only one correspond- ing to $\mu\,\rightarrow\,0$ . (In other words, we concentrate the portfolio entirely in the asset that has maximum mean return.) In many portfolio problems asset $n$ corresponds to a risk-free investment, with (deterministic) return . Assuming that $\Sigma$ , with its $r_{\mathrm{rf}}$ last row and column (which are zero) removed, is full rank, then the other extreme Pareto optimal portfolio is $x=e_{n}$ , i.e. , the portfolio is concentrated entirely in the risk-free asset. 

As a specific example, we consider a simple portfolio optimization problem with 4 assets, with price change mean and standard deviations given in the following table. 

Asset 4 is a risk-free asset, with a (certain) $3\%$ return. Assets 3, 2, and 1 have increasing mean returns, ranging from 7% to $12\%$ , as well as increasing standard deviations, which range from $5\%$ to $20\%$ . The correlation coefficients between the assets are $\rho_{12}=30\%$ , $\rho_{13}=-40\%$ , and $\rho_{23}=0\%$ . 

Figure 4.12 shows the optimal trade-oﬀcurve for this portfolio optimization problem. The plot is given in the conventional way, with the horizontal axis show- ing standard deviation ( i.e. , squareroot of variance) and the vertical axis showing expected return. The lower plot shows the optimal asset allocation vector $x$ for each Pareto optimal point. 

The results in this simple example agree with our intuition. For small risk, the optimal allocation consists mostly of the risk-free asset, with a mixture of the other assets in smaller quantities. Note that a mixture of asset 3 and asset 1, which are negatively correlated, gives some hedging, i.e. , lowers variance for a given level of mean return. At the other end of the trade-oﬀcurve, we see that aggressive growth portfolios ( i.e. , those with large mean returns) concentrate the allocation in assets 1 and 2, the ones with the largest mean returns (and variances). 

![](images/a6adf4c7f6209f6fbfe9a29fed19f53d4070e3b3d44f3293c3c3f26c7e838377.jpg) 
Figure 4.12 Top. Optimal risk-return trade-oﬀcurve for a simple portfolio optimization problem. The lefthand endpoint corresponds to putting all resources in the risk-free asset, and so has zero standard deviation. The righthand endpoint corresponds to putting all resources in asset 1, which has highest mean return. Bottom. Corresponding optimal allocations. 

# Bibliography 

Linear programming has been studied extensively since the 1940s, and is the subject of many excellent books, including Dantzig [ Dan63 ], Luenberger [ Lue84 ], Schrijver [ Sch86 ], Papadimitriou and Steiglitz [ PS98 ], Bertsimas and Tsitsiklis [ BT97 ], Vanderbei [ Van96 ], and Roos, Terlaky, and Vial [ RTV97 ]. Dantzig and Schrijver also provide detailed ac- counts of the history of linear programming. For a recent survey, see Todd [ Tod02 ]. 

Schaible [ Sch82 , Sch83 ] gives an overview of fractional programming, which includes linear-fractional problems and extensions such as convex-concave fractional problems (see exercise 4.7 ). The model of a growing economy in example 4.7 appears in von Neumann [ vN46 ]. 

Research on quadratic programming began in the 1950s (see, e.g. , Frank and Wolfe [ FW56 ], Markowitz [ Mar56 ], Hildreth [ Hil57 ]), and was in part motivated by the portfo- lio optimization problem discussed on page 155 (Markowitz [ Mar52 ]), and the LP with random cost discussed on page 154 (see Freund [ Fre56 ]). 

Interest in second-order cone programming is more recent, and started with Nesterov and Nemirovski [ NN94 , § 6.2.3]. The theory and applications of SOCPs are surveyed by Alizadeh and Goldfarb [ AG03 ], Ben-Tal and Nemirovski [ BTN01 , lecture 3] (where the problem is referred to as conic quadratic programming ), and Lobo, Vandenberghe, Boyd, and Lebret [ LVBL98 ]. 

Robust linear programming, and robust convex optimization in general, originated with Ben-Tal and Nemirovski [ BTN98 , BTN99 ] and El Ghaoui and Lebret [ EL97 ]. Goldfarb and Iyengar [ GI03a , GI03b ] discuss robust QCQPs and applications in portfolio optimiza- tion. El Ghaoui, Oustry, and Lebret [ EOL98 ] focus on robust semidefinite programming. 

Geometric programming has been known since the 1960s. Its use in engineering design was first advocated by Duffin, Peterson, and Zener [ DPZ67 ] and Zener [ Zen71 ]. Peterson [ Pet76 ] and Ecker [ Eck80 ] describe the progress made during the 1970s. These articles and books also include examples of engineering applications, in particular in chemical and civil engineering. Fishburn and Dunlop [ FD85 ], Sapatnekar, Rao, Vaidya, and Kang [ SRVK93 ], and Hershenson, Boyd, and Lee [ HBL01 ]) apply geometric programming to problems in integrated circuit design. The cantilever beam design example (page 163 ) is from Vanderplaats [ Van84 , page 147]. The variational characterization of the Perron- Frobenius eigenvalue (page 165 ) is proved in Berman and Plemmons [ BP94 , page 31]. 

Nesterov and Nemirovski [ NN94 , chapter 4] introduced the conic form problem ( 4.49 ) as a standard problem format in nonlinear convex optimization. The cone programming approach is further developed in Ben-Tal and Nemirovski [ BTN01 ], who also describe numerous applications. 

Alizadeh [ Ali91 ] and Nesterov and Nemirovski [ NN94 , § 6.4] were the first to make a systematic study of semidefinite programming, and to point out the wide variety of applications in convex optimization. Subsequent research in semidefinite programming during the 1990s was driven by applications in combinatorial optimization (Goemans and Williamson [ GW95 ]), control (Boyd, El Ghaoui, Feron, and Balakrishnan [ BEFB94 ], Scherer, Gahinet, and Chilali [ SGC97 ], Dullerud and Paganini [ DP00 ]), communications and signal processing (Luo [ Luo03 ], Davidson, Luo, Wong, and Ma [ DLW00 , MDW $^{-+}$ 02 ]), and other areas of engineering. The book edited by Wolkowicz, Saigal, and Vandenberghe [ WSV00 ] and the articles by Todd [ Tod01 ], Lewis and Overton [ LO96 ], and Vandenberghe and Boyd [ VB95 ] provide overviews and extensive bibliographies. Connections between SDP and moment problems, of which we give a simple example on page 170 , are explored in detail by Bertsimas and Sethuraman [ BS00 ], Nesterov [ Nes00 ], and Lasserre [ Las02 ]. The fastest mixing Markov chain problem is from Boyd, Diaconis, and Xiao [ BDX04 ]. 

Multicriterion optimization and Pareto optimality are fundamental tools in economics; see Pareto [ Par71 ], Debreu [ Deb59 ] and Luenberger [ Lue95 ]. The result in example 4.9 is known as the Gauss-Markov theorem (Kailath, Sayed, and Hassibi [ KSH00 , page 97]). 

# Exercises 

# Basic terminology and optimality conditions 

4.1 Consider the optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x_{1},x_{2})}\\ {{\mathrm{subject~to}}\quad2x_{1}+x_{2}\geq1}\\ &{x_{1}+3x_{2}\geq1}\\ &{x_{1}\geq0,\quad x_{2}\geq0.}\end{array}}
$$ 

Make a sketch of the feasible set. For each of the following objective functions, give the optimal set and the optimal value. 

(a) $f_{0}\!\left(x_{1},x_{2}\right)=x_{1}+x_{2}$ . (b) $f_{0}(x_{1},x_{2})=-x_{1}-x_{2}$ . (c) $f_{0}(x_{1},x_{2})=x_{1}$ . (d) $f_{0}(x_{1},x_{2})=\operatorname*{max}\{x_{1},x_{2}\}$ . (e) $f_{0}\bigl(x_{1},x_{2}\bigr)=x_{1}^{2}+9x_{2}^{2}$ . 

4.2 Consider the optimization problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{{}f_{0}(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x)}\end{array}
$$ 

with domain $\operatorname{dom}f_{0}=\{x\mid A x\prec b\}$ , where $A\in\mathbf{R}^{m\times n}$ (with rows $a_{i}^{T}$ ). We assume that $\mathbf{dom}\,f_{0}$ is nonempty. 

Prove the following facts (which include the results quoted without proof on page 141 ). 

(a) $\mathbf{dom}\ f_{0}$ is unbounded if and only if there exists a $v\neq0$ with $A v\preceq0$ . (b) $f_{0}$ is unbounde below if and only if there exists a $v$ with $A v\preceq0$ , $\boldsymbol{A}\boldsymbol{v}\ne0$ . There exi $v$ uch that $A v\ \preceq\ 0$ , $\boldsymbol{A}\boldsymbol{v}\,\neq\,0$ if and only if there exists no z $z\,\succ\,0$ ≻ such that A $A^{T}z=0$ = 0. This follows from the theorem of alternatives in example 2.21 , page 50 . (c) If $f_{0}$ is bounded below then its minimum is attained, i.e. , there exists an $x$ that satisfies the optimality condition ( 4.23 ). (d) The optimal set is affine: $X_{\mathrm{opt}}=\{x^{\star}+v\mid A v=0\}$ , where $x^{\star}$ is any optimal point. 

4.3 Prove that $x^{\star}=(1,1/2,-1)$ is optimal for the optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ (1/2)x^{T}P x+q^{T}x+r}\\ {{\mathrm{subject~to}}}&{-1\leq x_{i}\leq1,\quad i=1,2,3,}\end{array}}
$$ 

where 

$$
P=\left[\begin{array}{r r r}{{13}}&{{12}}&{{-2}}\\ {{12}}&{{17}}&{{6}}\\ {{-2}}&{{6}}&{{12}}\end{array}\right],\;\;\;\;\;\;\;\;q=\left[\begin{array}{r}{{-22.0}}\\ {{-14.5}}\\ {{13.0}}\end{array}\right],\;\;\;\;\;\;\;r=1.
$$ 

4.4 [P. Parrilo] Symmetries and convex optimization. Suppose $\mathcal{G}=\{Q_{1},\L.\L.\L,Q_{k}\}\subseteq\mathbf{R}^{n\times n}$ i a group, i.e. , closed under products and i verse. We say that the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is G - invariant , or symmetric with respect to G , if $f(Q_{i}x)=f(x)$ holds for all $x$ and $i=1,\ldots,k$ . $\begin{array}{r}{\overline{{x}}=(1/k)\sum_{i=1}^{k}Q_{i}x}\end{array}$ We define , which is the average of $x$ over its $\mathcal{G}$ -orbit. We define the fixed subspace of as 

$$
{\mathcal{F}}=\{x\mid Q_{i}x=x,\ i=1,.\,.\,.\,,k\}.
$$ 

(a) Show that for any $x\in\mathbf{R}^{n}$ , we have ${\overline{{x}}}\in{\mathcal{F}}$ . (b) Show that if $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex and $\mathcal{G}$ -invariant, then $f({\overline{{x}}})\leq f(x)$ (c) We say the optimization problem 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\end{array}}
$$ 

is $\mathcal{G}$ -invariant if the objective $f_{0}$ is $\mathcal{G}$ -invariant, and the feasible set is $\mathcal{G}$ -invariant, which means 

$$
f_{1}(x)\leq0,\ldots,f_{m}(x)\leq0\implies f_{1}(Q_{i}x)\leq0,\ldots,f_{m}(Q_{i}x)\leq0,
$$ 

for $i=1,\dots,k$ . Show that if the problem is convex and $\mathcal{G}$ nvariant, and there exists an optimal point, then there e n optimal point in F . In other words, we can adjoin the equality constraints x ∈F to the problem, without loss of generality. 

(d) As an example, suppose $f$ is convex and symmetric, i.e. , $f(P x)\,=\,f(x)$ for every permutation $P$ . Show that if $f$ has a minimizer, then it has a minimizer of the form $\alpha\mathbf{1}$ . means to minimize $f$ over $x\,\in\,\mathbf{R}^{\,n}$ , we can just as well minimize $f(t{\bf1})$ over t $t\in\mathbf{R}$ .) 

4.5 Equivalent convex problems. Show that the following three convex problems are equiva- lent. Carefully explain how the solution of each problem is obtained from the solution of the ot lems. The proble are the matrix $A\,\in\,\mathbf{R}^{m\times n}$ (with rows $a_{i}^{T}$ ), the vector $b\in\mathbf{R}^{m}$ , and the constant M > 0. 

(a) The robust least-squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{m}\phi(a_{i}^{T}x-b_{i}),}\end{array}
$$ 

with variable $x\in\mathbf{R}^{n}$ , where $\phi:\mathbf{R}\rightarrow\mathbf{R}$ is defined as 

$$
\phi(u)=\left\{\begin{array}{l l}{u^{2}}&{|u|\leq M}\\ {M(2|u|-M)}&{|u|>M.}\end{array}\right.
$$ 

(This function is known as the Huber penalty function ; see § 6.1.2 .) 

(b) The least-squares problem with variable weights 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\sum_{i=1}^{m}(a_{i}^{T}x-b_{i})^{2}/(w_{i}+1)+M^{2}\mathbf{1}^{T}w}\\ &{\mathrm{subject~to}\quad w\succeq0,}\end{array}
$$ 

with variables $x\in\mathbf{R}^{n}$ and $w\in\mathbf{R}^{m}$ , and domain $\mathcal{D}=\{(x,w)\in\mathbf{R}^{n}\!\times\!\mathbf{R}^{m}\ |\ w\succ-1\}$ . Hint. Optimize over $w$ assuming $x$ is fixed, to establish a relation with the problem in part (a). 

(This problem can be interpreted as a weighted least-squares problem in which we are allowed to adjust the weight of the i th residual. The weight is one if $w_{i}=0$ , and decreases if we increase $w_{i}$ . The second term in the objective penalizes large values of $w$ , i.e. , large adjustments of the weights.) 

(c) The quadratic program 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\sum_{i=1}^{m}(u_{i}^{2}+2M v_{i})}\\ {{\mathrm{subject~to}}}&{-u-v\prec A x-b\prec u+v}\\ &{0\preccurlyeq u\preccurlyeq M1}\\ &{v\succeq0.}\end{array}}
$$ 

4.6 Handling convex equality constraints. A convex optimization problem can have only linear equality constraint functions. In some special cases, however, it is possible to handle convex equality constraint functions, i.e. , constraints of the form $h(x)\,=\,0$ , where $h$ is convex. We explore this idea in this problem. Consider the optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{h(x)=0,}\end{array}}
$$ 

where $f_{i}$ and $h$ are convex functions with domain $\mathbf{R}^{n}$ . Unless $h$ is affine, this is not a convex optimization problem. Consider the related problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,\ldots,m,}\\ &{h(x)\leq0,}\end{array}}
$$ 

where the convex equality constraint has been relaxed to a convex inequality. This prob- lem is, of course, convex. 

Now suppose we can guarantee that at any optimal solution $x^{\star}$ of the convex prob- lem ( 4.66 ), we have $h(x^{\star})=0$ , i.e. , the inequality $h(x)\leq0$ is always active at the solution. Then we can solve the (nonconvex) problem ( 4.65 ) by solving the convex problem ( 4.66 ). Show that this is the case if there is an index $\scriptstyle{T^{*}}$ such that 

• $f_{0}$ is monotonically increasing in $x_{r}$ • $f_{1},\ldots,f_{m}$ are nondecreasing in $x_{r}$ • $h$ is monotonically decreasing in . $x_{r}$ 

We will see specific examples in exercises 4.31 and 4.58 .

 4.7 Convex-concave fractional problems. Consider a problem of the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)/(c^{T}x+d)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b}\end{array}}
$$ 

where $f_{0},f_{1},.\,.\,.\,,f_{m}$ are convex, and the domain of the objective function is defined as $\{x\in\mathbf{dom}\;f_{0}\ |\ c^{T}x+d>0\}$ . 

(a) Show that this is a quasiconvex optimization problem. (b) Show that the problem is equivalent to 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{g_{0}(y,t)}\\ {{\mathrm{subject~to}}\quad}&{g_{i}(y,t)\leq0,\quad i=1,\ldots,m}\\ &{A y=b t}\\ &{c^{T}y+d t=1,}\end{array}}
$$ 

where $g_{i}$ is the perspective of $f_{i}$ (see § 3.2.6 ). The variables are $y\in\mathbf{R}^{n}$ and $t\in\mathbf{R}$ Show that this problem is convex. 

(c) Following a similar argument, derive a convex formulation for the convex-concave fractional problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)/h(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b}\end{array}}
$$ 

where $f_{0},f_{1},.\,.\,.\,,f_{m}$ are convex, $h$ is concave, the domain of the objective function is defined as $\{x\in\mathbf{dom}\,f_{0}\cap\mathbf{dom}\,h\mid h(x)>0\}$ and $f_{0}(x)\geq0$ everywhere. As an example, apply your technique to the (unconstrained) problem with 

$$
f_{0}(x)=(\mathrm{\mathrm{tr}}\,F(x))/m,\qquad h(x)=(\mathrm{det}(F(x))^{1/m},
$$ 

$\mathbf{dom}(f_{0}/h)=\{x\mid F(x)\succ0\}$ , where $F(x)=F_{0}+x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}$ for given $F_{i}\,\in\,\mathbf{S}^{m}$ ∈ . In this problem, we minimize the ratio of the arith mean over the geometric mean of the eigenvalues of an affine matrix function $F(x)$ ). 

# Linear optimization problems 

4.8 Some simple LPs. Give an explicit solution of each of the following LPs. (a) Minimizing a linear function over an affine set. 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x=b.}\end{array}}
$$ 

(b) Minimizing a linear function over a halfspace. 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ c^{T}x}\\ &{{\mathrm{subject~to}}\quad a^{T}x\leq b,}\end{array}}
$$ 

where $a\ne0$ . 

(c) Minimizing a linear function over a rectangle. 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{c^{T}x}\\ &{{\mathrm{subject~to}}}&{l\preceq x\preceq u,}\end{array}}
$$ 

where $\boldsymbol{l}$ and $u$ satisfy $\mathit{l}\preceq u$ 

(d) Minimizing a linear function over the probability simplex. 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ c^{T}x}\\ &{{\mathrm{subject~to}}\quad\mathbf{1}^{T}x=1,\quad x\succeq0.}\end{array}}
$$ 

What happens if the equality constraint is replaced by an inequality $\mathbf{1}^{T}x\leq1?$ 

We can interpret this LP as a simple portfolio optimization problem. The vector $x$ represents the allocation of our total budget over diﬀerent assets, with $x_{i}$ the fraction invested in asset $i$ . The return of each inves is fixed and given by $-c_{i}$ , so our tota (which we want to maximize) is − $-c^{T}x$ . If we replace the budget constraint 1 ${\bf1}^{T}x=1$ = 1 with an inequality $\mathbf{1}^{T}x\leq1$ , we have the option of not investing a portion of the total budget. 

(e) Minimizing a linear function over a unit box with a total budget constraint. 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\boldsymbol c}^{T}{\boldsymbol x}}&{}\\ {{\mathrm{subject~to}}}&{{\mathbf1}^{T}{\boldsymbol x}=\alpha,}&{0\preceq{\boldsymbol x}\preceq{\mathbf1},}\end{array}
$$ 

where $\alpha$ is an integer between 0 and $n$ . What happens if $\alpha$ is not an integer (but satisfies $0\leq\alpha\leq n$ )? What if we change the equality to an inequality $\mathbf{1}^{T}x\leq\alpha$ ? 

(f) Minimizing a linear function over a unit box with a weighted budget constraint. 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{c^{T}x}\\ &{{\mathrm{subject~to}}}&{d^{T}x=\alpha,\quad0\preceq x\preceq\mathbf{1},}\end{array}}
$$ 

with $d\succ0$ , and $0\leq\alpha\leq\mathbf{1}^{T}d$ . 

4.9 Square LP. Consider the LP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b}\end{array}}
$$ 

with $A$ square and nonsingular. Show that the optimal value is given by 

$$
p^{\star}=\left\{\begin{array}{l l}{c^{T}A^{-1}b}&{A^{-T}c\preceq0}\\ {-\infty}&{\mathrm{otherwise}.}\end{array}\right.
$$ 

4.10 Converting general $L P$ to standard form. Work out the details on page 147 of § 4.3 . Explain in detail the relation between the feasible sets, the optimal solutions, and the optimal values of the standard form LP and the original LP. 

4.11 Problems involving $\ell_{1}$ - and $\ell_{\infty}$ -norms . Formulate the following problems as LPs. Explain in detail the relation between the optimal solution of each problem and the solution of its 

equivalent LP. (a) Minimize $||A x-b||_{\infty}$ ( $\ell_{\infty}$ -norm approximation). (b) Minimize $||A x-b||_{1}$ ( $\ell_{1}$ -norm approximation). (c) Minimize $\|A x-b\|_{1}$ subject to $\|x\|_{\infty}\leq1$ . (d) Minimize $\|{\boldsymbol{x}}\|_{1}$ subject to $\|A x-b\|_{\infty}\leq1$ . (e) Minimize $\|A x-b\|_{1}+\|x\|_{\infty}$ . In each problem, $A\in\mathbf{R}^{m\times n}$ and $b\in\mathbf{R}^{m}$ are given. (See § 6.1 for more problems involving approximation and constrained approximation.) 

4.12 Network ﬂow problem. Consider a network of $n$ nodes, with directed links connecting each pair of nodes. The variables in the problem are the ﬂows on each link: $x_{i j}$ will denote the ﬂow from node $i$ to node $j$ . The cost of the ﬂow along the link from node $i$ to node $j$ is given by $c_{i j}x_{i j}$ , where $c_{i j}$ are given constants. The total cost across the network is 

$$
C=\sum_{i,j=1}^{n}c_{i j}\,x_{i j}.
$$ 

Each link ﬂow $x_{i j}$ is also subject to a given lower bound $l_{i j}$ (usually assumed to be nonnegative) and an upper bound ${u}_{i j}$ . 

The external supply at node $i$ is given by $b_{i}$ , where $b_{i}>0$ means an external ﬂow enters the network at node $i$ , and $b_{i}<0$ means that at node $i$ , an amount $|b_{i}|$ ﬂows out of the network. We assume that 1 $\mathbf{1}^{T}b=0$ i.e. , the total external supply equals total external demand. At each node we have conservation of ﬂow: the total ﬂow into node $i$ along links and the external supply, minus the total ﬂow out along the links, equals zero. 

The problem is to minimize the total cost of ﬂow through the network, subject to the constraints described above. Formulate this problem as an LP. 

4.13 Robust LP with interval coefficients. Consider the problem, with variable $x\in\mathbf{R}^{n}$ , 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b{\mathrm{~for~all~}}A\in{\mathcal{A}},}\end{array}}
$$ 

where $\mathcal{A}\subseteq\mathbf{R}^{m\times n}$ is the set 

$$
\begin{array}{r}{\mathcal{A}=\{A\in{\bf R}^{m\times n}\mid\bar{A}_{i j}-V_{i j}\leq A_{i j}\leq\bar{A}_{i j}+V_{i j},\ i=1,\ldots,m,\ j=1,\ldots,n\}.}\end{array}
$$ 

(The matrices A and $V$ are given.) This problem can be interpreted as an LP where each coefficient of $A$ is only known to lie in an interval, and we require that $x$ must satisfy the constraints for all possible values of the coefficients. 

Express this problem as an LP. The LP you construct should be efficient, i.e. , it should not have dimensions that grow exponentially with $n$ or $_{m}$ . 

4.14 ximating a matrix in infinity norm. The $\ell_{\infty}$ -norm induced norm of a matrix $A\in$ $\mathbf{R}^{m\times n}$ , denoted $\|A\|_{\infty}$ , is given by 

$$
\|A\|_{\infty}=\operatorname*{sup}_{x\neq0}{\frac{\|A x\|_{\infty}}{\|x\|_{\infty}}}=\operatorname*{max}_{i=1,\ldots,m}\sum_{j=1}^{n}|a_{i j}|.
$$ 

This norm is sometimes called the max-row-sum norm, for obvious reasons (see § A.1.5 ). Consider the problem of approximating a matrix, in the max-row-sum norm, by a linear combination of o trices. That is, we are given $k+1$ matrices $A_{0}$ , . . . , $A_{k}\in\mathbf{R}^{m\times n}$ , and need to find x $x\in\mathbf{R}^{k}$ that minimizes 

$$
\|A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{k}A_{k}\|_{\infty}.
$$ 

Express this problem as a linear program. Explain the significance of any extra variables in your LP. Carefully explain how your LP formulation solves this problem, e.g. , what is the relation between the feasible set for your LP and this problem? 

4.15 Relaxation of Boolean $L P$ . In a Boolean linear program , the variable $x$ is constrained to have components equal to zero or one: 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{c^{T}x}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{x_{i}\in\{0,1\},\quad i=1,.\,.\,.\,,n.}\end{array}}
$$ 

In general, such problems are very difficult to solve, even though the feasible set is finite (containing at most $2^{n}$ points). 

In a general method called relaxation , the constraint that $x_{i}$ be zero or one is replaced with the linear inequalities $0\leq x_{i}\leq1$ : 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{c^{T}x}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{0\leq x_{i}\leq1,}&{i=1,.\,.\,.\,,n.}\end{array}}
$$ 

We refer to this problem as the $L P$ relaxation of the Boolean LP ( 4.67 ). The LP relaxation is far easier to solve than the original Boolean LP. 

(a) Show that the optimal value of the LP relaxation ( 4.68 ) is a lower bound on the optimal value of the Boolean LP ( 4.67 ). What can you say about the Boolean LP if the LP relaxation is infeasible? (b) It sometimes happens that the LP relaxation has a solution with $x_{i}\in\{0,1\}$ . What can you say in this case? 

4.16 nimum fuel optimal control. We consider a line ical system with state $x(t)\in$ $\mathbf{R}^{n}$ , $t\,=\,0,\ldots,N$ , and actuator or input signal $u(t)\,\in\,\mathbf{R}$ ∈ , for $t\,=\,0,.\,.\,.\,,N\,-\,1$ . The dynamics of the system is given by the linear recurrence 

$$
x(t+1)=A x(t)+b u(t),\quad t=0,.\,.\,,N-1,
$$ 

$A\,\in\,\mathbf{R}^{n\times n}$ and $b\,\in\,\mathbf{R}^{n}$ are given. We assume that the initial state is zero, i.e. , $x(0)=0$ 

The minimum fuel optimal control problem is to choose the inputs $u(0),\ldots,u(N-1)$ so as to minimize the total fuel consumed, which is given by 

$$
F=\sum_{t=0}^{N-1}f(u(t)),
$$ 

subject to the constraint that $x(N)\,=\,x_{\mathrm{{des}}}$ , where $N$ is the (given) time horizon, and $x_{\mathrm{des}}\in\mathbf{R}^{n}$ is the (given) desired final or target state. The function $f:\mathbf{R}\rightarrow\mathbf{R}$ is the fuel use map for the actuator, and gives the amount of fuel used as a function of the actuator signal amplitude. In this problem we use 

$$
f(a)={\left\{\begin{array}{l l}{|a|}&{|a|\leq1}\\ {2|a|-1}&{|a|>1.}\end{array}\right.}
$$ 

This means that fuel use is proportional to the absolute value of the actuator signal, for actuator signals between $^{-1}$ and $^{1}$ ; for larger actuator signals the marginal fuel efficiency is half. 

Formulate the minimum fuel optimal control problem as an LP. 

4.17 Optimal activity levels. We consider the selection of $n$ nonnegative activity levels, denoted $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ . These activities consume $m$ resources, which are limited. Activity $j$ consumes $A_{i j}x_{j}$ of resource $i$ , where $A_{i j}$ are given. The total resource consumption is additive, so the tota of r source $i$ consumed is $\begin{array}{r}{c_{i}=\sum_{j=1}^{n}A_{i j}x_{j}}\end{array}$ . (Ordinarily we have $A_{i j}\,\geq\,0$ , i.e. , activity j consumes resource $i$ . But we allow the possibility that $A_{i j}\,<\,0$ , which means that activity j actually generates resource $i$ as a by-product.) Each resource consumption is limited: we must have $c_{i}\leq c_{i}^{\operatorname*{max}}$ , where $c_{i}^{\operatorname*{max}}$ are given. Each activity generates revenue, which is a piecewise-linear concave function of the activity level: 

$$
r_{j}(x_{j})=\left\{\begin{array}{l l}{p_{j}x_{j}}&{0\leq x_{j}\leq q_{j}}\\ {p_{j}q_{j}+p_{j}^{\mathrm{disc}}(x_{j}-q_{j})}&{x_{j}\geq q_{j}.}\end{array}\right.
$$ 

Here $p_{j}\ >\ 0$ is the basic price, $q_{j}\ >\ 0$ is the quantity discount level, and $p_{j}^{\mathrm{disc}}$ is the quantity discount price, for (the product of) activity $j$ . (We have $0<p_{j}^{\mathrm{disc}}<p_{j}$ .) The total revenue is the sum of the revenues associated with each activity, i.e. , $\scriptstyle\sum_{j=1}^{n}r_{j}\left(x_{j}\right)$ ). The goal is to choose activity levels that maximize the total revenue while respecting the resource limits. Show how to formulate this problem as an LP. 

4.18 Separating hyperplanes and spheres. Suppose you are given two sets of points in $\mathbf{R}^{n}$ , $\{v^{1},v^{2},.\,.\,.\,,v^{K}\}$ and $\{w^{1},w^{2},.\,.\,.\,,w^{L}\}$ . Formulate the following two problems as LP fea- sibility problems. 

(a) Dete a hyperplane that separates the two sets, i.e. , find $a\in\mathbf{R}^{n}$ and $b\in\mathbf{R}$ with $a\ne0$ ̸ = 0 such that 

$$
a^{T}v^{i}\leq b,\quad i=1,\ldots,K,\qquad a^{T}w^{i}\geq b,\quad i=1,\ldots,L.
$$ 

Note that we requir $a\ne0$ ou have to make sure that your formulation excludes the trivial solution a = 0, b = 0. You can assume that 

$$
\mathbf{rank}\left[\begin{array}{c c c c c c c c}{v^{1}}&{v^{2}}&{\cdot\cdot\cdot}&{v^{K}}&{w^{1}}&{w^{2}}&{\cdot\cdot\cdot}&{w^{L}}\\ {1}&{1}&{\cdot\cdot\cdot}&{1}&{1}&{1}&{\cdot\cdot\cdot}&{1}\end{array}\right]=n+1
$$ 

( i.e. , the affine hull of the $K+L$ points has dimension $n$ ). 

(b) Determine a sphere separating the two sets of points, i.e. , find $x_{c}\in\mathbf{R}^{n}$ and $R\geq0$ such that 

$$
\|v^{i}-x_{c}\|_{2}\leq R,\quad i=1,\ldots,K,\qquad\|w^{i}-x_{c}\|_{2}\geq R,\quad i=1,\ldots,L.
$$ 

(Here is the center of the sphere; $R$ is its radius.) $x_{c}$ 

(See chapter 8 for more on separating hyperplanes, separating spheres, and related topics.) 

4.19 Consider the problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|A x-b\|_{1}/(c^{T}x+d)}\\ {\mathrm{subject~to}}&{\|x\|_{\infty}\leq1,}\end{array}
$$ 

wher $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , $c\in\mathbf{R}^{n}$ , and $d\in\mathbf{R}$ . We assume that $d>\|c\|_{1}$ , which implies that c $c^{T}x+d>0$ 0 for all feasible $x$ . 

(a) Show that this is a quasiconvex optimization problem. (b) Show that it is equivalent to the convex optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\|A y-b t\|_{1}}\\ {{\mathrm{subject~to}}}&{\|y\|_{\infty}\leq t}\\ &{c^{T}y+d t=1,}\end{array}}
$$ 

with variables $y\in\mathbf{R}^{n}$ , $t\in\mathbf{R}$ . 

4.20 Power assignment in a wireless communication system. We consider $n$ transmitters with powers $p_{1},.\,.\,.\,,p_{n}\,\geq\,0$ , transmitti $n$ eivers. These powers are the optimization variables in the problem. We $G\,\in\,\mathbf{R}^{n\times n}$ denote the matrix of path ains from t e transmitters to the receivers $G_{i j}\,\geq\,0$ ≥ th gain from transmitter j to receiver i The signal power at receiver i is then $S_{i}=G_{i i}p_{i}$ , and the interference power at receiver i s $\begin{array}{r}{I_{i}=\sum_{k\neq i}G_{i k}p_{k}}\end{array}$ . The signal to interference plus noise ratio , denoted SINR, at receiver i , is given by $S_{i}/(I_{i}\,+\,\sigma_{i})$ , where $\sigma_{i}~>~0$ is the (self-) noise power in receiver $i$ . The objective in the problem is to maximize the minimum SINR ratio, over all receivers, i.e. , to maximize 

$$
\operatorname*{min}_{i=1,\dots,n}\frac{S_{i}}{I_{i}+\sigma_{i}}.
$$ 

There are a number of constraints on the powers that must be satisfied, in addition to the e $p_{i}\,\geq\,0$ st is a maximum allowable power for each transmitter, i.e. , $p_{i}\,\leq\,P_{i}^{\operatorname*{max}}$ ≤ , where P $P_{i}^{\mathrm{max}}\,>\,0$ 0 is given. In addition, the transmitters are partitioned into groups, with each group sharing the same power supply, so there is a total power constraint for each group of transmitter powers. More precisely, we have subsets $K_{1},.\ldots,K_{m}$ of $\{1,\ldots,n\}$ with $K_{1}\cup\cdot\cdot\cup K_{m}=\{1,.\,.\,.\,,n\}$ , and $K_{j}\cap K_{l}=0$ f $j\neq l$ . For each group $K_{l}$ , the total associated transmitter power cannot exceed $P_{l}^{\mathrm{gp}}>0$ 0: 

$$
\sum_{k\in K_{l}}p_{k}\le P_{l}^{\mathrm{gr}},\quad l=1,\ldots,m.
$$ 

Finally, we have a limit $P_{k}^{\mathrm{{rv}}}>0$ 0 on the total received power at each receiver: 

$$
\sum_{k=1}^{n}G_{i k}p_{k}\leq P_{i}^{\mathrm{re}},\quad i=1,.\,.\,.\,,n.
$$ 

(This constraint reﬂects the fact that the receivers will saturate if the total received power is too large.) 

Formulate the SINR maximization problem as a generalized linear-fractional program. 

# Quadratic optimization problems 

4.21 Some simple QCQPs. Give an explicit solution of each of the following QCQPs. (a) Minimizing a linear function over an ellipsoid centered at the origin. 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ c^{T}x}\\ &{{\mathrm{subject~to}}\quad x^{T}A x\leq1,}\end{array}}
$$ 

$\mathrm{~\bf~}_{\cdot\cdot}^{A}\,\in\,{\bf~S}_{++}^{n}$ and $c\neq0$ . What is the solution if the problem is not convex $(A\not\in{\bf S}_{+}^{n}$ )? 

(b) Minimizing a linear function over an ellipsoid. 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad(x-x_{c})^{T}A(x-x_{c})\leq1,}\end{array}}
$$ 

where $A\in\mathbf{S}_{++}^{n}$ and $c\neq0$ . 

(c) Minimizing a quadratic form over an ellipsoid centered at the origin. 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ x^{T}B x}\\ &{{\mathrm{subject~to}}\quad x^{T}A x\leq1,}\end{array}}
$$ 

where $A\in\mathbf{S}_{++}^{n}$ and $B\in\mathbf{S}_{+}^{n}$ . Also consider the nonconvex extension with $B\notin\mathbf{S}_{+}^{n}$ (See $\S$ B.1 .) 

4.22 Consider the QCQP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\;(1/2)x^{T}P x+q^{T}x+r}\\ {{\mathrm{subject~to}}}&{x^{T}x\leq1,}\end{array}}
$$ 

with $P\in\mathbf{S}_{++}^{n}$ . Show that $x^{\star}=-(P+\lambda I)^{-1}q$ where $\lambda=\operatorname*{max}\{0,\lambda\}$ } and λ is the largest solution of the nonlinear equation 

$$
q^{T}(P+\lambda I)^{-2}q=1.
$$ 

4.23 $\ell_{4}$ -norm approximation via $Q C Q P$ . Formulate the $\ell_{4}$ -norm approximation problem 

$$
\begin{array}{r l}{\mathrm{minimize}\:}&{{}\|A x-b\|_{4}=(\sum_{i=1}^{m}(a_{i}^{T}x-b_{i})^{4})^{1/4}}\end{array}
$$ 

as a QCQP. The matrix $A\in\mathbf{R}^{m\times n}$ (with rows a $a_{i}^{T}$ ) and the vector $b\in\mathbf{R}^{m}$ are given.

 4.24 Complex $\ell_{1}$ -, $\ell_{2}$ - and $\ell_{\infty}$ -norm approximation. Consider the problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{p},
$$ 

where $A\in\mathbf{C}^{m\times n}$ , $b\in\mathbf{C}^{m}$ , and the variable is $x\in\mathbf{C}^{n}$ . The complex $\ell_{p}$ -norm is defined by 

$$
\|y\|_{p}=\left(\sum_{i=1}^{m}\left|y_{i}\right|^{p}\right)^{1/p}
$$ 

for $p\geq1$ , and $\|y\|_{\infty}=\max_{i=1,\ldots,m}|y_{i}|$ . For $p=1$ , $2$ , and $\infty$ , express the complex $\ell_{p}$ -norm approximation problem as a QCQP or SOCP with real variables and data. 

4.25 Linear separation of two sets of ellipsoids. Suppose we are given $K+L$ ellipsoids 

$$
\mathcal E_{i}=\{P_{i}u+q_{i}\mid\|u\|_{2}\le1\},\quad i=1,\ldots,K+L,
$$ 

ere $P_{i}\in\mathbf{S}^{n}$ . We terested in finding a hyper h ctly separates $\mathcal{E}_{1},\,.\,.\,.$ , $\dot{\mathcal{E}}_{K}$ E from E $\dot{\mathcal{E}}_{K+1}$ , . . . , E $\mathcal{E}_{K+L}$ , i.e. , we want to compute a $a\in\mathbf{R}^{n}$ ∈ , b $b\in\mathbf{R}$ ∈ such that 

$$
\boldsymbol{a}^{T}\boldsymbol{x}+\boldsymbol{b}>0\mathrm{~for~}\boldsymbol{x}\in\mathcal{E}_{1}\cup\dots\cup\mathcal{E}_{K},\qquad\boldsymbol{a}^{T}\boldsymbol{x}+\boldsymbol{b}<0\mathrm{~for~}\boldsymbol{x}\in\mathcal{E}_{K+1}\cup\dots\cup\mathcal{E}_{K+L},
$$ 

or prove that no such hyperplane exists. Express this problem as an SOCP feasibility problem. 

4.26 Hyperbolic constraints as $S O C$ constraints. Verify that $x\in\mathbf{R}^{n}$ , $y,z\in\mathbf{R}$ satisfy 

$$
x^{T}x\leq y z,\qquad y\geq0,\qquad z\geq0
$$ 

if and only if 

$$
\left\|{\left[\begin{array}{l}{\ \ 2x}\\ {y-z}\end{array}\right]}\right\|_{2}\leq y+z,\qquad y\geq0,\qquad z\geq0.
$$ 

Use this observation to cast the following problems as SOCPs. 

(a) Maximizing harmonic mean. 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\left(\sum_{i=1}^{m}1/(a_{i}^{T}x-b_{i})\right)^{-1},}\end{array}
$$ 

with domain $\{x\mid A x\succ b\}$ , where $a_{i}^{T}$ is the $i$ th row of $A$ . 

(b) Maximizing geometric mean. 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\left(\prod_{i=1}^{m}(a_{i}^{T}x-b_{i})\right)^{1/m},}\end{array}
$$ 

with domain $\{x\mid A x\succeq b\}$ , where $a_{i}^{T}$ is the $i$ th row of $A$ . 

4.27 Matrix fractional minimization via $S O C P$ . Express the following problem as an SOCP: 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\,(A x+b)^{T}(I+B\,\mathbf{diag}(x)B^{T})^{-1}(A x+b)}\\ &{{\mathrm{subject~to}}\quad x\succeq0,}\end{array}}
$$ 

with $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , $\boldsymbol{B}\in\mathbf{R}^{m\times n}$ . The variable is $x\in\mathbf{R}^{n}$ Hint. First show that the problem is equivalent to 

$$
\begin{array}{l r l}{\mathrm{minimize}}&{\boldsymbol{v}^{T}\boldsymbol{v}+\boldsymbol{w}^{T}\,\mathbf{diag}(x)^{-1}\boldsymbol{w}}\\ {\mathrm{subject~to}}&{\boldsymbol{v}+\boldsymbol{B}\boldsymbol{w}=A x+b}\\ &{x\succeq0,}\end{array}
$$ 

th variables $v\in\mathbf{R}^{m}$ , $w,x\in\mathbf{R}^{n}$ . (If $x_{i}=0$ we interpret $w_{i}^{2}/x_{i}$ as zero if $w_{i}=0$ and as ∞ otherwise.) Then use the results of exercise 4.26 . 

4.28 Robust quadratic programming. In § 4.4.2 we discussed robust linear programming as an application of second-order cone programming. In this problem we consider a similar robust variation of the (convex) quadratic program 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ (1/2)x^{T}P x+q^{T}x+r}\\ {{\mathrm{subject~to}}}&{\ A x\preceq b.}\end{array}}
$$ 

For simplicity we assume that only the matrix $P$ is subject to errors, and the other parameters $(q,\,r,\,A,\,b)$ are exactly known. The robust quadratic program is defined as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\quad\operatorname*{sup}_{P\in{\mathcal{E}}}((1/2)x^{T}P x+q^{T}x+r)}\\ {{\mathrm{subject~to}}}&{\;A x\preceq b}\end{array}}
$$ 

where $\mathcal{E}$ is the set of possible matrices $P$ . 

For each of the following sets $\mathcal{E}$ , express the robust QP as a convex problem. Be as specific as you can. If the problem can be expressed in a standard form ( e.g. , QP, QCQP, SOCP, SDP), say so. 

(a) A finite set of matrices: $\mathcal{E}=\{P_{1},.\,.\,.\,,P_{K}\}$ , where $P_{i}\in\mathbf{S}_{+}^{n}$ , $i=1,\dots,K$ . 

(b) A set spe y a nominal value $P_{0}\in\mathbf{S}_{+}^{n}$ plus a bound on the eigenvalues of the deviation P $P-P_{0}$ : 0 

$$
\mathcal{E}=\{P\in\mathbf{S}^{n}\mid-\gamma I\preceq P-P_{0}\preceq\gamma I\}
$$ 

where $\gamma\in\mathbf{R}$ and $P_{0}\in\mathbf{S}_{+}^{n}$ , 

(c) An ellipsoid of matrices: 

$$
\mathcal{E}=\left\{P_{0}+\sum_{i=1}^{K}P_{i}u_{i}\left|\begin{array}{l}{\|u\|_{2}\leq1}\\ {\|u\|_{2}\leq1}\end{array}\right\}.
$$ 

You can assume $P_{i}\in\mathbf{S}_{+}^{n}$ , $i=0,.\,.\,.\,,K$ . 

4.29 Maximizing probability of satisfying a linear inequality. Let $c$ be a random variable in $\mathbf{R}^{n}$ , normally distributed with mean c and covariance matrix $R$ . Consider the problem 

$$
\begin{array}{l r c l}{\mathrm{maximize}}&{\mathbf{probb}(c^{T}x\geq\alpha)}\\ {\mathrm{subject~to}}&{F x\preceq g,\quad A x=b.}\end{array}
$$ 

Assuming there exists a feasible point x for which $\bar{c}^{I}\tilde{x}\,\geq\,\alpha$ ≥ , show that this problem is equivalent to a convex or quasiconvex optimization problem. Formulate the problem as a QP, QCQP, or SOCP (if the problem is convex), or explain how you can solve it by solving a sequence of QP, QCQP, or SOCP feasibility problems (if the problem is quasiconvex). 

# Geometric programming 

4.30 A heated ﬂuid at temperature $T$ (degrees above ambient temperature) ﬂows in a pipe with fixed length and circular cross section with radius $r$ . A layer of insulation, with thickness $w\ll r$ , surrounds the pi to reduce heat loss through the pipe walls. The design variables in this problem are T , $r$ , and $w$ . The heat loss is (approximately) proportional to $T r/w$ , so over a fixed lifetime, the energy cost due to heat loss is given by $\alpha_{1}T r/w$ . The cost of the pipe, which has a fixed wall thickness, is approximately proportional to the total material, i.e. , it is given by $\alpha_{2}r$ . The cost of the insulation is also approximately proportional to the total insulation material, i.e. , $\alpha_{3}r w$ (using $w\ll r$ ). The total cost is the sum of these three costs. The heat ﬂow down the pipe is entirely due to the ﬂow of the ﬂuid, which has a fixed velocity, i.e. , it is given by $\alpha_{4}T r^{2}$ . The constants $\alpha_{i}$ are all positive, as are the variables $T$ , $r$ , and $w$ . Now the problem: maximize the total heat ﬂow down the pipe, subject to an upper limit $C_{\mathrm{max}}$ on total cost, and the constraints 

$$
T_{\operatorname*{min}}\leq T\leq T_{\operatorname*{max}},\qquad r_{\operatorname*{min}}\leq r\leq r_{\operatorname*{max}},\qquad w_{\operatorname*{min}}\leq w\leq w_{\operatorname*{max}},\quad w\leq0.1r.
$$ 

Express this problem as a geometric program. 

4.31 Recursive formulation of optimal beam design problem. Show that the GP ( 4.46 ) is equiv- alent to the GP 

$$
\begin{array}{r l}{\mathrm{minimize}\quad}&{\sum_{i=1}^{N}w_{i}h_{i}}\\ {\mathrm{subject~to}\quad}&{w_{i}/w_{\operatorname*{max}}\leq1,\quad w_{\operatorname*{min}}/w_{i}\leq1,\quad i=1,\ldots,N}\\ &{h_{i}/h_{\operatorname*{max}}\leq1,\quad h_{\operatorname*{min}}/h_{i}\leq1,\quad i=1,\ldots,N}\\ &{h_{i}/(w_{i}S_{\operatorname*{max}})\leq1,\quad S_{\operatorname*{min}}w_{i}/h_{i}\leq1,\quad i=1,\ldots,N}\\ &{6i F/(\sigma_{\operatorname*{max}}w_{i}h_{i}^{2})\leq1,\quad i=1,\ldots,N}\\ &{(2i-1)d_{i}/v_{i}+v_{i+1}/v_{i}\leq1,\quad i=1,\ldots,N}\\ &{(i-1/3)d_{i}/y_{i}+v_{i+1}/y_{i}+y_{i+1}/y_{i}\leq1,\quad i=1,\ldots,N}\\ &{y_{1}/y_{\operatorname*{max}}\leq1}\\ &{E w_{i}h_{i}^{3}d_{i}/(6F)=1,\quad i=1,\ldots,N.}\end{array}
$$ 

The variables are $w_{i}$ , $h_{i}$ , $v_{i}$ , $d_{i}$ , $y_{i}$ for $i=1,\cdot\cdot\cdot,N$ . 

4.32 Approximating a fu as a m Suppose the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is diﬀer- entiable at a point x $x_{0}\,\succ\,0$ ≻ 0, with f $f\!\left(x_{0}\right)\,>\,0$ 0. How would you find a monomial function ${\hat{f}}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ → such that $f(x_{0})={\hat{f}}(x_{0})$ ) and for $x$ near $x_{0}$ , ${\hat{f}}(x)$ ) is very near $f(x)$ ? 

4.33 Express the following problems as convex optimization problems. 

(a) Minimize $\operatorname*{max}\{p(x),q(x)\}$ , where $p$ and $q$ are posynomials. (b) Minimize $\exp(p(x))+\exp(q(x))$ , where $p$ and $q$ are posynomials. (c) Min mize $p(x)/(r(x)-q(x))$ , subject to $r(x)\,>\,q(x)$ , where $p,q$ are posynomials, and r is a monomial. 

4.34 Log-convexity of Perron-Frobenius eigenvalue. Let $A\in\mathbf{R}^{n\times n}$ be an elementwise positive matrix, i.e. , $A_{i j}\ >\ 0$ . (The results of this problem hold for irreducible nonnegative matrices as well.) Let $\lambda_{\mathrm{pf}}(A)$ denotes its Perron-Frobenius eigenvalue, i.e. , its eigenvalue of largest magnitude. (See the definition and the example on page 165 .) Show that $\log\lambda_{\mathrm{pf}}(A)$ is a convex function of $\log A_{i j}$ . This means, for example, that we have the inequality 

$$
\lambda_{\mathrm{pf}}(C)\leq(\lambda_{\mathrm{pf}}(A)\lambda_{\mathrm{pf}}(B))^{1/2}\,,
$$ 

where ${C_{i j}}=(A_{i j}B_{i j})^{1/2}$ , and $A$ and $B$ are elementwise positive matrices. 

Hint. Use the characterization of the Perron-Frobenius eigenvalue given in ( 4.47 ), or, alternatively, use the characterization 

$$
\log\lambda_{\mathrm{pf}}(A)=\operatorname*{lim}_{k\to\infty}(1/k)\log({\bf1}^{T}A^{k}{\bf1}).
$$ 

4.35 Signomial and geometric programs. A signomial is a linear combination of monomials of some positive variables $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ . Signomials are more general than posynomials, which are signomials with all positive coefficients. A signomial program is an optimization problem of the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p,}\end{array}}
$$ 

where $f_{0},\ldots,f_{m}$ and $h_{1},.\,.\,.\,,h_{p}$ are signomials. In general, signomial programs are very difficult to solve. 

Some signomial programs can be transformed to GPs, and therefore solved efficiently. Show how to do this for a signomial program of the following form: 

• The objective signomial $f_{0}$ is a posynomial, i.e. , its terms have only positive coeffi- cients. • Each inequality constraint signomial $f_{1},\ldots,f_{m}$ has exactly one term with a negative coefficient: $f_{i}=p_{i}-q_{i}$ where $p_{i}$ is posynomial, and $q_{i}$ is monomial. • Each equality constraint signomial $h_{1},.\,.\,.\,,h_{p}$ has e erm with a positive coefficient and one term with a negative coefficient: h $h_{i}=r_{i}-s_{i}$ − i where $r_{i}$ and $s_{i}$ are monomials. 

4.36 Explain how to reformulate a general GP as an equivalent GP in which every posynomial (in the objective and constraints) has at most two monomial terms. Hint. Express each sum (of monomials) as a sum of sums, each with two terms. 

4.37 Generalized posynomials and geometric programming. Let $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ be positive variables, e the functions $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $i=1,\dots,k$ , are posynomials of $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ . If $\phi:\mathbf{R}^{k}\rightarrow\mathbf{R}$ → is a polynomial with nonnegative coefficients, then the composition 

$$
h(x)=\phi{\bigl(}f_{1}(x),.\,.\,,f_{k}(x){\bigr)}
$$ 

is a posynomial, since posynomials are closed under products, sums, and multiplication by nonnegative scalars. For example, suppose $f_{1}$ and $f_{2}$ are posynomials, and consider the polynomial $\phi(z_{1},z_{2})=3z_{1}^{2}z_{2}+2z_{1}+3z_{2}^{3}$ (which has nonnegative coefficients). Then $h=3f_{1}^{2}f_{2}+2f_{1}+f_{2}^{3}$ is a posynomial. 

In this problem we consider a generalization of this idea, in which $\phi$ is allowed to be posynomial, i.e. , can have fractional exponents. Specifically, assume that $\phi:\mathbf{R}^{k}\ \rightarrow$ R is a posynomial, with all its exponents nonnegative. In this case we will call the function $h$ defined in ( 4.69 ) a generalized posynomial . As an example, suppose $f_{1}$ and $f_{2}$ are posynomials, and consider the posynomial (with nonnegative exponents) $\phi(z_{1},z_{2})=$ $2z_{1}^{0.3}z_{2}^{1.2}+z_{1}z_{2}^{0.5}+2$ + 2. Then the function 

$$
h(x)=2f_{1}(x)^{0.3}f_{2}(x)^{1.2}+f_{1}(x)f_{2}(x)^{0.5}+2
$$ 

is a generalized posynomial. Note that it is not a posynomial, however (unless $f_{\mathrm{1}}$ and $f_{2}$ are monomials or constants). 

A generalized geometric program (GGP) is an optimization problem of the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{h_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{h_{i}(x)\leq1,\quad i=1,\ldots,m}\\ &{g_{i}(x)=1,\quad i=1,\ldots,p,}\end{array}}
$$ 

where $g_{1},\ldots,g_{p}$ are monomials, and $h_{0},\ldots,h_{m}$ are generalized posynomials. 

Show how to express this generalized geometric program as an equivalent geometric pro- gram. Explain any new variables you introduce, and explain how your GP is equivalent to the GGP ( 4.70 ). 

# Semidefinite programming and conic form problems 

4.38 LMIs and SDPs with one variable . The generalized eigenvalues of a matrix pair $(A,B)$ , where $A,B\in\mathbf{S}^{n}$ , are defined as the roots of the polynomial $\operatorname*{det}(\lambda B-A)$ (see $\S$ A.5.3 ). Suppose $B$ is nonsingular, and that $A$ and $B$ can be simultaneously diagonalized by a congruence, i.e. , there exists a nonsingular $R\in{\mathbf{R}}^{n\times n}$ such that 

$$
\boldsymbol{R}^{T}\boldsymbol{A}\boldsymbol{R}=\mathbf{diag}(\boldsymbol{a}),\qquad\boldsymbol{R}^{T}\boldsymbol{B}\boldsymbol{R}=\mathbf{diag}(\boldsymbol{b}),
$$ 

wher $a,b\in\mathbf{R}^{n}$ A sufficient condition for this to hold is that there exists $t_{1}$ , $t_{2}$ such that t $t_{1}A+t_{2}B\succ0$ ≻ 0.) 

(a) Show that the generalized eigenvalues of $(A,B)$ are real, and given by $\lambda_{i}=a_{i}/b_{i}$ , $i=1,\dots,n$ . 

(b) Express the solution of the SDP 

$$
{\begin{array}{l l}{\operatorname{minimize}}&{c t}\\ {\operatorname{subject\to}}&{t B\preceq A,}\end{array}}
$$ 

with variable $t\in\mathbf{R}$ , in terms of $a$ and $b$ .

 4.39 SDPs and congruence transformations . Consider the SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ c^{T}x}\\ &{{\mathrm{subject~to}}\quad x_{1}F_{1}+x_{2}F_{2}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0,}\end{array}}
$$ 

with $F_{i},G\in\mathbf{S}^{k}$ , $c\in\mathbf{R}^{n}$ 

(a) Suppose $R\in\mathbf{R}^{k\times k}$ is nonsingular. Show that the SDP is equivalent to the SDP 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\ c^{T}x}\\ &{\mathrm{subject~to}\quad x_{1}\tilde{F}_{1}+x_{2}\tilde{F}_{2}+\cdot\cdot\cdot+x_{n}\tilde{F}_{n}+\tilde{G}\preceq0,}\end{array}
$$ 

where $\tilde{F}_{i}=R^{T}F_{i}R$ , ${\tilde{G}}=R^{T}G R$ . 

such that ${\tilde{F}}_{i}$ and $\tilde{G}$ (b) Suppose there exists a nonsingular $R$ are diagonal. Show that the SDP is equivalent to an LP. 

(c) Suppose there exists a nonsingular $R$ such that ${\tilde{F}}_{i}$ and G have the form i 

$$
\begin{array}{r}{\tilde{F}_{i}=\left[\begin{array}{l l}{\alpha_{i}I}&{a_{i}}\\ {a_{i}^{T}}&{\alpha_{i}}\end{array}\right],\quad i=1,\ldots,n,\qquad\tilde{G}=\left[\begin{array}{l l}{\beta I}&{b}\\ {b^{T}}&{\beta}\end{array}\right],}\end{array}
$$ 

where $\alpha_{i},\beta\in\mathbf{R}$ , $a_{i},b\in\mathbf{R}^{k-1}$ . Show that the SDP is equivalent to an SOCP with a single second-order cone constraint. 

4.40 LPs, QPs, QCQPs, and SOCPs as SDPs. Express the following problems as SDPs. (a) The LP ( 4.27 ). 

(b) The QP ( 4.3 CQP ( 4.35 ) and the SOCP ( 4.36 ). Hint. Suppose $A\in\mathbf{S}_{++}^{\prime}$ , $C\in\mathbf{S}^{s}$ , and B $B\in\mathbf{R}^{r\times s}$ ∈ . Then 

$$
\left[\begin{array}{c c}{A}&{B}\\ {B^{T}}&{C}\end{array}\right]\succeq0\iff C-B^{T}A^{-1}B\succeq0.
$$ 

For a more complete statement, which applies also to singular $A$ , and a proof, see $\S$ A.5.5 . 

(c) The matrix fractional optimization problem 

$$
{\mathrm{minimize}}\quad(A x+b)^{T}F(x)^{-1}(A x+b)
$$ 

where $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , 

$$
F(x)=F_{0}+x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n},
$$ 

with $F_{i}\in\mathbf{S}^{m}$ , and we take the domain of the objective to b $\{x\mid F(x)\succ0\}$ . You can assume the problem is feasible (there exists at least one x with $F(x)\succ0$ ≻ 0). 

4.41 L copositive matrices and $P_{0}$ -matrices. A matri $A\in\mathbf{S}^{n}$ said to be coposi e if $x^{T}A x\,\geq\,0$ ≥ 0 for all $x\succeq0$ xercise 35 ). A matrix A $A\,\in\,\mathbf{R}^{n\times n}$ ∈ is said to be a P $P_{0}$ - m ix if max i =1 ,...,n $x_{i}(A x)_{i}\,\geq\,0$ ≥ 0 for all x . Checking whether a matrix is copositive or a P -matrix is very difficult in general. However, there exist useful sufficient conditions that can be verified using semidefinite programming. 

(a) Show that $A$ is copositive if it can be decomposed as a sum of a positive semidefinite and an elementwise nonnegative matrix: 

$$
A=B+C,\qquad B\succeq0,\qquad C_{i j}\geq0,\quad i,j=1,.\;.\;.\;,n.
$$ 

Express the problem of finding $B$ and $C$ that satisfy ( 4.71 ) as an SDP feasibility problem. 

(b) Show that $A$ is a $P_{0}$ -matrix if there exists a positive diagonal matrix $D$ such that 

$$
D A+A^{T}D\succeq0.
$$ 

Express the problem of finding a $D$ that satisfies ( 4.72 ) as an SDP feasibility problem.

 4.42 Complex LMIs and $S D P s$ . A complex LMI has the form 

$$
x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0
$$ 

$F_{1},.\cdot\cdot\cdot,F_{n},\;G$ are complex $n\times n$ Hermitian matrices, i.e. , $\boldsymbol{F}_{i}^{H}=\boldsymbol{F}_{i}$ , ${\cal G}^{H}={\cal G}$ , and n $x\,\in\,\mathbf{R}^{n}$ ∈ is a real variable. A complex SDP is the problem of minimizing a (real) linear function of $x$ subject to a complex LMI constraint. 

Complex LMIs and SDPs can be transformed to real LMIs and SDPs, using the fact that 

$$
X\succeq0\iff\left[\begin{array}{c c}{{\Re X}}&{{-\Im X}}\\ {{\Im X}}&{{\Re X}}\end{array}\right]\succeq0,
$$ 

where $\Re X\in\mathbf{R}^{n\times n}$ is th eal part of the complex Hermitian matrix $X$ , and $\Im X\in\mathbf{R}^{n\times n}$ is the imaginary part of X . 

Verify this result, and show how to pose a complex SDP as a real SDP. 

4.43 Eigenvalue optimization via $S D P$ . Suppose $A:\mathbf{R}^{n}\rightarrow\mathbf{S}^{\pi n}$ is affine, i.e. , 

$$
A(x)=A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n}
$$ 

where $A_{i}\in\mathbf{S}^{m}$ . Let $\lambda_{1}(x)\geq\lambda_{2}(x)\geq\cdot\cdot\geq\lambda_{m}(x)$ denote the eigenvalues of $A(x)$ . Show how to pose the following problems as SDPs. 

(a) Minimize the maximum eigenvalue $\lambda_{1}(x)$ . (b) Minimize the spread of the eigenvalues, $\lambda_{1}(x)-\lambda_{m}(x)$ . 

(c) Minimize the condition number of $A(x)$ , subject to $A(x)\succ0$ . The condition number is defined as $\kappa(A(x))=\lambda_{1}(x)/\lambda_{m}(x)$ , with domain $\{x\mid A(x)\succ0\}$ . You may assume that $A(x)\succ0$ for at least one x . 

Hint. You need to minimize $\lambda/\gamma$ , subject to 

$$
0\prec\gamma I\preceq A(x)\preceq\lambda I.
$$ 

Change variables to $y=x/\gamma$ , $t=\lambda/\gamma$ , $s=1/\gamma$ . 

(d) Minimize the sum of the absolute values of the eigenvalues, $|\lambda_{1}(x)|+\cdot\cdot\cdot+|\lambda_{m}(x)|$ . Hint. Express $A(x)$ as $A(x)=A_{+}-A_{-}$ , where $A_{+}\succeq0$ , $A_{-}\succeq0$ . 

4.44 Optimization over polynomials. Pose the following problem as an SDP. Find the polyno- mial $p:\mathbf{R}\rightarrow\mathbf{R}$ , 

$$
p(t)=x_{1}+x_{2}t+\cdot\cdot\cdot+x_{2k+1}t^{2k},
$$ 

that satisfies given bounds $l_{i}\ \leq\ p(t_{i})\ \leq\ u_{i}$ , at $m$ specified points $t_{i}$ , and, of all the polynomials that satisfy these bounds, has the greatest minimum value: 

$$
\begin{array}{l r l}&{\mathrm{maximize}}&{\operatorname*{inf}_{t}p(t)}\\ &{\mathrm{subject~to}}&{l_{i}\leq p(t_{i})\leq u_{i},\quad i=1,.\,.\,.\,,m.}\end{array}
$$ 

The variables are $x\in\mathbf{R}^{2k+1}$ . 

Hint. Use the LMI characterization of nonnegative polynomials derived in exercise 2.37 , part (b). 

4.45 es00 , Par0 Sum-of-squares representation via LMIs. Consider a polynomial $p:\mathbf{R}^{n}\rightarrow$ R of d $2k$ . The polynomial is said to be ive efinite (PSD) if $p(x)\,\geq\,0$ n for all x $x\,\in\,\mathbf{R}^{\,n}$ ∈ . Except for special cases ( e.g. , n = 1 or k = 1), it is extremely difficult to determine whether or not a given polynomial is PSD, let alone solve an optimization problem, with the coefficients of $p$ as variables, with the constraint that $p$ be PSD. A famous sufficient condition for a polynomial to be PSD is that it have the form 

$$
p(x)=\sum_{i=1}^{r}q_{i}(x)^{2},
$$ 

for some polynomials , with degree no more than $k$ . A polynomial that has this $q_{i}$ $p$ sum-of-squares form is called SOS. 

The condition that a polynomial $p$ be SOS (viewed as a constraint on its coefficients) turns out to be equivalent to an LMI, and therefore a variety of optimization problems, with SOS constraints, can be posed as SDPs. You will explore these ideas in this problem. 

(a) Let $f_{1},\ldots,f_{s}$ be all monomials of degree $k$ or less. (Here we mean monomial in the standard sense, i.e. , $x_{1}^{m_{1}}\cdot\cdot\cdot x_{n}^{m_{n}}$ · · · , where $m_{i}\in\mathbf{Z}_{+}$ , and not in the sense used in geometric programming.) Show that if $p$ can be expressed as a positive semidefinite quadratic form $p\,=\,f^{T}V f$ , with $V\in\mathbf{S}_{+}^{s}$ , then $p$ is SOS. Conversely, show that if $p$ is SOS, then it can be expressed as a positive semidefinite quadratic form in the monomials, i.e. , $p=f^{T}V f$ , for some $V\in\mathbf{S}_{+}^{s}$ . 

(b) Show that the condition $p=f^{T}V f$ is a set of linear equality constraints relating the coefficients of $p$ and the matrix $V$ . Combined with part (a) above, this shows that the condition that be SOS is equivalent to a set of linear equalities relating $V$ and $p$ the coefficients of $p$ , and the matrix inequality $V\succeq0$ . (c) Work out the LMI conditions for SOS explicitly for the case where $p$ is polynomial of degree four in two variables. 

4.46 Multidimensional moments. The moments of a random variable $t$ on $\scriptstyle\mathbf{R}^{2}$ are defined as $\mu_{i j}\,=\,{\bf E}\,t_{1}^{i}t_{2}^{j}$ , where $i,j$ are nonnegative integers. In this problem we derive necessary conditions for a t of numbers $\mu_{i j}$ , $0\,\leq\,i,j\,\leq\,2k$ , $i+j\le2k$ , to be the moments of a distribution on R . 

Let $p:\mathbf{R}^{2}\rightarrow\mathbf{R}$ be a polynomial of degree $k$ with coefficients , $c_{i j}$ 

$$
p(t)=\sum_{i=0}^{k}\sum_{j=0}^{k-i}c_{i j}t_{1}^{i}t_{2}^{j},
$$ 

and let $t$ be a random variable with moments . Suppose $c\in\mathbf{R}^{(k+1)(k+2)/2}$ contains $\mu_{i j}$ the coefficients $c_{i j}$ in some spe der, and $\mu\in\mathbf{R}^{(k+1)(2k+1)}$ contains the m ments $\mu_{i j}$ in the same order. Show that ${\bf E}\,p(t)^{2}$ can be expressed as a quadratic form in c : 

$$
\mathbf{E}\,p(t)^{2}=c^{T}H(\mu)c,
$$ 

where $H\,:\,\mathbf{R}^{(k+1)(2k+1)}\,\rightarrow\,\mathbf{S}^{(k+1)(k+2)/2}$ is a linear function of $\mu$ . From this, conclude that $\mu$ must satisfy the LMI $H(\mu)\succeq0$ ⪰ 0. 

Remark: For random variables on $\mathbf{R}$ , the matrix $H$ can be taken as the Hankel matrix defined in ( 4.52 ). In this case, $H(\mu)\succeq0$ is a necessary and sufficient conditi for $\mu$ to be the moments of a distribution, or the limit of a sequence of moments. On R $\scriptstyle\mathbf{R}^{2}$ , however, the LMI is only a necessary condition. 

4.47 Maximum determinant positive semidefinite matrix completion. We consider a matrix $A\in\mathbf{S}^{n}$ , with some entries specified, and the others not specified. The positive semidefinite matrix completion problem is to determine values of the unspecified entries of the matrix so that $A\succeq0$ (or to determine that such a completion does not exist). 

(a) Explain why we can assume without loss of generality that the diagonal entries of $A$ are specified. (b) Show how to formulate the positive semidefinite completion problem as an SDP feasibility problem. (c) Assume that $A$ has at least one completion that is positive definite, and the diag- onal entries of $A$ are specified ( i.e. , fixed). The positive definite completion with largest determinant is called the maximum determinant completion . Show that the maximum determinant completion is unique. Show that if $A^{\star}$ is the maximum de- terminant completion, then $\left(A^{\star}\right)^{-1}$ has zeros in all the entries of the original matrix that were not specified. Hint. The gradient of the function $f(X)\,=\,\log\operatorname*{det}X$ is $\nabla f(X)=X^{-1}$ (see § A.4.1 ). (d) Suppose $A$ is specified on its tridiagonal part, i.e. , we are given $A_{11},.\cdot\cdot\cdot,A_{n n}$ and $A_{12},.\cdot\cdot,A_{n-1,n}$ . Show that if there exists a positive definite completion of $A$ , then there is a positive definite completion whose inverse is tridiagonal. 

4.48 Generalized eigenvalue minimization. Recall (from example 3.37 , or § A.5.3 ) that the largest generalized eigenvalue of a pair of matrices $(A,B)\in\mathbf{S}^{k}\times\mathbf{S}_{++}^{k}$ is given by 

$$
\lambda_{\operatorname*{max}}(A,B)=\operatorname*{sup}_{u\neq0}\frac{u^{T}A u}{u^{T}B u}=\operatorname*{max}\{\lambda\mid\operatorname*{det}(\lambda B-A)=0\}.
$$ 

As we have seen, this function is quasiconvex (if we take $\mathbf{S}^{k}\times\mathbf{S}_{++}^{k}$ as its domain). We consider the problem 

$$
\mathrm{minimize~}\quad\lambda_{\mathrm{max}}(A(x),B(x))
$$ 

where $A,B:\mathbf{R}^{n}\rightarrow\mathbf{S}^{k}$ are affine functions, defined as 

$$
A(x)=A_{0}+x_{1}A_{1}+\cdot\cdot\cdot+x_{n}A_{n},\qquad B(x)=B_{0}+x_{1}B_{1}+\cdot\cdot\cdot+x_{n}B_{n}.
$$ 

with $A_{i},B_{i}\in\mathbf{S}^{k}$ . 

(a) Give a family of convex functions $\phi_{t}:\mathbf{S}^{k}\times\mathbf{S}^{k}\rightarrow\mathbf{R}$ , that satisfy 

$$
\lambda_{\operatorname*{max}}(A,B)\leq t\iff\phi_{t}(A,B)\leq0
$$ 

for all $(A,B)\,\in\,\mathbf{S}^{k}\,\times\,\mathbf{S}_{++}^{k}$ . Show that this allows us to solve ( 4.73 ) by solving a sequence of convex feasibility problems. 

(b) Give a family of matrix-convex functions $\Phi_{t}:\mathbf{S}^{k}\times\mathbf{S}^{k}\rightarrow\mathbf{S}^{k}$ that satisfy 

$$
\lambda_{\operatorname*{max}}(A,B)\leq t\iff\Phi_{t}(A,B)\preceq0
$$ 

for all $(A,B)\,\in\,\mathbf{S}^{k}\,\times\,\mathbf{S}_{++}^{k}$ . Show that this allows us to solve ( 4.73 ) by solving a sequence of convex feasibility problems with LMI constraints. 

(c) Suppose $B(x)=(a^{T}x\!+\!b)I$ , with $a\ne0$ . Show that ( 4.73 ) is equivalent to the convex problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ \lambda_{\operatorname*{max}}{\big(}s A_{0}+y_{1}A_{1}+\cdot\cdot\cdot+y_{n}A_{n}{\big)}}\\ {{\mathrm{subject~to}}}&{\ a^{T}y+b s=1}\\ &{\ s\geq0,}\end{array}}
$$ 

with variables $y\in\mathbf{R}^{n}$ , $s\in\mathbf{R}$ . 

4.49 Generalized fractional programming. Let $K\,\in\,\mathbf{R}^{m}$ be a proper cone. Show that the function $f_{0}:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ , defined by 

$$
f_{0}(x)=\operatorname*{inf}\{t\ |\ C x+d\preceq_{K}t(F x+g)\},\qquad\mathbf{dom}\ f_{0}=\{x\ |\ F x+g\succ_{K}0\},
$$ 

with $C,F\in\mathbf{R}^{m\times n},\,d,g\in\mathbf{R}^{m}$ , is quasiconvex. 

A quasiconvex optimization problem with objective function of this form is called a gen- eralized fractional program . Express the generalized linear-fractional program of page 152 and the generalized eigenvalue minimization problem ( 4.73 ) as generalized fractional pro- grams. 

# Vector and multicriterion optimization 

4.50 $B i$ -criterion optimization. Figure 4.11 shows the optimal trade-oﬀcurve and the set of achievable values for the bi-criterion optimization problem 

$$
\mathrm{minimize}\ (\mathrm{w.r.t.}\ \mathbf{R}_{+}^{2})\quad(\|A x-b\|^{2},\|x\|_{2}^{2}),
$$ 

for some $A\in\mathbf{R}^{100\times10}$ , $b\in\mathbf{R}^{100}$ . Answer the following questions using information from the plot. We denote by $x_{\mathrm{ls}}$ the solution of the least-squares problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}.
$$ 

(a) What is ∥ x ls ∥ 2 ? 

(b) What is $\|A x_{\mathrm{ls}}-b\|_{2};$ ? 

(c) What is $||b||_{2}$ ? (d) Give the optimal value of the problem 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{\|A x-b\|_{2}^{2}}\\ {{\mathrm{subject~to}}}&{\|x\|_{2}^{2}=1.}\end{array}}
$$ 

(e) Give the optimal value of the problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|A x-b\|_{2}^{2}}\\ {\mathrm{subject~to}}&{\|x\|_{2}^{2}\leq1.}\end{array}
$$ 

(f) Give the optimal value of the problem 

$$
{\mathrm{minimize~}}\|A x-b\|_{2}^{2}+\|x\|_{2}^{2}
$$ 

(g) What is the rank of $A$ ? 

4.51 Monotone transformation of objective in vector optimization. Consider the vector opti- mization problem ( 4.56 ). Suppose we form a new vector optimization problem by replacing the objective $f_{0}$ with $\phi\circ f_{0}$ , where $\phi:\mathbf{R}^{q}\rightarrow\mathbf{R}^{q}$ satisfies 

$$
u\preceq_{K}v,\;u\neq v\Longrightarrow\phi(u)\preceq_{K}\phi(v),\;\phi(u)\neq\phi(v).
$$ 

Show that a point $x$ is Pareto optimal (or optimal) for one problem if and only if it is Pareto optimal (optimal) for the other, so the two problems are equivalent. In particular, composing each objective in a multicriterion problem with an increasing function does not aﬀect the Pareto optimal points. 

4.52 Pareto optimal points and the boundary of the set of achievable values. Consider a vector opt ization problem with cone $K$ . Let $\mathcal{P}$ denote the set of P values, and let O denote the set of achievable objective values. Show that P ⊆O ∩ ${\mathcal{P}}\subseteq{\mathcal{O}}\cap{\mathbf{bd}}\,{\mathcal{O}}$ O , i.e. , every Pareto optimal value is an achievable objective value that lies in the boundary of the set of achievable objective values. 

4.53 Suppose the vector optimization problem ( 4.56 ) is convex. Show that the set 

$$
{\mathcal{A}}={\mathcal{O}}+K=\{t\in\mathbf{R}^{q}\mid f_{0}(x)\preceq_{K}t{\mathrm{~for~some~feasible~}}x\},
$$ 

is nvex. Also show that the minimal elements of $\mathcal{A}$ are the same as the minimal points of . 

4.54 Scalarization and optimal points. Suppose a (not necessarily convex) vector optimization problem has an optimal point $x^{\star}$ . Show that $x^{\star}$ is a solution of the associated scalarized problem for any choice of $\lambda\succ_{K^{*}}(\!\!\!\!\!\backslash$ . Als the converse: If a point $x$ is a solution of the scalarized problem for any choice of λ $\lambda\succ_{K^{*}}$ ≻ 0, then it is an optimal point for the (not necessarily convex) vector optimization problem. 

4.55 Generalization of weighted-sum scalarization. In § 4.7.4 we showed how to obtain Pareto optimal solutions of a vector optimization problem by replacing the vector objective $f_{0}:$ ${\bf R}^{n}\ \rightarrow\ {\bf R}^{q}$ with the scalar objective $\lambda^{T}f_{0}$ , where $\lambda\,\,\succ_{K^{*}}\,\,\,0$ . Let $\psi\,:\,\mathbf{R}^{q}\,\rightarrow\,\mathbf{R}$ be a K -increasing function, i.e. , satisfying 

$$
u\preceq_{K}v,\;u\neq v\Longrightarrow\psi(u)<\psi(v).
$$ 

Show that any solution of the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\psi(f_{0}(x))}\\ {{\mathrm{subject~to}}\quad f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p}\end{array}}
$$ 

is Pareto optimal for the vector optimization problem 

$$
\begin{array}{l l}{\mathrm{minimize~(w.r.t.~\}K)}&{~f_{0}(x)}\\ {\mathrm{subject~to}}&{~f_{i}(x)\le0,\quad i=1,.\,.\,,m}\\ &{~h_{i}(x)=0,\quad i=1,.\,.\,,p.}\end{array}
$$ 

Note that $\psi(u)=\lambda^{T}u$ , where $\lambda\succ_{K^{*}}\,0$ , is a special case. 

As a related example, show that in a multicriterion optimization problem ( i.e. , a vector optimization problem with $f_{0}=F:\mathbf{R}^{n}\rightarrow\mathbf{R}^{q}$ , and $K=\mathbf{R}_{+}^{q}$ ), a unique solution of the scalar optimization problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{\,\operatorname*{max}_{i=1,\dots,q}F_{i}(x)}\\ {\mathrm{subject~to}\,}&{\,f_{i}(x)\le0,\quad i=1,.\,.\,,m}\\ &{\,h_{i}(x)=0,\quad i=1,.\,.\,,p,}\end{array}
$$ 

is Pareto optimal. 

# Miscellaneous problems 

4.56 [P. Parrilo] We consider the problem of minimizing the convex function $f_{0}\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ over the convex hull of the union of some convex sets, conv $\textstyle\left(\bigcup_{i=1}^{q}C_{i}\right)$  . These sets are described via convex inequalities, 

$$
C_{i}=\{x\mid f_{i j}(x)\leq0,\ j=1,.\,.\,.\,,k_{i}\},
$$ 

where $f_{i j}\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ are convex. Our goal is to formulate this problem as a convex optimization problem. 

The us ch is to introduc les $x_{1},.\,.\,.\,,x_{q}\,\in\,\mathbf{R}^{n}$ , with $x_{i}\,\in\,C_{i},\;\theta\,\in\,{\bf R}^{q}$ with θ $\theta\succeq0$ ⪰ 0, 1 $\mathbf{1}^{T}\boldsymbol{\theta}=1$ = 1, and a variable x $x\in\mathbf{R}^{n}$ ∈ , with $x=\theta_{1}x_{1}+\cdot\cdot\cdot+\theta_{q}x_{q}$ . This equality constraint is not affine in the variables, so this approach does not yield a convex problem. A more sophisticated formulation is given by 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{s_{i}f_{i j}(z_{i}/s_{i})\leq0,\quad i=1,\ldots,q,\quad j=1,\ldots,k_{i}}\\ &{\mathbf{1}^{T}s=1,\quad s\succeq0}\\ &{x=z_{1}+\cdot\cdot\cdot+z_{q},}\end{array}}
$$ 

bles $z_{1},.\,.\,.\,,z_{q}\;\in\;\mathbf{R}^{n}$ , $x\,\in\,\mathbf{R}^{\,n}$ d $s_{1},.\,.\,.\,,s_{q}\,\in\,\mathbf{R}$ . (When $s_{i}\,=\,0$ , we take $s_{i}f_{i j}\left(z_{i}/s_{i}\right)$ ) to be 0 if z = 0 and ∞ if z $z_{i}\neq0$ ̸ = 0.) Explain why this problem is convex, and equivalent to the original problem. 

4.57 Capacity of a communication channel. We consider a communication channel, with input $X(t)\in\{1,.\,.\,.\,,n\}$ , and output $Y(t)\in\{1,.\,.\,.\,,m\}$ , for $t=1,2,\ldots$ (in seconds, say). The relation between the input and the output is given statistically: 

$$
\mathbf{\Pi}=\mathbf{prob}(Y(t)=i|X(t)=j),\quad i=1,\ldots,m,\quad j=
$$ 

The matrix $\textstyle P\in\mathbf{R}^{m\times n}$ is called the channel transition matrix , and the channel is called a discrete memoryless channel . 

A famous result of Shannon states that information can be sent over the communication channel, with arbitrarily small probability of error, at any rate less than a number $C$ , called the channel capacity , in bits per second. Shannon also showed that the capacity of a discrete memoryless channel can be found by solving an optimization problem. Assume that $X$ has a probability distribution denoted $x\in\mathbf{R}^{n}$ , i.e. , 

$$
x_{j}=\mathbf{prob}(X=j),\quad j=1,\ldots,n.
$$ 

The mutual information between $X$ and $Y$ is given by 

$$
I(X;Y)=\sum_{i=1}^{m}\sum_{j=1}^{n}x_{j}p_{i j}\log_{2}{\frac{p_{i j}}{\sum_{k=1}^{n}x_{k}p_{i k}}}.
$$ 

Then the channel capacity $C$ is given by 

$$
C=\operatorname*{sup}_{x}I(X;Y),
$$ 

where the supremum is over all possible probability distributions for the input $X$ , i.e. , over $x\succeq0$ , $\mathbf{1}^{T}x=1$ . 

Show how the channel capacity can be computed using convex optimization. 

Hint. Introduce the variable $y\;=\;P x$ , which gives the probability distribution of the output $Y$ , and show that the mutual information can be expressed as 

$$
I(X;Y)=c^{T}x-\sum_{i=1}^{m}y_{i}\log_{2}y_{i},
$$ 

where $\begin{array}{r}{c_{j}=\sum_{i=1}^{m}p_{i j}\log_{2}p_{i j}}\end{array}$ , $j=1,\dotsc,n$ . 

4.58 Optimal consumption. In this problem we consider the optimal way to consume (or spend) an initial amount of money (or other asset) $k_{0}$ over time. The variables are $c_{0},\ldots,c_{T}$ , wher $c_{t}\geq0$ denot he consu eriod $t$ . The utility derived from a consumption level c is given by $u(c)$ ), where u $u:\mathbf{R}\rightarrow\mathbf{R}$ → is an increasing concave function. The present value of the utility derived from the consumption is given by 

$$
U=\sum_{t=0}^{T}\beta^{t}u(c_{t}),
$$ 

where $0<\beta<1$ is a discount factor . 

Let $k_{t}$ denote the amount of money available for investment in period $t$ . We assume that it earns an investment return given by $f(k_{t})$ , where $f:\mathbf{R}\rightarrow\mathbf{R}$ is an increasing, concave investment return function , which satisfies $f(0)\,=\,0$ . For example if the funds earn simple interest at rate $R$ percent per period, we have $f(a)=(R/100)a$ . The amount to be consumed, $i$ .e. , , is withdrawn at the end of the period, so we have the recursion $c_{t}$ 

$$
k_{t+1}=k_{t}+f(k_{t})-c_{t},\quad t=0,.\,.\,.\,,T.
$$ 

The initial sum $k_{0}>0$ en. We require $k_{t}\geq0$ , $t=1,\cdot\cdot\cdot,T{+}1$ (but more sophisticated models, which allow k $k_{t}<0$ 0, can be considered). 

Show how to formulate the problem of maximizing $U$ as a convex optimization problem. Explain how the problem you formulate is equivalent to this one, and exactly how the two are related. 

Hint. Show that we can replace the recursion for $k_{t}$ given above with the inequalities 

$$
k_{t+1}\leq k_{t}+f(k_{t})-c_{t},\quad t=0,.\,.\,.\,,T.
$$ 

(Interpretation: the inequalities give you the option of throwing money away in each period.) For a more general version of this trick, see exercise 4.6 . 

4.59 Robust optimization. In some optimization problems there is uncertainty or variation in the objective and constraint functions, due to parameters or factors that are either beyond our control or unknown. We can model this situation by making the objective and constraint func $f_{\mathrm{0}},\ldots,f_{m}$ functions of the optimization variable $x\,\in\,\mathbf{R}^{n}$ and a parameter vector u $u\,\in\,\mathbf{R}^{k}$ that is unknown, or varies. In the stochastic optimization 

approach, the parameter vector $u$ is modeled as a random variable with a known dis- tribution, and we work with the expected values $\mathbf{E}_{u}~f_{i}(x,u)$ . In the worst-case analysis approach, we are given a set $U$ that $u$ is known to lie in, and we work with the maximum or worst-case values sup $f_{i}(x,u)$ . To simplify the discussion, we assume there are no $u\!\in\!U$ equality constraints. 

(a) Stochastic optimization. We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\operatorname{\mathbf{E}}f_{0}(x,u)}\\ &{{\mathrm{subject~to}}\quad\operatorname{\mathbf{E}}f_{i}(x,u)\leq0,\quad i=1,\dots,m,}\end{array}}
$$ 

where the expectation is with respect to $u$ . Show that if $f_{i}$ are convex in $x$ for each $u$ , then this stochastic optimization problem is convex. 

(b) Worst-case optimization. We consider the problem 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{\operatorname*{sup}_{u\in U}f_{0}(x,u)}\\ {{\mathrm{subject~to}}}&{\operatorname*{sup}_{u\in U}f_{i}(x,u)\leq0,\quad i=1,.\,.\,,m.}\end{array}}
$$ 

Show that if $f_{i}$ are convex in $x$ for each $u$ , then this worst-case optimization problem is convex. 

(c) Finite set of possible parameter values. The observations made in parts (a) and (b) are most useful when we have analytical or easily evaluated expressions for the expected values $\mathbf{E}\,f_{i}(x,u)$ or the worst-case values $\operatorname*{sup}_{u\in U}f_{i}(x,u)$ . Suppose we are given the set of possible values of the parameter is finite, i.e. , we have $u\in\{u_{1},.\,.\,.\,,u_{N}\}$ . For the stochastic case, we are also given the probabilities of each value: $\mathbf{prob}(u=u_{i})=p_{i}$ , where $p\in\mathbf{R}^{N}$ , $p\succeq0$ , $\mathbf{1}^{T}p=1$ . In the worst-case formulation, we simply take $U\in\{u_{1},.\,.\,.\,,u_{N}\}$ . Show how to set up the worst-case and stochastic optimization problems explicitly ( i.e. , give explicit expressions for $\operatorname*{sup}_{u\in U}f_{i}$ and $\mathbf{E}_{u}~f_{i}$ ). 

4.60 Log-optimal investment strategy. We consider a portfolio problem with $n$ assets held over $N$ periods. At the beginning of each period, we re-invest our total wealth, redistributing he $n$ assets using a stant, allocation strategy $x\,\in\,\mathbf{R}^{\,n}$ , where $x\,\succeq\,0$ , $\mathbf{1}^{T}x\,=\,1$ = 1. In ther word $W(t-1)$ 1) is ou wealth at the beginning of period $t$ , then during eriod $t$ $x_{i}W(t-1)$ in asset i . We denot y $\lambda(t)$ the total return during period t , i.e. , $\lambda(t)=W(t)/W(t-1)$ − 1). At the end of the N periods our wealth has been multiplied by the factor $\textstyle\prod_{t=1}^{N}\lambda(t)$ ). We call 

$$
{\frac{1}{N}}\sum_{t=1}^{N}\log\lambda(t)
$$ 

the growth rate of the investment over the $N$ periods. We are interested in determining an allocation strategy $x$ that maximizes growth of our total wealth for large $N$ . 

We use a discrete stochastic model to account for the uncertainty in the returns. We assume that during each period there are $m$ possible scenarios, with probabilities $\pi_{j}$ , $j\ =\ 1,\ldots,m$ . In scenario $j$ , the return for asset $i$ over one period is given by . $p_{i j}$ Therefore, the return $\lambda(t)$ of our portfolio during period $t$ is a random variable, with $m$ possible values $p_{1}^{T}x,\ldots,p_{m}^{T}x$ , and distribution 

$$
\pi_{j}=\mathbf{prob}(\boldsymbol{\lambda}(t)=p_{j}^{T}\boldsymbol{x}),\quad j=1,.\,.\,.\,,m.
$$ 

We assume the same scenarios for each period, with (identical) independent distributions. Using the law of large numbers, we have 

$$
\operatorname*{lim}_{N\to\infty}\frac{1}{N}\log\bigg(\frac{W(N)}{W(0)}\bigg)=\operatorname*{lim}_{N\to\infty}\frac{1}{N}\sum_{t=1}^{N}\log\lambda(t)=\mathbf{E}\log\lambda(t)=\sum_{j=1}^{m}\pi_{j}\log(p_{j}^{T}x).
$$ 

In other words, with investment strategy $x$ , the long term growth rate is given by 

$$
R_{\mathrm{lt}}=\sum_{j=1}^{m}\pi_{j}\log(p_{j}^{T}x).
$$ 

The investment strategy $x$ that maximizes this quantity is called the log-optimal invest- ment strategy , and can be found by solving the optimization problem 

$$
\begin{array}{r}{\begin{array}{l l}{\mathrm{maximize}}&{\sum_{j=1}^{m}\pi_{j}\log(p_{j}^{T}x)}\\ {\mathrm{subject~to}}&{x\succeq0,\quad\mathbf{1}^{T}x=1,}\end{array}}\end{array}
$$ 

with variable $x\in\mathbf{R}^{n}$ 

Show that this is a convex optimization problem. 

4.61 Optimization with logistic model. A random variable $X\in\{0,1\}$ satisfies 

$$
\mathbf{prob}(X=1)=p=\frac{\exp(a^{T}x+b)}{1+\exp(a^{T}x+b)},
$$ 

where $x\in\mathbf{R}^{n}$ is a vector of v s that aﬀect the probability, and $a$ and $b$ are known parameters. We can think of X = 1 as the event that a consumer buys a product, and $x$ as a vector of variables that aﬀect the probability, e.g. , advertising eﬀort, retail price, discounted price, packaging expense, and other factors. The variable $x$ , which we are to optimize over, is subject to a set of linear constraints, $F x\preceq g$ . 

Formulate the following problems as convex optimization problems. 

(a) Maximizing buying probability. The goal is to choose $x$ to maximize $p$ . (b) Maximizing expected profit. Let $c^{T}x{+}d$ be the profit derived from selling the product, which we assume is positive for all feasible $x$ . The goal is to maximize the expected profit, which is $p(c^{T}x+d)$ . 

4.62 Optimal power and bandwidth allocation in a Gaussian broadcast channel. We consider a communication system in which a central node transmits messages to $n$ receivers. (‘Gaus- sian’ refers to the type of noise that corrupts the transmissions.) Each receiver channel is characterized by its (transmit) power level $P_{i}\,\geq\,0$ and its ban idth $W i\,\geq\,0$ . The power and bandwidth of a receiver channel determine its bit rate R i (the rate at which information can be sent) via 

$$
R i=\alpha_{i}W_{i}\log(1+\beta_{i}P_{i}/W_{i}),
$$ 

where $\alpha_{i}$ and $\beta_{i}$ are known positive constants. For ${\mathcal W}_{i}\,=\,0$ , we take $R_{i}\,=\,0$ (which is what you get if you take the limit as $W_{i}\to0$ ). 

The powers must satisfy a total power constraint, which has the form 

$$
P_{1}+\cdot\cdot\cdot+P_{n}=P_{\mathrm{tot}},
$$ 

where $P_{\mathrm{{tot}}}>0$ is a given total power available to allocate among the channels. Similarly, the bandwidths must satisfy 

$$
W_{1}+\cdot\cdot\cdot+W_{n}=W_{\mathrm{tot}},
$$ 

where $W_{\mathrm{tot}}\,>\,0$ is the (given) total available bandwidth. The optimization variables in this problem are the powers and bandwidths, i.e. , $P_{1},.\cdot\cdot\cdot,P_{n}$ , $W_{1},\dots,W_{n}$ . The objective is to maximize the total utility, 

$$
\sum_{i=1}^{n}u_{i}(R_{i}),
$$ 

where $u_{i}:\mathbf{R}\,\rightarrow\,\mathbf{R}$ is the utility function associated with the $i$ receiver. (You can think of $u_{i}(R_{i})$ as the revenue obtained for providing a bit rate R to receiver $i$ , so the objective is to maximize the total revenue.) You can assume that the utility functions $u_{i}$ are nondecreasing and concave. 

Pose this problem as a convex optimization problem. 

4.63 Optimally balancing manufacturing cost and yield. The vector $x\in\mathbf{R}^{n}$ denotes the nomi- nal parameters in a manufacturing process. The yield of the process, i.e. , the fraction of manufactured goods that is acceptable, is given by $Y(x)$ . We assume that $Y$ is log-concave (which is often the case; see example 3.43 ). The cost per unit to manufacture the product is given by $c^{T}x$ , where $c\in\mathbf{R}^{n}$ . The cost per acceptable unit is $c^{T}x/Y(x)$ . We want to minimize $c^{T}x/Y(x)$ , subject to some convex constraints on $x$ such as a linear inequalities $A x\preceq b$ . (You can assume that over the feasible set we have $c^{T}x>0$ and $Y(x)>0$ .) This problem is not a convex or quasiconvex optimization problem, but it can be solved using convex optimization and a one-dimensional search. The basic ideas are given below; you must supply all details and justification. 

(a) Show that the function $f:\mathbf{R}\rightarrow\mathbf{R}$ given by 

$$
f(a)=\operatorname*{sup}\{Y(x)\mid A x\preceq b,\ c^{T}x=a\},
$$ 

which gives the maximum yield versus cost, is log-concave. This means that by solving a convex optimization problem (in $x$ ) we can evaluate the function $f$ . 

(b) Suppose that we evaluate the function $f$ for enough values of $a$ to give a good approx- imation over the range of interest. Explain how to use these data to (approximately) solve the problem of minimizing cost per good product. 

4.64 Optimization with recourse. In an optimization problem with recourse, also called two- stage optimization , the cost function and constraints depend not only on our choice of variables, but also o a discrete random variable $s\in\{1,.\,.\,.\,,S\}$ , which is interpreted as specifying which of S scenarios occurred. The scenario random variable $s$ has known probability distribution $\pi$ , with $\pi_{i}=\mathbf{prob}(s=i)$ , $i=1,.\,.\,.\,,S$ . 

stage optimizat n, we are to choose the values of two variable , $x\,\in\,\mathbf{R}^{\,n}$ and q $z\,\in\,{\bf R}^{q}$ ∈ . The variable x must be chosen before the particular scenario s is known; the variable z , however, is chosen after the value of the scenario random variable is known. In other words, $z$ is a function of the scenario random variable $s$ . To describe our choice $z$ , we list the values we would choose under the diﬀerent scenarios, i.e. , we list the vectors 

$$
z_{1},.\,.\,.\,,z_{S}\in\mathbf{R}^{q}.
$$ 

Here $z_{3}$ is our choice of $z$ when $s=3$ occurs, and so on. The set of values 

$$
x\in\mathbf{R}^{n},\qquad z_{1},.\,.\,.\,,z_{S}\in\mathbf{R}^{q}
$$ 

is called the policy , since it tells us what choice to make for $x$ (independent of which scenario occurs), and also, what choice to make for $z$ in each possible scenario. 

The variable $z$ is called the recourse variable (or second-stage variable ), since it allows us to take some action or make a choice after we know which scenario occurred. In contrast, our choice of $x$ (which is called the first-stage variable ) must be made without any knowledge of the scenario. 

For simplicity we will consider the case with no constraints. The cost function is given by 

$$
f:\mathbf{R}^{n}\times\mathbf{R}^{q}\times\{1,.\,.\,,S\}\rightarrow\mathbf{R},
$$ 

where $f(x,z,i)$ gives the cost when the first-stage choice $x$ is made, second-stage choice $z$ is made, and scenario $i$ occurs. We will take as the overall objective, to be minimized over all policies, the expected cost 

$$
{\bf E}\,f(x,z_{s},s)=\sum_{i=1}^{S}\pi_{i}f(x,z_{i},i).
$$ 

Suppose that $f$ is a convex function of $(x,z)$ , for each scenario $i\,=\,1,\ldots,S$ . Explain how to find an optimal policy, i.e. , one that minimizes the expected cost over all possible policies, using convex optimization. 

4.65 Optimal operation of a hybrid vehicle. A hybrid vehicle has an internal combustion engine, a motor/generator connected to a storage battery, and a conventional (friction) brake. In this exercise we consider a (highly simplified) model of a parallel hybrid vehicle , in which both the motor/generator and the engine are directly connected to the drive wheels. The engine can provide power to the wheels, and the brake can take power from the wheels, turning it into heat. The motor/generator can act as a motor, when it uses energy stored in the battery to deliver power to the wheels, or as a generator, when it takes power from the wheels or engine, and uses the power to charge the battery. When the generator takes power from the wheels and charges the battery, it is called regenerative braking ; unlike ordinary friction braking, the energy taken from the wheels is stored , and can be used later. The vehicle is judged by driving it over a known, fixed test track to evaluate its fuel efficiency. 

A diagram illustrating the power ﬂow in the hybrid vehicle is shown below. The arrows indicate the direction in which the power ﬂow is considered positive. The engine power $p_{\mathrm{eng}}$ , for example, is positive when it is delivering power; the brake power $p_{\mathrm{br}}$ is positive when it is taking power from the wheels. The power $p_{\mathrm{req}}$ is the required power at the wheels. It is positive when the wheels require power ( e.g. , when the vehicle accelerates, climbs a hill, or cruises on level terrain). The required wheel power is negative when the vehicle must decelerate rapidly, or descend a hill. 

![](images/25b638dbbed076b48d8c7c00e8b2fc936843e2cf96b3190f758a15ff1de84b71.jpg) 
All of these powers are functions of time, which we discretize in one second intervals, with $t\,=\,1,2,.\,.\,.\,,T$ . The required wheel power $p_{\mathrm{req}}(1),\ldots,p_{\mathrm{req}}(T)$ is given. (The speed of the vehicle on the track is specified, so together with known road slope information, and known aerodynamic and other losses, the power required at the wheels can be calculated.) Power is conserved, which means we have 

$$
p_{\mathrm{req}}(t)=p_{\mathrm{eng}}(t)+p_{\mathrm{mg}}(t)-p_{\mathrm{br}}(t),\quad t=1,\ldots,T.
$$ 

The brake can only dissipate power, so we have $p_{\mathrm{br}}(t)\geq0$ for each $t$ . The engine can only provide power, and only up to a given limit $P_{\mathrm{eng}}^{\mathrm{max}}$ , i.e. , we have 

$$
0\leq p_{\mathrm{eng}}(t)\leq P_{\mathrm{eng}}^{\mathrm{max}},\quad t=1,\ldots,T.
$$ 

The motor/generator power is also limited: $p_{\mathrm{mag}}$ must satisfy 

$$
P_{\mathrm{mg}}^{\mathrm{min}}\leq p_{\mathrm{mg}}(t)\leq P_{\mathrm{mg}}^{\mathrm{max}},\quad t=1,.\,.\,,T.
$$ 

Here $P_{\mathrm{mg}}^{\mathrm{max}}>0$ 0 is the maximum motor power, and $-P_{\mathrm{mg}}^{\mathrm{min}}>0$ 0 is the maximum generator power. 

The battery charge or energy at time $t$ is denoted $E(t)$ , $t=1,\dots,T+1$ . The battery energy satisfies 

$$
E(t+1)=E(t)-p_{\mathrm{mg}}(t)-\eta|p_{\mathrm{mg}}(t)|,\quad t=1,\ldots,T,
$$ 

where $\eta>0$ is a known parameter. (The term $-p_{\mathrm{mg}}(t)$ represents the energy removed or added the battery by the motor/generator, ignoring any losses. The term $-\eta|p_{\mathrm{mg}}(t)|$ represents energy lost through inefficiencies in the battery or motor/generator.) 

The battery charge must be between 0 (empty) and its limit $E_{\mathrm{batt}}^{\mathrm{max}}$ (full), at all times. (If $E(t)=0$ , the battery is fully discharged, and no more energy can be extracted from it; when $E(t)=E_{\mathrm{{ball}}}^{\mathrm{{max}}}$ , the battery is full and cannot be charged.) To make the comparison with non-hybrid vehicles fair, we fix the initial battery charge to equal the final battery charge, so the net energy change is zero over the track: $E(1)\,=\,E(T+1)$ . We do not specify the value of the initial (and final) energy. 

The objective in the problem (to be minimized) is the total fuel consumed by the engine, which is 

$$
F_{\mathrm{total}}=\sum_{t=1}^{T}F(p_{\mathrm{eng}}(t)),
$$ 

where $F\,:\,\mathbf{R}\,\rightarrow\,\mathbf{R}$ is the fuel use characteristic of the engine. We assume that $F^{\prime}$ is positive, increasing, and convex. 

Formulate this problem as a convex optimization problem, with variables $p_{\mathrm{eng}}(t)$ , $p_{\mathrm{mg}}(t)$ , and $p_{\mathrm{br}}(t)$ for $t=1,\dots,T$ , and $E(t)$ for $t=1,\dots,T+1$ . Explain why your formulation is equivalent to the problem described above. 

# Chapter 5 

# Duality 

# 5.1 The Lagrange dual function 

# 5.1.1 The Lagrangian 

We consider an optimization problem in the standard form ( 4.1 ): 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n}$ . We assume its domain $\begin{array}{r}{\mathcal{D}=\bigcap_{i=0}^{m}\mathbf{dom}\,f_{i}\ \cap\ \bigcap_{i=1}^{p}\mathbf{dom}\,h_{i}}\end{array}$ ∩ T is nonempty, and denote the optimal value of ( 5.1 ) by p $p^{\star}$ ⋆ . We do not assume the problem ( 5.1 ) is convex. 

The basic idea in Lagrangian duality is to take the constraints in ( 5.1 ) into account by augmenting the objective function with a weighted sum of the constraint functions. We define the Lagrangian $L:\mathbf{R}^{n}\times\mathbf{R}^{m}\times\mathbf{R}^{p}\rightarrow\mathbf{R}$ associated with the problem ( 5.1 ) as 

$$
L(x,\lambda,\nu)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x),
$$ 

with $\mathbf{dom}\,L=\mathcal{D}\times\mathbf{R}^{m}\times\mathbf{R}^{p}$ . We refer to $\lambda_{i}$ as the Lagrange multiplier associated with the $i$ th inequality constra nt $f_{i}(x)\leq0$ ; similarly we refer to as the Lag ange $\nu_{i}$ multiplier associated with the i th equality constraint $h_{i}(x)=0$ . The vectors λ and $\nu$ are called the dual variables or Lagrange multiplier vectors associated with the problem ( 5.1 ). 

# 5.1.2 The Lagrange dual function 

We define the Lagrange dual function (or just dua n $g:\mathbf{R}^{m}\times\mathbf{R}^{p}\to\mathbf{R}$ as the minimum value of the Lagrangian over $x$ : for λ $\lambda\in\mathbf{R}^{m}$ ∈ , ν $\nu\in\mathbf{R}^{p}$ ∈ , 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x\in{\mathcal{D}}}L(x,\lambda,\nu)=\operatorname*{inf}_{x\in{\mathcal{D}}}\left(f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x)\right).
$$ 

When the Lagrangian is unbounded below in $x$ , the dual function takes on the value $-\infty$ . Since the dual function is the pointwise infimum of a family of affine functions of $(\lambda,\nu)$ , it is concave, even when the problem ( 5.1 ) is not convex. 

# 5.1.3 Lower bounds on optimal value 

The dual function yields lower bounds on the optimal value $p^{\star}$ of the problem ( 5.1 ): For any $\lambda\succeq0$ and any $\nu$ we have 

$$
g(\lambda,\nu)\leq p^{\star}.
$$ 

This important property is easily verified. Suppose x is a feasible point for the problem ( 5.1 ), i.e. , $f_{i}(\tilde{x})\leq0$ ≤ 0 and $h_{i}(\tilde{x})=0$ ) = 0, and $\lambda\succeq0$ . Then we have 

$$
\sum_{i=1}^{m}\lambda_{i}f_{i}(\tilde{x})+\sum_{i=1}^{p}\nu_{i}h_{i}(\tilde{x})\leq0,
$$ 

since each term in the first sum is nonpositive, and each term in the second sum is zero, and therefore 

$$
L(\tilde{x},\lambda,\nu)=f_{0}(\tilde{x})+\sum_{i=1}^{m}\lambda_{i}f_{i}(\tilde{x})+\sum_{i=1}^{p}\nu_{i}h_{i}(\tilde{x})\leq f_{0}(\tilde{x}).
$$ 

Hence 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x\in\mathcal{D}}L(x,\lambda,\nu)\leq L(\tilde{x},\lambda,\nu)\leq f_{0}(\tilde{x}).
$$ 

Since $g(\lambda,\nu)\leq f_{0}(\tilde{x})$ ) holds for every feasible point x , the inequality ( 5.2 ) follows. The lower bound ( 5.2 ) is illustrated in figure 5.1 , for a simple problem with $x\in\mathbf{R}$ and one inequality constraint. 

The inequality ( 5.2 ) holds, but is vacuous, when $g(\lambda,\nu)~=~-\infty$ . The dual function gives a nontrivial lower bound on $p^{\star}$ only when $\lambda\succeq0$ and $(\lambda,\nu)\in\mathbf{dom}\,g$ , i.e. , $g(\lambda,\nu)>-\infty$ . We refer to a pair $(\lambda,\nu)$ with $\lambda\succeq0$ and $(\lambda,\nu)\in\mathbf{dom}\,g$ as dual feasible , for reasons that will become clear later. 

# 5.1.4 Linear approximation interpretation 

The Lagrangian and lower bound property can be given a simple interpretation, based on a linear approximation of the indicator functions of the sets $\{0\}$ and $-\mathbf{R}_{+}$ . 

![](images/2634169511481191ec870c71377eb467e354e2a1f34eaefbe6a3df07f428fe62.jpg) 
Figure 5.1 Lower bound from a dual feasible point. The solid curve shows the objective function $f_{0}$ , and the dashed curve shows the constraint function $f_{1}$ . The feasible set is the interval $\left\lfloor-0.46,0.46\right\rfloor$ , which is b dotted vertical lines. The optimal point and re $x^{\star}=-0.46$ $p^{\star}=1.54$ (shown as a circle). The dotted curves show $L(x,\lambda)$ ) for λ $\lambda=0.1$ 1 , $0.2,\hdots,1.0$ . Each of these has a minimum value smaller than $p^{\star}$ , since on the feasible set (and for $\lambda\geq0$ ) we have $L(x,\lambda)\leq f_{0}(x)$ . 

![](images/65bb54438d8e331020e2d1d67116f86f7bb6c05becd3a43f25e4fe425c696be5.jpg) 
Figure 5.2 The dual function $g$ for the problem in figure 5.1 . Neither $f_{0}$ nor $f_{1}$ is convex, but the dual function is concave. The horizontal dashed line shows $p^{\star}$ , the optimal value of the problem. 

We first rewrite the original problem ( 5.1 ) as an unconstrained problem, 

$$
\begin{array}{r l}{\mathrm{minimize}\:}&{{}f_{0}(x)+\sum_{i=1}^{m}I_{-}(f_{i}(x))+\sum_{i=1}^{p}I_{0}(h_{i}(x)),}\end{array}
$$ 

where $I_{-}:\mathbf{R}\rightarrow\mathbf{R}$ is the indicator function for the nonpositive reals, 

$$
I_{-}(u)=\left\{\begin{array}{l l}{0}&{u\leq0}\\ {\infty}&{u>0,}\end{array}\right.
$$ 

and similarly, $I_{0}$ is the indicator function of $\{0\}$ . In the formulation ( 5.3 ), the func- tion $I_{-}(u)$ can be interpreted as expressing our irritation or displeasure associated with a constraint function value $u\,=\,f_{i}(x)$ : It is zero if $f_{i}(x)\,\leq\,0$ , and infinite if $f_{i}(x)>0$ . In a similar way, $I_{0}(u)$ gives our displeasure for an equality constraint value $u=h_{i}(x)$ . We can think of $I_{-}$ as a “brick wall” or “infinitely hard” displea- sure function; our displeasure rises from zero to infinite as $f_{i}(x)$ transitions from nonpositive to positive. 

Now suppose in the formulation ( 5.3 ) we replace the function $I_{-}(u)$ with the linear function $\lambda_{i}u$ , where $\lambda_{i}\geq0$ , function $I_{0}(u)$ with $\nu_{i}u$ . The ive becomes the Lagrangian function $L(x,\lambda,\nu)$ ), and the dual function value $g(\lambda,\nu)$ ) is the optimal value of the problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\,L(x,\lambda,\nu)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x).}\end{array}
$$ 

In this formulation, we use a linear or “soft” displeasure function in place of $I_{-}$ and $I_{0}$ . For an inequality constraint, our displeasure is zero when $f_{i}(x)=0$ , and is positive when $f_{i}(x)>0$ (assuming $\lambda_{i}>0$ ); our displeasure grows as the constraint becomes “more violated”. Unlike the original formulation, in which any nonpositive value of $f_{i}(x)$ is acceptable, in the soft formulation we actually derive pleasure from constraints that have margin, i.e. , from $f_{i}(x)<0$ . 

Clearly the approximation of the indicator function $I_{-}(u)$ with a linear function $\lambda_{i}u$ is rather poor. But the linear function is at least an underestimator of the indicator function. Since $\lambda_{i}u\le I_{-}(u)$ and $\nu_{i}u\le I_{0}(u)$ for all $u$ , we see immediately that the dual function yields a lower bound on the optimal value of the original problem. 

The idea of replacing the “hard” constraints with “soft” versions will come up again when we consider interior-point methods ( 11.2.1 ). 

# 5.1.5 Examples 

In this section we give some examples for which we can derive an analytical ex- pression for the Lagrange dual function. 

# Least-squares solution of linear equations 

We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad x^{T}x}\\ &{{\mathrm{subject~to}}\quad A x=b,}\end{array}}
$$ 

where $A\in\mathbf{R}^{p\times n}$ . This problem has no inequality constraints and $p$ (linear) equality constraints. The Lagrangian is $L(x,\nu)\,=\,x^{T}x+\nu^{T}(A x-b)$ , with domain $\mathbf{R}^{n}\ \times$ $\mathbf{R}^{p}$ . The dual function is given by $\begin{array}{r}{g(\nu)=\operatorname*{inf}_{x}L(x,\nu)}\end{array}$ . Since $L(x,\nu)$ is a convex quadratic function of $x$ , we can find the minimizing $x$ from the optimality condition 

$$
\nabla_{\boldsymbol{x}}L(\boldsymbol{x},{\nu})=2\boldsymbol{x}+\boldsymbol{A}^{T}{\nu}=0,
$$ 

which yields $x=-(1/2)A^{T}\nu$ . Therefore the dual function is 

$$
g(\nu)=L(-(1/2)A^{T}\nu,\nu)=-(1/4)\nu^{T}A A^{T}\nu-b^{T}\nu,
$$ 

which is a concave quadratic function, with domain $\mathbf{R}^{p}$ . The lower bound prop- erty ( 5.2 ) states that for any $\nu\in\mathbf{R}^{p}$ , we have 

$$
-(1/4)\nu^{T}A A^{T}\nu-b^{T}\nu\leq\operatorname*{inf}\{x^{T}x\mid A x=b\}.
$$ 

# Standard form LP 

Consider an LP in standard form, 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad A x=b}\\ &{x\succeq0,}\end{array}}
$$ 

which has inequality constraint functions $f_{i}(x)~=~-x_{i}$ , $\textit{i}=1,\ldots,n$ . To form the Lagrangian we introduce multipliers $\lambda_{i}$ for the $n$ inequality constraints and multipliers $\nu_{i}$ for the equality constraints, and obtain 

$$
L(x,\lambda,\nu)=c^{T}x-\sum_{i=1}^{n}\lambda_{i}x_{i}+\nu^{T}(A x-b)=-b^{T}\nu+(c+A^{T}\nu-\lambda)^{T}x.
$$ 

The dual function is 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x}L(x,\lambda,\nu)=-b^{T}\nu+\operatorname*{inf}_{x}(c+A^{T}\nu-\lambda)^{T}x,
$$ 

which is easily determined analytically, since a linear function is bounded below only when it is ide y zero. Thus, $g(\lambda,\nu)=-\infty$ except when $c+A^{T}\nu-\lambda=0$ , in which case it is $-b^{T}\nu$ : 

$$
g(\lambda,\nu)=\left\{\begin{array}{l l}{{-b^{T}\nu}}&{{A^{T}\nu-\lambda+c=0}}\\ {{-\infty}}&{{\mathrm{otherwise.}}}\end{array}\right.
$$ 

Note that the dual function $g$ is finite only on a proper affine subset of $\mathbf{R}^{m}\times\mathbf{R}^{\mathcal{P}}$ . We will see that this is a common occurrence. 

T property ( 5.2 ) is non only when $\lambda$ and $\nu$ satisfy $\lambda\succeq0$ and A $A^{T}\nu-\lambda+c=0$ − = 0. When this occurs, − $-b^{T}\nu$ is a lower bound on the optimal value of the LP ( 5.6 ). 

# Two-way partitioning problem 

We consider the (nonconvex) problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{x^{T}W x}}\\ {{\mathrm{subject~to}}}&{{x_{i}^{2}=1,\quad i=1,\ldots,n,}}\end{array}
$$ 

where $W\in\mathbf{S}^{n}$ . The constraints restrict the values of to $1$ or $-1$ , so oblem $x_{i}$ is equivalent to finding the vector with mponents $\pm1$ that minimizes x $x^{T}W x$ . The n feasible set here is finite (it contains 2 $2^{n}$ points) so this problem can in principle be solved by simply checking the objective value of each feasible point. Since the number of feasible points grows exponentially, however, this is possible only for small problems (say, with $n\leq30$ ). In general (and for $n$ larger than, say, 50) the problem ( 5.7 ) is very difficult to solve. 

We can interpret the problem ( 5.7 ) as a two-way partitioning problem on a set of $n$ elements, say, $\{1,\cdot\cdot\cdot,n\}$ : A feasible $x$ corresponds to the partition 

$$
\{1,\ldots,n\}~=~\{i\mid x_{i}=-1\}~\cup~\{i\mid x_{i}=1\}.
$$ 

The matrix coefficient $W_{i j}$ can be interpreted as the cost of having the elements $i$ and $j$ in the same partition, and $-W_{i j}$ is the cost of having $i$ and $j$ in diﬀerent partitions. The objective in ( 5.7 ) is the total cost, over all pairs of elements, and the problem ( 5.7 ) is to find the partition with least total cost. 

We now derive the dual function for this problem. The Lagrangian is 

$$
\begin{array}{r c l}{L(x,\nu)}&{=}&{x^{T}W x+\displaystyle\sum_{i=1}^{n}\nu_{i}(x_{i}^{2}-1)}\\ &{=}&{x^{T}(W+\mathbf{diag}(\nu))x-\mathbf{1}^{T}\nu.}\end{array}
$$ 

We obtain the Lagrange dual function by minimizing over $x$ : 

$$
\begin{array}{l c l}{g(\nu)}&{=}&{\displaystyle\operatorname*{inf}_{x}x^{T}(W+\mathbf{diag}(\nu))x-\mathbf{1}^{T}\nu}\\ &{=}&{\left\{\begin{array}{l l}{-\mathbf{1}^{T}\nu}&{W+\mathbf{diag}(\nu)\succeq0}\\ {-\infty}&{\mathrm{otherwise},}\end{array}\right.}\end{array}
$$ 

where we use the fact that the infimum of a quadratic form is either zero (if the form is positive semidefinite) or $-\infty$ (if the form is not positive semidefinite). 

This dual function provides lower bounds on the optimal value of the difficult problem ( 5.7 ). For example, we can take the specific value of the dual variable 

$$
\boldsymbol{\nu}=-\lambda_{\operatorname*{min}}(W)\mathbf{1},
$$ 

which is dual feasible, since 

$$
W+{\bf d i a g}(\nu)=W-\lambda_{\mathrm{min}}(W)I\succeq0.
$$ 

This yields the bound on the optimal value $p^{\star}$ 

$$
\boldsymbol{p}^{\star}\geq-\mathbf{1}^{T}\boldsymbol{\nu}=n\lambda_{\operatorname*{min}}(W).
$$ 

Remark 5.1 This lower bound on $p^{\star}$ can also be obtained without using the Lagrange dual function. First, we replace the constraints $x_{1}^{2}=1,.\ldots,x_{n}^{2}=1$ = 1 with $\textstyle\sum_{i=1}^{n}x_{i}^{2}=n$ , to obtain the modified problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{x^{T}W x}\\ {{\mathrm{subject~to}}\quad}&{\sum_{i=1}^{n}x_{i}^{2}=n.}\end{array}}
$$ 

The constraints of the original problem ( 5.7 ) imply the constraint here, so the optimal value of the problem ( 5.9 ) is a lower bound on $p^{\star}$ , the optimal value of ( 5.7 ). But the modified problem ( 5.9 ) is easily solved as an eigenvalue problem, with optimal value $n\lambda_{\operatorname*{min}}(W)$ . 

# 5.1.6 The Lagrange dual function and conjugate functions 

Recall from § 3.3 that the conjugate $f^{*}$ of a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is given by 

$$
f^{*}(y)=\operatorname*{sup}_{x\in\mathbf{dom}_{f}}\left(y^{T}x-f(x)\right).
$$ 

The conjugate function and Lagrange dual function are closely related. To see one simple connection, consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad f(x)}\\ &{{\mathrm{subject~to}}\quad x=0}\end{array}}
$$ 

(which is not very interesting, and solvable by inspection). This problem has Lagrangian $L(x,\nu)=f(x)+\nu^{T}x$ , and dual function 

$$
g(\nu)=\operatorname*{inf}_{x}\left(f(x)+\nu^{T}x\right)=-\operatorname*{sup}_{x}\left((-\nu)^{T}x-f(x)\right)=-f^{*}(-\nu).
$$ 

More generally (and more usefully), consider an optimization problem with linear inequality and equality constraints, 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(x)}}\\ {{\mathrm{subject~to}}}&{{A x\preceq b}}\\ {{}}&{{C x=d.}}\end{array}
$$ 

Using the conjugate of $f_{0}$ we can write the dual function for the problem ( 5.10 ) as 

$$
\begin{array}{r c l}{g(\lambda,\nu)}&{=}&{\displaystyle\operatorname*{inf}_{x}\big(f_{0}(x)+\lambda^{T}(A x-b)+\nu^{T}(C x-d)\big)}\\ &{=}&{\displaystyle-b^{T}\lambda-d^{T}\nu+\operatorname*{inf}_{x}\big(f_{0}(x)+(A^{T}\lambda+C^{T}\nu)^{T}x\big)}\\ &{=}&{\displaystyle-b^{T}\lambda-d^{T}\nu-f_{0}^{*}(-A^{T}\lambda-C^{T}\nu).}\end{array}
$$ 

The domain of $g$ follows from the domain of $f_{0}^{\ast}$ : 

$$
\mathbf{dom}\,g=\{(\lambda,\nu)\ |\ -A^{T}\lambda-C^{T}\nu\in\mathbf{dom}\,f_{0}^{*}\}.
$$ 

Let us illustrate this with a few examples. 

# Equality constrained norm minimization 

Consider the problem 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\|x\|}\\ {\operatorname{subject\to}}&{A x=b,}\end{array}}
$$ 

where $\|\cdot\|$ is any norm. Recall (from example 3.26 on page 93 ) that the conjugate of $f_{0}=\|\cdot\|$ is given by 

$$
f_{0}^{*}(y)=\left\{{\begin{array}{l l}{0}&{\|y\|_{*}\leq1}\\ {\infty}&{{\mathrm{otherwise}},}\end{array}}\right.
$$ 

the indicator function of the dual norm unit ball. 

Using the result ( 5.11 ) above, the dual function for the problem ( 5.12 ) is given by 

$$
g(\nu)=-b^{T}\nu-f_{0}^{*}(-A^{T}\nu)=\left\{\begin{array}{l l}{-b^{T}\nu}&{\|A^{T}\nu\|_{*}\leq1}\\ {-\infty}&{\mathrm{otherwise}.}\end{array}\right.
$$ 

# Entropy maximization 

Consider the entropy maximization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{f_{0}(x)=\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{\mathbf{1}^{T}x=1}\end{array}}
$$ 

where $\mathbf{dom}\,f_{0}\,=\,\mathbf{R}_{++}^{n}$ . The conjugate of the negative entropy function $u\log u$ , with scalar variable $u$ , is $e^{v-1}$ (see example 3.21 on page 91 ). Since $f_{0}$ is a sum of negative entropy functions of diﬀerent variables, we conclude that its conjugate is 

$$
f_{0}^{*}(y)=\sum_{i=1}^{n}e^{y_{i}-1},
$$ 

with $\mathbf{dom}\,f_{0}^{*}=\mathbf{R}^{n}$ . Using the result ( 5.11 ) above, the dual function of ( 5.13 ) is given by 

$$
g(\lambda,\nu)=-b^{T}\lambda-\nu-\sum_{i=1}^{n}e^{-a_{i}^{T}\lambda-\nu-1}=-b^{T}\lambda-\nu-e^{-\nu-1}\sum_{i=1}^{n}e^{-a_{i}^{T}\lambda}
$$ 

where $a_{i}$ is the $i$ th column of $A$ . 

# Minimum volume covering ellipsoid 

Consider the problem with variable $X\in\mathbf{S}^{n}$ , 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\ f_{0}(X)=\log\operatorname*{det}X^{-1}}\\ &{\mathrm{subject~to}\quad a_{i}^{T}X a_{i}\le1,\quad i=1,\ldots,m,}\end{array}
$$ 

where $\mathbf{dom}\,f_{0}=\mathbf{S}_{++}^{n}$ . The problem ( 5.14 ) has a simple geometric interpretation. With each $X\in\mathbf{S}_{++}^{n}$ we associate the ellipsoid, centered at the origin, 

$$
{\mathcal{E}}_{X}=\{z\mid z^{T}X z\leq1\}.
$$ 

The volume of this ellipsoid is proportional to $\left(\operatorname*{det}X^{-1}\right)^{1/2}$  , so the objective of ( 5.14 ) is, except for a constant and a factor of two, the logarithm of the volume of $\mathcal{E}_{X}$ . The constraints of the problem ( 5.14 ) are that $a_{i}\,\in\,{\mathcal{E}}_{X}$ . Thus the prob- lem ( 5.14 ) is to determine the minimum volume ellipsoid, centered at the origin, that includes the points $a_{1},\ldots,a_{m}$ . 

The inequality constraints in problem ( 5.14 ) are affine; they can be expressed as 

$$
\mathbf{tr}\left((a_{i}a_{i}^{T})X\right)\leq1.
$$ 

In example 3.23 (page 92 ) we found that the conjugate of $f_{0}$ is 

$$
f_{0}^{*}(Y)=\log\operatorname*{det}(-Y)^{-1}-n,
$$ 

with $\mathbf{dom}\,f_{0}^{*}=-\mathbf{S}_{++}^{n}$ − . Applying the result ( 5.11 ) above, the dual function for the problem ( 5.14 ) is given by 

$$
g(\lambda)=\left\{\begin{array}{l l}{\log\operatorname*{det}\left(\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\right)-\mathbf{1}^{T}\lambda+n}&{\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\succ0}\\ {-\infty}&{\mathrm{otherwise}.}\end{array}\right.
$$ 

Thus, for any $\lambda\succeq0$ with $\begin{array}{r}{\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\succ0}\end{array}$ ≻ 0, the number 

$$
\log\operatorname*{det}\left(\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\right)-\mathbf{1}^{T}\boldsymbol{\lambda}+\boldsymbol{n}
$$ 

is a lower bound on the optimal value of the problem ( 5.14 ). 

# 5.2 The Lagrange dual problem 

For each pair $(\lambda,\nu)$ with $\lambda\succeq0$ , the Lagrange dual function gives us a lower bound on the optimal value $p^{\star}$ of the optimization problem ( 5.1 ). Thus we have a lower bound that depends on some parameters $\lambda$ , $\nu$ . A natural question is: What is the best lower bound that can be obtained from the Lagrange dual function? 

This leads to the optimization problem 

$$
\begin{array}{l l}{\mathrm{maximize}}&{g(\lambda,\nu)}\\ {\mathrm{subject~to}}&{\lambda\succeq0.}\end{array}
$$ 

This problem is called the Lagrange dual problem associated with the problem ( 5.1 ). In this context the original problem ( 5.1 ) is sometimes called the primal problem . The term dual feasible , to describe a pair $(\lambda,\nu)$ with $\lambda\succeq0$ and $g(\lambda,\nu)\,>\,-\infty$ , now makes sense. It means, as the name implies, that $(\lambda,\nu)$ is feasible for the dual problem ( 5.16 ). We refer to $(\lambda^{\star},\nu^{\star})$ as dual optimal or optimal Lagrange multipliers if they are optimal for the problem ( 5.16 ). 

The Lagrange dual problem ( 5.16 ) is a convex optimization problem, since the objective to be maximized is concave and the constraint is convex. This is the case whether or not the primal problem ( 5.1 ) is convex. 

# 5.2.1 Making dual constraints explicit 

The examples above show that it is not uncommon for the domain of the dual function, 

$$
\mathbf{dom}\,g=\{(\lambda,\nu)\mid g(\lambda,\nu)>-\infty\},
$$ 

to have dimension smaller than $m+p$ . In many cases we can identify the affine hull of $\mathbf{dom}\,g$ , and describe it as a set of linear equality constraints. Roughly speaking, this means we can identify the equality constraints that are ‘hidden’ or ‘implicit’ in the objective $g$ of the dual problem ( 5.16 ). In this case we can form an equivalent problem, in which these equality constraints are given explicitly as constraints. The following examples demonstrate this idea. 

# Lagrange dual of standard form LP 

On page 219 we found that the Lagrange dual function for the standard form LP 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}}&{A x=b}\\ &{x\succeq0}\end{array}}
$$ 

is given by 

$$
g(\lambda,\nu)=\left\{\begin{array}{l l}{{-b^{T}\nu}}&{{A^{T}\nu-\lambda+c=0}}\\ {{-\infty}}&{{\mathrm{otherwise.}}}\end{array}\right.
$$ 

Strictly speaking, the Lagrange dual problem of the standard form LP is to maxi- mize this dual function $g$ subject to $\lambda\succeq0$ , i.e. , 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad g(\lambda,\nu)=\left\{\begin{array}{l l}{-b^{T}\nu}&{A^{T}\nu-\lambda+c=0}\\ {-\infty}&{\mathrm{otherwise}}\end{array}\right.}\\ &{\mathrm{subject~to}\quad\lambda\succeq0.}\end{array}
$$ 

Here $g$ is finite only when $A^{T}\nu-\lambda+c=0$ . We can form an equivalent problem by making these equality constraints explicit: 

$$
\begin{array}{l l}{\mathrm{maximize}}&{-b^{T}\nu}\\ {\mathrm{subject~to}}&{A^{T}\nu-\lambda+c=0}\\ &{\lambda\succeq0.}\end{array}
$$ 

This problem, in turn, can be expressed as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}\nu}\\ &{\mathrm{subject~to}\quad A^{T}\nu+c\succeq0,}\end{array}
$$ 

which is an LP in inequality form. 

Note the subtle distinctions between these three problems. The Lagrange dual of the standard form LP ( 5.17 ) is the problem ( 5.18 ), which is equivalent to (but not the same as) the problems ( 5.19 ) and ( 5.20 ). With some abuse of terminology, we refer to the problem ( 5.19 ) or the problem ( 5.20 ) as the Lagrange dual of the standard form LP ( 5.17 ). 

# Lagrange dual of inequality form LP 

In a similar way we can find the Lagrange dual problem of a linear program in inequality form 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b.}\end{array}}
$$ 

The Lagrangian is 

$$
L(x,\lambda)=c^{T}x+\lambda^{T}(A x-b)=-b^{T}\lambda+(A^{T}\lambda+c)^{T}x,
$$ 

so the dual function is 

$$
g(\lambda)=\operatorname*{inf}_{x}L(x,\lambda)=-b^{T}\lambda+\operatorname*{inf}_{x}(A^{T}\lambda+c)^{T}x.
$$ 

The infimum of a linear function is $-\infty$ , except in the special case when it is identically zero, so the dual function is 

$$
g(\lambda)={\left\{\begin{array}{l l}{-b^{T}\lambda}&{A^{T}\lambda+c=0}\\ {-\infty}&{{\mathrm{otherwise.}}}\end{array}\right.}
$$ 

The dual variable $\lambda$ is dual feasible if $\lambda\succeq0$ and $A^{T}\lambda+c=0$ . 

The Lagrange dual of the LP ( 5.21 ) is to maximize $g$ over all $\lambda\succeq0$ . Again we can reformulate this by explicitly including the dual feasibility conditions as constraints, as in 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}\lambda}\\ &{\mathrm{subject~to}\quad A^{T}\lambda+c=0}\\ &{\quad\quad\quad\lambda\succeq0,}\end{array}
$$ 

which is an LP in standard form. 

Note the interesting symmetry between the standard and inequality form LPs and their duals: The dual of a standard form LP is an LP with only inequality constraints, and vice versa. One can also verify that the Lagrange dual of ( 5.22 ) is (equivalent to) the primal problem ( 5.21 ). 

# 5.2.2 Weak duality 

The optimal value of the Lagrange dual problem, which we denote $d^{\star}$ , is, by def- inition, the best lower bound on $p^{\star}$ that can be obtained from the Lagrange dual function. In particular, we have the simple but important inequality 

$$
\begin{array}{r}{d^{\star}\leq p^{\star},}\end{array}
$$ 

which holds even if the original problem is not convex. This property is called weak duality . 

The weak duality inequality ( 5.23 ) holds when $d^{\star}$ and $p^{\star}$ are infinite. For exam primal problem is unbounded below, so that $p^{\star}=-\infty$ , we must have d $d^{\star}\,=\,-\infty$ −∞ , i.e. , the Lagrange dual p is infeasible. Conversely, if the dual problem is unbounded above, so that d $d^{\star}=\infty$ ∞ , we must have $p^{\star}=\infty$ , i.e. , the primal problem is infeasible. 

We refer to the diﬀerence $p^{\star}-d^{\star}$ as the optimal duality gap of the original problem, since it gives the gap between the optimal value of the primal problem and the best ( i.e. , greatest) lower bound on it that can be obtained from the Lagrange dual function. The optimal duality gap is always nonnegative. 

The bound ( 5.23 ) can sometimes be used to find a lower bound on the optimal value of a problem that is difficult to solve, since the dual problem is always convex, and in many cases can be solved efficiently, to find $d^{\star}$ . As an example, consider the two-way partitioning problem ( 5.7 ) described on page 219 . The dual problem is an SDP, 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-\mathbf{1}^{T}\nu}\\ &{\mathrm{subject~to}\quad W+\mathbf{diag}(\nu)\succeq0,}\end{array}
$$ 

with variable $\nu\,\in\mathbf{R}^{n}$ . Th can be solved efficiently, even for relatively large values of $n$ , such as n = 1000. Its optimal value is a lower bound on the optimal value of the two-way partitioning problem, and is always at least as good as the lower bound ( 5.8 ) based on $\lambda_{\operatorname*{min}}(W)$ . 

# 5.2.3 Strong duality and Slater’s constraint qualification 

If the equality 

$$
d^{\star}=p^{\star}
$$ 

holds, i.e. , the optimal duality gap is zero, then we say that strong duality holds. This means that the best bound that can be obtained from the Lagrange dual function is tight. 

Strong duality does not, in general, hold. But if the primal problem ( 5.1 ) is convex, i.e. , of the form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,\ldots,m,}\\ &{A x=b,}\end{array}}
$$ 

with $f_{0},\ldots,f_{m}$ convex, we usually (but not always) have strong duality. There are many results that establish conditions on the problem, beyond convexity, under which strong duality holds. These conditions are called constraint qualifications . 

On simple constraint qualification is Slater’s condition : There exists an $x\in$ relint such that 

$$
f_{i}(x)<0,\quad i=1,\ldots,m,\qquad A x=b.
$$ 

Such a point is sometimes called strictly feasible , since the inequality constraints hold with strict inequalities. Slater’s theorem states that strong duality holds, if Slater’s condition holds (and the problem is convex). 

Slater’s condition can be refined when some of the inequality constraint func- tions $f_{i}$ are affine. If the first $k$ constraint functions $f_{1},\ldots,f_{k}$ are affine, then strong duality holds provided the following weaker condition holds: There exists an $x\in\mathbf{relint}\,\mathcal{D}$ with 

$$
f_{i}(x)\leq0,\quad i=1,\ldots,k,\qquad f_{i}(x)<0,\quad i=k+1,\ldots,m,\qquad A x=b.
$$ 

In other words, the affine inequalities do not need to hold with strict inequal- ity. Note that the refined Slater condition ( 5.27 ) reduces to feasibility when the constraints are all linear equalities and inequalities, and $\mathbf{dom}\,f_{0}$ is open. 

Slater’s condition (and the refinement ( 5.27 )) not only implies strong duality for convex problems. It also implies that the dual optimal value is attained when $d^{\star}>-\infty$ , i.e. , there exists a dual feasible $(\lambda^{\star},\nu^{\star})$ with $g(\lambda^{\star},\nu^{\star})=d^{\star}=p^{\star}$ . We will prove that strong duality obtains, when the primal problem is convex and Slater’s condition holds, in $\S5.3.2$ . 

# 5.2.4 Examples 

# Least-squares solution of linear equations 

Recall the problem ( 5.5 ): 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad x^{T}x}\\ &{{\mathrm{subject~to}}\quad A x=b.}\end{array}}
$$ 

The associated dual problem is 

$$
\mathrm{maximize}\quad-(1/4)\nu^{T}A A^{T}\nu-b^{T}\nu,
$$ 

which is an unconstrained concave quadratic maximization problem. 

Slater’s condition is simply that the primal problem is feasible, so $p^{\star}\,=\,d^{\star}$ provided $b\in{\mathcal{R}}(A)$ , i.e. , $p^{\star}<\infty$ . In fact for this problem we always have strong even when $p^{\star}=\infty$ . This is the case when $b\notin\mathcal{R}(A)$ , so there is a $z$ with $A^{T}z=0$ $b^{T}z\neq0$ . I s that the dual function is unbounded above along the line $\{t z\mid t\in\mathbf{R}\}$ , so d $d^{\star}=\infty$ ∞ as well. 

# Lagrange dual of LP 

By the weaker form of Slater’s condition, we find that strong duality holds for any LP (in standard or inequality form) provided the primal problem is feasible. Applying this result to the duals, we conclude that strong duality holds for LPs if the dual is feasible. This leaves only one possible situation in which strong duality for LPs can fail: both the primal and dual problems are infeasible. This pathological case can, in fact, occur; see exercise 5.23 . 

# Lagrange dual of QCQP 

We consider the QCQP 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{(1/2)x^{T}P_{0}x+q_{0}^{T}x+r_{0}}}\\ {{\mathrm{subject~to}}}&{{(1/2)x^{T}P_{i}x+q_{i}^{T}x+r_{i}\leq0,\quad i=1,\ldots,m,}}\end{array}
$$ 

with $P_{0}\in\mathbf{S}_{++}^{n}$ , and $P_{i}\in\mathbf{S}_{+}^{n}$ , $i=1,\ldots,m$ . The Lagrangian is 

$$
L(x,\lambda)=(1/2)x^{T}P(\lambda)x+q(\lambda)^{T}x+r(\lambda),
$$ 

where 

$$
P(\lambda)=P_{0}+\sum_{i=1}^{m}\lambda_{i}P_{i},\qquad q(\lambda)=q_{0}+\sum_{i=1}^{m}\lambda_{i}q_{i},\qquad r(\lambda)=r_{0}+\sum_{i=1}^{m}\lambda_{i}r_{i}.
$$ 

It is possible to derive an expression for $g(\lambda)$ for general $\lambda$ , but it is quite compli- cated. If $\lambda\succeq0$ , however, we have $P(\lambda)\succ0$ and 

$$
g(\lambda)=\operatorname*{inf}_{x}L(x,\lambda)=-(1/2)q(\lambda)^{T}P(\lambda)^{-1}q(\lambda)+r(\lambda).
$$ 

We can therefore express the dual problem as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-(1/2)q(\lambda)^{T}P(\lambda)^{-1}q(\lambda)+r(\lambda)}\\ &{\mathrm{subject~to}\quad\lambda\succeq0.}\end{array}
$$ 

The Slater condition says that strong duality between ( 5.29 ) and ( 5.28 ) holds if the quadratic inequality constraints are strictly feasible, i.e. , there exists an $x$ with 

$$
(1/2)x^{T}{\cal P}_{i}x+q_{i}^{T}x+r_{i}<0,\quad i=1,.\,.\,,m.
$$ 

# Entropy maximization 

Our next example is the entropy maximization problem ( 5.13 ): 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\sum_{i=1}^{n}x_{i}\log x_{i}}\\ &{{\mathrm{subject~to}}\quad A x\preceq b}\\ &{\quad\mathbf{1}^{T}x=1,}\end{array}}
$$ 

with domain $\mathcal{D}=\mathbf{R}_{+}^{n}$ . The Lagrange dual function was derived on page 222 ; the dual problem is 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}\lambda-\nu-e^{-\nu-1}\sum_{i=1}^{n}e^{-a_{i}^{T}\lambda}}\\ &{\mathrm{subject~to}\quad\lambda\succeq0,}\end{array}
$$ 

with variables $\lambda\in\mathbf{R}^{m}$ , $\nu\in\mathbf{R}$ . The (weaker) Slater condition for ( 5.13 ) tells us optimal duality gap is zero if there exists an $x\,\succ\,0$ with $A x\ \preceq\ b$ and

 ${\bf1}^{T}x=1$ 

We can simplify the dual problem ( 5.30 ) by maximizing over the dual variable

 $\nu$ analytically. For fixed $\lambda$ , the objective function is maximized when the derivative with respect to $\nu$ is zero, i.e. , 

$$
\nu=\log\sum_{i=1}^{n}e^{-a_{i}^{T}\lambda}-1.
$$ 

Substituting this optimal value of $\nu$ into the dual problem gives 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}\lambda-\log\left(\sum_{i=1}^{n}e^{-a_{i}^{T}\lambda}\right)}\\ &{\mathrm{subject~to}\quad\lambda\succeq0,}\end{array}
$$ 

which is a geometric program (in convex form) with nonnegativity constraints. 

# Minimum volume covering ellipsoid 

We consider the problem ( 5.14 ): 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\log\operatorname*{det}X^{-1}}\\ {\mathrm{subject~to}}&{a_{i}^{T}X a_{i}\leq1,\quad i=1,.\,.\,,m,}\end{array}
$$ 

with domain $\mathcal{D}=\mathbf{S}_{++}^{n}$ . The Lagrange dual function is given by ( 5.15 ), so the dual problem can be expressed as 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\log\operatorname*{det}\left(\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\right)-\mathbf{1}^{T}\lambda+n}\\ {{\mathrm{subject~to}}}&{\lambda\succeq0}\end{array}}
$$ 

where we take $\log\operatorname*{det}X=-\infty$ if $X\neq0$ . 

The (weaker) Slater condition for the problem ( 5.14 ) is that there exists an $X\,\in\,\mathbf{S}_{++}^{n}$ with $a_{i}^{T}X a_{i}\,\leq\,1$ ≤ 1, for $i\,=\,1,.\,.\,.\,,m$ . This is always satisfied, so strong duality always obtains between ( 5.14 ) and the dual problem ( 5.31 ). 

# A nonconvex quadratic problem with strong duality 

On rare occasions strong duality obtains for a nonconvex problem. As an important example, we consider the problem of minimizing a nonconvex quadratic function over the unit ball, 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad x^{T}A x+2b^{T}x}\\ {{\mathrm{subject~to}}}&{x^{T}x\leq1,}\end{array}}
$$ 

where $A\in\mathbf{S}^{\mathcal{N}}$ , $A\not\geq0$ , and $b\in\mathbf{R}^{n}$ . Since $A\not\geq0$ , this is not a convex problem. This problem is sometimes called the trust region problem , and arises in minimizing a second-order approximation of a function over the unit ball, which is the region in which the approximation is assumed to be approximately valid. 

The Lagrangian is 

$$
L(\boldsymbol{x},\lambda)=\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x}+2\boldsymbol{b}^{T}\boldsymbol{x}+\lambda(\boldsymbol{x}^{T}\boldsymbol{x}-1)=\boldsymbol{x}^{T}(\boldsymbol{A}+\lambda\boldsymbol{I})\boldsymbol{x}+2\boldsymbol{b}^{T}\boldsymbol{x}-\lambda,
$$ 

so the dual function is given by 

$$
g(\lambda)=\left\{\begin{array}{l l}{-b^{T}(A+\lambda I)^{\dagger}b-\lambda}&{A+\lambda I\succeq0,\quad b\in\mathcal{R}(A+\lambda I)}\\ {-\infty}&{\mathrm{otherwise},}\end{array}\right.
$$ 

where $(A+\lambda I)^{\dagger}$ is the pseudo-inverse of $A+\lambda I$ . The Lagrange dual problem is thus 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}(A+\lambda I)^{\dagger}b-\lambda}\\ &{\mathrm{subject~to}\quad A+\lambda I\succeq0,\quad b\in\mathcal{R}(A+\lambda I),}\end{array}
$$ 

with variable $\lambda\,\in\,\mathbf{R}$ . Although it is not obvious from this expression, this is a convex optimization problem. In fact, it is readily solved since it can be expressed as 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}-\sum_{i=1}^{n}(q_{i}^{T}b)^{2}/(\lambda_{i}+\lambda)-\lambda}\\ {\mathrm{subject~to}}&{{}\lambda\geq-\lambda_{\operatorname*{min}}(A),}\end{array}
$$ 

where $\lambda_{i}$ and $q_{i}$ are the eigenvalues and corresponding (orthonormal) eigenvectors of $A$ , and we interpret $(q_{i}^{T}b)^{2}/0$ 0 as $0$ if $q_{i}^{T}b=0$ = 0 and as $\infty$ otherwise. 

Despite the fact that the original problem ( 5.32 ) is not convex, we always have zero optimal duality gap for this problem: The optimal values of ( 5.32 ) and ( 5.33 ) are always the same. In fact, a more general result holds: strong duality holds for any optimization problem with quadratic objective and one quadratic inequality constraint, provided Slater’s condition holds; see $\S$ B.1 . 

# 5.2.5 Mixed strategies for matrix games 

In this section we use strong duality to derive a basic result for zero-sum matrix games. We consider a game with two players. Player 1 makes a choice (or move ) $k\in\{1,.\,.\,.\,,n\}$ , and player 2 makes $l\in\{1,.\,.\,.\,,m\}$ . Player 1 then makes a payment of $P_{k l}$ to layer 2, where P $P\in\mathbf{R}^{n\times m}$ ∈ is the payoﬀmatrix for the game. The goal of player 1 is to make the payment as small as possible, while the goal of player 2 is to maximize it. 

The players use randomized or mixed strategies , which means that each player makes his or her choice randomly and independently of the other player’s choice, according to a probability distribution: 

$$
\mathbf{prob}(k=i)=u_{i},\quad i=1,.\,.\,,n,\qquad\mathbf{prob}(l=i)=v_{i},\quad i=1,.\,.\,,m.
$$ 

Here $u$ and $v$ give the probability distributions of the choices of the two players, $i$ .e. , their associated strategies. The expected payoﬀfrom player 1 to player 2 is then 

$$
\sum_{k=1}^{n}\sum_{l=1}^{m}u_{k}v_{l}P_{k l}=u^{T}P v.
$$ 

Player 1 wishes to choose $u$ to minimize $u^{T}P v$ , while player 2 wishes to choose $v$ to maximize $u^{T}P v$ . 

Let us first analyze the game from the point of view of player 1, assuming her strategy $u$ is known to player 2 (which clearly gives an advantage to player 2). Player 2 will choose $v$ to maximize $u^{T}P v$ , which results in the expected payoﬀ 

$$
\operatorname*{sup}\{u^{T}P v\mid v\succeq0,\ \mathbf{1}^{T}v=1\}=\operatorname*{max}_{i=1,\ldots,m}(P^{T}u)_{i}.
$$ 

The best thing player 1 can do is to choose $u$ to minimize this worst-case payoﬀto player 2, i.e. , to choose a strategy $u$ that solves the problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\operatorname*{max}_{i=1,\dots,m}(P^{T}\boldsymbol{u})_{i}}\\ {\mathrm{subject~to}}&{\boldsymbol{u}\succeq0,\quad\mathbf{1}^{T}\boldsymbol{u}=1,}\end{array}
$$ 

which is a piecewise-linear convex optimization problem. We will denote the opti- mal value of this problem as $p_{1}^{\star}$ . This is the smallest expected payoﬀplayer 1 can arrange to have, assuming that player 2 knows the strategy of player 1, and plays to his own maximum advantage. 

In a similar way we can consider the situation in which $v$ , the strategy of player 2, is known to player $1$ (which gives an advantage to player $1$ ). In this case player 1 chooses $u$ to minimize $u^{T}P v$ , which results in an expected payoﬀof 

$$
\operatorname*{inf}\{u^{T}P v\mid u\succeq0,\ \mathbf{1}^{T}u=1\}=\operatorname*{min}_{i=1,\ldots,n}(P v)_{i}.
$$ 

Player 2 chooses $v$ to maximize this, i.e. , chooses a strategy $v$ that solves the problem 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\operatorname*{min}_{i=1,\dots,n}(P v)_{i}}\\ &{\mathrm{subject~to}\quad v\succeq0,\quad\mathbf{1}^{T}v=1,}\end{array}
$$ 

which is another convex optimization problem, with piecewise-linear (concave) ob- jective. We will denote the optimal value of this problem as $p_{2}^{\star}$ . This is the largest expected payoﬀplayer 2 can guarantee getting, assuming that player 1 knows the strategy of player 2. 

It is intuitively obvious that knowing your opponent’s strategy gives an advan- tage (or at least, cannot hurt), and indeed, it is easily shown that we always have $p_{1}^{\star}\,\geq\,p_{2}^{\star}$ ≥ . We can interpret the diﬀerence, $p_{1}^{\star}-p_{2}^{\star}$ − , which is nonnegative, as the advantage conferred on a player by knowing the opponent’s strategy. 

Using duality, we can establish a result that is at first surprising: $p_{1}^{\star}\,=\,p_{2}^{\star}$ . In other words, in a matrix game with mixed strategies, there is $n o$ advantage to knowing your opponent’s strategy. We will establish this result by showing that the two problems ( 5.34 ) and ( 5.35 ) are Lagrange dual problems, for which strong duality obtains. 

We start by formulating ( 5.34 ) as an LP, 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad t}\\ &{{\mathrm{subject~to}}\quad u\succneq0,\quad\mathbf{1}^{T}u=1}\\ &{\quad\quad\quad P^{T}u\preceq t\mathbf{1},}\end{array}}
$$ 

with extra e $t\in\mathbf{R}$ . Introducing the multiplier $\lambda$ for $P^{T}u\preceq t{\bf1}$ , for $u\succeq0$ , $\mu$ and $\nu$ for 1 $\mathbf{1}^{T}u=1$ = 1, the Lagrangian is 

$$
t+\lambda^{T}(P^{T}u-t\mathbf{1})-\mu^{T}u+\nu(1-\mathbf{1}^{T}u)=\nu+(1-\mathbf{1}^{T}\lambda)t+(P\lambda-\nu\mathbf{1}-\mu)^{T}u,
$$ 

so the dual function is 

$$
g(\lambda,\mu,\nu)=\left\{\begin{array}{l l}{{\nu}}&{{{\bf1}^{T}\lambda=1,\quad P\lambda-\nu{\bf1}=\mu}}\\ {{-\infty}}&{{\mathrm{otherwise}.}}\end{array}\right.
$$ 

The dual problem is then 

$$
{\begin{array}{r l}&{{\mathrm{maximize}}\quad\nu}\\ &{{\mathrm{subject~to}}\quad\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1,\quad\mu\succeq0}\\ &{\quad\quad\quad\quad P\lambda-\nu\mathbf{1}=\mu.}\end{array}}
$$ 

Eliminating $\mu$ we obtain the following Lagrange dual of ( 5.34 ): 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\nu}\\ {\mathrm{subject~to}}&{\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1}\\ &{P\lambda\succeq\nu\mathbf{1},}\end{array}
$$ 

with variables $\lambda$ , $\nu$ . But this is clearly equivalent to ( 5.35 ). Since the LPs are feasible, we have strong duality; the optimal values of ( 5.34 ) and ( 5.35 ) are equal. 

# 5.3 Geometric interpretation 

# 5.3.1 Weak and strong duality via set of values 

We can give a simple geometric interpretation of the dual function in terms of the set 

$$
\mathcal{G}=\{(f_{1}(x),\dots,f_{m}(x),h_{1}(x),\dots,h_{p}(x),f_{0}(x))\in\mathbf{R}^{m}\times\mathbf{R}^{p}\times\mathbf{R}\mid x\in\mathcal{D}\},
$$ 

which is the set of values taken on by the constraint and objective functions. The optimal value $p^{\star}$ of ( 5.1 ) is easily expressed in terms of $\mathcal{G}$ as 

$$
p^{\star}=\operatorname*{inf}\{t\mid(u,v,t)\in\mathcal{G},\ u\preceq0,\ v=0\}.
$$ 

To evaluate the dual function at $(\lambda,\nu)$ , we minimize the affine function 

$$
(\lambda,\nu,1)^{T}(\boldsymbol{u},\boldsymbol{v},t)=\sum_{i=1}^{m}\lambda_{i}u_{i}+\sum_{i=1}^{p}\nu_{i}v_{i}+t
$$ 

over $(u,v,t)\in\mathcal{G}$ , i.e. , we have 

$$
g(\lambda,\nu)=\operatorname*{inf}\{(\lambda,\nu,1)^{T}(u,v,t)\mid(u,v,t)\in\mathcal{G}\}.
$$ 

In particular, we see that if the infimum is finite, then the inequality 

$$
(\lambda,\nu,1)^{T}(u,v,t)\geq g(\lambda,\nu)
$$ 

defines a supporting hyperplane to $\mathcal{G}$ . This is sometimes referred to as a nonvertical supporting hyperplane, because the last component of the normal vector is nonzero. Now suppose $\lambda\succeq0$ . Then, obviously, $t\geq(\lambda,\nu,1)^{T}(u,v,t)$ if $u\preceq0$ and $v=0$ . Therefore 

$$
\begin{array}{l l l}{p^{\star}}&{=}&{\operatorname*{inf}\{t\mid(u,v,t)\in\mathcal{G},~u\preceq0,~v=0\}}\\ &{\geq}&{\operatorname*{inf}\{(\lambda,\nu,1)^{T}(u,v,t)\mid(u,v,t)\in\mathcal{G},~u\preceq0,~v=0\}}\\ &{\geq}&{\operatorname*{inf}\{(\lambda,\nu,1)^{T}(u,v,t)\mid(u,v,t)\in\mathcal{G}\}}\\ &{=}&{g(\lambda,\nu),}\end{array}
$$ 

i.e. , we have weak duality. This interpretation is illustrated in figures 5.3 and 5.4 , for a simple problem with one inequality constraint. 

# Epigraph variation 

In this section we describe a variation on the geometric interpretation of duality in terms of $\mathcal{G}$ , which explains why strong duality obtains for (most) convex problems. We define the set $\mathcal{A}\subseteq\mathbf{R}^{\eta\iota}\times\mathbf{R}^{\nu}\times\mathbf{R}$ as 

$$
\mathcal{A}=\mathcal{G}+\left(\mathbf{R}_{+}^{m}\times\{0\}\times\mathbf{R}_{+}\right),
$$ 

or, more explicitly, 

$$
\begin{array}{r l}&{\mathcal{A}=\{(u,v,t)\mid\exists x\in\mathcal{D},\ f_{i}(x)\leq u_{i},\ i=1,.\,.\,.\,,m,}\\ &{\qquad h_{i}(x)=v_{i},\ i=1,.\,.\,,p,\ f_{0}(x)\leq t\},}\end{array}
$$ 

![](images/d552fefdd623f803e706d551261556bfe4836a622fc2e2bdb9ef1bad5a6b0a3a.jpg) 
gure 5.3 Geometric interpretation of dual function and low bound $g(\lambda)\leq$ $p^{\star}$ , for a problem with one (inequality) constraint. Given λ , we minimize $(\lambda,1)^{T}(u,t)$ over ${\mathcal{G}}\,=\,\{(f_{1}(x),f_{0}(x))\ |\ x\,\in\,{\mathcal{D}}\}$ . This yields a supporting plane with slope − $-\lambda$ . The intersection of this hyperplane with the = 0 axis gives $g(\lambda)$ . 

![](images/fe0a9c7383cb54ed2dbc6f6258fd4453b4fddd4ba7f4a057690e2a6ceb7548e6.jpg) 
Figure 5.4 Supporting hyperplanes corresponding to three dual feasible val- ues of $\lambda$ , including the optimum $\lambda^{\star}$ . Strong duality does not hold; the optimal duality gap $p^{\star}-d^{\star}$ is positive. 

![](images/7c5670588ef073e28b8193da43fee0f9f69c2bb5fec8f2c5dd5b68320841f7be.jpg) 
Figure 5.5 Geometric interpretation of dual function and low bound $g(\lambda)\leq$ $p^{\star}$ , for a problem with one (inequality) constraint. Given λ , we minimize $(\lambda,1)^{T}(u,t)$ over $\mathcal{A}=\{(u,t)\mid\exists x\in\mathcal{D}$ $f_{0}(x)\leq t$ , $f_{1}(x)\leq u\}$ . This yields a suppor yperplane slope − $-\lambda$ . The intersection of this hyperplane with the u = 0 axis gives $g(\lambda)$ ). 

We can think of $\mathcal{A}$ as a sort of epigraph form of $\mathcal{G}$ , since $\mathcal{A}$ includes all the points in $\mathcal{G}$ , as well as points that are ‘worse’, i.e. , those with larger objective or inequality constraint function values. 

We can express the optimal value in terms of $\mathcal{A}$ as 

$$
p^{\star}=\operatorname*{inf}\{t\mid(0,0,t)\in\mathcal{A}\}.
$$ 

To evaluate the dual function at a p int $(\lambda,\nu)$ with $\lambda\succeq0$ , we can minimize the affine function $(\lambda,\nu,1)^{T}(u,v,t)$ over A : If $\lambda\succeq0$ , then 

$$
g(\lambda,\nu)=\operatorname*{inf}\{(\lambda,\nu,1)^{T}(u,v,t)\mid(u,v,t)\in\mathcal{A}\}.
$$ 

If the infimum is finite, then 

$$
(\lambda,\nu,1)^{T}(u,v,t)\geq g(\lambda,\nu)
$$ 

defines a nonvertical supporting hyperplane to $\mathcal{A}$ In particular, since $\left(0,0,p^{\star}\right)\in\mathbf{bd}\mathcal{A}$ , we have 

$$
\begin{array}{r}{p^{\star}=(\lambda,\nu,1)^{T}(0,0,p^{\star})\geq g(\lambda,\nu),}\end{array}
$$ 

the weak duality lower bound. Strong duality holds if and only if we have equality in ( 5.38 ) for some dual feasible $(\lambda,\nu)$ , i.e. , there exists a nonvertical supporting hyperplane to $\mathcal{A}$ at its boundary point $(0,0,p^{\star})$ . 

This second interpretation is illustrated in figure 5.5 . 

# 5.3.2 Proof of strong duality under constraint qualification 

In this section we prove that Slater’s constraint qualification guarantees strong duality (and that the dual optimum is attained) for a convex problem. We consider the primal problem ( 5.25 ), with $f_{0},\ldots,f_{m}$ convex, and assume Slater’s condition holds: There exists $\tilde{{\boldsymbol{x}}}\,\in\,\mathbf{relint}\,\mathcal{D}$ ∈ D with $f_{i}(\tilde{x})\,<\,0$ $i\,=\,1,.\,.\,.\,,m$ , and $\boldsymbol{A}\tilde{\boldsymbol{x}}\,=\,\boldsymbol{b}$ . In order to simplify the proof, we m ditional assumptions: first that $\mathcal{D}$ has nonempty interior (hence, relint D $\mathcal{D}=\mathrm{int}\,\mathcal{D}$ D ) and second, that $\mathbf{rank}\,A\,=\,p$ . We assume that $p^{\star}$ is finite. (Since there is a feasible point, we can only have $p^{\star}=-\infty$ or $p^{\star}$ finite; if $p^{\star}=-\infty$ , then $d^{\star}=-\infty$ by weak duality.) 

The set $\mathcal{A}$ defined in ( 5.37 ) is readily shown o be convex if the underlying problem is convex. We define a second convex set as 

$$
\mathcal{B}=\{(0,0,s)\in\mathbf{R}^{m}\times\mathbf{R}^{p}\times\mathbf{R}\mid s<p^{\star}\}.
$$ 

The sets $\mathcal{A}$ and $\mathcal{B}$ do not intersect. To see this, suppose $(u,v,t)\in\mathcal{A}\cap\mathcal{B}$ . Since $(u,v,t)\in\mathcal{B}$ we have $u=0$ , $v=0$ $t<p^{\star}$ . Since $(u,v,t)\in\mathcal{A}$ , there exists an $x$ with $f_{i}(x)\leq0$ , $i=1,\ldots,m$ , $A x-b=0$ − = 0, and $f_{0}(x)\leq t<p^{\star}$ , which is impossible since $p^{\star}$ is the optimal value of the primal problem. 

By the separating hyperplane theorem of $\S2.5.1$ there exists $(\ddot{\lambda},\tilde{\nu},\mu)\neq0$ ̸ = 0 and $\alpha$ such that 

$$
(u,v,t)\in\mathcal{A}\implies\tilde{\lambda}^{T}u+\tilde{\nu}^{T}v+\mu t\ge\alpha,
$$ 

and 

$$
(u,v,t)\in\mathcal{B}\implies\tilde{\lambda}^{T}u+\tilde{\nu}^{T}v+\mu t\le\alpha.
$$ 

From ( 5.39 ) we conclude that $\overset{\cdot}{\lambda}\succeq0$ ⪰ 0 and $\mu\geq0$ . (Otherwise $\Tilde{\lambda}^{T}u+\mu t$ is unbounded below over $\mathcal{A}$ , contradicting ( 5.39 ).) The condition ( 5.40 ) simply means that $\mu t\leq\alpha$ for all $t<p^{\star}$ , and hence, $\mu p^{\star}\leq\alpha$ . Together with ( 5.39 ) we conclude that for any $x\in\mathcal{D}$ , 

$$
\sum_{i=1}^{m}\tilde{\lambda}_{i}f_{i}(x)+\tilde{\nu}^{T}(A x-b)+\mu f_{0}(x)\geq\alpha\geq\mu p^{\star}.
$$ 

Assume that $\mu>0$ . In that case we can divide ( 5.41 ) by $\mu$ to obtain 

$$
L(x,\tilde{\lambda}/\mu,\tilde{\nu}/\mu)\geq p^{\star}
$$ 

for all $x\in\mathcal{D}$ , from which it follows, by minimizing over $x$ , that $g(\lambda,\nu)\geq p^{\star}$ , where we define 

$$
\lambda=\tilde{\lambda}/\mu,\qquad\nu=\tilde{\nu}/\mu.
$$ 

By weak duality we have $g(\lambda,\nu)\,\leq\,p^{\star}$ , so in fact $g(\lambda,\nu)\,=\,p^{\star}$ . This shows that strong duality holds, and that the dual optimum is attained, at least in the case when $\mu>0$ . 

Now consider the case $\mu=0$ . From ( 5.41 ), we conclude that for all $x\in\mathcal{D}$ , 

$$
\sum_{i=1}^{m}\tilde{\lambda}_{i}f_{i}(x)+\tilde{\nu}^{T}(A x-b)\geq0.
$$ 

Applying this to the point x that satisfies the Slater condition, we have 

$$
\sum_{i=1}^{m}\tilde{\lambda}_{i}f_{i}(\tilde{x})\geq0.
$$ 

![](images/fcbdd183c9de5b156b1ed57f03b3cb88d675a50ab897c28cea658e2c554a6918.jpg) 
Figure 5.6 Illustration of strong duality proof, for a convex problem that sat- isfi Slater’s constraint qualification. The set $\mathcal{A}$ is shown shaded, and the set B is the thick vertical line segment, not including the point $(0,p^{\star})$ , shown as a small open circle. The two sets are convex and do not intersect, so they can be separated by a hyperplane. Slater’s constraint qualification guaran- tees that any separating hyperplane must be nonvertical, since it must pass to the left of the point $(\widetilde{u},\dot{t})=(f_{1}(\widetilde{x}),f_{0}(\widetilde{x}))$ )), where x is strictly feasible. 

Since $f_{i}(\tilde{x})\,<\,0$ 0 and $\tilde{\lambda}_{i}\;\geq\;0$ ≥ 0, we ude that = 0. From $(\tilde{\lambda},\tilde{\nu},\mu)\,\ne\,0$ ̸ d $\dot{\lambda}\,=\,0$ $\mu\,=\,0$ , we conclude that ˜ $\tilde{\nu}\neq0$ ̸ = 0. Then ( 5.42 ) implies that for all x ∈D , $\tilde{\nu}^{T}(A x-b)\geq0$ − ≥ 0. But x satisfies $\tilde{\nu}^{T}(A\tilde{x}\mathrm{~-~}b)\,=\,0$ 0, and since $\tilde{{\boldsymbol{x}}}\in\operatorname{int}\mathcal{D}$ ∈ D , there are points in D with $\tilde{\nu}^{T}(A x-b)<0$ − 0 unless A $A^{T}\tilde{\nu}=0$ = 0. This, of course, contradicts our assumption that rank $A=p$ . 

The geometric idea behind the proof is illustrated in figure 5.6 , for a simple problem with one inequality nstraint. The hyperplane separating $\mathcal{A}$ and $\mathcal{B}$ defines a supporting hyperplane to A at $(0,p^{\star})$ . Slater’s constraint qualification is used to establish that the hyperplane must be nonvertical ( i.e. , has a normal vector of the form $(\lambda^{\star},1)$ ). (For a simple example of a convex problem with one inequality constraint for which strong duality fails, see exercise 5.21 .) 

# 5.3.3 Multicriterion interpretation 

There is a natural connection between Lagrange duality for a problem without equality constraints, 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(x)}}\\ {{\mathrm{subject~to}}}&{{f_{i}(x)\leq0,\quad i=1,\ldots,m,}}\end{array}
$$ 

and the scalarization method for the (unconstrained) multicriterion problem 

$$
\mathrm{minimize}\ (\mathrm{w.r.t.}\ \mathbf{R}_{+}^{m+1})\quad F(x)=(f_{1}(x),\ldots,f_{m}(x),f_{0}(x))
$$ 

). In scalarization, we choose a positive vector $\tilde{\lambda}$ (see § 4.7.4 , and minimize the scalar function $\dot{\lambda}^{T}F(x)$ ); any minimizer is guaranteed to be Pareto optimal. Since we can scale λ by a positive constant, without aﬀecting the minimizers, we can, without loss of generality, take $\mathinner{\dot{\lambda}}=(\lambda,1)$ 1). Thus, in scalarization we minimize the function 

$$
\widetilde{\lambda}^{T}F(x)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x),
$$ 

which is exactly the Lagrangian for the problem ( 5.43 ). 

To establish that every Pareto optimal point of a convex multicriterion problem minimizes the function $\widetilde{\lambda}^{T}F(x)$ ) for some nonnegative weight vector $\tilde{\lambda}$ , we considered the set $\mathcal{A}$ , defined in ( 4.62 ), 

$$
\mathcal{A}=\{t\in\mathbf{R}^{m+1}\mid\exists x\in\mathcal{D},\ f_{i}(x)\leq t_{i},\ i=0,.\,.\,,m\},
$$ 

which is exactly the same as the set $\mathcal{A}$ defined in ( 5.37 ), that arises in Lagrange dual- ity. Here too we constructed the required weight vector as a supporting hyperplane to the set, at an arbitrary Pareto optimal point. In multicriterion optimization, we interpret the components of the weight vector as giving the relative weights between the objective functions. When we fix the last component of the weight vector (associated with $f_{0}$ ) to be one, the other weights have the interpretation of the cost relative to $f_{0}$ , i.e. , the cost relative to the objective. 

# 5.4 Saddle-point interpretation 

In this section we give several interpretations of Lagrange duality. The material of this section will not be used in the sequel. 

# 5.4.1 Max-min characterization of weak and strong duality 

It is possible to express the primal and the dual optimization problems in a form that is more symmetric. To simplify the discussion we assume there are no equality constraints; the results are easily extended to cover them. First note that 

$$
\begin{array}{l c l}{{\underset{\lambda\succeq0}{\operatorname*{sup}}\,L(x,\lambda)}}&{{=}}&{{\underset{\lambda\succeq0}{\operatorname*{sup}}\left(f_{0}(x)+\displaystyle\sum_{i=1}^{m}\lambda_{i}f_{i}(x)\right)}}\\ {{}}&{{=}}&{{\left\{\begin{array}{l c l}{{f_{0}(x)}}&{{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}}\\ {{\infty}}&{{\mathrm{otherwise.}}}\end{array}\right.}}\end{array}
$$ 

Indeed, suppose $x$ is not feasible, and $f_{i}(x)>0$ for some $i$ . Then $\begin{array}{r l}{\operatorname*{sup}_{\lambda\succeq0}L(x,\lambda)=}\end{array}$ $\infty$ , as ca $\lambda_{j}~=~0$ , $\textit{j}\neq\textit{i}$ , and $\lambda_{i}~\rightarrow~\infty$ On the other hand, if f $f_{i}(x)\ \leq\ 0$ ≤ 0, i $i\ =\ 1,\ldots,m$ , then the optimal choice of λ is $\lambda~=~0$ and $\begin{array}{r}{\operatorname*{sup}_{\lambda\succeq0}L(x,\lambda)=f_{0}(x)}\end{array}$ . This means that we can express the optimal value of the primal problem as 

$$
p^{\star}=\operatorname*{inf}_{x}\operatorname*{sup}_{\lambda\succeq0}L(x,\lambda).
$$ 

By the definition of the dual function, we also have 

$$
d^{\star}=\operatorname*{sup}_{\lambda\succeq0}\operatorname*{inf}_{x}\,L(x,\lambda).
$$ 

Thus, weak duality can be expressed as the inequality 

$$
\operatorname*{sup}_{\lambda\succeq0}\operatorname*{inf}_{x}\,L(x,\lambda)\leq\operatorname*{inf}_{x}\,\operatorname*{sup}_{\lambda\succeq0}\,L(x,\lambda),
$$ 

and strong duality as the equality 

$$
\operatorname*{sup}_{\lambda\succeq0}\operatorname*{inf}_{x}\,L(x,\lambda)=\operatorname*{inf}_{x}\,\operatorname*{sup}_{\lambda\succeq0}\,L(x,\lambda).
$$ 

Strong duality means that the order of the minimization over $x$ and the maximiza- tion over $\lambda\succeq0$ can be switched without aﬀecting the result. 

In fact, the inequality ( 5.45 ) does not depend on any properties of $L$ : We have 

$$
\operatorname*{sup}_{z\in Z}\operatorname*{inf}_{w\in W}f(w,z)\leq\operatorname*{inf}_{w\in W}\operatorname*{sup}_{z\in Z}f(w,z)
$$ 

for any $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ (and any $W\subseteq\mathbf{R}^{n}$ and $Z\subseteq\mathbf{R}^{m}$ ). This general inequality is called the max-min inequality . When equality holds, i.e. , 

$$
\operatorname*{sup}_{z\in Z}\operatorname*{inf}_{w\in W}f(w,z)=\operatorname*{inf}_{w\in W}\,\operatorname*{sup}_{z\in Z}f(w,z)
$$ 

we say that $f$ (and $W$ and $Z$ ) satisfy the strong max-min property or the saddle- point property. Of course the strong max-min property holds only in special cases, for example, when $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ is the Lagrangian of a problem for which strong duality obtains, W $W=\mathbf{R}^{n}$ , and ${\cal Z}={\bf R}_{+}^{m}$ . 

# 5.4.2 Saddle-point interpretation 

We refer to a pair $\tilde{w}\in W$ , $\tilde{z}\in Z$ as a saddle-point for $f$ (and $W$ and $Z$ ) if 

$$
f(\tilde{w},z)\leq f(\tilde{w},\tilde{z})\leq f(w,\tilde{z})
$$ 

for all $w\in W$ and $z\in Z$ . In other words, w minimizes $f(w,\tilde{z})$ ) (over $w\in W$ ) and z maximizes $f(\tilde{w},z)$ ) (over $z\in Z$ ): 

$$
f(\tilde{w},\tilde{z})=\operatorname*{inf}_{w\in W}f(w,\tilde{z}),\qquad f(\tilde{w},\tilde{z})=\operatorname*{sup}_{z\in Z}f(\tilde{w},z).
$$ 

This implies that the strong max-min property ( 5.47 ) holds, and that the common value is $f(\tilde{w},\tilde{z})$ ). 

Returning to our discussion of Lagrange duality, we see that if $x^{\star}$ and $\lambda^{\star}$ are primal and dual optimal points for a problem in which strong duality obtains, they form a saddle-point for the Lagrangian. The converse is also true: If $(x,\lambda)$ is a saddle-point of the Lagrangian, then $x$ is primal optimal, $\lambda$ is dual optimal, and the optimal duality gap is zero. 

# 5.4.3 Game interpretation 

We can interpret the max-min inequality ( 5.46 ), the max-min equality ( 5.47 ), and the saddle-point property, in terms of a continuous zero-sum game . If the first player chooses $w\in W$ , and the second player selects $z\in{Z}$ , then player 1 pays an amount $f(w,z)$ to player 2. Player 1 therefore wants to minimize $f$ , while player 2 wants to maximize $f$ . (The game is called continuous since the choices are vectors, and not discrete.) 

Suppose that player 1 makes his choice first, and then player 2, after learning the choice of player 1, makes her selection. Player 2 wants to maximize the payoﬀ $f(w,z)$ , and so will choose $z\in Z$ to maximize $f(w,z)$ . The resulting payoﬀwill be $\textstyle\operatorname*{sup}_{z\in Z}f(w,z)$ , which depends on $w$ , the choice of the first player. (We assume here that the supremum is achieved; if not the optimal payoﬀcan be arbitrarily close to $\textstyle\operatorname*{sup}_{z\in Z}f(w,z)$ .) Player 1 knows (or assumes) that player 2 will follow this strategy, and so will choose $w\in W$ to make this worst-case payoﬀto player 2 as small as possible. Thus player 1 chooses 

$$
\operatorname*{argmin}_{w\in W}\operatorname*{sup}_{z\in Z}f(w,z),
$$ 

which results in the payoﬀ 

$$
\operatorname*{inf}_{w\in W}\,\operatorname*{sup}_{z\in Z}f(w,z)
$$ 

from player 1 to player 2. 

Now suppose the order of play is reversed: Player 2 must choose $z\in Z$ first, and then player 1 chooses $w\in W$ (with knowledge of $z$ ). Following a similar argument, if the players follow the optimal strategy, player 2 should choose $z\in Z$ to maximize $\textstyle\operatorname*{inf}_{w\in W}f(w,z)$ , which results in the payoﬀof 

$$
\operatorname*{sup}_{z\in Z}\operatorname*{inf}_{w\in W}f(w,z)
$$ 

from player 1 to player 2. 

The max-min inequality ( 5.46 ) states the (intuitively obvious) fact that it is better for a player to go second, or more precisely, for a player to know his or her opponent’s choice before choosing. In other words, the payoﬀto player 2 will be larger if player 1 must choose first. When the saddle-point property ( 5.47 ) holds, there is no advantage to playing second. 

If $(\tilde{w},\tilde{z})$ ) is a saddle-point for $f$ (and $W$ and $Z$ ), then it is called a solution of the game; w is called the optimal choice or strategy for player 1, and z is called the optimal choice or strategy for player 2. In this case there is no advantage to playing second. 

Now consider the special case where the payoﬀfunction is the Lagrangian, $W=\mathbf{R}^{n}$ and ${\cal Z}={\bf R}_{+}^{m}$ . Here player 1 chooses the primal variable $x$ , while player 2 chooses the dual variable $\lambda\succeq0$ . By the gument above, the optimal choice for player 2, if she must choose first, is any λ which is dual optimal, which results in a payoﬀto player 2 of $d^{\star}$ . Conversely, if player 1 must choose first, his optimal choice is any primal optimal $x^{\star}$ , which results in a payoﬀof $p^{\star}$ . 

The optimal duality gap for the problem is exactly equal to the advantage aﬀorded the player who goes second, i.e. , the player who has the advantage of knowing his or her opponent’s choice before choosing. If strong duality holds, then there is no advantage to the players of knowing their opponent’s choice. 

# 5.4.4 Price or tax interpretation 

Lagrange duality has an interesting economic interpretation. Suppose the variable $x$ denotes how an enterprise operates and $f_{0}(x)$ denotes the cost of operating at $x$ , i.e. , $-f_{0}(x)$ is the profit (say, in dollars) made at the operating condition $x$ . Each constraint $f_{i}(x)\leq0$ represents some limit, such as a limit on resources ( e.g. , warehouse space, labor) or a regulatory limit ( e.g. , environmental). The operating condition that maximizes profit while respecting the limits can be found by solving the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(x)}}\\ {{\mathrm{subject~to}}}&{{f_{i}(x)\leq0,\quad i=1,.\,.\,,m.}}\end{array}
$$ 

The resulting optimal profit is $-p^{\star}$ . 

Now imagine a second scenario in which the limits can be violated, by paying an additional cost which is linear in the amount of violation, measured by $f_{i}$ . Thus the payment made by the enterprise for the $i$ th limit or constraint is $\lambda_{i}f_{i}(x)$ . Payments are also made to the firm for constraints that are not tight; if $f_{i}(x)<0$ , then $\lambda_{i}f_{i}(x)$ represents a payment to the firm. The coefficient $\lambda_{i}$ has the interpretation of the price for violating $f_{i}(x)\leq0$ ; its units are dollars per unit violation (as measured by $f_{i}$ ). For the same price the enterprise can sell any ‘unused’ portion of the $i$ th constraint. We assume $\lambda_{i}\geq0$ , i.e. , the firm must pay for violations (and receives income if a constraint is not tight). 

As an example, suppose the first constraint in the original problem, $f_{1}(x)\,\leq$ 0, represents a limit on warehouse space (say, in square meters). In this new arrangement, we open the possibility that the firm can rent extra warehouse space at a cost of $\lambda_{1}$ dollars per square meter and also rent out unused space, at the same rate. 

The total cost to the firm, for operating condition $x$ , and constraint prices $\lambda_{i}$ , is $\begin{array}{r}{L(x,\lambda)\,=\,f_{0}(x)+\sum_{i=1}^{n\imath}\lambda_{i}f_{i}(x)}\end{array}$ ). The firm will obviously operate so as to minimize its total cost $L(x,\lambda)$ ), which yields a cost $g(\lambda)$ . The dual function therefore represents the optimal cost to the firm, as a function of the constraint price vector $\lambda$ . The optimal dual value, $d^{\star}$ , is the optimal cost to the enterprise under the least favorable set of prices. 

Using this interpretation we can paraphrase weak duality as follows: The opti- mal cost to the firm in the second scenario (in which constraint violations can be bought and sold) is less than or equal to the cost in the original situation (which has constraints that cannot be violated), even with the most unfavorable prices. This is obvious: If $x^{\star}$ is optimal in the first scenario, then the operating cost of $x^{\star}$ in the second scenario will be lower than $f_{0}(x^{\star})$ , since some income can be derived from the constraints that are not tight. The optimal duality gap is then the min- imum possible advantage to the enterprise of being allowed to pay for constraint violations (and receive payments for nontight constraints). 

Now suppose strong duality holds, and the dual optimum is attained. We can interpret a dual optimal $\lambda^{\star}$ as a set of prices for which there is no advantage to the firm in being allowed to pay for constraint violations (or receive payments for nontight constraints). For this reason a dual optimal $\lambda^{\star}$ is sometimes called a set of shadow prices for the original problem. 

# 5.5 Optimality conditions 

We remind the reader that we do not assume the problem ( 5.1 ) is convex, unless explicitly stated. 

# 5.5.1 Certificate of suboptimality and stopping criteria 

If we can find a dual feasible $(\lambda,\nu)$ , we establish a lower bound on the optimal value of the primal problem: $p^{\star}\,\geq\,g(\lambda,\nu)$ . Thus a dual feasible point $(\lambda,\nu)$ provides a proof or certificate that $p^{\star}\geq g(\lambda,\nu)$ . Strong duality means there exist arbitrarily good certificates. 

Dual feasible points allow us to bound how suboptimal a given feasible point is, without knowing the exact value of $p^{\star}$ . Indeed, if $x$ is primal feasible and $(\lambda,\nu)$ is dual feasible, then 

$$
f_{0}(x)-p^{\star}\leq f_{0}(x)-g(\lambda,\nu).
$$ 

In particular, this establishes that $x$ is $\epsilon$ -suboptimal, with $\epsilon=f_{0}(\boldsymbol{x})-g(\lambda,\nu)$ . (It also establishes that $(\lambda,\nu)$ is $\epsilon$ -suboptimal for the dual problem.) 

We refer to the gap between primal and dual objectives, 

$$
f_{0}\mathopen{}\mathclose\bgroup\left(x\aftergroup\egroup\right)-g\mathopen{}\mathclose\bgroup\left(\lambda,\nu\aftergroup\egroup\right),
$$ 

as the duality gap associated with the primal feasible point $x$ and dual feasible point $(\lambda,\nu)$ . A primal dual feasible pair $x$ , $(\lambda,\nu)$ localizes the optimal value of the primal (and dual) problems to an interval: 

$$
p^{\star}\in[g(\lambda,\nu),f_{0}(x)],\qquad d^{\star}\in[g(\lambda,\nu),f_{0}(x)],
$$ 

the width of which is the duality gap. 

If the duality gap of the primal dual feasible pair $x$ , $(\lambda,\nu)$ is zero, i.e. , $f_{0}(x)=$ $g(\lambda,\nu)$ , then $x$ is primal optimal and $(\lambda,\nu)$ is dual optimal. We can think of $(\lambda,\nu)$ 

as a certificate that proves $x$ is optimal (and, similarly, we can think of $x$ as a certificate that proves $(\lambda,\nu)$ is dual optimal). 

These observations can be used in optimization algorithms to provide nonheuris- tic stopping criteria. Suppose an algorithm produces a sequence of primal feasible $x^{(k)}$ and dual feasible $(\lambda^{(k)},\nu^{(k)})$ , for $k=1,2,.\cdot\cdot.,$ , and $\epsilon_{\mathrm{abs}}>0$ is a given required absolute accuracy. Then the stopping criterion ( i.e. , the condition for terminating the algorithm) 

$$
f_{0}(x^{(k)})-g(\lambda^{(k)},\nu^{(k)})\leq\epsilon_{\mathrm{abs}}
$$ 

guarantees that when the algorithm terminates, $x^{(k)}$ is $\epsilon_{\mathrm{abs}}$ -suboptimal. Indeed, $(\lambda^{(k)},\nu^{(k)})$ is a certificate that proves it. (Of course strong duality must hold if this method is to work for arbitrarily small tolerances $\epsilon_{\mathrm{abs}}$ .) 

A similar condition can be used to guarantee a given relative accuracy $\epsilon_{\mathrm{rel}}>0$ . If 

$$
g\bigl(\lambda^{(k)},\nu^{(k)}\bigr)>0,\qquad\frac{f_{0}\bigl(x^{(k)}\bigr)-g\bigl(\lambda^{(k)},\nu^{(k)}\bigr)}{g\bigl(\lambda^{(k)},\nu^{(k)}\bigr)}\leq\epsilon_{\mathrm{rel}}
$$ 

holds, or 

$$
f_{0}\big(x^{(k)}\big)<0,\qquad\frac{f_{0}\big(x^{(k)}\big)-g\big(\lambda^{(k)},\nu^{(k)}\big)}{-f_{0}\big(x^{(k)}\big)}\leq\epsilon_{\mathrm{rel}}
$$ 

holds, then $p^{\star}\neq0$ and the relative error 

$$
\frac{f_{0}(x^{(k)})-p^{\star}}{|p^{\star}|}
$$ 

is guaranteed to be less than or equal to $\epsilon_{\mathrm{rel}}$ . 

# 5.5.2 Complementary slackness 

Suppose that the primal and dual optimal values are attained and equal (so, in particular, strong duality holds). Let $x^{\star}$ be a primal optimal and $(\lambda^{\star},\nu^{\star})$ be a dual optimal point. This means that 

$$
\begin{array}{r c l}{f_{0}(x^{\star})}&{=}&{g(\lambda^{\star},\nu^{\star})}\\ &{=}&{\displaystyle\operatorname*{inf}_{x}\left(f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}^{\star}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}^{\star}h_{i}(x)\right)}\\ &{\leq}&{f_{0}(x^{\star})+\displaystyle\sum_{i=1}^{m}\lambda_{i}^{\star}f_{i}(x^{\star})+\sum_{i=1}^{p}\nu_{i}^{\star}h_{i}(x^{\star})}\\ &{\leq}&{f_{0}(x^{\star}).}\end{array}
$$ 

The first line states that the optimal duality gap is zero, and the second line is the definition of the dual function. The third line follows since the infimum of the Lagrangian over $x$ is less than or equal to its value at $x=x^{\star}$ . The last inequality follows from $\lambda_{i}^{\star}\,\geq\,0$ ≥ 0, $f_{i}(x^{\star})\,\leq\,0$ , $i\,=\,1,.\,.\,.\,,m$ , and $h_{i}(x^{\star})\,=\,0$ , $i\,=\,1,\dots,p$ . We conclude that the two inequalities in this chain hold with equality. 

We can draw several interesting conclusions from this. For example, since the inequality in the third line is an equality, we conclude that $x^{\star}$ minimizes $L(x,\lambda^{\star},\nu^{\star})$ over $x$ . (The Lagrangian $L(x,\lambda^{\star},\nu^{\star})$ can have other minimizers; $x^{\star}$ is simply $a$ minimizer.) 

Another important conclusion is that 

$$
\sum_{i=1}^{m}\lambda_{i}^{\star}f_{i}(x^{\star})=0.
$$ 

Since each term in this sum is nonpositive, we conclude that 

$$
\lambda_{i}^{\star}f_{i}(x^{\star})=0,\quad i=1,.\,.\,.\,,m.
$$ 

This condition is known as complementary slackness ; it holds for any primal opti- mal $x^{\star}$ and any dual optimal $(\lambda^{\star},\nu^{\star})$ (when strong duality holds). We can express the complementary slackness condition as 

$$
\lambda_{i}^{\star}>0\implies f_{i}(x^{\star})=0,
$$ 

or, equivalently, 

$$
f_{i}(x^{\star})<0\implies\lambda_{i}^{\star}=0.
$$ 

Roughly speaking, this means the $\imath$ th optimal Lagrange multiplier is zero unless the $i$ th constraint is active at the optimum. 

# 5.5.3 KKT optimality conditions 

We now assume that the functions $f_{0},\ldots,f_{m},h_{1},\ldots,h_{p}$ are diﬀerentiable (and therefore have open domains), but we make no assumptions yet about convexity. 

# KKT conditions for nonconvex problems 

As above, let $x^{\star}$ and $(\lambda^{\star},\nu^{\star})$ be any primal and dual optimal points with zero duality gap. Since $x^{\star}$ minimizes $L(x,\lambda^{\star},\nu^{\star})$ over $x$ , it follows that its gradient must vanish at $x^{\star}$ , i.e. , 

$$
\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}\lambda_{i}^{\star}\nabla f_{i}(x^{\star})+\sum_{i=1}^{p}\nu_{i}^{\star}\nabla h_{i}(x^{\star})=0.
$$ 

Thus we have 

$$
\begin{array}{r c l}{{f_{i}(x^{\star})}}&{{\leq}}&{{0,\quad i=1,\ldots,m}}\\ {{h_{i}(x^{\star})}}&{{=}}&{{0,\quad i=1,\ldots,p}}\\ {{\lambda_{i}^{\star}}}&{{\geq}}&{{0,\quad i=1,\ldots,m}}\\ {{\lambda_{i}^{\star}f_{i}(x^{\star})}}&{{=}}&{{0,\quad i=1,\ldots,m}}\\ {{\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}\lambda_{i}^{\star}\nabla f_{i}(x^{\star})+\sum_{i=1}^{p}\nu_{i}^{\star}\nabla h_{i}(x^{\star})}}&{{=}}&{{0,}}\end{array}
$$ 

which are called the Karush-Kuhn-Tucker (KKT) conditions. 

To summarize, for any optimization problem with diﬀerentiable objective and constraint functions for which strong duality obtains, any pair of primal and dual optimal points must satisfy the KKT conditions ( 5.49 ). 

# KKT conditions for convex problems 

When the primal problem is convex, the KKT conditions are also sufficient for the points to be primal and dual optimal. In other words, if $f_{i}$ are convex and $h_{i}$ are , $\tilde{\lambda}$ affine, and x , ν are any points that satisfy the KKT conditions 

$$
\begin{array}{r c l}{{f_{i}(\tilde{x})}}&{{\leq}}&{{0,\quad i=1,\ldots,m}}\\ {{h_{i}(\tilde{x})}}&{{=}}&{{0,\quad i=1,\ldots,p}}\\ {{\tilde{\lambda}_{i}}}&{{\geq}}&{{0,\quad i=1,\ldots,m}}\\ {{\tilde{\lambda}_{i}f_{i}(\tilde{x})}}&{{=}}&{{0,\quad i=1,\ldots,m}}\\ {{\nabla f_{0}(\tilde{x})+\sum_{i=1}^{m}\tilde{\lambda}_{i}\nabla f_{i}(\tilde{x})+\sum_{i=1}^{p}\tilde{\nu}_{i}\nabla h_{i}(\tilde{x})}}&{{=}}&{{0,}}\end{array}
$$ 

then x and $(\ddot{\lambda},\tilde{\nu})$ ) are primal and dual optimal, with zero duality gap. 

To see this, note that the first two conditions state that x is primal feasible. Since $\tilde{\lambda}_{i}\;\geq\;0$ ≥ 0, $L(x,\tilde{\lambda},\tilde{\nu})$ ) is convex in $x$ ; the last KKT condition states that its gradient with respect to $x$ vanishes at $x={\dot{x}}$ , so it follows that x minimizes $L(x,\tilde{\lambda},\tilde{\nu})$ over $x$ . From this we conclude that 

$$
\begin{array}{r c l}{g(\tilde{\lambda},\tilde{\nu})}&{=}&{{\cal L}(\tilde{x},\tilde{\lambda},\tilde{\nu})}\\ &{=}&{f_{0}(\tilde{x})+\displaystyle\sum_{i=1}^{m}\tilde{\lambda}_{i}f_{i}(\tilde{x})+\sum_{i=1}^{p}\tilde{\nu}_{i}h_{i}(\tilde{x})}\\ &{=}&{f_{0}(\tilde{x}),}\end{array}
$$ 

where in the last line we use $h_{i}({\tilde{x}})\;=\;0$ ) = 0 and $\tilde{\lambda}_{i}f_{i}(\tilde{x})\;=\;0$ This shows that x and $(\tilde{\lambda},\tilde{\nu})$ ) have zero duality gap, and therefore are primal and dual optimal. In summary, for any convex optimization problem with diﬀerentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap. 

If a convex optimization problem with diﬀerentiable objective and constraint functions satisfies Slater’s condition, then the KKT conditions provide necessary and sufficient conditions for optimality: Slater’s condition implies that the optimal duality gap is zero and the dual optimum is attained, so $x$ is optimal if and only if there are $(\lambda,\nu)$ that, together with $x$ , satisfy the KKT conditions. 

The KKT conditions play an important role in optimization. In a few special cases it is possible to solve the KKT conditions (and therefore, the optimization problem) analytically. More generally, many algorithms for convex optimization are conceived as, or can be interpreted as, methods for solving the KKT conditions. 

Example 5.1 Equality constrained convex quadratic minimization. We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\,(1/2)x^{T}P x+q^{T}x+r}\\ &{{\mathrm{subject~to}}\quad A x=b,}\end{array}}
$$ 

where $P\in\mathbf{S}_{+}^{n}$ . The KKT conditions for this problem are 

$$
A x^{\star}=b,\qquad P x^{\star}+q+A^{T}\nu^{\star}=0,
$$ 

which we can write as 

$$
\left[\begin{array}{c c}{\boldsymbol{P}}&{\boldsymbol{A}^{T}}\\ {\boldsymbol{A}}&{\boldsymbol{0}}\end{array}\right]\left[\begin{array}{c}{\boldsymbol{x}^{\star}}\\ {\boldsymbol{\nu}^{\star}}\end{array}\right]=\left[\begin{array}{c}{-\boldsymbol{q}}\\ {\boldsymbol{b}}\end{array}\right].
$$ 

Solving this set of $m+n$ equations in the $m+n$ variables $x^{\star}$ , $\nu^{\star}$ gives the optimal primal and dual variables for ( 5.50 ). 

Example 5.2 Water-filling. We consider the convex optimization problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{-\sum_{i=1}^{n}\log\bigl(\alpha_{i}+x_{i}\bigr)}\\ {\mathrm{subject~to}}&{x\succeq0,\quad\mathbf{1}^{T}x=1,}\end{array}
$$ 

where $\alpha_{i}\,>\,0$ . This problem arises in information theory, in allocating power to a set of $n$ communication channels. The variable $x_{i}$ represents the transmitter power allocated to the $i$ th channel, and $\log(\alpha_{i}+x_{i})$ gives the capacity or communication rate of the channel, so the problem is to allocate a total power of one to the channels, in order to maximize the total communication rate. 

Introducing Lagrange multipliers $\lambda^{\star}\,\in\,\mathbf{R}^{n}$ for the y constraints $x^{\star}\,\succeq\,0$ , and a multiplier $\nu^{\star}\in\mathbf{R}$ for the equality constraint 1 $\mathbf{1}^{T}x\,=\,1$ = 1, we obtain the KKT conditions 

$$
\begin{array}{r}{x^{\star}\succeq0,\qquad\mathbf{1}^{T}x^{\star}=1,\qquad\lambda^{\star}\succeq0,\qquad\lambda_{i}^{\star}x_{i}^{\star}=0,\quad i=1,\ldots,n,}\end{array}
$$ 

$$
-1/(\alpha_{i}+x_{i}^{\star})-\lambda_{i}^{\star}+\nu^{\star}=0,\quad i=1,\ldots,n.
$$ 

We can directly solve these equations to find $x^{\star}$ , $\lambda^{\star}$ , and $\nu^{\star}$ . We start by noting that $\lambda^{\star}$ acts as a slack variable in the last equation, so it can be eliminated, leaving 

$$
\begin{array}{r}{x^{\star}\succeq0,\qquad\mathbf{1}^{T}x^{\star}=1,\qquad x_{i}^{\star}\left(\nu^{\star}-1/(\alpha_{i}+x_{i}^{\star})\right)=0,\quad i=1,\ldots,n,}\end{array}
$$ 

$$
\nu^{\star}\geq1/(\alpha_{i}+x_{i}^{\star}),\quad i=1,\cdot\,.\,.\,,n.
$$ 

If $\nu^{\star}<1/\alpha_{i}$ , this last condition can only hold if $x_{i}^{\star}>0$ 0, which by the third condition implies that $\nu^{\star}\,=\,1/\bigl(\alpha_{i}\,+\,x_{i}^{\star}\bigr)$ ). Solving for $\boldsymbol{x}_{i}^{\star}$ , we conclude that $x_{i}^{\star}\,=\,1/\nu^{\star}\,-\,\alpha_{i}$ − if $\nu^{\star}\,<\,1/\alpha_{i}$ . If $\nu^{\star}\,\geq\,{1}/{\alpha_{i}}$ , then $x_{i}^{\star}\ >\ 0$ 0 is impossible, because it would imply $\nu^{\star}\,\geq\,1/\alpha_{i}\,>\,1/\bigl(\alpha_{i}\,+\,x_{i}^{\star}\bigr)$ ), which violates the complementary slackness condition. Therefore, $x_{i}^{\star}=0$ = 0 if $\nu^{\star}\geq1/\alpha_{i}$ . Thus we have 

$$
\begin{array}{r}{x_{i}^{\star}=\left\{\begin{array}{l l}{1/\nu^{\star}-\alpha_{i}}&{\nu^{\star}<1/\alpha_{i}}\\ {0}&{\nu^{\star}\geq1/\alpha_{i},}\end{array}\right.}\end{array}
$$ 

or, put more simply, $x_{i}^{\star}\,=\,\operatorname*{max}\left\{0,1/\nu^{\star}\,-\,\alpha_{i}\right\}$ { − } . Substituting this expression for $x_{i}^{\star}$ into the condition 1 $\mathbf{1}^{T}x^{\star}=1$ = 1 we obtain 

$$
\sum_{i=1}^{n}\operatorname*{max}\{0,1/\nu^{\star}-\alpha_{i}\}=1.
$$ 

The lefthand side is a piecewise-linear increasing function of $1/\nu^{\star}$ , with breakpoints at $\alpha_{i}$ , so the equation has a unique solution which is readily determined. 

This solution method is called water-filling for the following reason. We think of $\alpha_{i}$ as the ground level above patch $i$ , and then ﬂood the region with water to a depth $1/\nu$ , as illustrated in figure 5.7 . The total amount of water used is then $\begin{array}{r}{\sum_{i=1}^{n}\operatorname*{max}\{0,1/\nu^{\star}-\alpha_{i}\}}\end{array}$ } . We then increase the ﬂood level until we have used a total amount of water equal to one. The depth of water above patch $i$ is then the optimal value $\boldsymbol{x}_{i}^{\star}$ . 

![](images/81c0de6b56933aec7c0ec2b902ee7b41f6f9601ad0f90eab023abe748c8638f8.jpg) 
Figure 5.7 Illustration of water-filling algorithm. The height of each patch is given by $\alpha_{i}$ . The region is ﬂooded to a level $1/\nu^{\star}$ which uses a total quantity of water equal to one. The height of the water (shown shaded) above each patch is the optimal value of $x_{i}^{\star}$ . 

![](images/e3d7c965b1e6bde365133a25ca69d667fa5eb6c66221679336c434777352b228.jpg) 
Figure 5.8 Two blocks connected by springs to each other, and the left and right walls. The blocks have width $w>0$ , and cannot penetrate each other or the walls. 

# 5.5.4 Mechanics interpretation of KKT conditions 

The KKT conditions can be given a nice interpretation in mechanics (which indeed, was one of Lagrange’s primary motivations). We illustrate the idea with a simple example. The system shown in figure 5.8 consists of two blocks attached to each other, and to walls at the left and right, by three springs. The position of the blocks are given by $x\in\mathbf{R}^{2}$ , where $x_{1}$ is the displacement of the (middle of the) left block, and $x_{2}$ is the displacement of the right block. The left wall is at position 0, and the right wall is at position $l$ . 

The potential energy in the springs, as a function of the block positions, is given by 

$$
f_{0}(x_{1},x_{2})=\frac12k_{1}x_{1}^{2}+\frac12k_{2}(x_{2}-x_{1})^{2}+\frac12k_{3}(l-x_{2})^{2},
$$ 

where $k_{i}~>~0$ are the stiﬀness constants of the three springs. The equilibrium position $x^{\star}$ is the position that minimizes the potential energy subject to the in- equalities 

$$
w/2-x_{1}\leq0,\qquad w+x_{1}-x_{2}\leq0,\qquad w/2-l+x_{2}\leq0.
$$ 

These constraints are called kinematic constraints , and express the fact that the blocks have width $w\ >\ 0$ , and cannot penetrate each other or the walls. The equilibrium position is therefore given by the solution of the optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{(1/2)\left(k_{1}x_{1}^{2}+k_{2}(x_{2}-x_{1})^{2}+k_{3}(l-x_{2})^{2}\right)}\\ {{\mathrm{subject~to}}\quad}&{w/2-x_{1}\leq0}\\ &{w+x_{1}-x_{2}\leq0}\\ &{w/2-l+x_{2}\leq0,}\end{array}}
$$ 

which is a QP. 

With $\lambda_{1}$ , $\lambda_{2}$ , $\lambda_{3}$ as Lagrange multipliers, the KKT conditions for this problem consist of the kinematic constraints ( 5.51 ), the nonnegativity constraints $\lambda_{i}\geq0$ , the complementary slackness conditions 

$$
\lambda_{1}(w/2-x_{1})=0,\qquad\lambda_{2}(w-x_{2}+x_{1})=0,\qquad\lambda_{3}(w/2-l+x_{2})=0,
$$ 

and the zero gradient condition 

$$
\left[\begin{array}{c}{k_{1}x_{1}-k_{2}(x_{2}-x_{1})}\\ {k_{2}(x_{2}-x_{1})-k_{3}(l-x_{2})}\end{array}\right]+\lambda_{1}\left[\begin{array}{c}{-1}\\ {0}\end{array}\right]+\lambda_{2}\left[\begin{array}{c}{1}\\ {-1}\end{array}\right]+\lambda_{3}\left[\begin{array}{c}{0}\\ {1}\end{array}\right]=0.
$$ 

The equation ( 5.54 ) can be interpreted as the force balance equations for the two blocks, provided we interpret the Lagrange multipliers as contact forces that act between the walls and blocks, as illustrated in figure 5.9 . The first equation states that the sum of the forces on the first block is zero: The term $-k_{1}x_{1}$ is the force exerted on the left blo by the left spring, the term $k_{2}(x_{2}-x_{1})$ is orce exerted by the middle spring, λ $\lambda_{1}$ is the force exerted by the left wall, and − $-\lambda_{2}$ is the force exerted by the right block. The contact forces must point away from the contact surface (as expressed by the constraints $\lambda_{1}\,\geq\,0$ and $-\lambda_{2}\,\leq\,0$ ), and are nonzero only when there is contact (as expressed by the first two complementary slackness conditions ( 5.53 )). In a similar way, the second equation in ( 5.54 ) is the force balance for the second block, and the last condition in ( 5.53 ) states that $\lambda_{3}$ is zero unless the right block touches the wall. 

In this example, the potential energy and kinematic constraint functions are convex, and (the refined form of) Slater’s constraint qualification holds provided $2w\leq\ell$ , i.e. , there is enough room between the walls to fit the two blocks, so we can conclude that the energy formulation of the equilibrium given by ( 5.52 ), gives the same result as the force balance formulation, given by the KKT conditions. 

# 5.5.5 Solving the primal problem via the dual 

We mentioned at the beginning of § 5.5.3 that if strong duality holds and a dual optimal solution $(\lambda^{\star},\nu^{\star})$ exists, then any primal optimal point is also a minimizer of $L(x,\lambda^{\star},\nu^{\star})$ . This fact sometimes allows us to compute a primal optimal solution from a dual optimal solution. 

More precisely, suppose we have strong duality and an optimal $(\lambda^{\star},\nu^{\star})$ is known. Suppose that the minimizer of $L(x,\lambda^{\star},\nu^{\star})$ , i.e. , the solution of 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}^{\star}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}^{\star}h_{i}(x),}\end{array}
$$ 

is unique. (For a convex problem this occurs, for example, if $L(x,\lambda^{\star},\nu^{\star})$ is a strictly convex function of $x$ .) Then if the solution of ( 5.55 ) is primal feasible, it must be primal optimal; if it is not primal feasible, then no primal optimal point can exist, i.e. , we can conclude that the primal optimum is not attained. This observation is interesting when the dual problem is easier to solve than the primal problem, for example, because it can be solved analytically, or has some special structure that can be exploited. 

Example 5.3 Entropy maximization. We consider the entropy maximization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)=\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{\mathbf{1}^{T}x=1}\end{array}}
$$ 

with domain $\mathbf{R}_{++}^{n}$ , and its dual problem 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}\lambda-\nu-e^{-\nu-1}\sum_{i=1}^{n}e^{-a_{i}^{T}\lambda}}\\ &{\mathrm{subject~to}\quad\lambda\succeq0}\end{array}
$$ 

where are the columns of $A$ (see pages 222 and 228 ). We assume that the weak $a_{i}$ form of Slater’s condition holds, i.e. , there exists an $x\succ0$ with $A x\preceq b$ and ${\bf1}^{T}x=1$ , so strong duality holds and an optimal solution $(\lambda^{\star},\nu^{\star})$ exists. 

Suppose we have solved the dual problem. The Lagrangian at $(\lambda^{\star},\nu^{\star})$ is 

$$
L(x,\lambda^{\star},\nu^{\star})=\sum_{i=1}^{n}x_{i}\log x_{i}+\lambda^{\star T}(A x-b)+\nu^{\star}(\mathbf{1}^{T}x-1)
$$ 

which is strictly convex on $_D$ and bounded below, so it has a unique solution $x^{\star}$ , given by 

$$
\boldsymbol{x}_{i}^{\star}=1/\exp(\boldsymbol{a}_{i}^{T}\boldsymbol{\lambda}^{\star}+\boldsymbol{\nu}^{\star}+1),\quad i=1,.\,.\,.\,,n.
$$ 

If $x^{\star}$ is primal feasible, it must be the optimal solution of the primal problem ( 5.13 ). If $x^{\star}$ is not primal feasible, then we can conclude that the primal optimum is not attained. 

Example 5.4 Minimizing a separable function subject to an equality constraint. We consider the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}({\boldsymbol{x}})=\sum_{i=1}^{n}f_{i}(x_{i})}\\ {{\mathrm{subject~to}}}&{\ a^{T}{\boldsymbol{x}}=b,}\end{array}}
$$ 

where $a\in\mathbf{R}^{n}$ , $b\in\mathbf{R}$ , and $f_{i}:\mathbf{R}\rightarrow\mathbf{R}$ are diﬀerentiable and strictly convex. The objective function is called separable since it is a sum of functions of the individual variables $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ . We assume that the domain of $f_{0}$ intersects the constraint set, i.e. , there exists a poin $x_{0}\in\mathbf{dom}\,f_{0}$ with $\boldsymbol{a}^{T}\boldsymbol{x}_{0}=\boldsymbol{b}$ . This implies the problem has a unique optimal point x $x^{\star}$ . 

The Lagrangian is 

$$
L(\boldsymbol{x},\nu)=\sum_{i=1}^{n}f_{i}(\boldsymbol{x}_{i})+\nu(\boldsymbol{a}^{T}\boldsymbol{x}-\boldsymbol{b})=-b\nu+\sum_{i=1}^{n}(f_{i}(\boldsymbol{x}_{i})+\nu a_{i}x_{i}),
$$ 

which is also separable, so the dual function is 

$$
\begin{array}{r c l}{{g(\nu)}}&{{=}}&{{\displaystyle-b\nu+\operatorname*{inf}_{x}\left(\sum_{i=1}^{n}(f_{i}(x_{i})+\nu a_{i}x_{i})\right)}}\\ {{}}&{{=}}&{{\displaystyle-b\nu+\sum_{i=1}^{n}\operatorname*{inf}_{x_{i}}(f_{i}(x_{i})+\nu a_{i}x_{i})}}\\ {{}}&{{=}}&{{\displaystyle-b\nu-\sum_{i=1}^{n}f_{i}^{*}(-\nu a_{i}).}}\end{array}
$$ 

The dual problem is thus 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}-b\nu-\sum_{i=1}^{n}f_{i}^{*}(-\nu a_{i}),}\end{array}
$$ 

with (scalar) variable $\nu\in\mathbf{R}$ . 

Now suppose we have found an optimal dual variable $\nu^{\star}$ . (There are several simple methods for solving a convex problem with one scalar variable, such as the bisection method.) Since each $f_{i}$ is strictly convex, the function $\boldsymbol{L}(\boldsymbol{x},\nu^{\star})$ is strictly convex in $x$ , and so has a unique minimizer x . But we also know that $x^{\star}$ minimizes $L(x,\nu^{\star})$ , so we must have $\tilde{x}=x^{\star}$ . We can recover $x^{\star}$ from $\nabla_{\boldsymbol{x}}L(\boldsymbol{x},\nu^{\star})=0$ , i.e. , by solving the equations $f_{i}^{\prime}(x_{i}^{\star})=-\nu^{\star}a_{i}$ − . 

# 5.6 Perturbation and sensitivity analysis 

When strong duality obtains, the optimal dual variables give very useful informa- tion about the sensitivity of the optimal value with respect to perturbations of the constraints. 

# 5.6.1 The perturbed problem 

We consider the following perturbed version of the original optimization prob- lem ( 5.1 ): 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad f_{i}(x)\leq u_{i},\quad i=1,\ldots,m}\\ &{h_{i}(x)=v_{i},\quad i=1,\ldots,p,}\end{array}}
$$ 

va e $x\in\mathbf{R}^{n}$ . This problem coincides with the original probl m ( 5.1 ) when = 0, v = 0. When $u_{i}$ is positive it means that we have relaxed the i th inequality constraint; when $u_{i}$ is negative, it means that we have tightened the constraint. Thus the perturbed problem ( 5.56 ) results from the original problem ( 5.1 ) by tight- ening or relaxing each inequality constraint by $u_{i}$ , and changing the righthand side of the equality constraints by $v_{i}$ . 

We define $p^{\star}(u,v)$ as the optimal value of the perturbed problem ( 5.56 ): 

$$
\begin{array}{r l}&{p^{\star}(u,v)=\operatorname*{inf}\{f_{0}(x)~|~\exists x\in{\mathcal{D}},~f_{i}(x)\leq u_{i},~i=1,.\,.\,,m,}\\ &{\qquad h_{i}(x)=v_{i},~i=1,.\,.\,.\,,p\}.}\end{array}
$$ 

We can have $p^{\star}(u,v)=\infty$ , which corresponds to perturbations of the constraints that result in infeasibility. Note that $p^{\star}(0,0)\,=\,p^{\star}$ , the optimal value of the un- perturbed problem ( 5.1 ). (We hope this slight abuse of notation will cause no confusion.) Roughly speaking, the function $p^{\star}:\mathbf{R}^{m}\times\mathbf{R}^{p}\rightarrow\mathbf{R}$ gives the optimal value of the problem as a function of perturbations to the righthand sides of the constraints. 

When the original problem is convex, the function $p^{\star}$ is a convex function of $u$ and $v$ ; indeed, its epigraph is precisely the closure of the set $\mathcal{A}$ defined in ( 5.37 ) (see exercise 5.32 ). 

# 5.6.2 A global inequality 

Now we assume that strong duality holds, and that the dual optimum is attained. (This is the case if the original problem is convex, and Slater’s condition is satisfied). Let $(\lambda^{\star},\nu^{\star})$ be optimal for the dual ( 5.16 ) of the unperturbed problem. Then for all $u$ and $v$ we have 

$$
\begin{array}{r}{p^{\star}(u,v)\geq p^{\star}(0,0)-\lambda^{\star T}u-\nu^{\star T}v.}\end{array}
$$ 

To establish this inequality, suppose that $x$ is any feasible point for the per- turbed problem, i.e. , $f_{i}(x)\leq u_{i}$ for $i=1,\cdot\cdot\cdot,m$ , and $h_{i}(x)=v_{i}$ for $i=1,\dots,p$ . Then we have, by strong duality, 

$$
\begin{array}{r c l}{\displaystyle p^{\star}(0,0)=g(\lambda^{\star},\nu^{\star})}&{\leq}&{\displaystyle f_{0}(\boldsymbol{x})+\sum_{i=1}^{m}\lambda_{i}^{\star}f_{i}(\boldsymbol{x})+\sum_{i=1}^{p}\nu_{i}^{\star}h_{i}(\boldsymbol{x})}\\ &{\leq}&{\displaystyle f_{0}(\boldsymbol{x})+\lambda^{\star^{T}}\boldsymbol{u}+\nu^{\star^{T}}\boldsymbol{v}.}\end{array}
$$ 

(The first inequality follows from the definition of $g(\lambda^{\star},\nu^{\star})$ ; the second follows since $\lambda^{\star}\succeq0$ .) We conclude that for any $x$ feasible for the perturbed problem, we have 

$$
\begin{array}{r}{f_{0}(x)\geq p^{\star}(0,0)-\lambda^{\star T}u-\nu^{\star T}v,}\end{array}
$$ 

from which ( 5.57 ) follows. 

# Sensitivity interpretations 

When strong duality holds, various sensitivity interpretations of the optimal La- grange variables follow directly from the inequality ( 5.57 ). Some of the conclusions are: 

![](images/5591fba3df703a44379db813b6b9fe989c3897eb21f493cb70676476861144d3.jpg) 
Figure 5.10 Optimal value $p^{\star}(u)$ of a convex problem with one constraint $f_{1}(x)\leq u$ , as nction of $u$ . For $u=0$ , we have the al unperturbed problem; for u < 0 the constraint is tightened, and for u > 0 the constraint is loosened. The affine function $p^{\star}(0)-\lambda^{\star}u$ is a lower bound on $p^{\star}$ . 

• If $\lambda_{i}^{\star}$ is large and we tighten the $i$ th constraint ( i.e. , choose $u_{i}<0$ ), then the optimal value $p^{\star}(u,v)$ is guaranteed to increase greatly. • If $\nu_{i}^{\star}$ is large and positive and we take $v_{i}<0$ , or if $\nu_{i}^{\star}$ is large and negative and we take $v_{i}>0$ , then the optimal value $p^{\star}(u,v)$ is guaranteed to increase greatly. • If $\lambda_{i}^{\star}$ is small, and we loosen the $i$ th constraint ( $u_{i}\,>\,0$ ), then the optimal value $p^{\star}(u,v)$ will not decrease too much. • If $\nu_{i}^{\star}$ is small and positive, and $v_{i}\,>\,0$ , or if $\nu_{i}^{\star}$ is small and negative and $v_{i}<0$ , then the optimal value $p^{\star}(u,v)$ will not decrease too much. 

The inequality ( 5.57 ), and the conclusions listed above, give a lower bound on the perturbed optimal value, but no upper bound. For this reason the results are not symmetric with respect to loosening or tightening a constraint. For example, suppose that $\lambda_{i}^{\star}$ is large, and we loosen the $i$ th constraint a bit ( i.e. , take $u_{i}$ small and positive). In this case the inequality ( 5.57 ) is not useful; it does not, for example, imply that the optimal value will decrease considerably. 

The inequality ( 5.57 ) is illustrated in figure 5.10 for a convex problem with one inequality constraint. The inequality states that the affine function $p^{\star}(0)-\lambda^{\star}u$ is a lower bound on the convex function $p^{\star}$ . 

# 5.6.3 Local sensitivity analysis 

Suppose now that $p^{\star}(u,v)$ is diﬀerentiable at $u=0$ , $v=0$ . Then, provided strong duality holds, the optimal dual variables $\lambda^{\star}$ , $\nu^{\star}$ are related to the gradient of $p^{\star}$ at $u=0$ , $v=0$ : 

$$
\lambda_{i}^{\star}=-\frac{\partial p^{\star}(0,0)}{\partial u_{i}},\qquad\nu_{i}^{\star}=-\frac{\partial p^{\star}(0,0)}{\partial v_{i}}.
$$ 

This property can be seen in the example shown in figure 5.10 , where $-\lambda^{\star}$ is the slope of $p^{\star}$ near $u=0$ . 

Thus, when $p^{\star}(u,v)$ is diﬀerentiable at $u=0$ , $v=0$ , and strong duality holds, the optimal Lagrange multipliers are exactly the local sensitivities of the optimal value with respect to constraint perturbations. In contrast to the non di e rent i able case, this interpretation is symmetric: Tightening the $i$ th inequality constraint a small amount ( i.e. , taking $u_{i}$ small and negative) yields an increase in $p^{\star}$ of approximately $-\lambda_{i}^{\star}u_{i}$ ; loosening the $i$ th constraint a small amount ( i.e. , taking $u_{i}$ small and positive) yields a decrease in $p^{\star}$ of approximately $\lambda_{i}^{\star}u_{i}$ . 

To show ( 5.58 ), suppose $p^{\star}(u,v)$ is diﬀerentiable and strong duality holds. For the perturbation $\boldsymbol{u}=t\boldsymbol{e}_{i},\,\boldsymbol{v}=0$ , where $e_{i}$ is the $i$ th unit vector, we have 

$$
\operatorname*{lim}_{t\to0}\frac{p^{\star}(t e_{i},0)-p^{\star}}{t}=\frac{\partial p^{\star}(0,0)}{\partial u_{i}}.
$$ 

The inequality ( 5.57 ) states that for $t>0$ , 

$$
\frac{p^{\star}(t e_{i},0)-p^{\star}}{t}\geq-\lambda_{i}^{\star},
$$ 

while for $t<0$ we have the opposite inequality. Taking the limit $t\rightarrow0$ , with $t>0$ , yields 

$$
\frac{\partial p^{\star}(0,0)}{\partial u_{i}}\geq-\lambda_{i}^{\star},
$$ 

while taking the limit with $t<0$ yields the opposite inequality, so we conclude that 

$$
\frac{\partial p^{\star}(0,0)}{\partial u_{i}}=-\lambda_{i}^{\star}.
$$ 

The same method can be used to establish 

$$
\frac{\partial p^{\star}(0,0)}{\partial v_{i}}=-\nu_{i}^{\star}.
$$ 

The local sensitivity result ( 5.58 ) gives us a quantitative measure of how active a constraint is at the optimum $x^{\star}$ . If $f_{i}(x^{\star})\,<\,0$ , then the constraint is inactive, and it follows that the constraint can be tightened or loosened a small amount without aﬀecting the optimal value. By complementary slackness, the associated optimal Lagrange multiplier must be zero. But now suppose that $f_{i}(x^{\star})=0$ , i.e. , the $i$ th constraint is active at the optimum. The $i$ th optimal Lagrange multiplier tells us how active the constraint is: If $\lambda_{i}^{\star}$ is small, it means that the constraint can be loosened or tightened a bit without much eﬀect on the optimal value; if $\lambda_{i}^{\star}$ is large, it means that if the constraint is loosened or tightened a bit, the eﬀect on the optimal value will be great. 

# Shadow price interpretation 

We can also give a simple geometric interpretation of the result ( 5.58 ) in terms of economics. We consider (for simplicity) a convex problem with no equality constraints, which satisfies Slater’s condition. The variable $x\,\in\,\mathbf{R}^{m}$ determines how a firm operates, and the objective $f_{0}$ is the cost, i.e. , $-f_{0}$ is the profit. Each constraint $f_{i}(x)\,\leq\,0$ represents a limit on some resource such as labor, steel, or warehouse space. The (negative) perturbed optimal cost function $-p^{\star}(u)$ tells us how much more or less profit could be made if more, or less, of each resource were made available to the firm. If it is diﬀerentiable near $u=0$ , then we have 

$$
\lambda_{i}^{\star}=-\frac{\partial p^{\star}(0)}{\partial u_{i}}.
$$ 

In other words, $\lambda_{i}^{\star}$ tells us approximately how much more profit the firm could make, for a small increase in availability of resource $i$ . 

It follows that $\lambda_{i}^{\star}$ would be the natural or equilibrium price for resource $i$ , if it were possible for the firm to buy or sell it. Suppose, for example, that the firm can buy or sell resource $i$ , at a price that is less than $\lambda_{i}^{\star}$ . In this case it would certainly buy some of the resource, which would allow it to operate in a way that increases its profit more than the cost of buying the resource. Conversely, if the price exceeds $\lambda_{i}^{\star}$ , the firm would sell some of its allocation of resource $i$ , and obtain a net gain since its income from selling some of the resource would be larger than its drop in profit due to the reduction in availability of the resource. 

# 5.7 Examples 

In this section we show by example that simple equivalent reformulations of a problem can lead to very diﬀerent dual problems. We consider the following types of reformulations: 

• Introducing new variables and associated equality constraints. 

• Replacing the objective with an increasing function of the original objective. • Making explicit constraints implicit, i.e. , incorporating them into the domain of the objective. 

# 5.7.1 Introducing new variables and equality constraints 

Consider an unconstrained problem of the form 

$$
{\mathrm{minimize}}\quad f_{0}(A x+b).
$$ 

Its Lagrange dual function is the constant $p^{\star}$ . So while we do have strong duality, i.e. , $p^{\star}=d^{\star}$ , the Lagrangian dual is neither useful nor interesting. 

Now let us reformulate the problem ( 5.59 ) as 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{f_{0}(y)}\\ {\operatorname{subject\to}}&{A x+b=y.}\end{array}}
$$ 

Here we have introduced new variables $y$ , as well as new equality constraints $A x+$ $b=y$ . The problems ( 5.59 ) and ( 5.60 ) are clearly equivalent. 

The Lagrangian of the reformulated problem is 

$$
L(x,y,\nu)=f_{0}(y)+\nu^{T}(A x+b-y).
$$ 

To find the dual function we minimize $L$ over $x$ and $y$ . Minimizing over $x$ we find that $g(\nu)=-\infty$ unless $A^{T}\nu=0$ , in which case we are left with 

$$
g(\nu)=b^{T}\nu+\operatorname*{inf}_{y}(f_{0}(y)-\nu^{T}y)=b^{T}\nu-f_{0}^{*}(\nu),
$$ 

where $f_{0}^{\ast}$ is the conjugate of $f_{0}$ . The dual problem of ( 5.60 ) can therefore be expressed as 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\,b^{T}\nu-f_{0}^{*}(\nu)}\\ {\mathrm{subject~to}}&{{}\,A^{T}\nu=0.}\end{array}
$$ 

Thus, the dual of the reformulated problem ( 5.60 ) is considerably more useful than the dual of the original problem ( 5.59 ). 

Example 5.5 Unconstrained geometric program. Consider the unconstrained geomet- ric program 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\log\left(\sum_{i=1}^{m}\exp(a_{i}^{T}x+b_{i})\right).}\end{array}
$$ 

We first reformulate it by introducing new variables and equality constraints: 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(y)=\log\left(\sum_{i=1}^{m}\exp y_{i}\right)}\\ {{\mathrm{subject~to}}}&{\ A x+b=y,}\end{array}}
$$ 

where $a_{i}^{T}$ are the rows of $A$ . The conjugate of the log-sum-exp function is 

$$
f_{0}^{*}(\nu)=\left\{\begin{array}{l l}{\sum_{i=1}^{m}\nu_{i}\log\nu_{i}}&{\nu\succeq0,\ \mathbf{1}^{T}\nu=1}\\ {\infty}&{\mathrm{otherwise}}\end{array}\right.
$$ 

(example 3.25 , page 93 ), so the dual of the reformulated problem can be expressed as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\boldsymbol{b}^{T}\boldsymbol{\nu}-\sum_{i=1}^{m}\boldsymbol{\nu}_{i}\log\boldsymbol{\nu}_{i}}\\ &{\mathrm{subject\to}\quad\mathbf{1}^{T}\boldsymbol{\nu}=1}\\ &{\quad\boldsymbol{A}^{T}\boldsymbol{\nu}=0}\\ &{\quad\boldsymbol{\nu}\succeq0,}\end{array}
$$ 

which is an entropy maximization problem. 

Example 5.6 Norm approximation problem. We consider the unconstrained norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|,
$$ 

where $||\cdot||$ is any norm. Here too the Lagrange dual function is constant, equal to the optimal value of ( 5.63 ), and therefore not useful. 

Once again we reformulate the problem as 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\|y\|}\\ {\operatorname{subject\to}}&{A x-b=y.}\end{array}}
$$ 

The Lagrange dual problem is, following ( 5.61 ), 

$$
\begin{array}{l r}{\mathrm{maximize}}&{b^{T}\nu}\\ {\mathrm{subject~to}}&{\|\nu\|_{*}\leq1}\\ &{A^{T}\nu=0,}\end{array}
$$ 

where we use the fact that the conjugate of a norm is the indicator function of the dual norm unit ball (example 3.26 , page 93 ). 

The idea of introducing new equality constraints can be applied to the constraint functions as well. Consider, for example, the problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{f_{0}\big(A_{0}x+b_{0}\big)}\\ {\mathrm{subject~to}}&{f_{i}\big(A_{i}x+b_{i}\big)\le0,\quad i=1,\dots,m,}\end{array}
$$ 

where $A_{i}\in\mathbf{R}^{k_{i}\times n}$ and $f_{i}:\mathbf{R}^{k_{i}}\rightarrow\mathbf{R}$ are convex. (For si we d e equality constraints here.) We introduce a new variable $y_{i}\in\mathbf{R}^{k_{i}}$ ∈ , for i $i=0,\ldots,m$ , and reformulate the problem as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(y_{0})}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(y_{i})\leq0,\quad i=1,\ldots,m}\\ &{A_{i}x+b_{i}=y_{i},\quad i=0,\ldots,m.}\end{array}}
$$ 

The Lagrangian for this problem is 

$$
L(x,y_{0},\ldots,y_{m},\lambda,\nu_{0},\ldots,\nu_{m})=f_{0}(y_{0})+\sum_{i=1}^{m}\lambda_{i}f_{i}(y_{i})+\sum_{i=0}^{m}\nu_{i}^{T}(A_{i}x+b_{i}-y_{i}).
$$ 

To find the dual function we minimize over and . The minimum over is $-\infty$ $x$ $y_{i}$ $x$ unless 

$$
\sum_{i=0}^{m}A_{i}^{T}\nu_{i}=0,
$$ 

in which case we have, for $\lambda\succ0$ , 

$$
\begin{array}{r l}{\lefteqn{g(\lambda,\nu_{0},\dots,\nu_{m})}}\\ &{=\begin{array}{l}{\displaystyle\sum_{i=0}^{m}\nu_{i}^{T}b_{i}+\operatorname*{inf}_{y_{0},\dots,y_{m}}}\\ {\displaystyle=}&{\displaystyle\sum_{i=0}^{m}\nu_{i}^{T}b_{i}+\operatorname*{inf}_{y_{0}}\left(f_{0}(y_{0})+\sum_{i=1}^{m}\lambda_{i}f_{i}(y_{i})-\sum_{i=0}^{m}\nu_{i}^{T}y_{i}\right)}\end{array}}\\ &{=\begin{array}{l}{\displaystyle\sum_{i=0}^{m}\nu_{i}^{T}b_{i}+\operatorname*{inf}_{y_{0}}\left(f_{0}(y_{0})-\nu_{0}^{T}y_{0}\right)+\sum_{i=1}^{m}\lambda_{i}\operatorname*{inf}_{y_{i}}\left(f_{i}(y_{i})-(\nu_{i}/\lambda_{i})^{T}y_{i}\right)}\end{array}}\\ &{=\begin{array}{l}{\displaystyle\sum_{i=0}^{m}\nu_{i}^{T}b_{i}-f_{0}^{*}(\nu_{0})-\sum_{i=1}^{m}\lambda_{i}f_{i}^{*}(\nu_{i}/\lambda_{i}).}\end{array}}\end{array}
$$ 

The last expression involves the perspective of the conjugate function, and is there- fore concave in the dual variables. Finally, we address the question of what happens n $\lambda\succeq0$ ut so $\lambda_{i}$ are zero. If $\lambda_{i}=0$ and $\nu_{i}\neq0$ , th the d al function is −∞ . If λ = 0 and ν = 0, however, the terms involving $y_{i}$ , ν $\nu_{i}$ , and λ are all zero. Thus, the expression above for $g$ is valid for all $\lambda\succeq0$ , if we take $\lambda_{i}f_{i}^{*}(\nu_{i}/\lambda_{i})=0$ when $\lambda_{i}=0$ and $\nu_{i}=0$ , and $\lambda_{i}f_{i}^{*}(\nu_{i}/\lambda_{i})=\infty$ ∞ when $\lambda_{i}=0$ and $\nu_{i}\neq0$ . Therefore we can express the dual of the problem ( 5.66 ) as 

$$
\begin{array}{r l}{\mathrm{maximize}}&{~\sum_{i=0}^{m}\nu_{i}^{T}b_{i}-f_{0}^{*}(\nu_{0})-\sum_{i=1}^{m}\lambda_{i}f_{i}^{*}(\nu_{i}/\lambda_{i})}\\ {\mathrm{subject~to}}&{\lambda\succeq0}\\ &{\sum_{i=0}^{m}A_{i}^{T}\nu_{i}=0.}\end{array}
$$ 

Example 5.7 Inequality constrained geometric program. The inequality constrained geometric program 

$$
\begin{array}{r l}{\mathrm{minimize}\,\,}&{\log\left(\sum_{k=1}^{K_{0}}e^{a_{0k}^{T}x+b_{0k}}\right)}\\ {\mathrm{subject~to}\,\,}&{\log\left(\sum_{k=1}^{K_{i}}e^{a_{i k}^{T}x+b_{i k}}\right)\le0,\quad i=1,\dots,m}\end{array}
$$ 

is of the form ( 5.65 ) with $f_{i}\,:\,\mathbf{R}^{K_{i}}\,\rightarrow\,\mathbf{R}$ given by $\begin{array}{r}{f_{i}(y)\,=\,\log\left(\sum_{k=1}^{K_{i}}e^{y_{k}}\right)}\end{array}$  . The conjugate of this function is 

$$
\begin{array}{r}{f_{i}^{*}(\ensuremath{\boldsymbol\nu})=\left\{\begin{array}{l l}{\sum_{k=1}^{K_{i}}\nu_{k}\log\nu_{k}}&{\ensuremath{\boldsymbol\nu}\succeq0,\quad\mathbf{1}^{T}\ensuremath{\boldsymbol\nu}=1}\\ {\infty}&{\mathrm{otherwise}.}\end{array}\right.}\end{array}
$$ 

Using ( 5.67 ) we can immediately write down the dual problem as 

$$
\begin{array}{r l}{\mathrm{maximize}\ }&{b_{0}^{T}\nu_{0}-\sum_{k=1}^{K_{0}}\nu_{0k}\log\nu_{0k}+\sum_{i=1}^{m}\left(b_{i}^{T}\nu_{i}-\sum_{k=1}^{K_{i}}\nu_{i k}\log(\nu_{i k}/\lambda_{i})\right)}\\ {\mathrm{subject~to}\ }&{\nu_{0}\succeq0,\quad\mathbf{1}^{T}\nu_{0}=1}\\ &{\nu_{i}\succeq0,\quad\mathbf{1}^{T}\nu_{i}=\lambda_{i},\quad i=1,\dots,m}\\ &{\lambda_{i}\geq0,\quad i=1,\dots,m}\\ &{\sum_{i=0}^{m}A_{i}^{T}\nu_{i}=0,}\end{array}
$$ 

which further simplifies to 

$$
\begin{array}{r l}{\mathrm{maximize}\ }&{b_{0}^{T}\nu_{0}-\sum_{k=1}^{K_{0}}\nu_{0k}\log{\nu_{0k}}+\sum_{i=1}^{m}\left(b_{i}^{T}\nu_{i}-\sum_{k=1}^{K_{i}}\nu_{i k}\log(\nu_{i k}/\mathbf{1}^{T}\nu_{i})\right)}\\ {\mathrm{subject~to}\ }&{\nu_{i}\succeq0,\quad i=0,\dots,m}\\ &{\mathbf{1}^{T}\nu_{0}=1}\\ &{\sum_{i=0}^{m}A_{i}^{T}\nu_{i}=0.}\end{array}
$$ 

# 5.7.2 Transforming the objective 

If we replace the objective $f_{0}$ by an increasing function of $f_{0}$ , the resulting problem is clearly equivalent (see § 4.1.3 ). The dual of this equivalent problem, however, can be very diﬀerent from the dual of the original problem. 

Example 5.8 We consider again the minimum norm problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|,
$$ 

where $||\cdot||$ is some norm. We reformulate this problem as 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{(1/2)\|y\|^{2}}\\ {\operatorname{subject\to}\quad A x-b=y.}\end{array}}
$$ 

Here we have introduced new variables, and replaced the objective by half its square. Evidently it is equivalent to the original problem. 

The dual of the reformulated problem is 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}-(1/2)\|\nu\|_{*}^{2}+b^{T}\nu}\\ {\mathrm{subject~to}}&{{}A^{T}\nu=0,}\end{array}
$$ 

where we use the fact that the conjugate of $(1/2)\|\cdot\|^{2}$ is $(1/2)\|\cdot\|_{*}^{2}$ (see example 3.27 , page 93 ). 

Note that this dual problem is not the same as the dual problem ( 5.64 ) derived earlier. 

# 5.7.3 Implicit constraints 

The next simple reformulation we study is to include some of the constraints in the objective function, by modifying the objective function to be infinite when the constraint is violated. 

Example 5.9 Linear program with box constraints. We consider the linear program 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&&{c^{T}x}\\ &{{\mathrm{subject~to}}}&&{A x=b}\\ &{}&&{l\prec x\prec u}\end{array}}
$$ 

where $A\,\in\,\mathbf{R}^{p\times n}$ and $\textit{l}\prec\textit{u}$ . The constraints $\mathrm{~\it~{~l~}~\preceq~x~}\preceq\mathrm{~\it~{~u~}~}$ are sometimes called box constraints or variable bounds . 

We can, of course, derive the dual of this linear program. The dual will have a Lagrange multiplier $\nu$ associated with the equality constraint, $\lambda_{1}$ associated with the inequality constraint $x\preceq u$ , and $\lambda_{2}$ associated with the inequality constraint $\mathit{l}\preceq x$ . The dual is 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\quad-b^{T}\nu-\lambda_{1}^{T}u+\lambda_{2}^{T}l}\\ {{\mathrm{subject~to}}}&{\;A^{T}\nu+\lambda_{1}-\lambda_{2}+c=0}\\ &{\quad\lambda_{1}\succeq0,\quad\lambda_{2}\succeq0.}\end{array}}
$$ 

Instead, let us first reformulate the problem ( 5.68 ) as 

$$
\begin{array}{l c l}{\mathrm{minimize}}&{f_{0}(\boldsymbol{x})}\\ {\mathrm{subject~to}}&{A\boldsymbol{x}=\boldsymbol{b},}\end{array}
$$ 

where we define 

$$
f_{0}(x)={\left\{\begin{array}{l l}{c^{T}x}&{l\preceq x\preceq u}\\ {\infty}&{{\mathrm{otherwise.}}}\end{array}\right.}
$$ 

The problem ( 5.70 ) is clearly equivalent to ( 5.68 ); we have merely made the explicit box constraints implicit. 

The dual function for the problem ( 5.70 ) is 

$$
\begin{array}{r c l}{g(\nu)}&{=}&{\displaystyle\operatorname*{inf}_{l\preceq x\preceq u}\left(c^{T}x+\nu^{T}(A x-b)\right)}\\ &{=}&{\displaystyle-b^{T}\nu-u^{T}(A^{T}\nu+c)^{-}+l^{T}(A^{T}\nu+c)^{+}}\end{array}
$$ 

where $y_{i}^{+}=\operatorname*{max}\{y_{i},0\}$ { } , $y_{i}^{-}=\operatorname*{max}\{-y_{i},0\}$ {− } . So here we are able to derive an analyt- ical formula for $g$ , which is a concave piecewise-linear function. 

The dual problem is the unconstrained problem 

$$
\mathrm{maximize}\quad-\boldsymbol{b}^{T}\boldsymbol{\nu}-\boldsymbol{u}^{T}(\boldsymbol{A}^{T}\boldsymbol{\nu}+\boldsymbol{c})^{-}+\boldsymbol{l}^{T}(\boldsymbol{A}^{T}\boldsymbol{\nu}+\boldsymbol{c})^{+},
$$ 

which has a quite diﬀerent form from the dual of the original problem. 

(The problems ( 5.69 ) and ( 5.71 ) are closely related, in fact, equivalent; see exer- cise 5.8 .) 

# 5.8 Theorems of alternatives 

# 5.8.1 Weak alternatives via the dual function 

In this section we apply Lagrange duality theory to the problem of determining feasibility of a system of inequalities and equalities 

$$
f_{i}(x)\leq0,\quad i=1,\ldots,m,\qquad h_{i}(x)=0,\quad i=1,\ldots,p.
$$ 

th ain of the inequality system ( 5.72 ), $\begin{array}{r l}{\mathcal{D}~=~\bigcap_{i=1}^{m}\mathbf{dom}\,f_{i}}&{{}\cap}\end{array}$ ∩ $\textstyle\bigcap_{i=1}^{p}\mathbf{dom}\,h_{i}$ T , is nonempty. We can think of ( 5.72 ) as the standard problem ( 5.1 ), with objective f $f_{\mathrm{0}}=0$ i.e. , 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{0}\\ {{\mathrm{subject~to}}\quad f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{h_{i}(x)=0,\quad i=1,\ldots,p.}\end{array}}
$$ 

This problem has optimal value 

$$
p^{\star}=\left\{\begin{array}{l l}{{0}}&{{(5.72)\mathrm{~is~fesicle}}}\\ {{\infty}}&{{(5.72)\mathrm{~is~infesicle,}}}\end{array}\right.
$$ 

so solving the optimization problem ( 5.73 ) is the same as solving the inequality system ( 5.72 ). 

# The dual function 

We associate with the inequality system ( 5.72 ) the dual function 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x\in\mathcal{D}}\left(\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x)\right),
$$ 

which is the same as the dual function for the optimization problem ( 5.73 ). Since $f_{\mathrm{0}}=0$ , the dual function is positive homogeneous in $(\lambda,\nu)$ : For $\alpha>0$ , $g(\alpha\lambda,\alpha\nu)=$ $\alpha g(\lambda,\nu)$ . The dual problem associated with ( 5.73 ) is to maximize $g(\lambda,\nu)$ subject to $\lambda\succeq0$ . Since $g$ is homogeneous, the optimal value of this dual problem is given by 

$$
d^{\star}=\left\{\begin{array}{l l}{\infty}&{\lambda\succeq0,\ g(\lambda,\nu)>0\ \mathrm{is\ f e a s i b l e}}\\ {0}&{\lambda\succeq0,\ g(\lambda,\nu)>0\ \mathrm{is\inf e a s i b l e.}}\end{array}\right.
$$ 

Weak duality tells us that $d^{\star}\leq p^{\star}$ . Combining this fact with ( 5.74 ) and ( 5.75 ) yields the following: If the inequality system 

$$
\lambda\succeq0,\qquad g(\lambda,\nu)>0
$$ 

is feasible (which means $d^{\star}=\infty$ ), then the inequality system ( 5.72 ) is infeasible (since we then have $p^{\star}=\infty$ ). Indeed, we can interpret any solution $(\lambda,\nu)$ of the inequalities ( 5.76 ) as a proof or certificate of infeasibility of the system ( 5.72 ). 

We can restate this implication in terms of feasibility of the original system: If the original inequality system ( 5.72 ) is feasible, then the inequality system ( 5.76 ) must be infeasible. We can interpret an $x$ which satisfies ( 5.72 ) as a certificate establishing infeasibility of the inequality system ( 5.76 ). 

Two systems of inequalities (and equalities) are called weak alternatives if at most one of the two is feasible. Thus, the systems ( 5.72 ) and ( 5.76 ) are weak alternatives. This is true whether or not the inequalities ( 5.72 ) are convex ( i.e. , $f_{i}$ convex, $h_{i}$ affine); moreover, the alternative inequality system ( 5.76 ) is always convex ( i.e. , $g$ is concave and the constraints $\lambda_{i}\geq0$ are convex). 

# Strict inequalities 

We can also study feasibility of the strict inequality system 

$$
f_{i}(x)<0,\quad i=1,.\,.\,,m,\qquad h_{i}(x)=0,\quad i=1,.\,.\,,p.
$$ 

With $g$ defined as for the nonstrict inequality system, we have the alternative inequality system 

$$
\lambda\succeq0,\qquad\lambda\neq0,\qquad g(\lambda,\nu)\geq0.
$$ 

We can show directly that ( 5.77 ) and ( 5.78 ) are weak alternatives. Suppose there exists an x with $f_{i}(\tilde{x})<0$ $h_{i}(\tilde{x})=0$ ) = 0. Then for any $\lambda\succeq0$ , $\lambda\neq0$ , and $\nu$ , 

$$
\lambda_{1}f_{1}(\tilde{x})+\cdot\cdot\cdot+\lambda_{m}f_{m}(\tilde{x})+\nu_{1}h_{1}(\tilde{x})+\cdot\cdot\cdot+\nu_{p}h_{p}(\tilde{x})<0.
$$ 

It follows that 

$$
\begin{array}{r c l}{g(\lambda,\nu)}&{=}&{\displaystyle\operatorname*{inf}_{x\in\mathcal{D}}\left(\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x)\right)}\\ &{\leq}&{\displaystyle\sum_{i=1}^{m}\lambda_{i}f_{i}(\tilde{x})+\sum_{i=1}^{p}\nu_{i}h_{i}(\tilde{x})}\\ &{<}&{0.}\end{array}
$$ 

Therefore, feasibility of ( 5.77 ) implies that there does not exist $(\lambda,\nu)$ satisfy- ing ( 5.78 ). 

Thus, we can prove infeasibility of ( 5.77 ) by producing a solution of the sys- tem ( 5.78 ); we can prove infeasibility of ( 5.78 ) by producing a solution of the system ( 5.77 ). 

# 5.8.2 Strong alternatives 

When the original inequality system is convex, i.e. , $f_{i}$ are convex and $h_{i}$ are affine, and some type of constraint qualification holds, then the pairs of weak alternatives described above are strong alternatives , which means that exactly one of the two alternatives holds. In other words, each of the inequality systems is feasible if and only if the other is infeasible. 

In this section we assume that $f_{i}$ are convex and $h_{i}$ are affine, so the inequality system ( 5.72 ) can be expressed as 

$$
f_{i}(x)\leq0,\quad i=1,\ldots,m,\qquad A x=b,
$$ 

where $A\in\mathbf{R}^{p\times n}$ 

# Strict inequalities 

We first study the strict inequality system 

$$
f_{i}(x)<0,\quad i=1,\ldots,m,\qquad A x=b,
$$ 

and its alternative 

$$
\lambda\succeq0,\qquad\lambda\neq0,\qquad g(\lambda,\nu)\geq0.
$$ 

We need one technical condition: There exists an $x\,\in\,\mathbf{relint}\,\mathcal{D}$ with $A x\,=\,b$ . In other words we not only assume that the linear equality constraints are consistent, but also that they have a solution in relint $\mathcal{D}$ . (Very often $\mathcal{D}=\mathbf{R}^{n}$ , so the condition is satisfied if the equality constraints are consistent.) Under this condition, exactly one of the inequality systems ( 5.79 ) and ( 5.80 ) is feasible. In other words, the inequality systems ( 5.79 ) and ( 5.80 ) are strong alternatives. 

We will establish this result by considering the related optimization problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad s}\\ &{{\mathrm{subject~to}}\quad f_{i}(x)-s\leq0,\quad i=1,\ldots,m}\\ &{\ A x=b}\end{array}}
$$ 

with variables $x,\ s$ , and domain $\mathcal{D}\times\mathbf{R}$ . The optimal value $p^{\star}$ of this problem is negative if and only if there exists a solution to the strict inequality system ( 5.79 ). The Lagrange dual function for the problem ( 5.81 ) is 

$$
\operatorname*{inf}_{x\in{\mathcal{D}},\;s}\left(s+\sum_{i=1}^{m}\lambda_{i}{\bigl(}f_{i}(x)-s{\bigr)}+\nu^{T}(A x-b)\right)={\left\{\begin{array}{l l}{g(\lambda,\nu)}&{\mathbf{1}^{T}\lambda=1}\\ {-\infty}&{{\mathrm{otherwise}}.}\end{array}\right.}
$$ 

Therefore we can express the dual problem of ( 5.81 ) as 

$$
\begin{array}{l l}{\mathrm{maximize}}&{g(\lambda,\nu)}\\ {\mathrm{subject~to}}&{\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1.}\end{array}
$$ 

Now we observe that Slater’s condition holds for the problem ( 5.81 ). By the hypothesis there exists an $\tilde{{\boldsymbol{x}}}\in\mathbf{relint}\,\mathcal{D}$ ∈ D with $A\tilde{x}=b$ b . Choosing any $\tilde{s}>\operatorname*{max}_{i}{f_{i}(\tilde{x})}$ yields a point $(\widetilde{x},\widetilde{s})$ ) which is strictly feasible for ( 5.81 ). Therefore we have $d^{\star}=p^{\star}$ , and the dual optimum $d^{\star}$ is attained. In other words, there exist $(\lambda^{\star},\nu^{\star})$ such that 

$$
\begin{array}{r}{g(\lambda^{\star},\nu^{\star})=p^{\star},\qquad\lambda^{\star}\succeq0,\qquad\mathbf{1}^{T}\lambda^{\star}=1.}\end{array}
$$ 

Now suppose that the strict inequality system ( 5.79 ) is infeasible, which means that $p^{\star}\geq0$ . Then $(\lambda^{\star},\nu^{\star})$ from ( 5.82 ) satisfy the alternate inequality system ( 5.80 ). imilarly, if the alternate inequality system ( 5.80 ) is feasible, then $d^{\star}\ =\ p^{\star}\ \geq$ 0, which shows that the strict inequality system ( 5.79 ) is infeasible. Thus, the inequality systems ( 5.79 ) and ( 5.80 ) are strong alternatives; each is feasible if and only if the other is not. 

# Nonstrict inequalities 

We now consider the nonstrict inequality system 

$$
f_{i}(x)\leq0,\quad i=1,.\,.\,.\,,m,\qquad A x=b,
$$ 

and its alternative 

$$
\lambda\succeq0,\qquad g(\lambda,\nu)>0.
$$ 

We will show these are strong alternatives, provided the following conditions hold: There exists an $x\,\in\,\mathbf{relint}\,\mathcal{D}$ with $A x\,=\,b$ d the optimal value $p^{\star}$ of ( 5.81 ) is attained. This holds, for example, if D $\mathcal{D}=\mathbf{R}^{n}$ and max i $f_{i}(x)\to\infty$ as $x\to\infty$ . With these assumptions we have, as in the strict case, that $p^{\star}=d^{\star}$ , and that both the primal and dual optimal values are attained. Now suppose that the nonstrict inequality system ( 5.83 ) is infeasible, which means that $p^{\star}>0$ . (Here we use the assumption that the primal optimal value is attained.) Then $(\lambda^{\star},\nu^{\star})$ from ( 5.82 ) satisfy the alternate inequality system ( 5.84 ). Thus, the inequality systems ( 5.83 ) and ( 5.84 ) are strong alternatives; each is feasible if and only if the other is not. 

# 5.8.3 Examples 

# Linear inequalities 

Consider the system of linear inequalities $A x\preceq b$ . The dual function is 

$$
g(\lambda)=\operatorname*{inf}_{x}\lambda^{T}(A x-b)=\left\{\begin{array}{l l}{-b^{T}\lambda}&{A^{T}\lambda=0}\\ {-\infty}&{\mathrm{otherwise.}}\end{array}\right.
$$ 

The alternative inequality system is therefore 

$$
\lambda\succeq0,\qquad A^{T}\lambda=0,\qquad b^{T}\lambda<0.
$$ 

These are, in fact, strong alternatives. This follows since the optimum in the related problem ( 5.81 ) is achieved, unless it is unbounded below. 

We now consider the system of strict linear inequalities $A x\prec b$ , which has the strong alternative system 

$$
\lambda\succeq0,\qquad\lambda\neq0,\qquad A^{T}\lambda=0,\qquad b^{T}\lambda\leq0.
$$ 

In fact we have encountered (and proved) this result before, in § 2.5.1 ; see ( 2.17 ) and ( 2.18 ) (on page 50 ). 

# Intersection of ellipsoids 

We consider $m$ ellipsoids, described as 

$$
{\mathcal{E}}_{i}=\{x\mid f_{i}(x)\leq0\},
$$ 

with $f_{i}(x)\,=\,x^{T}A_{i}x+2b_{i}^{T}x+c_{i}$ , $i\,=\,1,.\,.\,.\,,m$ , where $A_{i}\,\in\,\mathbf{S}_{++}^{\iota}$ . We ask when the intersection of these ellipsoids has nonempty interior. This is equivalent to feasibility of the set of strict quadratic inequalities 

$$
f_{i}(x)=x^{T}A_{i}x+2b_{i}^{T}x+c_{i}<0,\quad i=1,.\,.\,.\,,m.
$$ 

The dual function $g$ is 

$$
\begin{array}{r l l}{g(\boldsymbol{\lambda})}&{=}&{\underset{x}{\operatorname*{inf}}\left(x^{T}A(\boldsymbol{\lambda})x+2b(\boldsymbol{\lambda})^{T}x+c(\boldsymbol{\lambda})\right)}\\ &{=}&{\left\{\begin{array}{l l l}{-b(\boldsymbol{\lambda})^{T}A(\boldsymbol{\lambda})^{\dagger}b(\boldsymbol{\lambda})+c(\boldsymbol{\lambda})}&{A(\boldsymbol{\lambda})\succeq0,\quad b(\boldsymbol{\lambda})\in\mathcal{R}(A(\boldsymbol{\lambda}))}\\ {-\infty}&{\mathrm{otherwise},}\end{array}\right.}\end{array}
$$ 

where 

$$
A(\lambda)=\sum_{i=1}^{m}\lambda_{i}A_{i},\qquad b(\lambda)=\sum_{i=1}^{m}\lambda_{i}b_{i},\qquad c(\lambda)=\sum_{i=1}^{m}\lambda_{i}c_{i}.
$$ 

Note that for $\lambda\succeq0$ , $\lambda\neq0$ , we have $A(\lambda)\succ0$ , so we can simplify the expression for the dual function as 

$$
g(\lambda)=-b(\lambda)^{T}A(\lambda)^{-1}b(\lambda)+c(\lambda).
$$ 

The strong alternative of the system ( 5.85 ) is therefore 

$$
\lambda\succeq0,\qquad\lambda\neq0,\qquad-b(\lambda)^{T}A(\lambda)^{-1}b(\lambda)+c(\lambda)\geq0.
$$ 

We can give a simple geometric interpretation of this pair of strong alternatives. For any nonzero $\lambda\succeq0$ , the (possibly empty) ellipsoid 

$$
{\mathcal{E}}_{\lambda}=\{x\mid x^{T}A(\lambda)x+2b(\lambda)^{T}x+c(\lambda)\leq0\}
$$ 

contains $\mathcal{E}_{1}\cap\cdot\cdot\cdot\cap\mathcal{E}_{m}$ , since $f_{i}(x)\,\leq\,0$ implies $\begin{array}{r}{\sum_{i=1}^{m}\lambda_{i}f_{i}(x)\,\le\,0}\end{array}$ ≤ 0. Now, $\mathcal{E}_{\lambda}$ has empty interior if and only if 

$$
\operatorname*{inf}_{x}\left(x^{T}A(\lambda)x+2b(\lambda)^{T}x+c(\lambda)\right)=-b(\lambda)^{T}A(\lambda)^{-1}b(\lambda)+c(\lambda)\geq0.
$$ 

Therefore the alternative system ( 5.86 ) means that $\mathcal{E}_{\lambda}$ has empty interior. 

k duality is obvious: If ( 5.86 ) holds, then $\mathcal{E}_{\lambda}$ contains the intersection $\mathcal{E}_{1}\cap$ · · · ∩E , and has empty interior, so naturally the intersection has empty interior. The fact that these are strong alternatives states the (not obvious) fact that if the intersection $\mathcal{E}_{1}\cap\cdot\cdot\cdot\cap\mathcal{E}_{m}$ has empty interior, then we can construct an ellipsoid $\mathcal{E}_{\lambda}$ that contains the intersection and has empty interior. 

# Farkas’ lemma 

In this section we describe a pair of strong alternatives for a mixture of strict and nonstrict linear inequalities, known as Farkas’ lemma : The system of inequalities 

$$
A x\preceq0,\qquad c^{T}x<0,
$$ 

where $A\in\mathbf{R}^{m\times n}$ and $c\in\mathbf{R}^{n}$ , and the system of equalities and inequalities 

$$
A^{T}y+c=0,\qquad y\succeq0,
$$ 

are strong alternatives. 

We can prove Farkas’ lemma directly, using LP duality. Consider the LP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq0,}\end{array}}
$$ 

and its dual 

$$
{\begin{array}{r l}&{{\mathrm{maximize}}\quad0}\\ &{{\mathrm{subject~to}}\quad A^{T}y+c=0}\\ &{\quad\quad\quad y\succeq0.}\end{array}}
$$ 

The primal LP ( 5.89 ) is homogeneous, and so has optimal value $0$ , if ( 5.87 ) is not feasible, and optimal value $-\infty$ , if ( 5.87 ) is feasible. The dual LP $(5.90)$ has optimal v , if ( 5.88 ) is feasible, and optimal value $-\infty$ , if ( 5.88 ) is infeasible. 

Since x = 0 is feasible in ( 5.89 ), we can rule out the one case in which strong duality can fail for LPs, so we must have $p^{\star}=d^{\star}$ . Combined with the remarks above, this shows that ( 5.87 ) and ( 5.88 ) are strong alternatives. 

Example 5.10 Arbitrage-free bounds on price. We consider a set of $n$ assets, with prices at the beginning of an investment period $p_{1},\ldots,p_{n}$ , respectively. At the end of the investment period, the value of the assets is $v_{1},\dots,v_{n}$ . If $x_{1},\allowbreak\cdot\cdot\cdot,x_{n}$ represents the initial investment in each asset (with $x_{j}<0$ meaning a short position in asset $j$ ), the cost of the initial investment is $p^{T}x$ , and the final value of the investment is $v^{T}x$ . 

The value of the assets at the end of the investment period, $v$ , is uncertain. We will assume that only $_{m}$ possible scenarios, or outcomes, are possible. If outcome $i$ occurs, $v^{(i)}$ the final value of the assets is , and therefore, the overall value of the investments is $v^{(i)T}x$ . 

If there is an investment vector $x$ with $p^{T}x\,<\,0$ , and in all possible scenarios, the final value is nonnegative, i.e. , $v^{(i)T}x\geq0$ for $i=1,\ldots,m$ , then an arbitrage is said to exist. The condition $p^{T}x<0$ means you are paid to accept the investment mix, and the condition $v^{(i)T}x\geq0$ for $i=1,\dots,m$ means that no matter what outcome occurs, the final value is nonnegative, so an arbitrage corresponds to a guaranteed money-making investment strategy. It is generally assumed that the prices and values are such that no arbitrage exists. This means that the inequality system 

$$
\begin{array}{r}{V x\succeq0,\qquad p^{T}x<0}\end{array}
$$ 

is infeasible, where $V_{i j}=v_{j}^{(i)}$ . 

Using Farkas’ lemma, we have no arbitrage if and only if there exists $y$ such that 

$$
\begin{array}{r}{-\boldsymbol{V}^{T}\boldsymbol{y}+p=0,\qquad\boldsymbol{y}\succeq0.}\end{array}
$$ 

We can use this characterization of arbitrage-free prices and values to solve several interesting problems. 

Suppose, for example, that the values $V$ are known, and all prices except the last one, $p_{n}$ , are known. The set of prices $p_{n}$ that are consistent with the no-arbitrage assumption is an interval, which can be found by solving a pair of LPs. The optimal value of the LP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ p_{n}}\\ &{{\mathrm{subject~to}}\quad V^{T}y=p,\quad y\succeq0,}\end{array}}
$$ 

with variables $p_{n}$ and $y$ , gives the smallest possible arbitrage-free price for asset $n$ . Solving the same LP with maximization instead of minimization yields the largest possible price for asset $n$ . If the two values are equal, i.e. , the no-arbitrage assumption leads us to a unique price for asset $n$ , we say the market is complete . For an example, see exercise 5.38 . 

This method can be used to find bounds on the price of a derivative or option that is based on the final value of other underlying assets, i.e. , when the value or payoﬀ of asset $n$ is a function of the values of the other assets. 

# 5.9 Generalized inequalities 

In this section we examine how Lagrange duality extends to a problem with gen- eralized inequality constraints 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\preceq_{K_{i}}0,\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=0,\quad i=1,.\,.\,,p,}\end{array}}
$$ 

where $K_{i}\subseteq\mathbf{R}^{k_{i}}$ are proper cones. For now, w ob- lem ( 5.91 ). We assume the domain of ( 5.91 ), D $\begin{array}{r}{\mathcal{D}=\bigcap_{i=0}^{n^{\iota}}\mathbf{dom}\,f_{i}\,\cap\,\bigcap_{i=1}^{p}\mathbf{dom}\,h_{i}}\end{array}$ T ∩ T , is nonempty. 

# 5.9.1 The Lagrange dual 

With each gener equality $f_{i}(x)\ \preceq_{K_{i}}\ 0$ in ( 5.91 ) we associate a Lagrange multiplier vector λ $\lambda_{i}\in\mathbf{R}^{k_{i}}$ and define the associated Lagrangian as 

$$
L(x,\lambda,\nu)=f_{0}(x)+\lambda_{1}^{T}f_{1}(x)+\cdot\cdot\cdot+\lambda_{m}^{T}f_{m}(x)+\nu_{1}h_{1}(x)+\cdot\cdot\cdot+\nu_{p}h_{p}(x),
$$ 

where $\lambda=(\lambda_{1},.\,.\,.\,,\lambda_{m})$ and $\nu=(\nu_{1},.\,.\,.\,,\nu_{p})$ . The dual function is defined exactly as in a problem with scalar inequalities: 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x\in{\mathcal{D}}}L(x,\lambda,\nu)=\operatorname*{inf}_{x\in{\mathcal{D}}}\left(f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}^{T}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x)\right).
$$ 

Since the Lagrangian is affine in the dual variables $(\lambda,\nu)$ , and the dual function is a pointwise infimum of the Lagrangian, the dual function is concave. 

As in a problem with scalar inequalities, the dual function gives lower bounds on $p^{\star}$ , the optimal value of the primal problem ( 5.91 ). For a problem with scalar inequalities, we require $\lambda_{i}\,\geq\,0$ . Here the nonnegativity requirement on the dual variables is replaced by the condition 

$$
\lambda_{i}\succeq_{K_{i}^{*}}0,\quad i=1,.:.:,m,
$$ 

where $K_{i}^{*}$ denotes the dual cone of $K_{i}$ . In other words, the Lagrange multipliers associated with inequalities must be dual nonnegative. 

Weak duality follows immediately from the definition of dual cone. If $\lambda_{i}\succeq K_{i}^{*}$ 0 and $f_{i}({\tilde{x}})\preceq_{K_{i}}0$ ⪯ 0, then $\lambda_{i}^{T}f_{i}(\tilde{x})\leq0$ ≤ 0. Therefore for any primal feasible point ˜ and any $\lambda_{i}\succeq K_{i}^{*}$ 0, we have 

$$
f_{0}\big(\tilde{x}\big)+\sum_{i=1}^{m}\lambda_{i}^{T}f_{i}(\tilde{x})+\sum_{i=1}^{p}\nu_{i}h_{i}(\tilde{x})\leq f_{0}(\tilde{x}).
$$ 

Taking the infimum over x yields $g(\lambda,\nu)\leq p^{\star}$ . The Lagrange dual optimization problem is 

$$
\begin{array}{l l}{\mathrm{maximize}}&{g(\lambda,\nu)}\\ {\mathrm{subject~to}}&{\lambda_{i}\succeq_{K_{i}^{*}}0,\quad i=1,.\,.\,.\,,m.}\end{array}
$$ 

We always have weak duality , i.e. , $d^{\star}\leq p^{\star}$ , where $d^{\star}$ denotes the optimal value of the dual problem ( 5.92 ), whether or not the primal problem ( 5.91 ) is convex. 

# Slater’s condition and strong duality 

As might be expected, strong duality ( $d^{\star}\,=\,p^{\star}$ ) holds when the primal problem is convex and satisfies an appropriate constraint qualification. For example, a generalized version of Slater’s condition for the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\preccurlyeq_{K_{i}}0,\quad i=1,.\,.\,,m}\\ &{A x=b,}\end{array}}
$$ 

$f_{0}$ is convex and $f_{i}$ is $K_{i}$ -convex, is that there exists an $x\in\mathbf{relint}\,\mathcal{D}$ with $A x=b$ b and $f_{i}(x)\prec_{K_{i}}0$ , $i=1,\ldots,m$ . This condition implies strong duality (and also, that the dual optimum is attained). 

Example 5.11 Lagrange dual of semidefinite program. We consider a semidefinite program in inequality form, 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0}\end{array}}
$$ 

where $F_{1},.\;.\;.\;,F_{n},G\in\mathbf{S}^{k}$ . (Here $f_{1}$ is affine, and $K_{1}$ is $\mathbf{S}_{+}^{k}$ , the positive semidefinite cone.) 

We associate with the constraint a dual variable or multiplier $Z\,\in\,{\bf S}^{k}$ , so the La- grangian is 

$$
{\begin{array}{l l l}{L(x,Z)}&{=}&{{\boldsymbol{c}}^{T}x+\mathbf{tr}\left(\left(x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\right)Z\right)}\\ &{=}&{x_{1}(c_{1}+\mathbf{tr}(F_{1}Z))+\cdot\cdot\cdot+x_{n}(c_{n}+\mathbf{tr}(F_{n}Z))+\mathbf{tr}(G Z),}\end{array}}
$$ 

which is affine in $x$ . The dual function is given by 

$$
g(Z)=\operatorname*{inf}_{x}L(x,Z)={\left\{\begin{array}{l l}{\mathbf{tr}(G Z)}&{\mathbf{tr}(F_{i}Z)+c_{i}=0,\quad i=1,.\,.\,,n}\\ {-\infty}&{{\mathrm{otherwise}}.}\end{array}\right.}
$$ 

The dual problem can therefore be expressed as 

$$
{\begin{array}{r l}&{{\mathrm{maximize}}\quad\operatorname{\mathbf{tr}}(G Z)}\\ &{{\mathrm{subject~to}}\quad\operatorname{\mathbf{tr}}(F_{i}Z)+c_{i}=0,\quad i=1,\ldots,n}\\ &{\quad Z\succeq0.}\end{array}}
$$ 

(We use the fact that $\mathbf{S}_{+}^{k}$ is self-dual, i.e. , $(\mathbf{S}_{+}^{k})^{*}=\mathbf{S}_{+}^{k}$ ; see § 2.6 .) 

Strong duality obtains if the semidefinite program ( 5.93 ) is strictly feasible, i.e. , there exists an $x$ with 

$$
x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\prec0.
$$ 

Example 5.12 Lagrange dual of cone program in standard form. We consider the cone program 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad}&{A x=b}\\ &{x\succeq_{K}0,}\end{array}}
$$ 

where $A\,\in\,\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , and $K\subseteq\mathbf{R}^{n}$ is a proper cone. We associate with the equality constraint a multiplier $\nu\,\in\,\mathbf{R}^{m}$ , and with the nonnegativity constraint a multiplier $\lambda\in\mathbf{R}^{n}$ . The Lagrangian is 

$$
\boldsymbol{L}(\boldsymbol{x},\boldsymbol{\lambda},\nu)=\boldsymbol{c}^{T}\boldsymbol{x}-\boldsymbol{\lambda}^{T}\boldsymbol{x}+\nu^{T}\big(\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b}\big),
$$ 

so the dual function is 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x}L(x,\lambda,\nu)=\left\{\begin{array}{l l}{-b^{T}\nu}&{A^{T}\nu-\lambda+c=0}\\ {-\infty}&{\mathrm{otherwise}.}\end{array}\right.
$$ 

The dual problem can be expressed as 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\,\,-b^{T}\nu}\\ {{\mathrm{subject~to}}}&{\,\,A^{T}\nu+c=\lambda}\\ &{\,\,\lambda\succeq_{K^{*}}0.}\end{array}}
$$ 

By eliminating $\lambda$ and defining $y=-\nu$ , this problem can be simplified to 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\;b^{T}y}\\ &{\mathrm{subject~to}\quad A^{T}y\preceq_{K^{*}}c,}\end{array}
$$ 

which is a cone program in inequality form, involving the dual generalized inequality. duality obtains if the Slater condition holds, i.e. , there is an $x\,\succ_{K}\,0$ with $A x=b$ b . 

# 5.9.2 Optimality conditions 

The optimality conditions of $\S5.5$ are readily extended to problems with generalized inequalities. We first derive the complementary slackness conditions. 

# Complementary slackness 

Assume that the primal and dual optimal values are equal, and attained at the optimal points $x^{\star}$ , $\lambda^{\star}$ , $\nu^{\star}$ . As in § 5.5.2 , the complementary slackness conditions follow directly from the equality $f_{0}\big(x^{\star}\big)=g\big(\lambda^{\star},\nu^{\star}\big)$ , along with the definition of $g$ . We have 

$$
\begin{array}{r c l}{f_{0}(x^{\star})}&{=}&{g(\lambda^{\star},\nu^{\star})}\\ &{\leq}&{f_{0}(x^{\star})+\displaystyle\sum_{i=1}^{m}\lambda_{i}^{\star T}f_{i}(x^{\star})+\displaystyle\sum_{i=1}^{p}\nu_{i}^{\star}h_{i}(x^{\star})}\\ &{\leq}&{f_{0}(x^{\star}),}\end{array}
$$ 

and therefore we conclude that $x^{\star}$ minimizes $L(x,\lambda^{\star},\nu^{\star})$ , and also that the two sums in the second line are zero. Since the second sum is zero (since $x^{\star}$ satisfies the equality constraints), we have $\begin{array}{r}{\sum_{i=1}^{m}\lambda_{i}^{\star T}f_{i}(x^{\star})\,=\,0}\end{array}$ ) = 0. Since each term in this sum is nonpositive, we conclude that 

$$
\boldsymbol{\lambda}_{i}^{\star T}f_{i}(\boldsymbol{x}^{\star})=0,\quad i=1,\ldots,m,
$$ 

which generalizes the complementary slackness condition ( 5.48 ). From ( 5.94 ) we can conclude that 

$$
\lambda_{i}^{\star}\succ_{K_{i}^{*}}0\implies f_{i}(x^{\star})=0,\qquad f_{i}(x^{\star})\prec_{K_{i}}0,\iff\lambda_{i}^{\star}=0.
$$ 

However, in contrast to problems with scalar inequalities, it is possible to sat- isfy ( 5.94 ) with $\lambda_{i}^{\star}\neq0$ = 0 and $f_{i}(x^{\star})\neq0$ . 

# KKT conditions 

Now we add the assumption that the functions $f_{i}$ , $h_{i}$ are diﬀerentiable, and gener- ize the KKT conditions of § 5.5.3 to problems with generalized ineq lities. Since $x^{\star}$ minimizes $L(x,\lambda^{\star},\nu^{\star})$ , its gradient with respect to $x$ vanishes at x $x^{\star}$ : 

$$
\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}D f_{i}(x^{\star})^{T}\lambda_{i}^{\star}+\sum_{i=1}^{p}\nu_{i}^{\star}\nabla h_{i}(x^{\star})=0,
$$ 

where $D f_{i}(x^{\star})\in\mathbf{R}^{k_{i}\times n}$ is the derivative of $f_{i}$ evaluated at $x^{\star}$ (see § A.4.1 ). Thus, if strong duality holds, any primal optimal x $x^{\star}$ and any dual optimal $(\lambda^{\star},\nu^{\star})$ must satisfy the optimality conditions (or KKT conditions) 

$$
\begin{array}{r l r}{f_{i}(x^{\star})}&{{}\preceq_{K_{i}}}&{0,\quad i=1,\ldots,m}\\ {h_{i}(x^{\star})}&{{}=}&{0,\quad i=1,\ldots,p}\\ {\lambda_{i}^{\star}}&{{}\succeq_{K_{i}^{\star}}}&{0,\quad i=1,\ldots,m}\\ {\lambda_{i}^{\star^{T}}f_{i}(x^{\star})}&{{}=}&{0,\quad i=1,\ldots,m}\\ {\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}D f_{i}(x^{\star})^{T}\lambda_{i}^{\star}+\sum_{i=1}^{p}\nu_{i}^{\star}\nabla h_{i}(x^{\star})}&{{}=}&{0.}\end{array}
$$ 

If the primal problem is convex, the converse also holds, i.e. , the conditions ( 5.95 ) are sufficient conditions for optimality of $x^{\star}$ , $(\lambda^{\star},\nu^{\star})$ . 

# 5.9.3 Perturbation and sensitivity analysis 

The results of $\S5.6$ can be extended to problems involving generalized inequalities. We consider the associated perturbed version of the problem, 

$$
\begin{array}{l l}{\mathrm{minimize}}&{f_{0}(x)}\\ {\mathrm{subject~to}}&{f_{i}(x)\preceq_{K_{i}}u_{i},\quad i=1,\ldots,m}\\ &{h_{i}(x)=v_{i},\quad i=1,\ldots,p,}\end{array}
$$ 

where $u_{i}\,\in\,\mathbf{R}^{k_{i}}$ , and $v\,\,\in\,{\bf R}^{p}$ . We define $p^{\star}(u,v)$ as the optimal value of the perturbed problem. As in the case with scalar inequalities, $p^{\star}$ is a convex function when the original problem is convex. 

Now let $(\lambda^{\star},\nu^{\star})$ be optimal for the dual of the original (unperturbed) problem, which we assume has zero duality gap. Then for all $u$ and $v$ we have 

$$
p^{\star}(u,v)\geq p^{\star}-\sum_{i=1}^{m}\lambda_{i}^{\star T}u_{i}-\nu^{\star T}v,
$$ 

the analog of the global sensitivity inequality ( 5.57 ). The local sensitivity result holds as well: If $p^{\star}(u,v)$ is diﬀerentiable at $u\,=\,0$ , $v\,=\,0$ , then the optimal dual variables $\lambda_{i}^{\star}$ satisfies 

$$
\begin{array}{r}{\lambda_{i}^{\star}=-\nabla_{u_{i}}p^{\star}(0,0),}\end{array}
$$ 

the analog of ( 5.58 ). 

Example 5.13 Semidefinite program in inequality form. We consider a semidefinite program in inequality form, as in example 5.11 . The primal problem is 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad F(x)=x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n}$ (and $F_{1},.\,.\,.\,,F_{n},G\in\mathbf{S}^{k})$ ), and the dual problem is 

$$
{\begin{array}{l r l}&{{\mathrm{maximize}}}&{\mathbf{tr}(G Z)}\\ &{{\mathrm{subject~to}}}&{\mathbf{tr}(F_{i}Z)+c_{i}=0,\quad i=1,\ldots,n}\\ &{}&&{Z\succeq0,}\end{array}}
$$ 

with variable $Z\in\mathbf{S}^{k}$ 

Suppose that $x^{\star}$ and $Z^{\star}$ are primal and dual optimal, respectively, with zero duality gap. The complementary slackness condition is $\mathbf{tr}(F(x^{\star})Z^{\star})=0$ . Since $\boldsymbol{F}(\boldsymbol{x}^{\star})\preceq\boldsymbol{0}$ and $Z^{\star}\succeq0$ , we can conclude that $F(x^{\star})Z^{\star}=0$ . Thus, the complementary slackness condition can be expressed as 

$$
\begin{array}{r}{\mathcal{R}(F(x^{\star}))\perp\mathcal{R}(Z^{\star}),}\end{array}
$$ 

i.e. , the ranges of the primal and dual matrices are orthogonal. Let $p^{\star}(U)$ denote the optimal value of the perturbed SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ c^{T}x}\\ &{{\mathrm{subject~to}}\quad F(x)=x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq U.}\end{array}}
$$ 

Then we have, for all $U$ , $p^{\star}(U)\geq p^{\star}-\mathbf{tr}(Z^{\star}U)$ . If $p^{\star}(U)$ is diﬀerentiable at $U=0$ , then we have 

$$
\begin{array}{r}{\nabla p^{\star}(0)=-Z^{\star}.}\end{array}
$$ 

This means that for $U$ small, the optimal value of the perturbed SDP is very close to (the lower bound) $p^{\star}-\mathbf{tr}(Z^{\star}U)$ . 

# 5.9.4 Theorems of alternatives 

We can derive theorems of alternatives for systems of generalized inequalities and equalities 

$$
f_{i}(x)\preceq_{K_{i}}0,\quad i=1,.\,.\,.\,,m,\qquad h_{i}(x)=0,\quad i=1,.\,.\,.\,,p,
$$ 

where $K_{i}\subseteq\mathbf{R}^{k_{i}}$ are proper cones. We will also consider systems with strict in- equalities, 

$$
f_{i}(x)\prec_{K_{i}}0,\quad i=1,.\,.\,.\,,m,\qquad h_{i}(x)=0,\quad i=1,.\,.\,.\,,p.
$$ 

We assume that $\begin{array}{r}{\mathcal{D}=\bigcap_{i=0}^{m}\mathbf{dom}\,f_{i}\,\cap\,\bigcap_{i=1}^{p}\mathbf{dom}\,h_{i}}\end{array}$ ∩ is nonempty. 

# Weak alternatives 

We associate with the systems ( 5.96 ) and ( 5.97 ) the dual function 

$$
g(\lambda,\nu)=\operatorname*{inf}_{x\in\mathcal{D}}\left(\sum_{i=1}^{m}\lambda_{i}^{T}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x)\right)
$$ 

where $\lambda\,=\,(\lambda_{1},.\,.\,.\,,\lambda_{m})$ with $\lambda_{i}\,\in\,\mathbf{R}^{k_{i}}$ and $\nu\,\in\,\mathbf{R}^{p}$ . In analogy with ( 5.76 ), we claim that 

$$
\lambda_{i}\succeq_{K_{i}^{\star}}0,\quad i=1,\ldots,m,\qquad g(\lambda,\nu)>0
$$ 

is a weak alternative to the system ( 5.96 ). To verify this, suppose there exists an $x$ satisfying ( 5.96 ) and $(\lambda,\nu)$ satisfying ( 5.98 ). Then we have a contradiction: 

$$
\begin{array}{r}{0<g(\lambda,\nu)\leq\lambda_{1}^{T}f_{1}(x)+\cdot\cdot\cdot+\lambda_{m}^{T}f_{m}(x)+\nu_{1}h_{1}(x)+\cdot\cdot\cdot+\nu_{p}h_{p}(x)\leq0.}\end{array}
$$ 

Therefore at least one of the two systems ( 5.96 ) and ( 5.98 ) must be infeasible, i.e. , the two systems are weak alternatives. 

In a similar way, we can prove that ( 5.97 ) and the system 

$$
\lambda_{i}\succeq_{K_{i}^{*}}0,\quad i=1,\ldots,m,\qquad\lambda\neq0,\qquad g(\lambda,\nu)\geq0.
$$ 

form a pair of weak alternatives. 

# Strong alternatives 

We now assume that the functions $f_{i}$ are $K_{i}$ -convex, and the functions $h_{i}$ are affine. We first consider a system with strict inequalities 

$$
f_{i}(x)\prec_{K_{i}}0,\quad i=1,\ldots,m,\qquad A x=b,
$$ 

and its alternative 

$$
\lambda_{i}\succeq_{K_{i}^{\star}}0,\quad i=1,\ldots,m,\qquad\lambda\neq0,\qquad g(\lambda,\nu)\geq0.
$$ 

We have already seen that ( 5.99 ) and ( 5.100 ) are weak alternatives. They are also strong alternatives provided the following constraint qualification holds: There exists an $\tilde{x}\ \in\ \mathbf{relint}\,\mathcal{D}$ ∈ D with $\boldsymbol{A}\tilde{\boldsymbol{x}}\;=\;\boldsymbol{b}$ b . To prove this, we select a set of vectors $e_{i}\succ_{K_{i}}0$ , and consider the problem 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{s}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\preceq_{K_{i}}s e_{i},\quad i=1,\ldots,m}\\ &{A x=b}\end{array}}
$$ 

with variables $x$ and $s\in\mathbf{R}$ . Slater’ condition holds since $(\widetilde{x},\widetilde{s})$ ) satisfies the strict inequalities $f_{i}(\tilde{x})\prec_{K_{i}}\tilde{s}e_{i}$ provided ˜ is large enough. 

The dual of ( 5.101 ) is 

$$
\begin{array}{l l}{\mathrm{maximize}}&{g(\lambda,\nu)}\\ {\mathrm{subject\to}}&{\lambda_{i}\underbrace{\lor}_{=K_{i}^{*}}0,\quad i=1,.\,.\,.\,,m}\\ &{\sum_{i=1}^{m}e_{i}^{T}\lambda_{i}=1}\end{array}
$$ 

with variables $\lambda=(\lambda_{1},.\,.\,.\,,\lambda_{m})$ and $\nu$ . 

Now suppose the system ( 5.99 ) is infeasible. Then the optimal value of ( 5.101 ) is nonnegative. Since Slater’s condition is satisfied, we have strong duality and the dual optimum is attained. Therefore there exist $(\tilde{\lambda},\tilde{\nu})$ ) that satisfy the constraints of ( 5.102 ) and $g(\tilde{\lambda},\tilde{\nu})\ge0$ 0, i.e. , the system ( 5.100 ) has a solution 

we noted in the case of scalar inequalities, existence of an x $x\in\mathbf{relint}\,\mathcal{D}$ ∈ D with $A x=b$ is not sufficient for the system of nonstrict inequalities 

$$
f_{i}(x)\preceq_{K_{i}}0,\quad i=1,.\,.\,.\,,m,\qquad A x=b
$$ 

and its alternative 

$$
\lambda_{i}\succeq_{K_{i}^{\star}}0,\quad i=1,\ldots,m,\qquad g(\lambda,\nu)>0
$$ 

to be strong alternatives. An additional condition is required, e.g. , that the optimal value of ( 5.101 ) is attained. 

Example 5.14 Feasibility of a linear matrix inequality. The following systems are strong alternatives: 

$$
F(x)=x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\prec0,
$$ 

where $F_{i},G\in\mathbf{S}^{k}$ , and 

$$
Z\succeq0,\qquad Z\neq0,\qquad\mathbf{tr}(G Z)\geq0,\qquad\mathbf{tr}(F_{i}Z)=0,\quad i=1,\ldots,n,
$$ 

where $Z\,\in\,{\bf S}^{k}$ . This follows from the general result, if we take for $K$ the positive semidefinite cone $\mathbf{S}_{+}^{k}$ , and 

$$
g(Z)=\operatorname*{inf}_{x}\left(\mathbf{tr}(F(x)Z)\right)=\left\{{\begin{array}{l l}{\mathbf{tr}(G Z)}&{\mathbf{tr}(F_{i}Z)=0,\quad i=1,.\,.\,,n}\\ {-\infty}&{{\mathrm{otherwise.}}}\end{array}}\right.
$$ 

The nonstrict inequality case is slightly more involved, and we need an extra assump- tion on the matrices $F_{i}$ to have strong alternatives. One such condition is 

$$
\sum_{i=1}^{n}v_{i}F_{i}\succeq0\Longrightarrow\sum_{i=1}^{n}v_{i}F_{i}=0.
$$ 

If this condition holds, the following systems are strong alternatives: 

$$
F(x)=x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0
$$ 

and 

$$
Z\succeq0,\qquad\mathrm{\bf~tr}(G Z)>0,\qquad\mathrm{\bf~tr}(F_{i}Z)=0,\quad i=1,\ldots,n
$$ 

(see exercise 5.44 ). 

# Bibliography 

Lagrange duality is covered in detail by Luenberger [ Lue69 , chapter 8], Rockafellar [ Roc70 , part VI], Whittle [ Whi71 ], Hiriart-Urruty and Lemar´ echal [ HUL93 ], and Bertsekas, Nedi´ and Ozdaglar [ Ber03 ]. The name is derived from Lagrange’s method of multipliers for optimization problems with equality constraints; see Courant and Hilbert [ CH53 , chapter IV]. 

The max-min result for matrix games in § 5.2.5 predates linear programming duality. It is proved via a theorem of alternatives by von Neuman and Morgenstern [ vNM53 , page 153]. The strong duality result for linear programming on page 227 is due to von Neumann [ vN63 ] and Gale, Kuhn, and Tucker [ GKT51 ]. Strong duality for the nonconvex quadratic problem ( 5.32 ) is a fundamental result in the literature on trust region methods for nonlinear optimization (Nocedal and Wright [ NW99 , page 78]). It is also related to the S-procedure in control theory, discussed in appendix § B.1 . For an extension of the proof of strong duality of § 5.3.2 to the refined Slater condition ( 5.27 ), see Rockafellar [ Roc70 , page 277]. 

Conditions that guarantee the saddle-point property ( 5.47 ) can be found in Rockafel- lar [ Roc70 , part VII] and Bertsekas, Nedi´ c, and Ozdaglar [ Ber03 , chapter 2]; see also exercise 5.25 . 

The KKT conditions are named after Karush (whose unpublished 1939 Master’s thesis is summarized in Kuhn [ Kuh76 ]), Kuhn, and Tucker [ KT51 ]. Related optimality condi- tions were also derived by John [ Joh85 ]. The water-filling algorithm in example 5.2 has applications in information theory and communications (Cover and Thomas [ CT91 , page 252]). 

Farkas’ lemma was published by Farkas [ Far02 ]. It is the best known theorem of al- ternatives for systems of linear inequalities and equalities, but many variants exist; see Mangasarian [ Man94 , § 2.4]. The application of Farkas’ lemma to asset pricing (exam- ple 5.10 ) is discussed by Bertsimas and Tsitsiklis [ BT97 , page 167] and Ross [ Ros99 ]. 

The extension of Lagrange duality to problems with generalized inequalities appears in Isii [ Isi64 ], Luenberger [ Lue69 , chapter 8], Berman [ Ber73 ], and Rockafellar [ Roc89 , page 47]. It is discussed in the context of cone programming in Nesterov and Nemirovski [ NN94 , § 4.2] and Ben-Tal and Nemirovski [ BTN01 , lecture 2]. Theorems of alternatives for generalized inequalities were studied by Ben-Israel [ BI69 ], Berman and Ben-Israel [ BBI71 ], and Craven and Kohila [ CK77 ]. Bellman and Fan [ BF63 ], Wolkowicz [ Wol81 ], and Lasserre [ Las95 ] give extensions of Farkas’ lemma to linear matrix inequalities. 

# Exercises 

# Basic definitions 

5.1 A simple example. Consider the optimization problem 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{x^{2}+1}\\ {{\mathrm{subject~to}}}&{(x-2)(x-4)\leq0,}\end{array}}
$$ 

with variable $x\in\mathbf{R}$ . 

(a) Analysis of primal problem. Give the feasible set, the optimal value, and the optimal solution. (b) Lagrangian and dual function. Plot the objective $x^{2}+1$ versus $x$ . On the same plot, show the feasible set, optimal point and value, and plot the Lagrangian $L(x,\lambda)$ versus $x$ f w positive values of $\lambda$ . Verify the lower bound property $\displaystyle p^{\star}\geq\operatorname*{inf}_{x}L(x,\lambda)$ for $\lambda\geq0$ ≥ 0). Derive and sketch the Lagrange dual function $g$ . (c) Lagrange dual problem. State the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution $\lambda^{\star}$ . Does strong duality hold? 

(d) Sensitivity analysis. Let $p^{\star}(u)$ denote the optimal value of the problem 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{x^{2}+1}\\ &{{\mathrm{subject~to}}}&{(x-2)(x-4)\leq u,}\end{array}}
$$ 

as a function of the parameter $u$ . Plot $p^{\star}(u)$ . Verify that $d p^{\star}(0)/d u=-\lambda^{\star}$ . 

5.2 Weak duality for un nd i problems. The weak duality inequality, $d^{\star}\leq p^{\star}$ , clearly n d $d^{\star}=-\infty$ −∞ or p $p^{\star}=\infty$ w that it hol e other two cases as $p^{\star}=-\infty$ −∞ , then we must have d $d^{\star}=-\infty$ −∞ , and also, if d $d^{\star}=\infty$ ∞ , then we must have $p^{\star}=\infty$ . 

5.3 Problems with one inequality constraint. Express the dual problem of 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad f(x)\leq0,}\end{array}}
$$ 

with $c\neq0$ , in terms of the conjugate $f^{*}$ . Explain why the problem you give is convex. We do not assume $f$ is convex. 

# Examples and applications 

5.4 Interpretation of $L P$ dual via relaxed problems. Consider the inequality form LP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b,}\end{array}}
$$ 

with $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ . In this exercise we develop a simple geometric interpretation of the dual LP ( 5.22 ). 

Let $w\,\in\,\mathbf{R}_{+}^{m}$ . If $x$ is feasible for the LP, i.e. , satisfies $A x\preceq b$ , then it also satisfies the inequality 

$$
w^{T}A x\leq w^{T}b.
$$ 

Geometrically, for any $w\succeq0$ , the halfspace $H_{w}=\{x\mid w^{T}A x\leq w^{T}b\}$ contains the feasible set for the LP. Therefore if we minimize the objective c $c^{T}x$ over the halfspace $H_{w}$ we get a lower bound on $p^{\star}$ . 

(a) Derive an expression for the minimum value of $c^{T}x$ over the halfspace $H_{w}$ (which will depend on the choice of $w\succeq0$ ). (b) Formulate the problem of finding the best such bound, by maximizing the lower bound over $w\succeq0$ . (c) Relate the results of (a) and (b) to the Lagrange dual of the LP, given by ( 5.22 ). 

5.5 Dual of general $L P$ . Find the dual function of the LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad G x\preceq h}\\ &{A x=b.}\end{array}}
$$ 

Give the dual problem, and make the implicit equality constraints explicit. 

5.6 Lower bounds in Chebyshev approximation from least-squares. Consider the Chebyshev or $\ell_{\infty}$ -norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{\infty},
$$ 

where $A\,\in\,\mathbf{R}^{m\times n}$ and $\mathbf{rank}\,A\,=\,n$ . Let denote an optimal solution (there may be $x_{\mathrm{ch}}$ multiple optimal solutions; $x_{\mathrm{ch}}$ denotes one of them). 

The Chebyshev problem has no closed-form solution, but the corresponding least-squares problem does. Define 

$$
x_{\mathrm{ls}}=\operatorname{argmin}\|A x-b\|_{2}=(A^{T}A)^{-1}A^{T}b.
$$ 

We address the following question. Suppose that for a particular $A$ and $b$ we have com- puted the least-squares solution $x_{\mathrm{ls}}$ (but not $x_{\mathrm{ch}}$ ). How suboptimal is $x_{\mathrm{ls}}$ for the Chebyshev problem? In other words, how much larger is $\|A x_{\mathrm{ls}}-b\|_{\infty}$ than $\|A x_{\mathrm{ch}}-b\|_{\infty}$ ? 

(a) Prove the lower bound 

$$
\|A x_{\mathrm{ls}}-b\|_{\infty}\leq\sqrt{m}\,\|A x_{\mathrm{ch}}-b\|_{\infty},
$$ 

using the fact that for all $z\in\mathbf{R}^{m}$ , 

$$
{\frac{1}{\sqrt{m}}}\|z\|_{2}\leq\|z\|_{\infty}\leq\|z\|_{2}.
$$ 

(b) In example 5.6 (page 254 ) we derived a dual for the general norm approximation problem. Applying the results to the $\ell_{\infty}$ -norm (and its dual norm, the $\ell_{1}$ -norm), we can state the following dual for the Chebyshev approximation problem: 

$$
\begin{array}{l r c l}{{\mathrm{maximize}}}&{{b^{T}\nu}}\\ {{\mathrm{subject~to}}}&{{\|\nu\|_{1}\leq1}}\\ {{}}&{{A^{T}\nu=0.}}\end{array}
$$ 

Any feasible $\nu$ corresponds to a lower bound $b^{T}\nu$ on $\|A x_{\mathrm{ch}}-b\|_{\infty}$ . 

Denote the least-squares residual as $r_{\mathrm{ls}}=b-A x_{\mathrm{ls}}$ . Assuming $r_{\mathrm{ls}}\neq0$ , show that 

$$
\hat{\nu}=-r_{\mathrm{ls}}/\vert\vert r_{\mathrm{ls}}\vert\vert_{1},\qquad\tilde{\nu}=r_{\mathrm{ls}}/\vert\vert r_{\mathrm{ls}}\vert\vert_{1},
$$ 

are both feasible in ( 5.104 ). By duality $b^{T}\hat{\nu}$ and $b^{T}\tilde{\nu}$ are lower bounds on $\parallel\!A x_{\mathrm{ch}}\!\sim$ $b\|_{\infty}$ . Which is the better bound? How do these bounds compare with the bound derived in part (a)? 

5.7 Piecewise-linear minimization. We consider the convex piecewise-linear minimization problem 

$$
{\mathrm{minimize}}\quad\operatorname*{max}_{i=1,...,m}\bigl(a_{i}^{T}x+b_{i}\bigr)
$$ 

with variable $x\in\mathbf{R}^{n}$ . (a) Derive a dual problem, based on the Lagrange dual of the equivalent problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\,\operatorname*{max}_{i=1,\dots,m}y_{i}}\\ {\mathrm{subject~to}}&{\,a_{i}^{T}x+b_{i}=y_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

with variables $x\in\mathbf{R}^{n}$ , $y\in\mathbf{R}^{m}$ . 

(b) Formulate the piecewise-linear minimization problem ( 5.105 ) as an LP, and form the dual of the LP. Relate the LP dual to the dual obtained in part (a). 

(c) Suppose we approximate the objective function in ( 5.105 ) by the smooth function 

$$
f_{0}(x)=\log\left(\sum_{i=1}^{m}\exp(a_{i}^{T}x+b_{i})\right),
$$ 

and solve the unconstrained geometric program 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\log\left(\sum_{i=1}^{m}\exp(a_{i}^{T}x+b_{i})\right).}\end{array}
$$ 

A dual of this problem is given by ( ). Let $p_{\mathrm{pwl}}^{\star}$ and $p_{\mathrm{gr}}^{\star}$ be the optimal values of ( 5.105 ) and ( 5.106 ), respectively. Show that 

$$
0\leq p_{\mathrm{gr}}^{\star}-p_{\mathrm{pwr}}^{\star}\leq\log m.
$$ 

(d) Derive similar bounds for the diﬀerence between $p_{\mathrm{pwl}}^{\star}$ and the optimal value of 

$$
\begin{array}{r l}{\mathrm{minimize}\:}&{{}(1/\gamma)\log\left(\sum_{i=1}^{m}\exp(\gamma(a_{i}^{T}x+b_{i}))\right),}\end{array}
$$ 

where $\gamma>0$ is a parameter. What happens as we increase $\gamma$ ?

 5.8 Relate the two dual problems derived in example 5.9 on page 257 

5.9 Suboptimality of a simple covering ellipsoid. Recall the problem of determining the min- imum volume ellipsoid, centered at the origin, that contains the points $a_{1},.\,.\,.\,,a_{m}\,\in\,\mathbf{R}^{n}$ (problem ( 5.14 ), page 222 ): 

$$
\begin{array}{r l}{\mathrm{minimize}\quad}&{f_{0}(X)=\log\operatorname*{det}(X^{-1})}\\ {\mathrm{subject~to}\quad a_{i}^{T}X a_{i}\le1,\quad i=1,\ldots,m,}\end{array}
$$ 

with dom $f_{0}=\mathbf{S}_{++}^{n}$ . We assume that the vectors $a_{1},\dotsc,a_{m}$ span $\mathbf{R}^{n}$ (which implies that the problem is bounded below). 

(a) Show that the matrix 

$$
X_{\mathrm{sim}}=\left(\sum_{k=1}^{m}a_{k}a_{k}^{T}\right)^{-1},
$$ 

is feasible. Hint. Show that 

$$
\begin{array}{r}{\left[\begin{array}{c c}{\sum_{k=1}^{m}a_{k}a_{k}^{T}}&{a_{i}}\\ {a_{i}^{T}}&{1}\end{array}\right]\succeq0,}\end{array}
$$ 

and use Schur complements ( A.5.5 ) to prove that $a_{i}^{T}X a_{i}\leq1$ 1 for $i=1,\ldots,m$ . 

(b) Now we establish a bound on how suboptimal the feasible point $X_{\mathrm{sim}}$ is, via the dual problem, 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\log\operatorname*{det}\left(\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\right)-\mathbf{1}^{T}\boldsymbol{\lambda}+\boldsymbol{n}}\\ {\mathrm{subject~to}}&{\boldsymbol{\lambda}\succeq\mathbf{0},}\end{array}
$$ 

with th licit constraint $\begin{array}{r}{\sum_{i=1}^{m}\lambda_{i}a_{i}a_{i}^{T}\succ0}\end{array}$ 0. (This dual is derived on pag To derive a bound, we restrict our attention to dual variables of the form λ $\lambda\,=\,t{\bf1}$ , where t > $t\ >\ 0$ 0. Find (analytically) the optimal value of $t$ , and evaluate the dual objective at this $\lambda$ . Use this to prove that the volume of the ellipsoid $\{\boldsymbol{u}~|~\boldsymbol{u}^{T}\boldsymbol{X}_{\mathrm{sim}}\boldsymbol{u}\leq$ $1\}$ is no more than a factor $(m/n)^{n/2}$ more than the volume of the minimum volume ellipsoid. 

5.10 Optimal experiment design. The following problems arise in experiment design (see § 7.5 ). (a) $D$ -optimal design. 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\log\operatorname*{det}\left(\sum_{i=1}^{p}x_{i}v_{i}v_{i}^{T}\right)^{-1}}\\ {\mathrm{subject~to}}&{x\succeq0,\quad\mathbf{1}^{T}x=1.}\end{array}
$$ 

(b) A -optimal design. 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\,\mathbf{tr}\left(\sum_{i=1}^{p}x_{i}v_{i}v_{i}^{T}\right)^{-1}}\\ &{{\mathrm{subject~to}}\quad x\succeq0,\quad\mathbf{1}^{T}x=1.}\end{array}}
$$ 

The do roblems is $\{x\ \vert\ \sum_{i=1}^{p}x_{i}v_{i}v_{i}^{T}\,\succ\,0\}$ } . The variable is $x\,\in\,\mathbf{R}^{p}$ ; the n vectors $v_{1},\ldots,v_{p}\in\mathbf{R}^{n}$ ∈ are given. 

Derive dual problems by first introducing a new variable $X\,\in\,{\bf S}^{n}$ and an equality con- straint $\begin{array}{r}{X=\sum_{i=1}^{p}x_{i}v_{i}v_{i}^{T}}\end{array}$ , and then applying Lagrange duality. Simplify the dual prob- lems as much as you can. 

5.11 Derive a dual problem for 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{N}\|A_{i}{\boldsymbol x}+b_{i}\|_{2}+(1/2)\|{\boldsymbol x}-{\boldsymbol x}_{0}\|_{2}^{2}.}\end{array}
$$ 

lem data are $A_{i}\in\mathbf{R}^{m_{i}\times n}$ , $b_{i}\in\mathbf{R}^{m_{i}}$ d $x_{0}\in\mathbf{R}^{n}$ . First introduce new variables $y_{i}\in\mathbf{R}^{m_{i}}$ ∈ and equality constraints y $y_{i}=A_{i}x+b_{i}$ . 

5.12 Analytic centering. Derive a dual problem for 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x)}\end{array}
$$ 

with domain $\{x\mid a_{i}^{T}x<b_{i},\ i=1,.\,.\,.\,,m\}$ } . First introduce new variables $y_{i}$ and equality constraints $y_{i}=b_{i}-a_{i}^{T}x$ . 

(The solution of this problem is called the analytic center of the linear inequalities $a_{i}^{T}x\leq$ ≤ $b_{i}$ , $i=1,\cdot\cdot\cdot,m$ . Analytic centers have geometric applications (see § 8.5.3 ), and play an important role in barrier methods (see chapter 11 ).) 

5.13 Lagrangian relaxation of Boolean $L P$ . A Boolean linear program is an optimization prob- lem of the form 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{c^{T}x}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{x_{i}\in\{0,1\},\quad i=1,.\,.\,.\,,n,}\end{array}}
$$ 

and is, in general, very difficult to solve. In exercise 4.15 we studied the LP relaxation of this problem, 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{c^{T}x}\\ &{{\mathrm{subject~to}}}&{A x\preceq b}\\ &{}&{0\leq x_{i}\leq1,\quad i=1,.\,.\,.\,,n,}\end{array}}
$$ 

which is far easier to solve, and gives a lower bound on the optimal value of the Boolean LP. In this problem we derive another lower bound for the Boolean LP, and work out the relation between the two lower bounds. 

(a) Lagrangian relaxation. The Boolean LP can be reformulated as the problem 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{c^{T}x}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{x_{i}(1-x_{i})=0,\quad i=1,\ldots,n,}\end{array}}
$$ 

which has quadratic equality constraints. Find the Lagrange dual of this problem. The optimal value of the dual problem (which is convex) gives a lower bound on the optimal value of the Boolean LP. This method of finding a lower bound on the optimal value is called Lagrangian relaxation. 

(b) Show that the lower bound obtained via Lagrangian relaxation, and via the LP relaxation ( 5.107 ), are the same. Hint. Derive the dual of the LP relaxation ( 5.107 ). 

5.14 A penalty method for equality constraints. We consider the problem 

$$
\begin{array}{l c l}{\mathrm{minimize}}&{f_{0}(\boldsymbol{x})}\\ {\mathrm{subject~to}}&{A\boldsymbol{x}=\boldsymbol{b},}\end{array}
$$ 

where $f_{0}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex and diﬀerentiable, and $A\in\mathbf{R}^{m\times n}$ with rank $A=m$ . In a quadratic penalty method , we form an auxiliary function 

$$
\phi(x)=f_{0}(x)+\alpha\|A x-b\|_{2}^{2},
$$ 

where $\alpha\,>\,0$ is a parameter. This auxiliary function consists of the objective plus the penalty term $\alpha\|A x-b\|_{2}^{2}$ . The idea is that a minimizer of the auxiliary function, x , should be an approximate solution of the original problem. Intuition suggests that the larger the penalty weight $\alpha$ , the better the approximation x to a solution of the original problem. Suppose x is a minimizer of $\phi$ . Show how to find, from x , a dual feasible point for ( 5.108 ). Find the corresponding lower bound on the optimal value of ( 5.108 ). 

5.15 Consider the problem 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m,}\end{array}}
$$ 

where the functions $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are diﬀerentiable and convex. Let $h_{1},.\.\.,h_{m}:\mathbf{R}\rightarrow\mathbf{R}$ be increasing diﬀerentiable convex functions. Show that 

$$
\phi(x)=f_{0}(x)+\sum_{i=1}^{m}h_{i}(f_{i}(x))
$$ 

is convex. Suppose x minimizes $\phi$ . Show how to find from x a feasible point for the dual of ( 5.109 ). Find the corresponding lower bound on the optimal value of ( 5.109 ). 

5.16 An exact penalty method for inequality constraints. Consider the problem 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&&{f_{i}(x)\leq0,\quad i=1,\ldots,m,}\end{array}}
$$ 

where the functions $f_{i}\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ are diﬀerentiable and convex. In an exact penalty method, we solve the auxiliary problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad\phi(x)=f_{0}(x)+\alpha\operatorname*{max}_{i=1,\dots,m}\operatorname*{max}\{0,f_{i}(x)\},}\end{array}}
$$ 

where $\alpha>0$ is a parameter. The second term in $\phi$ penalizes deviations of $x$ from feasibility. The method is called an exact penalty method if for sufficiently large $\alpha$ , solutions of the auxiliary problem ( 5.111 ) also solve the original problem ( 5.110 ). 

(a) Show that $\phi$ is convex. 

(b) The auxiliary problem can be expressed as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)+\alpha y}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\leq y,\quad i=1,.\,.\,,m}\\ &{0\leq y}\end{array}}
$$ 

where the variables are $x$ and $y\in\mathbf{R}$ . Find the Lagrange dual of this problem, and express it in terms of the Lagrange dual function $g$ of ( 5.110 ). 

(c) Use the result in (b) to prove the following property. Suppose $\lambda^{\star}$ is an optimal solution of the Lagrange dual of ( 5.110 ), and that strong duality holds. If $\alpha\ >$ $\mathbf{1}^{T}\lambda^{\star}$ , then any solution of the auxiliary problem ( 5.111 ) is also an optimal solution of ( 5.110 ). 

5.17 Robust linear programming with polyhedral uncertainty. Consider the robust LP 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\ c^{T}x}\\ &{\mathrm{subject~to}\quad\operatorname*{sup}_{a\in\mathcal{P}_{i}}a^{T}x\leq b_{i},\quad i=1,\ldots,m,}\end{array}
$$ 

le $x\,\in\,\mathbf{R}^{\,n}$ , whe ${\mathcal P}_{i}\;=\;\{a\;\vert\;\,C_{i}a\;\preceq\;d_{i}\}$ . The pro em data are $c\,\in\,\mathbf{R}^{n}$ , $C_{i}\in\mathbf{R}^{m_{i}\times n}$ ∈ , d $d_{i}\in\mathbf{R}^{m_{i}}$ ∈ , and b $b\in\mathbf{R}^{m}$ ∈ . We assume the polyhedra P are nonempty. Show that this problem is equivalent to the LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad}&{d_{i}^{T}z_{i}\leq b_{i},\quad i=1,\ldots,m}\\ &{C_{i}^{T}z_{i}=x,\quad i=1,\ldots,m}\\ &{z_{i}\succeq0,\quad i=1,\ldots,m}\end{array}}
$$ 

with variables $x\in\mathbf{R}^{n}$ a $z_{i}\in\mathbf{R}^{m_{i}}$ , $i=1,\ldots,m$ . Hint. Find the dual of the problem of maximizing $a_{i}^{T}x$ over a $a_{i}\in\mathcal P_{i}$ ∈P (with variable a ). 

5.18 Separating hyperplane between two polyhedra. Formulate the following problem as an LP or an LP feasibility problem. Find a separating hyperplane that strictly separates two polyhedra 

$$
\mathcal{P}_{1}=\{x\mid A x\preceq b\},\qquad\mathcal{P}_{2}=\{x\mid C x\preceq d\},
$$ 

i.e. , find a vector $a\in\mathbf{R}^{n}$ and a scalar $\gamma$ such that 

$$
\boldsymbol{a}^{T}\boldsymbol{x}>\gamma\mathrm{~for~}\boldsymbol{x}\in\mathcal{P}_{1},\qquad\boldsymbol{a}^{T}\boldsymbol{x}<\gamma\mathrm{~for~}\boldsymbol{x}\in\mathcal{P}_{2}.
$$ 

You can assume that $\mathcal{P}_{1}$ and $\mathcal{P}_{2}$ do not intersect. Hint. The vector $a$ and scalar $\gamma$ must satisfy 

$$
\operatorname*{inf}_{x\in{\mathcal{P}}_{1}}a^{T}x>\gamma>\operatorname*{sup}_{x\in{\mathcal{P}}_{2}}a^{T}x.
$$ 

Use LP duality to simplify the infimum and supremum in these conditions.

 5.19 The sum of the largest elements of a vector. Define $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ as 

$$
f(x)=\sum_{i=1}^{r}x_{[i]},
$$ 

here $r$ is an integer between $^{1}$ and $n$ , and $x_{[1]}\geq x_{[2]}\geq\dots\geq x_{[r]}$ ar the components of x sorted in decreasing order. In other words, $f(x)$ ) is the sum of the r largest elements of $_{x}$ . In this problem we study the constraint 

$$
f(x)\leq\alpha.
$$ 

As we have seen in chapter 3 , page 80 , this is a convex constraint, and equivalent to a set of $n!/(r!(n-r)!)$ linear inequalities 

$$
x_{i_{1}}+\cdot\cdot\cdot+x_{i_{r}}\leq\alpha,\quad1\leq i_{1}<i_{2}<\cdot\cdot\cdot<i_{r}\leq n.
$$ 

The purpose of this problem is to derive a more compact representation. (a) Given a vector $x\in\mathbf{R}^{n}$ , show that $f(x)$ is equal to the optimal value of the LP 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\;x^{T}y}\\ {{\mathrm{subject~to}}}&{0\;{\preceq}\;y\;{\preceq}\;\mathbf{1}}\\ &{\;\mathbf{1}^{T}y=r}\end{array}}
$$ 

with $y\in\mathbf{R}^{n}$ as variable. 

(b) Derive the dual of the LP in part (a). Show that it can be written as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{r t+\mathbf{1}^{T}u}\\ {{\mathrm{subject~to}}}&{t\mathbf{1}+u\succeq x}\\ &{u\succeq0,}\end{array}}
$$ 

where the variables are $t\in\mathbf{R}$ $u\,\in\,\mathbf{R}^{\,n}$ . By duality this LP has the sam optimal value as the LP in (a), i.e. , $f(x)$ ). We therefore have the following result: x satisfies $f(x)\leq\alpha$ if and only if there exist $t\in\mathbf{R}$ , $u\in\mathbf{R}^{n}$ such that 

$$
r t+\mathbf{1}^{T}u\leq\alpha,\qquad t\mathbf{1}+u\succeq x,\qquad u\succeq0.
$$ 

These conditions form a set of $2n+1$ linear inequalities in the $2n+1$ variables $x,u,t$ . 

(c) As an application, we consider an extension of the classical Markowitz portfolio optimization problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ x^{T}\Sigma x}\\ &{{\mathrm{subject~to}}\quad{\overline{{p}}}^{T}x\geq r_{\mathrm{min}}}\\ &{\quad\mathbf{1}^{T}x=1,\quad x\succeq0}\end{array}}
$$ 

discussed in chapter 4 , page 155 . The variable is the portfolio $x\in\mathbf{R}^{n}$ ; $\overline{{p}}$ and $\Sigma$ are the mean and covariance matrix of the price change vector $p$ . 

Suppose we add a diversification constraint , requiring that no more than 80% of the total budget can be invested in any $10\%$ of the assets. This constraint can be expressed as 

$$
\sum_{i=1}^{\lfloor0.1n\rfloor}x_{[i]}\leq0.8.
$$ 

Formulate the portfolio optimization problem with diversification constraint as a QP. 

5.20 Dual of channel capacity problem. Derive a dual for the problem 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{-c^{T}x+\sum_{i=1}^{m}y_{i}\log y_{i}}\\ {{\mathrm{subject~to}}}&{P x=y}\\ &{x\succeq0,\quad\mathbf{1}^{T}x=1,}\end{array}}
$$ 

where $\textstyle P\in\mathbf{R}^{m\times n}$ has ative elements, and its columns add up to one ( i.e. , $P^{T}\mathbf{1}=$ 1). The variabre x$x\in\mathbf{R}^{n}$ ∈, $y\in\mathbf{R}^{m}$ (For $\begin{array}{r}{c_{j}=\sum_{e}^{m}\displaylimits_{i=1}^{n}p_{i j}\log p_{i j}}\end{array}$, the optimal value is,up to a factor log 2, the negative of the capacity of a discrete memoryless channel with channel transition probability matrix P ; see exercise 4.57 .) 

Simplify the dual problem as much as possible. 

# Strong duality and Slater’s condition 

5.21 A convex problem in which strong duality fails. Consider the optimization problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{e^{-x}}\\ {\operatorname{subject\to}\quad x^{2}/y\leq0}\end{array}}
$$ 

with variables $x$ and $y$ , and domain $\mathcal{D}=\{(x,y)\mid y>0\}$ . 

(a) Verify that this is a convex optimization problem. Find the optimal value. (b) Give the Lagrange dual problem, and find the optimal solution $\lambda^{\star}$ and optimal value $d^{\star}$ of the dual problem. What is the optimal duality gap? (c) Does Slater’s condition hold for this problem? (d) What is the optimal value $p^{\star}(u)$ of the perturbed problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{e^{-x}}\\ {\operatorname{subject\to}\quad x^{2}/y\leq u}\end{array}}
$$ 

as a function of $u$ ? Verify that the global sensitivity inequality 

$$
p^{\star}(u)\geq p^{\star}(0)-\lambda^{\star}u
$$ 

does not hold. 

5.22 Geometric interpretation of duality . For each of the following optimization problems, draw a sketch of the sets 

$$
\begin{array}{r l r}{\mathcal{G}}&{=}&{\{(u,t)\mid\exists x\in\mathcal{D},\ f_{0}(x)=t,\ f_{1}(x)=u\},}\\ {\mathcal{A}}&{=}&{\{(u,t)\mid\exists x\in\mathcal{D},\ f_{0}(x)\leq t,\ f_{1}(x)\leq u\},}\end{array}
$$ 

give the dual problem, and solve the primal and dual problems. Is the problem convex? Is Slater’s condition satisfied? Does strong duality hold? 

The domain of the problem is $\mathbf{R}$ unless otherwise stated. 

(a) Minimize $x$ subject to $x^{2}\leq1$ . (b) Minimize $x$ subject to $x^{2}\leq0$ . (c) Minimize $x$ subject to $|x|\leq0$ . (d) Minimize $x$ subject to $f_{1}(x)\leq0$ where 

$$
f_{1}(x)=\left\{\begin{array}{l l}{-x+2}&{x\geq1}\\ {x}&{-1\leq x\leq1}\\ {-x-2}&{x\leq-1.}\end{array}\right.
$$ 

(e) Minimize $x^{3}$ subject to $-x+1\leq0$ . 

(f) Minimize $x^{3}$ subject to $-x+1\leq0$ with domain $\mathcal{D}=\mathbf{R}_{+}$ 

5.23 Strong duality in linear programming. We prove that strong duality holds for the LP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b}\end{array}}
$$ 

and its dual 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}z}\\ &{\mathrm{subject~to}\quad A^{T}z+c=0,\quad z\succeq0,}\end{array}
$$ 

provided at least one of the problems is feasible. In other words, the only possible excep- tion to strong duality occurs when $p^{\star}=\infty$ and $d^{\star}=-\infty$ . 

(a) Suppose $p^{\star}$ is finite and $x^{\star}$ is an optimal solution. (If finite, the optimal value of an LP is attained.) Let $I\subseteq\{1,2,.\,.\,.\,,m\}$ be the set of active constraints at $x^{\star}$ : 

$$
a_{i}^{T}x^{\star}=b_{i},\quad i\in I,\quad\quad a_{i}^{T}x^{\star}<b_{i},\quad i\notin I.
$$ 

Show that there exists a $z\in\mathbf{R}^{m}$ that satisfies 

$$
z_{i}\geq0,\quad i\in I,\quad\quad z_{i}=0,\quad i\notin I,\quad\quad\sum_{i\in I}z_{i}a_{i}+c=0.
$$ 

Show that $z$ is dual optimal with objective value $c^{T}x^{\star}$ . 

Hint. Assume there exists no such $z$ , i.e. , − $-c\;\notin\;\{\sum_{i\in I}z_{i}a_{i}\;\mid\;z_{i}\;\geq\;0\}$ P . Reduce this to a contradiction by applying the strict separating hyperplane theorem of example 2.20 , page 49 . Alternatively, you can use Farkas’ lemma (see 5.8.3 ). 

(b) Suppose $p^{\star}=\infty$ and the dual problem is feas w that $d^{\star}=\infty$ . Hint. Show that there exists a nonzero $v\in\mathbf{R}^{m}$ such that A $A^{T}v=0$ = 0, $v\succeq0$ , $b^{T}v<0$ . If the dual is feasible, it is unbounded in the direction $v$ . 

(c) Consider the example 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\,x}\\ &{{\mathrm{subject~to}}\quad{\left[\begin{array}{l}{0}\\ {1}\end{array}\right]}\,x\preceq{\left[\begin{array}{l}{-1}\\ {\phantom{-}1}\end{array}\right]}\,.}\end{array}}
$$ 

Form dual LP, and solve the primal and dual problems. Show that $p^{\star}=\infty$ and d $d^{\star}=-\infty$ . 

5.24 Weak max-min inequality. Show that the weak max-min inequality 

$$
\operatorname*{sup}_{z\in Z}\operatorname*{inf}_{w\in W}f(w,z)\leq\operatorname*{inf}_{w\in W}\operatorname*{sup}_{z\in Z}f(w,z)
$$ 

always holds, with no assumptions on $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ , $W\subseteq\mathbf{R}^{n}$ , or $Z\subseteq\mathbf{R}^{m}$ . 

5.25 [ BL00 , page 95] Convex-concave functions and the saddle-point property. We derive con- ditions under which the saddle-point property 

$$
\operatorname*{sup}_{z\in Z}\operatorname*{inf}_{w\in W}f(w,z)=\operatorname*{inf}_{w\in W}\,\operatorname*{sup}_{z\in Z}f(w,z)
$$ 

holds, where $f:\mathbf{R}^{n}\times\mathbf{R}^{m}\rightarrow\mathbf{R}$ , $W\times Z\subseteq\mathbf{dom}\,f$ , and $W$ and $Z$ are nonempty. We will assume that the function 

$$
g_{z}(w)={\left\{\begin{array}{l l}{f(w,z)}&{w\in W}\\ {\infty}&{{\mathrm{otherwise}}}\end{array}\right.}
$$ 

is closed and convex for all $z\in{Z}$ , and the function 

$$
h_{w}(z)={\left\{\begin{array}{l l}{-f(w,z)}&{z\in Z}\\ {\infty}&{{\mathrm{otherwise}}}\end{array}\right.}
$$ 

is closed and convex for all $w\in W$ 

(a) The righthand side of ( 5.112 ) can be expressed as $p(0)$ , where 

$$
p(u)=\operatorname*{inf}_{w\in W}\,\operatorname*{sup}_{z\in Z}\,(f(w,z)+u^{T}z).
$$ 

Show that $p$ is a convex function. (b) Show that the conjugate of $p$ is given by 

$$
p^{*}(v)={\left\{\begin{array}{l l}{-\operatorname*{inf}_{w\in W}f(w,v)}&{v\in Z}\\ {\infty}&{{\mathrm{otherwise.}}}\end{array}\right.}
$$ 

(c) Show that the conjugate of $p^{*}$ is given by 

$$
p^{**}(u)=\operatorname*{sup}_{z\in Z}\operatorname*{inf}_{w\in W}(f(w,z)+u^{T}z).
$$ 

Combining this with (a), we can express the max-min equality ( 5.112 ) as $p^{**}(0)=$ $p(0)$ . 

(d) From exercises 3.28 and 3.39 (d we k ow that $p^{**}(0)\,=\,p(0)$ if $0\,\in\,\mathbf{int}\,\mathbf{dom}\,p$ . Conclude that this is the case if W and Z are bounded. (e) As another consequence of exercises 3.28 and 3.39 , we have $p^{**}(0)\,=\,p(0)$ if $0\,\in$ $\mathbf{dom}\,p$ and $p$ is closed. Show that $p$ is closed if the sublevel sets of $g_{z}$ are bounded. 

# Optimality conditions 

5.26 Consider the QCQP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ x_{1}^{2}+x_{2}^{2}}\\ {{\mathrm{subject~to}}}&{(x_{1}-1)^{2}+(x_{2}-1)^{2}\leq1}\\ &{(x_{1}-1)^{2}+(x_{2}+1)^{2}\leq1}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{2}$ 

(a) Sketch the feasible set and level sets of the objective. Find the optimal point $x^{\star}$ and optimal value $p^{\star}$ . (b) Give the KKT conditions. Do there exist Lagrange multipliers $\lambda_{1}^{\star}$ and $\lambda_{2}^{\star}$ that prove that $x^{\star}$ is optimal? (c) Derive and solve the Lagrange dual problem. Does strong duality hold? 

5.27 Equality constrained least-squares . Consider the equality constrained least-squares prob- lem 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\|A x-b\|_{2}^{2}}\\ {{\mathrm{subject~to}}}&{G x=h}\end{array}}
$$ 

where $A\in\mathbf{R}^{m\times n}$ with $\mathbf{rank}\,A=n$ , and $G\in\mathbf{R}^{p\times n}$ with $\mathbf{rank}\,G=p$ . 

Give the KKT conditions, and derive expressions for the primal solution $x^{\star}$ and the dual solution $\nu^{\star}$ . 

5.28 Prove (without using any linear programming code) that the optimal solution of the LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\quad47x_{1}+93x_{2}+17x_{3}-93x_{4}}\\ {{\mathrm{subject~to}}}&{\;{\left[\begin{array}{l l l l}{-1}&{\;\;-6}&{\;\;1}&{\;\;3}\\ {-1}&{\;\;-2}&{\;\;7}&{\;\;1}\\ {\;\;0}&{\;\;3}&{-10}&{-1}\\ {-6}&{-11}&{\;\;-2}&{\;\,12}\\ {\;\;1}&{\;\;6}&{-1}&{-3}\end{array}\right]}}{\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\\ {x_{3}}\\ {x_{4}}\end{array}\right]}\preceq{\left[\begin{array}{l}{-3}\\ {\;\;5}\\ {-8}\\ {-7}\\ {\;\;4}\end{array}\right]}}\end{array}}
$$ 

is unique, and given by $x^{\star}=(1,1,1,1)$ . 

5.29 The problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{-3x_{1}^{2}+x_{2}^{2}+2x_{3}^{2}+2\big(x_{1}+x_{2}+x_{3}\big)}}\\ {{\mathrm{subject~to}}}&{{x_{1}^{2}+x_{2}^{2}+x_{3}^{2}=1,}}\end{array}
$$ 

is a special case of ( 5.32 ), so strong duality holds even though the problem is not convex. Derive the KKT conditions. Find all solutions $x$ , $\nu$ that satisfy the KKT conditions. Which pair corresponds to the optimum? 

5.30 Derive the KKT conditions for the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\,\,\mathbf{tr}\,X-\log\operatorname*{det}X}\\ &{{\mathrm{subject~to}}\quad X s=y,}\end{array}}
$$ 

with variable $X\,\in\,{\bf S}^{n}$ and domain $\mathbf{S}_{++}^{n}$ . $y\,\in\,\mathbf{R}^{n}$ and $s\in\mathbf{R}^{n}$ are given, with $s^{T}y\,=\,1$ . Verify that the optimal solution is given by 

$$
X^{\star}=I+y y^{T}-\frac{1}{s^{T}s}s s^{T}.
$$ 

5.31 Supporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints, 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m.}\end{array}}
$$ 

Assume that $x^{\star}\in\mathbf{R}^{n}$ and $\boldsymbol{\lambda}^{\star}\in\mathbf{R}^{m}$ satisfy the KKT conditions 

$$
\begin{array}{r c l}{{f_{i}(x^{\star})}}&{{\leq}}&{{0,\quad i=1,\ldots,m}}\\ {{\lambda_{i}^{\star}}}&{{\geq}}&{{0,\quad i=1,\ldots,m}}\\ {{\lambda_{i}^{\star}f_{i}(x^{\star})}}&{{=}}&{{0,\quad i=1,\ldots,m}}\\ {{\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}\lambda_{i}^{\star}\nabla f_{i}(x^{\star})}}&{{=}}&{{0.}}\end{array}
$$ 

Show that 

$$
\nabla f_{0}\big(x^{\star}\big)^{T}\big(x-x^{\star}\big)\geq0
$$ 

for all feasible $x$ . In other words the KKT conditions imply the simple optimality criterion of $\S4.2.3$ . 

# Perturbation and sensitivity analysis 

5.32 Optimal value of perturbed problem. Let $f_{0},f_{1},.\,.\,.\,,f_{m}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ be convex. Show that the function 

$$
p^{\star}(u,v)=\operatorname*{inf}\{f_{0}(x)~|~\exists x\in{\mathcal{D}},~f_{i}(x)\leq u_{i},~i=1,\ldots,m,~A x-b=v\}
$$ 

is convex. This function is the optimal cost of the perturbed problem, as a function of the perturbations $u$ and $v$ (see 5.6.1 ). 

5.33 Parametrized $\ell_{1}$ -norm approximation. Consider the $\ell_{1}$ -norm minimization problem 

$$
\mathrm{minimize}\quad\|A x+b+\epsilon d\|_{1}
$$ 

with variable $x\in\mathbf{R}^{3}$ , and 

$$
A=\left[\begin{array}{r r r}{{-2}}&{{7}}&{{1}}\\ {{-5}}&{{-1}}&{{3}}\\ {{-7}}&{{3}}&{{-5}}\\ {{-1}}&{{4}}&{{-4}}\\ {{1}}&{{5}}&{{5}}\\ {{2}}&{{-5}}&{{-1}}\end{array}\right],\qquad b=\left[\begin{array}{r}{{-4}}\\ {{3}}\\ {{9}}\\ {{0}}\\ {{-11}}\\ {{5}}\end{array}\right],\qquad d=\left[\begin{array}{r}{{-10}}\\ {{-13}}\\ {{-27}}\\ {{-10}}\\ {{-7}}\\ {{14}}\end{array}\right].
$$ 

We denote by $p^{\star}(\epsilon)$ the optimal value as a function of $\epsilon$ . 

(a) Suppose $\epsilon=0$ . Prove that $x^{\star}=\mathbf{1}$ is optimal. Are there any other optimal points? (b) Show that $p^{\star}(\epsilon)$ is affine on an interval that includes $\epsilon=0$ . 

5.34 Consider the pair of primal and dual LPs 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ (c+\epsilon d)^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b+\epsilon f}\end{array}}
$$ 

and 

$$
\begin{array}{l r c l}{{\mathrm{maximize}}}&{{-(b+\epsilon f)^{T}z}}\\ {{\mathrm{subject~to}}}&{{A^{T}z+c+\epsilon d=0}}\\ &{{z\succeq0}}\end{array}
$$ 

where 

$$
A=\left[\begin{array}{r r r r}{-4}&{12}&{-2}&{1}\\ {-17}&{12}&{7}&{11}\\ {1}&{0}&{-6}&{1}\\ {3}&{3}&{22}&{-1}\\ {-11}&{2}&{-1}&{-8}\end{array}\right],\qquad b=\left[\begin{array}{r}{8}\\ {13}\\ {-4}\\ {27}\\ {-18}\end{array}\right],\qquad f=\left[\begin{array}{r}{6}\\ {15}\\ {-13}\\ {48}\\ {8}\end{array}\right],
$$ 

$c=(49,-34,-50,-5)$ , $d=(3,8,21,25)$ , and $\epsilon$ is a parameter. 

(a) Prove that $x^{\star}=(1,1,1,1)$ is optimal when $\epsilon=0$ , by constructing a dual optimal point $z^{\star}$ that has the same objective value as $x^{\star}$ . Are there any other primal or dual optimal solutions? 

(b) Give an explicit expression for the optimal value $p^{\star}(\epsilon)$ as a function of $\epsilon$ on an interval that contains $\epsilon=0$ . Specify the interval on which your expression is valid. Also give explicit expressions for the primal solution $x^{\star}(\epsilon)$ and the dual solution $z^{\star}(\epsilon)$ as a function of $\epsilon$ , on the same interval. Hint. First calculate $x^{\star}(\epsilon)$ and $z^{\star}(\epsilon)$ , assuming that the primal and dual constraints that are active at the optimum for $\epsilon=0$ , remain active at the optimum for values of $\epsilon$ around $0$ . Then verify that this assumption is correct. 

5.35 Sensitivity analysis for $G P s$ . Consider a GP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\leq1,\quad i=1,\ldots,m}\\ &{h_{i}(x)=1,\quad i=1,\ldots,p,}\end{array}}
$$ 

where $f_{0},\ldots,f_{m}$ are posynomials, $h_{1},.\,.\,.\,,h_{p}$ are monomials, and the domain of the prob- lem is $\mathbf{R}_{++}^{n}$ . We define the perturbed GP as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{f_{0}(x)}\\ {{\mathrm{subject~to}}\quad}&{f_{i}(x)\leq e^{u_{i}},\quad i=1,.\,.\,,m}\\ &{h_{i}(x)=e^{v_{i}},\quad i=1,.\,.\,,p,}\end{array}}
$$ 

and we denote the optimal value of the perturbed GP as $\boldsymbol{p}^{\star}(u,v)$ . We can think of $u_{i}$ and $v_{i}$ as relative, or fractional, perturbations of the constraints. For example, $u_{1}\,=\,-0.01$ corresponds to tightening the first inequality constraint by (approximately) 1%. 

Let $\lambda^{\star}$ and $\nu^{\star}$ be optimal dual variables for the convex form GP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\log f_{0}(y)}\\ {{\mathrm{subject~to}}\quad}&{\log f_{i}(y)\leq0,\quad i=1,\ldots,m}\\ &{\log h_{i}(y)=0,\quad i=1,\ldots,p,}\end{array}}
$$ 

with variables $y_{i}=\log x_{i}$ . Assuming that $\boldsymbol{p}^{\star}(u,v)$ is diﬀerentiable at $u=0$ , $v=0$ , relate $\lambda^{\star}$ and $\nu^{\star}$ to the derivatives of $\boldsymbol{p}^{\star}(u,v)$ at $u=0,\,v=0$ . Justify the statement “Relaxing the $i$ th constraint by $\alpha$ percent will give an improvement in the objective of around $\alpha\lambda_{i}^{\star}$ percent, for $\alpha$ small.” 

# Theorems of alternatives 

5.36 Alternatives for linear equalities. Consider the linear equations $A x=b$ , where $A\in\mathbf{R}^{m\times n}$ . From linear algebra we know that this equation has a n if and only $b\in{\mathcal{R}}(A)$ , which occurs if and only if $b\perp\mathcal{N}(A^{T})$ . In other words, Ax $\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ b has a solution if and only if there exists no $y\in\mathbf{R}^{m}$ such that $A^{T}y=0$ and $b^{T}y\neq0$ . Derive this result from the theorems of alternatives in § 5.8.2 . 

5.37 [ BT97 ] Existence of equilibrium distribution in finite state Markov chain . Let $P\in\mathbf{R}^{n\times n}$ be a matrix that satisfies 

$$
p_{i j}\geq0,\quad i,j=1,.\,.\,.\,,n,\qquad P^{T}{\bf1}={\bf1},
$$ 

i.e. , the coefficients are nonnegative and the columns sum to one. Use Farkas’ lemma to prove there exists a $y\in\mathbf{R}^{n}$ such that 

$$
P y=y,\qquad y\succeq0,\qquad\mathbf{1}^{T}y=1.
$$ 

(We can interpret $y$ as an equilibrium distribution of the Markov chain with $n$ states and transition probability matrix $P$ .) 

5.38 [ BT97 ] Option pricing. We apply the results of example 5.10 , page 263 , to a simple problem with three assets: a riskless asset with fixed return $r\,>\,1$ over the investment period of interest (for example, a bond), a stock, and an option on the stock. The option gives us the right to purchase the stock at the end of the period, for a predetermined price $K$ . 

We consider two scenarios. In the first scenario, the price of the stock goes up from $S$ at the beginning of the period, to $S u$ at the end of the period, where $u\,>\,r$ . In this scenario, we exercise the option only if $S u>K$ , in which case we make a profit of $S u-K$ . Otherwise, we do not exercise the option, and make zero profit. The value of the option at the end of the period, in the first scenario, is therefore $\operatorname*{max}\{0,S u-K\}$ . 

In the second scenario, the price of the stock goes down from $S$ to $S d$ , where $d<1$ . The value at the end of the period is $\operatorname*{max}\{0,S d-K\}$ . In the notation of example 5.10 , 

$$
V=\left[\begin{array}{l l l l l}{r}&{u S}&{\operatorname*{max}\{0,S u-K\}}\\ {r}&{d S}&{\operatorname*{max}\{0,S d-K\}}\end{array}\right],\qquad p_{1}=1,\qquad p_{2}=S,\qquad p_{3}=C,
$$ 

where $C$ is the price of the option. 

Show that for given $r$ , $S$ , $K$ , $_{u}$ , $d$ , the option price $C$ is uniquely determined by the no-arbitrage condition. In other words, the market for the option is complete. 

# Generalized inequalities 

5.39 SDP relaxations of two-way partitioning problem. We consider the two-way partitioning problem ( 5.7 ), described on page 219 , 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\boldsymbol x}^{T}{\boldsymbol W}{\boldsymbol x}}\\ {{\mathrm{subject~to}}}&{{\boldsymbol x}_{i}^{2}=1,\quad i=1,\ldots,n,}\end{array}
$$ 

with variable $x\,\in\,\mathbf{R}^{\,n}$ . The Lagrange dual of this (nonconvex) problem is given by the SDP 

$$
\begin{array}{l r c l}{{\mathrm{maximize}}}&{{-\mathbf{1}^{T}\nu}}\\ {{\mathrm{subject~to}}}&{{W+\mathbf{diag}(\nu)\succeq0}}\end{array}
$$ 

with variable $\nu\in\mathbf{R}^{n}$ . The optimal value of this SDP gives a lower bound on the optimal value of the partitioning problem ( 5.113 ). In this exercise we derive another SDP that gives a lower bound on the optimal value of the two-way partitioning problem, and explore the connection between the two SDPs. 

(a) Two-way partitioning problem in matrix form. Show that the two-way partitioning problem can be cast as 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{\operatorname{\mathbf{tr}}(W X)}\\ &{{\mathrm{subject~to}}}&{X\succeq0,\quad\operatorname{\mathbf{rank}}X=1}\\ &{}&{X_{i i}=1,\quad i=1,.\,.\,,n,}\end{array}}
$$ 

able $X~\in~\mathbf{S}^{n}$ Hint. Show that if $X$ is feasible, then it has the form $X=x x^{T}$ , where x $x\in\mathbf{R}^{n}$ satisfies $x_{i}\in\{-1,1\}$ (and vice versa). 

(b) SDP relaxation of two-way partitioning problem. Using the formulation in part (a), we can form the relaxation 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{\operatorname{\mathbf{tr}}(W X)}\\ &{{\mathrm{subject~to}}}&{X\succeq0}\\ &{}&{X_{i i}=1,\quad i=1,\dots,n,}\end{array}}
$$ 

with variable $X\,\in\,{\bf S}^{n}$ . This problem is an SDP, and therefore can be solved effi- ciently. Explain why its optimal value gives a lower bound on the optimal value of the two-way partitioning problem ( 5.113 ). What can you say if an optimal point $X^{\star}$ for this SDP has rank one? 

(c) We now have two SDPs that give a lower bound on the optimal value of the two-way partitioning problem ( 5.113 ): the SDP relaxation ( 5.115 ) found in part (b), and the Lagrange dual of the two-way partitioning problem, given in ( 5.114 ). What is the relation between the two SDPs? What can you say about the lower bounds found by them? Hint: Relate the two SDPs via duality. 

5.40 $E$ -optimal experiment design. A variation on the two optimal experiment design problems of exercise 5.10 is the $E$ -optimal design problem 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\:\lambda_{\mathrm{max}}\left(\sum_{i=1}^{p}x_{i}v_{i}v_{i}^{T}\right)^{-1}}\\ &{\mathrm{subject~to}\quad x\succeq0,\quad\mathbf{1}^{T}x=1.}\end{array}
$$ 

(See also $\S7.5$ .) Derive a dual for this problem, by first reformulating it as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\;1/t}\\ {{\mathrm{subject~to}}}&{\sum_{i=1}^{p}x_{i}v_{i}v_{i}^{T}\succeq t I}\\ &{x\succeq0,\quad\mathbf{1}^{T}x=1,}\end{array}}
$$ 

with variables $t\,\in\,\mathbf{R}$ , $x\,\in\,\mathbf{R}^{p}$ and domain $\mathbf{R}_{++}\,\times\,\mathbf{R}^{p}$ , and applying Lagrange duality. Simplify the dual problem as much as you can. 

5.41 Dual of fastest mixing Markov chain problem. On page 174 , we encountered the SDP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{t}\\ {{\mathrm{subject~to}}\quad}&{-t I\preceq P-(1/n)\mathbf{1}\mathbf{1}^{T}\preceq t I}\\ &{P\mathbf{1}=\mathbf{1}}\\ &{P_{i j}\geq0,\quad i,j=1,\ldots,n}\\ &{P_{i j}=0{\mathrm{~for~}}(i,j)\not\in{\mathcal{E}},}\end{array}}
$$ 

with variables $t\in\mathbf{R}$ , $P\in\mathbf{S}^{n}$ 

Show that the dual of this problem can be expressed as 

$$
\begin{array}{r l}{\mathrm{maximize}\,}&{\mathbf{1}^{T}z-(1/n)\mathbf{1}^{T}Y\mathbf{1}}\\ {\mathrm{subject~to}\,}&{\|Y\|_{2*}\leq1}\\ &{(z_{i}+z_{j})\leq Y_{i j}\mathrm{~for~}(i,j)\in\mathcal{E}}\end{array}
$$ 

wit ariables $z\,\in\mathbf{R}^{n}$ and $Y\,\in\,{\bf S}^{n}$ . The norm $||\cdot||_{2^{*}}$ is the dual of the spectral no on S $\mathbf{S}^{n}$ : $\begin{array}{r}{\|Y\|_{2*}\,=\,\sum_{i=1}^{n}|\lambda_{i}(Y)|}\end{array}$ | , the sum of the absolute values of the eigenvalues of Y . (See § A.1.6 , page 637 .) 

5.42 Lagrange dual of conic form problem in inequality form. Find the Lagrange dual problem of the conic form problem in inequality form 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad A x\preceq_{K}b}\end{array}}
$$ 

where $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , and $K$ is a proper cone in $\mathbf{R}^{m}$ . Make any implicit equality constraints explicit. 

5.43 Dual of $S O C P$ . Show that the dual of the SOCP 

$$
\begin{array}{l r}{\mathrm{minimize}\ }&{f^{T}x}\\ {\mathrm{subject~to}\ }&{\|A_{i}x+b_{i}\|_{2}\leq c_{i}^{T}x+d_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

with variables $x\in\mathbf{R}^{n}$ , can be expressed as 

$$
\begin{array}{r l}{\mathrm{maximize}}&{~\sum_{i=1}^{m}(b_{i}^{T}u_{i}-d_{i}v_{i})}\\ {\mathrm{subject~to}}&{~\sum_{i=1}^{m}(A_{i}^{T}u_{i}-c_{i}v_{i})+f=0}\\ &{||u_{i}||_{2}\leq v_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

a $u_{i}\in\mathbf{R}^{n_{i}}$ $v_{i}\in\mathbf{R}$ $i=1,\ldots,m$ . The problem data are $f\in\mathbf{R}^{n}$ , $A_{i}\in\mathbf{R}^{n_{i}\times n}$ , $b_{i}\in\mathbf{R}^{n_{i}}$ ∈ , c $c_{i}\in\mathbf{R}$ ∈ and d $d_{i}\in\mathbf{R}$ ∈ , i $i=1,\ldots,m$ . Derive the dual in the following two ways. 

(a) e new variables $y_{i}\,\in\,\mathbf{R}^{n_{i}}$ and $t_{i}\,\in\,{\bf R}$ and equalities $y_{i}\,=\,A_{i}x\,+\,b_{i},\ t_{i}\,=$ $c_{i}^{T}x+d_{i}$ , and derive the Lagrange dual. (b) Start from the conic formulation of the SOCP and use the conic dual. Use the fact that the second-order cone is self-dual. 

5.44 Strong alternatives for nonstrict LMIs. In example 5.14 , page 270 , we mentioned that the system 

$$
Z\succeq0,\qquad\mathrm{\bf~tr}(G Z)>0,\qquad\mathrm{\bf~tr}(F_{i}Z)=0,\quad i=1,\ldots,n,
$$ 

is a strong alternative for the nonstrict LMI 

$$
F(x)=x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0,
$$ 

if the matrices $F_{i}$ satisfy 

$$
\sum_{i=1}^{n}v_{i}F_{i}\succeq0\implies\sum_{i=1}^{n}v_{i}F_{i}=0.
$$ 

In this exercise we prove this result, and give an example to illustrate that the systems are not always strong alternatives. 

(a) Suppose ( 5.118 ) holds, and that the optimal value of the auxiliary SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ s}\\ &{{\mathrm{subject~to}}\quad F(x)\preceq s I}\end{array}}
$$ 

is positive. Show that the optimal value is attained. If follows from the discussion in 5.9.4 that the systems ( 5.117 ) and ( 5.116 ) are strong alternatives. 

Hint. The proof simplifies if you assume, without loss of generality, that the matrices $F_{1}$ , . . $F_{n}$ are independent, so ( 5.118 ) may be replaced by $\textstyle\sum_{i=1}^{n}v_{i}F_{i}\succeq0\Rightarrow v=0$ ⪰ (b) Take n = 1, and 

$$
G=\left[\begin{array}{l l}{0}&{1}\\ {1}&{0}\end{array}\right],\qquad F_{1}=\left[\begin{array}{l l}{0}&{0}\\ {0}&{1}\end{array}\right].
$$ 

Show that ( 5.117 ) and ( 5.116 ) are both infeasible. 

# Part II 

# Applications 

# Chapter 6 

# Approximation and fitting 

# 6.1 Norm approximation 

# 6.1.1 Basic norm approximation problem 

The simplest norm approximation problem is an unconstrained problem of the form 

$$
{\mathrm{minimize}}\quad\|A x-b\|
$$ 

where $A\in\mathbf{R}^{m\times n}$ and $b\in\mathbf{R}^{m}$ are problem data, $x\in\mathbf{R}^{n}$ is the variable, and $||\cdot||$ is a norm on $\mathbf{R}^{m}$ . A solution of the norm approximation problem is sometimes called an approximate solution of $A x\approx b$ , in the norm $||\cdot||$ . The vector 

$$
\boldsymbol{r}=\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b}
$$ 

is called the residual for the problem; its components are sometimes called the individual residuals associated with $x$ . 

The norm approximation problem ( 6.1 ) is a convex problem, and is solvable, $i$ .e. , there is always at least one optimal solution. Its optimal value is zero if and only if $b\in{\mathcal{R}}(A)$ ; the problem is more interesting and useful, however, hen $b\,\notin\,{\mathcal{R}}(A)$ . We can assume without loss of generality that the columns of A are endent; in particular, that $m\geq n$ . When $n=n$ the optimal point is simply $A^{-1}b$ , so we can assume that $m>n$ . 

# Approximation interpretation 

By expressing $A x$ as 

$$
A x=x_{1}a_{1}+\cdot\cdot\cdot+x_{n}a_{n},
$$ 

where $a_{1},.\,.\,.\,,a_{n}\,\in\,\mathbf{R}^{m}$ are the columns of $A$ , we see that the goal of the norm approximation problem is to fit or approximate the vector $b$ by a linear combination of the columns of $A$ , as closely as possible, with deviation measured in the norm $||\cdot||$ . 

The approximation problem is also called the regression problem . In this context the vectors $a_{1},\dotsc,a_{n}$ are called the regressors , and the vector $x_{1}a_{1}+\cdot\cdot\cdot+x_{n}a_{n}$ , where $x$ is an optimal solution of the problem, is called the regression of $b$ (onto the regressors). 

# Estimation interpretation 

A closely related interpretation of the norm approximation problem arises in the problem of estimating a parameter vector on the basis of an imperfect linear vector measurement. We consider a linear measurement model 

$$
y=A x+v,
$$ 

where $y\,\in\,\mathbf{R}^{m}$ ctor measurement, $x\,\in\,\mathbf{R}^{n}$ is a vector of parameters to be estimated, and v $v\in\mathbf{R}^{m}$ ∈ is some measurement error that is unknown, but presumed to be small (in the norm $\|\cdot\|$ ). The estimation problem is to make a sensible guess as to what $x$ is, given $y$ . 

If we guess that $x$ has the value x , then we are implicitly making the guess that $v$ has the value $y-A\hat{x}$ . Assuming that smaller values of $v$ (measured by $\|\cdot\|$ ) are more plausible than larger values, the most plausible guess for $x$ is 

$$
\begin{array}{r}{\hat{x}=\operatorname*{argmin}_{z}\lVert A z-y\rVert.}\end{array}
$$ 

(These ideas can be expressed more formally in a statistical framework; see chap- ter 7 .) 

# Geometric interpretation 

We consider the subspace $\mathcal{A}=\mathcal{R}(A)\subseteq\mathbf{R}^{m}$ , and a point $b\in\mathbf{R}^{m}$ . A projection of th point $b$ onto the subspace A , in the norm $\|\cdot\|$ , is any point in A that is closest to b , i.e. , any optimal point for the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\|u-b\|}\\ {{\mathrm{subject~to}}}&{u\in{\mathcal{A}}.}\end{array}}
$$ 

Parametrizing an arbitrary element of ${\mathcal{R}}(A)$ as $u\,=\,A x$ , we see that solving th norm approximation problem ( 6.1 ) is equivalent to computing a projection of b onto $\mathcal{A}$ . 

# Design interpretation 

We can interpret the norm approximation problem ( 6.1 ) as a problem of optimal design. The $n$ variables $x_{1},\dots,x_{n}$ are design variables whose values are to be determined. The vector $y=A x$ gives a vector of $m$ results , which we assume to be linear functions of the design variables $x$ . The vector $b$ is a vector of target or desired results . The goal is to choose a vector of design variables that achieves, as closely as possible, the desired results, i.e. , $A x\approx b$ . We can erpret the residual vector $r$ as the deviation between the actual results ( i.e. , Ax ) and the desired or target results ( i.e. , $b$ ). If we measure the quality of a design by the norm of the deviation between the actual results and the desired results, then the norm approximation problem ( 6.1 ) is the problem of finding the best design. 

# Weighted norm approximation problems 

An extension of the norm approximation problem is the weighted norm approxima- tion problem 

$$
{\mathrm{minimize}}\quad\|W(A x-b)\|
$$ 

where the problem data $W\in\mathbf{R}^{m\times m}$ is called the weighting matrix . The weight- ing matrix is often diagonal, in which case it gives diﬀerent relative emphasis to diﬀerent components of the residual vector $\boldsymbol{r}=\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b}$ . 

The weighted norm problem can be considered as a norm approximation prob- lem with norm $\lVert\cdot\rVert$ , and data ${\tilde{A}}=W A$ , $\dot{b}=W b$ , and therefore treated as a standard norm approximation problem ( 6.1 ). Alternatively, the weighted norm approxima- tion problem can be considered a norm approximation problem with data $A$ and $b$ , and the $W$ -weighted norm defined by 

$$
\|z\|_{W}=\|W z\|
$$ 

(assuming here that $W$ is nonsingular). 

# Least-squares approximation 

The most common norm approximation problem involves the Euclidean or $\ell_{2}$ - norm. By squaring the objective, we obtain an equivalent problem which is called the least-squares approximation problem , 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}=r_{1}^{2}+r_{2}^{2}+\cdot\cdot\cdot+r_{m}^{2},
$$ 

where the objective is the sum of squares of the residuals. This problem can be solved analytically by expressing the objective as the convex quadratic function 

$$
f(x)=x^{T}A^{T}A x-2b^{T}A x+b^{T}b.
$$ 

A point $x$ minimizes $f$ if and only if 

$$
\nabla f(x)=2A^{T}A x-2A^{T}b=0,
$$ 

i.e. , if and only if $x$ satisfies the so-called normal equations 

$$
\boldsymbol{A}^{T}\boldsymbol{A}\boldsymbol{x}=\boldsymbol{A}^{T}\boldsymbol{b},
$$ 

which always have a solution. Since we assume the columns of $A$ are independent, the least-squares approximation problem has the unique solution $x=(A^{T}A)^{-1}A^{T}b$ . 

# Chebyshev or minimax approximation 

When the $\ell_{\infty}$ -norm is used, the norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{\infty}=\operatorname*{max}\{|r_{1}|,.\,.\,,|r_{m}|\}
$$ 

is called the Chebyshev approximation problem , or minimax approximation problem , since we are to minimize the maximum (absolute value) residual. The Chebyshev approximation problem can be cast as an LP 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&&{t}\\ &{{\mathrm{subject~to}}}&{-t\mathbf{1}\preceq A x-b\preceq t\mathbf{1},}\end{array}}
$$ 

with variables $x\in\mathbf{R}^{n}$ and $t\in\mathbf{R}$ . 

# Sum of absolute residuals approximation 

When the $\ell_{1}$ -norm is used, the norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{1}=|r_{1}|+\cdot\cdot\cdot+|r_{m}|
$$ 

is called the sum of (absolute) residuals approximation problem, or, in the context of estimation, a robust estimator (for reasons that will be clear soon). Like the Chebyshev approximation problem, the $\ell_{1}$ -norm approximation problem can be cast as an LP 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{\mathbf{1}^{T}t}\\ {{\mathrm{subject~to}}}&{-t\preccurlyeq A x-b\preceq t,}\end{array}}
$$ 

with variables $x\in\mathbf{R}^{n}$ and $t\in\mathbf{R}^{m}$ . 

# 6.1.2 Penalty function approximation 

In $\ell_{p}$ -norm approximation, for $1\leq p<\infty$ , the objective is 

$$
\big(|r_{1}|^{p}+\cdot\cdot\cdot+|r_{m}|^{p}\big)^{1/p}\,.
$$ 

As in least-squares problems, we can consider the equivalent problem with objective 

$$
|r_{1}|^{p}+\cdot\cdot\cdot+|r_{m}|^{p},
$$ 

which is a separable and symmetric function of the residuals. In particular, the objective depends only on the amplitude distribution of the residuals, i.e. , the residuals in sorted order. 

We will consider a useful generalization of the $\ell_{p}$ -norm approximation problem, in which the objective depends only on the amplitude distribution of the residuals. The penalty function approximation problem has the form 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{\phi(r_{1})+\cdot\cdot\cdot+\phi(r_{m})}\\ {\operatorname{subject\to}}&{r=A x-b,}\end{array}}
$$ 

where $\phi:\mathbf{R}\rightarrow\mathbf{R}$ is called the (residual) penalty function . We assume that $\phi$ is convex, so the penalty function approximation problem is a convex optimization problem. In many cases, the penalty function $\phi$ is symmetric, nonnegative, and satisfies $\phi(0)=0$ , but we will not use these properties in our analysis. 

# Interpretation 

We can interpret the penalty function approximation problem ( 6.2 ) as follows. For the choice $x$ , we obtain the approximation $A x$ of $b$ , which has the associated resid- ual vector $r$ . A penalty function assesses a cost or penalty for each component of residual, given by $\phi(\boldsymbol{r}_{i})$ ; the total penalty is the sum of the penalties for each residual, i.e. , $\phi(r_{1})+\cdot\cdot\cdot+\phi(r_{m})$ . Diﬀerent choices of $x$ lead to diﬀerent resulting residuals, and therefore, diﬀerent total penalties. In the penalty function approxi- mation problem, we minimize the total penalty incurred by the residuals. 

![](images/531a43ccd2c84744c60553e3588962c1a9da91f6fe5a06922c30d37dedaa3ff9.jpg) 
Figure 6.1 Some common penalty functions: the quadratic penalty function $\phi(u)=u^{2}$ , the deadzone-linear penalty function with deadzone width $a=$ $1/4$ , and the log barrier penalty function with limit $a=1$ . 

Example 6.1 Some common penalty functions and associated approximation problems. 

• By taking $\phi(u)=|u|^{p}$ , where $p\geq1$ , the penalty function approximation prob- lem is equivalent to the $\ell_{p}$ -norm approximation problem. In particular, the quadratic penalty function $\phi(u)\,=\,u^{2}$ yields least-squares or Euclidean norm approximation, and the absolute value penalty function $\phi(u)\,=\,|u|$ yields $\ell_{1}$ - norm approximation. 

• The deadzone-linear penalty function (with deadzone width $a>0$ ) is given by 

$$
\phi(u)=\left\{\begin{array}{l l}{0}&{|u|\leq a}\\ {|u|-a}&{|u|>a.}\end{array}\right.
$$ 

The deadzone-linear function assesses no penalty for residuals smaller than $a$ . 

• The log barrier penalty function (with limit $a>0$ ) has the form 

$$
\phi(u)=\left\{\begin{array}{l l}{-a^{2}\log(1-(u/a)^{2})}&{|u|<a}\\ {\infty}&{|u|\ge a.}\end{array}\right.
$$ 

The log barrier penalty function assesses an infinite penalty for residuals larger than $a$ . 

A deadzone-linear, log barrier, and quadratic penalty function are plotted in fig- ure 6.1 . Note that the log barrier function is very close to the quadratic penalty for $|u/a|\le0.25$ (see exercise 6.1 ). 

Scaling the penalty function by a positive number does not aﬀect the solution of the penalty function approximation problem, since this merely scales the objective function. But the shape of the penalty function has a large eﬀect on the solution of the penalty function approximation problem. Roughly speaking, $\phi(u)$ is a measure of our dislike of a residual of value $u$ . If $\phi$ is very small (or even zero) for small values of $u$ , it means we care very little (or not at all) if residuals have these values. If $\phi(u)$ grows rapidly as $u$ becomes large, it means we have a strong dislike for large residuals; if $\phi$ becomes infinite outside some interval, it means that residuals outside the interval are unacceptable. This simple interpretation gives insight into the solution of a penalty function approximation problem, as well as guidelines for choosing a penalty function. 

As an example, let us compare $\ell_{1}$ -norm and $\ell_{2}$ -norm approximation, associ- ated with the penalty functions $\phi_{1}(u)\,=\,|u|$ and $\phi_{2}(u)\:=\:u^{2}$ , respectively. For

 $|u|=1$ , the two penalty functions assign the same penalty. For small $u$ we have

 $\phi_{1}(u)\gg\phi_{2}(u)$ , so $\ell_{1}$ -norm approximation puts relatively larger emphasis on small residuals compared to $\ell_{2}$ -norm approximation. For large $u$ we have $\phi_{2}(u)\gg\phi_{1}(u)$ , so $\ell_{1}$ -norm approximation puts less weight on large residuals, compared to $\ell_{2}$ -norm approximation. This diﬀerence in relative weightings for small and large residuals is reﬂected in the solutions of the associated approximation problems. The ampli- tude distribution of the optimal residual for the $\ell_{1}$ -norm approximation problem will tend to have more zero and very small residuals, compared to the $\ell_{2}$ -norm ap- proximation solution. In contrast, the $\ell_{2}$ -norm solution will tend to have relatively fewer large residuals (since large residuals incur a much larger penalty in $\ell_{2}$ -norm approximation than in $\ell_{1}$ -norm approximation). 

# Example 

ple will illustrate these ideas. We take a matrix $A\in\mathbf{R}^{100\times30}$ and vector $b\in\mathbf{R}^{100}$ ∈ (chosen at random, but the res typical), and compute the $\ell_{1}$ -norm and ℓ $\ell_{2}$ -norm approximate solutions of Ax $A x\:\approx\:b$ ≈ , as we penalty function 2 approximations with a deadzone-linear penalty (with a $a\ =\ 0.5$ 5) and log barrier penalty (with $a\ =\ 1$ ). Figure 6.2 shows the four associated penalty functions, and the amplitude distributions of the optimal residuals for these four penalty approximations. From the plots of the penalty functions we note that 

• The $\ell_{1}$ -norm penalty puts the most weight on small residuals and the least weight on large residuals. • The $\ell_{2}$ -norm penalty puts very small weight on small residuals, but strong weight on large residuals. • The deadzone-linear penalty function puts no weight on residuals smaller than 0 . 5, and relatively little weight on large residuals. • The log barrier penalty puts weight very much like the $\ell_{2}$ -norm penalty for small residuals, but puts very strong weight on residuals larger than around 0 . 8, and infinite weight on residuals larger than 1. Several features are clear from the amplitude distributions: 

• For the $\ell_{1}$ -optimal solution, many residuals are either zero or very small. The $\ell_{1}$ -optimal solution also has relatively more large residuals. 

![](images/4e66f3774deb029c4876a8c2ff2abb02e9effc125861b2d4644e363373a4b68f.jpg) 
Figure 6.2 Histogram of residual amplitudes for four penalty functions, with the (scaled) penalty functions also shown for reference. For the log barrier plot, the quadratic penalty is also shown, in dashed curve. 

![](images/1296972c23737fbae24edce6747a4194037347c162507f80eb1695d277e48f7f.jpg) 
Figure 6.3 A (nonconvex) penalty function that assesses a fixed penalty to residuals larger than a threshold (which in this example is one): $\phi(u)=u^{2}$ if $|u|\leq1$ and $\phi(u)=1$ if $|u|>1$ . As a result, penalty approximation with this function would be relatively insensitive to outliers. 

• The $\ell_{2}$ -norm approximation has many modest residuals, and relatively few larger ones. • For the deadzone-linear penalty, we see that many residuals have the value ± 0 . 5, right at the edge of the ‘free’ zone, for which no penalty is assessed. • For the log barrier penalty, we see that no residuals have a magnitude larger than 1, but otherwise the residual distribution is similar to the residual dis- tribution for $\ell_{2}$ -norm approximation. 

# Sensitivity to outliers or large errors 

In the estimation or regression context, an outlier is a measurement $y_{i}=a_{i}^{T}x+v_{i}$ for which the noise $v_{i}$ is relatively large. This is often associated with faulty data or a ﬂawed measurement. When outliers occur, any estimate of $x$ will be associated with a residual vector with some large components. Ideally we would like to guess which measurements are outliers, and either remove them from the estimation process or greatly lower their weight in forming the estimate. (We cannot, however, assign zero penalty for very large residuals, because then the optimal point would likely make all residuals large, which yields a total penalty of zero.) This could be accomplished using penalty function approximation, with a penalty function such as 

$$
\phi(u)=\left\{\begin{array}{l l}{u^{2}}&{|u|\leq M}\\ {M^{2}}&{|u|>M,}\end{array}\right.
$$ 

shown in figure 6.3 . This penalty function agrees with least-squares for any residual smaller than $M$ , but puts a fixed weight on any residual larger than $M$ , no matter how much larger it is. In other words, residuals larger than $M$ are ignored; they are assumed to be associated with outliers or bad data. Unfortunately, the penalty 

![](images/501b928c8e92de2805d685ab3774c9b87b51c1bd6d3c7b36af42e11fd06938cb.jpg) 
Figure 6.4 The solid line is the robust least-squares or Huber penalty func- tion $\phi_{\mathrm{hub}}$ , with $M\,=\,1$ . For $|u|\,\leq\,M$ it is quadratic, and for $|u|>M$ it grows linearly. 

function ( 6.3 ) is not convex, and the associated penalty function approximation problem becomes a hard combinatorial optimization problem. 

The sensitivity of a penalty function based estimation method to outliers de- pends on the (relative) value of the penalty function for large residuals. If we restrict ourselves to convex penalty functions (which result in convex optimization problems), the ones that are least sensitive are those for which $\phi(u)$ grows linearly, i.e. , like $|u|$ , for large $u$ . Penalty functions with this property are sometimes called robust , since the associated penalty function approximation methods are much less sensitive to outliers or large errors than, for example, least-squares. 

One obvious example of a robust penalty function is $\phi(u)=|u|$ , corresponding to $\ell_{1}$ -norm approximation. Another example is the robust least-squares or Huber penalty function , given by 

$$
\phi_{\mathrm{hub}}(u)=\left\{\begin{array}{l l}{u^{2}}&{|u|\leq M}\\ {M(2|u|-M)}&{|u|>M,}\end{array}\right.
$$ 

shown in figure 6.4 . This penalty function agrees with the least-squares penalty function for residuals smaller than $M$ , and then reverts to $\ell_{1}$ -like linear growth for larger residuals. The Huber penalty function can be considered a convex approx- imation of the outlier penalty function ( 6.3 ), in the following sense: They agree for $|u|\leq M$ , and for $|u|>M$ , the Huber penalty function is the convex function closest to the outlier penalty function ( 6.3 ). 

Example 6.2 Robust regression. Figure 6.5 shows 42 points $(t_{i},y_{i})$ in a plane, with two obvious outliers (one at the upper left, and one at lower right). The dashed line shows the least-squares approximation of the points by a straight line $f(t)=\alpha+\beta t$ . The coefficients $\alpha$ and $\beta$ are obtained by solving the least-squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{42}\bigl(y_{i}-\alpha-\beta t_{i}\bigr)^{2},}\end{array}
$$ 

![](images/3fb2cd950a4ac56dadc276be1d6b9366cdad453d801c1909164241907193a1cd.jpg) 
Figure 6.5 The 42 circles show points that can be well approximated by an affine function, except for the two outliers at upper left and lower right. The dashed line is the least-squares fit of a straight line $f(t)\;=\;\alpha+\beta t$ to the points, and is rotated away from the main locus of points, toward the outliers. The solid line shows the robust least-squares fit, obtained by minimizing Huber’s penalty function with $M=1$ . This gives a far better fit to the non-outlier data. 

with variables $\alpha$ and $\beta$ . The least-squares approximation is clearly rotated away from the main locus of the points, toward the two outliers. 

The solid line shows the robust least-squares approximation, obtained by minimizing the Huber penalty function 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{42}\phi_{\mathrm{hub}}\big(y_{i}-\alpha-\beta t_{i}\big),}\end{array}
$$ 

with $M=1$ . This approximation is far less aﬀected by the outliers. 

Since $\ell_{1}$ -norm approximation is among the (convex) penalty function approxi- mation methods that are most robust to outliers, $\ell_{1}$ -norm approximation is some- times called robust estimation or robust regression . The robustness property of $\ell_{1}$ -norm estimation can also be understood in a statistical framework; see page 353 . 

# Small residuals and $\ell_{1}$ -norm approximation 

We can also focus on small residuals. Least-squares approximation puts very small weight on small residuals, since $\phi(u)=u^{2}$ is very small when $u$ is small. Penalty functions such as the deadzone-linear penalty function put zero weight on small residuals. For penalty functions that are very small for small residuals, we expect the optimal residuals to be small, but not very small. Roughly speaking, there is little or no incentive to drive small residuals smaller. 

In contrast, penalty functions that put relatively large weight on small residuals, such as $\phi(u)~=~|u|$ , corresponding to $\ell_{1}$ -norm approximation, tend to produce optimal residuals many of which are very small, or even exactly zero. This means that in $\ell_{1}$ -norm approximation, we typically find that many of the equations are satisfied exactly, i.e. , we have $a_{i}^{T}x=b_{i}$ for many $i$ . This phenomenon can be seen in figure 6.2 . 

# 6.1.3 Approximation with constraints 

It is possible to add constraints to the basic norm approximation problem ( 6.1 ). When these constraints are convex, the resulting problem is convex. Constraints arise for a variety of reasons. 

• In an approximation problem, constraints can be used to rule out certain un- acceptable approximations of the vector $b$ , or to ensure that the approximator $A x$ satisfies certain properties. • In an estimation problem, the constraints arise as prior knowledge of the vector $x$ to be estimated, or from prior knowledge of the estimation error $v$ . • Constraints arise in a geometric setting in determining the projection of a point $b$ on a set more complicated than a subspace, for example, a cone or polyhedron. 

Some examples will make these clear. 

# Nonnegativity constraints on variables 

We can add the constraint $x\succeq0$ to the basic norm approximation problem: 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\|A x-b\|}\\ &{{\mathrm{subject~to}}\quad x\succeq0.}\end{array}}
$$ 

In an estimation setting, nonnegativity constraints arise when we estimate a vector $x$ of parameters known to be nonnegative, e.g. , powers, intensities, or rates. The geometric interpretation is that we are determining the projection of a vector $b$ onto the cone generated by the columns of $A$ . We can also interpret this problem as approximating $b$ using a nonnegative linear ( i.e. , conic) combination of the columns of $A$ . 

# Variable bounds 

Here we add the constraint $l\preceq x\preceq u$ , where $l,\ u\in\mathbf{R}^{r_{u}}$ are problem parameters: 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{\|A x-b\|}\\ {{\mathrm{subject~to}}}&{l\preceq x\preceq u.}\end{array}}
$$ 

In an estimation setting, variable bounds arise as prior knowledge of intervals in which each variable lies. The geometric interpretation is that we are determining the projection of a vector $b$ onto the image of a box under the linear mapping induced by $A$ . 

# Probability distribution 

We can impose the constraint that $x$ satisfy $x\succeq0$ , ${\bf1}^{T}x=1$ : 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\|A x-b\|}\\ &{{\mathrm{subject~to}}\quad x\succeq0,\quad\mathbf{1}^{T}x=1.}\end{array}}
$$ 

This would arise in the estimation of proportions or relative frequencies, which are nonnegative and sum to one. It can also be interpreted as approximating $b$ by a convex combination of the columns of $A$ . (We will have much more to say about estimating probabilities in 7.2 .) 

# Norm ball constraint 

We can add to the basic norm approximation problem the constraint that $x$ lie in a norm ball: 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\|A x-b\|}\\ {{\mathrm{subject~to}}}&{\|x-x_{0}\|\leq d,}\end{array}}
$$ 

where and $d$ are problem parameters. Such a constraint can be added for several $x_{0}$ reasons. 

• In an estimation setting, is a prior guess of what the parameter is, and $d$ $x_{0}$ $x$ is the maximum plausible deviation of our estimate from our prior guess. Our estimate of the parameter $x$ is the value x which best matches the measured data ( i.e. , minimizes $\|A z-b\|.$ ) among all plausible candidates ( i.e. , $\mathcal{Z}$ that satisfy $\Vert z-x_{0}\Vert\leq d$ ). • The constraint $\|x\!-\!x_{0}\|\leq d$ can denote a trust region . Here the linear relation $y=A x$ is only an approximation of some nonlinear relation $y=f(x)$ that is valid when $x$ is near some point $x_{0}$ , specifically $\|x-x_{0}\|\leq d$ . The problem is to minimize $\|A x-b\|$ but only over those $x$ for which the model $y=A x$ is trusted. 

These ideas also come up in the context of regularization; see § 6.3.2 . 

# 6.2 Least-norm problems 

The basic least-norm problem has the form 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\|x\|}\\ {\operatorname{subject\to}}&{A x=b}\end{array}}
$$ 

where th ata are $A\in\mathbf{R}^{m\times n}$ and $b\in\mathbf{R}^{m}$ , the variable is $x\in\mathbf{R}^{n}$ , and $||\cdot||$ is a n norm on R . A solution of the problem, which always exists if the linear equations $A x=b$ have a solution, is called a least-norm solution of $A x=b$ . The least-norm problem is, of course, a convex optimization problem. 

We can assume without loss of generality that the rows of $A$ are independent, so $m\leq n$ . When $m=n$ , the only feasible point is $x=A^{-1}b$ least-norm problem is interesting only when $m<n$ , i.e. , when the equation Ax $A x=b$ b is underdetermined. 

# Reformulation as norm approximation problem 

The least-norm problem ( 6.5 ) can be formulated as a norm approximation problem by eliminating the equality constraint. Let $x_{0}$ be any solution of $A x=b$ , and let $Z\,\in\,\mathbf{R}^{n\times k}$ be a m hose columns are a basis for the nullspac $A$ . The general solution of $A x=b$ can then be expressed as $x_{0}+Z u$ where u $u\in\mathbf{R}^{k}$ ∈ . The least-norm problem ( 6.5 ) can be expressed as 

$$
{\mathrm{minimize}}\quad\|x_{0}+Z u\|,
$$ 

with variable $u\,\in\,\mathbf{R}^{k}$ , which is a norm approximation problem. In particular, our analysis and discussion of norm approximation problems applies to least-norm problems as well (when interpreted correctly). 

# Control or design interpretation 

We can interpret the least-norm problem ( 6.5 ) as a problem of optimal design or optimal control. The $n$ variables $x_{1},\dots,x_{n}$ are design variables whose values are to be determined. In a control setting, the variables $x_{1},\dots,x_{n}$ represent inputs, whose values we are to choose. The vector $y=A x$ gives $m$ attributes or results of the design $x$ , which we assume to be linear functions of the design variables $x$ . The $m<n$ equations $A x=b$ represent $m$ specifications or requirements on the design. Since $m\,<\,n$ , the design is underspecified; there are $n-m$ degrees of freedom in the design (assuming $A$ is rank $m$ ). 

Among all the designs that satisfy the specifications, the least-norm problem chooses the smallest design, as measured by the norm $\|\cdot\|$ . This can be thought of as the most efficient design, in the sense that it achieves the specifications $A x=b$ , with the smallest possible $x$ . 

# Estimation interpretation 

We assume that $x$ is a vector of parameters to be estimated. We have $m\,<\,n$ perfect (noise free) linear measurements, given by $\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ . Since we have fewer measurements than parameters to estimate, our measurements do not completely determine $x$ . Any parameter vector $x$ that satisfies $A x=b$ is consistent with our measurements. 

To make a good guess about what $x$ is, without taking further measurements, we must use prior information. Suppose our prior information, or assumption, is that $x$ is more likely to be small (as measured by $||\cdot||$ ) than large. The least-norm problem chooses as our estimate of the parameter vector $x$ the one that is smallest (hence, most plausible) among all parameter vectors that are consistent with the measurements $A x=b$ . (For a statistical interpretation of the least-norm problem, see page 359 .) 

# Geometric interpretation 

We can also give a simple geometric interpretation of the least-norm problem ( 6.5 ). The feasible set $\{x\mid A x=b\}$ is affine, and th objective is the distance (measured by the norm $||\cdot||$ ) between $x$ and the point 0. The least-norm problem finds the point in the affine set with minimum distance to $0$ , i.e. , it determines the projection of the point 0 on the affine set $\{x\mid A x=b\}$ . 

# Least-squares solution of linear equations 

The most common least-norm problem involves the Euclidean or $\ell_{2}$ -norm. By squaring the objective we obtain the equivalent problem 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|x\|_{2}^{2}}\\ {\mathrm{subject~to}}&{A x=b,}\end{array}
$$ 

the unique solution of which is called the least-squares solution of the equations $A x=b$ . Like the least-squares approximation problem, this problem can be solved analytically. Introducing the dual variable $\nu\in\mathbf{R}^{m}$ , the optimality conditions are 

$$
2x^{\star}+A^{T}\nu^{\star}=0,\qquad A x^{\star}=b,
$$ 

which is a pair of linear equations, and readily solved. From the first equation we obtain $x^{\star}=-(1/2)A^{T}\nu^{\star}$ ; substituting this into the second equation we obtain

 $-(1/2)A A^{T}\nu^{\star}=b$ , and conclude 

$$
\nu^{\star}=-2(A A^{T})^{-1}b,\qquad x^{\star}=A^{T}(A A^{T})^{-1}b.
$$ 

(Since rank $A=m<n$ , the matrix $A A^{T}$ is invertible.) 

# Least-penalty problems 

A useful variation on the least-norm problem ( 6.5 ) is the least-penalty problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\ }&{\phi(x_{1})+\cdot\cdot\cdot+\phi(x_{n})}\\ {{\mathrm{subject~to}}}&{A x=b,}\end{array}}
$$ 

where $\phi:\mathbf{R}\rightarrow\mathbf{R}$ is convex, nonnegative, and satisfies $\phi(0)\,=\,0$ . The penalty function value $\phi(u)$ quantifies our dislike of a component of $x$ having value $u$ ; the least-penalty problem then finds $x$ that has least total penalty, subject to the constraint $A x=b$ . 

All of the discussion and interpretation of penalty functions in penalty function approximation can be transposed to the least-penalty problem, by substituting the amplitude distribution of $x$ (in the least-penalty problem) for the amplitude distribution of the residual $r$ (in the penalty approximation problem). 

# Sparse solutions via least $\ell_{1}$ -norm 

Recall from the discussion on page 300 that $\ell_{1}$ -norm approximation gives relatively large weight to small residuals, and therefore results in many optimal residuals small, or even zero. A similar eﬀect occurs in the least-norm context. The least $\ell_{1}$ -norm problem, 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|x\|_{1}}\\ {\mathrm{subject~to}}&{A x=b,}\end{array}
$$ 

tends to produce a solution $x$ with a large number of components equal to zero. In other words, the least $\ell_{1}$ -norm problem tends to produce sparse solutions of $A x=b$ , often with $m$ nonzero components. 

It is easy to find solutions of $\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ that have only $m$ nonzero components. Choose any set of $m$ indices (out of $1,\cdot\cdot\cdot,n$ ) which are to be the nonzero com- ponents of $x$ . The equation $\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ reduces to $\boldsymbol{\dot{A}}\boldsymbol{\tilde{x}}\,=\,\boldsymbol{b}$ , where $\tilde{A}$ $m\times m$ submatrix of $A$ obtained by selecting only the chosen columns, and ˜ $\tilde{x}\in\mathbf{R}^{m}$ ∈ is the subvector of $x$ containing the $m$ selected components. If A is nonsingular, then $\tilde{x}\,=\,\tilde{A}^{-1}b$ we can take , which gives a feasible solution $x$ with $m$ or less nonzero components. If $b\,\notin\,\mathcal{R}(\tilde{A})$ ), the equation $\tilde{A}\tilde{x}\,=\,b$ A is singular and is unsolvable, which means there is no feasible $x$ with the chosen set of nonzero components. If $\tilde{A}$ is singular and $b\in\mathcal{R}(\tilde{A})$ ), there is a feasible solution with fewer than $m$ nonzero components. 

This approach can be used to find the smallest $x$ with $m$ (or fewer) nonzero entries, but in general requires examining and comparing all $n!/(m!(n\!-\!m)!)$ choices of $m$ nonzero coefficients of the $n$ coefficients in $x$ . Solving the least $\ell_{1}$ -norm problem, on the other hand, gives a good heuristic for finding a sparse, and small, solution of $A x=b$ . 

# 6.3 Regularized approximation 

# 6.3.1 Bi-criterion formulation 

In the basic form of regularized approximation, the goal is to find a vector $x$ that is small (if possible), and also makes the residual $A x-b$ small. This is naturally described as a (convex) vector optimization problem with two objectives, $\|A x-b\|$ and $\|x\|$ : 

$$
\mathrm{minimize}\ (\mathrm{w.r.t.}\ \mathbf{R}_{+}^{2})\quad(\|A x-b\|,\|x\|)\,.
$$ 

The two norms can be diﬀerent: the first, used to measure the size of the residual, is on $\mathbf{R}^{m}$ ; the second, used to measure the size of $x$ , is on $\mathbf{R}^{n}$ . 

The optimal trade-oﬀbetween the two objectives can be found using several methods. The optimal trade-oﬀcurve of $||A x-b||$ versus $\|x\|$ , which shows how large one of the objectives must be made to have the other one small, can then be plotted. One endpoint of the optimal trade-oﬀcurve between $||A x-b||$ and $\|x\|$ y to describe. The minimum value of $\|x\|$ is zero, and is achieved only when = 0. For this value of $x$ , the residual norm has the value $\lVert b\rVert$ . 

The other endpoint of the trade-oﬀcurve is more complicated to describe. Let $C$ denote the set of mini zers of $\vert\vert A x-b\vert\vert$ (with no constraint on $\|x\|$ ). Then any minimum norm point in C is Pareto optimal, corresponding to the other endpoint of the trade-oﬀcurve. In other words, Pareto optimal points at this endpoint are given by minimum norm minimizers of $\vert\vert A x-b\vert\vert$ norms Euclidean, this Pareto optimal point is unique, and given by x $x\,=\,A^{\dagger}b$ , where A $A^{\dagger}$ is the pseudo- inverse of $A$ . (See § 4.7.6 , page 184 , and $\S$ A.5.4 .) 

# 6.3.2 Regularization 

Regularization is a common scalarization method used to solve the bi-criterion problem ( 6.7 ). One form of regularization is to minimize the weighted sum of the objectives: 

$$
{\mathrm{minimize}}\quad\|A x-b\|+\gamma\|x\|,
$$ 

where $\gamma>0$ is a problem parameter. As $\gamma$ varies over $(0,\infty)$ , the solution of ( 6.8 ) traces out the optimal trade-oﬀcurve. 

Another common method of regularization, especially when the Euclidean norm is used, is to minimize the weighted sum of squared norms, i.e. , 

$$
{\mathrm{minimize}}\quad\|A x-b\|^{2}+\delta\|x\|^{2},
$$ 

for a variety of values of $\delta>0$ . 

These regularized approximation problems each solve the bi-criterion problem of making both $||A x\textrm{--}b||$ and $\|x\|$ small, by adding an extra term or penalty associated with the norm of $x$ . 

# Interpretations 

Regularization is used in several contexts. In an estimation setting, the extra term penalizing large $\|x\|$ can be interpreted as our prior knowledge that $\|x\|$ is not too large. In an optimal design setting, the extra term adds the cost of using large values of the design variables to the cost of missing the target specifications. 

The constraint that $\|x\|$ be small can also reﬂect a modeling issue. It might be, for example, that $y\,=\,A x$ is only a good approximation of the true relationship $y=f(x)$ between $x$ and $y$ . In order to have $f(x)\approx b$ , we want $A x\approx b$ , and also need $x$ small in order to ensure that $f(x)\approx A x$ . 

We will see in § 6.4.1 and § 6 .2 that regularization can be used to take into account variation in the matrix A . Roughly speaking, a large $x$ is one for which variation in $A$ causes large variation in $A x$ , and hence should be avoided. 

Regularization is also used when the matrix $A$ is square, and the goal is to solve the linear equations $A x=b$ . In cases where $A$ is poorly conditioned, or even singular, regularization gives a compromise between solving the equations ( i.e. , making $||A x-b||$ zero) and keeping $x$ of reasonable size. 

Regularization comes up in a statistical setting; see 7.1.2 . 

# Tikhonov regularization 

The most common form of regularization is based on ( 6.9 ), with Euclidean norms, which results in a (convex) quadratic optimization problem: 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}+\delta\|x\|_{2}^{2}=x^{T}(A^{T}A+\delta I)x-2b^{T}A x+b^{T}b.
$$ 

This Tikhonov regularization problem has the analytical solution 

$$
\boldsymbol{x}=(A^{T}A+\delta\boldsymbol{I})^{-1}A^{T}\boldsymbol{b}.
$$ 

Since $A^{T}A+\delta I\succ0$ for any $\delta>0$ , the Tikhonov regularized east-squares solution requires no rank (or dimension) assumptions on the matrix A . 

# Smoothing regularization 

The idea of regularization, i.e. , adding to the objective a term that penalizes large $x$ , can be extended in several ways. In one useful extension we add a regularization term of the form $\|D x\|$ , in place of $\|x\|$ . In many applications, the matrix $D$ represents an approximate diﬀerentiation or second-order diﬀerentiation operator, so $\|D x\|$ represents a measure of the variat moothness of $x$ . 

For example, suppose that the vector x $x\,\in\,\mathbf{R}^{\,n}$ ∈ represents the value of some continuous physical parameter, say, temperature, along the interval $[0,1]$ : $x_{i}$ is the temperature at the point $i/n$ . A simple approximation of the gradient or first derivative of the parameter near $i/n$ is given by $n(x_{i+1}\mathrm{~-~}x_{i})$ , and a simple approximation of its second derivative is given by the second diﬀerence 

$$
n\left(n(x_{i+1}-x_{i})-n(x_{i}-x_{i-1})\right)=n^{2}(x_{i+1}-2x_{i}+x_{i-1}).
$$ 

If $\Delta$ is the (tridiagonal, Toeplitz) matrix 

$$
\Delta=n^{2}\left[\begin{array}{c c c c c c c c c}{1}&{-2}&{1}&{0}&{\cdots}&{0}&{0}&{0}&{0}\\ {0}&{1}&{-2}&{1}&{\cdots}&{0}&{0}&{0}&{0}\\ {0}&{0}&{1}&{-2}&{\cdots}&{0}&{0}&{0}&{0}\\ {\vdots}&{\vdots}&{\vdots}&{\vdots}&{}&{\vdots}&{\vdots}&{\vdots}&{\vdots}\\ {0}&{0}&{0}&{0}&{\cdots}&{-2}&{1}&{0}&{0}\\ {0}&{0}&{0}&{0}&{\cdots}&{1}&{-2}&{1}&{0}\\ {0}&{0}&{0}&{0}&{\cdots}&{0}&{1}&{-2}&{1}\end{array}\right]\in\mathbf{R}^{(n-2)\times n},
$$ 

then $\Delta x$ represents an approximation of the second derivative of the parameter, so $||\Delta x||_{2}^{2}$ represents a measure of the mean-square curvature of the parameter over the interval $[0,1]$ . 

The Tikhonov regularized problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\|A x-b\|_{2}^{2}+\delta\|\Delta x\|_{2}^{2}}\end{array}
$$ 

can be used to trade oﬀthe objective $\vert\vert A x-b\vert\vert^{2}$ , which might represent a measure of fit, or consistency with experimental data, and the objective $||\Delta x||^{2}$ , which is (approximately) the mean-square curvature of the underlying physical parameter. The parameter $\delta$ is used to control the amount of regularization required, or to plot the optimal trade-oﬀcurve of fit versus smoothness. 

We can also add several regularization terms. For example, we can add terms associated with smoothness and size, as in 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\|A x-b\|_{2}^{2}+\delta\|\Delta x\|_{2}^{2}+\eta\|x\|_{2}^{2}.}\end{array}
$$ 

Here, the parameter $\delta\geq0$ is used to control the smoothness of the approximate solution, and the parameter $\eta\geq0$ is used to control its size. 

Example 6.3 Optimal input design. We consider a dynamical system with scalar input sequence $u(0)$ , $u(1),\ldots,u(N)$ , and scalar output sequence $y(0)$ , $y(1),\ldots,y(N)$ , related by convolution: 

$$
y(t)=\sum_{\tau=0}^{t}h(\tau)u(t-\tau),\quad t=0,1,\ldots,N.
$$ 

The sequence $h(0)$ , $h(1),\ldots,h(N)$ is called the convolution kernel or impulse response of the system. 

Our goal is to choose the input sequence $u$ to achieve several goals. 

• Output tracking. The primary goal is that the output $y$ should track, or follow, a desired target or reference signal $y_{\mathrm{des}}$ . We measure output tracking error by the quadratic function 

$$
J_{\mathrm{track}}=\frac{1}{N+1}\sum_{t=0}^{N}(y(t)-y_{\mathrm{des}}(t))^{2}.
$$ 

• Small input. The input should not be large. We measure the magnitude of the input by the quadratic function 

$$
J_{\mathrm{mag}}=\frac{1}{N+1}\sum_{t=0}^{N}u{(t)}^{2}.
$$ 

• Small input variations. The input should not vary rapidly. We measure the magnitude of the input variations by the quadratic function 

$$
J_{\mathrm{der}}=\frac{1}{N}\sum_{t=0}^{N-1}(u(t+1)-u(t))^{2}.
$$ 

By minimizing a weighted sum 

$$
J_{\mathrm{track}}+\delta J_{\mathrm{der}}+\eta J_{\mathrm{mag}},
$$ 

where $\delta>0$ and $\eta>0$ , we can trade oﬀthe three objectives. Now we consider a specific example, with $N=200$ , and impulse response 

$$
h(t)=\frac{1}{9}(0.9)^{t}(1-0.4\cos(2t)).
$$ 

Figure 6.6 shows the optimal input, and corresponding output (along with the desired trajectory $y_{\mathrm{des}}$ ), for three values of the regularization parameters $\delta$ and $\eta$ . The top row shows the optimal input and corresponding output for $\delta=0$ , $\eta=0.005$ . In this case we have some regularization for the magnitude of the input, but no regularization for its variation. While the tracking is good ( i.e. , we have $J_{\mathrm{track}}$ is small), the input required is large, and rapidly varying. The second row corresponds to $\delta=0$ , $\eta=0.05$ . In this case we have more magnitude regularization, but still no regularization for variation in $u$ . The corresponding input is indeed smaller, at the cost of a larger tracking error. The bottom row shows the results for $\delta\,=\,0.3$ , $\eta\:=\:0.05$ . In this case we have added some regularization for the variation. The input variation is substantially reduced, with not much increase in output tracking error. 

# $\ell_{1}$ -norm regularization 

Regularization with an $\ell_{1}$ -norm can be used as a heuristic for finding a sparse solution. For example, consider the problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}+\gamma\|x\|_{1},
$$ 

![](images/d66de3bc8edc5903045ec98f467ac4b4c617c2f70c0d39783dccadb9acd2b9cb.jpg) 
Figure 6.6 Optimal inputs (left) and resulting outputs (right) for three values of the regularization parameters $\delta$ (which corresponds to input variation) and $\eta$ (which corresponds to input magnitude). The dashed line in the righthand plots shows the desired output . Top row: $\delta=0$ , $\eta=0.005$ ; middle row: $y_{\mathrm{des}}$ $\delta=0$ , $\eta=0.05$ ; bottom row: $\delta=0.3$ , $\eta=0.05$ . 

in which the residual is measured with the Euclidean norm and the regularization is done with an $\ell_{1}$ -norm. By varying the parameter $\gamma$ we can sweep out the optimal trade-oﬀcurve between $||A x-b||_{2}$ and $\|{\boldsymbol{x}}\|_{1}$ , which serves as an approximation of the optimal trade-oﬀcurve between $||A x-b||_{2}$ and the sparsity or cardinality $\mathbf{card}(x)$ of the vector $x$ , i.e. , the number of nonzero elements. The problem ( 6.11 ) can be recast and solved as an SOCP. 

Example 6.4 Regressor selection problem. We are given a matrix $A\,\in\,\mathbf{R}^{m\times n}$ , whose columns are potential regressors, nd a vector $b\in\mathbf{R}^{m}$ that is to be fit by linear combination of $k<n$ columns of A . The problem is to choose the subset of k regressors to be used, and the associated coefficients. We can express this problem as 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\|A x-b\|_{2}}\\ {{\mathrm{subject~to}}}&{\operatorname{card}(x)\leq k.}\end{array}}
$$ 

In general, this is a hard combinatorial problem. 

One straightforward approach is to check every possible sparsity pattern in $x$ with $k$ nonzero entries. For a fixed sparsity pattern, we can find the optimal $x$ by solving a l st-squares problem, i.e. , minimizing $\|\tilde{A}\tilde{x}-b\|_{2}$ − ∥ , where A denotes the submatrix of A obtained by keeping the columns corresponding to the sparsity pattern, and x is the subvector with the nonzero components of $x$ . This is done for each of the $n!/(k!(n-k)!)$ sparsity patterns with $k$ nonzeros. 

A good heuristic approach is to solve the problem ( 6.11 ) for diﬀerent values of $\gamma$ , finding the smallest value of $\gamma$ that results in a solution with $\mathbf{card}(x)=k$ . We then fix this sparsity pattern and find the value of $x$ that minimizes $||A x-b||_{2}$ . 

Figure 6.7 illustrates a numerical example with $A\in\mathbf{R}^{10\times20}$ , $\boldsymbol{x}\in\mathbf{R}^{20}$ , $b\in\mathbf{R}^{10}$ . The circles on the dashed curve are the (globally) Pareto optimal values for the trade-oﬀ betw n $\mathbf{card}(x)$ (vertical axis) and the residual $||A x\textrm{--}b||_{2}$ (horizontal axis). For each k , the Pareto optimal point was obtained by enumerating all possible sparsity patterns with $k$ nonzero entries, as described above. The circles on the solid curve were obtained with the heuristic approach, by using the sparsity patterns of the solutions of problem ( 6.11 ) for diﬀerent values of $\gamma$ . Note that for $\mathbf{card}(x)=1$ , the heuristic method actually finds the global optimum. 

This idea will come up again in basis pursuit ( § 6.5.4 ). 

# 6.3.3 Reconstruction, smoothing, and de-noising 

In this section we describe an important special case of the bi-criterion approxi- mation problem described above, and give some examples showing how diﬀerent regularization methods perform. In reconstruction problems , we start with a signal represented by a vector $x\,\in\,\mathbf{R}^{n}$ . The coefficients $x_{i}$ correspond to the value of some function of time, evaluated (or sampled , in the language of signal processing) at evenly spaced points. It is usually assumed that the signal does not vary too rapidly, which means that usually, we have $x_{i}\approx x_{i+1}$ . (In this section we consider signals in one dimension, e.g. , audio signals, but the same ideas can be applied to signals in two or more dimensions, e.g. , images or video.) 

![](images/bba3354dc7cf72e2650d05bf1cd1b8f73fb8a10187cc9258866b678dacb4f458.jpg) 
Figure 6.7 Sparse regressor selection with a matrix $A\in\mathbf{R}^{10\times20}$ . The circles on the dashed line are the Pareto optimal values for the trade-oﬀbetween the residual $||A x-b||_{2}$ and the number of nonzero elements $\mathbf{card}(x)$ . The points indicated by circles on the solid line are obtained via the ℓ $\ell_{1}$ 1 -norm regularized heuristic. 

The signal $x$ is corrupted by an additive noise $v$ : 

$$
x_{\mathrm{cor}}=x+v.
$$ 

The noise can be modeled in many diﬀerent ways, but here we simply assume that it is unknown, small, and, unlike the signal, rapidly varying. The goal is to form an estimate x of the original signal $x$ , given the corrupted signal $x_{\mathrm{{cor}}}$ . This process is called signal reconstruction (since we are trying to reconstruct the original signal from the corrupted version) or de-noising (since we are trying to remove the noise from the corrupted signal). Most reconstruction methods end up performing some sort of smoothing operation on $x_{\mathrm{{cor}}}$ to produce x , so the process is also called smoothing . 

One simple formulation of the reconstruction problem is the bi-criterion problem 

$$
\begin{array}{r l}{\mathrm{minimize~}(\mathrm{w.r.t.~}\mathbf{R}_{+}^{2})}&{{}\left(\|\hat{x}-x_{\mathrm{cor}}\|_{2},\phi(\hat{x})\right),}\end{array}
$$ 

where x is the variable and $x_{\mathrm{{cor}}}$ is a problem parameter. The function $\phi:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex, and is called the regularization function or smoothing objective . It is meant to measure the roughness, or lack of smoothness, of the estimate x . The reconstruction problem ( 6.12 ) seeks signals that are close (in $\ell_{2}$ -norm) to the cor- rupted signal, and that are smooth, i.e. , for which $\phi(\hat{x})$ ) is small. The reconstruction problem ( 6.12 ) is a convex bi-criterion problem. We can find the Pareto optimal points by scalarization, and solving a (scalar) convex optimization problem. 

# Quadratic smoothing 

The simplest reconstruction method uses the quadratic smoothing function 

$$
\phi_{\mathrm{quad}}(x)=\sum_{i=1}^{n-1}(x_{i+1}-x_{i})^{2}=\|D x\|_{2}^{2},
$$ 

where $D\in\mathbf{R}^{(n-1)\times n}$ is the bidiagonal matrix 

$$
D=\left[{\begin{array}{r r r r r r}{-1}&{1}&{0}&{\cdots}&{0}&{0}&{0}\\ {0}&{-1}&{1}&{\cdots}&{0}&{0}&{0}\\ {\vdots}&{\vdots}&{\vdots}&{}&{\vdots}&{\vdots}&{\vdots}\\ {0}&{0}&{0}&{\cdots}&{-1}&{1}&{0}\\ {0}&{0}&{0}&{\cdots}&{0}&{-1}&{1}\end{array}}\right].
$$ 

We can obtain the optimal trade-oﬀbetween $\|\hat{x}-x_{\mathrm{cor}}\|_{2}$ − ∥ and $||D\hat{x}||_{2}$ ∥ by minimizing 

$$
\|\hat{x}-x_{\mathrm{cor}}\|_{2}^{2}+\delta\|D\hat{x}\|_{2}^{2},
$$ 

where $\delta>0$ parametrizes the optimal trade-oﬀcurve. The solution of this quadratic problem, 

$$
\hat{x}=(I+\delta D^{T}D)^{-1}x_{\mathrm{cor}},
$$ 

can be computed very efficiently since $I+\delta D^{I}D$ is tridiagonal; see appendix C . 

# Quadratic smoothing example 

Figure 6.8 shows a signal $x\in\mathbf{R}^{4000}$ (top) and the corrupted signal $x_{\mathrm{{cor}}}$ (bottom). The optimal trade-oﬀcurve between the objectives $\|\hat{x}-x_{\mathrm{cor}}\|_{2}$ − ∥ and $||D\hat{x}||_{2}$ ∥ is shown in figure 6.9 . The extreme point on the left of the trade-oﬀcurve corresponds to $\hat{x}=x_{\mathrm{cor}}$ , and h jective value $\|D x_{\mathrm{cor}}\|_{2}=4.4$ . The extreme point on the right corresponds to ˆ = 0, for which $\|\hat{x}-x_{\mathrm{cor}}\|_{2}=\|x_{\mathrm{cor}}\|_{2}=16.2$ − ∥ ∥ ∥ 2. Note the clear knee in the trade-oﬀcurve near $\Vert\hat{x}-x_{\mathrm{cor}}\Vert_{2}\approx3$ 3. 

Figure 6.10 shows three smoothed signals on the optimal trade-oﬀcurve, cor- responding to $\Vert\hat{x}-x_{\mathrm{cor}}\Vert_{2}=8$ − ∥ = 8 (top), 3 (middle), and 1 (bottom). Comparing the reconstructed signals with the original signal $x$ , we see that the best reconstruction is obtained for $\|\hat{x}\,-\,x_{\mathrm{cor}}\|_{2}\,=\,3$ − ∥ = 3, which corresponds to the knee of the trade-oﬀ curve. For higher values of $\|\hat{x}-x_{\mathrm{cor}}\|_{2}$ − ∥ , there is too much smoothing; for smaller values there is too little smoothing. 

# Total variation reconstruction 

Simple quadratic smoothing works well as a reconstruction method when the orig- inal signal is very smooth, and the noise is rapidly varying. But any rapid varia- tions in the original signal will, obviously, be attenuated or removed by quadratic smoothing. In this section we describe a reconstruction method that can remove much of the noise, while still preserving occasional rapid variations in the original signal. The method is based on the smoothing function 

$$
\phi_{\mathrm{tw}}(\hat{x})=\sum_{i=1}^{n-1}|\hat{x}_{i+1}-\hat{x}_{i}|=\|D\hat{x}\|_{1},
$$ 

![](images/658de4416407701f58c7279f127495f829abb95a416a6101f8bb26bb2785188b.jpg) 
re 6.8 Top: the original signal $x\in\mathbf{R}^{4000}$ . Bottom: the corrupted signal $x_{\mathrm{cor}}$ . 

![](images/c54f7342578d8a68a2c4a094d47d8eff413f4224215d1e98a47289e961c8bfaf.jpg) 
Figure 6.9 Optimal trade-oﬀcurve between $||D\hat{x}||_{2}$ ∥ and $||\hat{x}\,-\,x_{\mathrm{cor}}||_{2}$ − ∥ . The curve has a clear knee near $\|\hat{x}-x_{\mathrm{cor}}\|\approx3$ − ∥≈ 3. 

![](images/fabcd6c587e2853e6b3d8014644cf5e8642b290735a3fd2f73f203fa5a7fd4c2.jpg) 
Figure 6.10 Three smoothed or reconstructed signals x . The top one cor- responds to $\|\hat{x}\,-\,x_{\mathrm{cor}}\|_{2}\,=\,8$ − ∥ = 8, the middle one to $\|\hat{x}\,-\,x_{\mathrm{cor}}\|_{2}\,=\,3$ − ∥ = 3, and the bottom one to $\|\hat{x}-x_{\mathrm{cor}}\|_{2}=1$ 

which is called the total variation of $x\,\in\,\mathbf{R}^{\,n}$ . Like the quadratic smoothness measure $\phi_{\mathrm{quad}}$ , the total variation function assigns large values to rapidly varying x . The total variation measure, however, assigns relatively less penalty to large values of $|x_{i+1}-x_{i}|$ . 

# Total variation reconstruction example 

Figure 6.11 shows a signal $x\,\in\,\mathbf{R}^{2000}$ (in the top plot), and the signal corrupted with noise $x_{\mathrm{{cor}}}$ . The signal is mostly smooth, but has several rapid variations or jumps in value; the noise is rapidly varying. 

We first use quadratic smoothing. Figure 6.12 shows three smoothed signals on the optimal trade-oﬀcurve between $||D\hat{x}||_{2}$ ∥ and $\|\hat{x}-x_{\mathrm{cor}}\|_{2}$ − ∥ . In the first two signals, the rapid variations in the original signal are also smoothed. In the third signal the steep edges in the signal are better preserved, but there is still a significant amount of noise left. 

Now we demonstrate total variation reconstruction. Figure 6.13 shows the optimal trade-oﬀcurve between $||D\hat{x}||_{1}$ ∥ and $\|\hat{x}-x_{\mathrm{cor}}\|_{2}$ − ∥ . Figure 6.14 shows the re- constructed signals on the optimal trade-oﬀcurve, for $\|D\hat{x}\|_{1}=5$ ∥ = 5 (top), $\|D\hat{x}\|_{1}=8$ ∥ (middle), and $\|D\hat{x}\|_{1}=10$ ∥ = 10 (bottom). We observe that, unlike quadratic smoothing, total variation reconstruction preserves the sharp transitions in the signal. 

![](images/872363ce5a68ca5d593ff28d0af3345e56698af9ef9654b63a51cac72a94dde9.jpg) 
Figure 6.11 A signal $x\in\mathbf{R}^{2000}$ , and the corrupted signal $x_{\mathrm{cor}}\in\mathbf{R}^{2000}$ . The noise is rapidly varying, and the signal is mostly smooth, with a few rapid variations. 

![](images/0165c42387f896606d4ec56551f101e9478ded255ab2b0b1a053b06ba7fdcd84.jpg) 
Figure 6.12 Three quadratically smoothed signals x . The top one corre- sponds to $\Vert\hat{x}\,-\,x_{\mathrm{cor}}\Vert_{2}\,=\,10$ − ∥ = 10, the middle one to $\|\hat{x}\,-\,x_{\mathrm{cor}}\|_{2}\,=\,7$ − ∥ = 7, and the bottom one to $\|\hat{x}-x_{\mathrm{cor}}\|_{2}=4$ − ∥ = 4. The top one greatly reduces the noise, but also excessively smooths out the rapid variations in the signal. The bottom smoothed signal does not give enough noise reduction, and still smooths out the rapid variations in the original signal. The middle smoothed signal gives the best compromise, but still smooths out the rapid variations. 

![](images/9a32e6fabbebadc6c847fb24b50ca171b05e06b9011859f12e57b9e905a8ac9d.jpg) 
Figure 6.13 Optimal trade-oﬀcurve between $||D\hat{x}||_{1}$ ∥ and $\|\hat{x}-x_{\mathrm{cor}}\|_{2}$ − ∥ . 

![](images/6d28ba4b269f0f8774d84c54c90372a2b65bd52aaa6085e6468d3507b0459111.jpg) 
Figure 6.14 Three reconstructed signals x , using total variation reconstruc- tion. The top one corresponds to $\|D\hat{x}\|_{1}=5$ ∥ = 5, the middle one to $\|D\hat{x}\|_{1}=8$ ∥ and the bottom one to $\|D\hat{x}\|_{1}=10$ ∥ = 10. The bottom one does not give quite enough noise reduction, while the top one eliminates some of the slowly vary- ing parts of the signal. Note that in total variation reconstruction, unlike quadratic smoothing, the sharp changes in the signal are preserved. 

# 6.4 Robust approximation 

# 6.4.1 Stochastic robust approximation 

We consider an approximation problem with basic objective $\|A x\!-\!b\|$ , but also wi h to take into account some uncertainty or possible variation in the data matrix A . (The same ideas can be extended to handle the case where there is uncertainty in both $A$ and $b$ .) In this section we consider some statistical models for the variation in $A$ . 

We assume that $A$ is a random variable taking values in ${\bf R}^{m\times n}$ , with mean A , so we can describe $A$ as 

$$
A=\bar{A}+U,
$$ 

where $U$ is a random matrix with zero mean. Here, the constant matrix A gives the average value of $A$ , and $U$ describes its statistical variation. 

It is natural to use the expected value of $\|A x-b\|$ as the objective: 

$$
{\mathrm{minimize}}\quad\mathbf{E}\,\|A x-b\|.
$$ 

We refer to this problem as the stochastic robust approximation problem . It is always a convex optimization problem, but usually not tractable since in most cases it is very difficult to evaluate the objective or its derivatives. 

One simple case in which the stochastic robust approximation problem ( 6.13 ) can be solved occurs when $A$ assumes only a finite number of values, i.e. , 

$$
\mathbf{prob}(A=A_{i})=p_{i},\quad i=1,.\,.\,,k,
$$ 

where $A_{i}\in\mathbf{R}^{m\times n}$ , ${\bf1}^{T}p=1$ , $p\succeq0$ . In this case the problem ( 6.13 ) has the form 

$$
{\mathrm{minimize}}\quad p_{1}\|A_{1}x-b\|+\cdot\cdot\cdot+p_{k}\|A_{k}x-b\|,
$$ 

which is often called a sum-of-norms problem . It can be expressed as 

$$
\begin{array}{l l}{\mathrm{minimize}}&{p^{T}t}\\ {\mathrm{subject~to}}&{\|A_{i}x-b\|\leq t_{i},\quad i=1,.\,.\,,k,}\end{array}
$$ 

where the variables are $x\in\mathbf{R}^{n}$ and $t\in\mathbf{R}^{k}$ . If the norm is the Euclidean norm, this sum-of-norms problem is an SOCP. If the norm is the $\ell_{1}$ - or $\ell_{\infty}$ -norm, the sum-of-norms problem can be expressed as an LP; see exercise 6.8 . 

Some variations on the stochastic robust approximation problem ( 6.13 ) are tractable. As an example, consider the stochastic robust least-squares problem 

$$
{\mathrm{minimize}}\quad\mathbf{E}\,\|A x-b\|_{2}^{2},
$$ 

where the norm is the Euclidean norm. We can express the objective as 

$$
\begin{array}{l l l}{{{\bf E}\,\|A x-b\|_{2}^{2}}}&{{=}}&{{{\bf E}(\bar{A}x-b+U x)^{T}(\bar{A}x-b+U x)}}\\ {{}}&{{=}}&{{(\bar{A}x-b)^{T}(\bar{A}x-b)+{\bf E}\,x^{T}U^{T}U x}}\\ {{}}&{{=}}&{{\|\bar{A}x-b\|_{2}^{2}+x^{T}P x,}}\end{array}
$$ 

where ${\cal P}\,=\,{\bf E}\,U^{T}{\cal U}$ . Therefore the stochastic robust approximation problem has the form of a regularized least-squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\|\bar{A}x-b\|_{2}^{2}+\|P^{1/2}x\|_{2}^{2},}\end{array}
$$ 

with solution 

$$
x=(\bar{A}^{T}\bar{A}+P)^{-1}\bar{A}^{T}b.
$$ 

This makes perfect sense: when the matrix $A$ is subject to variation, the vector $A x$ will have more variation the larger $x$ is, and Jensen’s inequality tells us that variatio $A x$ will increase the average value of $||A x-b||_{2}$ . So we need to bala making $A x-b$ − small with the desire for a small $x$ (to keep the variation in Ax small), which is the essential idea of regularization. 

This observation gives us another interpretation of the Tikhonov regularized least-squares problem ( 6.10 ), as a robust least-squares problem, taking into account possible variation in the matrix $A$ . The solution of the Tikhonov regularized least- squares problem ( 6.10 ) minimizes $\mathbf{E}\,||(A+U)x-b||^{2}$ , where $U_{i j}$ are zero mean, uncorrelated random variables, with variance $\delta/m$ (and here, A is deterministic). 

# 6.4.2 Worst-case robust approximation 

It is also possible to model the variation in the matrix $A$ using a set-based, worst- case approach. We describe the uncertainty by a set of possible values for $A$ : 

$$
A\in{\mathcal{A}}\subseteq\mathbf{R}^{m\times n},
$$ 

which we assume is nonempty and bounded. We define the associated worst-case error of a candidate approximate solution $x\in\mathbf{R}^{n}$ as 

$$
e_{\mathrm{wc}}(x)=\operatorname*{sup}\{\left\|A x-b\right\|\mid A\in\mathcal{A}\},
$$ 

which is always a convex function of $x$ . The (worst-case) robust approximation problem is to minimize the worst-case error: 

$$
{\mathrm{minimize}}\quad e_{\mathrm{wc}}(x)=\operatorname*{sup}\{\|A x-b\|\mid A\in\mathcal{A}\},
$$ 

where the variable is $x$ , and the problem data are $b$ and the set $\mathcal{A}$ . When $\mathcal{A}$ is the singleton ${\mathcal{A}}=\{A\}$ , the robust approximation problem ( 6.14 ) reduces to the basic norm approximation problem ( 6.1 ). The robust approximation problem is always a convex optimization problem, but its tractability depends on the norm used and the description of the uncertainty set $\mathcal{A}$ . 

Example 6.5 Comparison of stochastic and worst-case robust approximation. To illustrate the diﬀerence between the stochastic and worst-case formulations of the robust approximation problem, we consider the least-squares problem 

$$
\mathrm{minimize}\quad\|\boldsymbol{A}(\boldsymbol{u})\boldsymbol{x}-\boldsymbol{b}\|_{2}^{2},
$$ 

where $u\ \in\ \mathbf{R}$ is an uncertain parameter and $A(u)\;=\;A_{0}\,+\,u A_{1}$ . We consider a specific instance of the problem, with $A(u)\in\mathbf{R}^{20\times10}$ , $\|A_{0}\|=10$ , $\left\|A_{1}\right\|=1$ , and $u$ 

![](images/fa998eec7a2bdbac6271e30bc0e0eac2968d7643774ea8bd3921a2da0b929f7e.jpg) 
Figure 6.15 The r idual $r(u)\;=\;\|A(u)x\,-\,b\|_{2}$ as a unction of the un- certain parameter u for three approximate solutions x : (1) the nominal least-squares solution $x_{\mathrm{norm}}$ ; (2) the solution of the stochastic robust approx- imation problem $\scriptstyle x_{\mathrm{stoch}}$ (assuming $u$ is uniformly distributed on $[-1,1]$ ); and (3) the solution of the worst-case robust approximation problem $\boldsymbol{x}_{\mathrm{wc}}$ , as- suming the parameter $u$ lies in the al $[-1,1]$ . The nominal solution achieves the smallest residual when u = 0, but gives much larger residuals $u$ pproaches $^{-1}$ or $1$ . The worst-case solution has a larger residual when = 0, but its residuals do not rise much as the parameter $u$ varies over the interval $[-1,1]$ . 

interval $[-1,1]$ . (So, roughly speaking, the variation in the matrix $A$ is around $\pm10\%$ 10%.) 

We find three approximate solutions: 

• Nominal optimal. The optimal solution $x_{\mathrm{norm}}$ is found, assuming $A(u)$ has its nominal value $A_{0}$ . • Stochastic robust approximation. We find $\scriptstyle x_{\mathrm{stoch}}$ , which minimizes ${\mathbf E}\parallel A(u)x\textrm{--}$ $b\vert\vert_{2}^{2}$ , assuming the parameter $u$ is uniformly distributed on $[-1,1]$ . • Worst-case robust approximation. We find $x_{\mathrm{wc}}$ , which minimizes $\operatorname*{sup}_{-1\leq u\leq1}\|A(u)x-b\|_{2}=\operatorname*{max}\{\|(A_{0}-A_{1})x-b\|_{2},\|(A_{0}+A_{1})x-b\|_{2}\}.$ 

For each of these three values of $x$ , we plot the residual $r(u)=\|A(u)x-b\|_{2}$ as a function of the uncertain parameter $u$ , in figure 6.15 . These plots show how sensitive an approximate solution can be to variation in the parameter $u$ . The nominal solu- tion achieves the smallest residual when $u\,=\,0$ , but is quite sensitive to parameter variation: it gives much larger residuals as $u$ deviates from $0$ , and approaches $^{-1}$ or 1. The worst-case solution has a larger residual when $u=0$ , but its residuals do not rise much as $u$ varies over the interval $[-1,1]$ . The stochastic robust approximate solution is in between. 

The robust approximation problem ( 6.14 ) arises in many contexts and applica- tions. In an estimation setting, the set $\mathcal{A}$ gives our uncertainty in the linear relation between the vector to be estimated and our measurement vector. Sometimes the noise term $v$ in the model $\boldsymbol{y}\,=\,\boldsymbol{A}\boldsymbol{x}+\boldsymbol{v}$ is called additive noise or additive error , since it is added to the ‘ideal’ measurement $A x$ . In contrast, the variation in $A$ is called multiplicative error , since it multiplies the variable $x$ . 

In an optimal design setting, the variation can represent uncertainty (arising in manufacture, say) of the linear equations that relate the design variables $x$ to the results vector $A x$ . The robust approximation problem ( 6.14 ) is then interpreted as the robust design problem: find design variables $x$ that minimize the worst possible mismatch between $A x$ and $b$ , over all possible values of $A$ . 

# Finite set 

Here we have ${\mathcal{A}}=\{A_{1},.\,.\,.\,,A_{k}\}$ , and the robust approximation problem is 

$$
{\mathrm{minimize}}\quad\operatorname*{max}_{i=1,\ldots,k}\|A_{i}x-b\|.
$$ 

This problem is equivalent to the robust approximation problem with the polyhe- dral set $\mathcal{A}=\mathbf{conv}\{A_{1},\dots,A_{k}\}$ : 

$$
{\mathrm{minimize}}\quad\operatorname*{sup}\left\{\left\|A x-b\right\|\mid A\in\mathbf{conv}\{A_{1},.\,.\,,A_{n}\}\right\}
$$ 

We can cast the problem in epigraph form as 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{t}\\ {{\mathrm{subject~to}}}&{\|A_{i}x-b\|\leq t,\quad i=1,.\,.\,,k,}\end{array}}
$$ 

which can be solved in a variety of ways, depending on the norm used. If the norm is the Euclidean norm, this is an SOCP. If the norm is the $\ell_{1}$ - or $\ell_{\infty}$ -norm, we can express it as an LP. 

# Norm bound error 

Here the tainty set $\mathcal{A}$ is a norm ball, ${\mathcal{A}}=\{A+U\mid\|U\|\leq a\}$ | ∥ ∥≤ } , where $\|\cdot\|$ is a norm on R ${\bf R}^{m\times n}$ . In this case we have 

$$
\begin{array}{r}{e_{\mathrm{wc}}(x)=\operatorname*{sup}\{\|\bar{A}x-b+U x\|\mid\|U\|\leq a\},}\end{array}
$$ 

which must be carefully interpreted since the first norm appearing is on $\mathbf{R}^{m}$ (and is used to measure the size of the residual) and the second one appearing is on ${\bf R}^{m\times n}$ (used to define the norm ball $\mathcal{A}$ ). 

This expression for $e_{\mathrm{wc}}(x)$ can be simplified in several cases. As an example, let us take the Euclidean norm on $\mathbf{R}^{n}$ and the associated induced norm on ${\bf R}^{m\times n}$ , $i$ .e. , the maximum singular value. If ${\bar{A}}x-b\neq0$ 0 and $x\neq0$ , the supremum in the expression for $e_{\mathrm{wc}}(x)$ is attained for U $U=a u v^{T}$ , with 

$$
u=\frac{\bar{A}x-b}{\|\bar{A}x-b\|_{2}},\qquad v=\frac{x}{\|x\|_{2}},
$$ 

and the resulting worst-case error is 

$$
e_{\mathrm{wc}}(x)=\|\bar{A}x-b\|_{2}+a\|x\|_{2}.
$$ 

(It is easily verified that this expression is also valid if $x$ or $A x-b$ − is zero.) The robust approximation problem ( 6.14 ) then becomes 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\|\bar{A}x-b\|_{2}+a\|x\|_{2},}\end{array}
$$ 

which is a regularized norm problem, solvable as the SOCP 

$$
\begin{array}{l l}{\mathrm{minimize}}&{t_{1}+a t_{2}}\\ {\mathrm{subject~to}}&{\|\bar{A}x-b\|_{2}\leq t_{1},\quad\|x\|_{2}\leq t_{2}.}\end{array}
$$ 

Since the solution of this problem is the same as the solution of the regularized least-squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\|\bar{A}x-b\|_{2}^{2}+\delta\|x\|_{2}^{2}}\end{array}
$$ 

for some value of the regularization parameter $\delta$ , we have another interpretation of the regularized least-squares problem as a worst-case robust approximation prob- lem. 

# Uncertainty ellipsoids 

We can also describe the variation in $A$ by giving an ellipsoid of possible values for each row: 

$$
{\mathcal{A}}=\{[a_{1}\,\,\cdot\cdot\cdot\,\,a_{m}]^{T}\,\,|\,\,a_{i}\in{\mathcal{E}}_{i},\,\,i=1,.\,.\,.\,,m\},
$$ 

where 

$$
\mathcal{E}_{i}=\{\bar{a}_{i}+P_{i}u\mid\|u\|_{2}\leq1\}.
$$ 

The matrix $P_{i}\in\mathbf{R}^{n\times n}$ describes the variation in $a_{i}$ . We allow $P_{i}$ to have a nontriv- ial nullspace, in order to model the situation when the variation in $a_{i}$ is restricted to a subspace. As an extreme case, we take $P_{i}=0$ if there is no uncertainty in $a_{i}$ . 

With this ellipsoidal uncertainty description, we can give an explicit expression for the worst-case magnitude of each residual: 

$$
\begin{array}{l c l}{\displaystyle\operatorname*{sup}_{a_{i}\in\mathcal E_{i}}\left|a_{i}^{T}x-b_{i}\right|}&{=}&{\operatorname*{sup}\{\left|\bar{a}_{i}^{T}x-b_{i}+(P_{i}u)^{T}x\right|\,\left|\,\left\|u\right\|_{2}\leq1\}}\\ {\ }&{=}&{\displaystyle\left|\bar{a}_{i}^{T}x-b_{i}\right|+\|P_{i}^{T}x\|_{2}.}\end{array}
$$ 

Using this result we can solve several robust approximation problems. For example, the robust $\ell_{2}$ -norm approximation problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{e_{\mathrm{wc}}(x)=\operatorname*{sup}\{\|A x-b\|_{2}\ |\ a_{i}\in{\mathcal{E}}_{i},\ i=1,\ldots,m\}}\end{array}}
$$ 

can be reduced to an SOCP, as follows. An explicit expression for the worst-case error is given by 

$$
e_{\mathrm{wc}}(x)=\left(\sum_{i=1}^{m}\left(\operatorname*{sup}_{a_{i}\in\mathcal{E}_{i}}|a_{i}^{T}x-b_{i}|\right)^{2}\right)^{1/2}=\left(\sum_{i=1}^{m}(|\bar{a}_{i}^{T}x-b_{i}|+\|P_{i}^{T}x\|_{2})^{2}\right)^{1/2}.
$$ 

To minimize $e_{\mathrm{wc}}(x)$ we can solve 

$$
\begin{array}{l r}{\mathrm{minimize}\quad\|t\|_{2}}\\ {\mathrm{subject~to}\quad|\bar{a}_{i}^{T}x-b_{i}|+\|P_{i}^{T}x\|_{2}\leq t_{i},\quad i=1,\ldots,m,}\end{array}
$$ 

where we introduced new variables $t_{1},\dots,t_{m}$ . This problem can be formulated as 

$$
\begin{array}{l l}{\mathrm{minimize}\ }&{\|t\|_{2}}\\ {\mathrm{subject~to}\ }&{\bar{a}_{i}^{T}x-b_{i}+\|P_{i}^{T}x\|_{2}\leq t_{i},\quad i=1,\ldots,m}\\ &{-\bar{a}_{i}^{T}x+b_{i}+\|P_{i}^{T}x\|_{2}\leq t_{i},\quad i=1,\ldots,m,}\end{array}
$$ 

which becomes an SOCP when put in epigraph form. 

# Norm bounded error with linear structure 

As a g eralization of the norm bound description ${\mathcal{A}}=\{A+U\mid\|U\|\leq a\}$ | ∥ ∥≤ } , we can define as the image of a norm ball under an affine transformation: 

$$
{\mathcal{A}}=\{{\bar{A}}+u_{1}A_{1}+u_{2}A_{2}+\cdot\cdot\cdot+u_{p}A_{p}\mid\|u\|\leq1\},
$$ 

where $||\cdot||$ is a norm on $\mathbf{R}^{p}$ , and the $p+1$ matrices A, $A_{1},\cdot\cdot\cdot,A_{p}\,\in\,\mathbf{R}^{m\times n}$ are given. The worst-case error can be expressed as 

$$
\begin{array}{r c l}{\displaystyle e_{\mathrm{wc}}(x)}&{=}&{\displaystyle\operatorname*{sup}_{\|u\|\leq1}\|(\bar{A}+u_{1}A_{1}+\cdot\cdot\cdot+u_{p}A_{p})x-b\|}\\ &{=}&{\displaystyle\operatorname*{sup}_{\|u\|\leq1}\|P(x)u+q(x)\|,}\end{array}
$$ 

where $P$ and are defined as $q$ 

$$
P(x)={\left[\begin{array}{l l l l}{A_{1}x}&{A_{2}x}&{\cdots}&{A_{p}x}\end{array}\right]}\in\mathbf{R}^{m\times p},\qquad q(x)={\bar{A}}x-b\in\mathbf{R}^{m}.
$$ 

As a first example, we consider the robust Chebyshev approximation problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{e_{\mathrm{wc}}(x)=\operatorname*{sup}_{\|u\|_{\infty}\leq1}\|(\bar{A}+u_{1}A_{1}+\cdot\cdot\cdot+u_{p}A_{p})x-b\|_{\infty}.}\end{array}
$$ 

In this case we can derive an explicit expression for the worst-case error. Let $p_{i}(x)^{T}$ denote the $i$ th row of $P(x)$ . We have 

$$
\begin{array}{r c l}{e_{\mathrm{wc}}(x)}&{=}&{\underset{\|u\|_{\infty}\leq1}{\operatorname*{sup}}\,\|P(x)u+q(x)\|_{\infty}}\\ &{=}&{\underset{i=1,\ldots,m}{\operatorname*{max}}\,\underset{\|u\|_{\infty}\leq1}{\operatorname*{sup}}\,|p_{i}(x)^{T}u+q_{i}(x)|}\\ &{=}&{\underset{i=1,\ldots,m}{\operatorname*{max}}\,\big(\|p_{i}(x)\|_{1}+|q_{i}(x)|\big).}\end{array}
$$ 

The robust Chebyshev approximation problem can therefore be cast as an LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{t}\\ {{\mathrm{subject~to}}\quad-y_{0}\preceq{\bar{A}}x-b\preceq y_{0}}\\ &{-y_{k}\preceq{\bar{A}}_{k}x\preceq y_{k},\quad k=1,\ldots,p}\\ &{y_{0}+\sum_{k=1}^{p}y_{k}\preceq t{\mathbf{1}},}\end{array}}
$$ 

with variables $x\in\mathbf{R}^{n}$ , $y_{k}\in\mathbf{R}^{m}$ , $t\in\mathbf{R}$ . 

As another example, we consider the robust least-squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{{}e_{\mathrm{wc}}(x)=\operatorname*{sup}_{\|u\|_{2}\leq1}\|(\bar{A}+u_{1}A_{1}+\cdot\cdot\cdot+u_{p}A_{p})x-b\|_{2}.}\end{array}
$$ 

Here we use Lagrange duality to evaluate $e_{\mathrm{wc}}$ . The worst-case error $e_{\mathrm{wc}}(x)$ is the squareroot of the optimal value of the (nonconvex) quadratic optimization problem 

$$
\begin{array}{l l}{\mathrm{maximize}}&{||P(\boldsymbol{x})u+q(\boldsymbol{x})||_{2}^{2}}\\ {\mathrm{subject~to}}&{u^{T}u\leq1,}\end{array}
$$ 

with $u$ as variable. The Lagrange dual of this problem can be expressed as the SDP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{t+\lambda}\\ {{\mathrm{subject~to}}\quad}&{{\left[\begin{array}{l l l}{I}&{P(x)}&{q(x)}\\ {P(x)^{T}}&{\lambda I}&{0}\\ {q(x)^{T}}&{0}&{t}\end{array}\right]}\succeq0}\end{array}}
$$ 

with variables $t,\ \lambda\,\in\,\mathbf{R}$ . Moreover, as mentioned in $\S5.2$ and § B.1 (and proved in § B.4 ), strong duality holds for this pair of primal and dual problems. In other words, for fixed $x$ , we can compute $e_{\mathrm{wc}}(x)^{2}$ by solving the SDP ( 6.15 ) with variables $t$ and $\lambda$ . Optimizing jointly over $t$ , $\lambda$ , and $x$ is equivalent to minimizing $e_{\mathrm{wc}}(x)^{2}$ . We conclude that the robust least-squares problem is equivalent to the SDP ( 6.15 ) with $x$ , $\lambda$ , $t$ as variables. 

Example 6.6 Comparison of worst-case robust, Tikhonov regularized, and nominal least-squares solutions. We consider an instance of the robust approximation problem 

$$
\begin{array}{r}{\mathrm{minimize}\quad\operatorname*{sup}_{\|u\|_{2}\leq1}\|\big(\bar{A}+u_{1}A_{1}+u_{2}A_{2}\big)x-b\|_{2},}\end{array}
$$ 

with dimensions $m=50$ , $n=20$ . The matrix A has norm 10, and the two matrices $A_{1}$ and $A_{2}$ have norm 1, so the variation in the matrix $A$ is, roughly speaking, around $10\%$ . The uncertainty parameters $u_{1}$ and $u_{2}$ lie in the unit disk in $\mathbf{R}^{2}$ . 

We compute the optimal solution of the robust least-squares problem ( 6.16 ) $x_{\mathrm{{rms}}}$ , as well as the solution of the nominal least-squares problem $x_{\mathrm{ls}}$ ( i.e. , assuming $u=0$ ), and also the Tikhonov regularized solution , with $\delta=1$ . $x_{\mathrm{tip}}$ 

To illustrate the sensitivity of each of these approximate solutions to the parameter $u$ , we generate $10^{5}$ parameter vectors, uniformly distributed on the unit disk, and evaluate the residual 

$$
\|(A_{0}+u_{1}A_{1}+u_{2}A_{2})x-b\|_{2}
$$ 

for each parameter value. The distributions of the residuals are shown in figure 6.16 . 

We can make several observations. First, the residuals of the nominal least-squares solution are widely spread, from a smallest value around 0 . 52 to a largest value around 4 . 9. In particular, the least-squares solution is very sensitive to parameter variation. In contrast, both the robust least-squares and Tikhonov regularized so- lutions exhibit far smaller variation in residual as the uncertainty parameter varies over the unit disk. The robust least-squares solution, for example, achieves a residual between 2 . 0 and 2 . 6 for all parameters in the unit disk. 

# 6.5 Function fitting and interpolation 

In function fitting problems, we select a member of a finite-dimensional subspace of functions that best fits some given data or requirements. For simplicity we 

![](images/e3f81241d9f7671c1bdf7ea704c00ddb66b8f028913244307c55881463a3e155.jpg) 
Figure 6.16 Distribution of the residuals for the three solutions of a least- squares problem ( 6.16 ): , the least-squares solution assuming $u=0$ ; , $x_{\mathrm{ls}}$ $x_{\mathrm{tip}}$ the Tikhonov regularized solution with $\delta\,=\,1$ ; and $x_{\mathrm{rms}}$ , the robust least- squares solution. The histograms were obtained by generating $10^{5}$ values of the uncertain parameter vector $u$ from a uniform distribution on the unit disk in $\scriptstyle\mathbf{R}^{2}$ . The bins have width 0.1. 

consider real-valued functions; the ideas are readily extended to handle vector- valued functions as well. 

# 6.5.1 Function families 

We consider a family of ons $f_{1},.\,.\,.\,,f_{n}\,:\,\mathbf{R}^{k}\,\rightarrow\,\mathbf{R}$ , with common domain $\mathbf{dom}\,f_{i}=D$ . With each x $x\in\mathbf{R}^{n}$ n we associate the function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ given by 

$$
f(u)=x_{1}f_{1}(u)+\cdot\cdot\cdot+x_{n}f_{n}(u)
$$ 

with $\mathbf{dom}\,f\,=\,D$ . The family $\{f_{1},.\,.\,.\,,f_{n}\}$ is sometimes called the set of basis functions (for the fitting problem) even when the functions are not independent. The vector $x\in\mathbf{R}^{n}$ , which parametrizes the subspace of functions, is our optimiza- tion variable, and is sometimes called the coefficient vector . The basis functions generate a subspace $\mathcal{F}$ of functions on $D$ . 

In many applications the basis functions are specially chosen, using prior knowl- edge or experience, in order to reasonably model functions of interest with the finite-dimensional subspace of functions. In other cases, more generic function families are used. We describe a few of these below. 

# Polynomials 

One common subspace of functions on $\mathbf{R}$ consists of polynomials of degree less than $n$ . The simplest basis consists of the powers, i.e. , $f_{i}(t)=t^{i-1}$ , $i=1,\dots,n$ . In many applications, the same subspace is described using a diﬀerent basis, for example, a set of polynomials $f_{1},\ldots,f_{n}$ , of degree less than $n$ , that are orthonormal with respect to some positive function (or measure) $\phi:\mathbf{R}^{n}\rightarrow\mathbf{R}_{+}$ , i.e. , 

$$
\int f_{i}(t)f_{j}(t)\phi(t)\;d t={\left\{\begin{array}{l l}{1}&{i=j}\\ {0}&{i\neq j.}\end{array}\right.}
$$ 

Another common basis for polynomials is the Lagrange basis $f_{1},\ldots,f_{n}$ associated with distinct points $t_{1},\dots,t_{n}$ , which satisfy 

$$
f_{i}(t_{j})={\left\{\begin{array}{l l}{1}&{i=j}\\ {0}&{i\not=j.}\end{array}\right.}
$$ 

We can also consider polynomials on $\mathbf{R}^{k}$ , with a maximum total degree, or a maximum degree for each variable. 

As a related example, we have trigonometric polynomials of degree less than $n$ , with basis 

$$
k=1,\ldots,n-1,\qquad\cos k t,\quad k=0,\ldots,n-1
$$ 

# Piecewise-linear functions 

We start with a triangular iz ation of the domain $D$ , which means the following. We have a set of mesh or grid points $g_{1},\ldots,g_{n}\in\mathbf{R}^{k}$ , and a partition of $D$ into a set of simplexes: 

$$
D=S_{1}\cup\cdot\cdot\cdot\cup S_{m},\qquad\operatorname{int}(S_{i}\cap S_{j})=\emptyset\mathrm{~for~}i\neq j.
$$ 

![](images/98d964341f433cc0f94ca4301fbde36d70de52678503fea3a3d8b306722699ba.jpg) 
Figure 6.17 A piecewise-linear function of two variables, on the unit square. The triangulation consists of 98 simplexes, and a uniform grid of 64 points in the unit square. 

Each simplex is the convex hull of $k+1$ grid points, and we require that each grid point is a vertex of any simplex it lies in. 

Given a triangular iz ation, we can construct a piecewise-linear (or more precisely, piecewise-affine) function $f$ by assigning function values $f(g_{i})\;=\;x_{i}$ to the grid points, and then extending the function affinely on each simplex. The function $f$ can be expressed as ( 6.17 ) where the basis functions $f_{i}$ are affine on each simplex and are defined by the conditions 

$$
f_{i}(g_{j})={\left\{\begin{array}{l l}{1}&{i=j}\\ {0}&{i\not=j.}\end{array}\right.}
$$ 

By construction, such a function is continuous. Figure 6.17 shows an example for $k=2$ . 

# Piecewise polynomials and splines 

The idea of piecewise-affine functions on a triangulated domain is readily extended to piecewise polynomials and other functions. 

Piecewise polynomials are defined as polynomials (of some maximum degree) on each simplex of the triangulation, which are continuous, i.e. , the polynomials agree at the boundaries between simplexes. By further restricting the piecewise polynomials to have continuous derivatives up to a certain order, we can define various classes of spline functions . Figure 6.18 shows an example of a cubic spline, i.e. , a piecewise polynomial of degree 3 on $\mathbf{R}$ , with continuous first and second derivatives. 

![](images/17bd5165b7150588c9416c36610e6265ba65f87494616d249a863f4fef85695d.jpg) 
Figure 6.18 Cubic spline. A cubic spline is a piecewise polynomial, with continuous first and second derivatives. In this example, the cubic spline $f$ is formed from the three cubic polynomials $p_{1}$ (on $[u_{0},u_{1}]$ ), $p_{2}$ (on $\left[{{u_{1}},{u_{2}}}\right]$ ), and $p_{3}$ (on $\left[u_{2},u_{3}\right],$ ). Adjacent polynomials have the same function value, and equal first and second derivatives, at the boundary points $u_{1}$ and $u_{2}$ . In this example, the dimension of the family of functions is $n\,=\,6$ , since we have 12 polynomial coefficients (4 per cubic polynomial), and 6 equality constraints (3 each at $u_{1}$ and $u_{2}$ ). 

# 6.5.2 Constraints 

In this section we describe some constraints that can be imposed on the function $f$ , and therefore, on the variable $x\in\mathbf{R}^{n}$ . 

# Function value interpolation and inequalities 

Let $v$ be a point in $D$ . The value of $f$ at $v$ , 

$$
f(v)=\sum_{i=1}^{n}x_{i}f_{i}(v),
$$ 

is a linear function of $x$ . Therefore interpolation conditions 

$$
f(v_{j})=z_{j},\quad j=1,.\,.\,.\,,m,
$$ 

which require the function $f$ to h e the values $z_{j}\in\mathbf{R}$ at specified points $v_{j}\in D$ , form a set of linear equalities in x . More generally, inequalities on the function value at a given point, as in $l\leq f(v)\leq u$ , are linear inequalities on the variable $x$ . There are many other interesting convex constraints on $f$ (hence, $x$ ) that involve the function values at a finite set of points $v_{1},\dots,v_{N}$ . For example, the Lipschitz constraint 

$$
|f(v_{j})-f(v_{k})|\leq L\|v_{j}-v_{k}\|,\quad j,\ k=1,.\,.\,.\,,m,
$$ 

forms a set of linear inequalities in $x$ . 

We can also impose inequalities on the function values at an infinite number of points. As an example, consider the nonnegativity constraint 

$$
f(u)\geq0{\mathrm{~for~all~}}u\in D.
$$ 

This is a convex constraint on $x$ (since it is the intersection of an infinite number of halfspaces), but may not lead to a tractable problem except in special cases that exploit the particular structure of the functions. One simple example occurs when the functions are piecewise-linear. In this case, if the function values are nonnegative at the grid points, the function is nonnegative everywhere, so we obtain a simple (finite) set of linear inequalities. 

As a less trivial example, consider the case when the functions are polynomials on $\mathbf{R}$ , with even maximum degree $2k$ ( i.e. , $n=2k+1$ ), and $D=\mathbf{R}$ . As shown in exercise 2.37 , page 65 , the nonnegativity constraint 

$$
p(u)=x_{1}+x_{2}u+\cdot\cdot\cdot+x_{2k+1}u^{2k}\geq0\quad{\mathrm{for~all~}}u\in\mathbf{R},
$$ 

is equivalent to 

$$
x_{i}=\sum_{m+n=i+1}Y_{m n},\quad i=1,\ldots,2k+1,\qquad Y\succeq0,
$$ 

where $Y\in\mathbf{S}^{k+1}$ is an auxiliary variable. 

# Derivative constraints 

Suppose the basis functions $f_{i}$ are diﬀerentiable at a point $v\in D$ . The gradient 

$$
\nabla f(\boldsymbol{v})=\sum_{i=1}^{n}x_{i}\nabla f_{i}(\boldsymbol{v}),
$$ 

is a linear function of $x$ , so interpolation conditions on the derivative of $f$ at $v$ reduce to linear equality constraints on $x$ . Requiring that the norm of the gradient at $v$ not exceed a given limit, 

$$
\|\nabla f(v)\|=\left\|\sum_{i=1}^{n}x_{i}\nabla f_{i}(v)\right\|\leq M,
$$ 

is a convex constraint on $x$ . The same idea extends to higher derivatives. For example, if $f$ is twice diﬀerentiable at $v$ , the requirement that 

$$
l I\preceq\nabla^{2}f(v)\preceq u I
$$ 

is a linear matrix inequality in $x$ , hence convex. 

We can also impose constraints on the derivatives at an infinite number of points. For example, we can require that $f$ is monotone: 

$$
f(u)\geq f(v){\mathrm{~for~all~}}u,\ v\in D,\ u\succeq v.
$$ 

This is a convex constraint in $x$ , but may not lead to a tractable problem except in special cases. When $f$ is piecewise affine, for example, the monotonicity constraint is equivalent to the condition $\nabla f(v)\succeq0$ inside each of the simplexes. Since the gradient is a linear function of the grid point values, this leads to a simple (finite) set of linear inequalities. 

As another example, we can require that the function be convex, i.e. , satisfy 

$$
f{\big(}(u+v)/2{\big)}\leq{\big(}f(u)+f(v){\big)}/2{\mathrm{~for~all~}}u,\ v\in D
$$ 

(which is enough to ensure convexity when $f$ is continuous). This is a convex con- straint, which has a tractable representation in some cases. One obvious example is when $f$ is quadratic, in which case the convexity constraint reduces to the re- quirement that the quadratic part of $f$ be nonnegative, which is an LMI. Another example in which a convexity constraint leads to a tractable problem is described in more detail in 6.5.5 . 

# Integral constraints 

Any linear functional $\mathcal{L}$ on the subspace of functions can be expressed as a linear function of $x$ , i.e. , we have ${\mathcal{L}}(f)=c^{T}x$ . Evaluation of $f$ (or a derivative) at a point is just a special case. As another example, the linear functional 

$$
{\mathcal{L}}(f)=\int_{D}\phi(u)f(u)\;d u,
$$ 

where $\phi:\mathbf{R}^{k}\rightarrow\mathbf{R}$ , can be expressed as ${\mathcal{L}}(f)=c^{T}x$ , where 

$$
c_{i}=\int_{D}\phi(u)f_{i}(u)~d u.
$$ 

Thus, a constraint of the form $\mathcal{L}(f)=a$ is a linear equality constraint on $x$ . One example of such a constraint is the moment constraint 

$$
\int_{D}t^{m}f(t)\ d t=a
$$ 

(where $f:\mathbf{R}\rightarrow\mathbf{R}$ ). 

# 6.5.3 Fitting and interpolation problems 

Minimum norm function fitting 

In a fitting problem, we are given data 

$$
(u_{1},y_{1}),\quad.\cdot.,\quad(u_{m},y_{m})
$$ 

with $u_{i}\in D$ and $y_{i}\,\in\,\mathbf{R}$ , and seek a function $f\,\in\,{\mathcal{F}}$ that matches this data as closely as possible. For example in least-squares fitting we consider the problem 

$$
\begin{array}{r}{\mathrm{minimize}\quad\sum_{i=1}^{m}(f(u_{i})-y_{i})^{2},}\end{array}
$$ 

which is a simple least-squares problem in the variable $x$ . We can add a variety of constraints, for example linear inequalities that must be satisfied by $f$ at various points, constraints on the derivatives of $f$ , monotonicity constraints, or moment constraints. 

xample 6.7 Polynomial fitting. We are given data $u_{1},.\,.\,.\,,u_{m}\in\mathbf{R}$ and $v_{1},\ldots,v_{m}\in$ R , and hope to approximately fit a polynomial of the form 

$$
p(u)=x_{1}+x_{2}u+\cdot\cdot\cdot+x_{n}u^{n-1}
$$ 

to the data. For each $x$ we form the vector of errors, 

$$
e=\left(p(u_{1})-v_{1},.\,.\,.\,,p(u_{m})-v_{m}\right).
$$ 

To find the polynomial that minimizes the norm of the error, we solve the norm approximation problem 

$$
{\mathrm{minimize}}\quad\|e\|=\|A x-v\|
$$ 

with variable $x\in\mathbf{R}^{n}$ , where $A_{i j}=u_{i}^{j-1}$ $^{;1},\:i=1,.\;.\;.\;,m,\;j=1,.\;.\;.\;,n$ . 

Figure 6.19 shows an example with $m=40$ data points and $n=6$ ( i.e. , polynomials of maximum degree 5), for the $\ell_{2^{-}}$ and $\ell_{\infty}$ -norms. 

![](images/c72f993a98210b613454f32f0a97f6b24b804f44dfe6ed342cba9f4f728a45e5.jpg) 
Figure 6.19 Two polynomials of degree 5 that approximate the 40 data points shown as circles. The polynomial shown as a solid line minimizes the $\ell_{2}$ -norm of the error; the polynomial shown as a dashed line minimizes the $\ell_{\infty}$ -norm. 

![](images/df40c3b59261a5039ed16c03900f9e9f7a0954c848b944902adbbf7c38e56a3d.jpg) 
Figure 6.20 Two cubic splines that approximate the 40 data points shown as circles (which are the same as the data in figure 6.19 ). The spline shown as a solid line minimizes the $\ell_{2}$ -norm of the error; the spline shown as a dashed line minimizes the $\ell_{\infty}$ -norm. As in the polynomial approximation shown in figure 6.19 , the dimension of the subspace of fitting functions is 6. 

Example 6.8 Spline fitting. Figure 6.20 shows the same data as in example 6.7 , and two optimal fits with cubic splines. The interval $[-1,1]$ is divided into three equal intervals, and we consider piecewise polynomials, with maximum degree 3, with continuous first and second derivatives. The dimension of this subspace of functions is 6, the same as the dimension of polynomials with maximum degree 5, considered in example 6.7 . 

In the simplest forms of function fitting, we have $m\implies n$ , i.e. , the number of data points is much larger than the dimension of the subspace of functions. Smoothing is accomplished automatically, since all members of the subspace are smooth. 

# Least-norm interpolation 

In another variation of function fitting, we have fewer data points than the dimen- sion of the subspace of functions. In the simplest case, we require that the function we choose must satisfy the interpolation conditions 

$$
f(u_{i})=y_{i},\quad i=1,.\,.\,.\,,m,
$$ 

which are linear equality constraints on $x$ . Among the functions that satisfy these interpolation conditions, we might seek one that is smoothest, or smallest. These lead to least-norm problems. 

In the most general function fitting problem, we can optimize an objective (such as some measure of the error $e$ ), subject to a variety of convex constraints that represent our prior knowledge of the underlying function. 

# Interpolation, extrapolation, and bounding 

By evaluating the optimal function fit $\hat{f}$ at a point $v$ not in the original data set, we obtain a guess of what the value of the underlying function is, at the point $v$ . This is called interpolation when $v$ is between or near the given data points ( e.g. , $v\in\mathbf{conv}\{v_{1},\cdot\cdot\cdot,v_{m}\})$ , and extrapolation otherwise. 

We can also produce an interval in which the value $f(v)$ can lie, by maximizing and minimizing (the linear function) $f(v)$ , subject to the constraints. We can use the function fit to help identify faulty data or outliers. Here we might use, for example, an $\ell_{1}$ -norm fit, and look for data points with large errors. 

# 6.5.4 Sparse descriptions and basis pursuit 

In basis pursuit , there is a very large number of basis functions, and the goal is to find a good fit of the given data as a linear combination of a small number of the basis functions. (In this context the function family is linearly dependent, and is sometimes referred to as an over-complete basis or dictionary .) This is called basis pursuit since we are selecting a much smaller basis, from the given over-complete basis, to model the data. 

Thus we seek a function $f\in{\mathcal{F}}$ that fits the data well, 

$$
f(u_{i})\approx y_{i},\quad i=1,.\,.\,.\,,m,
$$ 

with a sparse coefficient vector $x$ , i.e. , $\mathbf{card}(x)$ small. In this case we refer to 

$$
f=x_{1}f_{1}+\cdot\cdot\cdot+x_{n}f_{n}=\sum_{i\in\mathcal{B}}x_{i}f_{i},
$$ 

where ${\mathcal{B}}=\{i\mid x_{i}\neq0\}$ is the set of indices of the chosen basis elements, as a sparse description of the data. Mathematically, basis pursuit is the same as the regressor selection problem (see § 6.4 ), but the interpretation (and scale) of the optimization problem are diﬀerent. 

Sparse descriptions and basis pursuit have many uses. They can be used for de-noising or smoothing, or data compression for efficient transmission or storage of a signal. In data compression, the sender and receiver both know the dictionary, or basis elements. To send a signal to the receiver, the sender first finds a sparse representation of the signal, and then sends to the receiver only the nonzero coef- ficients (to some precision). Using these coefficients, the receiver can reconstruct (an approximation of) the original signal. 

One common approach to basis pursuit is the same as the method for regressor selection described in § 6.4 , and based on $\ell_{1}$ -norm regularization as a heuristic for finding sparse descriptions. We first solve the convex problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{m}(f(u_{i})-y_{i})^{2}+\gamma\|x\|_{1},}\end{array}
$$ 

where $\gamma\,>\,0$ is a parameter used to trade oﬀthe quality of the fit to the data, and the sparsity of the coefficient vector. The solution of this problem can be used directly, or followed by a refinement step, in which the best fit is found, using the sparsity pattern of the solution of ( 6.18 ). In other words, we first solve ( 6.18 ), to obtain x . We then set ${\mathcal{B}}=\{i\ |\ {\hat{x}}_{i}\neq0\}$ ̸ } , i.e. , the set of indices corresponding to nonzero coefficients. Then we solve the least-squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{m}(f(u_{i})-y_{i})^{2}}\end{array}
$$ 

with variables $x_{i}$ , $i\in\mathcal{B}$ , and $x_{i}=0$ for $i\notin\mathcal{B}$ . 

In basis pursuit and sparse description applications it is not uncommon to have a very large dictionary, with $n$ on the order of $10^{4}$ or much more. To be eﬀective, algorithms for solving ( 6.18 ) must exploit problem structure, which derives from the structure of the dictionary signals. 

# Time-frequency analysis via basis pursuit 

In this section we illustrate basis pursuit and sparse representation with a simple example. We consider functions (or signals) on $\mathbf{R}$ , with the range of interest [0 , 1]. We think of the independent variable as time, so we use $t$ (instead of $u$ ) to denote it. 

We first describe the basis functions in the dictionary. Each basis function is a Gaussian sinusoidal pulse , or Gabor function , with form 

$$
e^{-(t-\tau)^{2}/\sigma^{2}}\cos(\omega t+\phi),
$$ 

![](images/1d2d3447bdab0e76fc605dd2cb309c07c5840a5bb86d728f829714321c6710b9.jpg) 
Figure 6.21 Three of the basis elements in the dictionary, all with center time $\tau=0.5$ and cosine phase. The top signal has frequency $\omega=0$ , the middle one has frequency $\omega=75$ , and the bottom one has frequency $\omega=150$ . 

where $\sigma>0$ gives the width of the pulse, $\tau$ is the time of (the center of) the pulse, $\omega\geq0$ is the frequency, and $\phi$ is the phase angle. All of the basis functions have width $\sigma=0.05$ . The pulse times and frequencies are 

$$
\tau=0.002k,\quad k=0,.\;.\;.\;,500,\quad\quad\omega=5k,\quad k=0,.\;.\;.\;,30.
$$ 

For each time $\tau$ , there is one basis element with frequency zero (and phase $\phi=0$ ), and 2 basis elements (cosine and sine, i.e. , phase $\phi=0$ and $\phi=\pi/2$ ) for each of 30 remaining frequencies, so all together there are $501\times61=30561$ basis elements. The basis elements are naturally indexed by time, frequency, and phase (cosine or sine), so we denote them as 

$$
\begin{array}{r l}{f_{\tau,\omega,\mathrm{c}},\quad}&{\tau=0,0.002,\hdots,1,\qquad\omega=0,5,\hdots,150,}\\ {f_{\tau,\omega,\mathrm{s}},\quad}&{\tau=0,0.002,\hdots,1,\qquad\omega=5,\hdots,150.}\end{array}
$$ 

Three of these basis functions (all with time $\tau=0.5$ ) are shown in figure 6.21 . 

Basis pursuit with this dictionary can be thought of as a time-frequency analysis of the data. If a basis element $f_{\tau,\omega,\mathrm{c}}$ or $f_{\tau,\omega,\mathrm{s}}$ appears in the sparse representation of a signal ( i.e. , with a nonzero coefficient), we can interpret this as meaning that the data contains the frequency $\omega$ at time $\tau$ . 

We will use basis pursuit to find a sparse approximation of the signal 

$$
y(t)=a(t)\sin\theta(t)
$$ 

![](images/1450c0334538607961bc659726e0a1bd59090adeaf92b1b04f2fbad1cd3da00b.jpg) 
Figure 6.22 Top. The original signal (solid line) and approximation $\hat{y}$ ob- tained by basis pursuit (dashed line) are almost indistinguishable. Bottom. The approximation error $y(t)-\hat{y}(t)$ ), with diﬀerent vertical scale. 

where 

$$
a(t)=1+0.5\sin(11t),\qquad\theta(t)=30\sin(5t).
$$ 

(This signal is chosen only because it is simple to describe, and exhibits noticeable changes in its spectral content over time.) We can interpret $a(t)$ as the signal amplitude, and $\theta(t)$ as its total phase. We can also interpret 

$$
\omega(t)=\left|{\frac{d\theta}{d t}}\right|=150|\cos(5t)|
$$ 

as the instantaneous frequency of the signal at time $t$ . The data are given as 501 uniformly spaced samples over the interval [0 , 1], i.e. , we are given 501 pairs $(t_{k},y_{k})$ with 

$$
t_{k}=0.005k,\quad y_{k}=y(t_{k}),\quad k=0,.\,.\,,500.
$$ 

We first solve the $\ell_{1}$ -norm regularized least-squares problem ( 6.18 ), with $\gamma=$ 1. The resulting optimal coefficient vector is very sparse, with only 42 nonzero coefficients out of 30561. We then find the least-squares fit of the original signal using these 42 basis vectors. The result $\hat{y}$ is compared with the original signal $y$ in figure 6.22 . The top figure shows the approximated signal (in dashed line) and, almost indistinguishable, the original signal $y(t)$ (in solid line). The bottom figure shows the error $y(t)-\hat{y}(t)$ ). As is clear from the figure, we have obtained an 

![](images/60c4584cb0b02196b9c044a50ea20a9bd9c2fa26a669a78f0ab34d78c5c048c6.jpg) 
Figure 6.23 Top: Original signal . Bottom: Time-frequency plot . The dashed curve shows the instantaneous frequency $\omega(t)=150|\cos(5t)|$ of the original signal. Each circle corresponds to a chosen basis element in the approxima- tion obtained by basis pursuit. The horizontal axis shows the time index $\tau$ , and the vertical axis shows the frequency index $\omega$ of the basis element. 

approximation y with a very good relative fit. The relative error is 

$$
\frac{(1/501)\sum_{i=1}^{501}(y(t_{i})-\hat{y}(t_{i}))^{2}}{(1/501)\sum_{i=1}^{501}y(t_{i})^{2}}=2.6\cdot10^{-4}.
$$ 

By plotting the pattern of nonzero coefficients versus time and frequency, we obtain a time-frequency analysis of the original data. Such a plot is shown in fig- ure 6.23 , along with the instantaneous frequency. The plot shows that the nonzero components closely track the instantaneous frequency. 

# 6.5.5 Interpolation with convex functions 

In some special cases we can solve interpolation problems involving an infinite- dimensional set of functions, using finite-dimensional convex optimization. In this section we describe an example. 

We start with the following question: When does there exist a convex function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ , with $\mathbf{dom}\,f=\mathbf{R}^{k}$ , that satisfies the interpolation conditions 

$$
f(u_{i})=y_{i},\quad i=1,.\,.\,.\,,m,
$$ 

at given points $u_{i}\in\mathbf{R}^{k}?$ (Here we do not restrict $f$ to lie in any finite-dimensional subspace of functions.) The answer is: if and only if there exist $g_{1},\ldots,g_{m}$ such that 

$$
y_{j}\geq y_{i}+g_{i}^{T}(u_{j}-u_{i}),\quad i,\ j=1,.\,.\,.\,,m.
$$ 

To see this, first suppose that $f$ is convex, $\mathbf{dom}\,f\,=\,\mathbf{R}^{k}$ , and $f(u_{i})\ =\ y_{i}$ , $i=1,\ldots,m$ . At each $u_{i}$ we can find a vector $g_{i}$ such that 

$$
f(z)\geq f(u_{i})+g_{i}^{T}(z-u_{i})
$$ 

for all $z$ . If $f$ is diﬀerentiable, we can take $g_{i}=\nabla f(u_{i})$ ; in the more general case, we can construct $g_{i}$ by finding a supporting hyperplane to $\mathbf{\epsilon}_{f}$ at $(u_{i},y_{i})$ . (The vectors $g_{i}$ are called subgradients .) By applying $(6.20)$ to $z=u_{j}$ , we obtain ( 6.19 ). Conversely, suppose $g_{1},\ldots,g_{m}$ satisfy ( 6.19 ). Define $f$ as 

$$
f(z)=\operatorname*{max}_{i=1,\dots,m}(y_{i}+g_{i}^{T}(z-u_{i}))
$$ 

for all $z\,\in\,\mathbf{R}^{k}$ . Clearly, $f$ is a (piecewise-linear) convex function. The inequali- ties ( 6.19 ) imply that $f(u_{i})=y_{i}$ , for $i=1,\ldots,m$ . 

We can use this result to solve several problems involving interpolation, approx- imation, or bounding, with convex functions. 

# Fitting a convex function to given data 

Perhaps the simplest application is to compute the least-squares fit of a convex function to given data $(u_{i},y_{i})$ , $i=1,\ldots,m$ : 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{\sum_{i=1}^{m}(y_{i}-f(u_{i}))^{2}}\\ &{{\mathrm{subject~to}}}&{f:\mathbf{R}^{k}\to\mathbf{R}{\mathrm{~is~convex,}}\quad\mathbf{dom}f=\mathbf{R}^{k}.}\end{array}}
$$ 

This is an infinite-dimensional problem, since the variable is $f$ , which is in the space of continuous real-valued functions on $\mathbf{R}^{k}$ . Using the result above, we can formulate this problem as 

$$
\begin{array}{l r}{\mathrm{minimize}}&{\sum_{i=1}^{m}(y_{i}-\hat{y}_{i})^{2}}\\ {\mathrm{subject~to}}&{\hat{y}_{j}\geq\hat{y}_{i}+g_{i}^{T}(u_{j}-u_{i}),\quad i,~j=1,.\,.\,,m,}\end{array}
$$ 

which is a QP with variables $\hat{y}\in\mathbf{R}^{m}$ ∈ and $g_{1},\ldots,g_{m}\in\mathbf{R}^{k}$ . The optimal value of this problem is zero if and only if the given data can be interpolated by a convex function, i.e. , if there is a convex function that satisfies $f(u_{i})=y_{i}$ . An example is shown in figure 6.24 . 

# Bounding values of an interpolating convex function 

As another simple example, suppose that we are given data $(u_{i},y_{i})$ , $i=1,\ldots,m$ , which can be interpolated by a convex function. We would like to determine the range of possible values of $f(u_{0})$ , where $u_{0}$ is another point in $\mathbf{R}^{k}$ , and $f$ is any convex function that interpolates the given data. To find the smallest possible value of $f(u_{0})$ we solve the LP 

$$
\begin{array}{r}{\begin{array}{l l}{\mathrm{minimize}}&{y_{0}}\\ {\mathrm{subject~to}}&{y_{j}\geq y_{i}+g_{i}^{T}(u_{j}-u_{i}),\quad i,\ j=0,.\ .\ ,m,}\end{array}}\end{array}
$$ 

![](images/a52de6b805fb0f7222a8aa1ef701fb6372c049d169f4ec8ca79b945e9e63ac2a.jpg) 
Figure 6.24 Least-squares fit of a convex function to data, shown as circles. The (piecewise-linear) function shown minimizes the sum of squared fitting error, over all convex functions. 

which is an LP with variables $y_{0}\in\mathbf{R}$ , $g_{0},\ldots,g_{m}\in\mathbf{R}^{k}$ By maximizing $y_{0}$ (which is also an LP) we find the largest possible value of $f(u_{0})$ ) for a convex function that interpolates the given data. 

# Interpolation with monotone convex functions 

As an extension of convex interpolation, we can consider interpolation with a convex and monotone nondecreasing function. It can be shown that there exists a convex function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ , with $\mathbf{dom}\,f=\mathbf{R}^{k}$ , that satisfies the interpolation conditions 

$$
f(u_{i})=y_{i},\quad i=1,.\,.\,.\,,m,
$$ 

and is monotone nondecreasing ( i.e. , $f(u)\geq f(v)$ whenever $u\succeq v$ ), if and only if there exist $g_{1},\ldots,g_{m}\in\mathbf{R}^{k}$ , such that 

$$
g_{i}\succeq0,\quad i=1,\ldots,m,\qquad y_{j}\geq y_{i}+g_{i}^{T}(u_{j}-u_{i}),\quad i,\ j=1,\ldots,m.
$$ 

In other words, we add to the convex interpolation conditions ( 6.19 ), the condition that the subgradients $g_{i}$ are all nonnegative. (See exercise 6.12 .) 

# Bounding consumer preference 

As an application, we consider a problem of predicting consumer preferences. We consider diﬀerent baskets of goods , consisting of diﬀerent amounts of $n$ consumer goods. A goods basket is s ecified by a vector $x\,\in\,[0,1]^{n}$ where $x_{i}$ denotes the amount of consumer good i . We assume the amounts are normalized so that $0\,\leq\,x_{i}\,\leq\,1$ , i.e. $x_{i}\,=\,0$ is the minimum and $x_{i}\,=\,1$ s the maximum possible amount of good i . Given two baskets of goods x and ˜ , a consumer can either prefer $x$ to x , or prefer x to $x$ , or consider $x$ and x equally attractive. We consider one model consumer, whose choices are repeatable. 

We model consumer preference in the following way. We assume there is an underlying utility function $u:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , with domain $[0,1]^{n}$ ; $u(x)$ gives a measure of the utility derived by the consumer from the goods basket $x$ . Given a choice between two baskets of goods, the consumer chooses the one that has larger utility, and will be ambivalent when the two baskets have equal utility. It is reasonable to assume that $u$ is monotone nondecreasing. This means that the consumer always prefers to have more of any good, with the amounts of all other goods the same. It is also reasonable to assume that $u$ is concave. This models satiation , or decreasing marginal utility as we increase the amount of goods. 

Now suppose we are given some consumer preference data, but we do not know the underlying utility function $u$ . Specifically, we have a set of goods baskets $a_{1},\cdot\cdot\cdot,a_{m}\in[0,1]^{n}$ , and some information about preferences among them: 

$$
u(a_{i})>u(a_{j})\mathrm{~for~}(i,j)\in\mathcal{P},\qquad u(a_{i})\geq u(a_{j})\mathrm{~for~}(i,j)\in\mathcal{P}_{\mathrm{weak}},
$$ 

where $\mathcal{P}$ , $\mathcal{P}_{\mathrm{weak}}\subseteq\{1,.\,.\,.\,,m\}\times\{1,.\,.\,.\,,m\}$ are given. Here $\mathcal{P}$ gives the set of known preferences: ( $(i,j)\in\mathcal{P}$ ∈P means that basket $a_{i}$ is known to be preferred to basket $a_{j}$ . The set $\mathcal{P}_{\mathrm{weak}}$ gives the set of known weak preferences: $(i,j)\in\mathcal{P}_{\mathrm{weak}}$ means that basket $a_{i}$ is preferred to basket $a_{j}$ , or that the two baskets are equally attractive. 

We first consider the following question: How can we determine if the given data are consistent, i.e. , whether or not there exists a concave nondecreasing utility function $u$ for which ( 6.22 ) holds? This is equivalent to solving the feasibility problem 

$$
\begin{array}{l l}{\mathrm{find}}&{u}\\ {\mathrm{subject~to}}&{u:\mathbf{R}^{n}\to\mathbf{R}\mathrm{~convex~and~nondecreasing}}\\ &{u(a_{i})>u(a_{j}),\quad(i,j)\in\mathcal{P}}\\ &{u(a_{i})\geq u(a_{j}),\quad(i,j)\in\mathcal{P}_{\mathrm{weak}},}\end{array}
$$ 

with the function $u$ as the (infinite-dimensional) optimization variable. Since the constraints in ( 6.23 ) are all homogeneous, we can express the problem in the equiv- alent form 

$$
\begin{array}{l l}{\mathrm{find}}&{u}\\ {\mathrm{subject~to}}&{u:\mathbf{R}^{n}\rightarrow\mathbf{R}\mathrm{~convex~and~nondecreasing}}\\ &{u(a_{i})\geq u(a_{j})+1,\quad(i,j)\in\mathcal{P}}\\ &{u(a_{i})\geq u(a_{j}),\quad(i,j)\in\mathcal{P}_{\mathrm{weak}},}\end{array}
$$ 

which uses only nonstrict inequalities. (It is clear that if $u$ satisfies ( 6.24 ), then it must satisfy ( 6.23 ); conversely, if $u$ satisfies ( 6.23 ), then it can be scaled to satisfy ( 6.24 ).) This problem, in turn, can be cast as a (finite-dimensional) linear programming feasibility problem, using the interpolation result on page 339 : 

$$
\begin{array}{l l}{\mathrm{find}}&{u_{1},\ldots,u_{m},~g_{1},\ldots,g_{m}}\\ {\mathrm{subject~to}}&{g_{i}\succeq0,\quad i=1,\ldots,m}\\ &{u_{j}\leq u_{i}+g_{i}^{T}(a_{j}-a_{i}),\quad i,j=1,\ldots,m}\\ &{u_{i}\geq u_{j}+1,\quad(i,j)\in\mathcal{P}}\\ &{u_{i}\geq u_{j},\quad(i,j)\in\mathcal{P}_{\mathrm{weak}}.}\end{array}
$$ 

By solving this linear programming feasibility problem, we can determine whether there exists a concave, nondecreasing utility function that is consistent with the given sets of strict and nonstrict preferences. If ( 6.25 ) is feasible, there is at least one such utility function (and indeed, we can construct one that is piecewise-linear, from a feasible $u_{1},\dots,u_{m}$ , $g_{1},\ldots,g_{m})$ . If ( 6.25 ) is not feasible, we can conclude that there is no concave increasing utility function that is consistent with the given sets of strict and nonstrict preferences. 

As an example, suppose that $\mathcal{P}$ and $\mathcal{P}_{\mathrm{weak}}$ are consumer preferences that are known to be consistent with at least one concave increasing utility function. Con- sider a air $(k,l)$ that is not in $\mathcal{P}$ or $\mathcal{P}_{\mathrm{weak}}$ , i.e. , consumer preference between baskets k and l is not known. In some cases we can conclude that a preference holds between basket $k$ and $l$ , even without knowing the underlying preference function. To do this we augment the known preferences ( 6.22 ) with the inequality $u(a_{k})\,\leq\,u(a_{l})$ , which means that basket $\boldsymbol{l}$ is preferred to basket $k$ , or they are equally attractive. We then solve the feasibility linear program ( 6.25 ), including the extra weak preference $u(a_{k})\leq u(a_{l})$ . If the augmented set of preferences is in- feasible, it means that any concave nondecreasing utility function that is consistent with the original given consumer preference data must also satisfy $u(a_{k})>u(a_{l})$ . In other words, we can conclude that basket $k$ is preferred to basket $l$ , without knowing the underlying utility function. 

Example 6.9 Here we give a simple numerical example that illustrates the discussion above. We consider baskets of two goods (so we can easily plot the goods baskets). To generate the consumer preference data $\mathcal{P}$ , we compute 40 random points in $[0,1]^{2}$ , and then compare them using the utility function 

$$
\begin{array}{r}{u(x_{1},x_{2})=(1.1x_{1}^{1/2}+0.8x_{2}^{1/2})/1.9.}\end{array}
$$ 

These goods baskets, and a few level curves of the utility function $u$ , are shown in figure 6.25 . 

We now use the consumer preference data (but not, of course, the true utility function $u$ ) to compare each of these 40 goods baskets to the basket $a_{0}=(0.5,0.5)$ . For each original basket $a_{i}$ , we solve the linear programming feasibility problem described above, to see if we can conclude that basket $a_{0}$ is preferred to basket $a_{i}$ . Similarly, we check whether we can conclude that basket $a_{i}$ is preferred to basket $a_{0}$ . For each basket $a_{i}$ , there are three possible outcomes: we can conclude that $a_{0}$ is definitely preferred to $a_{i}$ , that $a_{i}$ is definitely preferred to $a_{0}$ , or (if both LP feasibility problems are feasible) that no conclusion is possible. (Here, definitely preferred means that the preference holds for any concave nondecreasing utility function that is consistent with the original given data.) 

We find that 21 of the baskets are definitely rejected in favor of (0 . 5 , 0 . 5), and 14 of the baskets are definitely preferred. We cannot make any conclusion, from the consumer preference data, about the remaining 5 baskets. These results are shown in figure 6.26 . Note that goods baskets below and to the left of (0 . 5 , 0 . 5) will definitely be rejected in favor of (0 . 5 , 0 . 5), using only the monotonicity property of the utility function, and similarly, those points that are above and to the right of (0 . 5 , 0 . 5) must be preferred. So for these 17 points, there is no need to solve the feasibility LP ( 6.25 ). Classifying the 23 points in the other two quadrants, however, requires the concavity assumption, and solving the feasibility LP ( 6.25 ). 

![](images/a885e1d2e8a72499f56879954e6d07a4846737a61d56cd781f7ee42973c45386.jpg) 
Figure 6.25 Forty goods baskets $a_{1},\dotsc,a_{40}$ , shown as circles. The 0 . 1 , $0.2,\ldots,0.9$ level curves of the true utility function $u$ are shown as dashed lines. This utility function is used to find the consumer preference data $\mathcal{P}$ among the 40 baskets. 

![](images/8086118689e2cbe8117a7d533e09d450c755f252b45e1fd116bd51b3fb7d7fb1.jpg) 
Figure 6.26 Results of consumer preference analysis using the LP ( 6.25 ), for a new goods basket $a_{0}=(0.5,0.5)$ . The original baskets are displayed as open circles if they are definitely rejected $\left(u(a_{k})\,<\,u(a_{0}\right)\right)$ ), as solid black circles if they are definitely preferred ( $\check{\mathbf{\eta}}(a_{k})\;>\;u(a_{0}))$ , and as squares when no conclusion can be made. The level curve of the underlying utility function, that passes through (0 . 5 , 0 . 5), is shown as a dashed curve. The vertical and horizontal lines passing through (0 . 5 , 0 . 5) divide $[0,1]^{2}$ into four quadrants. Points in the upper right quadrant must be preferred to (0 . 5 , 0 . 5), by the monotonicity assumption on $u$ . Similarly, (0 . 5 , 0 . 5) must be preferred to the points in the lower left quadrant. For the points in the other two quadrants, the results are not obvious. 

# Bibliography 

The robustness properties of approximations with diﬀerent penalty functions were an- alyzed by Huber [ Hub64 , Hub81 ], who also proposed the penalty function ( 6.4 ). The log-barrier penalty function arises in control theory, where it is applied to the system closed-loop frequency response, and has several names, e.g. , central $\mathbf{H}_{\infty}$ , or risk-averse control; see Boyd and Barratt [ BB91 ] and the references therein. 

Regularized approximation is covered in many books, including Tikhonov and Arsenin

 [ TA77 ] and Hansen [ Han98 ]. Tikhonov regularization is sometimes called ridge regression

 (Golub and Van Loan [ GL89 , page 564]). Least-squares approximation with $\ell_{1}$ -norm regularization is also known under the name lasso (Tibshirani [ Tib96 ]). Other least- squares regularization and regressor selection techniques are discussed and compared in Hastie, Tibshirani, and Friedman [ HTF01 , § 3.4]. 

Total variation denoising was introduced for image reconstruction by Rudin, Osher, and Fatemi [ ROF92 ]. 

The robust least-squares problem with norm bounded uncertainty (page 321 ) was in- troduced by El Ghaoui and Lebret [ EL97 ], and Chandrasekaran, Golub, Gu, and Sayed [ CGGS98 ]. El Ghaoui and Lebret also give the SDP formulation of the robust least-squares problem with structured uncertainty (page 323 ). 

Chen, Donoho, and Saunders [ CDS01 ] discuss basis pursuit via linear programming. They refer to the $\ell_{1}$ -norm regularized problem ( 6.18 ) as basis pursuit denoising . Meyer and Pratt [ MP68 ] is an early paper on the problem of bounding utility functions. 

# Exercises 

# Norm approximation and least-norm problems 

6.1 Quadratic bounds for log barrier penalty. Let $\phi:\mathbf{R}\,\rightarrow\,\mathbf{R}$ be the log barrier penalty function with limit $a>0$ : 

$$
\phi(u)=\left\{\begin{array}{l l}{{-a^{2}\log(1-(u/a)^{2})}}&{{|u|<a}}\\ {{\infty}}&{{\mathrm{otherwise.}}}\end{array}\right.
$$ 

Show that if $u\in\mathbf{R}^{m}$ satisfies $||u||_{\infty}<a$ , then 

$$
\|u\|_{2}^{2}\leq\sum_{i=1}^{m}\phi(u_{i})\leq\frac{\phi(\|u\|_{\infty})}{\|u\|_{\infty}^{2}}\|u\|_{2}^{2}.
$$ 

his means that $\textstyle\sum_{i=1}^{m}\phi(u_{i})$ ) is well approximated by $||u||_{2}^{2}$ if $\|u\|_{\infty}$ is small compared to a . For example, if ∥ $\|u\|_{\infty}/a=0.25$ ∥ 25, then 

$$
||u||_{2}^{2}\leq\sum_{i=1}^{m}\phi(u_{i})\leq1.033\cdot||u||_{2}^{2}.
$$ 

6.2 $\ell_{1}$ -, $\ell_{2}$ -, and $\ell_{\infty}$ -norm approximation by a constant vector. What is the solution of the norm approximation problem with one scalar variable $x\in\mathbf{R}$ , 

$$
{\mathrm{minimize}}\quad\|x\mathbf{1}-b\|,
$$ 

for the $\ell_{1}$ -, $\ell_{2}$ -, and $\ell_{\infty}$ -norms? 

6.3 Formulate the following approximation problems as LPs, QPs, SOCPs, or SDPs. The problem data are $A\in\mathbf{R}^{m\times n}$ and $b\in\mathbf{R}^{m}$ . The rows of $A$ are denoted $a_{i}^{\dot{T}}$ . 

(a) Deadzone-linear penalty approximation : minimize $\begin{array}{r}{\sum_{i=1}^{m}\phi(a_{i}^{T}x-b_{i})}\end{array}$ − ), where 

$$
\phi(u)=\left\{\begin{array}{l l}{0}&{|u|\leq a}\\ {|u|-a}&{|u|>a,}\end{array}\right.
$$ 

where $a>0$ . 

(b) Log-barrier penalty approximation : minimize $\textstyle\sum_{i=1}^{m}\phi\bigl(a_{i}^{T}x-b_{i}\bigr)$ ), where 

$$
\phi(u)=\left\{\begin{array}{l l}{-a^{2}\log(1-(u/a)^{2})}&{|u|<a}\\ {\infty}&{|u|\geq a,}\end{array}\right.
$$ 

with $a>0$ . 

(c) Huber penalty approximation : minimize $\begin{array}{r}{\sum_{i=1}^{m}\phi(a_{i}^{T}x-b_{i})}\end{array}$ ), where 

$$
\phi(u)=\left\{\begin{array}{l l}{u^{2}}&{|u|\leq M}\\ {M(2|u|-M)}&{|u|>M,}\end{array}\right.
$$ 

with $M>0$ . 

(d) hebyshev approximation : minimize $\mathrm{max}_{i=1,\dots,m}\,\big|\log\bigl(a_{i}^{T}x\bigr)-\log b_{i}\big|$ − | . We assume $b\succ0$ ≻ 0. An equivalent convex form is 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{t}}\\ {{\mathrm{subject~to}}}&{{1/t\leq a_{i}^{T}x/b_{i}\leq t,}}&{{i=1,.\,.\,.\,,m,}}\end{array}
$$ 

with variables $x\in\mathbf{R}^{n}$ and $t\in\mathbf{R}$ , and domain $\mathbf{R}^{n}\times\mathbf{R}_{++}$ . (e) Minimizing the sum of the largest $k$ residuals: 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\sum_{i=1}^{k}|r|_{[i]}}\\ &{{\mathrm{subject~to}}\quad r=A x-b,}\end{array}}
$$ 

where $|r|_{[1]}\;\geq\;|r|_{[2]}\;\geq\;\cdot\cdot\;\geq\;|r|_{[m]}$ are the mbers $\left|\boldsymbol{r}_{1}\right|$ , $|r_{2}|$ , . . . , $|r_{m}|$ in decreasing order. (For k = 1, this reduces to ℓ $\ell_{\infty}$ -norm approximation; for k $k=m$ , it ∞ reduces to $\ell_{1}$ -norm approximation.) Hint. See exercise 5.19 . 

6.4 A diﬀerentiable approximation of $\ell_{1}$ -norm approximation. The function $\phi(u)=(u^{2}{+}\epsilon)^{1/2}$ , with parameter $\epsilon>0$ , is sometimes used as a diﬀerentiable approximation of the absolute value function $|u|$ . To approximately solve the $\ell_{1}$ -norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{1},
$$ 

where $A\in\mathbf{R}^{m\times n}$ , we solve instead the problem 

$$
\begin{array}{r}{\mathrm{minimize}\quad\sum_{i=1}^{m}\phi(a_{i}^{T}x-b_{i}),}\end{array}
$$ 

where $a_{i}^{T}$ is the $i$ th row of $A$ . We assume $\mathbf{rank}\,A=n$ . 

Let $p^{\star}$ denote the optimal value of the $\ell_{1}$ -norm approximation problem ( 6.26 ). Let x denote the optimal solution of the approximate problem ( 6.27 ), and let r denote the associated residual, $\hat{r}=A\hat{x}-b$ b . 

(a) Show that $\begin{array}{r}{p^{\star}\geq\sum_{i=1}^{m}\hat{r}_{i}^{2}/(\hat{r}_{i}^{2}+\epsilon)^{1/2}}\end{array}$ . 

(b) Show that 

$$
\|A\hat{x}-b\|_{1}\leq p^{\star}+\sum_{i=1}^{m}|\hat{r}_{i}|\left(1-\frac{|\hat{r}_{i}|}{(\hat{r}_{i}^{2}+\epsilon)^{1/2}}\right).
$$ 

(By evaluating the righthand side after computing x , we obtain a bound on how subop- timal x is for the $\ell_{1}$ -norm approximation problem.) 

6.5 Minimum length approximation. Consider the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ \operatorname{length}(x)}\\ {{\mathrm{subject~to}}}&{\|A x-b\|\leq\epsilon,}\end{array}}
$$ 

where $\mathrm{{length}}(x)\,=\,\operatorname*{min}\{k\,\mid\,x_{i}\,=\,0$ $i\;>\;k\}$ . roblem variable is $x\,\in\,\mathbf{R}^{\,n}$ ; the problem parameters are $A\in\mathbf{R}^{m\times n}$ ∈ , b $b\in\mathbf{R}^{m}$ ∈ , an $\epsilon>0$ 0. In a regression context, we are asked to find the minimum number of columns of A , taken in order, that can approximate the vector $b$ within $\epsilon$ . 

Show that this is a quasiconvex optimization problem. 

6.6 Duals of some penalty function approximation problems. Derive a Lagrange dual for the problem 

$$
\begin{array}{l c l}{{\mathrm{minimize}}}&{{\sum_{i=1}^{m}\phi(\boldsymbol{r_{i}})}}\\ {{\mathrm{subject~to}}}&{{\boldsymbol{r}=\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b},}}\end{array}
$$ 

for the following penalty functions $\phi:\mathbf{R}\rightarrow\mathbf{R}$ . The variables are $x\in\mathbf{R}^{n}$ , $r\in\mathbf{R}^{m}$ (a) Deadzone-linear penalty (with deadzone width $a=1$ ), 

$$
\phi(u)=\left\{\begin{array}{l l}{0}&{|u|\leq1}\\ {|u|-1}&{|u|>1.}\end{array}\right.
$$ 

(b) Huber penalty (with $M=1$ ), 

$$
\phi(u)=\left\{\begin{array}{l l}{u^{2}}&{|u|\leq1}\\ {2|u|-1}&{|u|>1.}\end{array}\right.
$$ 

(c) Log-barrier (with limit $a=1$ ), 

$$
\phi(u)=-\log(1-u^{2}),\qquad\mathrm{dom}\,\phi=(-1,1).
$$ 

(d) Relative deviation from one, 

$$
\phi(u)=\operatorname*{max}\{u,1/u\}=\left\{\begin{array}{l l}{u}&{u\geq1}\\ {1/u}&{u\leq1,}\end{array}\right.
$$ 

with $\mathbf{dom}\,\phi=\mathbf{R}_{++}$ . 

# Regularization and robust approximation 

6.7 $B i$ -criterion optimization with Euclidean norms. We consider the bi-criterion optimization problem 

$$
\mathrm{minimize}\ (\mathrm{w.r.t.}\ \mathbf{R}_{+}^{2})\quad(\|A x-b\|_{2}^{2},\|x\|_{2}^{2}),
$$ 

where $A\in\mathbf{R}^{m\times n}$ has rank $r$ , and $b\in\mathbf{R}^{m}$ . Show how to fin the solution of each of the following problems from the singular value decomposition of A , 

$$
\boldsymbol{A}=\boldsymbol{U}\,\mathbf{diag}(\boldsymbol{\sigma})\boldsymbol{V}^{T}=\sum_{i=1}^{r}\sigma_{i}\boldsymbol{u_{i}}\boldsymbol{v}_{i}^{T}
$$ 

(see A.5.4 ). 

(a) Tikhonov regularization: minimize $\|A x-b\|_{2}^{2}+\delta\|x\|_{2}^{2}$ ∥ ∥ . 

(b) Minimize $||A x-b||_{2}^{2}$ subject to $\|x\|_{2}^{2}=\gamma$ . 

(c) Maximize $||A x-b||_{2}^{2}$ subject to $\|x\|_{2}^{2}=\gamma$ 

Here $\delta$ and $\gamma$ are positive parameters. 

Your results provide efficient methods for computing the optimal trade-oﬀcurve and the set of achievable values of the bi-criterion problem. 

6.8 Formulate the following robust approximation problems as LPs, QPs, SOCPs, or SDPs. For each subproblem, consider the $\ell_{1}$ -, $\ell_{2}$ -, and the $\ell_{\infty}$ -norms. 

(a) Stochastic robust approximation with a finite set of parameter values , i.e. , the sum- of-norms problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i=1}^{k}p_{i}\|A_{i}x-b\|}\end{array}
$$ 

where $p\succeq0$ and $\mathbf{1}^{T}p=1$ . (See 6.4.1 .) (b) Worst-case robust approximation with coefficient bounds: 

$$
{\begin{array}{r l}{{\underset{\operatorname{minimize}}{\operatorname{minimize}}}}&{\operatorname*{sup}_{A\in{\mathcal{A}}}\left\|A x-b\right\|}\end{array}}
$$ 

where 

$$
\begin{array}{r}{\mathcal{A}=\{A\in\mathbf{R}^{m\times n}\mid l_{i j}\le a_{i j}\le u_{i j},\ i=1,\ldots,m,\ j=1,\ldots,n\}.}\end{array}
$$ 

Here the uncertainty set is described by giving upper and lower bounds for the components of $A$ . We assume $l_{i j}<u_{i j}$ . 

(c) Worst-case robust approximation with polyhedral uncertainty: 

$$
{\begin{array}{r l}{{\underset{\operatorname{minimize}}{\operatorname{minimize}}}}&{\operatorname*{sup}_{A\in{\mathcal{A}}}\left\|A x-b\right\|}\end{array}}
$$ 

where 

$$
{\mathcal{A}}=\{[a_{1}\,\,\cdot\cdot\cdot\,\,a_{m}]^{T}\,\,|\,\,C_{i}a_{i}\preceq d_{i},\,\,i=1,\.\,.\,.\,,m\}.
$$ 

The uncertainty is described by giving o $\mathcal{P}_{i}=\{a_{i}\mid C_{i}a_{i}\preceq d_{i}\}$ of possible values for each row. The param ers C $C_{i}\in\mathbf{R}^{p_{i}\times n}$ ∈ , d $d_{i}\in\mathbf{R}^{p_{i}}$ ∈ , i $i=1,\ldots,m$ , are given. We assume that the polyhedra P are nonempty and bounded. 

# Function fitting and interpolation 

6.9 Minimax rational function fitting. Show that the following problem is quasiconvex: 

$$
{\mathrm{minimize}}\quad\operatorname*{max}_{i=1,\dots,k}\left|{\frac{p(t_{i})}{q(t_{i})}}-y_{i}\right|
$$ 

where 

$$
a_{0}+a_{1}t+a_{2}t^{2}+\cdot\cdot\cdot+a_{m}t^{m},\qquad q(t)=1+b_{1}t+a_{m}t^{m}.
$$ 

and the domain of the objective function is defined as 

$$
D=\{(a,b)\in{\bf R}^{m+1}\times{\bf R}^{n}\mid q(t)>0,\ \alpha\leq t\leq\beta\}.
$$ 

In this problem we fit a rational function $p(t)/q(t)$ to given data, while constraining the denominator polynomial to be positive on the interval $[\alpha,\beta]$ . The optimization variables are the numerator and denomin ents $\boldsymbol{u}_{i}$ , $b_{i}$ . The interpolation points $t_{i}\in[\alpha,\beta]$ , and desired function values $y_{i}$ , $i=1,\ldots,k$ , are given. 

6.10 Fitting data with a concave nonnegative nondecreasing quadratic function. We are given the data 

$$
x_{1},\hdots,x_{N}\in\mathbf{R}^{n},\qquad y_{1},.\hdots,y_{N}\in\mathbf{R},
$$ 

and wish to fit a quadratic function of the form 

$$
f(\boldsymbol{x})=(1/2)\boldsymbol{x}^{T}\boldsymbol{P}\boldsymbol{x}+\boldsymbol{q}^{T}\boldsymbol{x}+\boldsymbol{r},
$$ 

where $P\in\mathbf{S}^{n}$ , $q\in\mathbf{R}^{n}$ , and $r\in\mathbf{R}$ are the parameters in the model (and, therefore, the variables in the fitting problem). 

model will be used only on the box ${\mathcal{B}}=\{x\in\mathbf{R}^{n}\mid l\preceq x\preceq u\}$ . You can assume that $l\prec u$ ≺ , and that the given data points $x_{i}$ are in this box. 

We will use the simple sum of squared errors objective, 

$$
\sum_{i=1}^{N}(f(x_{i})-y_{i})^{2},
$$ 

as the criterion for the fit. We also impose several constraints on the function $f$ . First, it must be concave. Second, it must be nonnegative on $\mathcal{B}$ , i.e. , $f(z)\,\geq\,0$ for all $z\,\in\,\mathcal{B}$ . Third, $f$ must be nondecreasing on B , i.e. , whenever $z,\;\;\tilde{z}\;\in\;{\mathcal{B}}$ ∈B satisfy $z\ \preceq\ \tilde{z}$ , we have $f\!\left(z\right)\leq f\!\left(\tilde{z}\right)$ ). 

Show how to formulate this fitting problem as a convex problem. Simplify your formula- tion as much as you can. 

6.11 Least-squares direct tion. Suppose $F_{1},.\,.\,.\,,F_{n}\,:\,\mathbf{R}^{k}\,\rightarrow\,\mathbf{R}^{p}$ , and we form the linear combination F $F:\mathbf{R}^{k}\rightarrow\mathbf{R}^{p}$ , 

$$
F(u)=x_{1}F_{1}(u)+\cdot\cdot\cdot+x_{n}F_{n}(u),
$$ 

where $x$ is the variable in the interpolation problem. 

In this problem we require that $\angle(F(v_{j}),q_{j})=0$ , $j=1,\cdot\cdot\cdot,m$ , where $q_{j}$ are given vectors $\mathbf{R}^{p}$ , which we assume satisfy $\|q_{j}\|_{2}\,=\,1$ . In other words, we require the direction of F to take on specified values at the points $v_{j}$ . To ensure that $F(v_{j})$ is not zero (which ngle un d), we impose the minimum length constraints $\|F(v_{j})\|_{2}\,\ge\,\epsilon$ , $j=1,\dots,m$ , where ǫ > 0 is given. 

Show how to find $x$ that minimizes $\|{\boldsymbol x}\|^{2}$ , and satisfies the direction (and minimum length) conditions above, using convex optimization. 

6.12 Interpolation with m ton A function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ is monotone nondecreas- ing (with respect to $\mathbf{R}_{+}^{k}$ ) if $f(u)\geq f(v)$ ≥ ) whenever $u\succeq v$ . 

(a) Show that there exists a monotone nondecreasing function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ , that satisfies $f(u_{i})=y_{i}$ for $i=1,\ldots,m$ , if and only if 

$$
y_{i}\geq y_{j}{\mathrm{~whenever~}}u_{i}\succeq u_{j},\quad i,~j=1,.\,.\,.\,,m.
$$ 

(b) Show that there exists a convex monotone nondecreasing function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ , with $\mathbf{dom}\,f\,=\,\mathbf{R}^{k}$ , that satisfies $f(u_{i})\,=\,y_{i}$ for $i\,=\,1,.\,.\,.\,,m$ , if and only if there exist $g_{i}\in\mathbf{R}^{k}$ , $i=1,\ldots,m$ , such that 

$$
g_{i}\succeq0,\quad i=1,\ldots,m,\qquad y_{j}\geq y_{i}+g_{i}^{T}(u_{j}-u_{i}),\quad i,\ j=1,\ldots,m.
$$ 

6.13 Interpolation with quasiconvex functions. Show that there exists a quasiconvex function $f:\mathbf{R}^{k}\rightarrow\mathbf{R}$ , that satisfies $f(u_{i})=y_{i}$ for $i=1,\ldots,m$ , if and only if there exist $g_{i}\in\mathbf{R}^{k}$ , $i=1,\ldots,m$ , such that 

$$
g_{i}^{T}(u_{j}-u_{i})\le-1\mathrm{~whenever~}y_{j}<y_{i},\quad i,\ j=1,\ldots,m.
$$ 

6.14 [ Nes00 ] Inte n with posit eal functions. Suppo $z_{1},.\,.\,.\,,z_{n}\,\in{\bf C}$ are $n$ distinct points w $|z_{i}|>1$ . We define $K_{\mathrm{np}}$ as the set of vectors y $y\in\mathbf{C}^{n}$ ∈ for which there exists a function f $f:\mathbf{C}\rightarrow\mathbf{C}$ → that satisfies the following conditions. 

• $f$ is positive-real , which means it is analytic outside the ( i.e $|z|>1$ ), and its real part is nonnegative outside the unit circle ( $\Re f(z)\geq0$ ℜ ≥ 0 for | $|z|>1$ | 1). • $f$ satisfies the interpolation conditions 

$$
\cdot(z_{1})=y_{1},\qquad f(z_{2})=y_{2},\qquad.\cdot\cdot,\qquad f(z_{n})=y_{n}
$$ 

If we denote the set of positive-real functions as $\mathcal{F}$ , then we can express $K_{\mathrm{np}}$ as 

$$
K_{\mathrm{np}}=\{y\in{\bf C}^{n}\mid\exists f\in{\mathcal F},\ y_{k}=f(z_{k}),\ k=1,.\,.\,.\,,n\}.
$$ 

(a) It can be shown that $f$ is positive-real if and only if there exists a nondecreasing function $\rho$ such that for all $z$ with $|z|>1$ , 

$$
f(z)=i\Im f(\infty)+\int_{0}^{2\pi}{\frac{e^{i\theta}+z^{-1}}{e^{i\theta}-z^{-1}}}\,d\rho(\theta),
$$ 

where $i=\sqrt{-1}$ (see [ KN77 , page 389]). Use this representation to show that $K_{\mathrm{np}}$ is a closed convex cone. 

(b) We will use the inner product $\Re(x^{H}y)$ between vectors $x,y\in\mathbf{C}^{n}$ , where $x^{H}$ denotes the complex conjugate transpose of x . Show that the dual cone of $K_{\mathrm{np}}$ is given by 

$$
K_{\mathrm{np}}^{*}=\left\{x\in\mathbf{C}^{n}\ \left|\ \Im(\mathbf{1}^{T}x)=0,\ \Re\left(\sum_{l=1}^{n}x_{l}{\frac{e^{-i\theta}+\bar{z}_{l}^{-1}}{e^{-i\theta}-\bar{z}_{l}^{-1}}}\right)\geq0\ \forall\theta\in[0,2\pi]\ \right\}.
$$ 

(c) Show that 

$$
K_{\mathrm{np}}^{*}=\left\{x\in{\bf C}^{n}~\left|~\exists Q\in{\bf H}_{+}^{n},~x_{l}=\sum_{k=1}^{n}\frac{Q_{k l}}{1-z_{k}^{-1}\bar{z}_{l}^{-1}},~l=1,\dots,n\right.\right\}
$$ 

where $\mathbf{H}_{+}^{n}$ denotes the set of positive semidefinite Hermitian matrices of size $n\times n$ . Use the following result (known as Riesz-Fej´ er theorem ; see [ KN77 , page 60]). A function of the form 

$$
\sum_{k=0}^{n}(y_{k}e^{-i k\theta}+\bar{y}_{k}e^{i k\theta})
$$ 

is nonnegative for all $\theta$ if and only if there exist $a_{0},.\,.\,.\,,a_{n}\in\mathbf{C}$ such that 

$$
\sum_{k=0}^{n}(y_{k}e^{-i k\theta}+{\bar{y}}_{k}e^{i k\theta})=\left|\sum_{k=0}^{n}a_{k}e^{i k\theta}\right|^{2}.
$$ 

(d) Show that $K_{\mathrm{np}}=\{y\in{\bf C}^{n}\mid P(y)\succeq0\}$ where $P(y)\in\mathbf{H}^{n}$ is defined as 

$$
P(y)_{k l}=\frac{y_{k}+\overline{{y}}_{l}}{1-z_{k}^{-1}\bar{z}_{l}^{-1}},\quad l,k=1,.\,.\,.\,,n.
$$ 

The matrix $P(y)$ is called the Nevanlinna-Pick matrix associated with the points $z_{k}$ , $y_{k}$ . 

Hint. As we noted in part (a), $K_{\mathrm{np}}$ is a closed convex cone, so $K_{\mathrm{np}}=K_{\mathrm{np}}^{**}$ .

 (e) As an application, pose the following problem as a convex optimization problem: 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\sum_{k=1}^{n}|f(z_{k})-w_{k}|^{2}}\\ &{\mathrm{subject~to}\quad f\in\mathcal{F}.}\end{array}
$$ 

The problem data are $n$ points $\omega_{k}$ with $|z_{k}|>1$ and $n$ complex numbers $w_{1}$ , . . . , $w_{n}$ . We optimize over all positive-real functions $f$ . 

# Chapter 7 

# Statistical estimation 

# 7.1 Parametric distribution estimation 

# 7.1.1 Maximum likelihood estimation 

We consider a family of probability distributions on $\mathbf{R}^{m}$ , indexed by a vector $x\in\mathbf{R}^{n}$ , with densities $p_{x}(\cdot)$ . When considered as a function of $x$ , for fixed $y\in\mathbf{R}^{m}$ , the function $p_{x}(y)$ is called the likelihood function. It is more convenient to work with its logarithm, which is called the log-likelihood function , and denoted $l$ : 

$$
l(x)=\log p_{x}(y).
$$ 

There are often constraints on the values of the parameter $x$ , which can repre- sent prior knowledge about $x$ , or the domain of the likelihood function. These constraints can be explicitly given, or incorporated into the likelihood function by assigning $p_{x}(y)=0$ (for all $y$ ) whenever $x$ does not satisfy the prior information constraints. (Thus, the log-likelihood function can be assigned the value $-\infty$ for parameters $x$ that violate the prior information constraints.) 

Now consider the problem of estimating the value of the parameter $x$ , based on observing one sample $y$ from the distribution. A widely used method, called maximum likelihood (ML) estimation , is to estimate $x$ as 

$$
\begin{array}{r}{\hat{x}_{\mathrm{ml}}=\mathrm{argmax}_{x}p_{x}(y)=\mathrm{argmax}_{x}l(x),}\end{array}
$$ 

i.e. , to choose as our estimate a value of the parameter that maximizes the like- lihood (or log-likelihood) function for the observed value of $y$ . If we have prior information about $x$ , such as $x\;\in\;C\;\subseteq\;\mathbf{R}^{n}$ , we can add the constraint $x\,\in\,C$ explicitly, or impose it implicitly, by redefining $p_{x}(y)$ to be zero for $x\notin C$ . 

The problem of finding a maximum likelihood estimate of the parameter vector $x$ can be expressed as 

$$
\begin{array}{l r c l}{{\mathrm{maximize}}}&{{l(x)=\log p_{x}(y)}}\\ {{\mathrm{subject~to}}}&{{x\in C,}}\end{array}
$$ 

where $x\,\in\,C$ gives the prior information or othe raints on the parameter n vector $x$ . In this optimization problem, the vector x $x\in\mathbf{R}^{n}$ ∈ (which is the parameter in the probability density) is the variable, and the vector $y\,\in\,\mathbf{R}^{m}$ (which is the observed sample) is a problem parameter. 

The maximum likelihood estimation problem ( 7.1 ) is a convex optimization problem if the log-likelihood function $l$ is concave for each value of , and the set $y$ $C$ can be described by a set of linear equality and convex inequality constraints, a situation which occurs in many estimation problems. For these problems we can compute an ML estimate using convex optimization. 

# Linear measurements with IID noise 

We consider a linear measurement model, 

$$
y_{i}=a_{i}^{T}x+v_{i},\quad i=1,\ldots,m,
$$ 

where $x\in\mathbf{R}^{n}$ is a vector of parameters to be estimated, $y_{i}\in\mathbf{R}$ are the measured or observed quantities, and $v_{i}$ are the measurement errors or noise. We assume that are independent, identically distributed (IID), with density on $\mathbf{R}$ . The $v_{i}$ $p$ likelihood function is then 

$$
p_{x}(y)=\prod_{i=1}^{m}p(y_{i}-a_{i}^{T}x),
$$ 

so the log-likelihood function is 

$$
l(x)=\log p_{x}(y)=\sum_{i=1}^{m}\log p(y_{i}-a_{i}^{T}x).
$$ 

The ML estimate is any optimal point for the problem 

$$
\begin{array}{r}{\mathrm{maximize}\quad\sum_{i=1}^{m}\log p(y_{i}-a_{i}^{T}x),}\end{array}
$$ 

with variable $x$ . If the density $p$ is log-concave, this problem is convex, and has the form of a penalty approximation problem (( 6.2 ), page 294 ), with penalty function $-\log p$ . 

Example 7.1 ML estimation for some common noise densities. 

• Gaussian noise. When $v_{i}$ are Gaussian with zero mean and variance $\sigma^{2}$ , the density is $p(z)=(2\pi\sigma^{2})^{-1/2}e^{-z^{2}/2\sigma^{2}}$ , and the log-likelihood function is 

$$
l(x)=-(m/2)\log(2\pi\sigma^{2})-\frac{1}{2\sigma^{2}}\|A x-y\|_{2}^{2},
$$ 

where $A$ is the matrix with rows $a_{1}^{T},\cdot\cdot\cdot,a_{m}^{T}$ . Therefore the ML estimate of $x$ is $x_{\mathrm{{ml}}}\,=\,\mathrm{argmin}_{x}\:\|A x\,-\,y\|_{2}^{2}$ , the solution of a least-squares approximation problem. • Laplacian noise. When $v_{i}$ are Laplacian, i.e. , have density $p(z)=(1/2a)e^{-|z|/a}$ (where $a>0$ ), the ML estimate is $\hat{x}=\mathrm{argmin}_{x}\,\|A x-y\|_{1}$ ∥ − ∥ , the solution of the $\ell_{1}$ -norm approximation problem. • Uniform noise. When $v_{i}$ are uniformly distributed on $[-a,a]$ , we have $p(z)=$ $1/(2a)$ on $[-a,a]$ , and an ML estimate is any $x$ satisfying $\|A x-y\|_{\infty}\leq a$ . 

# ML interpretation of penalty function approximation 

Conversely, we can interpret any penalty function approximation problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{{\sum_{i=1}^{m}\phi(b_{i}-a_{i}^{T}x)}}\end{array}}
$$ 

as a maximum likelihood estimation problem, with noise density 

$$
p(z)=\frac{e^{-\phi(z)}}{\int e^{-\phi(u)}~d u},
$$ 

and measurements $b$ . This observation gives a statistical interpretation of the penalty function approximation problem. Suppose, for example, that the penalty function $\phi$ grows very rapidly for large values, which means that we attach a very large cost or penalty to large residuals. The corresponding noise density function $p$ will have very small tails, and the ML estimator will avoid (if possible) estimates with any large residuals because these correspond to very unlikely events. 

We can also understand the robustness of $\ell_{1}$ -norm approximation to large errors in terms of maximum likelihood estimation. We interpret $\ell_{1}$ -norm approximation as maximum likelihood estimation with a noise density that is Laplacian; $\ell_{2}$ -norm approximation is maximum likelihood estimation with a Gaussian noise density. The Laplacian density has larger tails than the Gaussian, i.e. , the probability of a very large $v_{i}$ is far larger with a Laplacian than a Gaussian density. As a result, the associated maximum likelihood method expects to see greater numbers of large residuals. 

# Counting problems with Poisson distribution 

In a wide variety of problems the random variable $y$ is nonnegative integer valued, with a Poisson distribution with mean $\mu>0$ : 

$$
\mathbf{prob}(y=k)=\frac{e^{-\mu}\mu^{k}}{k!}.
$$ 

Often $y$ represents the count or number of events (such as photon arrivals, traffic accidents, etc.) of a Poisson process over some period of time. 

In a simple statistical model, the mean $\mu$ is modeled as an affine function of a vector $u\in\mathbf{R}^{n}$ : 

$$
\mu=a^{T}u+b.
$$ 

Here $u$ is called the vector of explanatory variables , and the vector $a\in\mathbf{R}^{n}$ and number $b\in\mathbf{R}$ are called the model parameters . For example, if is the number $y$ of traffic accidents in some region over some period, $u_{1}$ might be the total traffic ﬂow through the region during the period, $u_{2}$ the rainfall in the region during the period, and so on. 

We are given a number of observations which consist of pairs $(u_{i},y_{i})$ , $\textit{i}=$ $1,\cdot\cdot\cdot,m$ , where $y_{i}$ is the observed value of $y$ for which the value of the explanatory variable is $u_{i}\in\mathbf{R}^{n}$ . Our to find a maximum likelihood estimate of the model n parameters a $a\in\mathbf{R}^{n}$ ∈ and b $b\in\mathbf{R}$ ∈ R from these data. 

The likelihood function has the form 

$$
\prod_{i=1}^{m}{\frac{(a^{T}u_{i}+b)^{y_{i}}\exp(-(a^{T}u_{i}+b))}{y_{i}!}},
$$ 

so the log-likelihood function is 

$$
l(a,b)=\sum_{i=1}^{m}(y_{i}\log(a^{T}u_{i}+b)-(a^{T}u_{i}+b)-\log(y_{i}!)).
$$ 

We can find an ML estimate of $a$ and $b$ by solving the convex optimization problem 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\sum_{i=1}^{m}(y_{i}\log(a^{T}u_{i}+b)-(a^{T}u_{i}+b)),}\end{array}
$$ 

where the variables are $a$ and . 

# Logistic regression 

We consider a random variable $y\in\{0,1\}$ , with 

$$
\mathbf{prob}(y=1)=p,\qquad\mathbf{prob}(y=0)=1-p,
$$ 

where $p\,\in\,[0,1]$ , and is assumed to depend on a vector of explanatory variables $u\in\mathbf{R}^{n}$ . For example, $y=1$ might mean that an individual in a population acquires a certain disease. The probability of acquiring the disease is $p$ , which is modeled as a function of some explanatory variables $u$ , which might represent weight, age, height, blood pressure, and other medically relevant variables. 

The logistic model has the form 

$$
p=\frac{\exp(a^{T}u+b)}{1+\exp(a^{T}u+b)},
$$ 

where $a\,\in\,\mathbf{R}^{n}$ and $b~\in~\mathbf{R}$ are the model parameters that determine how the probability $p$ varies as a function of the explanatory variable $u$ . 

Now suppose we are given some data consisting of a set of values of the explana- ariables $u_{1},.\,.\,.\,,u_{m}\in\mathbf{R}^{n}$ along with the corresponding outcomes $y_{1},\dotsc,y_{m}\in$ $\{0,1\}$ Our job is to find a maximum likelihood estimate of the model parameters n $a\in\mathbf{R}^{\mathcal{N}}$ ∈ and $b\in\mathbf{R}$ . Finding an ML estimate of $a$ and $b$ is sometimes called logistic regression . 

We can re-order the data so for $u_{1},\dotsc,u_{q}$ , the outcome is $\textit{y}=\mathrm{~1~}$ , and for $u_{q+1},\cdot\cdot\cdot,u_{m}$ the outcome is $y=0$ . The likelihood function then has the form 

$$
\prod_{i=1}^{q}p_{i}\prod_{i=q+1}^{m}(1-p_{i}),
$$ 

where $p_{i}$ is given by the logistic model with explanatory variable $u_{i}$ . The log- likelihood function has the form 

$$
l(a,b)\;\;=\;\;\sum_{i=1}^{q}\log p_{i}+\sum_{i=q+1}^{m}\log(1-p_{i})
$$ 

![](images/1c3ba762afce1c52fa689e41d92ef2b7098752c566a46830536aa8948092fda8.jpg) 
Figure 7.1 Logistic regression. The circles show 50 points $\left({{u_{i}},{y_{i}}}\right)$ , where $u_{i}\,\in\,\mathbf{R}$ is the explan variable, and $y_{i}\ \in\ \{0,1\}$ is the outcome e data suggest that for u < 5 or so, the outcome is more likely to be y = 0, while for $u\,>\,5$ or so, the outcome is more likely to be $y\,=\,1$ . The data also suggest that for $u\,<\,2$ or so, the outcome is very likely to be $y\,=\,0$ , and for $u\ >\ 8$ or so, the outcome is very likely to be $y\;=\;1$ . The solid curve shows $\mathbf{prob}(y=1)=\exp(a u+b)/(1+\exp(a u+b))$ for the maximum likelihood parameters $a$ , $b$ . This maximum likelihood model is consistent with our informal observations about the data set. 

$$
\begin{array}{r l}{=}&{\displaystyle\sum_{i=1}^{q}\log\frac{\exp(a^{T}{u_{i}}+b)}{1+\exp(a^{T}{u_{i}}+b)}+\displaystyle\sum_{i=q+1}^{m}\log\frac{1}{1+\exp(a^{T}{u_{i}}+b)}}\\ {=}&{\displaystyle\sum_{i=1}^{q}(a^{T}{u_{i}}+b)-\displaystyle\sum_{i=1}^{m}\log(1+\exp(a^{T}{u_{i}}+b)).}\end{array}
$$ 

Since $l$ is a concave function of $a$ and $b$ , the logistic regression problem can be solved as a convex optimization problem. Figure 7.1 shows an example with $u\in\mathbf{R}$ . 

# Covariance estimation for Gaussian variables 

Suppos $y\,\in\,\mathbf{R}^{n}$ is a Gaussian random variable with zero mean and covariance matrix $R=\mathbf{E}\,y y^{T}$ , so its density is 

$$
p_{R}(y)=(2\pi)^{-n/2}\operatorname*{det}(R)^{-1/2}\exp(-y^{T}R^{-1}y/2),
$$ 

where $R\,\in\,\mathbf{S}_{++}^{n}$ . We want to estimate the covariance matrix $R$ based on $N$ in- dependent sampl $y_{1},\dotsc,y_{N}\in\mathbf{R}^{n}$ drawn from the distribution, and using prior knowledge about R . 

The log-likelihood function has the form 

$$
\begin{array}{l c l}{l(R)}&{=}&{\log p_{R}(y_{1},.\,.\,.\,,y_{N})}\end{array}
$$ 

$$
\begin{array}{r c l}{{}}&{{=}}&{{\displaystyle-(N n/2)\log(2\pi)-(N/2)\log\operatorname*{det}R-(1/2)\sum_{k=1}^{N}y_{k}^{T}R^{-1}y_{k}}}\\ {{}}&{{=}}&{{\displaystyle-(N n/2)\log(2\pi)-(N/2)\log\operatorname*{det}R-(N/2)\,{\bf t r}(R^{-1}Y),}}\end{array}
$$ 

where 

$$
Y=\frac{1}{N}\sum_{k=1}^{N}y_{k}y_{k}^{T}
$$ 

is the sample covariance of $y_{1},\dotsc,y_{N}$ . This log-likelihood function is not a concave function of $R$ (although it is concave on a subset of its domain $\mathbf{S}_{++}^{n}$ ; see exercise 7.4 ), but a change of variable yields a concave log-likelihood function. Let $S$ denote the inverse of the covariance matrix, $S=R^{-1}$ (which is called the information matrix ). Using $S$ in place of $R$ as a new parameter, the log-likelihood function has the form 

$$
l(S)=-(N n/2)\log(2\pi)+(N/2)\log\operatorname*{det}S-(N/2)\,{\bf t r}(S Y),
$$ 

which is a concave function of $S$ . 

Therefore the ML estimate of $S$ (hence, $R$ ) is found by solving the problem 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\log\operatorname*{det}S-\mathbf{tr}(S Y)}\\ {\mathrm{subject~to}}&{{}S\in\mathcal{S}}\end{array}
$$ 

where $S$ is our prior knowledge of $S=R^{-1}$ . (We also have the implicit constraint that $S\in\mathbf{S}_{++}^{n}$ .) Since the objective function is concave, this is a convex problem if the set S can be described by a set of linear equality and convex inequality constraints. 

First we examine the case in which no prior assumptions are made on $R$ (hence, $S$ ), other than $R\succ0$ . In th he problem ( 7.4 can be so ytically. The gradient of the objective is $S^{-1}\!-\!Y$ − , so the optimal S satisfies S $S^{-1}=Y$ if $Y\in\mathbf{S}_{++}^{n}$ . (If $Y\notin\mathbf{S}_{++}^{n}$ , the log-likelihood function is unbounded above.) Therefore, when we have no prior assumptions about $R$ , the maximum likelihood estimate of the covariance is, simply, the sample covariance: ${\hat{R}}_{\mathrm{ml}}=Y$ . 

Now we consider some examples of constraints on $R$ that can be expressed as convex constraints on the information matrix $S$ . We can handle lower and upper (matrix) bounds on $R$ , of the form 

$$
L\preceq R\preceq U,
$$ 

where $L$ and $U$ are symmetric and positive definite, as 

$$
\begin{array}{r}{U^{-1}\preceq R^{-1}\preceq L^{-1}.}\end{array}
$$ 

A condition number constraint on $R$ , 

$$
\lambda_{\operatorname*{max}}(R)\leq\kappa_{\operatorname*{max}}\lambda_{\operatorname*{min}}(R),
$$ 

can be expressed as 

$$
\lambda_{\operatorname*{max}}(S)\leq\kappa_{\operatorname*{max}}\lambda_{\operatorname*{min}}(S).
$$ 

This is equivalent to the existence of $u>0$ such that $u I\preceq S\preceq\kappa_{\operatorname*{max}}u I$ . W can therefore solve the ML problem, with the condition number constraint on R , by solving the convex problem 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\log\operatorname*{det}S-\mathbf{tr}(S Y)}\\ {\mathrm{subject~to}}&{u I\preceq S\preceq\kappa_{\mathrm{max}}u I}\end{array}
$$ 

where the variables are $S\in\mathbf{S}^{n}$ and $u\in\mathbf{R}$ . 

As another example, suppose we are given bounds on the variance of some linear functions of the underlying random vector $y$ , 

$$
{\bf E}(c_{i}^{T}y)^{2}\le\alpha_{i},\quad i=1,.\,.\,.\,,K.
$$ 

These prior assumptions can be expressed as 

$$
{\bf E}(c_{i}^{T}y)^{2}=c_{i}^{T}R c_{i}=c_{i}^{T}S^{-1}c_{i}\leq\alpha_{i},\quad i=1,\ldots,K.
$$ 

Since $c_{i}^{I}S^{-1}c_{i}$ is a convex function of $S$ (provided $S\succ0$ , which holds here), these bounds can be imposed in the ML problem. 

# 7.1.2 Maximum a posteriori probability estimation 

Maximum a posteriori probability (MAP) estimation can be considered a Bayesian version of maximum likelihood estimation, with a prior probability density on the underlying parameter $x$ . We assume that $x$ (the vector to be estimated) and $y$ (the observation) are random variables with a joint probability density $p(x,y)$ . This is in contrast to the statistical estimation setup, where $x$ is a parameter, not a random variable. 

The prior density of $x$ is given by 

$$
p_{x}(x)=\int p(x,y)\;d y.
$$ 

This density represents our prior information about what the values of the vector $x$ might be, before we observe the vector $y$ . Similarly, the prior density of $y$ is given by 

$$
p_{y}(y)=\int p(x,y)\ d x.
$$ 

This density represents the prior information about what the measurement or ob- servation vector $y$ will be. 

The conditional density of $y$ , given $x$ , is given by 

$$
p_{y|x}(x,y)=\frac{p(x,y)}{p_{x}(x)}.
$$ 

In the MAP estimation method, $p_{y\mid x}$ plays the role of the parameter dependent density $p_{x}$ in the maximum likelihood estimation setup. The conditional density of $x$ , given $y$ , is given by 

$$
p_{x|y}(x,y)=\frac{p(x,y)}{p_{y}(y)}=p_{y|x}(x,y)\frac{p_{x}(x)}{p_{y}(y)}.
$$ 

When we substitute the observed value $y$ into $p_{x\mid y}$ , we obtain the posterior density of $x$ . It represents our knowledge of $x$ after the observation. 

In the MAP estimation method, our estimate of $x$ , given the observation $y$ , is given by 

$$
\begin{array}{r c l}{{\hat{x}_{\mathrm{map}}}}&{{=}}&{{\mathrm{argmax}_{x}p_{x|y}(x,y)}}\\ {{}}&{{=}}&{{\mathrm{argmax}_{x}p_{y|x}(x,y)p_{x}(x)}}\\ {{}}&{{=}}&{{\mathrm{argmax}_{x}p(x,y).}}\end{array}
$$ 

In other words, we take as estimate of $x$ the value that maximizes the conditional density of $x$ , given the observed value of $y$ . The only diﬀerence between this estimate and the maximum likelihood estimate is the second term, $p_{x}(x)$ , appearing here. This term can be interpreted as taking our prior knowledge of $x$ into account. Note that if the prior density of $x$ is uniform over a set $C$ , then finding the MAP estimate is the same as maximizing the likelihood function subject to $x\in C$ , which is the ML estimation problem ( 7.1 ). 

Taking logarithms, we can express the MAP estimate as 

$$
\hat{x}_{\mathrm{map}}=\mathrm{argmax}_{x}(\log p_{y\mid x}(x,y)+\log p_{x}(x)).
$$ 

The first term is essentially the same as the log-likelihood function; the second term penalizes choices of $x$ that are unlikely, according to the prior density ( i.e. , $x$ with $p_{x}(x)$ small). 

Brushing aside the philosophical diﬀerences in setup, the only diﬀerence between finding the MAP estimate (via ( 7.6 )) and the ML estimate (via ( 7.1 )) is the presence of an extra term in the optimization problem, associated with the prior density of $x$ . Therefore, for any maximum likelihood estimation problem with concave log- likelihood function, we can add a prior density for $x$ that is log-concave, and the resulting MAP estimation problem will be convex. 

# Linear measurements with IID noise 

Suppose that $x\in\mathbf{R}^{n}$ and $y\in\mathbf{R}^{m}$ are related by 

$$
y_{i}=a_{i}^{T}x+v_{i},\quad i=1,\ldots,m,
$$ 

where are IID with density on $\mathbf{R}$ , and has prior density on $\mathbf{R}^{n}$ . The $v_{i}$ $p_{v}$ $x$ $p_{x}$ joint density of $x$ and $y$ is then 

$$
p(x,y)=p_{x}(x)\prod_{i=1}^{m}p_{v}(y_{i}-a_{i}^{T}x),
$$ 

and the MAP estimate can be found by solving the optimization problem 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\log p_{x}(x)+\sum_{i=1}^{m}\log p_{v}(y_{i}-a_{i}^{T}x).}\end{array}
$$ 

If $p_{x}$ and $p_{v}$ are log-concave, this problem is convex. The only diﬀerence between the MAP estimation problem ( 7.7 ) and the associated ML estimation problem ( 7.2 ) is the extra term $\log{p_{x}(x)}$ . 

For example, if $v_{i}$ are uniform on $[-a,a]$ , and the prior distribution of $x$ is Gaussian with mean x and covariance Σ, the MAP estimate is found by solving the QP 

$$
\begin{array}{l l}{\mathrm{minimize}}&{(x-\bar{x})^{T}\Sigma^{-1}(x-\bar{x})}\\ {\mathrm{subject~to}}&{\|A x-y\|_{\infty}\leq a,}\end{array}
$$ 

with variable $x$ . 

# MAP with perfect linear measurements 

Suppose $x\,\in\,\mathbf{R}^{n}$ is a vector of parameters to be estimated, with prior density $p_{x}$ . We have $m$ perfect (noise free, deterministic) linear measurements, given by $y=A x$ . In other words, the conditional distribution of $y$ , given $x$ , is a point mass with value one at the point $A x$ . The MAP estimate can be found by solving the problem 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\log p_{x}(x)}\\ {\mathrm{subject~to}}&{A x=y.}\end{array}
$$ 

If $p_{x}$ is log-concave, this is a convex problem. 

If under the prior distribution, the parameters $x_{i}$ are IID with density $p$ on $\mathbf{R}$ , then the MAP estimation problem has the form 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\sum_{i=1}^{n}\log p(x_{i})}\\ &{\mathrm{subject~to}\quad A x=y,}\end{array}
$$ 

which is a least-penalty problem (( 6.6 ), page 304 ), with penalty function $\phi(u)=$ $-\log p(u)$ . 

Conversely, we can interpret any least-penalty problem, 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{\phi(x_{1})+\cdot\cdot\cdot+\phi(x_{n})}\\ {{\mathrm{subject~to}}}&{A x=b}\end{array}}
$$ 

as a MAP estimation problem, with $m$ perfect linear measurements ( i.e. , $A x=b$ ) and $x_{i}$ IID with density 

$$
p(z)=\frac{e^{-\phi(z)}}{\int e^{-\phi(u)}~d u}.
$$ 

# 7.2 Nonparametric distribution estimation 

We consider a random varia e $X$ with values in the finite set $\{\alpha_{1},.\,.\,.\,,\alpha_{n}\}\subseteq\mathbf{R}$ . (We take the values to be in R for simplicity; the same ideas can be applied when the values are in $\mathbf{R}^{k}$ , for example.) The distribution of $X$ is characterized by $p\in\mathbf{R}^{n}$ , with prob $(X=\alpha_{k})=p_{k}$ . Clearly, $p$ satisfies $p\succeq0$ , ${\bf1}^{T}p=1$ . Conversely, if $p\in\mathbf{R}^{n}$ satisfie $p\succeq0$ , $\mathbf{1}^{T}p=1$ , then it defines a probability distribution for a random variable X , defined as $\mathbf{prob}(X=\alpha_{k})=p_{k}$ . Thus, the probability simplex 

$$
\{p\in\mathbf{R}^{n}\mid p\succeq0,\ \mathbf{1}^{T}p=1\}
$$ 

is in one-to-one correspondence with all possible probability distributions for a random variable $X$ taking values in $\{\alpha_{1},.\,.\,.\,,\alpha_{n}\}$ . 

In this section we discuss methods used to estimate the distribution $p$ based on a combination of prior information and, possibly, observations and measurements. 

# Prior information 

Many types of prior information about $p$ can be expressed in terms of linear equality constraints or inequalities. If $f:\mathbf{R}\rightarrow\mathbf{R}$ is any function, then 

$$
\operatorname{E}f(X)=\sum_{i=1}^{n}p_{i}f(\alpha_{i})
$$ 

is a linear function of $p$ . As a special case, if $C\subseteq\mathbf{R}$ , then prob ( $X\in C)$ is a linear function of $p$ : 

$$
\mathbf{prob}(X\in C)=c^{T}p,\qquad c_{i}=\left\{\begin{array}{l l}{1}&{\alpha_{i}\in C}\\ {0}&{\alpha_{i}\not\in C.}\end{array}\right.
$$ 

It follows that known expected values of certain functions ( e.g. , moments) or known probabilities of certain sets can be incorporated as linear equality constraints on $p\in\mathbf{R}^{n}$ . Inequalities on expected values or probabilities can be expressed as linear inequalities on $p\in\mathbf{R}^{n}$ . 

For example, suppose we know that $X$ has mean $\mathbf{E}\,X\,=\,\alpha$ , second moment $\mathbf{E}\,X^{2}=\beta$ , and $\mathbf{prob}(X\geq0)\leq0.3$ . This prior information can be expressed as 

$$
\mathbf{E}\,X=\sum_{i=1}^{n}\alpha_{i}p_{i}=\alpha,\qquad\mathbf{E}\,X^{2}=\sum_{i=1}^{n}\alpha_{i}^{2}p_{i}=\beta,\qquad\sum_{\alpha_{i}\geq0}p_{i}\leq0.3,
$$ 

which are two linear equalities and one linear inequality in $p$ 

We can also include some prior constraints that involve nonlinear functions of . As an example, the variance of $X$ is given by $p$ 

$$
\mathbf{var}(X)=\mathbf{E}\,X^{2}-(\mathbf{E}\,X)^{2}=\sum_{i=1}^{n}\alpha_{i}^{2}p_{i}-\left(\sum_{i=1}^{n}\alpha_{i}p_{i}\right)^{2}.
$$ 

The first term is a linear function of $p$ and the second term is concave quadratic in , so the variance of $X$ is a concave function of . It follows that a lower bound $p$ $p$ on the variance of $X$ can be expressed as a convex quadratic inequality on . $p$ 

As another example, suppose $A$ and $B$ are subsets of $\mathbf{R}$ , and consider the conditional probability of $A$ given $B$ : 

$$
\mathbf{prob}(X\in A|X\in B)={\frac{\mathbf{prob}(X\in A\cap B)}{\mathbf{prob}(X\in B)}}.
$$ 

This function is linear-fractional in $p\in\mathbf{R}^{n}$ : it can be expressed as 

$$
\mathbf{prob}(X\in A|X\in B)=c^{T}p/d^{T}p,
$$ 

where 

$$
c_{i}=\left\{\begin{array}{l l}{1}&{\alpha_{i}\in A\cap B}\\ {0}&{\alpha_{i}\not\in A\cap B}\end{array}\right.,\qquad d_{i}=\left\{\begin{array}{l l}{1}&{\alpha_{i}\in B}\\ {0}&{\alpha_{i}\not\in B.}\end{array}\right.
$$ 

Therefore we can express the prior constraints 

$$
l\leq\mathbf{prob}(X\in A|X\in B)\leq u
$$ 

as the linear inequality constraints on $p$ 

$$
l d^{T}p\leq c^{T}p\leq u d^{T}p.
$$ 

Several other types of prior information can be expressed in terms of nonlinear convex inequalities. For example, the entropy of $X$ , given by 

$$
-\sum_{i=1}^{n}p_{i}\log p_{i},
$$ 

is a concave function of $p$ , so we can impose a minimum value of entropy as a convex inequality on $p$ . If $q$ represents another distribution, i.e. , $q\succeq0$ , $\mathbf{1}^{T}q\,=\,1$ , then the Kullback-Leibler divergence between the distribution $q$ and the distribution $p$ is given by 

$$
\sum_{i=1}^{n}p_{i}\log(p_{i}/q_{i}),
$$ 

which is convex in $p$ (and $q$ as well; see example 3.19 , page 90 ). It follows that we can impose a maximum Kullback-Leibler divergence between $p$ and a given distribution $q$ , as a convex inequality on $p$ . 

In the next few paragraphs we express the prior information about the distribu- tion $p$ as $p\in\mathcal P$ . We assume that $\mathcal{P}$ can be described by a se of linear equalities and convex inequalities. We include in the prior information P the basic constraints $p\succeq0$ , $\mathbf{1}^{T}p=1$ . 

# Bounding probabilities and expected values 

Given prior information about the distribution, say $p\in\mathcal P$ , we can compute upper or lower bounds on the expected value of a function, or probability of a set. For example to determine a lower bound on $\mathbf{E}\,f(X)$ over all distributions that satisfy the prior information $p\in\mathcal P$ , we solve the convex problem 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\sum_{i=1}^{n}f(\alpha_{i})p_{i}}\\ &{\mathrm{subject~to}\quad p\in\mathscr{P}.}\end{array}
$$ 

# Maximum likelihood estimation 

We can use maximum likelihood estimation to estimate $p$ based on observations from the distribution. Suppose we observe $N$ independent samples from $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ the distribution. Let $k_{i}$ denote the number of these samples with value $\alpha_{i}$ , so that $k_{1}\,+\,\cdot\cdot\,+\,k_{n}\;=\;N$ , the total number of observed samples. The log-likelihood function is then 

$$
l(p)=\sum_{i=1}^{n}k_{i}\log p_{i},
$$ 

which is a concave function of $p$ . The maximum likelihood estimate of $p$ can be found by solving the convex problem 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad l(p)=\sum_{i=1}^{n}k_{i}\log p_{i}}\\ &{\mathrm{subject~to}\quad p\in\mathcal{P},}\end{array}
$$ 

with variable $p$ . 

# Maximum entropy 

The maximum entropy distribution consistent with the prior assumptions can be found by solving the convex problem 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\sum_{i=1}^{n}p_{i}\log p_{i}}\\ &{\mathrm{subject~to}\quad p\in\mathscr{P}.}\end{array}
$$ 

Enthusiasts describe the maximum entropy distribution as the most equivocal or most random, among those consistent with the prior information. 

# Minimum Kullback-Leibler divergence 

We can find the distribution $p$ that has minimum Kullback-Leibler divergence from a given prior distribution $q$ , among those consistent with prior information, by solving the convex problem 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\sum_{i=1}^{n}p_{i}\log(p_{i}/q_{i})}\\ &{\mathrm{subject~to}\quad p\in\mathscr{P},}\end{array}
$$ 

Note that when the prior distribution is the uniform distribution, i.e. , $q=(1/n)\mathbf{1}$ , this problem reduces to the maximum entropy problem. 

Example 7.2 We consider a probability distribution on 100 equidistant points $\alpha_{i}$ in the interval $[-1,1]$ . We impose the following prior assumptions: 

$$
\begin{array}{r c l}{\mathbf{E}\,X}&{\in}&{[-0.1,0.1]}\\ {\mathbf{E}\,X^{2}}&{\in}&{[0.5,0.6]}\\ {\mathbf{E}(3X^{3}-2X)}&{\in}&{[-0.3,-0.2]}\\ {\mathbf{prob}(X<0)}&{\in}&{[0.3,0.4].}\end{array}
$$ 

Along with the constraints $\mathbf{1}^{T}p=1$ , $p\succeq0$ , these constraints describe a polyhedron of probability distributions. 

Figure 7.2 shows the maximum entropy distribution that satisfies these constraints. The maximum entropy distribution satisfies 

$$
\begin{array}{r c l}{{\mathbf{E}\,X}}&{{=}}&{{0.056}}\\ {{\mathbf{E}\,X^{2}}}&{{=}}&{{0.5}}\\ {{\mathbf{E}(3X^{3}-2X)}}&{{=}}&{{-0.2}}\\ {{\mathbf{prob}(X<0)}}&{{=}}&{{0.4.}}\end{array}
$$ 

To illustrate bounding probabilities, we compute upper and lower bounds on the cumulative distribution $\mathbf{prob}(X~\leq~\alpha_{i})$ , for $\textit{i}=\,1,.\,.\,.\,,100$ . For each value of $i$ , 

![](images/4eeeaa6016699a87d0bc067cca8e3c72659a2eaefa71dabf236d5f5814fbd793.jpg) 
Figure 7.2 Maximum entropy distribution that satisfies the constraints ( 7.8 ). 

we solve two LPs: one that maximizes prob ( $X~\leq~\alpha_{i}$ ), and one that minimizes $\mathbf{prob}(X\,\leq\,\alpha_{i})$ , over all distributions consistent with the prior assumptions ( 7.8 ). The results are shown in figure 7.3 . The upper and lower curves show the upper and lower bounds, respectively; the middle curve shows the cumulative distribution of the maximum entropy distribution. 

Example 7.3 Bounding risk probability with known marginal distributions. Suppose $X$ and $Y$ are two random variables that give the return on two investments. We assume that $X$ takes values in $\{\alpha_{1},.\,.\,.\,,\alpha_{n}\}\subseteq\mathbf{R}$ and $Y$ takes values in $\{\beta_{1},.\,.\,.\,,\beta_{m}\}\subseteq\mathbf{R}$ , with $\mathbf{\Psi}_{p_{i j}}=\mathbf{prob}(X=\alpha_{i},Y=\beta_{j})$ ). The marginal distributions of the two returns $X$ and $Y$ are known, i.e. , 

$$
\sum_{j=1}^{m}p_{i j}=r_{i},\quad i=1,\ldots,n,\qquad\sum_{i=1}^{n}p_{i j}=q_{j},\quad j=1,\ldots,m,
$$ 

but otherwise nothing is known about the joint distribution $p$ . This defines a poly- hedron of joint distributions consistent with the given marginals. 

Now suppose we make both investments, so our total return is the random variable $X+Y$ . We are interested in computing an upper bound on the probability of some level of loss, or low return, i.e. , $\mathbf{prob}(X+Y<\gamma)$ . We can compute a tight upper bound on this probability by solving the LP 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\sum\left\{p_{i j}\mid\alpha_{i}+\beta_{j}<\gamma\right\}}\\ {\mathrm{subject~to}}&{(7.9),\quad p_{i j}\geq0,\quad i=1,\ldots n,\quad j=1,\ldots,m.}\end{array}
$$ 

The optimal value of this LP is the maximum probability of loss. The optimal solution $p^{\star}$ is the joint distribution, consistent with the given marginal distributions, that maximizes the probability of the loss. 

The same method can be applied to a derivative of the two investments. Let $R(X,Y)$ be the return of the derivative, where $R:{\mathbf{R}}^{2}\to{\mathbf{R}}$ . We can compute sharp lower 

![](images/0c4721fee3452b8ffa2b86f56a30dcc1a001bb4a41ae4dd662dad5aba57d5823.jpg) 
Figure 7.3 The top and bottom curves show the maximum and minimum possible values of the cumulative distribution function, prob ( $X\le\alpha_{i},$ ), over all distributions that satisfy ( 7.8 ). The middle curve is the cumulative dis- tribution of the maximum entropy distribution that satisfies ( 7.8 ). 

and upper bounds on $\mathbf{prob}(R<\gamma)$ by solving a similar LP, with objective function 

$$
\sum\left\{p_{i j}\mid R\big(\alpha_{i},\beta_{j}\big)<\gamma\right\},
$$ 

which we can minimize and maximize. 

# 7.3 Optimal detector design and hypothesis testing 

Suppose $X$ is a random variable with values in $\{1,\ldots,n\}$ , wi a distribution that depends n a parameter $\theta\in\{1,.\,.\,.\,,m\}$ . Th ons of X , for the $m$ possible values of θ , can be represented by a matrix P $P\in\mathbf{R}^{n\times m}$ , with elements 

$$
p_{k j}={\bf p r o b}(X=k\mid\theta=j).
$$ 

The $j$ th column of $P$ gives the probability distribution associated with the param- eter value $\theta=j$ . 

We consider the problem of estimating $\theta$ , based on an observed sample of $X$ . In other words, the sample $X$ is generated from one of the $m$ possible distributions, and we are to guess which one. The $m$ values of $\theta$ are called hypotheses , and guessing which hypothesis is correct ( i.e. , which distribution generated the observed sample $X$ ) is called hypothesis testing . In many cases one of the hypotheses corresponds to some normal situation, and each of the other hypotheses corresponds to some abnormal event. In this case hypothesis testing can be interpreted as observing a value of $X$ , and then guessing whether or not an abnormal event has occurred, and if so, which one. For this reason hypothesis testing is also called detection . 

In most cases there is no significance to the ordering of the hypotheses; they are . If ${\hat{\theta}}=\theta$ , where $\hat{\theta}$ simply $m$ diﬀerent hypotheses, arbitrarily labeled $\theta=1,\ldots,m$ denotes the estimate of $\theta$ , then we have correctly guessed the parameter value $\theta$ . If

 ${\hat{\theta}}\neq\theta$ ̸ , then we have (incorrectly) guessed the parameter value $\theta$ ; we have mistaken

 $\hat{\theta}$ ˆ for $\theta$ . In other cases, there is significance in the ordering of the hypotheses. In this case, an event such as ${\hat{\theta}}>\theta$ , i.e. , the event that we overestimate $\theta$ , is meaningful. 

It is also possible to parametrize $\theta$ by values other than $\{1,\cdot\cdot\cdot,m\}$ , say as $\theta\in$ $\{\theta_{1},.\,.\,.\,,\theta_{m}\}$ , where $\theta_{i}$ are (distinct) values. These values could be real numbers, or vectors, for example, specifying the mean and variance of the $k$ th distribution. In this case, a quantity such as $\lVert\hat{\boldsymbol{\theta}}\,-\,{\boldsymbol{\theta}}\rVert$ − ∥ , which is the norm of the parameter estimation error, is meaningful. 

# 7.3.1 Deterministic and randomized detectors 

A (deterministic) estimator or detector is a function $\psi$ from $\{1,\cdot\cdot\cdot,n\}$ (the set of possible observed values) into $\{1,\cdot\cdot\cdot,m\}$ (the set of hypotheses). If X is observed to have value $k$ , then our guess for the value of $\theta$ is ${\hat{\theta}}\ =\ \psi(k)$ ). One obvious deterministic detector is the maximum likelihood detector , given by 

$$
{\hat{\theta}}=\psi_{\mathrm{{ml}}}(k)=\underset{j}{\operatorname{argmax}}\,p_{k j}.
$$ 

When we observe the value $X\,=\,k$ , the maximum likelihood estimate of $\theta$ is a value that maximizes the probability of observing $X=k$ , over the set of possible distributions. 

We will consider a generalization of the deterministic detector, in which the estimate of $\theta$ , given an observed value of $X$ , is random. A randomized detector of $\theta$ is a random variable $\hat{\theta}\in\{1,\dots,m\}$ ∈{ } , with a distribution that depends on the observed value of $X$ . A randomized detector can be defined in terms of a matrix $\scriptstyle T\in\mathbf{R}^{m\times n}$ with elements 

$$
t_{i k}=\mathbf{prob}({\hat{\theta}}=i\mid X=k).
$$ 

, then the detector gives ${\hat{\theta}}=i$ The interpretation is as follows: if we observe $X=k$ with probability $t_{i k}$ . The $k$ th column of $T$ , which we will denote $t_{k}$ , gives the probability distribution of $\hat{\theta}$ , when we observe $X\,=\,k$ . If each column of $T$ is a unit vector, then the randomized detector is a deterministic detector, i.e. , θ is a (deterministic) function of the observed value of $X$ . 

At first glance, it seems that intentionally introducing additional randomiza- tion into the estimation or detection process can only make the estimator worse. But we will see below examples in which a randomized detector outperforms all deterministic estimators. 

We are interested in designing the matrix $T$ that defines the randomized detec- tor. Obviously the columns $t_{k}$ of $T$ must satisfy the (linear equality and inequality) constraints 

$$
\begin{array}{r}{t_{k}\succeq0,\qquad\mathbf{1}^{T}t_{k}=1.}\end{array}
$$ 

# 7.3.2 Detection probability matrix 

For the randomized detector defined by the matrix $T$ , we define the detection probability matrix as $D=T P$ . We have 

$$
D_{i j}=(T P)_{i j}=\mathbf{prob}(\hat{\theta}=i\mid\theta=j),
$$ 

is the probability of guessing $\boldsymbol{\hat{\theta}}\ =\ i$ so $D_{i j}$ , when in fact $\theta\:=\:j$ . The $m\,\times\,m$ detection probability matrix $D$ characterizes the performance of the randomized is the probability of guessing ${\hat{\theta}}=i$ detector defined by $T$ . The diagonal entry $D_{i i}$ when $\theta=i$ , i.e. , the probability of correctly detecting that $\theta=i$ . The oﬀ-diagonal entry $D_{i j}$ (with $i\neq j$ ) is the probability of mistaking $\theta\:=\:i$ for $\theta\:=\:j$ , i.e. , the probability that our guess is ${\hat{\theta}}=i$ , when in fact $\theta=j$ . If $D=I$ , the detector is is, we correctly guess ${\hat{\theta}}=\theta$ perfect: no matter what the parameter $\theta$ . 

The diagonal entries of $D$ , arranged in a vector, are called the detection proba- bilities , and denoted $P^{\mathrm{d}}$ : 

$$
\begin{array}{r}{P_{i}^{\mathrm{d}}=D_{i i}=\mathbf{prob}(\hat{\boldsymbol{\theta}}=i\mid\boldsymbol{\theta}=i).}\end{array}
$$ 

The error probabilities are the complements, and are denoted $P^{\mathrm{e}}$ : 

$$
P_{i}^{\mathrm{e}}=1-D_{i i}=\mathbf{prob}(\hat{\theta}\neq i\mid\theta=i).
$$ 

Since the columns of the detection probability matrix $D$ add up to one, we can express the error probabilities as 

$$
P_{i}^{\mathrm{e}}=\sum_{j\neq i}D_{j i}.
$$ 

# 7.3.3 Optimal detector design 

In this section we show that a wide variety of objectives for detector design are linear, affine, or convex piecewise-linear functions of $D$ , and therefore also of $T$ (which is the optimization variable). Similarly, a variety of constraints for detector design can be expressed in terms of linear inequalities in $D$ . It follows that a wide variety of optimal detector design problems can be expressed as LPs. We will see in $\S7.3.4$ that some of these LPs have simple solutions; in this section we simply formulate the problem. 

# Limits on errors and detection probabilities 

We can impose a lower bound on the probability of correctly detecting the $j$ th hypothesis, 

$$
\begin{array}{r}{P_{j}^{\mathrm{d}}=D_{j j}\geq L_{j},}\end{array}
$$ 

which is a linear inequality in $D$ (hence, $T$ ). Similarly, we can impose a maximum allowable probability for mistaking $\theta=i$ for $\theta=j$ : 

$$
D_{i j}\le U_{i j},
$$ 

which are also linear constraints on $T$ . We can take any of the detection prob- abilities as an objective to be maximized, or any of the error probabilities as an objective to be minimized. 

# Minimax detector design 

We can take as objective (to be minimized) the minimax error probability , $\operatorname*{max}_{j}P_{j}^{\mathrm{e}}$ , which is a piecewise-linear convex function of $D$ (hence, also of $T$ ). With this as the only objective, we have the problem of minimizing the maximum probability of detection error, 

$$
\begin{array}{l l l}{\mathrm{minimize}}&{\operatorname*{max}_{j}P_{j}^{\mathrm{e}}}\\ {\mathrm{subject~to}}&{t_{k}\succeq0,\quad\mathbf{1}^{T}t_{k}=1,\quad k=1,.\,.\,.\,,n,}\end{array}
$$ 

where the variables are $t_{1},\ldots,t_{n}\in\mathbf{R}^{\prime\prime\prime}$ . This can be reformulated as an LP. T minimax detector minimizes the worst-case (largest) probability of error over all m hypotheses. 

We can, of course, add further constraints to the minimax detector design prob- lem. 

# Bayes detector design 

In Bayes detector design, we have a prior distribution for the hypotheses, given by $q\in\mathbf{R}^{m}$ , where 

$$
q_{i}=\mathbf{prob}(\theta=i).
$$ 

In this case, the probabilities are interpreted as conditional probabilities of $X$ , $p_{i j}$ given $\theta$ . The probability of error for the detector is then given by $q^{T}P^{\mathrm{e}}$ , which is an affine function of $T$ . The Bayes optimal detector is the solution of the LP 

$$
\begin{array}{l l l l}{\mathrm{minimize}}&{q^{T}P^{\mathrm{e}}}&\\ {\mathrm{subject~to}}&{t_{k}\succeq0,\quad\mathbf{1}^{T}t_{k}=1,\quad k=1,\ldots,n.}\end{array}
$$ 

We will see in $\S7.3.4$ that this problem has a simple analytical solution. 

One special case is when $q=(1/m)\mathbf{1}$ . In this case the Bayes optimal detector minimizes the average probability of error, where the (unweighted) average is over the hypotheses. In $\S7.3.4$ we will see that the maximum likelihood detector ( 7.10 ) is optimal for this problem. 

# Bias, mean-square error, and other quantities 

In this section we assume that the ordering of the values of $\theta$ have some significance, i.e. , that the value $\theta=i$ can be interpreted as a larger value of the parameter than $\theta=j$ , when $i>j$ . This might be the case, for example, when $\theta=i$ corresponds to the hypothesis that $i$ events have occurred. Here we may be interested in quantities such as 

$$
\mathbf{prob}({\hat{\theta}}>\theta\mid\theta=i),
$$ 

which is the probability that we overestimate $\theta$ when $\theta\:=\:i$ . This is an affine function of $D$ : 

$$
\mathbf{prob}(\hat{\boldsymbol{\theta}}>\boldsymbol{\theta}\mid\boldsymbol{\theta}=i)=\sum_{j>i}D_{j i},
$$ 

so a maximum allowable value for this probability can be expressed as a linear inequality on $D$ (hence, $T$ ). As another example, the probability of misclassifying $\theta$ by more than one, when $\theta=i$ , 

$$
\mathbf{prob}(|\hat{\theta}-\theta|>1\mid\theta=i)=\sum_{|j-i|>1}D_{j i},
$$ 

is also a linear function of $D$ . 

We now suppose that the parameters have values $\{\theta_{1},.\,.\,.\,,\theta_{m}\}\,\subseteq\,\mathbf{R}$ . The es- timation or detection (parameter) error is then given by ${\hat{\theta}}-\theta$ − , and a number of quantities of interest are given by linear functions of $D$ . Examples include: 

Bias. The bias of the detector, when $\theta=\theta_{i}$ , is given by the linear function 

$$
\underset{i}{\mathbf{E}}(\hat{\theta}-\theta)=\sum_{j=1}^{m}(\theta_{j}-\theta_{i})D_{j i},
$$ 

where the subscript on $\mathbf{E}$ means the expectation is with respect to the dis- tribution of the hypothesis $\theta=\theta_{i}$ . 

• Mean square error. The mean square error of the detector, when $\theta=\theta_{i}$ , is given by the linear function 

$$
\mathbf{E}_{i}({\widehat{\theta}}-\theta)^{2}=\sum_{j=1}^{m}(\theta_{j}-\theta_{i})^{2}D_{j i}.
$$ 

• Average absolute error. The average absolute error of the detector, when $\theta=\theta_{i}$ , is given by the linear function 

$$
\mathbf{E}_{i}\left|\hat{\theta}-\theta\right|=\sum_{j=1}^{m}|\theta_{j}-\theta_{i}|D_{j i}.
$$ 

# 7.3.4 Multicriterion formulation and scalarization 

The optimal detector design problem can be considered a multicriterion problem, with the c nstraints ( 7.11 ), and the $m(m-1)$ objectives given by the oﬀ-diagonal entries of D , which are the probabilities of the diﬀerent types of detection error: 

$$
\begin{array}{l l}{\mathrm{minimize~(w.r.t.~}\mathbf{R}_{+}^{m(m-1)})}&{D_{i j},\quad i,~j=1,.\,.\,,m,\quad i\neq j}\\ {\mathrm{subject~to~}}&{t_{k}\succeq0,\quad\mathbf{1}^{T}t_{k}=1,\quad k=1,.\,.\,,n,}\end{array}
$$ 

with variables $t_{1},\ldots,t_{n}\in\mathbf{R}^{\prime\prime\prime}$ . Since each objective $D_{i j}$ is a linear function of the variables, this is a multicriterion linear program. 

We can scalarize this multicriterion problem by forming the weighted sum ob- jective 

$$
\sum_{i,j=1}^{m}W_{i j}D_{i j}=\mathbf{tr}(W^{T}D)
$$ 

where the weight matrix $W\in\mathbf{R}^{m\times m}$ satisfies 

$$
W_{i i}=0,\quad i=1,\ldots,m,\qquad W_{i j}>0,\quad i,\ j=1,\ldots,m,\quad i\ne j.
$$ 

This objective is a weighted sum of the $m(m-1)$ error probabilities, with weight associated with the error of guessing $\boldsymbol{\hat{\theta}}\,=\,i$ $W_{i j}$ when in fact $\theta\,=\,j$ . The weight matrix is sometimes called the loss matrix . 

To find a Pareto optimal point for the multicriterion problem ( 7.12 ), we form the scalar optimization problem 

$$
\begin{array}{l r}{\mathrm{minimize}}&{\mathbf{tr}(W^{T}D)}\\ {\mathrm{subject~to}}&{t_{k}\succeq0,\quad\mathbf{1}^{T}t_{k}=1,\quad k=1,.\,.\,.\,,n,}\end{array}
$$ 

which is an LP. This LP is separable in the variables $t_{1},\dots,t_{n}$ . The objective can be expressed as a sum of (linear) functions of $t_{k}$ : 

$$
\mathbf{tr}(W^{T}D)=\mathbf{tr}(W^{T}T P)=\mathbf{tr}(P W^{T}T)=\sum_{k=1}^{n}c_{k}^{T}t_{k},
$$ 

where $c_{k}$ is the $k$ th column of $W P^{T}$ . The constraints are separable ( i.e. , we have separate constraints on each $t_{i}$ ). Therefore we can solve the LP ( 7.13 ) by separately solving 

$$
\begin{array}{l r c l}{\mathrm{minimize}}&{c_{k}^{T}t_{k}}&{}\\ {\mathrm{subject~to}}&{t_{k}\succeq0,}&{\mathbf{1}^{T}t_{k}=1,}\end{array}
$$ 

for $k\,=\,1,\hdots,n$ . Each of these LPs has a simple analytical solution (see exer- cise 4.8 ). We first find an index $q$ such that $c_{k q}=\operatorname*{min}_{j}c_{k j}$ . Then we take $t_{k}^{\star}=e_{q}$ . This optimal point corresponds to a deterministic detector: when $X\,=\,k$ is ob- served, our estimate is 

$$
\begin{array}{r}{\hat{\boldsymbol{\theta}}=\underset{j}{\operatorname{argmin}}(W P^{T})_{j k}.}\end{array}
$$ 

Thus, for every weight matrix $W$ with positive oﬀ-diagonal elements we can find a deterministic detector that minimizes the weighted sum objective. This seems to suggest that randomized detectors are not needed, but we will see this is not the case. The Pareto optimal trade-oﬀsurface for the multicriterion LP ( 7.12 ) is piecewise-linear; the deterministic detectors of the form ( 7.14 ) correspond to the vertices on the Pareto optimal surface. 

# MAP and ML detectors 

Consider a Bayes detector design with prior distribution $q$ . The mean probability of error is 

$$
\boldsymbol{q}^{T}\boldsymbol{P}^{\mathrm{e}}=\sum_{j=1}^{m}\boldsymbol{q}_{j}\sum_{i\neq j}\boldsymbol{D}_{i j}=\sum_{i,j=1}^{m}W_{i j}\boldsymbol{D}_{i j},
$$ 

if we define the weight matrix $W$ as 

$$
W_{i j}=q_{j},\quad i,\ j=1,\ldots,m,\quad i\ne j,\quad\quad W_{i i}=0,\quad i=1,\ldots,m.
$$ 

Thus, a Bayes optimal detector is given by the deterministic detector ( 7.14 ), with 

$$
(W P^{T})_{j k}=\sum_{i\neq j}q_{i}p_{k i}=\sum_{i=1}^{m}q_{i}p_{k i}-q_{j}p_{k j}.
$$ 

The first term is independent of $j$ , so the optimal detector is simply 

$$
\begin{array}{r}{\hat{\theta}=\underset{j}{\operatorname{argmax}}(p_{k j}q_{j}),}\end{array}
$$ 

when $X\,=\,k$ is observed. The solution has a simple interpretation: Since $p_{k j}q_{j}$ gives the probability that $\theta=j$ and $X=k$ , this detector is a maximum a posteriori probability (MAP) detector. 

For the special case $q\,=\,(1/m)\mathbf{1}$ , i.e. , a uniform prior distribution on $\theta$ , this MAP detector reduces to a maximum likelihood (ML) detector: 

$$
{\hat{\theta}}={\underset{j}{\operatorname{argmax}}}\,p_{k j}.
$$ 

Thus, a maximum likelihood detector minimizes the (unweighted) average or mean probability of error. 

# 7.3.5 Binary hypothesis testing 

As an illustration, we consider the special case $m\;=\;2$ , which is called binary hypothesis testing . The random variable $X$ is generated from one of two distribu- tions, whic denote $p\in\mathbf{R}^{n}$ and $q\in\mathbf{R}^{n}$ , to simplify the notation. Ofte hypothesis θ = 1 corresponds to some normal situation, and the hypothesis θ $\theta=2$ corresponds to some abnormal event that we are trying to detect. If $\hat{\theta}=1$ = 1, we say the test is negative ( i.e. , we guess that the event did not occur); if $\widehat{\theta}=2$ = 2, we say the test is positive ( i.e. , we guess that the event did occur). 

The detection probability matrix $D\in\mathbf{R}^{2\times2}$ is traditionally expressed as 

$$
D=\left[\begin{array}{c c}{1-P_{\mathrm{fp}}}&{P_{\mathrm{in}}}\\ {P_{\mathrm{fp}}}&{1-P_{\mathrm{in}}}\end{array}\right].
$$ 

Here $P_{\mathrm{{fin}}}$ is the probability of a false negative ( i.e. , the test is negative when in fact the event has occurred) and $P_{\mathrm{fp}}$ is the probability of a false positive ( i.e. , the test is positive when in fact the event has not occurred), which is also called the false alarm probability . The optimal detector design problem is a bi-criterion problem, with objectives $P_{\mathrm{{fin}}}$ and $P_{\mathrm{fp}}$ . 

The optimal trade-oﬀcurve between $P_{\mathrm{{fin}}}$ and $P_{\mathrm{fp}}$ is called the receiver operating characteristic (ROC), and is determined by the distributions $p$ and $q$ . The ROC can be found by scalarizing the bi-criterion problem, as described in § 7.3.4 . For the weight matrix $W$ , an optimal detector ( 7.14 ) is 

$$
\hat{\theta}=\left\{\begin{array}{l l}{{1}}&{{W_{21}p_{k}>W_{12}q_{k}}}\\ {{2}}&{{W_{21}p_{k}\le W_{12}q_{k}}}\end{array}\right.
$$ 

![](images/2ebb0d5acea81a7eee508647ecd829cc7c4144aa720d6f8e2490d136c90d2128.jpg) 
Figure 7.4 Optimal trade-oﬀcurve between probability of a false negative, and probability of a false positive test result, for the matrix $P$ given in ( 7.15 ). The vertices of the trade-oﬀcurve, labeled 1–3, correspond to deterministic detectors; the point labeled 4, which is a randomized detector, is the mini- max detector. The dashed line shows $P_{\mathrm{{fin}}}=P_{\mathrm{{fp}}}$ , the points where the error probabilities are equal. 

when $X\,=\,k$ is observed. This is called a likelihood ratio threshold test : if the ratio $p_{k}/q_{k}$ is more than the threshold $W_{12}/W_{21}$ , the test is negative ( i.e. , $\hat{\theta}\,=$ 1); otherwise the test is positive. By choosing diﬀerent values of the threshold, we obtain (deterministic) Pareto optimal detectors that give diﬀerent levels of false positive versus false negative error probabilities. This result is known as the Neyman-Pearson lemma. 

The likelihood ratio detectors do not give all the Pareto optimal detectors; they are the vertices of the optimal trade-oﬀcurve, which is piecewise-linear. 

Example 7.4 We consider a binary hypothesis testing example with $n=4$ , and 

$$
P=\left[\begin{array}{l l}{0.70}&{0.10}\\ {0.20}&{0.10}\\ {0.05}&{0.70}\\ {0.05}&{0.10}\end{array}\right].
$$ 

The optimal trade-oﬀcurve between $P_{\mathrm{{fin}}}$ and $P_{\mathrm{fp}}$ , i.e. , the receiver operating curve, is shown in figure 7.4 . The left endpoint corresponds to the detector which is always negative, independent of the observed value of $X$ ; the right endpoint corresponds to the detector that is always positive. The vertices labeled 1, 2, and 3 correspond to the deterministic detectors 

$$
\begin{array}{r l r}{T^{(1)}}&{=}&{\left[\begin{array}{c c c c}{1}&{1}&{0}&{1}\\ {0}&{0}&{1}&{0}\end{array}\right],}\\ {T^{(2)}}&{=}&{\left[\begin{array}{c c c c}{1}&{1}&{0}&{0}\\ {0}&{0}&{1}&{1}\end{array}\right],}\end{array}
$$ 

$$
\begin{array}{r l r}{T^{(3)}}&{{}=}&{\left[\begin{array}{c c c c}{1}&{0}&{0}&{0}\\ {0}&{1}&{1}&{1}\end{array}\right],}\end{array}
$$ 

respectively. The point labeled 4 corresponds to the non deterministic detector 

$$
\begin{array}{r}{T^{(4)}=\left[\begin{array}{l l l l}{1}&{2/3}&{0}&{0}\\ {0}&{1/3}&{1}&{1}\end{array}\right],}\end{array}
$$ 

which is the minimax detector. This minimax detector yields equal probability of a false positive and false negative, which in this case is $1/6$ . Every deterministic detector has either a false positive or false negative probability that exceeds $1/6$ , so this is an example where a randomized detector outperforms every deterministic detector. 

# 7.3.6 Robust detectors 

So far we have assumed that $P$ , which gives the distribution of the observed variable $X$ , for each value of the parameter $\theta$ , is known. In this section we consider the case where these distributions are not known, but certain prior information about them is given. We assume that $P\in\mathcal{P}$ , where $\mathcal{P}$ is the set of possible distributions. With a randomized detector characterized by $T$ , the detection probability matrix $D$ now depends on the particular value of $P$ . We will judge the error probabilities by their worst-case values, over $P\in\mathcal P$ . We define the worst-case detection probability matrix $D^{\mathrm{wc}}$ as 

$$
D_{i j}^{\mathrm{wc}}=\operatorname*{sup}_{P\in\mathcal{P}}D_{i j},\quad i,\ j=1,\ldots,m,\quad i\neq j
$$ 

and 

$$
D_{i i}^{\mathrm{wc}}=\operatorname*{inf}_{P\in\mathcal{P}}D_{i i},\quad i=1,.\,.\,.\,,m.
$$ 

The oﬀ-diagonal entries give the largest possible probability of errors, and the diagonal entries give the smallest possible probability of detection, over $P\,\in\,\mathcal P$ . Note that $\textstyle\sum_{i=1}^{n}D_{i j}^{\mathrm{wc}}\neq1$ = 1 in general, i.e. , the columns of a worst-case detection probability matrix do not necessarily add up to one. 

We define the worst-case probability of error as 

$$
P_{i}^{\mathrm{wce}}=1-D_{i i}^{\mathrm{wc}}.
$$ 

Thus, $P_{i}^{\mathrm{wce}}$ is the largest probability of error, when $\theta=i$ , over all possible distri- butions in $\mathcal{P}$ . 

Using the worst-case detection probability matrix, or the worst-case probability of error vector, we can develop various robust versions of detector design problems. In the rest of this section we concentrate on the robust minimax detector design problem, as a generic example that illustrates the ideas. 

We define the robust minimax detector as the detector that minimizes the worst- case probability of error, over all hypotheses, i.e. , minimizes the objective 

$$
\operatorname*{max}_{i}P_{i}^{\mathrm{vec}}=\operatorname*{max}_{i=1,\dots,m}\operatorname*{sup}_{P\in\mathcal{P}}\left(1-(T P)_{i i}\right)=1-\operatorname*{min}_{i=1,\dots,m}\operatorname*{inf}_{P\in\mathcal{P}}(T P)_{i i}.
$$ 

The robust minimax detector minimizes the worst possible probability of error, over all $m$ hypotheses, and over all $P\in\mathcal P$ . 

# Robust minimax detector for finite $\mathcal{P}$ 

When the set of possible distributions is finite, the robust minimax detector design problem is readily formulated as an LP. With $\mathcal{P}=\{P_{1},.\,.\,.\,,P_{k}\}$ , we can find the robust minimax detector by solving 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\operatorname*{min}_{i=1,\dots,m}\operatorname*{inf}_{P\in\mathcal{P}}(T P)_{i i}=\operatorname*{min}_{i=1,\dots,m}\operatorname*{min}_{j=1,\dots,k}(T P_{j})_{i i}}\\ &{\mathrm{subject~to}\quad t_{i}\succeq0,\quad\mathbf{1}^{T}t_{i}=1,\quad i=1,\dots,n,}\end{array}
$$ 

The objective is piecewise-linear and concave, so this problem can be expressed as an LP. Note that we can just as well consider $\mathcal{P}$ to be the polyhedron $\mathbf{textstyle}\mathbf{cos}\,\mathcal{P}$ ; the associated worst-case detection matrix, and robust minimax detector, are the same. 

# Robust minimax detector for polyhedral $\mathcal{P}$ 

It is also possible to efficiently formulate the robust minimax detector problem as an LP when $\mathcal{P}$ is a polyhedron described by linear equality and inequality constraints. This formulation is less obvious, and relies on a dual representation of $\mathcal{P}$ . 

To simplify the discussion, we assume that $\mathcal{P}$ has the form 

$$
{\mathcal P}=\left\{P=[p_{1}\,\,\cdot\,\cdot\,\,p_{m}]\,\,\left|\,\,A_{k}p_{k}=b_{k},\,\,\mathbf{1}^{T}p_{k}=1,\,\,p_{k}\succeq0\right.\right\}.
$$ 

In other words, for each distribution p $p_{k}$ , we are given some expected values $A_{k}p_{k}=$ $b_{k}$ . (These might represent known moments, probabilities, etc.) The extension to the case where we are given inequalities on expected values is straightforward. The robust minimax design problem is 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\gamma}\\ {\mathrm{subject~to}}&{\operatorname*{inf}\{\tilde{t}_{i}^{T}p\mid A_{i}p=b_{i},\ \mathbf{1}^{T}p=1,\ p\succeq0\}\geq\gamma,\quad i=1,\ldots,m}\\ &{t_{i}\succeq0,\quad\mathbf{1}^{T}t_{i}=1,\quad i=1,\ldots,n,}\end{array}
$$ 

where $\ddot{t}_{i}^{I}$ denotes the $i$ th row of $T$ (so that $(T P)_{i i}=\dot{t}_{i}^{T}p_{i}$ ). By LP duality, 

$$
\operatorname*{inf}\{\widetilde{t}_{i}^{T}p\mid A_{i}p=b_{i},\ \mathbf{1}^{T}p=1,\ p\succeq0\}=\operatorname*{sup}\{\nu^{T}b_{i}+\mu\mid A_{i}^{T}\nu+\mu\mathbf{1}\preceq\widetilde{t}_{i}\}.
$$ 

Using this, the robust minimax detector design problem can be expressed as the LP 

$$
\begin{array}{l l}{\mathrm{maximize}}&{\gamma}\\ {\mathrm{subject~to}}&{\nu_{i}^{T}b_{i}+\mu_{i}\geq\gamma,\quad i=1,\ldots,m}\\ &{A_{i}^{T}\nu_{i}+\mu_{i}\mathbf{1}\preceq{\tilde{t}}_{i},\quad i=1,\ldots,m}\\ &{t_{i}\succeq0,\quad\mathbf{1}^{T}t_{i}=1,\quad i=1,\ldots,n,}\end{array}
$$ 

with variables $\nu_{1},\,\cdot\cdot\cdot\,,\nu_{m}$ , $\mu_{1},.\cdot\cdot\cdot,\mu_{n}$ , and $T$ (which has columns $t_{i}$ and rows $\dot{t}_{i}^{I}$ ). 

Example 7.5 Robust binary hypothesis testing. Suppose $m=2$ and the set $\mathcal{P}$ in ( 7.16 ) is defined by 

$$
A_{1}=A_{2}=A=\left[\begin{array}{l l l l}{a_{1}}&{a_{2}}&{\cdot\cdot\cdot}&{a_{n}}\\ {a_{1}^{2}}&{a_{2}^{2}}&{\cdot\cdot\cdot}&{a_{n}^{2}}\end{array}\right],\qquad b_{1}=\left[\begin{array}{l}{\alpha_{1}}\\ {\alpha_{2}}\end{array}\right],\qquad b_{2}=\left[\begin{array}{l}{\beta_{1}}\\ {\beta_{2}}\end{array}\right].
$$ 

Designing a robust minimax detector for this set $\mathcal{P}$ can be interpreted as a binary hypothesis testing problem: based on an observation of a random variable $X~\in$ $\{a_{1},.\,.\,.\,,a_{n}\}$ , choose between the following two hypotheses: 

Let $\tilde{t}^{I}$ denote the first row of $T$ (and so, $({\bf1}-\tilde{t})^{T}$ is the second row). For given $\dot{t}$ t , the worst-case probabilities of correct detection are 

$$
\left.\begin{array}{r c l}{{D_{11}^{\mathrm{wc}}}}&{{=}}&{{\displaystyle\operatorname*{inf}\left\{\tilde{t}^{T}p~\Bigg|\,\sum_{i=1}^{n}a_{i}p_{i}=\alpha_{1},~\sum_{i=1}^{n}a_{i}^{2}p_{i}=\alpha_{2},~\mathbf{1}^{T}p=1,~p\succeq0\right\}}}\\ {{D_{22}^{\mathrm{wc}}}}&{{=}}&{{\displaystyle\operatorname*{inf}\left\{(\mathbf{1}-\tilde{t})^{T}p~\Bigg|\,\sum_{i=1}^{n}a_{i}p_{i}=\beta_{1},~\sum_{i=1}^{n}a_{i}^{2}p_{i}=\beta_{2},~\mathbf{1}^{T}p=1,~p\succeq0\right\}.}}\end{array}\right.
$$ 

Using LP duality we can express $D_{11}^{\mathrm{wc}}$ as the optimal value of the LP 

$$
\begin{array}{l l}{\mathrm{maximize}}&{z_{0}+z_{1}\alpha_{1}+z_{2}\alpha_{2}}\\ {\mathrm{subject~to}}&{z_{0}+a_{i}z_{1}+a_{i}^{2}z_{2}\leq\tilde{t}_{i},\quad i=1,.\,.\,.\,,n,}\end{array}
$$ 

with variables $z_{\mathrm{0}}$ , $z_{1}$ , $z_{2}\in\mathbf{R}$ . Similarly $D_{22}^{\mathrm{wc}}$ is the optimal value of the LP 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad w_{0}+w_{1}\beta_{1}+w_{2}\beta_{2}}\\ &{\mathrm{subject~to}\quad w_{0}+a_{i}w_{1}+a_{i}^{2}w_{2}\leq1-\tilde{t}_{i},\quad i=1,\dots,n,}\end{array}
$$ 

with variables $w_{0}$ $w_{1}$ , $w_{2}\in\mathbf{R}$ To obtain the minimax detector, we have to maximize the minimum of $D_{11}^{\mathrm{wc}}$ and $D_{22}^{\mathrm{wc}}$ , i.e. , solve the LP 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\gamma}\\ &{\mathrm{subject~to}\quad z_{0}+z_{1}\alpha_{2}+z_{2}\alpha_{2}\geq\gamma}\\ &{\quad\quad\quad w_{0}+\beta_{1}w_{1}+\beta_{2}w_{2}\geq\gamma}\\ &{\quad\quad\quad z_{0}+z_{1}a_{i}+z a_{i}^{2}\leq\tilde{t}_{i},\quad i=1,\dots,n}\\ &{\quad\quad\quad w_{0}+w_{1}a_{i}+w_{2}a_{i}^{2}\leq1-\tilde{t}_{i},\quad i=1,\dots,n}\\ &{\quad\quad\quad0\preceq\tilde{t}\preceq{\bf1}.}\end{array}
$$ 

The variables are $z_{\mathrm{0}}$ , $z_{1}$ , $z_{2}$ , $w_{0}$ , $w_{1}$ , $w_{2}$ and $\dot{t}$ . 

# 7.4 Chebyshev and Chernoﬀbounds 

In this section we consider two types of classical bounds on the probability of a set, and show that generalizations of each can be cast as convex optimization problems. The original classical bounds correspond to simple convex optimization problems with analytical solutions; the convex optimization formulation of the general cases allow us to compute better bounds, or bounds for more complex situations. 

# 7.4.1 Chebyshev bounds 

Chebyshev bounds give an upper bound on the probability of a set based on known expected values of certain functions ( e.g. , mean and variance). The simplest ex- ample is Markov’s inequality: If $X$ is a random variable on $\mathbf{R}_{+}$ with $\mathbf{E}\,X\,=\,\mu$ , then we have $\mathbf{prob}(X\,\geq\,1)\,\leq\,\mu$ , no matter w t the distribution of $X$ An- other simple example is Chebyshev’s bound: If X is a random variable on R with $\mathbf{E}\,X\,=\,\mu\,$ and $\mathbf{E}(X-{\boldsymbol{\mu}})^{2}=\sigma^{2}\,$ , hen we have prob ( $|X-\mu|\geq1)\leq\sigma^{2}$ , again no matter what the distribution of X is. The idea behind these simple bounds can be generalized to a setting in which convex optimization is used to compute a bound on the probability. 

Let $X$ be a random variable on $S\subseteq\mathbf{R}^{m}$ , and $C\subseteq S$ be the set for which we ant to bound $\mathbf{prob}(X\in C)$ . Let $1_{C}$ denote the 0-1 indicator function of the set C , i.e. , $1_{C}(z)=1$ if z $z\in C$ and $1_{C}(z)=0$ if $z\notin C$ . 

Our prior knowledge of the distribution consists of known expected values of some functions: 

$$
\mathbf{E}\,f_{i}(X)=a_{i},\quad i=1,.\,.\,.\,,n,
$$ 

where $f_{i}:\mathbf{R}^{m_{\mathrm{~}}}\to\mathbf{R}$ . We take $f_{0}$ to be the constant function with value one, for which we always have $\mathbf{E}\,f_{0}(X)\,=\,a_{0}\,=\,1$ . Consider a linear combination of the functions $f_{i}$ , given by 

$$
f(z)=\sum_{i=0}^{n}x_{i}f_{i}(z),
$$ 

re $x_{i}\in\mathbf{R}$ , $i\,=\,0,\ldots,n$ . From our knowledge of $\mathbf{E}\,f_{i}(X)$ , we have $\mathbf{E}\,f(X)=$ $a^{T}x$ . 

Now suppose that $f$ satisfies the condition $f(z)\geq1_{C}(z)$ f all $z\in S$ , i.e. , $f$ is pointwise greater than or equal to the indicator function of C (on S ). Then we have 

$$
\mathbf{E}\,f(X)=a^{T}x\geq\mathbf{E}\,1_{C}(X)=\mathbf{prob}(X\in C).
$$ 

In other word $a^{T}x$ is an upper bound on $\mathbf{prob}(X\in C)$ , valid for all distributions supported on S , with $\mathbf{E}\,f_{i}(X)=a_{i}$ . 

We can search for the best such upper bound on $\mathbf{prob}(X\in C)$ , by solving the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{x_{0}+a_{1}x_{1}+\cdots+a_{n}x_{n}}\\ {{\mathrm{subject~to}}\ }&{f(z)=\sum_{i=0}^{n}x_{i}f_{i}(z)\geq1{\mathrm{~for~}}z\in C}\\ &{f(z)=\sum_{i=0}^{n}x_{i}f_{i}(z)\geq0{\mathrm{~for~}}z\in S,\ z\notin C,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n+1}$ . This problem is always convex, since the constraints can be expressed as 

$$
g_{1}(x)=1-\operatorname*{inf}_{z\in C}f(z)\leq0,\qquad g_{2}(x)=-\operatorname*{inf}_{z\in S\backslash C}f(z)\leq0
$$ 

( $g_{1}$ and $g_{2}$ are convex). The problem ( 7.17 ) can also be thought of as a semi-infinite linear program, i.e. , an optimization problem with a linear objective and an infinite number of linear inequalities, one for each $z\in S$ . 

In simple cases we can solve the problem ( 7.17 ) analytically. As an example, we take $S={\bf R}_{+}$ , $C=[1,\infty)$ , $f_{0}(z)=1$ , and $f_{1}(z)=z$ , wit $\mathbf{E}\,f_{1}(X)=\mathbf{E}\,X=\mu\leq1$ prior informatio cons $f(z)\,\geq\,0$ $z\in S$ redu $x_{0}\,\geq\,0$ , $x_{1}\geq0$ onstraint $f(z)\geq1$ ≥ 1 for z $z\in C$ ∈ , i.e. , x $x_{0}+x_{1}z\geq1$ ≥ 1 for all z $z\geq1$ ≥ 1, reduces to x $x_{0}+x_{1}\geq1$ ≥ 1. The problem ( 7.17 ) is then 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ x_{0}+\mu x_{1}}\\ &{{\mathrm{subject~to}}\quad x_{0}\geq0,\quad x_{1}\geq0}\\ &{\quad\quad\quad\quad x_{0}+x_{1}\geq1.}\end{array}}
$$ 

Since $0\leq\mu\leq1$ , the optimal point for this simple LP is $x_{0}=0$ , $x_{1}=1$ . This gives the classical Markov bound $\mathbf{prob}(X\geq1)\leq\mu$ . 

In other cases we can solve the problem ( 7.17 ) using convex optimization. 

Remark 7.1 Duality and the Chebyshev bound problem. The Chebyshev bound prob- lem ( 7.17 ) determines a bound on $\mathbf{prob}(X\,\in\,C)$ for all probability measures that satisfy the given expected value constraints. Thus we can think of the Chebyshev bound problem ( 7.17 ) as producing a bound on the optimal value of the infinite- dimensional problem 

$$
{\begin{array}{l r l}{{\mathrm{maximize}}}&{\int_{C}\pi(d z)}\\ {{\mathrm{subject~to}}}&{\int_{S}f_{i}(z)\pi(d z)=a_{i},\quad i=1,.\,.\,.\,,n}\\ &{\int_{S}\pi(d z)=1}\\ &{\pi\geq0,}\end{array}}
$$ 

where the variable is the measure $\pi$ , and $\pi\geq0$ means that the measure is nonnegative. 

Since the Chebyshev problem ( 7.17 ) produces a bound on the problem ( 7.18 ), it should not be a surprise that they are related by duality. While semi-infinite and infinite-dimensional problems are beyond the scope of this book, we can still formally construct a dual of the problem ( 7.17 ), introducing a Lagrange multiplier function $p:S\rightarrow\mathbf{R}$ , with $p(z)$ the Lagrange multiplier associated with the inequality $f(z)\geq1$ (for $z\in C$ ) or $f(z)\geq0$ (for $z\in S\backslash C.$ ). Using an integral over $z$ where we would have a sum in the finite-dimensional case, we arrive at the formal dual 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\int_{C}p(z)\ d z}\\ {{\mathrm{subject~to}}}&{\int_{S}f_{i}(z)p(z)\ d z=a_{i},\quad i=1,.\,.\,,n}\\ &{\int_{S}p(z)\ d z=1}\\ &{p(z)\geq0{\mathrm{~for~all~}}z\in S,}\end{array}}
$$ 

where the optimization variable is the function $p$ . This is, essentially, the same as ( 7.18 ). 

# Probability bounds with known first and second moments 

As an example, suppose that $S=\mathbf{R}^{m}$ , and that we are given the first and second moments of the random variable $X$ : 

$$
{\bf E}\,X=a\in{\bf R}^{m},\qquad{\bf E}\,X X^{T}=\Sigma\in{\bf S}^{m}.
$$ 

In other words, we are given the expected value of the $m$ functions $z_{i}$ , $i=1,\ldots,m$ , and the $m(m+1)/2$ functions $z_{i}z_{j}$ , $i,j=1,\cdot\cdot\cdot,m$ , but no other information about the distribution. 

In this case we can express $f$ as the general quadratic function 

$$
f(z)=z^{T}P z+2q^{T}z+r,
$$ 

whe variables ( i.e. , the vector $x$ in the discussion above) are ${\cal P}\in{\bf S}^{m}$ , $q\in\mathbf{R}^{m}$ , and r $r\in\mathbf{R}$ ∈ . From our knowledge of the first and second moments, we find that 

$$
\begin{array}{l c l}{{\bf E}\,f(X)}&{=}&{{\bf E}(X^{T}P X+2q^{T}X+r)}\\ &{=}&{{\bf E}\,{\bf t r}(P X X^{T})+2\,{\bf E}\,q^{T}X+r}\\ &{=}&{{\bf t r}(\Sigma P)+2q^{T}a+r.}\end{array}
$$ 

The constraint that $f(z)\,\geq\,0$ for all $\mathcal{Z}$ can be expressed as the linear matrix in- equality 

$$
\left[\begin{array}{c c}{{\boldsymbol{P}}}&{{\boldsymbol{q}}}\\ {{\boldsymbol{q}^{T}}}&{{\boldsymbol{r}}}\end{array}\right]\succeq0.
$$ 

In particular, we have $P\succeq0$ . 

Now suppose that the set $C$ is the complement of an open polyhedron, 

$$
C=\mathbf{R}^{m}\setminus\mathcal{P},\qquad\mathcal{P}=\{z\mid a_{i}^{T}z<b_{i},\ i=1,.\,.\,,k\}.
$$ 

The condition that $f(z)\geq1$ for all $z\in C$ is the same as requiring that 

$$
a_{i}^{T}z\ge b_{i}\implies z^{T}P z+2q^{T}z+r\ge1
$$ 

for $i=1,\dots,k$ . This, in turn, can be expressed as: there exist $\tau_{1},.\,.\,.\,,\tau_{k}\geq0$ such that 

$$
\left[\begin{array}{c c}{P}&{q}\\ {q^{T}}&{r-1}\end{array}\right]\succeq\tau_{i}\left[\begin{array}{c c}{0}&{a_{i}/2}\\ {a_{i}^{T}/2}&{-b_{i}}\end{array}\right],\quad i=1,\ldots,k.
$$ 

(See B.2 .) 

Putting it all together, the Chebyshev bound problem ( 7.17 ) can be expressed as 

$$
\begin{array}{r l}{\mathrm{minimize}\:\:}&{\mathbf{tr}(\Sigma P)+2q^{T}a+r}\\ {\mathrm{subject~to}\:\:}&{\left[\begin{array}{l l}{P}&{q}\\ {q^{T}}&{r-1}\end{array}\right]\succeq\tau_{i}\left[\begin{array}{c c}{0}&{a_{i}/2}\\ {a_{i}^{T}/2}&{-b_{i}}\end{array}\right],\quad i=1,\dots,k}\\ &{\tau_{i}\succeq0,\quad i=1,\dots,k}\\ &{\left[\begin{array}{c c}{P}&{q}\\ {q^{T}}&{r}\end{array}\right]\succeq0,}\end{array}
$$ 

which is a semidefinite program in the variables $P$ , , , and . The $q$ $r$ $\tau_{1},\dots,\tau_{k}$ optimal value, say $\alpha$ , is an uppe bound on $\mathbf{prob}(X\,\in\,C)$ over all distributions with mean $a$ and second moment Σ. Or, turning it around, $1-\alpha$ is a lower bound on $\mathbf{prob}(X\in{\mathcal{P}})$ . 

Remark 7.2 Duality and the Chebyshev bound problem. The dual SDP associated with ( 7.19 ) can be expressed as 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\sum_{i=1}^{k}\lambda_{i}}\\ {{\mathrm{subject~to}}}&{a_{i}^{T}z_{i}\geq b\lambda_{i},\quad i=1,\ldots,k}\\ &{\sum_{i=1}^{k}{\left[\begin{array}{l l}{Z_{i}}&{z_{i}}\\ {z_{i}^{T}}&{\lambda_{i}}\end{array}\right]}\preceq{\left[\begin{array}{l l}{\Sigma}&{a}\\ {a^{T}}&{1}\end{array}\right]}}\\ &{{\left[\begin{array}{l l}{Z_{i}}&{z_{i}}\\ {z_{i}^{T}}&{\lambda_{i}}\end{array}\right]}\succeq0,\quad i=1,\ldots,k.}\end{array}}
$$ 

The variables are $Z_{i}~\in~\mathbf{S}^{m}$ , $z_{i}~\in~\mathbf{R}^{m}$ , and $\lambda_{i}~\in~\mathbf{R}$ , for $i\;=\;1,\ldots,k$ . Since the SDP ( 7.19 ) is strictly feasible, strong duality holds and the dual optimum is attained. We can give an interesting probability interpretation to the dual problem. Suppose $Z_{i}$ , $z_{i}$ , $\lambda_{i}$ are dual feasible and that the first $r$ components of $\lambda$ are positive, and the rest are zero. For simplicity we also assume that $\textstyle\sum_{i=1}^{k}\lambda_{i}<1$ 1. We define 

$$
\begin{array}{r c l}{{x_{i}}}&{{=}}&{{(1/\lambda_{i})z_{i},~~~i=1,\ldots,r,}}\\ {{w_{0}}}&{{=}}&{{\displaystyle\frac{1}{\mu}\left(a-\sum_{i=1}^{r}\lambda_{i}x_{i}\right),}}\\ {{W}}&{{=}}&{{\displaystyle\frac{1}{\mu}\left(\Sigma-\sum_{i=1}^{r}\lambda_{i}x_{i}x_{i}^{T}\right),}}\end{array}
$$ 

where $\begin{array}{r}{\mu=1-\sum_{i=1}^{k}\lambda_{i}}\end{array}$ . With these definitions the dual feasibility constraints can be expressed as 

$$
a_{i}^{T}x_{i}\geq b_{i},\quad i=1,\ldots,r
$$ 

and 

$$
\sum_{i=1}^{r}\lambda_{i}\left[\begin{array}{c c}{{x_{i}x_{i}^{T}}}&{{x_{i}}}\\ {{x_{i}^{T}}}&{{1}}\end{array}\right]+\mu\left[\begin{array}{c c}{{W}}&{{w_{0}}}\\ {{w_{0}^{T}}}&{{1}}\end{array}\right]=\left[\begin{array}{c c}{{\Sigma}}&{{a}}\\ {{a^{T}}}&{{1}}\end{array}\right].
$$ 

Moreover, from dual feasibility, 

$$
\begin{array}{r c l}{\mu\left[\begin{array}{c c}{W}&{w_{0}}\\ {w_{0}^{T}}&{1}\end{array}\right]}&{=}&{\left[\begin{array}{c c}{\Sigma}&{a}\\ {a^{T}}&{1}\end{array}\right]-\displaystyle\sum_{i=1}^{r}\lambda_{i}\left[\begin{array}{c c}{x_{i}x_{i}^{T}}&{x_{i}}\\ {x_{i}^{T}}&{1}\end{array}\right]}\\ &{=}&{\left[\begin{array}{c c}{\Sigma}&{a}\\ {a^{T}}&{1}\end{array}\right]-\displaystyle\sum_{i=1}^{r}\left[\begin{array}{c c}{(1/\lambda_{i})z_{i}z_{i}^{T}}&{z_{i}}\\ {z_{i}^{T}}&{\lambda_{i}}\end{array}\right]}\\ &{\succeq}&{\left[\begin{array}{c c}{\Sigma}&{a}\\ {a^{T}}&{1}\end{array}\right]-\displaystyle\sum_{i=1}^{r}\left[\begin{array}{c c}{Z_{i}}&{z_{i}}\\ {z_{i}^{T}}&{\lambda_{i}}\end{array}\right]}\\ &{\succeq}&{0.}\end{array}
$$ 

Therefore, $W\,\succeq\,w_{0}w_{0}^{T}$ , so it can b factored as $\begin{array}{r}{W\,-\,w_{0}w_{0}^{T}\;=\;\sum_{i=1}^{s}w_{i}w_{i}^{T}}\end{array}$ P . Now consider a discrete random variable X with the following distribution. If $s\geq1$ , we take 

$$
\begin{array}{l l}{{X=x_{i}}}&{{\mathrm{with~probability}~\lambda_{i},\,\,\,i=1,\ldots,r}}\\ {{X=w_{0}+\sqrt{s}\,w_{i}}}&{{\mathrm{with~probability}~\mu/(2s),\,\,\,i=1,\ldots,s}}\\ {{X=w_{0}-\sqrt{s}\,w_{i}}}&{{\mathrm{with~probability}~\mu/(2s),\,\,\,\,i=1,\ldots,s.}}\end{array}
$$ 

If $s=0$ , we take 

$$
\begin{array}{l l}{X=x_{i}\;}&{\mathrm{with~probability~}\lambda_{i},\;\ i=1,.\,.\,,r}\\ {X=w_{0}}&{\mathrm{with~probability~}\mu.}\end{array}
$$ 

It is easily verified that $\mathbf{E}\,X=a$ and $\mathbf{E}\,X X^{T}=\Sigma$ , i.e. , the distribution matches the given moments. Furthermore, since $x_{i}\in C$ , 

$$
\mathbf{prob}(X\in C)\geq\sum_{i=1}^{r}\lambda_{i}.
$$ 

In particular, by applying this interpretation to the dual optimal solution, we can construct a distribution that satisfies the Chebyshev bound from ( 7.19 ) with equality, which shows that the Chebyshev bound is sharp for this case. 

# 7.4.2 Chernoﬀbounds 

Let $X$ be a random variable on $\mathbf{R}$ . The Chernoﬀbound states that 

$$
\mathbf{prob}(X\geq u)\leq\operatorname*{inf}_{\lambda\geq0}\mathbf{E}\,e^{\lambda(X-u)},
$$ 

which can be expressed as 

$$
\log\mathbf{prob}(X\geq u)\leq\operatorname*{inf}_{\lambda\geq0}\{-\lambda u+\log\mathbf{E}\,e^{\lambda X}\}.
$$ 

Recall (from example 3.41 , page 106 ) that the righthand term, $\log\mathbf{E}\,e^{\lambda X}$ , is called the cumulant generating function of the distribution, and is always convex, so the function to be minimized is convex. The bound ( 7.20 ) is most useful in cases when the cumulant generating function has an analytical expression, and the minimiza- tion over $\lambda$ can be carried out analytically. 

For example, if $X$ is Gaussian with zero mean and unit variance, the cumulant generating function is 

$$
\log\mathbf{E}\,e^{\lambda X}=\lambda^{2}/2,
$$ 

and the infimum over $\lambda\geq0$ of $-\lambda u+\lambda^{2}/2$ occurs with $\lambda\,=\,u$ (if $u\geq0$ ), so the Chernoﬀbound is (for $u\geq0$ ) 

$$
\mathbf{prob}(X\geq u)\leq e^{-u^{2}/2}.
$$ 

The idea behind the Chernoﬀbound can be extended to a more general setting, in which convex optimization is used to compute a bound on the probability of a set in $\mathbf{R}^{m}$ . Let $C\subseteq\mathbf{R}^{m}$ , and as in the de ription of Chebyshev bounds above, let $1_{C}$ denote the 0-1 indicator function of C . We will derive an upper bound on $\mathbf{prob}(X\in C)$ . (In principle we can compute $\mathbf{prob}(X\in C)$ , for example by Monte Carlo simulation, or numerical integration, but either of these can be a daunting computational task, and neither method produces guaranteed bounds.) 

Let $\lambda\in\mathbf{R}^{m}$ and $\mu\in\mathbf{R}$ , and consider the function $f:\mathbf{R}^{m}\rightarrow\mathbf{R}$ given by 

$$
f(z)=e^{\lambda^{T}z+\mu}.
$$ 

As in the development of Chebyshev bounds, if $f$ satisfies $f(z)\geq1_{C}(z)$ for all $z$ , then we can conclude that 

$$
\mathbf{prob}(X\in C)=\mathbf{E}\,1_{C}(X)\leq\mathbf{E}\,f(X).
$$ 

Clearly we have $f(z)\,\geq\,0$ for all $\mathcal{Z}$ ; to have $f(z)\,\geq\,1$ for $z\,\in\,C$ is the same as $\lambda^{T}z+\mu\geq0$ for all $z\in C$ , i.e. , $-\lambda^{T}z\leq\mu$ for all $z\in C$ . Thus, if $-\lambda^{T}z\leq\mu$ for all $z\in C$ , we have the bound 

$$
\mathbf{prob}(X\in C)\leq\mathbf{E}\exp(\lambda^{T}X+\mu),
$$ 

or, taking logarithms, 

$$
\log\mathbf{prob}(X\in C)\leq\mu+\log\mathbf{E}\exp(\boldsymbol{\lambda}^{T}X).
$$ 

From this we obtain a general form of Chernoﬀ’s bound: 

$$
\begin{array}{l l l}{\log\mathbf{prob}(X\in{\cal C})}&{\leq}&{\operatorname*{inf}\{\mu+\log\mathbf{E}\exp(\lambda^{T}X)\mid\;-\lambda^{T}z\leq\mu\mathrm{~for~all~}z\in{\cal C}\}}\\ &{=}&{\underset{\lambda}{\operatorname*{inf}}\left(\underset{z\in{\cal C}}{\operatorname*{sup}}(-\lambda^{T}z)+\log\mathbf{E}\exp(\lambda^{T}X)\right)}\\ &{=}&{\operatorname*{inf}\left(S_{\cal C}(-\lambda)+\log\mathbf{E}\exp(\lambda^{T}X)\right),}\end{array}
$$ 

where $S_{C}$ is the support function of $C$ . Note that the second term, $\log\mathbf{E}\exp(\lambda^{T}X)$ , is the cumulant generating function of the distribution, and is always convex (see example 3.41 , page 106 ). Evaluating this bound is, in general, a convex optimiza- tion problem. 

# Chernoﬀbound for a Gaussian variable on a polyhedron 

As a specific example, suppose that $X$ is a Gaussian random vector on $\mathbf{R}^{m}$ with zero mean and covariance $I$ , so its cumulant generating function is 

$$
\log\mathbf{E}\exp(\lambda^{T}X)=\lambda^{T}\lambda/2.
$$ 

We take $C$ to be a polyhedron described by inequalities: 

$$
C=\{x\mid A x\preceq b\},
$$ 

which we assume is nonempty. 

For use in the Chernoﬀbound, we use a dual characterization of the support function $S_{C}$ : 

$$
\begin{array}{l l l}{S_{C}(y)}&{=}&{\operatorname*{sup}\{y^{T}x\mid A x\preceq b\}}\\ &{=}&{-\operatorname*{inf}\{-y^{T}x\mid A x\preceq b\}}\\ &{=}&{-\operatorname*{sup}\{-b^{T}u\mid A^{T}u=y,\;u\succeq0\}}\\ &{=}&{\operatorname*{inf}\{b^{T}u\mid A^{T}u=y,\;u\succeq0\}}\end{array}
$$ 

where in the third line we use LP duality: 

$$
\operatorname*{inf}\{c^{T}x\mid A x\preceq b\}=\operatorname*{sup}\{-b^{T}u\mid A^{T}u+c=0,\;u\succeq0\}
$$ 

with $c=-y$ . Using this expression for $S_{C}$ in the Chernoﬀbound we obtain 

$$
\begin{array}{l c l}{\log\mathbf{prob}(X\in{\cal C})}&{\leq}&{\operatorname*{inf}_{\lambda}\left(S_{\cal C}(-\lambda)+\log\mathbf{E}\exp(\lambda^{T}X)\right)}\\ &{=}&{\operatorname*{inf}_{\lambda}\operatorname*{inf}_{u}\{b^{T}u+\lambda^{T}\lambda/2\ \left|\ u\succeq0,\ A^{T}u+\lambda=0\}.}\end{array}
$$ 

Thus, the Chernoﬀbound on $\mathbf{prob}(X\in C)$ is the exponential of the optimal value of the QP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ b^{T}u+\lambda^{T}\lambda/2}\\ &{{\mathrm{subject~to}}\quad u\succeq0,\quad A^{T}u+\lambda=0,}\end{array}}
$$ 

where the variables are $u$ and $\lambda$ . 

This problem has an interesting geometric interpretation. It is equivalent to 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad b^{T}u+(1/2)\|A^{T}u\|_{2}^{2}}\\ &{{\mathrm{subject~to}}\quad u\succeq0,}\end{array}}
$$ 

which is the dual of 

$$
\begin{array}{l c l}{{\mathrm{maximize}}}&{{-(1/2)\|x\|_{2}^{2}}}\\ {{\mathrm{subject~to}}}&{{A x\preceq b.}}\end{array}
$$ 

In other words, the Chernoﬀbound is 

$$
\mathbf{prob}(X\in C)\leq\exp(-\,\mathbf{dist}(0,C)^{2}/2),
$$ 

where $\mathbf{dist}(0,C)$ is the Euclidean distance of the origin to $C$ . 

Remark 7.3 The bound ( 7.22 ) can also be derived without using Chernoﬀ’s inequality. If the distance between $0$ an $C$ is $d$ , then there is a pace ${\mathcal{H}}=\{z\mid a^{T}z\geq d\}$ , with $\Vert a\Vert_{2}=1$ , that contains C . The random variable a $a^{T}X$ is ${\mathcal{N}}(0,1)$ , so 

$$
\mathbf{prob}(X\in C)\leq\mathbf{prob}(X\in{\mathcal{H}})=\Phi(-d),
$$ 

where $\Phi$ is the cumulative distribution function of a zero mean, unit variance Gaus- sian. Since $\Phi(-d)\leq e^{-d^{2}/2}$ for $d\geq0$ , this bound is at least as sharp as the Chernoﬀ bound ( 7.22 ). 

# 7.4.3 Example 

In this section we illustrate the Chebyshev and Cher no probability bounding methods with a detection example. We have a set of $m$ possible symbols or signals $s\,\in\,\{s_{1},s_{2},.\,.\,.\,.\,,s_{m}\}\,\subseteq\,\mathbf{R}^{n}$ , which is called the signal constellation . One of these signals is transmitted over a noisy channel. The received signal is $x\;=\;s\,+\,v$ , where $v$ is a noise, modeled as a random variable. We assume that $\mathbf{E}\,v\,=\,0$ and ${\bf E}\,v v^{T}\,=\,\sigma^{2}I$ , i.e. , the noise components $v_{1},\dots,v_{n}$ are zero mean, uncorrelated, and have variance $\sigma^{2}$ . The receiver must estimate which signal was sent on the basis of the received signal $x=s+v$ . The minimum distance detector chooses as estimate the symbol $s_{k}$ closest (in Euclidean norm) to $x$ . (If the noise $v$ is Gaussian, then minimum distance decoding is the same as maximum likelihood decoding.) 

If the signal $s_{k}$ is transmitted, correct detection occurs if $s_{k}$ is the estimate, given $x$ . This occurs when the signal $s_{k}$ is closer to $x$ than the other signals, i.e. , 

$$
\|x-s_{k}\|_{2}<\|x-s_{j}\|_{2},\qquad j\neq k.
$$ 

Thus, correct detection of symbol $s_{k}$ occurs if the random variable $v$ satisfies the linear inequalities 

$$
2(s_{j}-s_{k})^{T}(s_{k}+v)<\|s_{j}\|_{2}^{2}-\|s_{k}\|_{2}^{2},\quad j\neq k.
$$ 

These inequalities define the Voronoi region $V_{k}$ of $s_{k}$ in the signal constellation, i.e. , the set of points closer to $s_{k}$ than any other signal in the constellation. The probability of correct detection of $s_{k}$ is $\mathbf{prob}(s_{k}+v\in V_{k})$ . 

Figure 7.5 shows a simple example with m $m=7$ = 7 signals, with dimension $n=2$ . 

![](images/913f08aa9e4b1cae063595ef6d4d0db941d27afe2953470ce2fe352fadb5670b.jpg) 
Figure 7.5 A constellation of 7 signals $s_{1},.\,.\,.\,,s_{7}\in\mathbf{R}^{2}$ , shown as small circles. The line segments show the boundaries of the corresponding Voronoi regions. The minimum distance detector selects symbol $s_{k}$ when the received signal lies closer to $s_{k}$ than to any of the other points, i.e. , if the received signal is in the interior of the Voronoi region around symbol $s_{k}$ . The circles around each point have radius one, to show the scale. 

# Chebyshev bounds 

The SDP bound ( 7.19 ) provides a lower bound on the probability of correct detec- tion, and is plotted in figure 7.6 , as a function of the noise standard deviation $\sigma$ , for the three symbols $s_{1}$ , $s_{2}$ , and $s_{3}$ . These bounds hold for any noise distribution with zero mean and covariance $\sigma^{2}I$ . They are tight in the sense that there exists a noise distribution with zero mean and covariance $\Sigma=\sigma^{2}I$ , for which the proba- bility of error is equal to the lower bound. This is illustrated in figure 7.7 , for the first Voronoi set, and $\sigma=1$ . 

# Chernoﬀbounds 

We use the same example to illustrate the Chernoﬀbound. Here we assume that the noise is Gaussian, i.e. , $v\sim\mathcal{N}(0,\sigma^{2}I)$ . If symbol $s_{k}$ is transmitted, the probability of correct detection is the probability that $s_{k}+v\in V_{k}$ . To find a lower bound for this probability, we use the QP ( 7.21 ) to compute upper bounds on the probability that the ML detector selects symbol $i$ , $i=1,\ldots,m$ , $i\neq k$ . (Each of these upper bounds is related to the distance of $s_{k}$ to the Voronoi set $V_{i}$ .) Adding these upper bounds on the probabilities of mistaking $s_{k}$ for $s_{i}$ , we obtain an upper bound on the probability of error, and therefore, a lower bound on the probability of correct detection of symbol $s_{k}$ . The resulting lower bound, for $s_{1}$ , is shown in figure 7.8 , along with an estimate of the probability of correct detection obtained using Monte Carlo analysis. 

![](images/92004b8d3d455775b77e8a252fe169d10328f48bbd9e21cdaef8eef3508acd01.jpg) 
Figure 7.6 Chebyshev lower bounds on the probability of correct detection for symbols $s_{1}$ , $s_{2}$ , and $s_{3}$ . These bounds are valid for any noise distribution that has zero mean and covariance $\sigma^{2}I$ . 

![](images/d5ce86200ba9b95f6fae42695937a72aa87954443ff72fa7893559b7d2749ebe.jpg) 
Figure 7.7 The Chebyshev lower bound on the probability of correct detec- tion of symbol 1 is equal to 0 . 2048 when $\sigma=1$ . This bound is achieved by the discrete distribution illustrated in the figure. The solid circles are the possible values of the received signal $s_{1}+v$ . The point in the center of the ellipse has probability 0 . 2048. The five points on the boundary have a total probability 0 . 7952. The ellipse is defined by $x^{T}P x+2q^{T}x+r=1$ , where $P$ , $q$ , and $r$ are the optimal solution of the SDP ( 7.19 ). 

![](images/eab4a1f9613f07222177d3d717b111705079e88b440027a94d4966ceba024863.jpg) 
Figure 7.8 The Chernoﬀlower bound (solid line) and a Monte Carlo esti- mate (dashed line) of the probability of correct detection of symbol $s_{1}$ , as a function of $\sigma$ . In this example the noise is Gaussian with zero mean and covariance $\sigma^{2}I$ . 

# 7.5 Experiment design 

We consider the problem of estimating a vector $x\,\in\,\mathbf{R}^{\,n}$ from measurements or experiments 

$$
y_{i}=a_{i}^{T}x+w_{i},\quad i=1,.\,.\,.\,,m,
$$ 

where $w_{i}$ is measurement noise. We assume that $w_{i}$ are independent Gaussian random variables with zero mean and unit variance, and that the measurement vectors span $\mathbf{R}^{n}$ . The maximum likelihood estimate of , which is the $a_{1},\ldots,a_{m}$ $x$ same as the minimum variance estimate, is given by the least-squares solution 

$$
\hat{x}=\left(\sum_{i=1}^{m}a_{i}a_{i}^{T}\right)^{-1}\sum_{i=1}^{m}y_{i}a_{i}.
$$ 

The associated estimation error $e={\hat{x}}-x$ has zero mean and covariance matrix 

$$
E=\mathbf{E}\,e e^{T}=\left(\sum_{i=1}^{m}a_{i}a_{i}^{T}\right)^{-1}.
$$ 

The matrix $E$ characterizes the accuracy of the estimation, or the informativeness of the experiments. For example the $\alpha$ -confidence level ellipsoid for $x$ is given by 

$$
{\mathcal{E}}=\{z\mid(z-{\hat{x}})^{T}E^{-1}(z-{\hat{x}})\leq\beta\},
$$ 

where $\beta$ is a constant that depends on $n$ and $\alpha$ . 

We suppose that the vectors $a_{1},\ldots,a_{m}$ , which characterize the measurements, can be chosen among $p$ possible test vectors $v_{1},\ldots,v_{p}\in\mathbf{R}^{n}$ , i.e. , each $a_{i}$ is one of the $v_{j}$ . The goal of experiment design is to choose the vectors $a_{i}$ , from among the possible choices, so that the error covariance $E$ is small (in some sense). In other words, each of $m$ experiments or measurements can be chosen from a fixed menu of $p$ possible experiments; our job is to find a set of measurements that (together) are maximally informative. 

Let $m_{j}$ denote the number of experiments for which $a_{i}$ is chosen to have the value $v_{j}$ , so we have 

$$
m_{1}+\cdot\cdot\cdot+m_{p}=m.
$$ 

We can express the error covariance matrix as 

$$
E=\left(\sum_{i=1}^{m}a_{i}a_{i}^{T}\right)^{-1}=\left(\sum_{j=1}^{p}m_{j}v_{j}v_{j}^{T}\right)^{-1}.
$$ 

This shows that the error covariance depends only on the numbers of each type of experiment chosen ( i.e. , $m_{1},.\cdot\cdot\cdot,m_{p}$ ). 

The basic experiment design problem is as follows. Given the menu of possible choices for experiments, i.e. , $v_{1},\dots,v_{p}$ , and the total number $m$ of experiments to be carried out, choose the numbers of each type of experiment, i.e. , $m_{1},\,\cdot\cdot\cdot\,,m_{p}$ , to make the error covariance $E$ small (in some sense). The variables $m_{1},\dots,m_{p}$ must, of course, be integers and sum to $m$ , the given total number of experiments. This leads to the optimization problem 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize~}}({\mathrm{w.r.t.~}}\mathbf{S}_{+}^{n})}&{E=\left(\sum_{j=1}^{p}m_{j}v_{j}v_{j}^{T}\right)^{-1}}\\ &{{\mathrm{subject~to}}}&&{m_{i}\geq0,\quad m_{1}+\cdot\cdot\cdot+m_{p}=m}\\ &{}&&{m_{i}\in\mathbf{Z},}\end{array}}
$$ 

where the variables are the integers $m_{1},\dots,m_{p}$ . 

The basic experiment design problem ( 7.23 ) is a vector optimization problem over the positive semidefinite cone. If one experiment design results in $E$ , and another in $\ddot{E}$ $E\preceq E$ , with , then certainly the first experiment design is as good as or better than the second. For example, the confidence ellipsoid for the first experiment design (translated to the origin for comparison) is contained in the confidence ellipsoid of the second. We can also say that the first experiment design allows us to estimate $q^{T}x$ better ( i.e. , with lower variance) than the second experi- ment design, for any vector $q$ , since the variance of our estimate of $q^{T}x$ is given by $q^{T}E q$ for the first experiment design and $q^{T}\tilde{E}q$ for the second. We will see below several common scalarizations for the problem. 

# 7.5.1 The relaxed experiment design problem 

The basic experiment design problem ( 7.23 ) can be a hard combinatorial problem when $m$ , the total number of experiments, is comparable to $p$ , since in this case the $m_{i}$ are all small integers. In the case when $m$ is large compared to $p$ , however, a good approximate solution of ( 7.23 ) can be found by ignoring, or relaxing, the constraint that the $m_{i}$ are integers. Let $\lambda_{i}~=~m_{i}/m$ , which is the fraction of the total number of experiments for which $a_{j}~=~v_{i}$ , or the relative frequency of experiment $i$ . We can express the error covariance in terms of $\lambda_{i}$ as 

$$
E=\frac{1}{m}\left(\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}\right)^{-1}.
$$ 

The vector $\lambda\in\mathbf{R}^{p}$ satisfies $\lambda\succeq0$ , ${\bf1}^{T}\lambda=1$ , and also, each $\lambda_{i}$ is an integer multiple of $1/m$ . By ignoring this last constraint, we arrive at the problem 

$$
\begin{array}{l l}{\mathrm{minimize~}(\mathrm{w.r.t.~}\,\mathbf{S}_{+}^{n})}&{E=(1/m)\left(\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}\right)^{-1}}\\ {\mathrm{subject~to}}&{\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1,}\end{array}
$$ 

with variable $\lambda\in\mathbf{R}^{p}$ . To distinguish this from the original combinatorial experi- ment design problem ( 7.23 ), we refer to it as the relaxed experiment design problem . The relaxed experiment design problem ( 7.25 ) is a convex optimization problem, since the objective $E$ is an $\mathbf{S}_{+}^{n}$ -convex function of $\lambda$ . 

Several statements can be made about the relation between the (combinato- rial) experiment design problem ( 7.23 ) and the relaxed problem ( 7.25 ). Clearly the optimal value of the relaxed problem provides a lower bound on the optimal value of the combinatorial one, since the combinatorial problem has an additional constraint. From a solution of the relaxed problem ( 7.25 ) we can construct a sub- optimal solution of the combinatorial problem ( 7.23 ) as follows. First, we apply simple rounding to get 

$$
m_{i}=\mathbf{round}(m\lambda_{i}),\quad i=1,.\,.\,,p.
$$ 

Corresponding to this choice of $m_{1},\dots,m_{p}$ is the vector $\ddot{\lambda}$ , 

$$
\tilde{\lambda}_{i}=(1/m){\bf r o u n d}(m\lambda_{i}),\quad i=1,.\,.\,.\,,p.
$$ 

The vector $\tilde{\lambda}$ satisfies the constraint that each entry is an integer multiple of $1/m$ . Clearly we have $|\lambda_{i}-\tilde{\lambda}_{i}|\leq1/(2m)$ | ≤ ), so for $m$ large, we have $\lambda\approx{\tilde{\lambda}}$ . This implies that the constraint 1 ${\mathbf1}^{T}\ddot{\lambda}=1$ = 1 is nearly satisfied, for large $m$ , and also that the error covariance matrices associated with $\tilde{\lambda}$ and $\lambda$ are close. 

We can also give an alternative interpretation of the relaxed experiment design problem ( 7.25 ). We can interpret the vector $\lambda\,\in\,\mathbf{R}^{p}$ as defining a probability distribution on the experiments $v_{1},\dots,v_{p}$ . Our choice of λ corresponds to a random experiment : each experiment $a_{i}$ takes the form $v_{j}$ with probability $\lambda_{j}$ . 

In the rest of this section, we consider only the relaxed experiment design problem, so we drop the qualifier ‘relaxed’ in our discussion. 

# 7.5.2 Scalarizations 

Several scalarizations have been proposed for the experiment design problem ( 7.25 ), which is a vector optimization problem over the positive semidefinite cone. 

# $D$ -optimal design 

The most widely used scalarization is called $D$ -optimal design , in which we minimize the determinant of the error covariance matrix $E$ . This corresponds to designing the experiment to minimize the volume of the resulting confidence ellipsoid (for a fixed confidence level). Ignoring the constant factor $1/m$ in $E$ , and taking the logarithm of the objective, we can pose this problem as 

$$
\begin{array}{l}{\mathrm{minimize}\quad\log\operatorname*{det}\left(\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}\right)^{-1}}\\ {\mathrm{subject~to}\quad\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1,}\end{array}
$$ 

which is a convex optimization problem. 

# $E$ -optimal design 

In $E$ -optimal design , we minimize the norm of the error covariance matrix, i.e. , the maximum eigenvalue of $E$ . Since the diameter (twice the longest semi-axis) $\|E\|_{2}^{1/2}$ of the confidence ellipsoid $\mathcal{E}$ is proportional to , minimizing $\|E\|_{2}$ can be interpreted geometrically as minimizing the diameter of the confidence ellipsoid. $E$ -optimal design can also be interpreted as minimizing the maximum variance of $q^{T}e$ , ove all $q$ with $\|q\|_{2}=1$ . 

The E -optimal experiment design problem is 

$$
\begin{array}{l}{\mathrm{minimize}\quad\left\|\left(\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}\right)^{-1}\right\|_{2}}\\ {\mathrm{subject~to}\quad\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1.}\end{array}
$$ 

The objective is a convex function of $\lambda$ , so this is a convex problem. The $E$ -optimal experiment design problem can be cast as an SDP 

$$
\begin{array}{l r}{\mathrm{maximize}}&{t}\\ {\mathrm{subject~to}}&{\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}\succeq t I}\\ &{\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1,}\end{array}
$$ 

with variables $\lambda\in\mathbf{R}^{p}$ and $t\in\mathbf{R}$ . 

# $A$ -optimal design 

In $A$ -optimal experiment design , we minimize $\mathrm{\bftr}\,E$ , the trace of the covariance matrix. This objective is simply the mean of the norm of the error squared: 

$$
{\mathbf E}\,||e||_{2}^{2}={\mathbf E}\,\mathrm{tr}(e e^{T})=\mathrm{tr}\,E.
$$ 

The $A$ -optimal experiment design problem is 

$$
\begin{array}{l}{\mathrm{minimize}\quad\mathrm{\bf~tr}\left(\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}\right)^{-1}}\\ {\mathrm{subject~to}\quad\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1.}\end{array}
$$ 

This, too, is a convex problem. Like the $E$ -optimal experiment design problem, it can be cast as an SDP: 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\mathbf{1}^{T}u}\\ {\mathrm{subject~to}}&{\left[\begin{array}{l l}{\sum_{i=1}^{p}\lambda_{i}v_{i}v_{i}^{T}}&{e_{k}}\\ {e_{k}^{T}}&{u_{k}}\end{array}\right]\succeq0,\quad k=1,\ldots,n}\\ &{\lambda\succeq0,\quad\mathbf{1}^{T}\lambda=1,}\end{array}
$$ 

where the variables are $u\in\mathbf{R}^{n}$ and $\lambda\in\mathbf{R}^{p}$ , and here, is the $k$ th unit vector. $e_{k}$ 

# Optimal experiment design and duality 

The Lagrange duals of the three scalarizations have an interesting geometric mean- ing. 

The dual of the $D$ -optimal experiment design problem ( 7.26 ) can be expressed as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\log\operatorname*{det}W+n\log n}\\ &{\mathrm{subject~to}\quad v_{i}^{T}W v_{i}\leq1,\quad i=1,\dots,p,}\end{array}
$$ 

with variable $W\,\in\,{\bf S}^{n}$ and domain $\mathbf{S}_{++}^{n}$ (see exercise 5.10 ). This dual problem has a simple interpretation: The optimal solution $W^{\star}$ determines the minimum volume ellipsoid, centered at the origin, given by $\{x\mid x^{T}W^{\star}x\leq1\}$ , that contains the points $v_{1},\dots,v_{p}$ . (See also the discussion of problem ( 5.14 ) on page 222 .) By complementary slackness, 

$$
\lambda_{i}^{\star}(1-v_{i}^{T}W^{\star}v_{i})=0,\quad i=1,.\,.\,.\,,p,
$$ 

i.e. , the optimal experiment design only uses the experiments $v_{i}$ which lie on the surface of the minimum volume ellipsoid. 

The duals of the $E$ -optimal and $A$ -optimal design problems can be given a similar interpretation. The duals of problems ( 7.27 ) and ( 7.28 ) can be expressed as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\mathbf{tr}\,W}\\ &{\mathrm{subject~to}\quad v_{i}^{T}W v_{i}\leq1,\quad i=1,.\,.\,.\,,p}\\ &{\quad\quad\,W\succeq0,}\end{array}
$$ 

and 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad(\mathbf{tr}\,W^{1/2})^{2}}\\ &{\mathrm{subject~to}\quad v_{i}^{T}W v_{i}\leq1,\quad i=1,.\,.\,.\,,p,}\end{array}
$$ 

respectively. The variable in both problems is $W\,\in\,{\bf S}^{n}$ . In the second problem there is an implicit constraint $W\in\mathbf{S}_{+}^{n}$ . (See exercises 5.40 and 5.10 .) 

As for the $D$ -optimal design, the optimal solution $W^{\star}$ determines a minimal ipsoid $\{x\mid x^{T}W^{\star}x\leq1\}$ that contains the points . Moreover $W^{\star}$ and $v_{1},\dots,v_{p}$ $\lambda^{\star}$ satisfy the complementary slackness conditions ( 7.29 ), i.e. , the optimal design only uses experiments $v_{i}$ that lie on the surface of the ellipsoid defined by $W^{\star}$ . 

# Experiment design example 

We consider a problem with $x\in\mathbf{R}^{2}$ , and $p=20$ . The 20 candidate measurement vectors $a_{i}$ are shown as circles in figure 7.9 . The origin is indicated with a cross. The $D$ -optimal experiment has only two nonzero $\lambda_{i}$ , indicated as solid circles in figure 7.9 . The $E$ -optimal experiment has two nonzero $\lambda_{i}$ , indicated as solid circles in figure 7.10 . The $A$ -optimal experiment has three nonzero $\lambda_{i}$ , indicated as solid circles in figure 7.11 . We also sho he three ellipsoid $\{x\mid x^{T}W^{\star}x\leq1\}$ associated with the dual optimal solutions W $W^{\star}$ . The resulting 90% confidence ellipsoids are shown in figure 7.12 , along with the confidence ellipsoid for the ‘uniform’ design, with equal weight $\lambda_{i}=1/p$ on all experiments. 

![](images/1ce115bb62aa3537435e6b421bf6e22615a3dc547830ec79969d156ea97e0a98.jpg) 
Figure 7.9 Experiment design example. The 20 candidate measurement vec- tors are indicated with circles. The $D$ -optimal design uses the two measure- ment vectors indicated with solid circles, and puts an equal weight $\lambda_{i}=0.5$ on each of them. The ellipsoid is the minimum volume ellipsoid centered at the origin, that contains the points $v_{i}$ . 

![](images/421cb3c1e21ab29f1a898927aa4d61953c59e8d9fd0110d875cff1f7f8d85926.jpg) 
Figure 7.10 The $E$ -optimal design uses two measurement vectors. The dashed es are (part of) the boundary of the ellipsoid $\{x\mid x^{T}W^{\star}x\,\leq\,1\}$ where W $W^{\star}$ is the solution of the dual problem ( 7.30 ). 

![](images/d47cc75a4dd7e803554aef313e8762cd336fef9b7d020963bcee1de0a5598d53.jpg) 
Figure 7.11 The $A$ -optimal design uses three measurement vectors. The dashed line shows the ellipsoid $\{x\mid x^{T}W^{\star}x\leq1\}$ associated with the solution of the dual problem ( 7.31 ). 

![](images/fbc6044b5385698ec372c3e3882e413578cf64a8807fc6aecb599b756aa4664b.jpg) 
Figure 7.12 Shape of the $90\%$ confidence ellipsoids for $D$ -optimal, $A$ -optimal, $E$ -optimal, and uniform designs. 

# 7.5.3 Extensions 

# Resource limits 

Suppose that associated with each experiment is a cost $c_{i}$ , which could represent the economic cost, or time required, to carry out an experiment with $v_{i}$ . The total cost, or time required (if the experiments are carried out sequentially) is then 

$$
m_{1}c_{1}+\cdot\cdot\cdot+m_{p}c_{p}=m c^{T}\lambda.
$$ 

e can add a limit on total cost by adding the linear inequality $m c^{T}\lambda\leq B$ , where B is a budget, to the basic experiment design problem. We can add multiple linear inequalities, representing limits on multiple resources. 

# Multiple measurements per experiment 

We can also consider a generalization in which each experiment yields multiple measurements. In other words, when we carry out an experiment using one of the possible choices, we obtain several measurements. To model this situation we can use the same notation as before, with $v_{i}$ as matrices in $\mathbf{R}^{n\times k_{i}}$ : 

$$
v_{i}=\left[\begin{array}{l l l}{u_{i1}}&{\cdot\cdot\cdot}&{u_{i k_{i}}}\end{array}\right],
$$ 

where $k_{i}$ is the number of (scalar) measurements obtained when the experiment $v_{i}$ is carried out. The error covariance matrix, in this more complicated setup, has the exact same form. 

In conjunction with additional linear inequalities representing limits on cost or time, we can model discounts or time savings associated with performing groups of measurements simultaneously. Suppose, for example, that the cost of simulta- neously making (scalar) measurements $v_{1}$ and $v_{2}$ is less than the sum of the costs of making them separately. We can take $v_{3}$ to be the matrix 

$$
v_{3}={\left[\begin{array}{l l}{v_{1}}&{v_{2}}\end{array}\right]}
$$ 

and assign costs $c_{1}$ , $c_{2}$ , and $c_{3}$ associated with making the first measurement alone, the second measurement alone, and the two simultaneously, respectively. 

When we solve the experiment design problem, $\lambda_{1}$ will give us the fraction of times we should carry out the first experiment alone, $\lambda_{2}$ will give us the fraction of times we should carry out the second experiment alone, and $\lambda_{3}$ will give us the fraction of times we should carry out the two experiments simultaneously.

 (Normally we would expect a choice to be made here; we would not expect to have

 $\lambda_{1}>0$ , $\lambda_{2}>0$ , and $\lambda_{\mathrm{3}}>0$ .) 

# Bibliography 

ML and MAP estimation, hypothesis testing, and detection are covered in books on statistics, pattern recognition, statistical signal processing, or communications; see, for example, Bickel and Doksum [ BD77 ], Duda, Hart, and Stork [ DHS99 ], Scharf [ Sch91 ], or Proakis [ Pro01 ]. 

Logistic regression is discussed in Hastie, Tibshirani, and Friedman [ HTF01 , § 4.4]. For the covariance estimation problem of page 355 , see Anderson [ And70 ]. 

Generalizations of Chebyshev’s inequality were studied extensively in the sixties, by Isii [ Isi64 ], Marshall and Olkin [ MO60 ], Karlin and Studden [ KS66 , chapter 12], and others. The connection with semidefinite programming was made more recently by Bertsimas and Sethuraman [ BS00 ] and Lasserre [ Las02 ]. 

The terminology in § 7.5 ( $A$ -, $D$ -, and $E$ -optimality) is standard in the literature on optimal experiment design (see, for example, Pukelsheim [ Puk93 ]). The geometric interpretation of the dual $D$ -optimal design problem is discussed by Titterington [ Tit75 ]. 

# Exercises 

# Estimation 

7.1 Linear measurements with exponentially distributed noise. Show how to solve the ML estimation problem ( 7.2 ) when the noise is exponentially distributed, with density 

$$
p(z)=\left\{\begin{array}{l l}{(1/a)e^{-z/a}}&{z\geq0}\\ {0}&{z<0,}\end{array}\right.
$$ 

where $a>0$ . 

7.2 ML estimation and $\ell_{\infty}$ -norm approximation. We consider the linear measurement model $y=A x+v$ of page 352 , with a uniform noise distribution of the form 

$$
p(z)={\left\{\begin{array}{l l}{1/(2\alpha)}&{|z|\leq\alpha}\\ {0}&{|z|>\alpha.}\end{array}\right.}
$$ 

As mentioned in example 7.1 , page 352 , any $x$ that satisfies $\|A x\,-\,y\|_{\infty}\,\leq\,\alpha$ is a ML estimate. 

Now assume that the parameter $\alpha$ is not known, and we wish to estimate $\alpha$ , along with the parameters $x$ . Show that the ML estimates of $x$ and $\alpha$ are found by solving the $\ell_{\infty}$ -norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-y\|_{\infty},
$$ 

where $a_{i}^{T}$ are the rows of $A$ . 

7.3 Probit model. Suppose $y\in\{0,1\}$ is random variable given by 

$$
y={\left\{\begin{array}{l l}{1}&{a^{T}u+b+v\leq0}\\ {0}&{a^{T}u+b+v>0,}\end{array}\right.}
$$ 

where the vector $u\,\in\,\mathbf{R}^{\,n}$ is a vector of explanatory variables (as in the logistic model described on page 354 ), and $v$ is a zero mean unit variance Gaussian variable. Formulate the ML estimation problem of estimating $u$ and $b$ , given data consisting of pairs $\left({{u_{i}},{y_{i}}}\right)$ , $i=1,\dots,N$ , as a convex optimization problem. 

7.4 Estimation of covariance and mean of a multivariate normal distribution. We consider the problem of estimating the covariance matrix $R$ and the mean $a$ of a Gaussian probability density function 

$$
p_{\boldsymbol{R,a}}(\boldsymbol{y})=(2\pi)^{-n/2}\operatorname*{det}(\boldsymbol{R})^{-1/2}\exp(-(\boldsymbol{y}-\boldsymbol{a})^{T}\boldsymbol{R}^{-1}(\boldsymbol{y}-\boldsymbol{a})/2),
$$ 

based on $N$ independent samples $y_{1}$ , $y_{2},\ \cdot\cdot\cdot$ , $y_{N}\in\mathbf{R}^{n}$ 

(a) We first consider the estimation problem when there are no additional constraints on $R$ and . Let and $Y$ be the sample mean and covariance, defined as $a$ $\mu$ 

$$
\mu=\frac{1}{N}\sum_{k=1}^{N}y_{k},\qquad Y=\frac{1}{N}\sum_{k=1}^{N}(y_{k}-\mu)\bigl(y_{k}-\mu\bigr)^{T}.
$$ 

Show that the log-likelihood function 

$$
l(R,a)=-(N n/2)\log(2\pi)-(N/2)\log\operatorname*{det}R-(1/2)\sum_{k=1}^{N}(y_{k}-a)^{T}R^{-1}(y_{k}-a)
$$ 

can be expressed as 

$$
l(R,a)=\frac{N}{2}\left(-n\log(2\pi)-\log\operatorname*{det}R-\mathrm{\bf~tr}(R^{-1}Y)-(a-\mu)^{T}R^{-1}(a-\mu)\right).
$$ 

Use this expression to show that if $Y\succ0$ , the ML estimates of $R$ and $a$ are unique, and given by 

$$
a_{\mathrm{{ml}}}=\mu,\qquad R_{\mathrm{{ml}}}=Y.
$$ 

(b) The log-likelihood function includes a convex term $(-\log\operatorname*{det}R)$ , so it is not obvi- ously concave. Show that l is concave, jointly in R and a , in the region defined by 

$$
R\preceq2Y.
$$ 

This means we can use convex optimization to compute simultaneous ML estimates of $R$ and $a$ , subjec to convex constraints, as long as the constraints include $R\preceq2Y$ , i.e. , the estimate R must not exceed twice the unconstrained ML estimate. 

7.5 Markov chain estimation. Consider a Markov chain with $n$ states, and transition proba- bility matrix $P\in\mathbf{R}^{n\times n}$ defined as 

$$
P_{i j}=\mathbf{prob}(y(t+1)=i\mid y(t)=j).
$$ 

The tran b m s $P_{i j}\,\geq\,0$ and $\sum_{i\ldots i=1}^{n}P_{i j}=1$ $j\,=\,1,\cdot\cdot\,\cdot\,,n$ . We consider the problem of estimating the transition probabilities, given an observed sample sequence $y(1)=k_{1}$ , y $y(2)=k_{2}$ , . . . , $y(N)=k_{n}$ . 

(a) Show that if there are no other prior constraints on $P_{i j}$ , then the ML estimates are the empirical transition frequencies: $\hat{P}_{i j}$ is the ratio of the number of times the state transitioned from $j$ into $i$ , divided by the number of times it was $j$ , in the observed sample. (b) Suppose that an equilibrium distribution $p$ of the Markov chain is known, i.e. , a vector $q\in\mathbf{R}_{+}^{n}$ satisfying $\mathbf{1}^{T}q=1$ and $P q=q$ . Show that the problem of computing the ML estimate of $P$ , given the observed sequence and knowledge of , can be $q$ expressed as a convex optimization problem. 

7.6 Estimation of mean and variance. Consider a random variable $x\in\mathbf{R}$ with density $p$ , which is normalized, i.e. , has zero mean and unit variance. Consider a random variable $y=$ $(x+b)/a$ obtained by an affine transformation of , where $a>0$ . The random variable $x$ $y$ has mean $b/a$ and variance $1/a^{2}$ . As $a$ and $b$ vary over $\mathbf{R}_{+}$ and $\mathbf{R}$ , respectively, we generate a family of densities obtained from $p$ by scaling and shifting, uniquely parametrized by mean and variance. 

Show that if is log-concave, then finding the ML estimate of and $b$ , given samples $p$ $a$ $y_{1},\dotsc,y_{n}$ of $y$ , is a convex problem. 

As an example, work out an analytical solution for the ML estimates of $a$ and $b$ , assuming $p$ is a normalized Laplacian density, $p(x)=e^{-2|x|}$ . 

7.7 ML estimation of Poisson distributions. Suppose $x_{i}$ , $i=1,\dots,n$ , are independent random variables with Poisson distributions 

$$
\mathbf{prob}(x_{i}=k)=\frac{e^{-\mu_{i}}\mu_{i}^{k}}{k!},
$$ 

with unknown means $\mu_{i}$ . The variables $x_{i}$ represent the number of times that one of $n$ possible independent events occurs during a certain period. In emission tomography, for example, they might represent the number of photons emitted by $n$ sources. 

We consider an experiment designed to determine the means $\mu_{i}$ . The experiment involves detectors. If event $i$ occurs, it is detected by detector $j$ with probability . We assume $_{m}$ $p_{j i}$ the probabilities $p_{j i}$ re given (wi $p_{j i}\geq0$ , $\begin{array}{r}{\sum_{j=1}^{m}p_{j i}\leq1)}\end{array}$ 1). The total number of events recorded by detector j is denoted y $y_{j}$ , 

$$
y_{j}=\sum_{i=1}^{n}y_{j i},\quad j=1,\dotsc,m.
$$ 

Formulate the ML estimation problem of estimating the means $\mu_{i}$ , based on observed values of $y_{j}$ , $j=1,\dots,m$ , as a convex optimization problem. Hint. The variables $y_{j\,i}$ have Poisson distributions with means $p_{j i}\mu_{i}$ , i.e. , 

$$
\mathbf{prob}(y_{j i}=k)=\frac{e^{-p_{j i}\mu_{i}}(p_{j i}\mu_{i})^{k}}{k!}.
$$ 

The sum of $n$ independent Poisson variables with means $\lambda_{1}$ , , $\lambda_{n}$ has a Poisson distri- $\cdot\cdot\cdot$ bution with mean $\lambda_{1}+\cdot\cdot\cdot+\lambda_{n}$ . 

7.8 Estimation using sign measurements. We consider the measurement setup 

$$
y_{i}=\mathbf{sign}(a_{i}^{T}x+b_{i}+v_{i}),\quad i=1,.\,.\,.\,,m,
$$ 

where $x\in\mathbf{R}^{n}$ the vector t timated, and $y_{i}\in\{-1,1\}$ are the measurements. The n vectors a $a_{i}\in\mathbf{R}^{n}$ ∈ and scalars b $b_{i}\in\mathbf{R}$ ∈ are kn $v_{i}$ ID noises with a log-concave probability density. (You can assume that $a_{i}^{T}x+b_{i}+v_{i}=0$ = 0 does not occur.) Show that maximum likelihood estimation of $x$ is a convex optimization problem. 

7.9 Estimation with unknown sensor nonlinearity. We consider the measurement setup 

$$
y_{i}=f\big(a_{i}^{T}x+b_{i}+v_{i}\big),\quad i=1,\dots,m,
$$ 

$x\,\in\,\mathbf{R}^{\,n}$ is the ctor to be estimated, $y_{i}\,\in\,\mathbf{R}$ are the measurements, $a_{i}\,\in\,\mathbf{R}^{n}$ , $b_{i}\in\mathbf{R}$ known, and v $v_{i}$ are IID noises with log-concave probability density. The function $f:\mathbf{R}\rightarrow\mathbf{R}$ → , which represents a measur nlinearity, is not known. However, it is known that $f^{\prime}(t)\in[l,u]$ for all t , where 0 < l < u are given. 

Explain how to use convex optimization to find a maximum likelihood estimate of $x$ , as well as the function $f$ . (This is an infinite-dimensional ML estimation problem, but you can be informal in your approach and explanation.) 

7.10 Nonparametric distributions on $\mathbf{R}^{k}$ . We consider a random variable $x\in\mathbf{R}^{k}$ with values in a finite set $\{\alpha_{1},.\,.\,.\,,\alpha_{n}\}$ , and with distribution 

$$
p_{i}=\mathbf{prob}(x=\alpha_{i}),\quad i=1,.\,.\,.\,,n.
$$ 

Show that a lower bound on the covariance of $X$ , 

$$
{\cal S}\preceq{\bf E}(X-{\bf E}\,X)(X-{\bf E}\,X)^{T},
$$ 

is a convex constraint in $p$ . 

# Optimal detector design 

7.11 Randomized detectors. Show that every randomized detector can be expressed as a convex combination of a set of deterministic detectors: If 

$$
T={\left[\begin{array}{l l l l}{t_{1}}&{t_{2}}&{\cdot\cdot\cdot}&{t_{n}}\end{array}\right]}\in\mathbf{R}^{m\times n}
$$ 

satisfies $t_{k}\succeq0$ and $\mathbf{1}^{T}t_{k}=1$ , then $T$ can be expressed as 

$$
T=\theta_{1}T_{1}+\cdot\cdot\cdot+\theta_{N}T_{N},
$$ 

where $T_{i}$ is a zero-one matrix with exactly one element equal to one per column, and $\theta_{i}\geq0$ , $\textstyle\sum_{i=1}^{N}\theta_{i}=1$ = 1. What is the maximum number of deterministic detectors $N$ we may need? 

We can interpret this convex decomposition as follows. The randomized detector can be realized as a bank of $N$ deterministic detectors. When we observe $X=k$ , the estimator chooses a random index from the set $\{1,\cdot\cdot\cdot,N\}$ , with probability $\mathbf{prob}(j=i)=\theta_{i}$ , and then uses deterministic detector $T_{j}$ . 

7.12 Optimal action. In detector design, we are given a mat $P\,\in\,\mathbf{R}^{n\times m}$ (whose columns are probability distributions), and gn a matrix T $T\in\mathbf{R}^{m\times n}$ ∈ (whose columns are probability distributions), so that D $D=T P$ has large diagonal elements (and small oﬀ- diagonal elements). In this problem we study the dual problem: Given $P$ , find a matrix $S\in\mathbf{R}^{m\times n}$ (whose columns are probability distributions), so that $\Dot{D}=P S\in\mathbf{R}^{n\times n}$ ∈ has large diagonal elements (and small oﬀ-diagonal elements). To make the problem specific, we take the objective to be maximizing the minimum element of D on the diagonal. 

We can interpret this problem as follows. There are $n$ outcomes , which depend (stochas- tically) on which of $m$ inputs or actions we take: $P_{i j}$ is the probability that outcome $i$ occurs, given action $j$ . Our goal is find a (randomized) strategy that, to the extent pos- sible, causes any specified outcome to occur. The strategy is given by the matrix $S$ : $S_{j\,i}$ is the probability that we take action $j$ , when we want outcome $i$ to occur. The matrix $\tilde{D}$ gives the action error probability matrix: $\tilde{D}_{i j}$ is the probability that outcome $i$ occurs, when we want outcome $j$ to occur. In particular, $\tilde{D}_{i i}$ is the probability that outcome $i$ occurs, when we want it to occur. 

Show that this problem has a simple analytical solution. Show that (unlike the corre- sponding detector problem) there is always an optimal solution that is deterministic. Hint. Show that the problem is separable in the columns of $S$ . 

# Chebyshev and Chernoﬀbounds 

7.13 Chebyshev-type inequalities on a finite set. Assume $X$ is a random variable taking values in the set $\{\alpha_{1},\alpha_{2},.\,.\,.\,,\alpha_{m}\}$ , and let $S$ be a subset of $\{\alpha_{1},.\,.\,.\,,\alpha_{m}\}$ The distribution of $X$ is unknown, but we are given the expected values of n functions f : 

$$
{\bf E}\,f_{i}(X)=b_{i},\quad i=1,.\,.\,.\,,n.
$$ 

Show that the optimal value of the LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{x_{0}+\sum_{i=1}^{n}b_{i}x_{i}}\\ {{\mathrm{subject~to}}\quad}&{x_{0}+\sum_{i=1}^{n}f_{i}(\alpha)x_{i}\geq1,\quad\alpha\in S}\\ &{x_{0}+\sum_{i=1}^{n}f_{i}(\alpha)x_{i}\geq0,\quad\alpha\not\in S,}\end{array}}
$$ 

with variables $x_{0},\ \cdot\cdot\cdot$ , $x_{n}$ , is an upper bound on $\mathbf{prob}(X\in S)$ , valid for all distributions that satisfy ( 7.32 ). Show that there always exists a distribution that achieves the upper bound. 

# Chapter 8 

# Geometric problems 

# 8.1 Projection on a set 

The distance of a point $x_{0}\,\in\,\mathbf{R}^{n}$ to a closed set $C\subseteq\mathbf{R}^{n}$ , in the norm $||\cdot||$ , is defined as 

$$
\mathbf{dist}(x_{0},C)=\operatorname*{inf}\{||x_{0}-x||\mid x\in C\}.
$$ 

The infimum here is always achieved. We refer to any point $z\in C$ w ch is closest to $x_{0}$ , i.e. , satisfies $\|z-x_{0}\|=\mathbf{dist}(x_{0},C)$ , as a rojection of $x_{0}$ on C . I general there can be more than one projection of $x_{0}$ on C , i.e. , several points in C closest to $x_{0}$ . 

In some special cases we can establish that the projection of a point on a set is unique. For example, if $C$ is closed and convex, and the norm is strictly convex ( e.g. , the Euclidean norm), then for any there is always exactly one $z\in C$ which $x_{0}$ is closest to $x_{0}$ . As an interesting converse, we have the following result: If for every $x_{0}$ there is a unique Euclidean projection of $x_{0}$ on $C$ , then $C$ is closed and convex (see exercise 8.2 ). 

We use the notation $P_{C}:\mathbf{R}^{n}\rightarrow\mathbf{R}^{n}$ to denote any function for which $P_{C}(x_{0})$ is a projection of $x_{0}$ on C , i.e. , for all $x_{0}$ , 

$$
P_{C}(x_{0})\in C,\qquad\|x_{0}-P_{C}(x_{0})\|=\mathbf{dist}(x_{0},C).
$$ 

In other words, we have 

$$
P_{C}(x_{0})=\mathrm{argmin}\{\|x-x_{0}\|\mid x\in C\}.
$$ 

We refer to $P_{C}$ as projection on $C$ . 

Example 8.1 Projection on the unit square in $\scriptstyle\mathbf{R}^{2}$ . Consider the (boundary of the) unit square in $\mathbf{R}^{2}$ , i.e. , $C=\{x\in\mathbf{R}^{2}\mid\|x\|_{\infty}=1\}$ . We take $x_{0}=0$ . In the $\ell_{1}$ -norm, the four points $(1,0)$ , $(0,-1)$ , $(-1,0)$ , and $(0,1)$ are closest to $x_{0}=0$ , with distance 1, so we have ${\bf d i s t}(x_{0},C)=1$ in the $\ell_{1}$ -norm. The same statement holds for the $\ell_{2}$ -norm. 

In the $\ell_{\infty}$ -norm, all points in $C$ lie at a distance 1 from $x_{0}$ , and ${\bf d i s t}(x_{0},C)=1$ . 

Example 8.2 Projection onto rank- $k$ matrices. Consider the set of $m\times n$ matrices with rank less than or equal to $k$ , 

$$
C=\{X\in\mathbf{R}^{m\times n}\mid\mathbf{rank}\,X\leq k\},
$$ 

ith $k\ \leq\ \operatorname*{min}\{m,n\}$ , and let $X_{0}\,\,\in\,\,\mathbf{R}^{m\times n}$ . We can find a projection of $X_{0}$ on C , in the (spectral or maximum singular value) norm $||\cdot||_{2}$ , via the singular value decomposition. Let 

$$
X_{0}=\sum_{i=1}^{r}\sigma_{i}u_{i}v_{i}^{T}
$$ 

be the singular value decomposition of $X_{0}$ , where $\boldsymbol{r}\,=\,\mathbf{rank}\,X_{0}$ . Then the matrix $\begin{array}{r}{Y=\sum_{i=1}^{\operatorname*{min}\{k,r\}}\sigma_{i}u_{i}v_{i}^{T}}\end{array}$ is a projection of $X_{0}$ on $C$ . 

# 8.1.1 Projecting a point on a convex set 

If $C$ is convex, then we can compute the projection $P_{C}(x_{0})$ and the distance $\mathbf{dist}(x_{0},C)$ by solving a convex optimization problem. We represent the set $C$ by a set of linear equalities and convex inequalities 

$$
A x=b,\qquad f_{i}(x)\leq0,\quad i=1,\ldots,m,
$$ 

and find the projection of $x_{0}$ on $C$ by solving the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\|x-x_{0}\|}\\ {{\mathrm{subject~to}}\quad f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b,}\end{array}}
$$ 

with variable $x$ . This problem is feasible if and only if $C$ is nonempty; when it is feasible, its optimal value is $\mathbf{dist}(x_{0},C)$ , and any optimal point is a projection of $x_{0}$ on $C$ . 

# Euclidean projection on a polyhedron 

The projection of $x_{0}$ on a polyhedron described by linear inequalities $A x\preceq b$ can be computed by solving the QP 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|x-x_{0}\|_{2}^{2}}\\ {\mathrm{subject~to}}&{A x\preceq b.}\end{array}
$$ 

Some special cases have simple analytical solutions. 

• The Euclidean projection of $x_{0}$ on a hyperplane $C=\{x\mid a^{T}x=b\}$ is given by 

$$
\begin{array}{r}{P_{C}({\boldsymbol x}_{0})={\boldsymbol x}_{0}+(b-a^{T}{\boldsymbol x}_{0})a/\|a\|_{2}^{2}.}\end{array}
$$ 

• The Euclidean projection of $x_{0}$ on a halfspace $C=\{x\mid a^{T}x\leq b\}$ is given by 

$$
P_{C}({\boldsymbol x}_{0})=\left\{\begin{array}{l l}{{\boldsymbol x}_{0}+({b}-{a}^{T}{\boldsymbol x}_{0}){a}/\|{a}\|_{2}^{2}}&{{a}^{T}{\boldsymbol x}_{0}>b}\\ {{\boldsymbol x}_{0}}&{{a}^{T}{\boldsymbol x}_{0}\le b.}\end{array}\right.
$$ 

• The Euclidean projection of $x_{0}$ on a rectangle $C=\{x\mid l\preceq x\preceq u\}$ (where $l\prec u$ ) is given by 

$$
P_{C}(x_{0})_{k}={\left\{\begin{array}{l l}{l_{k}}&{x_{0k}\leq l_{k}}\\ {x_{0k}}&{l_{k}\leq x_{0k}\leq u_{k}}\\ {u_{k}}&{x_{0k}\geq u_{k}.}\end{array}\right.}
$$ 

# Euclidean projection on a proper cone 

Let $x=P_{K}(x_{0})$ denote the Euclidean projection of a point $x_{0}$ on a proper cone $K$ . The KKT conditions of 

$$
{\begin{array}{l l}{{\mathrm{minimize}}}&{\|x-x_{0}\|_{2}^{2}}\\ {{\mathrm{subject~to}}}&{x\succeq_{K}0}\end{array}}
$$ 

are given by 

$$
x\succeq_{K}0,\qquad x-x_{0}=z,\qquad z\succeq_{K^{*}}0,\qquad z^{T}x=0.
$$ 

Introducing the notation $x_{+}=x_{\rightmoon}$ and $x_{-}=z$ , we can express these conditions as 

$$
x_{0}=x_{+}-x_{-},\qquad x_{+}\succeq_{K}0,\qquad x_{-}\succeq_{K^{*}}0,\qquad x_{+}^{T}x_{-}=0.
$$ 

In other words, by projecting $x_{0}$ on the cone $K$ , we decompose it into the diﬀerence of two orthogonal elements: one nonnegative with respect to $K$ (and which is the projection of $x_{0}$ on $K$ ), and the other nonnegative with respect to $K^{*}$ . 

Some specific examples: 

• For $K=\mathbf{R}_{+}^{n}$ , we have $P_{K}(x_{0})_{k}\,=\,\operatorname*{max}\{x_{0k},0\}$ . The Euclidean projection of a vector onto the nonnegative orthant is found by replacing each negative component with $0$ . 

• For $K=\mathbf{S}_{+}^{n}$ , and the Euclidean (or Frobenius) norm $\|\cdot\|_{F}$ , we have $P_{K}(X_{0})=$ $\begin{array}{r}{\sum_{i=1}^{n}\operatorname*{max}\{0,\lambda_{i}\}v_{i}v_{i}^{T}}\end{array}$ , where $\begin{array}{r}{X_{0}=\sum_{i=1}^{n}\lambda_{i}v_{i}v_{i}^{T}}\end{array}$ is the eigenvalue decomposi- tion of $X_{0}$ . To project a symmetric matrix onto the positive semidefinite cone, we form its eigenvalue expansion and drop terms associated with negative eigenvalues. This matrix is also the projection onto the positive semidefinite cone in the $\ell_{2^{-}}$ , or spectral norm. 

# 8.1.2 Separating a point and a convex set 

Suppose $C$ is a closed convex set described by the equalities and inequalities ( 8.1 ). If $x_{0}\,\in\,C$ en $\mathbf{dist}(x_{0},C)\,=\,0$ and the optimal point for the problem ( 8.2 ) is $x_{0}$ . If $x_{0}\notin C$ ̸∈ then ${\bf d i s t}(x_{0},C)>0$ 0, and the optimal value of the problem ( 8.2 ) is positive. In this case we will see that any dual optimal point provides a separating hyperplane between the point $x_{0}$ and the set $C$ . 

The link between projecting a point on a convex set and finding a hyperplane that separates them (when the point is not in the set) should not be surprising. Indeed, our proof of the separating hyperplane theorem, given in § 2.5.1 , relies on 

![](images/960a5563f442aaed47e29e642f46b87292af0169b2e468bc0c51a2d6fa7e1494.jpg) 
Figure 8.1 A point and its Euclidean projection $P_{C}(x_{0})$ on a convex set $C$ . $x_{\mathrm{0}}$ The hyperplane midway between the two, with normal vector $P_{C}(x_{0})-x_{0}$ , strictly separates the point and the set. This property does not hold for general norms; see exercise 8.4 . 

finding the Euclidean distance between the sets. If $P_{C}(x_{0})$ denotes the Euclidean projection of on $C$ , where $x_{0}\notin C$ , then the hyperplane $x_{0}$ 

$$
(P_{C}(x_{0})-x_{0})^{T}(x-(1/2)(x_{0}+P_{C}(x_{0})))=0
$$ 

(strictly) separates $x_{0}$ from $C$ , as illustrated in figure 8.1 . In other norms, however, the clearest link between the projection problem and the separating hyperplane problem is via Lagrange duality. 

We first express ( 8.2 ) as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\\\\\\|y\|}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b}\\ &{x_{0}-x=y}\end{array}}
$$ 

with variables $x$ and $y$ . The Lagrangian of this problem is 

$$
L(x,y,\lambda,\mu,\nu)=\|y\|+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\nu^{T}(A x-b)+\mu^{T}(x_{0}-x-y)
$$ 

and the dual function is 

$$
g(\lambda,\mu,\nu)=\left\{\begin{array}{l l}{\operatorname*{inf}_{x}\left(\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\nu^{T}(A x-b)+\mu^{T}(x_{0}-x)\right)}&{\|\mu\|_{*}\leq1}\\ {-\infty}&{\mathrm{otherwise}}\end{array}\right.
$$ 

so we obtain the dual problem 

$$
\begin{array}{r l}{\mathrm{maximize}}&{~\mu^{T}x_{0}+\operatorname*{inf}_{x}\left(\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\nu^{T}(A x-b)-\mu^{T}x\right)}\\ {\mathrm{subject~to}}&{~\lambda\succeq0}\\ &{~\|\mu\|_{*}\leq1,}\end{array}
$$ 

with variables $\lambda$ , , . We can interpret the dual problem as follows. Suppose $\lambda$ , $\mu$ $\nu$ $\mu$ , $\nu$ are dual feasible with a positive dual objective value, i.e. , $\lambda\succeq0$ , $\|\mu\|_{*}\leq1$ , and 

$$
\mu^{T}\boldsymbol{x}_{0}-\mu^{T}\boldsymbol{x}+\sum_{i=1}^{m}\lambda_{i}f_{i}(\boldsymbol{x})+\nu^{T}(A\boldsymbol{x}-\boldsymbol{b})>0
$$ 

for all $x$ . This implies that $\mu^{T}x_{0}\,>\,\mu^{T}x$ for $x\,\in\,C$ , and therefore $\mu$ defines a strictly separating hyperplane. In particular, suppose ( 8.2 ) is strictly feasible, so strong duality holds. If $x_{0}\notin C$ , the optimal value is positive, and any dual optimal solution defines a strictly separating hyperplane. 

Note that this construction of a separating hyperplane, via duality, works for any norm. In contrast, the simple construction described above only works for the Euclidean norm. 

# Separating a point from a polyhedron 

The dual problem of 

is 

$$
{\begin{array}{r l}{\operatorname{minimize}\ }&{\|y\|}\\ {{\mathrm{subject~to}}}&{A x\preceq b}\\ &{x_{0}-x=y}\end{array}}
$$ 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{\ \mu^{T}x_{0}-b^{T}\lambda}\\ {{\mathrm{subject~to}}}&{A^{T}\lambda=\mu}\\ &{\|\mu\|_{*}\leq1}\\ &{\lambda\succeq0}\end{array}}
$$ 

which can be further simplified as 

$$
\begin{array}{r l}{\mathrm{maximize}}&{(A x_{0}-b)^{T}\lambda}\\ {\mathrm{subject~to}}&{\|A^{T}\lambda\|_{*}\leq1}\\ &{\lambda\succeq0.}\end{array}
$$ 

It is easily verified that if the dual objective is positive, then $A^{T}\lambda$ is the normal vector to a separating hyperplane: If $A x\preceq b$ , then 

$$
(A^{T}\lambda)^{T}{\boldsymbol{x}}=\lambda^{T}(A{\boldsymbol{x}})\leq\lambda^{T}{\boldsymbol{b}}<\lambda^{T}A x_{0},
$$ 

so $\mu=A^{T}\lambda$ defines a separating hyperplane. 

# 8.1.3 Projection and separation via indicator and support functions 

The ideas described above in § 8.1.1 and § 8.1.2 can be expressed in a compact fo in terms of the indicator function $I_{C}$ and the support function $S_{C}$ of the set C , defined as 

$$
S_{C}(x)=\operatorname*{sup}_{y\in C}x^{T}y,\qquad I_{C}(x)={\left\{\begin{array}{l l}{0}&{x\in C}\\ {+\infty}&{x\not\in C.}\end{array}\right.}
$$ 

The problem of projecting $x_{0}$ on a closed convex set $C$ can be expressed compactly as 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\|x-x_{0}\|}\\ &{{\mathrm{subject~to}}\quad I_{C}(x)\leq0,}\end{array}}
$$ 

or, equivalently, as 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\|y\|}\\ {{\mathrm{subject~to}}}&{I_{C}(x)\leq0}\\ &{x_{0}-x=y}\end{array}}
$$ 

where the variables are $x$ and $y$ . The dual function of this problem is 

$$
\begin{array}{c c l}{g(z,\lambda)}&{=}&{\underset{x,y}{\operatorname*{inf}}\left(\|y\|+\lambda I_{C}(x)+z^{T}(x_{0}-x-y)\right)}\\ &{=}&{\left\{\begin{array}{l l}{z^{T}x_{0}+\operatorname*{inf}_{x}\left(-z^{T}x+I_{C}(x)\right)}&{\|z\|_{*}\leq1,\quad\lambda\geq0}\\ {-\infty}&{\mathrm{otherwise}}\end{array}\right.}\\ &{=}&{\left\{\begin{array}{l l}{z^{T}x_{0}-S_{C}(z)}&{\|z\|_{*}\leq1,\quad\lambda\geq0}\\ {-\infty}&{\mathrm{otherwise}}\end{array}\right.}\end{array}
$$ 

so we obtain the dual problem 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad z^{T}x_{0}-S_{C}(z)}\\ &{\mathrm{subject~to}\quad\lVert z\rVert_{*}\leq1.}\end{array}
$$ 

If $z$ is dual optimal with a positive objective value, then $z^{T}x_{0}>z^{T}x$ for all $x\in C$ , i.e. , $z$ defines a separating hyperplane. 

# 8.2 Distance between sets 

The distance between two sets $C$ and $D$ , in a norm $||\cdot||$ , is defined as 

$$
\mathbf{dist}(C,D)=\operatorname*{inf}\{\|x-y\|\mid x\in C,\ y\in D\}.
$$ 

The two sets $C$ and $D$ do not intersect if $\mathbf{dist}(C,D)\ >\ 0$ . They intersect if $\mathbf{dist}(C,D)=0$ and the infimum in the definition is attained (which is the case, for example, if the sets are closed and one of the sets is bounded). 

The distance between sets can be expressed in terms of the distance between a point and a set, 

$$
\mathbf{dist}(C,D)=\mathbf{dist}(0,D-C),
$$ 

so the results of the previous section can be applied. In this section, however, we derive results specifically for problems involving distance between sets. This allows us to exploit the structure of the set $C-D$ , and makes the interpretation easier. 

# 8.2.1 Computing the distance between convex sets 

Suppose $C$ and $D$ are described by two sets of convex inequalities 

$$
C=\{x\mid f_{i}(x)\leq0,\;i=1,\ldots,m\},\qquad D=\{x\mid g_{i}(x)\leq0,\;i=1,\ldots,p\}.
$$ 

![](images/6297b7cc1606b9f1934de8622b95bda7daa0eaaa38bcf692d326e1397c6ff976.jpg) 
Figure 8.2 Euclidean distance between polyhedra $C$ and $D$ . The dashed line connects the two points in $C$ and $D$ , respectively, that are closest to each other in Euclidean norm. These points can be found by solving a QP. 

(We can include linear equalities, but exclude them here for simplicity.) We can find $\mathbf{dist}(C,D)$ by solving the convex optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\|x-y\|}\\ {{\mathrm{subject~to}}\quad f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{g_{i}(y)\leq0,\quad i=1,\ldots,p.}\end{array}}
$$ 

# Euclidean distance between polyhedra 

Let $C$ and $D$ be two polyhedra described by the se of li ar inequalities $A_{1}x\preceq b_{1}$ and $A_{2}x\preceq b_{2}$ , respectively. The distance between C and D is the distance between the closest pair of points, one in $C$ and the other in $D$ , as illustrated in figure 8.2 . The distance between them is the optimal value of the problem 

$$
\begin{array}{l r l}{\mathrm{minimize}}&{\|x-y\|_{2}}\\ {\mathrm{subject~to}}&{A_{1}x\preceq b_{1}}\\ &{A_{2}y\preceq b_{2}.}\end{array}
$$ 

We can square the objective to obtain an equivalent QP. 

# 8.2.2 Separating convex sets 

The dual of the problem ( 8.3 ) of finding the distance between two convex sets has an interesting geometric interpretation in terms of separating hyperplanes between the sets. We first express the problem in the following equivalent form: 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\|w\|}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{g_{i}(y)\leq0,\quad i=1,.\,.\,,p}\\ &{x-y=w.}\end{array}}
$$ 

The dual function is 

$$
g(\lambda,z,\mu)=\operatorname*{inf}_{x,y,w}\left(||w||+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\mu_{i}g_{i}(y)+z^{T}(x-y-w)\right)
$$ 

$$
\begin{array}{r l}{=}&{{}\left\{\begin{array}{l l}{\operatorname*{inf}_{x}\left(\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+z^{T}x\right)+\operatorname*{inf}_{y}\left(\sum_{i=1}^{p}\mu_{i}g_{i}(y)-z^{T}y\right)}&{\|z\|_{*}\leq1}\\ {-\infty}&{\mathrm{otherwise}}\end{array}\right.}\end{array}
$$ 

which results in the dual problem 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\operatorname*{inf}_{x}\left(\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+z^{T}x\right)+\operatorname*{inf}_{y}\left(\sum_{i=1}^{p}\mu_{i}g_{i}(y)-z^{T}y\right)}\\ &{\mathrm{subject~to}\quad\|z\|_{*}\leq1}\\ &{\quad\quad\lambda\succeq0,\quad\mu\succeq0.}\end{array}
$$ 

We can interpret this geometrically as follows. If $\lambda$ , are dual feasible with a $\mu$ positive objective value, then 

$$
\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+z^{T}x+\sum_{i=1}^{p}\mu_{i}g_{i}(y)-z^{T}y>0
$$ 

for all and . In particular, for $x\in C$ and $y\in D$ , we have $z^{T}x-z^{T}y>0$ , so we $x$ $y$ see that $\mathcal{Z}$ defines a hyperplane that strictly separates $C$ and D . 

Therefore, if strong duality holds between the two problems ( 8.5 ) and ( 8.6 ) (which is the case when ( 8.5 ) is strictly feasible), we can make the following con- clusion. If the distance between the two sets is positive, then they can be strictly separated by a hyperplane. 

# Separating polyhedra 

Applying these duality results to sets defined by linear inequalities $A_{1}x\preceq b_{1}$ and $A_{2}x\preceq b_{2}$ , we find the dual problem 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b_{1}^{T}\lambda-b_{2}^{T}\mu}\\ &{\mathrm{subject~to}\quad A_{1}^{T}\lambda+z=0}\\ &{\quad\quad\quad A_{2}^{T}\mu-z=0}\\ &{\quad\quad\quad\|z\|_{*}\leq1}\\ &{\quad\quad\quad\lambda\succeq0,\quad\mu\succeq0.}\end{array}
$$ 

If $\lambda$ , $\mu$ , and $z$ are dual feasible, then for all $x\in C$ , $y\in D$ , 

$$
\boldsymbol{z}^{T}\boldsymbol{x}=-\boldsymbol{\lambda}^{T}\boldsymbol{A}_{1}\boldsymbol{x}\ge-\boldsymbol{\lambda}^{T}\boldsymbol{b}_{1},\qquad\boldsymbol{z}^{T}\boldsymbol{y}=\boldsymbol{\mu}^{T}\boldsymbol{A}_{2}\boldsymbol{x}\le\boldsymbol{\mu}^{T}\boldsymbol{b}_{2},
$$ 

and, if the dual objective value is positive, 

$$
z^{T}x-z^{T}y\geq-\lambda^{T}b_{1}-\mu^{T}b_{2}>0,
$$ 

i.e. , $z$ defines a separating hyperplane. 

# 8.2.3 Distance and separation via indicator and support functions 

The ideas described above in $\S8.2.1$ and $\S8.2.2$ can be expressed in a compact form using indicator and support functions. The problem of finding the distance between two convex sets can be posed as the convex problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\|x-y\|}\\ {{\mathrm{subject~to}}\quad}&{I_{C}(x)\leq0}\\ &{I_{D}(y)\leq0,}\end{array}}
$$ 

which is equivalent to 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\|w\|}\\ {{\mathrm{subject~to}}\quad}&{I_{C}(x)\leq0}\\ &{I_{D}(y)\leq0}\\ &{x-y=w.}\end{array}}
$$ 

The dual of this problem is 

$$
\begin{array}{l c l}{\mathrm{maximize}}&{-S_{C}(-z)-S_{D}(z)}\\ {\mathrm{subject~to}}&{\|z\|_{*}\leq1.}\end{array}
$$ 

If $z$ is dual feasible with a positive objective value, then $S_{D}(z)<-S_{C}(-z)$ , i.e. , 

$$
\operatorname*{sup}_{x\in D}z^{T}x<\operatorname*{inf}_{x\in C}z^{T}x.
$$ 

In other words, $\mathcal{Z}$ defines a hyperplane that strictly separates $C$ and $D$ . 

# 8.3 Euclidean distance and angle problems 

Suppose $a_{1},\dotsc,a_{n}$ is a set of vectors in $\mathbf{R}^{n}$ , which we assume (for now) have known Euclidean lengths 

$$
l_{1}=\|a_{1}\|_{2},\quad\cdot\cdot\cdot,\quad l_{n}=\|a_{n}\|_{2}.
$$ 

We will refer to the set of vectors as a configuration , or, when they are indepen- dent, a basis . In this section we consider optimization problems involving various geometric properties of the configuration, such as the Euclidean distances between pairs of the vectors, the angles between pairs of the vectors, and various geometric measures of the conditioning of the basis. 

# 8.3.1 Gram matrix and realizability 

The lengths, distances, and angles can be expressed in terms of the Gram matrix associated with the vectors $a_{1},\dotsc,a_{n}$ , given by 

$$
G=A^{T}A,\qquad A=\left[\begin{array}{l l l}{a_{1}}&{\cdot\cdot\cdot}&{a_{n}}\end{array}\right],
$$ 

so that $G_{i j}=a_{i}^{T}a_{j}$ . The diagonal entries of $G$ are given by 

$$
G_{i i}=l_{i}^{2},\quad i=1,.\,.\,.\,,n,
$$ 

which (for now) we assume are known and fixed. The distance $d_{i j}$ between $a_{i}$ and $a_{j}$ is 

$$
\begin{array}{l c l}{{d_{i j}}}&{{=}}&{{\|a_{i}-a_{j}\|_{2}}}\\ {{\ }}&{{=}}&{{(l_{i}^{2}+l_{j}^{2}-2a_{i}^{T}a_{j})^{1/2}}}\\ {{\ }}&{{=}}&{{(l_{i}^{2}+l_{j}^{2}-2G_{i j})^{1/2}.}}\end{array}
$$ 

Conversely, we can express $G_{i j}$ in terms of $d_{i j}$ as 

$$
G_{i j}=\frac{l_{i}^{2}+l_{j}^{2}-d_{i j}^{2}}{2},
$$ 

which we note, for future reference, is an affine function of $d_{i j}^{2}$ The correlation coefficient $\rho_{i j}$ between (nonzero) $a_{i}$ and $a_{j}$ is given by 

$$
\rho_{i j}=\frac{a_{i}^{T}a_{j}}{\vert\vert a_{i}\vert\vert_{2}\vert\vert a_{j}\vert\vert_{2}}=\frac{G_{i j}}{l_{i}l_{j}},
$$ 

so that $G_{i j}=l_{i}l_{j}\rho_{i j}$ is a linear function of $\rho_{i j}$ . The angle $\theta_{i j}$ between (nonzero) $a_{i}$ and $a_{j}$ is given by 

$$
\theta_{i j}=\cos^{-1}\rho_{i j}=\cos^{-1}(G_{i j}/(l_{i}l_{j})),
$$ 

where we take $\cos^{-1}\rho\in[0,\pi]$ . Thus, we have $G_{i j}=l_{i}l_{j}\cos\theta_{i j}$ 

The lengths, distances, and angles are invariant under orthogonal transforma- tions: If $Q\,\in\,\mathbf{R}^{n\times n}$ is orthogonal, then the set of vectors $Q a_{i},.\;.\;.\;,Q a_{n}$ has the same Gram matrix, and therefore the same lengths, distances, and angles. 

# Realizability 

The Gram matrix $G=A^{T}A$ is, of course, symmetric and positive semidefinite. The converse is a basic result of linear algebra: A matrix $G\in\mathbf{S}^{n}$ is the Gram matrix of a set of vectors $a_{1},\dotsc,a_{n}$ if and nly if $G\succeq0$ . When $G\succeq0$ , w struct a configuration with Gram matrix G by finding a matrix A with A $A^{T}A=G$ . One solution of this equation is the symmetric squareroot $A=G^{1/2}$ en $G\succ0$ , we can find a solution via the Cholesky factorization of G : If $L L^{T}=G$ , then we can take $A=L^{T}$ . Moreover, we can construct all configurations with the given Gram , by orthogonal transformation: If ${\tilde{A}}^{T}{\tilde{A}}=G$ matrix $G$ , given any one solution $A$ is any solution, then ${\dot{A}}=Q A$ for some orthogonal matrix $Q$ . 

Thus, a set of lengths, distances, and angles (or correlation coefficients) is real- izable , i.e. , those of some configuration, if and only if the associated Gram matrix $G$ is positive semidefinite, and has diagonal elements $l_{1}^{2},.\cdot\cdot\cdot,l_{n}^{2}$ . 

We can use this fact to express several geometric problems as convex optimiza- tion problems, with $G\,\in\,\mathbf{S}^{n}$ as the optimization variable. Realizability imposes the constraint $G\succeq0$ and $G_{i i}=l_{i}^{2}$ , $i=1,\dots,n$ ; we list below several other convex constraints and objectives. 

# Angle and distance constraints 

We can fix an angle to have a certain value, $\theta_{i j}\ =\ \alpha$ , via the linear equality constraint $G_{i j}\ =\ l_{i}l_{j}\cos\alpha$ . More generally, we can impose a lower and upper bound on an angle, $\alpha\leq\theta_{i j}\leq\beta$ , by the constraint 

$$
l_{i}l_{j}\cos\alpha\geq G_{i j}\geq l_{i}l_{j}\cos\beta,
$$ 

which is a pair of linear inequalities on $G$ . (Here we use the fact that $\cos^{-1}$ is monotone decreasing.) We can maximize or minimize a particular angle $\theta_{i j}$ , by minimizing or maximizing $G_{i j}$ (again using monotonicity of $\cos^{-1}$ ). 

In a similar way we can impose constraints on the distances. To require that $d_{i j}$ lies in an interval, we use 

$$
\begin{array}{r l}{d_{\operatorname*{min}}\leq d_{i j}\leq d_{\operatorname*{max}}}&{\Longleftrightarrow\quad d_{\operatorname*{min}}^{2}\leq d_{i j}^{2}\leq d_{\operatorname*{max}}^{2}}\\ &{\Longleftrightarrow\quad d_{\operatorname*{min}}^{2}\leq l_{i}^{2}+l_{j}^{2}-2G_{i j}\leq d_{\operatorname*{max}}^{2},}\end{array}
$$ 

which is a pair of linear inequalities on $G$ . We can minimize or maximize a distance, by minimizing or maximizing its square, which is an affine function of $G$ . 

As a simple example, suppose we are given ranges ( i.e. , an interval of possible values) for some of the angles and some of the distances. We can then find the minimum and maximum possible value of some other angle, or some other distance, over all configurations, by solving two SDPs. We can reconstruct the two extreme configurations by factoring the resulting optimal Gram matrices. 

# Singular value and condition number constraints 

valu s of $A$ , $\sigma_{1}\,\geq\,\cdot\cdot\cdot\,\geq\,\sigma_{n}\,$ , are the squarer ots of e eigenvalues $\lambda_{1}\geq\cdot\cdot\cdot\geq\lambda_{n}$ ≥· · · ≥ of G . Therefore $\sigma_{1}^{2}$ is a convex function of G , and $\sigma_{n}^{2}$ is a concave function of G . Thus we can impose an upper bound on the maximum singular value of $A$ , or minimize it; we can impose a lower bound on the minimum singular value, or maximize it. The condition number of $A$ , $\sigma_{1}/\sigma_{n}$ , is a quasiconvex function of $G$ , so we can impose a maximum allowable value, or minimize it over all configurations that satisfy the other geometric constraints, by quasiconvex optimization. 

Roughly speaking, the constraints we can impose as convex constraints on $G$ are those that require $a_{1},\dotsc,a_{n}$ to be a well conditioned basis. 

# Dual basis 

When $G\succ0$ , $a_{1},.\ldots,a_{n}$ form a basis for $\mathbf{R}^{n}$ . The associated dual basis is $b_{1},\dots,b_{n}$ , where 

$$
b_{i}^{T}a_{j}={\left\{\begin{array}{l l}{1}&{i=j}\\ {0}&{i\not=j.}\end{array}\right.}
$$ 

The dual basis vectors $b_{1},\dots,b_{n}$ are simply the rows of the matrix $A^{-1}$ . As a result, the Gram matrix associated with the dual basis is $G^{-1}$ . 

We can express several geometric conditions on the dual basis as convex con- straints on $G$ . The (squared) lengths of the dual basis vectors, 

$$
\|b_{i}\|_{2}^{2}=e_{i}^{T}G^{-1}e_{i},
$$ 

are convex functions of $G$ , and so can be minimized. The trace of $G^{-1}$ , another convex function of $G$ , gives the sum of the squares of the lengths of the dual basis vectors (and is another measure of a well conditioned basis). 

# Ellipsoid and simplex volume 

The volume of the ellipsoid $\{A u\mid\|u\|_{2}\leq1\}$ , which gives another measure of how well conditioned the basis is, is given by 

$$
\gamma(\operatorname*{det}(\boldsymbol{A}^{T}\boldsymbol{A}))^{1/2}=\gamma(\operatorname*{det}G)^{1/2},
$$ 

where $\gamma$ is the volume of the unit ball in $\mathbf{R}^{n}$ . The log volume is therefore $\log{\gamma}+$ $(1/2)\log\operatorname*{det}G$ , which is a concave function of $G$ . We can therefore maximize the volume of the image ellipsoid, over a convex set of configurations, by maximizing $\log\operatorname*{det}G$ . 

The same holds for any set in $\mathbf{R}^{n}$ . The volume of the image under $A$ is its volume, multiplied by the factor $(\operatorname*{det}G)^{1/2}$ . For example, consider the image under $A$ of the unit simplex $\mathbf{conv}\{0,e_{1},.\,.\,.\,,e_{n}\}$ , i.e. , the simplex $\mathbf{conv}\{0,a_{1},.\,.\,.\,,a_{n}\}$ . The volume of this simplex is given by $\overline{{\gamma}}(\operatorname*{det}G)^{1/2}$ , where $\overline{\gamma}$ is the volume of the unit simplex in $\mathbf{R}^{n}$ . We can maximize the volume of this simplex by maximizing $\log\operatorname*{det}G$ . 

# 8.3.2 Problems involving angles only 

Suppose we only care about the angles (or correlation coefficients) between the vectors, and do not specify the lengths or distances between them. In this case it is intuitively clear that we can simply assume the vectors $a_{i}$ have length ${{l}_{i}}=1$ . This is easily verified: The Gram matrix has the form $\boldsymbol{G}=\mathbf{diag}(l)\boldsymbol{C}\,\mathbf{diag}(l)$ , where $l$ is the vector of lengths, and $C$ is the correlation matrix, i.e. , $C_{i j}\,=\,\cos\theta_{i j}$ . It follows that if $G\succeq0$ for any set of positive lengths, then $G\succeq0$ for all sets of positive lengths, and in particular, this occurs if and only if $C\succeq0$ (which is the same as assuming that all lengths are one). Thus, a set of angles $\theta_{i j}\,\,\in\,\,[0,\pi]$ , $i,j=1,\cdot\cdot\cdot,n$ is realizable if and only if $C\succeq0$ , which is a linear matrix inequality in the correlation coefficients. 

As an example, suppose we are given lower and upper bounds on some of the angles (which is equivalent to imposing lower and upper bounds on the correlation coefficients). We can then find the minimum and maximum possible value of some other angle, over all configurations, by solving two SDPs. 

Example 8.3 Bounding correlation coefficients. We consider an example in $\scriptstyle\mathbf{R}^{4}$ , where we are given 

$$
\begin{array}{l l}{{0.6\leq\rho_{12}\leq0.9,}}&{{0.8\leq\rho_{13}\leq0.9,}}\\ {{0.5\leq\rho_{24}\leq0.7,}}&{{-0.8\leq\rho_{34}\leq-0.4.}}\end{array}
$$ 

To find the minimum and maximum possible values of $\rho_{14}$ , we solve the two SDPs 

$$
\begin{array}{r l}&{\mathrm{minimize/maximize}\quad\rho_{14}}\\ &{\mathrm{subject~to}\quad\quad\quad\quad\quad\quad(8.7)}\\ &{\left[\begin{array}{c c c c}{1}&{\rho_{12}}&{\rho_{13}}&{\rho_{14}}\\ {\rho_{12}}&{1}&{\rho_{23}}&{\rho_{24}}\\ {\rho_{13}}&{\rho_{23}}&{1}&{\rho_{34}}\\ {\rho_{14}}&{\rho_{24}}&{\rho_{34}}&{1}\end{array}\right]\succeq0,}\end{array}
$$ 

with variables $\rho_{12},\rho_{13},\rho_{14},\rho_{23},\rho_{24},\rho_{34}$ . The minimum and maximum values (to two significant digits) are $-0.39$ and 0 . 23, with corresponding correlation matrices 

![](images/85f7002ef941c5094b76d9c20724c6fd59ed52c8b0003768da4ab3ed9e7d3573.jpg) 

# 8.3.3 Euclidean distance problems 

In a Euclidean distance problem , we are concerned only with the distances between the vectors, $d_{i j}$ , and do not care about the lengths of the vectors, or about the angles between them. These distances, of course, are invariant not only under orthogonal transformations, but also translation: The configuration $\tilde{a}_{1}=a_{1}+b,.\;.\;.\;,\tilde{a}_{n}=a_{n}+b$ has the same distances as the original configuration, for any $b\in\mathbf{R}^{n}$ . In particular, for the choice 

$$
b=-(1/n)\sum_{i=1}^{n}a_{i}=-(1/n)A{\bf1},
$$ 

we see that $\ddot{a}_{i}$ have the same distances as the original configuration, and also satisfy $\textstyle\sum_{i=1}^{n}{\tilde{a}}_{i}\;=\;0$ = 0. It follows that in a Euclidean distance problem, we can assume, without any loss of generality, that the average of the vectors $a_{1},\dotsc,a_{n}$ is zero, i.e. , $A\mathbf{1}=0$ . 

We can solve Euclidean distance problems by considering the lengths (which cannot occur in the objective or constraints of a Euclidean distance problem) as free variables in the optimization problem. Here we rely on the fact that there is a confi on with $d_{i j}\geq0$ d only if there are lengths $\iota_{1},.\,.\,.\,,\,\iota_{n}$ for which G $G\succeq0$ 0, where $G_{i j}=(l_{i}^{2}+l_{j}^{2}-d_{i j}^{2})/2$ 2. 

We define $z\,\in\,\mathbf{R}^{n}$ as z $z_{i}~=~l_{i}^{2}$ , and $D\,\in\,{\bf S}^{n}$ by $D_{i j}\;=\;d_{i j}^{2}$ (with, of course, $D_{i i}=0$ ). The condition that $G\succeq0$ for some choice of lengths can be expressed as 

$$
G=(z\mathbf{1}^{T}+\mathbf{1}z^{T}-D)/2\succeq0\mathrm{~for~some~}z\succeq0,
$$ 

which is an LMI in $D$ and $\mathcal{Z}$ . A matrix $D\;\in\;{\bf S}^{n}$ , with nonnegative elements, zero diagonal, and which satisfies ( 8.8 ), is called a Euclidean distance matrix . A matrix is a Euclidean distance matrix if and only if its entries are the squares of the Euclidean distances between the vectors of some configuration. (Given a Euclidean distance matrix $D$ and the associated length squared vector $z$ , we can reconstruct one, or all, configurations with the given pairwise distances using the method described above.) 

The condition ( 8.8 ) turns out to be equivalent to the simpler condition that $D$ is negative semidefinite on ${\bf1}^{\perp}$ , i.e. , 

$$
\begin{array}{r l}{\iff}&{u^{T}D u\leq0\mathrm{~for~all~}u\mathrm{~with~}\mathbf{1}^{T}u=0}\\ {\iff}&{(I-(1/n)\mathbf{1}\mathbf{1}^{T})D(I-(1/n)\mathbf{1}\mathbf{1}^{T})\preceq0.}\end{array}
$$ 

This simple matrix inequality, along with $D_{i j}\,\geq\,0$ , $D_{i i}=0$ , is the classical char- acterization of a Euclidean distance matrix. To see the equivalence, recall that we can assume $A{\bf1}\,=\,0$ , which implies that $\mathbf{1}^{T}G\mathbf{1}\,=\,\mathbf{1}^{T}A^{T}A\mathbf{1}\,=\,0$ . It follows that $G\succeq0$ if and only if $G$ is positive semidefinite on ${\bf1}^{\perp}$ , i.e. , 

$$
\begin{array}{r c l}{0}&{\preceq}&{(I-(1/n)\mathbf{11}^{T})G(I-(1/n)\mathbf{11}^{T})}\\ &{=}&{(1/2)(I-(1/n)\mathbf{11}^{T})(z\mathbf{1}^{T}+\mathbf{1}z^{T}-D)(I-(1/n)\mathbf{11}^{T})}\\ &{=}&{-(1/2)(I-(1/n)\mathbf{11}^{T})D(I-(1/n)\mathbf{11}^{T}),}\end{array}
$$ 

which is the simplified condition. 

In summary, a matrix $D\,\in\,{\bf S}^{n}$ is a Euclide distance matrix, i.e. , gives the squared distances between a set of $n$ vectors in R , if and only if 

$$
\begin{array}{c}{{D_{i i}=0,\quad i=1,.\,.\,,n,\qquad D_{i j}\geq0,\quad i,j=1,.\,.\,,n,}}\\ {{{}}}\\ {{(I-(1/n){\bf11}^{T})D(I-(1/n){\bf11}^{T})\preceq0,}}\end{array}
$$ 

which is a set of linear equalities, linear inequalities, and a matrix inequality in $D$ . Therefore we can express any Euclidean distance problem that is convex in the squared distances as a convex problem with variable $D\in\mathbf{S}^{n}$ . 

# 8.4 Extremal volume ellipsoids 

Suppose $C\subseteq\mathbf{R}^{n}$ is bounded and has nonempty interior. In this section consider the problems of finding the maximum volume ellipsoid that lies inside C , and the minimum volume ellipsoid that covers $C$ . Both problems can be formulated as convex programming problems, but are tractable only in special cases. 

# 8.4.1 The L¨ owner-John ellipsoid 

The minimum volume ellipsoid that contains a set $C$ is called the L¨ owner-John ellipsoid of the set $C$ , and is denoted $\mathcal{E}_{\mathrm{ij}}$ . To characterize $\mathcal{E}_{\mathrm{ij}}$ , it will be convenient to parametrize a general ellipsoid as 

$$
\mathcal{E}=\{v\ \vert\ \Vert A v+b\Vert_{2}\leq1\}\,,
$$ 

i.e. , the inverse image of the Euclidean unit ball under an affine mapping. We can assume without generality that $A\in\mathbf{S}_{++}^{n}$ , in which case the volume of $\mathcal{E}$ is proportional to det $\operatorname*{det}A^{-1}$ . The problem of computing the minimum volume ellipsoid containing $C$ can be expressed as 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\log\operatorname*{det}A^{-1}}\\ {\mathrm{subject~to}}&{{}\operatorname*{sup}_{v\in C}\|A v+b\|_{2}\leq1,}\end{array}
$$ 

the variables are $A\,\in\,\mathbf{S}^{n}$ and $b\in\mathbf{R}^{n}$ , and there is an im cit c nstraint $A\succ0$ ≻ 0. The objective and constraint functions are both convex in A and b , so the problem ( 8.10 ) is convex. Evaluating the constraint function in ( 8.10 ), however, involves solving a convex maximization problem, and is tractable only in certain special cases. 

# Minimum volume ellipsoid covering a finite set 

We consider the problem of finding the minimum volume ellipsoid that contains the finite set $C\;=\;\{x_{1},.\,.\,.\,,x_{m}\}\;\subseteq\;\mathbf{R}^{n}$ . An ellipsoid covers $C$ if and only if it covers its convex hull, so finding the minimum volume ellipsoid that covers $C$ is the same as finding the minimum volume ellipsoid containing the polyhedron $\mathbf{conv}\{x_{1},\dots,x_{m}\}$ . Applying ( 8.10 ), we can write this problem as 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\log\operatorname*{det}A^{-1}}\\ {\mathrm{subject~to}}&{\|A x_{i}+b\|_{2}\leq1,\quad i=1,\ldots,m}\end{array}
$$ 

where the variables are $A\in\mathbf{S}^{\mathcal{N}}$ and $b\in\mathbf{R}^{n}$ , and we have the implicit constraint $A\succ$ $0$ . The no m constraints $\lvert|A x_{i}\!+\!b\rvert|_{2}\leq1$ , $i=1,\ldots,m$ , are convex in e variables A and $b$ . They can be replaced wi the quared versions, ∥ $||A x_{i}+b||_{2}^{2}\leq1$ ∥ ≤ 1, which are convex quadratic inequalities in A and b . 

# Minimum volume ellipsoid covering union of ellipsoids 

Minimum volume covering ellipsoids can also be computed efficiently for certain sets $C$ that are defined by quadratic inequalities. In particular, it is possible to compute the L¨ owner-John ellipsoid for a union or sum of ellipsoids. 

As an example, consider the problem of finding the minimum volume ellip- soid $\mathcal{E}_{\mathrm{ij}}$ , that contains the ellipsoids $\mathcal{E}_{1},\ldots,\mathcal{E}_{m}$ (and therefore, the convex hull of their union). The ellipsoids $\mathcal{E}_{1}$ , . . . , $\mathcal{E}_{m}$ will be described by (convex) quadratic inequalities: 

$$
{\mathcal{E}}_{i}=\{x\mid x^{T}A_{i}x+2b_{i}^{T}x+c_{i}\leq0\},\quad i=1,\ldots,m,
$$ 

where $A_{i}\in\mathbf{S}_{++}^{n}$ . We parametrize the ellipsoid $\mathcal{E}_{\mathrm{ij}}$ as 

$$
\begin{array}{l l l}{\mathcal{E}_{\mathrm{li}}}&{=}&{\{x\mid\|A x+b\|_{2}\leq1\}}\\ &{=}&{\{x\mid x^{T}A^{T}A x+2(A^{T}b)^{T}x+b^{T}b-1\leq0\}}\end{array}
$$ 

where $A\,\in\,\mathbf{S}^{n}$ and $b\in\mathbf{R}^{n}$ . Now we use a result from $\S$ B.2 , that $\mathcal{E}_{i}\subseteq\mathcal{E}_{\mathrm{ij}}$ if and only if there exists a τ $\tau\geq0$ 0 such that 

$$
\left[\begin{array}{c c}{A^{2}-\tau A_{i}}&{A b-\tau b_{i}}\\ {(A b-\tau b_{i})^{T}}&{b^{T}b-1-\tau c_{i}}\end{array}\right]\preceq0.
$$ 

The volume of $\mathcal{E}_{\mathrm{ij}}$ is proportional to $\operatorname*{det}A^{-1}$ , so we can find the minimum volume ellipsoid that contains $\mathcal{E}_{1},\ldots,\mathcal{E}_{m}$ by solving 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\log\operatorname*{det}A^{-1}}\\ &{\mathrm{subject~to}\quad\tau_{1}\geq0,\ldots,\tau_{m}\geq0}\\ &{\left[\begin{array}{l l}{\phantom{-}A^{2}-\tau_{i}A_{i}}&{A b-\tau_{i}b_{i}}\\ {(A b-\tau_{i}b_{i})^{T}}&{b^{T}b-1-\tau_{i}c_{i}}\end{array}\right]\preceq0,\quad i=1,\ldots,m,}\end{array}
$$ 

or, replacing the variable $b$ by $\dot{b}=A b$ , 

$$
\begin{array}{r l}{\mathrm{minimize}\:\:}&{\log\operatorname*{det}A^{-1}}\\ {\mathrm{subject~to}\:\:}&{\tau_{1}\geq0,\ldots,\tau_{m}\geq0}\\ &{\left[\begin{array}{c c c}{A^{2}-\tau_{i}A_{i}}&{\tilde{b}-\tau_{i}b_{i}}&{0}\\ {(\tilde{b}-\tau_{i}b_{i})^{T}}&{-1-\tau_{i}c_{i}}&{\tilde{b}^{T}}\\ {0}&{\tilde{b}}&{-A^{2}}\end{array}\right]\preceq0,\quad i=1,\ldots,m,}\end{array}
$$ 

which is convex in the variables $A^{2}\in\mathbf{S}^{n}$ , $\ddot{b}$ , $\tau_{1}$ , . . . , $\tau_{m}$ . 

![](images/b3b9d566a5e12de188eca18fc02082ee25899583f782fd436dfa75dc3c89f830.jpg) 
Figure 8.3 The outer ellipse is the boundary of the L¨ owner-John ellipsoid, i.e. , the minimum volume ellipsoid that encloses the points $x_{1},\dots,x_{6}$ (shown as dots), and therefore the polyhedron $\mathcal{P}=\mathbf{conv}\{x_{1},\ldots,x_{6}\}$ . The smaller ellipse is the boundary of the L¨ owner-John ellipsoid, shrunk by a factor of $n=2$ about its center. This ellipsoid is guaranteed to lie inside $\mathcal{P}$ . 

# Efficiency of L¨ owner-John ellipsoidal approximation 

Let $\mathcal{E}_{\mathrm{ij}}$ be the L¨ owner-John ellipsoid of the convex set $C\subseteq\mathbf{R}^{n}$ , which is bounded and has nonempty interior, and let $x_{0}$ be its center. If we shrink the L¨ owner-John ellipsoid by a factor of $n$ , about its center, we obtain an ellipsoid that lies inside the set $C$ : 

$$
x_{0}+(1/n)(\mathcal{E}_{\mathrm{ij}}-x_{0})\subseteq C\subseteq\mathcal{E}_{\mathrm{ij}}.
$$ 

In other words, the L¨ owner-John ellipsoid approximates an arbitrary convex set, within a factor that depends only on the dimension $n$ . Figure 8.3 shows a simple example. 

The factor $1/n$ cannot be improved without additional assumptions on $C$ . Any simplex in $\mathbf{R}^{n}$ , for example, has the property that its L¨ owner-John ellipsoid must be shrunk by a factor $n$ to fit inside it (see exercise 8.13 ). 

We will prove this efficiency result for the special case $C=\mathbf{conv}\{x_{1},\ldots,x_{m}\}$ . We square the norm constraints in ( 8.11 ) and introduce variables $\dot{A}\,=\,A^{2}$ and $\dot{b}=A b$ , to obtain the problem 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\log\operatorname*{det}\tilde{A}^{-1}}\\ &{\mathrm{subject~to}\quad x_{i}{}^{T}\tilde{A}x_{i}-2\tilde{b}^{T}x_{i}+\tilde{b}^{T}\tilde{A}^{-1}\tilde{b}\leq1,\quad i=1,\dots,m.}\end{array}
$$ 

The KKT conditions for this problem are 

$$
\begin{array}{r l}&{\sum_{i=1}^{m}\lambda_{i}({x_{i}}{{x_{i}}^{T}}-\tilde{A}^{-1}\tilde{b}\tilde{b}^{T}\tilde{A}^{-1})=\tilde{A}^{-1},\qquad\sum_{i=1}^{m}\lambda_{i}({x_{i}}-\tilde{A}^{-1}\tilde{b})=0,}\\ &{\qquad\lambda_{i}\geq0,\qquad{x_{i}}^{T}\tilde{A}{x_{i}}-2\tilde{b}^{T}{x_{i}}+\tilde{b}^{T}\tilde{A}^{-1}\tilde{b}\leq1,\qquad i=1,\ldots,m,}\\ &{\qquad\lambda_{i}(1-{x_{i}}^{T}\tilde{A}{x_{i}}+2\tilde{b}^{T}{x_{i}}-\tilde{b}^{T}\tilde{A}^{-1}\tilde{b})=0,\quad i=1,\ldots,m.}\end{array}
$$ 

By a suitable affine change of coordinates, we can assume that ${\tilde{A}}=I$ and $\dot{b}=0$ i.e. , the minimum volume ellipsoid is the unit ball centered at the origin. The KKT conditions then simplify to 

$$
\sum_{i=1}^{m}\lambda_{i}{x_{i}}{x_{i}}^{T}=I,\qquad\sum_{i=1}^{m}\lambda_{i}x_{i}=0,\qquad\lambda_{i}(1-{x_{i}}^{T}x_{i})=0,\quad i=1,\ldots,m,
$$ 

plus the feasibility conditions $\|x_{i}\|_{2}\;\leq\;1$ and $\lambda_{i}~\geq~0$ . By taking the trace of both sides of the first equation, and using complementary slackness, we also have $\textstyle\sum_{i=1}^{m}\lambda_{i}=n$ . 

In the new coordinates the shrunk ellipsoid is a ball with radius $1/n$ , centered at the origin. We need to show that 

$$
\|x\|_{2}\leq1/n\implies x\in C=\mathbf{conv}\{x_{1},\ldots,x_{m}\}.
$$ 

Suppose $\|{\boldsymbol{x}}\|_{2}\leq1/n$ . From the KKT conditions, we see that 

$$
\boldsymbol{x}=\sum_{i=1}^{m}\lambda_{i}(\boldsymbol{x}^{T}\boldsymbol{x}_{i})\boldsymbol{x}_{i}=\sum_{i=1}^{m}\lambda_{i}(\boldsymbol{x}^{T}\boldsymbol{x}_{i}+1/n)\boldsymbol{x}_{i}=\sum_{i=1}^{m}\mu_{i}\boldsymbol{x}_{i},
$$ 

where $\mu_{i}=\lambda_{i}(x^{T}x_{i}+1/n)$ . From the Cauchy-Schwartz inequality, we note that 

$$
\mu_{i}=\lambda_{i}(\boldsymbol{x}^{T}\boldsymbol{x}_{i}+1/n)\geq\lambda_{i}(-\|\boldsymbol{x}\|_{2}\|\boldsymbol{x}_{i}\|_{2}+1/n)\geq\lambda_{i}(-1/n+1/n)=0.
$$ 

Furthermore 

$$
\sum_{i=1}^{m}\mu_{i}=\sum_{i=1}^{m}\lambda_{i}(x^{T}x_{i}+1/n)=\sum_{i=1}^{m}\lambda_{i}/n=1.
$$ 

This, along with ( 8.13 ), shows that $x$ is a convex combination of $x_{1},\allowbreak\cdot\cdot\cdot,x_{m}$ , hence $x\in C$ . 

# Efficiency of L¨ owner-John ellipsoidal approximation for symmetric sets 

If the set $C$ is symmetric about a point , then the factor $1/n$ can be tightened $x_{0}$ to $1/{\sqrt{n}}$ : 

$$
x_{0}+(1/\sqrt{n})(\mathcal{E}_{\mathrm{ij}}-x_{0})\subseteq C\subseteq\mathcal{E}_{\mathrm{ij}}.
$$ 

Again, the factor $1/{\sqrt{n}}$ is tight. The L¨ owner-John ellipsoid of the cube 

$$
C=\{x\in\mathbf{R}^{n}\mid\mathbf{\tau}-\mathbf{1}\preceq x\preceq\mathbf{1}\}
$$ 

is the ball with radius $\sqrt{n}$ . Scaling down by $1/{\sqrt{n}}$ yields a ball enclosed in $C$ , and touching the boundary at $x=\pm e_{i}$ . 

# Approximating a norm by a quadratic norm 

Let $||\cdot||$ be any norm on $\mathbf{R}^{n}$ , and let $C\,=\,\{x\,\mid\,\|x\|\,\leq\,1\}$ be its un ball. L ${\mathcal{E}}_{\mathrm{ij}}=\{x\ |\ x^{T}A x\leq1\}$ , with $A\in\mathbf{S}_{++}^{n}$ , be the L¨ owner-John ellipsoid of C . Since C is symmetric about the origin, the result above tells us that $(1/\sqrt{n})\mathcal{E}_{\mathrm{ij}}\subseteq C\subseteq\mathcal{E}_{\mathrm{ij}}$ . Let $||\cdot||_{\mathrm{ij}}$ denote the quadratic norm 

$$
\|z\|_{\mathrm{li}}=(z^{T}A z)^{1/2},
$$ 

whose unit ball is $\mathcal{E}_{\mathrm{ij}}$ . The inclusions $(1/\sqrt{n})\mathcal{E}_{\mathrm{ij}}\subseteq C\subseteq\mathcal{E}_{\mathrm{ij}}$ are equivalent to the inequalities 

$$
\|z\|_{\mathrm{ij}}\leq\|z\|\leq\sqrt{n}\|z\|_{\mathrm{ij}}
$$ 

for all $z\in\mathbf{R}^{n}$ . In other words, the quadratic norm $||\cdot||_{\mathrm{ij}}$ approximate he norm $||\cdot||$ within a factor of $\sqrt{n}$ . In particular, we see that any norm on R $\mathbf{R}^{n}$ can be approximated within a factor of $\sqrt{n}$ by a quadratic norm. 

# 8.4.2 Maximum volume inscribed ellipsoid 

We now consider the problem of finding the ellipsoid of maximum volume that lies inside a convex set $C$ , which we assume is bounded and has nonempty interior. To formulate this problem, we parametrize the ellipsoid as the image of the unit ball under an affine transformation, i.e. , as 

$$
\mathcal{E}=\left\{B u+d\ \lvert\lvert u\rvert\rvert_{2}\leq1\right\}.
$$ 

Again it can be assumed that $B\in\mathbf{S}_{++}^{n}$ , so the volume is proportional to $\operatorname*{det}B$ . We can find the maximum volume ellipsoid inside $C$ by solving the convex optimization problem 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}\log\operatorname*{det}B}\\ {\mathrm{subject~to}}&{{}\operatorname*{sup}_{\|u\|_{2}\leq1}I_{C}(B u+d)\leq0}\end{array}
$$ 

in the variables $B\in\mathbf{S}^{n}$ and $d\in\mathbf{R}^{n}$ , with implicit constraint $B\succ0$ . 

# Maximum volume ellipsoid in a polyhedron 

We consider the case where $C$ is a polyhedron described by a set of linear inequal- ities: 

$$
C=\{x\mid a_{i}^{T}x\leq b_{i},\ i=1,.\,.\,.\,,m\}.
$$ 

To apply ( 8.14 ) we first express the constraint in a more convenient form: 

$$
\begin{array}{r l}{\underset{\|u\|_{2}\leq1}{\operatorname*{sup}}I_{C}(B u+d)\leq0}&{\Longleftrightarrow\quad\underset{\|u\|_{2}\leq1}{\operatorname*{sup}}a_{i}^{T}(B u+d)\leq b_{i},\quad i=1,\ldots,m}\\ &{\Longleftrightarrow\quad\|B a_{i}\|_{2}+a_{i}^{T}d\leq b_{i},\quad i=1,\ldots,m.}\end{array}
$$ 

We can therefore formulate ( 8.14 ) as a convex optimization problem in the variables $B$ and $d$ : 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\log\operatorname*{det}B^{-1}}\\ {\mathrm{subject~to}}&{\|B a_{i}\|_{2}+a_{i}^{T}d\leq b_{i},\quad i=1,.\,.\,,m.}\end{array}
$$ 

# Maximum volume ellipsoid in an intersection of ellipsoids 

We can also find the maximum volume elli soid $\mathcal{E}$ that lies in the intersection of $m$ ellipsoids $\mathcal{E}_{1},\ldots,\mathcal{E}_{m}$ . We will describe E as ${\mathcal{E}}\,=\,\{B u+d\,\mid\,\|u\|_{2}\,\leq\,1\}$ with $B\in\mathbf{S}_{++}^{n}$ , and the other ellipsoids via convex quadratic inequalities, 

$$
{\mathcal{E}}_{i}=\{x\mid x^{T}A_{i}x+2b_{i}^{T}x+c_{i}\leq0\},\quad i=1,\ldots,m,
$$ 

where $A_{i}\in\mathbf{S}_{++}^{n}$ . We first work out the condition under which $\mathcal{E}\subseteq\mathcal{E}_{i}$ . This occurs if and only if 

$$
\begin{array}{r l}{\lefteqn{\operatorname*{sup}_{\|u\|_{2}\leq1}\big((d+B u)^{T}A_{i}(d+B u)+2b_{i}^{T}(d+B u)+c_{i}\big)}}\\ {=}&{d^{T}A_{i}d+2b_{i}^{T}d+c_{i}+\operatorname*{sup}_{\|u\|_{2}\leq1}\big(u^{T}B A_{i}B u+2(A_{i}d+b_{i})^{T}B u\big)}\\ {\leq}&{0.}\end{array}
$$ 

From § B.1 , 

$$
\operatorname*{sup}_{\|u\|_{2}\leq1}\big(u^{T}B A_{i}B u+2(A_{i}d+b_{i})^{T}B u\big)\leq-(d^{T}A_{i}d+2b_{i}^{T}d+c_{i})
$$ 

if and only if there exists a $\lambda_{i}\geq0$ such that 

$$
\left[\begin{array}{l l}{-\lambda_{i}-d^{T}A_{i}d-2b_{i}^{T}d-c_{i}}&{(A_{i}d+b_{i})^{T}B}\\ {B(A_{i}d+b_{i})}&{\lambda_{i}I-B A_{i}B}\end{array}\right]\succeq0.
$$ 

The maximum volume ellipsoid contained in $\mathcal{E}_{1},\dots,\mathcal{E}_{m}$ can therefore be found by solving the problem 

$$
\begin{array}{r}{\begin{array}{l l}{\mathrm{minimize}\quad\log\operatorname*{det}B^{-1}}\\ {\mathrm{subject~to}\quad\left[\begin{array}{l l}{-\lambda_{i}-d^{T}A_{i}d-2b_{i}^{T}d-c_{i}}&{(A_{i}d+b_{i})^{T}B}\\ {B(A_{i}d+b_{i})}&{\lambda_{i}I-B A_{i}B}\end{array}\right]\succeq0,\quad i=1,\ldots,m,}\end{array}}\end{array}
$$ 

with variables $B\in\mathbf{S}^{n}$ , $d\in\mathbf{R}^{n}$ , and $\lambda\in\mathbf{R}^{m}$ , or, equivalently, 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\log\operatorname*{det}B^{-1}}\\ &{\mathrm{subject~to}\quad\left[\begin{array}{c c c}{-\lambda_{i}-c_{i}+b_{i}^{T}A_{i}^{-1}b_{i}}&{0}&{(d+A_{i}^{-1}b_{i})^{T}}\\ {0}&{\lambda_{i}I}&{B}\\ {d+A_{i}^{-1}b_{i}}&{B}&{A_{i}^{-1}}\end{array}\right]\succeq0,\quad i=1,\dots,m.}\end{array}
$$ 

# Efficiency of ellipsoidal inner approximations 

Approximation efficiency results, similar to the ones for the L¨ owner-John ellipsoid, hold for the maximum volume inscribed ellipsoid. If $C\subseteq\mathbf{R}^{n}$ is convex, bounded, with nonempty interior, then the maximum volume inscribed ellipsoid, expanded by a factor of $n$ about its center, covers the set $C$ . The factor $n$ can be tightened to $\sqrt{n}$ if the set $C$ is symmetric about a point. An example is shown in figure 8.4 . 

# 8.4.3 Affine invariance of extremal volume ellipsoids 

The L¨ owner-John ellipsoid and the maximum volume inscribed ellipsoid are both affinely invariant. If $\mathcal{E}_{\mathrm{ij}}$ is the L¨ owner-John soid of $C$ , and ${\cal T}\,\in\,{\bf R}^{n\times n}$ is nonsingular, then the L¨ owner-John ellipsoid of T C is $T\mathcal{E}_{\mathrm{ij}}$ . A similar result holds for the maximum volume inscribed ellipsoid. 

To esta h this result, let $\mathcal{E}$ be any ellipsoid that covers $C$ . Then the psoid $T\mathcal{E}$ covers T C . The converse is also true: Every ellipsoid that covers T C has 

![](images/fcf2f592524a56e67341f4b4ba95e73913bd08986ee027761c74398b329ccd2e.jpg) 
Figure 8.4 The maximum volume ellipsoid (shown shaded) inscribed in a polyhedron $\mathcal{P}$ . The outer ellipse is the boundary of the inner ellipsoid, expanded by a factor $n\,=\,2$ about its center. The expanded ellipsoid is guaranteed to cover $\mathcal{P}$ . 

m $T\mathcal{E}$ , where $\mathcal{E}$ is an ellipsoid that covers $C$ . In other words, the ation $\dot{\mathcal{E}}=T\mathcal{E}$ E E gives a one-to ne correspondence between the ellipsoids covering T C and the ellipsoids covering C . Moreover, the volumes of the corresponding ellipsoids are all related by the ratio $\vert\operatorname*{det}T\vert$ , so in particular, if $\mathcal{E}$ has minimum volume among ellipsoids covering $C$ , then $T\mathcal{E}$ has minimum volume among ellipsoids covering $T C$ . 

# 8.5 Centering 

# 8.5.1 Chebyshev center 

Let $C\subseteq\mathbf{R}^{n}$ be bounded and have nonempty interior, and $x\in C$ . The depth of a point x $x\in C$ is defined as 

$$
\mathbf{depth}(x,C)=\mathbf{dist}(x,\mathbf{R}^{n}\setminus C),
$$ 

i.e. , the distance to the closest point in the exterior of $C$ . The depth gives the radius of the largest ball, centered at $x$ , that lies in $C$ . A Chebyshev center of the set $C$ is defined as any point of maximum depth in $C$ : 

$$
x_{\mathrm{cheb}}(C)=\operatorname{argmax}\mathbf{depth}(x,C)=\operatorname{argmax}\mathbf{dist}(x,\mathbf{R}^{n}\setminus C).
$$ 

A Chebyshev center is a point inside $C$ that is farthest from the exterior of $C$ ; it is also the center of the largest ball that lies inside $C$ . Figure 8.5 shows an example, in which $C$ is a polyhedron, and the norm is Euclidean. 

![](images/d54b0fe0031cfb191fa43365ecb1529eea540d582c3f1c49fd755eb6af0c504f.jpg) 
Figure 8.5 Chebyshev center of a polyhedron $C$ , in the Euclidean norm. The center $x_{\mathrm{cheb}}$ is the deepest point inside $C$ , in the sense that it is farthest from the exterior, or complement, of $C$ . The center $x_{\mathrm{cheb}}$ is also the center of the largest Euclidean ball (shown lightly shaded) that lies inside $C$ . 

# Chebyshev center of a convex set 

When the set $C$ is convex, the depth is a concave function for $x\in C$ , so computing the Chebyshev center is a convex optimization problem (see exercise 8.5 ). More specifically, suppose $C\subseteq\mathbf{R}^{n}$ is defined by a set of convex inequalities: 

$$
C=\{x\mid f_{1}(x)\leq0,.\,.\,.,f_{m}(x)\leq0\}.
$$ 

We can find a Chebyshev center by solving the problem 

$$
\begin{array}{l r c l}{{\mathrm{maximize}}}&{{R}}&{{}}\\ {{\mathrm{subject~to}}}&{{g_{i}(x,R)\leq0,\quad i=1,.\,.\,.\,,m,}}\end{array}
$$ 

where $g_{i}$ is defined as 

$$
g_{i}(x,R)=\operatorname*{sup}_{\|u\|\leq1}f_{i}(x+R u).
$$ 

Problem ( 8.16 ) is a convex optimization problem, since each function $g_{i}$ is the pointwise maximum of a family of convex functions of $x$ and $R$ , hence convex. However, evaluating $g_{i}$ involves solving a convex maximization problem (either numerically or analytically), which may be very hard. In practice, we can find the Chebyshev center only in cases where the functions $g_{i}$ are easy to evaluate. 

# Chebyshev center of a polyhedron 

Suppose $C$ is defined by a set of linear inequalities $a_{i}^{T}x\,\le\,b_{i},\ i\,=\,1,.\,.\,.\,,m$ ≤ . We have 

$$
g_{i}(x,R)=\operatorname*{sup}_{||u||\leq1}a_{i}^{T}(x+R u)-b_{i}=a_{i}^{T}x+R||a_{i}||_{*}-b_{i}
$$ 

if $R\geq0$ , so the Chebyshev center can be found by solving the LP 

$$
\begin{array}{l l}{\mathrm{maximize}}&{R}\\ {\mathrm{subject~to}}&{a_{i}^{T}x+R\|a_{i}\|_{*}\leq b_{i},\quad i=1,.\,.\,.\,,m}\\ &{R\geq0}\end{array}
$$ 

with variables $x$ and $R$ . 

Euclidean Chebyshev center of intersection of ellipsoids 

Let $C$ be an intersection of $m$ ellipsoids, defined by quadratic inequalities, 

$$
C=\{\boldsymbol{x}\mid\boldsymbol{x}^{T}A_{i}\boldsymbol{x}+2b_{i}^{T}\boldsymbol{x}+c_{i}\leq0,\;i=1,.\,.\,,m\},
$$ 

where $A_{i}\in\mathbf{S}_{++}^{n}$ . We have 

$$
\begin{array}{r l}{g_{i}(x,R)}&{=\displaystyle\ \operatorname*{sup}_{\|u\|_{2}\leq1}\left((x+R u)^{T}A_{i}(x+R u)+2b_{i}^{T}(x+R u)+c_{i}\right)}\\ &{=\displaystyle\ x^{T}A_{i}x+2b_{i}^{T}x+c_{i}+\operatorname*{sup}_{\|u\|_{2}\leq1}\left(R^{2}u^{T}A_{i}u+2R(A_{i}x+b_{i})^{T}u\right).}\end{array}
$$ 

From § B.1 , $g_{i}(x,R)\,\leq\,0$ if and only if there exists a $\lambda_{i}$ such that the matrix inequality 

$$
\left[\begin{array}{c c}{-x^{T}A_{i}x_{i}-2b_{i}^{T}x-c_{i}-\lambda_{i}}&{R(A_{i}x+b_{i})^{T}}\\ {R(A_{i}x+b_{i})}&{\lambda_{i}I-R^{2}A_{i}}\end{array}\right]\succeq0
$$ 

holds. Using this result, we can express the Chebyshev centering problem as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad R}\\ &{\mathrm{subject~to}\quad\left[\begin{array}{c c c}{-\lambda_{i}-c_{i}+b_{i}^{T}A_{i}^{-1}b_{i}}&{0}&{(x+A_{i}^{-1}b_{i})^{T}}\\ {0}&{\lambda_{i}I}&{R I}\\ {x+A_{i}^{-1}b_{i}}&{R I}&{A_{i}^{-1}}\end{array}\right]\succeq0,\quad i=1,\dots,m,}\end{array}
$$ 

which is an SDP with variables $R$ , $\lambda$ , and $x$ . Note that the Schur complement of $A_{i}^{-1}$ in the LMI constraint is equal to the lefthand side of ( 8.17 ). 

# 8.5.2 Maximum volume ellipsoid center 

The C byshev center $x_{\mathrm{cheb}}$ of a set $C\subseteq\mathbf{R}^{n}$ is the center of the largest ball that lies in C . As an extension of this idea, we define the maximum volume ellipsoid center of $C$ , denoted $x_{\mathrm{me}}$ , as the center of the maximum volume ellipsoid that lies in $C$ . Figure 8.6 shows an example, where $C$ is a polyhedron. 

The maximum volume ellipsoid center is readily computed when $C$ is defined by a set of linear inequalities, by solving the problem ( 8.15 ). (The optimal value of the variable $d\in\mathbf{R}^{n}$ is $x_{\mathrm{me}}$ .) Since the maximum volume ellipsoid inside $C$ is affine invariant, so is the maximum volume ellipsoid center. 

![](images/a4fa0f608ba1fc5ea0f5681d1500942dc0fd5cad6844b0364a77d2f37bd8c45c.jpg) 
Figure 8.6 The lightly shaded ellipsoid shows the maximum volume ellipsoid contained in the set $C$ , which is the same polyhedron as in figure 8.5 . Its center $x_{\mathrm{mve}}$ is the maximum volume ellipsoid center of $C$ . 

# 8.5.3 Analytic center of a set of inequalities 

The analytic center $x_{\mathrm{ac}}$ of a set of convex inequalities and linear equalities, 

$$
f_{i}(x)\leq0,\quad i=1,\ldots,m,\qquad F x=g
$$ 

is defined as an optimal point for the (convex) problem 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad-\sum_{i=1}^{m}\log(-f_{i}(x))}\\ &{\mathrm{subject~to}\quad F x=g,}\end{array}
$$ 

with variable $x\in\mathbf{R}^{n}$ and implicit constraints $f_{i}(x)<0$ , $i=1,\ldots,m$ . The objec- tive in ( 8.18 ) is called the logarithmic barrier associated with the set of inequalities. We assume here that the domain of the logarithmic barrier intersects the affine set defined by the equalities, i.e. , the strict inequality system 

$$
f_{i}(x)<0,\quad i=1,\ldots,m,\qquad F x=g
$$ 

is feasible. The logarithmic barrier is bounded below on the feasible set 

$$
C=\{x\mid f_{i}(x)<0,\ i=1,.\,.\,.\,,m,\ F x=g\},
$$ 

if $C$ is bounded. 

When $x$ is strictly feasible, i.e. , $F x=g$ and $f_{i}(x)<0$ for $i=1,\ldots,m$ , we can interpret $-f_{i}(x)$ as the margin or slack in the $i$ th inequality. The analytic center $x_{\mathrm{ac}}$ is the point that maximizes the product (or geometric mean) of these slacks or margins, subject to the equality constraints $\boldsymbol{F}\boldsymbol{x}=\boldsymbol{g}$ , and the implicit constraints $f_{i}(x)<0$ . 

The analytic center is not a function of the set $C$ described by the inequalities and equalities; two sets of inequalities and equalities can define the same set, but have diﬀerent analytic centers. Still, it is not uncommon to informally use the term ‘analytic center of a set $C^{\prime}$ to mean the analytic center of a particular set of equalities and inequalities that define it. 

The analytic center is, however, independent of affine changes of coordinates. It is also invariant under (positive) scalings of the inequality functions, and any re para me tri z ation of the equality constraints. In other words, if $\tilde{F}$ and g are such that $\tilde{F}x=\tilde{g}$ if and only if $F x=g$ , and $\alpha_{1},.\,.\,.\,,\alpha_{m}>0$ , then the analytic center of 

$$
\alpha_{i}f_{i}(x)\le0,\quad i=1,.\,.\,.\,,m,\qquad\tilde{F}x=\tilde{g},
$$ 

is the same as the analytic center of 

$$
f_{i}(x)\leq0,\quad i=1,\ldots,m,\qquad F x=g
$$ 

(see exercise 8.17 ). 

# Analytic center of a set of linear inequalities 

The analytic center of a set of linear inequalities 

$$
a_{i}^{T}x\leq b_{i},\quad i=1,.\,.\,.\,,m,
$$ 

is the solution of the unconstrained minimization problem 

$$
\begin{array}{r}{\begin{array}{r l}{\mathrm{minimize}}&{{}-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),}\end{array}}\end{array}
$$ 

with implicit constraint $b_{i}-a_{i}^{T}x>0$ i $i=1,\ldots,m$ . If the polyhedron defined by the linear inequalities is bounded, then the logarithmic barrier is bounded below and strictly convex, so the analytic center is unique. (See exercise 4.2 .) 

We can give a geometric interpretation of the analytic center of a set of linear inequalities. Since the analytic center is independent of positive scaling of the constraint functions, we can assume without loss of generality that $\Vert a_{i}\Vert_{2}=1$ . In s case, the slack $b_{i}-a_{i}^{T}x$ is the distance to the hyperplane ${\mathcal{H}}_{i}\,=\,\{x\,\mid\,a_{i}^{T}x\,=$ $b_{i}\}$ } . Therefore the analytic center $x_{\mathrm{ac}}$ is the point that maximizes the product of distances to the defining hyperplanes. 

# Inner and outer ellipsoids from analytic center of linear inequalities 

The analytic center of a set of linear inequalities implicitly defines an inscribed and a covering ellipsoid, defined by the Hessian of the logarithmic barrier function 

$$
-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),
$$ 

evaluated at the analytic center, i.e. 

$$
H=\sum_{i=1}^{m}d_{i}^{2}a_{i}a_{i}^{T},\qquad d_{i}={\frac{1}{b_{i}-a_{i}^{T}x_{\mathrm{ac}}}},\quad i=1,\ldots,m.
$$ 

We have $\mathcal{E}_{\mathrm{inner}}\subseteq\mathcal{P}\subseteq\mathcal{E}_{\mathrm{outer}}$ , where 

$$
\begin{array}{r c l}{\mathcal{P}}&{=}&{\{x\mid a_{i}^{T}x\leq b_{i},\;i=1,\ldots,m\},}\\ {\mathcal{E}_{\mathrm{inner}}}&{=}&{\{x\mid(x-x_{\mathrm{ac}})^{T}H(x-x_{\mathrm{ac}})\leq1\},}\\ {\mathcal{E}_{\mathrm{outer}}}&{=}&{\{x\mid x-x_{\mathrm{ac}}\}^{T}H(x-x_{\mathrm{ac}})\leq m(m-1)\}.}\end{array}
$$ 

![](images/6fc77eff6bd0979996024318f5a582b9b5a6d498d0ea8d3f8259547536018842.jpg) 
Figure 8.7 The dashed lines show five level curves of the logarithmic barrier function for the inequalities defining the polyhedron $C$ in figure 8.5 . The minimizer of the logarithmic barrier function, labeled $x_{\mathrm{ac}}$ , is the analytic center of the ine alities. The inner ellipsoid ${\mathcal{E}}_{\mathrm{inner}}=\{x\mid(x-x_{\mathrm{ac}})H(x-$ $x_{\mathrm{ac}})\leq1\}$ , where H is the Hessian of the logarithmic barrier function at $x_{\mathrm{ac}}$ , is shaded. 

This is a weaker result than the one for the maximum volume inscribed ellipsoid, which when scaled up by a factor of $n$ covers the polyhedron. The inner and outer ellipsoids defined by the Hessian of the logarithmic barrier, in contrast, are related by the scale factor $\left(m(m-1)\right)^{1/2}$ , which is alwa s at least $n$ . 

To show that $\mathcal{E}_{\mathrm{inner}}\subseteq\mathcal{P}$ , suppose $x\in\mathcal{E}_{\mathrm{inner}}$ , i.e. , 

$$
(x-x_{\mathrm{ac}})^{T}H(x-x_{\mathrm{ac}})=\sum_{i=1}^{m}(d_{i}a_{i}^{T}(x-x_{\mathrm{ac}}))^{2}\leq1.
$$ 

This implies that 

$$
a_{i}^{T}(x-x_{\mathrm{ac}})\leq1/d_{i}=b_{i}-a_{i}^{T}x_{\mathrm{ac}},\quad i=1,.\,.\,.\,,m,
$$ 

and therefore $a_{i}^{T}x\,\leq\,b_{i}$ ≤ for $i\,=\,1,.\,.\,.\,,m$ . (We have not used the fact that $x_{\mathrm{ac}}$ is the analytic center, so this result is valid if we replace $x_{\mathrm{ac}}$ with any strictly feasible point.) 

To establish that ${\mathcal{P}}\,\subseteq\,{\mathcal{E}}_{\mathrm{outer}}$ , we will need the fact that $x_{\mathrm{ac}}$ is the analytic center, and therefore the gradient of the logarithmic barrier vanishes: 

$$
\sum_{i=1}^{m}d_{i}a_{i}=0.
$$ 

Now assume $x\in\mathcal{P}$ . Then 

$$
\begin{array}{r l}{}&{(x-x_{\mathrm{ac}})^{T}H(x-x_{\mathrm{ac}})}\\ {=}&{\displaystyle\sum_{i=1}^{m}(d_{i}a_{i}^{T}(x-x_{\mathrm{ac}}))^{2}}\end{array}
$$ 

$$
\begin{array}{r l}{\displaystyle}&{=\ \displaystyle\sum_{i=1}^{m}d_{i}^{2}(1/d_{i}-a_{i}^{T}(x-x_{\mathrm{ac}}))^{2}-m}\\ {\displaystyle}&{=\ \displaystyle\sum_{i=1}^{m}d_{i}^{2}(b_{i}-a_{i}^{T}x)^{2}-m}\\ {\displaystyle}&{\leq\ \left(\displaystyle\sum_{i=1}^{m}d_{i}(b_{i}-a_{i}^{T}x)\right)^{2}-m}\\ {\displaystyle}&{=\ \left(\displaystyle\sum_{i=1}^{m}d_{i}(b_{i}-a_{i}^{T}x_{\mathrm{ac}})+\displaystyle\sum_{i=1}^{m}d_{i}a_{i}^{T}(x_{\mathrm{ac}}-x)\right)^{2}-m}\\ {\displaystyle}&{=\ m^{2}-m,}\end{array}
$$ 

which shows that $x\ \in\ \mathcal{E}_{\mathrm{outer}}$ . (The second equality follows from the fact that ${\textstyle\sum_{i=1}^{m}d_{i}a_{i}=0}$ = 0. The inequ rom $\begin{array}{r}{\sum_{i=1}^{m}y_{i}^{2}\leq\left(\sum_{i=1}^{m}y_{i}\right)^{2}}\end{array}$ ≤ P for $y\succeq0$ . The last equality follows from $\begin{array}{r}{\sum_{i=1}^{m}d_{i}a_{i}=0}\end{array}$ = 0, and the definition of $d_{i}$ .) 

# Analytic center of a linear matrix inequality 

The definition of analytic center can be extended to sets described by generalized inequalities with respect to a cone $K$ , if we define a logarithm on $K$ . For example, the analytic center of a linear matrix inequality 

$$
x_{1}A_{1}+x_{2}A_{2}+\cdot\cdot\cdot+x_{n}A_{n}\preceq B
$$ 

is defined as the solution of 

$$
{\mathrm{minimize}}\quad-\log\operatorname*{det}(B-x_{1}A_{1}-\cdot\cdot\cdot-x_{n}A_{n}).
$$ 

# 8.6 Classification 

In pattern recognition and classification problems we are given two sets of points in $\mathbf{R}^{n}$ , $\{x_{1},.\,.\,.\,,x_{N}\}$ and $\{y_{1},.\,.\,.\,,y_{M}\}$ , and wish to find a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ (within a given family of functions) that is positive on the first set and negative on the second, i.e. , 

$$
f(x_{i})>0,\quad i=1,\ldots,N,\qquad f(y_{i})<0,\quad i=1,\ldots,M.
$$ 

If these inequalities hold, we say that $f$ , or its 0-level set $\{x\mid f(x)=0\}$ , separates , classifies , or discriminates the two sets of points. We sometimes also consider weak separation , in which the weak versions of the inequalities hold. 

![](images/b363daa47ec8b4be9dffbd63091f143518e0ca1ea5bf70ee0435b78ac13b355c.jpg) 
Figure 8.8 The points $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ are shown as open circles, and the points $y_{1},\dotsc,y_{M}$ are shown as filled circles. These two sets are classified by an affine function $f$ , whose 0-level set (a line) separates them. 

# 8.6.1 Linear discrimination 

In linear discrimination, we seek an affine function $f(x)=a^{T}x-b$ that classifies the points, i.e. , 

$$
a^{T}x_{i}-b>0,\quad i=1,\ldots,N,\qquad a^{T}y_{i}-b<0,\quad i=1,\ldots,M.
$$ 

Geometrically, we seek a hyperplane that separates the two sets of points. Since the strict inequalities ( 8.20 ) are homogeneous in $a$ and $b$ , they are feasible if and only if the set of nonstrict linear inequalities 

$$
\boldsymbol{a}^{T}\boldsymbol{x}_{i}-\boldsymbol{b}\geq1,\quad i=1,\ldots,N,\qquad\boldsymbol{a}^{T}\boldsymbol{y}_{i}-\boldsymbol{b}\leq-1,\quad i=1,\ldots,M
$$ 

(in the variables $a$ , $b$ ) is feasible. Figure 8.8 shows a simple example of two sets of points and a linear discriminating function. 

# Linear discrimination alternative 

The strong alternative of the set of strict inequalities ( 8.20 ) is the existence of $\lambda$ , $\tilde{\lambda}$ such that 

$$
\lambda\succeq0,\qquad\tilde{\lambda}\succeq0,\qquad(\lambda,\tilde{\lambda})\neq0,\qquad\sum_{i=1}^{N}\lambda_{i}x_{i}=\sum_{i=1}^{M}\tilde{\lambda}_{i}y_{i},\qquad\mathbf{1}^{T}\lambda=\mathbf{1}^{T}\tilde{\lambda}
$$ 

(see § 5.8.3 ). Using the third and last conditions, we can express these alternative conditions as 

$$
\lambda\succeq0,\qquad\mathbf{1}^{T}\lambda=1,\qquad\tilde{\lambda}\succeq0,\qquad\mathbf{1}^{T}\tilde{\lambda}=1,\qquad\sum_{i=1}^{N}\lambda_{i}x_{i}=\sum_{i=1}^{M}\tilde{\lambda}_{i}y_{i}
$$ 

(by dividing by ${\bf1}^{T}\lambda$ , which is positive, and using the same symbols for the normal- and $\tilde{\lambda}$ ized $\lambda$ ). These conditions have a simple geometric interpretation: They state that there is a point in the convex hull of both $\{x_{1},.\,.\,.\,,x_{N}\}$ and $\{y_{1},.\,.\,.\,,y_{M}\}$ . In other words: the two sets of points can be linearly discriminated ( i.e. , discrimi- nated by an affine function) if and only if their convex hulls do not intersect. We have seen this result several times before. 

# Robust linear discrimination 

The existence of an affine classifying function $f(x)\,=\,a^{T}x\,-\,b$ is equivalent to a set of linear inequalities in the variables $a$ and b that define $f$ . If the two sets can be linearly discriminated, then there is a polyhedron of affine functions that discriminate them, and we can choose one that optimizes some measure of robust- ness. We might, for example, seek the function that gives the maximum possible ‘gap’ between the (positive) values at the points $x_{i}$ and the (negative) values at the points . To do this we have to normalize and $b$ , since otherwise we can scale $y_{i}$ $a$ $a$ and $b$ by a positive constant and make the gap in the values arbitrarily large. This leads to the problem 

$$
{\begin{array}{r l}&{{\mathrm{maximize}}\quad t}\\ &{{\mathrm{subject~to}}\quad a^{T}x_{i}-b\geq t,\quad i=1,\ldots,N}\\ &{\quad a^{T}y_{i}-b\leq-t,\quad i=1,\ldots,M}\\ &{\quad\|a\|_{2}\leq1,}\end{array}}
$$ 

with variables $a,\ b$ , and $t$ . The optimal value $t^{\star}$ of this convex problem (with linear objective, linear inequalities, and one quadratic inequality) is positive if and only if the two sets of points can be linearly discriminated. In this case the inequality $||a||_{2}\leq1$ is always tight at the optimum, i.e. , we have $\|a^{\star}\|_{2}=1$ . (See exercise 8.23 .) 

We can give a simple geometric interpretation of the robust linear discrimination problem ( 8.23 ). If $\Vert a\Vert_{2}=1$ (as is the case at any optimal point), $a^{T}x_{i}-b$ Euclidean distance from the point $x_{i}$ to the separating hyperplane H ${\mathcal{H}}=\{z\mid a^{T}z=$ { | $b\}$ . Similarly, $b\!-\!a^{T}y_{i}$ is the distance from the point $y_{i}$ to the hyperplane. Therefore the problem ( 8.23 ) finds the hyperplane that separates the two sets of points, and has maximal distance to the sets. In other words, it finds the thickest slab that separates the two sets. 

As suggested by the example shown in figure 8.9 , the optimal value $t^{\star}$ (which is half the slab thickness) turns out to be half the distance between the convex hulls of the two sets of points. This can be seen clearly from the dual of the robust linear discrimination problem ( 8.23 ). The Lagrangian (for the problem of minimizing $-t$ ) is 

$$
-t+\sum_{i=1}^{N}u_{i}(t+b-a^{T}x_{i})+\sum_{i=1}^{M}v_{i}(t-b+a^{T}y_{i})+\lambda(\|a\|_{2}-1).
$$ 

Minimizing over $b$ and $t$ yields the conditions $\mathbf{1}^{T}u=1/2$ , $\mathbf{1}^{T}v=1/2$ . When these hold, we have 

$$
\begin{array}{l l l}{g(u,v,\lambda)}&{=}&{\displaystyle\operatorname*{inf}_{a}\left(a^{T}(\sum_{i=1}^{M}{v_{i}y_{i}}-\sum_{i=1}^{N}{u_{i}x_{i}})+\lambda\|a\|_{2}-\lambda\right)}\end{array}
$$ 

![](images/91de7217ba03db763711f4dc317d791255e9e7415647dc0cb083a9edec403ebb.jpg) 
Figure 8.9 By solving the robust linear discrimination problem ( 8.23 ) we find an affine function that gives the largest gap in values between the two sets (with a normalization bound on the linear part of the function). Ge- ometrically, we are finding the thickest slab that separates the two sets of points. 

$$
\begin{array}{r l}{=}&{{}\left\{\begin{array}{l l}{-\lambda}&{\left\|\sum_{i=1}^{M}v_{i}y_{i}-\sum_{i=1}^{N}u_{i}x_{i}\right\|_{2}\leq\lambda}\\ {-\infty}&{\mathrm{otherwise}.}\end{array}\right.}\end{array}
$$ 

The dual problem can then be written as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-\left\|\sum_{i=1}^{M}v_{i}y_{i}-\sum_{i=1}^{N}u_{i}x_{i}\right\|_{2}}\\ &{\mathrm{subject~to}\quad u\succeq0,\quad\mathbf{1}^{T}u=1/2}\\ &{\quad v\succeq0,\quad\mathbf{1}^{T}v=1/2.}\end{array}
$$ 

erpret $\textstyle2\sum_{i=1}^{N}u_{i}x_{i}$ as a point in the convex hull of $\{x_{1},.\,.\,.\,,x_{N}\}$ and $\textstyle2\sum_{i=1}^{M}v_{i}y_{i}$ P as a point in the convex hull of $\{y_{1},.\,.\,.\,,y_{M}\}$ . The dual objective is to minimize (half) the distance between these two points, i.e. , find (half) the distance between the convex hulls of the two sets. 

# Support vector classifier 

When the two sets of points cannot be linearly separated, we might seek an affine function that approximately classifies the points, for example, one that minimizes the number of points misclassified. Unfortunately, this is in general a difficult combinatorial optimization problem. One heuristic for approximate linear discrim- ination is based on support vector classifiers , which we describe in this section. 

We start with the feasibility problem ( 8.21 ). We first relax the constraints by introducing nonnegative variables $u_{1},.\cdot\cdot\cdot,u_{N}$ and $v_{1},\dots,u_{M}$ , and forming the inequalities 

$$
a^{T}x_{i}-b\geq1-u_{i},\quad i=1,\ldots,N,\qquad a^{T}y_{i}-b\leq-(1-v_{i}),\quad i=1,\ldots,M.
$$ 

![](images/609a26d44653b92b1c918862516be42a654b437198edeaea1e033635ac027d78.jpg) 
Figure 8.10 Approximate linear discrimination via linear programming. The points $x_{1},.\cdot\cdot\cdot,x_{50}$ , shown as open circles, cannot be linearly separated from the points $y_{1},\dots,y_{50}$ , shown as filled circles. The classifier shown as a solid line was obtained by solving the LP ( 8.25 ). This classifier misclassifies one point. The dashed lines are the hyperplanes $a^{T}z-b=\pm1$ . Four points are correctly classified, but lie in the slab defined by the dashed lines. 

When $u\,=\,v\,=\,0$ , we recover the original constraints; by making $u$ and $v$ large enough, these inequalities can always be made feasible. We can think of $u_{i}$ as a measure of how much th onstraint $a^{T}x_{i}\,-\,b\,\geq\,1$ is viola d, and similarly for $v_{i}$ . Our goal is to find a , b , and sparse nonnegative u and v that satisfy the inequalities ( 8.24 ). As a heuristic for this, we can minimize the sum of the variables $u_{i}$ and $v_{i}$ , by solving the LP 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\mathbf{1}^{T}{\boldsymbol{u}}+\mathbf{1}^{T}{\boldsymbol{v}}}\\ {{\mathrm{subject~to}}\quad}&{a^{T}x_{i}-b\geq1-u_{i},\quad i=1,\ldots,N}\\ &{a^{T}y_{i}-b\leq-(1-v_{i}),\quad i=1,\ldots,M}\\ &{{\boldsymbol{u}}\succeq0,\quad{\boldsymbol{v}}\succeq0.}\end{array}}
$$ 

Figure 8.10 shows an example. In this example, the affine function $a^{T}z\mathrm{~-~}b$ mis- classifies 1 out of 100 points. Note however that when $0<u_{i}<1$ , the point $x_{i}$ classified by the affine function $a^{T}z\mathrm{~-~}b$ , but violates the inequality $a^{T}x_{i}-b\geq1$ − ≥ 1, and similarly for $y_{i}$ . The objective f ction in the $(8.25)$ n be interpreted as a relaxation of the nu s x i that violate a $a^{T}x_{i}-b\geq1$ − ≥ 1 plus the number of points $y_{i}$ that violate $a^{T}y_{i}\!-\!b\leq-1$ − ≤− 1. In rds, it is a relaxation of the number of points misclassified by the function a $a^{T}z-b$ − , p f points that are correctly classified but lie in the slab defined by − $-1<a^{T}z-b<1$ − 

More generally, we can consider the trade-oﬀbetween the number of misclas- sified points, and the width of the slab $\{z\ \vert\ \mathrm{~-~}1\ \leq\ a^{T}z\,-\,b\ \leq\ 1\}$ − ≤ − ≤ } , which is given by $2/\|a\|_{2}$ . The standard support vector classifier for the sets $\{x_{1},.\,.\,.\,,x_{N}\}$ , 

![](images/4d87102df05609c084e0ebf3ddbaad73f5c1ea9ce7e040e6469d07209f73511a.jpg) 
Figure 8.11 Approximate linear discrimination via support vector classifier, with $\gamma=0.1$ . The support vector classifier, shown as the solid line, misclas- sifies three points. Fifteen points are correctly classified but lie in the slab defined by $-1<a^{T}z-b<1$ , bounded by the dashed lines. 

$\{y_{1},.\,.\,.\,,y_{M}\}$ is defined as the solution of 

$$
\begin{array}{r l}{\mathrm{minimize}\:\:}&{\|a\|_{2}+\gamma(\mathbf{1}^{T}\boldsymbol{u}+\mathbf{1}^{T}\boldsymbol{v})}\\ {\mathrm{subject~to}\:\:}&{a^{T}x_{i}-b\geq1-u_{i},\quad i=1,\ldots,N}\\ &{a^{T}y_{i}-b\leq-(1-v_{i}),\quad i=1,\ldots,M}\\ &{u\succeq0,\quad v\succeq0,}\end{array}
$$ 

The first term is proportional to the inverse of the width of the slab defined by $-1\leq a^{T}z-b\leq1$ . The second term has the same interpretation as above, i.e. , it is a convex relaxation for the number of misclassified points (including the points in the slab). The parameter $\gamma$ , which is positive, gives the relative weight of the number of misclassified points (which we want to minimize), compared to the width of the slab (which we want to maximize). Figure 8.11 shows an example. 

# Approximate linear discrimination via logistic modeling 

Another approach to finding an affine function that approximately classifies two sets of points that cannot be linearly separated is based on the logistic model described in $\S7.1.1$ . We start by fitting the wo ets of points with a logistic model. Suppose $z$ is a random variable with values 0 or 1, with a distribution that depends on some (deterministic) explanatory variable $u\in\mathbf{R}^{n}$ , via a logistic model of the form 

$$
\begin{array}{r l}&{\mathbf{prob}(z=1)=(\exp(a^{T}u-b))/(1+\exp(a^{T}u-b))}\\ &{\mathbf{prob}(z=0)=1/(1+\exp(a^{T}u-b)).}\end{array}
$$ 

Now we assume that the given sets of points, $\{x_{1},.\,.\,.\,,x_{N}\}$ and $\{y_{1},.\,.\,.\,,y_{M}\}$ , arise as samples from the logistic model. Specifically, $\{x_{1},.\,.\,.\,,x_{N}\}$ are the values of $u$ r the $N$ samples fo ich $z=1$ , and $\{y_{1},.\,.\,.\,,y_{M}\}$ are the values of $u$ for the M samples for which z = 0. (This allows us to have $x_{i}=y_{j}$ , which would rule out discrimination between the two sets. In a logistic model, it simply means that we have two samples, with the same value of explanatory variable but diﬀerent outcomes.) 

We can determine $a$ and $b$ by maximum likelihood estimation from the observed samples, by solving the convex optimization problem 

$$
{\mathrm{minimize}}\quad-l(a,b)
$$ 

with variables $a$ , $b$ , where $l$ is the log-likelihood function 

$$
\begin{array}{r l}&{l(a,b)=\sum_{i=1}^{N}(a^{T}x_{i}-b)}\\ &{\quad-\sum_{i=1}^{N}\log(1+\exp(a^{T}x_{i}-b))-\sum_{i=1}^{M}\log(1+\exp(a^{T}y_{i}-b))}\end{array}
$$ 

see § 7 he two sets of points can be linearly separated, i.e. , if there exist $a$ , b with a $a^{T}x_{i}>b$ and $a^{T}y_{i}<b$ , then the optimization problem ( 8.27 ) is unbounded below. 

Once we find the maximum likelihood values of $a$ and $b$ , we can form a linear classifier $f(x)=a^{T}x-b$ for the two sets of points. This classifier has the following property: Assuming the data points are in fact generated from a logistic model with parameters $a$ and $b$ , it has the smallest probability of misc lassi cation, over all linear classifiers. The hyperplane $a^{T}u\,=\,b$ corresponds to the points where $\mathbf{prob}(z=1)=1/2$ , i.e. , the two outcomes are equally likely. An example is shown in figure 8.12 . 

Remark 8.1 Bayesian interpretation. Let $x$ and $z$ be two random variables, taking values in $\mathbf{R}^{n}$ and in $\{0,1\}$ , respectively. We assume that 

$$
\mathbf{prob}(z=1)=\mathbf{prob}(z=0)=1/2,
$$ 

and we denote by $p_{0}(x)$ and $p_{1}(x)$ the conditional probability densities of $x$ , given $z=0$ and given $z=1$ , respectively. We assume that and satisfy $p_{0}$ $p_{1}$ 

$$
{\frac{p_{1}(x)}{p_{0}(x)}}=e^{a^{T}x-b}
$$ 

for some $a$ and $b$ . Many common distributions satisfy this property. For example, $p_{0}$ and $p_{1}$ could be two normal densities on $\mathbf{R}^{n}$ with equal covariance matrices and diﬀerent means, or they could be two exponential densities on $\mathbf{R}_{+}^{n}$ . 

It follows from Bayes’ rule that 

$$
\begin{array}{l c r}{{\mathbf{prob}(z=1\mid x=u)}}&{{=}}&{{\displaystyle\frac{p_{1}(u)}{p_{1}(u)+p_{0}(u)}}}\\ {{\mathbf{prob}(z=0\mid x=u)}}&{{=}}&{{\displaystyle\frac{p_{0}(u)}{p_{1}(u)+p_{0}(u)},}}\end{array}
$$ 

from which we obtain 

$$
\begin{array}{l c l}{\mathbf{prob}(z=1\mid x=u)}&{=}&{\displaystyle\frac{\exp(a^{T}u-b)}{1+\exp(a^{T}u-b)}}\\ {\mathbf{prob}(z=0\mid x=u)}&{=}&{\displaystyle\frac{1}{1+\exp(a^{T}u-b)}.}\end{array}
$$ 

![](images/2f164bb4c723fbe95fd7257b9d058d727473ab0cd27da08ced211a06f84058b0.jpg) 
Figure 8.12 Approximate linear discrimination via logistic modeling. The points $x_{1},.\cdot\cdot\cdot,x_{50}$ , shown as open circles, cannot be linearly separated from the points $y_{1},\dots,y_{50}$ , shown as filled circles. The maximum likelihood lo- gistic model yields the hyperplane shown as a dark line, which misclassifies only two points. The two dashed lines show $a^{T}u-b=\pm1$ , where the proba- bility of each outcome, according to the logistic model, is 73%. Three points are correctly classified, but lie in between the dashed lines. 

The logistic model ( 8.26 ) can therefore be interpreted as the posterior distribution of $z$ , given that $x=u$ . 

# 8.6.2 Nonlinear discrimination 

We can just as well seek a nonlinear function $f$ , from a given subspace of functions, that is positive on one set and negative on another: 

$$
f(x_{i})>0,\ \ \ i=1,\ldots,N,\ \ \ \ \ \ f(y_{i})<0,\ \ \ i=1,\ldots,M.
$$ 

Provided $f$ is linear (or affine) in the parameters that define it, these inequalities can be solved in exactly the same way as in linear discrimination. In this section we examine some interesting special cases. 

# Quadratic discrimination 

Suppose we take $f$ to be quadratic: $f(x)\,=\,x^{T}P x+q^{T}x+r$ . The parameters $P\in\mathbf{S}^{n}$ , $q\in\mathbf{R}^{n}$ , $r\in\mathbf{R}$ must satisfy the inequalities 

$$
\begin{array}{r l}&{\boldsymbol{x}_{i}^{T}\boldsymbol{P}\boldsymbol{x}_{i}+\boldsymbol{q}^{T}\boldsymbol{x}_{i}+\boldsymbol{r}>0,\quad i=1,\ldots,N}\\ &{\boldsymbol{y}_{i}^{T}\boldsymbol{P}\boldsymbol{y}_{i}+\boldsymbol{q}^{T}\boldsymbol{y}_{i}+\boldsymbol{r}<0,\quad i=1,\ldots,M,}\end{array}
$$ 

which is a set of strict linear inequalities in the variables $P$ , $q$ , $r$ . As in linear discrimination, we note that $f$ is homogeneous in $P$ , , and , so we can find a $q$ $r$ solution to the strict inequalities by solving the nonstrict feasibility problem 

$$
\begin{array}{r l}&{{\boldsymbol{x}}_{i}^{T}{\boldsymbol{P}}{\boldsymbol{x}}_{i}+{\boldsymbol{q}}^{T}{\boldsymbol{x}}_{i}+{\boldsymbol{r}}\geq1,\quad i=1,\ldots,N}\\ &{{\boldsymbol{y}}_{i}^{T}{\boldsymbol{P}}{\boldsymbol{y}}_{i}+{\boldsymbol{q}}^{T}{\boldsymbol{y}}_{i}+{\boldsymbol{r}}\leq-1,\quad i=1,\ldots,M.}\end{array}
$$ 

The separating surface $\{z\mid z^{T}P z+q^{T}z+r=0\}$ is a quadratic surface, and the two classification regions 

$$
\{z\mid z^{T}P z+q^{T}z+r\leq0\},\qquad\{z\mid z^{T}P z+q^{T}z+r\geq0\},
$$ 

are defined by quadratic inequalities. Solving the quadratic discrimination problem, then, is the same as determining whether the two sets of points can be separated by a quadratic surface. 

We can impose conditions on the shape of the separating surface or classification regions by adding constraints on $P$ , $q$ , and $r$ . For example, we can require that $P\prec0$ , which means the separating surface is ellipsoidal. More specifically, it means that we seek an ellipsoid that contains all the points $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ , but none of the points $y_{1},\dotsc,y_{M}$ . This quadratic discrimination problem can be solved as an SDP feasibility problem 

$$
\begin{array}{l r l}{{\mathrm{find}}}&{{P,\;q,\;r}}\\ {{\mathrm{subject~to}}}&{{x_{i}^{T}P x_{i}+q^{T}x_{i}+r\geq1,\quad i=1,\ldots,N}}\\ &{{y_{i}^{T}P y_{i}+q^{T}y_{i}+r\leq-1,\quad i=1,\ldots,M}}\\ &{{P\preceq-I,}}\end{array}
$$ 

with variables $P\in\mathbf{S}^{n}$ , $q\in\mathbf{R}^{n}$ , and $r\in\mathbf{R}$ . (Here we use homogeneity in $P$ , $q$ , $r$ to express the constraint $P\prec0$ as $P\preceq-I$ .) Figure 8.13 shows an example. 

# Polynomial discrimination 

We consider the set of polynomials on $\mathbf{R}^{n}$ with degree less than or equal to $d$ : 

$$
f(x)=\sum_{i_{1}+\cdots+i_{n}\leq d}a_{i_{1}\cdots i_{d}}x_{1}^{i_{1}}\cdot\cdot\cdot x_{n}^{i_{n}}.
$$ 

We can determine whether or not two sets $\{x_{1},.\,.\,.\,,x_{N}\}$ and $\{y_{1},.\,.\,.\,,y_{M}\}$ can be separated by such a polynomial by solving a set of linear inequalities in the variables $a_{i_{1}\cdots i_{d}}$ . Geometrically, we are checking whether the two sets can be separated by an algebraic surface (defined by a polynomial of degree less than or equal to $d$ ). 

As an extension, the problem of determining the minimum degree polynomial on $\mathbf{R}^{n}$ that separates two sets of points can be solved via quasiconvex programming, since the degree of a polynomial is a quasiconvex function of the coefficients. This can be carried out by bisection on $d$ , solving a feasibility linear program at each step. An example is shown in figure 8.14 . 

![](images/c6557268531b0c8095bf7848475b6beeb9bf855d6b146f4c0ad344a0a0133539.jpg) 
Figure 8.13 Quadratic discrimination, with the condition that $P\prec0$ . This means that we seek an ellipsoid containing all of $x_{i}$ (shown as open circles) and none of the $y_{i}$ (shown as filled circles). This can be solved as an SDP feasibility problem. 

![](images/c4e51c9857565d94a34ed01091cdc89b6f590fffae769673405f07b4cf0d264c.jpg) 
Figure 8.14 Minimum degree polynomial discrimination in $\mathbf{R}^{2}$ . In this ex- ample, there exists no cubic polynomial that separates the points $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ (shown as open circles) from the points $y_{1},\dotsc,y_{M}$ (shown as filled circles), but they can be separated by fourth-degree polynomial, the zero level set of which is shown. 

# 8.7 Placement and location 

In this section we discuss a few variations on the following problem. We have $N$ points in $\mathbf{R}^{2}$ or $\mathbf{R}^{3}$ , and a list of pairs of points that must be connected by links. The positions of some of the $N$ points are fixed; our task is to determine the positions of the remaining points, i.e. , to place the remaining points. The objective is to place the points so that some measure of the total interconnection length of the links is minimized, subject to some additional constraints on the positions. As an example application, we can think of the points as locations of plants or warehouses of a company, and the links as the routes over which goods must be shipped. The goal is to find locations that minimize the total transportation cost. In another application, the points represent the position of modules or cells on an integrated circuit, and the links represent wires that connect pairs of cells. Here the goal might be to place the cells in such a way that the total length of wire used to interconnect the cells is minimized. 

The problem can be described in terms of an undirected graph with $N$ nodes, entin $N$ points. With each node we associate a variable $x_{i}\in\mathbf{R}^{k}$ , where = 2 or k = 3, which represents its location or position. The problem is to minimize 

$$
\sum_{(i,j)\in\mathcal{A}}f_{i j}(x_{i},x_{j})
$$ 

where $\mathcal{A}$ is the set of all links in the graph, and $f_{i j}\,:\,\mathbf{R}^{k}\,\times\,\mathbf{R}^{k}\,\rightarrow\,\mathbf{R}$ is a cost function associated with arc $(i,j)$ . (Alternatively, we can sum over all $i$ and $j$ , or over $i<j$ , and simply set $f_{i j}=0$ when links $i$ and $j$ are not connected.) Some of the coordinate vectors $x_{i}$ are given. The optimization variables are the remaining coordinates. Provided the functions $f_{i j}$ are convex, this is a convex optimization problem. 

# 8.7.1 Linear facility location problems 

In the simplest version of the problem the cost associated with arc $(i,j)$ is the distance between nodes $i$ and $j$ : $f_{i j}(x_{i},x_{j})=||x_{i}-x_{j}||$ , i.e. , we minimize 

$$
\sum_{(i,j)\in\mathcal{A}}\|x_{i}-x_{j}\|.
$$ 

We can use any norm, but the most common applications involve the Euclidean norm or the $\ell_{1}$ -norm. For example, in circuit design it is common to route the wires between cells along piecewise-linear paths, with each segment either horizontal or vertical. (This is called Manhattan routing , since paths along the streets in a city with a rectangular grid are also piecewise-linear, with each street aligned with one of two orthogonal axes.) In this case, the length of wire required to connect cell $i$ and cell $j$ is given by $\Vert{x_{i}-x_{j}}\Vert_{1}$ . 

We can include nonnegative weights that reﬂect diﬀerences in the cost per unit distance along diﬀerent arcs: 

$$
\sum_{(i,j)\in\mathcal{A}}w_{i j}\|x_{i}-x_{j}\|.
$$ 

By assigning a weight $w_{i j}\,=\,0$ to pairs of nodes that are not connected, we can express this problem more simply using the objective 

$$
\sum_{i<j}w_{i j}\|x_{i}-x_{j}\|.
$$ 

This placement problem is convex. 

Example 8.4 One free point. Consider the case where only one point $(u,v)\in\mathbf{R}^{2}$ is free, and we minimize the sum of the distances to fixed points $(u_{1},v_{1}),.\cdot.\,,\big(u_{K},v_{K}\big)$ . 

$\ell_{1}$ -norm . We can find a point that minimizes 

$$
\sum_{i=1}^{K}{\big(}|u-u_{i}|+|v-v_{i}|{\big)}
$$ 

analytically. An optimal point is any median of the fixed points. In other words, $u$ can be taken to be any median of the points $\{u_{1},.\,.\,.\,,u_{K}\}$ , and $v$ can be taken to be any edian of the points $\{v_{1},.\,.\,.\,,v_{K}\}$ . (If K is odd, the minimizer is unique; if K is even, there can be a rectangle of optimal points.) 

• Euclidean norm . The point $(u,v)$ that minimizes the sum of the Euclidean distances, 

$$
\sum_{i=1}^{K}\left(\left(u-u_{i}\right)^{2}+\left(v-v_{i}\right)^{2}\right)^{1/2},
$$ 

is called the Weber point of the given fixed points. 

# 8.7.2 Placement constraints 

We now list some interesting constraints that can be added to the basic placement problem, preserving convexity. We can require some positions $x_{i}$ to lie in a specified convex set, e.g. , a particular line, interval, square, or ellipsoid. We can constrain the relative position of one point with respect to one or more other points, for example, by limiting the distance between a pair of points. We can impose relative position constraints, e.g. , that one point must lie to the left of another point. 

The bounding box of a group of points is the smallest rectangle that contains the points. We can impose a constraint that limits the points $x_{1},\allowbreak\cdot\cdot\cdot,x_{p}$ (say) to lie in a bounding box with perimeter not exceeding $P_{\mathrm{max}}$ , by adding the constraints 

$$
\begin{array}{r}{u\preceq{x}_{i}\preceq v,\quad i=1,.\,.\,.\,,p,\qquad2\mathbf{1}^{T}(v-u)\leq P_{\operatorname*{max}},}\end{array}
$$ 

where $u$ , $v$ are additional variables. 

# 8.7.3 Nonlinear facility location problems 

More generally, we can associate a cost with each arc that is a nonlinear increasing function of the length, i.e. , 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i<j}w_{i j}h(\|x_{i}-x_{j}\|)}\end{array}
$$ 

where $h$ is an increasing (on $\mathbf{R}_{+}$ ) and convex function, and $w_{i j}\geq0$ . We call this a nonlinear placement or nonlinear facility location problem . 

One common example uses the Euclidean norm, and the function $h(z)\,=\,z^{2}$ , i.e. , we minimize 

$$
\sum_{i<j}w_{i j}\|x_{i}-x_{j}\|_{2}^{2}.
$$ 

This is called a quadratic placement problem . The quadratic placement problem can be solved analytically when the only constraints are linear equalities; it can be solved as a QP if the constraints are linear equalities and inequalities. 

Example 8.5 One free point. Consider the case where only one point $x$ is free, and we minimize the sum of the squares of the Euclidean distances to fixed points $x_{1},\dots,x_{K}$ , 

$$
\|x-x_{1}\|_{2}^{2}+\|x-x_{2}\|_{2}^{2}+\cdot\cdot\cdot+\|x-x_{K}\|_{2}^{2}.
$$ 

Taking derivatives, we see that the optimal $x$ is given by 

$$
{\frac{1}{K}}(x_{1}+x_{2}+\cdot\cdot\cdot+x_{K}),
$$ 

i.e. , the average of the fixed points. 

Some other interesting possibilities are the ‘deadzone’ function $h$ with deadzone width $2\gamma$ , defined as 

$$
h(z)={\left\{\begin{array}{l l}{0}&{|z|\leq\gamma}\\ {|z-\gamma|}&{|z|\geq\gamma,}\end{array}\right.}
$$ 

and the ‘quadratic-linear’ function $h$ , defined as 

$$
h(z)={\left\{\begin{array}{l l}{z^{2}}&{|z|\leq\gamma}\\ {2\gamma|z|-\gamma^{2}}&{|z|\geq\gamma.}\end{array}\right.}
$$ 

Example 8.6 We consider a placement problem in $\scriptstyle\mathbf{R}^{2}$ with 6 free points, 8 fixed points, and 27 links. Figures 8.15 – 8.17 show the optimal solutions for the criteria 

$$
\sum_{(i,j)\in\mathcal{A}}\|x_{i}-x_{j}\|_{2},\qquad\sum_{(i,j)\in\mathcal{A}}\|x_{i}-x_{j}\|_{2}^{2},\qquad\sum_{(i,j)\in\mathcal{A}}\|x_{i}-x_{j}\|_{2}^{4},
$$ 

i.e. , using the penalty functions $h(z)=z$ , $h(z)=z^{2}$ , and $h(z)=z^{4}$ . The figures also show the resulting distributions of the link lengths. 

Comparing the results, we see that the linear placement concentrates the free points in a small area, while the quadratic and fourth-order placements spread the points over larger areas. The linear placement includes many very short links, and a few very long ones (3 lengths under 0.2 and 2 lengths above 1.5.). The quadratic penalty function 

![](images/9169aab92ed00152ed0c0206bd6b4ca93b6eaeb700211ce6ee96034bf2e366d0.jpg) 
Figure 8.15 Linear placement. Placement problem with 6 free points (shown as dots), 8 fixed points (shown as squares), and 27 links. The coordinates of the free points minimize the sum of the Euclidean lengths of the links. The right plot is the distribution of the 27 link lengths. The dashed curve is the (scaled) penalty function $h(z)=z$ . 

![](images/bd43be1a640a0cde703f7c8cdc03dbfdcf35e53fcbcb4ca2066ed13c92d4b7e0.jpg) 
Figure 8.16 Quadratic placement. Placement that minimizes the sum of squares of the Euclidean lengths of the links, for the same data as in fig- ure 8.15 . The dashed curve is the (scaled) penalty function $h(z)=z^{2}$ . 

![](images/5d891fdd5b2df0eef96f3c002f5bac14c8bdf87343ab2b338863f630e178dcbd.jpg) 
Figure 8.17 Fourth-order placement. Placement that minimizes the sum of the fourth powers of the Euclidean lengths of the links. The dashed curve is the (scaled) penalty function $h(z)=z^{4}$ . 

puts a higher penalty on long lengths relative to short lengths, and for lengths under 0.1, the penalty is almost negligible. As a result, the maximum length is shorter (less than 1.4), but we also have fewer short links. The fourth-order function puts an even higher penalty on long lengths, and has a wider interval (between zero and about 0.4) where it is negligible. As a result, the maximum length is shorter than for the quadratic placement, but we also have more lengths close to the maximum. 

# 8.7.4 Location problems with path constraints 

# Path constraints 

A $p$ -link path along the points $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ is described by a sequence of nodes, $i_{0},\dots,i_{p}\in\{1,\dots,N\}$ . The length of the path is given by 

$$
\|x_{i_{1}}-x_{i_{0}}\|+\|x_{i_{2}}-x_{i_{1}}\|+\cdot\cdot\cdot+\|x_{i_{p}}-x_{i_{p-1}}\|,
$$ 

which is a convex function of $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ , so imposing an upper bound on the length of a path is a convex constraint. Several interesting placement problems involve path constraints, or have an objective based on path lengths. We describe one typical example, in which the objective is based on a maximum path length over a set of paths. 

# Minimax delay placement 

We consider a directed acyclic graph with nodes $1,\cdot\cdot\cdot,N$ , and arcs or links repre- sented by a set $\mathcal{A}$ f ordered pairs: $(i,j)\in\mathcal A$ and only if an arc points from $i$ to $j$ . We say node i is a sou ce node if no arc A points to it; it is a sink node or destination node if no arc in A leaves from it. We will be interested in the maximal paths in the graph, which begin at a source node and end at a sink node. 

The arcs of the graph are meant to model some kind of ﬂow, say of goods or information, in a network with nodes at positions $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ . The ﬂow starts at a source node, then moves along a path from node to node, ending at a sink or destination node. We use the distance between successive nodes to model prop- agation time, or shipment time, of the goods between nodes; the total delay or propagation time of a path is (proportional to) the sum of the distances between successive nodes. 

Now we can describe the minimax delay placement problem. Some of the node locations are fixed, and the others are free, i.e. , optimization variables. The goal is to choose the free node locations in order to minimize the maximum total delay, for any path from a source node to a sink node. Evidently this is a convex problem, since the objective 

$T_{\mathrm{max}}=\operatorname*{max}\{\|x_{i_{1}}-x_{i_{0}}\|+\cdot\cdot+\|x_{i_{p}}-x_{i_{p-1}}\|\mid i_{0},\ldots,i_{p}{\mathrm{~is~a~source-sinh~part}}\}$ 

is a convex function of the locations $x_{1},\allowbreak\cdot\cdot\cdot,x_{N}$ . 

While the problem of minimizing ( 8.29 ) is convex, the number of source-sink paths can be very large, exponential in the number of nodes or arcs. There is a useful reformulation of the problem, which avoids enumerating all sink-source paths. 

We first explain how we can evaluate the maximum delay $T_{\mathrm{max}}$ far more ef- ficiently than by evaluating the delay for every source-sink path, and taking the maximum. Let $\tau_{k}$ be the maximum total delay of any path from node $k$ to a sink node. Clearly we have $\tau_{k}=0$ when $k$ is a sink node. Consider a node $k$ , which has outgoing arcs to nodes $j_{1},\dots,j_{p}$ . For a path starting at node $k$ and ending at a sink node, its first arc must lead to one of the nodes $j_{1},\dots,j_{p}$ . If such a path first takes the arc leading to $j_{i}$ , and then takes the longest path from there to a sink node, the total length is 

$$
\|x_{j_{i}}-x_{k}\|+\tau_{j_{i}},
$$ 

i.e. , the length of the arc to $j_{i}$ , plus the total length of the longest path from $j_{i}$ to a sink node. It follows that the maximum delay of a path starting at node $k$ and leading to a sink node satisfies 

$$
\tau_{k}=\operatorname*{max}\{\|x_{j_{1}}-x_{k}\|+\tau_{j_{1}},.\,.\,.\,,\|x_{j_{p}}-x_{k}\|+\tau_{j_{p}}\}.
$$ 

(This is a simple dynamic programming argument.) 

The equations ( 8.30 ) give a recursion for finding the maximum delay from any node: we start at the sink nodes (which have maximum delay zero), and then work backward using the equations ( 8.30 ), until we reach all source nodes. The maximum delay over any such path is then the maximum of all the $\tau_{k}$ , which will occur at one of the source nodes. This dynamic programming recursion shows how the maximum delay along any source-sink path can be computed recursively, without enumerating all the paths. The number of arithmetic operations required for this recursion is approximately the number of links. 

Now we show how the recursion based on ( 8.30 ) can be used to formulate the minimax delay placement problem. We can express the problem as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\operatorname*{max}\{\tau_{k}\mid k{\mathrm{~a~source~node}}\}}\\ {{\mathrm{subject~to}}}&{\tau_{k}=0,\quad k{\mathrm{~a~sinh~node}}}\\ &{\tau_{k}=\operatorname*{max}\{\|x_{j}-x_{k}\|+\tau_{j}\mid{\mathrm{there~is~an~arc~from~}}k{\mathrm{~to~}}j\}.}\end{array}}
$$ 

with variables $\tau_{1},\dots,\tau_{N}$ and the free positions. This problem is not convex, but we can express it in an equivalent form that is convex, by replacing the equality constraints with inequalities. We introduce new variables $T_{1},.\,.\,.\,,T_{N}$ , which will be upper bounds on $\tau_{1},\dots,\tau_{N}$ , respectively. We will take $T_{k}=0$ for all sink nodes, and in place of ( 8.30 ) we take the inequalities 

$$
T_{k}\geq\operatorname*{max}\{\|x_{j_{1}}-x_{k}\|+T_{j_{1}},\ldots,\|x_{j_{p}}-x_{k}\|+T_{j_{p}}\}.
$$ 

If these inequalities are satisfied, then $T_{k}\geq\tau_{k}$ . Now we form the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\operatorname*{max}\{T_{k}\mid k{\mathrm{~a~source~node}}\}}\\ {{\mathrm{subject~to}}}&{T_{k}=0,\quad k{\mathrm{~a~sinh~node}}}\\ &{T_{k}\geq\operatorname*{max}\{\|x_{j}-x_{k}\|+T_{j}\mid{\mathrm{there~is~an~arc~from~}}k{\mathrm{~to~}}j\}.}\end{array}}
$$ 

This problem, with variables $T_{1},.\,.\,.\,,T_{N}$ and the free locations, is convex, and solves the minimax delay location problem. 

# 8.8 Floor planning 

In placement problems, the variables represent the coordinates of a number of points that are to be optimally placed. A ﬂoor planning problem can be considered an extension of a placement problem in two ways: 

• The objects to be placed are rectangles or boxes aligned with the axes (as opposed to points), and must not overlap. • Each rectangle or box to be placed can be reconfigured, within some limits. For example we might fix the area of each rectangle, but not the length and height separately. 

The objective is usually to minimize the size ( e.g. , area, volume, perimeter) of the bounding box , which is the smallest box that contains the boxes to be configured and placed. 

The non-overlap constraints make the general ﬂoor planning problem a compli- cated combinatorial optimization problem or rectangle packing problem. However, if the relative positioning of the boxes is specified, several types of ﬂoor planning problems can be formulated as convex optimization problems. We explore some of these in this section. We consider the two-dimensional case, and make a few comments on extensions to higher dimensions (when they are not obvious). 

We have $N$ cells or modules $C_{1},\ldots,C_{N}$ that are to be configured and placed in a rectangle with width $W$ and height $H$ , and lower left corner at the position $(0,0)$ . The geometry and position of the $i$ th cell is specified by its width $w_{i}$ and height $h_{i}$ , and the coordinates $(x_{i},y_{i})$ of its lower left corner. This is illustrated in figure 8.18 . 

The variables in the problem are $x_{i}$ , $y_{i}$ , $w_{i}$ , $h_{i}$ for $i=1,\cdot\cdot\cdot,N$ , and the width $W$ and height $H$ of the bounding rectangle. In all ﬂoor planning problems, we require that the cells lie inside the bounding rectangle, i.e. , 

$$
x_{i}\geq0,\qquad y_{i}\geq0,\qquad x_{i}+w_{i}\leq W,\qquad y_{i}+h_{i}\leq H,\qquad i=1,\ldots,N.
$$ 

![](images/32d0330b63dd30f0ca0a32172b7989ea2138af252032ecc1436f57727e246b2c.jpg) 
Figure 8.18 Floor planning problem. Non-overlapping rectangular cells are placed in a rectangle with width $W$ , height $H$ , and lower left corner at $(0,0)$ . The i th cell is specified by its width $w_{i}$ , height $h_{i}$ , and the coordinates of its lower left corner, $\left({x_{i},y_{i}}\right)$ . 

We also require that the cells do not overlap, except possibly on their boundaries: 

$$
\mathbf{int}\left(C_{i}\cap C_{j}\right)=\emptyset\quad{\mathrm{for~}}i\neq j.
$$ 

(It is also possible to require a positive minimum clearance between the cells.) The non-overlap constraint $\mathbf{int}(C_{i}\cap C_{j})=\emptyset$ holds if and only if for $i\neq j$ , 

These four geometric conditions correspond to the inequalities 

$$
x_{i}+w_{i}\leq x_{j},{\mathrm{~or~}}x_{j}+w_{j}\leq x_{i},{\mathrm{~or~}}y_{i}+h_{i}\leq y_{j},{\mathrm{~or~}}y_{j}+h_{j}\leq y_{i},
$$ 

at least one of which must hold for each $i\neq j$ . Note the combinatorial nature of these constraints: for each pair $i\neq j$ , at least one of the four inequalities above must hold. 

# 8.8.1 Relative positioning constraints 

The idea of relative positioning constraints is to specify, for each pair of cells, one of the four possible relative positioning conditions, i.e. , left, right, above, or below. One simple method to specify these constraints is to give two relations on $\{1,\cdot\cdot\cdot,N\}$ : $\mathcal{L}$ (meaning ‘left of’) and $\mathcal{B}$ (meaning ‘below’). We then impose the constraint that $C_{i}$ is to the left of $C_{j}$ if $(i,j)\in\mathcal{L}$ , and $C_{i}$ is below $C_{j}$ if $(i,j)\in\mathcal{B}$ . This yields the constraints 

$$
x_{i}+w_{i}\leq x_{j}{\mathrm{~for~}}(i,j)\in{\mathcal{L}},\qquad y_{i}+h_{i}\leq y_{j}{\mathrm{~for~}}(i,j)\in{\mathcal{B}},
$$ 

for $i,j\ =\ 1,\cdot\cdot\cdot,N$ . To ensure that the relations $\mathcal{L}$ and $\mathcal{B}$ specify the relative positioning of each pair of cells, we require that for each $(i,j)$ with $i\neq j$ , one of the following holds: 

$$
\left(i,j\right)\in\mathcal{L},\qquad\left(j,i\right)\in\mathcal{L},\qquad\left(i,j\right)\in\mathcal{B},\qquad\left(j,\right.
$$ 

and that $(i,i)\not\in{\mathcal{L}}$ , $(i,i)\notin\mathcal{B}$ . The inequalities ( 8.33 ) are a set of $N(N-1)/2$ linear inequalities in the variables. These inequalities imply the non-overlap inequali- ties $(8.32)$ , which are a set of $N(N-1)/2$ disj nctions of four linear inequalities. 

We can assume that the relations L and B are anti-symmetric ( i.e. , $(i,j)\in$ ${\mathcal L}\ \Rightarrow\ (j,i)\notin{\mathcal L})$ and transitive ( i.e. , $(i,j)\in\mathcal{L}$ , $(j,k)\in{\mathcal{L}}\Rightarrow\ (i,k)\in{\mathcal{L}},$ ). (If this were not the case, the relative positioning constraints would clearly be infeasible.) Transitivity corresponds to the obvious condition that if cell $C_{i}$ is to the left of cell $C_{j}$ , which is to the left of cell $C_{k}$ , then cell $C_{i}$ must be to the left of cell $C_{k}$ . In this case the inequality corresponding to $(i,k)\in{\mathcal{L}}$ is red ndant it is implied by the other two. By exploiting transitivity of the relations L and B we can remove redundant constraints, and obtain a compact set of relative positioning inequalities. 

A minimal set of relative positioning constraints is conveniently described using o directed acyclic graphs $\mathcal{H}$ an $\mathcal{V}$ (for horizontal and vertical). Both graphs have N nodes, corresponding to the N cells in the ﬂoor planning problem. The graph $\mathcal{H}$ generates the relation $\mathcal{L}$ as follows: we have $(i,j)\,\in\,{\mathcal{L}}$ if and only if there is (directed) path in $\mathcal{H}$ from $i$ to $j$ . Similarly, the graph $\nu$ gene ates the relation B : $(i,j)\in\mathcal{B}$ if and only if there is a (directed) path in $\nu$ from i to $j$ . To ensure that a relative positioning constraint is given for every pair of cells, we require that for every pair of cells, there is a directed path from one to the other in one of the graphs. 

Evidently, we only need to impose the inequalities that correspond to the edges of the graphs $\mathcal{H}$ and $\mathcal{V}$ ; the others follow from transitivity. We arrive at the set of inequalities 

$$
x_{i}+w_{i}\leq x_{j}{\mathrm{~for~}}(i,j)\in{\mathcal{H}},\qquad y_{i}+h_{i}\leq y_{j}{\mathrm{~for~}}(i,j)\in{\mathcal{V}},
$$ 

which is a set of linear inequalities, one for each edge in $\mathcal{H}$ and $\nu$ . The set of inequalities ( 8.34 ) is a subset of the set of inequalities ( 8.33 ), and equivalent. 

In a similar way, the $4N$ inequalities ( 8.31 ) can be reduced to a minimal, equiv- alent se . The constraint $x_{i}\geq0$ only need to be imposed on the left-most cells, i.e. , for i at are minimal in the relation L . These correspond to the sources in the graph H , i.e. , those nodes that have no edges pointing to them. Similarly, the inequalities $x_{i}+w_{i}\leq W$ only need to be imposed for the right-most cells. In the same way the vertical bounding box inequalities can be pruned to a minimal set. This yields the minimal equivalent set of bounding box inequalities 

$$
\begin{array}{r l}&{x_{i}\geq0\mathrm{~for~}i\mathrm{~}\mathcal{L}\mathrm{~minimal,~}\qquad x_{i}+w_{i}\leq W\mathrm{~for~}i\mathrm{~}\mathcal{L}\mathrm{~maximal,~}}\\ &{y_{i}\geq0\mathrm{~for~}i\mathrm{~}\mathcal{B}\mathrm{~minimal,~}\qquad y_{i}+h_{i}\leq H\mathrm{~for~}i\mathrm{~}\mathcal{B}\mathrm{~maximal.}}\end{array}
$$ 

A simple example is shown in figure 8.19 . In this example, the $\mathcal{L}$ minimal or left-most cells are $C_{1}$ , $C_{2}$ , and $C_{4}$ , and the only right-most cell is $C_{5}$ . The minimal set of inequalities specifying the horizontal relative positioning is given by 

$$
\begin{array}{r l}&{x_{1}\geq0,\qquad x_{2}\geq0,\qquad x_{4}\geq0,\qquad x_{5}+w_{5}\leq W,\qquad x_{1}+w_{1}\leq x_{3},}\\ &{\qquad\qquad x_{2}+w_{2}\leq x_{3},\qquad x_{3}+w_{3}\leq x_{5},\qquad x_{4}+w_{4}\leq x_{5}.}\end{array}
$$ 

![](images/ff4aeaadbf2e9463f83f2147ca6366e15ad9944ad5ec091e34be16f019dc3de0.jpg) 
igure 8.19 Example illustrating the horizontal and vertical graphs $\mathcal{H}$ and V that specify the lative posi ioning of the cells. If there is a ath from node $i$ to node $j$ in H , then cell i m t be placed to the left of cell j . If there is a path from node $i$ to node $j$ in V , then cell i must be placed below cell $j$ . The ﬂoorplan shown at right satisfies the relative positioning specified by the two graphs. 

The minimal set of inequalities specifying the vertical relative positioning is given by 

$$
\begin{array}{r l r l r l}&{y_{2}\geq0,}&&{y_{3}\geq0,}&&{y_{5}\geq0,}&&{y_{4}+h_{4}\leq H,}&&{y_{5}+h_{5}\leq H,}\\ &{}&&{y_{2}+h_{2}\leq y_{1},}&&{y_{1}+h_{1}\leq y_{4},}&&{y_{3}+h_{3}\leq y_{4}.}\end{array}
$$ 

# 8.8.2 Floor planning via convex optimization 

In this formulation, the variables are the bounding box width and height $W$ and $H$ , and the cell widths, heights, and positions: $w_{i}$ , $h_{i}$ , $x_{i}$ , and $w_{i}$ , for $i=1,\cdot\cdot\cdot,N$ . We impose the bounding box constraints ( 8.35 ) and the relative positioning con- straints ( 8.34 ), which are linear inequalities. As objective, we take the perimeter of the bounding box, i.e. , $2(W+H)$ , which is a linear function of the variables. We now list some of the constraints that can be expressed as convex inequalities or linear equalities in the variables. 

# Minimum spacing 

We can impose a minimum spacing $\rho>0$ between cells by changing the relative constraints from $x_{i}\,+\,w_{i}\;\leq\;x_{j}$ for $(i,j)\,\in\,{\mathcal{H}}$ , to $x_{i}+w_{i}+\rho\,\leq\,x_{j}$ for $(i,j)\in\mathcal{H}$ ∈H , and similarly for the vertical graph. We can have a diﬀerent mi mum acing associated with each edge in $\mathcal{H}$ and $\mathcal{V}$ . Another possibility is to fix W and H , and maximize the minimum spacing $\rho$ as objective. 

# Minimum cell area 

ch cell we specify a minimum area, i.e. , we require that $w_{i}h_{i}\,\geq\,A_{i}$ , where $A_{i}>0$ 0. These minimum cell area constraints can be expressed as convex inequali- ties in several ways, e.g. , $w_{i}\geq A_{i}/h_{i}$ , $(w_{i}h_{i})^{1/2}\geq A_{i}^{1/2}$ , or $\log w_{i}+\log h_{i}\geq\log A_{i}$ . 

# Aspect ratio constraints 

We can impose upper and lower bounds on the aspect ratio of each cell, i.e. , 

$$
l_{i}\le h_{i}/w_{i}\le u_{i}.
$$ 

Multiplying through by $w_{i}$ transforms these constraints into linear inequalities. We can also fix the aspect ratio of a cell, which results in a linear equality constraint. 

# Alignment constraints 

We can impose the constraint that two edges, or a center line, of two cells are aligned. For example, the horizontal center line of cell $i$ aligns with the top of cell $j$ when 

$$
y_{i}+h_{i}/2=y_{j}+h_{j}.
$$ 

These are linear equality constraints. In a similar way we can require that a cell is ﬂushed against the bounding box boundary. 

# Symmetry constraints 

We can require pairs of cells to be symmetric about a vertical or horizontal axis, that can be fixed or ﬂoating ( i.e. , whose position is fixed or not). For example, to specify that the pair of cells $i$ and $j$ are symmetric about the vertical axis $x=x_{\mathrm{axis}}$ , we impose the linear equality constraint 

$$
x_{\mathrm{axis}}-(x_{i}+w_{i}/2)=x_{j}+w_{j}/2-x_{\mathrm{axis}}.
$$ 

We can require that several pairs of cells be symmetric about an unspecified vertical axis by imposing these equality constraints, and introducing $x_{\mathrm{axis}}$ as a new variable. 

# Similarity constraints 

We can require that cell $i$ be an $a$ -scaled translate of cell $j$ by the equality con- straints $w_{i}=a w_{j}$ , $h_{i}=a h_{j}$ . Here the scaling factor $a$ must be fixed. By imposing only one of these constraints, we require that the width (or height) of one cell be a given factor times the width (or height) of the other cell. 

# Containment constraints 

We can require that a particular cell contains a given point, which imposes two lin- ear inequalities. We can require that a particular cell lie inside a given polyhedron, again by imposing linear inequalities. 

# Distance constraints 

We can impose a variety of constraints that limit the distance between pairs of cells. In the simplest case, we can limit the distance between the center points of cell $i$ and $j$ (or any other fixed points on the cells, such as lower left corners). For example, to limit the distance between the centers of cells $i$ and $j$ , we use the (convex) inequality 

$$
\left\|\big(x_{i}+w_{i}/2,y_{i}+h_{i}/2\big)-\big(x_{j}+w_{j}/2,y_{j}+h_{j}/2\big)\right\|\leq D_{i j}.
$$ 

As in placement problems, we can limit sums of distances, or use sums of distances as the objective. 

We can also limit the distance $\mathbf{dist}(C_{i},C_{j})$ between cell $i$ and cell $j$ , i.e. , the minimum distance between a point in cell $i$ and a point in cell $j$ . In the general case this can be done as follows. To limit the distance between cells $i$ and $j$ in the norm $||\cdot||$ , we can introduce four new variables $u_{i},\ v_{i}$ , $u_{j}$ , $v_{j}$ . The pair $(u_{i},v_{i})$ will represent a point in $C_{i}$ , and the pair $(u_{j},v_{j})$ will represent a point in $C_{j}$ . To ensure this we impose the linear inequalities 

$$
x_{i}\leq u_{i}\leq x_{i}+w_{i},\qquad y_{i}\leq v_{i}\leq y_{i}+h_{i},
$$ 

and similarly for cell $j$ . Finally, to limit $\mathbf{dist}(C_{i},C_{j})$ , we add the convex inequality 

$$
\begin{array}{r}{\Vert(u_{i},v_{i})-(u_{j},v_{j})\Vert\leq D_{i j}.}\end{array}
$$ 

In many specific cases we can express these distance constraints more efficiently, by exploiting the relative positioning constraints or deriving a more explicit formu- lation. As an example consider the $\ell_{\infty}$ -norm, and suppose cell $i$ lies to the left of cell $j$ (by a relative positioning constraint). The horizontal displacement between the two cells is $x_{j}-\left(x_{i}+w_{i}\right)$ Then we have $\mathbf{dist}(C_{i},C_{j})\leq D_{i j}$ if and only if 

$$
x_{j}-(x_{i}+w_{i})\leq D_{i j},\qquad y_{j}-(y_{i}+h_{i})\leq D_{i j},\qquad y_{i}-(y_{j}+h_{j})\leq D_{i j}.
$$ 

The first inequality states that the horizontal displacement between the right edge of cell $i$ and the left edge of cell $j$ does not exceed $D_{i j}$ . The second inequality requires that the bottom of cell $j$ is no more than $D_{i j}$ above the top of cell $i$ , and the third inequality requires that the bottom of cell $i$ is no more than $D_{i j}$ above the top of $\operatorname{cell}j$ . These three inequalities together are equivalent to $\mathbf{dist}(C_{i},C_{j})\leq D_{i j}$ . In this case, we do not need to introduce any new variables. 

We can limit the $\ell_{1}$ - (or $\ell_{2}$ -) distance between two cells in a similar way. Here we introduce one new variable $d_{v}$ , which will serve as a bound on the vertical displacement between the cells. To limit the $\ell_{1}$ -distance, we add the constraints 

$$
y_{j}-(y_{i}+h_{i})\leq d_{v},\qquad y_{i}-(y_{j}+h_{j})\leq d_{v},\qquad d_{v}\geq0
$$ 

and the constraints 

$$
x_{j}-\left(x_{i}+w_{i}\right)+d_{v}\leq D_{i j}.
$$ 

(The first term is the horizontal displacement and the second is an upper bound on the vertical displacement.) To limit the Euclidean distance between the cells, we replace this last constraint with 

$$
(x_{j}-(x_{i}+w_{i}))^{2}+d_{v}^{2}\leq D_{i j}^{2}.
$$ 

![](images/094d1f0a48be5ee586939a4de11468f67251cec9fd85a53abf20656363ae0852.jpg) 
Figure 8.20 Four instances of an optimal ﬂoor plan, using the relative po- sitioning constraints shown in figure 8.19 . In each case the objective is to minimize the perimeter, and the same minimum spacing constraint between cells is imposed. We also require the aspect ratios to lie between $1/5$ and 5. The four cases diﬀer in the minimum areas required for each cell. The sum of the minimum areas is the same for each case. 

Example 8.7 Figure 8.20 shows an example with 5 cells, using the ordering constraints of figure 8.19 , and four diﬀerent sets of constraints. In each case we impose the same minimum required spacing constraint, and the same aspect ratio constraint $1/5\leq w_{i}/h_{i}\leq5$ . The four cases diﬀer in the minimum require eas $A_{i}$ . The values of A are chosen so that the total minimum required area $\textstyle\sum_{i=1}^{5}A_{i}$ P is the same for each case. 

# 8.8.3 Floor planning via geometric programming 

The ﬂoor planning problem can also be formulated as a geometric program in the variables , , $h_{i}$ , $W$ , $H$ . The objectives and constraints that can be handled $x_{i},~y_{i}$ $w_{i}$ in this formulation are a bit diﬀerent from those that can be expressed in the convex formulation. 

First we note that the bounding box constraints ( 8.35 ) and the relative po- sitioning constraints ( 8.34 ) are posynomial inequalities, since the lefthand sides are sums of variables, and the righthand sides are single variables, hence monomi- als. Dividing these inequalities by the righthand side yields standard posynomial inequalities. 

In the geometric programming formulation we can minimize the bounding box area, since $W H$ is a monomial, hence posynomial. We can also exactly specify the area of each cell, since $w_{i}h_{i}\,=\,A_{i}$ is a monomial equality constraint. On the other hand alignment, symmetry, and distance constraints cannot be handled in the geometric programming formulation. Similarity, however, can be; indeed it is possible to require that one cell be similar to another, without specifying the scaling ratio (which can be treated as just another variable). 

# Bibliography 

The characterization of Euclidean distance matrices in $\S8.3.3$ appears in Schoenberg [ Sch35 ]; see also Gower [ Gow85 ]. 

Our use of the term L¨ owner-John ellipsoid follows Gr¨ otschel, Lov´ asz, and Schrijver [ GLS88 , page 69]. The efficiency results for ellipsoidal approximations in § 8.4 were proved by John [ Joh85 ]. Boyd, El Ghaoui, Feron, and Balakrishnan [ BEFB94 , § 3.7] give con- vex formulations of several ellipsoidal approximation problems involving sets defined as unions, intersections or sums of ellipsoids. 

The diﬀerent centers defined in § 8.5 have applications in design centering (see, for exam- ple, Seifi, Ponnambalan, and Vlach [ SPV99 ]), and cutting-plane methods (Elzinga and Moore [ EM75 ], Tarasov, Khachiyan, and Erlikh [ TKE88 ], and Ye [ Ye97 , chapter 8]). The inner ellipsoid defined by the Hessian of the logarithmic barrier function (page 420 ) is sometimes called the Dikin ellipsoid , and is the basis of Dikin’s algorithm for linear and quadratic programming [ Dik67 ]. The expression for the outer ellipsoid at the analytic center was given by Sonnevend [ Son86 ]. For extensions to nonpolyhedral convex sets, see Boyd and El Ghaoui [ BE93 ], Jarre [ Jar94 ], and Nesterov and Nemirovski [ NN94 , page 34]. 

Convex optimization has been applied to linear and nonlinear discrimination problems since the 1960s; see Mangasarian [ Man65 ] and Rosen [ Ros65 ]. Standard texts that dis- cuss pattern classification include Duda, Hart, and Stork [ DHS99 ] and Hastie, Tibshirani, and Friedman [ HTF01 ]. For a detailed discussion of support vector classifiers, see Vap- nik [ Vap00 ] or Sch¨ olkopf and Smola [ SS01 ]. 

The Weber point defined in example 8.4 is named after Weber [ Web71 ]. Linear and quadratic placement is used in circuit design (Kleinhaus, Sigl, Johannes, and Antre- ich [ KSJA91 , SDJ91 ]). Sherwani [ She99 ] is a recent overview of algorithms for placement, layout, ﬂoor planning, and other geometric optimization problems in VLSI circuit design. 

# Exercises 

# Projection on a set 

8.1 Uniqu ss of projection. Show that if $C\subseteq\mathbf{R}^{n}$ is nonempty, clos convex, and the norm ∥· ∥ is strictly convex, then for very $x_{0}$ there is exactly one x $x\in C$ ∈ closest to $x_{0}$ . In other words the projection of $x_{\mathrm{0}}$ on C is unique. 

8.2 [ Web94 , Val64 ] Cheb racterization of convexity. A et $C\in\mathbf{R}^{n}$ is called a Cheby- sh set if for every x $x_{0}\,\in\,\mathbf{R}^{n}$ ∈ , there is a unique point in C closest (in Euclidean norm) to x $x_{0}$ . From the result in exercise 8.1 , every nonempty, closed, convex set is a Chebyshev set. In this problem we show the converse, which is known as Motzkin’s theorem . Let $C\in\mathbf{R}^{n}$ be a Chebyshev set. 

(a) Show that $C$ is nonempty and closed. (b) Show that $P_{C}$ , the Euclidean projection on $C$ , is continuous. (c) $x_{0}\notin C$ . Show that $P_{C}(x)=P_{C}(x_{0})$ for all $x=\theta x_{0}+(1-\theta)P_{C}(x_{0})$ with $0\leq\theta\leq1$ ≤ ≤ 1. (d) ose $x_{0}\notin C$ . Show that $P_{C}(x)=P_{C}(x_{0})$ for all $x=\theta x_{0}+(1-\theta)P_{C}(x_{0})$ with $\theta\geq1$ ≥ 1. (e) Combining parts (c) and (d), we can conclude that all points on the ray with base $P_{C}(x_{0})$ and direction $x_{0}-P_{C}(x_{0})$ have projection $P_{C}(x_{0})$ . Show that this implies that C is convex. 

8.3 Euclidean projection on proper cones. 

(a) Nonnegative orthant. Show that Euclidean projection onto the nonnegative orthant is given by the expression on page 399 . (b) Positive semidefinite cone. Show that Euclidean projection onto the positive semidef- inite cone is given by the expression on page 399 . (c) Second-order cone. Show that the Euclidean projection of $(x_{0},t_{0})$ on the second- order cone 

$$
K=\{(x,t)\in\mathbf{R}^{n+1}\mid\|x\|_{2}\leq t\}
$$ 

is given by 

$$
P_{K}(x_{0},t_{0})=\left\{\begin{array}{l l}{0}&{\|x_{0}\|_{2}\leq-t_{0}}\\ {(x_{0},t_{0})}&{\|x_{0}\|_{2}\leq t_{0}}\\ {(1/2)(1+t_{0}/\|x_{0}\|_{2})(x_{0},\|x_{0}\|_{2})}&{\|x_{0}\|_{2}\geq|t_{0}|.}\end{array}\right.
$$ 

8.4 The Euclidean projection of a point on a convex set yields a simple separating hyperplane 

$$
\left(P_{C}(x_{0})-x_{0}\right)^{T}(x-(1/2)(x_{0}+P_{C}(x_{0})))=0.
$$ 

Find a counterexample that shows that this construction does not work for general norms. 

8.5 [ HUL93 , volume 1, page 154] Depth function and signed distance to bound ry. Let $C\subseteq\mathbf{R}^{n}$ be a nonempty convex set, and let $\mathbf{dist}(x,C)$ be the distance of $x$ to C in some norm. We already know that $\mathbf{dist}(x,C)$ is a convex function of $x$ . 

(a) Show that the depth function, 

$$
\mathbf{depth}(x,C)=\mathbf{dist}(x,\mathbf{R}^{n}\setminus C),
$$ 

is concave for $x\in C$ . 

(b) The signed distance to the boundary of $C$ is defined as 

$$
s(x)=\left\{\begin{array}{l l}{\mathrm{dist}(x,C)}&{x\not\in C}\\ {-\mathrm{\bf~depth}(x,C)}&{x\in C.}\end{array}\right.
$$ 

Thus, $s(x)$ is positive outside $C$ , zero on its boundary, and negative on its interior. Show that $s$ is a convex function. 

# Distance between sets 

8.6 Let $C$ , $D$ be convex sets. 

(a) Show that $\mathbf{dist}(C,x+D)$ is a convex function of $x$ . (b) Show that $\mathbf{dist}(t C,x+t D)$ is a convex function of $(x,t)$ for $t>0$ . 

8.7 Separation of ellipsoids . Let $\mathcal{E}_{1}$ and $\dot{\mathcal{E}_{2}}$ be two ellipsoids defined as 

$$
\mathcal{E}_{1}=\{x\mid(x-x_{1})^{T}P_{1}^{-1}(x-x_{1})\leq1\},\qquad\mathcal{E}_{2}=\{x\mid(x-x_{2})^{T}P_{2}^{-1}(x-x_{2})\leq1\}.
$$ 

with where $P_{1}$ , $P_{2}\in\mathbf{S}_{++}^{n}$ . Show that $\mathcal{E}_{1}\cap\mathcal{E}_{2}=\varnothing$ if and only if there exists an $a\in\mathbf{R}^{n}

$ 

$$
\|P_{2}^{1/2}a\|_{2}+\|P_{1}^{1/2}a\|_{2}<a^{T}(x_{1}-x_{2}).
$$ 

8.8 Intersection and containment of polyhedra. Let $\mathcal{P}_{1}$ and $\mathcal{P}_{2}$ be two polyhedra defined as 

$$
\mathcal{P}_{1}=\{x\mid A x\preceq b\},\qquad\mathcal{P}_{2}=\{x\mid F x\preceq g\},
$$ 

with $A\in\mathbf{R}^{m\times n}$ , $b\in\mathbf{R}^{m}$ , $F\in\mathbf{R}^{p\times n}$ , $g\in\mathbf{R}^{p}$ . Formulate each of the following problems as an LP feasibility problem, or a set of LP feasibility problems. 

(a) Find a point in the intersection $\mathcal{P}_{1}\cap\mathcal{P}_{2}$ . 

(b) Determine whether $\mathcal{P}_{1}\subseteq P_{2}$ . 

For each problem, derive a set of linear inequalities and equalities that forms a strong alternative, and give a geometric interpretation of the alternative. Repeat the question for two polyhedra defined as 

$$
\begin{array}{r}{\mathcal{P}_{1}=\mathbf{conv}\{v_{1},\dots,v_{K}\},\qquad\mathcal{P}_{2}=\mathbf{conv}\{w_{1},\dots,w_{L}\}.}\end{array}
$$ 

# Euclidean distance and angle problems 

8.9 Closest Euclidean distance matrix to given data. We are given data $\hat{d}_{i j}$ , for $i,j=1,\dotsc,n$ , which are corrupted measurements of the Euclidean distances between vectors in $\mathbf{R}^{k}$ : 

$$
\hat{d}_{i j}=\Vert x_{i}-x_{j}\Vert_{2}+v_{i j},\quad i,j=1,.\,.\,.\,,n,
$$ 

where $v_{i j}$ is some noise or error. These data satisfy $\hat{d}_{i j}\geq0$ ≥ 0 and $\hat{d}_{i j}=\hat{d}_{j i}$ , for all $i,j$ . The dimension $k$ is not specified. 

Show how to solve the following problem using convex optimization. Find a dimension $k$ $x_{1},.\,.\,.\,,x_{n}\,\in\,\mathbf{R}^{k}$ so that ${\textstyle\sum_{i,j=1}^{n}}(d_{i j}\,-\,\hat{d}_{i j})^{2}$ is minimized, where $d_{i j}=\|x_{i}-x_{j}\|_{2}$ , $i,j=1,\dotsc,n$ . In other words, given some data that are approximate Euclidean distances, you are to find the closest set of actual Euclidean distances, in the least-squares sense. 

8.10 x angle fitting. Suppose that $y_{1},\ldots,y_{m}\,\in\,\mathbf{R}^{k}$ are affine functions of a variable $x\in\mathbf{R}^{n}$ : 

$$
y_{i}=A_{i}x+b_{i},\quad i=1,\ldots,m,
$$ 

and $z_{1},.\,.\,.\,,z_{m}\in\mathbf{R}^{k}$ are given nonzero vectors. We want to choose the variable $x$ , subject to some convex constraints, ( e.g. , linear inequalities) to minimize the maximum angle between $y_{i}$ and $z_{i}$ , 

$$
\operatorname*{max}\{\angle{(y_{1},z_{1})},.\,.\,.\,,\angle{\bigl(y_{m},z_{m}\bigr)}\}.
$$ 

The angle between nonzero vectors is defined as usual: 

$$
\angle(u,v)=\cos^{-1}\left(\frac{u^{T}v}{||u||_{2}||v||_{2}}\right),
$$ 

where we take $\cos^{-1}(a)\,\in\,[0,\pi]$ . We are only interested in the case when the optimal objective value does not exceed $\pi/2$ . 

Formulate this problem as a convex or quasiconvex optimization problem. When the constraints on $x$ are linear inequalities, what kind of problem (or problems) do you have to solve? 

8.11 Smallest Euclidean cone containing given points. In $\mathbf{R}^{n}$ , we define a Euclidean cone , with center direction $c\neq0$ , and angular radius $\theta$ , with $0\leq\theta\leq\pi/2$ , as the set 

$$
\{x\in\mathbf{R}^{n}\mid\angle(c,x)\leq\theta\}.
$$ 

(A Euclidean cone is a second-order cone, i.e. , it can be represented as the image of the second-order cone under a nonsingular linear mapping.) 

Let $a_{1},\ldots,a_{m}\in\mathbf{R}^{n}$ ow would you find the Euclidean cone, of smallest angular radius, that contains a $a_{1},\ldots,a_{m}$ ? (In particular, you should explain how to solve the feasibility problem, i.e. , how to determine whether there is a Euclidean cone which contains the points.) 

# Extremal volume ellipsoids 

8.12 Show that the maximum volume ellipsoid enclosed in a set is unique. Show that the L¨ owner-John ellipsoid of a set is unique. 

8.13 L¨ owner-John ellipsoid of a simplex. In this exercise we show that the L¨ owner-John el- lipsoid of a simplex in $\mathbf{R}^{n}$ must be shrunk by a factor $n$ to fit inside the simplex. Since the L¨ owner-John ellipsoid is affinely invariant, it is sufficient to show the result for one particular simplex. 

Derive the L¨ owner-John ellipso $\mathcal{E}_{\mathrm{ij}}$ for the simplex $C=\mathbf{conv}\{0,e_{1},.\,.\,,e_{n}\}$ . Show that $\mathcal{E}_{\mathrm{ij}}$ must be shrunk by a factor 1 $1/n$ to fit inside the simplex. 

8.14 Efficiency of ellipsoidal inner approximation. Let $C$ be a polyhedron in $\mathbf{R}^{n}$ described as $C=\{x\mid A x\preceq b\}$ , and suppose that $\{x\mid A x\prec b\}$ is nonempty. 

(a) Show that the maximum volume ellipsoid enclosed in $C$ , expanded by a factor $n$ about its center, is an ellipsoid that contains $C$ . (b) Show that if $C$ is symmetric about the origin, i.e. , of the form $C=\{x\mid-\mathbf{1}\preceq A x\preceq$ ${\bf1}\}$ , then expanding the m ximum volume inscribed ellipsoid by a factor $\sqrt{n}$ gives an ellipsoid that contains C . 

8.15 Minimum volume ellipsoid covering union of ellipsoids. Formulate the following problem as a convex optimization problem. Find the minimum volume ellipsoid ${\mathcal{E}\,=\,\{x\,\mid\,(x\,-$ $x_{0})^{T}A^{-1}(x-x_{0})\leq1\}$ that contains $K$ given ellipsoids 

$$
{\mathcal{E}}_{i}=\{{\boldsymbol{x}}\ |\ {\boldsymbol{x}}^{T}A_{i}{\boldsymbol{x}}+2{\boldsymbol{b}}_{i}^{T}{\boldsymbol{x}}+c_{i}\leq0\},\quad i=1,.\,.\,.\,,K
$$ 

Hint. See appendix B . 

8.16 Maximum volume rectangle inside a polyhedron. Formulate the following problem as a convex optimization problem. Find the rectangle 

$$
{\mathcal{R}}=\{x\in\mathbf{R}^{n}\mid l\preceq x\preceq u\}
$$ 

of maximum volume, enclosed in a polyhedron ${\mathcal{P}}\,=\,\{x\,\mid\,A x\,\preceq\,b\}$ . The variables are $l,u\in\mathbf{R}^{r u}$ . Your formulation should not involve an exponential number of constraints. 

# Centering 

8.17 Affine invariance of analytic center . Show that the analytic center of a set of inequalities is affine invariant. Show that it is invariant with respect to positive scaling of the inequalities. 

8.18 Analytic center and redundant inequalities. Two sets of linear inequalities that describe the same polyhedron can have diﬀerent analytic centers. Show that by adding redundant inequalities, we can make any interior point $x_{0}$ of a polyhedron 

$$
{\mathcal{P}}=\{x\in\mathbf{R}^{n}\mid A x\preceq b\}
$$ 

the a c More specifically, suppose $A\in\mathbf{R}^{m\times n}$ and $A x_{0}\prec b$ . Show that exist c $c\in\mathbf{R}^{n}$ ∈ , $\gamma\in\mathbf{R}$ ∈ , and a positive integer $q$ , such that P is the solution set of the $m+q$ inequalities 

$$
A x\preceq b,\qquad c^{T}x\leq\gamma,\qquad c^{T}x\leq\gamma,\quad\ldots,\quad c^{T}x\leq\gamma
$$ 

(where the inequality $c^{T}x\leq\gamma$ is added $q$ times), and $x_{0}$ is the analytic center of ( 8.36 ).

 8.19 Let $x_{\mathrm{ac}}$ be the analytic center of a set of linear inequalities 

$$
a_{i}^{T}x\leq b_{i},\quad i=1,.\,.\,.\,,m,
$$ 

and define $H$ as the Hessian of the logarithmic barrier function at : $x_{\mathrm{ac}}$ 

$$
H=\sum_{i=1}^{m}\frac{1}{(b_{i}-a_{i}^{T}x_{\mathrm{ac}})^{2}}a_{i}a_{i}^{T}.
$$ 

Show that the $k$ th inequality is redundant ( i.e. , it can be deleted without changing the feasible set) if 

$$
b_{k}-a_{k}^{T}x_{\mathrm{ac}}\geq m(a_{k}^{T}H^{-1}a_{k})^{1/2}.
$$ 

8.20 Ellipsoidal approximation from analytic center of linear matrix inequality. Let $C$ be the solution set of the LMI 

$$
x_{1}A_{1}+x_{2}A_{2}+\cdot\cdot\cdot+x_{n}A_{n}\preceq B,
$$ 

where $A_{i},B\in\mathbf{S}^{m}$ , and let $x_{\mathrm{ac}}$ be its analytic center. Show that 

$$
\begin{array}{r}{\mathcal{E}_{\mathrm{inner}}\subseteq C\subseteq\mathcal{E}_{\mathrm{outer}},}\end{array}
$$ 

where 

$$
\begin{array}{r l r}{\mathcal{E}_{\mathrm{inner}}}&{=}&{\{x\mid(x-x_{\mathrm{ac}})^{T}H(x-x_{\mathrm{ac}})\leq1\},}\\ {\mathcal{E}_{\mathrm{outer}}}&{=}&{\{x\mid(x-x_{\mathrm{ac}})^{T}H(x-x_{\mathrm{ac}})\leq m(m-1)\},}\end{array}
$$ 

and $H$ is the Hessian of the logarithmic barrier function 

$$
-\log\operatorname*{det}(B-x_{1}A_{1}-x_{2}A_{2}-\cdot\cdot\cdot-x_{n}A_{n})
$$ 

evaluated at $x_{\mathrm{ac}}$ . 

8.21 [ BYT99 ] Maximum likelihood interpretation of analytic center. We use the linear mea- surement model of page 352 , 

$$
y=A x+v,
$$ 

where $A\in\mathbf{R}^{m\times n}$ . We assume the noise components $v_{i}$ are IID with support $[-1,1]$ . The set of parameters x consistent with the measurements $y\in\mathbf{R}^{m}$ is the polyhedron defined by the linear inequalities 

$$
-\mathbf{1}+y\preceq A x\preceq\mathbf{1}+y.
$$ 

Suppose the probability density function of $v_{i}$ has the form 

$$
p(v)=\left\{\begin{array}{l l}{\alpha_{r}(1-v^{2})^{r}}&{-1\leq v\leq1}\\ {0}&{\mathrm{otherwise},}\end{array}\right.
$$ 

where $r\geq1$ and $\alpha_{r}>0$ . Show that the maximum likelihood estimate of $x$ is the analytic center of ( 8.37 ). 

8.22 Center of gravity. The center of gravity of a set $C\subseteq\mathbf{R}^{n}$ with nonempty interior is defined as 

$$
x_{\mathrm{cg}}={\frac{\int_{C}u\;d u}{\int_{C}1\;d u}}.
$$ 

The center of gravity is affine invariant, and (clearly) a function of the set $C$ , and not its particular description. Unlike the centers described in the chapter, however, it is very difficult to compute the center of gravity, except in simple cases ( e.g. , ellipsoids, balls, simplexes). 

Show that the center of gravity $x_{\mathrm{cg}}$ is the minimizer of the convex function 

$$
f(x)=\int_{C}\left\|u-x\right\|_{2}^{2}\,d u.
$$ 

# Classification 

8.23 Robust linear discrimination. Consider the robust linear discrimination problem given in ( 8.23 ). 

(a) Show that the optimal value $t^{\star}$ is positive if and only if the two sets of points can be linearly separated. When the two sets of points can be linearly separated, show that the inequality $||a||_{2}\leq1$ is tight, i.e. , we have $\|a^{\star}\|_{2}=1$ , for the optimal $a^{\star}$ . 

(b) Using the change of variables ${\tilde{a}}\,=\,a/t$ , $\dot{b}=\dot{b}/t$ , prove that the problem ( 8.23 ) is equivalent to the QP 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\|\tilde{\boldsymbol{a}}\|_{2}}\\ {\mathrm{subject~to}}&{\tilde{\boldsymbol{a}}^{T}\boldsymbol{x}_{i}-\tilde{\boldsymbol{b}}\geq1,\quad i=1,.\,.\,,N}\\ &{\tilde{\boldsymbol{a}}^{T}\boldsymbol{y}_{i}-\tilde{\boldsymbol{b}}\leq-1,\quad i=1,.\,.\,,M.}\end{array}
$$ 

8.24 Linear discrimination maximally robust to weight errors. Suppose we are given two sets of points $\{x_{1},.\,.\,.\,,x_{N}\}$ and and $\{y_{1},.\,.\,.\,,y_{M}\}$ in $\mathbf{R}^{n}$ that can be linearly separated. In § 8.6.1 we showed how to find the affine function that discriminates the sets, and gives the largest gap in function values. We can also consider robustness with respect to changes in the vector $a$ , which is sometimes called the weight vector . For a given $a$ and $b$ for which $f(x)=a^{T}x-b$ separates the two sets, we define the weight error margin as the norm of the smallest $u\in\mathbf{R}^{n}$ such that the affine function $(a+u)^{T}x-b$ no longer separates the two sets of points. In other words, the weight error margin is the maximum $\rho$ such that 

$$
\begin{array}{r}{\left(a+u\right)^{T}x_{i}\geq b,\quad i=1,\ldots,N,\qquad\left(a+u\right)^{T}y_{j}\leq b,\quad i=1,\ldots,M,}\end{array}
$$ 

holds for all $u$ with $||u||_{2}\leq\rho$ . 

Show how to find $a$ and b that maximize the weight error margin, subject to the normal- ization constraint $\|a\|_{2}\leq1$ . 

8.25 Mos ting ellipsoid. We are given tw sets of vectors $x_{1},.\,.\,.\,,x_{N}\,\in\,\mathbf{R}^{n}$ , n and $y_{1},\cdot\cdot\cdot,y_{M}\,\in\,\mathbf{R}^{n}$ ∈ , and wish to find the ellipsoid E with minim c , mini d of the defining matrix) that satisfies x ∈E , i $i\,=\,1,.\,.\,.\,,N$ , and $y_{i}\not\in\operatorname{int}\mathcal{E}$ ̸∈ E , $i=1,\dots,M$ . Formulate this as a convex optimization problem. 

# Placement and ﬂoor planning 

8.26 Quadratic placement. We consider a placement problem in $\mathbf{R}^{2}$ , defined by an undirected graph $\mathcal{A}$ with $N$ nodes, and with quadratic costs: 

$$
\begin{array}{r l}{\underset{\operatorname{minimize}}{\operatorname{minimize}}}&{{}\sum_{(i,j)\in\mathcal{A}}\|x_{i}-x_{j}\|_{2}^{2}.}\end{array}
$$ 

The variables are the positions $x_{i}\in\mathbf{R}^{2}$ $i=1,\cdot\cdot\cdot,M$ . The positions $x_{i}$ , $i=M+1,.\,.\,.\,,N$ are given. We define two vectors $u,v\in\mathbf{R}^{M}$ ∈ by 

$$
u=(x_{11},x_{21},.\,.\,.\,,x_{M1}),\qquad v=(x_{12},x_{22},.\,.\,.\,,x_{M2}
$$ 

containing the first and second components, respectively, of the free nodes. Show that $u$ and $v$ can be found by solving two sets of linear equations, 

$$
C u=d_{1},\qquad C v=d_{2},
$$ 

where $C\in\mathbf{S}^{M}$ . Give a simple expression for the coefficients of $C$ in terms of the graph $\mathcal{A}$ .

 8.27 Problems with minimum distance constraints. We consider a problem with variables $x_{1},\allowbreak\cdot\cdot,x_{N}\in\mathbf{R}^{k}$ . The objective, $f_{0}(x_{1},\dots,x_{N})$ , is convex, and the constraints 

$$
f_{i}(x_{1},.\,.\,.\,,x_{N})\le0,\quad i=1,.\,.\,.\,,m,
$$ 

are convex ( i.e. , the functions $f_{i}\,:\,\mathbf{R}^{N k}\,\rightarrow\,\mathbf{R}$ are convex). In addition, we have the minimum distance constraints 

$$
\|x_{i}-x_{j}\|_{2}\ge D_{\operatorname*{min}},\quad i\ne j,\ \ i,j=1,.\,.\,,N.
$$ 

In general, this is a hard nonconvex problem. 

Following the approach taken in ﬂoorplanning, we can form a convex restriction of the problem, i.e. , a problem which is convex, but has a smaller feasible set. (Solving the restricted problem is therefore easy, and any solution is guaranteed to be feasible for the nonconvex problem.) Let $a_{i j}\in\mathbf{R}^{k}$ , for $i<j$ , $i,j=1,\ldots,N$ , satisfy $\|a_{i j}\|_{2}=1$ . Show that the restricted problem 

$$
\begin{array}{l l}{\mathrm{minimize~}}&{f_{0}\big(x_{1},\ldots,x_{N}\big)}\\ {\mathrm{subject~to}}&{f_{i}\big(x_{1},\ldots,x_{N}\big)\le0,\quad i=1,\ldots,m}\\ &{a_{i j}^{T}\big(x_{i}-x_{j}\big)\ge D_{\operatorname*{min}},\quad i<j,\ i,j=1,\ldots,N,}\end{array}
$$ 

is convex, and that every feasible point satisfies the minimum distance constraint. 

Remark. There are many good heuristics for choosing the directions $a_{i j}$ . One simple one starts with an approximate solution $\hat{x}_{1},\dots,\hat{x}_{N}$ (that need not satisfy the minimum distance constraints). We then set $a_{i j}=(\hat{x}_{i}-\hat{x}_{j})/\|\hat{x}_{i}-\hat{x}_{j}\|_{2}$ ∥ ∥ . 

# Miscellaneous problems 

8.28 Let $\mathcal{P}_{1}$ and $\mathcal{P}_{2}$ be two polyhedra described as 

$$
\mathcal{P}_{1}=\{x\mid A x\preceq b\}\,,\qquad\mathcal{P}_{2}=\{x\mid-{\bf1}\preceq C x\preceq{\bf1}\}
$$ 

where $A\in\mathbf{R}^{m\times n}$ , $C\in\mathbf{R}^{p\times n}$ , and $b\in\mathbf{R}^{m}$ . The polyhedron $\mathcal{P}_{2}$ is symmetric about the n origin. For $t\geq0$ and x $x_{c}\in\mathbf{R}^{n}$ , we use the notation $t\mathcal{P}_{2}+x_{c}$ to denote the polyhedron 

$$
t\mathcal{P}_{2}+x_{c}=\{t x+x_{c}\mid x\in\mathcal{P}_{2}\},
$$ 

which is obtained by first scaling $\mathcal{P}_{2}$ by a factor $t$ about the origin, and then translating its center to $x_{c}$ . 

Show how to solve the following two problems, via an LP, or a set of LPs. 

(a) Find the largest polyhedron $t\mathcal{P}_{2}+x_{c}$ enclosed in $\mathcal{P}_{1}$ , i.e. 

$$
\begin{array}{l r c l}{\mathrm{maximize}}&{t}\\ {\mathrm{subject~to}}&{t\mathcal{P}_{2}+x_{c}\subseteq\mathcal{P}_{1}}\\ &{t\geq0.}\end{array}
$$ 

(b) Find the smallest polyhedron $t\mathcal{P}_{2}+x_{c}$ containing $\mathcal{P}_{1}$ , i.e. , 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ t}\\ &{{\mathrm{subject~to}}\quad{\mathcal{P}}_{1}\subseteq t{\mathcal{P}}_{2}+x_{c}}\\ &{\quad t\geq0.}\end{array}}
$$ 

In both problems the variables are $t\in\mathbf{R}$ and $x_{c}\in\mathbf{R}^{n}

$ 

8.29 olyhedral approximations. Let ${\mathcal{P}}\,=\,\{x\,\in\,\mathbf{R}^{n}\;\mid\;A x\,\preceq\,b\}$ be a yhedron, and n $C\subseteq\mathbf{R}^{n}$ ⊆ a given set (not necessarily convex). Use the support function $S_{C}$ to formulate the following problem as an LP: 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad t}\\ &{{\mathrm{subject~to}}\quad C\subseteq t{\mathcal{P}}+x}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ {\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ &{\quad}\\ {\quad}\\ &{\end{\quad}\\ }\\ &{\end{\quad}\\ }\\ &{\end{\quad}\\ }\\ &{\end{\quad}\\ }\\ }\\ &
$$ 

Here $t\mathcal{P}+x=\{t u+x\mid u\in\mathcal{P}\}$ , the pol on $\mathcal{P}$ y a factor of $t$ about the origin, n and translated by x . The variables are t $t\in\mathbf{R}$ ∈ and x $x\in\mathbf{R}^{n}$ ∈ . 

8.30 Interpolation with piecewise-arc curve. A sequence of points $a_{1},.\,.\,.\,,a_{n}\in\mathbf{R}^{2}$ is given. We construct a curve that passes through these points, in order, and is an arc ( i.e. , part of a circle) or line segment (which we think of as an arc of infinite radius) between consecutive points. Many arcs connect $a_{i}$ and $\boldsymbol{a}_{i+1}$ ; we parameterize these arcs by giving the angle $\theta_{i}\in\left(-\pi,\pi\right)$ bet en its gent at $a_{i}$ and the line segm $[a_{i},a_{i+1}]$ s, $\theta_{i}=0$ means the arc between a and a $\boldsymbol{a}_{i+1}$ is in fact the line segment [ $[a_{i},a_{i+1}]$ ]; θ $\theta_{i}=\pi/2$ 2 means the arc between $a_{i}$ and $\boldsymbol{a}_{i+1}$ is a half-circle (above the linear segment $[a_{1},a_{2}])$ ; $\theta_{i}=-\pi/2$ means the arc between $a_{i}$ and $a_{i+1}$ is a half-circle (below the linear segment $[a_{1},a_{2}]$ ). This is illustrated below. 

![](images/3804dd22521c63c712f973a8e2af088eeea4b09e5d28b320430aedfa69cc876d.jpg) 

Our curve is completely specified by the angles $\theta_{1},\ldots,\theta_{n}$ , which can be chosen in the interval $(-\pi,\pi)$ The choice of $\theta_{i}$ aﬀects several properties of the curve, for example, its total arc length L , or the joint angle discontinuities , which can be described as follows. At each point $a_{i}$ , $i=2,\ldots,n-1$ , two arcs meet, one coming from the previous point and one going to the next point. If the tangents to these arcs exactly oppose each other, so the curve is diﬀerentiable at $a_{i}$ , we say there is no joint angle discontinuity at $a_{i}$ . In general, we define the join iscontinuity at $a_{i}$ as $|\theta_{i-1}\!+\!\theta_{i}\!+\!\psi_{i}|$ , w $\psi_{i}$ n the line segment [ $[a_{i},a_{i+1}]$ ] and the line segm t [ $[a_{i-1},a_{i}]$ ], i.e. , $\psi_{i}=\angle\big(a_{i}-a_{i+1},a_{i-1}-a_{i}\big)$ − − ). This is shown below. Note that the angles ψ are known (since the a are known). 

We define the total joint angle discontinuity as 

$$
D=\sum_{i=2}^{n}|\theta_{i-1}+\theta_{i}+\psi_{i}|.
$$ 

Formulate the problem of minimizing total arc length length $L$ , and total joint angle discontinuity $D$ , as a bi-criterion convex optimization problem. Explain how you would find the extreme points on the optimal trade-oﬀcurve. 

# Part III 

# Algorithms 

# Chapter 9 

# Unconstrained minimization 

# 9.1 Unconstrained minimization problems 

In this chapter we discuss methods for solving the unconstrained optimization problem 

$$
{\mathrm{minimize~}}\quad f(x)
$$ 

where $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex and twice continuously diﬀerentiable (which implies that $\mathbf{dom}\ f$ is open). We will assume that the problem is solvable, i.e. , there exists an optimal point $x^{\star}$ . (More precisely, the assumptions later in the chapter will imply that $x^{\star}$ exists and is unique.) We denote the optimal value, $\textstyle\operatorname*{inf}_{x}f(x)\;=$ $f(x^{\star})$ , as $p^{\star}$ . 

Since $f$ is diﬀerentiable and convex, a necessary and sufficient condition for a point $x^{\star}$ to be optimal is 

$$
\nabla f(x^{\star})=0
$$ 

(see § 4.2.3 ). Thus, solving the unconstrained minimization problem ( 9.1 ) is the same as finding a solution of ( 9.2 ), which is a set of $n$ equations in the $n$ variables $x_{1},\dots,x_{n}$ . In a few special cases, we can find a solution to the problem ( 9.1 ) by analytically solving the optimality equation ( 9.2 ), but usually the problem must be solved by an iterative algorithm. By this we mean an algorithm that computes a sequence of points $x^{(0)},\ x^{(1)},.\ .\ .\in\ \mathbf{dom}\ f$ with $f(x^{(k)})\to p^{\star}$ as $k\rightarrow\infty$ . Such a sequence of points is called a minimizing sequence for the problem ( 9.1 ). The algorithm is terminated when $f(x^{(k)})\,-\,p^{\star}\,\leq\,\epsilon$ , where $\epsilon\,>\,0$ is some specified tolerance. 

# Initial point and sublevel set 

The methods described in this chapter require a suitable starting point $x^{(0)}$ . The starting point must lie in $\mathbf{dom}\ f$ , and in addition the sublevel set 

$$
S=\{x\in\mathbf{dom}\ f\ |\ f(x)\leq f(x^{(0)})\}
$$ 

must be closed. This condition is satisfied for all $x^{(0)}\in\mathbf{dom}\,f$ if the function $f$ is closed , i.e. , all its sublevel sets are closed (see § A.3.3 ). Continuous functions with $\mathbf{dom}\,f\,=\,\mathbf{R}^{n}$ are closed, so if $\mathbf{dom}\,f\,=\,\mathbf{R}^{n}$ , the initial sublevel set condition is satisfied by any $x^{(0)}$ . Another important class of closed functions are continuous functions with open domains, for which $f(x)$ tends to infinity as $x$ approaches bd dom $f$ . 

# 9.1.1 Examples 

# Quadratic minimization and least-squares 

The general convex quadratic minimization problem has the form 

$$
\begin{array}{r l}{\mathrm{minimize~}}&{{}(1/2)x^{T}P x+q^{T}x+r,}\end{array}
$$ 

where $P\in\mathbf{S}_{+}^{n}$ , $q\in\mathbf{R}^{n}$ , and $r\in\mathbf{R}$ . This problem can be solved via the optimality conditions, $P x^{\star}+q=0$ , which is a set of linear equations. When $P\succ0$ , there is a unique solution, $x^{\star}=-P^{-1}q$ . In the more general case when $P$ is not positive definite, any solution of $P x^{\star}\;=\;-q$ is optimal for ( 9.4 ); if $P x^{\star}\;=\;-q$ does not have a solution, then the problem ( 9.4 ) is unbounded below (see exercise 9.1 ). Our ability to analytically solve the quadratic minimization problem ( 9.4 ) is the basis for Newton’s method, a powerful method for unconstrained minimization described in 9.5 . 

One special case of the quadratic minimization problem that arises very fre- quently is the least-squares problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}=x^{T}(A^{T}A)x-2(A^{T}b)^{T}x+b^{T}b.
$$ 

The optimality conditions 

$$
A^{T}A x^{\star}=A^{T}b
$$ 

are called the normal equations of the least-squares problem. 

# Unconstrained geometric programming 

As a second example, we consider an unconstrained geometric program in convex form, 

$$
{\mathrm{minimize}}\quad f(x)=\log\left(\sum_{i=1}^{m}\exp(a_{i}^{T}x+b_{i})\right).
$$ 

The optimality condition is 

$$
\nabla f(x^{\star})=\frac{1}{\sum_{j=1}^{m}\exp(a_{j}^{T}x^{\star}+b_{j})}\sum_{i=1}^{m}\exp(a_{i}^{T}x^{\star}+b_{i})a_{i}=0,
$$ 

which in general has no analytical solution, so here we must resort to an iterative algorithm. For this problem, $\mathbf{dom}\,f\,=\,\mathbf{R}^{n}$ , so any point can be chosen as the initial point $x^{(0)}$ . 

# Analytic center of linear inequalities 

We consider the optimization problem 

$$
\begin{array}{r}{\operatornamewithlimits{m i n i m i z e}\quad f(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),}\end{array}
$$ 

where the domain of $f$ is the open set 

$$
\mathbf{dom}\,f=\{x\mid a_{i}^{T}x<b_{i},\ i=1,\ldots,m\}.
$$ 

The objective function $f$ in this problem is called the logarithmic barrier for the inequalities $a_{i}^{T}{\boldsymbol{x}}\ \leq\ b_{i}$ ≤ . The solution of ( 9.5 ), if it exists, is called the analytic $x^{(0)}$ center of the inequalities. The initial point must satisfy the strict inequalities $a_{i}^{T}x^{(0)}<b_{i}$ , $i=1,\ldots,m$ . Since $f$ is closed, the sublevel set $S$ for any such point is closed. 

# Analytic center of a linear matrix inequality 

A closely related problem is 

$$
{\mathrm{minimize}}\quad f(x)=\log\operatorname*{det}F(x)^{-1}
$$ 

where $F:\mathbf{R}^{n}\rightarrow\mathbf{S}^{p}$ is affine, i.e. , 

$$
F(x)=F_{0}+x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n},
$$ 

with $F_{i}\in\mathbf{S}^{p}$ . Here the domain of $f$ is 

$$
\mathbf{dom}\,f=\{x\mid F(x)\succ0\}.
$$ 

The objective function $f$ is called the logarithmic barrier for the linear matrix inequality $F(x)\succeq0$ , and the solution (if it exis is called the analytic center of the linear matrix inequality. The initial point x $x^{(0)}$ must satisfy the strict linear matrix inequality $F(x^{(0)})\succ0$ . As in the previous example, the sublevel set of any such point will be closed, since $f$ is closed. 

# 9.1.2 Strong convexity and implications 

In much of this chapter (with he exception of § 9.6 ) we assume th objective function is strongly convex on S , which means that there exists an m > 0 such that 

$$
\nabla^{2}f(x)\succeq m I
$$ 

for all $x\in S$ . Strong convexity has several interesting consequences. For $x,y\in S$ we have 

$$
f(y)=f(x)+\nabla f(x)^{T}(y-x)+{\frac{1}{2}}(y-x)^{T}\nabla^{2}f(z)(y-x)
$$ 

for some $\mathcal{Z}$ on the line segment $[x,y]$ . By the strong convexity assumption ( 9.7 ), the last term on the righthand side is at least $(m/2)\|y-x\|_{2}^{2}$ , so we have the inequality 2 

$$
f(y)\geq f(x)+\nabla f(x)^{T}(y-x)+{\frac{m}{2}}||y-x||_{2}^{2}
$$ 

for all $x$ and $y$ in $S$ . When $m=0$ , we recover the basic inequality characterizing convexity; for $m\,>\,0$ we obtain a better lower bound on $f(y)$ than follows from convexity alone. 

We will first show that the inequality ( 9.8 ) can be used to bound $f(x)-p^{\star}$ , which is the suboptimality of the point $x$ , in terms of $||\nabla f(x)||_{2}$ . The righthand side of ( 9.8 ) is a convex quadratic function of $y$ (for fixed $x$ ). Setting the gradient with respect to $y$ equal to zero, we find that $\tilde{y}=x-(1/m)\nabla f(x)$ − ∇ ) minimizes the righthand side. Therefore we have 

$$
\begin{array}{r c l}{f(y)}&{\geq}&{f(x)+\nabla f(x)^{T}(y-x)+\displaystyle\frac{m}{2}\|y-x\|_{2}^{2}}\\ &{\geq}&{f(x)+\nabla f(x)^{T}(\tilde{y}-x)+\displaystyle\frac{m}{2}\|\tilde{y}-x\|_{2}^{2}}\\ &{=}&{f(x)-\displaystyle\frac{1}{2m}\|\nabla f(x)\|_{2}^{2}.}\end{array}
$$ 

Since this holds for any $y\in S$ , we have 

$$
p^{\star}\geq f(x)-\frac{1}{2m}\|\nabla f(x)\|_{2}^{2}.
$$ 

This inequality shows that if the gradient is small at a point, then the point is nearly optimal. The inequality ( 9.9 ) can also be interpreted as a condition for suboptimality which generalizes the optimality condition ( 9.2 ): 

$$
\|\nabla f(x)\|_{2}\leq(2m\epsilon)^{1/2}\Longrightarrow f(x)-p^{\star}\leq\epsilon.
$$ 

We can also derive a bound on $||\boldsymbol{x}\mathrm{~-~}\boldsymbol{x}^{\star}||_{2}$ , the distance between $x$ and any optimal point $x^{\star}$ , in terms of $||\nabla f(x)||_{2}$ : 

$$
\|\boldsymbol{x}-\boldsymbol{x}^{\star}\|_{2}\leq\frac{2}{m}\|\nabla f(\boldsymbol{x})\|_{2}.
$$ 

To see this, we apply ( 9.8 ) with $y=x^{\star}$ to obtain 

$$
\begin{array}{r c l}{p^{\star}=f(x^{\star})}&{\geq}&{f(x)+\nabla f(x)^{T}(x^{\star}-x)+\displaystyle\frac{m}{2}\|x^{\star}-x\|_{2}^{2}}\\ &{\geq}&{f(x)-\|\nabla f(x)\|_{2}\|x^{\star}-x\|_{2}+\displaystyle\frac{m}{2}\|x^{\star}-x\|_{2}^{2},}\end{array}
$$ 

where we use the Cauchy-Schwarz inequality in the second inequality. Since $p^{\star}\leq$ $f(x)$ , we must have 

$$
-\|\nabla f(x)\|_{2}\;\|x^{\star}-x\|_{2}+\frac{m}{2}\|x^{\star}-x\|_{2}^{2}\leq0,
$$ 

from which ( 9.11 ) follows. One consequence of ( 9.11 ) is that the optimal point $x^{\star}$ is unique. 

# Upper bound on $\nabla^{2}f(x)$ 

The inequality ( 9.8 ) implies that the sublevel sets contained in $S$ are bounded, so in particular, $S$ is bounded. T refore the maximum ei envalue of $\nabla^{2}f(x)$ , which is a continuous function of $x$ on S , is bounded above on S , i.e. , there exists a constant $M$ such that 

$$
\nabla^{2}f(x)\preceq M I
$$ 

for all $x\in S$ . This upper bound on the Hessian implies for any $x,\ y\in S$ , 

$$
f(\boldsymbol{y})\leq f(\boldsymbol{x})+\nabla f(\boldsymbol{x})^{T}(\boldsymbol{y}-\boldsymbol{x})+\frac{M}{2}\|\boldsymbol{y}-\boldsymbol{x}\|_{2}^{2},
$$ 

which is analogous to ( 9.8 ). Minimizing each side over $y$ yields 

$$
p^{\star}\leq f(x)-\frac{1}{2M}\|\nabla f(x)\|_{2}^{2},
$$ 

the counterpart of ( 9.9 ). 

# Condition number of sublevel sets 

From the strong convexity inequality ( 9.7 ) and the inequality ( 9.12 ), we have 

$$
m I\preceq\nabla^{2}f(x)\preceq M I
$$ 

for all $x\,\in\,S$ . The ratio $\kappa\,=\,M/m$ is thus an upper bound on the condition number of the matrix $\nabla^{2}f(x)$ , i.e. , the ratio of its largest eigenvalue to its smallest eigenvalue. We can also give a geometric interpretation of ( 9.15 ) in terms of the sublevel sets of $f$ . 

We define the width of a convex set $C\subseteq\mathbf{R}^{n}$ , in the direction $q$ , where $\|q\|_{2}=1$ , as 

$$
W(C,q)=\operatorname*{sup}_{z\in C}q^{T}z-\operatorname*{inf}_{z\in C}q^{T}z.
$$ 

The minimum width and maximum width of $C$ are given by 

$$
W_{\operatorname*{min}}=\operatorname*{inf}_{\|q\|_{2}=1}W(C,q),\qquad W_{\operatorname*{max}}=\operatorname*{sup}_{\|q\|_{2}=1}W(C,q).
$$ 

The condition number of the convex set $C$ is defined as 

$$
\mathbf{cond}(C)=\frac{W_{\mathrm{max}}^{2}}{W_{\mathrm{min}}^{2}},
$$ 

i.e. , the square of the ratio of its maximum width to its minimum width. The condition number of $C$ gives a measure of its anisotropy or eccentricity . If the condition number of a set $C$ is small (say, near one) it means that the set has approximately the same width in all directions, i.e. , it is nearly spherical. If the condition number is large, it means that the set is far wider in some directions than in others. 

Example 9.1 Condition number of an ellipsoid. Let $\mathcal{E}$ be the ellipsoid 

$$
{\mathcal{E}}=\{x\mid(x-x_{0})^{T}A^{-1}(x-x_{0})\leq1\},
$$ 

where $A\in\mathbf{S}_{++}^{n}$ . The width of $\mathcal{E}$ in the direction $q$ is 

$$
\begin{array}{l l l}{\displaystyle\operatorname*{sup}_{z\in\mathcal{E}}q^{T}z-\operatorname*{inf}_{z\in\mathcal{E}}q^{T}z}&{=}&{(\|A^{1/2}q\|_{2}+q^{T}x_{0})-(-\|A^{1/2}q\|_{2}+q^{T}x_{0})}\\ {\displaystyle}&{=}&{2\|A^{1/2}q\|_{2}.}\end{array}
$$ 

It follows that its minimum and maximum width are 

$$
W_{\mathrm{min}}=2\lambda_{\mathrm{min}}(A)^{1/2},\qquad W_{\mathrm{max}}=2\lambda_{\mathrm{max}}(A)^{1/2},
$$ 

and its condition number is 

$$
\mathbf{cond}(\mathcal{E})=\frac{\lambda_{\mathrm{max}}(A)}{\lambda_{\mathrm{min}}(A)}=\kappa(A),
$$ 

where $\kappa(A)$ denotes the condition number of the matrix $A$ , i.e. , the ratio of its maximum singular value to its minimum singular value. Thus the condition number of the ellipsoid $\mathcal{E}$ is the same as the condition number of the matrix $A$ that defines it. 

Now suppose $f$ satisfies $m I\,\preceq\,\nabla^{2}f(x)\,\preceq\,M I$ for all $x\,\in\,S$ . We will derive condition number of the $\alpha$ -sublev $C_{\alpha}=\{x\mid f(x)\leq\alpha\}$ , where $p^{\star}<\alpha\leq f(x^{(0)})$ ≤ ). Applying ( 9.13 ) and ( 9.8 ) with x $x=x^{\star}$ , we have 

$$
p^{\star}+(M/2)\|y-x^{\star}\|_{2}^{2}\geq f(y)\geq p^{\star}+(m/2)\|y-x^{\star}\|_{2}^{2}.
$$ 

This implies that $B_{\mathrm{inner}}\subseteq C_{\alpha}\subseteq B_{\mathrm{outer}}$ where 

$$
\begin{array}{r l r}{B_{\mathrm{inner}}}&{=}&{\{y\mid\|y-x^{\star}\|_{2}\leq(2(\alpha-p^{\star})/M)^{1/2}\},}\\ {B_{\mathrm{outer}}}&{=}&{\{y\mid\|y-x^{\star}\|_{2}\leq(2(\alpha-p^{\star})/m)^{1/2}\}.}\end{array}
$$ 

In other words, the $\alpha$ -sublevel set contains $B_{\mathrm{inner}}$ , and is contained in $B_{\mathrm{outer}}$ , which are balls with radii 

$$
\begin{array}{r}{(2(\alpha-p^{\star})/M)^{1/2},\qquad(2(\alpha-p^{\star})/m)^{1/2},}\end{array}
$$ 

respectively. The ratio of the radii squared gives an upper bound on the condition number of $C_{\alpha}$ : 

$$
\mathbf{cond}(C_{\alpha})\leq\frac{M}{m}.
$$ 

We can also give a geometric interpretation of the condition number $\kappa\bigl(\nabla^{2}f(x^{\star})\bigr)$ of the Hessian at the optimum. From the Taylor series expansion of $f$ around x $x^{\star}$ , 

$$
f(y)\approx p^{\star}+\frac{1}{2}(y-x^{\star})^{T}\nabla^{2}f(x^{\star})(y-x^{\star}),
$$ 

we see that, for $\alpha$ close to $p^{\star}$ , 

$$
C_{\alpha}\approx\{y\mid(y-x^{\star})^{T}\nabla^{2}f(x^{\star})(y-x^{\star})\leq2(\alpha-p^{\star})\},
$$ 

i.e. , the sublevel set is well approximated by an ellipsoid with center $x^{\star}$ . Therefore 

$$
\operatorname*{lim}_{\alpha\to p^{\star}}\operatorname{cond}(C_{\alpha})=\kappa(\nabla^{2}f(x^{\star})).
$$ 

We will see that the condition number of the sublevel sets of $f$ (which is bounded by $M/m$ ) has a strong eﬀect on the efficiency of some common methods for uncon- strained minimization. 

# The strong convexity constants 

It must be kept in mind that the constants $m$ and $M$ are known only in rare cases, so the inequality ( 9.10 ) cannot be used as a practical stopping criterion. It can be considered a conceptual stopping criterion; it shows that if the gradient of $f$ at $x$ is small enough, then the diﬀerence between $f(x)$ and $p^{\star}$ is small. If we terminate an algorithm when $\|\nabla f(x^{(k)})\|_{2}\leq\eta$ , wher $\eta$ all enough to be (very likely) smaller than ( $(m\epsilon)^{1/2}$ , then we have $f(x^{(k)})-p^{\star}\leq\epsilon$ (very likely). 

In the following sections we give convergence proofs for algorithms, which in- clude bounds on the number of iterations required before $f(x^{(k)})-p^{\star}\leq\epsilon$ , where $\epsilon$ is some positive tolerance. Many of these bounds involve the (usually unknown) constants $m$ and $M$ , so the same comments apply. These results are at least con- ceptually useful; they establish that the algorithm converges, even if the bound on the number of iterations required to reach a given accuracy depends on constants that are unknown. 

We will encounter one important exception to this situation. In § 9.6 we will study a special class of convex functions, called self-concordant , for which we can provide a complete convergence analysis (for Newton’s method) that does not de- pend on any unknown constants. 

# 9.2 Descent methods 

The algorithms described in this chapter produce a minimizing sequence $x^{(k)}$ , $k=$ $1,\cdot\cdot\cdot$ , where 

$$
{x^{(k+1)}}={x^{(k)}}+{t^{(k)}}\Delta{x^{(k)}}
$$ 

and $t^{(k)}>0$ (except when $x^{(k)}$ is optimal). Here the concatenated symbols $\Delta$ and $x$ that form $\Delta x$ are to be read as a single entity, a vector in $\mathbf{R}^{n}$ called the step or search direction (even though it need not have unit norm), and $k=0,1,\hdots$ denotes the iterat on number. The scalar $t^{(k)}\geq0$ is called the step size or step length at iteration k (even though it is not equal to $\|x^{(k+1)}\,-\,x^{(k)}\|$ unless $\|\Delta x^{(k)}\|\,=\,1$ ). The terms ‘search step’ and ‘scale factor’ are more accurate, but ‘search direction’ and ‘step length’ are the ones widely used. When we focus on one iteration of an algorithm, we sometimes drop the superscripts and use the lighter notation $x^{+}=x+t\Delta x$ , or $x:=x+t\Delta x$ , in place of $x^{(k+1)}=x^{(k)}+t^{(k)}\Delta x^{(k)}$ . 

All the methods we study are descent methods , which means that 

$$
f(x^{(k+1)})<f(x^{(k)}),
$$ 

except when $x^{(k)}$ is optimal. This implie $k$ we have $x^{(k)}\in S$ , the initial sublevel set, and in particular we have $x^{(k)}\,\in\,\mathbf{dom}\,f$ ∈ . From convexity we know that $\nabla f({\boldsymbol{x}}^{(k)})^{T}({\boldsymbol{y}}-{\boldsymbol{x}}^{(k)})\geq0$ implies $f(y)\geq f(x^{(k)})$ , so the search direction in a descent method must satisfy 

$$
\nabla f(x^{(k)})^{T}\Delta x^{(k)}<0,
$$ 

i.e. , it must make an acute angle with the negative gradient. We call such a direction a descent direction (for $f$ , at $x^{(k)}$ ). 

The outline of a general descent method is as follows. It alternates between two steps: determining a descent direction $\Delta x$ , and the selection of a step size $t$ . 

Algorithm 9.1 General descent method. 

given a starting point $x\in\mathbf{dom}\ f$ . repeat 

1. Determine a descent direction $\Delta x$ . 2. Line search. Choose a step size $t>0$ . 3. Update. $x:=x+t\Delta x$ . until stopping criterion is satisfied. 

The second step is called the line search since selection of the step size $t$ deter- mines where along the line $\{x+t\Delta x\mid t\in{\bf R}_{+}\}$ the next iterate will be. (A more accurate term might be ray search. ) 

A practical descent method has the same general structure, but might be or- ganized diﬀerently. For example, the stopping criterion is often checked while, or immediately after, the descent direction $\Delta x$ is computed. The stopping criterion is often of the form $\|\nabla f(x)\|_{2}\leq\eta$ , where $\eta$ is small and positive, as suggested by the suboptimality condition ( 9.9 ). 

# Exact line search 

One line search method sometimes used in practice is exact line search , in which $t$ is chosen to minimize $f$ along the ray $\{x+t\Delta x\mid t\geq0\}$ : 

$$
\begin{array}{r}{t=\operatorname*{argmin}_{s\geq0}\ f(x+s\Delta x).}\end{array}
$$ 

An exact line search is used when the cost of the minimization problem with one variable, required in ( 9.16 ), is low compared to the cost of computing the search direction itself. In some special cases the minimizer along the ray can be found an- alytically, and in others it can be computed efficiently. (This is discussed in § 9.7.1 .) 

# Backtracking line search 

Most line searches used in practice are inexact : the step length is chosen to ap- proximately minimize $f$ along the ray $\{x+t\Delta x\mid t\geq0\}$ , or even to just reduce $f$ ‘enough’. Many inexact line search methods have been proposed. One inexact line search method that is very simple and quite eﬀective is called backtracking line search. It depends on two constants $\alpha$ , $\beta$ with $0<\alpha<0.5$ , $0<\beta<1$ . 

Algorithm 9.2 Backtracking line search. 

given a descent direction $\Delta x$ for $f$ at $x\in\mathbf{dom}\ f$ , $\alpha\in(0,0.5)$ , $\beta\in(0,1)$ . $t:=1$ . 

![](images/e2d63cd0c223d3f1261cc57e99e4975192c1bef207047d97138bc0253d23d4e9.jpg) 
Figure 9.1 Backtracking line search. The curve shows $f$ , restricted to the line over which we search. The lower dashed line shows the linear extrapolation of $f$ , and the upper dashed line has a slope a factor of $\alpha$ smaller. The backtracking condition is that $f$ lies below the upper dashed line, i.e. , $0\leq$ $t\leq t_{0}$ . 

The line search is called backtracking because it starts with unit step size and then reduces it by the fact $\beta$ until the stopping condition $f(x+t\Delta x)\leq f(x)+$ $\alpha t\nabla f(x)^{T}\Delta x$ hol s. Since ∆ x is a descent direction, we have $\nabla f(x)^{T}\Delta x<0$ , so for small enough t we have 

$$
f(x+t\Delta x)\approx f(x)+t\nabla f(x)^{T}\Delta x<f(x)+\alpha t\nabla f(x)^{T}\Delta x,
$$ 

which shows that the backtracking line search eventually terminates. The constant $\alpha$ can be interpreted as the fraction of the decrease in $f$ predicted by linear extrap- olation that we will accept. (The reason for requiring $\alpha$ to be smaller than 0 . 5 will become clear later.) 

The backtracking condition is illustrated in figure 9.1 . This figure suggests, and it can be shown, that the backtracking exit inequality $f(x+t\Delta x)\leq f(x)+$ $\alpha t\nabla f(x)^{T}\Delta x$ holds for $t\geq0$ in an i terval $(0,t_{0}]$ . It follows that the backtracking line search stops with a step length t that satisfies 

$$
t=1,\qquad\mathrm{or}\qquad t\in(\beta t_{0},t_{0}].
$$ 

The first case occurs when the step length $t=1$ satisfies the backtracking condition, i.e. , $1\leq t_{0}$ . In particular, we can say that the step length obtained by backtracking line search satisfies 

$$
t\geq\operatorname*{min}\{1,\beta t_{0}\}.
$$ 

When $\mathbf{dom}\ f$ is not all of $\mathbf{R}^{n}$ , the condition $f(x+t\Delta x)\leq f(x)+\alpha t\nabla f(x)^{T}\Delta x$ in the backtracking line search must be interpreted carefully. By our convention that $f$ is infinite outside its domain, the inequality implies that $x+t\Delta x\in\mathbf{dom}\,f$ . In a practical implementation, we first multiply $t$ by $\beta$ until $x+t\Delta x\,\in\,\mathbf{dom}\,f$ ; 

then we start to check whether the inequality $f(x+t\Delta x)\leq f(x)+\alpha t\nabla f(x)^{T}\Delta x$ holds. 

The parameter $\alpha$ is typically chosen between 0 . 01 and 0 . 3, meaning that we accept a decrease in $f$ between $1\%$ and $30\%$ of the prediction based on the linear extrapolation. The parameter $\beta$ is often chosen to be between 0 . 1 (which corre- sponds to a very crude search) and 0 . 8 (which corresponds to a less crude search). 

# 9.3 Gradient descent method 

A natural choice for the search direction is the negative gradient $\Delta x=-\nabla f(x)$ . The resulting algorithm is called the gradient algorithm or gradient descent method . 

Algorithm 9.3 Gradient descent method. 

given a starting point $x\in\mathbf{dom}\ f$ 

repeat 

1. $\Delta x:=-\nabla f(x)$ . 2. Line search. Choose step size $t$ via exact or backtracking line search. 3. Update. $x:=x+t\Delta x$ . until stopping criterion is satisfied. 

The stopping criterion is usually of the form $\|\nabla f(x)\|_{2}\leq\eta$ , where $\eta$ is small and positive. In most implementations, this condition is checked after step 1, rather than after the update. 

# 9.3.1 Convergence analysis 

In this section we present a simple convergence analysis for the gradient method, using the lighter notation $x^{+}=x+t\Delta x$ for $x^{(k+1)}=x^{(k)}+t^{(k)}\Delta x^{(k)}$ , where $\Delta x=$ $-\nabla f(x)$ . We assume $f$ is strongly convex on $S$ , so there are positive constants $m$ and $M$ such that $m I\preceq\nabla^{2}f(x)\preceq M I$ for all $x\in S$ . Define the function ${\ddot{f}}:\mathbf{R}\rightarrow\mathbf{R}$ → by $\ddot{f}(t)=f(x-t\nabla f(x))$ − ∇ i.e. , $f$ as a function of the step length $t$ in the negative gradient direction. In the following discussion we will only consider $t$ for which $x\mathrm{~-~}t\nabla f(x)\,\in\,S$ . From the inequality ( 9.13 ), with $y\,=\,x\,-\,t\nabla f(x)$ , we obtain a quadratic upper bound on $\ddot{f}$ : 

$$
\widetilde{f}(t)\leq f(x)-t\|\nabla f(x)\|_{2}^{2}+\frac{M t^{2}}{2}\|\nabla f(x)\|_{2}^{2}.
$$ 

# Analysis for exact line search 

We now assume that an exact line search is used, and minimize over $t$ both sides of the inequality ( 9.17 ). On the lefthand side we get $\tilde{f}(t_{\mathrm{exact}})$ ), where $t_{\mathrm{exact}}$ is the step length that minimizes $\ddot{f}$ . The righthand side is a simple quadratic, which is minimized by $t\,=\,1/M$ , and has minimum value $f(x)\,-\,(1/(2M))\|\nabla f(x)\|_{2}^{2}$ . Therefore we have 

$$
f(x^{+})=\tilde{f}(t_{\mathrm{exact}})\leq f(x)-\frac{1}{2M}\|\nabla(f(x))\|_{2}^{2}.
$$ 

Subtracting $p^{\star}$ from both sides, we get 

$$
f(x^{+})-p^{\star}\leq f(x)-p^{\star}-\frac{1}{2M}\|\nabla f(x)\|_{2}^{2}.
$$ 

We combine this with $||\nabla f(x)||_{2}^{2}\,\geq\,2m(f(x)-p^{\star})$ ≥ − ) (which follows from ( 9.9 )) to conclude 

$$
f(x^{+})-p^{\star}\leq(1-m/M)(f(x)-p^{\star}).
$$ 

Applying this inequality recursively, we find that 

$$
f(x^{(k)})-p^{\star}\leq c^{k}(f(x^{(0)})-p^{\star})
$$ 

where $c=1-m/M<1$ , which shows that $f(x^{(k)})$ converges to $p^{\star}$ as $k\rightarrow\infty$ . In particular, we must have $f(x^{(k)})-p^{\star}\leq\epsilon$ after at most 

$$
\frac{\log((f(x^{(0)})-p^{\star})/\epsilon)}{\log(1/c)}
$$ 

iterations of the gradient method with exact line search. 

This bound on the number of iterations required, even though crude, can give some insight into the gradient method. The numerator, 

$$
\log((f(x^{(0)})-p^{\star})/\epsilon)
$$ 

can be interpreted as the log of the ratio of the initial suboptimality ( i.e. , gap between $f(x^{(0)})$ and $p^{\star}$ ), to the final suboptimality ( i.e. , less than $\epsilon$ ). This term suggests that the number of iterations depends on how good the initial point is, and what the final required accuracy is. 

The denominator appearing in the bound ( 9.19 ), $\log(1/c)$ , is a function of $M/m$ , which we have seen is a bound on the condition number of $\nabla^{2}f(x)$ over $S$ , or the condition number of the sublevel sets $\{z\mid f(z)\leq\alpha\}$ . For large condition number bound $M/m$ , we have 

$$
\log(1/c)=-\log(1-m/M)\approx m/M,
$$ 

so our bound on the number of iterations required increases approximately linearly with increasing $M/m$ . 

We will see that the gradient method does in fact require a large number of iterations when the Hessian of $f$ , near $x^{\star}$ , has a large condition number. Conversely, when the sublevel sets of $f$ are relatively isotropic, so that the condition number bound $M/m$ can be chosen to be relatively small, the bound ( 9.18 ) shows that convergence is rapid, since $c$ is small, or at least not too close to one. 

The bound ( 9.18 ) shows that the error $f(x^{(k)})-p^{\star}$ converges to zero at least as fast as a geometric series. In the context of iterative numerical methods, this is called linear convergence , since the error lies below a line on a log-linear plot of error versus iteration number. 

# Analysis for backtracking line search 

Now we consider the case where a backtracking line search is used in the gradient descent method. We will show that the backtracking exit condition, 

$$
\tilde{f}(t)\leq f(x)-\alpha t\|\nabla f(x)\|_{2}^{2},
$$ 

is satisfied whenever $0\leq t\leq1/M$ . First note that 

$$
0\leq t\leq1/M\implies-\,t+{\frac{M t^{2}}{2}}\leq-t/2
$$ 

(which follows from convexity of $-t{+}M t^{2}/2$ ). Using this result and the bound ( 9.17 ), we have, for $0\leq t\leq1/M$ , 

$$
\begin{array}{r c l}{\tilde{f}(t)}&{\leq}&{f(x)-t\|\nabla f(x)\|_{2}^{2}+\displaystyle\frac{M t^{2}}{2}\|\nabla(f(x))\|_{2}^{2}}\\ &{\leq}&{f(x)-(t/2)\|\nabla f(x)\|_{2}^{2}}\\ &{\leq}&{f(x)-\alpha t\|\nabla f(x)\|_{2}^{2},}\end{array}
$$ 

since $\alpha<1/2$ . Therefore the backtracking line search terminates either with $t=1$ or with a value $t\,\geq\,\beta/M$ . This provides a lower bound on the decrease in the objective function. In the first case we have 

$$
f(x^{+})\leq f(x)-\alpha\|\nabla f(x)\|_{2}^{2},
$$ 

and in the second case we have 

$$
f(x^{+})\leq f(x)-(\beta\alpha/M)\|\nabla f(x)\|_{2}^{2}.
$$ 

Putting these together, we always have 

$$
f(x^{+})\leq f(x)-\operatorname*{min}\{\alpha,\beta\alpha/M\}\|\nabla f(x)\|_{2}^{2}.
$$ 

Now we can proceed exactly as in the case of exact line search. We subtract $p^{\star}$ from both sides to get 

$$
f(x^{+})-p^{\star}\leq f(x)-p^{\star}-\operatorname*{min}\{\alpha,\beta\alpha/M\}\|\nabla f(x)\|_{2}^{2},
$$ 

and combine this with $||\nabla f(x)||_{2}^{2}\geq2m(f(x)-p^{\star})$ ≥ − ) to obtain 

$$
f(x^{+})-p^{\star}\leq(1-\operatorname*{min}\{2m\alpha,2\beta\alpha m/M\})(f(x)-p^{\star}).
$$ 

From this we conclude 

$$
f(x^{(k)})-p^{\star}\leq c^{k}(f(x^{(0)})-p^{\star})
$$ 

where 

$$
c=1-\operatorname*{min}\{2m\alpha,2\beta\alpha m/M\}<1.
$$ 

In particular, $f(x^{(k)})$ converges to $p^{\star}$ at least as fast as a geometric series with an exponent that depends (at least in part) on the condition number bound $M/m$ . In the terminology of iterative methods, the convergence is at least linear. 

![](images/6fd1c0d9955dcbde5deb5c6f9d91ae2427b3722917f5597bfef953115b5d0e21.jpg) 
Figure 9.2 Some contour lines of the function $f(x)=(1/2)(x_{1}^{2}+10x_{2}^{2})$ ). The condition number of the sublevel sets, which are ellipsoids, is exactly 10. The figure shows the iterates of the gradient method with exact line search, started at $x^{(0)}=(10,1)$ . 

# 9.3.2 Examples 

# A quadratic problem in $\mathbf{R}^{2}$ 

Our first example is very simple. We consider the quadratic objective function on $\mathbf{R}^{2}$ 

$$
f(x)={\frac{1}{2}}(x_{1}^{2}+\gamma x_{2}^{2}),
$$ 

where $\gamma>0$ . Clearly, the optimal point is $x^{\star}=0$ , and the optimal value is $0$ . The Hessian of $f$ is constant, and has eigenvalues $1$ and , so the condition numbers of $\gamma$ the sublevel sets of $f$ are all exactly 

$$
{\frac{\operatorname*{max}\{1,\gamma\}}{\operatorname*{min}\{1,\gamma\}}}=\operatorname*{max}\{\gamma,1/\gamma\}.
$$ 

The tightest choices for the strong convexity constants $m$ and $M$ are 

$$
m=\operatorname*{min}\{1,\gamma\},\qquad M=\operatorname*{max}\{1,\gamma\}.
$$ 

We apply the gradient descent method with exact line search, starting at the point $x^{(0)}=(\gamma,1)$ . In this case we can derive the following closed-form expressions for the iterates $x^{(k)}$ and their function values (exercise 9.6 ): 

$$
x_{1}^{(k)}=\gamma\left({\frac{\gamma-1}{\gamma+1}}\right)^{k},\qquad x_{2}^{(k)}=\left(-{\frac{\gamma-1}{\gamma+1}}\right)^{k},
$$ 

and 

$$
f(x^{(k)})={\frac{\gamma(\gamma+1)}{2}}\left({\frac{\gamma-1}{\gamma+1}}\right)^{2k}=\left({\frac{\gamma-1}{\gamma+1}}\right)^{2k}f(x^{(0)}).
$$ 

This is illustrated in figure 9.2 , for $\gamma=10$ . 

For this simple example, convergence is exactly linear, i.e. , the error is exactly a geometric series, reduced by the factor $|(\gamma-1)/(\gamma+1)|^{2}$ at each iteration. For $\gamma=1$ , the exact solution is found in one iteration; for $^{'}\gamma$ not far from one (say, between $1/3$ and 3) convergence is rapid. The convergence is very slow for $\gamma\gg1$ or $\gamma\ll1$ . 

We can compare the convergence with the bound derived above in § 9.3.1 . Using the least conservative values $m=\operatorname*{min}\{1,\gamma\}$ and $M=\operatorname*{max}\{1,\gamma\}$ , the bound ( 9.18 ) guarantees that the error in each iteration is reduced at least by the factor $c=$ $(1-m/M)$ . We have seen that the error is in fact reduced exactly by the factor 

$$
\left(\frac{1-m/M}{1+m/M}\right)^{2}
$$ 

in each iteration. For small $m/M$ , which corresponds to large condition number, the upper bound ( 9.19 ) implies that the number of iterations required to obtain a given level of accuracy grows at most like $M/m$ . For this example, the exact number of iterations required grows approximately like $(M/m)/4$ , i.e. , one quarter of the value of the bound. This shows that for this simple example, the bound on the number of iterations derived in our simple analysis is only about a factor of four conservative (using the least conservative values for $m$ and $M$ ). In particular, the convergence rate (as well as its upper bound) is very dependent on the condition number of the sublevel sets. 

# A nonquadratic problem in $\mathbf{R}^{2}$ 

We now consider a nonquadratic example in $\mathbf{R}^{2}$ , with 

$$
f(x_{1},x_{2})=e^{x_{1}+3x_{2}-0.1}+e^{x_{1}-3x_{2}-0.1}+e^{-x_{1}-0.1}.
$$ 

We apply the gradient method with a backtracking line search, with $\alpha~=~0.1$ , $\beta=0.7$ . Figure 9.3 shows some level curves of $f$ , and the iterates $x^{(k)}$ generated by the gradient method (shown as small circles). The lines connecting successive iterates show the scaled steps, 

$$
\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}=-t^{(k)}\nabla f(\boldsymbol{x}^{(k)}).
$$ 

Figure 9.4 shows the error $f(x^{(k)})-p^{\star}$ versus iteration $k$ . The plot reveals that the error converges to zero approximately as a geometric series, i.e. , the convergence is approximately linear. In this example, the error is reduced from about 10 to about $10^{-7}$ in 20 iterations, so the error is reduced by a factor of approximately $10^{-8/20}\,\approx\,0.4$ each iteration. This reasonably rapid convergence is predicted by our convergence analysis, since the sublevel sets of $f$ are not too badly conditioned, which in turn means that $M/m$ can be chosen as not too large. 

To compare backtracking line search with an exact line search, we use the gradient method with an exact line search, on the same problem, and with the same starting point. The results are given in figures 9.5 and 9.4 . Here too the convergence is approximately linear, about twice as fast as the gradient method with backtracking line search. With exact line search, the error is reduced by about $10^{-11}$ in 15 iterations, i.e. , a reduction by a factor of about $10^{-11/15}\approx0.2$ per iteration. 

![](images/096eaf1f97b4b1d9b9fdda2d9fc64770a5b1076c4ff8e53d43a6cd657911ba43.jpg) 
Figure 9.3 Iterates of the gradient method with backtracking line search, for the problem in $\mathbf{R}^{2}$ with objective $f$ given in ( 9.20 ). The dashed curves are level curves of $f$ , and the small circles are the iterates of the gradient method. The solid lines, which connect successive iterates, show the scaled steps $t^{(k)}\Delta x^{(k)}$ . 

![](images/41e77b53b6a3d8318328c0ffff7611c1cbacaf0111e978265bc8a506355914cc.jpg) 
Figure 9.4 Error $f(x^{(k)})-p^{\star}$ versus iteration $k$ of the g ient method wit backtracking and exact line search, for the problem in R with objective f given in ( 9.20 ). The plot shows nearly linear convergence, with the error reduced approximately by the factor 0 . 4 in each iteration of the gradient method with backtracking line search, and by the factor 0 . 2 in each iteration of the gradient method with exact line search. 

![](images/603524abd7f2ea2001002b2231ec255174875469d2ea103cd9086ffd2cb84c8b.jpg) 
Figure 9.5 Iterates of the gradient method with exact line search for the problem in $\mathbf{R}^{2}$ with objective $f$ given in ( 9.20 ). 

# A problem in $\mathbf{R}^{100}$ 

We next consider a larger example, of the form 

$$
f(x)=c^{T}x-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),
$$ 

with $m=500$ terms and $n=100$ variables. 

The progress of the gradient method with backtracking line search, with pa- rameters $\alpha=0.1$ , $\beta=0.5$ , is shown in figure 9.6 . In this example we see an initial approximately linear and fairly rapid convergence for about 20 iterations, followed by a slower linear convergence. Overall, the error is reduced by a factor of around $10^{6}$ in around 175 iterations, which gives an average error reduction by a factor of around $10^{-6/175}\approx0.92$ per iteration. The initial convergence rate, for the first 20 iterations, is around a factor of 0 . 8 per iteration; the slower final convergence rate, after the first 20 iterations, is around a factor of 0 . 94 per iteration. 

Figure 9.6 shows the convergence of the gradient method with exact line search. The convergence is again approximately linear, with an overall error reduction by approximately a factor $10^{-6/140}\approx0.91$ per iteration. This is only a bit faster than the gradient method with backtracking line search. 

Finally, we examine the inﬂuence of the backtracking line search parameters $\alpha$ and $\beta$ on the convergence rate, by determining the number of iterations required to obtain $f(x^{(k)})-p^{\star}\leq10^{-5}$ . In the first experiment, we fix $\beta\,=\,0.5$ , and vary $\alpha$ from 0 . 05 to 0 . 5. The number of iterations required varies from about 80, for larger values of $\alpha$ , in the range 0 . 2–0 . 5, to about 170 for smaller values of $\alpha$ . This, and other experiments, suggest that the gradient method works better with fairly large $\alpha$ , in the range 0 . 2–0 . 5. 

Similarly, we can study the eﬀect of the choice of $\beta$ by fixing $\alpha\:=\:0.1$ and varying $\beta$ from 0 . 05 to 0 . 95. Again the variation in the total number of iterations is not large, ranging from around 80 (when $\beta\approx0.5$ ) to around 200 (for $\beta$ small, or near 1). This experiment, and others, suggest that $\beta\approx0.5$ is a good choice. 

![](images/ea28b070c06c78744326d7486a2244be6f1e3821115af5d4eca4ee4f0bccb744.jpg) 
Figure 9.6 Error $f(x^{(k)})-p^{\star}$ versus iteration $k$ for th dient method with backtracking and exact line search, for a problem in R $\mathbf{R}^{100}$ . 

These experiments suggest that the eﬀect of the backtracking parameters on the convergence is not large, no more than a factor of two or so. 

# Gradient method and condition number 

Our last experiment will illustrate the importance of the condition number of $\nabla^{2}f(x)$ (or the sublevel sets) on the rate of convergence of the gradient . We start with the function given by ( 9.21 ), but replace the variable $x$ by x $x=T\bar{x}$ , where 

$$
T=\mathbf{diag}((1,\gamma^{1/n},\gamma^{2/n},.\,.\,.\,,\gamma^{(n-1)/n})),
$$ 

i.e. , we minimize 

$$
\bar{f}(\bar{x})=c^{T}T\bar{x}-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}T\bar{x}).
$$ 

This gives us a family of optimization problems, indexed by $\gamma$ , which aﬀects the problem condition number. 

Figure 9.7 shows the number of iterations required to achieve $\bar{f}(\bar{x}^{(k)})-\bar{p}^{\star}<10^{-5}$ − as a function of , using a backtracking line search with $\alpha=0.3$ and $\beta=0.7$ . This $\gamma$ plot shows that for diagonal scaling as small as $10:1$ ( i.e. , $\gamma=10$ ), the number of iterations grows to more than a thousand; for a diagonal scaling of 20 or more, the gradient method slows to essentially useless. 

The condition number of the Hessian $\nabla^{2}f(\bar{x}^{\star})$ ) at the optimum is shown in figure 9.8 . For large and small $\gamma$ , the condition number increases roughly as $\operatorname*{max}\{\gamma^{2},1/\gamma^{2}\}$ , in a very similar way as the number of iterations depends on $\gamma$ . This shows again that the relation between conditioning and convergence speed is a real phenomenon, and not just an artifact of our analysis. 

![](images/74a839f5f059bf86fc25a1f4fe50ac7f25f951d5ec381f8cf2b95f54b4888fef.jpg) 
Figure 9.7 Number of iterations of the gradient method applied to prob- lem ( 9.22 ). The vertical axis shows the number of iterations required to obtain $\bar{f}(\bar{x}^{(k)})-\bar{p}^{\star}<10^{-5}$ − . The horizontal axis shows $^\prime\gamma$ , which is a param- eter that controls the amount of diagonal scaling. We use a backtracking line search with $\alpha=0.3$ , $\beta=0.7$ . 

![](images/4712af63409f46fab830e9483eca4bbaf3c97d65bf859b51a2e9bb8e80d796f9.jpg) 
Figure 9.8 Condition number of the Hessian of the function at its minimum, as a function of $\gamma$ . By comparing this plot with the one in figure 9.7 , we see that the condition number has a very strong inﬂuence on convergence rate. 

# Conclusions 

From the numerical examples shown, and others, we can make the conclusions summarized below. 

• The gradient method often exhibits approximately linear convergence, i.e. , the error $f(x^{(k)})-p^{\star}$ converges to zero approximately as a geometric series. • The choice of backtracking parameters $\alpha$ , $\beta$ has a noticeable but not dramatic eﬀect on the convergence. An exact line search sometimes improves the con- vergence of the gradient method, but the eﬀect is not large (and probably not worth the trouble of implementing the exact line search). • The convergence rate depends greatly on the condition number of the Hessian, or the sublevel sets. Convergence can be very slow, even for problems that are moderately well conditioned (say, with condition number in the 100s). When the condition number is larger (say, 1000 or more) the gradient method is so slow that it is useless in practice. 

The main advantage of the gradient method is its simplicity. Its main disadvantage is that its convergence rate depends so critically on the condition number of the Hessian or sublevel sets. 

# 9.4 Steepest descent method 

The first-order Taylor approximation of $f(x+v)$ around $x$ is 

$$
f(x+v)\approx{\widehat{f}}(x+v)=f(x)+\nabla f(x)^{T}v.
$$ 

The second term on the righthand side, $\nabla f(x)^{T}v$ , is the directional derivative of $f$ at $x$ in the direction $v$ . It gives the approximate change in $f$ for a small step $v$ . The step $v$ is a descent direction if the directional derivative is negative. 

We now address the question of how to choose $v$ to make the directional deriva- tive as negative as possible. Since the directional derivative $\nabla f(x)^{T}v$ is linear in $v$ , it can be made as negative as we like by taking $v$ large (provided $v$ is a descent direction, i.e. , $\nabla f(x)^{T}v<0$ ). To make the question sensible we have to limit the size of $v$ , or normalize by the length of $v$ . 

Let $\|\cdot\|$ be any norm on $\mathbf{R}^{n}$ . We define a normalized steepest descent direction (with respect to the norm $||\cdot||$ ) as 

$$
\begin{array}{r}{\Delta x_{\mathrm{nsd}}=\operatorname*{argmin}\{\nabla f(x)^{T}v\mid\|v\|=1\}.}\end{array}
$$ 

(We say $^\mathrm{a}$ ’ steepest descent direction because there can be multiple minimizers.) A normalized steepest descent direction $\Delta x_{\mathrm{nsd}}$ is a step of unit norm that gives the largest decrease in the linear approximation of $f$ . 

A normalized steepest descent direction can be interpreted geometrically as follows. We can just as well define $\Delta x_{\mathrm{nsd}}$ as 

$$
\begin{array}{r}{\Delta x_{\mathrm{nsd}}=\operatorname*{argmin}\{\nabla f(x)^{T}v\mid\|v\|\leq1\},}\end{array}
$$ 

i.e. , as the direction in the unit ball of $||\cdot||$ that extends farthest in the direction $-\nabla f(x)$ . 

It is also convenient to consider a steepest descent step $\Delta x_{\mathrm{sd}}$ that is unnormal- ized , by scaling the normalized steepest descent direction in a particular way: 

$$
\Delta x_{\mathrm{sd}}=\|\nabla f(x)\|_{*}\Delta x_{\mathrm{nsd}},
$$ 

where $||\cdot||_{*}$ denotes the dual norm. Note that for the steepest descent step, we have 

$$
\nabla f(x)^{T}\Delta x_{\mathrm{sd}}=\|\nabla f(x)\|_{*}\nabla f(x)^{T}\Delta x_{\mathrm{nsd}}=-\|\nabla f(x)\|_{*}^{2}
$$ 

(see exercise 9.7 ). 

The steepest descent method uses the steepest descent direction as search direc- tion. 

Algorithm 9.4 Steepest descent method. 

given a starting point $x\in\mathbf{dom}\ f$ 

# repeat 

1. Compute steepest descent direction $\Delta x_{\mathrm{sd}}$ . 2. Line search. Choose $t$ via backtracking or exact line search. 3. Update. $x:=x+t\Delta x_{\mathrm{sd}}$ . until stopping criterion is satisfied. 

When exact line search is used, scale factors in the descent direction have no eﬀect, so the normalized or unnormalized direction can be used. 

# 9.4.1 Steepest descent for Euclidean and quadratic norms 

# Steepest descent for Euclidean norm 

If we take the norm $||\cdot||$ to be the Euclidean norm we find that the steepest descent direction is simply the negative gradient, i.e. , $\Delta x_{\mathrm{sd}}\;=\;-\nabla f(x)$ . The steepest descent method for the Euclidean norm coincides with the gradient descent method. 

# Steepest descent for quadratic norm 

We consider the quadratic norm 

$$
\|z\|_{P}=(z^{T}P z)^{1/2}=\|P^{1/2}z\|_{2},
$$ 

where $P\in\mathbf{S}_{++}^{n}$ . The normalized steepest descent direction is given by 

$$
\Delta x_{\mathrm{nsd}}=-\left(\nabla f(x)^{T}P^{-1}\nabla f(x)\right)^{-1/2}P^{-1}\nabla f(x).
$$ 

The dual norm is given by ∥ $\|z\|_{*}\,=\,\|P^{-1/2}z\|_{2}$ ∥ ∥ ∥ , so the steepest descent step with respect to $||\cdot||_{P}$ is given by 

$$
\Delta x_{\mathrm{sd}}=-P^{-1}\nabla f(x).
$$ 

The normalized steepest descent direction for a quadratic norm is illustrated in figure 9.9 . 

![](images/5bab0ae7794f00362d63e50c57c2177b475b69a29261ca4157ba42637dc2e483.jpg) 
Figure 9.9 Normalized steepest descent direction for a quadratic norm. The ellipsoid shown is the unit ball of the norm, translated to the point $x$ . The normalized steepest descent direction $\Delta x_{\mathrm{nsd}}$ at $x$ extends as far as possible in the direction $-\nabla f(x)$ while staying in the ellipsoid. The gradient and normalized steepest descent directions are shown. 

# Interpretation via change of coordinates 

We can give an interesting alternative interpretation of the steepest descent direc- tion $\Delta x_{\mathrm{sd}}$ as the gradient search direction after a change of coordinates is applied to the problem. Define $\bar{u}=P^{1/2}u$ , so we have $||u||_{P}=||\bar{u}||_{2}$ ∥ . Using this change of coordinates, we can solve the original problem of minimizing $f$ by solving the equivalent problem of minimizing the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , given by 

$$
\bar{f}(\bar{u})=f(P^{-1/2}\bar{u})=f(u).
$$ 

If we apply the gradient method to $f$ , the search direction at a point x (which corresponds to the point $x=P^{-1/2}\bar{x}$ x for the original problem) is 

$$
\Delta\bar{x}=-\nabla\bar{f}(\bar{x})=-P^{-1/2}\nabla f(P^{-1/2}\bar{x})=-P^{-1/2}\nabla f(x).
$$ 

This gradient search direction corresponds to the direction 

$$
\Delta x=P^{-1/2}\left(-P^{-1/2}\nabla f(x)\right)=-P^{-1}\nabla f(x)
$$ 

for the original variable $x$ . In other words, the steepest descent method in the quadratic norm $||\cdot||_{P}$ can be thought radient method applied to the $\bar{x}=P^{1/2}x$ problem after the change of coordinates ¯ . 

# 9.4.2 Steepest descent for $\ell_{1}$ -norm 

As another example, we consider the steepest descent method for the $\ell_{1}$ -norm. A normalized steepest descent direction, 

$$
\begin{array}{r}{\Delta x_{\mathrm{nsd}}=\operatorname*{argmin}\{\nabla f(x)^{T}v\mid\|v\|_{1}\leq1\},}\end{array}
$$ 

![](images/fbc6e71dd27a2dbe541a07e525722838c4a85a10b60f1b6e824a356c75a18651.jpg) 
Figure 9.10 Normalized steepest descent direction for the $\ell_{1}$ -norm. The diamond is the unit ball of the $\ell_{1}$ -norm, translated to the point $x$ . The normalized steepest descent direction can always be chosen in the direction of a standard basis vector; in this example we have $\Delta x_{\mathrm{nsd}}=e_{1}$ . 

is easily characterized. Let $i$ be any index for which $\|\nabla f(x)\|_{\infty}=|(\nabla f(x))_{i}|$ . Then a normalized steepest descent direction $\Delta x_{\mathrm{nsd}}$ for the $\ell_{1}$ -norm is given by 

$$
\Delta x_{\mathrm{nsd}}=-\mathrm{sign}\left({\frac{\partial f(x)}{\partial x_{i}}}\right)e_{i},
$$ 

where is the $i$ th standard basis vector. An unnormalized steepest descent step $e_{i}$ is then 

$$
\Delta x_{\mathrm{sd}}=\Delta x_{\mathrm{nsd}}\|\nabla f(x)\|_{\infty}=-\frac{\partial f(x)}{\partial x_{i}}e_{i}.
$$ 

Thus, the normalized steepest descent step in $\ell_{1}$ -norm can always be chosen to be a standard basis vector (or a negative standard basis vector). It is the coordinate axis direction along which the approximate decrease in $f$ is greatest. This is illustrated in figure 9.10 . 

The steepest descent algorithm in the $\ell_{1}$ -norm has a very natural interpretation: At each iteration we select a component of $\nabla f(x)$ with maximum absolute value, and then decrease or increase the corresponding component of $x$ , according to the sign of $(\nabla f(x))_{i}$ . The algorithm is sometimes called a coordinate-descent algorithm, since only one component of the variable $x$ is updated at each iteration. This can greatly simplify, or even trivialize, the line search. 

Example 9.2 Frobenius norm scaling. In $\S4.5.4$ we encountered the unconstrained geometric program 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\sum_{i,j=1}^{n}M_{i j}^{2}d_{i}^{2}/d_{j}^{2},}\end{array}
$$ 

where $M\in\mathbf{R}^{n\times n}$ is given, and the variable is $d\in\mathbf{R}^{n}$ . Using the change of variables $x_{i}=2\log d_{i}$ we can express this geometric program in convex form as 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{f({\boldsymbol{x}})=\log\left(\sum_{i,j=1}^{n}M_{i j}^{2}e^{x_{i}-x_{j}}\right).}\end{array}}
$$ 

It is easy to minimize $f$ one component at a time. Keeping all components except the $k$ th fixed, we can write $f(x)=\log(\alpha_{k}+\beta_{k}e^{-x_{k}}+\gamma_{k}e^{x_{k}})$ , where 

$$
\alpha_{k}=M_{k k}^{2}+\sum_{i,j\neq k}M_{i j}^{2}e^{x_{i}-x_{j}},\qquad\beta_{k}=\sum_{i\neq k}M_{i k}^{2}e^{x_{i}},\qquad\gamma_{k}=\sum_{j\neq k}M_{k j}^{2}e^{-x_{j}}.
$$ 

The minimum of $f(x)$ , as a function of $x_{k}$ , is obtained for $x_{k}\,=\,\log(\beta_{k}/\gamma_{k})/2$ . So for this problem an exact line search can be carried out using a simple analytical formula. 

The $\ell_{1}$ -steepest descent algorithm with exact line search consists of repeating the following steps. 

1. Compute the gradient 

$$
(\nabla f(x))_{i}=\frac{-\beta_{i}e^{-x_{i}}+\gamma_{i}e^{x_{i}}}{\alpha_{i}+\beta_{i}e^{-x_{i}}+\gamma_{i}e^{x_{i}}},\quad i=1,\dots,n.
$$ 

2. Select a largest (in absolute value) component of $\nabla f(x)$ : $|\nabla f(x)|_{k}=\|\nabla f(x)\|_{\infty}$ . 3. Minimize $f$ over the scalar variable $x_{k}$ , by setting $x_{k}=\log(\beta_{k}/\gamma_{k})/2$ . 

# 9.4.3 Convergence analysis 

In this section we extend the convergence analysis for the gradient method with backtracking line search to the steepest descent method for an arbitrary norm. We will use the fact that any norm can be bounded in terms of the Euclidean norm, so there exists constants $\gamma,\ \tilde{\gamma}\in(0,1]$ 1] such that 

$$
\|x\|\geq\gamma\|x\|_{2},\qquad\|x\|_{*}\geq\tilde{\gamma}\|x\|_{2}
$$ 

(see A.1.4 ). 

Again we assume $f$ is strongly convex on the initial sublevel set $S$ . The upper bound $\nabla^{2}f(x)\preceq M I$ implies an upper bound on the function $f(x+t\Delta x_{\mathrm{sd}})$ as a function of t : 

$$
\begin{array}{r c l}{f(x+t\Delta x_{\mathrm{sd}})}&{\leq}&{f(x)+t\nabla f(x)^{T}\Delta x_{\mathrm{sd}}+\displaystyle\frac{M\|\Delta x_{\mathrm{sd}}\|_{2}^{2}}{2}t^{2}}\\ &{\leq}&{f(x)+t\nabla f(x)^{T}\Delta x_{\mathrm{sd}}+\displaystyle\frac{M\|\Delta x_{\mathrm{sd}}\|^{2}}{2\gamma^{2}}t^{2}}\\ &{=}&{f(x)-t\|\nabla f(x)\|_{*}^{2}+\displaystyle\frac{M}{2\gamma^{2}}t^{2}\|\nabla f(x)\|_{*}^{2}.}\end{array}
$$ 

The step size $\hat{t}\;=\;\gamma^{2}/M$ (which minimizes the quadratic upper bound ( 9.26 )) satisfies the exit condition for the backtracking line search: 

$$
f(x+\hat{t}\Delta x_{\mathrm{sd}})\leq f(x)-\frac{\gamma^{2}}{2M}\|\nabla f(x)\|_{*}^{2}\leq f(x)+\frac{\alpha\gamma^{2}}{M}\nabla f(x)^{T}\Delta x_{\mathrm{sd}}
$$ 

since $\alpha<1/2$ and $\nabla f(x)^{T}\Delta x_{\mathrm{sd}}=-\|\nabla f(x)\|_{*}^{2}$ . The line search therefore returns a step size $t\geq\operatorname*{min}\{1,\beta\gamma^{2}/M\}$ , and we have 

$$
\begin{array}{r c l}{f(x^{+})=f(x+t\Delta x_{\mathrm{sd}})}&{\leq}&{f(x)-\alpha\operatorname*{min}\{1,\beta\gamma^{2}/M\}\|\nabla f(x)\|_{*}^{2}}\\ &{\leq}&{f(x)-\alpha\tilde{\gamma}^{2}\operatorname*{min}\{1,\beta\gamma^{2}/M\}\|\nabla f(x)\|_{2}^{2}.}\end{array}
$$ 

Subtracting $p^{\star}$ from both sides and using ( 9.9 ), we obtain 

$$
f(x^{+})-p^{\star}\leq c(f(x)-p^{\star}),
$$ 

where 

$$
c=1-2m\alpha\tilde{\gamma}^{2}\operatorname*{min}\{1,\beta\gamma^{2}/M\}<1.
$$ 

Therefore we have 

$$
f(x^{(k)})-p^{\star}\leq c^{k}(f(x^{(0)})-p^{\star}),
$$ 

i.e. , linear convergence exactly as in the gradient method. 

# 9.4.4 Discussion and examples 

# Choice of norm for steepest descent 

The choice of norm used to define the steepest descent direction can have a dra- matic eﬀect on the convergence rate. For simplicity, we consider the case of steep- est descent with quadra c $P$ -norm. In § 9.4.1 , we showed that the steepest descent method with quadratic P -norm is the same as the gradient method applied to the problem after the change of coordinates $\bar{x}\,=\,P^{1/2}x$ . We know that the gradient method works well when the condition numbers of the sublevel sets (or the Hes- sian near the optimal point) are moderate, and works poorly when the condition numbers are large. It follows that when the sublevel sets, after the change of coor- dinates $\bar{x}=P^{1/2}x$ , are moderately conditioned, the steepest descent method will work well. 

This observation provides a prescription for choosing $P$ : It should be chosen so that the sublevel sets of $f$ , transformed by $P^{-1/2}$ , are well conditioned. For example if an approximation $\hat{H}$ of the Hessian at the optimal point $H(x^{\star})$ were known, a very good choice of $P$ would be $P\,=\,\hat{H}$ , since the Hessian of $\ddot{f}$ at the optimum is then 

$$
\hat{H}^{-1/2}\nabla^{2}f(x^{\star})\hat{H}^{-1/2}\approx I,
$$ 

and so is likely to have a low condition number. 

This same idea can be described without a change of coordinates. Saying that a sublevel set has low condition number after the change of coordinates $\bar{x}=P^{1/2}x$ is the same as saying that the ellipsoid 

$$
{\mathcal{E}}=\{x\mid x^{T}P x\leq1\}
$$ 

approximates the shape of the sublevel set. (In other words, it gives a good ap- proximation after appropriate scaling and translation.) 

This dependence of the convergence rate on the choice of $P$ can be viewed from two sides. The optimist’s viewpoint is that for any problem, there is always a 

![](images/a60acdca145a747296d86f7ba6eb323f16353346405dc7d7b8b0564535950292.jpg) 
Figure 9.11 Steepest descent method with a quadratic norm $\|\cdot\|_{P_{1}}$ . ellip are the boundaries of the norm balls $\{x\mid\|x-x^{(k)}\|_{P_{1}}\leq1\}$ at x $x^{(0)}$ and x $x^{(1)}$ . 

choice of $P$ for which the steepest descent method works very well. The challenge, of course, is to find such a $P$ . The pessimist’s viewpoint is that for any problem, there are a huge number of choices of $P$ for which steepest descent works very poorly. In summary, we can say that the steepest descent method works well in cases where we can identify a matrix $P$ for which the transformed problem has moderate condition number. 

# Examples 

In this section we illustrate some of these ideas using the nonquadratic problem in $\mathbf{R}^{2}$ with objective function ( 9.20 ). We apply the steepest descent method to the problem, using the two quadratic norms defined by 

$$
P_{1}=\left[\begin{array}{l l}{{2}}&{{0}}\\ {{0}}&{{8}}\end{array}\right],\qquad P_{2}=\left[\begin{array}{l l}{{8}}&{{0}}\\ {{0}}&{{2}}\end{array}\right].
$$ 

In both cases we use a backtracking line search with $\alpha=0.1$ and $\beta=0.7$ . 

Figures 9.11 and 9.12 show the iterates for steepest descent with norm $\|\cdot\|_{P_{1}}$ and norm $\|\cdot\|_{P_{2}}$ . Figure 9.13 shows the error versus iteration number for both norms. Figure 9.13 shows that the choice of norm strongly inﬂuences the convergence. With the norm $\|\cdot\|_{P_{1}}$ , convergence is a bit more rapid than the gradient method, whereas with the norm $||\cdot||_{P_{2}}$ , convergence is far slower. 

This can be explained by examining the problems after the changes of coor- dinates $\bar{x}\,=\,P_{1}^{1/2}x$ and $\bar{x}\,=\,P_{2}^{1/2}x$ , respectively. Figures 9.14 and 9.15 show the problems in the transformed coordinates. The change of variables associated with $P_{1}$ yields sublevel sets with modest condition number, so convergence is fast. The change of variables associated with $P_{2}$ yields sublevel sets that are more poorly conditioned, which explains the slower convergence. 

![](images/60add2bad9226386b284f316f34512db4ae040abd7728280497c4c30638ae2c1.jpg) 
Figure 9.12 Steepest descent method, with quadratic norm $||\cdot||_{P_{2}}$ 

![](images/2f020df70fa502ff9b5a194fb8922ebbcca64bf651eb84a0d249aded1ac7f231.jpg) 
Figure 9.13 Error $f(x^{(k)})-p^{\star}$ versus iteration $k$ , for the steepest descent method with the quadratic norm $||\cdot||_{P_{1}}$ and the quadratic norm $||\cdot||_{P_{2}}$ . Convergence is rapid for the norm $\|\cdot\|_{P_{1}}$ and very slow for $||\cdot||_{P_{2}}$ . 

![](images/201615f7c8f30ed306de74c31a30f1e457b1241ee6957030488ccadf6870d661.jpg) 
Figure 9.14 The iterates of steepest descent with norm $||\cdot||_{P_{1}}$ , after the change of coordinates. This change of coordinates reduces the condition number of the sublevel sets, and so speeds up convergence. 

![](images/47ad72fc6374f3cb8599aa364eb1e5dc4cfaa3cd6a33051c601fa426b5a0c5b8.jpg) 
Figure 9.15 The iterates of steepest descent with norm $||\cdot||_{P_{2}}$ , after the change of coordinates. This change of coordinates increases the condition number of the sublevel sets, and so slows down convergence. 

![](images/931791b407116caefa8cda928d6925e30b3b2afe7b17b120dd4d2fb53c82dac3.jpg) 
Figure 9.16 The function $f$ (shown solid) and its second-order approximation $\hat{f}$ at $x$ (dashed). The Newton step $\Delta x_{\mathrm{{nt}}}$ is what must be added to $x$ to give the minimizer of $\hat{f}$ . 

# 9.5 Newton’s method 

9.5.1 The Newton step 

For $x\in\mathbf{dom}\,f$ , the vector 

$$
\Delta x_{\mathrm{nt}}=-\nabla^{2}f(x)^{-1}\nabla f(x)
$$ 

is called the Newton step (for $f$ , at $x$ ). Positive definiteness of $\nabla^{2}f(x)$ implies that 

$$
\nabla f(x)^{T}\Delta x_{\mathrm{nt}}=-\nabla f(x)^{T}\nabla^{2}f(x)^{-1}\nabla f(x)<0
$$ 

unless $\nabla f(x)=0$ , so the Newton step is a descent direction (unless $x$ is optimal). The Newton step can be interpreted and motivated in several ways. 

# Minimizer of second-order approximation 

The second-order Taylor approximation (or model) $\hat{f}$ of $f$ at $x$ is 

$$
\widehat{\boldsymbol{f}}(\boldsymbol{x}+\boldsymbol{v})=\boldsymbol{f}(\boldsymbol{x})+\nabla\boldsymbol{f}(\boldsymbol{x})^{T}\boldsymbol{v}+\frac{1}{2}\boldsymbol{v}^{T}\nabla^{2}\boldsymbol{f}(\boldsymbol{x})\boldsymbol{v},
$$ 

which is a convex quadratic function of $v$ , and is minimized when $v=\Delta x_{\mathrm{nt}}$ . Thus, the Newton step $\Delta x_{\mathrm{{nt}}}$ is what should be added to the point $x$ to minimize the second-order approximation of $f$ at $x$ . This is illustrated in figure 9.16 . 

This interpretation gives us some insight into the Newton step. If the function $f$ is quadratic, then $x+\Delta{}x_{\mathrm{{nt}}}$ is the exact minimizer of $f$ . If the function $f$ is nearly quadratic, intuition suggests that $x+\Delta{}x_{\mathrm{nt}}$ should be a very good estimate of the minimizer of $f$ , i.e. , $x^{\star}$ . Since $f$ is twice diﬀerentiable, the quadratic model of $f$ will be very accurate when $x$ is near $x^{\star}$ . It follows that when $x$ is near $x^{\star}$ , the point $x+\Delta{}x_{\mathrm{nt}}$ should be a very good estimate of $x^{\star}$ . We will see that this intuition is correct. 

![](images/921158a322dad3c009f279e342acc15a32fbecc834d1076e3e72b3460714badb.jpg) 
Figure 9.17 The dashed lines are level curves of a convex function. The ellipsoid shown (with solid line) is $\{x+v\mid v^{T}\nabla^{2}f(x)v\,\leq\,1\}$ . Th ow shows $-\nabla f(x)$ , the gradient descent direction. The Newton step ∆ $\Delta x_{\mathrm{{nt}}}$ is eepest descent direction in the norm $\big|\cdot\big|_{\nabla^{2}f(x)}$ . The figure also shows $\Delta x_{\mathrm{nsd}}$ , the normalized steepest descent direction for the same norm. 

# Steepest descent direction in Hessian norm 

The Newton step is also the steepest descent direction at $x$ , for the quadratic norm defined by the Hessian $\nabla^{2}f(x)$ , i.e. , 

$$
\begin{array}{r}{\|u\|_{\nabla^{2}f(x)}=(u^{T}\nabla^{2}f(x)u)^{1/2}.}\end{array}
$$ 

This gives another insight into why the Newton step should be a good search direction, and a very good search direction when $x$ is near $x^{\star}$ . 

Recall from our discussion above that steepest descent, with quadratic norm $||\cdot||_{P}$ , converges very rapidly when the Hessian, after the ssociated change of coordinates, has small condition number. In particular, near x $x^{\star}$ , a very good choice is $P=\nabla^{2}f(x^{\star})$ . When $x$ is near $x^{\star}$ , we have $\nabla^{2}f(x)\approx\nabla^{2}f(x^{\star})$ , which explains why the Newton step is a very good choice of search direction. This is illustrated in figure 9.17 . 

# Solution of linearized optimality condition 

If we linearize the optimality condition $\nabla f(x^{\star})=0$ near $x$ we obtain 

$$
\nabla f(x+v)\approx\nabla f(x)+\nabla^{2}f(x)v=0,
$$ 

which is a linear equation in $v$ , with solution $v=\Delta x_{\mathrm{nt}}$ . So the Newton step $\Delta x_{\mathrm{{nt}}}$ is what must be added to $x$ so that the linearized optimality condition holds. Again, this suggests that when $x$ is near $x^{\star}$ (so the optimality conditions almost hold), the update $x+\Delta{}x_{\mathrm{nt}}$ should be a very good approximation of $x^{\star}$ . 

Whe $n=1$ , i.e. , $f:\mathbf{R}\rightarrow\mathbf{R}$ , this interpretation is particularly simple. The solution x ⋆ of the minimization problem is characterized by $f^{\prime}(x^{\star})=0$ , i.e. , it is 

![](images/cb16e5913310838889f2beaf7a824239d1785327a3073b8950c23c8062a3ae56.jpg) 
Figure 9.18 The solid curve is the derivative $f^{\prime}$ of the function $f$ shown in figure 9.16 . ${\widehat{f}}^{\prime}$ is the linear approximation of $f^{\prime}$ at $x$ . The Newton step $\Delta x_{\mathrm{{nt}}}$ is the diﬀerence between the root of ${\widehat{f}}^{\prime}$ and the point $x$ . 

the zero-crossing of the derivative $f^{\prime}$ , which is monotonically increasing since $f$ is convex. Given our current approximation $x$ of the solution, we form a first-order Taylor approximation of $f^{\prime}$ at $x$ . The zero-crossing of this affine approximation is then $x+\Delta{}x_{\mathrm{nt}}$ . This interpretation is illustrated in figure 9.18 . 

# Affine invariance of the Newton step 

An important feature of the Newton step is that it is independent of linear (or affine) changes of coordinates. Suppose ${\cal T}\,\in\,{\bf R}^{n\times n}$ is nonsingular, and define $f(y)=f(T y)$ ). Then we have 

$$
\nabla\bar{f}(y)=T^{T}\nabla f(x),\qquad\nabla^{2}\bar{f}(y)=T^{T}\nabla^{2}f(x)T,
$$ 

where $x=T y$ . The Newton step for $f$ at $y$ is therefore 

$$
\begin{array}{r c l}{\Delta y_{\mathrm{nt}}}&{=}&{-\left(T^{T}\nabla^{2}f(x)T\right)^{-1}\left(T^{T}\nabla f(x)\right)}\\ &{=}&{-T^{-1}\nabla^{2}f(x)^{-1}\nabla f(x)}\\ &{=}&{T^{-1}\Delta x_{\mathrm{nt}},}\end{array}
$$ 

where $\Delta x_{\mathrm{{nt}}}$ is the Newton step for $f$ at $x$ . Hence the Newton steps of $f$ and $f$ are related by the same linear transformation, and 

$$
x+\Delta x_{\mathrm{nt}}=T(y+\Delta y_{\mathrm{nt}}).
$$ 

# The Newton decrement 

The quantity 

$$
\boldsymbol{\lambda}(\boldsymbol{x})=\left(\nabla f(\boldsymbol{x})^{T}\nabla^{2}f(\boldsymbol{x})^{-1}\nabla f(\boldsymbol{x})\right)^{1/2}
$$ 

is called the Newton decrement at $x$ . We will see that the Newton decrement plays an important role in the analysis of Newton’s method, and is also useful as a stopping criterion. We can relate the Newton decrement to the quantity $f(x)-\operatorname*{inf}_{y}{\widehat{f}}(y)$ ), where $\hat{f}$ is the second-order approximation of $f$ at $x$ : 

$$
f(x)-\operatorname*{inf}_{y}{\widehat{f}}(y)=f(x)-{\widehat{f}}(x+\Delta x_{\mathrm{{nt}}})={\frac{1}{2}}\lambda(x)^{2}.
$$ 

Thus, $\lambda^{2}/2$ is an estimate of $f(x)-p^{\star}$ , based on the quadratic approximation of $f$ at $x$ . 

We can also express the Newton decrement as 

$$
\begin{array}{r}{\lambda(x)=\left(\Delta x_{\mathrm{nt}}^{T}\nabla^{2}f(x)\Delta x_{\mathrm{nt}}\right)^{1/2}.}\end{array}
$$ 

This shows that $\lambda$ is the norm of the Newton step, in the quadratic norm defined by the Hessian, i.e. , the norm 

$$
\|u\|_{\nabla^{2}f(x)}=\left(u^{T}\nabla^{2}f(x)u\right)^{1/2}.
$$ 

The Newton decrement comes up in backtracking line search as well, since we have 

$$
\nabla f(x)^{T}\Delta x_{\mathrm{nt}}=-\lambda(x)^{2}.
$$ 

This is the constant used in a backtracking line search, and can be interpreted as the directional derivative of $f$ at $x$ in the direction of the Newton step: 

$$
-\lambda(x)^{2}=\nabla f(x)^{T}\Delta x_{\mathrm{nt}}=\left.\frac{d}{d t}f(x+\Delta x_{\mathrm{nt}}t)\right|_{t=0}.
$$ 

Finally, we note that the Newton decrement is, like the Newton step, affine in- variant. In other words, the Newton decrement of $f(y)=f(T y)$ ) at , where $T$ is $y$ nonsingular, is the same as the Newton decrement of $f$ at $x=T y$ . 

# 9.5.2 Newton’s method 

Newton’s method, as outlined below, is sometimes called the damped Newton method or guarded Newton method, to distinguish it from the pure Newton method, which uses a fixed step size $t=1$ . 

Algorithm 9.5 Newton’s method. 

given a starting point $x\in\mathbf{dom}\ f$ , tolerance $\epsilon>0$ . repeat 

1. Compute the Newton step and decrement. ∆ x nt := − $\begin{array}{r l}{\cdot\nabla^{2}f(x)^{-1}\nabla f(x);\,}&{{}\lambda^{2}:=\nabla f(x)^{T}\nabla^{2}f(x)^{-1}\nabla f(x)}\end{array}$ ∇ ∇ ∇ ). 2. Stopping criterion. quit if $\lambda^{2}/2\le\epsilon$ . 3. Line search. Choose step size $t$ by backtracking line search. 4. Update. $x:=x+t\Delta x_{\mathrm{nt}}$ . 

This is essentially the general descent method described in § 9.2 , using the New- ton step as search direction. The only diﬀerence (which is very minor) is that the stopping criterion is checked after computing the search direction, rather than after the update. 

# 9.5.3 Convergence analysis 

We assume, as before, that $f$ is twice continuously diﬀerentiable, and strongly convex with constant $m$ , i.e $\nabla^{2}f(x)\succeq m I$ for $x\in S$ . We have see this also implies that there exists an M > 0 such that $\nabla^{2}f(x)\preceq M I$ for all x $x\in S$ . 

In addition, we assume that the Hessian of $f$ is Lipschitz continuous on $S$ with constant $L$ , i.e. , 

$$
\|\nabla^{2}f(x)-\nabla^{2}f(y)\|_{2}\leq L\|x-y\|_{2}
$$ 

for all $x$ , $y\in S$ . The coefficient $L$ , which can be interpreted as a bound on the third derivative of $f$ , can be taken to be zero for a quadratic function. More generally $L$ measures how well $f$ can be approximated by a quadratic model, so we can expect the Lipschitz constant $L$ to play a critical role in the performance of Newton’s method. Intuition suggests that Newton’s method will work very well for a function whose quadratic model varies slowly ( i.e. , has small $L$ ). 

# Idea and outline of convergence proof 

We first give the idea and outline of the convergence proof, and the main conclusion, and then the details of the proof. We will show there are numbers $\eta$ and $\gamma$ with $0<\eta\leq m^{2}/L$ and $\gamma>0$ such that the following hold. 

• If $\|\nabla f(x^{(k)})\|_{2}\geq\eta$ , then 

$$
f(x^{(k+1)})-f(x^{(k)})\leq-\gamma.
$$ 

If $\|\nabla f(x^{(k)})\|_{2}<\eta$ , then the backtracking line search selects $t^{(k)}=1$ and 

$$
\frac{L}{2m^{2}}\|\nabla f(x^{(k+1)})\|_{2}\leq\left(\frac{L}{2m^{2}}\|\nabla f(x^{(k)})\|_{2}\right)^{2}.
$$ 

Let us analyze the implications of the second condition. Suppose that it is satisfied for iteration $k$ , i.e. , $||\nabla f(x^{(k)})||_{2}~<~\eta$ . Since $\eta\;\leq\;m^{2}/L$ , we have $\|\nabla f(x^{(k+1)})\|_{2}\,<\,\eta$ , i.e. , the second condition is also satisfied at iteration $k+1$ . Continuing recursively, we conclude that once the second condition holds, it will hold for all future iterates, i.e. , for all $l\geq k$ , we hav $\|\nabla f(x^{(l)})\|_{2}<\eta$ . Therefore for all $l\geq k$ , the algorithm takes a full Newton step t = 1, and 

$$
\frac{L}{2m^{2}}\|\nabla f(x^{(l+1)})\|_{2}\leq\left(\frac{L}{2m^{2}}\|\nabla f(x^{(l)})\|_{2}\right)^{2}.
$$ 

Applying this inequality recursively, we find that for $l\geq k$ , 

$$
\frac{L}{2m^{2}}\|\nabla f(x^{(l)})\|_{2}\leq\left(\frac{L}{2m^{2}}\|\nabla f(x^{(k)})\|_{2}\right)^{2^{l-k}}\leq\left(\frac{1}{2}\right)^{2^{l-k}},
$$ 

and hence 

$$
f(x^{(l)})-p^{\star}\leq\frac{1}{2m}\|\nabla f(x^{(l)})\|_{2}^{2}\leq\frac{2m^{3}}{L^{2}}\left(\frac{1}{2}\right)^{2^{l-k+1}}.
$$ 

This last inequality shows that convergence is extremely rapid once the second condition is satisfied. This phenomenon is called quadratic convergence . Roughly speaking, the inequality ( 9.35 ) means that, after a sufficiently large number of iterations, the number of correct digits doubles at each iteration. 

The iterations in Newton’s method naturally fall into two stages. The second stage, which occurs once the condition $\|\nabla f(x)\|_{2}\leq\eta$ holds, is called the quadrat- ically convergent stage . We refer to the first stage as the damped Newton phase , because the algorithm can choose a step size $t<1$ . The quadratically convergent stage is also called the pure Newton phase, since in these iterations a step size $t=1$ is always chosen. 

Now we can estimate the total complexity. First we derive an upper bound on the number of iterations in the damped Newton phase. Since $f$ decreases by at least $\gamma$ at each iteration, the number of damped Newton steps cannot exceed 

$$
\frac{f(x^{(0)})-p^{\star}}{\gamma},
$$ 

since if it did, $f$ would be less than $p^{\star}$ , which is impossible. 

We can bound the number of iterations in the quadratically convergent phase using the inequality ( 9.35 ). It implies that we must have $f(x)-p^{\star}\leq\epsilon$ after no more than 

$$
\log_{2}\log_{2}(\epsilon_{0}/\epsilon)
$$ 

iterations in the quadratically convergent phase, where $\epsilon_{0}=2m^{3}/L^{2}$ . Overall, then, the number of iterations until $f(x)-p^{\star}\leq\epsilon$ is bounded above by 

$$
\frac{f(x^{(0)})-p^{\star}}{\gamma}+\log_{2}\log_{2}(\epsilon_{0}/\epsilon).
$$ 

The term $\log_{2}\log_{2}(\epsilon_{0}/\epsilon)$ , which bounds the number of iterations in the quadrati- cally convergent phase, grows extremely slowly with required accuracy $\epsilon$ , and can be considered a constant for practical purposes, say five or six. (Six iterations of the quadratically convergent stage gives an accuracy of about $\epsilon\approx5\cdot10^{-20}\epsilon_{0}$ .) 

Not quite accurately, then, we can say that the number of Newton iterations required to minimize $f$ is bounded above by 

$$
\frac{f(x^{(0)})-p^{\star}}{\gamma}+6.
$$ 

A more precise statement is that ( 9.37 ) is a bound on the number of iterations to compute an extremely good approximation of the solution. 

# Damped Newton phase 

We now establish the inequality ( 9.32 ). Assume $\|\nabla f(x)\|_{2}\geq\eta$ . We first derive a lower bound on the step size selected by the line search. Strong convexity implies that $\nabla^{2}f(x)\preceq M I$ on $S$ , and therefore 

$$
\begin{array}{r c l}{f(x+t\Delta x_{\mathrm{nt}})}&{\leq}&{f(x)+t\nabla f(x)^{T}\Delta x_{\mathrm{nt}}+\displaystyle\frac{M\|\Delta x_{\mathrm{nt}}\|_{2}^{2}}{2}t^{2}}\\ &{\leq}&{f(x)-t\lambda(x)^{2}+\displaystyle\frac{M}{2m}t^{2}\lambda(x)^{2},}\end{array}
$$ 

where we use ( 9.30 ) and 

$$
\begin{array}{r}{\lambda(x)^{2}=\Delta x_{\mathrm{nt}}^{T}\nabla^{2}f(x)\Delta x_{\mathrm{nt}}\geq m\Vert\Delta x_{\mathrm{nt}}\Vert_{2}^{2}.}\end{array}
$$ 

The step size $\hat{t}=m/M$ satisfies the exit condition of the line search, since 

$$
f{}(x+\hat{t}\Delta x_{\mathrm{nt}})\leq f(x)-\frac{m}{2M}\lambda(x)^{2}\leq f(x)-\alpha\hat{t}\lambda(x)^{2}.
$$ 

Therefore the line search returns a step size $t\geq\beta m/M$ , resulting in a decrease of the objective function 

$$
\begin{array}{r c l}{f(x^{+})-f(x)}&{\leq}&{-\alpha t\lambda(x)^{2}}\\ &{\leq}&{-\alpha\beta\displaystyle\frac{m}{M}\lambda(x)^{2}}\\ &{\leq}&{\displaystyle-\alpha\beta\displaystyle\frac{m}{M^{2}}\|\nabla f(x)\|_{2}^{2}}\\ &{\leq}&{\displaystyle-\alpha\beta\eta^{2}\frac{m}{M^{2}},}\end{array}
$$ 

where we use 

$$
\boldsymbol{\lambda}(\boldsymbol{x})^{2}=\nabla f(\boldsymbol{x})^{T}\nabla^{2}f(\boldsymbol{x})^{-1}\nabla f(\boldsymbol{x})\ge(1/M)\|\nabla f(\boldsymbol{x})\|_{2}^{2}.
$$ 

Therefore, ( 9.32 ) is satisfied with 

$$
\gamma=\alpha\beta\eta^{2}\frac{m}{M^{2}}.
$$ 

# Quadratically convergent phase 

We now establish the inequality ( 9.33 ). Assume $||\nabla f(x)||_{2}<\eta$ . We first show that the backtracking line search selects unit steps, provided 

$$
\eta\leq3(1-2\alpha)\frac{m^{2}}{L}.
$$ 

By the Lipschitz condition ( 9.31 ), we have, for $t\geq0$ , 

$$
\begin{array}{r}{\|\nabla^{2}f(x+t\Delta x_{\mathrm{nt}})-\nabla^{2}f(x)\|_{2}\leq t L\|\Delta x_{\mathrm{nt}}\|_{2},}\end{array}
$$ 

and therefore 

$$
\begin{array}{r}{\left|\Delta x_{\mathrm{nt}}^{T}\left(\nabla^{2}f(x+t\Delta x_{\mathrm{nt}})-\nabla^{2}f(x)\right)\Delta x_{\mathrm{nt}}\right|\leq t L\|\Delta x_{\mathrm{nt}}\|_{2}^{3}.}\end{array}
$$ 

With $\ddot{f}(t)\,=\,f(x\,+\,t\Delta x_{\mathrm{nt}})$ ), we have $\dot{f}^{\prime\prime}(t)\,=\,\Delta x_{\mathrm{nt}}^{T}\nabla^{2}f(x+t\Delta x_{\mathrm{nt}})\Delta x_{\mathrm{nt}}$ ∇ , so the inequality above is 

$$
\begin{array}{r}{|\tilde{f}^{\prime\prime}(t)-\tilde{f}^{\prime\prime}(0)|\leq t L\|\Delta x_{\mathrm{nt}}\|_{2}^{3}.}\end{array}
$$ 

We will use this inequality to determine an upper bound on $\tilde{f}(t)$ ). We start with 

$$
\widetilde{f}^{\prime\prime}(t)\leq\widetilde{f}^{\prime\prime}(0)+t L\|\Delta x_{\mathrm{nt}}\|_{2}^{3}\leq\lambda(x)^{2}+t\frac{L}{m^{3/2}}\lambda(x)^{3},
$$ 

where we use $\dot{f}^{\prime\prime}(0)=\lambda(x)^{2}$ and $\lambda(x)^{2}\geq m\Vert\Delta x_{\mathrm{rt}}\Vert_{2}^{2}$ . We integrate the inequality to get 

$$
\begin{array}{r c l}{{\tilde{f}^{\prime}(t)}}&{{\le}}&{{\tilde{f}^{\prime}(0)+t\lambda(x)^{2}+t^{2}\displaystyle\frac{L}{2m^{3/2}}\lambda(x)^{3}}}\\ {{}}&{{=}}&{{-\lambda(x)^{2}+t\lambda(x)^{2}+t^{2}\displaystyle\frac{L}{2m^{3/2}}\lambda(x)^{3},}}\end{array}
$$ 

using $\ddot{f}^{\prime}(0)=-\lambda(x)^{2}$ − . We integrate once more to get 

$$
\tilde{f}(t)\leq\tilde{f}(0)-t\lambda(x)^{2}+t^{2}{\frac{1}{2}}\lambda(x)^{2}+t^{3}{\frac{L}{6m^{3/2}}}\lambda(x)^{3}.
$$ 

Finally, we take $t=1$ to obtain 

$$
f(x+\Delta x_{\mathrm{nt}})\leq f(x)-\frac{1}{2}\lambda(x)^{2}+\frac{L}{6m^{3/2}}\lambda(x)^{3}.
$$ 

Now suppose $\|\nabla f(x)\|_{2}\leq\eta\leq3(1-2\alpha)m^{2}/L$ . By strong convexity, we have 

$$
\lambda(x)\leq3(1-2\alpha)m^{3/2}/L,
$$ 

and by ( 9.39 ) we have 

$$
\begin{array}{l l l}{f(x+\Delta x_{\mathrm{nt}})}&{\leq}&{f(x)-\lambda(x)^{2}\left(\displaystyle\frac{1}{2}-\frac{L\lambda(x)}{6m^{3/2}}\right)}\\ &{\leq}&{f(x)-\alpha\lambda(x)^{2}}\\ &{=}&{f(x)+\alpha\nabla f(x)^{T}\Delta x_{\mathrm{nt}},}\end{array}
$$ 

which shows that the unit step $t=1$ is accepted by the backtracking line search. Let us now examine the rate of convergence. Applying the Lipschitz condition, we have 

$$
\begin{array}{r c l}{\|\nabla f(x^{+})\|_{2}}&{=}&{\|\nabla f(x+\Delta x_{\mathrm{nt}})-\nabla f(x)-\nabla^{2}f(x)\Delta x_{\mathrm{nt}}\|_{2}}\\ &{=}&{\bigg\|\displaystyle\int_{0}^{1}\big(\nabla^{2}f(x+t\Delta x_{\mathrm{nt}})-\nabla^{2}f(x)\big)\,\Delta x_{\mathrm{nt}}\,d t\bigg\|_{2}}\\ &{\leq}&{\displaystyle\frac{L}{2}\|\Delta x_{\mathrm{nt}}\|_{2}^{2}}\\ &{=}&{\displaystyle\frac{L}{2}\|\nabla^{2}f(x)^{-1}\nabla f(x)\|_{2}^{2}}\\ &{\leq}&{\displaystyle\frac{L}{2m^{2}}\|\nabla f(x)\|_{2}^{2},}\end{array}
$$ 

i.e. , the inequality ( 9.33 ). 

In conclusion, the algorithm selects unit steps and satisfies the condition ( 9.33 ) if $\|\nabla f(x^{(k)})\|_{2}<\eta$ , where 

$$
\eta=\operatorname*{min}\left\{1,3(1-2\alpha)\right\}\frac{m^{2}}{L}.
$$ 

Substituting this bound and ( 9.38 ) into ( 9.37 ), we find that the number of iterations is bounded above by 

$$
6+\frac{M^{2}L^{2}/m^{5}}{\alpha\beta\operatorname*{min}\{1,9(1-2\alpha)^{2}\}}(f(x^{(0)})-p^{\star}).
$$ 

![](images/3020ec02eccc97bd55ecb5393e4fffd52e377d5a168ef46fdbea13f7f6cdd0ad.jpg) 
Figure 9.19 Newton’s method for the problem in $\mathbf{R}^{2}$ , with objective $f$ given in ( 9.20 ), and backtracking line search parameters $\alpha\,=\,0.1$ , $\beta\,=\,0.7$ . Also shown are the ellipsoids $\{x\mid\|x-x^{(k)}\|_{\nabla^{2}f(x^{(k)})}\leq1\}$ at the first two iterates. 

# 9.5.4 Examples 

Example in $\mathbf{R}^{2}$ 

We first apply Newton’s method with backtracking line search on the test func- tion ( 9.20 ), with line search parameters $\alpha=0.1$ , $\beta=0.7$ . Figure 9.19 shows the Newton iterates, and also the ellipsoids 

$$
\{x\mid\|x-x^{(k)}\|_{\nabla^{2}}f(x^{(k)})\leq1\}
$$ 

for the first two iterates $k=0$ , 1. The method works well because these ellipsoids give good approximations of the shape of the sublevel sets. 

Figure 9.20 shows the error versus iteration number for the same example. This plot shows that convergence to a very high accuracy is achieved in only five iterations. Quadratic convergence is clearly apparent: The last step reduces the error from about $10^{-5}$ to $10^{-10}$ . 

# Example in $\mathbf{R}^{100}$ 

Figure 9.21 shows the convergence of Newton’s method with backtracking and exact line search for a problem in $\mathbf{R}^{100}$ . The objective function has the form ( 9.21 ), with the same problem data and the same starting point as was used in figure 9.6 . The plot for the backtracking line search shows that a very high accuracy is attained in eight iterations. Like the example in $\mathbf{R}^{2}$ , quadratic convergence is clearly evident after about the third iteration. The number of iterations in Newton’s method with exact line search is only one smaller than with a backtracking line search. This is also typical. An exact line search usually gives a very small improvement in convergence of Newton’s method. Figure 9.22 shows the step sizes for this example. After two damped steps, the steps taken by the backtracking line search are all full, i.e. , $t=1$ . 

Experiments with the values of the backtracking parameters $\alpha$ and $\beta$ reveal that they have little eﬀect on the performance of Newton’s method, for this example 

![](images/21108d5508f5a484f6970d4a00df997bdc123682e16f0d2e78394e53b12812e6.jpg) 
Figure 9.20 Error versus iteration $k$ of Newton’s method for the problem in $\mathbf{R}^{2}$ . Convergence to a very high accuracy is achieved in five iterations. 

![](images/81cec27fc2147a1fdff3926b5f26bfdc7574f7b30f316c150c1f1c331511d041.jpg) 
Figure 9.21 Error versus iteration for Newton’s method for the problem in $\mathbf{R}^{100}$ . The backtracking line search parameters are $\alpha=0.01$ , $\beta=0.5$ . Here too convergence is extremely rapid: a very high accuracy is attained in only seven or eight iterations. The convergence of Newton’s method with exact line search is only one iteration faster than with backtracking line search. 

![](images/2d96f665b5956594ea80a2981aebb4dfc671a43492778db7d9147d9bc1e2f80a.jpg) 
Figure 9.22 The step size $t$ versus iteration for Newton’s method with back- tracking and exact line search, applied to the problem in $\mathbf{R}^{100}$ . The back- tracking line search takes one backtracking step in the first two iterations. After the first two iterations it always selects $t=1$ . 

(and others). With $\alpha$ fixed at 0 . 01, and values of $\beta$ varying between 0 . 2 and 1, the number of iterations required varies between 8 and 12. With $\beta$ fixed at 0 . 5, the number of iterations is 8, for all values of $\alpha$ between 0 . 005 and 0 . 5. For these reasons, most practical implementations use a backtracking line search with a small value of $\alpha$ , such as 0 . 01, and a larger value of $\beta$ , such as 0 . 5. 

# Example in $\mathbf{R}^{10000}$ 

In this last example we consider a larger problem, of the form 

$$
{\mathrm{minimize~}}-\sum_{i=1}^{n}\log(1-x_{i}^{2})-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x)
$$ 

with $m=100000$ and $n=10000$ . The problem data are randomly generated $a_{i}$ sparse vectors. Figure 9.23 shows the convergence of Newton’s method with back- tracking line search, with parameters $\alpha=0.01$ , $\beta=0.5$ . The performance is very similar to the previous convergence plots. A linearly convergent initial phase of about 13 iterations is followed by a quadratically convergent phase, that achieves a very high accuracy in 4 or 5 more iterations. 

# Affine invariance of Newton’s method 

A very important feature of Newton’s method is that it is independent of linear (or affine) changes of coordinates. Let $x^{(k)}$ be the $k$ th iterate of Newton’s method, applied to $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ . Suppose ${\cal T}\,\in\,{\bf R}^{n\times n}$ is nonsingular, and define $f(y)=$ $f(T y)$ . If we use Newton’s method (with the same backtracking parameters) to 

![](images/6f039317449ec9ef0937e3b569a6401fcf4f6e318e3959f3363d78e89414e8b1.jpg) 
Figure 9.23 Error versus iteration of Newton’s method, for a problem in $\mathbf{R}^{10000}$ . A backtracking line search with parameters $\alpha=0.01$ , $\beta=0.5$ is used. Even for this large scale problem, Newton’s method requires only 18 iterations to achieve very high accuracy. 

minimize $f$ , starting from $y^{(0)}=T^{-1}x^{(0)}$ , then we have 

$$
T y^{(k)}=x^{(k)}
$$ 

for all $k$ . In other words, Newton’s method is the same: The iterates are related by the same change of coordinates. Even the stopping criterion is the same, since the Newton decrement for $f$ at $y^{(k)}$ is the same as the Newton decrement for $f$ at $x^{(k)}$ . This is in stark contrast to the gradient (or steepest descent) method, which is strongly aﬀected by changes of coordinates. 

As an example, consider the family of problems given in ( 9.22 ), indexed by the parameter $\gamma$ , which aﬀects the condition number of the sublevel sets. We observed (in figures 9.7 and 9.8 ) that the gradient method slows to useless for values of $\gamma$ smaller than 0 . 05 or larger than 20. In contrast, Newton’s method (with $\alpha=0.01$ , $\beta=0.5$ ) solves this problem (in fact, to a far higher accuracy) in nine iterations, for all values of $\gamma$ between $10^{-10}$ and $10^{10}$ . 

In a real implementation, with finite precision arithmetic, Newton’s method is not exactly independent of affine changes of coordinates, or the condition number of the sublevel sets. But we can say that condition numbers ranging up to very large values such as $10^{10}$ do not adversely aﬀect a real implementation of Newton’s method. For the gradient method, a far smaller range of condition numbers can be tolerated. While choice of coordinates (or condition number of sublevel sets) is a first-order issue for gradient and steepest descent methods, it is a second-order issue for Newton’s method; its only eﬀect is in the numerical linear algebra required to compute the Newton step. 

# Summary 

Newton’s method has several very strong advantages over gradient and steepest descent methods: 

• Convergence of Newton’s method is rapid in general, and quadratic near $x^{\star}$ . Once the quadratic convergence phase is reached, at most six or so iterations are required to produce a solution of very high accuracy. • Newton’s method is affine invariant. It is insensitive to the choice of coordi- nates, or the condition number of the sublevel sets of the objective. • Ne method scales well with problem size. Its p rmance on problems in R $\mathbf{R}^{10000}$ is similar to its performance on problems in R $\mathbf{R}^{10}$ , with only a modest increase in the number of steps required. • The good performance of Newton’s method is not dependent on the choice of algorithm parameters. In contrast, the choice of norm for steepest descent plays a critical role in its performance. 

The main disadvantage of Newton’s method is the cost of forming and storing the Hessian, and the cost of computing the Newton step, which requires solving a set of linear equations. We will see in § 9.7 that in many cases it is possible to exploit problem structure to substantially reduce the cost of computing the Newton step. 

Another alternative is provided by a family of algorithms for unconstrained op- timization called quasi-Newton methods . These methods require less computational eﬀort to form the search direction, but they share some of the strong advantages of Newton methods, such as rapid convergence near $x^{\star}$ . Since quasi-Newton meth- ods are described in many books, and tangential to our main theme, we will not consider them in this book. 

# 9.6 Self-concordance 

There are two major shortcomings of the classical convergence analysis of Newton’s method given in § 9.5.3 . The first is a practical one: The resulting complexity estimates involve the three constants $m$ , $M$ , and $L$ , which are almost never known in practice. As a result, the bound ( 9.40 ) on the number of Newton steps required is almost never known specifically, since it depends on three constants that are, in general, not known. Of course the convergence analysis and complexity estimate are still conceptually useful. 

The second shortcoming is that while Newton’s method is affinely invariant, the classical analysis of Newton’s method is very much dependent on the coordinate system used. If we change coordinates the constants $m$ , $M$ , and $L$ all change. If for no reason other than aesthetic, we should seek an analysis of Newton’s method that is, like the method itself, independent of affine changes of coordinates. In other words, we seek an alternative to the assumptions 

$$
m I\preceq\nabla^{2}f(x)\preceq M I,\qquad\|\nabla^{2}f(x)-\nabla^{2}f(y)\|_{2}\leq L\|x-y\|_{2},
$$ 

that is independent of affine changes of coordinates, and also allows us to analyze Newton’s method. 

A simple and elegant assumption that achieves this goal was discovered by Nesterov and Nemirovski, who gave the name self-concordance to their condition. Self-concordant functions are important for several reasons. 

• They include many of the logarithmic barrier functions that play an impor- tant role in interior-point methods for solving convex optimization problems. • The analysis of Newton’s method for self-concordant functions does not de- pend on any unknown constants. • Self-concordance is an affine-invariant property, i.e. , if we apply a linear transformation of variables to a self-concordant function, we obtain a self- concordant function. Therefore the complexity estimate that we obtain for Newton’s method applied to a self-concordant function is independent of affine changes of coordinates. 

# 9.6.1 Definition and examples 

# Self-concordant functions on R 

We start by considering functions on $\mathbf{R}$ . A convex function $f:\mathbf{R}\rightarrow\mathbf{R}$ is self- concordant if 

$$
|f^{\prime\prime\prime}(x)|\leq2f^{\prime\prime}(x)^{3/2}
$$ 

for all $x\in\mathbf{dom}\,f$ . Since linear and (convex) quadratic functions have zero third derivative, they are evidently self-concordant. Some more interesting examples are given below. 

# Example 9.3 Logarithm and entropy. 

• Negative logarithm. The function $f(x)~=~-\log x$ is self-concordant. Using $f^{\prime\prime}(x)=1/x^{2}$ , $f^{\prime\prime\prime}(x)=-2/x^{3}$ , we find that 

$$
{\frac{|f^{\prime\prime\prime}(x)|}{2f^{\prime\prime}(x)^{3/2}}}={\frac{2/x^{3}}{2(1/x^{2})^{3/2}}}=1,
$$ 

so the defining inequality ( 9.41 ) holds with equality. 

• Negative entropy plus negative logarithm. The function $f(x)=x\log x-\log x$ is self-concordant. To verify this, we use 

$$
f^{\prime\prime}(x)=\frac{x+1}{x^{2}},\qquad f^{\prime\prime\prime}(x)=-\frac{x+2}{x^{3}}
$$ 

to obtain 

$$
\frac{|f^{\prime\prime\prime}(x)|}{2f^{\prime\prime}(x)^{3/2}}=\frac{x+2}{2(x+1)^{3/2}}.
$$ 

The function on the righthand side is maximized on $\mathbf{R}_{+}$ by $x\,=\,0$ , where its value is 1. 

The negative entropy function by itself is not self-concordant; see exercise 11.13 . 

We should make two important remarks about the self-concordance defini- tion ( 9.41 ). The first concerns the mysterious constant 2 that appears in the definition. In fact, this constant is chosen for convenience, in order to simplify the formulas later on; any other positive constant could be used instead. Suppose, for example, that the convex function $f:\mathbf{R}\rightarrow\mathbf{R}$ satisfies 

$$
|f^{\prime\prime\prime}(x)|\leq k f^{\prime\prime}(x)^{3/2}
$$ 

where $k$ is some positive constant. Then the function $\dot{f}(x)=(k^{2}/4)f(x)$ ) satisfies 

$$
\begin{array}{l c l}{{\tilde{f}^{\prime\prime\prime}(x)|}}&{{=}}&{{(k^{2}/4)|f^{\prime\prime\prime}(x)|}}\\ {{}}&{{\leq}}&{{(k^{3}/4)f^{\prime\prime}(x)^{3/2}}}\\ {{}}&{{=}}&{{(k^{3}/4)\left((4/k^{2})\tilde{f}^{\prime\prime}(x)\right)^{3/2}}}\\ {{}}&{{=}}&{{2\tilde{f}^{\prime\prime}(x)^{3/2}}}\end{array}
$$ 

and therefore is self-concordant. This shows that a function that satisfies ( 9.42 ) for some positive $k$ can be scaled to satisfy the standard self-concordance inequal- ity ( 9.41 ). So what is important is that the third derivative of the function is bounded by some multiple of the $3/2$ -power of its second derivative. By appropri- ately scaling the function, we can change the multiple to the constant 2. 

The second comment is a simple calculation that shows why self-concordance is so important: it is affine invariant. Suppose we define the function $\tilde{f}$ by $\ddot{f}(y)=$ $f(a y+b)$ , where $a\ne0$ . Then $\tilde{f}$ is self-concordant if and only if $f$ is. To see this, we substitute 

$$
\tilde{f}^{\prime\prime}(y)=a^{2}f^{\prime\prime}(x),\qquad\tilde{f}^{\prime\prime\prime}(y)=a^{3}f^{\prime\prime\prime}(x),
$$ 

where $x~=~a y+b$ , into the self-concordance inequality for $\tilde{f}$ , i.e. , $|\widetilde{f}^{\prime\prime\prime}(y)|\ \le$ | ≤ $2\tilde{f}^{\prime\prime}(y)^{3/2}$ , to obtain 

$$
|a^{3}f^{\prime\prime\prime}(x)|\leq2(a^{2}f^{\prime\prime}(x))^{3/2},
$$ 

which (after dividing by $a^{3}$ ) is the self-concordance inequality for $f$ . Roughly speaking, the self-concordance condition ( 9.41 ) is a way to limit the third derivative of a function, in a way that is independent of affine coordinate changes. 

# Self-concordant functions on $\mathbf{R}^{n}$ 

We now consider functions on $\mathbf{R}^{n}$ with $n>1$ . We say a function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is self-concordant if it is self-concordant along every line in its domain, i.e. , if the function $\tilde{f}(t)=f(x+t v)$ ) is a self-concordant function of $t$ for all $x\in\mathbf{dom}\,f$ and for all $v$ . 

# 9.6.2 Self-concordant calculus 

# Scaling and sum 

Self-concordance is preserved by scaling by a factor exceeding one: If $f$ is self- concordant and $a\geq1$ , then $a f$ is self-concordant. Self-concordance is also preserved by addition: If $f_{1}$ , $f_{2}$ are self-concordant, then $f_{\mathrm{1}}+f_{\mathrm{2}}$ is self-concordant. To show this, it is sufficient to consider functions f 1 , $f_{2}:\mathbf{R}\rightarrow\mathbf{R}$ . We have 

$$
\begin{array}{r c l}{|f_{1}^{\prime\prime\prime}(x)+f_{2}^{\prime\prime\prime}(x)|}&{\leq}&{|f_{1}^{\prime\prime\prime}(x)|+|f_{2}^{\prime\prime\prime}(x)|}\\ &{\leq}&{2(f_{1}^{\prime\prime}(x)^{3/2}+f_{2}^{\prime\prime}(x)^{3/2})}\\ &{\leq}&{2(f_{1}^{\prime\prime}(x)+f_{2}^{\prime\prime}(x))^{3/2}.}\end{array}
$$ 

In the last step we use the inequality 

$$
(u^{3/2}+v^{3/2})^{2/3}\leq u+v,
$$ 

which holds for $u,~v\geq0$ . 

# Composition with affine function 

If $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ is self-concordant, and $A\,\in\,\mathbf{R}^{n\times m}$ , $b\,\in\,\mathbf{R}^{n}$ , then $f(A x+b)$ is self-concordant. 

Example 9.4 Log barrier for linear inequalities. The function 

$$
f(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),
$$ 

with d $\mathbf{dom}\ f=\{x\ |\ a_{i}^{T}x<b_{i},\ i=1,.\,.\,,m\}$ } , is self-concordant. Ea $-\log(b_{i}-$ $a_{i}^{T}x$ ) is the composition of $-\log y$ with the affine transformation y $y=b_{i}-a_{i}^{T}x$ − , and hence self-concordant. Therefore the sum is also self-concordant. 

Example 9.5 Log-determinant. The function $f(X)=-\log\operatorname*{det}X$ is self-concordant on $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ . To show this, we consider the function $\tilde{f}(t)=f(X+t V)$ ), where $X\succ0$ and $V\in\mathbf{S}^{n}$ . It can be expressed as 

$$
\begin{array}{r c l}{{\tilde{f}(t)}}&{{=}}&{{-\log\operatorname*{det}(X^{1/2}(I+t X^{-1/2}V X^{-1/2})X^{1/2})}}\\ {{}}&{{=}}&{{-\log\operatorname*{det}X-\log\operatorname*{det}(I+t X^{-1/2}V X^{-1/2})}}\\ {{}}&{{=}}&{{-\log\operatorname*{det}X-\displaystyle\sum_{i=1}^{n}\log(1+t\lambda_{i})}}\end{array}
$$ 

where $\lambda_{i}$ are the eigenvalues of $X^{-1/2}V X^{-1/2}$ . Each term $-\log(1+t\lambda_{i})$ is a self- concordant function of $t$ , so the sum, $\ddot{f}$ , is self-concordant. It follows that $f$ is self-concordant. 

Example 9.6 Log of concave quadratic. The function 

$$
\boldsymbol{f}(\boldsymbol{x})=-\log(\boldsymbol{x}^{T}\boldsymbol{P}\boldsymbol{x}+\boldsymbol{q}^{T}\boldsymbol{x}+\boldsymbol{r}),
$$ 

where $P\in-\mathbf{S}_{+}^{n}$ , is self-concordant on 

$$
\mathbf{dom}\ f=\{\boldsymbol{x}\ |\ \boldsymbol{x}^{T}P\boldsymbol{x}+\boldsymbol{q}^{T}\boldsymbol{x}+\boldsymbol{r}>0\}.
$$ 

To show this, it suffices to consider the case $n=1$ (since by restricting $f$ to a line, the general case reduces to the $n=1$ case). We can then express $f$ as 

$$
f(x)=-\log({p x}^{2}+q x+r)=-\log\left(-p(x-a)(b-x)\right)
$$ 

where dom $f=(a,b)$ ( i.e. , $a$ and $b$ are the roots of $p x^{^{2}}\!+\!q x\!+\!r)$ ). Using this expression we have 

$$
f(x)=-\log(-p)-\log(x-a)-\log(b-x),
$$ 

which establishes self-concordance. 

# Composition with logarithm 

Let $g:\mathbf{R}\rightarrow\mathbf{R}$ be a convex function with $\mathbf{dom}\,g=\mathbf{R}_{++}$ , and 

$$
|g^{\prime\prime\prime}(x)|\leq3\frac{g^{\prime\prime}(x)}{x}
$$ 

for all $x$ . Then 

$$
f(x)=-\log(-g(x))-\log x
$$ 

is self-concordant on $\{x\mid x>0,\ g(x)<0\}$ . (For a proof, see exercise 9.14 .) 

The condition ( 9.43 ) is homogeneous and preserved under addition. It is sat- isfied by all (convex) quadratic functions, i.e. , functions of the form $a x^{2}+b x+c$ , where $a\geq0$ . Therefore if ( 9.43 ) holds for a function $g$ , then it holds for the function $g(x)+a x^{2}+b x+c$ , where $a\geq0$ . 

Example 9.7 The following functions $g$ satisfy the condition ( 9.43 ). 

• $g(x)=-x^{p}$ for $0<p\le1$ . • $g(x)=-\log x$ . • $g(x)=x\log x$ . • $g(x)=x^{p}$ for $-1\leq p\leq0$ . • $g(x)=(a x+b)^{2}/x$ . 

It follows that in each case, the function $f(x)=-\log(-g(x))\!-\!\log x$ is self-concordant. More generally, the function $f(x)\;=\;-\log(-g(x)\,-\,a x^{2}\,-\,b x\,-\,c)\,-\,\log x$ is self- concordant on its domain, 

$$
\{x\mid x>0,\ g(x)+a x^{2}+b x+c<0\},
$$ 

provided $a\geq0$ . 

Example 9.8 The composition with logarithm rule allows us to show self-concordance of the following functions. 

We leave the details as an exercise (exercise 9.15 ). 

# 9.6.3 Properties of self-concordant functions 

In § 9.1.2 we used strong convexity to derive bounds on the suboptimality of a point $x$ in terms of the norm of the gradient at $x$ . For strictly convex self-concordant functions, we can obtain similar bounds in terms of the Newton decrement 

$$
\lambda(x)=\left(\nabla f(x)^{T}\nabla^{2}f(x)^{-1}\nabla f(x)\right)^{1/2}.
$$ 

(It can be shown that the Hessian of a strictly convex self-concordant function is positive definite everywhere; see exercise 9.17 .) Unlike the bounds based on the norm of the gradient, the bounds based on the Newton decrement are not aﬀected by an affine change of coordinates. 

For future reference we note that the Newton decrement can also be expressed as 

$$
\lambda(x)=\operatorname*{sup}_{v\neq0}\frac{-v^{T}\nabla f(x)}{(v^{T}\nabla^{2}f(x)v)^{1/2}}
$$ 

(see exercise 9.9 ). In other words, we have 

$$
\begin{array}{r}{\frac{-v^{T}\nabla f(x)}{(v^{T}\nabla^{2}f(x)v)^{1/2}}\leq\lambda(x)}\end{array}
$$ 

for any nonzero $v$ , with equality for $v=\Delta x_{\mathrm{nt}}$ . 

# Upper and lower bounds on second derivatives 

Suppose $f:\mathbf{R}\rightarrow\mathbf{R}$ is a strictly convex self-concordant function. We can write the self-concordance inequality ( 9.41 ) as 

$$
\left\lvert\frac{d}{d t}\left(f^{\prime\prime}(t)^{-1/2}\right)\right\rvert\leq1
$$ 

for all $t\in\mathbf{dom}\,f$ (see exercise 9.16 ). Assuming $t\geq0$ and the interval between 0 and $t$ is in $\mathbf{dom}\ f$ , we can integrate ( 9.45 ) between $0$ and $t$ to obtain 

$$
-t\leq\int_{0}^{t}\frac{d}{d\tau}\left(f^{\prime\prime}(\tau)^{-1/2}\right)d\tau\leq t,
$$ 

i.e. , $-t\le f^{\prime\prime}(t)^{-1/2}-f^{\prime\prime}(0)^{-1/2}\le t$ . From this we obtain lower and upper bounds on $f^{\prime\prime}(t)$ : 

$$
\frac{f^{\prime\prime}(0)}{\left(1+t f^{\prime\prime}(0)^{1/2}\right)^{2}}\leq f^{\prime\prime}(t)\leq\frac{f^{\prime\prime}(0)}{\left(1-t f^{\prime\prime}(0)^{1/2}\right)^{2}}.
$$ 

The lower bound is valid for all nonnegative $t\in\mathbf{dom}\ f$ ; the upper bound is valid if $t\in\mathbf{dom}\ f$ and $0\leq t<f^{\prime\prime}(0)^{-1/2}$ . 

# Bound on suboptimality 

Let $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ be a strictly convex self-concordant function, and let $v$ be a descent direction ( i.e. , any direction satisfying $v^{T}\nabla f(x)<0$ , not necessarily the Newton direction). Define $\Dot{f}:\mathbf{R}\to\mathbf{R}$ → as $\dot{f}(t)\,=\,f(x+t v)$ ). By definition, the function $\tilde{f}$ is self-concordant. 

Integrating the lower bound in ( 9.46 ) yields a lower bound on ${\ddot{f}}^{\prime}(t)$ ): 

$$
\tilde{f}^{\prime}(t)\geq\tilde{f}^{\prime}(0)+\tilde{f}^{\prime\prime}(0)^{1/2}-\frac{\tilde{f}^{\prime\prime}(0)^{1/2}}{1+t\tilde{f}^{\prime\prime}(0)^{1/2}}.
$$ 

Integrating again yields a lower bound on $\tilde{f}(t)$ ): 

$$
\tilde{f}(t)\geq\tilde{f}(0)+t\tilde{f}^{\prime}(0)+t\tilde{f}^{\prime\prime}(0)^{1/2}-\log(1+t\tilde{f}^{\prime\prime}(0)^{1/2}).
$$ 

The righthand side reaches its minimum at 

$$
\bar{t}=\frac{-\tilde{f}^{\prime}(0)}{\tilde{f}^{\prime\prime}(0)+\tilde{f}^{\prime\prime}(0)^{1/2}\tilde{f}^{\prime}(0)},
$$ 

and evaluating at t provides a lower bound on $\ddot{f}$ : 

$$
\begin{array}{r c l}{\displaystyle\operatorname*{inf}_{t\ge0}\tilde{f}(t)}&{\ge}&{\tilde{f}(0)+\bar{t}\tilde{f}^{\prime}(0)+\bar{t}\tilde{f}^{\prime\prime}(0)^{1/2}-\log(1+\bar{t}\tilde{f}^{\prime\prime}(0)^{1/2})}\\ &{=}&{\tilde{f}(0)-\tilde{f}^{\prime}(0)\tilde{f}^{\prime\prime}(0)^{-1/2}+\log(1+\tilde{f}^{\prime}(0)\tilde{f}^{\prime\prime}(0)^{-1/2}).}\end{array}
$$ 

The inequality ( 9.44 ) can be expressed as 

$$
\lambda(x)\ge-\tilde{f}^{\prime}(0)\tilde{f}^{\prime\prime}(0)^{-1/2}
$$ 

(with equality when $v=\Delta x_{\mathrm{nt}}$ ), since we have 

$$
\begin{array}{r}{\tilde{f}^{\prime}(0)=v^{T}\nabla f(x),\qquad\tilde{f}^{\prime\prime}(0)=v^{T}\nabla^{2}f(x)v.}\end{array}
$$ 

Now using the fact that $u+\log(1-u)$ is a monotonically decreasing function of $u$ , and the inequality above, we get 

$$
\operatorname*{inf}_{t\geq0}\tilde{f}(t)\geq\tilde{f}(0)+\lambda(x)+\log(1-\lambda(x)).
$$ 

This inequality holds for any descent direction $v$ . Therefore 

$$
p^{\star}\geq f(x)+\lambda(x)+\log(1-\lambda(x))
$$ 

provided $\lambda(x)\,<\,1$ . The function $-\left(\lambda+\log(1-\lambda)\right)$ is plotted in figure 9.24 . It satisfies 

$$
-\left(\lambda+\log(1-\lambda)\right)\approx\lambda^{2}/2,
$$ 

for small $\lambda$ , and the bound 

$$
-\left(\lambda+\log(1-\lambda)\right)\leq\lambda^{2}
$$ 

for $\lambda\leq0.68$ . Thus, we have the bound on suboptimality 

$$
p^{\star}\geq f(x)-\lambda(x)^{2},
$$ 

valid for $\lambda(x)\leq0.68$ . 

Recall that $\lambda(x)^{2}/2$ is the estimate of $f(x)-p^{\star}$ , based on the quadratic model at $x$ ; the inequality ( 9.50 ) shows that for self-concordant functions, doubling this estimate gives us a provable bound. In particular, it shows that for self-concordant functions, we can use the stopping criterion 

$$
\lambda(x)^{2}\leq\epsilon,
$$ 

(where $\epsilon<0.68^{2}$ ), and guarantee that on exit $f(x)-p^{\star}\leq\epsilon$ . 

![](images/61951ee3a091a5aa6235a4d33aab9b0db823ef1c286c2aa6dc30f423d512be31.jpg) 
Figure 9.24 The s line is the function $-(\lambda\!+\!\log(1\!-\!\lambda))$ , which for small $\lambda$ is approximately $\lambda^{2}/2$ 2. The dashed line shows λ $\lambda^{2}$ , which is an upper bound in the interval $0\leq\lambda\leq0.68$ . 

# 9.6.4 Analysis of Newton’s method for self-concordant functions 

We now analyze Newton’s method with backtracking line search, when applied to a strictly convex self-concordant function $f$ . As before, we assume that a starting point $x^{(0)}$ is known, and that the sublevel set $S=\{x\mid f(x)\leq f(x^{(0)})\}$ is clos . We also assume that $f$ is bounded below. (This implies that $f$ has a minimizer x $x^{\star}$ ; see exercise 9.19 .) 

The analysis is very similar to the classical analysis given in § 9.5.2 , except that we use self-concordance as the basic assumption instead of strong convexity and the Lipschitz condition on the Hessian, and the Newton decrement will play the role of the norm of the gradient. We will show that there are numbers $\eta$ and $\gamma>0$ , with $0<\eta\leq1/4$ , that depend only on the line search parameters $\alpha$ and $\beta$ , such that the following hold: 

• If $\lambda(x^{(k)})>\eta$ , then 

$$
f(x^{(k+1)})-f(x^{(k)})\leq-\gamma.
$$ 

• If $\lambda(x^{(k)})\leq\eta$ , then the backtracking line search selects $t=1$ and 

$$
2\lambda(x^{(k+1)})\leq\left(2\lambda(x^{(k)})\right)^{2}.
$$ 

These are the analogs of ( 9.32 ) and ( 9.33 ). As in § 9.5.3 , the second condition can be applied recursively, so we can conclude that for all $l\geq k$ , we have $\lambda(x^{(l)})\le\eta$ , and 

$$
2\lambda(x^{(l)})\leq\left(2\lambda(x^{(k)})\right)^{2^{l-k}}\leq(2\eta)^{2^{l-k}}\leq\left(\frac{1}{2}\right)^{2^{l-k}}.
$$ 

As a consequence, for all $l\geq k$ , 

$$
f(x^{(l)})-p^{\star}\leq\lambda(x^{(l)})^{2}\leq\frac{1}{4}\left(\frac{1}{2}\right)^{2^{l-k+1}}\leq\left(\frac{1}{2}\right)^{2^{l-k+1}},
$$ 

and hence $f(x^{(l)})-p^{\star}\leq\epsilon$ if $l-k\geq\log_{2}\log_{2}(1/\epsilon)$ . 

The first inequality implies that the damped phase cannot require more than $(f(x^{(0)})-p^{\star})/\gamma$ steps. Thus the total num of iterations required to obtain an accuracy $f(x)-p^{\star}\leq\epsilon$ , starting at a point x $x^{(0)}$ , is bounded by 

$$
\frac{f(x^{(0)})-p^{\star}}{\gamma}+\log_{2}\log_{2}(1/\epsilon).
$$ 

This is the analog of the bound ( 9.36 ) in the classical analysis of Newton’s method. 

# Damped Newton phase 

Let $\tilde{f}(t)=f(x+t\Delta x_{\mathrm{nt}})$ ), so we have 

$$
\tilde{f}^{\prime}(0)=-\lambda(x)^{2},\qquad\tilde{f}^{\prime\prime}(0)=\lambda(x)^{2}.
$$ 

If we integrate the upper bound in ( 9.46 ) twice, we obtain an upper bound for $\tilde{f}(t)$ ): 

$$
\begin{array}{l l l}{{\tilde{f}(t)}}&{{\leq}}&{{\tilde{f}(0)+t\tilde{f}^{\prime}(0)-t\tilde{f}^{\prime\prime}(0)^{1/2}-\log\left(1-t\tilde{f}^{\prime\prime}(0)^{1/2}\right)}}\\ {{}}&{{=}}&{{\tilde{f}(0)-t\lambda(x)^{2}-t\lambda(x)-\log(1-t\lambda(x)),}}\end{array}
$$ 

valid for $0\leq t<1/\lambda(x)$ . 

We can use this bound to show the backtracking line search always results in a step size $t\geq\beta/(1+\lambda(x))$ . To prove this we note that the point $\hat{t}=1/(1+\lambda(x))$ satisfies the exit condition of the line search: 

$$
\begin{array}{l c l}{\hat{f}(\hat{t})}&{\leq}&{\hat{f}(0)-\hat{t}\lambda(x)^{2}-\hat{t}\lambda(x)-\log(1-\hat{t}\lambda(x))}\\ &{=}&{\tilde{f}(0)-\lambda(x)+\log(1+\lambda(x))}\\ &{\leq}&{\tilde{f}(0)-\alpha\cfrac{\lambda(x)^{2}}{1+\lambda(x)}}\\ &{=}&{\tilde{f}(0)-\alpha\lambda(x)^{2}\hat{t}.}\end{array}
$$ 

The second inequality follows from the fact that 

$$
-x+\log(1+x)+{\frac{x^{2}}{2(1+x)}}\leq0
$$ 

for $x\geq0$ . Since $t\geq\beta/(1+\lambda(x))$ , we have 

$$
\tilde{f}(t)-\tilde{f}(0)\leq-\alpha\beta\frac{\lambda(x)^{2}}{1+\lambda(x)},
$$ 

so ( 9.51 ) holds with 

$$
\gamma=\alpha\beta\frac{\eta^{2}}{1+\eta}.
$$ 

# Quadratically convergent phase 

We will show that we can take 

$$
\eta=(1-2\alpha)/4,
$$ 

(which satisfies $0<\eta<1/4$ , since $0<\alpha<1/2$ ), i.e. , if $\lambda(x^{(k)})\leq(1-2\alpha)/4$ , then the backtracking line search accepts the unit step and ( 9.52 ) holds. 

We first note that the upper bound ( 9.54 ) implies that a unit step $t=1$ yields a point in $\mathbf{dom}\,f$ if $\lambda(x)<1$ . Moreover, if $\lambda(x)\leq(1-2\alpha)/2$ , we have, using ( 9.54 ), 

$$
\begin{array}{r c l}{\tilde{f}(1)}&{\leq}&{\tilde{f}(0)-\lambda(x)^{2}-\lambda(x)-\log(1-\lambda(x))}\\ &{\leq}&{\tilde{f}(0)-\displaystyle\frac{1}{2}\lambda(x)^{2}+\lambda(x)^{3}}\\ &{\leq}&{\tilde{f}(0)-\alpha\lambda(x)^{2},}\end{array}
$$ 

so the unit step satisfies the condition of sufficient decrease. (The second line follows from the fact that $\textstyle-x-\log(1-x)\leq{\frac{1}{2}}x^{2}+x^{3}$ for $0\leq x\leq0.81$ .) 

The inequality $(9.52)$ follows from the following fact, proved in exercise 9.18 . If $\lambda(x)<1$ , and $x^{+}=x-\nabla^{2}f(x)^{-1}\nabla f(x)$ , then 

$$
\lambda(x^{+})\leq\frac{\lambda(x)^{2}}{(1-\lambda(x))^{2}}.
$$ 

In particular, if $\lambda(x)\leq1/4$ , 

$$
\lambda(x^{+})\leq2\lambda(x)^{2},
$$ 

which proves that ( 9.52 ) holds when $\lambda(x^{(k)})\leq\eta$ . 

# The final complexity bound 

Putting it all together, the bound ( 9.53 ) on the number of Newton iterations be- comes 

$$
\frac{f(x^{(0)})-p^{\star}}{\gamma}+\log_{2}\log_{2}(1/\epsilon)=\frac{20-8\alpha}{\alpha\beta(1-2\alpha)^{2}}\bigl(f(x^{(0)})-p^{\star}\bigr)+\log_{2}\log_{2}(1/\epsilon).
$$ 

This expression depends only on the line search parameters $\alpha$ and $\beta$ , and the final accuracy $\epsilon$ . Moreover the term involving $\epsilon$ can be safely replaced by the constant six, so the bound really depends only on $\alpha$ and $\beta$ . For typical values of $\alpha$ and $\beta$ , the const scales $f(x^{(0)})-p^{\star}$ is on the order of several hundred ple, with α $\alpha\,=\,0.1$ = 0 . 1, $\beta\,=\,0.8$ , the scaling factor is 375. With tolerance ǫ $\epsilon\,=\,10^{-10}$ , we obtain the bound 

$$
375(f(x^{(0)})-p^{\star})+6.
$$ 

We will see that this bound is fairly conservative, but does capture what appears to be the general form of the worst-case number of Newton steps required. A more refined analysis, such as the one originally given by Nesterov and Nemirovski, gives a similar bound, with a substantially smaller constant scaling $f(x^{(0)})-p^{\star}$ . 

![](images/42088d2b6a62bcb9c73ea282981cc00863e057ffe9627a5e969a3bf6ac9fee2a.jpg) 
Figure 9.25 Number of Newton iterations required to minimize self- concordant functions versus $f(x^{(0)})\,-\,p^{\star}$ . The function $f$ has the form $\begin{array}{r}{f(x)\,=\,-\sum_{i=1}^{m}\log(b_{i}\,-\,a_{i}^{T}x)}\end{array}$ − ), where the problem data and $b$ are ran- $a_{i}$ domly generated. The circles show problems with $m\,=\,100$ , $n\,=\,50$ ; the squares show problems with $m\,=\,1000$ , $n\,=\,500$ ; and the diamonds show problems with $m=1000$ , $n=50$ . Fifty instances of each are shown. 

# 9.6.5 Discussion and numerical examples 

# A family of self-concordant functions 

It is interesting to compare the upper bound ( 9.57 ) with the actual number of iterations required to minimize a self-concordant function. We consider a family of problems of the form 

$$
f(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x).
$$ 

The problem data $a_{i}$ and $b$ were generated as follows. For each problem instance, the coefficients of $a_{i}$ were generated from independent normal distributions with mean zero and unit variance, and the coefficients $b$ were generated from a uniform distribution on $[0,1]$ . Problem instances which were unbounded below were dis- carded. For each problem we first compute $x^{\star}$ . We then generate a starting point by choosing a random direction $v$ , and taking $\boldsymbol{x}^{(0)}=\boldsymbol{x}^{\star}+\boldsymbol{s}\boldsymbol{v}$ , where $s$ is chosen so that $f(x^{(0)})-p^{\star}$ has a prescribed value between $0$ and 35. (We should point out that starting points with values $f(x^{(0)})-p^{\star}=10$ or higher are actually very close to the boundary of the polyhedron.) We then minimize the function using New- ton’s method with a backtracking line search with parameters $\alpha\,=\,0.1$ , $\beta\,=\,0.8$ , and tolerance $\epsilon=10^{-10}$ . 

Figure 9.25 shows the number of Newton iterations required $f(x^{(0)})-p^{\star}$ for 150 problem instances. The circles show 50 problems with m = 100, n = 50; the squares show 50 problems with $m=1000$ , $n=500$ ; and the diamonds show 50 problems with $m=1000$ , $n=50$ . 

For the values of the backtracking parameters used, the complexity bound found above is 

$$
375(f(x^{(0)})-p^{\star})+6,
$$ 

clearly a much larger value than the number of iterations required (for these 150 instances). The plot suggests that there is a valid bound of the same form, but with a much smaller constant (say, around 1 . 5) scaling $f(x^{(0)})-p^{\star}$ . Indeed, the expression 

$$
f(x^{(0)})-p^{\star}+6
$$ 

is not a bad gross predictor of the number of Newton steps required, although it is clearly not the only factor. First, there are plenty of problems instances where the number of Newton steps is somewhat smaller, which correspond, we can guess, to

 ‘lucky’ starting points. Note also that for the larger problems, with 500 variables

 (represented by the squares), there seem to be even more cases where the number of Newton steps is unusually small. 

We should mention here that the problem family we study is not just self- concordant, but in fact minimally self-concordant, by which we mean that $\alpha f$ is not self-concordant for $\alpha\,<\,1$ . Hence, the bound ( 9.58 ) cannot be improved by simply scaling $f$ . (The function $f(x)\;=\;-20\log x$ is an example of a self- concordant function which is not minimally self-concordant, since $(1/20)f$ is also self-concordant.) 

# Practical importance of self-concordance 

We have already observed that Newton’s method works in general very well for strongly convex objective functions. We can justify this vague statement empir- ically, and also using the classical analysis of Newton’s method, which yields a complexity bound, but one that depends on several constants that are almost al- ways unknown. 

For self-concordant functions we can say somewhat more. We have a complexity bound that is completely explicit, and does not depend on any unknown constants. Empirical studies suggest that this bound can be tightened considerably, but its general form, a small constant plus a multiple of $f(x^{(0)})-p^{\star}$ , seems to predict, at least crudely, the number of Newton steps required to minimize an approximately minimally self-concordant function. 

It is not yet clear whether self-concordant functions are in practice more easily minimized by Newton’s method than non-self-concordant functions. (It is not even clear how one would make this statement precise.) At the moment, we can say that self-concordant functions are a class of functions for which we can say considerably more about the complexity of Newton’s method than is the case for non-self-concordant functions. 

# 9.7 Implementation 

In this section we discuss some of the issues that arise in implementing an un- constrained minimization algorithm. We refer the reader to appendix C for more details on numerical linear algebra. 

# 9.7.1 Pre-computation for line searches 

In the simplest implementation of a line search, $f(x+t\Delta x)$ is evaluated for each value of $t$ in the same way that $f(z)$ is evaluated for any $z\in\mathbf{dom}\,f$ . But in some cases we can exploit the fact that $f$ (and its derivatives, in an exact line search) are to be evaluated at many points along the ray $\{x+t\Delta x\mid t\geq0\}$ to reduce the total computational eﬀort. This usually requires some pre-computation, which is often on the same order as computing $f$ at any point, after which $f$ (and its derivatives) can be computed more efficiently along the ray. 

Suppose that $x\in\mathbf{dom}\,f$ $\Delta x\in\mathbf{R}^{n}$ , and define $\ddot{f}$ as $f$ restricted to the line or ray determined by $x$ and ∆ $\Delta x$ , i.e. , $\dot{f}(t)=f(x+t\Delta x)$ ). In a backtracking line search we must evaluate $\tilde{f}$ for several, and possibly many, values of $t$ ; in an exact line search method we must evaluate $\tilde{f}$ and one or more derivatives at a number of values of $t$ . In the simple method described above, we evaluate $\tilde{f}(t)$ ) by first forming $z=x+t\Delta x$ , and then evaluating $f(z)$ . To evaluate $\widetilde{f}^{\prime}(t)$ ), we form $z=x+t\Delta x$ , then evaluate $\nabla f(z)$ , and then compute ${\dot{f}}^{\prime}(t)=\nabla f(z)^{T}\Delta x$ ∇ . In some representative examples below we show how $\ddot{f}$ can be computed at a number of values of $t$ more efficiently. 

# Composition with an affine function 

A very general case in which pre-computation can speed up the line search process occurs when the objective has the form $f(x)=\phi(A x+b)$ , where $A\in\mathbf{R}^{p\times n}$ , and $\phi$ is easy to evaluate (for example, separable). To evaluate $\tilde{f}(t)=f(x+t\Delta x)$ ) for k values of $t$ using the simple approach, we form $A(x+t\Delta x)+b$ for each value of $t$ (which costs $2k p n$ ﬂops), and then evaluate $\phi(A(x+t\Delta x)+b)$ for each value of $t$ . This can be done more efficiently by first computing $A x+b$ and $A\Delta x$ (4 pn ﬂops), then forming $A(x+t\Delta x)+b$ for each value of $t$ using 

$$
A(x+t\Delta x)+b=(A x+b)+t(A\Delta x),
$$ 

which costs $2k p$ ﬂops. The total cost, keeping only the dominant terms, is $\scriptstyle4p n+2k p$ ﬂops, compared to $2k p n$ for the simple method. 

# Analytic center of a linear matrix inequality 

Here we give an example that is more specific, and more complete. We consider the problem ( 9.6 ) of computing the analytic center of a linear matrix inequality, i.e. , minimizing $\log\operatorname*{det}F(x)^{-1}$ , w e $x\in\mathbf{R}^{n}$ and $F:\mathbf{R}^{n}\rightarrow\mathbf{S}^{p}$ is affine. Along the line through $x$ with direction ∆ $\Delta x$ x we have 

$$
\tilde{f}(t)=\log\operatorname*{det}(F(x+t\Delta x))^{-1}=-\log\operatorname*{det}(A+t B)
$$ 

where 

$$
A=F(x),\qquad B=\Delta x_{1}F_{1}+\cdot\cdot\cdot+\Delta x_{n}F_{n}\in\mathbf{S}^{p}.
$$ 

Since $A\succ0$ , it has a Cholesky factorization $A=L L^{T}$ , where $L$ is lower triangular and nonsingular. Therefore we can express $\ddot{f}$ ˜ as 

$$
{\tilde{f}}(t)=-\log\operatorname*{det}\left(L(I+t L^{-1}B L^{-T})L^{T}\right)=-\log\operatorname*{det}A-\sum_{i=1}^{p}\log(1+t\lambda_{i})
$$ 

where $\lambda_{1},.\,.\,.\,,\lambda_{p}$ are the eigenvalues of $L^{-1}B L^{-T}$ . Once these eigenvalues are computed, we can evaluate $\tilde{f}(t)$ ), for any $t$ , with $4p$ simple arithmetic computations, by using the formula on the right hand side of ( 9.59 ). We can evaluate ${\ddot{f}}^{\prime}(t)$ ) (and similarly, any higher derivative) in $4p$ operations, using the formula 

$$
\tilde{f}^{\prime}(t)=-\sum_{i=1}^{p}\frac{\lambda_{i}}{1+t\lambda_{i}}.
$$ 

Let us compare the two methods for carrying out a line search, assuming that we need to evaluate $f(x+t\Delta x)$ for $k$ values of $t$ . In the simple method, for each value of $t$ we form $F(x{+}t\Delta x)$ , and then evaluate $f(x{+}t\Delta x)$ as $-\log\operatorname*{det}F(x{+}t\Delta x)$ . For example, we can find the Cholesky factorization of $F(x+t\Delta x)=L L^{T}$ , and then evaluate 

$$
-\log\operatorname*{det}F(x+t\Delta x)=-2\sum_{i=1}^{p}\log L_{i i}.
$$ 

The cost is $n p^{2}$ to form $F(x+t\Delta x)$ , plus $(1/3)p^{3}$ for the Cholesky factorization. Therefore the total cost of the line search is 

$$
k(n p^{2}+(1/3)p^{3})=k n p^{2}+(1/3)k p^{3}.
$$ 

Using the method outlined above, we first form $A$ , which costs $n p^{2}$ , and factor it, which costs $(1/3)p^{3}$ . We also form $B$ (which costs $n p^{2}$ ), and $L^{-1}B L^{-T}$ , which costs $2p^{3}$ . The eigenvalues of this matrix are then computed, at a cost of about $(4/3)p^{3}$ ﬂops. This pre-computation requires a total of $2n p^{2}+(11/3)p^{3}$ ﬂops. After finishing this pre-computation, we can now evaluate ${\ddot{f}}(t)$ ) for each value of $t$ at a cost of $4p$ ﬂops. The total cost is then 

$$
2n p^{2}+(11/3)p^{3}+4k p.
$$ 

Assuming $k$ is small compared to $p(2n+(11/3)p)$ , this means the entire line search can be carried out at an eﬀort comparable to simply evaluating $f$ . Depending on the values of $k$ , $p$ , and $n$ , the savings over the simple method can be as large as order $k$ . 

# 9.7.2 Computing the Newton step 

In this section we brieﬂy describe some of the issues that arise in implementing Newton’s method. In most cases, the work of computing the Newton step $\Delta x_{\mathrm{{nt}}}$ dominates the work involved in the line search. To compute the Newton step $\Delta x_{\mathrm{{nt}}}$ , we first evaluate and form the Hessian matrix $H=\nabla^{2}f(x)$ and the gradient $g\,=\,\nabla f(x)$ at $x$ . Then we solve the system of linear equations $H\,\Delta x_{\mathrm{{nt}}}\,=\,-g$ to find the Newton step. This set of equations is sometimes called the Newton system (since its solution gives the Newton step) or the normal equations , since the same type of equation arises in solving a least-squares problem (see 9.1.1 ). 

While a general linear equation solver can be used, it is better to use methods that take advantage of the symmetry and positive definiteness of $H$ . The most common approach is to form the Cholesky factorization of $H$ , i.e. , to compute a lower triangular matrix $L$ that satisfies $L L^{T}=H$ see § C.3.2 ). We $L w=$ $-g$ by forward substitution, to obtain $w=-L^{-1}g$ − , and then solve $L^{T}\Delta x_{\mathrm{nt}}=w$ by back substitution, to obtain 

$$
\Delta x_{\mathrm{nt}}=L^{-T}w=-L^{-T}L^{-1}g=-H^{-1}g.
$$ 

We can compute the Newton decrement as $\lambda^{2}=-\Delta x_{\mathrm{nt}}^{T}g$ , or use the formula 

$$
\lambda^{2}=g^{T}H^{-1}g=\|L^{-1}g\|_{2}^{2}=\|w\|_{2}^{2}.
$$ 

If a dense (unstructured) Cholesky factorization is used, the cost of the forward and back substitution is dominated by the cost of the Cholesky factorization, which is $(1/3)n^{3}$ ﬂops. The total cost of computing the Newton step $\Delta x_{\mathrm{{nt}}}$ is thus $F{+}(1/3)n^{3}$ ﬂops, where $F$ is the cost of forming $H$ and . $g$ 

It is often possible to solve the ewton system $H\Delta x_{\mathrm{{nt}}}=-g$ more efficiently, by exploiting special structure in H , such as band structure or sparsity. In this context, ‘structure of $H^{\prime}$ means structure that is the same for all $x$ . For example, when we say that $H$ is tridiagonal’ we mean that for every $x\in\mathbf{dom}\,f$ , $\nabla^{2}f(x)$ is tridiagonal. 

# Band structure 

If the Hessian $H$ is banded with bandwidth $k$ , i.e. , $H_{i j}=0$ for $|i-j|>k$ , then the banded Cholesky factorization can be used, as well as banded forward and back tions. The cost of computing the Newton step $\Delta x_{\mathrm{{nt}}}\,=\,-H^{-1}g$ is then $F+n k^{2}$ ﬂops (assuming $k\ll n$ ), compared to $F+(1/3)n^{3}$ for a dense factorization and substitution method. 

The Hessian band structure condition 

$$
\nabla^{2}f(x)_{i j}={\frac{\partial^{2}f(x)}{\partial x_{i}\partial x_{j}}}=0\quad{\mathrm{for}}\quad|i-j|>k,
$$ 

for all $x~\in~\mathbf{dom}\,f$ , has an interesting interpretation in terms of the objective function $f$ . Roughly speaking it means that in the objective function, each variable $x_{i}$ couples nonlinearly only to the $2k+1$ variables $x_{j}$ , $j\,=\,i\,-\,k,.\,.\,.\,,i+k$ . This occurs when $f$ has the partial separability form 

$$
f(x)=\psi_{1}(x_{1},\ldots,x_{k+1})+\psi_{2}(x_{2},\ldots,x_{k+2})+\cdot\cdot\cdot+\psi_{n-k}(x_{n-k},\ldots,x_{n}),
$$ 

wh re $\psi_{i}:\mathbf{R}^{k+1}\rightarrow\mathbf{R}$ . In other words, $f$ can be expressed as a sum of functions of k consecutive variables. 

Example 9.9 Consider the problem of minimizing $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , which has the form 

$$
f(x)=\psi_{1}(x_{1},x_{2})+\psi_{2}(x_{2},x_{3})+\cdot\cdot\cdot+\psi_{n-1}(x_{n-1},x_{n}),
$$ 

where $\psi_{i}:\mathbf{R}^{2}\to\mathbf{R}$ are convex and twice diﬀerentiable. Because of this form, the Hessian ∇ $\nabla^{2}f$ is tridiagonal, since $\partial^{2}f/\partial x_{i}\partial x_{j}=0$ for $|i-j|>1$ . (And conversely, if the Hessian of a function is tridiagonal for all $x$ , then it has this form.) 

Using Cholesky factorization and forward and back substitution algorithms for tridi- agonal matrices, we can solve the Newton system for this problem in order $n$ ﬂops. This should be compared to order $n^{3}$ ﬂops, if the special form of $f$ were not exploited. 

# Sparse structure 

More generally we can exploit sparsity of the Hessian $H$ in solving the Newton system. This sparse structure occurs whenever each variable $x_{i}$ is nonlinearly coupled (in the objective) to only a few other variables, or equivalently, when the objective function can be expressed as a sum of functions, each depending on only a few variables, and each variable appearing in only a few of these functions. 

To solve $H\Delta x=-g$ when $H$ i sparse, a sparse Cholesky fact rization is used to compute a permutation matrix P and lower triangular matrix L for which 

$$
H=P L L^{T}P^{T}.
$$ 

The cost of this factorization depends on the particular sparsity pattern, but is often far smaller than $(1/3)n^{3}$ , and an empirical complexity of order $n$ (for large $n$ ) is not uncommon. The forward and back substitution are very similar to the basic method without the permutation. We solve ${\mathit{L w}}\ =\ -P^{T}g$ using forward substitution, and then solve $L^{T}v=w$ by back substitution to obtain 

$$
\begin{array}{r}{v=L^{-T}w=-L^{-T}L^{-1}P^{T}g.}\end{array}
$$ 

The Newton step is then $\Delta x=P v$ . 

Since the sparsity pattern of $H$ does not change as $x$ varies (or more precisely, since we only exploit sparsity that does not change with $x$ ) we can use the same permutation matrix $P$ for each of the Newton steps. The step of determining a good permutation matrix $P$ , which is called the symbolic factorization step, can be done once, for the whole Newton process. 

# Diagonal plus low rank 

There are many other types of structure that can be exploited in solving the New- ton system $H\Delta x_{\mathrm{{nt}}}\,=\,-g$ . Here we brieﬂy descri one, and refer the reader to appendix C for more details. Suppose the Hessian H can be expressed as a diago- nal matrix plus one of low rank, say, $p$ . This occurs when the objective function $f$ has the special form 

$$
f(x)=\sum_{i=1}^{n}\psi_{i}(x_{i})+\psi_{0}(A x+b)
$$ 

where $A\,\in\,\mathbf{R}^{p\times n}$ , $\psi_{1},.\,.\,.\,,\psi_{n}\,:\,\mathbf{R}\,\rightarrow\,\mathbf{R}$ , and $\psi_{0}:\mathbf{R}^{p}\,\rightarrow\,\mathbf{R}$ . In other words, $f$ is a separable function, plus a function that depends on a low dimensional affine function of $x$ . 

To find the Newton step $\Delta x_{\mathrm{{nt}}}$ for ( 9.60 ) we must solve the Newton system $H\Delta x_{\mathrm{nt}}=-g$ , with 

$$
H=D+A^{T}H_{0}A.
$$ 

Here $D\,=\,\mathbf{diag}(\psi_{1}^{\prime\prime}(x_{1}),.\,.\,.\,,\psi_{n}^{\prime\prime}(x_{n}))$ )) is diagonal, and $H_{0}\,=\,\nabla^{2}\psi_{0}(A x+b)$ is the Hessian of $\psi_{0}$ . If we compute the Newton step without exploiting the structure, the cost of solving the Newton system is $(1/3)n^{3}$ ﬂops. 

Let $H_{0}=L_{0}L_{0}^{T}$ be the Cholesky factorization of $H_{0}$ . We introduce the tempo- rary variable $w=L_{0}^{T}A\Delta x_{\mathrm{nt}}\in\mathbf{R}^{p}$ , and express the Newton system as 

$$
D\Delta x_{\mathrm{nt}}+A^{T}L_{0}w=-g,\qquad w=L_{0}^{T}A\Delta x_{\mathrm{nt}}.
$$ 

Substituting $\Delta x_{\mathrm{nt}}=-D^{-1}(A^{T}L_{0}w+g)$ (from the first equation) into the second equation, we obtain 

$$
\begin{array}{r}{(I+L_{0}^{T}A D^{-1}A^{T}L_{0})w=-L_{0}^{T}A D^{-1}g,}\end{array}
$$ 

which is a system of $p$ linear equations. 

Now we proceed as follows to compute the Newton step $\Delta x_{\mathrm{{nt}}}$ . First we compute the Cholesky factorization of $H_{0}$ , which costs $(1/3)p^{3}$ . We then form the dense, positive definite symmetric matrix appearing on the lefthand side of ( 9.61 ), which costs $2p^{2}n$ . We then solve ( 9.61 ) for $w$ using a Cholesky factorization and a back and forward substitution, which costs $(1/3)p^{3}$ ﬂops. Finally, we compute $\Delta x_{\mathrm{{nt}}}$ using

 $\Delta x_{\mathrm{nt}}\,=\,-D^{-1}(A^{T}L_{0}w\,+\,g)$ , which costs $2n p$ ﬂops. The total cost of computing

 $\Delta x_{\mathrm{{nt}}}$ is (keeping only the dominant term) $2p^{2}n$ ﬂops, which is far smaller than

 $(1/3)n^{3}$ for $p\ll n$ . 

# Bibliography 

Dennis and Schnabel [ DS96 ] and Ortega and Rheinboldt [ OR00 ] are two standard refer- ences on algorithms for unconstrained minimization and nonlinear equations. The result on quadratic convergence, assuming strong convexity and Lipschitz continuity of the Hes- sian, is attributed to Kantorovich [ Kan52 ]. Polyak [ Pol87 , § 1.6] gives some insightful comments on the role of convergence results that involve unknown constants, such as the results derived in $\S9.5.3$ . 

Self-concordant functions were introduced by Nesterov and Nemirovski [ NN94 ]. All our results in $\S9.6$ and exercises 9.14 – 9.20 can be found in their book, although often in a more general form or with diﬀerent notation. Renegar [ Ren01 ] gives a concise and elegant presentation of self-concordant functions and their role in the analysis of primal-dual interior-point algorithms. Peng, Roos, and Terlaky [ PRT02 ] study interior-point methods from the viewpoint of self-regular functions , a class of functions that is similar, but not identical, to self-concordant functions. 

References for the material in 9.7 are given at the end of appendix C 

# Exercises 

# Unconstrained minimization 

9.1 Minimizing a quadratic function. Consider the problem of minimizing a quadratic function: 

$$
{\mathrm{minimize}}\quad f(x)=(1/2)x^{T}P x+q^{T}x+r,
$$ 

where $P\in\mathbf{S}^{n}$ (but we do not assume $P\succeq0$ ). 

(a) Show that if $P\not\succeq0$ , i.e. , the objective function $f$ is not convex, then the problem is unbounded below. (b) Now supp $P\succeq0$ (so the objective function is convex), but the optimality condition $P x^{\star}=-q$ − does not have a solution. Show that the problem is unbounded below. 

9.2 Minimizing a quadratic-over-linear fractional function. Consider the problem of minimiz- ing the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , defined as 

$$
f(x)={\frac{||A x-b||_{2}^{2}}{c^{T}x+d}},\qquad\mathbf{dom}\ f=\{x\mid c^{T}x+d>0\}.
$$ 

We assume $\mathbf{rank}\,A=n$ and $b\notin\mathcal{R}(A)$ 

(a) Show that $f$ is closed. 

(b) Show that the minimizer $x^{\star}$ of $f$ is given by 

$$
x^{\star}=x_{1}+t x_{2}
$$ 

where $x_{1}=(A^{T}A)^{-1}A^{T}b$ , $x_{2}=(A^{T}A)^{-1}c$ , and $t\in\mathbf{R}$ can be calculated by solving a quadratic equation. 

9.3 Initial point and sublevel set condition. Consider the function $f(x)=x_{1}^{2}\!+x_{2}^{2}$ with domain dom $f=\{(x_{1},x_{2})\mid x_{1}>1\}$ . 

(a) What is $p^{\star}$ ? 

(b) raw the sublevel set $S=\{x\mid f(x)\leq f(x^{(0)})\}$ for ${\boldsymbol x}^{(0)}=(2,2)$ . Is the sublevel set S closed? Is $f$ strongly convex on S ? (c) What happens if we apply the gradient method with backtracking line search, start- ing at $x^{(0)}$ ? Does $f(x^{(k)})$ converge to $p^{\star}$ ? 

9.4 Do you agree with the following argument? The $\ell_{1}$ -norm of a vector $x\,\in\,\mathbf{R}^{m}$ can be expressed as 

$$
\|x\|_{1}=(1/2)\operatorname*{inf}_{y\succ0}\left(\sum_{i=1}^{m}x_{i}^{2}/y_{i}+\mathbf{1}^{T}y\right).
$$ 

Therefore the $\ell_{1}$ -norm approximation problem 

$$
\mathrm{minimize\quad\|\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b}\|_{1}}
$$ 

is equivalent to the minimization problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{{}f(x,y)=\sum_{i=1}^{m}(a_{i}^{T}x-b_{i})^{2}/y_{i}+\mathbf{1}^{T}y,}\end{array}
$$ 

with $\mathbf{dom}\ f=\{(x,y)\in\mathbf{R}^{n}\times\mathbf{R}^{m}\ |\ y\succ0\}$ here $a_{i}^{T}$ is the $i$ th row of $A$ . Since $f$ is twice diﬀerentiable and convex, we can solve the ℓ $\ell_{1}$ 1 -norm approximation problem by applying Newton’s method to $_{(9.62)}$ . 

9.5 cktracking line search. Su pose $f$ is strongly convex with $m I\,\preceq\,\nabla^{2}f(x)\,\preceq\,M I$ . Let $\Delta x$ x be a descent direction at x . Show that the backtracking stopping condition holds for 

$$
0<t\leq-\frac{\nabla f(x)^{T}\Delta x}{M\|\Delta x\|_{2}^{2}}.
$$ 

Use this to give an upper bound on the number of backtracking iterations. 

# Gradient and steepest descent methods 

9.6 Quadratic problem in $\mathbf{R}^{2}$ . Verify the expressions for the iterates $x^{(k)}$ in the first example of $\S9.3.2$ . 

9.7 Let $\Delta x_{\mathrm{nsd}}$ and $\Delta x_{\mathrm{sd}}$ be the normalized and unnormalized steepest descent directions at $x$ , for the norm $||\cdot||$ . Prove the following identities. 

$\nabla f(x)^{T}\Delta x_{\mathrm{nsd}}=-\|\nabla f(x)\|_{*}.$ (a) ∇ . (b) $\nabla f(x)^{T}\Delta x_{\mathrm{sd}}=-\|\nabla f(x)\|_{*}^{2}$ . (c) $\begin{array}{r}{\Delta x_{\mathrm{sd}}=\operatorname*{argmin}_{v}(\nabla f(x)^{T}v+(1/2)\|v\|^{2}).}\end{array}$ . 

9.8 Steepest descent method in $\ell_{\infty}$ -norm. Explain how to find a steepest descent direction in the $\ell_{\infty}$ -norm, and give a simple interpretation. 

# Newton’s method 

9.9 Newton decrement. Show that the Newton decrement $\lambda(x)$ satisfies 

$$
\lambda(x)=\operatorname*{sup}_{v^{T}\nabla^{2}f(x)v=1}(-v^{T}\nabla f(x))=\operatorname*{sup}_{v\neq0}\frac{-v^{T}\nabla f(x)}{(v^{T}\nabla^{2}f(x)v)^{1/2}}.
$$ 

9.10 The pure Newton method. Newton’s method with fixed step size $t=1$ can diverge if the initial point is not close to $x^{\star}$ . In this problem we consider two examples. 

(a) $f(x)=\log(e^{x}+e^{-x})$ has a unique minimizer $x^{\star}=0$ . Run Newton’s method with fixed step size $t=1$ , starting at $x^{(0)}=1$ and at $x^{(0)}=1.1$ . (b) $f(x)=-\log x+x$ has a un nimizer $x^{\star}=1$ . Run Newton’s method with fixed step size $t=1$ , starting at x $x^{(0)}=3$ = 3. Plot $f$ and $f^{\prime}$ , and show the first few iterates. 

9.11 Gradient and New for composition functions. Suppose $\phi:\mathbf{R}\rightarrow\mathbf{R}$ is increasing and convex, and f $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ n → is convex, so $g(x)=\phi{\bigl(}f(x){\bigr)}$ is c vex. (We assume that $f$ and $g$ are twice diﬀerentiable.) The problems of minimizing f and minimizing $g$ are clearly equivalent. 

Compare the gradient method and Newton’s method, applied to $f$ and $g$ . How are the search directions related? How are the methods related if an exact line search is used? Hint. Use the matrix inversion lemma (see § C.4.3 ). 

9.12 Trus If $\nabla^{2}f(x)$ is singular (or very ill-conditioned), the Newton $\Delta x_{\mathrm{nt}}=-\nabla^{2}f(x)^{-1}\nabla f(x)$ −∇ ∇ ) is not well defined. Instead we can define a search direction $\Delta x_{\mathrm{tr}}$ as the solution of 

$$
\begin{array}{l}{\mathrm{minimize}\quad\ (1/2)\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}+\boldsymbol{g}^{T}\boldsymbol{v}}\\ {\mathrm{subject~to}\quad\|\boldsymbol{v}\|_{2}\leq\gamma,}\end{array}
$$ 

where $H=\nabla^{2}f(x)$ , $g=\nabla f(x)$ , an $\gamma$ i a positive constant. The point $x\!+\!\Delta x_{\mathrm{tr}}$ he second- mation of f at x , subject to the constraint t at ∥ $\|(x{+}\Delta x_{\mathrm{tr}}){-}x\|_{2}\leq$ − ∥ ≤ γ . The set { $\{v\mid\|v\|_{2}\leq\gamma\}$ | ∥ ∥ ≤ } is called the trust region . The parameter γ , the size of the trust region, reﬂects our confidence in the second-order model. Show that $\Delta x_{\mathrm{tr}}$ minimizes 

$$
(1/2)\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}+\boldsymbol{g}^{T}\boldsymbol{v}+\hat{\beta}\|\boldsymbol{v}\|_{2}^{2},
$$ 

for some $\hat{\beta}$ . This quadratic function can be interpreted as a regularized quadratic model for $f$ around $x$ . 

# Self-concordance 

9.13 Self-concordance and the inverse barrier. 

(a) Show that $f(x)=1/x$ with domain $(0,8/9)$ is self-concordant. (b) Show that the function 

$$
f(x)=\alpha\sum_{i=1}^{m}{\frac{1}{b_{i}-a_{i}^{T}x}}
$$ 

with $\mathbf{dom}\,f\,=\,\{x\,\in\,\mathbf{R}^{n}\,\mid\,a_{i}^{T}x\,<\,b_{i},\,\,i\,=\,1,.\,.\,,m\}$ } , is self-concordant if $\mathbf{dom}\ f$ is bounded and 

$$
\alpha>(9/8)\operatorname*{max}_{i=1,\ldots,m}\operatorname*{sup}_{x\in\mathbf{dom}_{f}}(b_{i}-a_{i}^{T}x).
$$ 

9.14 Composition with logarithm. Let $g:\mathbf{R}\rightarrow\mathbf{R}$ be a convex function with $\mathbf{dom}\,g\,=\mathbf{R}_{++}$ , and 

$$
|g^{\prime\prime\prime}(x)|\leq3\frac{g^{\prime\prime}(x)}{x}
$$ 

r all $x$ . Prove that $f(x)=-\log(-g(x))-\log x$ is self-concordant on $\{x\mid x>0,\ g(x)<$ $0\}$ } . Hint. Use the inequality 

$$
\frac{3}{2}r p^{2}+q^{3}+\frac{3}{2}p^{2}q+r^{3}\leq1
$$ 

which holds for $p,q,r\in\mathbf{R}_{+}$ with $p^{2}+q^{2}+r^{2}=1$ . 

9.15 Prove that the following functions are self-concordant. In your proof, restrict the function to a line, and apply the composition with logarithm rule. 

(a) $f(x,y)=-\log(y^{2}-x^{T}x)$ on $\{(x,y)\mid\|x\|_{2}<y\}$ . (b) $f(x,y)=-2\log y-\log(y^{2/p}-x^{2})$ , with $p\geq1$ , on $\{(x,y)\in\mathbf{R}^{2}\mid|x|^{p}<y\}$ . (c) $f(x,y)=-\log y-\log(\log y-x)$ on $\{(x,y)\mid e^{x}<y\}$ . 

9.16 Let $f:\mathbf{R}\rightarrow\mathbf{R}$ be a self-concordant function. 

(a) Suppose $f^{\prime\prime}(x)\,\neq\,0$ . Show that the self-concordance condition ( 9.41 ) can be ex- pressed as 

$$
\left\vert{\frac{d}{d x}}\left(f^{\prime\prime}(x)^{-1/2}\right)\right\vert\leq1.
$$ 

Find the ‘extreme’ self-concordant functions of one variable, i.e. , the functions $f$ and $\tilde{f}$ that satisfy 

$$
\frac{d}{d x}\left(f^{\prime\prime}(x)^{-1/2}\right)=1,\qquad\frac{d}{d x}\left(\tilde{f}^{\prime\prime}(x)^{-1/2}\right)=-1,
$$ 

respectively. 

(b) Show that either $f^{\prime\prime}(x)=0$ for all $x\in\mathbf{dom}\,f$ , or $f^{\prime\prime}(x)>0$ for all $x\in\mathbf{dom}\ f$ . 

9.17 Upper and lower bounds on the Hessian of a self-concordant function. (a) Let $f:\mathbf{R}^{2}\rightarrow\mathbf{R}$ be a self-concordant function. Show that 

$$
\begin{array}{r l r}{\left|\displaystyle\frac{\partial^{3}f(x)}{\partial^{3}x_{i}}\right|}&{\leq}&{2\left(\displaystyle\frac{\partial^{2}f(x)}{\partial x_{i}^{2}}\right)^{3/2},\quad i=1,2,}\\ {\left|\displaystyle\frac{\partial^{3}f(x)}{\partial x_{i}^{2}\partial x_{j}}\right|}&{\leq}&{2\displaystyle\frac{\partial^{2}f(x)}{\partial x_{i}^{2}}\left(\displaystyle\frac{\partial^{2}f(x)}{\partial x_{j}^{2}}\right)^{1/2},\quad i\neq j}\end{array}
$$ 

for all $x\in\mathbf{dom}\ f$ 

Hint. If $h:\mathbf{R}^{2}\times\mathbf{R}^{2}\times\mathbf{R}^{2}\rightarrow\mathbf{R}$ is a symmetric trilinear form, i.e. , 

$$
\begin{array}{l c l}{{h(u,v,w)}}&{{=}}&{{a_{1}u_{1}v_{1}w_{1}+a_{2}\bigl(u_{1}v_{1}w_{2}+u_{1}v_{2}w_{1}+u_{2}v_{1}w_{1}\bigr)}}\\ {{}}&{{}}&{{+\;a_{3}\bigl(u_{1}v_{2}w_{2}+u_{2}v_{1}w_{1}+u_{2}v_{2}w_{1}\bigr)+a_{4}u_{2}v_{2}w_{2},}}\end{array}
$$ 

then 

$$
\operatorname*{sup}_{u,v,w\neq0}{\frac{h(u,v,w)}{||u||_{2}||v||_{2}||w||_{2}}}=\operatorname*{sup}_{u\neq0}{\frac{h(u,u,u)}{||u||_{2}^{3}}}.
$$ 

(b) Let $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ b a self-concord t function. Show that th pace of $\nabla^{2}f(x)$ is independent of x . Show that if f is strictly convex, then ∇ $\nabla^{2}f(x)$ ) is nonsingular for all $x\in\mathbf{dom}\ f$ . Hi hat if $\boldsymbol{w}^{T}\nabla^{2}f(\boldsymbol{x})\boldsymbol{w}=0$ for some $x\in\mathbf{dom}\ f$ , then $\boldsymbol{w}^{T}\nabla^{2}f(y)\boldsymbol{w}=0$ for all y $y\in\mathbf{dom}\ f$ ∈ . To show this, apply the result in (a) to the self-concordant function $\dot{f}(t,s)=f(x+t(y-x)+s w)$ − ). 

(c) Let $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ be a self-concordant function. Suppose $x\in\mathbf{dom}\ f$ , $v\in\mathbf{R}^{n}$ . Show that 

$$
(1-t\alpha)^{2}\boldsymbol{\nabla}^{2}f(x)\preceq\boldsymbol{\nabla}^{2}f(x+t\boldsymbol{v})\preceq\frac{1}{(1-t\alpha)^{2}}\boldsymbol{\nabla}^{2}f(x)
$$ 

for $x+t v\in\mathbf{dom}\,f$ , $0\leq t<\alpha$ , where $\alpha=(\boldsymbol{v}^{T}\nabla^{2}f(\boldsymbol{x})\boldsymbol{v})^{1/2}

$ 

9.18 Quadratic convergence . Let $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ convex self-c se $\lambda(x)<1$ , and define $x^{+}=x-\nabla^{2}f(x)^{-1}\nabla f(x)$ −∇ ∇ ). Prove that $\lambda(x^{+})\leq\lambda(x)^{2}/(1-$ ≤ − $\lambda(x))^{2}$ . Hint. Use the inequalities in exercise 9.17 , part (c). 

9.19 Bound on the distance from the optimum. Let $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}$ be a strictly convex self- concordant function. 

(a) Suppose $\lambda(\bar{x})<1$ 1 and the sublevel set $\{x\mid f(x)\leq f({\bar{x}})\}$ } is closed. Show that the minimum of f is attained and 

$$
\bigl((\bar{x}-x^{\star})^{T}\nabla^{2}f(\bar{x})(\bar{x}-x^{\star})\bigr)^{1/2}\leq\frac{\lambda(\bar{x})}{1-\lambda(\bar{x})}.
$$ 

(b) Show that if $f$ has a closed sublevel set, and is bounded below, then its minimum is attained. 

9.20 Conjugate of a self-concordant function. Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is closed, strict convex, and self-concordant. We show that its conjugate (or Legendre transform) f $f^{*}$ is self- concordant. 

(a) Show that for each $y\,\in\,\mathbf{dom}\,f^{*}$ , there is a unique $x\,\in\,\mathbf{dom}\,f$ that satisfies $y=$ $\nabla f(x)$ . Hint. Refer to the result of exercise 9.19 . 

(b) Suppose ${\bar{y}}=\nabla f({\bar{x}})$ ∇ ). Define 

$$
g(t)=f\big(\bar{x}+t v\big),\qquad h(t)=f^{*}\big(\bar{y}+t w\big)
$$ 

where $v\in\mathbf{R}^{n}$ and $w=\nabla^{2}f(\bar{x})v$ . Show that 

$$
g^{\prime\prime}(0)=h^{\prime\prime}(0),\qquad g^{\prime\prime\prime}(0)=-h^{\prime\prime\prime}(0).
$$ 

Use these identities to show that $f^{*}$ is self-concordant. 

9.21 Optimal line search parameters. Consider the upper bound ( 9.56 ) on the number of Newton iterations required to minimize a strictly convex self-concordant functions. What is the minimum value of the upper bound, if we minimize over $\alpha$ and $\beta$ ? 

9.22 Suppose that $f$ is strictly convex and satisfies ( 9.42 ). Give a bound on the number of $x^{(0)}$ Newton steps required to compute $p^{\star}$ within $\epsilon$ , starting at . 

# Implementation 

9.23 Pre-computation for line searches. For each of the following functions, explain how the computational cost of a line search can be reduced by a pre-computation. Give the cost of the pre-computation, and the cost of evaluating $g(t)=f(x+t\Delta x)$ and $g^{\prime}(t)$ with and without the pre-computation. 

$$
\begin{array}{r}{f(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x).}\end{array}
$$ 

9.24 Exploiting block diagonal structure in the Newton system. Suppose the Hessian $\nabla^{2}f(x)$ of a convex function f is block diagonal. How do we exploit this structure when computing the Newton step? What does it mean about $f$ ? 

9.25 Smoothed fit to given data. Consider the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{{}f(x)=\sum_{i=1}^{n}\psi(x_{i}-y_{i})+\lambda\sum_{i=1}^{n-1}(x_{i+1}-x_{i})^{2}}\end{array}}
$$ 

where $\lambda>0$ is smoothing parameter, $\psi$ is a convex penalty function, and $x\in\mathbf{R}^{n}$ is the variable. We can interpret $x$ as a smoothed fit to the vector $y$ . 

(a) What is the structure in the Hessian of $f$ ? 

(b) Extend to the problem of making a smooth fit to two-dimensional data, i.e. , mini- mizing the function 

$$
\sum_{i,j=1}^{n}\psi(x_{i j}-y_{i j})+\lambda\left(\sum_{i=1}^{n-1}\sum_{j=1}^{n}(x_{i+1,j}-x_{i j})^{2}+\sum_{i=1}^{n}\sum_{j=1}^{n-1}(x_{i,j+1}-x_{i j})^{2}\right),
$$ 

9.26 Newton equations with linear structure. Consider the problem of minimizing a function of the form 

$$
f(x)=\sum_{i=1}^{N}\psi_{i}(A_{i}x+b_{i})
$$ 

where $A_{i}\in\mathbf{R}^{m_{i}\times n},\;b_{i}\in\mathbf{R}^{m_{i}}$ , and the fu cti ns $\psi_{i}:\mathbf{R}^{m_{i}}\to\mathbf{R}$ are twice diﬀerentiable and convex. The Hessian H and gradient g of f at x are given by 

$$
H=\sum_{i=1}^{N}{\cal A}_{i}^{T}H_{i}{\cal A}_{i},\qquad g=\sum_{i=1}^{N}{\cal A}_{i}^{T}g_{i}.
$$ 

where $H_{i}=\nabla^{2}\psi_{i}(A_{i}x+b_{i})$ and $g_{i}=\nabla\psi_{i}\big(A_{i}x+b_{i}\big)$ 

Describe how you would implement Newton’s method for minimizing $f$ . Assume that $n\gg m_{i}$ , the matrices $A_{i}$ are very sparse, but the Hessian $H$ is dense. 

9.27 Analytic center of linear inequalities with variable bounds. Give the most efficient method for computing the Newton step of the function 

$$
f(x)=-\sum_{i=1}^{n}\log(x_{i}+1)-\sum_{i=1}^{n}\log(1-x_{i})-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),
$$ 

with dom $f=\{x\in\mathbf{R}^{n}\mid-\mathbf{1}\prec x\prec\mathbf{1},A x\prec b\}$ , where $a_{i}^{T}$ is the $i$ th row of $A$ . Assume $A$ is dense, and distinguish two cases: $m\geq n$ and $m\leq n$ . (See also exercise 9.30 .) 

9.28 Analytic center of quadratic inequalities. Describe an efficient method for computing the Newton step of the function 

$$
f(\boldsymbol{x})=-\sum_{i=1}^{m}\log(-\boldsymbol{x}^{T}A_{i}\boldsymbol{x}-\boldsymbol{b}_{i}^{T}\boldsymbol{x}-\boldsymbol{c}_{i}),
$$ 

with $\mathbf{dom}\,f\,=\,\{x\,\mid\,x^{T}A_{i}x+b_{i}^{T}x+c_{i}\,<\,0,\ i=1,\ldots,m\}$ } . Assume that the matrices $A_{i}\in\mathbf{S}_{++}^{n}$ are large and sparse, and $m\ll n$ . Hint. The Hessian and gradient of $f$ at $x$ are given by 

$$
H=\sum_{i=1}^{m}(2\alpha_{i}A_{i}+\alpha_{i}^{2}(2A_{i}x+b_{i})(2A_{i}x+b_{i})^{T}),\qquad g=\sum_{i=1}^{m}\alpha_{i}(2A_{i}x+b_{i}),
$$ 

where $\alpha_{i}=1/(-x^{T}A_{i}x-b_{i}^{T}x-c_{i})$ − ). 

9.29 Exploiting structure in two-stage optimization. This exercise continues exercise 4.64 , which describes optimization with recourse, or two-stage optimization. Using the notation and assumptions in exercise 4.64 , we assume in addition that the cost function $f$ is a twice diﬀerentiable function of $(x,z)$ , for each scenario $i=1,.\,.\,.\,,S$ . 

Explain how to efficiently compute the Newton step for the problem of finding the optimal policy. How does the approximate ﬂop count for your method compare to that of a generic method (which exploits no structure), as a function of $S$ , the number of scenarios? 

# Numerical experiments 

9.30 Gradient and Newton methods. Consider the unconstrained problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{{}f(x)=-\sum_{i=1}^{m}\log(1-a_{i}^{T}x)-\sum_{i=1}^{n}\log(1-x_{i}^{2}),}\end{array}
$$ 

with variable $x\in\mathbf{R}^{n}$ , and $\mathbf{dom}\,f=\{x\ |\ a_{i}^{T}x<1,\ i=1,.\,.\,.\,,m,\ |x_{i}|<1,\ i=1\}$ | | = 1 , . . . , n } . This is the problem of computing the analytic center of the set of linear inequalities 

$$
a_{i}^{T}x\leq1,\quad i=1,\ldots,m,\qquad|x_{i}|\leq1,\quad i=1,\ldots,n.
$$ 

Note that we can choose $x^{(0)}=0$ as our initial point. You can generate instances of this problem by choosing $a_{i}$ from some distribution on $\mathbf{R}^{n}$ . 

(a) Use the gradient method to solve the problem, using reasonable choices for the back- tracking parameters, and a stopping criterion of the form $||\nabla f(x)||_{2}\leq\eta$ . Plot the objective function and step length versus iteration number. (Once you have deter- mined $p^{\star}$ to high accuracy, you ca also lot $f-p^{\star}$ versus iteration.) Experiment with the backtracking parameters α and β to see their eﬀect on the total number of iterations required. Carry these experiments out for several instances of the problem, of diﬀerent sizes. (b) Repeat using Newton’s method, with stopping criterion based on the Newton decre- ment $\lambda^{2}$ . Look for quadratic convergence. You do not have to use an efficient method to compute the Newton step, as in exercise 9.27 ; you can use a general purpose dense solver, although it is better to use one that is based on a Cholesky factorization. 

Hint. Use the chain rule to find expressions for $\nabla f(x)$ and $\nabla^{2}f(x)$ . 

9.31 Some approximate Newton methods. The cost of Newton’s method is dominated by the cost of evaluating the Hessian $\nabla^{2}f(x)$ and the cost of solving the Newton system. For large problems, it is sometimes useful to replace the Hessian by a positive definite approximation that makes it easier to form and solve for the search step. In this problem we explore some common examples of this idea. 

For each of the approximate Newton methods described below, test the method on some instances of the analytic centering problem described in exercise 9.30 , and compare the results to those obtained using the Newton method and gradient method. 

(a) Re-using the Hessian. We evaluate and factor the Hessian only every $N$ iterations, where $N>1$ , and use the search step $\Delta x=-H^{-1}\nabla f(x)$ , where $H$ is he last Hessian evaluated. (We need to evaluate and factor the Hessian once every N steps; for the other steps, we compute the search direction using back and forward substitution.) (b) Diagonal approximation. We replace the Hessian by its diagonal, so we only have to evaluate the $n$ second derivatives $\partial^{2}f(x)/\partial x_{i}^{2}$ , and computing the search step is very easy. 

9.32 Gauss-Newton method for convex nonlinear least-squares problems. We consider a (non- linear) least-squares problem, in which we minimize a function of the form 

$$
f(x)={\frac{1}{2}}\sum_{i=1}^{m}{f_{i}(x)^{2}},
$$ 

where $f_{i}$ are twice diﬀerentiable functions. The gradient and Hessian of $f$ at $x$ are given by 

$$
\nabla f(x)=\sum_{i=1}^{m}f_{i}(x)\nabla f_{i}(x),\qquad\nabla^{2}f(x)=\sum_{i=1}^{m}\left(\nabla f_{i}(x)\nabla f_{i}(x)^{T}+f_{i}(x)\nabla^{2}f_{i}(x)\right).
$$ 

We consider the case when $f$ is convex. This occurs, for example, if each $f_{i}$ is either nonnegative and convex, or nonpositive and concave, or affine. 

The Gauss-Newton method uses the search direction 

$$
\Delta x_{\mathrm{gen}}=-\left(\sum_{i=1}^{m}\nabla f_{i}(x)\nabla f_{i}(x)^{T}\right)^{-1}\left(\sum_{i=1}^{m}f_{i}(x)\nabla f_{i}(x)\right).
$$ 

(We assume here that the inverse exists, i.e. , the vectors $\nabla f_{1}(x),\ldots,\nabla f_{m}(x)$ span $\mathbf{R}^{n}$ .) This search direction can be considered an approximate Newton direction (see exer- cise 9.31 ), obtained by dropping the second derivative terms from the Hessian of $f$ . 

We can give another simple interpretation of the Gauss-Newton search direction $\Delta x_{\mathrm{gen}}$ . Using the first-order approximation $f_{i}(x+v)\approx f_{i}(x)+\nabla f_{i}(x)^{T}v$ we obtain the approxi- mation 

$$
f(\boldsymbol{x}+\boldsymbol{v})\approx\frac{1}{2}\sum_{i=1}^{m}(f_{i}(\boldsymbol{x})+\nabla f_{i}(\boldsymbol{x})^{T}\boldsymbol{v})^{2}.
$$ 

The Gauss-Newton search step $\Delta x_{\mathrm{gen}}$ is precisely the value of $v$ that minimizes this ap- proximation of $f$ . (Moreover, we conclude that $\Delta x_{\mathrm{gen}}$ can be computed by solving a linear least-squares problem.) 

Test the Gauss-Newton method on some problem instances of the form 

$$
f_{i}(x)=(1/2)x^{T}A_{i}x+b_{i}^{T}x+1,
$$ 

with $A_{i}\in\mathbf{S}_{++}^{n}$ and $b_{i}^{T}A_{i}^{-1}b_{i}\le2$ ≤ 2 (which ensures that $f$ is convex). 

# Chapter 10 

# Equality constrained minimization 

# 10.1 Equality constrained minimization problems 

In this chapter we describe methods for solving a convex optimization problem with equality constraints, 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad f(x)}\\ &{{\mathrm{subject~to}}\quad A x=b,}\end{array}}
$$ 

where $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex and twice conti uously diﬀerentiable, and $A\in\mathbf{R}^{p\times n}$ with rank $A=p<n$ . The assumptions on A mean that there are fewer equality constraints than variables, and that the equality constraints are independent. We will assume that an optimal solution $x^{\star}$ exists, and use $p^{\star}$ to denote the optimal value, $p^{\star}=\operatorname*{inf}\{f(x)\mid A x=b\}=f(x^{\star})$ . 

Recall (from § 4.2. .3 ) that a point $x^{\star}\in\mathbf{dom}\,f$ is optimal for ( 10.1 ) if p and only if there is a ν $\nu^{\star}\in\mathbf{R}^{p}$ such that 

$$
A x^{\star}=b,\qquad\nabla f(x^{\star})+A^{T}\nu^{\star}=0.
$$ 

Solving the equality constrained optimization problem ( 10.1 ) is therefore equivalent to finding a solution of the KKT equations ( 10.2 ), which is a set of $n+p$ equations in the $n+p$ variables $x^{\star},\ \nu^{\star}$ . The first set of equations, $A x^{\star}\ =\ b$ , are called the primal feasibility equations , which are linear. The second set of equations, $\nabla f(x^{\star})+A^{T}\nu^{\star}\,=\,0$ , are called the dual feasibility equations , and are in general nonlinear. As with unconstrained optimization, there are a few problems for which we can solve these optimality conditions analytically. The most important special case is when $f$ is quadratic, which we examine in § 10.1.1 . 

Any equality constrained minimization problem can be reduced to an equiv- alent unconstrained problem by eliminating the equality constraints, after which the methods of chapter 9 can be used to solve the problem. Another approach is to solve the dual problem (assuming the dual function is twice diﬀerentiable) using an unconstrained minimization method, and then recover the solution of the equality constrained problem ( 10.1 ) from the dual solution. The elimination and dual methods are brieﬂy discussed in $\S10.1.2$ and $\S10.1.3$ , respectively. 

The bulk of this chapter is devoted to extensions of Newton’s method that di- rectly handle equality constraints. In many cases these methods are preferable to methods that reduce an equality constrained problem to an unconstrained one. One reason is that problem structure, such as sparsity, is often destroyed by elimination (or forming the dual); in contrast, a method that directly handles equality con- straints can exploit the problem structure. Another reason is conceptual: methods that directly handle equality constraints can be thought of as methods for directly solving the optimality conditions ( 10.2 ). 

# 10.1.1 Equality constrained convex quadratic minimization 

Consider the equality constrained convex quadratic minimization problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad f({\boldsymbol{x}})=(1/2)x^{T}P x+q^{T}{\boldsymbol{x}}+r}\\ &{{\mathrm{subject~to}}\quad A{\boldsymbol{x}}={\boldsymbol{b}},}\end{array}}
$$ 

where $P\,\in\,{\bf S}_{+}^{n}$ and $A\,\in\,\mathbf{R}^{p\times n}$ . This problem is important on its own, and also because it forms the basis for an extension of Newton’s method to equality con- strained problems. 

Here the optimality conditions ( 10.2 ) are 

$$
A x^{\star}=b,\qquad P x^{\star}+q+A^{T}\nu^{\star}=0,
$$ 

which we can write as 

$$
\left[\begin{array}{c c}{\boldsymbol{P}}&{\boldsymbol{A}^{T}}\\ {\boldsymbol{A}}&{\boldsymbol{0}}\end{array}\right]\left[\begin{array}{c}{\boldsymbol{x}^{\star}}\\ {\boldsymbol{\nu}^{\star}}\end{array}\right]=\left[\begin{array}{c}{-\boldsymbol{q}}\\ {\boldsymbol{b}}\end{array}\right].
$$ 

This set of $n+p$ linear equations in the $n+p$ variables $x^{\star}$ , $\nu^{\star}$ is called the $K K T$ system for the equality constrained quadratic optimization problem ( 10.3 ). The coefficient matrix is called the KKT matrix . 

When the KKT matrix is nonsingular, there is a unique optimal primal-dual pair $(x^{\star},\nu^{\star})$ . If the KKT matrix is singular, but the KKT system is solvable, any solution yields an optimal pair $(x^{\star},\nu^{\star})$ . If the KKT system is not solvable, the quadratic optimization problem is unbounded below or infeasible. Indeed, in this case there exist $v\in\mathbf{R}^{n}$ and $w\in\mathbf{R}^{p}$ such that 

$$
\begin{array}{r}{P v+A^{T}w=0,\qquad A v=0,\qquad-q^{T}v+b^{T}w>0.}\end{array}
$$ 

Let x be any feasible point. The point $x={\hat{x}}+t v$ is feasible for all $t$ and 

$$
\begin{array}{r c l}{f(\hat{x}+t v)}&{=}&{f(\hat{x})+t(v^{T}P\hat{x}+q^{T}v)+(1/2)t^{2}v^{T}P v}\\ &{=}&{f(\hat{x})+t(-\hat{x}^{T}A^{T}w+q^{T}v)-(1/2)t^{2}w^{T}A v}\\ &{=}&{f(\hat{x})+t(-b^{T}w+q^{T}v),}\end{array}
$$ 

which decreases without bound as $t\to\infty$ . 

# Nonsingularity of the KKT matrix 

Recall our assumption that ${\cal P}\,\in\,{\bf S}_{+}^{n}$ and $\mathbf{rank}\,A\;=\;p\;<\;n$ . There are several conditions equivalent to nonsingularity of the KKT matrix: 

• $\mathcal{N}(P)\cap\mathcal{N}(A)=\{0\}$ , i.e. , $P$ and $A$ have no nontrivial common nullspace. • $A x=0$ , $x\neq0\implies x^{T}P x>0$ ⇒ i.e. , $P$ is positive definite on the nullspace of A . $F^{T}P F\succ0$ , where $F\in\mathbf{R}^{n\times(n-p)}$ is a matrix for which $\mathcal{R}(F)=\mathcal{N}(A)$ . 

(See exercise 10.1 .) As an important special case, we note that if $P\succ0$ , the KKT matrix must be nonsingular. 

# 10.1.2 Eliminating equality constraints 

One general approach to solving the equality constrained problem ( 10.1 ) is to elim- inate the equality constraints, as described in § 4.2.4 , and then solve the resulting unconstrained problem using methods for unconstrained minimization. We first find a matrix $F\;\in\;\mathbf{R}^{n\times(n-p)}$ and vector $\hat{x}\;\in\;\mathbf{R}^{\,n}$ ∈ n that parametrize the (affine) feasible set: 

$$
\{x\mid A x=b\}=\{F z+{\hat{x}}\mid z\in\mathbf{R}^{n-p}\}.
$$ 

Here x can be chosen as any particular solutio of $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ , and $F\,\in\,\mathbf{R}^{n\times(n-p)}$ is any matrix whose range is the nullspace of A . We then form the reduced or eliminated optimization problem 

$$
\mathrm{minimize}\quad\tilde{f}(z)=f(F z+\hat{x}),
$$ 

which is an unconstrained problem with variable $z\in\mathbf{R}^{n-p}$ . From its solution $z^{\star}$ , we can find the solution of the equality constrained problem as $x^{\star}=F z^{\star}+\hat{x}$ . 

We can also construct an optimal dual variable $\nu^{\star}$ for the equality constrained problem, as 

$$
\begin{array}{r}{\nu^{\star}=-(A A^{T})^{-1}A\nabla f(x^{\star}).}\end{array}
$$ 

To show that this expression is correct, we must verify that the dual feasibility condition 

$$
\nabla f(x^{\star})+A^{T}\bigl(-(A A^{T})^{-1}A\nabla f(x^{\star})\bigr)=0
$$ 

holds. To show this, we note that 

$$
\left[\begin{array}{c}{F^{T}}\\ {A}\end{array}\right]\left(\nabla f(x^{\star})-A^{T}(A A^{T})^{-1}A\nabla f(x^{\star})\right)=0,
$$ 

where in the top block we use $F^{T}\nabla f(x^{\star})=\nabla\tilde{f}(z^{\star})=0$ ) = 0 and $A F=0$ . Since the matrix on the left is nonsingular, this implies ( 10.6 ). 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\sum_{i=1}^{n}f_{i}(x_{i})}\\ {{\mathrm{subject~to}}}&{\sum_{i=1}^{n}x_{i}=b,}\end{array}}
$$ 

where the functions $f_{i}:\mathbf{R}\rightarrow\mathbf{R}$ are convex and twice diﬀerentiable, and $b\in\mathbf{R}$ is a problem parameter. We interpret this as the problem of optimally allocating a single resource, with a fixed total amount $b$ (the budget ) to $n$ otherwise independent activities. 

We can eliminate $x_{n}$ (for example) using the parametrization 

$$
x_{n}=b-x_{1}-\cdot\cdot\cdot-x_{n-1},
$$ 

which corresponds to the choices 

$$
\hat{x}=b e_{n},\qquad F=\left[\begin{array}{c}{{I}}\\ {{-{\bf1}^{T}}}\end{array}\right]\in{\bf R}^{n\times(n-1)}.
$$ 

The reduced problem is then 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\ }&{f_{n}(b-x_{1}-\cdots-x_{n-1})+\sum_{i=1}^{n-1}f_{i}(x_{i}),}\end{array}}
$$ 

with variables $x_{1},\allowbreak\cdot\cdot\cdot,x_{n-1}$ . 

# Choice of elimination matrix 

There are, of course, many possible choices for the elimination matrix $F$ , which can $\mathbf{R}^{n\times(n-p)}$ be chosen as any matrix in with $\mathcal{R}(F)=\mathcal{N}(A)$ . If $F$ is one such matrix, and $T\in\mathbf{R}^{(n-p)\times(n-p)}$ is nonsingular, then ${\tilde{F}}=F T$ is also a suitable elimination matrix, since 

$$
\mathcal{R}(\tilde{F})=\mathcal{R}(F)=\mathcal{N}(A).
$$ 

and $\ddot{F}$ Conversely, if $F$ are any two suitable elimination matrices, then there is some nonsingular $T$ such that ${\tilde{F}}=F T$ . 

If we eliminate the equality constraints using $F$ , we solve the unconstrained problem 

$$
\begin{array}{r}{\mathrm{minimize}\quad f(F z+\hat{x}),}\end{array}
$$ 

while if $\tilde{F}$ is used, we solve the unconstrained problem 

$$
\mathrm{minimize}\quad f(\tilde{F}\tilde{z}+\hat{x})=f(F(T\tilde{z})+\hat{x}).
$$ 

This problem is equivalent to the one above, and is simply obtained by the change of coordinates $z\,=\,T\tilde{z}$ z . In other words, changing the elimination matrix can be thought of as changing variables in the reduced problem. 

# 10.1.3 Solving equality constrained problems via the dual 

Another approach to solving ( 10.1 ) is to solve the dual, and then recover the optimal primal variable $x^{\star}$ , as described in § 5.5.5 . The dual function of ( 10.1 ) is 

$$
\begin{array}{r c l}{g(\nu)}&{=}&{-b^{T}\nu+\underset{x}{\operatorname*{inf}}(f(x)+\nu^{T}A x)}\\ &{=}&{-b^{T}\nu-\underset{x}{\operatorname*{sup}}\left((-A^{T}\nu)^{T}x-f(x)\right)}\\ &{=}&{-b^{T}\nu-f^{*}(-A^{T}\nu),}\end{array}
$$ 

where $f^{*}$ is the conjugate of $f$ , so the dual problem is 

$$
{\mathrm{maximize}}\quad-b^{T}\nu-f^{*}(-A^{T}\nu).
$$ 

Since by assumption there is an optimal point, the problem is strictly feasible, so Slater’s condition holds. Therefore strong duality holds, and the dual optimum is attained, i.e. , there exists a $\nu^{\star}$ with $g(\nu^{\star})=p^{\star}$ . 

If the dual function $g$ is twice diﬀerentiable, then the methods for unconstrained minimization described in chapter 9 can be used to maximize $g$ . (In general, the dual function $g$ need not be twice diﬀerentiable, even if $f$ is.) Once we find an optimal dual variable $\nu^{\star}$ , we reconstruct an optimal primal solution $x^{\star}$ from it. (This is not always straightforward; see 5.5.5 .) 

Example 10.2 Equality constrained analytic center. We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ f({\boldsymbol{x}})=-\sum_{i=1}^{n}\log x_{i}}\\ &{{\mathrm{subject~to}}\quad A{\boldsymbol{x}}={\boldsymbol{b}},}\end{array}}
$$ 

where $A\in\mathbf{R}^{p\times n}$ , with implicit constraint $x\succ0$ . Using 

$$
f^{*}(y)=\sum_{i=1}^{n}(-1-\log(-y_{i}))=-n-\sum_{i=1}^{n}\log(-y_{i})
$$ 

(with $\mathbf{dom}\,f^{*}=-\mathbf{R}_{++}^{n}$ ), the dual problem is 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}g(\nu)=-b^{T}\nu+n+\sum_{i=1}^{n}\log(A^{T}\nu)_{i},}\end{array}
$$ 

with implicit constraint $A^{T}\nu~\succ~0$ . Here we can easily solve the dual feasibility equation, i.e. , find the $x$ that minimizes $L(x,\nu)$ : 

$$
\nabla f(x)+A^{T}\nu=-(1/x_{1},\dots,1/x_{n})+A^{T}\nu=0,
$$ 

and so 

$$
\boldsymbol{x}_{i}(\nu)=1/(\boldsymbol{A}^{T}\nu)_{i}.
$$ 

To solve the equality constrained analytic centering problem ( 10.7 ), we solve the (unconstrained) dual problem ( 10.8 ), and then recover the optimal solution of ( 10.7 ) via ( 10.9 ). 

# 10.2 Newton’s method with equality constraints 

In this section we describe an extension of Newton’s method to include equality constraints. The method is almost the same as Newton’s method without con- straints, except for two diﬀerences: The initial point must be feasible ( i.e. , satisfy $x\,\in\,\mathbf{dom}\,f$ and $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ ), and the definition of Newton step is modified to take the equality constraints into account. In particular, we make sure that the Newton step $\Delta x_{\mathrm{{nt}}}$ is a feasible direction, i.e. , $A\Delta x_{\mathrm{nt}}=0$ . 

# 10.2.1 The Newton step 

# Definition via second-order approximation 

To derive the Newton step $\Delta x_{\mathrm{{nt}}}$ for the equality constrained problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad f(x)}\\ &{{\mathrm{subject~to}}\quad A x=b,}\end{array}}
$$ 

at the feasible point $x$ , we replace the objective with its second-order Taylor ap- proximation near $x$ , to form the problem 

$$
\begin{array}{r l}{\mathrm{minimize}~}&{{}\widehat{f}(x+v)=f(x)+\nabla f(x)^{T}v+(1/2)v^{T}\nabla^{2}f(x)v}\\ {\mathrm{subject~to}~}&{{}A(x+v)=b,}\end{array}
$$ 

with variable $v$ . This is a (convex) quadratic minimization problem with equality constraints, and can be solved analytically. We define $\Delta x_{\mathrm{{nt}}}$ , the Newton step at $x$ , as the solution of the convex quadratic problem ( 10.10 ), assuming the associated KKT matrix is nonsingular. In other words, the Newton step $\Delta x_{\mathrm{{nt}}}$ is what must be added to $x$ to solve the problem when the quadratic approximation is used in place of $f$ . 

From our analysis in § 10.1.1 of the equality constrained quadratic problem, the Newton step $\Delta x_{\mathrm{{nt}}}$ is characterized by 

$$
\left[\begin{array}{c c}{\nabla^{2}f(x)}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {w}\end{array}\right]=\left[\begin{array}{c}{-\nabla f(x)}\\ {0}\end{array}\right],
$$ 

where $w$ is the associated optimal dual variable for the quadratic problem. The Newton step is defined only at points for which the KKT matrix is nonsingular. 

As in Newton’s method for unconstrained problems, we observe that when the objective $f$ is exactly quadratic, the Newton update $x+\Delta{}x_{\mathrm{nt}}$ exactly solves the equality constrained minimization problem, and in this case the vector $w$ is the op- timal dual variable for the original problem. This suggests, as in the unconstrained case, that when $f$ is nearly quadratic, $x+\Delta{}x_{\mathrm{nt}}$ should be a very good estimate of the solution $x^{\star}$ , and $w$ should be a good estimate of the optimal dual variable $\nu^{\star}$ . 

# Solution of linearized optimality conditions 

We can interpret the Newton step $\Delta x_{\mathrm{{nt}}}$ , and the associated vector $w$ , as the solu- tions of a linearized approximation of the optimality conditions 

$$
A x^{\star}=b,\qquad\nabla f(x^{\star})+A^{T}\nu^{\star}=0.
$$ 

We substitute $x+\Delta{}x_{\mathrm{nt}}$ for $x^{\star}$ and $w$ for $\nu^{\star}$ , and replace the gradient term in the second equation by its linearized approximation near $x$ , to obtain the equations 

$$
A(x+\Delta x_{\mathrm{int}})=b,\qquad\nabla f(x+\Delta x_{\mathrm{int}})+A^{T}w\approx\nabla f(x)+\nabla^{2}f(x)\Delta x_{\mathrm{int}}+A^{T}w=0.
$$ 

Using $A x=b$ , these become 

$$
A\Delta x_{\mathrm{nt}}=0,\qquad\nabla^{2}f(x)\Delta x_{\mathrm{nt}}+A^{T}w=-\nabla f(x),
$$ 

which are precisely the equations ( 10.11 ) that define the Newton step. 

# The Newton decrement 

We define the Newton decrement for the equality constrained problem as 

$$
\begin{array}{r}{\lambda(x)=(\Delta x_{\mathrm{nt}}^{T}\nabla^{2}f(x)\Delta x_{\mathrm{nt}})^{1/2}.}\end{array}
$$ 

This is exactly the same expression as ( 9.29 ), used in the unconstrained case, and the same interpretations hold. For example, $\lambda(x)$ is the norm of the Newton step, in the norm determined by the Hessian. 

Let 

$$
\boldsymbol{\hat{f}}(\boldsymbol{x}+\boldsymbol{v})=\boldsymbol{f}(\boldsymbol{x})+\nabla f(\boldsymbol{x})^{T}\boldsymbol{v}+(1/2)\boldsymbol{v}^{T}\nabla^{2}f(\boldsymbol{x})\boldsymbol{v}
$$ 

be the second-order Taylor approximation of $f$ at $x$ . The diﬀerence between $f(x)$ and the minimum of the second-order model satisfies 

$$
f(x)-\operatorname*{inf}\{\widehat{f}(x+v)\mid A(x+v)=b\}=\lambda(x)^{2}/2,
$$ 

exactly as in the unconstrained case (see exercise 10.6 ). This means that, as in the unconstrained case, $\lambda(x)^{2}/2$ gives an estimate of $f(x)-p^{\star}$ , based on the quadratic model at $x$ , and also that $\lambda(x)$ (or a multiple of $\lambda(x)^{2}$ ) serves as the basis of a good stopping criterion. 

The Newton decrement comes up in the line search as well, since the directional derivative of $f$ in the direction $\Delta x_{\mathrm{{nt}}}$ is 

$$
\frac{d}{d t}f(\boldsymbol{x}+t\Delta x_{\mathrm{nt}})\bigg|_{t=0}=\nabla f(\boldsymbol{x})^{T}\Delta x_{\mathrm{nt}}=-\lambda(x)^{2},
$$ 

as in the unconstrained case. 

# Feasible descent direction 

Suppose that $A x=b$ . We say that $v\in\mathbf{R}^{n}$ is a feasible direction if $A v=0$ . In this case, every point of the form $x+t v$ is also feasible, i.e. , $A(x+t v)=b$ . We say that $v$ is a descent direction for $f$ at $x$ , if for small $t>0$ , $f(x+t v)<f(x)$ . 

The Newton step is always a feasible descent direction (except when $x$ is opti- mal, in which case $\Delta x_{\mathrm{nt}}=0$ ). Indeed, the second set of equations that define $\Delta x_{\mathrm{{nt}}}$ are $A\Delta x_{\mathrm{nt}}=0$ , which shows it is a feasible direction; that it is a descent direction follows from ( 10.14 ). 

# Affine invariance 

Like the Newton step and decrement for unconstrained optimization, the New- ton step and decrement for equality constrained optimization are affine invariant. Suppose $T\in\mathbf{R}^{n\times n}$ is nonsingular, and define $f(y)=f(T y)$ ). We have 

$$
\nabla\bar{f}(y)=T^{T}\nabla f(T y),\qquad\nabla^{2}\bar{f}(y)=T^{T}\nabla^{2}f(T y)T,
$$ 

and the equality constraint $A x=b$ becomes $A T y=b$ . 

Now consider the problem of minimizing $f(y)$ ), subject to $A T y=b$ . The Newton step $\Delta{y_{\mathrm{{nt}}}}$ at $y$ is given by the solution of 

$$
\begin{array}{r}{\left[\begin{array}{c c}{T^{T}\nabla^{2}f(T y)T}&{T^{T}A^{T}}\\ {A T}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta y_{\mathrm{nt}}}\\ {\bar{w}}\end{array}\right]=\left[\begin{array}{c}{-T^{T}\nabla f(T y)}\\ {0}\end{array}\right].}\end{array}
$$ 

Comparing with the Newton step $\Delta x_{\mathrm{{nt}}}$ for $f$ at $x=T y$ , given in ( 10.11 ), we see that 

$$
T\Delta y_{\mathrm{nt}}=\Delta x_{\mathrm{nt}}
$$ 

(and $w=\bar{w}$ ), i.e. , the Newton steps at $y$ and $x$ are related by the same change of coordinates as $T y=x$ . 

# 10.2.2 Newton’s method with equality constraints 

The outline of Newton’s method with equality constraints is exactly the same as for unconstrained problems. 

Algorithm 10.1 Newton’s method for equality constrained minimization. given starting point $x\in\mathbf{dom}\ f$ with $A x=b$ , tolerance $\epsilon>0$ . 

1. Compute the Newton step and decrement $\Delta x_{\mathrm{{nt}}}$ , $\lambda(x)$ . 2. Stopping criterion. quit if $\lambda^{2}/2\leq\epsilon$ . 3. Line search. Choose step size $t$ by backtracking line search. 4. Update. $x:=x+t\Delta x_{\mathrm{nt}}$ . 

The method is called a feasible descent method , since all the iterates are feasi- ble, with $f(x^{(k+1)})<f(x^{(k)})$ (unless $x^{(k)}$ is optimal). Newton’s method requires that the KKT matrix be invertible at each $x$ ; we will be more precise about the assumptions required for convergence in § 10.2.4 . 

# 10.2.3 Newton’s method and elimination 

We now show that the iterates in Newton’s method for the equality constrained problem ( 10.1 ) coincide with the iterates in Newton’s method applied to the re- duce problem . Suppose $F$ satisfies $\mathcal{R}(F)=\mathcal{N}(A)$ and $\mathbf{rank}\,F\,=\,n\,-\,p$ , and ˆ satisfies A $A\hat{x}=b$ . The gradient and Hessian of the reduced objective function $\tilde{f}(z)=f(F z+\hat{x})$ ) are 

$$
\begin{array}{r}{\nabla\tilde{f}(z)=F^{T}\nabla f(F z+\hat{x}),\qquad\nabla^{2}\tilde{f}(z)=F^{T}\nabla^{2}f(F z+\hat{x})F.}\end{array}
$$ 

From the Hessian expression, we see that the Newton step for the equality con- strained problem is defined, i.e. , the KKT matrix 

$$
\left[\begin{array}{c c}{{\nabla^{2}f(x)}}&{{A^{T}}}\\ {{A}}&{{0}}\end{array}\right]
$$ 

is invertible, if and only if the Newton step for the reduced problem is defined, i.e. , $\nabla^{2}{\ddot{f}}(z)$ ) is invertible. 

The Newton step for the reduced problem is 

$$
\Delta z_{\mathrm{int}}=-\nabla^{2}\tilde{f}(z)^{-1}\nabla\tilde{f}(z)=-(F^{T}\nabla^{2}f(x)F)^{-1}F^{T}\nabla f(x),
$$ 

where $\boldsymbol{x}=\boldsymbol{F}\boldsymbol{z}+\boldsymbol{\hat{x}}$ x . This search direction for the reduced problem corresponds to the direction 

$$
F\Delta z_{\mathrm{int}}=-F(F^{T}\nabla^{2}f(x)F)^{-1}F^{T}\nabla f(x)
$$ 

for the original, equality constrained problem. We claim this is precisely the same as the Newton direction $\Delta x_{\mathrm{{nt}}}$ for the original problem, defined in ( 10.11 ). 

To show this, we take $\Delta x_{\mathrm{nt}}=F\Delta z_{\mathrm{nt}}$ , choose 

$$
w=-(A A^{T})^{-1}A(\nabla f(x)+\nabla^{2}f(x)\Delta x_{\mathrm{nt}}),
$$ 

and verify that the equations defining the Newton step, 

$$
\begin{array}{r}{\nabla^{2}f(x)\Delta x_{\mathrm{nt}}+A^{T}w+\nabla f(x)=0,\qquad A\Delta x_{\mathrm{nt}}=0,}\end{array}
$$ 

hold. The second equation, $A\Delta x_{\mathrm{nt}}=0$ , is satisfied because $A F=0$ . To verify the first equation, we observe that 

$$
\begin{array}{r l}{\left[\begin{array}{l}{F^{T}}\\ {A}\end{array}\right]\left(\nabla^{2}f(x)\Delta x_{\mathrm{nt}}+A^{T}w+\nabla f(x)\right)}&{}\\ {=}&{\left[\begin{array}{l}{F^{T}\nabla^{2}f(x)\Delta x_{\mathrm{nt}}+F^{T}A^{T}w+F^{T}\nabla f(x)}\\ {\quad A\nabla^{2}f(x)\Delta x_{\mathrm{nt}}+A A^{T}w+A\nabla f(x)}\end{array}\right]}\\ {=}&{0.}\end{array}
$$ 

Since the matrix on the left of the first line is nonsingular, we conclude that ( 10.16 ) holds. 

In a similar way, the Newton decrement $\ddot{\lambda}(z)$ ) of $\ddot{f}$ at $z$ and the Newton decrement of $f$ at $x$ turn out to be equal: 

$$
\begin{array}{r c l}{{\tilde{\lambda}(z)^{2}}}&{{=}}&{{\Delta z_{\mathrm{nt}}^{T}\nabla^{2}\tilde{f}(z)\Delta z_{\mathrm{nt}}}}\\ {{}}&{{=}}&{{\Delta z_{\mathrm{nt}}^{T}F^{T}\nabla^{2}f(x)F\Delta z_{\mathrm{nt}}}}\\ {{}}&{{=}}&{{\Delta x_{\mathrm{nt}}^{T}\nabla^{2}f(x)\Delta x_{\mathrm{nt}}}}\\ {{}}&{{=}}&{{\lambda(x)^{2}.}}\end{array}
$$ 

# 10.2.4 Convergence analysis 

We saw above that applying Newton’s method with equality constraints is exactly the same as applying Newton’s method to the reduced problem obtained by elimi- nating the equality constraints. Everything we know about the convergence of New- ton’s method for unconstrained problems therefore transfers to Newton’s method for equality constrained problems. In particular, the practical performance of New- ton’s method with equality constraints is exactly like the performance of Newton’s $x^{(k)}$ method for unconstrained problems. Once is near $x^{\star}$ , convergence is extremely rapid, with a very high accuracy obtained in only a few iterations. 

# Assumptions 

We make the following assumptions. 

• The sublevel set $S\,=\,\{x\mid x\,\in\,\mathbf{dom}\,f$ $f(x)\,\leq\,f(x^{(0)})$ , $A x\,=\,b\}$ is closed, where $x^{(0)}~\in~\mathbf{dom}\,f$ satisfies Ax $A x^{(0)}~=~b$ . This is the case if $f$ is closed (see § A.3.3 ). • On the set $S$ , we have $\nabla^{2}f(x)\preceq M I$ , and $\left\|\left[\begin{array}{c c}{{\nabla^{2}f(x)}}&{{A^{T}}}\\ {{A}}&{{0}}\end{array}\right]^{-1}\right\|_{2}\leq K,$ (10.17) 

i.e. , the inverse of the KKT matrix is bounded on $S$ . (Of course the inverse must exist in order for the Newton step to be defined at each point in $S$ .) 

• For $x,\ \tilde{x}\,\in\,S$ ∈ , $\nabla^{2}f$ satisfies the Lipschitz condition $\|\nabla^{2}f(x)-\nabla^{2}f(\tilde{x})\|_{2}\leq$ ∥ ≤ $L\|x-{\tilde{x}}\|_{2}$ ∥ . 

# Bounded inverse KKT matrix assumption 

The condition ( 10.17 ) plays the role of the strong convexity assumption in the standard Newton method ( § 9.5.3 , page 488 ). When there are no equality con- 10.1 the ndition $\|\nabla^{2}f(x)^{-1}\|_{2}\leq K$ on $S$ , so we can take $K\,=\,1/m$ , if ∇ $\nabla^{2}f(x)\,\succeq\,m I$ ⪰ on S , where m > 0. With equality constraints, the condition is not as simple as a positive lower bound on the minimum eigenvalue. Since the KKT matrix is symmetric, the condition ( 10.17 ) is that its eigenvalues, $n$ of which are positive, and $p$ of which are negative, are bounded away from zero. 

# Analysis via the eliminated problem 

The assumptions above imply that the eliminated objective function $\ddot{f}$ , together with the associated initial point $z^{(0)}$ , where $x^{(0)}=\hat{x}+F z^{(0)}$ , satisfy the assump- tions required in the convergence analysis of Newton’s method for unconstrained problems, given in $\S9.5.3$ (with diﬀerent constants m , $\tilde{M}$ , and L ). It follows that Newton’s method with equality constraints converges to $x^{\star}$ (and $\nu^{\star}$ as well). 

To show that the assumptions above imply that the eliminated problem satisfies the assumptions for the unconstrained Newton method is mostly straightforward (see exercise 10.4 ). Here we show the one implication that is tricky: that the bounded inverse KKT condition, together with the upper bound $\nabla^{2}f(x)\,\preceq\,M I$ , implies that $\nabla^{2}{\dot{f}}(z)\succeq m I$ ⪰ for some positive constant $m$ . More specifically we will show that this inequality holds for 

$$
m=\frac{\sigma_{\mathrm{min}}(F)^{2}}{K^{2}M},
$$ 

which is positive, since $F$ is full rank. 

We show this by contradiction. Suppose tha $F^{I}H F\nsubseteq m I$ where $H=\nabla^{2}f(x)$ . we can find $u$ , with $\Vert u\Vert_{2}=1$ , such that u $u^{T}F^{T}H F u<m$ , i.e. , $||H^{1/2}F u||_{2}<$ $m^{1/2}$ . Using $A F=0$ , we have 

$$
\left[\begin{array}{c c}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{F u}\\ {0}\end{array}\right]=\left[\begin{array}{c}{H F u}\\ {0}\end{array}\right],
$$ 

and so 

$$
\left\|\left[\begin{array}{r r}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]^{-1}\right\|_{2}\geq\frac{\left\|\left[\begin{array}{r}{F u}\\ {0}\end{array}\right]\right\|_{2}}{\left\|\left[\begin{array}{r}{H F u}\\ {0}\end{array}\right]\right\|_{2}}=\frac{\|F u\|_{2}}{\|H F u\|_{2}}.
$$ 

Using $\|F u\|_{2}\ge\sigma_{\mathrm{min}}(F)$ and 

$$
\begin{array}{r}{\|H F u\|_{2}\le\|H^{1/2}\|_{2}\|H^{1/2}F u\|_{2}<M^{1/2}m^{1/2},}\end{array}
$$ 

we conclude 

$$
\left\|\left[\begin{array}{c c}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]^{-1}\right\|_{2}\geq\frac{\|F u\|_{2}}{\|H F u\|_{2}}>\frac{\sigma_{\operatorname*{min}}(F)}{M^{1/2}m^{1/2}}=K,
$$ 

using our expression for $m$ given in ( 10.18 ). 

# Convergence analysis for self-concordant functions 

If $f$ is self-concordant, then so is $\tilde{f}(z)\,=\,f(F z+\hat{x})$ ). It follows that if $f$ is self- concordant, we have the exact same complexity estimate as for unconstrained prob- lems: the number of iterations required to produce a solution within an accuracy $\epsilon$ is no more than 

$$
\frac{20-8\alpha}{\alpha\beta(1-2\alpha)^{2}}(f(x^{(0)})-p^{\star})+\log_{2}\log_{2}(1/\epsilon),
$$ 

where $\alpha$ and $\beta$ are the backtracking parameters (see ( 9.56 )). 

# 10.3 Infeasible start Newton method 

Newton’s method, as described in $\S10.2$ , is a feasible descent method. In this section we describe a generalization of Newton’s method that works with initial points, and iterates, that are not feasible. 

# 10.3.1 Newton step at infeasible points 

As in Newton’s method, we start with the optimality conditions for the equality constrained minimization problem: 

$$
A x^{\star}=b,\qquad\nabla f(x^{\star})+A^{T}\nu^{\star}=0.
$$ 

Let $x$ denote the current point, which we do not assume to be feasible, but we do assume satisfies $x\in\mathbf{dom}\,f$ . Our goal is to find a step $\Delta x$ so that $x+\Delta x$ satisfies (at least approximately) the optimality conditions, i.e. , $x+\Delta x\approx x^{\star}$ . To do this we substitute $x+\Delta x$ for $x^{\star}$ and $w$ for $\nu^{\star}$ in the optimality conditions, and use the first-order approximation 

$$
\nabla f(x+\Delta x)\approx\nabla f(x)+\nabla^{2}f(x)\Delta x
$$ 

for the gradient to obtain 

$$
A(x+\Delta x)=b,\qquad\nabla f(x)+\nabla^{2}f(x)\Delta x+A^{T}w=0.
$$ 

This is a set of linear equations for $\Delta x$ and $w$ , 

$$
\left[\begin{array}{c c}{\nabla^{2}f(x)}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x}\\ {w}\end{array}\right]=-\left[\begin{array}{c}{\nabla f(x)}\\ {A x-b}\end{array}\right].
$$ 

The equations are the same as the equations ( 10.11 ) that define the Newton step at a feasible point $x$ , with one diﬀerence: the second block component of the righthand side contains $A x-b$ , which is the residual vector for the linear equality constraints. When $x$ is feasible, the residual vanishes, and the equations ( 10.19 ) reduce to the equations ( 10.11 ) that define the standard Newton step at a feasible point $x$ . Thus, if $x$ is feasible, the step $\Delta x$ defined by ( 10.19 ) coincides with the Newton step described above (but defined only when $x$ is feasible). For this reason we use the notation $\Delta x_{\mathrm{{nt}}}$ for the step $\Delta x$ defined by ( 10.19 ), and refer to it as the Newton step at $x$ , with no confusion. 

# Interpretation as primal-dual Newton step 

We can give an interpretation of the equations ( 10.19 ) in terms of a primal-dual method for the equality constrained problem. By a primal-dual method, we mean one in which we update both the primal variable $x$ , and the dual variable $\nu$ , in order to (approximately) satisfy the optimality conditions. 

We express the optimality conditions as $r(x^{\star},\nu^{\star})\,=\,0$ , where $r:\mathbf{R}^{n}\times\mathbf{R}^{p}\rightarrow$ ${\bf R}^{n}\times{\bf R}^{p}$ is defined as 

$$
r(x,\nu)=(r_{\mathrm{dual}}(x,\nu),r_{\mathrm{pi}}(x,\nu)).
$$ 

Here 

$$
\boldsymbol{r}_{\mathrm{dual}}(\boldsymbol{x},\nu)=\nabla f(\boldsymbol{x})+\boldsymbol{A}^{T}\nu,\qquad\boldsymbol{r}_{\mathrm{phi}}(\boldsymbol{x},\nu)=\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b}
$$ 

are the dual residual and primal residual , respectively. The first-order Taylor ap- proximation of $r$ , near our current estimate $y$ , is 

$$
r(y+z)\approx\hat{r}(y+z)=r(y)+D r(y)z,
$$ 

where $D r(y)\,\in\,{\bf R}^{(n+p)\times(n+p)}$ is the derivative of $r$ , evaluated at $y$ (see $\S$ A.4.1 ). We define the primal-dual Newton step $\Delta y_{\mathrm{{pd}}}$ as the step $z$ for which the Taylor approximation $\hat{r}(y+z)$ ) vanishes, i.e. , 

$$
D r(y)\Delta y_{\mathrm{{pd}}}=-r(y).
$$ 

Note that here we consider both $x$ and $\nu$ as variables; $\Delta y_{\mathrm{{pd}}}=\left(\Delta x_{\mathrm{{pd}}},\Delta\nu_{\mathrm{{pd}}}\right)$ gives both a primal and a dual step. 

Evaluating the derivative of $r$ , we can express ( 10.20 ) as 

$$
\left[\begin{array}{c c}{\nabla^{2}f(x)}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{pd}}}\\ {\Delta\nu_{\mathrm{pd}}}\end{array}\right]=-\left[\begin{array}{c}{r_{\mathrm{dual}}}\\ {r_{\mathrm{pri}}}\end{array}\right]=-\left[\begin{array}{c}{\nabla f(x)+A^{T}\nu}\\ {A x-b}\end{array}\right].
$$ 

Writing $\nu+\Delta\nu_{\mathrm{pd}}$ as $\nu^{+}$ , we can express this as 

$$
\left[\begin{array}{c c}{\nabla^{2}f(x)}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{pd}}}\\ {\nu^{+}}\end{array}\right]=-\left[\begin{array}{c}{\nabla f(x)}\\ {A x-b}\end{array}\right],
$$ 

which is exactly the same set of equations as ( 10.19 ). The solutions of ( 10.19 ), ( 10.21 ), and $\left(10.22\right)$ are therefore related as 

$$
\Delta x_{\mathrm{nt}}=\Delta x_{\mathrm{pd}},\qquad w=\nu^{+}=\nu+\Delta\nu_{\mathrm{pd}}.
$$ 

This shows that the (infeasible) Newton step is the same as the primal part of the primal-dual step, and the associated dual vector $w$ is the updated primal-dual variable $\nu^{+}=\nu+\Delta\nu_{\mathrm{pd}}$ . 

The two expressions for the Newton step and dual variable (or dual step), given by ( 10.21 ) and ( 10.22 ), are of course equivalent, but each reveals a diﬀerent feature of the Newton step. The equation ( 10.21 ) shows that the Newton step and the associated dual step are obtained by solving a set of equations, with the primal and dual residuals as the righthand side. The equation ( 10.22 ), which is how we originally defined the Newton step, gives the Newton step and the updated dual variable, and shows that the current value of the dual variable is not needed to compute the primal step, or the updated value of the dual variable. 

# Residual norm reduction property 

The Newton direction, at an infeasible point, is not necessarily a descent direction for $f$ . From ( 10.19 ), we note that 

$$
\begin{array}{r c l}{\displaystyle\frac{d}{d t}f(x+t\Delta x)\bigg|_{t=0}}&{=}&{\nabla f(x)^{T}\Delta x}\\ &{=}&{-\Delta x^{T}\left(\nabla^{2}f(x)\Delta x+A^{T}w\right)}\\ &{=}&{-\Delta x^{T}\nabla^{2}f(x)\Delta x+(A x-b)^{T}w,}\end{array}
$$ 

which is not necessarily negative (unless, of course, $x$ is feasible, i.e. , $A x=b$ ). The primal-dual interpretation, however, shows that the norm of the residual decreases in the Newton direction, i.e. , 

$$
\frac{d}{d t}\left\|r(y+t\Delta y_{\mathrm{pd}})\right\|_{2}^{2}\bigg|_{t=0}=2r(y)^{T}D r(y)\Delta y_{\mathrm{pd}}=-2r(y)^{T}r(y).
$$ 

Taking the derivative of the square, we obtain 

$$
\frac{d}{d t}\left\|r(y+t\Delta y_{\mathrm{pd}})\right\|_{2}\bigg|_{t=0}=-\|r(y)\|_{2}.
$$ 

This allows us to use $\|r\|_{2}$ to measure the progress of the infeasible start Newton method, for example, in the line search. (For the standard Newton method, we use the function value $f$ to measure progress of the algorithm, at least until quadratic convergence is attained.) 

# Full step feasibility property 

The Newton step $\Delta x_{\mathrm{{nt}}}$ defined by ( 10.19 ) has the property (by construction) that 

$$
A(x+\Delta x_{\mathrm{{nt}}})=b.
$$ 

It follows that, if a step length of one is taken using the Newton step $\Delta x_{\mathrm{{nt}}}$ , the following iterate will be feasible. Once $x$ is feasible, the Newton step becomes a feasible direction, so all future iterates will be feasible, regardless of the step sizes taken. 

More generally, we can analyze the eﬀect of a damped step on the equality constraint residual $r_{\mathrm{pri}}$ . With a step length $t\,\in\,[0,1]$ , the next iterate is $x^{+}\,=$ $x+t\Delta x_{\mathrm{nt}}$ , so the equality constraint residual at the next iterate is 

$$
r_{\mathrm{{pri}}}^{+}=A(x+\Delta x_{\mathrm{{nt}}}t)-b=(1-t)(A x-b)=(1-t)r_{\mathrm{{pri}}},
$$ 

using ( 10.24 ). Thus, a damped step, with length $t$ , causes the residual to be scaled down by a factor $1-t$ . Now suppose that we have $x^{(i+1)}\,=\,x^{(i)}+t^{(i)}\Delta x_{\mathrm{nt}}^{(i)}$ , for $i=0,\ldots,k-1$ , where $\Delta x_{\mathrm{nt}}^{(i)}$ is the Newton step at the point $x^{(i)}\in\mathbf{dom}\,f$ , and $t^{(i)}\in[0,1]$ . Then we have 

$$
r^{(k)}=\left(\prod_{i=0}^{k-1}(1-t^{(i)})\right)r^{(0)},
$$ 

where $r^{(i)}=A x^{(i)}-b$ is the residual of $x^{(i)}$ . This formula shows that the primal residual at each step is in the direction of the initial primal residual, and is scaled down at each step. It also shows that once a full step is taken, all future iterates are primal feasible. 

# 10.3.2 Infeasible start Newton method 

We can develop an extension of Newton’s method, using the Newton step $\Delta x_{\mathrm{{nt}}}$ defined by ( 10.19 ), with $x^{(0)}\,\in\,\mathbf{dom}\,f$ , but not necessarily satisfying $A x^{(0)}\,=\,b$ . We also use the dual part of the Newton step: $\Delta\nu_{\mathrm{rt}}\,=\,w\,-\,\nu$ in the notation of ( 10.19 ), or equivalently, $\Delta\nu_{\mathrm{nt}}=\Delta\nu_{\mathrm{pd}}$ in the notation of ( 10.21 ). 

Algorithm 10.2 Infeasible start Newton method. 

given starting point $x\in\mathbf{dom}\ f$ , $\nu$ , tolerance $\epsilon>0$ , $\alpha\in(0,1/2)$ , $\beta\in(0,1)$ . repeat 

1. Compute primal and dual Newton steps $\Delta x_{\mathrm{{nt}}}$ , $\Delta\nu_{\mathrm{{nt}}}$ . 2. B cking line search on $\|\boldsymbol{r}\|_{2}$ . := 1. while $\|r(x+t\Delta x_{\mathrm{nt}},\nu+t\Delta\nu_{\mathrm{nt}})\|_{2}>(1-\alpha t)\|r(x,\nu)\|_{2},\quad t:=\beta t.$ 3. Update. $x:=x+t\Delta x_{\mathrm{nt}}$ , $\nu:=\nu+t\Delta\nu_{\mathrm{nt}}$ . until $A x=b$ and $||r(x,\nu)||_{2}\leq\epsilon$ . 

This algorithm is very similar to the standard Newton method with feasible start- ing point, with a few exceptions. First, the search directions include the extra correction terms that depend on the primal residual. Second, the line search is carried out using the norm of the residual, instead of the function value $f$ . Finally, the algorithm terminates when primal feasibility has been achieved, and the norm of the (dual) residual is small. 

The line search in step 2 deserves some comment. Using the norm of the residual in the line search can increase the cost, compared to a line search based on the function value, but the increase is usually negligible. Also, we note that the line search must terminate in a finite number of steps, since ( 10.23 ) shows that the line search exit condition is satisfied for small $t$ . 

The equation ( 10.24 ) shows that if at some iteration the step length is chosen to be one, the next iterate will be feasible. Thereafter, all iterates will be feasible, and therefore the search direction for the infeasible start Newton method coincides, once a feasible iterate is obtained, with the search direction for the (feasible) Newton method described in 10.2 . 

There are many variations on the infeasible start Newton method. For example, we can switch to the (feasible) Newton method described in $\S10.2$ once feasibility is achieved. (In other words, we change the line search to one based on $f$ , and terminate when $\lambda(x)^{2}/2\leq\epsilon$ .) Once feasibility is achieved, the infeasible start and the standard (feasible) Newton method diﬀer only in the backtracking and exit conditions, and have very similar performance. 

# Using infeasible start Newton method to simplify initialization 

The main advantage of the infeasible start Newton method is in the initialization required. If $\mathbf{dom}\,f\,=\,\mathbf{R}^{\,n}$ , then initializing the (feasible) Newton method simply requires computing a solution to $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ , and there is no particular advantage, other than convenience, in using the infeasible start Newton method. 

When $\mathbf{dom}\ f$ is not all of $\mathbf{R}^{n}$ , finding a point in $\mathbf{dom}\ f$ that satisfies $A x=b$ can itself be a challenge. One general approach, probably the best when $\mathbf{dom}\ f$ is complex and not known to intersect $\{z\mid A z=b\}$ , is to use a phase I method (de- scribed in § 11.4 ) to compute such a point (or verify that $\mathbf{dom}\,f$ does not intersect $\{z\mid A z=b\}$ $\mathbf{dom}\ f$ is relatively simple, and known to contain a point satisfying $A x=b$ , the infeasible start Newton method gives a simple alternative. 

One common example occurs when $\mathbf{\deltadom}\,f\ =\ \mathbf{R}_{++}^{n}$ , as in the equality con- strained analytic centering problem described in example 10.2 . To initialize New- ton’s method for the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{-\sum_{i=1}^{n}\log x_{i}}\\ {{\mathrm{subject~to}}}&{A x=b,}\end{array}}
$$ 

requires finding a point $x^{(0)}\succ0$ with $A x=b$ , which is equivalent to solving a stan- dard form LP feasibility problem. This can be carried out using a phase I method, or alternatively, using the infeasible start Newton method, with any positive initial $x^{(0)}=\mathbf{1}$ point, e.g. , . 

The same trick can be used to initialize unconstrained problems where a starting point in $\mathbf{dom}\ f$ is not known. As an example, we consider the dual of the equality constrained analytic centering problem ( 10.25 ), 

$$
\begin{array}{r l}{\mathrm{maximize}}&{{}g(\nu)=-b^{T}\nu+n+\sum_{i=1}^{n}\log(A^{T}\nu)_{i}.}\end{array}
$$ 

To initialize this problem for the (feasible start) Newton method, we must find a point $\ensuremath{\boldsymbol\nu}^{(0)}$ that satisfies $A^{T}\nu^{(0)}\succ0$ , i.e. , we must solve a set of linear inequalities. This can be done using a phase I method, or using an infeasible start Newton method, after reformulating the problem. We first express it as an equality con- strained problem, 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-b^{T}\nu+n+\sum_{i=1}^{n}\log y_{i}}\\ &{\mathrm{subject~to}\quad y=A^{T}\nu,}\end{array}
$$ 

with new variable $y\in\mathbf{R}^{n}$ . We can now e the infeasible start Newton method, starting with any positive $y^{(0)}$ (and any ν $\ensuremath{\boldsymbol\nu}^{(0)}$ ). 

The disadvantage of using the infeasible start Newton method to initialize prob- lems for which a strictly feasible starting point is not known is that there is no clear way to detect that there exists no strictly feasible point; the norm of the residual will simply converge, slowly, to some positive value. (Phase I methods, in contrast, can determine this fact unambiguously.) In addition, the convergence of the infea- sible start Newton method, before feasibility is achieved, can be slow; see § 11.4.2 . 

# 10.3.3 Convergence analysis 

In this section we show that the infeasible start Newton method converges to the optimal point, provided certain assumptions hold. The convergence proof is very similar to those for the standard Newton method, or the standard Newton method with equality constraints. We show that once the norm of the residual is small enough, the algorithm takes full steps (which implies that feasibility is achieved), and convergence is subsequently quadratic. We also show that the norm of the residual is reduced by at least a fixed amount in each iteration before the region of quadratic convergence is reached. Since the norm of the residual cannot be negative, this shows that within a finite number of steps, the residual will be small enough to guarantee full steps, and quadratic convergence. 

# Assumptions 

We make the following assumptions. 

The sublevel set 

$$
S=\{(x,\nu)\mid x\in\mathbf{dom}\,f,\ \|r(x,\nu)\|_{2}\leq\|r(x^{(0)},\nu^{(0)})\|_{2}\}
$$ 

is closed. If $f$ is closed, then $\|r\|_{2}$ is a closed fun therefore this con- dition is satisfied for any $x^{(0)}\in\mathbf{dom}\,f$ and any ν $\boldsymbol{\nu}^{(0)}\in{\mathbf{R}}^{p}$ ∈ (see exercise 10.7 ). 

On the set $S$ , we have 

$$
\|D r(x,\nu)^{-1}\|_{2}=\left\|\left[\begin{array}{c c}{{\nabla^{2}f(x)}}&{{A^{T}}}\\ {{A}}&{{0}}\end{array}\right]^{-1}\right\|_{2}\leq K,
$$ 

for some $K$ . 

• For $(x,\nu)$ , $(\tilde{x},\tilde{\nu})\in S$ ∈ , $D r$ satisfies the Lipschitz condition 

$$
\begin{array}{r}{\|D r(x,\nu)-D r(\tilde{x},\tilde{\nu})\|_{2}\leq L\|(x,\nu)-(\tilde{x},\tilde{\nu})\|_{2}.}\end{array}
$$ 

(This is equivalent to $\nabla^{2}f(x)$ satisfying a Lipschitz condition; see exer- cise 10.7 .) 

As we will see below, these assumptions imply that $\mathbf{dom}\ f$ and $\{z\ \mid\ A z\ =\ b\}$ intersect, and that there is an optimal point $(x^{\star},\nu^{\star})$ . 

# Comparison with standard Newton method 

The assumptions above are very similar to the ones made in § 10.2.4 (page 529 ) for the analysis of the standard Newton method. The second and third assump- tions, the bounded inverse KKT matrix and Lipschitz condition, are essentially the same. The sublevel set condition ( 10.26 ) for the infeasible start Newton method is, however, more general than the sublevel set condition made in 10.2.4 . 

As an example, consider the equality constrained maximum entropy problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad f(x)=\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}\quad A x=b,}\end{array}}
$$ 

with $\mathbf{dom}\,f=\mathbf{R}_{++}^{n}$ . The objective $f$ is not closed; it has sublevel sets that are not closed, so the assumptions made in the standard Newton method may not hold, at least for some initial points. The problem here is that the negative entropy function does not converge to $\infty$ as $x_{i}\,\rightarrow\,0$ . On the other hand the sublevel set condition ( 10.26 ) for the infeasible start Newton method does hold for this problem, since the norm of the gradient of the negative entropy function does converge to $\infty$ as $x_{i}\to0$ . Thus, the infeasible start Newton method is guaranteed to solve the equality constrained maximum entropy problem. (We do not know whether the standard Newton method can fail for this problem; we are only observing here that our convergence analysis does not hold.) Note that if the initial point satisfies the equality constraints, the only diﬀerence between the standard and infeasible start Newton methods is in the line searches, which diﬀer only during the damped stage. 

# A basic inequality 

We basic inequality. Let $y=(x,\nu)\in S$ with $||r(y)||_{2}\neq0$ , and let ∆ $\Delta y_{\mathrm{nt}}=\left(\Delta x_{\mathrm{nt}},\Delta\nu_{\mathrm{nt}}\right)$ ) be the Newton step at $y$ . Define 

$$
t_{\mathrm{max}}=\operatorname*{inf}\{t>0\ \lvert\ y+t\Delta y_{\mathrm{nt}}\not\in S\}.
$$ 

If $y+t\Delta y_{\mathrm{nt}}\in S$ for all $t\geq0$ , we follow the usual c nvention an $t_{\mathrm{max}}=\infty$ Otherwise, t $t_{\mathrm{max}}$ is the smallest positive value of t such that ∥ $\|r(y+t\Delta y_{\mathrm{nt}})\|_{2}\,=$ ∥ $||r(y^{(0)})||_{2}$ . In particular, it follows that $y+t\Delta y_{\mathrm{nt}}\in S$ for $0\leq t\leq t_{\mathrm{max}}$ . 

We will show that 

$$
\|r(y+t\Delta y_{\mathrm{nt}})\|_{2}\leq(1-t)\|r(y)\|_{2}+(K^{2}L/2)t^{2}\|r(y)\|_{2}^{2}
$$ 

for $0\leq t\leq\operatorname*{min}\{1,t_{\operatorname*{max}}\}$ . We have 

$$
\begin{array}{r c l}{r(y+t\Delta y_{\mathrm{net}})}&{=}&{r(y)+\displaystyle\int_{0}^{1}D r(y+\tau t\Delta y_{\mathrm{int}})t\Delta y_{\mathrm{net}}\,d\tau}\\ &{=}&{r(y)+t D r(y)\Delta y_{\mathrm{net}}+\displaystyle\int_{0}^{1}(D r(y+\tau t\Delta y_{\mathrm{net}})-D r(y))t\Delta y_{\mathrm{net}}\,d\tau}\\ &{=}&{r(y)+t D r(y)\Delta y_{\mathrm{net}}+e}\\ &{=}&{(1-t)r(y)+e,}\end{array}
$$ 

using $D r(y)\Delta y_{\mathrm{nt}}=-r(y)$ , and defining 

$$
e=\int_{0}^{1}(D r(y+\tau t\Delta y_{\mathrm{nt}})-D r(y))t\Delta y_{\mathrm{nt}}\,d\tau.
$$ 

Now suppose $0\leq t\leq t_{\operatorname*{max}}$ , so $y+\tau t\Delta y_{\mathrm{nt}}\in S$ for $0\leq\tau\leq1$ . We can bound $\|e\|_{2}$ as follows: 

$$
\begin{array}{r c l}{\|e\|_{2}}&{\leq}&{\|t\Delta y_{\mathrm{Int}}\|_{2}\displaystyle\int_{0}^{1}\|D r(y+\tau t\Delta y_{\mathrm{nt}})-D r(y)\|_{2}\,d\tau}\\ &{\leq}&{\|t\Delta y_{\mathrm{nt}}\|_{2}\displaystyle\int_{0}^{1}L\|\tau t\Delta y_{\mathrm{nt}}\|_{2}\,d\tau}\\ &{=}&{(L/2)t^{2}\|\Delta y_{\mathrm{nt}}\|_{2}^{2}}\\ &{=}&{(L/2)t^{2}\|D r(y)^{-1}r(y)\|_{2}^{2}}\\ &{\leq}&{(K^{2}L/2)t^{2}\|r(y)\|_{2}^{2},}\end{array}
$$ 

using the Lipschitz condition on the second line, and th $||D r(y)^{-1}||_{2}\leq K$ on the last. Now we can derive the bound ( 10.28 ): For 0 $0\leq t\leq\operatorname*{min}\{1,t_{\operatorname*{max}}\}$ ≤ ≤ { } , 

$$
\begin{array}{l l l}{\|r(y+t\Delta y_{\mathrm{nt}})\|_{2}}&{=}&{\|(1-t)r(y)+e\|_{2}}\\ &{\leq}&{(1-t)\|r(y)\|_{2}+\|e\|_{2}}\\ &{\leq}&{(1-t)\|r(y)\|_{2}+(K^{2}L/2)t^{2}\|r(y)\|_{2}^{2}.}\end{array}
$$ 

# Damped Newton phase 

We first show that if $\|r(y)\|_{2}\;>\;1/(K^{2}L)$ , one iteration of the infeasible start Newton method reduces $\|r\|_{2}$ by at least a certain minimum amount. 

The righthand side of the basic inequality ( 10.28 ) is quadratic in $t$ , and mono- tonically decreasing between $t=0$ and its minimizer 

$$
\bar{t}=\frac{1}{K^{2}L||r(y)||_{2}}<1.
$$ 

We must have $t_{\mathrm{max}}\,>\,t$ , because the opposite would imply $\|r(y+t_{\mathrm{max}}\Delta y_{\mathrm{n}t})\|_{2}<$ $\|r(y)\|_{2}$ , which is false. The basic inequality is therefore valid at t $t=t$ ¯ , and therefore 

$$
\begin{array}{r c l}{\|r(y+\bar{t}\Delta y_{\mathrm{nt}})\|_{2}}&{\leq}&{\|r(y)\|_{2}-1/(2K^{2}L)}\\ &{\leq}&{\|r(y)\|_{2}-\alpha/(K^{2}L)}\\ &{=}&{(1-\alpha\bar{t})\|r(y)\|_{2},}\end{array}
$$ 

which shows that the step length t satisfies the line search exit condition. Therefore we have $t\geq\beta t$ ¯ , where $t$ is the step length chosen by the backtracking algorithm. From $t\geq\beta t$ we have (from the exit condition in the backtracking line search) 

$$
\begin{array}{l l l}{\displaystyle\|r(y+t\Delta y_{\mathrm{nt}})\|_{2}}&{\leq}&{(1-\alpha t)\|r(y)\|_{2}}\\ &{\leq}&{(1-\alpha\beta\bar{t})\|r(y)\|_{2}}\\ &{=}&{\left(1-\displaystyle\frac{\alpha\beta}{K^{2}L\|r(y)\|_{2}}\right)\|r(y)\|_{2}}\\ &{=}&{\|r(y)\|_{2}-\displaystyle\frac{\alpha\beta}{K^{2}L}.}\end{array}
$$ 

Thus, as long as we have $\|r(y)\|_{2}\,>\,1/(K^{2}L)$ , we obtain a minimum decrease in $\|r\|_{2}$ , per iteration, of $\alpha\beta/(K^{2}L)$ . It follows that a maximum of 

$$
\frac{||r(y^{(0)})||_{2}K^{2}L}{\alpha\beta}
$$ 

iterations can be taken before we have $\|r(y^{(k)})\|_{2}\leq1/(K^{2}L)$ . 

# Quadratically convergent phase 

Now suppose $\|r(y)\|_{2}\le1/(K^{2}L)$ . The basic inequality gives 

$$
\|r(y+t\Delta y_{\mathrm{nt}})\|_{2}\leq(1-t+(1/2)t^{2})\|r(y)\|_{2}
$$ 

for $0\leq t\leq\operatorname*{min}\{1,t_{\operatorname*{max}}\}$ . We must have $t_{\operatorname*{max}}>1$ , because otherwise it would follow from ( 10.29 ) that $||r(y+t_{\mathrm{max}}\Delta y_{\mathrm{nt}})||_{2}<||r(y)||_{2}$ , wh ontradicts the definition of $t_{\mathrm{max}}$ . The inequality ( 10.29 ) therefore holds with t = 1, i.e. , we have 

$$
\|r(y+\Delta y_{\mathrm{nt}})\|_{2}\leq(1/2)\|r(y)\|_{2}\leq(1-\alpha)\|r(y)\|_{2}.
$$ 

This shows that the backtracking line search exit criterion is satisfied for $t\,=\,1$ , step will be taken. Moreover, for all future iterations we have $||r(y)||_{2}\leq$ $1/(K^{2}L)$ ), so a full step will be taken for all following iterations. 

We can write the inequality ( 10.28 ) (for $t=1$ ) as 

$$
\frac{K^{2}L\|r(y^{+})\|_{2}}{2}\leq\left(\frac{K^{2}L\|r(y)\|_{2}}{2}\right)^{2},
$$ 

where $y^{+}=y+\Delta{y_{\mathrm{{nt}}}}$ . Therefore, if $r(y^{+k})$ denotes the residual $k$ steps after an iteration in which $\|r(y)\|_{2}\le1/K^{2}L$ , we have 

$$
\frac{K^{2}L\|r(y^{+k})\|_{2}}{2}\leq\left(\frac{K^{2}L\|r(y)\|_{2}}{2}\right)^{2^{k}}\leq\left(\frac{1}{2}\right)^{2^{k}},
$$ 

i.e. , we have quadratic convergence of $||r(y)||_{2}$ to zero. 

To show that the sequence of iterates converges, we will show that it is a Cauchy sequence. Suppose $y$ is an iterate satisfying $\|r(y)\|_{2}\leq1/(K^{2}L)$ , and $y^{+k}$ denotes the $k$ th iterate after . Since these iterates are in the region of quadratic conver- $y$ gence, the step size is one, so we have 

$$
\begin{array}{l l l}{\displaystyle\|y^{+k}-y\|_{2}}&{\leq}&{\displaystyle\|y^{+k}-y^{+(k-1)}\|_{2}+\cdots+\|y^{+}-y\|_{2}}\\ &{=}&{\displaystyle\|D r(y^{+(k-1)})^{-1}r(y^{+(k-1)})\|_{2}+\cdots+\|D r(y)^{-1}r(y)\|_{2}}\\ &{\leq}&{\displaystyle K\left(\|r(y^{+(k-1)})\|_{2}+\cdots+\|r(y)\|_{2}\right)}\\ &{\leq}&{\displaystyle K\|r(y)\|_{2}{\sum_{i=0}^{k-1}}\left(\frac{K^{2}L\|r(y)\|_{2}}{2}\right)^{2^{i}-1}}\\ &{\leq}&{\displaystyle K\|r(y)\|_{2}{\sum_{i=0}^{k-1}}\left(\frac{1}{2}\right)^{2^{i}-1}}\\ &{\leq}&{2K\|r(y)\|_{2}}\end{array}
$$ 

where rd line we use the assumption th $\|D r^{-1}\|_{2}\leq K$ for all iterates. Since ∥ $||r(y^{(k)})||_{2}$ ∥ converges to zero, we conclude y $y^{(k)}$ is a Cauchy sequence, and therefore converges. By continuity of $r$ , the limit point $y^{\star}$ satisfies $r(y^{\star})=0$ . This establishes our earlier claim that the assumptions at the beginning of this section imply that there is an optimal point $(x^{\star},\nu^{\star})$ . 

# 10.3.4 Convex-concave games 

The proof of convergence for the infeasible start Newton method reveals that the method can be used for a larger class of problems than equality constrained convex optimization problems. Suppos $r~:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}^{n}$ is diﬀerentiable, ts derivative satisfies a Lipschitz condition on S , and $||D r(x)^{-1}||_{2}$ is bounded on S , where 

$$
S=\{x\in\mathbf{dom}\,r\mid\|r(x)\|_{2}\leq\|r(x^{(0)})\|_{2}\}
$$ 

is a closed set. Then the infeasible start Newton method, started at $x^{(0)}$ , converges to a solution of $r(x)=0$ in $S$ . In the infeasible start Newton method, we apply this to the specific case in which $r$ is the residual for the equality constrained convex optimization problem. But it applies in several other interesting cases. One interesting example is solving a convex-concave game . (See § 5.4.3 and exercise 5.25 for discussion of other, related games). 

An unconstrained (zero-sum, two-player) game on $\mathbf{R}^{p}\times\mathbf{R}^{q}$ is defined by its payoﬀfunction $f:\mathbf{R}^{p+q}\rightarrow\mathbf{R}$ . The meaning is that player 1 chooses a value (or move) $u\,\in\,\mathbf{R}^{p}$ , and player 2 chooses a value (or move) $v\,\in\,{\bf R}^{q}$ ; based on these choices, player 1 makes a payment to player 2, in the amount $f(u,v)$ . The goal of player $1$ is to minimize this payment, while the goal of player 2 is to maximize it. 

If player 1 makes his choice $u$ first, and player 2 knows the choice, then player 2 will choose $v$ to maximize $f(u,v)$ , which results in a payoﬀof sup $f(u,v)$ (assuming v the supremum is achieved). If player 1 assumes that player 2 will make this choice, he should choose $u$ to minimize $\operatorname*{sup}_{v}f(u,v)$ . The resulting payoﬀ, from player 1 to player 2, will then be 

$$
\operatorname*{inf}_{u}\,\operatorname*{sup}_{v}\,f(u,v)
$$ 

(assuming that the supremum is achieved). On the other hand if player 2 makes the first choice, the strategies are reversed, and the resulting payoﬀfrom player 1 to player 2 is 

$$
\operatorname*{sup}_{v}\ \operatorname*{inf}_{u}\ f(u,v).
$$ 

The payoﬀ( 10.30 ) is always greater than or equal to the payoﬀ( 10.31 ); the dif- ference between the two payoﬀs can be interpreted as the advantage aﬀorded the player who makes the second move, with knowledge of the other player’s move. We say that $(u^{\star},v^{\star})$ is a solution of the game, or a saddle-point for the game, if for all $u,~v$ , 

$$
f(u^{\star},v)\leq f(u^{\star},v^{\star})\leq f(u,v^{\star}).
$$ 

When a solution exists, there is no advantage to making the second move; $f(u^{\star},v^{\star})$ is the common value of both payoﬀs ( 10.30 ) and ( 10.31 ). (See exercise 3.14 .) 

The game is called convex-concave if for each $v$ , $f(u,v)$ is a convex function of $u$ , and for each $u$ , $f(u,v)$ is a concave function of $v$ . When $f$ is diﬀerentiable (and convex-concave), a saddle-point for the game is characterized by $\nabla f(u^{\star},v^{\star})=0$ . 

# Solution via infeasible start Newton method 

We can use the infeasible start Newton method to compute a solution of a convex- concave game with twice diﬀerentiable payoﬀfunction. We define the residual as 

$$
\boldsymbol{r}(u,v)=\nabla f(u,v)=\left[\begin{array}{l}{\nabla_{u}f(u,v)}\\ {\nabla_{v}f(u,v)}\end{array}\right],
$$ 

and apply the infeasible start Newton method. In the context of games, the infea- sible start Newton method is simply called Newton’s method (for convex-concave games). 

We can guarantee convergence of the (infeasible start) Newton method provided $D r=\nabla^{2}f$ has bounded inverse, and satisfies a Lipschitz condition on the sublevel set 

$$
S=\{(u,v)\in\mathbf{dom}\,f\ |\ \|r(u,v)\|_{2}\leq\|r(u^{(0)},v^{(0)})\|_{2}\},
$$ 

where $u^{(0)},\ v^{(0)}$ are the starting players’ choices. 

There is a simple analog of the strong convexity condition in an unconstrained minimization problem. We say the game with payoﬀfunction $f$ is strongly convex- concave if for some $m>0$ , we have $\nabla_{u u}^{2}f(u,v)\succeq m I$ ⪰ and $\nabla_{v v}^{2}f(u,v)\preceq-m I$ ⪯− , for all $(u,v)\in S$ . Not surprisingly, this strong convex-concave assumption implies the bounded inverse condition (exercise 10.10 ). 

# 10.3.5 Examples 

# A simple example 

We illustrate the infeasible start Newton method on the equality constrained an- alytic center problem ( 10.25 ). Our first example is an instance with dimensions $n=100$ and $m\,=\,50$ , generated randomly, for which the problem is feasible and bounded below. The infeasible start Newton method is used, with initial primal and dual points $x^{(0)}\,=\,{\bf1}$ , $\nu^{(0)}\,=\,0$ , and backtracking parameters $\alpha\,=\,0.01$ and $\beta\,=\,0.5$ . The plot in figure 10.1 shows the norms of the primal and dual residu- als separately, versus iteration number, and the plot in figure 10.2 shows the step lengths. A full Newton step is taken in iteration 8, so the primal residual becomes (almost) zero, and remains (almost) zero. After around iteration 9 or so, the (dual) residual converges quadratically to zero. 

# An infeasible example 

We also consider a problem instance, of the same dimensions as the example above, for which $\mathbf{dom}\ f$ does not intersect $\{z\mid A z\,=\,b\}$ , i.e. , the problem is infeasible. (This violates the basic assumption in the chapter that problem ( 10.1 ) is solvable, as well as the assumptions made in $\S10.2.4$ ; the example is meant only to show what happens to the infeasible start Newton method when $\mathbf{dom}\ f$ does not intersect $\{z\mid A z=b\}$ .) The norm of the residual for this example is shown in figure 10.3 , and the step length in figure 10.4 . Here, of course, the step lengths are never one, and the residual does not converge to zero. 

# A convex-concave game 

Our final example involves a convex-concave game on $\mathbf{R}^{100}\times\mathbf{R}^{100}$ , with payoﬀ function 

$$
f(\boldsymbol{u},\boldsymbol{v})=\boldsymbol{u}^{T}\boldsymbol{A}\boldsymbol{v}+\boldsymbol{b}^{T}\boldsymbol{u}+\boldsymbol{c}^{T}\boldsymbol{v}-\log(1-\boldsymbol{u}^{T}\boldsymbol{u})+\log(1-\boldsymbol{v}^{T}\boldsymbol{v}),
$$ 

defined on 

$$
\mathrm{dom}\ f=\{(u,v)\ |\ u^{T}u<1,\ v^{T}v<1\}.
$$ 

The problem data $A,\ b$ , and $c$ were randomly generated. The progress of the (infeasible start) Newton method, started at $u^{(0)}\,=\,v^{(0)}\,=\,0$ , with backtracking parameters $\alpha=0.01$ and $\beta=0.5$ , is shown in figure 10.5 . 

# 10.4 Implementation 

# 10.4.1 Elimination 

To implement the elimination method, we have to calculate a full rank matrix $F$ and an x such that 

$$
\{x\mid A x=b\}=\{F z+{\hat{x}}\mid z\in\mathbf{R}^{n-p}\}.
$$ 

Several methods for this are described in $\S$ C.5 . 

# 10.4.2 Solving KKT systems 

In this section we describe methods that can be used to compute the Newton step or infeasible Newton step, both of which involve solving a set of linear equations 

![](images/30500b0d8ee667c9fd4b0af7002f0cc0c66f1cace5029d148461c5e4d7f42c79.jpg) 
Figure 10.1 Progress of infeasible start Newton method on an equality con- strained analytic centering problem with 100 variables and 50 constraints. The figure shows $||r_{\mathrm{phi}}||_{2}$ (solid line), and $||r_{\mathrm{dual}}||_{2}$ (dashed line). Note that feasibility is achieved (and maintained) after 8 iterations, and convergence is quadratic, starting from iteration 9 or so. 

![](images/8ea75fa66be210f94c80235327a3a2a0fdfcda6b7f94d65b828c4559bb35338a.jpg) 
Figure 10.2 Step length versus iteration number for the same example prob- lem. A full step is taken in iteration 8, which results in feasibility from iteration 8 on. 

![](images/d0848c809889b85c8129ed25544dba7c4608abdb68f07179352f374fe3564851.jpg) 
Figure 10.3 Progress of infeasible start Newton method on an equality con- strained analytic centering problem with 100 variables and 50 constraints, for which d $\mathbf{\sigma}\mathbf{om}\,f=\mathbf{R}_{++}^{100}$ does not intersect $\{z\mid A z=b\}$ . The figure shows $||r_{\mathrm{phi}}||_{2}$ (solid line), and $||r_{\mathrm{dual}}||_{2}$ (dashed line). In this case, the residuals do not converge to zero. 

![](images/71f163e7745bb9a22540f15ee6e0f68804bd4b5b7ef0c6603f75bd8fbfc26c7c.jpg) 
Figure 10.4 Step length versus iteration number for the infeasible example problem. No full steps are taken, and the step lengths converge to zero. 

![](images/a8a9f6401f2f83ee3f1b320615873b8a2bfacd0f1d934973bfedb9e890f4e152.jpg) 
Figure 10.5 Progress of (infeasible start) Newton method on a convex- concave game. Quadratic convergence becomes apparent after about 5 iter- ations. 

with KKT form 

$$
\left[\begin{array}{c c}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{v}\\ {w}\end{array}\right]=-\left[\begin{array}{c}{g}\\ {h}\end{array}\right].
$$ 

Here we assume $H\in\mathbf{S}_{+}^{n}$ , and $A\in\mathbf{R}^{p\times n}$ with $\mathbf{rank}\,A=p<n$ . Similar methods can be used to compute the Newton step for a convex-concave game, in which the bottom right entry of the coefficient matrix is negative semidefinite (see exer- cise 10.13 ). 

# Solving full KKT system 

One straightforward approach is to simply solve the KKT system ( 10.33 ), which is a set of $n+p$ linear equations in $n+p$ variables. The KKT matrix is symmetric, but not positive definite, so a good way to do this is to use an LDL T factorization (see § C.3.3 ). If no structure of the matrix is exploited, the cost is $(1/3)(n+p)^{3}$ ﬂops. This can be a reasonable approach when the problem is small ( i.e. , $n$ and $p$ are not too large), or when $A$ and $H$ are sparse. 

# Solving KKT system via elimination 

A method that is often better than directly solving the full KKT system is based on eliminating the variable $v$ (see § C.4 ). We start by describing the simplest case, in which $H\succ0$ . Starting from the first of the KKT equations 

$$
H v+A^{T}w=-g,\qquad A v=-h,
$$ 

we solve for $v$ to obtain 

$$
v=-H^{-1}(g+A^{T}w).
$$ 

Substituting this into the second KKT equation yields $A H^{-1}(g+A^{T}w)=h$ , so we have 

$$
w=(A H^{-1}A^{T})^{-1}(h-A H^{-1}g).
$$ 

These formulas give us a method for computing $v$ and $w$ . 

The matrix appearing in the formula for $w$ is the Schur complement $S$ of $H$ in the KKT matrix: 

$$
{\cal S}=-A H^{-1}A^{T}.
$$ 

Because of the special structure of the KKT matrix, and our assumption that $A$ has rank , the matrix $S$ is negative definite. $p$ 

Algorithm 10.3 Solving KKT system by block elimination. 

given KKT system with $H\succ0$ . 

1. Form $H^{-1}A^{T}$ and $H^{-1}g$ . 2. Form Schur complement $S=-A H^{-1}A^{T}$ . 3. Determine $w$ by solving $S w=A H^{-1}g-h$ . 4. Determine $v$ by solving $H v=-A^{T}w-g$ . 

Step 1 can be done by a Cholesky factorization of $H$ , followed by $p+1$ solves, which costs $f+(p+1)s$ , where $f$ is the cost of factoring $H$ and $s$ is the cost of an associated solve. Step 2 requires a $p\times n$ by $n\times p$ matrix multiplication. If we exploit no structure in this calculation, the cost is $p^{2}n$ ﬂops. (Since the result is symmetric, we only need to compute the upper triangular part of $S$ .) In some cases special structure in $A$ and $H$ can be exploited to carry out step 2 more efficiently. Step 3 can be carried out by holesky factorization of $-S$ , which costs $(1/3)p^{3}$ ﬂops if no further structure of S is exploited. Step 4 can be carried out using the factorization of $H$ already calculated in step 1, so the cost is $2n p+s$ ﬂops. The total ﬂop count, assuming that no structure is exploited in forming or factoring the Schur complement, is 

$$
f+p s+p^{2}n+(1/3)p^{3}
$$ 

ﬂops (keeping only dominant terms). If we exploit structure in forming or factoring $S$ , the last two terms are even smaller. 

If $H$ can be factored efficiently, then block elimination gives us a ﬂop count advantage over directly solving the KKT system using an LDL $^\mathrm{T}$ factorization. For example, if $H$ is diagonal (which corresponds to a separable objective function), we have $f=0$ and $s=n$ , so the total cost is $p^{2}n+(1/3)p^{3}$ ﬂops, which grows only linearly with $n$ . If $H$ is banded with bandwidth $k\ll n$ , then $f=n k^{2}$ , $s=4n k$ , so the total cost is around $n k^{2}+4n k p+p^{2}n+(1/3)p^{3}$ which still grows only linearly with $n$ . Other structures of $H$ that can be exploited are block diagonal (which corresponds to block separable objective function), sparse, or diagonal plus low rank; see appendix C and § 9.7 for more details and examples. 

Example 10.3 Equality constrained analytic center. We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad-\sum_{i=1}^{n}\log x_{i}}\\ &{{\mathrm{subject~to}}\quad A x=b.}\end{array}}
$$ 

Here the objective is separable, so the Hessian at $x$ is diagonal: 

$$
H=\mathbf{diag}(x_{1}^{-2},.\,.\,.\,,x_{n}^{-2}).
$$ 

If we compute the Newton direction using a generic method such as an LDL $^\mathrm{T}$ factor- ization of the KKT matrix, the cost is $(1/3)(n+p)^{3}$ ﬂops. 

If we compute the Newton step using block elimination, the cost is $n p^{2}+(1/3)p^{3}$ ﬂops. This is much smaller than the cost of the generic method. 

In fact this cost is the same as that of computing the Newton step for the dual prob- lem, described in example 10.2 on page 525 . For the (unconstrained) dual problem, the Hessian is 

$$
H_{\mathrm{dual}}=-A D A^{T},
$$ 

where $D$ is diagonal, with $D_{i i}=(A^{T}\nu)_{i}^{-2}$ . Forming this matrix costs $n p^{2}$ ﬂops, and solving for the Newton step by a Cholesky factorization of $-H_{\mathrm{dual}}$ costs $(1/3)p^{3}$ ﬂops. 

Example 10.4 Minimum length piecewise-linear curve subject to equality constraints. We consider a piecewise-linear curve in $\scriptstyle\mathbf{R}^{2}$ with knot points $(0,0)$ , $(1,x_{1}),\,.\cdot.$ , $(n,x_{n})$ . To find the minimum length curve that satisfies the equality constraints $A x=b$ , we form the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\,\,\left(1+x_{1}^{2}\right)^{1/2}+\sum_{i=1}^{n-1}\left(1+(x_{i+1}-x_{i})^{2}\right)^{1/2}}\\ {{\mathrm{subject~to}}}&{A x=b,}\end{array}}
$$ 

with variable $x\,\in\,\mathbf{R}^{\,n}$ , and $A\,\in\,\mathbf{R}^{p\times n}$ . In this probl , the objective is a sum of functions of pairs of adjacent variables, so the Hessian H is tridiagonal. Using block elimination, we can compute the Newton step in around $p^{2}n+(1/3)p^{3}$ ﬂops. 

# Elimination with singular $H$ 

The block elimination method described above obviously does not work when $H$ is singular, but a simple variation on the method can be used in this more general case. The more general method is based on the following result: The KKT matrix if and only if $H\,+\,A^{T}Q A\,\succ\,0$ for some $Q\succeq0$ , in which case, $H+A^{T}Q A\succ0$ ≻ 0 for all $Q\succ0$ . (See exer conclude, for example, that if the KKT matrix is nonsingular, then $H+A^{T}A\succ0$ 0. 

Let $Q\succeq0$ be a matrix for which $H+A^{T}Q A\succ0$ . Then the KKT system ( 10.33 ) is equivalent to 

$$
\left[\begin{array}{c c}{H+A^{T}Q A}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{v}\\ {w}\end{array}\right]=-\left[\begin{array}{c}{g+A^{T}Q h}\\ {h}\end{array}\right],
$$ 

which can be solved using elimination since $H+A^{T}Q A\succ0$ . 

# 10.4.3 Examples 

In this section we describe some longer examples, showing how structure can be exploited to efficiently compute the Newton step. We also include some numerical results. 

# Equality constrained analytic centering 

We consider the equality constrained analytic centering problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad f({\boldsymbol{x}})=-\sum_{i=1}^{n}\log x_{i}}\\ &{{\mathrm{subject~to}}\quad{\boldsymbol{A}}{\boldsymbol{x}}={\boldsymbol{b}}.}\end{array}}
$$ 

(See examples 10.2 and 10.3 .) We compare three methods, for a problem of size $p=100$ , $n=500$ . 

The first method is Newton’s method with equality constraints ( § 10.2 ). The Newton step $\Delta x_{\mathrm{{nt}}}$ is defined by the KKT system ( 10.11 ): 

$$
\left[\begin{array}{c c}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {w}\end{array}\right]=\left[\begin{array}{c}{-g}\\ {0}\end{array}\right],
$$ 

where $H\,=\,\mathbf{diag}(1/x_{1}^{2},.\,.\,.\,,1/x_{n}^{2})$ ), and $g\,=\,-(1/x_{1},.\,.\,.\,,1/x_{n})$ . As explained in example 10.3 , page 546 , the KKT system can be efficiently solved by elimination, i.e. , by solving 

$$
A H^{-1}A^{T}w=-A H^{-1}g,
$$ 

and setting $\Delta x_{\mathrm{nt}}=-H^{-1}(A^{T}w+g)$ . In other words, 

$$
\Delta x_{\mathrm{nt}}=-\,\mathbf{diag}(x)^{2}A^{T}w+x,
$$ 

where $w$ is the solution of 

$$
A\,\mathbf{diag}(x)^{2}A^{T}w=b.
$$ 

Figure 10.6 shows the error versus iteration. The diﬀerent curves correspond to four diﬀerent starting points. We use a backtracking line search with $\alpha\:=\:0.1$ , $\beta=0.5$ . 

The second method is Newton’s method applied to the dual 

$$
{\begin{array}{r l}{{\mathrm{maximize}}}&{{}g(\nu)=-b^{T}\nu+\sum_{i=1}^{n}\log(A^{T}\nu)_{i}+n}\end{array}}
$$ 

(see example 10.2 , page 525 ). Here the Newton step is obtained from solving 

$$
A\,\mathbf{diag}(y)^{2}A^{T}\Delta\nu_{\mathrm{nt}}=-b+A y
$$ 

where $y=(1/(A^{T}\nu)_{1},.\,.\,.\,,1/(A^{T}\nu)_{n})$ . Comparing ( 10.35 ) and ( 10.34 ) we see that both methods have the same complexity. In figure 10.7 we show the error for four diﬀerent starting points. We use a backtracking line search with $\alpha=0.1$ , $\beta=0.5$ . 

The third method is the infeasible start Newton method of $\S10.3$ , applied to the optimality conditions 

$$
\nabla f(x^{\star})+A^{T}\nu^{\star}=0,\qquad A x^{\star}=b.
$$ 

The Newton step is obtained by solving 

$$
\left[\begin{array}{c c}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {\Delta\nu_{\mathrm{nt}}}\end{array}\right]=-\left[\begin{array}{c}{g+A^{T}\nu}\\ {A x-b}\end{array}\right],
$$ 

![](images/5c3d6812fa6ead773f439390e4d95dfa14dd1c9cac6adbc63c593d36676cb978.jpg) 
Figure 10.6 Error $f(x^{(k)})-p^{\star}$ in Newton’s method, applie quality constrained analytic centering problem of size $\textsl{p}=\textsl{100}$ , n = 500. The diﬀerent curves correspond to four diﬀerent starting points. Final quadratic convergence is clearly evident. 

![](images/7ad045062302a3161b2fa1aed31c8fa126fc298c250c18ef88bc64c61b7973b8.jpg) 
Figure 10.7 Error $|g(\nu^{(k)})-p^{\star}|$ in Newton’s method, applied to the dual of the equality constrained analytic centering problem. 

![](images/2d249385cddcb0f8e2177c4505548a15ed73fdc1fb10fd240518575485a15d27.jpg) 
Figure 10.8 Residual $\|r(x^{(k)},\nu^{(k)})\|_{2}$ in the infeasible start Newton method, applied to the equality constrained analytic centering problem. 

where $H=\mathbf{diag}(1/x_{1}^{2},.\,.\,.\,,1/x_{n}^{2})$ ), and $g=-(1/x_{1},\dots,1/x_{n})$ . This KKT system can be efficiently solved by elimination, at the same cost as ( 10.34 ) or ( 10.35 ). For example, if we first solve 

$$
A\,\mathbf{diag}(x)^{2}A^{T}{\boldsymbol{w}}=2A x-b,
$$ 

then $\Delta\nu_{\mathrm{{nt}}}$ and $\Delta x_{\mathrm{{nt}}}$ follow from 

$$
\Delta\nu_{\mathrm{nt}}=w-\nu,\qquad\Delta x_{\mathrm{nt}}=x-\mathbf{diag}(x)^{2}A^{T}w.
$$ 

Figure 10.8 shows the norm of the residual 

$$
\boldsymbol{r}(\boldsymbol{x},\nu)=\left(\nabla f(\boldsymbol{x})+\boldsymbol{A}^{T}\nu,\boldsymbol{A}\boldsymbol{x}-\boldsymbol{b}\right)
$$ 

versus iteration, for four diﬀerent starting points. We use a backtracking line search with $\alpha=0.1$ , $\beta=0.5$ . 

The figures show that for this problem, the dual method appears to be faster, but only by a factor of two or three. It takes about six iterations to reach the region of quadratic convergence, as opposed to 12–15 in the primal method and 10–20 in the infeasible start Newton method. 

The methods also diﬀer in the initialization they require. The primal method requires knowledge of a primal feasible point, i.e. , sat $A x^{(0)}=b$ , $x^{(0)}\succ0$ . The dual method requires a dual feasible point, i.e. , A $A^{T}\nu^{(0)}\,\succ\,0$ ≻ 0. Depending on the problem, one or the other might be more readily available. The infeasible start Newton method requires no initialization; the only requirement is that $x^{(0)}\succ0$ . 

# Optimal network ﬂow 

We consider a connected directed graph or network with $n$ edges and $p+1$ nodes. We let $x_{j}$ denote the ﬂow or traffic on arc $j$ , with $x_{j}\,>\,0$ meaning ﬂow in the direction of the arc, and $x_{j}\,<\,0$ meaning ﬂow in the direction opposite the arc. There is also a given external source (or sink) ﬂow $s_{i}$ that enters (if $s_{i}\,>\,0$ ) or leaves (if $s_{i}\,<\,0$ ) node $i$ . The ﬂow must satisfy a conservation equation, which states that at each node, the total ﬂow entering the node, including the external sources and sinks, is zero. This conservation equation can be expressed as $\tilde{A}x=s$ where $\tilde{A}\in\mathbf{R}^{(p+1)\times n}$ ∈ is the node incidence matrix of the graph, 

$$
\tilde{A}_{i j}=\left\{\begin{array}{c l}{{1}}&{{\mathrm{arc~}j\mathrm{~leaves~node~}i}}\\ {{-1}}&{{\mathrm{arc~}j\mathrm{~notin~as~node~}i}}\\ {{0}}&{{\mathrm{otherwise.}}}\end{array}\right.
$$ 

The ﬂow conservation equation $\dot{A}x\,=\,s$ is inconsistent unless $\mathbf{1}^{T}s=0$ , which we assume is the case. (In other words, the total of the source ﬂows must equal the total of the sink ﬂows.) The ﬂow conservation equations $\tilde{A}x=s$ are also redundant, since $\mathbf{1}^{T}\tilde{A}=0$ = 0. To obtain an independent set of equations we can delete any one equation, to obtain $A x=b$ , where $A\in\mathbf{R}^{p\times n}$ is the reduced node inciden rix p of the graph ( i.e. , the node incidence matrix with one row removed) and b $b\in\mathbf{R}^{p}$ ∈ is reduced source vector ( i.e. , $s$ with the associated entry removed). 

In summary, ﬂow conservation is given by $A x=b$ , where $A$ is the reduced node incidence matrix of the graph and $b$ is the reduced source vector. The matrix $A$ is very sparse, since each column has at most two nonzero entries (which can only be $+1$ or $^{-1}$ ). 

We will take traffic ﬂows $x$ as the variables, and the sources as given. We introduce the objective function 

$$
f(x)=\sum_{i=1}^{n}\phi_{i}(x_{i}),
$$ 

where $\phi_{i}:\mathbf{R}\rightarrow\mathbf{R}$ is the ﬂow cost function for arc $i$ . We assume that the ﬂow cost functions are strictly convex and twice diﬀerentiable. 

The problem of choosing the best ﬂow, that satisfies the ﬂow conservation re- quirement, is 

$$
\begin{array}{l l}{\mathrm{minimize}}&{\sum_{i=1}^{n}\phi_{i}(x_{i})}\\ {\mathrm{subject~to}}&{A x=b.}\end{array}
$$ 

Here the Hessian $H$ is diagonal, since the objective is separable. 

We have several choices for computing the Newton step for the optimal network ﬂow problem ( 10.36 ). The most straightforward is to solve the full KKT system, using a sparse LDL $^\mathrm{r}$ factorization. 

For this problem it is probably better to compute the Newton step using block elimination. We can characterize the sparsity pattern of the Schur complement $S=-A H^{-1}A^{T}$ in terms of the graph: We have $S_{i j}\neq0$ if and only if node $i$ and node j are connected by an arc. It follows that if the network is sparse, i.e. , if each node is connected by an arc to only a few other nodes, then the Schur complement $S$ is sparse. In this case, we can exploit sparsity in forming $S$ , and in the associated factorization and solve steps, as well. We can expect the computational complexity of computing the Newton step to grow approximately linearly with the number of arcs (which is the number of variables). 

# Optimal control 

We consider the problem 

$$
\begin{array}{l l}{\mathrm{minimize~}}&{\sum_{t=1}^{N}\phi_{t}(z(t))+\sum_{t=0}^{N-1}\psi_{t}(u(t))}\\ {\mathrm{subject~to}}&{z(t+1)=A_{t}z(t)+B_{t}u(t),\quad t=0,\ldots,N-1.}\end{array}
$$ 

Here 

• $z(t)\in\mathbf{R}^{k}$ is the system state at time $t$ • $u(t)\in\mathbf{R}^{l}$ is the input or control action at time $t$ • $\phi_{t}:\mathbf{R}^{k}\rightarrow\mathbf{R}$ is the state cost function • $\psi_{t}:\mathbf{R}^{l}\rightarrow\mathbf{R}$ is the input cost function $N$ is called the time horizon for the problem. 

We assume that the input and state cost functions are strictly convex and twice dif- ferentiable. The variables in the problem are $u(0),\cdot\cdot\cdot,u(N{-}1)$ , and $z(1),\ldots,z(N)$ . The initial state $z(0)$ is given. The linear equality constraints are called the state equations or dynamic evolution equations . We define the overall optimization vari- able $x$ as 

$$
x=(u(0),z(1),u(1),.\,.\,.\,,u(N-1),z(N))\in{\bf R}^{N(k+l)}.
$$ 

Since the objective is block separable ( i.e. , a sum of functions of $z(t)$ and $u(t)$ ), the Hessian is block diagonal: 

$$
H={\bf d i a g}(R_{0},Q_{1},.\,.\,,R_{N-1},Q_{N}),
$$ 

where 

$$
R_{t}=\nabla^{2}\psi_{t}(u(t)),\quad t=0,.\,,N-1,\qquad Q_{t}=\nabla^{2}\phi_{t}(z(t)),\quad t=1,.\,,N.
$$ 

We can collect all the equality constraints ( i.e. , the state equations) and express them as $A x=b$ where 

$$
\begin{array}{r l r}{A}&{=}&{\left[\begin{array}{c c c c c c c c}{-B_{0}}&{I}&{0}&{0}&{0}&{\cdots}&{0}&{0}&{0}\\ {0}&{-A_{1}}&{-B_{1}}&{I}&{0}&{\cdots}&{0}&{0}&{0}\\ {0}&{0}&{0}&{-A_{2}}&{-B_{2}}&{\cdots}&{0}&{0}&{0}\\ {\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}\\ {0}&{0}&{0}&{0}&{0}&{\cdots}&{I}&{0}&{0}\\ {0}&{0}&{0}&{0}&{0}&{\cdots}&{-A_{N-1}}&{-B_{N-1}}&{I}\end{array}\right]}\\ {b}&{=}&{\left[\begin{array}{c}{A_{0}(0)}\\ {0}\\ {0}\\ {\vdots}\\ {0}\\ {0}\end{array}\right].}\end{array}
$$ 

The number of rows of $A$ ( i.e. , equality constraints) is $N k$ . 

Directly solving the KKT system for the Newton step, using a dense LDL T factorization, would cost 

$$
(1/3)(2N k+N l)^{3}=(1/3)N^{3}(2k+l)^{3}
$$ 

ﬂops. Using a sparse LDL $^\mathrm{r}$ factorization would give a large improvement, since the method would exploit the many zero entries in $A$ and $H$ . 

In fact we can do better by exploiting the special block structure of $H$ and $A$ , using block elimination to compute the Newton step. The Schur complement $S=-A H^{-1}A^{T}$ turns out to be block tridiagonal, with $k\times k$ blocks: 

$$
\begin{array}{r c l}{S}&{=}&{-A H^{-1}A^{T}}\\ &{=}&{\left[\begin{array}{c c c c c c}{S_{11}}&{Q_{1}^{-1}A_{1}^{T}}&{0}&{\cdots}&{0}&{0}\\ {A_{1}Q_{1}^{-1}}&{S_{22}}&{Q_{2}^{-1}A_{2}^{T}}&{\cdots}&{0}&{0}\\ {0}&{A_{2}Q_{2}^{-1}}&{S_{33}}&{\cdots}&{0}&{0}\\ {\vdots}&{\vdots}&{\vdots}&{\ddots}&{\vdots}&{\vdots}\\ {0}&{0}&{0}&{\cdots}&{S_{N-1,N-1}}&{Q_{N-1}^{-1}A_{N-1}^{T}}\\ {0}&{0}&{0}&{\cdots}&{A_{N-1}Q_{N-1}^{-1}}&{S_{N N}}\end{array}\right]}\end{array}
$$ 

where 

$$
\begin{array}{r c l}{{S_{11}}}&{{=}}&{{-B_{0}R_{0}^{-1}B_{0}^{T}-Q_{1}^{-1},}}\\ {{S_{i i}}}&{{=}}&{{-A_{i-1}Q_{i-1}^{-1}A_{i-1}^{T}-B_{i-1}R_{i-1}^{-1}B_{i-1}^{T}-Q_{i}^{-1},\quad i=2,\ldots,N.}}\end{array}
$$ 

n particular, $S$ is banded, with bandwidth $2k-1$ , so we c actor it in order $k^{3}N$ ﬂops. Therefore we can compute the Newton step in order k $k^{3}N$ ﬂops, assuming $k\ll N$ . Note that this grows linea with the time horizon $N$ , whereas for a generic method, the ﬂop count grows like N $N^{3}$ . 

For this problem we could go one step further and exploit the block tridiagonal structure of $S$ . Applying a standard block tridiagonal factorization method would result in the classic Riccati recursion for solving a quadratic optimal control prob- lem. Still, using only the banded nature of $S$ yields an algorithm that is the same order. 

# Analytic center of a linear matrix inequality 

We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ f(X)=-\log\operatorname*{det}X}\\ &{{\mathrm{subject~to}}\quad\mathbf{tr}(A_{i}X)=b_{i},\quad i=1,.\,.\,,p,}\end{array}}
$$ 

where $X\,\in\,{\bf S}^{n}$ is the variable, $A_{i}\,\in\,\mathbf{S}^{n}$ , $b_{i}\,\in\,\mathbf{R}$ , and $\mathbf{\dom}\,f\,=\,\mathbf{S}_{++}^{n}$ . The KKT conditions for this problem are 

$$
-X^{\star-1}+\sum_{i=1}^{m}\nu_{i}^{\star}A_{i}=0,\qquad\mathrm{tr}(A_{i}X^{\star})=b_{i},\quad i=1,\ldots,p.
$$ 

The dimension of the variable $X$ is $n(n+1)/2$ . We could simply ignore the special matrix structure of $X$ , and consider it as (vector) variable $x\in\mathbf{R}^{n(n+1)/2}$ , and solve the problem ( 10.37 ) using a generic method for a problem with $n(n\!+\!1)/2$ variables and $p$ equality constraints. The cost for computing a Newton step would then be at least 

$$
(1/3)(n(n+1)/2+p)^{3}
$$ 

ﬂops, which is order $n^{6}$ in $n$ . We will see that there are a number of far more attractive alternatives. 

A first option is to solve the dual problem. The conjugate of $f$ is 

$$
f^{*}(Y)=\log\operatorname*{det}(-Y)^{-1}-n
$$ 

with $\mathbf{dom}\,f^{*}=-\mathbf{S}_{++}^{n}$ (see example 3.23 , page 92 ), so the dual problem is 

$$
\begin{array}{r}{\begin{array}{r l}{\mathrm{maximize}}&{{}-b^{T}\nu+\log\operatorname*{det}(\sum_{i=1}^{p}\nu_{i}A_{i})+n,}\end{array}}\end{array}
$$ 

omain $\begin{array}{r}{\left\{\nu\mid\sum_{i=1}^{p}\nu_{i}A_{i}\succ0\right\}}\end{array}$ ≻ } . This is an unconstrained oblem with variable

 $\nu\in\mathbf{R}^{p}$ ∈ . The optimal X can be recovered from the optimal ν $\nu^{\star}$ by solving the first

 (dual feasibility) equation in $(10.38)$ , i.e. , $\begin{array}{r}{X^{\star}=\left(\sum_{i=1}^{p}\nu_{i}^{\star}A_{i}\right)^{-1}}\end{array}$ . 

Let us work out the cost of computing the Newton step for the dual prob- lem ( 10.39 ). We have to form the gradient and Hessian of $g$ , and then solve for the Newton step. The gradient and Hessian are given by 

$$
\begin{array}{r c l}{{\nabla^{2}g(\nu)_{i j}}}&{{=}}&{{-\,{\bf t r}(A^{-1}A_{i}A^{-1}A_{j}),\quad i,\ j=1,.\,.\,,p,}}\\ {{\nabla g(\nu)_{i}}}&{{=}}&{{{\bf t r}(A^{-1}A_{i})-b_{i},\quad i=1\,.\,.\,,p,}}\end{array}
$$ 

where $\begin{array}{r}{A\,=\,\sum_{i=1}^{p}\nu_{i}A_{i}}\end{array}$ . To form $\nabla^{2}g(\nu)$ and $\nabla g(\nu)$ we proceed as follows. We first form A ( $p n^{2}$ ﬂops), and $A^{-1}A_{j}$ for each $j$ ( $2p n^{3}$ ﬂops). Then we form the matrix ∇ $\nabla^{2}g(\nu)$ ). ch of the $p(p+1)/2$ entries of $\nabla^{2}g(\nu)$ is the inner product of n two matrices in S $\mathbf{S}^{n}$ , each of which costs $n(n+1)$ ﬂops, so the total is (dropping dominated t $(1/2)p^{2}n^{2}$ ﬂops. Forming $\nabla g(\nu)$ is cheap since we already have the matrices $A^{-1}A_{i}$ i . Finally, we solve for the Newton step $-\nabla^{2}g(\nu)^{-1}\nabla g(\nu)$ , which costs $(1/3)p^{3}$ ﬂops. All together, and keeping only the leading terms, the total cost of computing the Newton step is $2p n^{3}+(1/2)p^{2}n^{2}+(1/3)p^{3}$ . Note that this is order $n^{3}$ in $n$ , which is far better than the simple primal method described above, which is order $n^{6}$ . 

We can also solve the primal problem more efficiently, by exploiting its special matrix structure. To derive the KKT system for the Newton step $\Delta X_{\mathrm{{nt}}}$ at a feasible $X$ , we replace $X^{\star}$ in the KKT conditions by $X+\Delta X_{\mathrm{nt}}$ and $\nu^{\star}$ by $w$ , and linearize the first equation using the first-order approximation 

$$
(X+\Delta X_{\mathrm{nt}})^{-1}\approx X^{-1}-X^{-1}\Delta X_{\mathrm{nt}}X^{-1}.
$$ 

This gives the KKT system 

$$
-X^{-1}+X^{-1}\Delta X_{\mathrm{nt}}X^{-1}+\sum_{i=1}^{p}w_{i}A_{i}=0,\qquad\mathrm{tr}(A_{i}\Delta X_{\mathrm{nt}})=0,\quad i=1,\ldots,p.
$$ 

a set of $n(n+1)/2+p$ linear equations in the variables $\Delta X_{\mathrm{nt}}\,\in\,{\bf S}^{n}$ and $w\,\in\mathbf{R}^{p}$ ∈ . If we solved these equations using a generic method, the cost would be order n $n^{6}$ . 

We can use block elimination to solve the KKT system ( 10.40 ) far more effi- ciently. We eliminate the variable $\Delta X_{\mathrm{{nt}}}$ , by solving the first equation to get 

$$
\Delta X_{\mathrm{nt}}=X-X\left(\sum_{i=1}^{p}w_{i}A_{i}\right)X=X-\sum_{i=1}^{p}w_{i}X A_{i}X.
$$ 

Substituting this expression for $\Delta X_{\mathrm{{nt}}}$ into the other equation gives 

$$
\mathrm{\bftr}(A_{j}\Delta X_{\mathrm{nt}})=\mathrm{\bftr}(A_{j}X)-\sum_{i=1}^{p}w_{i}\,\mathrm{\bftr}(A_{j}X A_{i}X)=0,\quad j=1,\dots,p.
$$ 

This is a set of $p$ linear equations in $w$ : 

$$
C w=d
$$ 

where $C_{i j}=\mathbf{tr}(A_{i}X A_{j}X)$ , $d_{i}=\mathbf{tr}(A_{i}X)$ . The coefficient matrix $C$ is symmetric and positive definite, so a Cholesky factorization can be used to find $w$ . Once we have $w$ , we can compute $\Delta X_{\mathrm{{nt}}}$ from ( 10.41 ). 

The cost of this method is as follows. We form the products $A_{i}X$ ( $2p n^{3}$ ﬂops), and then form the matrix $C$ . Each of the $p(p+1)/2$ entries of $C$ is the inner product of two matrices in $\mathbf{R}^{n\times n}$ , so forming $C$ costs $p^{2}n^{2}$ ﬂops. Then we solve for $w\,=\,C^{-1}d$ , which costs $(1/3)p^{3}$ . Finally we compute $\Delta X_{\mathrm{{nt}}}$ . If we use the first expression in ( 10.41 ), i.e. , first compute the sum and then pre- and post- multiply with $X$ , the cost is approximately $p n^{2}+3n^{3}$ . All together, the total cost is $2p n^{3}+p^{2}n^{2}+(1/3)p^{3}$ ﬂops to form the Newton step for the primal problem, using block elimination. This is far better than the simple method, which is order $n^{6}$ . Note also that the cost is the same as that of computing the Newton step for the dual problem. 

# Bibliography 

The two key assumptions in our analysis of the infeasible start Newton method (the derivative $D r$ has a bounded inverse and satisfies a Lipschitz condition) are central to most convergence proofs of Newton’s method; see Ortega and Rheinboldt [ OR00 ] and Dennis and Schnabel [ DS96 ]. 

The relative merits of solving KKT systems via direct factorization of the full system, or via elimination, have been extensively studied in the context of interior-point methods for linear and quadratic programming; see, for example, Wright [ Wri97 , chapter 11] and Nocedal and Wright [ NW99 , § 16.1-2]. The Riccati recursion from optimal control can be interpreted as a method for exploiting the block tridiagonal structure in the Schur complement $S$ of the example on page 552 . This observation was made by Rao, Wright, and Rawlings [ RWR98 , 3.3]. 

# Exercises 

Equality constrained minimization 

10.1 Nonsingularity of the KKT matrix. Consider the KKT matrix 

$$
\left[\begin{array}{c c}{{\boldsymbol{P}}}&{{\boldsymbol{A}^{T}}}\\ {{\boldsymbol{A}}}&{{\boldsymbol{0}}}\end{array}\right],
$$ 

where $P\in\mathbf{S}_{+}^{n}$ , $A\in\mathbf{R}^{p\times n}$ , and rank $A=p<n$ . (a) Show that each of the following statements is equivalent to nonsingularity of the 

KKT matrix. • $\mathcal{N}(P)\cap\mathcal{N}(A)=\{0\}$ . • $A x=0$ , $x\neq0\Longrightarrow x^{T}P x>0$ ⇒ 0. • $F^{T}P F\succ0$ , where $F\in\mathbf{R}^{n\times(n-p)}$ is a matrix for which $\mathcal{R}(F)=\mathcal{N}(A)$ . • $P+A^{T}Q A\succ0$ for some $Q\succeq0$ . (b) Show that if the KKT matrix is nonsingular, then it has exactly $n$ positive and $p$ negative eigenvalues. 

10.2 Projected gradient method. In this problem we explore an extension of the gradient method to equality constrained minimization problems. Suppose $f$ is convex and diﬀerentiable, and $x\in\mathbf{dom}\ f$ satisfies $A x=b$ , w $A\in\mathbf{R}^{p\times n}$ ith ran $\mathbf{k}\,A=p<n$ . The Euclidean projection of the negative gradient −∇ $-\nabla f(x)$ ) on N ${\mathcal{N}}(A)$ ) is given by 

$$
\Delta x_{\mathrm{pg}}=\operatorname*{argmin}_{A u=0}\|\!-\!\nabla f(x)-u\|_{2}.
$$ 

(a) Let $(v,w)$ be the unique solution of 

$$
\left[\begin{array}{c c}{\boldsymbol{I}}&{\boldsymbol{A}^{T}}\\ {\boldsymbol{A}}&{\boldsymbol{0}}\end{array}\right]\left[\begin{array}{c}{\boldsymbol{v}}\\ {\boldsymbol{w}}\end{array}\right]=\left[\begin{array}{c}{-\boldsymbol{\nabla}f(\boldsymbol{x})}\\ {\boldsymbol{0}}\end{array}\right].
$$ 

Show that $v=\Delta x_{\mathrm{pg}}$ and $\boldsymbol{w}=\operatorname{argmin}_{\boldsymbol{y}}\|\nabla f(\boldsymbol{x})+A^{T}\boldsymbol{y}\|_{2}$ . 

(b) What is the relation between the projected negative gradient $\Delta x_{\mathrm{pg}}$ and the negative ${\boldsymbol{F}}^{T}{\boldsymbol{F}}=I!$ gradient of the reduced problem ( 10.5 ), assuming ? 

(c) The projected gradient method for solving an equality constrained minimization problem uses the step $\Delta x_{\mathrm{pg}}$ , and a backtracking line search on $f$ . Use the re- sults of part (b) to give some conditions under which the projected gradient method to the optimal solution, when started from a point $x^{(0)}\,\in\,\mathbf{dom}\,f$ with $A x^{(0)}=b$ . 

# Newton’s method with equality constraints 

10.3 Dual Newton method. In this problem we explore Newton’s method for solving the dual of the equality constrained minimization problem ( 10.1 ). We assume that $f$ is twice diﬀerentiable, $\nabla^{2}f(x)\succ0$ for all $x\in\mathbf{dom}\,f$ , and that for each $\nu\in\mathbf{R}^{p}$ , the Lagrangian $L(x,\nu)=f(x)+\nu^{T}(A x-b)$ has a unique minimizer, which we denote $x(\nu)$ . 

(a) Show that the dual function $g$ is twice diﬀerentiable. Find an expression for the Newton step dual function $g$ , evaluated at $\nu$ , in terms of $f$ , $\nabla f$ , and $\nabla^{2}f$ , evaluated at $x=x(\nu)$ ). You can use the results of exercise 3.40 . (b) Suppose there exists a $K$ such that 

$$
\left\|\left[\begin{array}{c c}{{\nabla^{2}f(x)}}&{{A^{T}}}\\ {{A}}&{{0}}\end{array}\right]^{-1}\right\|_{2}\leq K
$$ 

for all $x\in\mathbf{dom}\ f$ . Show that $g$ is strongly concave, with $\nabla^{2}g(\nu)\preceq-(1/K)I

$ 

10.4 Strong convexity and Lipschitz constant of the reduced problem. Suppose $f$ satisfies the assumptions given on page 529 . Show that the reduced objective function $\dot{f}(z)=f(F z\!+\!\hat{x})$ is strongly convex, and that its Hessian is Lipschitz continuous (on the associated sublevel set $\tilde{S}$ ). Express the strong convexity and Lipschitz constants of $\ddot{f}$ in terms of $K$ , $M$ , $L$ , and the maximum and minimum singular values of $F^{\prime}$ . 

10.5 Adding a quadratic term to the objective. Suppose $Q\succeq0$ . The problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{f(x)+(A x-b)^{T}Q(A x-b)}\\ {{\mathrm{subject~to}}\quad}&{A x=b}\end{array}}
$$ 

is equivalent to the original equality constrained optimization problem ( 10.1 ). Is the Newton step for this problem the same as the Newton step for the original problem? 

10.6 The Newton decrement. Show that ( 10.13 ) holds, i.e. , 

$$
f(x)-\operatorname*{inf}\{\widehat{f}(x+v)\mid A(x+v)=b\}=\lambda(x)^{2}/2.
$$ 

# Infeasible start Newton method 

10.7 Assumptions for infeasible start Newton method. Consider the set of assumptions given on page 536 . 

(a) Suppose that the function $f$ is closed. Show that this implies that the norm of the residual, $||r(x,\nu)||_{2}$ , is closed. (b) Show that $D r$ satisfies a Lipschitz condition if and only if $\nabla^{2}f$ does. 

10.8 Infeasible start Newton method and initially satisfied equality constraints. Suppose we use the infeasible start Newton method to minimize $f(x)$ subject to $a_{i}^{T}x=b_{i}$ , $i=1,\dots,p$ . 

(a) Suppose the initial point $x^{(0)}$ satisfies the linear equality $a_{i}^{T}x=b_{i}$ . Show that the linear equality will remain satisfied for future iterates, i.e. , if $a_{i}^{T}{\boldsymbol{x}}^{(k)}=b_{i}$ for all $k$ . (b) Suppose that one of the equality constraints becomes satisfied at iteration $k$ , i.e. , we have $a_{i}^{T}{\boldsymbol{x}}^{(k-1)}\,\neq\,b_{i}$ ̸ , $a_{i}^{T}{\boldsymbol{x}}^{(k)}\,=\,b_{i}$ . Show that at iteration $k$ , all the equality constraints are satisfied. 

10.9 Equality constrained entropy maximization. Consider the equality constrained entropy maximization problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{f(x)=\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{A x=b,}\end{array}}
$$ 

with dom $f=\mathbf{R}_{++}^{n}$ and $A\in\mathbf{R}^{p\times n}$ . We assume the problem is feasible and that rank $A=$ $p<n$ . 

(a) Show that the problem has a unique optimal solution $x^{\star}$ (b) Find $A$ , $b$ , and feasible $x^{(0)}$ for which the sublevel set 

$$
\{x\in\mathbf{R}_{++}^{n}\mid A x=b,\ f(x)\leq f(x^{(0)})\}
$$ 

is not closed. Thus, the assumptions listed in § 10.2.4 , page 529 , are not satisfied for some feasible initial points. 

(c) Show that the problem ( 10.42 ) satisfies the assumptions for the infeasible start Newton method listed in § 10.3.3 , page 536 , for any feasible starting point. (d) Derive the Lagrange dual of ( 10.42 ), and explain how to find the optimal solution of ( 10.42 ) from the optimal solution of the dual problem. Show that the dual problem satisfies the assumptions listed in § 10.2.4 , page 529 , for any starting point. 

The results of part (b), (c), and (d) do not mean the standard Newton method will fail, or that the infeasible start Newton method or dual method will work better in practice. It only means our convergence analysis for the standard Newton method does not apply, while our convergence analysis does apply to the infeasible start and dual methods. (See exercise 10.15 .) 

10.10 Bounded inverse derivative condition for strongly convex-concave game. Consider a convex- concave game with payoﬀfunction $f$ (see page 541 ). Suppose $\nabla_{u u}^{2}f(u,v)\,\succeq\,m I$ ⪰ and $\nabla_{v v}^{2}f(u,v)\preceq-m I$ ⪯− , for all $(u,v)\in\mathbf{dom}\,f$ . Show that 

$$
\Vert D r(u,v)^{-1}\Vert_{2}=\Vert\nabla^{2}f(u,v)^{-1}\Vert_{2}\leq1/m.
$$ 

# Implementation 

10.11 Consider the resource allocation problem described in example 10.1 . You can assume the $f_{i}$ are strongly convex, i.e. , $f_{i}^{\prime\prime}(z)\geq m>0$ ≥ 0 for all $z$ . 

(a) Find the computational eﬀort required to compute a Newton step for the reduced problem. Be sure to exploit the special structure of the Newton equations. (b) Explain how to solve the problem via the dual. You can assume that the conjugate functions $f_{i}^{*}$ , and their derivatives, are readily computable, and that the equation $f_{i}^{\prime}(x)=\nu$ is readily solved for $x$ , given $\nu$ . What is the computational complexity of finding a Newton step for the dual problem? (c) What is the computational complexity of computing a Newton step for the resource allocation problem? Be sure to exploit the special structure of the KKT equations. 

10.12 Describe an efficient way to compute the Newton step for the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\,\mathbf{tr}(X^{-1})}\\ &{{\mathrm{subject~to}}\quad\mathbf{tr}(A_{i}X)=b_{i},\quad i=1,.\,.\,,p}\end{array}}
$$ 

with domain $\mathbf{S}_{++}^{n}$ , assuming $p$ and $n$ have the same order of magnitude. Also derive the Lagrange dual problem and give the complexity of finding the Newton step for the dual problem. 

10.13 Elimination method for computing Newton step for convex-concave game. Consider a convex-concave game with payoﬀfunction $f:\mathbf{R}^{\nu}\times\mathbf{R}^{q}\rightarrow\mathbf{R}$ (see page 5 e assume $f$ con r all $(u,v)\in\mathbf{dom}\,f$ and some m > 0, we have $\nabla_{u u}^{2}f(u,v)\succeq m I$ ∇ ⪰ and ∇ $\nabla_{v v}^{2}f(u,v)\preceq-m I$ ⪯− . 

(a) Show how to compute the Newton step using Cholesky factorizations of $\nabla_{u u}^{2}f(u,v)$ and $-\nabla^{2}f_{v v}(u,v)$ . Compare the cos ethod with the cost of using an LDL T factorization of $\nabla f(u,v)$ , assuming ∇ $\nabla^{2}f(u,v)$ ) is dense. (b) you can exploit diagonal or block diagon in $\nabla_{u u}^{2}f(u,v)$ ) and/or $\nabla_{v v}^{2}f(u,v)$ ∇ ). How much do you save, if you assume ∇ $\nabla_{u v}^{2}f(u,v)$ ) is dense? 

# Numerical experiments 

10.14 Log-optimal investment. Consider the log-optimal investment problem described in exer- cise 4.60 , without the constraint $x\succeq0$ . Use Newton’s method to compute the solution, with the following problem data: there are $n\,=\,3$ assets, and $m\:=\:4$ scenarios, with returns 

$$
p_{1}=\left[\begin{array}{c}{2}\\ {1.3}\\ {1}\end{array}\right],\qquad p_{2}=\left[\begin{array}{c}{2}\\ {0.5}\\ {1}\end{array}\right],\qquad p_{3}=\left[\begin{array}{c}{0.5}\\ {1.3}\\ {1}\end{array}\right],\qquad p_{4}=\left[\begin{array}{c}{0.5}\\ {0.5}\\ {1}\end{array}\right].
$$ 

The probabilities of the four scenarios are given by $\pi=(1/3,1/6,1/3,1/6)$ . 

10.15 Equality constrained entropy maximization. Consider the equality constrained entropy maximization problem 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{f(x)=\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{A x=b,}\end{array}}
$$ 

with $\mathbf{\Gamma}\mathbf{dom}\,f\,=\,\mathbf{R}_{++}^{n}$ and $A\,\in\,\mathbf{R}^{p\times n}$ , with $p<\,n$ . (See exercise 10.9 for some relevant analysis.) 

Generate a problem instance with $n=100$ and $p=30$ by choosing $A$ randomly (checking that it has full rank), choosing x as a random positive vector ( e.g. , with entries uniformly distributed on $[0,1]$ ) and then setting $\boldsymbol{b}=A\boldsymbol{\hat{x}}$ x . (Thus, x is feasible.) 

Compute the solution of the problem using the following methods. 

$x^{(0)}=\hat{x}$ 

$x^{(0)}={\hat{x}}$ (b) Infeasible start Newton method. You can use initial point (to compare with the standard Newton method), and also the initial point $x^{(0)}=\mathbf{1}$ . (c) Dual Newton method , i.e. , the standard Newton method applied to the dual problem. 

Verify that the three methods compute the same optimal point (and Lagrange multiplier). Compare the computational eﬀort per step for the three methods, assuming relevant structure is exploited. (Your implementation, however, does not need to exploit structure to compute the Newton step.) 

10.16 Convex-concave game. Use the infeasible start Newton method to solve convex-concave games of the form ( 10.32 ), with randomly generated data. Plot the norm of the residual and step length versus iteration. Experiment with the line search parameters and initial point (which must satisfy $||u||_{2}<1$ , $\|v\|_{2}<1$ , however). 

# Chapter 11 

# Interior-point methods 

# 11.1 Inequality constrained minimization problems 

In this chapter we discuss interior-point methods for solving convex optimization problems that include inequality constraints, 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{A x=b,}\end{array}}
$$ 

$f_{0},.\,.\,.\,,f_{m}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are convex and twice continuously diﬀerentiable, and $A\in\mathbf{R}^{p\times n}$ ∈ with rank $A=p<n$ . We assume that the problem is solvable, i.e. , an optimal x $x^{\star}$ exists. We denote the optimal value $f_{0}(x^{\star})$ as $p^{\star}$ . 

We a ume that the problem is strictly feasible, i.e. , there exists $x\in\mathcal{D}$ that satisfies Ax $A x=b$ b and $f_{i}(x)<0$ for $i=1,\ldots,m$ . This means that Slater’s constraint quali ation holds, so there exist dual optimal $\lambda^{\star}\in\mathbf{R}^{m}$ , $\nu^{\star}\in\mathbf{R}^{p}$ , which together with x $x^{\star}$ satisfy the KKT conditions 

$$
\begin{array}{r c l c l}{{A x^{\star}=b,}}&{{f_{i}(x^{\star})}}&{{\leq}}&{{0,}}&{{i=1,\ldots,m}}\\ {{\lambda^{\star}}}&{{\succeq}}&{{0}}&{{}}&{{}}\\ {{\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}\lambda_{i}^{\star}\nabla f_{i}(x^{\star})+A^{T}\nu^{\star}}}&{{=}}&{{0}}&{{}}\\ {{\lambda_{i}^{\star}f_{i}(x^{\star})}}&{{=}}&{{0,}}&{{i=1,\ldots,m.}}\end{array}
$$ 

Interior-point methods solve the problem ( 11.1 ) (or the KKT conditions ( 11.2 )) by applying Newton’s method to a sequence of equality constrained problems, or to a sequence of modified versions of the KKT conditions. We will concentrate on a particular interior-point algorithm, the barrier method , for which we give a proof of convergence and a complexity analysis. We also describe a simple primal-dual interior-point method (in § 11.7 ), but do not give an analysis. 

We can view interior-point methods as another level in the hierarchy of convex optimization algorithms. Linear equality constrained quadratic problems are the simplest. For these problems the KKT conditions are a set of linear equations, which can be solved analytically. Newton’s method is the next level in the hierarchy. We can think of Newton’s method as a technique for solving a linear equality constrained optimization problem, with twice diﬀerentiable objective, by reducing it to a sequence of linear equality constrained quadratic problems. Interior-point methods form the next level in the hierarchy: They solve an optimization problem with linear equality and inequality constraints by reducing it to a sequence of linear equality constrained problems. 

# Examples 

Many problems are already in the form ( 11.1 ), and satisfy the assumption that the objective and constraint functions are twice diﬀerentiable. Obvious examples are LPs, QPs, QCQPs, and GPs in convex form; another example is linear inequality constrained entropy maximization, 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{F x\preceq g}\\ &{A x=b,}\end{array}}
$$ 

with domain $\mathcal{D}=\mathbf{R}_{++}^{n}$ 

Many other problems do not have the required form ( 11.1 ), with twice diﬀeren- tiable objective and constraint functions, but can be reformulated in the required form. We have already seen many examples of this, such as the transformation of an unconstrained convex piecewise-linear minimization problem 

$$
{\begin{array}{r l}{\operatorname{minimize}}&{\operatorname*{max}_{i=1,\dots,m}(a_{i}^{T}x+b_{i})}\end{array}}
$$ 

(with non di e rent i able objective), to the LP 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{t}\\ {{\mathrm{subject~to}}}&{a_{i}^{T}x+b_{i}\leq t,\quad i=1,\ldots,m}\end{array}}
$$ 

(which has twice diﬀerentiable objective and constraint functions). 

Other convex optimization problems, such as SOCPs and SDPs, are not readily recast in the required form, but can be handled by extensions of interior-point methods to problems with generalized inequalities, which we describe in 11.6 . 

# 11.2 Logarithmic barrier function and central path 

Our goal is to approximately formulate the inequality constrained problem ( 11.1 ) as an equality constrained problem to which Newton’s method can be applied. Our first step is to rewrite the problem ( 11.1 ), making the inequality constraints implicit in the objective: 

$$
\begin{array}{l l}{\mathrm{minimize}}&{f_{0}(\boldsymbol{x})+\sum_{i=1}^{m}I_{-}(f_{i}(\boldsymbol{x}))}\\ {\mathrm{subject~to}}&{A\boldsymbol{x}=\boldsymbol{b},}\end{array}
$$ 

where $I_{-}:\mathbf{R}\rightarrow\mathbf{R}$ is the indicator function for the nonpositive reals, 

$$
I_{-}(u)=\left\{\begin{array}{l l}{0}&{u\leq0}\\ {\infty}&{u>0.}\end{array}\right.
$$ 

![](images/bbf0fa62518e40eccf6886f06e418a420234adb601d534467d54b56700783c37.jpg) 
Figure 11.1 The dashed lines show the function $I_{-}(u)$ , and the solid curves $\widehat{I}_{-}(u)=-(1/t)\log(-u)$ show − − ), for $t=0.5$ , $1$ , 2. The curve for $t=2$ gives the best approximation. 

The problem ( 11.3 ) has no inequality constraints, but its objective function is not (in general) diﬀerentiable, so Newton’s method cannot be applied. 

# 11.2.1 Logarithmic barrier 

The basic idea of the barrier method is to approximate the indicator function $I_{-}$ by the function 

$$
\widehat{I}_{-}(u)=-(1/t)\log(-u),\qquad\mathrm{\bf~dom}\,\widehat{I}_{-}=-{\bf R}_{++},
$$ 

where $t\ >\ 0$ is a parameter that sets the accuracy of the approximation. Like $I_{-}$ , the funct $\widehat{I}_{-}$ is convex and nondecreasing, and (by our convention) takes on the value ∞ for $u\,>\,0$ . Unlike $I_{-}$ , how ver, $\widehat{I}_{-}$ is diﬀerentiable and closed: it increases to ∞ as u increases to 0. Figure 11.1 shows the function $I_{-}$ , and the approximati $\widehat{I}_{-}$ b , for several values of t . As t increases, the approximation becomes more accurate. 

Substituting $\widehat{I}_{-}$ for $I_{-}$ in ( 11.3 ) gives the approximation 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)+\sum_{i=1}^{m}-(1/t)\log(-f_{i}(x))}\\ {{\mathrm{subject~to}}}&{A x=b.}\end{array}}
$$ 

The objective here is convex, since $-(1/t)\log(-u)$ is convex and increasing in $u$ , and diﬀerentiable. Assuming an appropriate closedness condition holds, Newton’s method can be used to solve it. 

The function 

$$
\phi(x)=-\sum_{i=1}^{m}\log(-f_{i}(x)),
$$ 

with $\mathbf{dom}\,\phi=\{x\in\mathbf{R}^{n}\mid f_{i}(x)<0,\ i=1,.\,.\,,m\}$ , is called the logarithmic barrier or log barrier for the problem ( 11.1 ). Its domain is the set of points that satisfy the inequality constraints of ( 11.1 ) strictly. No matter what value the positive parameter $t$ has, the logarithmic barrier grows without bound if $f_{i}(x)\,\to\,0$ , for any $i$ . 

Of course, the problem ( 11.4 ) is only an approximation of the original prob- lem ( 11.3 ), so one question that arises immediately is how well a solution of ( 11.4 ) approximates a solution of the original problem ( 11.3 ). Intuition suggests, and we will soon confirm, that the quality of the approximation improves as the parameter $t$ grows. 

On the other hand, when the parameter $t$ is large, the function $f_{0}+(1/t)\phi$ is difficult to minimize by Newton’s method, since its Hessian varies rapidly near the boundary of the feasible set. We will see that this problem can be circumvented by solving a sequence of problems of the form ( 11.4 ), increasing the parameter $t$ (and therefore the accuracy of the approximation) at each step, and starting each Newton minimization at the solution of the problem for the previous value of $t$ . 

For future reference, we note that the gradient and Hessian of the logarithmic barrier function $\phi$ are given by 

$$
\begin{array}{r c l}{{\nabla\phi(x)}}&{{=}}&{{\displaystyle\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla f_{i}(x),}}\\ {{\nabla^{2}\phi(x)}}&{{=}}&{{\displaystyle\sum_{i=1}^{m}\frac{1}{f_{i}(x)^{2}}\nabla f_{i}(x)\nabla f_{i}(x)^{T}+\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla^{2}f_{i}(x)}}\end{array}
$$ 

(see § A.4.2 and § A.4.4 ). 

# 11.2.2 Central path 

We now consider in more detail the minimization problem ( 11.4 ). It will simplify notation later on if we multiply the objective by $t$ , and consider the equivalent problem 

$$
\begin{array}{l c l}{{\mathrm{minimize}}}&{{t f_{0}(x)+\phi(x)}}\\ {{\mathrm{subject~to}}}&{{A x=b,}}\end{array}
$$ 

which has the same minimizers. We assume for now that the problem ( 11.6 ) can be solved via Newton’s method, and, in particular, that it has a unique solution for each $t>0$ . (We will discuss this assumption in more detail in 11.3.3 .) 

For $t>0$ we define $x^{\star}(t)$ as the solution of ( 11.6 ). The central path associated with problem ( 11.1 ) is defined as the set of points $x^{\star}(t)$ , $t\ >\ 0$ , which we call the central points . Points on the central path are characterized by the following necessary and sufficient conditions: $x^{\star}(t)$ is strictly feasible, i.e. , satisfies 

$$
A x^{\star}(t)=b,\qquad f_{i}(x^{\star}(t))<0,\quad i=1,\ldots,m,
$$ 

p and there exists a $\hat{\nu}\in\mathbf{R}^{p}$ such that 

$$
\begin{array}{r c l}{0}&{=}&{t\nabla f_{0}(x^{\star}(t))+\nabla\phi(x^{\star}(t))+A^{T}\hat{\nu}}\end{array}
$$ 

$$
\begin{array}{r l}{=}&{{}t\nabla f_{0}(x^{\star}(t))+\displaystyle\sum_{i=1}^{m}\frac{1}{-f_{i}(x^{\star}(t))}\nabla f_{i}(x^{\star}(t))+A^{T}\hat{\nu}}\end{array}
$$ 

holds. 

Example 11.1 Inequality form linear programming. The logarithmic barrier function for an LP in inequality form, 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b,}\end{array}}
$$ 

is given by 

$$
\phi(x)=-\sum_{i=1}^{m}\log(b_{i}-a_{i}^{T}x),\qquad\operatorname{dom}\phi=\{x\mid A x\prec b\},
$$ 

where $a_{1}^{T}$ , . . . , $a_{m}^{T}$ are the rows of $A$ . The gradient and Hessian of the barrier function are 

$$
\nabla\phi(x)=\sum_{i=1}^{m}\frac{1}{b_{i}-a_{i}^{T}x}a_{i},\qquad{\nabla}^{2}\phi(x)=\sum_{i=1}^{m}\frac{1}{(b_{i}-a_{i}^{T}x)^{2}}a_{i}a_{i}^{T},
$$ 

or, more compactly, 

$$
\nabla\phi(\boldsymbol{x})=\boldsymbol{A}^{T}\boldsymbol{d},\qquad\nabla^{2}\phi(\boldsymbol{x})=\boldsymbol{A}^{T}\,\mathbf{diag}(\boldsymbol{d})^{2}\boldsymbol{A},
$$ 

where the eleme $d\,\in\,\mathbf{R}^{m}$ are given by $d_{i}\,=\,1/(b_{i}\,-\,a_{i}^{T}x)$ ). Since $x$ is strictly feasible, we have d $d\succ0$ ≻ 0, so the Hessian of $\phi$ is nonsingular if and only if $A$ has rank $n$ . The centrality condition ( 11.7 ) is 

$$
t c+\sum_{i=1}^{m}{\frac{1}{b_{i}-a_{i}^{T}x}}a_{i}=t c+A^{T}d=0.
$$ 

We can give a simple geometric interpretation of the centrality condition. At a point $x^{\star}(t)$ on the central path the gradient $\nabla\phi(x^{\star}(t))$ , which is normal to the level set of $\phi$ through $x^{\star}(t)$ , must be parallel to $-c$ . In other words, the hyperplane $c^{T}x=c^{T}x^{\star}(t)$ is tangent to the level set of $\phi$ through $x^{\star}(t)$ . Figure 11.2 shows an example with $m=6$ and $n=2$ . 

# Dual points from central path 

From ( 11.7 ) we can derive an important property of the central path: Every central point yields a dual feasible point, and hence a lower bound on the optimal value $p^{\star}$. More specifically, define 

$$
\lambda_{i}^{\star}(t)=-\frac{1}{t f_{i}(x^{\star}(t))},\quad i=1,\ldots,m,\qquad\nu^{\star}(t)=\hat{\nu}/t.
$$ 

We claim that the pair $\lambda^{\star}(t)$ , $\nu^{\star}(t)$ is dual feasible. 

First, it is clear that $\lambda^{\star}(t)~\succ~0$ because $f_{i}(x^{\star}(t))\ <\ 0$ , $i~=~1,\cdot\cdot\cdot,m$ . By expressing the optimality conditions ( 11.7 ) as 

$$
\nabla f_{0}(x^{\star}(t))+\sum_{i=1}^{m}\lambda_{i}^{\star}(t)\nabla f_{i}(x^{\star}(t))+A^{T}\nu^{\star}(t)=0,
$$ 

![](images/a8984fc1e4836d9f92f73c0df4e63c5d81805bcb36ec1f17386608697275d106.jpg) 
Figure 11.2 Central path for an LP with $n\,=\,2$ and $m\,=\,6$ . The dashed curves show three contour lines of the logarithmic barrier function $\phi$ . The central path converges to the op point $x^{\star}$ as $t\to\infty$ . Also shown is the point on the central path with t = 10. The optimality condition ( 11.9 ) at this point can be verified geometrically: The line $c^{T}x=c^{T}x^{\star}(10)$ is tangent to the contour line of $\phi$ through $x^{\star}(10)$ . 

we see that $x^{\star}(t)$ minimizes the Lagrangian 

$$
L(x,\lambda,\nu)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\nu^{T}(A x-b),
$$ 

for $\lambda=\lambda^{\star}(t)$ and $\nu=\nu^{\star}(t)$ , which means that $\lambda^{\star}(t)$ , $\nu^{\star}(t)$ is a dual feasible pair. Therefore the dual function $g(\lambda^{\star}(t),\nu^{\star}(t))$ is finite, and 

$$
\begin{array}{r c l}{g(\lambda^{\star}(t),\nu^{\star}(t))}&{=}&{f_{0}(x^{\star}(t))+\displaystyle\sum_{i=1}^{m}\lambda_{i}^{\star}(t)f_{i}(x^{\star}(t))+\nu^{\star}(t)^{T}(A x^{\star}(t)-b)}\\ &{=}&{f_{0}(x^{\star}(t))-m/t.}\end{array}
$$ 

In particular, the duality gap associated with $x^{\star}(t)$ and the dual feasible pair $\lambda^{\star}(t)$ , $\nu^{\star}(t)$ is simply $m/t$ . As an important consequence, we have 

$$
f_{0}(x^{\star}(t))-p^{\star}\leq m/t,
$$ 

i.e. , $x^{\star}(t)$ is no more than $m/t$ -suboptimal. This confirms the intuitive idea that $x^{\star}(t)$ converges to an optimal point as $t\to\infty$ . 

Example 11.2 Inequality form linear programming. The dual of the inequality form LP ( 11.8 ) is 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\,-\boldsymbol{b}^{T}\boldsymbol{\lambda}}\\ &{\mathrm{subject~to}\quad\boldsymbol{A}^{T}\boldsymbol{\lambda}+\boldsymbol{c}=0}\\ &{\quad\quad\quad\quad\quad\boldsymbol{\lambda}\succeq0.}\end{array}
$$ 

From the optimality conditions ( 11.9 ), it is clear that 

$$
\lambda_{i}^{\star}(t)=\frac{1}{t(b_{i}-a_{i}^{T}x^{\star}(t))},\quad i=1,\dots,m,
$$ 

is dual feasible, with dual objective value 

$$
-b^{T}\lambda^{\star}(t)=c^{T}x^{\star}(t)+(A x^{\star}(t)-b)^{T}\lambda^{\star}(t)=c^{T}x^{\star}(t)-m/t.
$$ 

# Interpretation via KKT conditions 

We can also interpret the central path conditions ( 11.7 ) as a continuous deformation of the KKT optimality conditions ( 11.2 ). A point $x$ is equal to $x^{\star}(t)$ if and only if there exists $\lambda$ , $\nu$ such that 

$$
\begin{array}{r c l l}{{A x=b,}}&{{f_{i}(x)}}&{{\leq}}&{{0,}}&{{i=1,\ldots,m}}\\ {{}}&{{\lambda}}&{{\succeq}}&{{0}}&{{}}\\ {{\nabla f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}\nabla f_{i}(x)+A^{T}\nu}}&{{=}}&{{0}}&{{}}\\ {{-\lambda_{i}f_{i}(x)}}&{{=}}&{{1/t,}}&{{i=1,\ldots,m.}}\end{array}
$$ 

The only diﬀerence between the KKT conditions ( 11.2 ) and the centrality condi- tions ( 11.11 ) is that the complementarity condition $-\lambda_{i}f_{i}(x)\,=\,0$ is replaced by the condition $-\lambda_{i}f_{i}(x)=1/t$ . In particular, for large t , $x^{\star}(t)$ and the associated dual point $\lambda^{\star}(t)$ , $\nu^{\star}(t)$ ‘almost’ satisfy the KKT optimality conditions for ( 11.1 ). 

# Force field interpretation 

We can give a simple mechanics interpretation of the central path in terms of potential forces acting on a particle in the strictly feasible set $C$ . For simplicity we assume that there are no equality constraints. 

We associate with each constraint the force 

$$
F_{i}(x)=-\nabla\left(-\log(-f_{i}(x))\right)=\frac{1}{f_{i}(x)}\nabla f_{i}(x)
$$ 

acting on the particle when it is at position $x$ . The potential associated with the total force field generated by the constraints is the logarithmic barrier $\phi$ . As the particle moves toward the boundary of the feasible set, it is strongly repelled by the forces generated by the constraints. 

Now we imagine another force acting on the particle, given by 

$$
F_{0}(x)=-t\nabla f_{0}(x),
$$ 

when the particle is at position $x$ . This objective force field acts to pull the particle in the negative gradient direction, i.e. , toward smaller $f_{0}$ . The parameter $t$ scales the objective force, relative to the constraint forces. 

The central point $x^{\star}(t)$ is the point where the constraint forces exactly balance the objective force felt by the particle. As the parameter $t$ increases, the particle is more strongly pulled toward the optimal point, but it is always trapped in $C$ by the barrier potential, which becomes infinite as the particle approaches the boundary. 

$$
F_{i}(x)=\frac{-a_{i}}{b_{i}-a_{i}^{T}x}.
$$ 

![](images/057966d33c341420484e7d086fa5501bf4e0067b91fcb270a979094d4c80878d.jpg) 
Figure 11.3 Force field interpretation of central path . The central path is shown as the dashed curve. The two points $x^{\star}(1)$ and $x^{\star}(3)$ are shown as dots in the left and right plots, respectively. The objective force, which is equal to $-c$ and $-3c$ , respectively, is shown as a heavy arrow. The other arrows represent the constraint forces, which are given by an inverse-distance law. As the strength of the objective force varies, the equilibrium position of the particle traces out the central path. 

This force is in the direction of the inward pointing normal to the constraint plane ${\mathcal{H}}_{i}\,=\,\{x\,\mid\,a_{i}^{T}x\,=\,b_{i}\}$ } , and has magnitude inversely proportional to the distance to $\mathcal{H}_{i}$ , i.e. , 

$$
||F_{i}(x)||_{2}=\frac{||a_{i}||_{2}}{b_{i}-a_{i}^{T}x}=\frac{1}{\mathbf{dist}(x,\mathscr{H}_{i})}.
$$ 

In other words, each constraint hyperplane has an associated repulsive force, given by the inverse distance to the hyperplane. 

The term $t c^{T}x$ is the potential associated with a constant force $-t c$ on the particle. This ‘objective force’ pushes the particle in the direction of low cost. Thus, $x^{\star}(t)$ is the equilibrium position of the particle when it is subject to the inverse-distance constraint forces, and the objective force $-t c$ . When $t$ is very large, the particle is pushed almost to the optimal point. The strong objective force is balanced by the opposing constraint forces, which are large because we are near the feasible boundary. 

Figure 11.3 illustrates this interpretation for a small LP with $n=2$ and $m=5$ . The lefthand plot shows $x^{\star}(t)$ for $t=1$ , as well as the constraint forces acting on it, which balance the objective force. The righthand plot shows $x^{\star}(t)$ and the associated forces for $t=3$ . The larger value of objective force moves the particle closer to the optimal point. 

# 11.3 The barrier method 

We have seen that the point $x^{\star}(t)$ is $m/t$ -suboptimal, and that a certificate of this accuracy is provided by the dual feasible pair $\lambda^{\star}(t)$ , $\nu^{\star}(t)$ . This suggests a very straightforward method for solving the original problem ( 11.1 ) with a guaranteed specified accuracy $\epsilon$ : We simply take $t=m/\epsilon$ and solve the equality constrained problem 

$$
\begin{array}{l l}{{\mathrm{minimize}}}&{{(m/\epsilon)f_{0}(x)+\phi(x)\nonumber}}\\ {{\mathrm{subject~to}}}&{{A x=b}}\end{array}
$$ 

using Newton’s method. This method could be called the unconstrained minimiza- tion method , since it allows us to solve the inequality constrained problem ( 11.1 ) to a guaranteed accuracy by solving an unconstrained, or linearly constrained, prob- lem. Although this method can work well for small problems, good starting points, and moderate accuracy ( i.e. , $\epsilon$ not too small), it does not work well in other cases. As a result it is rarely, if ever, used. 

# 11.3.1 The barrier method 

A simple extension of the unconstrained minimization method does work well. It is based on solving a sequence of unconstrained (or linearly constrained) mini- mization problems, using the last point found as the starting point for the next unconstrained minimization problem. In other words, we compute $x^{\star}(t)$ for a se- quence of increasing values of $t$ , until $t\geq m/\epsilon$ , which guarantees that we have an $\epsilon$ -suboptimal solution of the original problem. When the method was first proposed by Fiacco and McCormick in the 1960s, it was called the sequential unconstrained minimization technique (SUMT). Today the method is usually called the barrier method or path-following method . A simple version of the method is as follows. 

Algorithm 11.1 Barrier method. 

given strictly feasible $x$ , $t:=t^{(0)}>0$ , $\mu>1$ , tolerance $\epsilon>0$ . 

repeat 

1. Centering step. Compute $x^{\star}(t)$ by minimizing $t f_{0}+\phi$ , subject to $A x=b$ , starting at $x$ . 2. Update. $x:=x^{\star}(t)$ . 3. Stopping criterion. quit if $m/t<\epsilon$ . 4. Increase $t$ . $t:=\mu t$ . 

At each iteration (except the first one) we compute the central point $x^{\star}(t)$ starting from the previously computed central point, and then increase $t$ by a factor $\mu>1$ . The algorithm can also return $\lambda=\lambda^{\star}(t)$ , and $\nu=\nu^{\star}(t)$ , a dual $\epsilon$ -suboptimal point, or certificate for $x$ . 

We refer to each execution of step 1 as a centering step (since a central point is being computed) or an outer iteration , and to the first centering step (the com- putation of $x^{\star}(t^{(0)}),$ ) as the initial centering step . (Thus the simple algorithm with $t^{(0)}\,=\,m/\epsilon$ consists of only the initial centering step.) Although any method for linearly constrained minimization can be used in step 1, we will assume that New- ton’s method is used. We refer to the Newton iterations or steps executed during the centering step as inner iterations . At each inner step, we have a primal fea- sible point; we have a dual feasible point, however, only at the end of each outer (centering) step. 

# Accuracy of centering 

We should make some comments on the accuracy to which we solve the centering problems. Computing $x^{\star}(t)$ exactly is not necessary since the central path has no significance beyond the fact that it leads to a solution of the original problem as $t\to\infty$ ; inexact centering will still yield a sequence of points $x^{(k)}$ that converges to an optimal point. Inexact centering, however, means that the points $\lambda^{\star}(t)$ , $\nu^{\star}(t)$ , computed from ( 11.10 ), are not exactly dual feasible. This can be corrected by adding a correction term to the formula ( 11.10 ), which yields a dual feasible point provided the computed $x$ is near the central path, i.e. , $x^{\star}(t)$ (see exercise 11.9 ). 

On the other hand, the cost of computing an extremely accurate minimizer of $t f_{0}+\phi$ , as compared to the cost of computing a good minimizer of $t f_{0}+\phi$ , is only marginally more, i.e. , a few Newton steps at most. For this reason it is not unreasonable to assume exact centering. 

# Choice of $\mu$ 

The choice of the parameter $\mu$ involves a trade-oﬀin the number of inner and outer iterations required. If is small ( i.e. , near 1) then at each outer iteration $t$ increases $\mu$ by a small factor. As a result the initial point for the Newton process, i.e. , the previous iterate $x$ , is a very good starting point, and the number of Newton steps needed to compute the next iterate is small. Thus for small $\mu$ we expect a small number of Newton steps per outer iteration, but of course a large number of outer iterations since each outer iteration reduces the gap by only a small amount. In this case the iterates (and indeed, the iterates of the inner iterations as well) closely follow the central path. This explains the alternate name path-following method . 

On the other hand if $\mu$ is large we have the opposite situation. After each outer iteration $t$ increases a large amount, so the current iterate is probably not a very good approximation of the next iterate. Thus we expect many more inner iterations. This ‘aggressive’ updating of $t$ results in fewer outer iterations, since the duality gap is reduced by the large factor $\mu$ at each outer iteration, but more inner iterations. With $\mu$ large, the iterates are widely separated on the central path; the inner iterates veer way oﬀthe central path. 

This trade-oﬀin the choice of $\mu$ is confirmed both in practice and, as we will see, in theory. In practice, small values of $\mu$ ( i.e. , near one) result in many outer iterations, with just a few Newton steps for each outer iteration. For $\mu$ in a fairly large range, from around 3 to 100 or so, the two eﬀects nearly cancel, so the total number of Newton steps remains approximately constant. This means that the choice of $\mu$ is not particularly critical; values from around 10 to 20 or so seem to work well. When the parameter $\mu$ is chosen to give the best worst-case bound on the total number of Newton steps required, values of $\mu$ near one are used. 

# Choice of $t^{(0)}$ 

Another important issue is the choice of initial value of $t$ . Here the trade-oﬀis simple: If $t^{(0)}$ is chosen too large, the first outer iteration will require too many it- erations. If $t^{(0)}$ is chosen too small, the algorithm will require extra outer iterations, and possibly too many inner iterations in the first centering step. 

Since $m/t^{(0)}$ is the duality gap that will result from the first centering step, one reasonable choice is to choose $t^{(0)}$ so that $m/t^{(0)}$ is approximately of the same order as $f_{0}\mathopen{}\mathclose\bgroup\left(x^{(0)}\aftergroup\egroup\right)-p^{\star}$ , or times this amount. For example, if a dual feasible point $\lambda$ , $\mu$ $\nu$ is known, with duality gap $\eta=f_{0}\bigl(x^{(0)}\bigr)-g\bigl(\lambda,\nu\bigr)$ , then we can take $t^{(0)}=m/\eta$ . Thus, in the first outer iteration we simply compute a pair with the same duality gap as the initial primal and dual feasible points. 

Another possibility is suggested by the central path condition ( 11.7 ). We can interpret 

$$
\operatorname*{inf}_{\nu}\left\|t\nabla f_{0}(x^{(0)})+\nabla\phi(x^{(0)})+A^{T}\nu\right\|_{2}
$$ 

as a measure for the deviation of $x^{(0)}$ from the point $x^{\star}(t)$ ), and choose for $t^{(0)}$ the value that minimizes ( 11.12 ). (This value of $t$ and $\nu$ can be found by solving a least-squares problem.) 

A variation on this approach uses an affine-invariant measure of deviation be- tween $x$ and $x^{\star}(t)$ in place of the Euclidean norm. We choose $t$ and $\nu$ that minimize 

$$
\begin{array}{r}{\alpha(t,\nu)=\left(t\nabla f_{0}(x^{(0)})+\nabla\phi(x^{(0)})+A^{T}\nu\right)^{T}H_{0}^{-1}\left(t\nabla f_{0}(x^{(0)})+\nabla\phi(x^{(0)})+A^{T}\nu\right),}\end{array}
$$ 

where 

$$
H_{0}=t\nabla^{2}f_{0}(x^{(0)})+\nabla^{2}\phi(x^{(0)}).
$$ 

(It can be shown that $\operatorname*{inf}_{\nu}\alpha(t,\nu)$ is the square of the Newton decrement of $t f_{0}+\phi$ at $x^{(0)}$ .) Since $\alpha$ is a quadratic-over-linear function of $\nu$ and $t$ , it is convex. 

# Infeasible start Newton method 

In one variation on the barrier method, an infeasible start Newton method (de- scribed in § 10.3 ) is d for the centering steps. Thus, the barrier method is ini- tialized with a poin $x^{(0)}$ satisfies $x^{(0)}\in\mathbf{dom}\,f_{0}$ and $f_{i}(x^{(0)})<0$ , $i=1,\ldots,m$ , but not necessarily $A x^{(0)}=b$ . Assuming the problem is strictly feasible, a full New- ton step is taken at some point during the first centering step, and thereafter, the iterates are all primal feasible, and the algorithm coincides with the (standard) barrier method. 

# 11.3.2 Examples 

# Linear programming in inequality form 

Our first example is a small LP in inequality form, 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x\preceq b}\end{array}}
$$ 

with $A\,\in\,\mathbf{R}^{100\times50}$ . The data were generated randomly, in such a way that the problem is strictly primal and dual feasible, with optimal value $p^{\star}=1$ . 

The initial point $x^{(0)}$ is on the central path, with a duality gap of 100. The barrier method is used to solve the problem, and terminated when the duality gap is less than $10^{-6}$ . The centering problems are solved by Newton’s method with backtracking, using parameters $\alpha\:=\:0.01$ , $\beta\:=\:0.5$ . The stopping criterion for 

![](images/4f42add96151258d3b5861f9424fa01501e0988692e17028fe08d5a2ecb8a4f8.jpg) 
Figure 11.4 Progress of barrier method for a small LP, showing duality gap versus cumulative number of Newton steps. Three plots are shown, corresponding to three values of the parameter $\mu$ : 2, 50, and 150. In each case, we have approximately linear convergence of duality gap. 

Newton’s method is $\lambda(x)^{2}/2\leq10^{-5}$ , where $\lambda(x)$ is the Newton decrement of the function $t c^{T}x+\phi(x)$ . 

The progress of the barrier method, for three values of the parameter $\mu$ , is shown in figure 11.4 . The vertical axis shows the duality gap on a log scale. The horizontal axis shows the cumulative total number of inner iterations, i.e. , Newton steps, which is the natural measure of computational eﬀort. Each of the plots has a staircase shape, with each stair associated with one outer iteration. The width of each stair tread ( i.e. , horizontal portion) is the number of Newton steps required for that outer iteration. The height of each stair riser ( i.e. , the vertical portion) is exactly equal to (a factor of) $\mu$ , since the duality gap is reduced by the factor $\mu$ at the end of each outer iteration. 

The plots illustrate several typical features of the barrier method. First of all, the method works very well, with approximately linear convergence of the duality gap. This is a consequence of the approximately constant number of Newton steps required to re-center, for each value of $\mu$ . For $\mu\,=\,50$ and $\mu=150$ , the barrier method solves the problem with a total number of Newton steps between 35 and 40. 

The plots in figure 11.4 clearly show the trade-oﬀin the choice of $\mu$ . For $\mu=2$ , the treads are short; the number of Newton steps required to re-center is around 2 or 3. But the risers are also short, since the duality gap reduction per outer iteration is only a factor of 2. At the other extreme, when $\mu=150$ , the treads are longer, typically around 7 Newton steps, but the risers are also much larger, since the duality gap is reduced by the factor 150 in each outer iteration. 

The trade-oﬀin choice of $\mu$ is further examined in figure 11.5 . We use the barrier method to solve the LP, terminating when the duality gap is smaller than $10^{-3}$ , for 25 values of $\mu$ between 1 . 2 and 200. The plot shows the total number of Newton steps required to solve the problem, as a function of the parameter $\mu$ . 

![](images/793fedc3426670ec3982dd04d1f543ecce2a65437eb3ff3b41e076cfdd021ff6.jpg) 
Figure 11.5 Trade-oﬀin the choice of the parameter $\mu$ , for a small LP. The vertical axis shows the total number of Newton steps required to reduce the duality gap from 100 to $10^{-3}$ , and the horizontal axis shows $\mu$ . The plot shows the barrier method works well for values of $\mu$ larger than around 3, but is otherwise not sensitive to the value of $\mu$ . 

This plot shows that the barrier method performs very well for a wide range of values of $\mu$ , from around 3 to 200. As our intuition suggests, the total number of Newton steps rises when $\mu$ is too small, due to the larger number of outer iterations required. One interesting observation is that the total number of Newton steps does not vary much for values of $\mu$ larger than around 3. Thus, as $\mu$ increases over this range, the decrease in the number of outer iterations is oﬀset by an increase in the number of Newton steps per outer iteration. For even larger values of $\mu$ , the performance of the barrier method becomes less predictable ( i.e. , more dependent on the particular problem instance). Since the performance does not improve with larger values of $\mu$ , a good choice is in the range $10\mathrm{~-~}100$ . 

# Geometric programming 

We consider a geometric program in convex form, 

$$
\begin{array}{r l}{\mathrm{minimize}\:\:}&{\log\left(\sum_{k=1}^{K_{0}}\exp(a_{0k}^{T}x+b_{0k})\right)}\\ {\mathrm{subject~to}\:\:}&{\log\left(\sum_{k=1}^{K_{i}}\exp(a_{i k}^{T}x+b_{i k})\right)\le0,\quad i=1,\ldots,m,}\end{array}
$$ 

with variable $x\in\mathbf{R}^{n}$ , and associated logarithmic barrier 

$$
\phi(x)=-\sum_{i=1}^{m}\log\left(-\log\sum_{k=1}^{K_{i}}\exp(a_{i k}^{T}x+b_{i k})\right).
$$ 

The problem instance we consider has $n=50$ variables and $m=100$ inequalities (like the small LP considered above). The objective and constraint functions all 

![](images/70e3b7d3d3e9d6a0a8cb165a235dd27db31dc7a70d4d5552f4a0c9edf9e45519.jpg) 
Figure 11.6 Progress of barrier method for a small GP, showing duality gap versus cumulative number of Newton steps. Again we have approximately linear convergence of duality gap. 

have $K_{i}=5$ terms. The problem instance was generated randomly, in such a way that it is strictly primal and dual feasible, with optimal value one. 

We start with a point $x^{(0)}$ on the central path, with a duality gap of 100. The barrier method is used to solve the problem, with parameters $\mu=2$ , $\mu=50$ , and $\mu=150$ , and terminated when the duality gap is less than $10^{-6}$ . The centering problems are solved using Newton’s method, with the same parameter values as in the LP example, i.e. , $\alpha=0.01$ , $\beta=0.5$ , and stopping criterion $\lambda(x)^{2}/2\leq10^{-5}$ . 

Figure 11.6 shows the duality gap versus cumulative number of Newton steps. This plot is very similar to the plot for LP, shown in figure 11.4 . In particular, we see an approximately constant number of Newton steps required per centering step, and therefore approximately linear convergence of the duality gap. 

The variation of the total number of Newton steps required to solve the problem, versus the parameter $\mu$ , is very similar to that in the LP example. For this GP, the total number of Newton steps required to reduce the duality gap below $10^{-3}$ is around 30 (ranging from around 20 to 40 or so) for values of $\mu$ between 10 and 200. So here, too, a good choice of is in the range $10\mathrm{~-~}100$ . $\mu$ 

# A family of standard form LPs 

In the examples above we examined the progress of the barrier method, in terms of duality gap versus cumulative number of Newton steps, for a randomly generated instance of an LP and a GP, with similar dimensions. The results for the two examples are remarkably similar; each shows approximately linear convergence of duality gap with the number of Newton steps. We also examined the variation in performance with the parameter $\mu$ , and found essentially the same results in the two cases. For $\mu$ above around 10, the barrier method performs very well, requiring around 30 Newton steps to bring the duality gap down from $10^{2}$ to $10^{-6}$ . In both cases, the choice of $\mu$ hardly aﬀects the total number of Newton steps required (provided $\mu$ is larger than 10 or so). 

In this section we examine the performance of the barrier method as a function of the problem dimensions. We consider LPs in standard form, 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x=b,\quad x\succeq0}\end{array}}
$$ 

with $A\,\in\,\mathbf{R}^{m\times n}$ , and explore the total number of Newton steps required as a function of the number of variables $n$ and number of equality constraints $m$ , for a family of randomly generated problem instances. We take $n=2m$ , i.e. , twice as many variables as constraints. 

The problems were generated as follows. The elements of $A$ are independent and identicall ed, with zero mean, unit iance normal distribution ${\mathcal{N}}(0,1)$ . We take b $b\,=\,A x^{(0)}$ where the elements of x $x^{(0)}$ are independent, and uniformly distributed in $[0,1]$ . This ensures that the problem is strictly primal feasible, since $x^{(0)}\,\succ\,0$ is feasible. To construct the cost vector $c$ , we first compute a vector $z\in\mathbf{R}^{m}$ with elements distributed according to ${\mathcal{N}}(0,1)$ and a $s\in\mathbf{R}^{n}$ with elements from a uniform distribution on $[0,1]$ . We then take c $c=A^{T}z+s$ . This guarantees that the problem is strictly dual feasible, since $A^{T}z\prec c$ . 

The algorithm parameters we use are $\mu=100$ , and the same parameters for the centering steps in the examples above: backtracking parameters $\alpha=0.01$ , $\beta=0.5$ , and criterion $\lambda(x)^{2}/2\,\le\,10^{-5}$ . The initial point is on the central path with t $t^{(0)}\,=\,1$ = 1 ( i.e. , gap $n$ ). The algorithm is terminated when the initial duality gap is reduced by a factor $10^{4}$ , i.e. , after completing two outer iterations. 

Figure 11.7 shows the duality gap versus iteration number for three problem instances, with dimensions $m=50$ , $m=500$ , and $m=1000$ . The plots look very much like the others, with approximately linear convergence of the duality gap. The plots show a small increase in the number of Newton steps required as the problem size grows from 50 constraints (100 variables) to 1000 constraints (2000 variables). 

To examine the eﬀect of problem size on the number of Newton steps required, we generate 100 problem instances for each of 20 values of $m$ , ranging from $m=10$ to $m\,=\,1000$ . We solve each of these 2000 problems using the barrier method, noting the number of Newton steps required. The results are summarized in fig- ure 11.8 , which shows the mean and standard deviation in the number of Newton steps, for each value of $m$ . The first comment we make is that the standard de- viation is around 2 iterations, and appears to be approximately independent of problem size. Since the average number of steps required is near 25, this means that the number of Newton steps required varies only around $\pm10\%$ . 

The plot shows that the number of Newton steps required grows only slightly, from around 21 to around 27, as the problem dimensions increase by a factor of 100. This behavior is typical for the barrier method in general: The number of Newton steps required grows very slowly with problem dimensions, and is almost always around a few tens. Of course, the computational eﬀort to carry out one Newton step grows with the problem dimensions. 

![](images/0f32a6bf68cdf97aa50e12c96118439b18acaf1bc5410756049184c219b18268.jpg) 
Figure 11.7 Progress of barrier method for three randomly generated stan- dard form LPs of diﬀerent dimensions, showing duality gap versus cumula- tive number of Newton steps. The number of variables in each problem is $n\,=\,2m$ . Here too we see approximately linear convergence of the duality gap, with a slight increase in the number of Newton steps required for the larger problems. 

![](images/55f85316169330e952105f9b1d3a64b66c51b8fe43e48bcaf0aeff65755c7838.jpg) 
Figure 11.8 Average number of Newton steps required to solve 100 randomly generated LPs of diﬀerent dimensions, with $n=2m$ . Error bars show stan- dard deviation, around the average value, for each value of $_{m}$ . The growth in the number of Newton steps required, as the problem dimensions range over a 100:1 ratio, is very small. 

# 11.3.3 Convergence analysis 

Convergence analysis for the barrier method is straightforward. Assuming that $t f_{0}+\phi$ can be minimized by Newton’s method for $t=t^{(0)}$ , $\mu t^{(0)}$ , $\mu^{2}t^{(0)},.\;.\;.$ , the duality gap after the initial centering step, and $k$ additional centering steps, is $m/(\mu^{k}t^{(0)})$ . Therefore the desired accuracy $\epsilon$ is achieved after exactly 

$$
\left\lceil\frac{\log(m/(\epsilon t^{(0)}))}{\log\mu}\right\rceil
$$ 

centering steps, plus the initial centering step. 

It follows that the barrier method works provided the centering problem ( 11.6 ) is solvable by N s method, for $t\geq t^{(0)}$ . For the standard Newton method, it suffices that for $t\geq t^{(0)}$ ≥ , the function $t f_{0}\!+\!\phi$ satisfies the conditions given in § 10.2.4 , page 529 : its initial sublevel set is closed, the associated inverse KKT matrix is bounded, and the Hessian satisfies a Lipschitz condition. (Another set of sufficient conditions, based on self-concordance, will be discussed in detail in § 11.5 .) If the infeasible start Newton method is used for centering, then the conditions listed in § 10.3.3 , page 536 , are sufficient to guarantee convergence. 

Assuming that $f_{0},\ldots,f_{m}$ are closed, a simple modification of the original problem ensures that these conditions hold. By adding a constraint of the form to the problem, it follows that $\|x\|_{2}^{2}\,\leq\,R^{2}$ ≤ $t f_{0}+\phi$ is strongly convex, for every $t\geq0$ ≥ 0; in particular convergence of Newton’s method, for the centering steps, is guaranteed. (See exercise 11.4 .) 

While this analysis shows that the barrier method does converge, under reason- able assumptions, it does not address a basic question: As the parameter $t$ increases, do the centering problems become more difficult (and therefore take more and more iterations)? Numerical evidence suggests that for a wide variety of problems, this is not the case; the centering problems appear to require a nearly constant number of Newton steps to solve, even as $t$ increases. We will see (in § 11.5 ) that this issue can be resolved, for problems that satisfy certain self-concordance conditions. 

# 11.3.4 Newton step for modified KKT equations 

In the barrier method, the Newton step $\Delta x_{\mathrm{{nt}}}$ , and associated dual variable are given by the linear equations 

$$
\left[\begin{array}{c c}{t\nabla^{2}f_{0}(x)+\nabla^{2}\phi(x)}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {\nu_{\mathrm{nt}}}\end{array}\right]=-\left[\begin{array}{c}{t\nabla f_{0}(x)+\nabla\phi(x)}\\ {0}\end{array}\right].
$$ 

In this section we show how these Newton steps for the centering problem can be interpreted as Newton steps for directly solving the modified KKT equations 

$$
\begin{array}{r c l}{\nabla f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}\nabla f_{i}(x)+A^{T}\nu}&{=}&{0}\\ {-\lambda_{i}f_{i}(x)}&{=}&{1/t,\quad i=1,\ldots,m}\\ {A x}&{=}&{b}&\end{array}
$$ 

in a particular way. 

To solve the modified KKT equations ( 11.15 ), which is a set of $n+p+m$ nonlinear equations in the $n+p+m$ variables $x$ , $\nu$ , and $\lambda$ , we first eliminate the variables $\lambda_{i}$ , using $\lambda_{i}=-1/(t f_{i}(x))$ . This yields 

$$
\nabla f_{0}(x)+\sum_{i=1}^{m}{\frac{1}{-t f_{i}(x)}}\nabla f_{i}(x)+A^{T}\nu=0,\qquad A x=b,
$$ 

which is a set of $n+p$ equations in the $n+p$ variables $x$ and $\nu$ . 

To find the Newton step for solving the set of nonlinear equations ( 11.16 ), we form the Taylor approximation for the nonlinear term occurring in the first equation. For $v$ small, we have the Taylor approximation 

$$
\begin{array}{r l r}{\lefteqn{\nabla f_{0}(x+v)+\sum_{i=1}^{m}\frac{1}{-t f_{i}(x+v)}\nabla f_{i}(x+v)}}\\ &{\approx}&{\nabla f_{0}(x)+\sum_{i=1}^{m}\frac{1}{-t f_{i}(x)}\nabla f_{i}(x)+\nabla^{2}f_{0}(x)v}\\ &{}&{+\sum_{i=1}^{m}\frac{1}{-t f_{i}(x)}\nabla^{2}f_{i}(x)v+\sum_{i=1}^{m}\frac{1}{t f_{i}(x)^{2}}\nabla f_{i}(x)\nabla f_{i}(x)^{T}v.}\end{array}
$$ 

The Newton step is obtained by replacing the nonlinear term in equation ( 11.16 ) by this Taylor approximation, which yields the linear equations 

$$
H\boldsymbol{v}+\boldsymbol{A}^{T}\boldsymbol{\nu}=-g,\qquad\boldsymbol{A}\boldsymbol{v}=0,
$$ 

where 

$$
\begin{array}{r c l}{{H}}&{{=}}&{{\displaystyle\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}\frac{1}{-t f_{i}(x)}\nabla^{2}f_{i}(x)+\sum_{i=1}^{m}\frac{1}{t f_{i}(x)^{2}}\nabla f_{i}(x)\nabla f_{i}(x)^{T}}}\\ {{g}}&{{=}}&{{\displaystyle\nabla f_{0}(x)+\sum_{i=1}^{m}\frac{1}{-t f_{i}(x)}\nabla f_{i}(x).}}\end{array}
$$ 

Now we observe that 

$$
H=\nabla^{2}f_{0}(x)+(1/t)\nabla^{2}\phi(x),\qquad g=\nabla f_{0}(x)+(1/t)\nabla\phi(x),
$$ 

so, from ( 11.14 ), the Newton steps $\Delta x_{\mathrm{{nt}}}$ and $\nu_{\mathrm{nt}}$ in the barrier method centering step satisfy 

$$
\begin{array}{r}{t H\Delta x_{\mathrm{nt}}+A^{T}\nu_{\mathrm{nt}}=-t g,\qquad A\Delta x_{\mathrm{nt}}=0.}\end{array}
$$ 

Comparing this with ( 11.17 ) shows that 

$$
v=\Delta x_{\mathrm{nt}},\qquad\nu=(1/t)\nu_{\mathrm{nt}}.
$$ 

This shows that the Newton step for the centering problem ( 11.6 ) can be inter- preted, after scaling the dual variable, as the Newton step for solving the modified KKT equations ( 11.16 ). 

In this approach, we first eliminated the variable $\lambda$ from the modified KKT equations, and then applied Newton’s method to solve the resulting set of equations. Another variation on this approach is to directly apply Newton’s method to the modified KKT equations, without first eliminating $\lambda$ . This method yields the so- called primal-dual search directions , discussed in $\S$ 11.7 . 

# 11.4 Feasibility and phase I methods 

The barrier method requires a strictly feasible starting point $x^{(0)}$ . When such a point is not known, the barrier method is preceded by a preliminary stage, called phase $I$ , in which a strictly feasible point is computed (or the constraints are found to be infeasible). The strictly feasible point found during phase I is then used as the starting point for the barrier method, which is called the phase $I I$ stage. In this section we describe several phase I methods. 

# 11.4.1 Basic phase I method 

We consider a set of inequalities and equalities in the variables $x\in\mathbf{R}^{n}$ , 

$$
f_{i}(x)\leq0,\quad i=1,.\,.\,.\,,m,\qquad A x=b,
$$ 

where $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ are convex, with continuous second deriv We assume that we are given a point $x^{(0)}\in\mathbf{dom}\,f_{1}\cap\cdot\cdot\cdot\cap\mathbf{dom}\,f_{m}$ , with Ax $A x^{(0)}=b$ b . 

Our goal is to find a strictly feasible solution of these inequalities and equalities, or determine that none exists. To do this we form the following optimization problem: 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad s}\\ &{{\mathrm{subject~to}}\quad f_{i}(x)\leq s,\quad i=1,.\,.\,,m}\\ &{\ A x=b}\end{array}}
$$ 

in the variables $x\in\mathbf{R}^{n}$ , $s\in\mathbf{R}$ . The variable $s$ can be interpreted as a bound on the maximum infeasibility of the inequalities; the goal is to drive the maximum infeasibility below zero. 

This problem is always strictly feasible, since we can choose $x^{(0)}$ as starting point for $x$ , and for $s$ , we can choose any number larger than $\mathrm{max}_{i=1,...,m}\ f_{i}(x^{(0)})$ . We can therefore apply the barrier method to solve the problem ( 11.19 ), which is called the phase I optimization problem associated with the inequality and equality system ( 11.19 ). 

We can distinguish three cases depending on the sign of the optimal value $\bar{p}^{\star}$ of ( 11.19 ). 

1. If $\bar{p}^{\star}<0$ 0, then ( 11.18 ) has a strictly feasible solution. Moreover if $(x,s)$ is feasible for ( 11.19 ) with $s<0$ , then $x$ satisfies $f_{i}(x)<0$ . This means we do not need to solve the optimization problem ( 11.19 ) with high accuracy; we can terminate when $s<0$ . 

2. If $\bar{p}^{\star}\,>\,0$ 0, then ( 11.18 ) is infeasible. As in case 1, we do not need to solve the phase I optimization problem ( 11.19 ) to high accuracy; we can terminate when a dual feasible point is found with positive dual objective (which proves that $\bar{p}^{\star}>0$ 0). In this case, we can construct the alternative that proves ( 11.18 ) is infeasible from the dual feasible point. 

3. If $\bar{p}^{\star}\,=\,0$ = 0 and the minimum is attained at $x^{\star}$ and $s^{\star}\,=\,0$ , then the set of inequalities is feasible, but not strictly feasible. If $\bar{p}^{\star}=0$ = 0 and the minimum is not attained, then the inequalities are infeasible. 

In practice it is impossible to determine exactly that $\bar{p}^{\star}\,=\,0$ = 0. Instead, an optimization algorithm applied to ( 11.19 ) will terminate with the conclusion that $|\bar{p}^{\star}|<\epsilon$ | for some small, positive $\epsilon$ . This allows us to conclude that the inequalities $f_{i}(x)\,\le\,-\epsilon$ are infeasible, while the inequalities $f_{i}(x)\,\leq\,\epsilon$ are feasible. 

# Sum of infeasibilities 

There are many variations on the basic phase I method just described. One method is based on minimizing the sum of the infeasibilities, instead of the maximum infeasibility. We form the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\mathbf{1}^{T}s}\\ &{{\mathrm{subject~to}}\quad f_{i}(x)\leq s_{i},\quad i=1,\ldots,m}\\ &{\ A x=b}\\ &{\ s\succeq0.}\end{array}}
$$ 

For fixed $x$ , the optimal value of $s_{i}$ is $\operatorname*{max}\{f_{i}(x),0\}$ , so in this problem we are minimizing the sum of the infeasibilities. The optimal value of ( 11.20 ) is zero and achieved if and only if the original set of equalities and inequalities is feasible. 

This sum of infeasibilities phase I method has a very interesting property when the system of equalities and inequalities ( 11.19 ) is infeasible. In this case, the op- timal point for the phase I problem ( 11.20 ) often violates only a small number, say $r$ , of the inequalities. Therefore, we have computed a point that satisfies many $(m\mathrm{~-~}r)$ of the inequalities, i.e. , we have identified a large subset of inequalities that is feasible. In this case, the dual variables associated with the strictly satisfied inequalities are zero, so we have also proved infeasibility of a subset of the inequal- ities. This is more informative than finding that the $m$ inequalities, together, are mutually infeasible. (This phenomenon is closely related to $\ell_{1}$ -norm regularization, or basis pursuit, used to find sparse approximate solutions; see § 6.1.2 and § 6.5.4 ). 

Example 11.4 Comparison of phase $I$ methods. We apply two phase I methods to an infeasible set of inequalities $A x\preceq b$ with dimensions $m=100$ , $n=50$ . The first method is the basic phase I method 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{s}\\ &{{\mathrm{subject~to}}}&{A x\preceq b+\mathbf{1}s,}\end{array}}
$$ 

which minimizes the maximum infeasibility. The second method minimizes the sum of the infeasibilities, i.e. , solves the LP 

$$
{\begin{array}{r l}{\operatorname{minimize}\quad}&{\mathbf{1}^{T}s}\\ {{\mathrm{subject~to}}}&{A x\preceq b+s}\\ &{s\succeq0.}\end{array}}
$$ 

Figure 11.9 shows the distributions of the infeasibilities $b_{i}-a_{i}^{T}x$ for these two values of $x$ , denoted $x_{\mathrm{max}}$ and $x_{\mathrm{sum}}$ , respectively. The point $x_{\mathrm{max}}$ satisfies 39 of the 100 inequalities, whereas the point $x_{\mathrm{sum}}$ satisfies 79 of the inequalities. 

![](images/1b66ef82651dc9b956bd81e1b7db5f2e906e1115d8974a68ccc82706216f5740.jpg) 
Figure 11.9 Distrib the infeasibilities $b_{i}-a_{i}^{T}x$ for an infeasible set of 100 inequalities a $a_{i}^{T}x\,\leq\,b_{i}$ ≤ , with 50 variables. The vector $x_{\mathrm{max}}$ used in the left plot was obtained by the basic phase I algorithm. It satisfies 39 of the 100 inequalities. In the right plot the vector $x_{\mathrm{sum}}$ was obtained by minimizing the sum of the infeasibilities. This vector satisfies 79 of the 100 inequalities. 

# Termination near the phase II central path 

A simple variation on the basic phase I method, using the barrier method, has the property that (when the equalities and inequalities are strictly feasible) the central path for the phase I problem intersects the central path for the original optimization problem ( 11.1 ). 

We assume a point $x^{(0)}\in\mathcal{D}=\mathbf{dom}\,f_{0}\cap\mathbf{dom}\,f_{1}\cap\cdot\cdot\cdot\cap\mathbf{dom}\,f_{m}$ , with $A x^{(0)}=b$ is given. We form the phase I optimization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ s}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq s,\quad i=1,.\,.\,,m}\\ &{f_{0}(x)\leq M}\\ &{A x=b,}\end{array}}
$$ 

where $M$ is a constant chosen to be larger than $\operatorname*{max}\{f_{0}(x^{(0)}),p^{\star}\}$ . 

We assume now that the original problem ( 11.1 ) is strictly feasible, so the optimal value $\bar{p}^{\star}$ ⋆ of ( 11.21 ) is negative. The central path of ( 11.21 ) is characterized by 

$$
\sum_{i=1}^{m}\frac{1}{s-f_{i}(x)}=\bar{t},\qquad\frac{1}{M-f_{0}(x)}\nabla f_{0}(x)+\sum_{i=1}^{m}\frac{1}{s-f_{i}(x)}\nabla f_{i}(x)+A^{T}\nu=0,
$$ 

where t is the parameter. If $(x,s)$ is on the central path and $s=0$ , then $x$ and $\nu$ satisfy 

$$
t\nabla f_{0}(x)+\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla f_{i}(x)+A^{T}\nu=0
$$ 

for $t=1/(M-f_{0}(x))$ . This means that $x$ is on the central path for the original optimization problem ( 11.1 ), with associated duality gap 

$$
m(M-f_{0}(x))\leq m(M-p^{\star}).
$$ 

# 11.4.2 Phase I via infeasible start Newton method 

We can also carry out the phase I stage using an infeasible start Newton method, applied to a modified version of the original problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b.}\end{array}}
$$ 

We first express the problem in the (obviously equivalent) form 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq s,\quad i=1,\ldots,m}\\ &{A x=b,\quad s=0,}\end{array}}
$$ 

with the additional variable $s\in\mathbf{R}$ . To start the barrier method, we use an infeasible start Newton method to solve 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad t^{(0)}f_{0}(x)-\sum_{i=1}^{m}\log(s-f_{i}(x))}\\ &{\mathrm{subject~to}\quad A x=b,\quad s=0.}\end{array}
$$ 

This can be initialized with any $x\,\in\,{\mathcal{D}}$ , and any $s\,>\,\operatorname*{max}_{i}\,f_{i}(x)$ . Provided the problem is strictly feasible, the infeasible start Newton method will eventually take an undamped step, and thereafter we will have $s=0$ , i.e. , $x$ strictly feasible. 

The same trick can be applied if a point in $\mathcal{D}$ , the common domain of the functions, is not known. We simply apply the infeasible start Newton method to the problem 

$$
\begin{array}{l l}{{\mathrm{minimize}}}&{{t^{(0)}f_{0}(x+z_{0})-\sum_{i=1}^{m}\log(s-f_{i}(x+z_{i}))}}\\ {{\mathrm{subject~to}}}&{{A x=b,\quad s=0,\quad z_{0}=0,\quad.\,.\,,\quad z_{m}=0}}\end{array}
$$ 

with variables $x$ , $z_{0},\ldots,z_{m}$ , and $s\in\mathbf{R}$ . We initialize $z_{i}$ so that $x+z_{i}\in\mathbf{dom}\,f_{i}$ . 

The main disadvantage of this approach to the phase I problem is that there is no good stopping criterion when the problem is infeasible; the residual simply fails to converge to zero. 

# 11.4.3 Examples 

We consider a family of linear feasibility problems, 

$$
A x\preceq b(\gamma)
$$ 

where $A\in\mathbf{R}^{50\times20}$ and $b(\gamma)=b+\gamma\Delta b$ . The problem data are chosen so that the inequalities are strictly feasible for $\gamma>0$ and infeasible for $\gamma<0$ . For $\gamma=0$ the problem is feasible but not strictly feasible. 

Figure 11.10 shows the total number of Newton steps required to find a strictly feasible point, or a certificate of infeasibility, for 40 values of $\gamma$ in $[-1,1]$ . We use the basic phase I method of § 11.4.1 , i.e. , for each value of $\gamma$ , we form the LP 

$$
{\begin{array}{l c l}{{\mathrm{minimize}}}&{s}\\ {{\mathrm{subject~to}}}&{A x\preceq b(\gamma)+s\mathbf{1}.}\end{array}}
$$ 

The barrier method is used with $\mu=10$ , and starting point $x=0$ , $s=-\operatorname*{min}_{i}b_{i}(\gamma)+$ 1. The method terminates when a point $(x,s)$ with $s<0$ is found, or a feasible solution $z$ of the dual problem 

$$
{\begin{array}{l l}{{\mathrm{maximize}}}&{-b(\gamma)^{T}z}\\ {{\mathrm{subject~to}}}&{A^{T}z=0}\\ &{\mathbf{1}^{T}z=1}\\ &{z\succeq0}\end{array}}
$$ 

is found with $-b(\gamma)^{T}z>0$ . 

The plot shows that when the inequalities are feasible, with some margin, it takes around 25 Newton steps to produce a strictly feasible point. Conversely, when the inequalities are infeasible, again with some margin, it takes around 35 steps to produce a certificate proving infeasibility. The phase I eﬀort increases as the set of inequalities approaches the boundary between feasible and infeasible, $i$ .e. , near zero. When is very near zero, so the inequalities are very near the $\gamma$ $\gamma$ boundary between feasible and infeasible, the number of steps grows substantially. Figure 11.11 shows the total number of Newton steps required for values of $\gamma$ near zero. The plots show an approximately logarithmic increase in the number of steps required to detect feasibility, or prove infeasibility, for problems very near the boundary between feasible and infeasible. 

This example is typical: The cost of solving a set of convex inequalities and linear equalities using the barrier method is modest, and approximately constant, as long as the problem is not very close to the boundary between feasibility and infeasibility. When the problem is very close to the boundary, the number of Newton steps required to find a strictly feasible point or produce a certificate of infeasibility grows. When the problem is exactly on the boundary between strictly feasible and infeasible, for example, feasible but not strictly feasible, the cost becomes infinite. 

# Feasibility using infeasible start Newton method 

We also solve the same set of feasibility problems using the infeasible start Newton method, applied to the problem 

$$
\begin{array}{l c l}{{\mathrm{minimize}}}&{{-\sum_{i=1}^{m}\log s_{i}}}\\ {{\mathrm{subject~to}}}&{{A x+s=b(\gamma).}}\end{array}
$$ 

We use backtracking parameters $\alpha\,=\,0.01$ , $\beta\,=\,0.9$ , and initialize with $x^{(0)}\,=\,0$ , $s^{(0)}=\mathbf{1}$ , $\nu^{(0)}=0$ . We consider only feasible problems ( i.e. , $\gamma>0$ ) and terminate once a feasible point is found. (We do not consider infeasible problems, since in that case the residual simply converges to a positive number.) Figure 11.12 shows the number of Newton steps required to find a feasible point, as a function of $\gamma$ . 

![](images/062bfd017ba2f206b01482480ab343ad0cd1cca0a80a9e8b36391932ada0f0b4.jpg) 
Figure 11.10 Number of Newton iterations required to detect feasibility or bility of a set of linear inequalities $A x\,\preceq\,b+\gamma\Delta b$ parametrized by $\gamma\in\mathbf{R}$ ∈ . The inequalities are strictly feasible for $\gamma\,>\,0$ 0, and infeasible for $\gamma<0$ . For $\gamma$ larger than around 0 . 2, about 30 steps are required to compute a strictly feasible point; for $\gamma$ less than $-0.5$ or so, it takes around 35 steps to produce a certificate proving infeasibility. For values of $\gamma$ in between, and especially near zero, more Newton steps are required to determine feasibility. 

![](images/d99635b15dc9fe58de0ee7089211c5a765b4c592de58c702513e3f35645d6bf6.jpg) 
Figure 11.11 Left. Number of Newton iterations required to find a proof of infeasibility versus $\gamma$ , for $\gamma$ small and negative. Right. Number of Newton iterations required to find a strictly feasible point versus $\gamma$ , for $\gamma$ small and positive. 

![](images/cb80578592ebda59c2de8969f1b6583ed04f86cecbec9ab17379670420bc8a90.jpg) 
Figure 11.12 Number of iterations required to find a feasible point for a set of linear inequalities $A x\preceq b+\gamma\Delta b$ parametrized by $\gamma\in\mathbf{R}$ . The infeasible start Newton method is used, and terminated when a feasible point is found. $x^{(0)}=0$ For $\gamma=10$ , the starting point happened to be feasible ( $0$ iterations). 

The plot shows that for $\gamma$ larger than 0 . 3 or so, it takes fewer than 20 Newton steps to find a feasible point. In these cases the method is more efficient than a phase I method, which takes a total of around 30 Newton steps. For smaller values of $\gamma$ , the number of Newton steps required grows dramatically, approximately as $1/\gamma$ . For $\gamma=0.01$ , the infeasible start Newton method requires several thousand iterations to produce a feasible point. In this region the phase I approach is far more efficient, requiring only 40 iterations or so. 

These results are quite typical. The infeasible start Newton method works very well provided the inequalities are feasible, and not very close to the boundary between feasible and infeasible. But when the feasible set is just barely nonempty (as is the case in this example with small $^{'}\gamma$ ), a phase I method is far better. Another advantage of the phase I method is that it gracefully handles the infeasible case; the infeasible start Newton method, in contrast, simply fails to converge. 

# 11.5 Complexity analysis via self-concordance 

Using the complexity analysis of Newton’s method for self-concordant functions ( § 9.6.4 , page 503 , and § 10.2.4 , page 531 ), we can give a complexity analysis of the barrier method. The analysis applies to many common problems, and leads to several interesting conclusions: It gives a rigorous bound on the total number of Newton steps required to solve a problem using the barrier method, and it justifies our observation that the centering problems do not become more difficult as $t$ increases. 

# 11.5.1 Self-concordance assumption 

We make two assumptions. 

• The function $t f_{0}+\phi$ is closed and self-concordant for all $t\geq t^{(0)}$ The sublevel sets of ( 11.1 ) are bounded. 

The second assumption implies that the centering problem has bounded sublevel sets (see exercise 11.3 ), and, therefore, the centering problem is solvable. The bounded sublevel set assumption also implies that the Hessian of $t f_{0}+\phi$ is positive definite everywhere (see exercise 11.14 ). While the self-concordance assumption restricts the complexity analysis to a particular class of problems, it is important to emphasize that the barrier method works well in general, whether or not the self-concordance assumption holds. 

The self-concordance assumption holds for a variety of problems, including all linear and quadratic problems. If the functions $f_{i}$ are linear or quadratic, then 

$$
t f_{0}-\sum_{i=1}^{m}\log(-f_{i})
$$ 

is self-concordant for all values of $t\geq0$ (see § 9.6 ). The complexity analysis given below therefore applies to LPs, QPs, and QCQPs. 

In other cases, it is possible to reformulate the problem so the assumption of self-concordance holds. As an example, consider the linear inequality constrained entropy maximization problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{F x\preceq g}\\ &{A x=b.}\end{array}}
$$ 

The function 

$$
t f_{0}(x)+\phi(x)=t\sum_{i=1}^{n}x_{i}\log x_{i}-\sum_{i=1}^{m}\log(g_{i}-f_{i}^{T}x),
$$ 

where $f_{1}^{T},\dots,f_{m}^{T}$ are the rows of $F$ , is not closed (unless $F x\preceq g$ implies $x\succeq0$ ), or self-concordant. We can, however, add the redundant inequality constraints $x\succeq0$ to obtain the equivalent problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\sum_{i=1}^{n}x_{i}\log x_{i}}\\ {{\mathrm{subject~to}}}&{F x\preceq g}\\ &{A x=b}\\ &{x\succeq0.}\end{array}}
$$ 

For this problem we have 

$$
t f_{0}(x)+\phi(x)=t\sum_{i=1}^{n}x_{i}\log x_{i}-\sum_{i=1}^{n}\log x_{i}-\sum_{i=1}^{m}\log(g_{i}-f_{i}^{T}x),
$$ 

which is self-concordant and closed, for any $t\geq0$ . (The function $\,t y\log y-\log y$ is self-concordant on $\mathbf{R}_{++}$ , for all $t~\geq~0$ ; see exercise 11.13 .) The complexity analysis therefore applies to the reformulated linear inequality constrained entropy maximization problem ( 11.23 ). 

As a more exotic example, consider the GP 

$$
\begin{array}{r l}&{\mathrm{minimize}\:\:\:\:\:\:f_{0}(x)=\log\left(\sum_{k=1}^{K_{0}}\exp(a_{0k}^{T}x+b_{0k})\right)}\\ &{\mathrm{subject~to}\:\:\:\log\left(\sum_{k=1}^{K_{i}}\exp(a_{i k}^{T}x+b_{i k})\right)\le0,\quad i=1,\ldots,m.}\end{array}
$$ 

It is not clear whether or not the function 

$$
t f_{0}(x)+\phi(x)=t\log\left(\sum_{k=1}^{K_{0}}\exp(a_{0k}^{T}x+b_{0k})\right)-\sum_{i=1}^{m}\log\left(-\log\sum_{k=1}^{K_{i}}\exp(a_{i k}^{T}x+b_{i k})\right)
$$ 

is self-concordant, so although the barrier method works, the complexity analysis of this section need not hold. 

We can, however, reformulate the GP in a form that definitely satisfies the self- concordance assumption. For each (monomial) term $\exp(a_{i k}^{T}x+b_{i k})$ ) we introduce a new variable $y_{i k}$ that serves as an upper bound, 

$$
\exp(a_{i k}^{T}x+b_{i k})\leq y_{i k}.
$$ 

Using these new variables we can express the GP in the form 

$$
\begin{array}{l l}{\mathrm{minimize~}}&{\sum_{k=1}^{K_{0}}y_{0k}}\\ {\mathrm{subject~to}}&{\sum_{k=1}^{K_{i}}y_{i k}\leq1,\quad i=1,\ldots,m}\\ &{a_{i k}^{T}x+b_{i k}-\log y_{i k}\leq0,\quad i=0,\ldots,m,\quad k=1,\ldots,K_{i}}\\ &{y_{i k}\geq0,\quad i=0,\ldots,m,\quad k=1,\ldots,K_{i}.}\end{array}
$$ 

The associated logarithmic barrier is 

$$
\sum_{i=0}^{m}\sum_{k=1}^{K_{i}}\left(-\log y_{i k}-\log(\log y_{i k}-a_{i k}^{T}x-b_{i k})\right)-\sum_{i=1}^{m}\log\left(1-\sum_{k=1}^{K_{i}}y_{i k}\right),
$$ 

which is closed and self-concordant (example 9.8 , page 500 ). Since the objective is linear, it follows that $t f_{0}+\phi$ is closed and self-concordant for any $t$ . 

# 11.5.2 Newton iterations per centering step 

The complexity theory of Newton’s method for self-concordant functions, developed in § 9.6.4 (page 503 ) and § 10.2.4 (page 531 ), shows that the number of Newton iterations required to minimize a closed strictly convex self-concordant function $f$ is bounded above by 

$$
\frac{f(x)-p^{\star}}{\gamma}+c.
$$ 

Here $x$ is the starting point for Newton’s method, and $p^{\star}=\operatorname*{inf}_{x}f(x)$ is the optimal value. The constant $\gamma$ depends only on the backtracking parameters $\alpha$ and $\beta$ , and is given by 

$$
\frac{1}{\gamma}=\frac{20-8\alpha}{\alpha\beta(1-2\alpha)^{2}}.
$$ 

The constant $c$ depends only on the tolerance , $\epsilon_{\mathrm{nt}}$ 

$$
c=\log_{2}\log_{2}(1/\epsilon_{\mathrm{nt}}),
$$ 

and can reasonably be approximated as $c=6$ . The expression ( 11.24 ) is a quite conservative bound on the number of Newton steps required, but our interest in this section is only to establish a complexity bound, concentrating on how it increases with problem size and algorithm parameters. 

In this section we use this result to derive a bound on the number of Newton steps required for one outer iteration of the barrier method, i.e. , for computing $x^{\star}(\mu t)$ , starting from $x^{\star}(t)$ . To lighten the notation we use $x$ to denote $x^{\star}(t)$ , the current iterate, and we use $x^{+}$ to denote $x^{\star}(\mu t)$ , the next iterate. We use $\lambda$ and $\nu$ to denote $\lambda^{\star}(t)$ and $\nu^{\star}(t)$ , respectively. 

The self-concordance assumption implies that 

$$
\frac{\mu t f_{0}(x)+\phi(x)-\mu t f_{0}(x^{+})-\phi(x^{+})}{\gamma}+c
$$ 

is an upper bound on the number of Newton steps required to compute $x^{+}=x^{\star}(\mu t)$ , starting at $x\,=\,x^{\star}(t)$ . Unfortunately we do not know $x^{+}$ , and hence the upper bound ( 11.25 ), until we actually compute $x^{+}$ , i.e. , carry out the Newton algorithm (whereupon we know the exact number of Newton steps required to compute $x^{\star}(\mu t)$ , which defeats the purpose). We can, however, derive an upper bound on ( 11.25 ), as follows: 

$$
\begin{array}{r l}{\lefteqn{\mu t f_{0}(x)+\phi(x)-\mu t f_{0}(x^{+})-\phi(x^{+})}}\\ {=}&{\mu t f_{0}(x)-\mu t f_{0}(x^{+})+\displaystyle\sum_{i=1}^{m}\log(-\mu t\lambda_{i}f_{i}(x^{+}))-m\log\mu}\\ {\leq}&{\mu t f_{0}(x)-\mu t f_{0}(x^{+})-\mu t\displaystyle\sum_{i=1}^{m}\lambda_{i}f_{i}(x^{+})-m-m\log\mu}\\ {=}&{\mu t f_{0}(x)-\mu t\left(f_{0}(x^{+})+\displaystyle\sum_{i=1}^{m}\lambda_{i}f_{i}(x^{+})+\nu^{T}(A x^{+}-b)\right)-m-m\log\mu}\\ {\leq}&{\mu t f_{0}(x)-\mu t g(\lambda,\nu)-m-m\log\mu}\\ {=}&{m(\mu-1-\log\mu).}\end{array}
$$ 

This chain of equalities and inequalities needs some explanation. To obtain the second line from the first, we use $\lambda_{i}=-1/(t f_{i}(x))$ . In the first inequality we use the t $\log a\leq a-1$ for $a>0$ . To obtain the fourth line from the third, we use $A x^{+}=b$ , so the extra term $\nu^{T}(A x^{+}-b)$ is zero. The second inequality follows 

![](images/265f3fc842545b6982a215974608d968a70dceb19a0a932540163c0f279aaea3.jpg) 
Figure 11.13 The function $\mu-1-\log\mu$ , versus $\mu$ . The number of Newton steps required for one outer iteration of the barrier method is bounded by $(m/\gamma)(\mu-1-\log\mu)+c$ . 

from the definition of the dual function: 

$$
\begin{array}{r c l}{g(\lambda,\nu)}&{=}&{\displaystyle\operatorname*{inf}_{z}\left(f_{0}(z)+\sum_{i=1}^{m}\lambda_{i}f_{i}(z)+\nu^{T}(A z-b)\right)}\\ &{\leq}&{\displaystyle f_{0}(x^{+})+\sum_{i=1}^{m}\lambda_{i}f_{i}(x^{+})+\nu^{T}(A x^{+}-b).}\end{array}
$$ 

The last line follows from $g(\lambda,\nu)=f_{0}(x)-m/t$ . 

The conclusion is that 

$$
\frac{m(\mu-1-\log\mu)}{\gamma}+c
$$ 

is an upper bound on ( 11.25 ), and therefore an upper bound on the number of Newton steps required for one outer iteration of the barrier method. The function $\mu-1-\log\mu$ is shown in figure 11.13 . For small $\mu$ it is approximately quadratic; for large $\mu$ it grows approximately linearly. This fits with our intuition that for $\mu$ near one, the number of Newton steps required to center is small, whereas for large $\mu$ , it could well grow. 

The bound ( 11.26 ) shows that the number of Newton steps required in each centering step is bounded by a quantity that depends mostly on $\mu$ , the factor by which $t$ is updated in each outer step of the barrier method, and $m$ , the number of inequality constraints in the problem. It also depends, weakly, on the parameters $\alpha$ and $\beta$ used in the line search for the inner iterations, and in a very weak way on the tolerance used to terminate the inner iterations. It is interesting to note that the bound does not depend on $n$ , the dimension of the variable, or $p$ , the number of equality constraints, or the particular values of the problem data, i.e. , the objective and constraint functions (provided the self-concordance assumption in $\S$ 11.5.1 holds). Finally, we note that it does not depend on $t$ ; in particular, as $t\to\infty$ , a uniform bound on the number of Newton steps per outer iteration holds. 

# 11.5.3 Total number of Newton iterations 

We can now give an upper bound on the total number of Newton steps in the barrier method, not counting the initial centering step (which we will analyze later, as part of phase I). We multiply ( 11.26 ), which bounds the number of Newton steps per outer iteration, by ( 11.13 ), the number of outer steps required, to obtain 

$$
N=\left\lceil\frac{\log(m/(t^{(0)}\epsilon))}{\log\mu}\right\rceil\left(\frac{m(\mu-1-\log\mu)}{\gamma}+c\right),
$$ 

an upper bound on the total number of Newton steps required. This formula shows that when the self-concordance assumption holds, we can bound the number of Newton steps required by the barrier method, for any value of $\mu>1$ . 

If we fix and , the bound $N$ is proportional to $\log(m/(t^{(0)}\epsilon))$ , which is the $\mu$ $m$ log of the ratio of the initial duality gap $m/t^{(0)}$ to the final duality gap $\epsilon$ , i.e. , the log of the required duality gap reduction. We can therefore say that the barrier method converges at least linearly, since the number of steps required to reach a given precision grows logarithmically with the inverse of the precision. 

If , and the required duality gap reduction factor, are fixed, the bound $N$ grows $\mu$ linearly with $m$ , the number of inequalities. The bound $N$ is independent of the other problem dimensions $n$ and $p$ , and the particular problem data or functions. We will see below that by a particular choice of $\mu$ , that depends on $m$ , we can obtain a bound on the number of Newton steps that grows only as $\sqrt{m}$ , instead of as $m$ . 

Finally, we analyze the bound $N$ as a function of the algorithm parameter . As approaches one, the first term in $N$ grows large, and therefore so does $\mu$ $\mu$ $N$ . This is consistent with our intuition and observation that for near one, the $\mu$ number of outer iterations is very large. As becomes large, the bound $N$ grows $\mu$ approximately as $\mu/\log\mu$ , this time because the bound on the number of Newton iterations required per outer iteration grows. This, too, is consistent with our observations. As a result, the bound $N$ has a minimum value as a function of . $\mu$ 

The variation of the bound with the parameter $\mu$ is illustrated in figure 11.14 , which shows the bound ( 11.27 ) versus $\mu$ for the values 

$$
c=6,\qquad\gamma=1/375,\qquad m/(t^{(0)}\epsilon)=10^{5},\qquad m=100.
$$ 

The bound is qualitatively consistent with intuition, and our observations: it grows very large as $\mu$ approaches one, and increases, more slowly, as $\mu$ becomes large. The bound $N$ has a minimum at $\mu\approx1.02$ , which gives a bound on the total number of Newton iterations around 8000. The complexity analysis of Newton’s method is conservative, but the basic trade-oﬀin the choice of $\mu$ is reﬂected in the plot. (In practice, far larger values of $\mu$ , from around 2 to 100, work very well, and require a total number of Newton iterations on the order of a few tens.) 

# Choosing $\mu$ as a function of $m$ 

When $\mu$ (and the required duality gap reduction) is fixed, the bound ( 11.27 ) grows linearly with $m$ , the number of inequalities. It turns out we can obtain a better 

![](images/f99a2bcfc364f027ec4e753d362c176c1a35f02b1aea95c96edd5fa5fff9f74a.jpg) 
Figure 11.14 The upper bound $N$ on the total number of Newton iterations, given by equation ( 11.27 ), for $c=6$ , $\gamma=1/375$ , $m=100$ , and a duality gap reduction factor $m/(t^{(0)}\epsilon)=10^{5}$ , versus the barrier algorithm parameter $\mu$ . 

exponent for $m$ by making $\mu$ a function of $m$ . Suppose we choose 

$$
\mu=1+1/{\sqrt{m}}.
$$ 

Then we can bound the second term in ( 11.27 ) as 

$$
\begin{array}{l c l}{\mu-1-\log\mu}&{=}&{1/\sqrt{m}-\log(1+1/\sqrt{m})}\\ &{\leq}&{1/\sqrt{m}-1/\sqrt{m}+1/(2m)}\\ &{=}&{1/(2m)}\end{array}
$$ 

(using $-\log(1+a)\leq-a+a^{2}/2$ for $a\geq0$ ). Using concavity of the logarithm, we also have 

$$
\log\mu=\log(1+1/{\sqrt{m}})\geq(\log2)/{\sqrt{m}}.
$$ 

Using these inequalities we can bound the total number of Newton steps by 

$$
\begin{array}{r c l}{N}&{\leq}&{\left\lceil\frac{\log(m/(t^{(0)}\epsilon))}{\log\mu}\right\rceil\left(\frac{m(\mu-1-\log\mu)}{\gamma}+c\right)}\\ &{\leq}&{\left\lceil\sqrt{m}\frac{\log(m/(t^{(0)}\epsilon))}{\log2}\right\rceil\left(\frac{1}{2\gamma}+c\right)}\\ &{=}&{\left\lceil\sqrt{m}\log_{2}(m/(t^{(0)}\epsilon))\right\rceil\left(\frac{1}{2\gamma}+c\right)}\\ &{\leq}&{c_{1}+c_{2}\sqrt{m},}\end{array}
$$ 

where 

$$
c_{1}=\frac{1}{2\gamma}+c,\qquad c_{2}=\log_{2}(m/(t^{(0)}\epsilon))\left(\frac{1}{2\gamma}+c\right).
$$ 

Here $c_{1}$ depends (and only weakly) on algorithm parameters for the centering Newton steps, and $c_{2}$ depends on these and the required duality gap reduction. Note that the term $\log_{2}(m/(t^{(0)}\epsilon))$ is exactly the number of bits of required duality gap reduction. 

For xed duality gap reduction, the bound ( 11.29 ) grows as $\sqrt{m}$ , whereas the bound N in ( 11.27 ) grows like $m$ , if the parameter $\mu$ is held constant. For this reason the barrier method, with parameter value ( 11.28 ), is said to be an order $\sqrt{m}$ method. 

In practice, we would not use the value $\mu=1+1/\sqrt{m}$ , which is far too small, or even decrease $\mu$ as a function of $m$ . Our only interest in this value of $\mu$ is that it (approximately) minimizes our (very conservative) upper bound on the number of Newton steps, and yields an overall estimate that grows as $\sqrt{m}$ , instead of $m$ . 

# 11.5.4 Feasibility problems 

In this section we analyze the complexity of a (minor) variation on the basic phase I method described in 11.4.1 , used to solve a set of convex inequalities, 

$$
f_{1}(x)\leq0,\quad.\ldots,\quad f_{m}(x)\leq0,
$$ 

where $f_{1},\ldots,f_{m}$ are convex, with continuous second derivatives. (We will consider equality constraints later.) We assume that the phase I problem 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{s}\\ &{{\mathrm{subject~to}}}&{f_{i}(x)\leq s,\quad i=1,.\,.\,,m}\end{array}}
$$ 

satisfies the conditions in § 11.5.1 . In particular we assume that the feasible set of the inequalities ( 11.30 ) (which of course can be empty) is contained in a Euclidean ball of radius $R$ : 

$$
\{x\mid f_{i}(x)\leq0,\ i=1,\ldots,m\}\subseteq\{x\mid\|x\|_{2}\leq R\}.
$$ 

We can interpret $R$ as a prior bound on the norm of any point in the feasible set of the inequalities. This assumption implies that the sublevel sets of the phase I prob- lem are bounded. Without loss of generality, we will start the phase I method at the point $x=0$ . We define $F=\operatorname*{max}_{i}{f_{i}(0)}$ , which is the maximum constraint violation, assumed to be positive (since otherwise $x=0$ satisfies the inequalities ( 11.30 )). 

We define $\bar{p}^{\star}$ as the optimal value of the phase I optimization problem ( 11.31 ). The sign of $\bar{p}^{\star}$ ⋆ determines whether or not the set of inequalities ( 11.30 ) is feasible. The magnitude of $\bar{p}^{\star}$ also has a meaning. If $\bar{p}^{\star}$ is positive and large (say, near $F$ , the largest value it can have) it means that the set of inequalities is quite infeasible, in the sense that for each $x$ , at least one of the inequalities is substantially violated (by at least $\bar{p}^{\star}$ ). On the other hand, if $\bar{p}^{\star}$ is negative and large, it means that the set of inequalities is quite feasible, in the sense that there is not only an $x$ for which $f_{i}(x)$ are all nonpositive, but in fact there is an $x$ for which $f_{i}(x)$ are all quite negative (no more than $\bar{p}^{\star}$ ). Thus, the magnitude $|\bar{p}^{\star}|$ | is a measure of how clearly the set of inequalities is feasible or infeasible, and therefore related to the difficulty of determining feasibility of the inequalities ( 11.30 ). In particular, if $|\bar{p}^{\star}|$ | is small, it means the problem is near the boundary between feasibility and infeasibility. 

To determine feasibility of the inequalities, we use a variation on the basic phase I problem ( 11.31 ). We add a redundant linear inequality $a^{T}x\leq1$ , to obtain 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad s}\\ &{{\mathrm{subject~to}}\quad f_{i}(x)\leq s,\quad i=1,\ldots,m}\\ &{\ a^{T}x\leq1.}\end{array}}
$$ 

specify $a$ later. Our choice will satisfy $\|a\|_{2}\leq1/R$ , so $\|x\|_{2}\leq R$ implies $a^{T}x\leq1$ ≤ 1, i.e. , the extra constraint is ant. 

We will choose $a$ and $s_{0}$ so that x = 0, $s\,=\,s_{0}$ is on the central path of the problem ( 11.32 ), with a parameter value $t^{(0)}$ , i.e. , they minimize 

$$
t^{(0)}s-\sum_{i=1}^{m}\log(s-f_{i}(x))-\log(1-a^{T}x).
$$ 

Setting to zero the derivative with respect to $s$ , we get 

$$
t^{(0)}=\sum_{i=1}^{m}{\frac{1}{s_{0}-f_{i}(0)}}.
$$ 

Setting to zero the gradient with respect to $x$ yields 

$$
a=-\sum_{i=1}^{m}{\frac{1}{s_{0}-f_{i}(0)}}\nabla f_{i}(0).
$$ 

So it remains only to pick the parameter $s_{0}$ ; once we have chosen $s_{0}$ , the vector $a$ is given by ( 11.34 ), and the parameter $t^{(0)}$ is given by ( 11.33 ). Since $x\,=\,0$ and $s\,=\,s_{0}$ must be strictly feasible for the phase I problem ( 11.32 ), we must choose $s_{0}>F$ . 

We must also pick $s_{0}$ to make sure that $||a||_{2}\leq1/R$ . From ( 11.34 ), we have 

$$
\|a\|_{2}\leq\sum_{i=1}^{m}\frac{1}{s_{0}-f_{i}(0)}\|\nabla f_{i}(0)\|\leq\frac{m G}{s_{0}-F},
$$ 

$G=\mathrm{max}_{i}\,\|\nabla f_{i}(0)\|_{2}$ . Therefore we can take $s_{0}=m G R+F$ , which ensures $||a||_{2}\leq1/R$ ∥ ∥ , so the extra linear inequality is redundant. 

Using ( 11.33 ), we have 

$$
t^{(0)}=\sum_{i=1}^{m}{\frac{1}{m G R+F-f_{i}(0)}}\geq{\frac{1}{m G R}},
$$ 

since $F=\operatorname*{max}_{i}f_{i}(0)$ . Thus $x=0$ , are on the central path for the phase I $s=s_{0}$ problem ( 11.32 ), with initial duality gap 

$$
\frac{m+1}{t^{(0)}}\leq(m+1)m G R.
$$ 

To solve the original inequalities ( 11.30 ) we need to determine the sign of $\bar{p}^{\star}$ ⋆ . We can stop when either the primal objective value of ( 11.32 ) is negative, or the dual objective value is positive. One of these two cases must occur when the duality gap for ( 11.32 ) is less than $|\bar{p}^{\star}|$ . 

We use the barrier method to solve ( 11.32 ), starting from a central point with duality gap no more than $(m+1)m G R$ , and terminating when (or before) the duality gap is less than $|\bar{p}^{\star}|$ | . Using the results of the previous section, this requires no more than 

$$
\left\lceil\sqrt{m+1}\log_{2}\frac{m(m+1)G R}{|\bar{p}^{\star}|}\right\rceil\left(\frac{1}{2\gamma}+c\right)
$$ 

Newton steps. (Here we take $\mu=1+1/\sqrt{m+1}$ , which gives a better complexity exponent for $m$ than a fixed value of $\mu$ .) 

The bound ( 11.35 ) grows only slightly faster than $\sqrt{m}$ , and depends weakly on the algorithm parameters used in the centering steps. It is approximately propor- tional to $\log_{2}((G R)/|\bar{p}^{\star}|)$ | ), which can be interpreted as a measure of how difficult the particular feasibility problem is, or how close it is to the boundary between feasibility and infeasibility. 

# Feasibility problems with equality constraints 

We can apply the same analysis to feasibility problems that include equality con- straints, by eliminating the equality constraints. This does not aﬀect the self- concordance of the problem, but it does mean that $G$ and $R$ refer to the reduced, or eliminated, problem. 

# 11.5.5 Combined phase I/phase II complexity 

In this section we give an end-to-end complexity analysis for solving the problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m}\\ &{A x=b}\end{array}}
$$ 

using (a variation on) the barrier method. First we solve the phase I problem 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{s}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq s,\quad i=1,\ldots,m}\\ &{f_{0}(x)\leq M}\\ &{A x=b}\\ &{a^{T}x\leq1,}\end{array}}
$$ 

which we assume satisfies the self-concordance and bounded sublevel set assump- tions of § 11.5.1 . Here we have added two redundant inequalities to the basic phase I problem. The constraint $f_{0}(x)\,\leq\,M$ is added to guarantee that the phase I cen- tral path intersects the ce ral path for phase II, as described in section § 11.4.1 (see ( 11.21 )). The number M is a prior bound on the optimal value of the problem. The second added constraint is the linear inequality $a^{T}x\,\leq\,1$ , where $a$ is chosen as described in $\S11.5.4$ . We use the barrie hod to solve this problem, with $\mu=1+1/\sqrt{m+2}$ , and the starting points x = 0, $s=s_{0}$ given in 11.5.4 . 

To either find a strictly feasible point, or determine the problem is infeasible, requires no more than 

$$
N_{\mathrm{I}}=\left\lceil\sqrt{m+2}\log_{2}\frac{(m+1)(m+2)G R}{|\bar{p}^{\star}|}\right\rceil\left(\frac{1}{2\gamma}+c\right)
$$ 

Newton steps, where $G$ and $R$ are as given in § 11.5.4 . If the problem is infe e we are done; if it is feasible, then we find a point in phase I, associated with s = 0, that lies on the central path of the phase II problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ f_{0}(x)}\\ &{{\mathrm{subject~to}}\quad f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{\ A x=b}\\ &{\ a^{T}x\leq1.}\end{array}}
$$ 

The associated initial duality gap of this initial point is no more than $(m+1)(M-$ $p^{*}$ ) (see ( 11.22 )). We assume the phase II problem also satisfies the the self- concordance and bounded sublevel set assumptions in §11.5.1. 

We now proceed to phase II, again using the barrier method. We must reduce the duality gap from its initial value, which is no more than $(m+1)(M-p^{*})$ , to some tolerance $\epsilon>0$ . This takes at most 

$$
N_{\mathrm{II}}=\left\lceil\sqrt{m+1}\log_{2}\frac{(m+1)(M-p^{\star})}{\epsilon}\right\rceil\left(\frac{1}{2\gamma}+c\right)
$$ 

Newton steps. 

The total number of Newton steps is therefore no more than $N_{\mathrm{I}}+N_{\mathrm{II}}$ . This bound grows with the number of inequalities $m$ approximately as $\sqrt{m}$ , and includes two terms that depend on the particular problem instance, 

$$
\log_{2}\frac{G R}{|\bar{p}^{\star}|},\qquad\log_{2}\frac{M-p^{\star}}{\epsilon}.
$$ 

# 11.5.6 Summary 

The complexity analysis given in this section is mostly of theoretical interest. In particular, we remind the reader that the choice $\mu=1+1/\sqrt{m}$ , discussed in this section, would be a very poor one to use in practice; its only advantage is that it results in a bound that grows like $\sqrt{m}$ instead of $m$ . Likewise, we do not recommend adding the redundant inequality $a^{T}x\leq1$ in practice. 

The actual bounds obtained from the analysis given here are far higher than the numbers of iterations actually observed. Even the order in the bound appears to be conservative. The best bounds on the number of Newton steps grow like $\sqrt{m}$ , whereas practical experience suggests that the number of Newton steps hardly grows at all with $m$ (or any other parameter, in fact). 

Still, it is comforting to know that when the self-concordance condition holds, we can give a uniform bound on the number of Newton steps required in each centering step of the barrier method. An obvious potential pitfall of the barrier method is the possibility that as $t$ grows, the associated centering problems might become more difficult, requiring more Newton steps. While practical experience suggests that this is not the case, the uniform bound bolsters our confidence that it cannot happen. 

Finally, we mention that it is not yet clear whether or not there is a practical advantage to formulating a problem so that the self-concordance condition holds. All we can say is that when the self-concordance condition holds, the barrier method will work well in practice, and we can give a worst case complexity bound. 

# 11.6 Problems with generalized inequalities 

In this section we show how the barrier method can be extended to problems with generalized inequalities. We consider the problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ f_{0}({\boldsymbol{x}})}\\ &{{\mathrm{subject~to}}\quad f_{i}({\boldsymbol{x}})\preceq_{K_{i}}0,\quad i=1,.\,.\,.\,,m}\\ &{\quad\quad\quad\quad A{\boldsymbol{x}}={\boldsymbol{b}},}\end{array}}
$$ 

where $f_{0}:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is convex, $f_{i}:\mathbf{R}^{n}\rightarrow\mathbf{R}^{k_{i}}$ , $i\,=\,1,\ldots,k$ , are $K_{i}$ -convex, and $K_{i}\subseteq\mathbf{R}^{k_{i}}$ are proper cones. As i e assume t functions $f_{i}$ are twice continuously diﬀerentiable, that $A\in\mathbf{R}^{p\times n}$ ∈ with rank $A=p$ , and that the problem is solvable. 

The KKT conditions for problem ( 11.38 ) are 

$$
\begin{array}{r c l}{A x^{\star}}&{{}=}&{b}\\ {f_{i}(x^{\star})}&{{}\preceq_{K_{i}}}&{{}0,\quad i=1,\ldots,m}\\ {\lambda_{i}^{\star}}&{{}\succeq_{K_{i}^{\star}}}&{{}0,\quad i=1,\ldots,m}\\ {\nabla f_{0}(x^{\star})+\sum_{i=1}^{m}D f_{i}(x^{\star})^{T}\lambda_{i}^{\star}+A^{T}\nu^{\star}}&{{}=}&{{}0}\\ {\lambda_{i}^{\star^{T}}f_{i}(x^{\star})}&{{}=}&{{}0,\quad i=1,\ldots,m,}\end{array}
$$ 

where $D f_{i}(x^{\star})\,\in\,\mathbf{R}^{k_{i}\times n}$ is the derivative of $f_{i}$ at $x^{\star}$ . We will assume that prob- lem ( 11.38 ) is strictly feasible, so the KKT conditions are necessary and sufficient conditions for optimality of $x^{\star}$ . 

The development of the method is parallel to the case with scalar constraints. Once we develop a generalization of the logarithm function that applies to general proper cones, we can define a logarithmic barrier function for the problem ( 11.38 ). From that point on, the development is essentially the same as in the scalar case. In particular, the central path, barrier method, and complexity analysis are very similar. 

# 11.6.1 Logarithmic barrier and central path 

# Generalized logarithm for a proper cone 

We first define the analog of the logarithm, $\log x$ , for a proper cone $K\subseteq\mathbf{R}^{q}$ . We say that $\psi:\mathbf{R}^{q}\rightarrow\mathbf{R}$ is a generalized logarithm for $K$ if 

• $\psi$ is concave, closed, twice continuously diﬀerentiable, $\mathbf{dom}\,\psi=\mathbf{int}\,K$ , and $\nabla^{2}\psi(y)\prec0$ for $y\in\mathbf{int}\,K$ . 

• There is a constant $\theta>0$ such that for all $y\succ_{K}$ 0, and all $s>0$ , 

$$
\psi(s y)=\psi(y)+\theta\log s.
$$ 

In other words, $\psi$ behaves like a logarithm along any ray in the cone $K$ . 

We call the constant $\theta$ the degree of $\psi$ (since $\exp\psi$ is a homogeneous function of degree $\theta$ ). Note that a generalized logarithm is only defined up to an additive constant; if $\psi$ is a generalized logarithm for $K$ , then so is $\psi+a$ , where $a\in\mathbf{R}$ . The ordinary logarithm is, of course, a generalized logarithm for $\mathbf{R}_{+}$ . 

We will use the following two properties, which are satisfied by any generalized logarithm: If $y\succ_{K}0$ , then 

$$
\begin{array}{r}{\nabla\psi(y)\succ_{K^{*}}0,}\end{array}
$$ 

which implies $\psi$ is $K$ -increasing (see § 3.6.1 ), and 

$$
\boldsymbol{y}^{T}\nabla\boldsymbol{\psi}(\boldsymbol{y})=\boldsymbol{\theta}.
$$ 

The first property is proved in exercise 11.15 . The second property follows imme- diately from diﬀerentiating $\psi(s y)=\psi(y)+\theta\log s$ with respect to $s$ . 

Example 11.5 ative orthant. he fu $\begin{array}{r}{\psi({x})=\sum_{i=1}^{n}\log x_{i}}\end{array}$ is a generalized logarithm for $K=\mathbf{R}_{+}^{n}$ , with degree n . For x $x\succ0$ ≻ 0, 

$$
\nabla\psi(x)=(1/x_{1},.\,.\,.\,,1/x_{n}),
$$ 

so $\nabla\psi({x})\succ0$ , and $\boldsymbol{x}^{T}\nabla\boldsymbol{\psi}(\boldsymbol{x})=n$ . 

Example 11.6 Second-order cone. The function 

$$
\psi({\boldsymbol{x}})=\log\left(x_{n+1}^{2}-\sum_{i=1}^{n}x_{i}^{2}\right)
$$ 

is a generalized logarithm for the second-order cone 

$$
K=\left\{x\in\mathbf{R}^{n+1}\;\left|\;\left(\sum_{i=1}^{n}x_{i}^{2}\right)^{1/2}\leq x_{n+1}\right.\right\},
$$ 

with degree 2. The gradient of $\psi$ at a point $x\in\mathbf{int}\,K$ is given by 

$$
\begin{array}{r c l}{\displaystyle\frac{\partial\psi(x)}{\partial x_{j}}}&{=}&{\displaystyle\frac{-2x_{j}}{\left(x_{n+1}^{2}-\sum_{i=1}^{n}x_{i}^{2}\right)},\quad j=1,\ldots,n}\\ {\displaystyle\frac{\partial\psi(x)}{\partial x_{n+1}}}&{=}&{\displaystyle\frac{2x_{n+1}}{\left(x_{n+1}^{2}-\sum_{i=1}^{n}x_{i}^{2}\right)}.}\end{array}
$$ 

The identities $\nabla\psi(x)\in\mathbf{int}\,K^{*}=\mathbf{int}\,K$ and $\boldsymbol{x}^{T}\nabla\psi(\boldsymbol{x})=2$ are easily verified. 

Example 11.7 Positive semidefinite cone. The function $\psi(X)=\log\operatorname*{det}X$ is a gen- eralized logarithm for the cone $\mathbf{S}_{+}^{p}$ . The degree is $p$ , since 

$$
\log\operatorname*{det}(s X)=\log\operatorname*{det}X+p\log s
$$ 

for $s>0$ . The gradient of $\psi$ at a point $X\in\mathbf{S}_{++}^{p}$ is equal to 

$$
\nabla\psi(X)=X^{-1}.
$$ 

Thus, we have $\nabla\psi(X)=X^{-1}\succ0$ , and the inner product of $X$ and $\nabla\psi(X)$ is equal to $\mathbf{tr}(X X^{-1})=p$ . 

# Logarithmic barrier functions for generalized inequalities 

Returning to problem ( 11.38 ), let $\psi_{1},.\,.\,.\,,\psi_{m}$ be generalized logarithms for the cones $K_{1},\ldots,K_{m}$ , respectively, with degrees $\theta_{1},\ldots,\theta_{m}$ . We define the logarithmic barrier function for problem ( 11.38 ) as 

$$
\phi(x)=-\sum_{i=1}^{m}\psi_{i}(-f_{i}(x)),\qquad\mathrm{dom}\,\phi=\{x\mid f_{i}(x)\prec_{K_{i}}0,\,i=1,\ldots,m\}.
$$ 

Convexity of $\phi$ follows from the fact that the functions $\psi_{i}$ are $K_{i}$ -increasing, and the functions $f_{i}$ are $K_{i}$ -convex (see the composition rule of 3.6.2 ). 

# The central path 

The next step is to define the central path for problem ( 11.38 ). We define the central point $x^{\star}(t)$ , for $t\geq0$ , as the minimizer of $t f_{0}+\phi$ , subject to $A x=b$ , i.e. , as the solution of 

$$
\begin{array}{l l}{\mathrm{minimize}}&{t f_{0}(\boldsymbol{x})-\sum_{i=1}^{m}\psi_{i}(-f_{i}(\boldsymbol{x}))}\\ {\mathrm{subject~to}}&{A\boldsymbol{x}=\boldsymbol{b}}\end{array}
$$ 

(assuming the minimizer exists, and is unique). Central points are characterized by the optimality condition 

$$
\begin{array}{r l r}{\lefteqn{t\nabla f_{0}(x)+\nabla\phi(x)+A^{T}\nu}}\\ &{=}&{\displaystyle{t\nabla f_{0}(x)+\sum_{i=1}^{m}D f_{i}(x)^{T}\nabla\psi_{i}(-f_{i}(x))+A^{T}\nu}=0,}\end{array}
$$ 

for some $\nu\in\mathbf{R}^{p}$ , where $D f_{i}(x)$ is the derivative of $f_{i}$ at $x$ . 

# Dual points on central path 

As in the scalar case, points on the central path give dual feasible points for the problem ( 11.38 ). For $i=1,\ldots,m$ , define 

$$
\lambda_{i}^{\star}(t)=\frac{1}{t}\nabla\psi_{i}(-f_{i}(x^{\star}(t))),
$$ 

and let $\nu^{\star}(t)\;=\;\nu/t$ , where $\nu$ is the optimal dual variable in ( 11.41 ). We will show that $\lambda_{1}^{\star}(t),\ldots,\lambda_{m}^{\star}(t)$ ), together with $\nu^{\star}(t)$ , are dual feasible for the original problem ( 11.38 ). 

First, $\lambda_{i}^{\star}(t)\succ_{K_{i}^{*}}0$ 0, by the monotonicity property ( 11.40 ) of generalized loga- rithms. Second, it follows from ( 11.41 ) that the Lagrangian 

$$
L(x,\lambda^{\star}(t),\nu^{\star}(t))=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}^{\star}(t)^{T}f_{i}(x)+\nu^{\star}(t)^{T}(A x-b)
$$ 

is minimized over $x$ by $x=x^{\star}(t)$ . The dual function $g$ evaluated at $(\lambda^{\star}(t),\nu^{\star}(t))$ is therefore equal to 

$$
\begin{array}{c c l}{g(\lambda^{\star}(t),\nu^{\star}(t))}&{=}&{f_{0}(x^{\star}(t))+\displaystyle\sum_{i=1}^{m}\lambda_{i}^{\star}(t)^{T}f_{i}(x^{\star}(t))+\nu^{\star}(t)^{T}(A x^{\star}(t)-b)}\\ &{=}&{f_{0}(x^{\star}(t))+(1/t)\displaystyle\sum_{i=1}^{m}\nabla\psi_{i}(-f_{i}(x^{\star}(t)))^{T}f_{i}(x^{\star}(t))}\\ &{=}&{f_{0}(x^{\star}(t))-(1/t)\displaystyle\sum_{i=1}^{m}\theta_{i},}\end{array}
$$ 

where $\theta_{i}$ is the degree of $\psi_{i}$ . In the last line, we use the fact that $y^{T}\nabla\psi_{i}(y)=\theta_{i}$ for $y\succ_{K_{i}}0$ , and therefore 

$$
\lambda_{i}^{\star}(t)^{T}f_{i}(x^{\star}(t))=-\theta_{i}/t,\quad i=1,.\,.\,,m.
$$ 

Thus, if we define 

$$
\overline{{\theta}}=\sum_{i=1}^{m}\theta_{i},
$$ 

then the primal feasible point $x^{\star}(t)$ and the dual feasible point $(\lambda^{\star}(t),\nu^{\star}(t))$ have duality gap $\overline{{\theta}}/t$ . This is just like the scalar case, except that $\overline{{\theta}}$ , the sum of the degrees of the generalized logarithms for the cones, appears in place of $m$ , the number of inequalities. 

Example 11.8 Second-order cone programming. We consider an SOCP with variable $x\in\mathbf{R}^{n}$ : 

$$
\begin{array}{l r}{\mathrm{minimize}\ }&{f^{T}x}\\ {\mathrm{subject~to}\ }&{\|A_{i}x+b_{i}\|_{2}\leq c_{i}^{T}x+d_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

where $A_{i}\in\mathbf{R}^{n_{i}\times n}$ . As we have seen in example 11.6 , the function 

$$
\psi(y)=\log\left(y_{p+1}^{2}-\sum_{i=1}^{p}y_{i}^{2}\right)
$$ 

is a generalized logarithm for the second-order cone in $\mathbf{R}^{p+1}$ , with degree 2. The corresponding logarithmic barrier function for ( 11.44 ) is 

$$
\phi(\boldsymbol{x})=-\sum_{i=1}^{m}\log((c_{i}^{T}\boldsymbol{x}+d_{i})^{2}-\|A_{i}\boldsymbol{x}+b_{i}\|_{2}^{2}),
$$ 

with $\mathbf{dom}\,\phi=\{x\ |\ \|A_{i}x+b_{i}\|_{2}<c_{i}^{T}x+d_{i},\ i=1,.\,.\,,m\}$ } . The optimality condition on the central path is $t f+\nabla\phi(x^{\star}(t))=0$ , where 

$$
\nabla\phi(x)=-2\sum_{i=1}^{m}\frac{1}{(c_{i}^{T}x+d_{i})^{2}-\|A_{i}x+b_{i}\|_{2}^{2}}\left((c_{i}^{T}x+d_{i})c_{i}-A_{i}^{T}(A_{i}x+b_{i})\right).
$$ 

It follows that the point 

$$
z_{i}^{\star}(t)=-\frac{2}{t\alpha_{i}}(A_{i}x^{\star}(t)+b_{i}),\qquad w_{i}^{\star}(t)=\frac{2}{t\alpha_{i}}(c_{i}^{T}x^{\star}(t)+d_{i}),\qquad i=1,\dots,m,
$$ 

where $\alpha_{i}=(c_{i}^{T}x^{\star}(t)+d_{i})^{2}-\|A_{i}x^{\star}(t)+b_{i}\|_{2}^{2}$ −∥ ∥ , is strictly feasible in the dual problem 

$$
\begin{array}{r l}{\mathrm{maximize}}&{\:-\sum_{i=1}^{m}\bigl(b_{i}^{T}z_{i}+d_{i}w_{i}\bigr)}\\ {\mathrm{subject~to}}&{\:\sum_{i=1}^{m}\bigl(A_{i}^{T}z_{i}+c_{i}w_{i}\bigr)=f}\\ &{\|z_{i}\|_{2}\leq w_{i},\quad i=1,.\,.\,,m.}\end{array}
$$ 

The duality gap associated with $x^{\star}(t)$ and $(z^{\star}(t),w^{\star}(t))$ is 

$$
\sum_{i=1}^{m}\left(\bigl(A_{i}x^{\star}(t)+b_{i}\bigr)^{T}z_{i}^{\star}(t)+\bigl(c_{i}^{T}x^{\star}(t)+d_{i}\bigr)w_{i}^{\star}(t)\right)=\frac{2m}{t},
$$ 

which agrees with the general formula $\overline{{\theta}}/t$ , since $\theta_{i}=2$ . 

Example 11.9 Semidefinite programming in inequality form. We consider the SDP with variable $x\in\mathbf{R}^{n}$ , 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{c^{T}x}\\ {{\mathrm{subject~to}}\quad F(x)=x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}+G\preceq0}\end{array}}
$$ 

where $G,F_{1},.\,.\,.\,,F_{n}\in\mathbf{S}^{p}$ . The dual problem is 

$$
{\begin{array}{r l}&{{\mathrm{maximize}}\quad\operatorname{\mathbf{tr}}(G Z)}\\ &{{\mathrm{subject~to}}\quad\operatorname{\mathbf{tr}}(F_{i}Z)+c_{i}=0,\quad i=1,\ldots,n}\\ &{\quad Z\succeq0.}\end{array}}
$$ 

Using the generalized logarithm $\log\operatorname*{det}X$ for the positive semidefinite cone $\mathbf{S}_{+}^{p}$ , we have the barrier function (for the primal problem) 

$$
\phi(x)=\log\operatorname*{det}(-F(x)^{-1})
$$ 

with $\mathbf{dom}\,\phi=\{x\mid F(x)\prec0\}$ . For strictly feasible $x$ , the gradient of $\phi$ is equal to 

$$
\frac{\partial\phi(x)}{\partial x_{i}}=\mathbf{tr}(-F(x)^{-1}F_{i}),\quad i=1,\ldots,n,
$$ 

which gives us the optimality conditions that characterize central points: 

$$
t c_{i}+\mathbf{tr}(-F(x^{\star}(t))^{-1}F_{i})=0,\quad i=1,.\,.\,.\,,n.
$$ 

Hence the matrix 

$$
Z^{\star}(t)=\frac{1}{t}\left(-F(x^{\star}(t))\right)^{-1}
$$ 

is strictly dual feasible, and the duality gap associated with $x^{\star}(t)$ and $Z^{\star}(t)$ is $p/t$ . 

# 11.6.2 Barrier method 

We have seen that the key properties of the central path generalize to problems with generalized inequalities. 

• Computing a point on the central path involves minimizing a twice diﬀer- entiable convex function subject to equality constraints (which can be done using Newton’s method). • With the central point $x^{\star}(t)$ we can associate a dual feasible point $(\lambda^{\star}(t),\nu^{\star}(t))$ with associated duality gap $\overline{{\theta}}/t$ . In particular, $x^{\star}(t)$ is no more than $\overline{{\theta}}/t$ - suboptimal. 

This means we can apply the barrier method, exactly as described in $\S11.3$ , to the problem ( 11.38 ). The number of outer iterations, or centering steps, required to compute a central point with duality gap $\epsilon$ starting at $x^{\star}(t^{(0)})$ is equal to 

$$
\left\lceil\frac{\log(\overline{{\theta}}/(t^{(0)}\epsilon))}{\log\mu}\right\rceil,
$$ 

plus one initial centering step. The only diﬀerence between this result and the associated one for the scalar case is that $\overline{{\theta}}$ takes the place of $m$ . 

# Phase I and feasibility problems 

The phase I methods described in § 11.4 are readily extended to problems with generalized inequalities. Let $e_{i}\ \succ_{K_{i}}\ 0$ be some given, $K_{i}$ -positive vectors, for $i=1,\ldots,m$ . To determine feasibility of the equalities and generalized inequalities 

$$
f_{1}(x)\preceq_{K_{1}}0,\quad\ldots,\quad f_{L}(x)\preceq_{K_{m}}0,\qquad A x=b,
$$ 

we solve the problem 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{s}}&{{}}\\ {{\mathrm{subject~to}}}&{{f_{i}(x)\preceq_{K_{i}}s e_{i},\quad i=1,\ldots,m}}\\ {{}}&{{A x=b,}}&{{}}\end{array}
$$ 

with variables $x$ and $s~\in~\mathbf{R}$ . The optimal value $\bar{p}^{\star}$ ⋆ determines the feasibility of the equalities and generalized inequalities, exactly as in the case of ordinary inequalities. When $\bar{p}^{\star}$ is positive, any dual feasible point with positive objective gives an alternative that proves the set of equalities and generalized inequalities is infeasible (see page 270 ). 

# 11.6.3 Examples 

# A small SOCP 

We solve an SOCP 

$$
\begin{array}{l}{\mathrm{minimize}\quad\ f^{T}x}\\ {\mathrm{subject~to}\quad\lVert A_{i}x+b_{i}\rVert_{2}\leq c_{i}^{T}x+d_{i},\quad i=1,.\,.\,,m,}\end{array}
$$ 

![](images/1e9daafaa4171326f5915f254601c5dfb658d2e33c19a98dcc3726243964e7f8.jpg) 
Figure 11.15 Progress of barrier method for an SOCP, showing duality gap versus cumulative number of Newton steps. 

with $\boldsymbol{x}\,\in\,\mathbf{R}^{50}$ , $m\,=\,50$ , and $A_{i}\,\in\,\mathbf{R}^{5\times50}$ . The problem instance was randomly generated, in such a way that the problem is strictly primal and dual feasible, and has optimal value $p^{\star}=1$ . We start with a point $x^{(0)}$ on the central path, with a duality gap of 100. 

The barrier method is used to solve the problem, using the barrier function 

$$
\phi(x)=-\sum_{i=1}^{m}\log\left((c_{i}^{T}x+d_{i})^{2}-\|A_{i}x+b_{i}\|_{2}^{2}\right).
$$ 

The centering problems are solved using Newton’s method, with the same algorithm parameters as in the examples of § 11.3.2 : backtracking parameters $\alpha=0.01$ , $\beta=$ 0 . 5, and a stopping criterion $\lambda(x)^{2}/2\leq10^{-5}$ . 

Figure 11.15 shows the duality gap versus cumulative number of Newton steps. The plot is very similar to those for linear and geometric programming, shown in figures 11.4 and 11.6 , respectively. We see an approximately constant number of Newton steps required per centering step, and therefore approximately linear convergence of the duality gap. For this example, too, the choice of $\mu$ has little eﬀect on the total number of Newton steps, provided $\mu$ is at least 10 or so. As in the examples for linear and geometric programming, a reasonable choice of $\mu$ is in the range $10-100$ , which results in a total number of Newton steps around 30 (see figure 11.16 ). 

# A small SDP 

Our next example is an SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad\sum_{i=1}^{n}x_{i}F_{i}+G\preceq0}\end{array}}
$$ 

![](images/85708621530ade354b49ddcea1c63d4449d844f238001feb2a25308e95307723.jpg) 
Figure 11.16 Trade-oﬀin the choice of the parameter $\mu$ , for a small SOCP. The vertical axis shows the total number of Newton steps required to reduce the duality gap from 100 to $10^{-3}$ , and the horizontal axis shows $\mu$ . 

with variable ${\boldsymbol{x}}\,\in\,\mathbf{R}^{100}$ , and $F_{i}\,\in\,{\bf S}^{100}$ , $G\,\in\,\mathbf{S}^{100}$ . The problem instance was generated randomly, in such a way that the problem is strictly primal and dual feasible, with $p^{\star}=1$ . The initial point is on the central path, with a duality gap of 100. 

We apply the barrier method with logarithmic barrier function 

$$
\phi(x)=-\log\operatorname*{det}\left(-\sum_{i=1}^{n}x_{i}F_{i}-G\right).
$$ 

The progress of the barrier method for three values of $\mu$ is shown in figure 11.17 . Note the similarity with the plots for linear, geometric, and second-order cone programming, shown in figures 11.4 , 11.6 , and 11.15 . As in the other examples, the parameter $\mu$ has only a small eﬀect on the efficiency, provided it is not too small. The number of Newton steps required to reduce the duality gap by a factor $10^{5}$ , versus $\mu$ , is shown in figure 11.18 . 

# A family of SDPs 

In this section we examine the performance of the barrier method as a function of the problem dimensions. We consider a family of SDPs of the form 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\mathbf{1}^{T}x}\\ &{{\mathrm{subject~to}}\quad A+\mathbf{diag}(x)\succeq0,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n}$ , and parameter $A\,\in\,\mathbf{S}^{n}$ . The matrices $A$ are generated as follows. For $i\geq j$ , the coefficients $A_{i j}$ are generated from indepen ent ${\mathcal{N}}(0,1)$ distributions. For $i<j$ , we set $A_{i j}=A_{j i}$ , so $A\in\mathbf{S}^{n}$ . We then scale A so that its (spectral) norm is one. 

![](images/276bd09c7d4ddb4f5a132be1eac94cd89b45b6069f79b9058b21d2f1d4a5bf6c.jpg) 
Figure 11.17 Progress of barrier method for a small SDP, showing duality gap versus cumulative number of Newton steps. Three plots are shown, corresponding to three values of the parameter $\mu$ : 2, 50, and 150. 

![](images/0d09df40a05e60430fbfb18437a72bda8e8a32777ec6caa5656d9309319f36c4.jpg) 
Figure 11.18 Trade-oﬀin the choice of the parameter $\mu$ , for a small SDP. The vertical axis shows the total number of Newton steps required to reduce the duality gap from 100 to $10^{-3}$ , and the horizontal axis shows $\mu$ . 

![](images/29045a08ebda90cba10ac6ffb0e44acacd7da34dd5c352dc34634925e5ab7ff7.jpg) 
Figure 11.19 Progress of barrier method for three randomly generated SDPs of the form ( 11.47 ), with diﬀerent dimensions. The plot shows duality gap versus cumulative number of Newton steps. The number of variables in each problem is $n$ . 

The algorithm parameters are $\mu=20$ , and the same parameters for the center- ing steps as in the examples above: backtracking parameters $\alpha\,=\,0.01$ , $\beta\,=\,0.5$ , and criterion $\lambda(x)^{2}/2\,\le\,10^{-5}$ . The initial point is on the central path with t $t^{(0)}\,=\,1$ = 1 ( i.e. , gap $n$ ). The algorithm is terminated when the initial duality gap is reduced by a factor 8000, i.e. , after completing three outer iterations. 

Figure 11.19 shows the duality gap versus iteration number for three problem instances, with dimensions $n=50$ , $n=500$ , and $n=1000$ . The plots look very much like the others, and very much like the ones for LPs. 

To examine the eﬀect of problem size on the number of Newton steps required, we generate 100 problem instances for each of 20 values of $n$ , ranging from $n=10$ to $n=1000$ . We solve each of these 2000 problems using the barrier method, noting the number of Newton steps required. The results are summarized in figure 11.20 , which shows the mean and standard deviation in the number of Newton steps, for each value of $n$ . The plot looks very much like the one for LPs, shown in figure 11.8 . In particular, the number of Newton steps required grows very slowly, from around 20 to 26 iterations, as the problem dimensions increase by a factor of 100. 

# 11.6.4 Complexity analysis via self-concordance 

In this section we extend the complexity analysis of the barrier method for problems with ordinary inequalities (given in § 11.5 ), to problems with generalized inequali- ties. We have already seen that the number of outer iterations is given by 

$$
\left\lceil\frac{\log(\overline{{\theta}}/t^{(0)}\epsilon)}{\log\mu}\right\rceil,
$$ 

![](images/5c335bb69def90ce40f2c68cc3931761fc8a8a43c4f660615f0825a7e900ab12.jpg) 
Figure 11.20 Average number of Newton steps required to solve 100 ran- domly generated SDPs ( 11.47 ) for each of 20 values of $n$ , the problem size. Error bars show standard deviation, around the average value, for each value of $n$ . The growth in the average number of Newton steps required, as the problem dimensions range over a $100\!:\!1$ ratio, is very small. 

plus one initial centering step. It remains to bound the number of Newton steps required in each centering step, which we will do using the complexity theory of Newton’s method for self-concordant functions. For simplicity, we will exclude the cost of the initial centering. 

We make the same ptions as in $\S11.5$ : The function $t f_{0}+\phi$ is closed and self-concordant for all t $t\geq t^{(0)}$ , and the sublevel sets of ( 11.38 ) are bounded. 

Example 11.10 Second-order cone programming. The function 

$$
-\psi(x)=-\log\left(x_{p+1}^{2}-\sum_{i=1}^{p}x_{i}^{2}\right),
$$ 

is self-concordant (see example 9.8 ), so the logarithmic barrier function ( 11.45 ) sat- isfies the closedness and self-concordance assumption for the SOCP ( 11.44 ). 

Example 11.11 Semidefinite programming. The self-concordance assumption holds for general semidefinite programs, using $\log\operatorname*{det}X$ as generalized logarithm for the positive semidefinite cone. For example, for the standard form SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\operatorname{\mathbf{tr}}(C X)}\\ &{{\mathrm{subject~to}}\quad\operatorname{\mathbf{tr}}(A_{i}X)=b_{i},\quad i=1,\ldots,p}\\ &{\quad X\succeq0,}\end{array}}
$$ 

with variable $X\,\in\,{\bf S}^{n}$ the function $t^{(0)}\,{\bf t r}(C X)-\log\operatorname*{det}X$ is self-concordant (and closed), for any t $t^{(0)}\geq0$ 0. 

We will see that, exactly as in the scalar case, we have 

$$
\mu t f_{0}(x^{\star}(t))+\phi(x^{\star}(t))-\mu t f_{0}(x^{\star}(\mu t))-\phi(x^{\star}(\mu t))\le\overline{{\theta}}(\mu-1-\log\mu).
$$ 

Therefore when the self-concordance and bounded sublevel set conditions hold, the number of Newton steps per centering step is no more than 

$$
{\frac{{\overline{{\theta}}}(\mu-1-\log\mu)}{\gamma}}+c,
$$ 

exactly as in the barrier method for problems with ordinary inequalities. Once we establish the basic bound ( 11.48 ), the complexity analysis for problems with generalized inequalities is identical to the analysis for problems with ordinary in- equalities, with one exception: $\overline{{\theta}}$ is the sum of the degrees of the cones, instead of the number of inequalities. 

# Generalized logarithm for dual cone 

We will use conjugates to prove the bound ( 11.48 ). Let $\psi$ be a generalized logarithm for the proper cone $K$ , with degree $\theta$ . The conjugate of the (convex) function $-\psi$ is 

$$
(-\psi)^{*}(v)=\operatorname*{sup}_{u}\big(v^{T}u+\psi(u)\big)\,.
$$ 

This function is convex, and has domain $-K^{*}=\{v\mid v\prec_{K^{*}}0\}$ . Define $\overline{{\psi}}$ by 

$$
{\overline{{\psi}}}(v)=-(-\psi)^{*}(-v)=\operatorname*{inf}_{u}\left(v^{T}u-\psi(u)\right),\qquad\operatorname{dom}{\overline{{\psi}}}=\operatorname{int}K^{*}.
$$ 

The function $\overline{{\psi}}$ is concave, and in fact is a generalized logarithm for the dual cone $K^{*}$ , with the same parameter $\theta$ (see exercise 11.17 ). We call $\overline{{\psi}}$ the dual logarithm associated with the generalized logarithm $\psi$ . 

From ( 11.49 ) we obtain the inequality 

$$
\begin{array}{r}{\overline{{\psi}}(v)+\psi(u)\leq u^{T}v,}\end{array}
$$ 

which holds for any $u\succ_{K}$ 0, $v\succ_{K^{*}}0$ , with equality holding if and only $\nabla\psi(u)=v$ (or equivalently, $\nabla\overline{{\psi}}(v)\;=\;u_{i}$ ). (This inequality is just a variation on Young’s inequality, for concave functions.) 

Example 11.12 Second-order cone. The second-order cone has generalized logarithm $\begin{array}{r}{\psi({\boldsymbol x})=\log(x_{p+1}^{2}-\sum_{i=1}^{p}x_{i}^{2})}\end{array}$ P ), with $\begin{array}{r}{\mathbf{dom}\,\psi=\{x\in\mathbf{R}^{p+1}\mid x_{p+1}>(\sum_{i=1}^{p}x_{i}^{2})^{1/2}\}}\end{array}$ } . The associated dual logarithm is 

$$
\overline{{\psi}}(y)=\log\left(y_{p+1}^{2}-\sum_{i=1}^{p}y_{i}^{2}\right)+2-\log4,
$$ 

with $\begin{array}{r}{\mathbf{dom}\,\psi\,=\,\{y\,\in\,\mathbf{R}^{p+1}\,\mid\,y_{p+1}\,>\,(\sum_{i=1}^{p}y_{i}^{2})^{1/2}\}}\end{array}$ } (see exercise 3.36 ). Except for a constant, it is the same as the original generalized logarithm for the second-order cone. 

Example 11.13 Positive semidefinite cone. The dual logarithm associated with $\psi(X)=\log\operatorname*{det}X$ , with $\mathbf{dom}\,\psi=\mathbf{S}_{++}^{\nu}$ , is 

$$
{\overline{{\psi}}}(Y)=\log\operatorname*{det}Y+p,
$$ 

with domain $\mathbf{\operatorname{dom}\psi^{*}}=\mathbf{S}_{++}^{p}$ (see example 3.23 ). Again, it is the same generalized logarithm, except for a constant. 

# Derivation of the basic bound 

To simplify notation, we denote $x^{\star}(t)$ as $x$ , $x^{\star}(\mu t)$ as $x^{+}$ , $\lambda_{i}^{\star}(t)$ ) as $\lambda_{i}$ , and $\nu^{\star}(t)$ as $\nu$ . From $t\lambda_{i}=\nabla\psi_{i}(-f_{i}(x))$ (in ( 11.42 )) and property ( 11.43 ), we conclude that 

$$
\psi_{i}(-f_{i}(x))+\overline{{{\psi}}}_{i}(t\lambda_{i})=-t\lambda_{i}^{T}f_{i}(x)=\theta_{i},
$$ 

i.e. , the inequality ( 11.50 ) holds with equality for the pair $u=-f_{i}(x)$ and $v=t\lambda_{i}$ . The same inequality for the pair $u=-f_{i}(x^{+})$ , $v=\mu t\lambda_{i}$ gives 

$$
\psi_{i}(-f_{i}(x^{+}))+\overline{{\psi}}_{i}(\mu t\lambda_{i})\leq-\mu t\lambda_{i}^{T}f_{i}(x^{+}),
$$ 

which becomes, using logarithmic homogeneity of $\overline{{\psi}}_{i}$ , 

$$
\psi_{i}(-f_{i}(x^{+}))+\overline{{\psi}}_{i}(t\lambda_{i})+\theta_{i}\log\mu\leq-\mu t\lambda_{i}^{T}f_{i}(x^{+}).
$$ 

Subtracting the equality ( 11.51 ) from this inequality, we get 

$$
-\psi_{i}(-f_{i}(x))+\psi_{i}(-f_{i}(x^{+}))+\theta_{i}\log\mu\leq-\theta_{i}-\mu t\lambda_{i}^{T}f_{i}(x^{+}),
$$ 

and summing over $i$ yields 

$$
\phi(x)-\phi(x^{+})+\overline{{{\theta}}}\log\mu\leq-\overline{{{\theta}}}-\mu t\sum_{i=1}^{m}\lambda_{i}^{T}f_{i}(x^{+}).
$$ 

We also have, from the definition of the dual function, 

$$
\begin{array}{r c l}{f_{0}(x)-\overline{{\theta}}/t}&{=}&{g(\lambda,\nu)}\\ &{\leq}&{f_{0}(x^{+})+\displaystyle\sum_{i=1}^{m}\lambda_{i}^{T}f_{i}(x^{+})+\nu^{T}(A x^{+}-b)}\\ &{=}&{f_{0}(x^{+})+\displaystyle\sum_{i=1}^{m}\lambda_{i}^{T}f_{i}(x^{+}).}\end{array}
$$ 

Multiplying this inequality by $\mu t$ and adding to the inequality ( 11.52 ), we get 

$$
\phi(x)-\phi(x^{+})+\overline{{{\theta}}}\log\mu+\mu t f_{0}(x)-\mu\overline{{{\theta}}}\leq\mu t f_{0}(x^{+})-\overline{{{\theta}}},
$$ 

which when re-arranged gives 

$$
\mu t f_{0}(x)+\phi(x)-\mu t f_{0}(x^{+})-\phi(x^{+})\leq\overline{{{\theta}}}(\mu-1-\log\mu),
$$ 

the desired inequality ( 11.48 ). 

# 11.7 Primal-dual interior-point methods 

In this section we describe a basic primal-dual interior-point method. Primal- dual interior-point methods are very similar to the barrier method, with some diﬀerences. 

• There is only one loop or iteration, i.e. , there is no distinction between inner and outer iterations as in the barrier method. At each iteration, both the primal and dual variables are updated. • The search directions in a primal-dual interior-point method are obtained from Newton’s method, applied to modified KKT equations ( i.e. , the opti- mality conditions for the logarithmic barrier centering problem). The primal- dual search directions are similar to, but not quite the same as, the search directions that arise in the barrier method. • In a primal-dual interior-point method, the primal and dual iterates are not necessarily feasible. 

Primal-dual interior-point methods are often more efficient than the barrier method, especially when high accuracy is required, since they can exhibit better than linear convergence. For several basic problem classes, such as linear, quadratic, second-order cone, geometric, and semidefinite programming, customized primal- dual methods outperform the barrier method. For general nonlinear convex op- timization problems, primal-dual interior-point methods are still a topic of active research, but show great promise. Another advantage of primal-dual algorithms over the barrier method is that they can work when the problem is feasible, but not strictly feasible (although we will not pursue this). 

In this section we present a basic primal-dual method for ( 11.1 ), without conver- gence analysis. We refer the reader to the references for a more thorough treatment of primal-dual methods and their convergence analysis. 

# 11.7.1 Primal-dual search direction 

As in the barrier method, we start with the modified KKT conditions ( 11.15 ), expressed as $r_{t}(x,\lambda,\nu)=0$ , where we define 

$$
\boldsymbol{r}_{t}(x,\lambda,\nu)=\left[\begin{array}{c}{\nabla f_{0}(x)+D f(x)^{T}\lambda+A^{T}\nu}\\ {-\,\mathbf{diag}(\lambda)f(x)-(1/t)\mathbf{1}}\\ {A x-b}\end{array}\right],
$$ 

and $t>0$ . Here $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ and its derivative matrix $D f$ are given by 

$$
\begin{array}{r}{f(x)=\left[\begin{array}{c}{f_{1}(x)}\\ {\vdots}\\ {f_{m}(x)}\end{array}\right],\qquad D f(x)=\left[\begin{array}{c}{\nabla f_{1}(x)^{T}}\\ {\vdots}\\ {\nabla f_{m}(x)^{T}}\end{array}\right].}\end{array}
$$ 

If $x$ , $\lambda$ , $\nu$ satisfy $r_{t}(x,\lambda,\nu)\,=\,0$ (and $f_{i}(x)<0$ ), then $x\,=\,x^{\star}(t)$ , $\lambda=\lambda^{\star}(t)$ , and $\nu\,=\,\nu^{\star}(t)$ . In particular, $x$ is primal feasible, and $\lambda,\ \nu$ are dual feasible, with duality gap $m/t$ . The first block component of $r_{t}$ , 

$$
\boldsymbol{r}_{\mathrm{dual}}=\nabla f_{0}(\boldsymbol{x})+D f(\boldsymbol{x})^{T}\boldsymbol{\lambda}+\boldsymbol{A}^{T}\boldsymbol{\nu},
$$ 

is called the dual residual , and the last block component, $r_{\mathrm{pi}}=A x-b$ , is called the primal residual . The middle block, 

$$
r_{\mathrm{cent}}=-\,\mathbf{diag}(\lambda)f(x)-(1/t)\mathbf{1},
$$ 

is the centrality residual , i.e. , the residual for the modified complementarity condi- tion. 

Now consider the Newton step for solving the nonlinear equations $r_{t}(x,\lambda,\nu)=$ $0$ , for fixed $t$ (without first eliminating $\lambda$ , as in § 11.3.4 ), at a point $(x,\lambda,\nu)$ that satisifes $f(x)\prec0$ , $\lambda\succ0$ . We will denote the current point and Newton step as 

$$
y=(x,\lambda,\nu),\qquad\Delta y=(\Delta x,\Delta\lambda,\Delta\nu),
$$ 

respectively. The Newton step is characterized by the linear equations 

$$
r_{t}(y+\Delta y)\approx r_{t}(y)+D r_{t}(y)\Delta y=0,
$$ 

$$
\left[\begin{array}{c c c}{\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}\nabla^{2}f_{i}(x)}&{D f(x)^{T}}&{A^{T}}\\ {-\,\mathbf{diag}(\lambda)D f(x)}&{-\,\mathbf{diag}(f(x))}&{0}\\ {A}&{0}&{0}\end{array}\right]\left[\begin{array}{c c c}{\Delta x}\\ {\Delta\lambda}\\ {\Delta\nu}\end{array}\right]=-\left[\begin{array}{c c c}{r_{\mathrm{dual}}}\\ {r_{\mathrm{cent}}}\\ {r_{\mathrm{pri}}}\end{array}\right].
$$ 

The primal-dual search direction $\Delta y_{\mathrm{{pd}}}\;=\;\left(\Delta x_{\mathrm{{pd}}},\Delta\lambda_{\mathrm{{pd}}},\Delta\nu_{\mathrm{{pd}}}\right)$ is defined as the solution of ( 11.54 ). 

The primal and dual search directions are coupled, both through the coefficient matrix and the residuals. For example, the primal search direction $\Delta x_{\mathrm{pd}}$ depends on the current value of the dual variables $\lambda$ and $\nu$ , as well as $x$ . We note also that if satisfies $A x=b$ , i.e. , the primal feasibility residual is zero, then we have $x$ $r_{\mathrm{pri}}$ $A\Delta x_{\mathrm{{pd}}}\,=\,0$ , so $\Delta x_{\mathrm{pd}}$ defines a (primal) feasible direction: for any $s$ , $x+s\Delta x_{\mathrm{pd}}$ will satisfy $A(x+s\Delta x_{\mathrm{pd}})=b$ . 

# Comparison with barrier method search directions 

The primal-dual search directions are closely related to the search directions used in the barrier method, but not quite the same. We start with the linear equa- tions ( 11.54 ) that define the primal-dual search directions. We eliminate the vari- able $\Delta\lambda_{\mathrm{{pd}}}$ , using 

$$
\Delta\lambda_{\mathrm{pd}}=-\,{\bf d i a g}(f(x))^{-1}\,{\bf d i a g}(\lambda)D f(x)\Delta x_{\mathrm{pd}}+{\bf d i a g}(f(x))^{-1}r_{\mathrm{cent}},
$$ 

which comes from the second block of equations. Substituting this into the first block of equations gives 

$$
\begin{array}{r l}{\left[\begin{array}{c c}{H_{\mathrm{pd}}}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{pd}}}\\ {\Delta\nu_{\mathrm{pd}}}\end{array}\right]}\\ {=}&{-\left[\begin{array}{c}{r_{\mathrm{dual}}+D f(x)^{T}\,\mathbf{diag}(f(x))^{-1}r_{\mathrm{cent}}}\\ {r_{\mathrm{pi}}}\end{array}\right]}\\ {=}&{-\left[\begin{array}{c}{\nabla f_{0}(x)+(1/t)\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla f_{i}(x)+A^{T}\nu}\\ {r_{\mathrm{pi}}}\end{array}\right],}\end{array}
$$ 

where 

$$
H_{\mathrm{pd}}=\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}\nabla^{2}f_{i}(x)+\sum_{i=1}^{m}\frac{\lambda_{i}}{-f_{i}(x)}\nabla f_{i}(x)\nabla f_{i}(x)^{T}.
$$ 

We can compare ( 11.55 ) to the equation ( 11.14 ), which defines the Newton step for the centering problem in the barrier method with parameter $t$ . This equation can be written as 

$$
\begin{array}{r l}{\left[\begin{array}{c c}{H_{\mathrm{bar}}}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{bar}}}\\ {\nu_{\mathrm{bar}}}\end{array}\right]}\\ {=}&{-\left[\begin{array}{c}{t\nabla f_{0}(x)+\nabla\phi(x)}\\ {r_{\mathrm{pri}}}\end{array}\right]}\\ {=}&{-\left[\begin{array}{c}{t\nabla f_{0}(x)+\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla f_{i}(x)}\\ {r_{\mathrm{pri}}}\end{array}\right],}\end{array}
$$ 

where 

$$
H_{\mathrm{bar}}=t\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}{\frac{1}{-f_{i}(x)}}\nabla^{2}f_{i}(x)+\sum_{i=1}^{m}{\frac{1}{f_{i}(x)^{2}}}\nabla f_{i}(x)\nabla f_{i}(x)^{T}.
$$ 

(Here we give the general expression for the infeasible Newton step; if the current $x$ is feasible, i.e. , $r_{\mathrm{tri}}=0$ , then $\Delta x_{\mathrm{bar}}$ coincides with the feasible Newton step $\Delta x_{\mathrm{{nt}}}$ defined in ( 11.14 ).) 

Our first observation is that the two systems of equations ( 11.55 ) and ( 11.57 ) are very similar. The coefficient matrices in ( 11.55 ) and ( 11.57 ) have the same structure; indeed, the matrices $H_{\mathrm{pd}}$ and $H_{\mathrm{bar}}$ are both positive linear combinations of the matrices 

$$
\nabla^{2}f_{1}(x),\cdot\cdot\cdot,\nabla^{2}f_{m}(x),\qquad\nabla f_{1}(x)\nabla f_{1}(x)^{T},
$$ 

This means that the same method can be used to compute the primal-dual search directions and the barrier method Newton step. 

We can say more about the relation between the primal-dual equations ( 11.55 ) and the barrier method equations ( 11.57 ). Suppose we divide the first block of equation ( 11.57 ) by $t$ , and define the variable $\Delta\nu_{\mathrm{bar}}=(1/t)\nu_{\mathrm{bar}}-\nu$ (where $\nu$ is arbitrary). Then we obtain 

$$
\left[\begin{array}{c c}{(1/t)H_{\mathrm{bar}}}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{bar}}}\\ {\Delta\nu_{\mathrm{bar}}}\end{array}\right]=-\left[\begin{array}{c}{\nabla f_{0}(x)+(1/t)\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla f_{i}(x)+A^{T}\nu}\\ {r_{\mathrm{pi}}}\end{array}\right].
$$ 

In this form, the righthand side is identical to the righthand side of the primal-dual equations (evaluated at the same $x$ , $\lambda$ , and $\nu$ ). The coefficient matrices diﬀer only in the $1,1$ block: 

$$
\begin{array}{r c l}{H_{\mathrm{pd}}}&{=}&{\displaystyle\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}\nabla^{2}f_{i}(x)+\sum_{i=1}^{m}\frac{\lambda_{i}}{-f_{i}(x)}\nabla f_{i}(x)\nabla f_{i}(x)^{T},}\\ {(1/t)H_{\mathrm{bar}}}&{=}&{\displaystyle\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}\frac{1}{-t f_{i}(x)}\nabla^{2}f_{i}(x)+\sum_{i=1}^{m}\frac{1}{t f_{i}(x)^{2}}\nabla f_{i}(x)\nabla f_{i}(x)^{T}.}\end{array}
$$ 

When $x$ and $\lambda$ satisfy $-f_{i}(x)\lambda_{i}=1/t$ , the coefficient matrices, and therefore also the search directions, coincide. 

# 11.7.2 The surrogate duality gap 

In the primal-dual interior-point method the iterates $x^{(k)}$ , $\lambda^{(k)}$ , and $\nu^{(k)}$ are not necessarily feasible, except in the limit as the algorithm converges. This means that we cannot easily evaluate a duality gap $\eta^{(k)}$ associated with step $k$ of the algorithm, as we do in (the outer steps of) the barrier method. Instead we define the surrogate duality gap , for any $x$ that satisfies $f(x)\prec0$ and $\lambda\succeq0$ , as 

$$
{\widehat{\eta}}(x,\lambda)=-f(x)^{T}\lambda.
$$ 

The surrogate gap $\hat{\eta}$ would be the duality gap, if $x$ were primal feasible and $\lambda,~\nu$ were dual feasible, i.e. , if $r_{\mathrm{{phi}}}~=~0$ and $r_{\mathrm{dual}}\;=\;0$ . Note that the value of the parameter $t$ that corresponds to the surrogate duality gap $\hat{\eta}$ is $m/\hat{\eta}$ . 

# 11.7.3 Primal-dual interior-point method 

We can now describe the basic primal-dual interior-point algorithm. Algorithm 11.2 Primal-dual interior-point method. given $x$ that satisfies $f_{1}(x)<0,.\,.\,.\,,f_{m}(x)<0$ , $\lambda\succ0$ , $\mu>1$ , $\epsilon_{\mathrm{{freeas}}}>0$ , $\epsilon>0$ . 

1. Determine $t$ . Set $t:=\mu m/\hat{\eta}$ . 2. Compute primal-dual search direction $\Delta y_{\mathrm{pd}}$ . 3. Line search and update. Determine step length $s>0$ and set $y:=y+s\Delta y_{\mathrm{pd}}$ until $||r_{\mathrm{tri}}||_{2}\leq\epsilon_{\mathrm{meas}}$ , $||r_{\mathrm{dual}}||_{2}\leq\epsilon_{\mathrm{efeas}}$ , and $\hat{\eta}\le\epsilon$ ≤ . 

In step 1, the parameter $t$ is set to a factor $\mu$ times $m/\hat{\eta}$ , which is the value of $t$ associated with the current surrogate duality gap $\hat{\eta}$ . If $x$ , $\lambda$ , and $\nu$ were central, with parameter $t$ (and therefore with duality gap $m/t$ ), then in step 1 we would increase $t$ by the factor , which is exactly the update used in the barrier method. $\mu$ Values of the parameter $\mu$ on the order of 10 appear to work well. 

The primal-dual interior-point algorithm terminates when $x$ is primal feasible and $\lambda,~\nu$ are dual feasible (within the tolerance $\epsilon_{\mathrm{Eas}}$ ) and the surrogate gap is smaller than the tolerance $\epsilon$ . Since the primal-dual interior-point method often has faster than linear convergence, it is common to choose $\epsilon_{\mathrm{Eas}}$ and $\epsilon$ small. 

# Line search 

The line search in the primal-dual interior point method is a standard backtracking line search, based on the norm of the residual, and modified to ensure that $\lambda\succ0$ an $f(x)\prec0$ . denote the current iterate as $x$ , λ , and $\nu$ , and the next iterate as x $x^{+}$ , λ $\lambda^{+}$ , and ν $\nu^{+}$ , i.e. , 

$$
x^{+}=x+s\Delta x_{\mathrm{pd}},\qquad\lambda^{+}=\lambda+s\Delta\lambda_{\mathrm{pd}},\qquad\nu^{+}=\nu+s\Delta\nu_{\mathrm{pd}}.
$$ 

The residual, evaluated at $y^{+}$ , will be denoted $r^{+}$ 

We first compute the largest positive step length, not exceeding one, that gives $\lambda^{+}\succeq0$ , i.e. , 

$$
\begin{array}{r c l}{s^{\operatorname*{max}}}&{=}&{\operatorname*{sup}\{s\in[0,1]\mid\lambda+s\Delta\lambda\succeq0\}}\\ &{=}&{\operatorname*{min}\left\{1,\ \operatorname*{min}\{-\lambda_{i}/\Delta\lambda_{i}\mid\Delta\lambda_{i}<0\}\right\}.}\end{array}
$$ 

We start the backtracking with $s=0.99s^{\mathrm{max}}$ , and multiply $s$ by $\beta\in(0,1)$ until we have $f(x^{+})\prec0$ . We continue multiplying $s$ by $\beta$ until we have 

$$
\lVert r_{t}(x^{+},\lambda^{+},\nu^{+})\rVert_{2}\leq(1-\alpha s)\lVert r_{t}(x,\lambda,\nu)\rVert_{2}.
$$ 

Common choices for the backtracking parameters $\alpha$ and $\beta$ are the same as those for Newton’s method: $\alpha$ is typically chosen in the range 0 . 01 to 0 . 1, and $\beta$ is typically chosen in the range 0 . 3 to 0 . 8. 

One iteration of the primal-dual interior-point algorithm is the same as one step of the infeasible Newton method, applied to solving $r_{t}(x,\lambda,\nu)=0$ , but modified to ensure $\lambda\succ0$ and $f(x)\prec0$ (or, equivalently, with $\mathbf{dom}\,r_{t}$ restricted to $\lambda\succ0$ and $f(x)\prec0)$ ). The same arguments used in the proof of convergence of the infeasible start Newton method show that the line search for the primal-dual method always terminates in a finite number of steps. 

# 11.7.4 Examples 

We illustrate the performance of the primal-dual interior-point method for the same problems considered in § 11.3.2 . The only diﬀerence is that instead of starting with a point on the central path, as in $\S$ .2 , we start the primal-dual interior- $x^{(0)}$ point method at a randomly generated x , that satisfies $f(x)~\prec~0$ , and take $\lambda_{i}^{(0)}\;=\;-1/f_{i}(x^{(0)})$ − ), so the initial value of the surrogate gap is $\hat{\eta}\,=\,100$ The parameter values we use for the primal-dual interior-point method are 

$$
\mu=10,\qquad\beta=0.5,\qquad\epsilon=10^{-8},\qquad\alpha=0.01.
$$ 

# Small LP and GP 

We first consider the small LP used in § 11.3.2 , with $m\:=\:100$ inequalities and $n=50$ variables. Figure 11.21 shows the progress of the primal-dual interior-point method. Two plots are shown: the surrogate gap $\hat{\eta}$ , and the norm of the primal and dual residuals, 

$$
r_{\mathrm{meas}}=\left(\|r_{\mathrm{pri}}\|_{2}^{2}+\|r_{\mathrm{dual}}\|_{2}^{2}\right)^{1/2},
$$ 

versus iteration number. (The initial point is primal feasible, so the plot shows the norm of the dual feasibility residual.) The plots show that the residual converges to zero rapidly, and becomes zero to numerical precision in 24 iterations. The surrogate gap also converges rapidly. Compared to the barrier method, the primal- dual interior-point method is faster, especially when high accuracy is required. 

Figure 11.22 shows the progress of the primal-dual interior-point method on the GP considered in § 11.3.2 . The convergence is similar to the LP example. 

![](images/0292e37b39866d0c7a7408b2619ddd814437eba5abcf8691d7ff0cd939315f6e.jpg) 
Figure 11.21 Progress of the primal-dual interior-point method for an LP, showing surrogate duality gap η and the norm of the primal and dual resid- uals, versus iteration number. The residual converges rapidly to zero within 24 iterations; the surrogate gap also converges to a very small number in about 28 iterations. The primal-dual interior-point method converges faster than the barrier method, especially if high accuracy is required. 

![](images/b945aba130c46b43b8aeb633fd3041204629be79f60ce0fffabdc95a0cd1bdb9.jpg) 
Figure 11.22 Progress of primal-dual interior-point method for a GP, show- ing surrogate duality gap $\hat{\eta}$ and the norm of the primal and dual residuals versus iteration number. 

![](images/71d7e2acf794e1a2788c799dec34211f2fca296025dc6be7074882f421de1dc6.jpg) 
Figure 11.23 Number of iterations required to solve randomly generated standard LPs of diﬀerent dimensions, with $n=2m$ . Error bars show stan- dard deviation, around the average value, for 100 instances of each dimen- sion. The growth in the number of iterations required, as the problem di- mensions range over a 100:1 ratio, is approximately logarithmic. 

# A family of LPs 

Here we examine the performance of the primal-dual method as a function of the problem dimensions, for the same family of standard form LPs considered in $\S11.3.2$ . We use the primal-dual interior-point method to solve the same 2000 instances, which consist of 100 instances for each value of $m$ . The primal-dual algorithm is started at $x^{(0)}=\mathbf{1}$ , $\boldsymbol{\lambda}^{(0)}=\mathbf{1}$ , $\nu^{(0)}=0$ , and terminated using tolerance $\epsilon=10^{-8}$ . Figure 11.23 shows the average, and standard deviation, of the number of iterations required versus $m$ . The number of iterations ranges from 15 to 35, and grows approximately as the logarithm of $m$ . Comparing with the results for the barrier method shown in figure 11.8 , we see that the number of iterations in the primal-dual method is only slightly higher, despite the fact that we start at infeasible starting points, and solve the problem to a much higher accuracy. 

# 11.8 Implementation 

The main eﬀort in the barrier method is computing the Newton step for the cen- tering problem, which consists of solving sets of linear equations of the form 

$$
\begin{array}{r}{\left[\begin{array}{c c}{H}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {\nu_{\mathrm{nt}}}\end{array}\right]=-\left[\begin{array}{c}{g}\\ {0}\end{array}\right],}\end{array}
$$ 

where 

$$
\begin{array}{r c l}{H}&{=}&{\displaystyle t\nabla^{2}f_{0}(x)+\sum_{i=1}^{m}\frac{1}{f_{i}(x)^{2}}\nabla f_{i}(x)\nabla f_{i}(x)^{T}+\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla^{2}f_{i}(x)}\end{array}
$$ 

$$
\begin{array}{r c l}{g}&{=}&{t\nabla f_{0}(x)+\displaystyle\sum_{i=1}^{m}\frac{1}{-f_{i}(x)}\nabla f_{i}(x).}\end{array}
$$ 

The Newton equations for the primal-dual method have exactly the same structure, so our observations in this section apply to the primal-dual method as well. 

The coefficient matrix of ( 11.60 ) has KKT structure, so all of the discussion in § 9.7 and § 10.4 applies here. In particular, the equations can be solved by elimi- nation, and structure such as sparsity or diagonal plus low rank can be exploited. Let us give some generic examples in which the special structure of the KKT equa- tions can be exploited to compute the Newton step more efficiently. 

# Sparse problems 

If the original problem is sparse, which means that the objective and every con- straint function each depend on only a modest number of variables, then the gradi- ents and Hessian matrices of the objective and constraint functions are all sparse, as is the coefficient matrix $A$ . Provided $m$ is not too big, the matrix $H$ is then likely to be sparse, so a sparse matrix method can be used to compute the Newton step. The method will likely work well if there are a few relatively dense rows and columns in the KKT matrix, which would occur, for example, if there were a few equality constraints involving a large number of variables. 

# Separable objective and a few linear inequality constraints 

Suppose the objective function is separable, and there are only a relatively small number of linear equality and inequality co traints. Then $\nabla^{2}f_{0}(x)$ is diagon and the terms $\nabla^{2}f_{i}(x)$ vanish, so the matrix H is diagonal plus low rank. Since H is easily inverted, we can solve the KKT equations efficiently. The same method can be applied whenever $\nabla^{2}f_{0}(x)$ is easily inverted, e.g. , banded, sparse, or block diagonal. 

# 11.8.1 Standard form linear programming 

We first discuss the implementation of the barrier method for the standard form LP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad A x=b,\quad x\succeq0,}\end{array}}
$$ 

with $A\in\mathbf{R}^{m\times n}$ . The Newton equations for the centering problem 

$$
{\begin{array}{l r}{{\mathrm{minimize}}}&{t c^{T}x-\sum_{i=1}^{n}\log x_{i}}\\ {{\mathrm{subject~to}}}&{A x=b}\end{array}}
$$ 

are given by 

$$
\left[\begin{array}{c c}{\mathbf{diag}(x)^{-2}}&{A^{T}}\\ {A}&{0}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {\nu_{\mathrm{nt}}}\end{array}\right]=\left[\begin{array}{c}{-t c+\mathbf{diag}(x)^{-1}\mathbf{1}}\\ {0}\end{array}\right].
$$ 

These equations are usually solved by block elimination of $\Delta x_{\mathrm{{nt}}}$ . From the first equation, 

$$
\begin{array}{r c l}{\Delta x_{\mathrm{nt}}}&{=}&{{\bf d i a g}(x)^{2}(-t c+{\bf d i a g}(x)^{-1}{\bf1}-A^{T}\nu_{\mathrm{nt}})}\\ &{=}&{-t\,{\bf d i a g}(x)^{2}c+x-{\bf d i a g}(x)^{2}A^{T}\nu_{\mathrm{nt}}.}\end{array}
$$ 

Substituting in the second equation yields 

$$
\boldsymbol{A}\,\mathbf{diag}(x)^{2}\boldsymbol{A}^{T}\boldsymbol{\nu}_{\mathrm{nt}}=-t\boldsymbol{A}\,\mathbf{diag}(x)^{2}\boldsymbol{c}+\boldsymbol{b}.
$$ 

The coefficient matrix is positive definite since by assumption rank $A=m$ . More- over if $A$ is sparse, then usually $A\,\mathbf{diag}(x)^{2}A^{T}$ is sparse, so a sparse Cholesky factorization can be used. 

# 11.8.2 $\ell_{1}$ -norm approximation 

Consider the $\ell_{1}$ -norm approximation problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{1}
$$ 

with $A\in\mathbf{R}^{m\times n}$ . We will discuss the implementation assuming $m$ and $n$ are large, and A is structured, e.g. , sparse, and compare it with the cost of the corresponding least-squares problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}\ .
$$ 

We start by expressing the $\ell_{1}$ -norm approximation problem as an LP by intro- ducing auxiliary variables $y\in\mathbf{R}^{m}$ : 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\mathbf{1}^{T}y}\\ &{{\mathrm{subject~to}}\quad{\left[\begin{array}{l l}{A}&{-I}\\ {-A}&{-I}\end{array}\right]}\,{\left[\begin{array}{l}{x}\\ {y}\end{array}\right]}\preceq{\left[\begin{array}{l}{b}\\ {-b}\end{array}\right]}\,.}\end{array}}
$$ 

The Newton equation for the centering problem is 

$$
\left[\begin{array}{r r}{A^{T}}&{-A^{T}}\\ {-I}&{-I}\end{array}\right]\left[\begin{array}{c c}{D_{1}}&{0}\\ {0}&{D_{2}}\end{array}\right]\left[\begin{array}{c c}{A}&{-I}\\ {-A}&{-I}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {\Delta y_{\mathrm{nt}}}\end{array}\right]=-\left[\begin{array}{c}{A^{T}g_{1}}\\ {g_{2}}\end{array}\right]
$$ 

where 

$$
D_{1}={\bf d i a g}(b-A x+y)^{-2},\qquad D_{2}={\bf d i a g}(-b+A x+y)^{-2}
$$ 

and 

$$
\begin{array}{l l l}{{g_{1}}}&{{=}}&{{{\bf d i a g}(b-A x+y)^{-1}{\bf1}-{\bf d i a g}(-b+A x+y)^{-1}{\bf1}}}\\ {{g_{2}}}&{{=}}&{{t{\bf1}-{\bf d i a g}(b-A x+y)^{-1}{\bf1}-{\bf d i a g}(-b+A x+y)^{-1}{\bf1}.}}\end{array}
$$ 

If we multiply out the lefthand side, this can be simplified as 

$$
\left[\begin{array}{c c}{A^{T}(D_{1}+D_{2})A}&{-A^{T}(D_{1}-D_{2})}\\ {-(D_{1}-D_{2})A}&{D_{1}+D_{2}}\end{array}\right]\left[\begin{array}{c}{\Delta x_{\mathrm{nt}}}\\ {\Delta y_{\mathrm{nt}}}\end{array}\right]=-\left[\begin{array}{c}{A^{T}g_{1}}\\ {g_{2}}\end{array}\right].
$$ 

Applying block elimination to $\Delta{y_{\mathrm{{nt}}}}$ , we can reduce this to 

$$
A^{T}D A\Delta x_{\mathrm{{nt}}}=-A^{T}g
$$ 

where 

$$
D=4D_{1}D_{2}(D_{1}+D_{2})^{-1}=2(\mathbf{diag}(y)^{2}+\mathbf{diag}(b-A x)^{2})^{-1}
$$ 

and 

$$
g=g_{1}+(D_{1}-D_{2})(D_{1}+D_{2})^{-1}g_{2}.
$$ 

After solving for $\Delta x_{\mathrm{{nt}}}$ , we obtain $\Delta{y_{\mathrm{{nt}}}}$ from 

$$
\begin{array}{r}{\Delta y_{\mathrm{nt}}=(D_{1}+D_{2})^{-1}(-g_{2}+(D_{1}-D_{2})A\Delta x_{\mathrm{nt}}).}\end{array}
$$ 

It is interesting to note that ( 11.61 ) are the normal equations of a weighted least- squares problem 

$$
\begin{array}{r l}{\mathrm{minimize}}&{{}\|D^{1/2}\big(A\Delta x+D^{-1}g\big)\|_{2}.}\end{array}
$$ 

In other words, the cost of solving the $\ell_{1}$ -norm approximation problem is the cost of solving a relatively small number of weighted least-squares problems with the same matrix $A$ , and weights that change at each iteration. If $A$ has structure that allows us to solve the least-squares problem fast (for example, by exploiting sparsity), then we can solve ( 11.61 ) fast. 

# 11.8.3 Semidefinite programming in inequality form 

We consider the SDP 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad c^{T}x}\\ &{{\mathrm{subject~to}}\quad\sum_{i=1}^{n}x_{i}F_{i}+G\preceq0,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n}$ , and parameters $F_{1},.\,.\,.\,,F_{n},G\in\mathbf{S}^{p}$ . The associated centering problem, using the log-determinant barrier function, is 

$$
{\begin{array}{r}{{\mathrm{minimize}}\quad t c^{T}x-\log\operatorname*{det}(-\sum_{i=1}^{n}x_{i}F_{i}-G).}\end{array}}
$$ 

The Newton step $\Delta x_{\mathrm{{nt}}}$ is found from $H\,\Delta x_{\mathrm{nt}}=-g$ , where the Hessian and gradient are given by 

$$
\begin{array}{r c l}{H_{i j}}&{=}&{\mathbf{tr}(S^{-1}F_{i}S^{-1}F_{j}),\quad i,\ j=1,\ldots,n}\\ {g_{i}}&{=}&{t c_{i}+\mathbf{tr}(S^{-1}F_{i}),\quad i=1,\ldots,n,}\end{array}
$$ 

where $\begin{array}{r}{S=-\sum_{i=1}^{n}x_{i}F_{i}-G}\end{array}$ − . One standard approach is to form $H$ (and ), and $g$ then solve the Newton equation via Cholesky factorization. 

We first consider the unstructured case, i.e. , we assume all matrices are dense. We will also just keep track of the order in the ﬂop count, with respect to the problem dimensions $n$ and $p$ . We first form $S$ , which costs order $n p^{2}$ ﬂops. We then compute the matrices $S^{-1}F_{i}$ , for each $i$ , via Cholesky factorization of $S$ , and then back substitution with the columns of $F_{i}$ (or forming $S^{-1}$ and multiplying by $F_{i}$ ). This cost is order $p^{3}$ for each $i$ , so the total cost is order $n p^{3}$ . Finally, we form $H_{i j}$ as the inner product of the matrices $S^{-1}F_{i}$ and $S^{-1}F_{j}$ , which costs order $p^{2}$ ﬂops. Since we do this for $n(n+1)/2$ such pairs, the cost is order $n^{2}p^{2}$ . Solving for the Newton direction costs order $n^{3}$ . The dominating order is thus $\operatorname*{max}\{n p^{3},n^{2}p^{2},n^{3}\}$ . 

It is not possible, in general, to exploit sparsity in the matrices $F_{i}$ and $G$ , since $H$ is often dense, even when $F_{i}$ and $G$ are sparse. One exception is when $F_{i}$ and $G$ have a common block diagonal structure, in which case all the operations described above can be carried out block by block. 

It is often possible to exploit (common) sparsity in $F_{i}$ and $G$ to form the (dense) Hessian $H$ more efficiently. If we can find an ordering that results in $S$ having a reasonably sparse Cholesky factor, then we can compute the matrices $S^{-1}F_{i}$ efficiently, and form $H_{i j}$ far more efficiently. 

One interesting example that arises frequently is an SDP with matrix inequality 

$$
\mathbf{diag}(x)\preceq B.
$$ 

This corresponds to $F_{i}\,=\,E_{i i}$ , where $E_{i i}$ is the matrix with $i,i$ entry one and all others zero. In this case, the matrix $H$ can be found very efficiently: 

$$
H_{i j}=(S^{-1})_{i j}^{2},
$$ 

where $S=B-\mathbf{diag}(x)$ . The cost of forming $H$ is thus the cost forming $S^{-1}$ , which is at most ( i.e. , when no other structure is exploited) order n $n^{3}$ . 

# 11.8.4 Network rate optimization 

We consider a variation on the optimal network ﬂow problem described in § 10.4.3 (page 550 ), which is sometimes called the network rate optimization problem. The network is described as a directed graph with $L$ arcs or links. Goods, or packets of information, travel on the network, passing through the links. The network supports $n$ ﬂows , with (nonnegative) rates $x_{1},\dots,x_{n}$ , which are the optimization variables. Each ﬂow moves along a fixed, or pre-determined, path (or route ) in the network, from a source node to a destination node. Each link can support multiple ﬂows passing through it. The total traffic on a link is the sum of the ﬂow rates of the ﬂows that travel over the link. Each link has a positive capacity , which is the maximum total traffic it can handle. 

We can describe these link capacity limits using the ﬂow-link incidence matrix $A\in\mathbf{R}^{L\times n}$ , defined as 

$$
A_{i j}=\left\{\begin{array}{c l}{{1}}&{{\mathrm{flow}\ j\ \mathrm{passes\through\link}\ i}}\\ {{0}}&{{\mathrm{otherwise}.}}\end{array}\right.
$$ 

The total traffic on link $i$ is then given by $(A x)_{i}$ , so the link capacity constraints can be expressed as $A x\preceq c$ , where is the capacity of link $i$ . Usually each pa $c_{i}$ passes through only a small fraction of the total number of links, so the matrix A is sparse. 

In the network rate problem the paths are fixed (and encoded in the matrix $A$ , which is a problem parameter); the variables are the ﬂow rates $x_{i}$ . The objective is to choose the ﬂow rates to maximize a separable utility function $U$ , given by 

$$
U(x)=U_{1}(x_{1})+\cdot\cdot\cdot+U_{n}(x_{n}).
$$ 

We assume that each $U_{i}$ (and hence, $U$ ) is concave and nondecreasing. We can think of $U_{i}(x_{i})$ as the income derived from supporting the $i$ th ﬂow at rate $x_{i}$ ; $U(x)$ is then the total income associated with the ﬂows. The network rate optimization problem is then 

$$
\begin{array}{l l}{\mathrm{maximize}}&{U(x)}\\ {\mathrm{subject~to}}&{A x\preceq c,\quad x\succeq0,}\end{array}
$$ 

which is a convex optimization problem. 

Let us apply the barrier method to solve this problem. At each step we must minimize a function of the form 

$$
-t U(x)-\sum_{i=1}^{L}\log(c-A x)_{i}-\sum_{j=1}^{n}\log x_{j},
$$ 

using Newton’s method. The Newton step $\Delta x_{\mathrm{{nt}}}$ is found by solving the linear equations 

$$
\begin{array}{r}{(D_{0}+A^{T}D_{1}A+D_{2})\Delta x_{\mathrm{nt}}=-g,}\end{array}
$$ 

where 

$$
\begin{array}{r c l}{{D_{0}}}&{{=}}&{{-t\,\mathbf{diag}(U_{1}^{\prime\prime}(x),\ldots,U_{n}^{\prime\prime}(x))}}\\ {{D_{1}}}&{{=}}&{{\mathbf{diag}(1/(c-A x)_{1}^{2},\ldots,1/(c-A x)_{L}^{2})}}\\ {{D_{2}}}&{{=}}&{{\mathbf{diag}(1/x_{1}^{2},\ldots,1/x_{n}^{2})}}\end{array}
$$ 

are diagonal matrices, and $g\in\mathbf{R}^{n}$ . We can describe the sparsity structure of this $n\times n$ coefficient matrix precisely: 

$$
(D_{0}+A^{T}D_{1}A+D_{2})_{i j}\neq0
$$ 

if and only if ﬂow $i$ and ﬂow $j$ share a link. If the paths are relatively short, and each link has relatively few paths passing through it, then this matrix is sparse, so a sparse Cholesky factorization can be used. We can also solve the Newton system efficiently when some, but not too many, of the rows and columns are relatively dense. This occurs when a few of the ﬂows intersect with a large number of the other ﬂows, which might occur if a few ﬂows are relatively long. 

We can also use the matrix inversion lemma to compute the Newton step by solving a system with $L\times L$ coefficient matrix, with form 

$$
(D_{1}^{-1}+A(D_{0}+D_{2})^{-1}A^{T})y=-A(D_{0}+D_{2})^{-1}g,
$$ 

and then computing 

$$
\Delta x_{\mathrm{nt}}=-(D_{0}+D_{2})^{-1}(g+A^{T}y).
$$ 

Here too we can precisely describe the sparsity pattern: 

$$
(D_{1}^{-1}+A(D_{0}+D_{2})^{-1}A^{T})_{i j}\neq0
$$ 

if and only if there is a path that passes through link $i$ and link $j$ . If most paths are short, this matrix is sparse. This matrix will be sparse, with a few dense rows and columns, if there are a few bottlenecks, i.e. , a few links over which many ﬂows travel. 

# Bibliography 

The early history of the barrier method is described in detail by Fiacco and McCormick [ FM90 , § 1.2]. The method was a popular algorithm for convex optimization in the 1960s, along with closely related techniques such as the method of centers (Liˆ e˜ u and Huard [ LH66 ]; see also exercise 11.11 ), and penalty (or exterior-point) methods [ FM90 , § 4]. Interest declined in the 1970s amid concerns about the ill-conditioning of the Newton equations of the centering problem ( 11.6 ) for high values of $t$ . 

The barrier method regained popularity in the 1980s, after Gill, Murray, Saunders, Tom- + lin, and Wright [ GMS 86 ] pointed out the close connections with Karmarkar’s polynomial- time projective algorithm for linear programming [ Kar84 ]. The focus of research through- out the 1980s remained on linear (and to a lesser extent, quadratic) programming, result- ing in diﬀerent variations of the basic interior-point methods, and improved worst-case complexity results (see Gonzaga [ Gon92 ]). Primal-dual methods emerged as the algo- rithms of choice for practical implementations (see Mehrotra [ Meh92 ], Lustig, Marsten, and Shanno [ LMS94 ], Wright [ Wri97 ]). 

In their 1994 book, Nesterov and Nemirovski extended the complexity theory of linear programming interior-point methods to nonlinear convex optimization problems, using the convergence theory of Newton’s method for self-concordant functions. They also developed interior-point methods for problems with generalized inequalities, and discussed ways of reformulating problems to satisfy the self-concordance assumption. The geometric programming reformulation on page 587 , for example, is from [ NN94 , 6.3.1]. 

As mentioned on page 585 , the complexity analysis shows that, contrary to what one might expect, the centering problems in the barrier method do not become more difficult as $t$ increases, at least not in exact arithmetic. Practical experience, supported by theoretical results (Forsgren, Gill, and Wright [ FGW02 , § 4.3.2], Nocedal and Wright [ NW99 , page 525]), also indicates that the eﬀects of ill-conditioning on the computed solution of the Newton system are more benign than thought earlier. 

Recent research on interior-point methods has concentrated on extending the primal-dual methods for linear programming, which converge faster and reach higher accuracies than (primal) barrier methods, to nonlinear convex problems. One popular approach, along the lines of the simple primal-dual method of § 11.7 , is based on linearizing modified KKT equations for a convex optimization problem in standard form, i.e. , problem ( 11.1 ). More sophisticated algorithms of this type diﬀer from algorithm 11.2 in the strategy used to select $t$ (which is crucial to achieve superlinear asymptotic convergence), and the line search. We refer to Wright [ Wri97 , chapter 8], Ralph and Wright [ RW97 ], den Hertog [ dH93 ], Terlaky [ Ter96 ], and the survey by Forsgren, Gill, and Wright [ FGW02 , § 5] for details and references. 

Other authors adopt the cone programming framework as starting point for extending primal-dual interior-point methods for linear programming to convex optimization (see for example, Nesterov and Todd [ NT98 ]). This approach has resulted in efficient and accurate primal-dual methods for semidefinite and second-order programming (see the surveys by Todd [ Tod01 ] and Alizadeh and Goldfarb [ AG03 ]). 

As for linear programming, primal-dual methods for semidefinite programming are usually described as variations of Newton’s method applied to modified KKT equations. Unlike in linear programming, however, the linearization can be carried out in many diﬀerent ways, which lead to diﬀerent search directions and algorithms; see Helmberg, Rendl, Vanderbei, and Wolkowicz [ HRVW96 ], Kojima, Shindo, and Harah [ KSH97 ], Monteiro

 [ Mon97 ], Nesterov and Todd [ NT98 ], Zhang [ Zha98 ], Alizadeh, Haeberly, and Overton

 [ AHO98 ], and Todd, Toh, and T¨ ut¨ unc¨ u [ TTT98 ]. 

Great progress has also been made in the area of initialization and infeasibility detection. Homogeneous self-dual formulations provide an elegant and efficient alternative to the classical two-phase approach of 11.4 ; see Ye, Todd, and Mizuno [ YTM94 ], Xu, Hung, and Ye [ XHY96 ], Andersen and Ye [ AY98 ] and Luo, Sturm, and Zhang [ LSZ00 ] for details. The primal-dual interior-point methods for semidefinite and second-order cone program- ming have been implemented in a number of software packages, including SeDuMi [ Stu99 ], SDPT3 [ TTT02 ], SDPA [ FKN98 ], CSDP [ Bor02 ], and DSDP [ BY02 ], A user-friendly in- terface to several of these codes is provided by YALMIP [ L¨ of04 ]. 

The following books document the recent developments in this rapidly advancing field in greater detail: Vanderbei [ Van96 ], Wright [ Wri97 ], Roos, Terlaky, and Vial [ RTV97 ] Ye [ Ye97 ], Wolkowicz, Saigal, and Vandenberghe [ WSV00 ], Ben-Tal and Nemirovski, [ BTN01 ], Renegar [ Ren01 ], and Peng, Roos, and Terlaky [ PRT02 ]. 

# Exercises 

# The barrier method 

11.1 Barrier method example. Consider the simple problem 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\ x^{2}+1}\\ &{{\mathrm{subject~to}}\quad2\leq x\leq4,}\end{array}}
$$ 

which has feasible set [2 , 4], and optimal point $x^{\star}=2$ . Plot $f_{0}$ , and $t f_{0}+\phi$ , for several values of $t>0$ , versus $x$ . Label $x^{\star}(t)$ . 

11.2 What happens if the barrier method is applied to the LP 

$$
{\begin{array}{l r l}{{\mathrm{minimize}}}&{x_{2}}\\ {{\mathrm{subject~to}}}&{x_{1}\leq x_{2},}&{0\leq x_{2},}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{2}$ ? 

11.3 Boundedness of centering problem. Suppose the sublevel sets of ( 11.1 ), 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{A x=b,}\end{array}}
$$ 

are bounded. Show that the sublevel sets of the associated centering problem, 

$$
\begin{array}{l c l}{{\mathrm{minimize}}}&{{t f_{0}(x)+\phi(x)}}\\ {{\mathrm{subject~to}}}&{{A x=b,}}\end{array}
$$ 

are bounded. 

11.4 Adding a norm bound to ensure strong convexity of the centering problem. Suppose we add the constraint $x^{T}x\leq R^{2}$ to the problem ( 11.1 ): 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\ f_{0}(x)}\\ {{\mathrm{subject~to}}}&{\ f_{i}(x)\leq0,\quad i=1,\ldots,m}\\ &{\ A x=b}\\ &{\ x^{T}x\leq R^{2}.}\end{array}}
$$ 

Let $\ddot{\phi}$ denote the logarithmic barrier function for this modified problem. Find $a>0$ for which $\nabla^{2}(t f_{0}(x)+\tilde{\phi}(x))\succeq a I$ holds, for all feasible $x$ . 

11.5 Barrier method for second-order cone programming. Consider the SOCP (without equality constraints, for simplicity) 

$$
\begin{array}{l r}{\mathrm{minimize}\ }&{f^{T}x}\\ {\mathrm{subject~to}\ }&{\|A_{i}x+b_{i}\|_{2}\leq c_{i}^{T}x+d_{i},\quad i=1,.\,.\,,m.}\end{array}
$$ 

The constraint functions in this problem are not diﬀerentiable (since the Euclidean norm $\|u\|_{2}$ is not diﬀerentiable at $u=0$ ) so the (standard) barrier method cannot be applied. In § 11.6 , we saw that this SOCP can be solved by an extension of the barrier method that handles generalized inequalities. (See example 11.8 , page 599 , and page 601 .) In this exercise, we show how the standard barrier method (with scalar constraint functions) can be used to solve the SOCP. 

We first reformulate the SOCP as 

$$
\begin{array}{l l}{\mathrm{minimize~}}&{f^{T}x}\\ {\mathrm{subject~to}}&{\|A_{i}x+b_{i}\|_{2}^{2}/(c_{i}^{T}x+d_{i})\leq c_{i}^{T}x+d_{i},\quad i=1,\ldots,m}\\ &{c_{i}^{T}x+d_{i}\geq0,\quad i=1,\ldots,m.}\end{array}
$$ 

The constraint function 

$$
f_{i}(x)=\frac{||A_{i}x+b_{i}||_{2}^{2}}{c_{i}^{T}x+d_{i}}-c_{i}^{T}x-d_{i}
$$ 

is the composition of a quadratic-over-linear function with an affine function, and is twice diﬀerentiable (and convex), provided we define its domain as dom $f_{i}=\{x\mid c_{i}^{T}x\!+\!d_{i}>0\}$ } . Note that the two problems ( 11.63 ) and ( 11.64 ) are not exactly equivalent. If $c_{i}^{T}x^{\star}\!+\!d_{i}=0$ for some $i$ , where $x^{\star}$ is the optimal solution of the SOCP ( 11.63 ), then the reformulated problem ( 11.64 ) is not solvable; $x^{\star}$ is not in its domain. Nevertheless we will see that the barrier method, applied to ( 11.64 ), produces arbitrarily accurate suboptimal solutions of ( 11.64 ), and hence also for ( 11.63 ). 

(a) Form the log barrier $\phi$ for the problem ( 11.64 ). Compare it to the log barrier that arises when the SOCP ( 11.63 ) is solved using the barrier method for generalized inequalities (in $\S$ 11.6 ). (b) Show that if $t f^{T}x+\phi(x)$ is minimized, the minimizer $x^{\star}(t)$ is $2m/t$ -suboptimal for the problem ( 11.63 ). It follows that the standard barrier method, applied to the reformulated problem ( 11.64 ), solves the SOCP ( 11.63 ), in the sense of producing arbitrarily accurate suboptimal solutions. This is the case even though the optimal point $x^{\star}$ need not be in the domain of the reformulated problem ( 11.64 ). 

11.6 General barriers . The log barrier is based on the approximation $-(1/t)\log(-u)$ of the indic $\widehat{I}_{-}(u)$ ) (see § 11.2.1 , page 563 ). We can also construct barriers from other approximations, which in turn yield generalizations of the central path and barrier method. Let h $h:\mathbf{R}\rightarrow\mathbf{R}$ → R be a twice d , clo creasing convex function, $\mathbf{\deltadom}\,h\ =\ -\mathbf{R}_{++}$ − . (This implie $h(u)~\to\;\infty$ as $u~\rightarrow~0$ 0.) One such function is $h(u)=-\log(-u)$ − − ); another example is $h(u)=-1/u$ − (for u < 0). 

Now consider the optimization problem (without equality constraints, for simplicity) 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{f_{0}(x)}}\\ {{\mathrm{subject~to}}}&{{f_{i}(x)\leq0,\quad i=1,.\,.\,,m,}}\end{array}
$$ 

where $f_{i}$ are twice diﬀerentiable. We define the $h$ -barrier for this problem as 

$$
\phi_{h}(x)=\sum_{i=1}^{m}h(f_{i}(x)),
$$ 

with domain $\{x\ |\ f_{i}(x)\,<\,0,\ i=\,1,.\,.\,.\,,m\}$ . When $h(u)\,=\,-\log(-u)$ , this is the usual ogarithmic barrier; when $h(u)\,=\,-1/u$ , $\phi_{h}$ is called the inverse barrier . We define the h -central path as 

$$
x^{\star}(t)=\mathrm{argmin}\ t f_{0}(x)+\phi_{h}(x),
$$ 

where $t\ >\ 0$ is a parameter. (We assume that for each $t$ , the minimizer exists and is unique.) 

(a) Explain why $t f_{0}(x)+\phi_{h}(x)$ is convex in $x$ , for each $t>0$ . 

(b) Show how to construct a dual feasible $\lambda$ from $x^{\star}(t)$ . Find the associated duality gap. (c) For what functions $h$ does the duality gap found in part (b) depend only on $t$ and $_{m}$ (and no other problem data)? 

11.7 Tangent to central path. This problem concerns $d x^{\star}(t)/d t$ , which gives the tangent to the central path at the point $x^{\star}(t)$ . For simplicity, we consider a problem without equality constraints; the results readily generalize to problems with equality constraints. 

(a) Find an explicit expression for $d x^{\star}(t)/d t$ . Hint. Diﬀerentiate the centrality equa- tions ( 11.7 ) with respect to $t$ . 

(b) Show that $f_{0}(x^{\star}(t))$ decreases as $t$ increases. Thus, the objective value in the barrier method decreases, as the parameter $t$ is increased. (We already know that the duality gap, which is $m/t$ , decreases as $t$ increases.) 

11.8 Predictor-corrector method for centering problems. In the standard barrier method, $x^{\star}(\mu t)$ is computed using Newton’s method, starting from the initial point $x_{*}^{\star}(t)$ . One alternative has been proposed is to make an app tion or prediction x of $x^{\star}(\mu t)$ , and then start the Newton m for computing x $x^{\star}(\mu t)$ ) from b . The idea is that this should reduce the number of Newton steps, since b is (presumably) a better initial point than $x^{\star}(t)$ ). This method of centering is called a predictor-corrector method , since it first makes a prediction of what $x^{\star}(\mu t)$ ) is, then corrects the prediction using Newton’s method. The most widely used predictor is the first-order predictor, based on the tangent to the central path, explored in exercise 11.7 . This predictor is given by 

$$
\widehat{x}=x^{\star}(t)+\frac{d x^{\star}(t)}{d t}(\mu t-t).
$$ 

Derive an exp e first-o redictor x . Compare it to the Newton update obtained, i.e. , $x^{\star}(t)+\Delta x_{\mathrm{nt}}$ , where ∆ $\Delta x_{\mathrm{{nt}}}$ is the Newton step for $\mu t f_{0}(x)+\phi(x)$ , at $x^{\star}(t)$ . What can you say when the objective f $f_{0}$ is linear? (For simplicity, you can consider a problem without equality constraints.) 

11.9 Dual feasible points near the central path. Consider the problem 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&&{f_{i}(x)\leq0,\quad i=1,.\,.\,,m,}\end{array}}
$$ 

with variable $x\in\mathbf{R}^{n}$ . We assume the functions $f_{i}$ are convex and twice diﬀerentiable. (We assume for simplicity there are no equality constraints.) Recall (from § 11.2.2 , page 565 ) that $\lambda_{i}\,=\,-1/(t f_{i}(x^{\star}(t)))$ , $i\,=\,1,.\,.\,.\,,m$ , is dual feasibl , and in fact, $x^{\star}(t)$ minimizes $L(x,\lambda)$ . This allows us to evaluate the dual function for λ , which turns out to be $g(\lambda)=$ $f_{0}\!\left(x^{\star}(t)\right)-m/t$ . In particular, we conclude that $x^{\star}(t)$ is $m/t$ -suboptimal. 

In this problem we consider what happens when a point $x$ is close to $x^{\star}(t)$ , but not quite centered. (This would occur if the centering steps were terminated early, or not carried ccuracy.) In this case, of cou we cannot claim that $\lambda_{i}\,=\,-1/\bigl(t f_{i}(x)\bigr)$ , $i\,=\,1,.\,.\,.\,,m$ , is dual feasible, or that x is m/t -suboptimal. However, it turns out that a slightly more complicated formula does yield a dual feasible point, provided $x$ is close enough to centered. 

Let $\Delta x_{\mathrm{{nt}}}$ be the Newton step at $x$ of the centering problem 

$$
\begin{array}{r l}{\mathrm{minimize}\,}&{{}t f_{0}(x)-\sum_{i=1}^{m}\log(-f_{i}(x)).}\end{array}
$$ 

A formula that often gives a dual feasible point when $\Delta x_{\mathrm{{nt}}}$ is small ( i.e. , for $x$ nearly centered) is 

$$
\lambda_{i}=\frac{1}{-t f_{i}(x)}\left(1+\frac{\nabla f_{i}(x)^{T}\Delta x_{\mathrm{nt}}}{-f_{i}(x)}\right),\quad i=1,\ldots,m.
$$ 

In this case, the vector $x$ does not minimize $L(x,\lambda)$ , so there is no general formula for the dual function value $g(\lambda)$ associated with $\lambda$ . (If we have an analytical expression for the dual objective, however, we can simply evaluate $g(\lambda)$ .) Verify that for a QCQP 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{(1/2)x^{T}P_{0}x+q_{0}^{T}x+r_{0}}}\\ {{\mathrm{subject~to}}}&{{(1/2)x^{T}P_{i}x+q_{i}^{T}x+r_{i}\leq0,\quad i=1,\ldots,m,}}\end{array}
$$ 

the fo a for $\lambda$ yields a dual feasible point ( i.e. , $\lambda\succeq0$ and $L(x,\lambda)$ is bounded below) when ∆ $\Delta x_{\mathrm{{nt}}}$ is sufficiently small. 

Hint. Define 

$$
x_{0}=x+\Delta x_{\mathrm{nt}},\qquad x_{i}=x-\frac{1}{t\lambda_{i}f_{i}(x)}\Delta x_{\mathrm{nt}},\quad i=1,\ldots,m.
$$ 

Show that 

$$
\nabla f_{0}{\bigl(}x_{0}{\bigr)}+\sum_{i=1}^{m}\lambda_{i}\nabla f_{i}{\bigl(}x_{i}{\bigr)}=0.
$$ 

Now use $f_{i}(z)\,\geq\,f_{i}(x_{i})+\nabla f_{i}(x_{i})^{T}(z-x_{i}),\;i\,=\,0,.\,.\,.\,,m$ , to derive a lower bound on $L(z,\lambda)$ . 

11.10 Another parametrization of the central path. We consider the problem ( 11.1 ), with central path $x^{\star}(t)$ for $t>0$ , defined as the solution of 

$$
\begin{array}{l l}{\mathrm{minimize}}&{t f_{0}(x)-\sum_{i=1}^{m}\log(-f_{i}(x))}\\ {\mathrm{subject~to}}&{A x=b.}\end{array}
$$ 

In this problem we explore another parametrization of the central path. For $u>p^{\star}$ , let $z^{\star}(u)$ denote the solution of 

$$
\begin{array}{r l}{\mathrm{minimize}\,\,}&{{}-\log(u-f_{0}(x))-\sum_{i=1}^{m}\log(-f_{i}(x))}\\ {\mathrm{subject~to}\,\,}&{{}A x=b.}\end{array}
$$ 

Show that the curve defined by $z^{\star}(u)$ , for $u\,>\,p^{\star}$ , is the central path. (In other words, for each $u>p^{\star}$ , there is a $t>0$ for which $x^{\star}(t)=z^{\star}(u)$ , and conversely, for each $t>0$ , there is an $u>p^{\star}$ for which $z^{\star}(u)=x^{\star}(t)$ ). 

11.11 Method of analytic centers. In this problem we consider a variation on the barrier method, based on the parametrization of the central path described in exercise 11.10 . For simplic- ity, we consider a problem with no equality constraints, 

$$
{\begin{array}{r l r l}&{{\mathrm{minimize}}}&&{f_{0}(x)}\\ &{{\mathrm{subject~to}}}&&{f_{i}(x)\leq0,\quad i=1,\ldots,m.}\end{array}}
$$ 

The method of analytic centers starts with any strictly feasible initial point $x^{(0)}$ , and any $u^{(0)}>f_{0}(x^{(0)})$ . We then set 

$$
\boldsymbol{u}^{(1)}=\boldsymbol{\theta}\boldsymbol{u}^{(0)}+(1-\boldsymbol{\theta})f_{0}(\boldsymbol{x}^{(0)}),
$$ 

where $\theta\in(0,1)$ is an algorithm parameter (usually chosen small), and then compute the next iterate as 

$$
\boldsymbol{x}^{(1)}=\boldsymbol{z}^{\star}(\boldsymbol{u}^{(1)})
$$ 

(using Newton’s method, starting from $x^{(0)}$ ). Here $z^{\star}(s)$ denotes the minimizer of 

$$
-\log(s-f_{0}(x))-\sum_{i=1}^{m}\log(-f_{i}(x)),
$$ 

which we assume exists and is unique. This process is then repeated. The point $z^{\star}(s)$ is the analytic center of the inequalities 

$$
f_{0}(x)\leq s,\quad f_{1}(x)\leq0,.\,.\,.\,,f_{m}(x)\leq0,
$$ 

hence the algorithm name. 

$x^{(k)}$ Show that the method of centers works, i.e. , converges to an optimal point. Find a stopping criterion that guarantees that $x$ is $\epsilon$ -suboptimal, where $\epsilon>0$ . 

Hint. The points $x^{(k)}$ are on the central path; see exercise 11.10 . Use this to show that 

$$
u^{+}-p^{\star}\leq\frac{m+\theta}{m+1}(u-p^{\star}),
$$ 

where $u$ and $u^{+}$ are the values of $u$ on consecutive iterations. 

11.12 Barrier method for convex-concave games. We consider a convex-concave game with inequality constraints, 

$$
\begin{array}{l r c l}{{\mathrm{minimize}_{w}\,\,\mathrm{maximize}_{z}}}&{{f_{0}(w,z)}}\\ {{\mathrm{subject~to}}}&{{f_{i}(w)\leq0,\quad i=1,.\,.\,,m}}\\ &{{\tilde{f}_{i}(z)\leq0,\quad i=1,.\,.\,,\tilde{m}.}}\end{array}
$$ 

Here $w\,\in\,\mathbf{R}^{n}$ is the variable associated with minimizing the objective, and $z\,\in\,\mathbf{R}^{\,\bar{n}}$ is the variable associated with maximizing the objective. The constraint functions $f_{i}$ and $\tilde{f}_{i}$ are convex and diﬀerentiable, and the objective function $f_{0}$ is diﬀerentiable and convex- concave, i.e. , convex in $w$ , for each $z$ , and concave in $z$ , for each $w$ . We assume for simplicity that $\mathbf{dom}\ f_{0}=\mathbf{R}^{n}\times\mathbf{R}^{n}$ . 

A solution or saddle-point for the game is a pair $w^{\star}$ , $z^{\star}$ , for which 

$$
f_{0}(w^{\star},z)\le f_{0}(w^{\star},z^{\star})\le f_{0}(w,z^{\star})
$$ 

holds for every feasible $w$ and $z$ . (For background on convex-concave games and functions, see § 5.4.3 , § 10.3.4 and exercises 3.14 , 5.24 , 5.25 , 10.10 , and 10.13 .) In this exercise we show how to solve this game using an extension of the barrier method, and the infeasible start Newton method (see 10.3 ). 

(a) Let $t>0$ . Explain why the function 

$$
t f_{0}(w,z)-\sum_{i=1}^{m}\log(-f_{i}(w))+\sum_{i=1}^{\bar{m}}\log(-\tilde{f}_{i}(z))
$$ 

is convex-concave in $(w,z)$ . We will assume that it has a unique saddle-point, $(w^{\star}(t),z^{\star}(t))$ , which can be found using the infeasible start Newton method. 

(b) As in the barrier method for solving a convex optimization problem, we can derive a simple bound on the suboptimality of $(w^{\star}(t),z^{\star}(t))$ , which depends only on the problem dimensions, and decreases to zero as $t$ increases. Let $W$ and $Z$ denote the feasible sets for $w$ and $z$ , 

$$
W=\{w\mid f_{i}(w)\leq0,\ i=1,\ldots,m\},\qquad Z=\{z\mid\tilde{f}_{i}(z)\leq0,\ i=1,\ldots,\tilde{m}\}.
$$ 

Show that 

$$
\begin{array}{r l r}{f_{0}\big(w^{\star}(t),z^{\star}(t)\big)}&{\leq}&{\displaystyle\operatorname*{inf}_{w\in W}f_{0}\big(w,z^{\star}(t)\big)+\frac{m}{t},}\\ {f_{0}\big(w^{\star}(t),z^{\star}(t)\big)}&{\geq}&{\displaystyle\operatorname*{sup}_{z\in Z}f_{0}\big(w^{\star}(t),z\big)-\frac{\tilde{m}}{t},}\end{array}
$$ 

and therefore 

$$
\operatorname*{sup}_{z\in Z}\;f_{0}\big(w^{\star}(t),z\big)-\operatorname*{inf}_{w\in W}f_{0}\big(w,z^{\star}(t)\big)\leq\frac{m+\tilde{m}}{t}.
$$ 

# Self-concordance and complexity analysis 

11.13 Self-concordance and negative entropy. 

(a) Show that the negative entropy function $x\log x$ (on $\mathbf{R}_{++}$ ) is not self-concordant. (b) Show that for any $t>0$ , $\,t x\log x-\log x$ is self-concordant (on $\mathbf{R}_{++}$ ). 

11.14 Self-concordance and the centering problem. Let $\phi$ be the logarithmic barrier function of problem ( 11.1 ). Suppose that the sublevel sets of ( 11.1 ) are bounded, and that $t f_{0}+\phi$ is closed and self-concordant. Show that $t\nabla^{2}f_{0}(x)+\nabla^{2}\phi(x)\succ0$ , for all $x\in\mathbf{dom}\,\phi$ . Hint. See exercises 9.17 and 11.3 . 

# Barrier method for generalized inequalities 

11.15 Generalized logarithm is $K$ -increasing. Let $\psi$ be a generalized logarithm for the proper cone $K$ . Suppose $y\succ_{K}0$ . 

(a) Show that $\nabla\psi({\boldsymbol{y}})\succeq_{K^{*}}0$ , i.e. , that $\psi$ is $K$ -nondecreasing. Hint. If $\nabla\psi(y)\not\perp_{K^{*}}0$ , then there is some $w\succ_{K}0$ for which $\boldsymbol{w}^{T}\nabla\boldsymbol{\psi}(\boldsymbol{y})\,\leq\,0$ . Use the inequality $\psi(s w)\leq$ $\psi(y)+\nabla\psi(y)^{T}(s w-y)$ , with $s>0$ . (b) t $\nabla\psi(y)\;\succ_{K^{*}}$ 0, $\psi$ is $K$ -increasing. Hint. Show that $\nabla^{2}\psi(y)\prec0$ ∇ ≺ 0, ∇ $\nabla\psi(y)\succeq_{K^{*}}0$ ⪰ 0 imply ∇ $\nabla\psi(y)\succ_{K^{*}}0$ ≻ 0. 

11.16 [ NN94 , page 41] Properties of a generalized logarithm. Let $\psi$ be a generalized logarithm for the proper cone $K$ , with degree $\theta$ . Prove that the following properties hold at any $y\succ_{K}0$ . 

(a) $\nabla\psi(s y)=\nabla\psi(y)/s$ for all $s>0$ . (b) $\nabla\psi(y)=-\nabla^{2}\psi(y)y$ . (c) $y^{T}\nabla\psi^{2}(y)y=-\theta$ . (d) $\nabla\psi(y)^{T}\nabla^{2}\psi(y)^{-1}\nabla\psi(y)=-\theta$ . 

11.17 Dual generalized logarithm. Let $\psi$ be a generalized logarithm for the proper cone $K$ , with degree $\theta$ . Show that the dual generalized logarithm $\psi$ , defined in ( 11.49 ), satisfies 

$$
\begin{array}{r}{\overline{{\psi}}(s v)=\psi(v)+\theta\log s,}\end{array}
$$ 

for $v\succ_{K^{*}}$ 0, $s>0$ . 

11.18 Is the function 

$$
\psi(y)=\log\left(y_{n+1}-\frac{\sum_{i=1}^{n}{y_{i}^{2}}}{y_{n+1}}\right),
$$ 

with $\begin{array}{r}{\mathbf{dom}\,\psi\,=\,\{y\,\in\,\mathbf{R}^{n+1}\,\mid\,y_{n+1}\,>\,\sum_{i=1}^{n}y_{i}^{2}\}}\end{array}$ , a generalized logarithm for the second- order cone in $\mathbf{R}^{n+1}$ ? 

# Implementation 

11.19 Yet another method for computing the Newton step. Show that the Newton step for the barrier method, which is given by the solution of the linear equations ( 11.14 ), can be found by solving a larger set of linear equations with coefficient matrix 

$$
\left[\begin{array}{c c c}{t\nabla^{2}f_{0}(x)+\sum_{i}\frac{1}{-f_{i}(x)}\nabla^{2}f_{i}(x)}&{D f(x)^{T}}&{A^{T}}\\ {D f(x)}&{-\,\mathbf{diag}(f(x))^{2}}&{0}\\ {A}&{0}&{0}\end{array}\right]
$$ 

where $f(x)=(f_{1}(x),.\,.\,.\,,f_{m}(x))$ . For what types of problem structure might solving this larger system be interesting? 

11.20 Network rate optimization via the dual problem. In this problem we examine a dual method for solving the network rate optimizat problem of $\S$ 11.8.4 . To simplify the presentation we assume that the utility functions U are strictly concave, with $\mathbf{dom}\,U_{i}\,=\,\mathbf{R}_{++}$ , and that they satisfy $U_{i}^{\prime}(x_{i})\to\infty$ →∞ as $x_{i}\to0$ and $U_{i}^{\prime}(x_{i})\rightarrow0$ → 0 as $x_{i}\to\infty$ . 

(a) Express the dual problem of ( 11.62 ) in terms of the conjugate utility functions $V_{i}=(-U_{i})^{*}$ , defined as 

$$
V_{i}(\lambda)=\operatorname*{sup}_{x>0}(\lambda x+U_{i}(x)).
$$ 

$\mathbf{\dom}\ V_{i}\ =\ -\mathbf{R}_{++}$ , and that for each $\lambda\,<\,0$ there is a unique $x$ with $U_{i}^{\prime}(x)=-\lambda$ − . 

(b) Describe a barrier method for the dual problem. Compare the complexity per iter- ation with the plexity of the od in $\S$ 11.8.4 . Distinguish the same two cases as in § 11.8.4 ( $A^{T}A$ is sparse and AA is sparse). 

# Numerical experiments 

11.21 Log-Chebyshev approximation with bounds. We consider an approximation problem: find $x\in\mathbf{R}^{n}$ , that satisfies variab nds $l\preceq x\preceq u$ , and yields $A x\approx b$ , wher $b\in\mathbf{R}^{m}$ . You can assume that l $\textit{l}\prec u$ ≺ and b $b\succ0$ ≻ 0 (for reasons we explain below). We let a $a_{i}^{T}$ denote the i th row of the matrix A . 

We judge the approximation $A x\approx b$ by the maximum fractional deviation , which is 

$$
\operatorname*{max}_{i=1,\dots,n}\operatorname*{max}\{(a_{i}^{T}x)/b_{i},b_{i}/(a_{i}^{T}x)\}=\operatorname*{max}_{i=1,\dots,n}\frac{\operatorname*{max}\{a_{i}^{T}x,b_{i}\}}{\operatorname*{min}\{a_{i}^{T}x,b_{i}\}},
$$ 

when $A x\succ0$ ; we define the maximum fractional deviation as $\infty$ if $A x\neq0$ . 

The problem of minimizing the maximum fractional deviation is called the fractional Chebyshev approximation problem , or the logarithmic Chebyshev approximation problem , since it is equivalent to minimizing the objective 

$$
\operatorname*{max}_{i=1,\dots,n}|\log a_{i}^{T}x-\log b_{i}|.
$$ 

(See also exercise 6.3 , part (c).) 

(a) Formulate the fractional Chebyshev approximation problem (with variable bounds) as a convex optimization problem with twice diﬀerentiable objective and constraint functions. (b) Implement a barrier method that solves the fractional Chebyshev approximation problem. You can assume an initial point $x^{(0)}$ , satisfying $l\prec x^{(0)}\prec u$ , $A x^{(0)}\succ0$ , is known. 

11.22 Maximum volume rectangle inside a polyhedron. Consider the problem described in exer- cise 8.16 , i.e. , finding the maximum volume rectangle ${\mathcal R}=\{x\ |\ l\preceq x\preceq u\}$ that lies in a polyhedron described by a set of linear inequalities, P ${\mathcal{P}}=\{x\mid A x\preceq b\}$ { ⪯ } . Implement a barrier m d for g this problem You can as me that b $b\succ0$ ≻ 0, which means that for small l $l\prec0$ ≺ 0 and u $u\succ0$ ≻ 0, the rectangle R lies inside P . 

Test your implementation on several simple examples. Find the maximum volume rect- angle that lies in the polyhedron defined by 

$$
A=\left[\begin{array}{r r}{{0}}&{{-1}}\\ {{2}}&{{-4}}\\ {{2}}&{{1}}\\ {{-4}}&{{4}}\\ {{-4}}&{{0}}\end{array}\right],\qquad b={\bf1}.
$$ 

Plot this polyhedron, and the maximum volume rectangle that lies inside it. 

11.23 SDP bounds and heuristics for the two-way partitioning problem. In this exercise we consider the two-way partitioning problem ( 5.7 ), described on page 219 , and also in ex- ercise 5.39 : 

$$
\begin{array}{l r c l}{{\mathrm{minimize}}}&{{\boldsymbol x}^{T}{\boldsymbol W}{\boldsymbol x}}\\ {{\mathrm{subject~to}}}&{{\boldsymbol x}_{i}^{2}=1,\quad i=1,\ldots,n,}\end{array}
$$ 

iable $x\,\in\,\mathbf{R}^{\,n}$ . We assume, without loss of generality, that $W\,\in\,{\bf S}^{n}$ s isfies $W_{i i}\,=\,0$ = 0. We denote the optimal value of the partitioning problem as p $p^{\star}$ , and x $x^{\star}$ will denote an optimal partition. (Note that $-x^{\star}$ is also an optimal partition.) 

The Lagrange dual of the two-way partitioning problem ( 11.65 ) is given by the SDP 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad-\mathbf{1}^{T}\nu}\\ &{\mathrm{subject~to}\quad W+\mathbf{diag}(\nu)\succeq0,}\end{array}
$$ 

with variable $\nu\in\mathbf{R}^{n}$ . The dual of this SDP is 

$$
{\begin{array}{l r l}&{{\mathrm{minimize}}}&{\operatorname{\mathbf{tr}}(W X)}\\ &{{\mathrm{subject~to}}}&{X\succeq0}\\ &{}&{X_{i i}=1,\quad i=1,\dots,n,}\end{array}}
$$ 

with variable $X~\in~\mathbf{S}^{n}$ . (This SDP can be interpreted as a relaxation of the two-way partitioning problem ( 11.65 ); see exercise 5.39 .) The optimal values of these two SDPs are equal, and give a lower bound, which we denote $d^{\star}$ , on the optimal value $p^{\star}$ . Let $\nu^{\star}$ and $X^{\star}$ denote optimal points for the two SDPs. 

(a) Implement a barrier method that solves the SDP ( 11.66 ) and its dual ( 11.67 ), given the weight matrix $W$ . Explain how you obtain nearly optimal $\nu$ and $X$ , give for- mulas for any Hessians and gradients that your method requires, and explain how you compute the Newton step. Test your implementation on some small problem instances, comparing the bound you find with the optimal value (which can be found by checking the objective value of all $2^{n}$ partitions). Try your implementation on a randomly chosen problem instance large enough that you cannot find the optimal partition by exhaustive search ( e.g. , $n=100$ ). (b) $A$ heuristic for partitioning. In exercise 5.39 , you found that if $X^{\star}$ has rank one, then it must have the form $X^{\star}\,=\,x^{\star}(x^{\star})^{T}$ , where $x^{\star}$ is optimal for the two-way partitioning problem. This suggests the following simple heuristic for finding a good partition (if not the best): solve the SDPs above, to find $X^{\star}$ (and the bound $d^{\star}$ ). Let $v$ denote an eigenvector of $X^{\star}$ associated with its largest eigenvalue, and let ${\hat{x}}=\mathbf{sign}(v)$ ). The vector x is our guess for a good partition. Try this heuristic on some small problem instances, and the large problem instance you used in part (a). Compare the objective value of your heuristic partition, $\hat{x}^{T}W\hat{x}$ , with the lower bound $d^{\star}$ . (c) $A$ randomized method. Another heuristic technique for finding a good partition, given the solution $X^{\star}$ of the SDP ( 11.67 ), is based on randomization . The method is simple: we generate independent samples $x^{(1)},\ldots,x^{(K)}$ from a normal distribution on $\mathbf{R}^{n}$ , with zero mean and covariance $X^{\star}$ . For each sample we consider the heuristic approximate solution ${\hat{x}}^{(k)}=\mathbf{sign}(x^{(k)})$ ). We then take the best among these, i.e. , the one with lowest cost. Try out this procedure on some small problem instances, and the large problem instance you considered in part (a). (d) $A$ uristic refinement. Suppose you are given a partition $x$ , i.e. , $x_{i}\in\{-1,1\}$ , $i=1,\dots,n$ . How does the objective value change if we move element i from one set to the other, i.e. , change $x_{i}$ to $-x_{i}$ ? Now consider the following simple greedy algorithm: given a starting partition $x$ , move the element that gives the largest reduction in the objective. Repeat this procedure until no reduction in objective can be obtained by moving an element from one set to the other. Try this heuristic on some problem instances, including the large one, starting from various initial partitions, including $x=\mathbf{1}$ , the heuristic approximate solution found in part (b), and the randomly generated approximate solutions found in part (c). How much does this greedy refinement improve your approximate solutions from parts (b) and (c)? 

11.24 Barrier and primal-dual interior-point methods for quadratic programming. Implement a barrier method, and a primal-dual method, for solving the QP (without equality con- straints, for simplicity) 

$$
{\begin{array}{l r c l}{{\mathrm{minimize}}}&{(1/2)x^{T}P x+q^{T}x}\\ {{\mathrm{subject~to}}}&{A x\preceq b,}\end{array}}
$$ 

with $A\in\mathbf{R}^{m\times n}$ . You can assume a strictly feasible initial point is given. Test your codes on several examples. For the barrier method, plot the duality gap versus Newton steps. For the primal-dual interior-point method, plot the surrogate duality gap and the norm of the dual residual versus iteration number. 

# Appendices 
## A. Mathematical background 
In this appendix we give a brief review of some basic concepts from analysis and linear algebra. The treatment is by no means complete, and is meant mostly to set out our notation. 
### A.1 Norms 
### A.1.1 Inner product, Euclidean norm, and angle 
The standard inner product on $\mathbf{R}^{n}$ , the set of real $n$ -vectors, is given by 

$$
\langle x,y\rangle=x^{T}y=\sum_{i=1}^{n}x_{i}y_{i},
$$ 
for $x,y\;\in\;\mathbf{R}^{n}$ . In this book we use notation $x^{T}y$ , instead of $\langle x,y\rangle$ . 

The Euclidean norm , or $\ell_{2}$ -norm, of a vector $x\in\mathbf{R}^{n}$ is defined as 

$$
\|x\|_{2}=(x^{T}x)^{1/2}=(x_{1}^{2}+\cdot\cdot\cdot+x_{n}^{2})^{1/2}.
$$ 
> 一个向量的欧几里得范数定义为向量自己和自己的标准内积的开根号


The Cauchy-Schwartz inequality states that $|x^{T}y|\leq\|x\|_{2}\|y\|_{2}$ for any $x,y\in\mathbf{R}^{n}$ 
> 柯西-施瓦茨不等式：对于任意 $x, y\in\mathbf R^n$，二者的内积的绝对值不大于二者各自欧式范数的乘积


The (unsigned) angle between nonzero vectors $x,y\in\mathbf{R}^{n}$ is defined as 

$$
\angle(x,y)=\cos^{-1}\left(\frac{x^{T}y}{\|x\|_{2}\|y\|_{2}}\right),
$$ 
where we take $\cos^{-1}(u)\in[0,\pi]$ . We say $x$ and $y$ are orthogonal if $x^{T}y=0$ 
> 向量归一化后的内积定义了向量之间夹角的余弦值，余弦值范围为 $[0,\pi]$，即向量之间的夹角范围为 $[0, 180]$ 度


The standard inner product on ${\bf R}^{m\times n}$ , the set of $m\times n$ real matrices, is given by 

$$
\langle X,Y\rangle=\mathbf{tr}(X^{T}Y)=\sum_{i=1}^{m}\sum_{j=1}^{n}X_{i j}Y_{i j},
$$ 
for $X,Y\in\mathbf{R}^{m\times n}$ . (Here $\mathbf{tr}$ denotes trace of a matrix, i.e. , the sum of its diagonal elements.) We use the notation $\mathbf{tr}(X^{T}Y)$ instead of $\langle X,Y\rangle$ . Note that the inner product of two matrices is the inner product of the associated vectors, in $\mathbf{R}^{m n}$ , obtained by listing the coefficients of the matrices in some order, such as row major. 
> 对于形状相同的矩阵，定义两个矩阵之间的标准内积为矩阵按元素相乘然后求和，也可以写为矩阵乘法之后求迹
> 因此两个 $\mathbf R^{m\times n}$ 的矩阵之间的内积就是它们在 $\mathbf R^{mn}$ 中相关的向量之间的内积


The Frobenius norm of a matrix $X\in\mathbf{R}^{m\times n}$ is given by 

$$
\|X\|_{F}=\left(\mathbf{tr}(X^{T}X)\right)^{1/2}=\left(\sum_{i=1}^{m}\sum_{j=1}^{n}X_{i j}^{2}\right)^{1/2}.
$$ 
The Frobenius norm is the Euclidean norm of the vector obtained by listing the coefficients of the matrix. (The $\ell_{2}$ - norm of a matrix is a diﬀerent norm; see $\S$ A.1.5 .) 
> 一个矩阵的弗罗比尼乌斯范数定义为矩阵自己和自己的标准内积的开根号
> 因此矩阵的弗罗比尼乌斯范数就是它相关的向量的欧几里得范数 (矩阵的 $l_2$ 范数另有定义)


The standard inner product on $\mathbf{S}^{n}$ , the set of symmetric $n\times n$ matrices, is given by 

$$
\langle X,Y\rangle=\operatorname{tr}(X Y)=\sum_{i=1}^{n}\sum_{j=1}^{n}X_{i j}Y_{i j}=\sum_{i=1}^{n}X_{i i}Y_{i i}+2\sum_{i<j}X_{i j}Y_{i j}.
$$ 
### A.1.2 Norms, distance, and unit ball 
A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ with $\mathbf{dom}\,f=\mathbf{R}^{\,n}$ is called a norm if 

- $f$ is nonnegative: $f(x)\geq0$ for all $x\in\mathbf{R}^{n}$ 
- $f$ is definite: $f(x)=0$ only if $x=0$ 
- $f$ is homogeneous: $f(t x)=|t|f(x)$ , for all $x\in\mathbf{R}^{n}$ and $t\in\mathbf{R}$ 
- $f$ satisfies the triangle inequality: $f(x+y)\leq f(x)+f(y)$ , for all $x,y\in\mathbf{R}^{n}$ 

We use the notation $f(x)\;=\;\|x\|$ , whic is meant to suggest that a norm is a generalization of the absolute value on $\mathbf R$ . When we specify a particular norm, we use the notation $\|{\boldsymbol{x}}\|_{\mathrm{symb}}$ , where the subscript is a mnemonic to indicate which norm is meant. 
> 一个从 $\mathbf R^n$ 映射到 $\mathbf R$ 的函数，如果满足：
> 非负：$\forall x\in \mathbf R^n, f (x) \ge 0$
> 正定：$f (x) = 0$ 仅在 $x=0$ 时成立
> 齐次：$\forall x \in \mathbf R^n, t \in \mathbf R, f (tx) = |t|f (x)$
> 三角不等式：$f (x+y)\le f (x) + f (y)$
> 则该函数是一个范数，范数是对于 $\mathbf R$ 中的绝对值概念的泛化，记为 $\|x\|$


A norm is a measure of the length of a vector $x$ ; we can measure the distance between two vectors $x$ and $y$ as the length of their diﬀerence, i.e. , 

$$
\mathbf{dist}(x,y)=\|x-y\|.
$$ 
We refer to $\mathbf{dist}(x,y)$ as the distance between $x$ and $y$ , in the norm $\|\cdot\|$ . 
> 范数是向量的长度的衡量，我们通过两个向量之间的差的范数来衡量两个向量之间的距离


The set of all vectors with norm less than or equal to one, 

$$
{\mathcal{B}}=\{x\in\mathbf{R}^{n}\mid\|x\|\leq1\},
$$ 
is called the unit ball of the norm $\|\cdot\|$ . 

The unit ball satisfies the following properties: 
 
 - $\mathcal{B}$ is symmetric about the origin, i.e. , $x\in{\mathcal{B}}$ if and only if $-x\in{\mathcal{B}}$ 
 - $\mathcal{B}$ is convex 
 - $\mathcal{B}$ is closed, bounded, and has nonempty interior 

> 某个范数的单位球定义为该范数下所有的范数值不大于1的向量集合

Conversely, if $C\subseteq\mathbf{R}^{n}$ is any set satisfying these three conditions, then it is the unit ball of a norm, which is given by 

$$
\left\|x\right\|=\left(\operatorname*{sup}\{t\geq0\mid t x\in C\}\right)^{-1}.
$$ 
### A.1.3 Examples 
The simplest example of a norm is the absolute value on $\mathbf{R}$ . Another simple example is the Euclidean or $\ell_{2}$ -norm on $\mathbf{R}^{n}$ , defined above in ( A.1 ). 
Two other frequently used norms on $\mathbf{R}^{n}$ are the sum-absolute-value , or $\ell_{1}$ -norm , given by 

$$
\|x\|_{1}=|x_{1}|+\cdot\cdot\cdot+|x_{n}|,
$$ 
and the Chebyshev or $\ell_{\infty}$ -norm , given by 

$$
\|x\|_{\infty}=\operatorname*{max}\{|x_{1}|,.\,.\,.\,,|x_{n}|\}.
$$ 
> 常见的向量范数：欧几里得范数、$\mathscr l_1$ 范数，$\mathscr l_{\infty}$ 范数
> $\mathbf R^n$ 中的 $\mathscr l_1$ 范数定义为向量的各个成分的绝对值的和，切比雪夫/ $\mathscr l_{\infty}$ 范数定义为向量的各个成分的绝对值的最大值


These three norms are part of a family parametrized by a constant traditionally denoted $p$ , with $p\geq1$ : the $\ell_{p}$ -norm is defined by 

$$
\|x\|_{p}=(|x_{1}|^{p}+\cdot\cdot\cdot+|x_{n}|^{p})^{1/p}.
$$ 
This yields the $\ell_{1}$ -norm when $p=1$ and the Euclidean norm when $p=2$ . 

It is easy to show that for any $x\in\mathbf{R}^{n}$ , 

$$
\operatorname*{lim}_{p\to\infty}\|x\|_{p}=\operatorname*{max}\{|x_{1}|,.\,.\,.\,,|x_{n}|\},
$$

so the $\ell_{\infty}$ -norm also fits in this family, as a limit. 
> 证明：夹挤定理


Anothe important family of norms are the quadratic norms . For $P\in\mathbf{S}_{++}^{n}$ , we define the $P$ -quadratic norm as 

$$
\|x\|_{P}=(x^{T}P x)^{1/2}=\|P^{1/2}x\|_{2}.
$$ 
The unit ball of a quadratic norm is an ellipsoid (and conversely, if the unit ball of a norm is an ellipsoid, the norm is a quadratic norm). 
> 对于一个 $n$ 阶的对称正定矩阵 $P$，定义和它相关的 $P$ -二次范数/二次型
> 二次范数的单位球是一个椭球，一个范数的单位球是椭球时，该范数是二次范数
> (二次型 $x^TP x = \sum_{i}\sum_j p_{ij}x_ix_j$)


Some common norms on ${\bf R}^{m\times n}$ are the Frobenius norm, defined above in ( A.2 ), the sum-absolute-value norm, 

$$
\|X\|_{\mathrm{sav}}=\sum_{i=1}^{m}\sum_{j=1}^{n}|X_{i j}|,
$$

and the maximum-absolute-value norm, 

$$
||X||_{\operatorname{max}}=\operatorname*{max}\{|X_{i j}|\ |\ i=1,.\,.\,,m,\ j=1,.\,.\,,n\}.
$$ 
We will encounter several other important norms of matrices in $\S$ A.1.5 . 
> 常见的矩阵范数：弗罗比尼乌斯范数、绝对值和范数 (对应向量的 $\mathscr l_1$ 范数)、最大绝对值范数 (对应向量的 $\mathscr l_{\infty}$ 范数)
### A.1.4 Equivalence of norms 
Suppose that $||\cdot||_{\mathrm{a}}$ and $||\cdot||_{\mathrm{b}}$ are norms on $\mathbf{R}^{n}$ . 
A basic result of analysis is that there exist positive constants $\alpha$ and $\beta$ such that, for all $x\in\mathbf{R}^{n}$ , 

$$
\alpha\|{\boldsymbol{x}}\|_{\mathrm{a}}\leq\|{\boldsymbol{x}}\|_{\mathrm{b}}\leq\beta\|{\boldsymbol{x}}\|_{\mathrm{a}}.
$$ 
This means that the norms are equivalent , i.e. , they define the same set of open subsets, the same set of convergent sequences, and so on (see § A.2 ). (We conclude that any norms on any finite-dimensional vector space are equivalent, but on infinite-dimensional vector spaces, the result need not hold.) 
> 对于 $\mathbf R^n$ 中的两个范数 $\|\cdot \|_a, \|\cdot \|_b$，如果存在正数 $\alpha, \beta$ 使得对于所有的 $x \in \mathbf R^n$ 都有 $\alpha \|x\|_a \le \|x\|_b \le \beta \|x\|_a$，则说明这两个范数等价，它们定义了相同的开放子集、相同的收敛序列
> 这个结论在有限维度向量空间成立，在无限维度向量空间则不一定成立


Using convex analysis, we can give a more specific result: If $\|\cdot\|$ is any norm on $\mathbf{R}^{n}$ , then there exists a quadratic norm $||\cdot||_{P}$ for which 

$$
\|x\|_{P}\leq\|x\|\leq{\sqrt{n}}\|x\|_{P}
$$

holds for all $x$ . In other words, any norm on $\mathbf{R}^{n}$ can be uniformly approximated, within a factor of $\sqrt{n}$ , by a quadratic norm. (See 8.4.1 .) 
### A.1.5 Operator norms 
Suppose $||\cdot||_{\mathrm{a}}$ an $||\cdot||_{\mathrm{b}}$ norms on $\mathbf{R}^{m}$ and $\mathbf{R}^{n}$ , respectively. We define the operator norm of $X\in\mathbf{R}^{m\times n}$ , induced by the norms $||\cdot||_{\mathrm{a}}$ and $\|\cdot\|_{\mathrm{b}}$ , as 

$$
\|X\|_{\mathrm{a,b}}=\operatorname*{sup}\left\{\|X u\|_{\mathrm{a}}\;|\;\|u\|_{\mathrm{b}}\leq1\right\}.
$$ 
(It can be shown that this defines a norm on ${\bf R}^{m\times n}$ .) 
> 定义范数 $\|\cdot \|_a, \|\cdot \|_b$ 导出的对于 $X\in \mathbb R^{m\times n}$ 的算子范数

When $||\cdot||_{\mathrm{a}}$ and $||\cdot||_{\mathrm{b}}$ are both Euclidean norms, the operator norm of $X$ is its maximum singular value , and is denoted $\|X\|_{2}$ : 

$$
\|X\|_{2}=\sigma_{\mathrm{max}}(X)=(\lambda_{\mathrm{max}}(X^{T}X))^{1/2}.
$$ 

(This agrees with the Euclidean norm on $\mathbf{R}^{m}$ , when $X\,\in\,\mathbf{R}^{m\times1}$ , so there no clash of notation.) This norm is also called the spectral norm or $\ell_{2}$ -norm of X . 

As another example, the norm induced by the $\ell_{\infty}$ -norm on $\mathbf{R}^{m}$ and $\mathbf{R}^{n}$ , denoted $\|X\|_{\infty}$ , is the max-row-sum norm , 

$$
\|X\|_{\infty}=\operatorname*{sup}\left\{\|X u\|_{\infty}\mid\|u\|_{\infty}\leq1\right\}=\operatorname*{max}_{i=1,\ldots,m}\sum_{j=1}^{n}|X_{i j}|.
$$ 

The norm induced by the $\ell_{1}$ -norm on $\mathbf{R}^{m}$ and $\mathbf{R}^{n}$ , denoted $\|X\|_{1}$ , is the max- column-sum norm , 

$$
\|X\|_{1}=\operatorname*{max}_{j=1,\ldots,n}\sum_{i=1}^{m}|X_{i j}|.
$$ 
### A.1.6 Dual norm
Let $\|\cdot\|$ be a norm on $\mathbf{R}^{n}$ . The associated dual norm , denoted $||\cdot||_{*}$ , is defined as

$$
\|z\|_{*}=\operatorname*{sup}\{z^{T}x\mid\|x\|\leq1\}.
$$ 
(This can be shown to be a norm.) The dual norm can be interpreted as the operator norm of $z^{T}$ , i erpreted as a $1\times n$ matrix, with the norm $||\cdot||$ on $\mathbf{R}^{n}$ , and the absolute value on R : 

$$
\|z\|_{*}=\operatorname*{sup}\{|z^{T}x|\mid\|x\|\leq1\}.
$$ 

From the definition of dual norm we have the inequality 

$$
z^{T}x\leq\|x\|\,\|z\|_{*},
$$ 

which holds for all $x$ and $z$ . This inequality is tight, in the following sense: for any $x$ there is a $z$ for which the inequality holds with equality. (Similarly, for any $\mathcal{Z}$ there is an $x$ that gives equality.) The dual of the dual norm is the original norm: we have $\|x\|_{**}=\|x\|$ for all $x$ . (This need not hold in infinite-dimensional vector spaces.) 

The dual of the Euclidean norm is the Euclidean norm, since 

$$
\operatorname*{sup}\{z^{T}x\mid\|x\|_{2}\leq1\}=\|z\|_{2}.
$$ 

(This follows from the Cauchy-Schwarz inequality; for nonzero $z$ , the value of $x$ that maximizes $z^{T}x$ er $\|{\boldsymbol{x}}\|_{2}\leq1$ $z/\|z\|_{2}$ .) 

The dual of the ℓ $\ell_{\infty}$ -norm is the ℓ $\ell_{1}$ -norm: 

$$
\operatorname*{sup}\{z^{T}x\mid\|x\|_{\infty}\leq1\}=\sum_{i=1}^{n}|z_{i}|=\|z\|_{1},
$$ 

and the dual of the $\ell_{1}$ -norm is the $\ell_{\infty}$ -norm. More generally, the dual of the $\ell_{p}$ -norm is the $\ell_{q}$ -norm, where $q$ satisfies $1/p+1/q=1$ , i.e. , $q=p/(p-1)$ . 

As another example, consider the $\ell_{2}$ - or spectral norm on ${\bf R}^{m\times n}$ . The associated dual norm is 

$$
\|Z\|_{2*}=\operatorname*{sup}\{\mathbf{tr}(Z^{T}X)\mid\|X\|_{2}\leq1\},
$$ 

which turns out to be the sum of the singular values, 

$$
\|Z\|_{2*}=\sigma_{1}(Z)+\cdot\cdot\cdot+\sigma_{r}(Z)=\mathbf{tr}(Z^{T}Z)^{1/2},
$$ 

where $r=\mathbf{rank}\,Z$ . This norm is sometimes called the nuclear norm. 

## A.2 Analysis 
### A.2.1 Open and closed sets 
An element $x\in C\subseteq\mathbf{R}^{n}$ is called an interior point of $C$ if there exists an $\epsilon>0$ for which 

$$
\{y\mid\|y-x\|_{2}\leq\epsilon\}\subseteq C,
$$ 
i.e. , there exists a ball centered at $x$ that lies entirely in $C$ . 
> 存在一个以 $x$ 为球心的球，球内所有点都在集合 $C$ 内，则点 $x$ 就是集合 $C$ 的内点

The set of all points interior to $C$ is called the interior of $C$ and is denoted $\operatorname{int}C$ . (Since all norms on $\mathbf{R}^{n}$ are equivalent to the Euclidean norm, all norms generate the same set of interior points.) 
> 集合 $C$ 的所有内点构成的集合称为它的内部，记为 $\text{int}C$
> 因为 $\mathbf R^n$ 中所有的范数都等价于欧几里得范数，因此所有的范数都生成相同的内点集合

A set $C$ is open if $\mathrm{int}\,C=C$ , i.e. , every point in $C$ is an interior point. A set $C\subseteq\mathbf{R}^{n}$ is closed if its complement $\mathbf{R}^{n}\setminus C=\{x\in\mathbf{R}^{n}\mid x\notin C\}$ is open. 
> 如果 $\text{int}C = C$，则称集合 $C$ 开放
> 若集合 $C$ 的补集是开放集，则集合 $C$ 是封闭的

The closure of a set $C$ is defined as 

$$
\operatorname{cl}C=\mathbf{R}^{n}\setminus\operatorname{int}(\mathbf{R}^{n}\setminus C),
$$ 
i.e. , the complement of the interior of the complement of $C$ . 
> 集合 $C$ 的闭包定义为它的补集的内点的补集

A point $x$ is in the closure of $C$ if for every $\epsilon>0$ , there is a $y\in C$ with $\|x-y\|_{2}\leq\epsilon$ . 
> 如果一个点属于 $C$ 的闭包，则以这个点为球心的任意小的球内都会至少包含 $C$ 中的一个点

We can also describe closed sets and the closure in terms of convergent sequences and limit points. A set $C$ is closed if and only if it contains the limit point of every convergent sequence in it. I other words, if $x_{1},x_{2},.\cdot\cdot.$ converges to $x$ , and $x_{i}\in C$ , then $x\in C$ . The closure of C is the set of all limit points of convergent sequences in C . 
> 用收敛序列描述：一个集合是封闭的当且仅当它包含了集合中所有收敛序列的极限点
> 集合 $C$ 的闭包就是 $C$ 中所有收敛序列的极限点构成的集合

The boundary of the set $C$ is defined as 

$$
\mathbf{bd}\,C=\mathbf{cl}\,C\setminus\mathbf{int}\,C.
$$ 
> 集合 $C$ 的边界定义为它的闭包减去它的内部

A boundary point $x$ ( i.e. , a point $x\in\mathbf{bd}\,C$ ) satisfies the following property: For all $\epsilon$ > 0, there exists $y\in C$ and $z\not\in C$ with 

$$
\|y-x\|_{2}\leq\epsilon,\qquad\|z-x\|_{2}\leq\epsilon,
$$ 
i.e. , there exist arbitrarily close points in $C$ , and also arbitrarily close points not in $C$ . We can characterize closed and open sets in terms of the boundary operation: $C$ is closed if it contain its boundary, i.e. , bd $C\subseteq C$ . It is open if it contains no boundary points, i.e. , $C\cap\mathbf{bd}\,C=\emptyset$ . 
> 集合 $C$ 的边界点满足离它任意近的所有点中，有属于集合 $C$ 的，也有不属于集合 $C$ 的
> 开集合和它的边界的交集是空集，闭集合包含了它的边界
### A.2.2 Supremum and infimum 
Suppose $C\subseteq\mathbf{R}$ . A number $a$ is a upper bound on $C$ if for each $x\in C$ , $x\leq a$ . The set of upper bounds on a set C is either empty (in which case we say C is unbounded above), all of $\mathbf{R}$ (only when $C=\emptyset$ ), or a closed infinite interval $[b,\infty)$ . The number b is called the least upper bound or supremum of the set C , and is denoted $\operatorname{supp}C$ . We take $\operatorname*{sup}\varnothing=-\infty$ , and $\operatorname*{sup}C=\infty$ if $C$ is unbounded above. When $\operatorname*{sup}C\in C$ , we say the supremum of C is attained or achieved. 
> 对于 $C\subseteq \mathbf R$，如果一个数 $a$ 是 $C$ 的上界，说明它大于等于 $C$ 中的任意元素
> 集合 $C$ 的最小上界即上界中最小的那个点

When the set $C$ is finite, $\operatorname{supp}C$ is the maximum of its elements. Some authors use the notation $\operatorname*{max}C$ to denote supremum, when it is attained, but we follow standard mathematical convention, using $\operatorname*{max}C$ only when the set $C$ is finite. 
> 如果集合 $C$ 有限，则它的最小上界就是它最大的元素

We define lower bound, and infimum, in a similar way. A number $a$ is a lower bound on $C\subseteq\mathbf{R}$ if for each $x\in C$ , $a\leq x$ . The infimum ( greatest lower bound ) of a set $C\subseteq\mathbf{R}$ is defined as i $\operatorname{pf}C=-\operatorname{sup}(-C)$ . When C is finite, the infimum is the minimum of its elements. We take $\operatorname*{inf}\varnothing\,=\,\infty$ , and $\operatorname*{inf}C\,=\,-\infty$ if $C$ is unbounded below, i.e. , has no lower bound. 
> 最大下界的定义同理
## A.3 Functions 
### A.3.1 Function notation 
Our notation for functions is mostly standard, with one exception. When we write 

$$
f:A\rightarrow B
$$ 
we mean that $f$ is a function on the set $\mathbf{dom}\ f\subseteq A$ into the set $B$ ; in particular we can have $\mathbf{dom}\ f$ a proper subset of the set $A$ . Thus the notation $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{n n}$ means that $f$ maps (some) $n$ -vectors into $m$ -vectors; it does not mean that $f(x)$ is defined for every $x\in\mathbf{R}^{n}$ . 
> $f: A\to B$ 表示 $f$ 将定义域 $\mathbf{dom}f \subseteq A$ 中的元素映射到集合 $B$ 中，并且定义域 $\mathbf{dom}f$ 允许是 $A$ 的真子集，也就是不要求 $f$ 对于 $A$ 中的每一个元素都有定义

This convention is similar to function declarations in computer languages. Specifying the data types of the input and output arguments of a function gives the syntax of that function; it does not guarantee that any input argument with the specified data type is valid. 

As an example consider the function $f:\mathbf{S}^{n}\rightarrow\mathbf{R}$ , given by 

$$
f(X)=\log\operatorname*{det}X,\tag{A.3}
$$ 
with $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ . The notation $f:\mathbf{S}^{n}\rightarrow\mathbf{R}$ specifies the syntax of $f$ : it takes as argument a symmetric $n\times n$ matrix, and returns a real number. The notation $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ specifies which symmetric $n\times n$ matrices are valid input arguments for $f$ ( i.e. , only positive definite ones). The formula ( A.3 ) specifies what $f(X)$ is, for $X\in\mathbf{dom}\,f$ . 
### A.3.2 Continuity 
A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ is continuous at $x\in\mathbf{dom}\,f$ if for all $\epsilon>0$ there exists a $\delta$ such that 

$$
y\in\mathbf{dom}\,f,\quad\|y-x\|_{2}\leq\delta\implies\|f(y)-f(x)\|_{2}\leq\epsilon.
$$ 

> 函数在 $x$ 点处连续：对于任意小的 $\epsilon$ ，都存在 $\delta$ ，使得点 $x$ 的 $\delta$ 邻域内的点的像和 $f (x)$ 之间的距离不大于 $\epsilon$
> 也就是无论函数值有多靠近 $f(x)$，都可以在 $x$ 的周围找到原像点


Continuity can be described in terms of limits: whenever the sequence $x_{1},x_{2},.\cdot\cdot.$ in $\mathbf{dom}\ f$ converges to a point $x\in\mathbf{dom}\,f$ , the sequence $f(x_{1}),f(x_{2}),.\,.\,.$ converges to $f(x)$ , i.e. , 

$$
\operatorname*{lim}_{i\to\infty}f(x_{i})=f(\operatorname*{lim}_{i\to\infty}x_{i}).
$$ 
> 用极限描述：当一个原像点序列收敛到 $x$ 时，像点序列也收敛到 $f (x)$

A function $f$ is continuous if it is continuous at every point in its domain. 
### A.3.3 Closed functions 
A function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is said to be closed if, for each $\alpha\in\mathbf{R}$ , the sublevel set 

$$
\{x\in\operatorname{dom}f\ |\ f(x)\leq\alpha\}
$$ 
is closed. This is equivalent to the condition that the epigraph of $f$ , 

$$
\mathbf{epi}\,f=\{(x,t)\in\mathbf{R}^{n+1}\mid x\in\mathbf{dom}\,f,\ f(x)\leq t\},
$$ 

is closed. (This definition is general, but is usually only applied to convex func- tions.) 

If $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ is continuous, and $\mathbf{dom}\ f$ is closed, then $f$ is closed. If $f:\mathbf{R}^{n}\rightarrow$ $\mathbf{R}$ is continuous, with $\mathbf{dom}\ f$ open, then $f$ is closed if and only if $f$ converges to $\infty$ along every sequence converging to a boundary point of $\mathbf{dom}\ f$ . In other words, if $\textstyle\operatorname*{lim}_{i\to\infty}x_{i}=x\in\mathbf{bd}\,\mathbf{dom}\,f$ , with $x_{i}\in\mathbf{dom}\,f$ , we have $\begin{array}{r}{\operatorname*{lim}_{i\to\infty}f(x_{i})=\infty}\end{array}$ . 

Example A.1 Examples on $\mathbf{R}$ 

• The function $f:\mathbf{R}\rightarrow\mathbf{R}$ , with $f(x)=x\log x$ , $\mathbf{dom}\ f=\mathbf{R}_{++}$ , is not closed. • The function $f:\mathbf{R}\rightarrow\mathbf{R}$ , with 

$$
f(x)={\left\{\begin{array}{l l}{x\log x}&{x>0}\\ {0}&{x=0,}\end{array}\right.}\qquad\mathbf{dom}\ f=\mathbf{R}_{+},
$$ 

is closed. 

• The function $f(x)=-\log x$ , $\mathbf{dom}\ f=\mathbf{R}_{++}$ , is closed. 

## A.4 Derivatives 
### A.4.1 Derivative and gradient 
Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{m}$ and $x\in\operatorname{int}\mathbf{dom}\,f$ . The function $f$ is diﬀerentiable at $x$ if there exists a matrix $D f(x)\in\mathbf{R}^{m\times n}$ that satisfies 

$$
\operatorname*{lim}_{z\in{\bf d o m}\,f,\,z\neq x,\,\,z\to x}\frac{\|f(z)-f(x)-D f(x)(z-x)\|_{2}}{\|z-x\|_{2}}=0,\tag{A.4}
$$ 
in which case we refer to $D f(x)$ as the derivative (or Jacobian ) of $f$ at $x$ . (There can be at most one matrix that satisfies ( A.4 ).) The function $f$ is diﬀerentiable if $\mathbf{dom}\ f$ is open, and it is diﬀerentiable at every point in its domain. 
> 如果函数 $f: \mathbf R^n \to \mathbf R^m$ 在 $x$ 处可导，说明存在满足式 A.4 的矩阵 $D f (x)\in \mathbf R^{m\times n}$
> 我们称 $Df (x)$ 为函数 $f$ 在点 $x$ 处的导数/雅可比矩阵
> 如果 $\textbf{dom}f$ 是开集，且 $f$ 在 $\textbf{dom}f$ 上处处可导，则函数 $f$ 是可微的

The affine function of $z$ given by 

$$
f(x)+D f(x)(z-x)
$$ 
is called the first-order approximation of $f$ at (or near) $x$ . Evidently this function agrees with $f$ at $z=x$ ; when $z$ is close to $x$ , this affine function is very close to $f$ . 
> 形式为 $f (x) + Df (x)(z-x)$ 的关于 $z$ 的仿射函数称为 $f$ 在 $x$ 处的一阶近似
> 显然当 $z=x$ 时，其函数值与 $f$ 相同，当 $z$ 接近 $x$ 时，该仿射函数与 $f$ 接近

The derivative can be found by deriving the first-order approximation of the function $f$ at $x$ ( i.e. , the matrix $D f(x)$ that satisfies ( A.4 )), or from partial derivatives: 

$$
D f(x)_{i j}={\frac{\partial f_{i}(x)}{\partial x_{j}}},\qquad i=1,\ldots,m,\quad j=1,\ldots,n.
$$ 
> $f$ 的导数/雅可比矩阵可以通过式 A.4 推导出函数 $f$ 在 $x$ 处的一阶近似来计算，也可以根据偏导数来计算
#### Gradient 
When $f$ is real-valued ( i.e. , $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ ) the derivative $D f(x)$ is a $1\times n$ matrix, i.e. , it is a row vector. Its transpose is called the gradient of the function: 

$$
\nabla f(x)=D f(x)^{T},
$$ 
which is a (column) vector, i.e. , in $\mathbf{R}^{n}$ . 
> 如果 $f$ 是一个实值函数，即 $m=1$ 时，其导数 $Df (x)$ 就是一个 $1\times n$ 的矩阵，即是一个行向量
> 我们将该行向量的转置记为该函数的梯度向量 $\nabla f (x)$

Its components are the partial derivatives of $f$ : 

$$
\nabla f({\boldsymbol{x}})_{i}={\frac{\partial f({\boldsymbol{x}})}{\partial x_{i}}},\quad i=1,\ldots,n.
$$ 
> 梯度向量也由 $f$ 的偏导数构成

The first-order approximation of $f$ at a point $x\in\operatorname{int}\mathbf{dom}f$ can be expressed as (the affine function of $z$ ) 

$$
f(x)+\nabla f(x)^{T}(z-x).
$$ 
> $f$ 在点 $x\in\textbf{dom}f$ 上的一阶近似（关于$z$ 的仿射函数）
#### Examples 
As a simple example consider the quadratic function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , 

$$
f(x)=(1/2)x^{T}P x+q^{T}x+r,
$$ 
where $P\in\mathbf{S}^{n}$ , $q\in\mathbf{R}^{n}$ , and $r\in\mathbf{R}$ . Its derivative at $x$ is the row vector $D f(x)=$ $x^{T}P+q^{T}$ , and its gradient is 

$$
\nabla f(x)=P x+q.
$$ 
As a more interesting example, we consider the function $f:\mathbf{S}^{n}\rightarrow\mathbf{R}$ , given by 

$$
f(X)=\log\operatorname*{det}X,\qquad\mathbf{dom}\,f=\mathbf{S}_{++}^{n}.
$$ 
One (tedious) way to find the gradient of $f$ is to introduce a basis for $\mathbf{S}^{n}$ , find the gradient of the associated function, and finally translate the result back to $\mathbf{S}^{n}$ . Instead, we will dire y find th pproximation of $f$ at $X\in\mathbf{S}_{++}^{n}$ . Let $Z\in\mathbf{S}_{++}^{n}$ be close to X , and let ∆ $\Delta X=Z-X$ (which is assumed to be small). We have 

$$
\begin{array}{l l l}{\log\operatorname*{det}Z}&{=}&{\log\operatorname*{det}(X+\Delta X)}\\ &{=}&{\log\operatorname*{det}\Big(X^{1/2}(I+X^{-1/2}\Delta X X^{-1/2})X^{1/2}\Big)}\\ &{=}&{\log\operatorname*{det}X+\log\operatorname*{det}(I+X^{-1/2}\Delta X X^{-1/2})}\\ &{=}&{\log\operatorname*{det}X+\displaystyle\sum_{i=1}^{n}\log(1+\lambda_{i}),}\end{array}
$$ 
where $\lambda_{i}$ is the $i$ th eigenvalue of $X^{-1/2}\Delta X X^{-1/2}$ . Now we use the fact that $\Delta X$ is small, which implies $\lambda_{i}$ are small, so to first order we have $\log(1+\lambda_{i})\approx\lambda_{i}$ . Using this first-order approximation in the expression above, we get 

$$
\begin{array}{l l l}{\log\operatorname*{det}Z}&{\approx}&{\log\operatorname*{det}X+\displaystyle\sum_{i=1}^{n}\lambda_{i}}\\ &{=}&{\log\operatorname*{det}X+\mathbf{tr}(X^{-1/2}\Delta X X^{-1/2})}\\ &{=}&{\log\operatorname*{det}X+\mathbf{tr}(X^{-1}\Delta X)}\\ &{=}&{\log\operatorname*{det}X+\mathbf{tr}\left(X^{-1}(Z-X)\right),}\end{array}
$$ 
where we have used the fact that the sum of the eigenvalues is the trace, and the property $\mathbf{tr}(A B)=\mathbf{tr}(B A)$ . 

Thus, the first-order approximation of $f$ at $X$ is the affine function of $Z$ given by 

$$
f(Z)\approx f(X)+\mathbf{tr}\left(X^{-1}(Z-X)\right).
$$ 
Noting that the second term on the righthand side is the standard inner product of $X^{-1}$ and $Z-X$ , we can identify $X^{-1}$ as the gradient of $f$ at $X$ . Thus, we can write the simple formula 

$$
\nabla f(X)=X^{-1}.
$$ 
This result should not be surprising, since the derivative of $\log x$ , on $\mathbf{R}_{++}$ , is $1/x$ . 
### A.4.2 Chain rule 
Suppose $f\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}^{m}$ is diﬀerentiable at $x\;\in\;\operatorname{int}\mathbf{dom}\,f$ an $g\,:\,\mathbf{R}^{m}\,\rightarrow\,\mathbf{R}^{p}$ n p is diﬀerentiable at $f(x)\,\in\,\operatorname{int}\mathbf{dom}\,g$ . Define the composition h $h\,:\,\mathbf{R}^{n}\,\rightarrow\,\mathbf{R}^{p}$ → by $h(z)=g(f(z))$ . Then h is diﬀerentiable at $x$ , with derivative 

$$
D h(x)=D g(f(x))D f(x).
$$ 
As an example, suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $g:\mathbf{R}\rightarrow\mathbf{R}$ , and $h(x)=g(f(x))$ . Taking the transpose of $D h(x)=D g(f(x))D f(x)$ yields 

$$
\nabla h(x)=g^{\prime}(f(x))\nabla f(x).
$$ 
#### Composition with affine function 
Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{n n}$ is diﬀerentiable, $A\in\mathbf{R}^{n\times p}$ , and $b\in\mathbf{R}^{n}$ . Define $g:\mathbf{R}^{\nu}\rightarrow$ m $\mathbf{R}^{m}$ as $g(x)=f(A x+b)$ , with $\mathbf{dom}\,g=\{x\mid A x+b\in\mathbf{dom}\,f\}$ . The derivative of $g$ is, by the chain rule ( A.5 ), $D g(x)=D f(A x+b)A$ . 

When $f$ is real-valued ( i.e. , $m=1$ ), we obtain the formula for the gradient of a composition of a function with an affine function, 

$$
\nabla g(x)=A^{T}\nabla f(A x+b).
$$ 
le, suppose that $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $x,v\,\in\,\mathbf{R}^{n}$ , and we define the function

 $\tilde{f}:{\mathbf{R}}\rightarrow{\mathbf{R}}$ → by $\tilde{f}(t)=f(x+t v)$ ). (Roughly speaking, $\tilde{f}$ is $f$ , restricted to the line

 $\{x+t v\mid t\in\mathbf{R}\}$ .) Then we have 

$$
D\tilde{f}(t)=\tilde{f}^{\prime}(t)=\nabla f(x+t v)^{T}v.
$$ 
(The scalar $\ddot{f}^{\prime}(0)$ (0) is the directional derivative of $f$ , at $x$ , in the direction $v$ .) Example A.2 Consider the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , with $\mathbf{dom}\,f=\mathbf{R}^{n}$ and 

$$
f(\boldsymbol{x})=\log\sum_{i=1}^{m}\exp(a_{i}^{T}\boldsymbol{x}+b_{i}),
$$ 
where $a_{1},.\,.\,.\,,a_{m}\,\in\,\mathbf{R}^{n}$ , and $b_{1},.\,.\,.\,,b_{m}\,\in\,\mathbf{R}$ . We can find a simple sion for its gradient by noting that it is the composition of the affine function $A x+b$ , where $A\,\in\,\mathbf{R}^{m\times n}$ with rows $a_{1}^{T},\cdot\cdot\cdot,a_{m}^{T}$ , and the function $g:\mathbf{R}^{m}\rightarrow\mathbf{R}$ given by $g(y)=$ $\begin{array}{r}{\log\!\left(\sum_{i=1}^{m}\exp{y_{i}}\right)}\end{array}$ ). Simple diﬀerentiation (or the formula ( A.6 )) shows that 

$$
\nabla g(y)=\frac{1}{\sum_{i=1}^{m}\exp y_{i}}\left[\begin{array}{c}{\exp y_{1}}\\ {\vdots}\\ {\exp y_{m}}\end{array}\right],
$$ 
so by the composition formula we have 

$$
\nabla f(x)=\frac{1}{\mathbf{1}^{T}z}A^{T}z
$$ 
where $z_{i}=\exp(a_{i}^{T}x+b_{i})$ ), $i=1,\ldots,m$ . 

Example A.3 We derive an expression for $\nabla f(x)$ , where 

$$
f(x)=\log\operatorname*{det}(F_{0}+x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}),
$$ 
where $F_{0},.\,.\,.\,,F_{n}\in\mathbf{S}^{p}$ , and 

$$
\mathbf{dom}\ f=\{x\in\mathbf{R}^{n}\mid F_{0}+x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}\succ0\}.
$$ 
The function $f$ is the composition of the affine mapping from $x\in\mathbf{R}^{n}$ to $F_{0}+x_{1}F_{1}+$ $\cdot\cdot+x_{n}F_{n}\in\mathbf{S}^{p}$ , with the function $\log\operatorname*{det}X$ . We use the chain rule to evaluate 

$$
\frac{\partial f(\boldsymbol{x})}{\partial x_{i}}=\mathbf{tr}(F_{i}\nabla\log\operatorname*{det}(\boldsymbol{F}))=\mathbf{tr}(F^{-1}F_{i}),
$$ 
where $F=F_{0}+x_{1}F_{1}+\cdot\cdot\cdot+x_{n}F_{n}$ . Thus we have 

$$
\nabla f({\boldsymbol{x}})=\left[\begin{array}{c}{\mathbf{tr}({\boldsymbol{F}}^{-1}{\boldsymbol{F}}_{1})}\\ {\vdots}\\ {\mathbf{tr}({\boldsymbol{F}}^{-1}{\boldsymbol{F}}_{n})}\end{array}\right].
$$ 
### A.4.3 Second derivative 
In this section we review the second derivative of a real-valued function $f:\mathbf{R}^{n}\rightarrow \mathbf R$  . The second derivative or Hessian matrix of $f$ at $x\ \in\ \operatorname{int}\mathbf{dom}\,f$ , denoted $\nabla^{2}f(x)$ , is given by 

$$
\nabla^{2}f(x)_{i j}={\frac{\partial^{2}f(x)}{\partial x_{i}\partial x_{j}}},\qquad i=1,\ldots n,\quad j=1,\ldots,n,
$$ 
provided $f$ is twice diﬀerentiable at $x$ , where the partial derivatives are evaluated at $x$ . 
> 实值函数 $f$ 在 $x\in\text{int}\textbf{dom}f$ 上的二阶导数/海森矩阵记作 $\nabla^2 f (x)$


The second-order approximation of $f$ , at or near $x$ , is the quadratic function of $z$ defined by 

$$
\widehat{f}(z)=f(x)+\nabla f(x)^{T}(z-x)+(1/2)(z-x)^{T}\nabla^{2}f(x)(z-x).
$$ 
> $f$ 在 $x$ 附近的二阶近似是关于 $z$ 的二次函数，定义如上

This second-order approximation satisfies 

$$
\operatorname*{lim}_{z\in{\bf d o m}\,f,\,\,z\neq x,\,\,z\to x}\frac{|f(z)-\widehat{f}(z)|}{\|z-x\|_{2}^{2}}=0.
$$ 
Not surprisingly, the second derivative can be interpreted as the derivative of the first derivative. If $f$ is diﬀerentiable, the gradient mapping is the function $\nabla f:\mathbf{R}^{n}\rightarrow\mathbf{R}^{n}$ , with $\mathbf{dom}\,\nabla f=\mathbf{dom}\,f$ , with value $\nabla f(x)$ at $x$ . The derivative of this mapping is 

$$
D\nabla f(x)=\nabla^{2}f(x).
$$ 
#### Examples 
As a simple example consider the quadratic function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , 

$$
f(x)=(1/2)x^{T}P x+q^{T}x+r,
$$ 
where $P\in\mathbf{S}^{n}$ , $q\in\mathbf{R}^{n}$ , and $r\in\mathbf{R}$ . Its gradient is $\nabla f(x)=P x+q$ , so its Hessian is given by $\nabla^{2}f(x)=P$ . The second-order approximation of a quadratic function is itself. 

As a more complicated example, we consider again the function $f:\mathbf{S}^{n}\rightarrow\mathbf{R}$ , given by $f(X)=\log\operatorname*{det}X$ , with $\mathbf{dom}\,f=\mathbf{S}_{++}^{n}$ . To find the second-order approxi- mation (and therefore, the Hessian), we will derive a first-order approximation of the gradient, $\nabla f(X)=X^{-1}$ . For $Z\in\mathbf{S}_{++}^{n}$ near $X\in\mathbf{S}_{++}^{n}$ , and $\Delta X=Z-X$ , we have 

$$
\begin{array}{l c l}{{Z^{-1}}}&{{=}}&{{(X+\Delta X)^{-1}}}\\ {{\ }}&{{=}}&{{\left(X^{1/2}(I+X^{-1/2}\Delta X X^{-1/2})X^{1/2}\right)^{-1}}}\\ {{\ }}&{{=}}&{{X^{-1/2}(I+X^{-1/2}\Delta X X^{-1/2})^{-1}X^{-1/2}}}\\ {{\ }}&{{\approx}}&{{X^{-1/2}(I-X^{-1/2}\Delta X X^{-1/2})X^{-1/2}}}\\ {{\ }}&{{=}}&{{X^{-1}-X^{-1}\Delta X X^{-1},}}\end{array}
$$ 
using the first-order approximation $(I+A)^{-1}\approx I-A$ , valid for $A$ small. 

This approximation is enough for us to identify the Hessian of $f$ at $X$ . The Hessian is a quadratic form on $\mathbf{S}^{n}$ . Such a quadratic form is cumbersome to de- scribe in the general case, since it requires four indices. But from the first-order approximation of the gradient above, the quadratic form can be expressed as 

$$
-\operatorname{\mathbf{\sigma}}(X^{-1}U X^{-1}V),
$$ 
where $U,V\,\in\,\mathbf{S}^{n}$ are the arguments of the quadratic form. (This generalizes the expression for the scalar case: $(\log x)^{\prime\prime}=-1/x^{2}$ .) 

Now we have the second-order approximation of $f$ near $X$ : 

$$
\begin{array}{r c l}{f(Z)}&{=}&{f(X+\Delta X)}\\ &{\approx}&{f(X)+\mathbf{tr}(X^{-1}\Delta X)-(1/2)\,\mathbf{tr}(X^{-1}\Delta X X^{-1}\Delta X)}\\ &{\approx}&{f(X)+\mathbf{tr}\left(X^{-1}(Z-X)\right)-(1/2)\,\mathbf{tr}\left(X^{-1}(Z-X)X^{-1}(Z-X)\right).}\end{array}
$$ 
### A.4.4 Chain rule for second derivative 
A general chain rule for the second derivative is cumbersome in most cases, so we will state it only for some special cases that we will need. 
#### Composition with scalar function 
Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $g:\mathbf{R}\rightarrow\mathbf{R}$ , and $h(x)=g(f(x))$ . Simply working out the partial derivatives yields 

$$
\nabla^{2}h(x)=g^{\prime}(f(x))\nabla^{2}f(x)+g^{\prime\prime}(f(x))\nabla f(x)\nabla f(x)^{T}.
$$ 
#### Composition with affine function 
Suppose $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ , $A\in\mathbf{R}^{n\times m}$ , and $b\in\mathbf{R}^{n}$ . Define $g:\mathbf{R}^{m}\rightarrow\mathbf{R}$ by $g(x)=$ $f(A x+b)$ . Then we have 

$$
\nabla^{2}g(x)=A^{T}\nabla^{2}f(A x+b)A.
$$ 

As an example, consider the restriction of a real-valued function $f$ to a line, i.e. , the function $\tilde{f}(t)=f(x+t v)$ ), where $x$ and $v$ are fixed. Then we have 

$$
\begin{array}{r}{\nabla^{2}\tilde{f}(t)=\tilde{f}^{\prime\prime}(t)=v^{T}\nabla^{2}f(x+t v)v.}\end{array}
$$ 

Example A.4 We consider the function $f:\mathbf{R}^{n}\rightarrow\mathbf{R}$ from example A.2 , 

$$
f(x)=\log\sum_{i=1}^{m}\exp(a_{i}^{T}x+b_{i}),
$$ 

where $a_{1},.\,.\,.\,,a_{m}\in\mathbf{R}^{n}$ , and $b_{1},.\,.\,.\,,b_{m}\in\mathbf{R}$ . By noting that $f(x)=g(A x+b)$ , where $\begin{array}{r}{g(y)=\log\bigl(\sum_{i=1}^{m}\exp y_{i}\bigr)}\end{array}$ ), we can obtain a simple formula for the Hessian of $f$ . Taking partial derivatives, or using the formula ( A.8 ), noting that $g$ is the composition of m log with $\textstyle\sum_{i=1}^{m^{u}}\exp y_{i}$ , yields 

$$
\begin{array}{r}{\nabla^{2}g(y)=\mathbf{diag}(\nabla g(y))-\nabla g(y)\nabla g(y)^{T},}\end{array}
$$ 

where $\nabla g(y)$ is given in ( A.7 ). By the composition formula we have 

$$
\nabla^{2}f(x)=A^{T}\left({\frac{1}{\mathbf{1}^{T}z}}\,\mathbf{diag}(z)-{\frac{1}{(\mathbf{1}^{T}z)^{2}}}z z^{T}\right)A,
$$ 

where $z_{i}=\exp(a_{i}^{T}x+b_{i})$ ), $i=1,\ldots,m$ . 

## A.5 Linear algebra 
### A.5.1 Range and nullspace 
Let $A\in\mathbf{R}^{m\times n}$ ( i.e. , $A$ is a real matrix with $m$ rows and $n$ columns). The range of $A$ , denoted ${\mathcal{R}}(A)$ , is the set of all vectors in $\mathbf{R}^{m}$ that can be written as linear combinations of the columns of $A$ , i.e. , 

$$
{\mathcal{R}}(A)=\{A x\mid x\in\mathbf{R}^{n}\}.
$$ 
> 实矩阵 $A\in \mathbf R^{m\times n}$ 的列空间/范围指的是所有可以写为 $A$ 的列向量的线性组合的向量的集合

The range ${\mathcal{R}}(A)$ is a subspace of $\mathbf{R}^{m}$ , i.e. , it is it lf a vector space. Its dimension is the rank of A , denoted rank A . The rank of A can never be greater than the minimum of $m$ and $n$ . We say $A$ has full rank if $\operatorname{rank}A=\operatorname*{min}\{m,n\}$ . 
> $A$ 的列空间是一个向量空间，且是 $\mathbf R^m$ 的子空间，它的维度定义为 $A$ 的秩

The nullspace (or kernel ) of $A$ , denoted ${\mathcal{N}}(A)$ , is the set of all vectors $x$ mapped into zero by A : 

$$
{\mathcal{N}}(A)=\{x\mid A x=0\}.
$$ 
The nullspace is a subspace of $\mathbf{R}^{n}$ . 
> $A$ 的零空间是和 $A$ 的行空间正交的向量空间
#### Orthogonal decomposition induced by $A$ 
If $\mathcal{V}$ is a subspace of $\mathbf{R}^{n}$ , its orthogonal complement , denoted $\mathcal{V}^{\perp}$ , is defined as 

$$
{\mathcal{V}}^{\perp}=\{x\mid z^{T}x=0{\mathrm{~for~all~}}z\in{\mathcal{V}}\}.
$$ 
(As one would expect of a complement, we have $\mathcal{V}^{\perp\perp}=\mathcal{V}$ 

A basic result of linear algebra is that, for any A $A\in\mathbf{R}^{m\times n}$ ∈ , we have 

$$
{\mathcal{N}}(A)={\mathcal{R}}(A^{T})^{\perp}.
$$ 
(Applying the result to $A^{T}$ we also have ${\mathcal{R}}(A)={\mathcal{N}}(A^{T})^{\perp}$ .) This result is often stated as 

$$
{\mathcal{N}}(A)\;{\stackrel{\perp}{\oplus}}\;{\mathcal{R}}(A^{T})=\mathbf{R}^{n}.
$$ 
$\overset{\perp}{\oplus}$ Here the symbol ⊕ refers to orthogonal direct sum e. , the sum of two subspaces that are orthogonal. The decomposition ( A.9 ) of R is called the orthogonal de- composition induced by $A$ . 
### A.5.2 Symmetric eigenvalue decomposition 
Suppose $A\in\mathbf{S}^{\mathcal{N}}$ , i.e. , $A$ is a real symmetric $n\times n$ matrix. Then $A$ can be factored as 

$$
A=Q\Lambda Q^{T},
$$ 
where $Q\in\mathbf{R}^{n\times n}$ is orthogonal , i.e. , satisfies $Q^{T}Q=I$ , and $\boldsymbol{\Lambda}=\mathbf{diag}(\lambda_{1},.\,.\,.\,,\lambda_{n})$ . The (real) numbers $\lambda_{i}$ are the eigenvalues of $A$ , and are the roots of the charac- teristic polynom al $\operatorname*{det}(s I\,-\,A)$ . The columns of $Q$ form an orthonormal set of eigenvectors of A . The factorization ( A.10 ) is called the spectral decomposition or (symmetric) eigenvalue decomposition of $A$ . 

We order th eigenvalues as $\lambda_{1}\,\geq\,\lambda_{2}\,\geq\,\cdot\cdot\,\geq\,\lambda_{n}$ . We use the notation $\lambda_{i}(A)$ to refer to the i th largest eigenvalue of A $A\,\in\,{\bf S}$ ∈ S . We usually write the largest or maximum eigenvalue as $\lambda_{1}(A)=\lambda_{\operatorname*{max}}(A)$ , and the least or minimum eigenvalue as $\lambda_{n}(A)=\lambda_{\operatorname*{min}}(A)$ . 

The determinant and trace can be expressed in terms of the eigenvalues, 

$$
\operatorname*{det}A=\prod_{i=1}^{n}\lambda_{i},\qquad\operatorname{tr}A=\sum_{i=1}^{n}\lambda_{i},
$$ 

as can the spectral and Frobenius norms, 

$$
\|A\|_{2}=\operatorname*{max}_{i=1,\ldots,n}|\lambda_{i}|=\operatorname*{max}\{\lambda_{1},-\lambda_{n}\},\qquad\|A\|_{F}=\left(\sum_{i=1}^{n}\lambda_{i}^{2}\right)^{1/2}.
$$ 

# Definiteness and matrix inequalities 

The largest and smallest eigenvalues satisfy 

$$
\lambda_{\operatorname*{max}}(A)=\operatorname*{sup}_{x\neq0}{\frac{x^{T}A x}{x^{T}x}},\qquad\lambda_{\operatorname*{min}}(A)=\operatorname*{inf}_{x\neq0}{\frac{x^{T}A x}{x^{T}x}}.
$$ 

In particular, for any $x$ , we have 

$$
\lambda_{\operatorname*{min}}(A)x^{T}x\leq x^{T}A x\leq\lambda_{\operatorname*{max}}(A)x^{T}x,
$$ 

with both inequalities tight for (diﬀerent) choices of $x$ . 

A matrix $A\,\in\,\mathbf{S}^{n}$ is called positive definite if for all $x\neq0$ , $x^{T}A x\,>\,0$ . We denote this as $A\succ0$ . By the inequality above, see that $A\succ0$ if and only a its eigenvalues are positive, i.e. , $\lambda_{\operatorname*{min}}(A)>0$ If − $-A$ is positive definite, we say A is negative definite , which w write as A $A\prec0$ ≺ 0. We use $\mathbf{S}_{++}^{n}$ to denote the set of positive definite matrices in S $\mathbf{S}^{n}$ . 

If $A$ satisfies $x^{T}A x\;\geq\;0$ for all $x$ , we say that $A$ is positive semidefinite or nonnegat e definite . If − $-A$ is nonnegative definite, i.e. , if $x^{T}A x\leq0$ for all $x$ , we say that A is negative semidefinite or nonpositive definite . We use $\mathbf{S}_{+}^{n}$ to denote the set of nonnegative definite matrices in $\mathbf{S}^{n}$ . 

For $A,B\in\mathbf{S}^{n}$ , we use $A\prec B$ to mean $B-A\succ0$ , and so on. These inequal- ities are called matrix inequalities , or generalized inequalities associated with the positive semidefinite cone. 

# Symmetric squareroot 

Let $A\in\mathbf{S}_{+}^{n}$ , with eigenvalue decomposition $A=Q\,\mathbf{diag}(\lambda_{1},.\,.\,.\,,\lambda_{n})Q^{T}$ . We define the (symmetric) squareroot of $A$ as 

$$
A^{1/2}=Q\,\mathbf{diag}(\lambda_{1}^{1/2},.\,.\,.\,,\lambda_{n}^{1/2})Q^{T}.
$$ 

The squareroot $A^{1/2}$ is the unique symmetric positive semidefinite solution of the equation $X^{2}=A$ . 

# A.5.3 Generalized eigenvalue decomposition 

The generalized eigenvalues of a pair of symmetric matrices $(A,B)\in\mathbf{S}^{n}\times\mathbf{S}^{n}$ are defined as the roots of the polynomial $\operatorname*{det}(s B-A)$ . 

We are usually interested in matrix pairs with $B\;\in\;\mathbf{S}_{++}^{n}$ . In this case the generalized eigenvalues are also the eigenvalues of $B^{-1/2}A B^{-1/2}$ (which are real). As with the standard eigenvalue decomposition, we order the generalized eigen- values in nonincreasing o $\lambda_{1}\,\geq\,\lambda_{2}\,\geq\,\cdot\cdot\cdot\,\geq\,\lambda_{n}$ , and denote the maximum generalized eigenvalue by $\lambda_{\operatorname*{max}}(A,B)$ ). 

When $B\in\mathbf{S}_{++}^{n}$ , the pair of matrices can be factored as 

$$
A=V\Lambda V^{T},\qquad B=V V^{T},
$$ 

where $V\,\in\,\mathbf{R}^{n\times n}$ is nonsingular, and $\boldsymbol{\Lambda}\:=\:\mathbf{diag}(\lambda_{1},.\,.\,.\,,\lambda_{n})$ , where $\lambda_{i}$ are the generalized eigenvalues of the pair $(A,B)$ . The decomposition ( A.11 ) is called the generalized eigenvalue decomposition . 

The generalized eigenvalue decomposition is related to the standard eigenvalue decomposition of the matrix $B^{-1/2}A B^{-1/2}$ . If $Q\Lambda Q^{T}$ is the eigenvalue decompo- sition of $B^{-1/2}A B^{-1/2}$ , then ( A.11 ) holds with $V=B^{1/2}Q$ . 

# A.5.4 Singular value decomposition 

Suppose $A\in\mathbf{R}^{m\times n}$ with rank $A=r$ . Then $A$ can be factored as 

$$
A=U\Sigma V^{T},
$$ 

where $U\,\in\,\mathbf{R}^{m\times r}$ satisfies $U^{T}U\;=\;I$ , $V\,\in\,\mathbf{R}^{n\times r}$ satisfies $V^{T}V\,=\,I$ , and $\Sigma\,=$ $\mathbf{diag}(\sigma_{1},.\,.\,.\,,\sigma_{r})$ , with 

$$
\sigma_{1}\geq\sigma_{2}\geq\cdot\cdot\cdot\geq\sigma_{r}>0.
$$ 

The factorization ( A.12 ) is called the singular value decomposition (SVD) of $A$ . The columns of $U$ are called left singular vectors of $A$ , the columns of $V$ are right singular vectors , and the numbers $\sigma_{i}$ are the singular values . The singular value decomposition can be written 

$$
A=\sum_{i=1}^{r}\sigma_{i}u_{i}v_{i}^{T},
$$ 

where $u_{i}\,\in\,\mathbf{R}^{m}$ are the left singular vectors, and $v_{i}\,\in\,\mathbf{R}^{n}$ are the right singular vectors. 

The singular value decomposition of a matrix $A$ is closely related to the eigen- value decomposition of the (symmetric, nonnegative definite) matrix $A^{T}A$ . Us- ing ( A.12 ) we can write 

$$
\boldsymbol{A}^{T}\boldsymbol{A}=\boldsymbol{V}\boldsymbol{\Sigma}^{2}\boldsymbol{V}^{T}=\left[\begin{array}{l l}{\boldsymbol{V}}&{\tilde{\boldsymbol{V}}}\end{array}\right]\left[\begin{array}{l l}{\boldsymbol{\Sigma}^{2}}&{\boldsymbol{0}}\\ {\boldsymbol{0}}&{\boldsymbol{0}}\end{array}\right]\left[\begin{array}{l l}{\boldsymbol{V}}&{\tilde{\boldsymbol{V}}}\end{array}\right]^{T},
$$ 

where $\ddot{V}$ is any matrix for which $[V\ \tilde{V}]$ ] is orthogonal. The righthand expression is the eigenvalue decomposition of $A^{T}A$ , so we conclude that its nonzero eigenvalues are the singular values of $A$ squared, and the associated eigenvectors of $A^{T}A$ are the right singular vectors of $A$ . A similar analysis of $A A^{T}$ shows that its nonzero eigenvalues are also the squares of the singular values of $A$ , and the associated eigenvectors are the left singular vectors of $A$ . 

The first or largest singular value is also written as $\sigma_{\mathrm{max}}(A)$ . It can be expressed as 

$$
\sigma_{\operatorname*{max}}(A)=\operatorname*{sup}_{x,y\neq0}{\frac{x^{T}A y}{\|x\|_{2}\|y\|_{2}}}=\operatorname*{sup}_{y\neq0}{\frac{\|A y\|_{2}}{\|y\|_{2}}}.
$$ 

The righthand expression shows that the maximum singular value is the $\ell_{2}$ operator norm of $A$ . The minimum singular value of $A\in\mathbf{R}^{m\times n}$ is given by 

$$
\sigma_{\mathrm{min}}(A)=\left\{\begin{array}{l l}{\sigma_{r}(A)}&{r=\mathrm{min}\{m,n\}}\\ {0}&{r<\mathrm{min}\{m,n\},}\end{array}\right.
$$ 

which is positive if and only if $A$ is full rank. 

The singular values of a symmetric matrix are the absolute values of its nonzero eigenvalues, sorted into descending order. The singular values of a symmetric positive semidefinite matrix are the same as its nonzero eigenvalues. 

The condition number of a nonsingular $A\in\mathbf{R}^{n\times n}$ , denoted $\mathbf{cond}(A)$ or $\kappa(A)$ , is defined as 

$$
\operatorname{cond}(A)=\|A\|_{2}\|A^{-1}\|_{2}=\sigma_{\operatorname*{max}}(A)/\sigma_{\operatorname*{min}}(A).
$$ 

# Pseudo-inverse 

Let $A=U\Sigma V^{T}$ be the singular value decomposition of $A\in\mathbf{R}^{m\times n}$ , with rank $A=$ $r$ . We define the pseudo-inverse or Moore-Penrose inverse of $A$ as 

$$
A^{\dagger}=V\Sigma^{-1}U^{T}\in\mathbf{R}^{n\times m}.
$$ 

Alternative expressions are 

$$
{\cal A}^{\dagger}=\operatorname*{lim}_{\epsilon\rightarrow0}({\cal A}^{T}{\cal A}+\epsilon{\cal I})^{-1}{\cal A}^{T}=\operatorname*{lim}_{\epsilon\rightarrow0}{\cal A}^{T}({\cal A}{\cal A}^{T}+\epsilon{\cal I})^{-1},
$$ 

where the limits are taken with $\epsilon\ >\ 0\$ , which ensures that the inverses in the expressions exist. If $\mathbf{rank}\,A\,=\,n$ , then $A^{\dagger}=(A^{T}A)^{-1}A^{T}$ . If $\mathbf{rank}\,A\,=\,m$ , then $A^{\dagger}=A^{T}(A A^{T})^{-1}$ . If $A$ is square and nonsingular, then $A^{\dagger}=A^{-1}$ . 

The pseudo-inverse comes up in problems involving least-squares, minimum norm, quadratic minimization, and (Euclidean) projection. For example, $A^{\dagger}b$ is a solution of the least-squares problem 

$$
{\mathrm{minimize}}\quad\|A x-b\|_{2}^{2}
$$ 

in general. When the solution is not unique, $A^{\dagger}b$ gives the solution with minimum (Euclidean) norm. As another example, the matrix $A A^{\dagger}=U U^{T}$ gives (Euclidean) projection on ${\mathcal{R}}(A)$ . The matrix $A^{\dagger}A\,=\,V V^{T}$ gives (Euclidean) projection on ${\mathcal{R}}(A^{T})$ . 

The optimal value $p^{\star}$ of the (general, nonconvex) quadratic optimization prob- lem 

$$
\begin{array}{r l}{\mathrm{minimize~}}&{{}(1/2)x^{T}P x+q^{T}x+r,}\end{array}
$$ 

where $P\in\mathbf{S}^{n}$ , can be expressed as 

$$
\begin{array}{r}{p^{\star}=\left\{\begin{array}{l l}{-(1/2)q^{T}P^{\dagger}q+r}&{P\succeq0,\quad q\in\mathcal{R}(P)}\\ {-\infty}&{\mathrm{otherwise}.}\end{array}\right.}\end{array}
$$ 

(This generalizes the expression $p^{\star}=-(1/2)q^{T}P^{-1}q+r$ , valid for $P\succ0$ .) 

# A.5.5 Schur complement 

Consider a matrix $X\in\mathbf{S}^{n}$ partitioned as 

$$
\boldsymbol{X}=\left[\begin{array}{c c}{\boldsymbol{A}}&{\boldsymbol{B}}\\ {\boldsymbol{B}^{T}}&{\boldsymbol{C}}\end{array}\right],
$$ 

where $A\in\mathbf{S}^{k}$ . If $\operatorname*{det}A\neq0$ , the matrix 

$$
S=C-B^{T}A^{-1}B
$$ 

is called the Schur complement of $A$ in $X$ . Schur complements arise in several contexts, and appear in many important formulas and theorems. For example, we have 

$$
\operatorname*{det}X=\operatorname*{det}A\operatorname*{det}S.
$$ 

# Inverse of block matrix 

The Schur complement comes up in solving linear equations, by eliminating one block of variables. We start with 

$$
\begin{array}{r}{\left[\begin{array}{c c}{A}&{B}\\ {B^{T}}&{C}\end{array}\right]\left[\begin{array}{c}{x}\\ {y}\end{array}\right]=\left[\begin{array}{c}{u}\\ {v}\end{array}\right],}\end{array}
$$ 

and assume that $\operatorname*{det}A\,\neq\,0$ . If we eliminate $x$ from the top block equation and substitute it into the bottom block equation, we obtain $v=B^{T}A^{-1}u+S y$ , so 

$$
y=S^{-1}(v-B^{T}A^{-1}u).
$$ 

Substituting this into the first equation yields 

$$
x=\left(A^{-1}+A^{-1}B S^{-1}B^{T}A^{-1}\right)u-A^{-1}B S^{-1}v.
$$ 

We can express these two equations as a formula for the inverse of a block matrix: 

$$
\left[\begin{array}{c c}{A}&{B}\\ {B^{T}}&{C}\end{array}\right]^{-1}=\left[\begin{array}{c c}{A^{-1}+A^{-1}B S^{-1}B^{T}A^{-1}}&{-A^{-1}B S^{-1}}\\ {-S^{-1}B^{T}A^{-1}}&{S^{-1}}\end{array}\right].
$$ 

In particular, we see that the Schur complement is the inverse of the $2,2$ block entry of the inverse of $X$ . 

# Minimization and definiteness 

The Schur complement arises when you minimize a quadratic form over some of the variables. Suppose $A\succ0$ , and consider the minimization problem 

$$
{\mathrm{minimize}}\quad u^{T}A u+2v^{T}B^{T}u+v^{T}C v
$$ 

with variable $u$ . The solution is $u=-A^{-1}B v$ , and the optimal value is 

$$
\operatorname*{inf}_{\boldsymbol{u}}\ \left[\begin{array}{l}{\boldsymbol{u}}\\ {\boldsymbol{v}}\end{array}\right]^{T}\left[\begin{array}{c c}{\boldsymbol{A}}&{\boldsymbol{B}}\\ {\boldsymbol{B}^{T}}&{\boldsymbol{C}}\end{array}\right]\left[\begin{array}{l}{\boldsymbol{u}}\\ {\boldsymbol{v}}\end{array}\right]=\boldsymbol{v}^{T}\boldsymbol{S}\boldsymbol{v}.
$$ 

From this we can derive the following characterizations of positive definiteness or semidefiniteness of the block matrix $X$ : 

• $X\succ0$ if and only if $A\succ0$ and $S\succ0$ . • If $A\succ0$ , then $X\succeq0$ if and only if $S\succeq0$ . 

# Schur complement with singular $A$ 

Some Schur complement results have generalizations to the case when $A$ is singular, although the details are more complicated. As an example, if $A\succeq0$ and $B v\ \in$ ${\mathcal{R}}(A)$ , then the quadratic minimization problem ( A.13 ) (with variable $u$ ) is solvable, and has optimal value 

$$
v^{T}(C-B^{T}A^{\dagger}B)v,
$$ 

where $A^{\dagger}$ is the pseudo-inverse of $A$ . The problem is unbounded if $B v\notin\mathcal{R}(A)$ or if $A\not\geq0$ . 

The range condition $B v\,\in\,{\mathcal{R}}(A)$ can also be expressed as $(I-A A^{\dagger})B v\,=\,0$ , so we have the following characterization of positive semidefiniteness of the block matrix $X$ : 

$$
X\succeq0\quad\Longleftrightarrow\quad A\succeq0,\quad(I-A A^{\dagger})B=0,\quad C-B^{T}A^{\dagger}B\succeq0.
$$ 

Here e matrix $C-B^{T}A^{\dagger}B$ serves as a generalization of the Schur complement, when A is singular. 

# Bibliography 

Some basic references for the material in this appendix are Rudin [ Rud76 ] for analysis, and Strang [ Str80 ] and Meyer [ Mey00 ] for linear algebra. More advanced linear algebra texts include Horn and Johnson [ HJ85 , HJ91 ], Parlett [ Par98 ], Golub and Van Loan [ GL89 ], Trefethen and Bau [ TB97 ], and Demmel [ Dem97 ]. 

The concept of closed function ( § A.3.3 ) appears frequently in convex optimization, al- though the terminology varies. The term is used by Rockafellar [ Roc70 , page 51], Hiriart- Urruty and Lemar´ echal [ HUL93 , volume 1, page 149], Borwein and Lewis [ BL00 , page 76], and Bertsekas, Nedi´ c, and Ozdaglar [ Ber03 , page 28]. 

# Appendix B 

# Problems involving two quadratic functions 

In this appendix we consider some optimization problems that involve two quadratic, but not necessarily convex, functions. Several strong results hold for these prob- lems, even when they are not convex. 

# B.1 Single constraint quadratic optimization 

We consider the problem with one constraint 

$$
\begin{array}{r l}&{\mathrm{minimize}\quad\ x^{T}A_{0}x+2b_{0}^{T}x+c_{0}}\\ &{\mathrm{subject~to}\quad x^{T}A_{1}x+2b_{1}^{T}x+c_{1}\leq0,}\end{array}
$$ 

with variable $x\in\mathbf{R}^{n}$ nd problem parameters $A_{i}\in\mathbf{S}^{n}$ , $b_{i}\in\mathbf{R}^{n}$ , $c_{i}\in\mathbf{R}$ . We do not assume that $A_{i}\succeq0$ ⪰ 0, so problem ( B.1 ) is not a convex optimization problem. The Lagrangian of ( B.1 ) is 

$$
L(x,\lambda)=x^{T}(A_{0}+\lambda A_{1})x+2(b_{0}+\lambda b_{1})^{T}x+c_{0}+\lambda c_{1},
$$ 

and the dual function is 

$$
\begin{array}{r l}{g(\lambda)=\underset{x}{\operatorname*{inf}}\ L(x,\lambda)}\\ {=\ }&{\left\{\begin{array}{l l}{c_{0}+\lambda c_{1}-(b_{0}+\lambda b_{1})^{T}(A_{0}+\lambda A_{1})^{\dagger}(b_{0}+\lambda b_{1})}&{A_{0}+\lambda A_{1}\succeq0,}\\ &{b_{0}+\lambda b_{1}\in\mathcal{R}(A_{0}+\lambda A_{1})}\\ {-\infty}&{\mathrm{otherwise}}\end{array}\right.}\end{array}
$$ 

(see § A.5.4 ). Using a Schur complement, we can express the dual problem as 

$$
\begin{array}{r l}&{\mathrm{maximize}\quad\gamma}\\ &{\mathrm{subject~to}\quad\lambda\geq0}\\ &{\left[\begin{array}{l l}{A_{0}+\lambda A_{1}}&{b_{0}+\lambda b_{1}}\\ {(b_{0}+\lambda b_{1})^{T}}&{c_{0}+\lambda c_{1}-\gamma}\end{array}\right]\succeq0,}\end{array}
$$ 

an SDP with two variables $\gamma,\lambda\in\mathbf{R}$ . 

The first result is that strong duality holds for problem ( B.1 ) and its Lagrange dual ( B.2 ), provided Slater’s constraint qualification is satisfied, i.e. , there exists an $x$ with $x^{T}A_{1}x+2b_{1}^{T}x+c_{1}<0$ 0. In other words, if ( B.1 ) is strictly feasible, the optimal values of ( B.1 ) and ( B.2 ) are equal. (A proof is given in § B.4 .) 

# Relaxation interpretation 

The dual of the SDP ( B.2 ) is 

$$
{\begin{array}{r l}{{\mathrm{minimize}}\quad}&{\mathbf{tr}(A_{0}X)+2b_{0}^{T}x+c_{0}}\\ {{\mathrm{subject~to}}\quad}&{\mathbf{tr}(A_{1}X)+2b_{1}^{T}x+c_{1}\leq0}\\ &{{\left[\begin{array}{l l}{X}&{x}\\ {x^{T}}&{1}\end{array}\right]}\succeq0,}\end{array}}
$$ 

an SDP with variables $X~\in~\mathbf{S}^{n}$ , $x\;\in\;\mathbf{R}^{n}$ . This dual SDP has an interesting interpretation in terms of the original problem ( B.1 ). 

We first note that ( B.1 ) is equivalent to 

$$
{\begin{array}{r l}{{\mathrm{minimize}}}&{\,\mathbf{tr}(A_{0}X)+2b_{0}^{T}x+c_{0}}\\ {{\mathrm{subject~to}}}&{\,\mathbf{tr}(A_{1}X)+2b_{1}^{T}x+c_{1}\leq0}\\ &{\,X=x x^{T}.}\end{array}}
$$ 

In this formulation we express the quadratic terms $x^{T}A_{i}x$ as $\mathbf{tr}(A_{i}x x^{T})$ , and then introduce a new variable $X=x x^{T}$ . Problem ( B.4 ) has a linear objective function, one linear inequality constraint, and a nonlinear equality constraint $X=x x^{T}$ . The next step is to replace the equality constraint by an inequality $X\succeq x x^{T}$ : 

$$
{\begin{array}{r l}&{{\mathrm{minimize}}\quad\mathbf{tr}(A_{0}X)+b_{0}^{T}x+c_{0}}\\ &{{\mathrm{subject~to}}\quad\mathbf{tr}(A_{1}X)+b_{1}^{T}x+c_{1}\leq0}\\ &{\quad\quad\quad\quad X\succeq x x^{T}.}\end{array}}
$$ 

This problem is called a relaxation of ( B.4 ), since we have replaced one of the constraints with a looser constraint. Finally we note that the inequality in ( B.5 ) can be expressed as a linear matrix inequality by using a Schur complement, which gives ( B.3 ). 

A number of interesting facts follow immediately from this interpretation of ( B.3 ) as a relaxation of ( B.1 ). First, it is obvious that the optimal value of ( B.3 ) is less than or equal to the optimal value of ( B.1 ), since we minimize the same objec- tive function over a larger set. Second, we can conclude that if $X\,=\,x x^{T}$ at the optimum of ( B.3 ), then $x$ must be optimal in ( B.1 ). 

Combining the result above, that strong duality holds between ( B.1 ) and ( B.2 ) (if ( B.1 ) is strictly feasible), with strong duality between the dual SDPs ( B.2 ) and ( B.3 ), we conclude that strong duality holds between the original, nonconvex quadratic problem ( B.1 ), and the SDP relaxation ( B.3 ), provided ( B.1 ) is strictly feasible. 

# B.2 The S-procedure 

The next result is a theorem of alternatives for a pair of (nonconvex) quadratic nequalities. Let $A_{1},A_{2}\in\mathbf{S}^{n}$ , $b_{1}$ , $b_{2}\in\mathbf{R}^{n}$ , $c_{1}$ , $c_{2}\in\mathbf{R}$ , and suppose there exists an x with 

$$
\hat{x}^{T}A_{2}\hat{x}+2b_{2}^{T}\hat{x}+c_{2}<0.
$$ 

Then there exists an $x\in\mathbf{R}^{n}$ satisfying 

$$
x^{T}A_{1}x+2b_{1}^{T}x+c_{1}<0,\qquad x^{T}A_{2}x+2b_{2}^{T}x+c_{2}\leq0,
$$ 

if and only if there exists no $\lambda$ such that 

$$
\begin{array}{r}{\boldsymbol\lambda\ge0,\qquad\left[\begin{array}{l l}{A_{1}}&{b_{1}}\\ {b_{1}^{T}}&{c_{1}}\end{array}\right]+\boldsymbol\lambda\left[\begin{array}{l l}{A_{2}}&{b_{2}}\\ {b_{2}^{T}}&{c_{2}}\end{array}\right]\succeq0.}\end{array}
$$ 

In other words, ( B.6 ) and ( B.7 ) are strong alternatives. 

This result is readily shown to be equivalent to the result from $\S$ B.1 , and a proof is given in $\S$ B.4 . Here we point out that the two inequality systems are clearly weak alternatives, since ( B.6 ) and ( B.7 ) together lead to a contradiction: 

$$
\begin{array}{r c l}{0}&{\leq}&{\left[\begin{array}{l}{x}\\ {1}\end{array}\right]^{T}\left(\left[\begin{array}{l l}{A_{1}}&{b_{1}}\\ {b_{1}^{T}}&{c_{1}}\end{array}\right]+\lambda\left[\begin{array}{l l}{A_{2}}&{b_{2}}\\ {b_{2}^{T}}&{c_{2}}\end{array}\right]\right)\left[\begin{array}{l}{x}\\ {1}\end{array}\right]}\\ &{=}&{x^{T}A_{1}x+2b_{1}^{T}x+c_{1}+\lambda(x^{T}A_{2}x+2b_{2}^{T}x+c_{2})}\\ &{<}&{0.}\end{array}
$$ 

This theorem of alternatives is sometimes called the $S$ -procedure , and is usually stated in the following form: the implication 

$$
x^{T}F_{1}x+2g_{1}^{T}x+h_{1}\leq0\quad\Longrightarrow\quad x^{T}F_{2}x+2g_{2}^{T}x+h_{2}\leq0,
$$ 

where $F_{i}\in\mathbf{S}^{n}$ , $g_{i}\in\mathbf{R}^{n}$ , $h_{i}\in\mathbf{R}$ , holds if and only if there exists a $\lambda$ such that 

$$
\lambda\geq0,\qquad\left[\begin{array}{l l}{F_{2}}&{g_{2}}\\ {g_{2}^{T}}&{h_{2}}\end{array}\right]\preceq\lambda\left[\begin{array}{l l}{F_{1}}&{g_{1}}\\ {g_{1}^{T}}&{h_{1}}\end{array}\right],
$$ 

provided there exists a point x with $\hat{x}^{T}F_{1}\hat{x}+2g_{1}^{T}\hat{x}+h_{1}<0$ 0. (Note that sufficiency is clear.) 

Example B.1 Ellipsoid containment. An ellipsoid $\mathcal{E}\subseteq\mathbf{R}^{n}$ with nonempty interior can be represented as the sublevel set of a quadratic function, 

$$
{\mathcal{E}}=\{x\mid x^{T}F x+2g^{T}x+h\leq0\},
$$ 

where $F\,\in\,{\bf S}_{++}$ and $h\,-\,g^{T}F^{-1}g\,<\,0$ . Suppose E is another ellipsoid with similar representation, 

$$
\boldsymbol{\tilde{\mathcal{E}}}=\{\boldsymbol{x}\;|\;\boldsymbol{x}^{T}\boldsymbol{\tilde{F}}\boldsymbol{x}+2\boldsymbol{\tilde{g}}^{T}\boldsymbol{x}+\boldsymbol{\tilde{h}}\leq\boldsymbol{0}\},
$$ 

with $\tilde{F}\in{\bf S}_{++}$ ∈ $\dot{h}-\tilde{g}^{T}\tilde{F}^{-1}\tilde{g}<0$ − 0. By the S-procedure, we see that $\mathcal{E}\subseteq\tilde{\mathcal{E}}$ E if and only if there is a λ > 0 such that 

$$
\left[\begin{array}{c c}{\boldsymbol{\tilde{F}}}&{\boldsymbol{\tilde{g}}}\\ {\boldsymbol{\tilde{g}}^{T}}&{\boldsymbol{\tilde{h}}}\end{array}\right]\preceq\boldsymbol{\lambda}\left[\begin{array}{c c}{\boldsymbol{F}}&{\boldsymbol{g}}\\ {\boldsymbol{g}^{T}}&{\boldsymbol{h}}\end{array}\right].
$$ 

# B.3 The field of values of two symmetric matrices 

The following result is the basis for the proof of the strong duality result in § B.1 e S-procedure in $\S\mathrm{B.2}$ . If $A,B\,\in\,{\bf S}^{n}$ , then for all $X\,\in\,{\bf S}_{+}^{n}$ , there exists an $x\in\mathbf{R}^{n}$ such that 

$$
x^{T}A x=\mathbf{tr}(A X),\qquad x^{T}B x=\mathbf{tr}(B X).
$$ 

Remark B.1 Geometric interpretation. This result has an interesting interpretation in terms of the set 

$$
W(A,B)=\{(x^{T}A x,x^{T}B x)\mid x\in\mathbf{R}^{n}\},
$$ 

which is a cone in $\mathbf{R}^{2}$ . It is the cone generated by the set 

$$
F(A,B)=\{({\boldsymbol{x}}^{T}A{\boldsymbol{x}},{\boldsymbol{x}}^{T}B{\boldsymbol{x}})\mid\|{\boldsymbol{x}}\|_{2}=1\},
$$ 

which is called the 2-dimensional field of values of the pair $(A,B)$ . Geometrically, $W(A,B)$ is the image of the set of rank-one positive semidefinite matrices under the linear transformation $f:\mathbf{S}^{n}\rightarrow\mathbf{R}^{2}$ defined by 

$$
f(X)=(\mathbf{tr}(A X),\mathbf{tr}(B X)).
$$ 

The result that for every $X\in\mathbf{S}_{+}^{n}$ there exists an $x$ satisfying ( B.8 ) means that 

$$
W(A,B)=f(\mathbf{S}_{+}^{n}).
$$ 

In other words, $W(A,B)$ is a convex cone. 

The proof is constructive and uses induction on the rank of $X$ . Suppose it is true for all $X\in\mathbf{S}_{+}^{n}$ with $1\leq\operatorname{rank}X\leq k$ , where $k\geq2$ ere exists an $x$ such that ( B.8 ) holds. Then the result also holds if rank $X=k+1$ + 1, as can be seen as follows. A matrix $X\in\mathbf{S}_{+}^{n}$ with rank $X=k+1$ can be expressed as $X=y y^{T}+Z$ where $y\ne0$ and $Z\in\mathbf{S}_{+}^{n}$ with rank $Z=k$ . By assumption, there exists a $z$ such that $\mathbf{tr}(A Z)=z^{T}A z$ , $\mathbf{tr}(A Z)=z^{T}B z$ . Therefore 

$$
\mathbf{tr}(A X)=\mathbf{tr}(A(y y^{T}+z z^{T})),\qquad\mathbf{tr}(B X)=\mathbf{tr}(B(y y^{T}+z z^{T})).
$$ 

The rank of $y y^{T}+z z^{T}$ is one or two, so by assumption there exists an $x$ such that ( B.8 ) holds. 

It refore sufficient to prove the resul $\mathbf{rank}\,X\leq2$ . If r nk $X=0$ rank X = 1 there is nothing to prove. If rank = 2, we can factor X as X $X=V V^{T}$ where $V\,\in\,\mathbf{R}^{n\times2}$ , with linearly endent columns $v_{1}$ $v_{2}$ . Without loss of generality we can assume that V $V^{T}A V$ is diagonal. (If V $V^{T}A V$ is not diagonal we replace $V$ with $V P$ where $V^{T}A V=P\,\mathbf{diag}(\lambda)P^{T}$ is the eigenvalue decomposition of $V^{T}A V$ .) We will write $V^{T}A V$ and $V^{T}B V$ as 

$$
\begin{array}{r}{V^{T}A V=\left[\begin{array}{c c}{\lambda_{1}}&{0}\\ {0}&{\lambda_{2}}\end{array}\right],\qquad V^{T}B V=\left[\begin{array}{c c}{\sigma_{1}}&{\gamma}\\ {\gamma}&{\sigma_{2}}\end{array}\right],}\end{array}
$$ 

and define 

$$
w=\left[\begin{array}{l}{\mathbf{tr}(A X)}\\ {\mathbf{tr}(B X)}\end{array}\right]=\left[\begin{array}{l}{\lambda_{1}+\lambda_{2}}\\ {\sigma_{1}+\sigma_{2}}\end{array}\right].
$$ 

We need to show that $w=(x^{T}A x,x^{T}B x)$ for some $x$ . 

We distinguish two cases. First, assume $(0,\gamma)$ is a linear combination of the vectors $(\lambda_{1},\sigma_{1})$ and $(\lambda_{2},\sigma_{2})$ : 

$$
0=z_{1}\lambda_{1}+z_{2}\lambda_{2},\qquad\gamma=z_{1}\sigma_{1}+z_{2}\sigma_{2},
$$ 

for some $z_{1}$ , $z_{2}$ . In this case we choose $x=\alpha v_{1}\!+\!\beta v_{2}$ , where $\alpha$ and $\beta$ are determined by solving two quadratic equations in two variables 

$$
\alpha^{2}+2\alpha\beta z_{1}=1,\qquad\beta^{2}+2\alpha\beta z_{2}=1.
$$ 

This will give the desired result, since 

$$
\begin{array}{r l}{\left[\begin{array}{l}{(\alpha v_{1}+\beta v_{2})^{T}A(\alpha v_{1}+\beta v_{2})}\\ {(\alpha v_{1}+\beta v_{2})^{T}B(\alpha v_{1}+\beta v_{2})}\end{array}\right]}\\ {=}&{\alpha^{2}\left[\begin{array}{l}{\lambda_{1}}\\ {\sigma_{1}}\end{array}\right]+2\alpha\beta\left[\begin{array}{l}{0}\\ {\gamma}\end{array}\right]+\beta^{2}\left[\begin{array}{l}{\lambda_{2}}\\ {\sigma_{2}}\end{array}\right]}\\ {=}&{(\alpha^{2}+2\alpha\beta z_{1})\left[\begin{array}{l}{\lambda_{1}}\\ {\sigma_{1}}\end{array}\right]+(\beta^{2}+2\alpha\beta z_{2})\left[\begin{array}{l}{\lambda_{2}}\\ {\sigma_{2}}\end{array}\right]}\\ {=}&{\left[\begin{array}{l}{\lambda_{1}+\lambda_{2}}\\ {\sigma_{1}+\sigma_{2}}\end{array}\right].}\end{array}
$$ 

It remains to show that the equations ( B.9 ) are solvable. To see this, we first note that $\alpha$ and $\beta$ must be nonzero, so we can write the equations equivalently as 

$$
\alpha^{2}(1+2(\beta/\alpha)z_{1})=1,\qquad(\beta/\alpha)^{2}+2(\beta/\alpha)(z_{2}-z_{1})=1.
$$ 

The equation $t^{2}+2t(z_{2}-z_{1})=1$ has a positive and a neg t least one of these roots (the root with the same sign as $z_{1}$ ) satisfies 1 + 2 $1+2t z_{1}>0$ 0, so we can choose 

$$
\alpha=\pm1/\sqrt{1+2t z_{1}},\ \ \ \ \ \ \beta=t\alpha.
$$ 

This yields two solutions $(\alpha,\beta)$ that satisfy ( B.9 ). (If both roots of $t^{2}\!+\!2t(z_{2}\!-\!z_{1})=$ $1$ satisfy $1+2t z_{1}>0$ , we obtain four solutions.) 

Next, assume that $(0,\gamma)$ is not a linear combination of $(\lambda_{1},\sigma_{1})$ and $(\lambda_{2},\sigma_{2})$ . In particular, this means that $(\lambda_{1},\sigma_{1})$ and $(\lambda_{2},\sigma_{2})$ are linearly dependent. Therefore their sum $w=(\lambda_{1}+\lambda_{2},\sigma_{1}+\sigma_{2})$ is a nonnegative multiple of $(\lambda_{1},\sigma_{1})$ , or $(\lambda_{2},\sigma_{2})$ , or both. If $w=\dot{\alpha}^{2}(\lambda_{1},\sigma_{1})$ for some $\alpha$ , we can choose $x=\alpha v_{1}$ . If $w=\beta^{2}(\lambda_{2},\sigma_{2})$ for some $\beta$ , we can choose $x=\beta v_{2}$ . 

# B.4 Proofs of the strong duality results 

We first pro e the S-procedure result given in § B.2 . The assumption of strict feasibility of ˆ implies that the matrix 

$$
\left[\begin{array}{l l}{A_{2}}&{b_{2}}\\ {b_{2}^{T}}&{c_{2}}\end{array}\right]
$$ 

has at least one negative eigenvalue. Therefore 

$$
\tau\geq0,\quad\tau\left[\begin{array}{l l}{A_{2}}&{b_{2}}\\ {b_{2}^{T}}&{c_{2}}\end{array}\right]\succeq0\quad\Longrightarrow\quad\tau=0.
$$ 

We can apply the theorem of alternatives for nonstrict linear matrix inequalities, given in example 5.14 , which states that ( B.7 ) is infeasible if and only if 

$$
X\succeq0,\qquad\mathbf{tr}\left(X\left[\begin{array}{l l}{A_{1}}&{b_{1}}\\ {b_{1}^{T}}&{c_{1}}\end{array}\right]\right)<0,\qquad\mathbf{tr}\left(X\left[\begin{array}{l l}{A_{2}}&{b_{2}}\\ {b_{2}^{T}}&{c_{2}}\end{array}\right]\right)\leq0
$$ 

is feasible. From $\S$ B.3 this is equivalent to feasibility of 

$$
\left[\begin{array}{l}{v}\\ {w}\end{array}\right]^{T}\left[\begin{array}{l l}{A_{1}}&{b_{1}}\\ {b_{1}^{T}}&{c_{1}}\end{array}\right]\left[\begin{array}{l}{v}\\ {w}\end{array}\right]<0,\qquad\left[\begin{array}{l}{v}\\ {w}\end{array}\right]^{T}\left[\begin{array}{l l}{A_{2}}&{b_{2}}\\ {b_{2}^{T}}&{c_{2}}\end{array}\right]\left[\begin{array}{l}{v}\\ {w}\end{array}\right]\leq0.
$$ 

If $w\ \ne\ 0$ , then $\scriptstyle x\ =\ v/w$ is feasible in ( B.6 ). If $w\;=\;0$ , we have $v^{T}A_{1}v\;<\;0$ , $v^{T}A_{2}v\leq0$ , so $x={\hat{x}}+t v$ satisfies 

$$
\begin{array}{r c l}{x^{T}A_{1}x+2b_{1}^{T}x+c_{1}}&{=}&{\hat{x}^{T}A_{1}\hat{x}+2b_{1}^{T}\hat{x}+c_{1}+t^{2}v^{T}A_{1}v+2t\big(A_{1}\hat{x}+b_{1}\big)^{T}v}\\ {x^{T}A_{2}x+2b_{2}^{T}x+c_{2}}&{=}&{\hat{x}^{T}A_{2}\hat{x}+2b_{2}^{T}\hat{x}+c_{2}+t^{2}v^{T}A_{2}v+2t\big(A_{2}\hat{x}+b_{2}\big)^{T}v}\\ &{<}&{2t\big(A_{2}\hat{x}+b_{2}\big)^{T}v,}\end{array}
$$ 

i.e. , $x$ becomes feasible as $t\to\pm\infty$ , depending on the sign of $(A_{2}\hat{x}+b_{2})^{T}v$ . 

Finally, we prove the result in § B.1 , i.e. , that the optimal values of ( B.1 ) and ( B.2 ) are equal if ( B.1 ) is strictly feasible. To do this we note that $\gamma$ is a lower bound for the optimal value of ( B.1 ) if 

$$
x^{T}A_{1}x+b_{1}^{T}x+c_{1}\leq0\quad\Longrightarrow\quad x^{T}A_{0}x+b_{0}^{T}x+c_{0}\geq\gamma.
$$ 

By the S-procedure this is true if and only if there exists a $\lambda\geq0$ such that 

$$
\begin{array}{r}{\left[\begin{array}{l l}{A_{0}}&{b_{0}}\\ {b_{0}^{T}}&{c_{0}-\gamma}\end{array}\right]+\lambda\left[\begin{array}{l l}{A_{1}}&{b_{1}}\\ {b_{1}^{T}}&{c_{1}}\end{array}\right]\succeq0,}\end{array}
$$ 

i.e. , , $\lambda$ are feasible in ( B.2 ). $\gamma$ 

# Bibliography 

The results in this appendix are known under diﬀerent names in diﬀerent disciplines. The term S-procedure is from control; see Boyd, El Ghaoui, Feron, and Balakrishnan [ BEFB94 , pages 23, 33] for a survey and references. Variations of the S-procedure are known in linear algebra in the context of joint diagonalization of a pair of symmetric matrices; see, for example, Calabi [ Cal64 ] and Uhlig [ Uhl79 ]. Special cases of the strong duality result are studied in the nonlinear programming literature on trust-region methods (Stern and Wolkowicz [ SW95 ], Nocedal and Wright [ NW99 , page 78]). 

Bri Bri61 ] proves that the field of values of a pa matrices $A,B\in\mathbf{S}^{n}$ set $F(A,B)$ ) defined in remark B.1 ) is a convex set if n > 2, and that the set $W(A,B)$ is a convex cone (for any $n$ ). Our proof in § B.3 is based on Hestenes [ Hes68 ]. Many related results and additional references can be found in Horn and Johnson [ HJ91 , § 1.8] and Ben-Tal and Nemirovski [ BTN01 , 4.10.5]. 

# Appendix C 

# Numerical linear algebra background 

In this appendix we give a brief overview of some basic numerical linear algebra, concentrating on methods for solving one or more sets of linear equations. We focus on direct ( i.e. , noniterative) methods, and how problem structure can be exploited to improve efficiency. There are many important issues and methods in numerical linear algebra that we do not consider here, including numerical stability, details of matrix factorizations, methods for parallel or multiple processors, and iterative methods. For these (and other) topics, we refer the reader to the references given at the end of this appendix. 

# C.1 Matrix structure and algorithm complexity 

We concentrate on methods for solving the set of linear equations 

$$
A x=b
$$ 

where $A\,\in\,\mathbf{R}^{n\times n}$ and $b\,\in\,\mathbf{R}^{n}$ . We ass $A$ nonsingular, so the solution is unique for all values of b , and given by x ${\boldsymbol{x}}\,=\,A^{-1}{\boldsymbol{b}}$ . This basic problem arises in many optimization algorithms, and often accounts for most of the computation. In the context of solving the linear equations ( C.1 ), the matrix $A$ is often called the coefficient matrix , and the vector $b$ is called the righthand side . 

The standard generic methods for solving ( C.1 ) require a computational eﬀort that grows approximately like $n^{3}$ . These methods assume nothing more about $A$ than nonsingularity, and so are generally applicable. For $n$ several hundred or smaller, these generic methods are probably the best methods to use, except in the most demanding real-time applications. For $n$ more than a thousand or so, the generic methods of solving $A x=b$ become less practical. 

# Coefficient matrix structure 

In many cases the coefficient matrix $A$ has some special structure or form that can be exploited to solve the equation $A x=b$ more efficiently, using methods tailored for the special structure. For example, in the Newton system $\nabla^{2}f(x)\Delta x_{\mathrm{nt}}\ =$ $-\nabla f(x)$ , the coefficient matrix is symmetric and positive definite, which allows us to use a solution method that is around twice as fast as the generic method (and also has better round o properties). There are many other types of structure that can be exploited, with computational savings (or algorithm speedup) that is usually far more than a factor of two. In many cases, the eﬀort is reduced to something proportional to $n^{2}$ or even $n$ , as compared to $n^{3}$ for the generic methods. Since these methods are usually applied when $n$ is at least a hundred, and often far larger, the savings can be dramatic. 

A wide variety of coefficient matrix structures can be exploited. Simple exam- ples related to the sparsity pattern ( i.e. , the pattern of zero and nonzero entries in the matrix) include banded, block diagonal, or sparse matrices. A more subtle exploitable structure is diagonal plus low rank. Many common forms of convex optimization problems lead to linear equations with coefficient matrices that have these exploitable structures. (There are many other matrix structures that can be exploited, e.g. , Toeplitz, Hankel, and circulant, that we will not consider in this appendix.) 

We refer to a generic method that does not exploit any sparsity pattern in the matrices as one for dense matrices . We refer to a method that does not exploit any structure at all in the matrices as one for unstructured matrices . 

# C.1.1 Complexity analysis via ﬂop count 

The cost of a numerical linear algebra algorithm is often expressed by giving the total number of ﬂoating-point operations or ﬂops required to carry it out, as a function of various problem dimensions. We define a ﬂop as one addition, sub- traction, multiplication, or division of two ﬂoating-point numbers. (Some authors define a ﬂop as one multiplication followed by one addition, so their ﬂop counts are smaller by a factor up to two.) To evaluate the complexity of an algorithm, we count the total number of ﬂops, express it as a function (usually a polynomial) of the dimensions of the matrices and vectors involved, and simplify the expression by ignoring all terms except the leading ( i.e. , highest order or dominant) terms. 

As an example, suppose that a particular algorithm requires a total of 

$$
m^{3}+3m^{2}n+m n+4m n^{2}+5m+22
$$ 

ﬂops, where $m$ and $n$ are problem dimensions. We would normally simplify this ﬂop count to 

$$
m^{3}+3m^{2}n+4m n^{2}
$$ 

ﬂops, since these are the leading terms in the problem dimensions $m$ and $n$ . If dition we assumed that $m\ll n$ , we would further simplify the ﬂop count to $4m n^{2}$ . 

Flop counts were originally popularized when ﬂoating-point operations were rel- atively slow, so counting the number gave a good estimate of the total computation time. This is no longer the case: Issues such as cache boundaries and locality of reference can dramatically aﬀect the computation time of a numerical algorithm. However, ﬂop counts can still give us a good rough estimate of the computation time of a numerical algorithm, and how the time grows with increasing problem size. Since a ﬂop count no longer accurately predicts the computation time of an algorithm, we usually pay most attention to its order or orders, i.e. , its largest exponents, and ignore diﬀerences in ﬂop counts smaller than a factor of two or so. For example, an algorithm with ﬂop count $5n^{2}$ is considered comparable to one with a ﬂop count $4n^{2}$ , but faster than an algorithm with ﬂop count $(1/3)n^{3}$ . 

# C.1.2 Cost of basic matrix-vector operations 

# Vector operations 

To compute the inner product $x^{T}y$ of two vectors $x,y\in\mathbf{R}^{n}$ we form the products , and then add them, which requires multiplies and $n-1$ additions, or $2n-1$ $x_{i}y_{i}$ $n$ ﬂops. As mentioned above, we keep only the leading term, and say that the inner product requires $2n$ ﬂops, or even more approximately, order $n$ ﬂops. A scalar- vector multiplication $\alpha x$ , where $\alpha\in\mathbf{R}$ and $x\,\in\,\mathbf{R}^{n}$ costs $n$ ﬂops. The addition $x+y$ of two vectors $x,y\in\mathbf{R}^{n}$ also costs $n$ ﬂops. 

If the vectors $x$ and $y$ are sparse, i.e. , have only a few nonzero terms, these basic operations can be carried out faster (assuming the vectors are stored using an appropriate data structure). For example, if $x$ is a sparse vector with $N$ nonzero entries, then the inner product $x^{T}y$ can be computed in $2N$ ﬂops. 

# Matrix-vector multiplication 

A matrix-vector multiplication $y=A x$ where $A\in\mathbf{R}^{m\times n}$ costs $2m n$ ﬂops: W have to calculate $m$ components of $y$ , each of which is the product of a row of A with $x$ , i.e. , an inner product of two vectors in $\mathbf{R}^{n}$ . 

Matrix-vector products can often be accelerated by taking advantage of struc- ture in $A$ . For example, if $A$ is diagonal, then $A x$ can be computed in $n$ ﬂops, stead of $2n^{2}$ ﬂops for ultiplication by a general $n\times n$ matri More generally, if A is sparse, with only N nonzero elements (out of $m n$ ), then 2 $2N$ ﬂops are needed to form $A x$ , since we can skip multiplications and additions with zero. 

As a less obvious example, suppose the $A$ has ra $p\ll\operatorname*{min}\{m,n\}$ d is represented (stored) the factored form A $A=U V$ , where $U\in\mathbf{R}^{m\times p}$ ∈ , V $V\in\mathbf{R}^{p\times n}$ ∈ . Then we can compute Ax by first computing V x (which costs 2 pn ﬂops), and then computing $U(V x)$ (which costs $2m p$ ﬂops), so the total is $2p(m+n)$ ﬂops. Since $p\ll\operatorname*{min}\{m,n\}$ , this is small compared to $2m n$ . 

# Matrix-matrix multiplication 

The matrix-matrix product $C=A B$ , where $A\in\mathbf{R}^{m\times n}$ and $B\in\mathbf{R}^{n\times p}$ , costs 2 mnp ﬂops. We have $m p$ elements in C to calculate, each of which is an inner product of two vectors of length $n$ . Again, we can often make substantial savings by taking advantage of structure in $A$ and $B$ . For example, if $A$ and $B$ are sparse, we can accelerate the multiplication by skipping additions and multiplications with zero. If and we know that $C$ is symmetric, then we can calculate the matrix $n l=p$ product in $m^{2}n$ ﬂops, since we only have to compute the $(1/2)m(m+1)$ elements in the lower triangular part. 

To form the product of several matrices, we can carry out the matrix-matrix multiplications in diﬀerent ways, which have diﬀerent ﬂop counts in general. The est ex omputing the product $D\,=\,A B C$ , where $A\,\in\,\mathbf{R}^{m\times n}$ , $B\,\in$ $\mathbf{R}^{n\times p}$ , and C $C\in\mathbf{R}^{p\times q}$ ∈ . Here we can compute D in o ways, using matrix-matrix multiplies. One method is to first form the product AB (2 mnp ﬂops), and then form $D=(A B)C$ (2 mpq ﬂops), so the total is $2m p(n{+}q)$ ﬂops. Alternatively, we can first form the product $B C$ (2 npq ﬂops), and then form $D=A(B C)$ (2 mnq ﬂops), with a total of $2n q(m\!+\!p)$ ﬂops. The first method is better when $2m p(n\!+\!q)<2n q(m\!+\!p)$ , i.e. , when 

$$
\frac{1}{n}+\frac{1}{q}<\frac{1}{m}+\frac{1}{p}.
$$ 

This assumes that no structure of the matrices is exploited in carrying out matrix- matrix products. 

For products of more than three matrices, there are many ways to parse the product into matrix-matrix multiplications. Although it is not hard to develop an algorithm that determines the best parsing ( i.e. , the one with the fewest required ﬂops) given the matrix dimensions, in most applications the best parsing is clear. 

# C.2 Solving linear equations with factored matrices 

# C.2.1 Linear equations that are easy to solve 

We start by examining some cases for which $A x=b$ is easily solved, i.e. , $x=A^{-1}b$ is easily computed. 

# Diagonal matrices 

Suppose $A$ gonal and nonsingu $a_{i i}\neq0$ for all $i$ ). The set of linear equations $A x=b$ can be written as a $a_{i i}x_{i}=b_{i}$ , $i=1,\dots,n$ . The solution is given by $x_{i}=b_{i}/a_{i i}$ , and can be calculated in $n$ ﬂops. 

# Lower triangular matrices 

A matrix $A\,\in\,\mathbf{R}^{n\times n}$ is lower triangular if $a_{i j}\,=\,0$ for $\textit{j}>\textit{i}$ . A lower triangular matrix is called unit lower triangular if the diagonal elements are equal to one. A lower triangular matrix is nonsingular if and only if $a_{i i}\neq0$ for all $i$ . 

Suppose $A$ is lower triangular and nonsingular. The equations $A x=b$ are 

$$
{\left[\begin{array}{c c c c}{a_{11}}&{0}&{\cdots}&{0}\\ {a_{21}}&{a_{22}}&{\cdots}&{0}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {a_{n1}}&{a_{n2}}&{\cdots}&{a_{n n}}\end{array}\right]}{\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\\ {\vdots}\\ {x_{n}}\end{array}\right]}={\left[\begin{array}{l}{b_{1}}\\ {b_{2}}\\ {\vdots}\\ {b_{n}}\end{array}\right]}\,.
$$ 

From the first row, we have $a_{11}x_{1}\,=\,b_{1}$ , from which we conclude $x_{1}\,=\,b_{1}/a_{11}$ . From the second row we have $a_{21}x_{1}+a_{22}x_{2}=b_{2}$ , so we can express $x_{2}$ as $x_{2}=$ $(b_{2}\!-\!a_{21}x_{1})/a_{22}$ . (We have already computed $x_{1}$ , so every number on the righthand side is known.) Continuing this way, we can express each component of $x$ in terms of previous components, yielding the algorithm 

$$
\begin{array}{r c l}{x_{1}}&{:=}&{b_{1}/a_{11}}\\ {x_{2}}&{:=}&{(b_{2}-a_{21}x_{1})/a_{22}}\\ {x_{3}}&{:=}&{(b_{3}-a_{31}x_{1}-a_{32}x_{2})/a_{33}}\\ &&{\vdots}\\ {x_{n}}&{:=}&{(b_{n}-a_{n1}x_{1}-a_{n2}x_{2}-\cdot\cdot\cdot-a_{n,n-1}x_{n-1})/a_{n n}.}\end{array}
$$ 

This procedure is called forward substitution , since we successively compute the components of $x$ by substituting the known values into the next equation. 

Let us give a ﬂop count for forward substitution. We start by calculating $x_{1}$ (1 ﬂop). We substitute $x_{1}$ in the second equation to find $x_{2}$ (3 ﬂops), then substitute $x_{1}$ and $x_{2}$ in the third equation to find $x_{3}$ (5 ﬂops), etc. The total number of ﬂops is 

$$
1+3+5+\cdot\cdot\cdot+(2n-1)=n^{2}.
$$ 

Thus, when $A$ is lower triangular and nonsingular, we can compute $x=A^{-1}b$ in $n^{2}$ ﬂops. 

If the matrix $A$ has additional structure, in addition to being lower triangular, then forward substitution can be more efficient than $n^{2}$ ﬂops. For example, if $A$ is sparse (or banded), with at most $k$ nonzero entries per row, then each forward substitution step requires at most $2k\!+\!1$ ﬂops, so the overall ﬂop count is $2(k\!+\!1)n$ , or $2k n$ after dropping the term $2n$ . 

# Upper triangular matrices 

A matrix $A\in\mathbf{R}^{n\times n}$ is upper triangular if $A^{T}$ is lower triangular, i.e. , if $a_{i j}=0$ for $j<i$ . We can solve linear equations with nonsingular upper triangular coefficient matrix in a way similar to forward substitution, except that we start by calculating $x_{n}$ , then $x_{n-1}$ , and so on. The algorithm is 

$$
\begin{array}{r c l}{x_{n}}&{:=}&{b_{n}/a_{n n}}\\ {x_{n-1}}&{:=}&{(b_{n-1}-a_{n-1,n}x_{n})/a_{n-1,n-1}}\\ {x_{n-2}}&{:=}&{(b_{n-2}-a_{n-2,n-1}x_{n-1}-a_{n-2,n}x_{n})/a_{n-2,n-2}}\\ &{\vdots}&\\ {x_{1}}&{:=}&{(b_{1}-a_{12}x_{2}-a_{13}x_{3}-\cdot\cdot\cdot-a_{1n}x_{n})/a_{11}.}\end{array}
$$ 

This is called backward substitution or back substitution since we determine the coefficients in backward order. The cost to compute $x\ =\ A^{-1}b$ via backward substitution is $n^{2}$ ﬂops. If $A$ is upper triangular and sparse (or banded), with at most $k$ nonzero entries per row, then back substitution costs $2k n$ ﬂops. 

# Orthogonal matrices 

A matrix $A\in\mathbf{R}^{n\times n}$ is orthogonal if $A^{T}A=I$ , i.e. , $A^{-1}=A^{T}$ n this case we compute x $x=A^{-1}b$ by a simple matrix-vector product x ${\boldsymbol{x}}=A^{T}{\boldsymbol{b}}$ , which costs 2 $2n^{2}$ in general. 

If the matrix $A$ has additional structure, we can compute $x=A^{-1}b$ even more efficiently than $2n^{2}$ ﬂops. For example, if $A$ has the form $A\,=\,I\,-\,2u u^{T}$ , where $\|u\|_{2}=1$ , we can compute 

$$
\boldsymbol{x}=\boldsymbol{A}^{-1}\boldsymbol{b}=(\boldsymbol{I}-2\boldsymbol{u}\boldsymbol{u}^{T})^{T}\boldsymbol{b}=\boldsymbol{b}-2(\boldsymbol{u}^{T}\boldsymbol{b})\boldsymbol{u}
$$ 

by first computing $u^{T}b$ , then forming $b-2(u^{T}b)u$ , which costs $4n$ ﬂops. 

# Permutation matrices 

Let $\pi=(\pi_{1},.\,.\,.\,,\pi_{n})$ be a permutation of $(1,2,\ldots,n)$ . The associated permutation matrix $A\in\mathbf{R}^{n\times n}$ is given by 

$$
A_{i j}=\left\{{\begin{array}{l l}{1}&{j=\pi_{i}}\\ {0}&{{\mathrm{otherwise.}}}\end{array}}\right.
$$ 

In each row (or column) of a permutation matrix there is exactly one entry with value one; all other entries are zero. Multiplying a vector by a permutation matrix simply permutes its coefficients: 

$$
A x=\left(x_{\pi_{1}},.\,.\,.\,,x_{\pi_{n}}\right).
$$ 

The inverse of a permutation matrix is the permutation matrix associated with the inverse permutation $\pi^{-1}$ . This turns out to be $A^{T}$ , which shows that permutation matrices are orthogonal. 

If $A$ is a permutation matrix, solving $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ is very easy: $x$ is obtained by permuting the entries of $b$ by $\pi^{-1}$ . This requires no ﬂoating point operations, according to our definition (but, depending on the implementation, might involve copying ﬂoating point numbers). We can reach the same conclusion from the equation $x=A^{T}b$ . The matrix $A^{T}$ (like $A$ ) has only one nonzero entry per row, with value one. Thus no additions are required, and the only multiplications required are by one. 

# C.2.2 The factor-solve method 

The basic approach to solving $\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ is based on expressing $A$ as a product of nonsingular matrices, 

$$
A=A_{1}A_{2}\cdot\cdot\cdot A_{k},
$$ 

so that 

$$
x=A^{-1}b=A_{k}^{-1}A_{k-1}^{-1}\cdot\cdot\cdot A_{1}^{-1}b.
$$ 

We can compute $x$ using this formula, working from right to left: 

$$
\begin{array}{r c l}{{z_{1}}}&{{:=}}&{{A_{1}^{-1}b}}\\ {{z_{2}}}&{{:=}}&{{A_{2}^{-1}z_{1}=A_{2}^{-1}A_{1}^{-1}b}}\\ {{}}&{{}}&{{}}\\ {{}}&{{\vdots}}&{{}}\\ {{z_{k-1}}}&{{:=}}&{{A_{k-1}^{-1}z_{k-2}=A_{k-1}^{-1}\cdot\cdot\cdot A_{1}^{-1}b}}\\ {{x}}&{{:=}}&{{A_{k}^{-1}z_{k-1}=A_{k}^{-1}\cdot\cdot\cdot A_{1}^{-1}b.}}\end{array}
$$ 

The $i$ th step of this process requires computing $z_{i}\,=\,A_{i}^{-1}z_{i-1}$ , i.e. , solving the linear equations $A_{i}z_{i}=z_{i-1}$ . If each of these equations is easy to solve ( e.g. , if $A_{i}$ is diagonal, lower or upper triangular, a permutation, etc.), this gives a method for computing $x=A^{-1}b$ . 

The step of expressing $A$ in factored form ( i.e. , computing the factors $A_{i}$ ) is called the factorization step , and the process of computing $x=A^{-1}b$ recursively, by solving a sequence problems of the form $A_{i}z_{i}=z_{i-1}$ , is often called the solve step . The total ﬂop count for solving $A x=b$ using this factor-solve method is $f+s$ , where $f$ is the ﬂop count for computing the factorization, and $s$ is the total ﬂop count for the solve step. In many cases, the cost of the factorization, $f$ , dominates the total solve cost $s$ . In this case, the cost of solving $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ , i.e. , computing $x=A^{-1}b$ , is just $f$ . 

# Solving equations with multiple righthand sides 

Suppose we need to solve the equations 

$$
A x_{1}=b_{1},\qquad A x_{2}=b_{2},\qquad.\ldots,\qquad A x_{m}=b_{m},
$$ 

where $A\,\in\,\mathbf{R}^{n\times n}$ is nonsingular. In other words, we need to solve $m$ sets of linear equations, with the same coefficient matrix, but diﬀerent righthand sides. Alternatively, we can think of this as computing the matrix 

$$
X=A^{-1}B
$$ 

where 

$$
X={\left[\begin{array}{l l l l}{x_{1}}&{x_{2}}&{\cdot\cdot\cdot}&{x_{m}}\end{array}\right]}\in\mathbf{R}^{n\times m},\qquad B={\left[\begin{array}{l l l l}{b_{1}}&{b_{2}}&{\cdot\cdot\cdot}&{b_{m}}\end{array}\right]}\in\mathbf{R}^{n\times m}.
$$ 

To do this, we first factor $A$ , which costs $f$ . Then for $i\,=\,1,.\,.\,.\,,m$ we compute $A^{-1}b_{i}$ using the solve step. Since we only factor $A$ once, the total eﬀort is 

$$
f+m s.
$$ 

In other words, we amortize the factorization cost over the set of $m$ solves. Had we (needlessly) repeated the factorization step for each $i$ , the cost would be $m(f+s)$ . 

When the factorization cost $f$ dominates the solve cost $s$ , the factor-solve method allows us to solve a small number of linear systems, with the same co- efficient matrix, at essentially the same cost as solving one. This is because the most expensive step, the factorization, is done only once. 

We can use the factor-solve method to compute the inverse $A^{-1}$ by solving $A x=e_{i}$ for $i=1,\dots,n$ , i.e. , by computing $A^{-1}I$ . This requires one factorization and $n$ solves, so the cost is $f+n s$ . 

# C.3 LU, Cholesky, and LDL $\boldsymbol{\mathsf{T}}$ factorization C.3.1 LU factorization 

Every nonsingular matrix $A\in\mathbf{R}^{n\times n}$ can be factored as 

$$
A=P L U
$$ 

$P\in\mathbf{R}^{n\times n}$ is a permutation matrix, $L\in\mathbf{R}^{n\times n}$ is unit lowe iangular, and $U\in\mathbf{R}^{n\times n}$ is upper triangular and nonsingu called the LU facto on of A . We can also write the factorization as $P^{T}A=L U$ , where the matrix P $P^{I}A$ is obtained from $A$ by re-ordering the rows. The standard algorithm for computing an LU factorization is called Gaussian elimination with partial pivoting or Gaussian elimination with row pivoting . The cost is $(2/3)n^{3}$ ﬂops if no structure in $A$ is exploited, which is the case we consider first. 

# Solving sets of linear equations using the LU factorization 

The LU factorization, combined with the factor-solve approach, is the standard method for solving a general set of linear equations $A x=b$ . 

Algorithm C.1 Solving linear equations by LU factorization. 

given a set of linear equations $A x=b$ , with $A$ nonsingular. 1. LU factorization. Factor $A$ as $A=P L U$ ( $(2/3)n^{3}$ ﬂops). 2. Permutation. Solve $P z_{1}=b$ (0 ﬂops). 3. Forward substitution. Solve $L z_{2}=z_{1}$ ( $n^{2}$ ﬂops). 4. Backward substitution. Solve $U x=z_{2}$ ( $n^{2}$ ﬂops). 

The total cost is $(2/3)n^{3}+2n^{2}$ , or $(2/3)n^{3}$ ﬂops if we keep only the leading term. If we need to solve multiple sets of linear equations with diﬀerent righthand sides, i.e. , $A x_{i}=b_{i}$ , $i=1,\ldots,m$ , the cost is 

$$
(2/3)n^{3}+2m n^{2},
$$ 

since we factor $A$ once, and carry out $m$ pairs of forward and backward substi- tutions. For example, we can solve two sets of linear equations, with the same coefficient matrix but diﬀerent righthand sides, at essentially the same cost as solving one. We can compute the inverse $A^{-1}$ by solving the equations $A x_{i}=e_{i}$ , where is the $\imath$ th column of $A^{-1}$ , and is the $i$ th unit vector. This costs $(8/3)n^{3}$ , $x_{i}$ $e_{i}$ i.e. , about $3n^{3}$ ﬂops. 

If the matrix $A$ has certain structure, for example banded or sparse, the LU fac- torization can be computed in less than $(2/3)n^{3}$ ﬂops, and the associated forward and backward substitutions can also be carried out more efficiently. 

# LU factorization of banded matrices 

Suppose the matrix $\boldsymbol{A}\ \in\ \mathbf{R}^{n\times n}$ is anded , i.e. , $a_{i j}\;=\;0$ if $|i\,-\,j|\ >\ k$ , e $k<n-1$ is called the bandwidth of A . We are interested in the case where k $k\ll n$ ≪ , i.e. , the bandwidth is much smaller than the size of the matrix. In this case an LU factorization of $A$ can be computed in roughly $4n k^{2}$ ﬂops. The resulting upper triangular matrix $U$ has bandwidth at most $2k$ , and the lower triangular matrix $L$ has at most $k+1$ nonzeros per column, so the forward and back substitutions can be carried out in order $6n k$ ﬂops. Therefore if $A$ is banded, the linear equations $A x=b$ can be solved in about $4n k^{2}$ ﬂops. 

# LU factorization of sparse matrices 

When the matrix $A$ is sparse, the LU factorization usually includes both row and column permutations, i.e. , $A$ is factored as 

$$
A=P_{1}L U P_{2},
$$ 

where $P_{1}$ and $P_{2}$ are permutation matrices, $L$ is lower triangular, and $U$ is upper triangular. If the factors $L$ and $U$ are sparse, the forward and backward substi- tutions can be carried out efficiently, and we have an efficient method for solving $A x=b$ . The sparsity of the factors $L$ and $U$ depends on the permutations $P_{1}$ and $P_{2}$ , which are chosen in part to yield relatively sparse factors. 

The cost of computing the sparse LU factorization depends in a complicated way on the size of $A$ , the number of nonzero elements, its sparsity pattern, and the particular algorithm used, but is often dramatically smaller than the cost of a dense LU factorization. In many cases the cost grows approximately linearly with $n$ , when $n$ is large. This means that when $A$ is sparse, we can solve $A x=b$ very efficiently, often with an order approximately $n$ . 

# C.3.2 Cholesky factorization 

If $A\in\mathbf{R}^{n\times n}$ is symmetric and positive definite, then it can be factored as 

$$
A=L L^{T}
$$ 

where $L$ is lower triangular and nonsingular with positive diagonal elements. This is called the Cholesky factorization of $A$ , and can be interpreted as a symmetric LU factorization (with ${\cal L}\,=\,{\cal U}^{T}$ ). The matrix $L$ , which is uniquely determined by $A$ , is called the Cholesky factor of $A$ . The cost of computing the Cholesky factorization of a dense matrix, i.e. , without exploiting any structure, is $(1/3)n^{3}$ ﬂops, half the cost of an LU factorization. 

# Solving positive definite sets of equations using Cholesky factorization 

The Cholesky factorization can be used to solve $\mathit{A x}\:=\:b$ when $A$ is symmetric positive definite. 

given a set of linear equations $A x=b$ , with $A\in\mathbf{S}_{++}^{n}$ . 1. Cholesky factorization. Factor $A$ as $A=L L^{T}$ ( $(1/3)n^{3}$ ﬂops). 2. Forward substitution. Solve $L z_{1}=b$ ( $n^{2}$ ﬂops). 3. Backward substitution. Solve $L^{T}x=z_{1}$ ( $n^{2}$ ﬂops). 

The total cost is $(1/3)n^{3}+2n^{2}$ , or roughly $(1/3)n^{3}$ ﬂops. 

There are specialized algorithms, with a complexity much lower than $(1/3)n^{3}$ , for Cholesky factorization of banded and sparse matrices. 

# Cholesky factorization of banded matrices 

If $A$ is symmetric positive definite and banded with bandwidth $k$ , then its Cholesky factor $L$ is banded with bandwidth $k$ , and can be calculated in $n k^{2}$ ﬂops. The cost of the associated solve step is $4n k$ ﬂops. 

# Cholesky factorization of sparse matrices 

When $A$ is symmetric positive definite and sparse, it is usually factored as 

$$
A=P L L^{T}P^{T},
$$ 

where $P$ is a permutation matrix and $L$ is lower triangular with positive diagonal elements. We can also express this as $P^{T}A P\,=\,L L^{T}$ , i.e. , $L L^{T}$ is the Cholesky factorization of $P^{I}A P$ . We can interpret this as first re-ordering the variables and equations, and then forming the (standard) Cholesky factorization of the resulting permuted matrix. Since $P^{I}A P$ is positive definite for any permutation matrix $P$ , we are free to choose any permutation matrix; for each choice there is a unique associated Cholesky factor $L$ . The choice of $P$ , however, can greatly aﬀect the sparsity of the factor $L$ , which in turn can greatly aﬀect the efficiency of solving $A x=b$ . Various heuristic methods are used to select a permutation $P$ that leads to a sparse factor $L$ . 

Example C.1 Cholesky factorization with an arrow sparsity pattern. Consider a sparse matrix of the form 

$$
\boldsymbol{A}=\left[\begin{array}{l l}{1}&{\boldsymbol{u}^{T}}\\ {\boldsymbol{u}}&{\boldsymbol{D}}\end{array}\right]
$$ 

where $D\in\mathbf{R}^{n\times n}$ sitive diagonal, and $u\in\mathbf{R}^{n}$ . It c be shown that $A$ is positive definite if u $u^{T}D^{-1}u<1$ 1. The Cholesky factorization of A is 

$$
\left[\begin{array}{c c}{1}&{\boldsymbol{u}^{T}}\\ {\boldsymbol{u}}&{\boldsymbol{D}}\end{array}\right]=\left[\begin{array}{c c}{1}&{\boldsymbol{0}}\\ {\boldsymbol{u}}&{\boldsymbol{L}}\end{array}\right]\left[\begin{array}{c c}{1}&{\boldsymbol{u}^{T}}\\ {\boldsymbol{0}}&{\boldsymbol{L}^{T}}\end{array}\right]
$$ 

where $L$ is lower triangular with $L L^{T}=D-u u^{T}$ . For general $u$ , the matrix $D-u u^{T}$ is dense, so we can expect $L$ to be dense. Although the matrix $A$ is very sparse (most of its rows have just two nonzero elements), its Cholesky factors are almost completely dense. 

On the other hand, suppose we permute the first row and column of $A$ to the end. After this re-ordering, we obtain the Cholesky factorization 

$$
\left[\begin{array}{c c}{\boldsymbol{D}}&{\boldsymbol{u}}\\ {\boldsymbol{u}^{T}}&{\boldsymbol{1}}\end{array}\right]=\left[\begin{array}{c c}{\boldsymbol{D}^{1/2}}&{\boldsymbol{0}}\\ {\boldsymbol{u}^{T}\boldsymbol{D}^{-1/2}}&{\sqrt{\boldsymbol{1}-\boldsymbol{u}^{T}\boldsymbol{D}^{-1}\boldsymbol{u}}}\end{array}\right]\left[\begin{array}{c c}{\boldsymbol{D}^{1/2}}&{\boldsymbol{D}^{-1/2}\boldsymbol{u}}\\ {\boldsymbol{0}}&{\sqrt{\boldsymbol{1}-\boldsymbol{u}^{T}\boldsymbol{D}^{-1}\boldsymbol{u}}}\end{array}\right].
$$ 

Now the Cholesky factor has a diagonal 1,1 block, so it is very sparse. 

This example illustrates that the re-ordering greatly aﬀects the sparsity of the Cholesky factors. Here it was quite obvious what the best permutation is, and all good re- ordering heuristics would select this re-ordering and permute the dense row and column to the end. For more complicated sparsity patterns, it can be very difficult to find the ‘best’ re-ordering ( i.e. , resulting in the greatest number of zero elements in $L$ ), but various heuristics provide good suboptimal permutations. 

For the sparse Cholesky factorization, the re-ordering permutation $P$ is often determined using only sparsity pattern of the matrix $A$ , and not the particular numerical values of the nonzero elements of $A$ . Once $P$ is chosen, we can also determine the sparsity pattern of $L$ without knowing the numerical values of the nonzero entries of $A$ . These two steps combined are called the symbolic factorization of $A$ , and form the first step in a sparse Cholesky factorization. In contrast, the permutation matrices in a sparse LU factorization do depend on the numerical values in $A$ , in addition to its sparsity pattern. 

The symbolic factorization is then followed by the numerical factorization , i.e. , the calculation of the nonzero elements of $L$ . Software packages for sparse Cholesky factorization often include separate routines for the symbolic and the numerical factorization. This is useful in many applications, because the cost of the symbolic factorization is significant, and often comparable to the numerical factorization. Suppose, for example, that we need to solve $m$ sets of linear equations 

$$
A_{1}x=b_{1},\qquad A_{2}x=b_{2},\qquad.\,.\,.\,,\qquad A_{m}x=b_{m}
$$ 

where the matrices $A_{i}$ are symmetric positive definite, with diﬀerent numerical values, but the same sparsity pattern. Suppose the cost of a symbolic factorization is $f_{\mathrm{symb}}$ , the cost of a numerical factorization is $f_{\mathrm{num}}$ , and the cost of the solve step is $s$ . Then we can solve the $m$ sets of linear equations in 

$$
f_{\mathrm{symb}}+m(f_{\mathrm{num}}+s)
$$ 

ﬂops, since we only need to carry out the symbolic factorization once, for all $m$ sets of equations. If instead we carry out a separate symbolic factorization for each set of linear equations, the ﬂop count is $m(f_{\mathrm{symb}}+f_{\mathrm{num}}+s)$ . 

# C.3.3 LDL T factorization 

Every nonsingular symmetric matrix $A$ can be factored as 

$$
A=P L D L^{T}P^{T}
$$ 

where $P$ is a permutation matrix, $L$ is lower triangular with positive diagonal elements, and $D$ is block diag nal, with nonsin lar $1\times1$ and $2\times2$ diagonal blocks. This is called an LDL $\mathrm{T}$ factorization of A . (The Cholesky factorization can be considered a special case of LDL T factorization, with $P=I$ and $D=I$ .) An LDL $\mathrm{urcorner}$ factorization can be computed in $(1/3)n^{3}$ ﬂops, if no structure of $A$ is exploited. 

Algorithm C.3 Solving linear equations by $L D L^{T}$ factorization. 

given a set of linear equations $A x=b$ , with $A\in\mathbf{S}^{n}$ nonsingular. 1. $L D L^{T}$ factorization. Factor $A$ as $A=P L D L^{T}P$ ( $(1/3)n^{3}$ ﬂops). 2. Permutation. Solve $P z_{1}=b$ (0 ﬂops). 3. Forward substitution. Solve $L z_{2}=z_{1}$ ( $n^{2}$ ﬂops). 4. (Block) diagonal solve. Solve ${\cal D}z_{3}=z_{2}$ (order $n$ ﬂops). 5. Backward substitution. Solve $L^{T}z_{4}=z_{3}$ ( $n^{2}$ ﬂops). 6. Permutation. Solve $P^{T}x=z_{4}$ (0 ﬂops). 

The total cost is, keeping only the dominant term, $(1/3)n^{3}$ ﬂops. 

# LDL T factorization of banded and sparse matrices 

As with the LU and Cholesky factorizations, there are specialized methods for calculating the LDL T factorization of a sparse or banded matrix. These are similar to the analogous methods for Cholesky factorization, with the additional factor $D$ . T In a sparse LDL factorization, the permutation matrix $P$ cannot be chosen only on the basis of the sparsity pattern of $A$ (as in a sparse Cholesky factorization); it also depends on the particular nonzero values in the matrix $A$ . 

# C.4 Block elimination and Schur complements 

# C.4.1 Eliminating a block of variables 

In this section we describe a general method that can be used to solve $\boldsymbol{A}\boldsymbol{x}\:=\:\boldsymbol{b}$ by first eliminating a subset of the variables, and then solving a smaller system of linear equations for the remaining variables. For a dense unstructured matrix, this approach gives no advantage. But when the submatrix of $A$ associated with the eliminated variables is easily factored (for example, if it is block diagonal or banded) the method can be substantially more efficient than a general method. 

Suppose we partition the variable $x\in\mathbf{R}^{n}$ into two blocks or subvectors, 

$$
x={\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\end{array}\right]}\,,
$$ 

where $x_{1}\in\mathbf{R}^{n_{1}}$ , $x_{2}\in\mathbf{R}^{n_{2}}$ . We conformally partition the linear equations $A x=b$ as 

$$
\left[\begin{array}{l l}{A_{11}}&{A_{12}}\\ {A_{21}}&{A_{22}}\end{array}\right]\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\end{array}\right]=\left[\begin{array}{l}{b_{1}}\\ {b_{2}}\end{array}\right]
$$ 

where $A_{11}\in\mathbf{R}^{n_{1}\times n_{1}}$ , $A_{22}\in\mathbf{R}^{n_{2}\times n_{2}}$ . Assuming that the submatrix $A_{11}$ is invert- ible, we can eliminate x $x_{1}$ from the equations, as follows. Using the first equation, we can express $x_{1}$ in terms of $x_{2}$ : 

$$
x_{1}=A_{11}^{-1}(b_{1}-A_{12}x_{2}).
$$ 

Substituting this expression into the second equation yields 

$$
\big(A_{22}-A_{21}A_{11}^{-1}A_{12}\big)x_{2}=b_{2}-A_{21}A_{11}^{-1}b_{1}.
$$ 

We refer to this as the reduced equation obtained by eliminating $x_{1}$ from the orig- inal equation. The reduced equation ( C.5 ) and the equation ( C.4 ) together are equivalent to the original equations ( C.3 ). The matrix appearing in the reduced equation is called the Schur complement of the first block $A_{11}$ in $A$ : 

$$
S=A_{22}-A_{21}A_{11}^{-1}A_{12}
$$ 

(see also § A.5.5 ). The Schur complement $S$ is nonsingular if and only if $A$ is nonsingular. 

The two equations ( C.5 ) and ( C.4 ) give us an alternative approach to solving the original system of equations ( C.3 ). We first form the Schur complement $S$ , then find $x_{2}$ by solving ( C.5 ), and then calculate $x_{1}$ from ( C.4 ). We can summarize this method as follows. 

Algorithm C.4 Solving linear equations by block elimination. given a nonsingular set of linear equations ( C.3 ), with $A_{11}$ nonsingular. 

1. Form $A_{11}^{-1}A_{12}$ and $A_{11}^{-1}b_{1}$ . 2. Form $S=A_{22}-A_{21}A_{11}^{-1}A_{12}$ and $\dot{b}=b_{2}-A_{21}A_{11}^{-1}b_{1}$ − . 3. Determine $x_{2}$ by solving $S x_{2}=\tilde{b}$ . 4. Determine $x_{1}$ by solving $A_{11}x_{1}=b_{1}-A_{12}x_{2}$ . 

Remark C.1 Interpretation as block factor-solve. Block elimination can be interpreted in terms of the factor-solve approach described in § C.2.2 , based on the factorization 

$$
\begin{array}{r}{\left[\begin{array}{c c}{A_{11}}&{A_{12}}\\ {A_{21}}&{A_{22}}\end{array}\right]=\left[\begin{array}{c c}{A_{11}}&{\boldsymbol{0}}\\ {A_{21}}&{\boldsymbol{S}}\end{array}\right]\left[\begin{array}{c c}{\boldsymbol{I}}&{\boldsymbol{A}_{11}^{-1}\boldsymbol{A}_{12}}\\ {\boldsymbol{0}}&{\boldsymbol{I}}\end{array}\right],}\end{array}
$$ 

which can be considered a block LU factorization. This block LU factorization sug- gests the following method for solving ( C.3 ). We first do a ‘block forward substitution’ to solve 

$$
\left[\begin{array}{c c}{A_{11}}&{0}\\ {A_{21}}&{S}\end{array}\right]\left[\begin{array}{c}{z_{1}}\\ {z_{2}}\end{array}\right]=\left[\begin{array}{c}{b_{1}}\\ {b_{2}}\end{array}\right],
$$ 

and then solve 

$$
\begin{array}{r}{\left[\begin{array}{l l}{I}&{A_{11}^{-1}A_{12}}\\ {0}&{I}\end{array}\right]\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\end{array}\right]=\left[\begin{array}{l}{z_{1}}\\ {z_{2}}\end{array}\right]}\end{array}
$$ 

by ‘block backward substitution’. This yields the same expressions as the block elimination method: 

$$
\begin{array}{l c l}{{z_{1}}}&{{=}}&{{A_{11}^{-1}b_{1}}}\\ {{z_{2}}}&{{=}}&{{S^{-1}(b_{2}-A_{21}z_{1})}}\\ {{x_{2}}}&{{=}}&{{z_{2}}}\\ {{x_{1}}}&{{=}}&{{z_{1}-A_{11}^{-1}A_{12}z_{2}.}}\end{array}
$$ 

In fact, the modern approach to the factor-solve method is based on block factor and solve steps like these, with the block sizes optimally chosen for the processor (or processors), cache sizes, etc. 

# Complexity analysis of block elimination method 

To analyze the (possible) advantage of solving the set of linear equations using block elimination, we carry out a ﬂop count. We let $f$ and $s$ denote the cost of factoring $A_{11}$ and carrying out the associated solve step, respectively. To keep the analysis simple we assume (for now) that $A_{12}$ , $A_{22}$ , and $A_{21}$ are treated as dense, unstructured matrices. The ﬂop counts for each of the four steps in solving $A x=b$ using block elimination are: 

1. Computing $A_{11}^{-1}A_{12}$ and $A_{11}^{-1}b_{1}$ requires factoring $A_{11}$ and $n_{2}+1$ solves, so it costs $f+(n_{2}+1)s$ , or just $f+n_{2}s$ , dropping the dominated term $s$ . 

2. Forming the Schur complement $S$ requires the matrix multiply $A_{21}(A_{11}^{-1}A_{12})$ ), which costs $2n_{2}^{2}n_{1}$ , and an $n_{2}\times n_{2}$ m which costs $n_{2}^{2}$ (and can be dropped). The cost of forming $\dot{b}=b_{2}-A_{21}A_{11}^{-1}b_{1}$ − is dominated by the cost of forming $S$ , and so can be ignored. The total cost of step 2, ignoring dominated terms, is then $2n_{2}^{2}n_{1}$ . 

4. Forming $b_{1}\,{-}\,A_{12}x_{2}$ costs $2n_{1}n_{2}\!+\!n_{1}$ ﬂops. To compute $x_{1}=A_{11}^{-1}(b_{1}\!-\!A_{12}x_{2})$ − ), we can use the factorization of $A_{11}$ already computed in step $1$ , so only the solve is necessary, which costs $s$ . Both of these costs are dominated by other terms, and can be ignored. 

The total cost is then 

$$
f+n_{2}s+2n_{2}^{2}n_{1}+(2/3)n_{2}^{3}
$$ 

ﬂops. 

# Eliminating an unstructured matrix 

We first consider the case when no structure in $A_{11}$ is exploited. We factor $A_{11}$ using a standard LU factorization, so $f=(2/3)n_{1}^{3}$ , and then solve using a forward and a backward substitution, so $s=2n_{1}^{2}$ . The ﬂop count for solving the equations via block elimination is then 

$$
(2/3)n_{1}^{3}+n_{2}(2n_{1}^{2})+2n_{2}^{2}n_{1}+(2/3)n_{2}^{3}=(2/3)(n_{1}+n_{2})^{3},
$$ 

which is the same as just solving the larger set of equations using a standard LU factorization. In other words, solving a set of equations by block elimination gives no advantage when no structure of $A_{11}$ is exploited. 

On the other hand, when the structure of $A_{11}$ allows us to factor and solve more efficiently than the standard method, block elimination can be more efficient than applying the standard method. 

# Eliminating a diagonal matrix 

If $A_{11}$ is diagonal, no factorization is needed, and we can carry out a solve in $n_{1}$ ﬂops, so we have $f\,=\,0$ and $s\ =\ n_{1}$ . Substituting these values into ( C.6 ) and keeping only the leading terms yields 

$$
2n_{2}^{2}n_{1}+(2/3)n_{2}^{3},
$$ 

ﬂops, which is far smaller than $(2/3)(n_{1}\!+\!n_{2})^{3}$ , the cost using the standard method. In particular, the ﬂop count of the standard method grows cubicly in $n_{1}$ , whereas for block elimination the ﬂop count grows only linearly in $n_{1}$ . 

# Eliminating a banded matrix 

If $A_{11}$ is banded with bandwidth $k$ , we can carry out the factorization in about $f=4k^{2}n_{1}$ ﬂops, and the solve can be done in about $s=6k n_{1}$ ﬂops. The overall complexity of solving $A x=b$ using block elimination is 

$$
4k^{2}n_{1}+6n_{2}k n_{1}+2n_{2}^{2}n_{1}+(2/3)n_{2}^{3}
$$ 

ﬂops. Assuming $k$ is small compared to and , this simplifies to $2n_{2}^{2}n_{1}\!+\!(2/3)n_{2}^{3}$ , $n_{1}$ $n_{2}$ the same as when $A_{11}$ is diagonal. In particular, the complexity grows linearly in $n_{1}$ , as opposed to cubicly in $n_{1}$ for the standard method. 

A matrix for which $A_{11}$ is banded is sometimes called an arrow matrix since the sparsity pattern, when $n_{\mathrm{1}}\gg n_{\mathrm{2}}$ , looks like an arrow pointing down and right. Block elimination can solve linear equations with arrow structure far more efficiently than the standard method. 

# Eliminating a block diagonal matrix 

Suppose that $A_{11}$ is block diagonal, with (square) block sizes $m_{1},.\cdot\cdot\cdot,m_{k}$ , where $n_{1}\;=\;m_{1}\,+\,\cdot\cdot\,+\,m_{k}$ . In this case we can factor $A_{11}$ by factoring each block separately, and similarly we can carry out the solve step on each block separately. Using standard methods for these we find 

$$
f=(2/3)m_{1}^{3}+\cdot\cdot\cdot+(2/3)m_{k}^{3},\qquad s=2m_{1}^{2}+\cdot\cdot\cdot+2m_{k}^{2},
$$ 

so the overall complexity of block elimination is 

$$
(2/3)\sum_{i=1}^{k}m_{i}^{3}+2n_{2}\sum_{i=1}^{k}m_{i}^{2}+2n_{2}^{2}\sum_{i=1}^{k}m_{i}+(2/3)n_{2}^{3}.
$$ 

If the block sizes are small compared to $n_{1}$ and $n_{\mathrm{1}}\gg n_{\mathrm{2}}$ , the savings obtained by block elimination is dramatic. 

The linear equations $A x=b$ , where $A_{11}$ is block diagonal, are called partially separable for the following reason. If the subvector $x_{2}$ is fixed, the remaining equations decouple into $k$ sets of independent linear equations (which can be solved separately). The subvector $x_{2}$ is sometimes called the complicating variable since the equations are much simpler when $x_{2}$ is fixed. Using block elimination, we can solve partially separable linear equations far more efficiently than by using a standard method. 

# Eliminating a sparse matrix 

If $A_{11}$ is sparse, we can eliminate $A_{11}$ using a sparse factorization and sparse solve steps, so the values of $f$ and $s$ in ( C.6 ) are much less than for unstructured $A_{11}$ . When $A_{11}$ in ( C.3 ) is sparse and the other blocks are dense, and $n_{2}\ll n_{1}$ , we say that A is a sparse matrix with a few dense rows and columns. Eliminating the sparse block $A_{11}$ provides an efficient method for solving equations which are sparse except for a few dense rows and columns. 

An alternative is to simply apply a sparse factorization algorithm to the entire matrix $A$ . Most sparse solvers will handle dense rows and columns, and select a permutation that results in sparse factors, and hence fast factorization and solve times. This is more straightforward than using block elimination, but often slower, especially in applications where we can exploit structure in the other blocks (see, e.g. , example C.4 ). 

Remark C.2 As already suggested in remark C.1 , these two methods for solving sys- tems with a few dense rows and columns are closely related. Applying the elimination method by factoring $A_{11}$ and $S$ as 

$$
A_{11}=P_{1}L_{1}U_{1}P_{2},\qquad S=P_{3}L_{2}U_{2},
$$ 

can be interpreted as factoring $A$ as 

$$
\begin{array}{r l}{\left[\begin{array}{c c}{A_{11}}&{A_{12}}\\ {A_{21}}&{A_{22}}\end{array}\right]=}&{{}}\\ {\left[\begin{array}{c c}{P_{1}}&{0}\\ {0}&{P_{3}}\end{array}\right]\left[\begin{array}{c c}{L_{1}}&{0}\\ {P_{3}^{T}A_{21}P_{2}^{T}U_{1}^{-1}}&{L_{2}}\end{array}\right]\left[\begin{array}{c c}{U_{1}}&{L_{1}^{-1}P_{1}^{T}A_{12}}\\ {0}&{U_{2}}\end{array}\right]\left[\begin{array}{c c}{P_{2}}&{0}\\ {0}&{I}\end{array}\right],}\end{array}
$$ 

followed by forward and backward substitutions. 

# C.4.2 Block elimination and structure 

# Symmetry and positive definiteness 

There are variants of the block elimination method that can be used when $A$ is symmetric, or symmetric and positive definite. When $A$ is symmetric, so are $A_{11}$ and the Schur complement $S$ , so a symmetric factorization can be used for $A_{11}$ and $S$ . Symmetry can also be exploited in the other operations, such as the matrix multiplies. Overall the savings over the nonsymmetric case is around a factor of two. 

Positive definiteness can also be exploited in block elimination. When $A$ is sym- metric and positive definite, so are $A_{11}$ and the Schur complement $S$ , so Cholesky factorizations can be used. 

# Exploiting structure in other blocks 

Our complexity analysis above assumes that we exploit no structure in the matrices $A_{12}$ , $A_{21}$ , $A_{22}$ , and the Schur complement $S$ , i.e. , they are treated as dense. But in many cases there is structure in these blocks that can be exploited in forming the Schur complement, factoring it, and carrying out the solve steps. In such cases the computational savings of the block elimination method over a standard method can be even higher. 

Example C.2 Block triangular equations. Suppose that $A_{12}\;=\;0$ , i.e. , the linear equations $A x=b$ have block lower triangular structure: 

$$
\left[\begin{array}{c c}{A_{11}}&{0}\\ {A_{21}}&{A_{22}}\end{array}\right]\left[\begin{array}{c}{x_{1}}\\ {x_{2}}\end{array}\right]=\left[\begin{array}{c}{b_{1}}\\ {b_{2}}\end{array}\right].
$$ 

In this case the Schur complement is just $S=A_{22}$ , and the block elimination method reduces to block forward substitution: 

$$
\begin{array}{r l r}{x_{1}}&{{}:=}&{A_{11}^{-1}b_{1}}\\ {x_{2}}&{{}:=}&{A_{22}^{-1}\big(b_{2}-A_{21}x_{1}\big).}\end{array}
$$ 

Example C.3 Block diagonal and banded systems. Suppose that $A_{11}$ is block diagonal, with maximum block size $l~\times\,l$ , and that $A_{12}$ , $A_{21}$ , and $A_{22}$ are banded, say with bandwidth $k$ . In this case, $A_{11}^{-1}$ is also block diagonal, with the same block sizes as $A_{11}$ . Therefore the product $A_{11}^{-1}A_{12}$ is also banded, with bandwidth $k+l$ , and the Schur complement, $S\,=\,A_{22}\,-\,A_{21}A_{11}^{-1}A_{12}$ is banded with bandwidth $2k+\ell$ . This means that forming the Schur complement $S$ can be done more efficiently, and that the factorization and solve steps with $S$ can be done efficiently. In particular, for fixed maximum block size $\boldsymbol{\ell}$ and bandwidth $k$ , we can solve $A x=b$ with a number of ﬂops that grows linearly with $n$ . 

Example C.4 KKT structure. Suppose that the matrix $A$ has KKT structure , i.e. , 

$$
\boldsymbol{A}=\left[\begin{array}{c c}{A_{11}}&{A_{12}}\\ {A_{12}^{T}}&{0}\end{array}\right],
$$ 

where $A_{11}\,\in\,\mathbf{S}_{++}^{\nu}$ , and $A_{12}\,\in\,\mathbf{R}^{p\times m}$ with $\mathbf{rank}\,A_{12}\,=\,m$ . Since $A_{11}\,\succ\,0$ , we can use a Cholesky factorization. The Schur complement $S=-A_{12}^{T}A_{11}^{-1}A_{12}$ is negative definite, so we can factor $-S$ using a Cholesky factorization. 

# C.4.3 The matrix inversion lemma 

The idea of block elimination is to remove variables, and then solve a smaller set of equations that involve the Schur complement of the original matrix with respect to the eliminated variables. The same idea can be turned around: When we recognize a matrix as a Schur complement, we can introduce new variables, and create a larger set of equations to solve. In most cases there is no advantage to doing this, since we end up with a larger set of equations. But when the larger set of equations has some special structure that can be exploited to solve it, introducing variables can lead to an efficient method. The most common case is when another block of variables can be eliminated from the larger matrix. 

We start with the linear equations 

$$
(A+B C)x=b,
$$ 

where $A\in\mathbf{R}^{n\times n}$ is nonsingular, and $B\in\mathbf{R}^{n\times p}$ , $C\in\mathbf{R}^{p\times n}$ . We introduce a new variable $y=C x$ , and rewrite the equations as 

$$
A x+B y=b,\qquad y=C x,
$$ 

or, in matrix form, 

$$
\left[{\begin{array}{r r}{A}&{B}\\ {C}&{-I}\end{array}}\right]\left[{\begin{array}{r}{x}\\ {y}\end{array}}\right]=\left[{\begin{array}{r}{b}\\ {0}\end{array}}\right].
$$ 

Note that our original coefficient matrix, $A+B C$ , is the Schur complement of $-I$ in the larger matrix that appears in ( C.8 ). If we were to eliminate the variable $y$ from ( C.8 ), we would get back the original equation ( C.7 ). 

In some cases, it can be more efficient to solve the larger set of equations ( C.8 ) than the original, smaller set of equations ( C.7 ). This would be the case, for example, if $A$ , $B$ , and $C$ were relatively sparse, but the matrix $A+B C$ were far less sparse. 

After introducing the new variable $y$ , we can eliminate the original variable $x$ from the larger set of equations ( C.8 ), using $x=A^{-1}(b-B y)$ . Substituting this into the second equation $y=C x$ , we obtain 

$$
(I+C A^{-1}B)y=C A^{-1}b,
$$ 

so that 

$$
y=(I+C A^{-1}B)^{-1}C A^{-1}b.
$$ 

Using $x=A^{-1}(b-B y)$ , we get 

$$
x=\left(A^{-1}-A^{-1}B(I+C A^{-1}B)^{-1}C A^{-1}\right)b.
$$ 

Since $b$ is arbitrary, we conclude that 

$$
\left(A+B C\right)^{-1}=A^{-1}-A^{-1}B\left(I+C A^{-1}B\right)^{-1}C A^{-1}.
$$ 

This is known as the matrix inversion lemma , or the Sherman-Woodbury-Morrison formula . 

The matrix inversion lemma has many applications. For example if $p$ is small (or even just not very large), it gives us a method for solving $(A+B C)x\,=\,b$ , provided we have an efficient method for solving $A u=v$ . 

# Diagonal or sparse plus low rank 

Suppose that $A$ is diagonal with nonzero diagonal elements, and we want to solve an equation of the form ( C.7 ). The straightforward solution would consist in first forming the matrix $D=A+B C$ , and then solving $D x\,=\,b$ . If the product $B C$ is dense, then the complexity of this method is $2p n^{2}$ ﬂops to form $A+B C$ , plus $(2/3)n^{3}$ ﬂops for the LU factorization of $D$ , so the total cost is 

$$
2p n^{2}+(2/3)n^{3}
$$ 

ﬂops. The matrix inversion lemma suggests a more efficient method. We can calculate $x$ by evaluating the expression ( C.9 ) from right to left, as follows. We first evaluate $z\,=\,A^{-1}b$ ( $n$ ﬂops, since $A$ is diagonal). Then we form the matrix $E=I+C A^{-1}B$ ( $2p^{2}n$ ﬂops). Next we solve $E w=C z$ , which is a set of linear $p$ equations in $p$ variables. The cost is $(2/3)p^{3}$ ﬂops, plus $2p n$ to form $C z$ . Finally, we evaluate $x\,=\,z\,-\,A^{-1}B w$ (2 pn ﬂops for the matrix-vector product $B w$ , plus lower order terms). The total cost is 

$$
2p^{2}n+(2/3)p^{3}
$$ 

ﬂops, dropping dominated terms. Comparing with the first method, we see that the second method is more efficient when $p<\,n$ . In particular if $p$ is small and fixed, the complexity grows linearly with $n$ . 

Another important application of the matrix inversion lemma occurs when $A$ is sparse and nonsingular, and the matrices $B$ and $C$ are dense. Again we can compare two methods. The first method is to form the (dense) matrix $A+B C$ , and to solve ( C.7 ) using a dense LU factorization. The cost of this method is $2p n^{2}\!+\!(2/3)n^{3}$ ﬂops. The second method is based on evaluating the expression ( C.9 ), using a sparse LU factorization of $A$ . Specifically, suppose that $f$ is the cost of factoring $A$ as $A=P_{1}L U P_{2}$ , and $s$ is the cost of solving the factored system $P_{1}L U P_{2}x=d$ . We can evaluate ( C.9 ) from right to left as follows. We first factor $A$ , and solve $p+1$ linear systems 

$$
A z=b,\qquad A D=B,
$$ 

to find $z\in\mathbf{R}^{n}$ , and $D\in\mathbf{R}^{n\times p}$ . The cost is $f+(p+1)s$ ﬂops. Next, we form the matrix $E=I+C D$ , and solve 

$$
E w=C z,
$$ 

which is a set of $p$ linear equations in $p$ variables $w$ . The cost of this step is $2p^{2}n+(2/3)p^{3}$ plus lower order terms. Finally, we evaluate $x=z-D w$ , at a cost of $2p n$ ﬂops. This gives us a total cost of 

$$
f+p s+2p^{2}n+(2/3)p^{3}
$$ 

ﬂops. If $f\ll(2/3)n^{3}$ and $s\ll2n^{2}$ , this is much lower than the complexity of the first method. 

called the augmented system associated with ( C.7 ). If $A$ is very sparse and is small, $p$ then solving the augmented system using a sparse solver can be much faster than solving the system ( C.7 ) using a dense solver. 

The augmented system approach is closely related to the method that we described above. Suppose 

$$
A=P_{1}L U P_{2}
$$ 

is a sparse LU factorization of $A$ , and 

$$
I+C A^{-1}B=P_{3}\tilde{L}\tilde{U}
$$ 

is a dense LU factorization of $I+C A^{-1}B$ . Then 

$$
\begin{array}{r l}&{\left[\begin{array}{c c}{A}&{B}\\ {C}&{-I}\end{array}\right]}\\ &{\quad=\left[\begin{array}{c c}{P_{1}}&{0}\\ {0}&{P_{3}}\end{array}\right]\left[\begin{array}{c c}{L}&{0}\\ {P_{3}^{T}C P_{2}^{T}U^{-1}}&{-\tilde{L}}\end{array}\right]\left[\begin{array}{c c}{U}&{L^{-1}P_{1}^{T}B}\\ {0}&{\tilde{U}}\end{array}\right]\left[\begin{array}{c c}{P_{2}}&{0}\\ {0}&{I}\end{array}\right],}\end{array}
$$ 

and this factorization can be used to solve the augmented system. It can be verified that this is equivalent to the method based on the matrix inversion lemma that we described above. 

Of course, if we solve the augmented system using a sparse LU solver, we have no control over the permutations that are selected. The solver might choose a factor- ization diﬀerent from ( C.10 ), and more expensive to compute. In spite of this, the augmented system approach remains an attractive option. It is easier to implement than the method based on the matrix inversion lemma, and it is numerically more stable. 

# Low rank updates 

Suppose $A\in\mathbf{R}^{n\times n}$ is nonsingular, $u,v\in\mathbf{R}^{n}$ with $1+v^{T}A^{-1}u\ne0$ , and we want to solve two sets of linear equations 

$$
A x=b,\qquad(A+u v^{T})\tilde{x}=b.
$$ 

The solution x of the second system is called a rank-one update of $x$ . The matrix inversion lemma allows us to calculate the rank-one update x very cheaply, once we have computed $x$ . We have 

$$
\begin{array}{r c l}{{\tilde{x}}}&{{=}}&{{(A+u v^{T})^{-1}b}}\\ {{}}&{{=}}&{{(A^{-1}-\displaystyle\frac{1}{1+v^{T}A^{-1}u}A^{-1}u v^{T}A^{-1})b}}\\ {{}}&{{=}}&{{x-\displaystyle\frac{v^{T}x}{1+v^{T}A^{-1}u}A^{-1}u.}}\end{array}
$$ 

We can therefore solve both systems by factoring $A$ , computing $x\,=\,A^{-1}b$ and $w=A^{-1}u$ , and then evaluating 

$$
\tilde{x}=x-\frac{v^{T}x}{1+v^{T}w}w.
$$ 

The overall cost is $f+2s$ , as opposed to $2(f+s)$ if we were to solve for x from scratch. 

# C.5 Solving underdetermined linear equations 

To conclude this appendix, we mention a few important facts about underdeter- mined linear equations 

$$
A x=b,
$$ 

where $A\in\mathbf{R}^{p\times n}$ with $p<n$ . We assume that rank $A=p$ , so there is at least one solution for all b . 

In many applications it is sufficient to find just one particular solution x . In other situations we might need a complete parametrization of all solutions as 

$$
\{x\mid A x=b\}=\{F z+{\hat{x}}\mid z\in\mathbf{R}^{n-p}\}
$$ 

where $F$ is a matrix whose columns form a basis for the nullspace of $A$ . 

# Inverting a nonsingular submatrix of $A$ 

The solution of the underdetermined system is straightforward if a $p\!\times\!p$ nonsingular submatrix of $A$ is known. We start by assuming that the first columns of $A$ are $p$ independent. Then we can write the equation $A x=b$ as 

$$
A x={\left[\begin{array}{l l}{A_{1}}&{A_{2}}\end{array}\right]}{\left[\begin{array}{l}{x_{1}}\\ {x_{2}}\end{array}\right]}=A_{1}x_{1}+A_{2}x_{2}=b,
$$ 

where $A_{1}\in\mathbf{R}^{p\times p}$ is nonsingular. We can express $x_{1}$ as 

$$
x_{1}=A_{1}^{-1}(b-A_{2}x_{2})=A_{1}^{-1}b-A_{1}^{-1}A_{2}x_{2}.
$$ 

This expression allows us to easily calculate a solution: we simply take $\hat{x}_{2}\,=\,0$ $\hat{x}_{1}\;=\;A_{1}^{-1}b$ . The cost is equal to the cost of solving one square set of $p$ linear equations $A_{1}\hat{x}_{1}=b$ . 

We can also parametrize all sol of $\boldsymbol{A}\boldsymbol{x}\,=\,\boldsymbol{b}$ , using $x_{2}\,\in\,\mathbf{R}^{n-p}$ as a free parameter. The general solution of Ax $A x=b$ can be expressed as 

$$
\begin{array}{r}{\boldsymbol{x}=\left[\begin{array}{c}{x_{1}}\\ {x_{2}}\end{array}\right]=\left[\begin{array}{c}{-A_{1}^{-1}A_{2}}\\ {I}\end{array}\right]x_{2}+\left[\begin{array}{c}{A_{1}^{-1}b}\\ {0}\end{array}\right].}\end{array}
$$ 

This gives a parametrization of the form ( C.12 ) with 

$$
\boldsymbol{F}=\left[\begin{array}{c}{-\boldsymbol{A}_{1}^{-1}\boldsymbol{A}_{2}}\\ {\boldsymbol{I}}\end{array}\right],\qquad\hat{\boldsymbol{x}}=\left[\begin{array}{c}{\boldsymbol{A}_{1}^{-1}\boldsymbol{b}}\\ {\boldsymbol{0}}\end{array}\right].
$$ 

To summarize, assume that the cost of factoring $A_{1}$ is $f$ and the cost of solving one system of the form $A_{1}x=d$ is $s$ . Then the cost of finding one solution of ( C.11 ) is $f+s$ . The cost of parametrizing all solutions ( i.e. , calculating $F$ and x ) is $f+s(n-p+1)$ . 

Now we consider the general case, when the first columns of $A$ need not be $p$ independent. Since $\mathbf{rank}\,A\,=\,p$ , we can select a set of columns of $A$ that is $p$ independent, permute them to the front, and then apply the method described above. In other words, we find a permutation matrix $P$ such that the first $p$ columns of $\tilde{A}=A P$ are independent, i.e. , 

$$
\tilde{A}=A P=\left[\begin{array}{l l}{A_{1}}&{A_{2}}\end{array}\right],
$$ 

where $A_{1}$ is invertible. The general solution of $\boldsymbol{\ddot{A}}\boldsymbol{\tilde{x}}=\boldsymbol{b}$ , where ${\tilde{x}}\,=\,P^{T}x$ x , is then given by 

$$
\widetilde{x}=\left[\begin{array}{c}{{-A_{1}^{-1}A_{2}}}\\ {{I}}\end{array}\right]\widetilde{x}_{2}+\left[\begin{array}{c}{{A_{1}^{-1}b}}\\ {{0}}\end{array}\right].
$$ 

The general solution of $A x=b$ is then given by 

$$
\boldsymbol{x}=P\boldsymbol{\tilde{x}}=P\left[\begin{array}{c}{-A_{1}^{-1}A_{2}}\\ {I}\end{array}\right]\boldsymbol{z}+P\left[\begin{array}{c}{A_{1}^{-1}b}\\ {0}\end{array}\right],
$$ 

where $z\in\mathbf{R}^{n-p}$ is a free parameter. This idea useful when it is easy to identify a nonsingular or easily inverted submatrix of A , for example, a diagonal matrix with nonzero diagonal elements. 

# The QR factorization 

If $C\in\mathbf{R}^{n\times p}$ with $p\leq n$ and rank $C=p$ , then it can be factored as 

$$
\boldsymbol{C}=\left[\begin{array}{l l}{Q_{1}}&{Q_{2}}\end{array}\right]\left[\begin{array}{l}{R}\\ {0}\end{array}\right],
$$ 

where $Q_{1}\in\mathbf{R}^{n\times p}$ and $Q_{2}\in\mathbf{R}^{n\times(n-p)}$ satisfy 

$$
Q_{1}^{T}Q_{1}=I,\qquad Q_{2}^{T}Q_{2}=I,\qquad Q_{1}^{T}Q_{2}=0,
$$ 

and $R\in\mathbf{R}^{p\times p}$ is upper triangular with nonzero diagonal elements. This is called the $Q R$ factorization of $C$ . The QR factorization can be calculated in $2p^{2}(n-p/3)$ ﬂops. (The matrix $Q$ is stored in a factored form that makes it possible to efficiently compute matrix-vector products $Q x$ and $Q^{T}x$ .) 

The QR factorization can be used to solve the underdetermined set of linear equations ( C.11 ). Suppose 

$$
\boldsymbol{A}^{T}=\left[\begin{array}{l l}{\boldsymbol{Q}_{1}}&{\boldsymbol{Q}_{2}}\end{array}\right]\left[\begin{array}{l}{\boldsymbol{R}}\\ {\boldsymbol{0}}\end{array}\right]
$$ 

is the QR factorization of $A^{T}$ . Substituting in the equations it is clear that $\hat{x}\,=$ $Q_{1}R^{-T}b$ satisfies the equations: 

$$
A\hat{x}=R^{T}Q_{1}^{T}Q_{1}R^{-T}b=b.
$$ 

Moreover, the columns of $Q_{2}$ form a basis for the nullspace of $A$ , so the complete solution set can be parametrized as 

$$
\{x={\hat{x}}+Q_{2}z\mid z\in\mathbf{R}^{n-p}\}.
$$ 

The QR factorization method is the most common method for solving under- determined equations. One drawback is that it is difficult to exploit sparsity. The factor $Q$ is usually dense, even when $C$ is very sparse. 

# LU factorization of a rectangular matrix 

If $C\in\mathbf{R}^{n\times p}$ with $p\leq n$ and rank $C=p$ , then it can be factored as 

$$
C=P L U
$$ 

where $P\in\mathbf{R}^{n\times n}$ is a permutation $\boldsymbol{L}\in\mathbf{R}^{n\times p}$ is unit lower triangular ( i.e. , ${{l}_{i j}}\mathrm{~=~}0$ for $i<j$ and ${{\mathit{l}}_{i i}}=1$ ), and U $U\in\mathbf{R}^{p\times p}$ ∈ is nonsingular and upper triangular. The cost is $(2/3)p^{3}+p^{2}(n-p)$ ﬂops if no structure in $C$ is exploited. 

If the matrix $C$ is sparse, the LU factorization usually includes row and column permutations, $i$ .e. , we factor $C$ as 

$$
C=P_{1}L U P_{2}
$$ 

where $P_{1}$ , $P_{2}\in\mathbf{R}^{p\times p}$ are permutation matrices. The LU factorization of a sparse rectangular matrix can be calculated very efficiently, at a cost that is much lower than for dense matrices. 

The LU factorization can be used to solve underdetermined sets of linear equa- tions. Suppose $A^{T}=P L U$ is the LU factorization of the matrix $A^{T}$ in ( C.11 ), and we partition $L$ as 

$$
\begin{array}{r}{L=\left[\begin{array}{c c}{L_{1}}\\ {L_{2}}\end{array}\right],}\end{array}
$$ 

where $L_{1}\in\mathbf{R}^{p\times p}$ and $L_{2}\in\mathbf{R}^{(n-p)\times p}$ . It is easily verified that the solution set can be parametrized as ( C.12 ) with 

$$
\hat{x}=P\left[\begin{array}{c}{L_{1}^{-T}U^{-T}b}\\ {0}\end{array}\right],\qquad F=P\left[\begin{array}{c}{-L_{1}^{-T}L_{2}^{T}}\\ {I}\end{array}\right].
$$ 