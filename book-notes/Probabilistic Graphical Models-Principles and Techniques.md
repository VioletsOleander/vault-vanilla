# 1 Introduction
## 1.1 Motivation
Most tasks require a person or an automated system to reason: to take the available information and reach conclusions, both about what might be true in the world and about how to act. For example, a doctor needs to take information about a patient — his symptoms, test results, personal characteristics (gender, weight) — and reach conclusions about what diseases he may have and what course of treatment to undertake. A mobile robot needs to synthesize data from its sonars, cameras, and other sensors to conclude where in the environment it is and how to move so as to reach its goal without hitting anything. A speech-recognition system needs to take a noisy acoustic signal and infer the words spoken that gave rise to it.
> 大多数任务需要推理：使用可得的信息推出结论

In this book, we describe a general framework that can be used to allow a computer system to answer questions of this type. In principle, one could write a special-purpose computer program for every domain one encounters and every type of question that one may wish to answer. The resulting system, although possibly quite successful at its particular task, is often very brittle: If our application changes, significant changes may be required to the program. Moreover, this general approach is quite limiting, in that it is hard to extract lessons from one successful solution and apply it to one which is very different.
> 本书描述让计算机系统回答这类问题的通用框架

We focus on a diferent approach, based on the concept of a declarative representation. In this approach, we construct, within the computer, a model of the system about which we would model like to reason. This model encodes our knowledge of how the system works in a computer-readable form. This representation can be manipulated by various algorithms that can answer questions based on the model. For example, a model for medical diagnosis might represent our knowledge about different diseases and how they relate to a variety of symptoms and test results. A reasoning algorithm can take this model, as well as observations relating to a particular patient, and answer questions relating to the patient’s diagnosis. 
> 我们聚焦基于 declarative representation 的方法，在计算机内构建关于需要推导的系统的模型，该模型将我们的知识编码为计算机可理解的表示
> 我们用算法运用这些表示来解决问题

**The key property of a declarative representation is the separation of knowledge and reasoning. The representation has its own clear semantics, separate from the algorithms that one can apply to it. Thus, we can develop a general suite of algorithms that apply any model within a broad class, whether in the domain of medical diagnosis or speech recognition. Conversely, we can improve our model for a specific application domain without having to modify our reasoning algorithms constantly.**
> declarative representation 的关键性质是知识和推理的分离
> 表示本身有自己的语义，和运用在它之上的算法分离
> 这使得我们可以开发更通用的算法，同时，可以在不常修改推理算法的情况下提高模型对于特定应用领域的能力 (提高表示)
 
Declarative representations, or model-based methods, are a fundamental component in many fields, and models come in many flavors. Our focus in this book is on models for complex systems that involve a significant amount of uncertainty. Uncertainty appears to be an inescapable aspect of most real-world applications. It is a consequence of several factors. We are often uncertain about the true state of the system because our observations about it are partial: only some aspects of the world are observed; for example, the patient’s true disease is often not directly observable, and his future prognosis is never observed. Our observations are also noisy — even those aspects that are observed are often observed with some error. The true state of the world is rarely determined with certainty by our limited observations, as most relationships are simply not deterministic, at least relative to our ability to model them. For example, there are few (if any) diseases where we have a clear, universally true relationship between the disease and its symptoms, and even fewer such relationships between the disease and its prognosis. Indeed, while it is not clear whether the universe (quantum mechanics aside) is deterministic when modeled at a suffciently fine level of granularity, it is quite clear that it is not deterministic relative to our current understanding of it. To summarize, uncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and possibly even because of innate nondeterminism.
> 我们聚焦包含大量 uncertainty 的系统
> uncertainty 来自于：我们的观察是片面 partial 的，我们的观察是存在错误 noisy 的，也就是来自于我们观察/建模世界的能力是有限的
> 或许世界内在也是不确定的

Because of this ubiquitous and fundamental uncertainty about the true state of world, we need to allow our reasoning system to consider different possibilities. One approach is simply to consider any state of the world that is possible. Unfortunately, it is only rarely the case that we can completely eliminate a state as being impossible given our observations. In our medical diagnosis example, there is usually a huge number of diseases that are possible given a particular set of observations. Most of them, however, are highly unlikely. If we simply list all of the possibilities, our answers will often be vacuous of meaningful content (e.g., “the patient can have any of the following 573 diseases”). **Thus, to obtain meaningful conclusions, we need to reason not just about what is possible, but also about what is probable.**
> uncertainty 的存在使得我们需要允许推理系统考虑概率
> 但为了得到更有意义的结果，在考虑可能性 possible 的同时，也需要考虑更可能性 probable

The calculus of *probability* theory (see section 2.1) provides us with a formal framework for considering multiple possible outcomes and their likelihood. It defines a set of mutually exclusive and exhaustive possibilities, and associates each of them with a probability — a number between 0 and 1, so that the total probability of all possibilities is 1. This framework allows us to consider options that are unlikely, yet not impossible, without reducing our conclusions to content-free lists of every possibility.
> 理论：概率微积分理论

**Furthermore, one finds that probabilistic models are very liberating. Where in a more rigid formalism we might find it necessary to enumerate every possibility, here we can often sweep a multitude of annoying exceptions and special cases under the “probabilistic rug,” by introducing outcomes that roughly correspond to “something unusual happens.”**
> 概率模型更具自由性，因为不需列举所有可能时间，通过引入概率模型，可以将许多特殊案例和异常情况归入“概率毯”下，即认为它们是“不寻常发生的事件”

In fact, as we discussed, this type of approximation is often inevitable, as we can only rarely (if ever) provide a deterministic specification of the behavior of a complex system. Probabilistic models allow us to make this fact explicit, and therefore often provide a model which is more faithful to reality.
> 我们极少可以确定规范复杂系统的行为，概率模型让我们表达出不确定性，因此更贴近现实
## 1.2 Structured Probabilistic Models
This book describes a general-purpose framework for constructing and using probabilistic models of complex systems. We begin by providing some intuition for the principles underlying this framework, and for the models it encompasses. This section requires some knowledge obasic concepts in probability theory; a reader unfamiliar with these concepts might wish to read section 2.1 first.
> 本书描述为复杂系统构建和使用概率模型的通用框架

Complex systems are characterized by the presence of multiple interrelated aspects, many of which relate to the reasoning task. For example, in our medical diagnosis application, there are multiple possible diseases that the patient might have, dozens or hundreds of symptoms and diagnostic tests, personal characteristics that often form predisposing factors for disease, and many more matters to consider. These domains can be characterized in terms of a set of random variable, where the value of each variable defines an important property of the world. For example, a particular disease, such as Flu, may be one variable in our domain, which takes on two values, for example, present or absent; a symptom, such as Fever, may be a variable in our domain, one that perhaps takes on continuous values. The set of possible variables and their values is an important design decision, and it depends strongly on the questions we may wish to answer about the domain.
> 领域中存在变量，可能变量的集合以及它们的值属于设计决策，它取决于我们对于领域需要回答的问题

Our task is to reason probabilistically about the values of one or more of the variables, possibly given observations about some others. In order to do so using principled probabilistic joint probability reasoning, we need to construct a joint distribution over the space of possible assignments to distribution some set of random variables $\mathcal X$. This type of model allows us to answer a broad range of interesting queries. For example, we can make the observation that a variable $X_i$ takes on the posterior specific value $x_i$, and ask, in the resulting posterior distribution, what the probability distribution distribution is over values of another variable $X_j$.
> 我们的任务是概率上分析这些变量的值 (可能给定关于其他变量的观测)
> 为此需要在构建随机变量集合的取值集合上构建联合分布
### 1.2.1 Probabilistic Graphical Models
Specifying a joint distribution over 64 possible values, as in example 1.1, already seems fairly daunting. When we consider the fact that a typical medical- diagnosis problem has dozens or even hundreds of relevant attributes, the problem appears completely intractable. This book describes the framework of probabilistic graphical models, which provides a mechanism for exploiting structure in complex distributions to describe them compactly, and in a way that allows them to be constructed and utilized effectively.
> 概率图模型框架提供了利用复杂分布中的结构来描述联合分布的方法

Probabilistic graphical models use a graph-based representation as the basis for compactly encoding a complex distribution over a high-dimensional space. In this graphical representation, illustrated in figure 1.1, the nodes (or ovals) correspond to the variables in our domain, and the edges correspond to direct probabilistic interactions between them. 
> 概率图中，节点对应于域中的变量，边对应于节点/变量之间的直接概率关系

![[Probabilistic Graph Models-Fig1.1.png]]
For example, figure 1.1a (top) illustrates one possible graph structure for our flu example. In this graph, we see that there is no direct interaction between Muscle Pain and Season, but both interact directly with Flu.

There is a dual perspective that one can use to interpret the structure of this graph. 
> 概率图的解释从两个方向出发

From one perspective, the graph is a compact representation of a set of independencies that hold in the distribution; these properties take the form $X$ is independent of $Y$ given $Z$, denoted $(X\perp Y | Z)$, for some subsets of variables $X,Y,Z$. 
> 其一：概率图给出了联合分布中的独立情况
> 其形式为 $X$ 在给定 $Z$ 的情况下独立于 $Y$，即 $(X\perp Y | Z)$

For example, our “target” distribution $P$ for the preceding example — the distribution encoding our beliefs about this particular situation — may satisfy the conditional independence $(Congestion \perp Season | Flu; Hayfever)$. This statement asserts that

$$
P(Congestion \mid Flu,Hayfever,Season) = P(Congestion\mid Flu,Hayfever)
$$
that is, if we are interested in the distribution over the patient having congestion, and we know whether he has the flu and whether he has hayfever, the season is no longer informative. Note that this assertion does not imply that Season is independent of Congestion; only that all of the information we may obtain from the season on the chances of having congestion we already obtain by knowing whether the patient has the flu and has hayfever. Figure 1.1a (middle) shows the set of independence assumptions associated with the graph in figure 1.1a (top). 
> 在条件 $Flu;Hayfever$ 下，$Congestion$ 和 $Season$ 独立
> 即在知道了 $Flu;Hayfever$ 的情况下，$Season$ 不会给 $Congestion$ 提供更多信息


The other perspective is that the graph defines a skeleton for compactly representing a highdimensional distribution: Rather than encode the probability of every possible assignment to all of the variables in our domain, we can “break up” the distribution into smaller factors, each over a much smaller space of possibilities. We can then define the overall joint distribution as a product of these factors.
> 其二：概率图定义了紧凑表示高维分布的框架
> 我们不是为域内的所有变量编码所有可能值的概率，而是将分布分解为小因子，每个因子都覆盖一个更小的概率空间
> 完整的联合分布定义为这些因子的积

 For example, figure 1.1(a-bottom) shows the factorization of the distribution associated with the graph in figure 1.1 (top). It asserts, for example, that the probability of the event “spring, no flu, hayfever, sinus congestion, muscle pain” can be obtained by multiplying five numbers: $P(Season = spring)$, $P(Flu = false \mid Seacon=sping)$, $P(hayfever=true\mid Seacon=spring)$, $P(congestion=true\mid hayfever=true,Flu=false)$, $P(Muscle Pain=true | Flu=False)$ This parameterization is significantly more compact, requiring only $3+4+4+4+2 = 17$ nonredundant parameters, as opposed to 63 nonredundant parameters for the original joint distribution (the 64th parameter is fully determined by the others, as the sum over all entries in the joint distribution must sum to 1). 
 The graph structure defines the factorization of a distribution $P$ associated with it — the set of factors and the variables that they encompass.

**It turns out that these two perspectives — the graph as a representation of a set of independencies, and the graph as a skeleton for factorizing a distribution — are, in a deep sense, equivalent. The independence properties of the distribution are precisely what allow it to be represented compactly in a factorized form. Conversely, a particular factorization of the distribution guarantees that certain independencies hold.**
> 这两个方面本质是等价的，分布的独立性使得完整分布可以用分解的形式被紧凑地表示，并且完整分布地特定分解也保证了一定的独立性是保持的

We describe two families of graphical representations of distributions. One, called Bayesian networks, uses a directed graph (where the edges have a source and a target), as shown in Markov network figure 1.1a (top). The second, called Markov networks, uses an undirected graph, as illustrated in figure 1.1b (top). It too can be viewed as defining a set of independence assertions (figure 1.1b [middle] or as encoding a compact factorization of the distribution (figure 1.1b [bottom]). Both representations provide the duality of independencies and factorization, but they differ in the set of independencies they can encode and in the factorization of the distribution that they induce.
> 我们将描述两类分布图表示
> 其一是贝叶斯网络，使用有向图
> 其二是 Markov 网络，使用无向图
> 两种表示都反映了独立性和可分解性之间的对偶关系
### 1.2.2 Representation, Inference, Learning
The graphical language exploits structure that appears present in many distributions that we want to encode in practice: the property that variables tend to interact directly only with very few others. Distributions that exhibit this type of structure can generally be encoded naturally and compactly using a graphical model.
> 图语言实质上描述了一个很常见的性质：变量通常会与其他一部分变量交互
> 具有这一性质的分布都可以用图表示

This framework has many advantages. 
First, it often allows the distribution to be written down tractably, even in cases where the explicit representation of the joint distribution is astronomically large. Importantly, the type of representation provided by this framework is *transparent*, in that a human expert can understand and evaluate its semantics and properties. This property is important for constructing models that provide an accurate reflection of our understanding of a domain. Models that are opaque can easily give rise to unexplained, and even undesirable, answers.
> 图模型表示清晰，且可以以可解的方式表示极巨大的联合分布

Second, as we show, the same structure often also allows the distribution to be used effectively for *inference* — answering queries using the distribution as our model of the world. In particular, we provide algorithms for computing the posterior probability of some variables given evidence on others. For example, we might observe that it is spring and the patient has muscle pain, and we wish to know how likely he is to have the flu, a query that can formally be written as $P(Flu = true \mid Season = spring; MusclePatin=true)$
. These inference algorithms work directly on the graph structure and are generally orders of magnitude faster than manipulating the joint distribution explicitly.
> 图模型便于推理

Third, this framework facilitates the effective construction of these models, whether by a human expert or automatically, by learning from data a model that provides a good approximation to our past experience. For example, we may have a set of patient records from a doctor’s office and wish to learn a probabilistic model encoding a distribution consistent with our aggregate data-driven experience. Probabilistic graphical models support a data-driven approach to model construction approach that is very effective in practice. In this approach, a human expert provides some rough guidelines on how to model a given domain. For example, the human usually specifies the attributes that the model should contain, often some of the main dependencies that it should encode, and perhaps other aspects. The details, however, are usually filled in automatically, by fitting the model to data. The models produced by this process are usually much better reflections of the domain than models that are purely hand-constructed. Moreover, they can sometimes reveal surprising connections between variables and provide novel insights about a domain.
> 图模型便于构建

**These three components — representation, inference, and learning — are critical components in constructing an intelligent system. We need a declarative representation that is a reasonable encoding of our world model. We need to be able to use this representation effectively to answer a broad range of questions that are of interest. And we need to be able to acquire this distribution, combining expert knowledge and accumulated data. Probabilistic graphical models are one of a small handful of frameworks that support all three capabilities for a broad range of problems.**
> 智能系统的三个成分：表示、推理、学习
> declarative representation 应该是世界的合理编码，我们使用表示来回答问题
# 2 Foundations 
In this chapter, we review some important background material regarding key concepts from probability theory, information theory, and graph theory. This material is included in a separate introductory chapter, since it forms the basis for much of the development in the remainder of the book. Other background material — such as discrete and continuous optimization, algorithmic complexity analysis, and basic algorithmic concepts — is more localized to particular topics in the book. Many of these concepts are presented in the appendix; others are presented in concept boxes in the appropriate places in the text. All of this material is intended to focus only on the minimal subset of ideas required to understand most of the discussion in the remainder of the book, rather than to provide a comprehensive overview of the field it surveys. We encourage the reader to explore additional sources for more details about these areas. 
## 2.1 Probability Theory 
The main focus of this book is on complex probability distributions. In this section we briefly review basic concepts from probability theory. 
### 2.1.1 Probability Distributions 
When we use the word “probability” in day-to-day life, we refer to a degree of confidence that an event of an uncertain nature will occur. For example, the weather report might say “there is a low probability of light rain in the afternoon.” Probability theory deals with the formal foundations for discussing such estimates and the rules they should obey. 
> 概率指对于一个不确定事件发生的信心

Before we discuss the representation of probability, we need to define what the events are to which we want to assign a probability. These events might be diferent outcomes of throwing a die, the outcome of a horse race, the weather configurations in California, or the possible failures of a piece of machinery. 
> 首先定义需要赋予概率的事件
#### 2.1.1.1 Event Spaces 
Formally, we define events by assuming that there is an agreed upon space of possible outcomes, which we denote by $\Omega$ . For example, if we consider dice, we might set $\Omega=\{1,2,3,4,5,6\}$ . In the case of a horse race, the space might be all possible orders of arrivals at the finish line, a much larger space. 
> 我们定义事件时，假设了存在一个包含所有可能结果的空间

In addition, we assume that there is a measurable events $\mathcal S$ to which we are willing to assign probabilities. Formally, each event $\alpha\in S$ is a subset of Ω . In our die example, the event $\{6\}$ represents the case where the die shows 6, and the event $\{1,3,5\}$ represents the case of an odd outcome. In the horse-race example, we might consider the event “Lucky Strike wins,” which contains all the outcomes in which the horse Lucky Strike is first. 
> 每一个事件都是空间 $\Omega$ 的子集

Probability theory requires that the event space satisfy three basic properties:

 • It contains the empty event $\varnothing$ , and the trivial event $\Omega$ .
 • It is closed under union. That is, if $\alpha,\beta\in S$ , then so is $\alpha\cup\beta$ .
 • It is closed under complementation. That is, if $\alpha\in\mathcal S$ , then so is $\Omega-\alpha$ . 

> 事件空间满足：
> 包含空集和本身
> 对于并集运算封闭
> 对于补集运算封闭

The requirement that the event space is closed under union and complementation implies that it is also closed under other Boolean operations, such as intersection and set diference. 
> 上述最后两条说明了事件空间在其他布尔运算下也封闭，例如交集和差集
#### 2.1.1.2 Probability Distributions 
***Definition 2.1*** 
A probability distribution $P$ over $(\Omega,S)$ is a mapping from events in $\mathcal S$ to real values that satisfies the following conditions: 

• $P(\alpha)\ge0$ for all $\alpha\in\mathcal S$ . 
• $P(\Omega)=1$ . 
• If $\alpha,\beta\in{\mathcal{S}}$ and $\alpha\cap\beta=\emptyset$ , then $P(\alpha\cup\beta)=P(\alpha)+P(\beta).$ . 

> 在事件空间和事件上的一个概率分布 $P$ 是一个将事件 $\mathcal S$ 映射到实数的映射，满足：
> 像都非负
> 空间本身的像是1
> 对于两个不相交事件，该映射保留了并集运算（映射为加法）

The first condition states that probabilities are not negative. The second states that the “trivial event,” which allows all possible outcomes, has the maximal possible probability of 1 . The third condition states that the probability that one of two mutually disjoint events will occur is the sum of the probabilities of each event. 
These two conditions imply many other conditions. Of particular interest are $P(\varnothing)=0$ , and $P(\alpha\cup\beta)=P(\alpha)+P(\beta)-P(\alpha\cap\beta)$ . 
> 以上的三个形式还可以推出其他的性质，如上所示
#### 2.1.1.3 Interpretations of Probability 
Before we continue to discuss probability distributions, we need to consider the interpretations that we might assign to them. Intuitively, the probability $P(\alpha)$ of an event $\alpha$ quantifies the degree of confidence that $\alpha$ will occur. If $P(\alpha)=1$ , we are certain that one of the outcomes in $\alpha$ occurs, and if $P(\alpha)\,=\,0$ , we consider all of them impossible. Other probability values represent options that lie between these two extremes. 

This description, however, does not provide an answer to what the numbers mean. There are two common interpretations for probabilities. 
> 对于概率，有两个常见的解释

The *frequentist* interpretation views probabilities as frequencies of events. More precisely, the probability of an event is the fraction of times the event occurs if we repeat the experiment indefinitely. For example, suppose we consider the outcome of a particular die roll. In this case, the statement $P(\alpha)=0.3$ , for $\alpha=\{1,3,5\}$ , states that if we repeatedly roll this die and record the outcome, then the fraction of times the outcomes in $\alpha$ will occur is 0.3 . More precisely, the limit of the sequence of fractions of outcomes in $\alpha$ in the first roll, the first two rolls, the first three rolls, . . . , the first $n$ rolls, . . . is 0.3

The frequentist interpretation gives probabilities a tangible semantics. When we discuss concrete physical systems (for example, dice, coin flips, and card games) we can envision how these frequencies are defined. It is also relatively straightforward to check that frequencies must satisfy the requirements of proper distributions. 
> 频率学派将概率视作事件的频率，即将概率定义为无限重复事件下，是事件发生的次数占实验次数的比值
> 频率学派的解释给了概率真实的语义

The frequentist interpretation fails, however, when we consider events such as “It will rain tomorrow afternoon.” Although the time span of “Tomorrow afternoon” is somewhat ill defined, we expect it to occur exactly once. It is not clear how we define the frequencies of such events. Several attempts have been made to define the probability for such an event by finding a *reference class* of similar events for which frequencies are well defined; however, none of them has proved entirely satisfactory. Thus, the frequentist approach does not provide a satisfactory interpretation for a statement such as “the probability of rain tomorrow afternoon is 0.3.” 

**An alternative interpretation views probabilities as subjective degrees of belief. Under this interpretation, the statement $P(\alpha)\,=\,0.3$ represents a subjective statement about one’s own degree of belief that the event $\alpha$ will come about.** Thus, the statement “the probability of rain tomorrow afternoon is 50 percent” tells us that in the opinion of the speaker, the chances of rain and no rain tomorrow afternoon are the same. Although tomorrow afternoon will occur only once, we can still have uncertainty about its outcome, and represent it using numbers (that is, probabilities). 
> 另一种解释将概率视作信念的主观程度，一个事件的概率表示人对于该事件会发生的信念的把握程度
> 概率被解释为人的主观感受

This description still does not resolve what exactly it means to hold a particular degree of belief. What stops a person from stating that the probability that Bush will win the election is 0.6 and the probability that he will lose is 0.8? The source of the problem is that we need to explain how subjective degrees of beliefs (something that is internal to each one of us) are reflected in our actions. 

This issue is a major concern in subjective probabilities. One possible way of attributing degrees of beliefs is by a betting game. Suppose you believe that $P(\alpha)=0.8$ . Then you would be willing to place a bet of \$1 against \$3 . To see this, note that with probability 0.8 you gain a dollar, and with probability 0.2 you lose \$3 , so on average this bet is a good deal with expected gain of 20 cents. In fact, you might be even tempted to place a bet of \$1 against \$4 . Under this bet the average gain is 0 , so you should not mind. However, you would not consider it worthwhile to place a bet \$1 against \$4 and 10 cents, since that would have negative expected gain. Thus, by finding which bets you are willing to place, we can assess your degrees of beliefs. 
> 但我们需要解释我们对于信念的主观程度如何反映在行动中
> 可以解释为如果期望收益大于0，则我们就可以接受这个概率

The key point of this mental game is the following. If you hold degrees of belief that do not satisfy the rule of probability, then by a clever construction we can find a series of bets that would result in a sure negative outcome for you. Thus, the argument goes, a rational person must hold degrees of belief that satisfy the rules of probability. 

In the remainder of the book we discuss probabilities, but we usually do not explicitly state their interpretation. Since both interpretations lead to the same mathematical rules, the technical definitions hold for both interpretations. 
### 2.1.2 Basic Concepts in Probability 
#### 2.1.2.1 Conditional Probability 
To use a concrete example, suppose we consider a distribution over a population of students taking a certain course. The space of outcomes is simply the set of all students in the population. Now, suppose that we want to reason about the students’ intelligence and their final grade. 
We can define the event $\alpha$ to denote “all students with grade A,” and the event $\beta$ to denote “all students with high intelligence.” Using our distribution, we can consider the probability of these events, as well as the probability of $\alpha\cap\beta$ (the set of intelligent students who got grade A). 
This, however, does not directly tell us how to update our beliefs given new evidence. Suppose we learn that a student has received the grade A; what does that tell us about her intelligence? 

This kind of question arises every time we want to use distributions to reason about the real world. More precisely, after learning that an event $\alpha$ is true, how do we change our probability about $\beta$ occurring? The answer is via the notion of conditional probability . 
Formally, the conditional probability of $\beta$ given $\alpha$ is defined as 

$$
P(\beta\mid\alpha)={\frac{P(\alpha\cap\beta)}{P(\alpha)}}
$$ 
That is, the probability that $\beta$ is true given that we know $\alpha$ is the relative proportion of outcomes satisfying $\beta$ among these that satisfy $\alpha$ . (Note that the conditional probability is not defined when $\begin{array}{r}{P(\alpha)=0.}\end{array}$ .) 
> 在给定 $\alpha$ 的情况下 $\beta$ 为真的概率定义为在满足了 $\alpha$ 的所有事件中，同时满足 $\beta$ 的相对比例
> 注意在 $P(\alpha) = 0$ 时，条件概率没有定义

The conditional probability given an event (say $\alpha$ ) satisfies the properties of definition 2.1 (see exercise 2.4), and thus it is a probability distribution by its own right. Hence, we can think of the conditioning operation as taking one distribution and returning another over the same probability space. 
> 条件概率的定义满足定义 2.1 对于概率的定义，因此条件概率本身也是一个概率分布
> 我们可以将条件运算视作输入一个概率分布，输出一个在相同概率空间的另一个概率分布
#### 2.1.2.2 Chain Rule and Bayes Rule 
From the definition of the conditional distribution, we immediately see that 

$$
P(\alpha\cap\beta)=P(\alpha)P(\beta\mid\alpha).
$$ 
This equality is known as the chain rule of conditional probabilities. More generally, if $\alpha_{1},.\,.\,.\,,\alpha_{k}$ are events, then we can write 

$$
P(\alpha_{1}\cap\ldots\cap\alpha_{k})=P(\alpha_{1})P(\alpha_{2}\mid\alpha_{1})\cdot\cdot\cdot P(\alpha_{k}\mid\alpha_{1}\cap\ldots\cap\alpha_{k-1}).
$$ 
In other words, we can express the probability of a combination of several events in terms of the probability of the first, the probability of the second given the first, and so on. It is important to notice that we can expand this expression using any order of events — the result will remain the same. 

Another immediate consequence of the definition of conditional probability is Bayes’ rule 

$$
P(\alpha\mid\beta)={\frac{P(\beta\mid\alpha)P(\alpha)}{P(\beta)}}.
$$ 
A more general conditional version of Bayes’ rule, where all our probabilities are conditioned on some background event $\gamma,$ , also holds: 

$$
P(\alpha\mid\beta\cap\gamma)={\frac{P(\beta\mid\alpha\cap\gamma)P(\alpha\mid\gamma)}{P(\beta\mid\gamma)}}.
$$ 
Bayes’ rule is important in that it allows us to compute the conditional probability $P(\alpha\mid\beta)$ from the “inverse” conditional probability $P(\beta\mid\alpha)$ . 


Example 2.1 
*Consider the student population, and let Smart denote smart students and GradeA denote students who got grade A. Assume we believe (perhaps based on estimates from past statistics) that $P(G r a d e A\mid S m a r t)\,=\,0.6$ , and now we learn that a particular student received grade A. Can we estimate the probability that the student is smart? According to Bayes’ rule, this depends on our prior probability for students being smart (before we learn anything about them) and the prior probability of students receiving high grades. For example, suppose that $P(S m a r t)\,=\,0.3$ and $P(G r a d e A)\;=\;0.2,$ , then we have that $P(S m a r t\mid G r a d e A)\;=\;0.6*0.3/0.2\;=\;0.9$ . That is, an A grade strongly suggests that the student is smart. On the other hand, if the test was easier and high grades were more common, say, $P(G r a d e A)~=~0.4$ then we would get that $P(S m a r t\mid G r a d e A)=0.6*0.3/0.4=0.45,$ , which is much less conclusive about the student.* 

Another classic example that shows the importance of this reasoning is in disease screening. To see this, consider the following hypothetical example (none of the mentioned figures are related to real statistics). 


Example 2.2 
*Suppose that a tuberculosis (TB) skin test is 95 percent accurate. That is, if the patient is TB-infected, then the test will be positive with probability 0 . 95 , and if the patient is not infected, then the test will be negative with probability 0.95 . Now suppose that a person gets a positive test result. What is the probability that he is infected? Naive reasoning suggests that if the test result is wrong 5 percent of the time, then the probability that the subject is infected is 0.95 . That is, 95 percent of subjects with positive results have TB.* 

*If we consider the problem by applying Bayes’ rule, we see that we need to consider the prior probability of TB infection, and the probability of getting positive test result. Suppose that 1 in 1000 of the subjects who get tested is infected. That is, $P(T B)=0.001$ . What is the probability of getting a positive t rom our description, we see that $0.001\cdot0.95$ positive result, and $0.999{\cdot}0.05$ · u $P(P o s i t i v e)=0.0509$ . Applying Bayes’ rule, we get that $P(T B\mid P o s i t i v e)=0.001\cdot0.95/0.0509\approx0.0187.$ Thus, although a subject with a positive test is much more probable to be TB-infected than is a random subject, fewer than 2 percent of these subjects are $T\!B$ -infected.* 
### 2.1.3 Random Variables and Joint Distributions 
#### 2.1.3.1 Motivation 
Our discussion of probability distributions deals with events. Formally, we can consider any event from the set of measurable events. The description of events is in terms of sets of outcomes. In many cases, however, it would be more natural to consider attributes of the outcome. For example, if we consider a patient, we might consider attributes such as “age,” “gender,” and “smoking history” that are relevant for assigning probability over possible diseases and symptoms. We would like then consider events such as “age $>55$ , heavy smoking history, and sufers from repeated cough.” 
> 我们之前考虑的是定义在事件上的概率分布，事件就是结果集合
> 我们现在考虑结果的属性，我们将事件定义为其属性满足特定要求的结果的集合

To use a concrete example, consider again a distribution over a population of students in a course. Suppose that we want to reason about the intelligence of students, their final grades, and so forth. We can use an event such as GradeA to denote the subset of students that received the grade A and use it in our formulation. However, this discussion becomes rather cumbersome if we also want to consider students with grade B, students with grade C, and so on. Instead, we would like to consider a way of directly referring to a student’s grade in a clean, mathematical way. 

The formal machinery for discussing attributes and their values in diferent outcomes are random variables . A random variable is a way of reporting an attribute of the outcome. For example, suppose we have a random variable Grade that reports the final grade of a student, then the statement $P(G r a d e=A)$ is another notation for $P(G r a d e A)$ . 
> 我们将定义了结果集合的属性称为随机变量，随机变量是报告一个结果的属性的一个方式，例如，分数>90，定义了结果 Grade=A
#### 2.1.3.2 What Is a Random Variable? 
Formally, a random variable, such as Grade , is defined by a function that associates with each outcome in $\Omega$ a value. For example, Grade is defined by a function $f_{G r a d e}$ that maps each person in $\Omega$ to his or her grade (say, one of A, B, or C). The event $G r a d e\,=\,A$ is a shorthand for the event $\{\omega\,\in\,\Omega\,:\,f_{G r a d e}(\omega)\,=\,A\}$ . In our example, we might also have a random variable Intelligence that (for simplicity) takes as values either “high” or “low.” In this case, the event “Intelligen  = high" refers, as can be expected, to the set of smart (high intelligence) students. 
> 随机变量定义为将事件集合 $\Omega$ 中的每个结果关联到一个值的函数
> 该函数输入一个事件，输出一个和该事件的性质相关联的值

Random variables can take diferent sets of values. We can think of categorical (or discrete ) random variables that take one of a few values, as in our two preceding examples. We can also talk about random variables that can take infinitely many values (for example, integer or real values), such as Height that denotes a student’s height. We use $V a l(X)$ to denote the set of values that a random variable $X$ can take. 
> 随机变量的取值可以是离散或者连续

In most of the discussion in this book we examine either categorical random variables or random variables that take real values. We will usually use uppercase roman letters $X,Y,Z$ to denote random variables. 
In discussing generic random variables, we often use a lowercase letter to refer to a value of a random variable. Thus, we use $x$ to refer to a generic value of $X$ . 
For example, in statements such as $P(X\,=\,x)\,\geq\,0$ for all $x\in\mathit{V a l}(X)$ . When we discuss categorical random variables, we use the notation $x^{1},\cdot\cdot\cdot,x^{k}$ , for $k\,=\,|\mathit{V a l}(X)|$ (the number of elements in $V a l(X))$ ), when we need to enumerate the specific values of $X$ , for example, in statements such as 

$$
\sum_{i=1}^{k}P(X=x^{i})=1.
$$ 
The distribution over such a variable is called a multinomial. 
> 多项式分布

In the case of a binary-valued random variable $X$ , where $V a l(X)=\{f a l s e,t r u e\}$ , we often use $x^{1}$ to denote the value true for $X$ , and x $x^{0}$ to denote the value false . The distribution of such a random variable is called a Bernoulli distribution . 
> 二值随机变量：伯努利分布

We also use boldface type to denote sets of random variables. Thus, $\pmb X,\pmb Y$ , or $\pmb Z$ are typically used to denote a set of random variables, while $\pmb x,\;\pmb y,\;\pmb z$ denote assignments of values to the variables in these sets. We extend the definition of $V a l(X)$ to refer to sets of variables in the obvious way. Thus, ${\pmb x}$ is always a member of $V a l(X)$ . 

For $Y\subseteq X$ , we use $\pmb x\langle \pmb Y\rangle$ to refer to to the assignment within ${\pmb x}$ to the variables in $Y$ . 
>表示 $\pmb Y$ 中的随机变量的赋值在 $\pmb x$ 内

For two assignments $\pmb x$ (to X)  and $\pmb y$ (to Y) , we say that $\pmb{x}\sim\pmb{y}$ if they agree on the their intersection, that is, $x\langle X\cap Y\rangle=y\langle X\cap Y\rangle$ 
> $\pmb x \sim \pmb y$ 表示两个随机变量的取值在 X, Y 交集内

In many cases, the notation $P(X=x)$ is redundant, since the fact that $x$ is a value of X is already reported by our choice of letter. Thus, in many texts on probability, the identity of a random variable is not explicitly mentioned, but can be inferred through the notation used for its value. Thus, we use $P(x)$ as a shorthand for $P(X=x)$ when the identity of the random variable is clear from the context. 
Another shorthand notation is that $\textstyle\sum x$ refers to a sum over all possible values that $X$ can take. Thus, the preceding statement will often appear as $\textstyle\sum_{x}P(x)=1$ . 
Finally, another standard notation has to do with conjunction. Rather than write $P((X=x)\cap(Y=y))$  , we write $P(X=x,Y=y)$ , or just $P(x,y)$ . 
#### 2.1.3.3 Marginal and Joint Distributions 
Once we define a random variable $X$ , we can consider the distribution over events that can be described using $X$ . This distribution is often referred to as the marginal distribution over the random variable $X$ . We denote this distribution by $P(X)$ . 
> 我们定义在随机变量上的边际分布为可以用随机变量描述的事件上的分布

Returning to our population example, consider the random variable Intelligence . The marginal distribution over Intelligence assigns probability to specific events such as $P(I n t e l l i g e n c e=h i g h)$ and $P(I n t e l l i g e n c e=l o w)$ , as well as to the trivial event $P(I n t e l l i g e n c e\in\{h i g h,l o w\})$ . Note that these probabilities are defined by the probability distribution over the original space. For concreteness, suppose that $P(I n t e l l i g e n c e=h i g h)=0.3$ , $P(I n t e l l i g e n c e=l o w)=0.7.$ . 

If we consider the random variable Grade , we can also define a marginal distribution. This is a distribution over all events that can be described in terms of the Grade variable. In our example, we have that $P(G r a d e=A)=0.25$ , $P(G r a d e=B)=0.37,$ , and $P(G r a d e=C)=0.38$ . 

It should be fairly obvious that the marginal distribution is a probability distribution satisfying the properties of definition 2.1. In fact, the only change is that we restrict our attention to the subsets of $\mathcal S$ that can be described with the random variable $X$ . 
> 边际分布满足定义 2.1 定义的概率分布，差别在于我们将注意力限制在了事件空间的子空间，即可以用随机变量 $\pmb X$ 描述的事件子集上

In many situations, we are interested in questions that involve the values of several random variables. For example, we might be interested in the event “ Intelligence $=h i g h$ and $G r a d e=A$ .” To discuss such events, we need to consider the joint distribution over these two random variables. 
In general, the joint distribution over a set $\mathcal{X}=\{X_{1},\ldots,X_{n}\}$ of random variables is denoted by $P(X_{1},\cdot\cdot\cdot,X_{n})$ and is the distribution that assigns probabilities to events that are specified in terms of these random variables. We use $\xi$ to refer to a full assignment to the variables in $\mathcal{X}$ , that is, $\xi\in V a l(\mathcal{X})$ 
> 涉及多个随机变量时，我们考虑联合分布，联合分布定义于随机变量集合上，或者说是给和这些随机变量相关的事件赋值概率的分布

The joint distribution of two random variables has to be consistent with the marginal distribution, in that $\begin{array}{r}{P(x)=\sum_{y}P(x,y)}\end{array}$ . This relationship is shown in figure 2.1, where we compute the marginal distribution over Grade by summing the probabilities along each row. Similarly, we find the marginal distribution over Intelligence by summing out along each column. 
The resulting sums are typically written in the row or column margins, whence the term “marginal distribution.” 

Suppose we have a joint distribution over the variables ${\mathcal X}\ =\ \{X_{1},.\,.\,.\,,X_{n}\}$ . The most fine-grained events we can discuss using these variables are ones of the form “ ${\mathit{X}}_{1}\,=\,{\mathit{x}}_{1}$ and $X_{2}=x_{2},\,.\,.\,.,$ and $X_{n}=x_{n}$ “ for a choice of values $x_{1},\ldots,x_{n}$ for all the variables. 
Moreover, any two such events must be either identical or disjoint, since they both assign values to all the variables in $\mathcal{X}$ . In addition, any event defined using variables in $\mathcal{X}$ must be a union of a set of such events. 
Thus, we are effectively working in a *canonical outcome space* : a space where each outcome corresponds to a joint assignment to $X_{1},\ldots, X_{n}$ . More precisely, all our probability computations remain the same whether we consider the original outcome space (for example, all students), or the canonical space (for example, all combinations of intelligence and grade). We use $\xi$ to denote these *atomic outcomes* : those assigning a value to each variable in $\mathcal{X}$ . 
> 标准的结果空间：空间中每一个结果对应于对随机变量 $X_1,\dots,X_n$ 的一个联合赋值，也就是将一个个的事件定义为了一个个的随机变量赋值
> 在原始的结果空间和在标准的结果空间的概率计算是等价的
> 标准的结果空间中，每个结果都是原子结果

For example, if we let $\mathcal{X}=\{I n t e l l i g e n c e, G r a d e\}$ , there are six atomic outcomes, shown in figure 2.1. The figure also shows one possible joint distribution over these six outcomes. 

Based on this discussion, from now on we will not explicitly specify the set of outcomes and measurable events, and instead implicitly assume the canonical outcome space. 
> 我们都假设使用标准的结果空间
#### 2.1.3.4 Conditional Probability 
The notion of conditional probability extends to induced distributions over random variables. For example, we use the notation P ( Intelligence | $G r a d e=A$ ) to denote the conditional distribution over the events describable by Intelligence given the knowledge that the student’s grade is A. 

Note that the conditional distribution over a random variable given an observation of the value of another one is not the same as the marginal distribution. In our example, P ( Intelligence = $h i g h)\;=\; 0.3$ , and $P (I n t e l l i g e n c e=h i g h\mid G r a d e= A)\;=\; 0.18/0.25\;=\; 0.72$ . Thus, clearly P ( Intelligence | $G r a d e=A_{\rangle}$ ) is diferent from the marginal distribution $P (I n t e l l i g e n c e)$ . The latter distribution represents our prior knowledge about students before learning anything else about a particular student, while the conditional distribution represents our more informed distribution after learning her grade. 

We will often use the notation $P (X\mid Y)$ to represent a set of conditional probabilty distributions. 
> $P (X\mid Y)$ 表示一个条件概率分布的集合

Intuitively, for each value of Y , this object assigns a probability over values of X using the conditional probability. This notation allows us to write the shorthand version of the chain rule: $P (X, Y)=P (X) P (Y\mid X)$ , which can be extended to multiple variables as 

$$
P (X_{1},\ldots, X_{k})=P (X_{1}) P (X_{2}\mid X_{1})\cdot\cdot\cdot P (X_{k}\mid X_{1},\ldots, X_{k-1}).
$$ 
Similarly, we can state Bayes’ rule in terms of conditional probability distributions: 

$$
P (X\mid Y)={\frac{P (X) P (Y\mid X)}{P (Y)}}.
$$ 
### 2.1.4 Independence and Conditional Independence 
#### 2.1.4.1 Independence 
As we mentioned, we usually expect $P (\alpha\mid\beta)$ to be diferent from $P (\alpha)$ . That is, learning that $\beta$ is true changes our probability over α . However, in some situations equality can occur, so that $P (\alpha\mid\beta)=P (\alpha)$ . That is, learning that $\beta$ occurs did not change our probability of $\alpha$ . 

***Definition 2.2*** 
We say that an event $\alpha$ is independent of event $\beta$ in $P$ , denoted $P\vDash (\alpha\perp\beta).$  if $P (\alpha\mid\beta)=$ $P (\alpha)$ or if $P (\beta)=0$ . 
> 在概率分布 $P$ 中，若 $\alpha$ 在 $\beta$ 下的条件概率分布等于 $\alpha$ 的边际分布，或者 $\beta$ 的概率是 0，则事件 $\alpha$ 独立于 $\beta$ ，记为 $P\vDash (\alpha \perp \beta)$

We can also provide an alternative definition for the concept of independence: 

***Proposition 2.1*** 
A distribution $P$ satifies $\alpha \perp \beta$ if and only if $P(\alpha \cap \beta) = P(\alpha) P(\beta)$

Proof 
Consider first the case where $P (\beta)\,=\, 0$ ; here, we also have $P (\alpha\cap\beta)\,=\, 0$ , and so the equivalence immediately hold 
When $P (\beta)\,\neq\, 0$ we can use the chain rule; we write $P (\alpha\cap\beta)=P (\alpha\mid\beta) P (\beta)$ . Since α is independent of β , we have that $P (\alpha\mid\beta)=P (\alpha)$ . Thus, $P (\alpha\cap\beta)=P (\alpha) P (\beta)$ . 
Conversely, suppose that $P (\alpha\cap\beta)=P (\alpha) P (\beta)$ . Then, by definition, we have that 

$$
P (\alpha\mid\beta)={\frac{P (\alpha\cap\beta)}{P (\beta)}}={\frac{P (\alpha) P (\beta)}{P (\beta)}}=P (\alpha).
$$ 
As an immediate consequence of this alternative definition, we see that independence is a symmetric notion. That is, $(\alpha\perp\beta)$ implies $(\beta\perp\alpha)$ . 
> 通过该引理，可以立刻知道独立性是对称的


Example 2.3 
*For example, suppose that we toss two coins, and let $\alpha$ be the event “the first toss results in a head” and $\beta$ the event “the second toss results in a head.” It is not hard to convince ourselves that we expect that these two events to be independent. Learning that $\beta$ is true would not change our probability of $\alpha$ . In this case, we see two diferent physical processes (that is, coin tosses) leading to the events, which makes it intuitive that the probabilities of the two are independent. In certain cases, the same process can lead to independent events. For example, consider the event $\alpha$ denoting “the die outcome is even” and the event $\beta$ denoting “the die outcome is 1 or $2.^{\prime\prime}$ It is easy to check that if the die is fair (each of the six possible outcomes has probability $\frac{1}{6}$ ), then these two events are independent.* 
#### 2.1.4.2 Conditional Independence 
**While independence is a useful property, it is not often that we encounter two independent events. A more common situation is when two events are independent given an additional event**. For example, suppose we want to reason about the chance that our student is accepted to graduate studies at Stanford or MIT. Denote by Stanford the event “admitted to Stanford” and by MIT the event “admitted to MIT.” In most reasonable distributions, these two events are not independent. If we learn that a student was admitted to Stanford, then our estimate of her probability of being accepted at MIT is now higher, since it is a sign that she is a promising student. 

Now, suppose that both universities base their decisions only on the student’s grade point average (GPA), and we know that our student has a GPA of A. In this case, we might argue that learning that the student was admitted to Stanford should not change the probability that she will be admitted to MIT: Her GPA already tells us the information relevant to her chances of admission to MIT, and finding out about her admission to Stanford does not change that. Formally, the statement is 

$P (M I T\mid S t a n f o r d, G r a d e A)=P (M I T\mid G r a d e A).$ In this case, we say that MIT is conditionally independent of Stanford given GradeA 
> 在给定条件下，两个事件独立，即条件独立
> 两个事件条件独立不表示两个事件独立，在不给定条件的情况下，两个事件可能不相互独立

***Definition 2.3 conditional independence*** 
We say that an event $\alpha$ is conditionally independent of event $\beta$ given event $\gamma$ in $P$ , denoted $P\vDash (\alpha\perp\beta\mid\gamma)$  , if $P(\alpha\mid\beta\cap\gamma)=P (\alpha\mid\gamma)$ or if $P (\beta\cap\gamma)=0$ . 
> 概率分布 $P$ 中，若事件 $\alpha$ 在同时给定 $\beta$ 和 $\gamma$ 的条件下发生的概率等于事件 $\alpha$ 在仅给定 $\gamma$ 的条件下发生的概率，或者事件 $\beta$ 和 $\gamma$ 同时发生的概率为 0，则称事件 $\alpha$ 在给定条件 $\gamma$ 的条件下条件独立于事件 $\beta$ ，记为 $P \vDash (\alpha \perp \beta \mid \gamma)$

It is easy to extend the arguments we have seen in the case of (unconditional) independencies to give an alternative definition. 

***Proposition 2.2*** 
$P$ satisfies $P\vDash (\alpha \perp \beta \mid \gamma)$ if and only if $P(\alpha \cap \beta \mid \gamma) = P(\alpha \mid \gamma)P(\beta \mid \gamma)$
> $P (\alpha , \beta \mid \gamma) = P (\alpha \mid \beta, \gamma) P (\beta \mid \gamma) = P (\alpha \mid \gamma) P (\beta \mid \gamma)$
#### 2.1.4.3 Independence of Random Variables 
Until now, we have focused on independence between events. Thus, we can say that two events, such as one toss landing heads and a second also landing heads, are independent. However, we would like to say that any pair of outcomes of the coin tosses is independent. To capture such statements, we can examine the generalization of independence to sets of random variables. 
> 我们将事件之间的独立性推广到随机变量之间的独立性

***Definition 2.4*** 
Let $X, Y, Z$ be sets of random variables. We say that $X$ is conditionally independent of $Y$ given $Z$ in a distribution $P$ if $P$ satisfies $(X=x\;\bot\; Y=y\;\vert\; Z=z)$ for all values $\pmb{x}\,\in\, V a l (\pmb{X})$ , $\pmb{y}\in V a l (\pmb{Y})$ , and $z\in V a l (Z)$ . The variables in the set Z are often said to be observed. if the $Z$ is empty, then instead of writing $(X\perp Y\mid\emptyset)$ , we write $(X\perp Y)$ and say that X and Y are marginally independent . 
> 给定随机变量 $X,Y,Z$ ，如果概率分布 $P$ 满足对于随机变量 $X, Y, Z$ 的所有取值 $x, y, z$ 都有 $(X = x \perp Y = y \mid Z = z)$ ，则称随机变量 $X$ 在给定 $Z$ 的条件下条件独立于 $Y$
> $Z$ 往往称为是被观察到的随机变量
> 如果 $Z$ 是空集，则直接称随机变量 X 和 Y 边际独立

Thus, an independence statement over random variables is a universal quantification over all possible values of the random variables. 
> 随机变量之间相互独立表示它们之间的所有可能取值都互不相关

The alternative characterization of conditional independence follows immediately: 

***Proposition 2.3*** 
The distribution $P$ satisfies $(\pmb X \perp \pmb Y\mid \pmb Z)$ if and only if $P(\pmb X \cap \pmb Y \mid \pmb Z) = P(\pmb X \mid \pmb Z)P(\pmb Y \mid \pmb Z)$

Suppose we learn about a conditional independence. Can we conclude other independence properties that must hold in the distribution? We have already seen one such example: 

- **Symmetry** : 

$$
(X\perp Y\mid Z)\Longrightarrow (Y\perp X\mid Z).\tag{2.7}
$$ 
There are several other properties that hold for conditional independence, and that often provide a very clean method for proving important properties about distributions. Some key properties are: 

- **Decomposition** : 

$$
(X\bot Y, W\mid Z)\Longrightarrow (X\bot Y\mid Z).\tag{2.8}
$$ 
> 证明：
> 由 $(X \perp Y, W \mid Z)$ 可知 $P(X,Y,W\mid Z)=P(X\mid Z)P(Y,W\mid Z)$
> 故 $\sum_W P (X, Y, W\mid Z) = \sum_W P (X\mid Z) P (Y, W\mid Z)=P(X\mid Z)\sum_W P(Y,W\mid Z)$
> 即 $P (X, Y\mid Z) = P (X\mid Z) P (Y\mid Z)$
> 故 $(X\perp Y\mid Z)$

- **Weak union** : 

$$
(X\perp Y, W\mid Z)\Longrightarrow (X\perp Y\mid Z, W).\tag{2.9}
$$ 
> 证明：
> $P (X, Y\mid Z, W) = P (X\mid Y, Z, W) P (Y\mid Z, W)$
> 由 $(X\perp Y, W\mid Z)$ 可知 $P (X\mid Y, Z, W) = P (X\mid Z)$
> 根据 decomposition，因为 $(X\perp Y, W \mid Z)$ ，故 $(X\perp W \mid Z)$
> 因此 $P (X\mid Z) = P (X\mid Z, W)$
> 故 $P (X, Y\mid Z, W) = P (X\mid Z, W) P (Y\mid Z, W)$
> 故 $(X\perp Y \mid Z, W)$

- **Contraction** : 

$$
(X\bot W\mid Z, Y)\,\&\, (X\bot Y\mid Z)\Longrightarrow (X\bot Y, W\mid Z).\tag{2.10}
$$ 
> 证明：
> $P (X, Y, W \mid Z) = P (X\mid Y, W, Z) P (Y, W\mid Z)$
> 由 $(X\perp W \mid Z, Y)$ 可知 $P (X, Y, W\mid Z) = P (X\mid Z, Y) P (Y, W\mid Z)$
> 由 $(X\perp Y \mid Z)$ 可知 $P (X, Y, W \mid Z) = P (X\mid Z) P (Y, W\mid Z)$
> 故 $(X\perp Y, W \mid Z)$

An additional important property does not hold in general, but it does hold in an important subclass of distributions. 

***Definition 2.5*** 
A distribution $P$ is said to be positive if for all events $\alpha\in{\mathcal{S}}$  such that $\alpha\neq\emptyset$ , we have that $P (\alpha)>0$ . 
> postive distribution: 只要事件不为空，发生的概率就大于0

For positive distributions, we also have the following property: 

- **Intersection** : For positive distributions, and for mutually disjoint sets $X, Y, Z, W$ : 

$$
(X\bot Y\mid Z, W)\,\&\, (X\bot W\mid Z, Y)\Longrightarrow (X\bot Y, W\mid Z).\tag{2.11}
$$ 
> 证明：
> 由 $P (X\perp Y \mid Z, W)$ 可知 $P (X\mid Y, Z, W) = P (X\mid Z, W)$
> 由 $P (X\perp W \mid Z, Y)$ 可知 $P (X\mid Y, Z, W) = P (X\mid Z, Y)$
> 故 $P (X\mid Z, W) = P (X\mid Z, Y)$
> 即 $\frac {P (X, W\mid Z)}{P (W\mid Z)} = \frac {P (X, Y\mid Z)}{P (Y\mid Z)}$（这一步要求 $P$ 是正分布）
> 故 $P (X, W\mid Z) P (Y\mid Z) = P (X, Y \mid Z) P (W\mid Z)$
> 同时对 $W$ 求和得到 $\sum_W P (X, W\mid Z) P (Y\mid Z) = \sum_W P (X, Y\mid Z) P (W\mid Z)$
> 即 $P (X\mid Z) P (Y\mid Z) = P (X, Y\mid Z)$
> 故 $(X\perp Y\mid Z)$
> 根据 constraction，容易知道 $(X\perp Y, W \mid Z)$

The proof of these properties is not difcult. 

For example, to prove Decomposition, assume that $(X\ \bot\ Y, W\ |\ Z)$ holds. Then, from the definition of conditional independence, we have that $P (X, Y, W\mid Z)\,=\, P (X\mid Z) P (Y, W\mid Z)$ . Now, using basic rules of probability and arithmetic, we can show 

$$
{\begin{array}{r c l}{P (X, Y\mid Z)}&{=}&{\displaystyle\sum_{w}P (X, Y, w\mid Z)}\\ &{=}&{\displaystyle\sum_{w}P (X\mid Z) P (Y, w\mid Z)}\\ &{=}&{\displaystyle P (X\mid Z)\sum_{w}P (Y, w\mid Z)}\\ &{=}&{P (X\mid Z) P (Y\mid Z).}\end{array}}
$$ 
The only property we used here is called “reasoning by cases” (see exercise 2.6). We conclude that $(X\perp Y\mid Z)$ . 
> reasoning by cases: 逐例分析，将一个随机变量的所有取值列出来分析
### 2.1.5 Querying a Distribution 
Our focus throughout this book is on using a joint probability distribution over multiple random variables to answer queries of interest. 
#### 2.1.5.1 Probability Queries 
Perhaps the most common query type is the probability query . Such a query consists of two parts: 

- **The evidence** : a subset $E$ of random variables in the model, and an instantiation $e$ to these variables; 
- **The query variable**: a subset of $Y$ of random variables in the network

Our task is to compute:

$$
P (Y\mid E=e),
$$ 
that is, the posterior probability distribution over the values $y$ of $Y$ , conditioned on the fact that $E=e$ . 
This expression can also be viewed as the marginal over $Y$ , in the distribution we obtain by conditioning on $e$ . 
> 概率查询：查询变量 $Y$ 在证据 $E$ 的某个特定值下的条件概率分布
#### 2.1.5.2 MAP Queries 
A second important type of task is that of finding a high-probability joint assignment to some subset of variables. 
> 找到对于某个变量子集最高概率的赋值

The simplest variant of this type of task is the $M A P$ query (also called most probable explanation (MPE) ), whose aim is to find the MAP assignment — the most likely assignment to all of the (non-evidence) variables. More precisely, if we let $W=\mathcal{X}-E$ , our task is to find the most likely assignment to the variables in W given the evidence $E=e$ : 

$$
\operatorname{MAP}(W\mid e)=\arg\operatorname*{max}_{w}P (w, e),\tag{2.12}
$$ 
where, in general, $\operatorname{arg\, max}_{x}f (x)$ represents the value of $x$ for which $f (x)$ is maximal. Note that there might be more than one assignment that has the highest posterior probability. In this case, we can either decide that the MAP task is to return the set of possible assignments, or to return an arbitrary member of that set. 
> MAP 查询：在给定证据 $E=e$ 的情况下，变量子集 $W$ 最有可能的取值 $w$，注意取值可以不止一个

It is important to understand the diference between MAP queries and probability queries. In a MAP query, we are finding the most likely joint assignment to $W$ . To find the most likely assignment to a single variable $A$ , we could simply compute $P (A\mid e)$ and then pick the most likely value. **However, the assignment where each variable individually picks its most likely value can be quite diferent from the most likely joint assignment to all variables simultaneously.** This phenomenon can occur even in the simplest case, where we have no evidence. 
> 一组随机变量最优可能的联合取值不同于各个随机变量各自最优可能的单独取值
#### 2.1.5.3 Marginal MAP Queries 
To motivate our second query type, let us return to the phenomenon demonstrated in example 2.4. 
Now, consider a medical diagnosis problem, where the most likely disease has multiple possible symptoms, each of which occurs with some probability, but not an overwhelming probability. On the other hand, a somewhat rarer disease might have only a few symptoms, each of which is very likely given the disease. As in our simple example, the MAP assignment to the data and the symptoms might be higher for the second disease than for the first one. The solution here is to look for the most likely assignment to the disease variable (s) only, rather than the most likely assignment to both the disease and symptom variables. This approach suggests the use of a more general query type. 

In the marginal MAP query, we have a subset of variables $Y$ that forms our query. The task is to find the most likely assignment to the variables in $Y$ given the evidence $E=e$ : 

$$
\operatorname{MAP}(Y\mid e)=\arg\operatorname*{max}_{y}P (y\mid e).
$$ 
If we let $Z=\mathcal{X}-Y-E$ , the marginal MAP task is to compute: 

$$
\operatorname{MAP}(Y\mid e)=\arg\operatorname*{max}_{Y}\sum_{Z}P (Y, Z\mid e).
$$ 
Thus, marginal MAP queries contain both summations and maximizations; in a way, it contains elements of both a conditional probability query and a MAP query. 

Note that example 2.4 shows that marginal MAP assignments are not monotonic: the most $\operatorname{MAP}(Y_{1}\mid e)$ might be completely diferent from the assignment to $Y_{1}$ in $\operatorname{MAP}(\{Y_{1}, Y_{2}\}\mid e)$  . 
Thus, in particular, we cannot use a MAP query to give us the correct answer to a marginal MAP query. 
### 2.1.6 Continuous Spaces 
In the previous section, we focused on random variables that have a finite set of possible values. In many situations, we also want to reason about continuous quantities such as weight, height, duration, or cost that take real numbers in $I\!\! R$ . 
> 我们还需要考虑取值是连续的随机变量

When dealing with probabilities over continuous random variables, we have to deal with some technical issues. For example, suppose that we want to reason about a random variable $X$ that can take values in the range between 0 and 1 . That is, $V a l (X)$ is the interval $[0,1]$ . Moreover, assume that we want to assign each number in this range equal probability. What would be the probability of a number $x$ ? Clearly, since each $x$ has the same probability, and there are infinite number of values, we must have that $P (X=x)=0$ . This problem appears even if we do not require uniform probability. 
#### 2.1.6.1 Probability Density Functions 
How do we define probability over a continuous random variable? We say that a function $p: I\!\! R\mapsto I\!\! R$ is a probability density function or $(P D F)$ for $X$ if it is a nonnegative integrable function such that 
> 概率密度函数定义为一个非负的可积函数，积分和是 1

$$
\int_{V a l (X)}p (x) d x=1.
$$ 
That is, the integral over the set of possible values of $X$ is 1. 
The PDF defines a distribution for $X$ as follows: 
for any $x$ in our event space: 

$$
P (X\leq a)=\intop_{-\infty}^{a}p (x) d x.
$$ 
The function $P$ is the cumulative distribution for $X$ . 
> $P$ 定义为随机变量 $X$  的累计分布

We can easily employ the rules of probability to see that by using the density function we can evaluate the probability of other events. For example, 

$$
P (a\leq X\leq b)=\intop_{a}^{b}p (x) d x.
$$ 
Intuitively, the value of a PDF $p (x)$ at a point $x$ is the incremental amount that $x$ adds to the cumulative distribution in the integration process. The higher the value of $p$ at and around $x$ , the more mass is added to the cumulative distribution as it passes $x$ . The simplest PDF is the uniform distribution. 
> 直觉上，$x$ 在概率密度函数上的取值 $p (x)$ 就是它在积分过程中对累计分布添加的增量，取值越高，增量越大

***Definition 2.6*** 
uniform distribution A variable $X$ has $^a$ uniform distribution over $[a, b]$ , denoted $X\sim\mathrm{Unif}[\mathrm{a},\mathrm{b}]$ if it has the PDF 

$$
p (x)={\left\{\begin{array}{l l}{{\frac{1}{b-a}}}&{b\geq x\geq a}\\ {0}&{{\mathrm{otherwise.}}}\end{array}\right.}
$$ 
Thus, the probability of any subinterval of $[a, b]$ is proportional its size relative to the size of $[a, b]$ . 
> 各个子区间的概率和它们的长度成比例

Note that, if $b-a<1$ , then the density can be greater than 1 . Although this looks unintuitive, this situation can occur even in a legal PDF, if the interval over which the value is greater than 1 is not too large. We have only to satisfy the constraint that the total area under the PDF is 1 . 

As a more complex example, consider the Gaussian distribution. 

***Definition 2.7*** 
A random variable $X$ has $a$ Gaussian distribution with mean $\mu$ and variance $\sigma^{2}$ , denoted $X\sim$ ${\mathcal{N}}\left (\mu;\sigma^{2}\right)$ , if it has the PDF 

$$
p (x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}.
$$ 
$A$ standard Gaussian is one with mean 0 and variance 1 . 

A Gaussian distribution has a bell-like curve, where the mean parameter $\mu$ controls the location of the peak, that is, the value for which the Gaussian gets its maximum value. The variance parameter $\sigma^{2}$ determines how peaked the Gaussian is: the smaller the variance, the more peaked the Gaussian. 

Figure 2.2 shows the probability density function of a few diferent Gaussian distributions. 

More technically, the probability density function is specified as an exponential, where the expression in the exponent corresponds to the square of the number of standard deviations $\sigma$ that $x$ is away from the mean $\mu$ . The probability of $x$ decreases exponentially with the square of its deviation from the mean, as measured in units of its standard deviation. 
> 概率密度函数是指数函数，指数部分的表达式和随机变量相对于均值的距离和随机变量的方差有关
> $p (x)$ 随着它相对于均值的距离的平方指数下降，衡量单位是它的标准差
#### 2.1.6.2 Joint Density Functions 
The discussion of density functions for a single variable naturally extends for joint distributions of continuous random variables. 

***Definition 2.8***
Let $P$ be a joint distribution over continuous random variables $X_{1},\dots, X_{n}$ . A function $p (x_{1},\dots, x_{n})$ is a joint density function of $X_{1},\dots, X_{n}$ if 

- $p (x_{1},\cdot\cdot\cdot, x_{n})\geq0$ for all values $x_{1},\dots, x_{n}$ of $X_{1},\dots, X_{n}$ . 
- $p$ is an integrable function. 
- For any choice of $a_{1},\dotsc, a_{n}$ , and $b_{1},\dots, b_{n},$ 

$$P (a_{1}\leq X_{1}\leq b_{1},\ldots, a_{n}\leq X_{n}\leq b_{n})=\intop_{a_{1}}^{b_{1}}\cdot\cdot\cdot\intop_{a_{n}}^{b_{n}}p (x_{1},\ldots, x_{n}) d x_{1}\ldots d x_{n}.$$

> 联合概率密度函数：
> 定义域上非负
> 可积
> 可以分段积分 

Thus, a joint density specifies the probability of any joint event over the variables of interest. 

Both the uniform distribution and the Gaussian distribution have natural extensions to the multivariate case. The definition of a multivariate uniform distribution is straightforward. We defer the definition of the multivariate Gaussian to section 7.1. 

From the joint density we can derive the marginal density of any random variable by integrating out the other variables. Thus, for example, if $p (x, y)$ is the joint density of $X$ and $Y$ , then 

$$
p (x)=\intop_{-\infty}^{\infty}p (x, y) d y.
$$ 
To see ahy this equality holds, note that the event $a\leq X\leq b$ is, by definition, equal to the event “ ${}^{u}a\leq X\leq b$ and $-\infty\leq Y\leq\infty$ .” This rule is the direct analogue of marginalization for discrete variables. 
Note that, as with discrete probability distributions, we abuse notation a bit and use $p$ to denote both the joint density of $X$ and $Y$ and the marginal density of $X$ . 
In cases where the distinction is not clear, we use subscripts, so that $p_{X}$ will be the marginal density, of $X$ , and $p_{X, Y}$ the joint density. 
#### 2.1.6.3 Conditional Density Functions 
As with discrete random variables, we want to be able to describe conditional distributions of continuous variables. Suppose, for example, we want to define $P (Y\mid X=x)$ . Applying the definition of conditional distribution (equation (2.1)), we run into a problem, since $P (X=x)=$ 0 . Thus, the ratio of $P (Y, X=x)$ and $P (X=x)$ is undefined. 

To avoid this problem, we might consider conditioning on the event $x-\epsilon\,\leq\, X\,\leq\, x+\epsilon,$ , which can have a positive probability. Now, the conditional probability is well defined. Thus, we might consider the limit of this quantity when $\epsilon\rightarrow0$ . We define 

$$
P (Y\mid x)=\operatorname*{lim}_{\epsilon\to0}P (Y\mid x-\epsilon\leq X\leq x+\epsilon).
$$ 
When does this limit exist? If there is a continuous joint density function $p (x, y)$ , then we can derive the form for this term. To do so, consider some event on $Y$ , say $a\leq Y\leq b$ . Recall that 

$$
\begin{array}{r c l}{P (a\leq Y\leq b\mid x-\epsilon\leq X\leq x+\epsilon)}&{=}&{\displaystyle\frac{P (a\leq Y\leq b, x-\epsilon\leq X\leq x+\epsilon)}{P (x-\epsilon\leq X\leq x+\epsilon)}}\\ &{=}&{\displaystyle\frac{\int_{a}^{b}\int_{x-\epsilon}^{x+\epsilon}p (x^{\prime}, y) d y d x^{\prime}}{\int_{x-\epsilon}^{x+\epsilon}p (x^{\prime}) d x^{\prime}}.}\end{array}
$$ 
When $\epsilon$ is sufciently small, we can approximate 

$$
\int_{x-\epsilon}^{x+\epsilon}p (x^{\prime}) d x^{\prime}\approx2\epsilon p (x).
$$ 
Using a similar approximation for $p (x^{\prime}, y)$ , we get 

$$
\begin{array}{r c l}{P (a\leq Y\leq b\mid x-\epsilon\leq X\leq x+\epsilon)}&{\approx}&{\displaystyle\frac{\int_{a}^{b}2\epsilon p (x, y) d y}{2\epsilon p (x)}}\\ &{=}&{\displaystyle\int_{a}^{b}\frac{p (x, y)}{p (x)}d y.}\end{array}
$$ 
We conclude that $\textstyle{\frac{p (x, y)}{p (x)}}$ is the density of $P (Y\mid X=x)$ . 

***Definition 2.9*** 
Let $p (x, y)$ be the joint density of $X$ and $Y$ . The conditional density function of $Y$ given $X$ is defined as 
> 条件概率密度函数

$$
p (y\mid x)={\frac{p (x, y)}{p (x)}}
$$ 
When $p (x)=0$ , the conditional density is undefined. 
> 若 $p (x)$，则条件密度没有定义

The conditional density $p (y\mid x)$ characterizes the conditional distribution $P (Y\mid X=x)$ we defined earlier. 

The properties of joint distributions and conditional distributions carry over to joint and conditional density functions. In particular, we have the chain rule 

$$
p (x, y)=p (x) p (y\mid x)
$$ 
and Bayes’ rule 

$$
p (x\mid y)={\frac{p (x) p (y\mid x)}{p (y)}}.
$$ 
As a general statement, whenever we discuss joint distributions of continuous random variables, we discuss properties with respect to the joint density function instead of the joint distribution, as we do in the case of discrete variables. Of particular interest is the notion of (conditional) independence of continuous random variables. 

***Definition 2.10***
Let $X, Y$ , and $Z$ be sets of continuous random variables with joint density $p (X, Y, Z)$ . We say that $X$ is conditionally independent of $Y$ given $Z$ if 
$p(x\mid z) = p(x\mid y, z)$ for all $x, y, z$ such that $p (z) > 0$
### 2.1.7 Expectation and Variance 
#### 2.1.7.1 Expectation 
Let $X$ be a discrete random variable that takes numerical values; then the expectation of $X$ under the distribution $P$ is 

$$
E_{P}[X]=\sum_{x}x\cdot P (x).
$$ 
If $X$ is a continuous variable, then we use the density function 

$$
E_{P}[X]=\int x\cdot p (x) d x.
$$ 
For example, if we consider $X$ to be the outcome of rolling a fair die with probability $1/6$ for each outcome, then $\begin{array}{l}{\displaystyle{{\cal E}[X]\,=\, 1\,\cdot\,\frac{1}{6}\,+\, 2\,\cdot\,\frac{1}{6}\,+\,\cdot\,\cdot\,+\, 6\,\cdot\,\frac{1}{6}\,=\, 3.5}}\end{array}$ · · · · · · . 
On the other hand, if we consider a biased die where $P (X\,=\, 6)\,=\, 0.5$ and $P (X\,=\, x)\,=\, 0.1$ for $x\,<\, 6$ , then $\pmb{{\cal E}}[X]=1\cdot0.1+\cdot\cdot\cdot+5\cdot0.1+\cdot\cdot\cdot+6\cdot0.5=4.5$ · · · · · · · · · . 

Often we are interested in expectations of a function of a random variable (or several random variables). Thus, we might consider extending the definition to consider the expectation of a functional term such as $X^{2}+0.5X$ . 
Note, however, that any function $g$ of a set of random variables $X_{1},\ldots, X_{k}$ is essentially random variable $Y$ : For any outcome $\omega\in\Omega$ , we define the value of Y as $g (f_{X_{1}}(\omega),.\,.\,.\,, f_{X_{k}}(\omega))$ . 
> 对于随机变量集合定义的任意函数实际上定义了一个新的随机变量

Based on this discussion, we often define new random variables by a functional term. For example $Y=X^{2}$ , or $Y=e^{X}$ . We can also consider functions that map values of one or more categorical random variables to numerical values. One such function that we use quite often is the indicator function , which we denote ${\pmb 1}\{{\pmb X}={\pmb x}\}$  . This function takes value 1 when $X=x$ , and 0 otherwise. 

In addition, we often consider expectations of functions of random variables without bothering to name the random variables they define. For example $\pmb{{\cal E}}_{P}[X+Y]$ . Nonetheless, we should keep in mind that such a term does refer to an expectation of a random variable. 

We now turn to examine properties of the expectation of a random variable. 

First, as can be easily seen, the expectation of a random variable is a linear function in that random variable. Thus, 

$$
E[a\cdot X+b]=a E[X]+b.
$$

> 求期望是线性运算

A more complex situation is when we consider the expectation of a function of several random variables that have some joint behavior. An important property of expectation is that the expectation of a sum of two random variables is the sum of the expectations. 

***Proposition 2.4*** 
linearity of expectation 

$$
E[X+Y]=E[X]+E[Y].
$$ 
This property is called linearity of expectation. 
> 期望的线性性质，即便随机变量相互独立也满足

It is important to stress that this identity is true even when the variables are not independent. As we will see, this property is key in simplifying many seemingly complex problems. 

Finally, what can we say about the expectation of a product of two random variables? In general, very little: 


Example 2.5 
*Consider two random variables $X$ and $Y$ , each of which takes the value $+1$ with probability $1/2$ , and the value $-1$ wi probability $1/2$ . If $X$ and $Y$ are independent, then $E[X\cdot Y]=0$  . On the other hand, if X and $Y$ are correlated in that they always take the same value, then $E[X\cdot Y]=1.$* 


However, when $X$ and $Y$ are independent, then, as in our example, we can compute the expectation simply as a product of their individual expectations: 

***Proposition 2.5*** 
If $X$ and $Y$ are independent, then 
> 要求随机变量相互独立

$$
E[X\cdot Y]=E[X]\cdot E[Y].
$$ 
We often also use the expectation given some evidence. The conditional expectation of $X$ given $y$ is 
> 条件期望：和式中乘上的是条件概率

$$
E_{P}[X\mid\pmb{y}]=\sum_{x}x\cdot P (x\mid\pmb{y}).
$$ 
#### 2.1.7.2 Variance 
The expectation of $X$ tells us the mean value of $X$ . However, It does not indicate how far $X$ deviates from this value. A measure of this deviation is the variance of $X$ . 

$$
{\pmb V}\! a r_{P}[X]={\pmb E}_{P}\Big[\big (X-{\pmb E}_{P}[X]\big)^{2}\Big].
$$ 
Thus, the variance is the expectation of the squared difference between $X$ and its expected value. 
> 方差：随机变量相对于期望的距离平方的期望

It gives us an indication of the spread of values of $X$ around the expected value. 

An alternative formulation of the variance is 

$$
{V}\! a r[X]=E\big[X^{2}\big]-\big (E[X]\big)^{2}\,.\tag{2.16}
$$ 
(see exercise 2.11). Similar to the expectation, we can consider the expectation of a functions of random variables. 
> 方差同样只是定义于随机变量的一个函数

***Proposition 2.6*** 
If $X$ and $Y$ are independent, then 

$$
{V}a r[X+Y]=Va r[X]+Va r[Y].
$$ 
It is straightforward to show that the variance scales as a quadratic function of $X$ . In particular, we have: 

$$
\mathbb{W}a r[a\cdot X+b]=a^{2}\mathbb{W}a r[X].
$$ 
For this reason, we are often interested in the square root of the variance, which is called the standard deviation of the random variable. We define 

$$
\sigma_{X}={\sqrt{\mathbb{W}a r[X]}}.
$$ 
The intuition is that it is improbable to encounter values of $X$ that are farther than several standard deviations from the expected value of $X$ . Thus, $\sigma_{X}$ is a normalized measure of “distance” from the expected value of $X$ . 
As an example consider the Gaussian distribution of definition 2.7. 


***Proposition 2.7*** 
Let $X$ be a random variable with Gaussian distribution $N (\mu,\sigma^{2})$ , then $E[X]=\mu$ and $\mathbb{W}a r[X]=$ $\sigma^{2}$ . 

Thus, the parameters of the Gaussian distribution specify the expectation and the variance of the distribution. As we can see from the form of the distribution, the density of values of $X$ drops exponentially fast in the distance $\textstyle{\frac{x-\mu}{\sigma}}$ . 

Not all distributions show such a rapid decline in the probability of outcomes that are distant from the expectation. However, even for arbitrary distributions, one can show that there is a decline. 

***Theorem 2.1*** 
Chebyshev’s inequality 

$$
P (|X-\pmb{E}_{P}[X]|\ge t)\le\frac{Va r_{P}[X]}{t^{2}}.
$$ 
We can restate this inequality in terms of standard deviations: We write $t=k\sigma_{X}$ to get 

$$
P (|X-\pmb{E}_{P}[X]|\ge k\sigma_{X})\le\frac{1}{k^{2}}.
$$ 
Thus, for example, the probability of $X$ being more than two standard deviations away from $E[X]$ is less than $1/4$ . 

## 2.2 Graphs 
Perhaps the most pervasive concept in this book is the representation of a probability distribution using a graph as a data structure. In this section, we survey some of the basic concepts in graph theory used in the book. 
### 2.2.1 Nodes and Edges 
***Definition 2.11*** 
A graph is a data structure $\mathcal{K}$ consisting of a set of nodes and a set of edges. Throughout most this book, we will assume that the set of nodes is $\mathcal{X}=\{X_{1},\cdot\cdot\cdot, X_{n}\}$ . A pair of nodes $X_{i}, X_{j}$ can be connected by a directed edge $X_{i}\rightarrow X_{j}$ or an undirected edge $X_{i}{-}X_{j}$ . 
Thus, the set of edge  $\mathcal{E}$ is a set of pairs, where each pair is one of $X_{i}\rightarrow X_{j}$ → , $X_{j}\,\rightarrow\, X_{i}$ → , or $X_{i}{-}X_{j}$ for $X_{i}, X_{j}\in{\mathcal{X}}$  , $i<j$ . 

We assume throughout the book tha pair $X_{i}, X_{j}$ , at most one type of edge exists, we cannot have bot $X_{i}\,\rightarrow\, X_{j}$ and $X_{j}\,\rightarrow\, X_{i}$ nor can we have $X_{i}\rightarrow X_{j}$ and $X_{i}{-}X_{j}$ . the notation $X_{i}\gets X_{j}$ equivalent to $X_{j}\rightarrow X_{i}$ , and notation $X_{j}{-}X_{i}$ is equivalent to $X_{i}{-}X_{j}$ . We use $X_{i}\rightleftharpoons X_{j}$ to represent the case where $X_{i}$ and $X_{j}$ are connected via some edge, whether directed (in any direction) or undirected. 
> 我们假设本书中两点之间有向边和无向边不会同时存在

In many cases, we want to restrict attention to graphs that contain only edges of one kind or another. We say that a graph is directed if all edges are either $X_{i}\rightarrow X_{j}$ or $X_{j}\rightarrow X_{i}$ We usually denote directed graphs as $\mathcal{G}$ . We say that a graph is undirected if all edges are $X_{i}{-}X_{j}$ . We denote undirected graphs as H . We sometimes convert a general graph to an undirected graph by ignoring the directions on the edges. 


***Definition 2.11***
Given a graph $\mathcal{K}=(\mathcal{X},\mathcal{E})$ , its undirected version is a graph $\mathcal{H}=(\mathcal{X},\mathcal{E}^{\prime})$ where ${\mathcal{E}}^{\prime}=\{X{-}Y\ :$ $X\rightleftharpoons Y\in{\mathcal{E}}\}$ . 
> 忽视方向将有向图转化为无向图

Whenever we have that $X_{i}\,\rightarrow\, X_{j}\,\in\,{\mathcal{E}}$ , we say that $X_{j}$ is the child of $X_{i}$ in $\mathcal{K}$ , and that $X_{i}$ parent of $X_{j}$ in K . When have $X_{i}{-}X_{j}\,\in\,{\mathcal{E}}$ ∈E , we say th $X_{i}$ ighbor of $X_{j}$ n $\mathcal{K}$ (and vice versa). We a X and Y are adjacent wh r $X\rightleftharpoons Y\in{\mathcal{E}}$ ∈E . We use $\mathrm{Pa}_{X}$ to denote the parents of X , $\operatorname{Ch}_{X}$ to denote its children, and $\mathrm{Nb}_{X}$ to denote its neighbors. We defi the boundary of $X$ , denoted Boun ry $\cdot_{X}$ , to be $\mathrm{Pa}_{X}\cup\mathrm{Nb}_{X}$ ; for DAGs, this set is simply X ’s parents, and for undirected graphs X ’s neighbors. Figure 2.3 shows an example of a graph $\mathcal{K}$ . There we ve that $A$ is the only parent $C$ $F, I$ are the children o $C$ . The only neighbor of C is D , but its adjacent nodes are $A, D, F, I$ . The degree of a node X is the number of edges in which it participates. Its indegree is the number of directed edges $Y\rightarrow X$ . The degree of a graph is the maximal degree of a node in the graph. 
> 定义点的 boundary 为它的父节点和邻居的并集
> 对于有向无环图，boundary 就是它的父节点集合
> 对于无向图，boundary 就是邻居节点集合
> 图的度是图中节点度的最大值
### 2.2.2 Subgraphs 
In many cases, we want to consider only the part of the graph that is associated with a particular subset of the nodes. 

***Definition 2.12*** 
Let $\mathcal{K}=(\mathcal{X},\mathcal{E})$ , and let $X\subset{\mathcal{X}}$ , we define the induced subgraph $\mathcal{K}[X]$ to be the graph $(X,{\mathcal{E}}^{\prime})$ where $\mathcal{E}^{\prime}$ are all the edges $X\rightleftharpoons Y\in{\mathcal{E}}$ such that $X, Y\in X$ . 
> 导出子图：有结点子集定义，导出子图内包含了结点子集内所有互相连接的边

For example, figure 2.4a shows the induced subgraph $\mathcal{K}[C, D, I]$ . A type of subgraph that is often of particular interest is one that contains all possible edges. 

***Definition 2.13*** 
A subgraph over $X$ is complete if every two nodes in $X$ are connected by some edge. The set $X$ often called a clique ; we say that a clique $X$ is maximal if for any superset of nodes $Y\supset X$ , $Y$ is not a clique. Although the subset of nodes $X$ can be arbitrary, we are often interested in sets of nodes that preserve certain aspects of the graph structure. 
> 在一个节点集上的子图中，任意节点两两相连，称该子图是完全的，且该节点集是团
> 如果团的超集中没有团，则团是最大的

***Definition 2.14*** 
We say that a subset of nodes $X \subset \mathcal X$ is upwardly closed in $\mathcal K$ if, for any $x\in X$, we have $\text{Boundary}_x \subset X$. We define the upward closure of $X$ to be the minimal upwardly closed subset $Y$ that contains $X$. We define the upwardly closed subgraph of $X$ , denoted $\mathcal{K}^{+}[X]$ , to be the induced subgraph over $Y$ , $\mathcal{K}[Y]$ . 
> 对于一个节点集，如果它其中的任意节点的边界都在该节点集内，则该节点集是向上封闭的
> 一个节点集的上闭包就是包含了该节点集并且是向上封闭的最小节点集
> 一个节点集的向上封闭子图就是该节点集的上闭包的导出子图

For example, the set $A, B, C, D, E$ is the ard closure of the set $\{C\}$ in $\mathcal{K}$ . The upwardly closed subgraph of $\{C\}$ is shown in figure 2.4b. The upwardly closed subgraph of $\{C, D, I\}$ is shown in figure 2.4c. 
### 2.2.3 Paths and Trails 
Using the basic notion of edges, we can define diferent types of longer-range connections in the graph

***Definition 2.15*** 
We say that X1; : : : ; Xk form a path in the graph K = (X; E) if, for every i = 1; : : : ; k - 1, path we have that either Xi ! Xi+1 or Xi—Xi+1. A path is directed if, for at least one i, we have Xi ! Xi+1.
> 一组连续的节点相互按照排序两两连接（有向或无向，有向即 $X_i \rightarrow X_{i+1}$, 无向即 $X_i - X_{i+1}$），则形成了路径
> 如果其中至少一对连接是有向连接，路径就是有向路径


***Definition 2.16***
We say that $X_1,\dots, X_k$ form a trail in the graph K = (X; E) if, for every $i=1,\dots, k-1$, wehave that Xi ⇋ Xi+1.
> 迹 trail 比路径的要求更加宽松，路径要求连续的节点之间 $X_1,\dots ,X_k$ 的两两连接是按照排序的，即必须 $X_i \rightarrow X_{i+1}$ 或者 $X_i - X_{i+1}$，而迹则不要求一定要从小到大，只需要 $X_i, X_{i+1}$ 之间存在连接即可

In the graph K of figure 2.3, A; C; D; E; I is a path, and hence also a trail. On the other hand, A; C; F; G; D is a trail, which is not a path.


***Definition 2.17*** 
A graph is connected if for every Xi; Xj there is a trail between Xi and Xj.
> 连通图：任意两个节点之间都存在迹

We can now define longer-range relationships in the graph.

***Definition 2.18*** 
We say that X is an ancestor of Y in K = (X; E), and that Y is a descendant of X, if there ancestor descendant exists a directed path X1; : : : ; Xk with X1 = X and Xk = Y . We use DescendantsX to denote X’s descendants, AncestorsX to denote X’s ancestors, and NonDescendantsX to denote the set of nodes in X - DescendantsX.
> 如果图中存在一条从 $X$ 到 $Y$ 的有向路径，则称 $X$ 是 $Y$ 的祖先，$Y$ 是 $X$ 的后代，$X$ 的非后代就是节点集减去它的后代集

In our example graph K, we have that F; G; I are descendants of C. The ancestors of C are A, via the path A; C, and B, via the path B; E; D; C.

A final useful notion is that of an ordering of the nodes in a directed graph that is consistent with the directionality its edges.
> 有向图中，节点之间的顺序和边的指向是一致的

***Definition 2.19*** 
Let G = (X; E) be a graph. An ordering of the nodes $X_1,\dots, X_n$ is a topological ordering relative to K if, whenever we have Xi ! Xj 2 E, then i < j.
> 一个节点序列，只要满足任意出现在序列中的两个节点的先后顺序不违反图中边指向的顺序，这个序列就是一个拓扑序列

Appendix A.3.1 presents an algorithm for finding such a topological ordering.
### 2.2.4 Cycles and Loops 
Note that, in general, we can have a cyclic path that leads from a node to itself, making that node its own descendant. 
> 可以让节点指向自己，则自己是自己的后代

***Definition 2.20*** cycle
A cycle in K is a directed path $X_1, \dots, X_k$ where X1 = Xk. A graph is acyclic if it contains nocycles.
> 环：图中的一条有向路径的起点和终点相同，就构成了一个环，如果图没有环，就是无环图

A directed acyclic graph (DAG) is one of the central concepts in this book, as DAGs are the basic graphical representation that underlies Bayesian networks. For some of this book, we also use acyclic graphs that are partially directed. The graph K of figure 2.3 is acyclic. However, if we add the undirected edge A—E to K, we have a path A; C; D; E; A from A to itself. Clearly, adding a directed edge E ! A would also lead to a cycle. Note that prohibiting cycles does not imply that there is no trail from a node to itself. For example, K contains several trails: C; D; E; I; C as well as C; D; G; F; C.
> 没有环不代表没有指向节点自己的迹

An acyclic graph containing both directed and undirected edges is called a partially directed PDAG acyclic graph or PDAG. The acyclicity requirement on a PDAG implies that the graph can be chain component decomposed into a directed graph of chain components, where the nodes within each chain component are connected to each other only with undirected edges. The acyclicity of a PDAG guarantees us that we can order the components so that all edges point from lower-numbered components to higher-numbered ones.
> 既有有向边又有无向边的无环图称为部分有向无环图
> 部分有向无环图可以看作是一个由 chain components 构成的有向图，而 chain components 内部的边都是无向边
> 我们可以对 chain components 进行排序（根据边的关系）

***Definition 2.21*** 
Let K be a PDAG over X. Let $K_1,\dots, K_\ell$ be a disjoint partition of X such that:

• the induced subgraph over Ki contains no directed edges;
• for any pair of nodes X 2 Ki and Y 2 Kj for i < j, an edge between X and Y can only be a directed edge X ! Y .

> 对于一个部分有向无环图，对于它的节点集，对于该节点集的一个不相交的划分，满足：
> 划分成分导出的子图不包含有向边
> 任意一个来自于两个不同划分成分的节点对，如果二者之间有边，只能是按划分成分需要由低指向高的有向边

Each component Ki is called a chain component.
> 此时每个划分成分 $K_i$ 都称为 chain 成分

Because of its chain structure, a PDAG is also called a chain graph.
> 部分有向无环图也可以称为链图

Note that when the PDAG is an undirected graph, the entire graph forms a single chain component. Conversely, when the PDAG is a directed graph (and therefore acyclic), each node in the graph is its own chain component
> 部分有向无环图完全无向时，整个图就是一个 chain component
> 完全有向时，各个节点成为 chain component

***Definition 2.22*** loop
A loop in $\mathcal{K}$ is a trail $X_{1},\ldots, X_{k}$ where $X_{1}=X_{k}$ . A graph is singly connected if it contains no loops. A node in a singly connected graph is called $^a$ leaf if it has exactly one adjacent node. A singly connected directed graph is also called $^a$ polytree . A singly connected undirected graph is called $^a$ forest ; if it is also connected, it is called a tree . 
> loop：一个两端都是相同节点的迹称为回路
> 没有回路的图称为单连通图
> 单连通图内的一个节点只有一个邻接节点时，称为叶子
> 单连通有向图称为 polytree
> 单连通无向图称为森林，如果该图是连通的，称为树

We can also define a notion of a forest, or of a tree, for directed graphs. 

***Definition 2.24***
A directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is also connected. 
> 对于有向图，如果每个节点最多一个父节点，则称为有向森林
> 如果连通，就是树

Note that polytrees are very diferent from trees. For example, figure 2.5 shows a graph that is a polytree but is not a tree, because several nodes have more than one parent. As we will discuss later in the book, loops in the graph increase the computational cost of various tasks. 
> 对于有向图，polytree 中一个节点可以有多个父节点，树中只能有一个

We conclude this section with a final definition relating to loops in the graph. This definition will play an important role in evaluating the cost of reasoning using graph-based representations. 

***Definition 2.24*** 
$X_{1}{\mathrm{-}}X_{2}{\mathrm{-}}\cdot\cdot\cdot{\mathrm{-}}X_{k}{\mathrm{-}}X_{1}$ be a loop in the graph; $^a$ chord in the l p is an edge connecting $X_{i}$ and $X_{j}$ for two nonconsecutive nodes $X_{i}, X_{j}$ . An undirected graph H is said to be chordal if any loop $X_{1}{\mathrm{-}}X_{2}{\mathrm{-}}\cdot\cdot\cdot{\mathrm{-}}X_{k}{\mathrm{-}}X_{1}$ for $k\geq4$ has a chord. 
> 在一个回路中，一个 chord 是一个连接了回路中两个不相邻节点的边
> 如果一个无向图中的任意回路在>=4个不同节点时就一定有 chord，就是 chordal graph

Thus, for example, a loop $\mathit{A-B-C-D-A}$ (as in figure 1.1b) is nonchordal, but adding an edge $A{-}C$ would render it chordal. In other words, in a chordal graph, the longest “minimal loop” (one that has no shortcut) is a triangle. Thus, chordal graphs are often also called triangulated . 

We can extend the notion of chordal graphs to graphs that contain directed edges. 

***Definition 2.25***
A graph K is said to be chordal if its underlying undirected graph is chordal.
> 对于一个有向图，如果转化为无向图是 chordal，则它是 chordal

## 2.4 Exercises
**Exercise 2.5**
Let $\pmb X, \pmb Y, \pmb Z$ be three disjoint set of variables such that $\mathcal X = \pmb X \cup \pmb Y \cup \pmb Z$，Prove that $P\vDash (\pmb X \perp \pmb Y\mid \pmb Z)$ if and only if we can write $P$ in the form:

$$
P(\mathcal X) = \phi_1(\pmb X, \pmb Z)\phi_2(\pmb Y, \pmb Z)
$$

证明：
(1) $P\vDash (\pmb X \perp \pmb Y \mid \pmb Z)\Rightarrow P(\mathcal X) = \phi_1(\pmb X, \pmb Z)\phi_2(\pmb Y, \pmb Z)$
$P\vDash (\pmb X \perp \pmb Y \mid \pmb Z)$ 表明了 

$$
\begin{align}
P(\mathcal X) &= P(\pmb X, \pmb Y, \pmb Z)\\
&=P(\pmb X, \pmb Y\mid \pmb Z)P(\pmb Z)\\
&=P(\pmb X\mid \pmb Z)P(\pmb Y\mid \pmb Z)P(\pmb Z)\\
&=\phi_1(\pmb X, \pmb Z)\phi_2(\pmb Y, \pmb Z)P(\pmb Z)
\end{align}
$$

因为 $P (\pmb Z)$ 仅和 $\pmb Z$ 有关，它可以作为被吸收入 $\phi_1 (\pmb X, \pmb Z)$ 或 $\phi_2 (\pmb Y, \pmb Z)$，因此
$P(\mathcal X) = \phi_1(\pmb X, \pmb Z)\phi_2(\pmb Y, \pmb Z)$

(2) $P(\mathcal X) = \phi_1(\pmb X, \pmb Z)\phi_2(\pmb Y, \pmb Z)\Rightarrow P\vDash (\pmb X \perp \pmb Y \mid \pmb Z)$

$$
\begin{align}
P(\pmb X,\pmb Y\mid \pmb Z) &= \frac {P(\pmb X, \pmb Y, \pmb Z)}{P(\pmb Z)}\\
&=\frac {\phi_1(\pmb X,\pmb Z)\phi_2(\pmb Y, \pmb Z)}{P(\pmb Z)}\\
\end{align}
$$

显然等式右边可以分解为两个项的乘积，第一个项仅决定于 $\pmb X,\pmb Z$，第二个项仅决定于 $\pmb Y, \pmb Z$，也就是 $P (\pmb X, \pmb Y \mid \pmb Z) = P (\pmb X\mid \pmb Z) P (\pmb Y \mid \pmb Z)$，即 $P\vDash (\pmb X \perp \pmb Y \mid \pmb Z)$

# Part 1 Representation
# 3 The Baysian Network Representation
Our goal is to represent a joint distribution $P$ over some set of random variables $\mathcal X=\{X_1,\dots, X_n\}$. Even in the simplest case where these variables are binary-valued, a joint distribution requires the specification of $2^{n-1}$ numbers — the probabilities of the $2^n$ different assignments of values $x_1,\dots, x_n$. 
> 目标是表示出包含 $n$ 个变量的联合分布 $P$

For all but the smallest $n$, the explicit representation of the joint distribution is unmanageable from every perspective. Computationally, it is very expensive to manipulate and generally too large to store in memory. Cognitively, it is impossible to acquire so many numbers from a human expert; moreover, the numbers are very small and do not correspond to events that people can reasonably contemplate. Statistically, if we want to learn the distribution from data, we would need ridiculously large amounts of data to estimate this many parameters robustly. These problems were the main barrier to the adoption of probabilistic methods for expert systems until the development of the methodologies described in this book.
> 由于指数爆炸问题，不能直接表示每个事件的概率

In this chapter, we first show how independence properties in the distribution can be used to represent such high-dimensional distributions much more compactly. We then show how a combinatorial data structure — a directed acyclic graph — can provide us with a general purpose modeling language for exploiting this type of structure in our representation.
> 本章先讨论如何使用分布中的独立性质来紧凑表示高维分布
> 然后讨论有向无环图的使用 
## 3.1 Exploiting Independence Properties
The compact representations we explore in this chapter are based on two key ideas: the representation of independence properties of the distribution, and the use of an alternative parameterization that allows us to exploit these finer-grained independencies.
> 本章讨论的紧凑表示基于两个关键思想：
> 分布的独立表示性质、使用参数化以利用这些细粒度的独立性
### 3.1.1 Independent Random Variables 
To motivate our discussion, consider a simple setting where we know that each $X_{i}$ represents the outcome of a toss of coin $i$. In this case, we typically assume that the diferent coin tosses are marginally independent (definition 2.4), so that our distribution $P$ will satisfy $\left(X_{i}\perp X_{j}\right)$ for any $i, j$ . More generally (strictly more generally — see exercise 3.1), we assume that the distribution satisfies $(X\perp Y)$ for any disjoint subsets of the variables $X$ and $Y$. Therefore, we have that: 
> 我们让每个 $X_i$ 表示投掷硬币 $i$ 的结果，分布 $P$ 满足对于任意的 $i, j$， $X_i, X_j$ 之间边际独立
> 同时假设分布满足 $X\perp Y$

$$
P(X_{1},\cdot\cdot\cdot,X_{n})=P(X_{1})P(X_{2})\cdot\cdot\cdot P(X_{n}).
$$ 
If we use the standard parameterization of the joint distribution, this independence structure is obscured, and the representation of the distribution requires $2^{n}$ parameters. 
> 这个联合分布包含了 $2^n$ 个可能的事件，显然不能用 $2^n$ 个参数建模每个事件的概率

However, we can use a more natural set of parameters for specifying this distribution: If $\theta_{i}$ is the probability with which coin $i$ lands heads, the joint distribution $P$ can be specified using the $n$ parameters $\theta_{1},\ldots,\theta_{n}$ . These parameters implicitly specify the $2^{n}$ probabilities in the joint distribution. For example, the pro that all coin t eads is $\theta_{1}\cdot\theta_{2}\cdot...\cdot\theta_{n}$ . More generally, letting $\theta_{x_{i}}=\theta_{i}$ when $x_{i}=x_{i}^{1}$ and $\theta_{x_{i}}=1-\theta_{i}$ when $x_{i}=x_{i}^{0}$ , we can define: 
> 我们用一个参数就可以表示每个随机变量的两种取值的概率，因此用 $n$ 个参数就可以表示整个联合分布

$$
P(x_{1},.\,.\,.\,,x_{n})=\prod_{i}\theta_{x_{i}}.\tag{3.1}
$$ 
This representation is limited, and there are many distributions that we cannot capture by choosing values for $\theta_{1},\ldots,\theta_{n}$ . This fact is obvious not only from intuition, but also from a somewhat m al perspe.
The space of all $2^{n}-1$ dimensional subspace of I $I\!\!R^{2^{n}}\ -$ — the set { $\{(p_{1},.\,.\,,\dot{p_{2^{n}}})\,\in\,I\!\!R^{2^{n^{\prime}}}\,:\,\,p_{1}+.\,.\,.+p_{2^{n}}\,=\,1\}$ . On the other hand, the space of all joint distributions specified in a factorized way as in equation (3.1) is an $n$ -dimensional manifold in ${I\!\!R}^{2^{n}}$ . 
> 所有联合分布的空间是 $\mathbb R^{2^n}$ 的一个 $2^n -1$ 维子空间，而使用 $n$ 个参数可以建模的联合分布的空间仅是 $\mathbb R^{2^n}$ 空间的一个 $n$ 维流形，因此这类表示也是受限的

A key concept here is the notion of independent parameters — parameters whose values are not determined by others. For example, when specifying an arbitrary multinomial distribution over a $k$ dimensional sp e have $k-1$ independent parameters: the last probability is fully determined by the first $k-1$ − . In the case where we have an arbitrary istribution over $n$ binary random variables, the number of independent parameters is 2 $2^{n}\mathrm{~-~}1$ − . On the other hand, the number of independent parameters for distributions represented as $n$ independent binomial coin tosses is $n$ . Therefore, the two spaces of distributions cannot be the same. (While this argument might seem trivial in this simple case, it turns out to be an important tool for comparing the expressive power of diferent representations.) 
> 这里涉及的一个概念是独立参数，即值不依赖于其他参数的参数
> 例如，指定 $k$ 维空间的任意一个多项式分布需要 $k-1$ 个独立参数，表示最后一个概率的参数由前面的 $k-1$ 个参数决定
> 本例中，$n$ 个 binary 随机变量的联合分布空间需要 $2^n - 1$ 个独立参数，而 $n$ 个相互独立的伯努利试验仅需要 $n$ 个独立参数建模联合分布，因此这两个联合分布空间是不可能相同的
> 独立参数的数量可以用于比较两个表示的表示能力

As this simple example shows, certain families of distributions — in this case, the distributions generated by $n$ independent random variables — permit an alternative parameter iz ation that is substantially more compact than the naive representation as an explicit joint distribution. Of course, in most real-world applications, the random variables are not marginally independent. However, a generalization of this approach will be the basis for our solution. 
> 本例展示了对于特定的分布族可以用一个使用更少独立参数的表示替代原有的朴素且需要大量参数的表示，这就是我们的基本思想
### 3.1.2 The Conditional Parameterization 
Let us begin with a simple example that illustrates the basic intuition. Consider the problem faced by a company trying to hire a recent college graduate. The company’s goal is to hire intelligent employees, but there is no way to test intelligence directly. However, the company has access to the student’s SAT scores, which are informative but not fully indicative. 
Thus, our probability space is induced by the two random variables Intelligence $(I)$ and $S A T\ (S)$ . For simplicity, we assume that e of these takes two va s: $V a l(I)=\{i^{1},i^{0}\}$ present the values high intelligence ( $(i^{1})$ ) and low intelligence ( $(i^{0})$ ); similarly V $V a l(S)\,=\,\{s^{1},s^{0}\}$ , which also represent the values high (score) and low (score), respectively. 
> 我们的概率空间由两个随机变量 $I, S$ 导出 

Thus, our joint distribution in this case has four entries. For example, one possible joint distribution $P$ would be 
> 图见书，这里列出了联合分布 $P$ 中的所有可能结果及其概率

There is, however, an alternative, and even more natural way of representing the same joint distribution. Using the chain rule of conditional probabilities (see equation (2.5)), we have that 

$$
P(I,S)=P(I)P(S\mid I).
$$ 
Intuitively, we are representing the process in a way that is more compatible with causality. Var- ious factors (genetics, upbringing, . . . ) first determined (stochastically) the student’s intelligence. His performance on the SAT is determined (stochastically) by his intelligence. We note that the models we construct are not required to follow causal intuitions, but they often do. We return to this issue later on. 
> 用条件概率分布表示该联合概率分布，我们采用了一个更符合因果关系的表示

From a mathematical perspective, this equation leads to the following alternative way of representing the joint distribution. Instead of specifying the various joint entries $P(I,S)$ , we would specify it in the form of $P(I)$ and $P(S\mid I)$ . Thus, for example, we can represent the joint distribution of equation (3.2) using the following two tables, one representing the prior distribution over $I$ and the other the conditional probability distribution (CPD) of $S$ given $I$ : 
> 建模了因果关系之后，我们不再用一张大表表示整个联合分布，而是用了两个表，一个表表示 $I$ 上的先验分布，另一个表表示给定 $I$ 时，$S$ 的条件概率分布

The CPD $P(S\mid I)$ represents the probability that the student will succeed on his SATs in the two possible cases: the case where the student’s intelligence is low, and the case where it is high. The CPD asserts that a student of low intelligence is extremely unlikely to get a high SAT score $(P(s^{1}\mid i^{0})=0.05)$ ; on the other hand, a student of high intelligence is likely, but far from certain, to get a high SAT score $(P(s^{1}\mid i^{1})=0.8)$ . 

It is instructive to consider how we could parameterize this alternative representation. Here, we are using three binomial distributions, one for $P(I)$ , and two for $P(S\mid i^{0})$ and $P(S\mid i^{1})$ . Hence, we can parameterize this representation using three independent parameters, say $\theta_{i^{1}}$ , $\theta_{s^{1}|i^{1}}$ , and $\theta_{s^{1}|i^{0}}$ . Our representation of the joint distribution as a four-outcome multinomial also required three parameters. Thus, although the conditional representation is more natural than the explicit representation of the joint, it is not more compact. 
However, as we will soon see, the conditional parameterization provides a basis for our compact representations of more complex distributions. 
> 对于这个表示的参数化，我们使用了三个二项分布：$P (I), P (S\mid i^0), P (S\mid i^1)$，因此使用三个参数 $\theta_{i^1}, \theta_{s^1\mid i^1}, \theta_{s^1\mid i^1}$

Although we will only define Bayesian networks formally in section 3.2.2, it is instructive to see how this example would be represented as one. The Bayesian network, as shown in figure 3.1a, would have a node for each of the two random variables $I$ and $S$ , with an edge from $I$ to $S$ representing the direction of the dependence in this model. 
### 3.1.3 The Naive Bayes Model 
We now describe perhaps the simplest example where a conditional parameter iz ation is com- bined with conditional independence assumptions to produce a very compact representation of a high-dimensional probability distribution. Importantly, unlike the previous example of fully independent random variables, none of the variables in this distribution are (marginally) independent. 
> 本节描述如何结合条件参数化和条件独立假设以紧凑表示一个高维概率分布
#### 3.1.3.1 The Student Example 
Elaborating our example, we now assume that the company also has access to the student’s grade $G$ in some course. In this case, our probability space is the joint distribution over the three relevant random variables $I,\,S,$ , and $G$ . Assuming that $I$ and $S$ are as before, and that $G$ takes on three values $g^{1},g^{2},g^{3}$ , representing the grades $A,\,B$ , and $C$ , respectively, then the joint distribution has twelve entries. 

Before we even consider the speciﬁc numerical aspects of our distribution $P$ in this example, we can see that independence does not help us: for any reasonable $P$ , there are no indepen- dencies that hold. The student’s intelligence is clearly correlated both with his SAT score and with his grade. The SAT score and grade are also not independent: if we condition on the fact that the student received a high score on his SAT, the chances that he gets a high grade in his class are also likely to increase. Thus, we may assume that, for our particular distribution $P$ , $P(g^{1}\mid s^{1})>P(g^{1}\mid s^{0})$ . 
> 本例中的随机变量 $I, G, S$ 显然不满足独立性，但可以假设满足某种条件独立性

However, it is quite plausible that our distribution $P$ in this case satisﬁes a conditional independence property. If we know that the student has high intelligence, a high grade on the SAT no longer gives us information about the student’s performance in the class. More formally: 
> 已知 $i^1$ 之后，$s^1$ 就不会给出更多关于 $g$ 的信息

$$
P(g\mid i^{1},s^{1})=P(g\mid i^{1}).
$$ 
More generally, we may well assume that 

$$
P\vDash(S\perp G\mid I).\tag{3.4}
$$ 
Note that this independence statement holds only if we assume that the student’s intelligence is the only reason why his grade and SAT score might be correlated. In other words, it assumes that there are no correlations due to other factors, such as the student’s ability to take timed exams. These assumptions are also not “true” in any formal sense of the word, and they are often only approximations of our true beliefs. (See box 3. C for some further discussion.) 
> 该条件独立性的假设是 $I$ 是 $S$ 和 $G$ 在 $P$ 中展现出高度相关的唯一原因，即假设了 $S$ 和 $G$ 没有由于其他原因而出现的相关性

As in the case of marginal independence, conditional independence allows us to provide a compact speciﬁcation of the joint distribution. Again, the compact representation is based on a very natural alternative parameter iz ation. By simple probabilistic reasoning (as in equation (2.5)), we have that 

$$
P(I,S,G)=P(S,G\mid I)P(I).
$$ 
But now, the conditional independence assumption of equation (3.4) implies that 

$$
P(S,G\mid I)=P(S\mid I)P(G\mid I).
$$ 
Hence, we have that 

$$
P(I,S,G)=P(S\mid I)P(G\mid I)P(I).\tag{3.5}
$$

> 利用条件独立性，我们将联合分布再次表示成条件概率分布的乘积的形式

Thus, we have factorized the joint distribution $P(I,S,G)$ as a product of three conditional probability distributions (CPDs). This factorization immediately leads us to the desired alternative parameter iz ation. In order to specify fully a joint distribution satisfying equation (3.4), we need the following three CPDs: $P(I),\,P(S\mid I),$ , and $P(G\mid I)$ . The ﬁrst two might be the same as in equation (3.3). The latter might be 
> 之后，我们对分布 $P (I), P (S\mid I), P (G\mid I)$ 进行参数化

$$
\frac{I\;\;\parallel\;\;g^{1}\;\;\;\;\;g^{2}\;\;\;\;\;g^{3}}{i^{0}\;\parallel\;0.2\;\;\;\;0.34\;\;\;\;0.46\;\;}
$$ 
Together, these three CPDs fully specify the joint distribution (assuming the conditional inde- pendence of equation (3.4)). For example, 

$$
\begin{array}{l c l}{P(i^{1},s^{1},g^{2})}&{=}&{P(i^{1})P(s^{1}\mid i^{1})P(g^{2}\mid i^{1})}\\ &{=}&{0.3\cdot0.8\cdot0.17=0.0408.}\end{array}
$$ 
Once again, we note that this probabilistic model would be represented using the Bayesian network shown in ﬁgure 3.1b. 

In this case, the alternative parameter iz ation is more compact than the joint. We now have three binomial distributions — $P(I)$ , $P(S\mid i^{1})$ and $P(S\mid i^{0})$ , and two three-valued multino- mial distributions — $P(G\mid i^{1})$ and $P(G\mid i^{0})$ . Each of the binomials requires one independent parameter, and each three-valued multinomial requires two independent parameters, for a total of seven. By contrast, our joint distribution has twelve entries, so that eleven independent parameters are required to specify an arbitrary joint distribution over these three variables. 

It is important to note another advantage of this way of representing the joint: modularity. When we added the new variable $G$ , the joint distribution changed entirely. Had we used the explicit representation of the joint, we would have had to write down twelve new numbers. In the factored representation, we could reuse our local probability models for the variables $I$ and $S$ , and specify only the probability model for $G-$ the CPD $P(G\mid I)$ . This property will turn out to be invaluable in modeling real-world systems. 
> 这类表示还有一个优势就是模块性，当加入新的变量 $G$ 之后，虽然联合分布完全改变了，但我们仍可以复用一部分的局部概率分布，例如 $P (I)$
#### 3.1.3.2 The General Model 
![[Probabilistic Graph Models-Fig3.2.png]]

This example is an instance of a much more general model commonly called the naive Bayes model (also known as the Idiot Bayes model ). The naive Bayes model assumes that instances fall into one of a number of mutually exclusive and exhaustive classes . Thus, we have a class variable $C$ that takes on va es in some set $\{c^{1},\cdot\cdot\cdot,c^{k}\}$ . In our example, the class variable is the student’s intelligence I , and there are two classes of instances — students with high intelligence and students with low intelligence. 
> 上例使用的就是朴素贝叶斯模型，朴素贝叶斯模型假设实例落入一个互斥且完备的类集合中的一个类，记为一个类变量 $C$ 从 $\{c^1,\dots, c^k\}$ 中取值
> 上例中，类变量就是 $I$，有两类实例：IQ 高的学生和 IQ低的学生

The model also includes some number of features $X_{1},\dots,X_{n}$ whose values are typically observed. The naive Bayes assumption is that the features are conditionally independent given the instance’s class. In other words, within each class of instances, the diferent properties can be determined independently. 
> 朴素贝叶斯模型还包含了一定数量的特征 $X_1,\dots, X_n$，特征的值是被观测到的
> 朴素贝叶斯假设特征在给定了实例的类别的情况下是条件独立的，也就是在每类实例中，不同的属性都是独立决定的

More formally, we have that 

$$
(X_{i}\perp \pmb X_{-i}\mid C)\quad{\mathrm{~for~all~}}i,\tag{3.6}
$$ 
where $\pmb X_{-i}\;=\;\{X_{1},.\,.\,.\,,X_{n}\}\,-\,\{X_{i}\}$ . 

This model can be represented using the Bayesian network of ﬁgure 3.2. In this example, and later on in the book, we use a darker oval to represent variables that are always observed when the network is used. 

Based on these independence assumptions, we can show that the model factorizes as: 

$$
P(C,X_{1},\dots,X_{n})=P(C)\prod_{i=1}^{n}P(X_{i}\mid C).\tag{3.7}
$$ 
(See exercise 3.2.) Thus, in this model, we can represent the joint distribution using a small set of factors: a prior distribution $P(C)$ , specifying how likely an instance is to belong to diferent classes a priori, and a set of CPDs $P(X_{j}\mid C)$ , one for each of the $n$ ﬁnding variables. 
> 基于这些假设，我们将联合分布分解如上
> 因此，在朴素贝叶斯模型中，我们用一个先验分布 $P (C)$ (表示实例属于哪个类别的概率)，以及一系列条件概率分布 $P (X_j\mid C)$ 表示联合分布

These factors can be encoded using a very small number of parameters. For example, if all of the variables are binary, the number of independent parameters required to specify the distribution is $2n+1$ (see exercise 3.6). Thus, the number of parameters is linear in the number of variables, as opposed to exponential for the explicit representation of the joint. 
> 这种表示下，参数的数量和随机变量的数量是呈线性关系的
> 显式表示联合分布中，则是呈指数关系

**Box 3. A — Concept: The Naive Bayes Model.** 
The naive Bayes model, despite the strong as- sumptions that it makes, is often used in practice, because of its simplicity and the small number of parameters required. The model is generally used for classiﬁcation — deciding, based on the values of the evidence variables for $a$ given instance, the class to which the instance is most likely to belong.  We might also want to compute our conﬁdence in this decision, that is, the extent to which our model favors one class $c^{1}$ over another $c^{2}$ .
> 朴素贝叶斯模型常用于分类：给定一个实例的许多特征变量的值，决定它的类别
> 同时可以计算该决定相对于其他决定的置信度比例

 Both queries can be addressed by the following ratio: 

$$
{\frac{P(C=c^{1}\mid x_{1},\ldots,x_{n})}{P(C=c^{2}\mid x_{1},\ldots,x_{n})}}={\frac{P(C=c^{1})}{P(C=c^{2})}}\prod_{i=1}^{n}{\frac{P(x_{i}\mid C=c^{1})}{P(x_{i}\mid C=c^{2})}};\tag{3.8}
$$ 
see exercise 3.2). This formula is very natural, since it computes the posterior probability ratio of $c^{1}$ versus $c^{2}$ as a product of their prior probability ratio (the first term), multiplied by a set of terms $\scriptstyle{\frac{P(x_{i}|C=c^{1})}{P(x_{i}|C=c^{2})}}$ that measure the relative support of the finding $x_{i}$ for the two classes. 

This model was used in the early days of medical diagnosis , where the diferent values of the class variable represented diferent diseases that the patient could have. The evidence variables represented diferent symptoms, test results, and the like. Note that the model makes several strong assumptions that are not generally true, speciﬁcally that the patient can have at most one disease, and that, given the patient’s disease, the presence or absence of diferent symptoms, and the values of diferent tests, are all independent. This model was used for medical diagnosis because the small number of interpretable parameters made it easy to elicit from experts. For example, it is quite natural to ask of an expert physician what the probability is that a patient with pneumonia has high fever. Indeed, several early medical diagnosis systems were based on this technology, and some were shown to provide better diagnoses than those made by expert physicians. 

However, later experience showed that the strong assumptions underlying this model decrease its diagnostic accuracy. In particular, the model tends to overestimate the impact of certain evidence by “overcounting” it. For example, both hypertension (high blood pressure) and obesity are strong indicators of heart disease. However, because these two symptoms are themselves highly correlated, equation (3.8), which contains a multiplicative term for each of them, double-counts the evidence they provide about the disease. Indeed, some studies showed that the diagnostic performance of a naive Bayes model degraded as the number of features increased; this degradation was often traced to violations of the strong conditional independence assumption. This phenomenon led to the use of more complex Bayesian networks, with more realistic independence assumptions, for this application (see box 3. D). 
> 条件独立性假设在实际中可能产生的问题是高估某个证据的影响，例如实际中，在类别 $C$ 下，特征 $X_i, X_j$ 实际上是高度相关的 (交集大)，因此 $P (X_i, X_j\mid C)$ 会小于 $P (X_i\mid C) P (X_j\mid C)$，此时特征 $X_i, X_j$ 的出现会过度支持类别 $C$
> 随着变量的数量增大，朴素贝叶斯效果往往下降，往往因为对于条件独立性的违反变得更加严重了

Nevertheless, the naive Bayes model is still useful in a variety of applications, particularly in the context of models learned from data in domains with a large number of features and a rela- tively small number of instances, such as classifying documents into topics using the words in the documents as features; see box 17. E). 
## 3.2 Bayesian Networks 
Bayesian networks build on the same intuitions as the naive Bayes model by exploiting conditional independence properties of the distribution in order to allow a compact and natural representation. However, they are not restricted to representing distributions satisfying the strong independence assumptions implicit in the naive Bayes model. They allow us the ﬂexibility to tailor our representation of the distribution to the independence properties that appear reasonable in the current setting. 
> 贝叶斯网络的假设不像朴素贝叶斯一样简单，我们可以灵活定义贝叶斯网络中的条件独立性

The core of the Bayesian network representation is a directed acyclic graph (DAG) $\mathcal{G}$ , whose nodes are the random variables in our domain and whose edges correspond, intuitively, to direct inﬂuence of one node on another. This graph $\mathcal{G}$ can be viewed in two very diferent ways: 
> 贝叶斯网络使用 DAG 表示，节点是随机变量，边表示变量之间的影响关联
> 网络用分解的方式表示了联合分布，以及表示了分布之间的条件独立性

- as a data structure that provides the skeleton for representing a joint distribution compactly in a factorized way; 
- as a compact representation for a set of conditional independence assumptions about a distribution. 

As we will see, these two views are, in a strong sense, equivalent. 
### 3.2.1 The Student Example Revisited 
We begin our discussion with a simple toy example, which will accompany us, in various versions, throughout much of this book. 

![[Probabilistic Graph Theory-Fig3.3.png]]

#### 3.2.1.1 The Model 
Consider our student from before, but now consider a slightly more complex scenario. The student’s grade, in this case, depends not only on his intelligence but also on the difculty of the course, represented by a random variable $D$ whose domain is $V a l(D)=\{e a s y,h a r d\}$ . Our student asks his professor for a recommendation letter. The professor is absentminded and never remembers the names of her students. She can only look at his grade, and she writes her letter for him based on that information alone. The quality of her letter is a random variable $L$ , whose domain is $V a l(L)=\left\{s t r o n g,w e a k\right\}$ . The actual quality of the letter depends stochastically on the grade. (It can vary depending on how stressed the professor is and the quality of the cofee she had that morning.) 

We therefore have ﬁve random variables in this domain: the student’s intelligence $(I)_{i}$ , the course difculty $(D)$ , the grade $(G)$ , the student’s SAT score $(S)$ , and the quality of the recom- mendation letter $(L)$ . All of the variables except $G$ are binary-valued, and $G$ is ternary-valued. Hence, the joint distribution has 48 entries. 

As we saw in our simple illustrations of ﬁgure 3.1, a Bayesian network is represented using a directed graph whose nodes represent the random variables and whose edges represent direct inﬂuence of one variable on another. We can view the graph as encoding a generative sampling process executed by nature, where the value for each variable is selected by nature using a distribution that depends only on its parents. In other words, each variable is a stochastic function of its parents. 
> 我们将被贝叶斯网络视为它编码了自然执行的生成过程，其中每个随机变量的值都由自然通过一个分布选择，这个分布决定于该变量的父节点，即每个变量都是它的父节点的一个随机函数

Based on this intuition, perhaps the most natural network structure for the distribution in this example is the one presented in ﬁgure 3.3. The edges encode our intuition about the way the world works. The course difculty and the student’s intelligence are determined independently, and before any of the variables in the model. The student’s grade depends on both of these factors. The student’s SAT score depends only on his intelligence. The quality of the professor’s recommendation letter depends (by assumption) only on the student’s grade in the class. Intuitively, each variable in the model depends directly only on its parents in the network. We formalize this intuition later. 

The second component of the Bayesian network representation is a set of local probability models that represent the nature of the dependence of each variable on its parents. One such model, $P(I)$ , represents the distribution in the population of intelligent versus less intelligent student. Another, $P(D)$ , represents the distribution of difcult and easy classes. The distribution over the student’s grade is a conditional distribution $P(G\mid I,D)$ . It speciﬁes the distribution over the student’s grade, inasmuch as it depends on the student’s intelligence and the course difculty. Speciﬁcally, we would have a diferent distribution for each assignment of values $i,d$ . For example, we might believe that a smart student in an easy class is 90 percent likely to get an A, 8 percent likely to get a B, and 2 percent likely to get a C. Conversely, a smart student in a hard class may only be 50 percent likely to get an A. 
In general, each variable $X$ in the model is associated with a conditional probability distribution (CPD) that speciﬁes a distribution over the values of $X$ given each possible joint assignment of values to its parents in the model. For a node with no parents, the CPD is conditioned on the empty set of variables. Hence, the CPD turns into a marginal distribution, such as $P(D)$ or $P(I)$ . One possible choice of CPDs for this domain is shown in ﬁgure 3.4. The network structure together with its CPDs is a Bayesian network $\mathcal{B}$ ; we use $\mathcal{B}^{s t u d e n t}$ to refer to the Bayesian network for our student example. 
> 贝叶斯网络表示的第二个成分是一系列局部概率模型，它们表示了各个变量和其父变量之间的依赖关系
> 贝叶斯网络中的每个变量都从属于一个条件概率分布，每个父变量的变量条件于空集，因此其条件概率分布就是边际分布

How do we use this data structure to specify the joint distribution? Consider some particular state in this space, for example, $i^{1},d^{0},g^{2},s^{1},l^{0}$ . Intuitively, the probability of this event can be computed from the probabilities of the basic events that comprise it: the probability that the student is intelligent; the probability that the course is easy; the probability that a smart student gets a B in an easy class; the probability that a smart student gets a high score on his SAT; and the probability that a student who got a B in the class gets a weak letter. The total probability of this state is: 

$$
\begin{array}{l l l}{{P(i^{1},d^{0},g^{2},s^{1},l^{0})}}&{{=}}&{{P(i^{1})P(d^{0})P(g^{2}\mid i^{1},d^{0})P(s^{1}\mid i^{1})P(l^{0}\mid g^{2})}}\\ {{}}&{{=}}&{{0.3\cdot0.6\cdot0.08\cdot0.8\cdot0.4=0.004608.}}\end{array}
$$ 
Clearly, we can use the same process for any state in the joint probability space. In general, we will have that 

$$
P(I,D,G,S,L)=P(I)P(D)P(G\mid I,D)P(S\mid I)P(L\mid G).\tag{3.9}
$$ 
chain rule for Bayesian networks 

This equation is our ﬁrst example of the chain rule for Bayesian networks which we will deﬁne in a general setting in section 3.2.3.2. 
#### 3.2.1.2 Reasoning Patterns 
A joint distribution $P_{\mathcal{B}}$ speciﬁes (albeit implicitly) the probability $P_{\mathcal B}(Y\,=\,y\;\mid\;E\,=\,e)$ of any event $y$ given any observations $e$ , as discussed in section 2.1.3.3: We condition the joint distribution on the event $E=e$ by eliminating the entries in the joint inconsistent with our observation $e$ , and renormalizing the resulting entries to sum to 1; we compute the probability of the event $y$ by summing the probabilities of all of the entries in the resulting posterior distribution that are consistent with $y$ .
> 联合分布对于任何事件指定了条件概率分布 $P (Y = y \mid E = e)$，我们通过消除和观察 $e$ 不一致的项，然后将剩下的项规范化到和为 1，来条件于 $E = e$，通过对于剩下的项中全部和 $y$ 一致的项求和，得到事件 $y$ 的概率

To illustrate this process, let us consider our B student network and see how the probabilities of various events change as evidence is obtained. 

Consider a particular student, George, about whom we would like to reason using our model. We might ask how likely George is to get a strong recommendation $(l^{1})$ from his professor in Econ101. Knowing nothing else about George or Econ101, this probability is about 50.2 percent. More precisely, let $P_{\mathcal{B}^{s t u d e n t}}$ be the joint distribution deﬁned by the preceding BN; then we have that $P_{\mathcal{B}^{s t u d e n t}}(l^{1})\approx0.502$ . We now ﬁnd out that George is not so intelligent $(i^{0})$ ; the probability that he gets a strong letter from the professor of Econ101 goes down to around 38.9 percent; that is, $P_{\mathcal{B}^{s t u d e n t}}(l^{1}\mid i^{0})\approx0.389.$ . We now further discover that Econ101 is an easy class $(d^{0})$ . The probability that George gets a strong letter from the professor is now $P_{\mathcal{B}^{s t u d e n t}}(l^{1}\mid i^{0},d^{0})\approx0.513$ . Queries such as these, where we predict the “downstream” efects of various factors (such as George’s intelligence), are instances of causal reasoning or prediction . 
> 从贝叶斯网络的最顶端节点自上而下推理的过程/预测多个因素导致的效果的过程，就是因果推理或者预测

Now, consider a recruiter for Acme Consulting, trying to decide whether to hire George based on our previous model. A priori, the recruiter believes that George is 30 percent likely to be intelligent. He obtains George’s grade record for a particular class Econ101 and sees that George received a C in the class $(g^{3})$ . His probability that George has high intelligence goes down signiﬁcantly, to about 7.9 percent; that is, $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid g^{3})\approx0.079$ . We note that the probability that the class is a difcult one also goes up, from 40 percent to 62.9 percent. 

Now, assume that the recruiter fortunately (for George) lost George’s transcript, and has only the recommendation letter from George’s professor in Econ101, which (not surprisingly) is evidential reasoning weak. The probability that George has high intelligence still goes down, but only to 14 percent: $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid l^{0})\approx0.14$ . Note that if the recruiter has both the grade and the letter, we have the same probability as if he had only the grade: $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid g^{3},l^{0})\approx0.079;$ ; we will revisit this issue. Queries such as this, where we reason from efects to causes, are instances of evidential reasoning or explanation . 
> 从效果推理导致它的原因的过程，就是证据推理或者解释

Finally, George submits his SAT scores to the recruiter, and astonishingly, his SAT score is high. The probability that George has high intelligence goes up dramatically, from 7.9 percent to 57.8 percent: $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid g^{3},s^{1})\approx0.578$ . Intuitively, the reason that the high SAT score outweighs the poor grade is that students with low intelligence are extremely unlikely to get good scores on their SAT, whereas students with high intelligence can still get C’s. However, smart students are much more likely to get C’s in hard classes. Indeed, we see that the probability that Econ101 is a difcult class goes up from the 62.9 percent we saw before to around 76 percent. 

This last pattern of reasoning is a particularly interesting one. The information about the SAT gave us information about the student’s intelligence, which, in conjunction with the student’s grade in the course, told us something about the difculty of the course. In efect, we have one causal factor for the Grade variable — Intelligence — giving us information about another — Diffculty . 

Let us examine this pattern in its pure form. As we said, $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid g^{3})\approx0.079$ other hand, if we now discover that Econ101 is a hard class, we have that $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid g^{3},d^{1})\approx$ B | ≈ 0 . 11 . In efect, we have provided at least a partial explanation for George’s grade in Econ101. To take an even more striking example, if George gets a B in Econ 101, we have that $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid$ $g^{2})\approx0.175$ . On the other hand, if Econ101 is a hard class, we get $P_{\mathcal{B}^{s t u d e n t}}(i^{1}\mid g^{2},d^{1})\approx0.34$ . In efect we have *explained away* the poor grade via the difculty of the class. 
**Explaining away is an instance of a general reasoning pattern called intercausal reasoning , where diferent causes of the same efect can interact. This type of reasoning is a very common pattern in human reasoning.** 
> 解释是因果关系间推理的一个实例，在因果关系间推理中，相同效果的不同导因会相互交互 （例如，在已知 Intelligence 高并且 Grade 低的情况下，就可以推理出 Difficulty 高的概率是较高的）
> 因果关系间推理也是人类推理的常见模式

For example, when we have fever and a sore throat, and are concerned about mononucleosis, we are greatly relieved to be told we have the ﬂu. Clearly, having the ﬂu does not prohibit us from having mononucleosis. Yet, having the ﬂu provides an alternative explanation of our symptoms, thereby reducing substantially the probability of mononucleosis. 

This intuition of providing an alternative explanation for the evidence can be made very precise. As shown in exercise 3.3, if the ﬂu deterministic ally causes the symptoms, the probability of mononucleosis goes down to its prior probability (the one prior to the observations of any symptoms). On the other hand, if the ﬂu might occur without causing these symptoms, the probability of mononucleosis goes down, but it still remains somewhat higher than its base level. Explaining away, however, is not the only form of intercausal reasoning. The inﬂuence can go in any direction. Consider, for example, a situation where someone is found dead and may have been murdered. The probabilities that a suspect has motive and opportunity both go up. If we now discover that the suspect has motive, the probability that he has opportunity goes up. (See exercise 3.4.) 

It is important to emphasize that, although our explanations used intuitive concepts such as cause and evidence, there is nothing mysterious about the probability computations we performed. They can be replicated simply by generating the joint distribution, as deﬁned in equation (3.9), and computing the probabilities of the various events directly from that. 
### 3.2.2 Basic Independencies in Bayesian Networks 
As we discussed, a Bayesian network graph $\mathcal{G}$ can be viewed in two ways. In the previous section, we showed, by example, how it can be used as a skeleton data structure to which we can attach local probability models that together deﬁne a joint distribution. 
In this section, we provide a formal semantics for a Bayesian network, starting from the perspective that the graph encodes a set of conditional independence assumptions. We begin by understanding, intuitively, the basic conditional independence assumptions that we want a directed graph to encode. We then formalize these desired assumptions in a deﬁnition. 
> 上一个部分我们展示了如何表现出贝叶斯网络架构中的局部条件概率分布，并结合它们定义了联合分布
> 本节介绍贝叶斯网络的形式语义
#### 3.2.2.1 Independencies in the Student Example 
In the Student example, we used the intuition that edges represent direct dependence. For example, we made intuitive statements such as “the professor’s recommendation letter depends only on the student’s grade in the class”; this statement was encoded in the graph by the fact that there are no direct edges into the $L$ node except from $G$ . This intuition, that “a node depends directly only on its parents,” lies at the heart of the semantics of Bayesian networks. 
> 我们在贝叶斯网络中使用边表示变量之间的独立性
> 当 $G$ 节点仅被 $L$ 节点出来的一条边指向时，表示该变量仅仅直接依赖于 $L$ 变量，这就是贝叶斯网络的语义的核心

We give formal semantics to this assertion using conditional independence statements. For example, the previous assertion can be stated formally as the assumption that $L$ is conditionally independent of all other nodes in the network given its parent $G$ : 

$$
(L\perp I,D,S\mid G).\tag{3.10}
$$

In other words, once we know the student’s grade, our beliefs about the quality of his recommendation letter are not inﬂuenced by information about any other variable. 
> 形式化地，我们将上一个声明表示为一个假设：在网络中给定它的父节点 $G$，$L$ 与所有其他的节点条件独立，也就是其他的任意节点都不会给出关于这个节点更多的信息

Similarly, to formalize our intuition that the student’s SAT score depends only on his intelligence, we can say that $S$ is conditionally independent of all other nodes in the network given its parent $I$ : 

$$
(S\perp D,G,L\mid I).\tag{3.11}
$$ 
Now, let us consider the $G$ node. Following the pattern blindly, we may be tempted to assert that $G$ is conditionally independent of all other variables in the network given its parents. However, this assumption is false both at an intuitive level and for the speciﬁc example distribution we used earlier. Assume, for example, that we condition on $i^{1},d^{1}$ ; that is, we have a smart student in a difcult class. In this setting, is $G$ independent of $L?$ Clearly, the answer is no: if we observe $l^{1}$ (the student got a strong letter), then our probability in $g^{1}$ (the student received an A in the course) should go up; that is, we would expect 

$$
P(g^{1}\mid i^{1},d^{1},l^{1})>P(g^{1}\mid i^{1},d^{1}).
$$ 
Indeed, if we examine our distribution, the latter probability is 0.5 (as speciﬁed in the CPD), whereas the former is a much higher 0.712 . 

Thus, we see that we do not expect a node to be conditionally independent of all other nodes given its parents. In particular, even given its parents, it can still depend on its descendants. 
> 但给定一个节点的父节点，该节点实际并不会条件独立于网络中的所有节点，它仍然会依赖于自己的子节点，

Can it depend on other nodes? For example, do we expect $G$ to depend on $S$ given $I$ and $D?$ Intuitively, the answer is no. Once we know, say, that the student has high intelligence, his SAT score gives us no additional information that is relevant toward predicting his grade. Thus, we would want the property that: 

$$
(G\perp S\mid I,D).\tag{3.12}
$$ 
It remains only to consider the variables $I$ and $D$ , which have no parents in the graph. 

Thus, in our search for independencies given a node’s parents, we are now looking for marginal independencies. As the preceding discussion shows, in our distribution $P_{\mathcal{B}^{s t u d e n t}}$ , $I$ is not independent of its descendants $G,\,L,$ or $S$ . Indeed, the only nondescendant of I is D . Indeed, we assumed implicitly that Intelligence and Difculty are independent. Thus, we expect that: 

$$
(I\perp D).\tag{3.13}
$$ 
This analysis might seem somewhat surprising in light of our earlier examples, where learning something about the course difculty drastically changed our beliefs about the student’s intelligence. In that situation, however, we were reasoning in the presence of information about the student’s grade. In other words, we were demonstrating the dependence of $I$ and $D$ given $G$ . This phenomenon is a very important one, and we will return to it. 
> 我们实际上假设了 $I$ 边际独立于 $D$，但回忆到在之前我们实际上观察到了 $I, D$ 在给定 $G$ 的情况下是不独立的，二者并不矛盾

For the va able $D$ , both $I$ and $S$ are nondescendants. Recall that, if $(I\perp D)$ then $(D\perp I)$ . The variable S increases our beliefs in the student’s intelligence, but knowing that the student is smart (or not) does not inﬂuence our beliefs in the difculty of the course. Thus, we have that 

$$
(D\perp I,S).\tag{3.14}
$$ 
We can see a pattern emerging. Our intuition tells us that the parents of a variable “shield” it from probabilistic inﬂuence that is causal in nature. In other words, once I know the value of the parents, no information relating directly or indirectly to its parents or other ancestors can inﬂuence my beliefs about it. However, information about its descendants can change my beliefs about it, via an evidential reasoning process. 
> 对于一个变量，一旦我们知道了其父变量的值，就没有与父变量或其他祖先直接相关或间接相关的信息可以影响我们对于该变量的概念
> 但关于该变量的子变量的信息则有影响（通过证据推理过程）
#### 3.2.2.2 Bayesian Network Semantics 
We are now ready to provide the formal deﬁnition of the semantics of a Bayesian network structure. We would like the formal deﬁnition to match the intuitions developed in our example. 

**Deﬁnition 3.1**
A Bayesian network structure G is a directed acyclic graph whose nodes represent random variables Bayesian network structure X1; : : : ; Xn. Let PaG Xi denote the parents of Xi in G, and NonDescendantsXi denote the variables in the graph that are not descendants of Xi. Then G encodes the following set of conditional local independence assumptions, called the local independencies, and denoted by I‘(G):

$$
(X_{i}\perp\mathrm{NonDS}_{X_{i}}\mid\mathrm{Pa}_{X_{i}}^{\mathcal{G}}).
$$ 
In other words, the local independencies state that each node $X_{i}$ is conditionally independent of its nondescendants given its parents. 
> 定义：贝叶斯网络是一个有向无环图，其中节点表示随机变量 $X_1,\dots, X_n$
> 贝叶斯网络编码了条件独立性假设，称为局部独立性，即对于网络中的每个变量 $X_i$，$X_i$ 在给定它的父节点的条件下和不是它的子孙节点的其他节点条件独立
> 我们记网络 $G$ 编码的条件独立性假设为 $\mathcal I_{\mathscr l}(\mathcal G)$

Returning to the Student network $G_{s t u d e n t},$ the local Markov independencies are precisely the ones dictated by our intuition, and speciﬁed in equation (3.10) – equation (3.14). 

Box 3. B — Case Study: The Genetics Example. One of the very earliest uses of a Bayesian net- work model (long before the general framework was deﬁned) is in the area of genetic pedigrees. In this setting, the local independencies are particularly intuitive. In this application, we want to model the transmission of a certain property, say blood type, from parent to child. The blood type of a person is an observable quantity that depends on her genetic makeup. Such properties are called phenotypes . The genetic makeup of a person is called genotype . 

To model this scenario properly, we need to introduce some background on genetics. The human genetic material consists of 22 pairs of autosomal chromosomes and a pair of the sex chromosomes (X and Y). Each chromosome contains a set of genetic material, consisting (among other things) of genes that determine a person’s properties. A region of the chromosome that is of interest is called $a$ locus ; a locus can have several variants, called alleles . 

For concreteness, we focus on autosomal chromosome pairs. In each autosomal pair, one chro- mosome is the paternal chromosome, inherited from the father, and the other is the maternal chromosome, inherited from the mother. For genes in an autosomal pair, a person has two copies of the gene, one on each copy of the chromosome. Thus, one of the gene’s alleles is inherited from the person’s mother, and the other from the person’s father. For example, the region containing the gene that encodes a person’s blood type is a locus. This gene comes in three variants, or alleles: $A$ , $B$ , and $O$ . Thus, a person’s genotype is denoted by an ordered pair, such as $\langle A,B\rangle$ ; with thr choices for each entry in the pair, there are 9 possible genotypes. The blood type phenotype is a function of both copies of the gene. For example, if the person has an $A$ allele and an $O$ allele, her observed blood type is “A.” If she has two $O$ alleles, her observed blood type is “O.” 

To represent this domain, we would have, for each person, two variables: one representing the person’s genotype, and the other her phenotype. We use the name $G(p)$ to represent person p ’s genotype, and $B(p)$ to represent her blood type. 

In this example, the independence assumptions arise immediately from the biology. Since the blood type is a function of the genotype, once we know the genotype of a person, additional evidence about other members of the family will not provide new information about the blood type. Similarly, the process of genetic inheritance implies independence assumption. Once we know the genotype of both parents, we know what each of them can pass on to the ofspring. Thus, learning new information about ancestors (or nondescendants) does not provide new information about the genotype of the ofspring. These are precisely the local independencies in the resulting network structure, shown for a simple family tree in ﬁgure 3.B.1. The intuition here is clear; for example, Bart’s blood type is correlated with that of his aunt Selma, but once we know Homer’s and Marge’s genotype, the two become independent. 

To deﬁne the probabilistic model fully, we need to specify the CPDs. There are three types of CPDs in this model: 

• The penetrance model $P(B(c)\mid G(c))$ , which describes the probability of diferent variants of a particular phenotype (say diferent blood types) given the person’s genotype. In the case of the blood type, this CPD is a deterministic function, but in other cases, the dependence can be more complex. • The transmission model $P(G(c)\mid G(p),G(m))$ , where $c$ is a person and $p,m$ her father and mother, respectively. Each parent is equally likely to transmit either of his or her two alleles to the child. • Genotype priors $P(G(c))$ , used when person c has no parents in the pedigree. These are the general genotype frequencies within the population. 

Our discussion of blood type is simpliﬁed for several reasons. First, some phenotypes, such as late-onset diseases, are not a deterministic function of the genotype. Rather, an individual with a particular genotype might be more likely to have the disease than an individual with other genotypes. Second, the genetic makeup of an individual is deﬁned by many genes. Some phenotypes might depend on multiple genes. In other settings, we might be interested in multiple phenotypes, which (naturally) implies a dependence on several genes. Finally, as we now discuss, the inheritance patterns of diferent genes are not independent of each other. 

Recall that each of the person’s autosomal chromosomes is inherited from one of her parents. However, each of the parents also has two copies of each autosomal chromosome. These two copies, within each parent, recombine to produce the chromosome that is transmitted to the child. Thus, the maternal chromosome inherited by Bart is a combination of the chromosomes inherited by his mother Marge from her mother Jackie and her father Clancy. The recombination process is stochastic, but only a handful recombination events take place within a chromosome in a single generation. Thus, if Bart inherited the allele for some locus from the chromosome his mother inherited from her mother Jackie, he is also much more likely to inherit Jackie’s copy for a nearby locus. Thus, to construct an appropriate model for multilocus inheritance, we must take into consideration the probability of a recombination taking place between pairs of adjacent loci. 

We can facilitate this modeling by introducing selector variables that capture the inheritance pattern along the chromosome. In particular, for each locus $\ell$ and each child $c_{z}$ , we have a variable $S(\ell,c,m)$ that takes the value 1 if the locus $\ell$ in c ’s maternal chromosome was inherited from c ’s maternal grandmother, and 2 if this locus was inherited from c ’s maternal grandfather. We have a similar selector variable $S(\ell,c,p)$ for c ’s paternal chromosome. We can now model correlations induced by low recombination frequency by correlating the variables $S(\ell,c,m)$ and $S(\ell^{\prime},c,m)$ for adjacent loci $\ell,\ell^{\prime}$ . 

This type of model has been used extensively for many applications. In genetic counseling and prediction, one takes a phenotype with known loci and a set of observed phenotype and genotype data for some individuals in the pedigree to infer the genotype and phenotype for another person in the pedigree (say, a planned child). The genetic data can consist of direct measurements of the relevant disease loci (for some individuals) or measurements of nearby loci, which are correlated with the disease loci. 

In linkage analysis, the task is a harder one: identifying the location of disease genes from pedigree data using some number of pedigrees where a large fraction of the individuals exhibit a disease phenotype. Here, the available data includes phenotype information for many individuals in the pedigree, as well as genotype information for loci whose location in the chromosome is known. Using the inheritance model, the researchers can evaluate the likelihood of these observations under diferent hypotheses about the location of the disease gene relative to the known loci. By repeated calculation of the probabilities in the network for diferent hypotheses, researchers can pinpoint the area that is “linked” to the disease. This much smaller region can then be used as the starting point for more detailed examination of genes in that area. This process is crucial, for it can allow the researchers to focus on a small area (for example, $1/10,000$ of the genome). 

As we will see in later chapters, the ability to describe the genetic inheritance process using a sparse Bayesian network provides us the capability to use sophisticated inference algorithms that allow us to reason about large pedigrees and multiple loci. It also allows us to use algorithms for model learning to obtain a deeper understanding of the genetic inheritance process, such as recombination rates in diferent regions or penetrance probabilities for diferent diseases. 
### 3.2.3 Graphs and Distributions 
The formal semantics of a Bayesian network graph is as a set of independence assertions. On the other hand, our Student BN was a graph annotated with CPDs, which deﬁned a joint distribution via the chain rule for Bayesian networks. In this section, we show that these two deﬁnitions are, in fact, equivalent. 
A distribution $P$ satisﬁes the local independencies associated with a graph $\mathcal{G}$ if and only if $P$ is representable as a set of CPDs associated with the graph $\mathcal{G}$ . We begin by formalizing the basic concepts. 
> 贝叶斯网络图的形式语义就是一系列独立假设
> 一个分布 $P$  满足和图 $G$ 相关的局部独立性当且仅当 $P$ 可以用和 $G$ 相关的一系列条件概率分布表示
#### 3.2.3.1 I-Maps 
We ﬁrst deﬁne the set of independencies associated with a distribution $P$ . 

**Deﬁnition 3.2**
Let $P$ be a distribution over $\mathcal{X}$ . e deﬁne $\mathcal{Z}(P)$ to be the set of independence assertions of the form $(X\perp Y\mid Z)$ that hold in P . 
> 定义：
> 令 $\mathcal I (P)$ 表示 $P$ 中满足的形式为 $(X\perp Y \mid Z)$ 的独立声明

We can now rewrite the statement that $^{a}P$ satisﬁes the local independencies associated with ${\mathcal{G}}"$ simply as ${\mathcal{T}}_{\ell}({\mathcal{G}})\subseteq{\mathcal{Z}}(P)$ . In this case, we say that $\mathcal{G}$ is an $I^{,}$ -map (independency map) for $P$ . However, it is useful to deﬁne this concept more broadly, since diferent variants of it will be used throughout the book. 
> 此时，$P$ 满足和 $G$ 相关的局部独立性可以简单记为 $\mathcal I_{\mathscr l}(G) \subseteq \mathcal I (P)$
> 我们称此时 $\mathcal G$ 是 $P$ 的一个 I-map（独立性映射）

**Deﬁnition 3.3** I-map 
Let K be any graph object associated with a set of independencies I(K). We say that K is anI-map for a set of independencies I if I(K) ⊆ I|I-map|
> 定义：
> 令 $\mathcal K$ 是和独立性集合 $\mathcal I (\mathcal K)$ 相关的任意图对象，对于一个独立性集合 $\mathcal I$，如果 $\mathcal I (\mathcal K) \subseteq \mathcal I$，则我们称 $\mathcal K$ 是 $\mathcal I$ 的一个 I-map 

We now say that $\mathcal{G}$ is an I-map for $P$ if $\mathcal{G}$ is an I-map fo $\mathcal{Z}(P)$ . 
> 因此，如果 $\mathcal G$ 是 $\mathcal I(P)$ 的一个 I-map ($\mathcal G$ 编码的独立性集合是 $P$ 编码的独立性集合的子集)，则 $\mathcal G$ 是 $P$ 的一个 I-map

As we can see from the direction of the inclusion, for G o be an I-map of $P$ , it is necessary that $\mathcal{G}$ does not m lead us regar ng independencies in P : any independence that $\mathcal{G}$ asserts must also hold in P . Conversely, P may have additional independencies that are not reﬂected in $\mathcal{G}$ . 
> 显然，如果 $\mathcal G$ 是 $P$ 的 I-map，则任意 $\mathcal G$ 中成立的独立性必须在 $P$ 中存在，另外 $P$ 可以有没有在 $\mathcal G$ 中反应的额外的独立性

Let us illustrate the concept of an I-map on a very simple example. 

Example 3.1
Consider a joint probability space over two independent random variables $X$ and $Y$ . There are three possibl over t o nodes: $\mathcal{G}_{\varnothing}$ , whi isconnected pair $\begin{array}{r l}{X}&{{}\;Y;\mathcal{G}_{X\rightarrow Y}}\end{array}$ G , which → has the edge $X\rightarrow Y$ → ; and G ${\mathcal{G}}_{Y\to X}$ → , which contains $Y\rightarrow X$ → . The graph $\mathcal{G}_{\varnothing}$ encodes the assumption that $(X\perp Y)$ . The latter two encode no independence assumptions. Consider the following two distributions: 

In the example on the left, $X$ and $Y$ are independent in $P$ ; for example, $P(x^{1})=0.48+0.12=$ 0 . 6 , $P(y^{1})\,=\,0.8,$ , and $P(x^{1},y^{1})\,=\,0.48\,=\,0.6\cdot0.8$ Thus, $(X\bot Y)\in{\mathcal{Z}}(P)$ , and we have that $\mathcal{G}_{\varnothing}$ an I-map of P . In fact, all three graphs are I-maps of P : ${\mathcal{T}}_{\ell}({\mathcal{G}}_{X\to Y})$ is empty, so that $P$ all the independenci s in it (si ilarly for ${\mathcal{G}}_{Y\rightarrow X.}$ ). In th example n the right, $(X\bot Y)\not\in{\mathcal{Z}}(P)$ ⊥ ̸∈I , so that $\mathcal{G}_{\varnothing}$ is not an I-map of P . Both other graphs are I-maps of P . 
#### 3.2.3.2 I-Map to Factorization 
A BN structure $\mathcal{G}$ encodes a set of conditional independence assumptions; every distribution for which G is an I-map must satisfy these assumptions. This property is the key to allowing the compact factorized representation that we saw in the Student example in section 3.2.1. The basic principle is the same as the one we used in the naive Bayes decomposition in section 3.1.3. 
> 一个贝叶斯网络结构 $\mathcal G$ 编码了一系列条件独立性假设，被该网络 I-map 的任意分布都必须满足这些条件独立性假设

Consider any distribution $P$ for which our Student BN $G_{s t u d e n t}$ is an I-map. We will de- compose the joint distribution and show that it factorizes into local probabilistic models, as in section 3.2.1. Consider the joint distribution $P(I,D,G,L,S)$ ; from the chain rule for probabil- ities (equation (2.5)), we can decompose this joint distribution in the following way: 

$$
P(I,D,G,L,S)=P(I)P(D\mid I)P(G\mid I,D)P(L\mid I,D,G)P(S\mid I,D,G,L).\tag{3.15}
$$ 
This transformation relies on no assumptions; it holds for any joint distribution $P$ . However, it is also not very helpful, since the conditional probabilities in the factorization on the right-hand side are neither natural nor compact. For example, the last factor requires the speciﬁcation of 24 conditional probabilities: $P(s^{1}\mid i,d,g,l)$ for every assignment of values $i,d,g,l$ . 
> 我们先对联合分布进行链式分解，链式分解不依赖于任何假设

This form, however, allows us to apply the conditional independence assumptions induced from the BN. Let us assume that $G_{s t u d e n t}$ is an I-map for our distribution $P$ . In particular, from equation (3.13), we have that $(D\perp I)\in{\mathcal{Z}}(P)$ . From that, we can conclude that $P(D\mid I)=$ $P(D)$ , allowing us to simplify the second factor on the right-hand side. Similarly, we know from equation (3.10) that $(L\ \bot\ I,D\ |\ G)\in{\mathcal{Z}}(P)$ . Hence, $P(L\mid I,D,G)=P(L\mid G)$ , allowing us to simplify the third term. Using equation (3.11) in a similar way, we obtain that 

$$
P(I,D,G,L,S)=P(I)P(D)P(G\mid I,D)P(L\mid G)P(S\mid I).\tag{3.16}
$$ 
This factorization is precisely the one we used in section 3.2.1. 
> 但链式分解允许我们使用条件独立性假设，得到更紧凑的表示

This result tells us that any entry in the joint distribution can be computed as a product of factors, one for each variable. Each factor represents a conditional probability of the variable given its parents in the network. This factorization applies to any distribution $P$ for which $G_{s t u d e n t}$ is an I-map. 
> 注意以上对于联合分布的分解对于任意被 $\mathcal G$ I-map 的分布都适用

We now state and prove this fundamental result more formally. 

**Deﬁnition 3.4** factorization
Let $\mathcal G$ be a BN graph over the variables $X_1,\dots,X_n$. We say that a distribution P over the samespace factorizes according to G if P can be expressed as a product factorization
> 定义：
> 令 $\mathcal G$ 是在变量 $X_1,\dots, X_n$ 上的贝叶斯图，如果在同一空间中的分布 $P$ 可以被表示成以下乘积，则称它根据 $\mathcal G$ 分解

$$
P(X_1,\dots,X_n) = \prod_{i=1}^n P(X_i\mid \text{Pa}_{X_i}^{\mathcal G})\tag{2.17}
$$

This equation is called the chain rule for Bayesian networks. The individual factors $P (X_i \mid \text{Pa}_{X_i}^{\mathcal G}$) Bayesian networks are called conditional probability distributions (CPDs) or local probabilistic models. chain rule for Bayesian networks 
> 该式被称为贝叶斯网络的链式法则，其中的独立因子 $P (X_i \mid \text{Pa}_{X_i}^{\mathcal G})$ 被称为条件概率分布或者局部概率模型

**Deﬁnition 3.5** Bayesian network 
Bayesian network is a pair B = (G; P ) where P factorizes over G, and where P is specified as a set of CPDs associated with G’s nodes. The distribution P is often annotated PB.
> 定义：
> 一个贝叶斯网络就是一个元组 $\mathcal B = (\mathcal G, P)$，其中 $P$ 按照 $\mathcal G$ 分解，并可以用和 $\mathcal G$ 相关的节点的一系列条件概率分布表示，$P$ 也可以记为 $P_{\mathcal B}$

We can now prove that the phenomenon we observed for Gstudent holds more generally

**Theorem 3.1** 
Let $\mathcal{G}$ be a BN str ture o er a set o rando variables $\mathcal{X}$ , and let $P$ e a joint distribution over the same space. If is an I-map for P , then P factorizes according to . 
> 定理：
> 令 $\mathcal G$ 是一个在随机变量集合 $\mathcal X$ 上的贝叶斯网络，令 $P$ 是相同空间上的一个联合分布，如果 $\mathcal G$ 是 $P$ 的一个 I-map， 则 $P$ 根据 $\mathcal G$ 分解

Proof 
Assume, without loss of generality, that $X_{1},\ldots,X_{n}$ is a topological ordering of the variables in $\mathcal{X}$ relative to $\mathcal{G}$ (see deﬁnition 2.19). As in our example, we ﬁrst use the chain rule for probabilities: 

$$
P(X_{1},\dots,X_{n})=\prod_{i=1}^{n}P(X_{i}\mid X_{1},\dots,X_{i-1}).
$$ 
No sider one of the fact $P_{\cdot}(X_{i}\mid X_{1},.\,.\,,X_{i-1})$ . As $\mathcal{G}$ is an map for $P$ , we have tha $\cdot X_{i}\perp$ escendants $X_{i}$ | $|\operatorname{Pa}_{X_{i}}^{\mathcal{G}}\}\in\mathcal{Z}(P)$ I . By assumption, all of $X_{i}$ ’s parents are in the set $X_{1},.\ldots,X_{i-1}$ . Furthermore, none of $X_{i}$ ’s descendants can possibly be in the set. Hence,

$$
\left\{X_{1},.\,.\,.\,,X_{i-1}\right\}=\mathrm{Pa}_{X_{i}}\cup Z
$$ 
where $Z\subseteq{\mathrm{NonDS}}_{X_{i}}$ . From the local independencies for $X_{i}$ and from the decom- position property (equation (2.8)) it follows that $(X_{i}\perp Z\mid\mathrm{Pa}_{X_{i}})$ . Hence, we have that 

$$
P(X_{i}\mid X_{1},.\,.\,,X_{i-1})=P(X_{i}\mid\mathrm{Pa}_{X_{i}}).
$$ 
Applying this transformation to all of the factors in the chain rule decomposition, the result follows.
> 证明：
> 先利用拓扑排序，对 $P$ 进行链式分解
> 因为 $\mathcal G$ 是 $P$ 的 I-map，则分解项 $P (X_i \mid X_1,\dots, X_{i-1})$ 中，除了 $X_i$ 的父节点都可以排除（它们条件独立于 $X_i$），得到 $P (X_i \mid \text{Pa}_{X_i})$
> 因此 $P$ 可以表示为式 2.17 的形式，也就是多个局部分布的乘积

Thus, the condition independence assumptions implied by a BN structure $\mathcal{G}$ allow us to factorize a distribution P for which $\mathcal{G}$ is an I-map into small CPDs. Note that the proof is con- structive, providing a precise algorithm for constructing the factorization given the distribution $P$ and the graph $\mathcal{G}$ . 
> 故只要 $\mathcal G$ 是 $P$ 的 I-map，$P$ 就可以按照 $\mathcal G$ 分解为多个条件概率分布的乘积，以上的证明也提供了如何构建该分解的步骤

The resulting factorized representation can be substantially ，more compact, particularly for sparse structures. 

Example 3.2 
In our Student example, the number of independent parameters is ﬁfteen: we have two binomial distributions $P(I)$ and $P(D)$ , with one independent parameter each; we have four multinomial distributions over $G$ — one for each assignment of values to $I$ and $D$ — each with two independent parameters; we have three binomial distributions over $L$ , each with one independent parameter; and similarly two binomial distributions over $S$ , each with an independent parameter. The speciﬁcation of the full joint distribution would require $48-1=47$ independent parameters. 

More generally, in a distribution over $n$ binary random variables, the speciﬁcation of the joint distribut n requires $2^{n}-1$ independen parameters. If the distribution factorizes according to a graph G where each has at most k parents, the total number of independent parameters required is less than n $n\cdot2^{k}$ · (see exercise 3.6). 
In many applications, we can assume a certain locality of inﬂuence between variables: although each variable is generally correlated with many of the others, it often depends directly on only a small number of other variables. Thus, in many cases, $k$ will be very small, even though $n$ is large. As a consequence, the number of parameters in the Bayesian network representation is typically exponentially smaller than the number of parameters of a joint distribution. This property is one of the main beneﬁts of the Bayesian network representation. 
> 在许多应用中，我们都可以假设变量之间的影响存在某种局部性，虽然每个变量会和许多其他变量相关，但它经常仅仅直接依赖于少部分的变量
> 因此，贝叶斯网络表示会指数级地减少表示联合分布所需要的参数量
#### 3.2.3.3 Factorization to I-Map 
Theorem 3.1 shows one direction of the fundamental connection between the conditional in- dependencies encoded by the BN structure and the factorization of the distribution into local probability models: that the conditional independencies imply factorization. The converse also holds: factorization according to $\mathcal{G}$ implies the associated conditional independencies. 
> 定理3.1展示了贝叶斯网络编码的条件独立性 imply 了分解
> 反过来其实也成立，根据 $\mathcal G$ 的分解 imply 了相关的条件独立性

**Theorem 3.2** 
Let $\mathcal{G}$ be a BN ructure over a set of ran m var bles $\mathcal{X}$ and let $P$ be a joint distribution over the same space. If $P$ factorizes according to G , then G is an I-map for P . 
> 定理：
> 令 $\mathcal G$ 是随机变量集合 $\mathcal X$ 上的贝叶斯网络，令 $P$ 是相同空间的一个联合分布，则如果 $P$ 根据 $\mathcal G$ 分解，则 $\mathcal G$ 是 $P$ 的一个 I-map

We illustrate this theorem by example, leaving the proof as an exercise (exercise 3.9). Let $P$ be so e distribution that factorizes according to $G_{s t u d e n t}$ . We need to show that $\mathcal{T}_{\ell}(G_{s t u d e n t})$ holds in P . Consider the indep dence assumption for the random variable $S-(S\perp D,G,L\mid I)$ . To prove that it holds for P , we need to show that 

$$
P(S\mid I,D,G,L)=P(S\mid I).
$$ 
By deﬁnition, 

$$
P(S\mid I,D,G,L)=\frac{P(S,I,D,G,L)}{P(I,D,G,L)}.
$$ 
By the chain rule for BNs equation (3.16), the numerator is equal to $P(I)P(D)P(G\mid I,D)P(L\mid$ $G)P(S\mid I)$ . By the process of marginalizing over a joint distribution, we have that the denominator is: 

$$
\begin{array}{r c l}{{P(I,D,G,L)}}&{{=}}&{{\displaystyle\sum_{S}P(I,D,G,L,S)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{S}P(I)P(D)P(G\mid I,D)P(L\mid G)P(S\mid I)}}\\ {{}}&{{=}}&{{\displaystyle P(I)P(D)P(G\mid I,D)P(L\mid G)\sum_{S}P(S\mid I)}}\\ {{}}&{{=}}&{{P(I)P(D)P(G\mid I,D)P(L\mid G),}}\end{array}
$$ 
where the last step is a consequence of the fact that $P(S\mid I)$ is a distribution over values of $S$ , and therefore it sums to 1. We therefore have that 

$$
\begin{array}{r c l}{P(S\mid I,D,G,L)}&{=}&{\displaystyle\frac{P(S,I,D,G,L)}{P(I,D,G,L)}}\\ &{=}&{\displaystyle\frac{P(I)P(D)P(G\mid I,D)P(L\mid G)P(S\mid I)}{P(I)P(D)P(G\mid I,D)P(L\mid G)}}\\ &{=}&{P(S\mid I).}\end{array}
$$ 
Box 3. C — **Skill: Knowledge Engineering.** 
Our discussion of Bayesian network construction fo- cuses on the process of going from a given distribution to a Bayesian network. Real life is not like that. We have a vague model of the world, and we need to crystallize it into a network structure and parameters. This task breaks down into several components, each of which can be quite subtle. Unfortunately, modeling mistakes can have signiﬁcant consequences for the quality of the answers obtained from the network, or to the cost of using the network in practice. 
> 网络的错误建模往往会导致结果错误严重

**Picking variables** When we model a domain, there are many possible ways to describe the relevant entities and their attributes. Choosing which random variables to use in the model is often one of the hardest tasks, and this decision has implications throughout the model. A common problem is using ill-deﬁned variables. For example, deciding to include the variable Fever to describe a patient in a medical domain seems fairly innocuous. However, does this random variable relate to the internal temperature of the patient? To the thermometer reading (if one is taken by the medical staf)? Does it refer to the temperature of the patient at a speciﬁc moment (for example, the time of admission to the hospital) or to occurrence of a fever over a prolonged period? Clearly, each of these might be a reasonable attribute to model, but the interaction of Fever with other variables depends on the speciﬁc interpretation we use. 

As this example shows, we must be precise in deﬁning the variables in the model. The clarity test is a good way of evaluating whether they are sufciently well deﬁned. Assume that we are a million years after the events described in the domain; can an omniscient being, one who saw everything, determine the value of the variable? For example, consider a Weather variable with a value sunny. To be absolutely precise, we must deﬁne where we check the weather, at what time, and what fraction of the sky must be clear in order for it to be sunny. For a variable such as Heart-attack, we must specify how large the heart attack has to be, during what period of time it has to happen, and so on. By contrast, a variable such as Risk-of-heart-attack is meaningless, as even an omniscient being cannot evaluate whether a person had high risk or low risk, only whether the heart attack occurred or not. Introducing variables such as this confounds actual events and their probability. Note, however, that we can use a notion of “risk group,” as long as it is deﬁned in terms of clearly speciﬁed attributes such as age or lifestyle. 
> 检验模型中变量的定义：clarity test（清楚）

If we are not careful in our choice of variables, we will have a hard time making sure that evidence observed and conclusions made are coherent. 

Generally speaking, we want our model to contain variables that we can potentially observe or that we may want to query. However, sometimes we want to put in a hidden variable that is neither observed nor directly of interest. Why would we want to do that? Let us consider an example relating to a cholesterol test. Assume that, for the answers to be accurate, the subject has to have eaten nothing after 10:00 PM the previous evening. If the person eats (having no willpower), the results are consistently of. We do not really care about a Willpower variable, nor can we observe it. However, without it, all of the diferent cholesterol tests become correlated. To avoid graphs where all the tests are correlated, it is better to put in this additional hidden variable, rendering them conditionally independent given the true cholesterol level and the person’s willpower. 
> 一般来说，我们希望模型包含我们可以观察到或者我们想要查询的变量
> 但有时我们会包括一个我们观察不到，也不会查询的隐变量，这类隐变量的取值会对结果产生很大的影响

On the other hand, it is not necessary to add every variable that might be relevant. In our Student example, the student’s SAT score may be afected by whether he goes out for drinks on the night before the exam. Is this variable important to represent? The probabilities already account for the fact that he may achieve a poor score despite being intelligent. It might not be worthwhile to include this variable if it cannot be observed. 
> 另一方面，对于一个已有的变量，也不必要加入任何可能与它相关的变量作为隐变量，要考虑其重要性，该变量的影响可能已经在这个已有的变量的取值中表示了

It is also important to specify a reasonable domain of values for our variables. In particular, if our partition is not ﬁne enough, conditional independence assumptions may be false. For example, we might want to construct a model where we have a person’s cholesterol level, and two cholesterol tests that are conditionally independent given the person’s true cholesterol level. We might choose to deﬁne the value normal to correspond to levels up to 200, and high to levels above 200. But it may be the case that both tests are more likely to fail if the person’s cholesterol is marginal (200–240). In this case, the assumption of conditional independence given the value (high/normal) of the cholesterol test is false. It is only true if we add a marginal value. 
> 变量的合理取值范围同样重要，对于一个父变量，有时只有在合理的取值范围内，其自变量条件于它才会与其他变量有条件独立性，否则可能还是会与其他变量存在相关性

**Picking structure** As we saw, there are many structures that are consistent with the same set of independencies. One successful approach is to choose a structure that reﬂects the causal order and dependencies, so that causes are parents of the efect. Such structures tend to work well. Either because of some real locality of inﬂuence in the world, or because of the way people perceive the world, causal graphs tend to be sparser. It is important to stress that the causality is in the world, not in our inference process. For example, in an automobile insurance network, it is tempting to put Previous-accident as a parent of Good-driver, because that is how the insurance company thinks about the problem. This is not the causal order in the world, because being a bad driver causes previous (and future) accidents. In principle, there is nothing to prevent us from directing the edges in this way. However, a noncausal ordering often requires that we introduce many additional edges to account for induced dependencies (see section 3.4.1). 
> 常用的结构是反映了世界中的因果关系的结构
> 要注意我们指的因果关系是执因索果，而不是推理过程中的执果索引（不然就反过来了）

One common approach to constructing a structure is a backward construction process. We begin with a variable of interest, say Lung-Cancer. We then try to elicit a prior probability for that variable. If our expert responds that this probability is not determinable, because it depends on other factors, that is a good indication that these other factors should be added as parents for that variable (and as variables into the network). For example, we might conclude using this process that Lung-Cancer really should have Smoking as a parent, and (perhaps not as obvious) that Smoking should have Gender and Age as a parent. This approach, called extending the conversation , avoids probability estimates that result from an average over a heterogeneous population, and therefore leads to more precise probability estimates. 
> 构建一个结构的常用方法是反向构建过程
> 我们从一个感兴趣的变量开始，然后尝试得到该变量的先验概率，如果该变量的先验概率是不可决定的（因为它依赖于其他因素），则我们再将其他因素加入为该变量的父节点

When determining the structure, however, we must also keep in mind that approximations are inevitable. For many pairs of variables, we can construct a scenario where one depends on the other. For example, perhaps Difculty depends on Intelligence, because the professor is more likely to make a class difcult if intelligent students are registered. In general, **there are many weak inﬂuences that we might choose to model, but if we put in all of them, the network can become very complex.** Such networks are problematic from a representational perspective: they are hard to understand and hard to debug, and eliciting (or learning) parameters can get very difcult. Moreover, as reasoning in Bayesian networks depends strongly on their connectivity (see section 9.4), adding such edges can make the network too expensive to use. 
> 在决定结构时，我们需要知道近似是不可避免的
> 对于许多对的变量，我们可以构造出一个变量依赖于另一个变量的场景，一般情况下，我们可能会选择建模许多弱的依赖，但如果我们将它们都放在网络中，网络就会变得非常复杂
> 过于复杂的网络是有问题的，它们难以理解、debug 以及难以学习参数，同时边太多的网络的推理也会太昂贵

This ﬁnal consideration may lead us, in fact, to make approximations that we know to be wrong. For example, in networks for fault or medical diagnosis, the correct approach is usually to model each possible fault as a separate random variable, allowing for multiple failures. However, such networks might be too complex to perform efective inference in certain settings, and so we may sometimes resort to a single fault approximation , where we have a single random variable encoding the primary fault or disease. 
> 为了避免过于复杂的情况，我们可能需要进行近似，例如，仅仅建模主要的因素为随机变量

**Picking probabilities** One of the most challenging tasks in constructing a network manually is eliciting probabilities from people. This task is somewhat easier in the context of causal models, since the parameters tend to be natural and more interpretable. Nevertheless, people generally dislike committing to an exact estimate of probability. 

One approach is to elicit estimates qualitatively, using abstract terms such as “common,” “rare,” and “surprising,” and then assign these to numbers using a predeﬁned scale. This approach is fairly crude, and often can lead to misinterpretation. There are several approaches developed for assisting in eliciting probabilities from people. For example, one can visualize the probability of the event as an area (slice of a pie), or ask people how they would compare the probability in question to certain predeﬁned lotteries. Nevertheless, probability elicitation is a long, difcult process, and one whose outcomes are not always reliable: the elicitation method can often inﬂuence the results, and asking the same question using diferent phrasing can often lead to signiﬁcant diferences in the answer. For example, studies show that people’s estimates for an event such as “Death by disease” are signiﬁcantly lower than their estimates for this event when it is broken down into diferent possibilities such as “Death from cancer,” “Death from heart disease,” and so on. 
> 一种评估概率的方式是使用类似 “common”，“rare”，“suprising” 等词，然后使用预定义的范围为它们赋值概率，这个方法较粗略

How important is it that we get our probability estimates exactly right? In some cases, small errors have very little efect. For example, changing a conditional probability of 0.7 to 0.75 generally does not have a signiﬁcant efect. Other errors, however, can have a signiﬁcant efect: 
> 一些情况下，概率估计的些小偏差对于结果没有太大影响
> 但其他类别的概率误差则会有很大的影响

- **Zero probabilities** : A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that **one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely  but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors.** For example, in one of the early versions of the the Pathﬁnder system (box 3. D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible. As a general rule, very few things (except deﬁnitions) have probability zero, and we must be careful in assigning zeros. 
> 0概率：一个常见的错误就是给一个非常罕见的事件赋值零概率，但这导致的问题是我们不能以零概率的变量为条件，因此除非一个事件不可能，赋值零概率会导致不可恢复的错误

- **Orders of magnitude**: Small diferences in very low probability events can make a large diference to the network conclusions. Thus, a (conditional) probability of $10^{-4}$ is very diferent from $10^{-5}$ . 
> 数量级：对于非常小的概率，虽然数量级差异不会在取值上呈现出太大差异，但是会对网络的推理产生很大影响

- **Relative values:** The qualitative behavior of the conclusions reached by the network — the value that has the highest probability — is fairly sensitive to the relative sizes of $P(x\mid y)$ for diferent values $y$ of $\mathrm{Pa}_{X}$ . For example, it is important that the network encode correctly that the probability of having a high fever is greater when the patient has pneumonia than when he has the ﬂu.  sensitivity analysis  medical diagnosis expert system  Pathﬁnder 
> 相对值：由网络推理出的行为（概率值最高的事件）对于 $P (x\mid y)$ 中不同 $y \in \text{Pa}_x$ 的相对值是较为敏感的

A very useful tool for estimating network parameters is sensitivity analysis , which allows us to determine the extent to which a given probability parameter afects the outcome. This process allows us to evaluate whether it is important to get a particular CPD entry right. It also helps us ﬁgure out which CPD entries are responsible for an answer to some query that does not match our intuitions. 
> 评估网络参数的一个有用工具是敏感性分析，这允许我们决定给定的概率影响结果的程度

Box 3. D — Case Study: Medical Diagnosis Systems. One of the earliest applications of Bayesian networks was to the task of medical diagnosis . In the 1980s, a very active area of research was the construction of expert systems — computer-based systems that replace or assist an expert in per- forming a complex task. One such task that was tackled in several ways was medical diagnosis. This task, more than many others, required a treatment of uncertainty, due to the complex, nondeter- ministic relationships between ﬁndings and diseases. Thus, it formed the basis for experimentation with various formalisms for uncertain reasoning. 

The Pathﬁnder expert system was designed by Heckerman and colleagues (Heckerman and Nath- wani 1992a; Heckerman et al. 1992; Heckerman and Nathwani 1992b) to help a pathologist diagnose diseases in lymph nodes. Ultimately, the model contained more than sixty diferent diseases and around a hundred diferent features. It evolved through several versions, including some based on non probabilistic formalisms, and several that used variants of Bayesian networks. Its diagnostic ability was evaluated over real pathological cases and compared to the diagnoses of pathological experts. 

One of the ﬁrst models used was a simple naive Bayes model, which was compared to the models based on alternative uncertainty formalisms, and judged to be superior in its diagnostic ability. It therefore formed the basis for subsequent development of the system. 

The same evaluation pointed out important problems in the way in which parameters were elicited from the expert. First, it was shown that 10 percent of the cases were diagnosed incorrectly, because the correct disease was ruled out by a ﬁnding that was unlikely, but not impossible, to manifest in that disease. Second, in the original construction, the expert estimated the probabilities P ( Finding | Disease ) by ﬁxing a single disease and evaluating the probabilities of all its ﬁndings. 

It was found that the expert was more comfortable considering a single ﬁnding and evaluating its probability across all diseases. This approach allows the expert to compare the relative values of the same ﬁnding across multiple diseases, as described in box 3.C. 

With these two lessons in mind, another version of Pathﬁnder — Pathﬁnder III — was con- structed, still using the naive Bayes model. Finally, Pathﬁnder IV used a full Bayesian network, with a single disease hypothesis but with dependencies between the features. Pathﬁnder IV was con- structed using a similarity network (see box 5. B), signiﬁcantly reducing the number of parameters that must be elicited. Pathﬁnder IV, viewed as a Bayesian network, had a total of around 75,000 parameters, but the use of similarity networks allowed the model to be constructed with fewer than 14,000 distinct parameters. Overall, the structure of Pathﬁnder IV took about 35 hours to deﬁne, and the parameters 40 hours. 

A comprehensive evaluation of the performance of the two models revealed some important insights. First, the Bayesian network performed as well or better on most cases than the naive Bayes model. In most of the cases where the Bayesian network performed better, the use of richer dependency models was a contributing factor. As expected, these models were useful because they address the strong conditional independence assumptions of the naive Bayes model, as described in box 3.A. Somewhat more surprising, they also helped in allowing the expert to condition the probabilities on relevant factors other than the disease, using the process of extending the conversation described in box 3. C, leading to more accurate elicited probabilities. Finally, the use of similarity networks led to more accurate models, for the smaller number of elicited parameters reduced irrelevant ﬂuctuations in parameter values (due to expert inconsistency) that can lead to spurious dependencies. 

Overall, the Bayesian network model agreed with the predictions of an expert pathologist in 50 / 53 cases, as compared with 47 / 53 cases for the naive Bayes model, with signiﬁcant therapeutic implications. A later evaluation showed that the diagnostic accuracy of Pathﬁnder IV was at least as good as that of the expert used to design the system. When used with less expert pathologists, the system signiﬁcantly improved the diagnostic accuracy of the physicians alone. Moreover, the system showed greater ability to identify important ﬁndings and to integrate these ﬁndings into a correct diagnosis. 

Unfortunately, multiple reasons prevent the widespread adoption of Bayesian networks as an aid for medical diagnosis, including legal liability issues for misdiagnoses and incompatibility with the physicians’ workﬂow. However, several such systems have been ﬁelded, with signiﬁcant success. Moreover, similar technology is being used successfully in a variety of other diagnosis applications (see box 23. C). 
## 3.3 Independencies in Graphs 
Dependencies and independencies are key properties of a distribution and are crucial for under- standing its behavior. As we will see, independence properties are also important for answering queries: they can be exploited to reduce substantially the computation cost of inference. There- fore, it is important that our representations make these properties clearly visible both to a user and to algorithms that manipulate the BN data structure. 

As we discussed, a graph structure $\mathcal{G}$ encodes a c ain set of cond onal independence assumptions $\mathcal{T}_{\ell}(\mathcal{G})$ . Knowing only that a distribution P factorizes over G , we can conclude that it satisﬁes $\mathcal{T}_{\ell}(\mathcal{G})$ . An imm iate question is whether there are other independencies that we can “rea of” directly from G . That is, are there other independencies that hold for every distribution P that factorizes over ? 
> 我们已经讨论过，一个图结构 $\mathcal G$ 编码了一些条件独立假设 $\mathcal I (\mathcal G)$，我们现在探究对于可以根据 $\mathcal G$ 分解的分布 $P$，是否存在其他独立性可以从图中得到
### 3.3.1 D-separation 
Our aim in this section is to understand when we can guara tee that an independence $(\boldsymbol{\textbf{X}}\perp$ $Y\mid Z)$ holds in a distribution associated with a BN structure G . To understand when a property is guaranteed to hold, it helps to consider its converse: “Can we imagine a case where it does not?” Thus, we focus our discussion on analyzing when it is possible that $X$ can inﬂuence $Y$ given $Z$ . If we construct an example where this inﬂuence occurs, then the converse property $(X\ \bot\ Y\ |\ Z)$ cannot f the distributions t ctorize over $\mathcal{G}$ , and hence the independence property ( $(X\perp Y\mid Z)$ cannot follow from I $\mathcal{T}_{\ell}(\mathcal{G})$ G . 
> 我们本节的目标是理解对于一个和 $\mathcal G$ 相关的分布，我们什么时候可以保证独立性 $(X \perp Y \mid Z)$ 存在

We therefore begin with an intuitive case analysis: Here, we try to understand when an observation regarding a variable $X$ can possibly change our beliefs about $Y$ , in the presence of evidence about the variables $Z$ . Although this analysis will be purely intuitive, we will show later that our conclusions are actually provably correct. 
> 我们尝试理解在给定 $Z$ 的情况下，什么时候关于变量 $X$ 的观测会可能地改变我们关于 $Y$ 的信念

**Direct connection** We begin with the simple case, when $X$ and $Y$ are directly connected via an edge, say $X\rightarrow Y$ . For any net rk st cture $\mathcal{G}$ that contains the edge $X\rightarrow Y$ , it is possible to construct a distribution where X and Y are correlated regardless of any evidence about any of the other variables in the network. In other words, if $X$ and $Y$ are directly connected, we can always get examples where they inﬂuence each other, regardless of $Z$ . 
> 当 $X, Y$ 直接通过一条边连接，显然二者是直接相关的，对于任意包含 $X\rightarrow Y$ 的网络 $\mathcal G$，我们总是构造出一个 X Y 相互关联的分布，无论网络中的其他变量如何，因此无论 $Z$ 是否给定，二者都会相互影响

In particular, assume that $V a l(X)\,=\,V a l(Y)$ ; we can simply set $X\,=\,Y$ . That, by itself, however, is not enough; if (given the evidence $Z$ ) $X$ deterministic ally takes some particular value, say 0 , then $X$ and $Y$ both deterministic ally take that value, and are uncorrelated. We therefore set the network so that $X$ is (for example) uniformly distributed, regardless of the values of any of its parents. This construction sufces to induce a correlation between $X$ and $Y$ , regardless of the evidence. 

**Indirect connection** Now consider the more complicated case when $X$ and $Y$ are not directly connected, but there is a trail between them in the graph. We begin by considering the simplest such case: a three-node network, where $X$ and $Y$ are not directly connected, but where there is a trail between them via $Z$ . It turns out that this simple case is the key to understanding the whole notion of indirect interaction in Bayesian networks. 

There are four cases where $X$ and $Y$ are connected via $Z$ , as shown in ﬁgure 3.5. The ﬁrst two correspond to causal chains (in either direction), the third to a common cause, and the fourth to a common efect. We analyze each in turn. 
> 考虑 XY 没有直接连接，但是之间有一条迹，
> 例如 $X, Y$ 没有直接连接，而是通过 $Z$ 连接，这样的连接一共有4种方式

![[Probabilistic Graph Theory-Fig3.5.png]]

**Indirect causal effect** (ﬁgure 3.5a). To gain intuition, let us return to the Student example, where we had a causal trail $I\to G\to L$ . Let us begin with the case where $G$ is not observed. Intuitively, if we observe that the student is intelligent, we are more inclined to believe that he gets an A, and therefore that his recommendation letter is strong. In other words, the probability of these latter events is higher conditioned on the observation that the student is intelligent.  

In fact, we saw precisely this behavior in the distribution of ﬁgure 3.4. Thus, in this case, we believe that $X$ can inﬂuence $Y$ via $Z$ . 

Now assume that $Z$ is observed, that is, $Z\in Z$ . As we saw in our analysis of the Student example, if we observe the student’s grade, then (as we assumed) his intelligence no longer inﬂuences his letter. In t, the local indep denc s f this network tell us that $(L\perp I\mid G)$ . Thus, we conclude that X cannot inﬂuence $Y$ via Z if Z is observed. 
> 1. 间接的因果影响：在 $Z$ 还未被观察到的情况下，$X$ 通过 $Z$ 影响 $Y$，即 $X\rightarrow Z \rightarrow Y$，当 $Z$ 已经被观察到，$X$ 和 $Y$ 条件独立

**Indirect evidential efect** (ﬁgure 3.5b). Returning to the Student example, we have a chain $I\,\rightarrow\, G\,\rightarrow\, L$ . We have already seen that observing a strong recommendation letter for the student changes our beliefs in his intelligence. Conversely, once the grade is observed, the letter gives no additional information about the student’s intelligence. Thus, our analysis in the case $Y\rightarrow Z\rightarrow X$ here is identical to the causal case: $X$ can inﬂuence $Y$ via $Z$ , but only if $Z$ is not observed. The similarity is not surprising, as dependence is a symmetrical notion. Speciﬁcally, if $(X\perp Y)$ does not hold, then $(Y\perp X)$ does not hold either. 
> 2. 间接的证据影响，和第一类相对称，依赖是一个对称的概念，也就是说如果 $X\perp Y$ 不成立的话，$Y\perp X$ 也是不成立的
> 因此，在 $Z$ 没有被观察到的情况下， $Y$ 或通过 $Z$ 影响 $X$，即 $Y \rightarrow Z \rightarrow X$

**Common cause** (ﬁgure 3.5c). This case is one that we have analyzed extensively, both within the simple naive Bayes model of section 3.1.3 and within our Student example. Our example has the student’s intelligence $I$ as a parent of his grade $G$ and his SAT score $S$ . As we discussed, $S$ and $G$ are correlated in this model, in that observing (say) a high SAT score gives us information about a student’s intelligence and hence helps us predict his grade. However, once we observe $I$ , this correlation disappears, and $S$ gives us no additional information about $G$ . Once again, for this network, this conclusion follows from the local independence assumption for the node $G$ (or for $S$ ). Thus, our conclusion here is identical to the previous two cases: $X$ can inﬂuence $Y$ via $Z$ if and only if $Z$ is not observed. 
> 3. 共同成因：当且仅当 $Z$ 没有被观察到时，$X$ 可以通过 $Z$ 影响 $Y$，当 $Z$ 被观察到时，$X, Y$ 之间条件独立

**Common efect** (ﬁgure 3.5d). In all of the three previous cases, we have seen a common pattern: $X$ can inﬂuence $Y$ via $Z$ if and only if $Z$ is not observed. Therefore, we might expect that this pattern is universal, and will continue through this last case. Somewhat surprisingly, this is not the case. Let us return to the Student example and consider $I$ and $D$ , which are parents of $G$ . When $G$ is not observed, we have that $I$ and $D$ are independent. In fact, this conclusion follows (once again) from the local independencies from the network. Thus, in this case, inﬂuence cannot “ﬂow” along the trail $X\rightarrow Z\leftarrow Y$ if the intermediate node $Z$ is not observed. 

On the other hand, consider the behavior when $Z$ is observed. In our discussion of the Student example, we analyzed precisely this case, which we called intercausal reasoning; we showed, for example, that the probability that the student has high intelligence goes down dramatically when we observe that his grade is a C $\scriptstyle (G\;=\; g^{3})$ ), but then goes up when we observe that the class is a difcult one $D=d^{1}$ . Thus, in presence of the evidence $G=g^{3}$ , we have that $I$ and $D$ are correlated. 
> 4. 共同影响：当 $Z$ 没有被观测到时，$X$ 和 $Y$ 是相互独立的，如果 $Z$ 被观测到，则 $X, Y$ 是相关的
> 并且，即便 $Z$ 没有被直接观测到，而是 $Z$ 的子孙节点被观测到，则 $X, Y$ 也会是相关的

Let us consider a variant of this last case. Assume that we do not observe the student’s grade, but we do observe that he received a weak recommendation letter $(L\,=\, l^{0}$ ). Intuitively, the same phenomenon happens. The weak letter is an indicator that he received a low grade, and therefore it sufces to correlate $I$ and $D$ . 

When inﬂuence can ﬂow from $X$ to $Y$ via $Z$ , we say that the trail $X\rightleftharpoons Z\rightleftharpoons Y$ is active . The results of our analysis for active two-edge trails are summarized thus: 
> 当影响可以从 $X$ 流经 $Y$ 到 $Z$，我们说迹 $X\rightleftharpoons Z \rightleftharpoons Y$ 是活跃的，我们总结对于双边迹的分析结果如下

- Causal trail $X\rightarrow Z\rightarrow Y$ : active if and only if $Z$ is not observed.
- Evidential trail $X\leftarrow Z\leftarrow Y$ : active if and only if $Z$ is not observed.
- Common cause $X\leftarrow Z\rightarrow Y$ : active if and only if $Z$ is not observed.
- Common efect $X\rightarrow Z\leftarrow Y$ : active if and only if either $Z$ or one of $Z$ ’s descendants is observed. 
> 因果迹：$X\rightarrow Z \rightarrow Y$，当仅当 $Z$ 没有被观察到时活跃
> 证据迹：$X\leftarrow Z\leftarrow Y$，当仅当 $Z$ 没有被观察到时活跃
> 共同原因：$X\leftarrow Z \rightarrow Y$，当仅当 $Z$ 没有被观察到时活跃
> 共同影响：$X\rightarrow Z \leftarrow Y$，当仅当 $Z$ 或者 $Z$ 的其中一个子孙被观察到时活跃

A structure where $X\rightarrow Z\leftarrow Y$ (as in ﬁgure 3.5d) is also called a $\nu$ -structure . 
> 结构为 $X \rightarrow Z \leftarrow Y$ 的结构（共同影响）称为 v-结构

It is useful to view probabilistic inﬂuence as a ﬂow in the graph. Our analysis here tells us when inﬂuence from $X$ can “ﬂow” through $Z$ to afect our beliefs about $Y$ . 

**General Case** Now co der case of a longer trail $X_{1}\,\,\rightleftharpoons\,\,\cdot\cdot\,\,\rightleftharpoons\,\, X_{n}$ . Intuitively, for inﬂuence to “ﬂow” from $X_{1}$ to $X_{n}$ , it needs to ﬂow through every single node on the trail. In other words, $X_{1}$ can inﬂuence $X_{n}$ if every two-edge trail $X_{i-1}\rightleftharpoons X_{i}\rightleftharpoons X_{i+1}$ along the trail allows inﬂuence to ﬂow. We can summarize this intuition in the following deﬁnition: 
> 考虑一个长的迹 $X_{1}\,\,\rightleftharpoons\,\,\cdot\cdot\,\,\rightleftharpoons\,\, X_{n}$，如果影响需要从 $X_1$ 流到 $X_n$ ，则它需要流过迹中的每一个节点

**Deﬁnition 3.6** active trail
Let $\mathcal{G}$ be a BN str $X_{1}\,\rightleftharpoons\,.\,.\,\rightleftharpoons\, X_{n}$ a rail in $\mathcal{G}$ . Let $Z$ be a subset of observed variables . The trail $X_{1}\rightleftharpoons...=X_{n}$ is active given Z if 

-  W never we have a $\nu$ -structure $X_{i-1}\to X_{i}\gets X_{i+1}$ , then $X_{i}$ or one of its descendants are in Z ;
- no other node along the trail is in $Z$ . 

> 定义：
> 对于贝叶斯网络中的一个迹 $X_1 \rightleftharpoons \dots \rightleftharpoons X_n$，令 $Z$ 表示观测到的变量的一个子集，给定 $Z$ 时，迹 $X_1 \rightleftharpoons \dots \rightleftharpoons X_n$ 在以下情况下活跃：
> - 对于其中的 v-结构 $X_{i-1}\rightarrow X_i \leftarrow X_{i+1}$，满足 $X_i$ 或 $X_i$ 的一个子孙在 $Z$ 中
> - 迹中没有其他的节点在 $Z$ 中

Note that if $X_{1}$ or $X_{n}$ are in $Z$ the trail is not active. 
> 注意如果 $X_1$ 或 $X_n$ 在 $Z$ 中，则这个迹也是不活跃的

In our Stude ve that $D\to G\leftarrow I\to S$ is not an active trail for $Z=\emptyset$ , because the v-structure $D\rightarrow G\leftarrow I$ → ← not activated. That same trail is active when $Z=\{L\}$ , because observing the descendant of G activates he v-structure. O and, when $Z=\{L, I\}$ , the trail is not active, because observing I blocks the trail $G\gets I\to S$ . 

What about graphs where there is more than one trail between two nodes? Our ﬂow intuition continues to carry through: one node can inﬂuence another if there is any trail along which inﬂuence can ﬂow. Putting these intuitions together, we obtain the notion of $d$ -separation , which provides us with a notion of separation between nodes in a directed graph (hence the term d-separation, for directed separation): 
> 对于两个节点之间存在多个迹的情况，则只要其中任意一条迹允许影响流过，则两个节点就是相关的

**Deﬁnition 3.7** 
Let $X, Y,$ $Z$ be three sets of nodes in $\mathcal{G}$ . We say that $X$ and $Y$ $\mathrm{d}$ para n $Z$ , d oted $\operatorname{d-sep}_{\mathcal{G}}(X; Y\mid Z).$ , if there is no active trail between any node $X\in X$ ∈ and $Y\in Y$ ∈ given Z . We use $\mathcal{Z}(\mathcal{G})$ to denote the set of independencies that correspond to d-separation: 

$$
{\mathcal{I}}({\mathcal{G}})=\{(X\perp Y\mid Z)\;:\;{\mathrm{d-sep}}_{\mathcal{G}}(X; Y\mid Z)\}.
$$ 
> 定义：
> 令 $X, Y, Z$ 是 $\mathcal G$ 中的三个节点集，如果给定 $Z$ 时，对于任意节点 $x \in X$ 和 $y \in Y$ 之间都不存在活跃的迹，则我们称 $X, Y$ 在给定 $Z$ 时是 d-seperation 的，记作 $\text{d-sep}_{\mathcal G}(X; Y\mid Z)$
> 因为不存在活跃的迹，故节点之间就是条件独立的，因此 $\mathcal I (\mathcal G)$ 实际上就是将图 $\mathcal G$ 中所有的 d-seperation 的节点表示为 $\perp$

This set is also called the set of global Markov independencies . The similarity between the nota- tion $\mathcal{Z}(\mathcal{G})$ and our notation $\mathcal{Z}(P)$ is not coincidental: As we discuss later, the in pendencies in $\mathcal{Z}(\mathcal{G})$ are precisely those that are guaranteed to hold for every distribution over G . 
> 集合 $\mathcal I (\mathcal G)$ 也称为全局的 Markov 独立集合
### 3.3.2 Soundness and Completeness 
So far, our deﬁnition of d-separation has been based on our intuitions regarding ﬂow of inﬂuence, and on our one example. As yet, we have no guarantee that this analysis is “correct.” Perhaps there is a distribution over the BN where $X$ can inﬂuence $Y$ despite the fact that all trails between them are blocked. 

Hence, the ﬁrst property we want to ensure for $\mathrm{d}$ -separation as a method for determining independence is soundness : if we ﬁnd that two nodes $X$ and $Y$ are d-separated given some $Z$ , then we are guaranteed that they are, in fact, conditionally independent given $Z$ . 
> 我们首先需要保证 d-seperation 这个概念的可靠性：如果我们在给定 $Z$ 的情况下找到两个节点 $X, Y$ 时 d-seperation 的，则我们可以保证这两个节点在给定 $Z$ 的情况下是条件独立的

**Theroem 3.3**
If a distribution $P$  factorizes according to $\mathcal G$, then $\mathcal I (\mathcal G)\subseteq \mathcal I (\mathcal P)$
> 定理：
> 如果一个分布 $P$ 根据 $\mathcal G$ 分解，则 $\mathcal I (\mathcal G)\subseteq \mathcal I (\mathcal P)$

In other words, any independence reported by $\mathrm{d}$ -separation is satisﬁed by the underlying dis- tribution. The proof of this theorem requires some additional machinery that we introduce in chapter 4, so we defer the proof to that chapter (see section 4.5.1.1). 
> 换句话说，任意由 d-seperation 表示的独立性都会被 underlying 的分布满足

A second desirable property is the complementary one — completeness : d-separation detects all possible independencies. More precisely, if we have that two variables $X$ and $Y$ are indepen- dent given $Z$ , then they are d-separated. A careful examination of the completeness property reveals that it is ill deﬁned, inasmuch as it does not specify the distribution in which $X$ and $Y$ are independent. 
> 我们还需要保证 d-seperation 这个概念的完整性：d-sepration 检测到所有可能的独立性，也就是说，如果两个节点 XY 在给定 Z 时条件独立，则它们一定是 d-seperation 的

To formalize this property, we ﬁrst deﬁne the following notion: 

**Deﬁnition 3.8** faithful 
A distribution $P$ is faithful to $\mathcal{G}$ i whenever $(X\perp Y\mid Z)\in{\mathcal{Z}}(P)$ , then $\operatorname{d-sep}_{\mathcal{G}}(X; Y\mid Z)$ . In other words, any independence in P is reﬂected in the d-separation properties of the graph. 
> 定义：
> 对于分布 P，只要 $(X\perp Y\mid Z) \in \mathcal I (P)$，就有 $\text{d-sep}_{\mathcal G}(X; Y\mid Z)$，则我们称分布 P 是忠实于图 $\mathcal G$ 的，换句话说，$P$ 中的任意独立性都在图 $\mathcal G$ 中的 d-sepration 中得到反映，也就是图中的条件独立性包含了 $P$ 中的条件独立性

We can now provide one candidate formalization of the completeness property is as follows: 

- For y distrib tion $P$ that fact zes ver $\mathcal{G}$ , e hav that $P$ is faithful to $\mathcal{G}$ ; that is, if $X$ and $Y$ are not d-se arated given Z in G , then X and $Y$ are dependent in all distributions P that factorize over G . 

> 我们现在将 d-seperation 的完整性描述如下：
> 对于任意根据 $\mathcal G$ 分解的分布 P，P 都忠实于 $\mathcal G$
> 也就是说，如果 XY 在给定 $\mathcal G$ 中的 Z 时不是 d-seperation 的，因为所有根据 $\mathcal G$ 分解的分布 $P$ 都忠实于 $\mathcal G$，则对于所有根据 $\mathcal G$ 分解的分布 $P$，都不存在 $(X\perp Y\mid Z)$，也就是 $X, Y$ 是相关的

This property is the obvious converse to our notion of soundness: If true, the two together would imply that, for any $P$ that factorizes over $\mathcal{G}$ , we have that $\mathcal{Z}(P)=\mathcal{Z}(\mathcal{G})$ . Unfortunately, this highly desirable property is easily shown to be false: Even if a distribution factorizes over $\mathcal{G}$ , it can still contain additional independencies that are not reﬂected in the structure. 
> 可靠性：$\mathcal G$ 中有 d-seperatoin，则根据 $\mathcal G$ 分解的 $P$ 中有对应的条件独立性
> 完整性：根据 $\mathcal G$ 分解的 $P$ 中存在条件独立性，则 $\mathcal G$ 中有对应的 d-seperation
> 可靠性和完整性同时成立时，就表示对于任意在 $\mathcal G$ 上分解的 $P$，有 $\mathcal I (P) = \mathcal I (\mathcal G)$，但这个性质往往不容易成立，往往在根据 $\mathcal G$ 分解的 $P$ 中会存在额外的独立性条件

Example 3.3 Consider a distribution $P$ over two variables $A$ and $B$ , where $A$ and $B$ are independent. One possible I-map for $P$ is the network $A\rightarrow B$ . For example, we can set the CPD for B to be 

This example clearly violates e ﬁrst candidate deﬁnition of completeness, because the graph $\mathcal{G}$ is an I-map for the distribution P , yet there are independencies that hold for this distribution but do not follow from $d$ -separation. In fact, these are not independencies that we can hope to discover by examining the network structure. 

Thus, the completeness property does not hold for this candidate deﬁnition of completeness. We therefore adopt a weak er yet still useful deﬁnition: 
> 我们考虑为 completeness 采用一个更弱的定义

- If $(X\perp Y\mid Z)$ i ll dis ibutions $P$ $\mathcal{G}$ , then $d{\mathfrak{-s e p}}_{\mathcal{G}}(X; Y\mid Z)$ . And the contrapositive: If X nd Y $Y$ are not d-separated given Z in $\mathcal{G}$ , then X and Y are dependent in some distribution P that factorizes over $\mathcal{G}$ . 
> 如果 $(X\perp Y \mid Z)$ 在所有根据 $\mathcal G$ 分解的分布 $P$ 中成立，则 $\text{d-sep}_{\mathcal G}(X; Y\mid Z)$ 成立，同时，如果在 $\mathcal G$ 中 $X, Y$ 在给定 $Z$ 时不是 d-seperated，则 $X, Y$ 在某个根据 $\mathcal G$ 分解的 $P$ 中是相关的

Using this deﬁnition, we can show: 

**Theorem 3.4** 
Let $\mathcal{G}$ be a BN s cture. If $X$ and $Y$ not d-separated gi en $Z$ in $\mathcal{G}$ , then $X$ and $Y$ are dependent given Z in some distribution P that factorizes over G . 
> 定理：
> 如果在 $\mathcal G$ 中 $X, Y$ 在给定 $Z$ 时不是 d-seperated 的（相互依赖），则 $X, Y$ 在*某个*根据 $\mathcal G$ 分解的分布 $P$ 中是相关的，也就是 $P$ 中没有额外的独立性

**Proof** The proof constructs a distribution $P$ that makes $X$ and $Y$ correlated. The construction is roughly as follows. As $X$ and $Y$ are not d-separated, there exists an active trail $U_{1},\dots, U_{k}$ between them. We deﬁne CPDs for the variables on the trail so as to make each pair $U_{i}, U_{i+1}$ correlated; in the case of a v-structure $U_{i}\,\to\, U_{i+1}\,\leftarrow\, U_{i+2}$ , we deﬁne the CPD of $U_{i+1}$ so as to ensure correlation, and also deﬁne the CPDs of the path to some downstream evidence node, in a way that guarantees that the downstream evidence activates the correlation between $U_{i}$ and $U_{i+2}$ . All other CPDs in the graph are chosen to be uniform, and thus the construction guarantees that inﬂuence only ﬂows along this single path, preventing cases where the inﬂuence of two (or more) paths cancel out. The details of the construction are quite technical and laborious, and we omit them. 
> 证明：构造一个 $X, Y$ 相关的，且根据 $\mathcal G$ 分解的分布 $P$

We can view the completeness result as telling us that our deﬁniti n of $\mathcal{Z}(\mathcal{G})$ is the maximal one. For any independence assertion that not a consequence f d-separation in $\mathcal{G}$ , we can always ﬁnd a counterexample distribution P that factorizes over G . In fact, this result can be strengthened signiﬁcantly: 
> 我们认为完整性告诉了我们 $\mathcal I (\mathcal G)$ 的定义是最大的，也就是对于任意不是 $\mathcal G$ 中的 d-seperation 的结果的独立性断言，我们都可以找到一个根据 $\mathcal G$ 分解的反例 $P$ ($P$ 中不存在不满足 d-seperation 结果的独立性断言)
> 我们可以强化这一结果

**Theorem 3.5** 
For almost all distributions $P$ that factorize over $\mathcal{G}$ , that is, for all distributions except for a set of measure zero in the space of CPD parameter iz at ions, we have that $\mathcal{Z}(P)=\mathcal{Z}(\mathcal{G})$ . 
> 定理：
> 对于根据 $\mathcal G$ 分解的*几乎全部*分布 $P$（也就是除了 CPD 参数化空间中测度为零的集合），我们有 $\mathcal I (\mathcal G) = \mathcal I (P)$

This result strengthens theorem 3.4 in two distinct ways: First, whereas theorem 3.4 shows that any dependency in the graph can be found in some distribution, this new result shows that there exists a single distribution that is faithful to the graph, that is, where all of the dependencies in the graph hold simultaneously. Second, not only does this property hold for a single distribution, but it also holds for almost all distributions that factorize over $\mathcal{G}$ . 
> 定理3.4表明了图中的任意依赖都可以在某个分布中找到，该定理说明了存在忠实于图的分布（分布中的独立性被图中的独立性包含），并且事实上对于几乎所有根据 $\mathcal G$ 分解的分布，这一点都是成立的

**Proof** At a high level, the proof is based on the following argument: Each conditional inde- pendence assertion is a set of polynomial equalities over the space of CPD parameters (see exercise 3.13). A basic property of polynomials is that a polynomial is either identically zero or it is nonzero almost everywhere (its set of roots has measure zero). Theorem 3.4 implies that polynomials corresponding to assertions outside $\mathcal{Z}(\mathcal{G})$ cannot be entically zero, because they have at least one counterexample. Thus, the set of distributions P , which exhibit any one of these “spurious” independence assertions, has measure zero. The set of distributions that do not satisfy $\mathcal{Z}(P)=\mathcal{Z}(\mathcal{G})$ is the union of these separate sets, one for each spurious independence assertion. The union of a ﬁnite number of sets of measure zero is a set of measure zero, proving the result. 
> 证明：每个条件独立性声明都是在 CPD 的参数空间中的一个多项式等式集合，而多项式的一个基本性质就是要么处处为零要么几乎都非零（即其根集合的测度为零）
> 定理3.4说明不存在于 $\mathcal I (\mathcal G)$ 中的独立性断言不能处处为零，因为它们至少存在一个反例，那么要为零的话，它们的测度就是零，因此展现出 “虚假的” 独立性的分布 $P$ 的测度是零
> 不满足 $\mathcal I (\mathcal G) = \mathcal I (P)$ 的分布构成的集合就是这些包含了 “虚假的” 独立性断言的分布的集合的并集，而有限个测度为零的集合的并集的测度仍然是零
> 因此除了测度为零的分布以外，都有 $\mathcal I (\mathcal G) = \mathcal I (P)$

**These results state that for almost all parameter iz at ions $P$ of the graph $\mathcal{G}$ (that is, for almost all possible choices of CPDs for the variables), the d-separation test precisely characterizes the independencies that hold for $P$ .** In other words, even if we have a dis bution $P$ that satisﬁes more independencies than $\mathcal{Z}(\mathcal{G})$ , a slight perturbation of the CPDs of P will almost always eliminate these “extra” independencies. This guarantee seems to state that such independencies are always accidental, and we will never encounter them in practice. However, as we illustrate in example 3.7, there are cases where our CPDs have certain local structure that is not accidental, and that implies these additional independencies that are not detected by $\mathrm{d}$ -separation. 
> 这个结果说明了对于几乎所有 $\mathcal G$ 参数化的 $P$ (对于 $\mathcal G$ 中的变量的几乎所有可能条件概率分布的选择)，d-seperation 测试可以精确地表征 $P$ 中存在的独立性
> 换句话说，即便我们有满足比 $\mathcal I (\mathcal G)$ 中更多独立性的分布 $P$，一个对 $P$ 中的条件概率分布的轻微扰动会几乎总是消除这些“额外”的福利性
### 3.3.3 An Algorithm for d-Separation 
The notion of $\mathrm{d}$ -separation allows us to infer independence properties of a distribution $P$ that factorizes over $\mathcal{G}$ simply by examining the connectivity of $\mathcal{G}$ . However, in order to be useful, we need to be able to determine d-separation efectively. Our deﬁnition gives us a constructive solution, but a very inefcient one: We can enumerate all trails between $X$ and $Y$ , and check each one to see whether it is active. The running time of this algorithm depends on the number of trails in the graph, which can be exponential in the size of the graph. 
> 要知道根据 $\mathcal G$ 分解的 $P$ 中存在的独立性，我们需要知道 $\mathcal G$ 中存在的 d-seperation
> 目前，我们只知道通过列举出 $X, Y$ 中所有的路径，然后检查二者之间是否存在活跃的路径，以确定二者是否为 d-seperation，该算法的时间依赖于图中的路径数量，往往和图的大小成指数比

Fortunately, there is a much more efcient algorithm that requires only linear time in the size of the graph. The algorithm has two phases. We begin by traversing the graph bottom up, from the leaves to the roots, marking all nodes that are in $Z$ or that have descendants in $Z$ . Intuitively, these nodes will serve to enable v-structures. In the second phase, we traverse breadth-ﬁrst from $X$ to $Y$ , stopping the traversal along a trail when we get to a blocked node. A node is blocked if: (a) it is the “middle” node in a v-structure and unmarked in phase I, or (b) is not such a node and is in $Z$ . If our breadth-ﬁrst search gets us from $X$ to $Y$ , then there is an active trail between them. 
> 存在时间和图大小成线性关系的算法，算法分两阶段：
> 第一阶段：从下至上，从叶子到根节点遍历图，标记所有在 $Z$ 中或者在 $Z$ 中有子孙的节点，这些节点会被用于 enable v-structure
> 第二阶段：从 $X, Y$ 广度优先遍历，对于遍历时沿着的一个迹，在达到一个 blocked 的节点时停止对这个迹的追踪
> 一个 blocked 的节点满足：它是一个 v-structure 中心的节点，并且在第一阶段没有被标记 or 它不是一个 v-structure 中心的节点，并且它在 $Z$ 中
> 如果广度优先遍历最终可以从 $X$ 到 $Y$，则说明 $X, Y$ 之间存在一条活跃路径

The precise algorithm is shown in algorithm 3.1. The ﬁrst phase is straightforward. The second phase is more subtle. For efciency, and to avoid inﬁnite loops, the algorithm must keep track of all nodes that have been visited, so as to avoid visiting them again. However, in graphs with loops (multiple trails between a pair of nodes), an intermediate node $Y$ might be involved in several trails, which may require diferent treatment within the algorithm: 
> 为了避免无限循环，算法必须维护 visited，防止重复访问
> 但是对于本身有 loop 的图（即一对节点之间存在多个 trial），有时一个中间节点 $Y$ 会在多个 trial 中被包含，此时还需要进一步修改算法

Example 3.4 
Consider the Bayesian network of ﬁgure 3.6, where our task is to ﬁnd all nodes reachable from $X$ . Assume tha $Y$ bserved, that is, $Y\in Z$ . Assume that the lgorithm ﬁrst encounters $Y$ via the direct edge $Y\rightarrow X$ → . Any extension of this t $Y$ nd hence the algo hm stops the traversal alon this trail. However, the trail $X\leftarrow Z\rightarrow Y\leftarrow W$ ← ← is not blocked by Y $Y$ . Thus, when we encounter $Y$ or the second time via the edge $Z\rightarrow Y$ → , we should not ignore it. Therefore, after the ﬁrst visit to Y , we can mark it as visited for the purpose of trails coming in from children of $Y$ , but not for the purpose of trails coming in from parents of $Y$ . 

In general, we see that, for each node $Y$ , we must keep track separately of whether it has been visited from the top and whether it has been visited from the bottom. Only when both directions have been explored is the node no longer useful for discovering new active trails. 

Based on this intuition, we can now show that the algorithm achieves the desired result: 

**Theorem 3.6** 
The algor hm Re hable $({\mathcal{G}}, X, Z)$ returns the set of all nodes reachable from $X$ via trails that are active in G given Z . 

The proof is left as an exercise (exercise 3.14). 
### 3.3.4 I-Equivalence 
The notion of $\mathcal{Z}(\mathcal{G})$ speciﬁes a set of conditional independence assertions that are associated with a graph. This notion allows us to abstract away the details of the graph structure, viewing it purely as a speciﬁcation of independence properties. In particular, one important implication of this perspective is the observation that very diferent BN structures can actually be equivalent, in that they encode precisely the same set of conditional independence assertions. Consider, for example, the three networks in ﬁgure 3.5a, (b), (c). All three of them encode precisely the same independence assumptions: $(X\perp Y\mid Z)$ . 
> $\mathcal I (\mathcal G)$ 指定了所有和图相关的条件独立断言，我们借助它抽象化图的结构，将图仅仅视作一系列独立性规定
> 因此，结构细节上不同的贝叶斯网络实质上是可以等价的，也就是编码了完全相同的独立性断言

**Deﬁnition 3.9** I-equivalence 
> 定义：
> 如果两个 $\mathcal X$ 上的图结构满足 $\mathcal I (\mathcal K_1) = \mathcal I (\mathcal K_2)$，则它们是 I-equivalent 的
> 定义于 $\mathcal X$ 上的所有图可以根据 I-equivalence 关系划分为互斥且完备的 I-equivalence 等价类，也就是由 I-equivalence 等价关系导出的等价类

Note that the v-structure network in ﬁgure $3.5\mathrm{d}$ induces a very diferent set of $\mathrm{d}$ -separation assertions, and hence it does not fall into the same I-equivalence class as the ﬁrst three. Its I-equivalence class contains only that single network. 
> v-structure 网络的 I-equivalence 往往只有它自己

I-equivalence of two graphs immediately implies that any distribution $P$ that can be factorized over one of these graphs can be factorized over the other. **Furthermore, there is no intrinsic property of $P$ that would allow us to associate it with one graph rather than an equivalent one. This observation has important implications with respect to our ability to determine the directionality of inﬂuence.** In particular, although we can determine, for a distribution $P (X, Y)$ , whether $X$ and $Y$ are correlated, there is nothing in the distribution that can help us determine whether the correct structure is $X\rightarrow Y$ or $Y\rightarrow X$ . We return to this point when we discuss the causal interpretation of Bayesian networks in chapter 21. 
> 两个图是 I-equivalent 时，表明可以根据其中任意一图分解的 P 也可以根据其他图分解
> 显然，我们也应该将 P 和一个 I-equivalent 关联，而不是关联仅仅一张图，这个思想会帮助我们决定影响的方向性
> 一般我们可以知道 $X, Y$ 是相关的，但分布中不会有其他的信息帮助我们决定是 $X\rightarrow Y$ 还是 $Y\rightarrow X$

The d-separation criterion allows us to test for I-equivalence using a very simple graph-based algorithm. We start by considering the trails in the networks. 

**Deﬁnition 3.10** skeleton 
The skeleton of a Bayesian network graph $\mathcal{G}$ over $\mathcal{X}$ is an undirected graph over $\mathcal{X}$ that contains an edge $\{X, Y\}$ for every edge $(X, Y)$ in G . 
> 定义：
> $\mathcal X$ 上的贝叶斯网络的 skeleton 是 $\mathcal G$ 导出的无向图

In the networks of ﬁgure 3.7, the networks (a) and (b) have the same skeleton. 

If two networks have a common skeleton, then the set of trails between two variables $X$ and $Y$ is same in both networks. If they do not have a common skeleton, we can ﬁnd a trail in one network that does not exist in the other and use this trail to ﬁnd a counterexample for the equivalence of the two networks. 
> 两个 skeleton 的图中，$X, Y$ 之间的 trail 集合是一样的，否则，可以找到一张图内存在但另一张中不存在的 trail

Ensuring that the two networks have the same trails is clearly not enough. For example, the networks in ﬁgure 3.5 all have the same skeleton. Yet, as the preceding discussion shows, the network of ﬁgure $3.5\mathrm{d}$ is not equivalent to the networks of ﬁgure 3.5a–(c). The diference, is of course, the v-structure in ﬁgure $3.5\mathrm{d}$ . Thus, it seems that if the two networks have the same skeleton and exactly the same set of v-structures, they are equivalent. Indeed, this property provides a sufcient condition for I-equivalence: 

**Theorem 3.7** 
> 定理：
> 对于 $\mathcal X$ 是上的两张图 $\mathcal G_1, \mathcal G_2$，如果二者具有相同的 skeleton，并且具有相同的 v-structure 集合，则二者是 I-equivalent 的 (充分条件)

The proof is left as an exercise (see exercise 3.16).

Unfortunately, this characterization is not an equivalence: there are graphs that are Iequivalent but do not have the same set of v-structures. As a counterexample, consider complete graphs over a set of variables. Recall that a complete graph is one to which we cannot add additional arcs without causing cycles. Such graphs encode the empty set of conditional in- dependence assertions. Thus, any two complete graphs are I-equivalent. Although they have the same skeleton, they invariably have diferent v-structures. Thus, by using the criterion on theorem 3.7, we can conclude (in certain cases) only that two networks are I-equivalent, but we cannot use it to guarantee that they are not. 
> 存在 v-structure 集合不同但是 I-equivalent 的图
> 考虑一个完全图（完全图即再加任意一个边就会导致环），完全图中的条件独立性断言是空集，因此任意两个完全图都是 I-equivalence，然而它们的 v-structure 可以不同

We can provide a stronger condition that does correspond exactly to I-equivalence. Intuitively, the ique dependence pattern that we want to associate with a v-structure $X\rightarrow Z\leftarrow Y$ is that X and Y are independent (conditionally on their parents), but dependent given Z . If there is a direct edge between $X$ and $Y$ , as there was in our example of the complete graph, the ﬁrst part of this pattern is eliminated. 

**Deﬁnition 3.11** immorality covering edge 
A $\nu$ -structure $X\rightarrow Z\leftarrow Y$ immorality if there is no direct edge between $X$ and $Y$ . If there is such an edge, it is called a covering edge for the $\nu$ -structure. 
> 定义：
> 如果在 $X, Y$ 之间没有直接的边，则 v-structure $X\rightarrow Z \leftarrow Y$ 就是一个 immorality，如果 $X, Y$ 之间存在直接连接的边，则该边被称为 v-structure 的 covering edge

Note that not every v-structure is an immorality, so that two networks with the same immoralities do not necessarily have the same v-structures. For example, two diferent complete directed graphs always have the same immoralities (none) but diferent v-structures. 
> 因为不是所有的 v-structure 都是 immorality，因此两个具有相同 immorality 的网络并不必要是具有相同 v-structure 的
> 例如，两个不同的完全有向图总是有相同的 immorality (None)，但是可以有不同的 v-structure

**Theorem 3.8** 
Let $\mathcal{G}_{1}$ and $\mathcal{G}_{2}$ be two graphs over $\mathcal{X}$ . Then $\mathcal{G}_{1}$ and $\mathcal{G}_{2}$ have the same skeleton and the same set of immoralities if and only if they are I-equivalent. 
> 定理：
> 令 $\mathcal G_1, \mathcal G_2$ 是 $\mathcal X$ 上的两个图，则当且仅当它们是 I-equivalent 时，$\mathcal G_1, \mathcal G_2$ 具有相同的 skeleton 和相同的 immorality 集合

The proof of this (more difcult) result is also left as an exercise (see exercise 3.17). 
We conclude with a ﬁnal characterization of I-equivalence in terms of local operations on the graph structure. 

**Deﬁnition 3.12** covered edge 
An edge $X\rightarrow Y$ in a graph $\mathcal G$ is said to be covered if $\text{Pa}_Y^{\mathcal G} = \text{Pa}_X^{\mathcal G} \cup \{X\}$
> 定义：
> 如果 $Y$ 的父节点就是 $X$ 的父节点加上 $X$，则边 $X\rightarrow Y$ 就是 covered edge

**Theorem 3.9** 
Two graphs $\mathcal{G}$ and ${\mathcal{G}}^{\prime}$ are $I_{\cdot}$ -equivalent if a d only if there exists a sequence of n orks $\mathcal{G}\;=$ $\mathcal{G}_{1},\ldots,\mathcal{G}_{k}=\mathcal{G}^{\prime}$ that are all I-equivalent to G such that the only diference between G $\mathcal{G}_{i}$ and $\mathcal{G}_{i+1}$ is a single reversal of a covered edge. 
> 定理：
> 当且仅当存在一个网络序列 $\mathcal G_1, \dots, \mathcal G_k = \mathcal G'$ 都是 $\mathcal G$ 的 I-equivalence，并且 $\mathcal G_{i+1}, \mathcal G_{i}$ 之间唯一的差异就是一个 covered edge 的反向时，$\mathcal G, \mathcal G'$ 是 I-equivalent

The proof of this theorem is left as an exercise (exercise 3.18). 
## 3.4 From Distributions to Graphs 
In the previous sections, we showed at, if $P$ factorizes ove $\mathcal{G}$ , we can derive a rich set of independence assertions that hold for P by simply examining G . This result immediately leads to the idea that we can use a graph as a way of revealing the structure in a distribution. In particular, we can test for independencies in $P$ by constructing a graph $\mathcal{G}$ that represents $P$ and testing d-separation in $\mathcal{G}$ . As we will see, having a graph that reveals the structure in P has other important consequences, in terms of reducing the number of parameters required to specify or learn the distribution, and in terms of the complexity of performing inference on the network. 
> 之前的部分中，我们展示了如果 $P$ 在 $\mathcal G$ 上分解，我们可以通过检查 $\mathcal G$ 就得到在 $P$ 中成立的一系列独立性断言，故我们可以用图表示分布的结构
> 特别地，我们可以通过构造一个表示 $P$ 的图，然后测试图中的 d-seperation 来揭示 $P$ 中的独立性
> 使用图表示分布可以帮助减少学习分布所需的参数，以及减少推理的复杂度

In this section, we examine the following question: Given a distribution $P$ , to what extent can we construct a graph $\mathcal{G}$ whose independencies are a reasonable surrogate for the independencies in $P\Lsh$ It is important to emphasize that we will never actually take a fully speciﬁed distribution $P$ and construct a graph $\mathcal{G}$ for it: As we discussed, a full joint distribution is much too large to represent explicitly. However, answering this question is an important conceptual exercise, which will help us later on when we try to understand the process of constructing a Bayesian network that represents our model of the world, whether manually or by learning from data. 
> 本节探讨以下问题：
> 给定分布 $P$，我们可以构造一个表示 $P$ 中独立性的图 $\mathcal G$ 到什么程度，注意我们当然不会为一个 fully specified 的分布构造一个非常大的图
### 3.4.1 Minimal I-Maps 
One approach to ﬁnding a graph that represents a distribution $P$ is simply to take any graph that is an I-map for $P$ . The problem with this naive approach is clear: As we saw in example 3.3, the complete graph is an I-map for any distribution, yet it does not reveal any of the independence structure in the distribution. However, examples such as this one are not very interesting. The graph that we used as an I-map is clearly and trivially unrepresentative of the distribution, in that there are edges that are obviously redundant. This intuition leads to the following deﬁnition, which we also deﬁne more broadly: 
> 最简单的方法是找到一个是 $P$ 的 I-map 的图，但也存在问题
> 例如，完全图是任意分布的 I-map（不存在依赖性），显然其中存在太多冗余边（依赖性）

**Deﬁnition 3.13** minimal I-map 
A graph $\mathcal{K}$ is $^a$ minimal I-map fo a set of independencies $\mathcal{T}$ if it is an I-map for $\mathcal{T}$ , and if the removal of even a single edge from renders it not an I-map. 
> 定义：
> 如果 $\mathcal K$ 是独立性集合 $\mathcal I$ 的 I-map，并且 $\mathcal K$ 移除任意一个边都会导致它不是 I-map，称 $\mathcal K$ 是 $\mathcal I$  的的 I-极小map（不存在多余的依赖性，删边就是增加独立性）
> minimal I-map 是在删边之后就会出现 $P$ 中没有的独立性，但这不代表着它本身就包含了 $P$ 中所有的独立性

This notion of an I-map applies to multiple types of graphs, both Bayesian networks and other types of graphs that we will encounter later on. Moreover, because it refers to a set of independencies $\mathcal{T}$ , i an be used to deﬁne an I-map for a distribution $P$ , by taking $\mathcal{Z}=\mathcal{Z}(P)$ , or to another graph ′ , by taking $\mathcal{Z}=\mathcal{Z}(K^{\prime})$ . 
> 最小 I-map 的概念不仅可以应用于贝叶斯网络，也可以应用于其他类型的图

Recall that deﬁnition 3.5 deﬁnes a Bayesian network to be a distribution $P$ that factorizes over $\mathcal{G}$ , thereby implying that $\mathcal{G}$ is an I-map for $P$ . is standard to restrict the deﬁnition even further, by requiring that $\mathcal{G}$ be a minimal I-map for P . 

How do we obtain a minimal I-map for the set of independencies induced by a given dis- tribution $P$ ? The proof of the factorization theorem (theorem 3.1) gives us a procedure, which is shown in algorithm 3.2. We assume we are given a predetermined variable ordering , say, $\{X_{1},\cdot\cdot\cdot, X_{n}\}$ . We n examine each variable $X_{i}$ , $i=1,\dots, n$ in turn. For each $X_{i}$ , we pick som inimal subset U of $\{X_{1},.\,.\,.\,, X_{i-1}\}$ to be $X_{i}$ ’s parents in $\mathcal{G}$ . More precisely, we requ that U satisfy $(X_{i}\mid\bot\;\{X_{1},\bot\;.\;.\;, X_{i-1}\}-U\mid U)$ , and that no no can be removed from U without violating this property. We then set U to be the parents of $X_{i}$ . 
> 考虑对于一个给定的分布 $P$，找到它的最小 I-map
> 参考 factorization 定理的证明过程，我们假设给定一个预定义的变量顺序 $\{X_1,\dots, X_n\}$，然后轮流检查变量 $X_i, i= 1,\dots, n$ ，
> 对于每一个 $X_i$，我们在 $\{X_1,\dots, X_{i-1}\}$ 中选择出某个最小的子集 $\pmb U$，用于表示 $\mathcal G$ 中 $X_i$ 的父变量集合，我们要求 $\pmb U$ 满足 $(X_i \perp \{X_1,\dots, X_{i-1}\}- \pmb U\mid \pmb U)$ （给定父变量，和其他变量条件独立），并且在不违反这一性质的前提下，没有节点可以从 $\pmb U$ 中被移除（$\pmb U$ 不能不包含 $X_i$ 的全部父变量，同时因为最小，$\pmb U$ 不能包含不是 $X_i$ 的父变量的节点，否则可以在不违反该性质的前提下被移除）
> 然后，我们将 $\pmb U$ 设定为 $X_i$ 的 parents

The proof of theorem 3.1 tells us that, if each node $X_{i}$ is independent of $X_{1},\dots, X_{i-1}$ given its parent $\mathcal{G}$ , then $P$ factorizes over $\mathcal{G}$ . We can then conclude from theore 3.2 that $\mathcal{G}$ is an I-map for P . By constructio $\mathcal{G}$ is minimal, so that $\mathcal{G}$ is a minimal I-map for P . 
> 定理3.1的证明告诉我们，如果每个节点 $X_i$ 在给定它在 $\mathcal G$ 中的 parents 的前提下独立于 $X_1,\dots, X_{i-1}$，则 $P$ 根据 $\mathcal G$ 分解，因此我们构造出的 $\mathcal G$ 是 $P$ 的一个 I-map
> 因为构造时 $\pmb U$ 是最小的，则构造出的 $\mathcal G$ 是 $P$ 的最小 I-map

Note that our choice of U may not be unique. Consider, for example, a case where two variables $A$ and $B$ are logically equivalent, that is, our distribution $P$ only gives positive probability to instantiations where $A$ and $B$ have the same value. Now, consider a node $C$ that is correlated with $A$ . Clearly, we can choose either $A$ or $B$ to be a parent of $C$ , but having chosen the one, we cannot choose the other without violating minimality. Hence, the minimal parent set $U$ in our construction is not necessarily unique. However, one can show that, if the distribution is positive (see deﬁnition 2.5), that is, if for any instantiation $\xi$ to all the network variables $\mathcal{X}$ we have that $P (\xi)>0$ , then the choice of parent set, given an ordering, unique. 
> $\pmb U$ 的选择不一定是唯一的
> 但如果 $P$ 是一个 positive 分布（只要事件不为空，概率就大于0），即对于所有网络变量 $\mathcal X$ 的任意实例化 $\xi$，我们都有 $P (\xi) > 0$，则可以证明给定一个顺序，对于 parent set 的选择是唯一的

![[Probabilistic Graph Theory-Algorithm3.2.png]]

Under this assumption, algorithm 3.2 can produce all minimal I-maps for P : Let be any min mal I-map for $P$ . If we give call Build-Minimal-I-Map with an orderi $\prec$ that is topological for G , then, due to the uniqueness argument, the algorithm must return G . 
> 在这一假设下，algorithm3.2可以为 $P$ 生成所有的最小 I-map：令 $\mathcal G$ 是 $P$ 的任意最小 I-map，如果我们为算法3.2给定一个 $\mathcal G$ 的拓扑排序 $\prec$，则算法一定会返回 $\mathcal G$
> 不同的拓扑排序会返回不同的 $\mathcal G$

At ﬁrst glance, the minimal I-map seems to be a reasonable candidate for capturing the structure in the distribution: It seems that if $\mathcal{G}$ is a minim I-map for a d ribution $P$ , then we should be able to “read of” all of the independencies in P directly from G . Unfortunately, this intuition is false. 
> 但即便 $\mathcal G$ 是 $P$ 的最小 I-map，我们也不能从 $\mathcal G$ 中“读出” $P$ 中全部的独立性，因为排序顺序实际上存在影响

Example 3.5 
Consider the distribution PBstudent, as defined in figure 3.4, and let us go through the process of constructing a minimal I-map for PBstudent. We note that the graph Gstudent precisely reflects the independencies in this distribution PBstudent (that is, I(PBstudent) = I(Gstudent)), so that we can use Gstudent to determine which independencies hold in PBstudent.

Our construction process for three diferent orderings. Throughout this process, it is important to remember that we are testing independencies relative to the distribution $P_{\mathcal{B}^{\mathrm{stunderit}}}$ . We can use $G_{\mathrm{stadium}}$ (ﬁgure 3.4) to guide our intuition about which independencies hold in $P_{\mathcal{B}^{\mathrm{stunderit}}}$ , but we can always resort to testing these independencies in the joint distribution $P_{\mathcal{B}^{\mathrm{stunderit}}}$ . 

The ﬁrst ordering is a very natural one: $D, I, S, G, L$ . We add one node at a time and see which of the possible edges from the preceding nodes are redundant. We start by adding $D$ , then $I$ . We an now remove the dge from $D$ to $I$ because this particular distribution satisﬁes $(I\perp D)$ , so $I$ is independent of D given its other parents (the empty set). Continuing on, we add S , but we n remove the edge from $D$ to $S$ be use ur distribution satisﬁes $(S\perp D\mid I)$ d G , but we can move the edge from S to G , because the distribution satisﬁes ( $\left (G\perp S\mid I, D\right)$ ⊥ | ) . Finally, we add L , but we can remove all edges from $D, I, S$ . Thus, our ﬁnal output is the graph in ﬁgure $3.8a_{!}$ , which is precisely our original network for this distribution. 
> 对于一个节点序列，我们一次加入一个节点，然后看哪些从前面节点到当前节点的边是多余的（多余的父节点）

Now, consider a somewhat less natural ordering: $L, S, G, I, D$ . In this case, the resulting I-map is not quite as natural or as sparse. To see this, let us consider the sequence of steps. We start by adding $L$ to the graph. Since it is the ﬁrst variable in the ordering, it must be a root. Next, we consider $S$ . The decision is whether to have $L$ as a parent of $S$ . Clearly, we need an edge from $L$ to $S$ , because the quality of the student’s letter is correlated with his SAT score in this distribution, and $S$ has no other parents that help render it independent of $L$ . Formally, we have th $(S\perp L)$ does not hold in the distribution. In the next iteration of the algorithm, we introduce G . Now, all possible s sets of $\{L, S\}$ are p tential rents set for $G$ . Clearly, $G$ depen nt on $L$ . Moreover, although G is independent of S given I , it is not independent of S given L . Hence, we must add the edge between $S$ and $G$ . Carrying out the procedure, we end up with the graph shown in ﬁgure $3.8b$ . 

Finally, consider the ordering: $L, D, S, I, G$ . In this case, a similar analysis results in the graph shown in ﬁgure $3.8c,$ which is almost a complete graph, missing only the edge from $S$ to $G$ , which we can remove because $G$ is independent of $S$ given $I$ . 

Note that the graphs in ﬁgure 3.8b, c really are minimal I-maps for this distribution. However, they fail to capture some or all of the independencies that hold in the distribution. Thus, they show that the fact that $\mathcal{G}$ i a minimal I-map for $P$ is far from a guarantee that $\mathcal{G}$ captures the independence structure in P . 
### 3.4.2 Perfect Maps 
We aim to ﬁnd a graph $\mathcal{G}$ that precisely captures the independencies in a given distribution $P$ . 
> 我们希望找到可以精确捕获给定分布 $P$ 中的独立性的图 $\mathcal G$

***Definition 3.14*** perfect map 
We say that a graph $\mathcal{K}$ $a$ perfect map (P-m ) for a set of independencies $\mathcal{T}$ if we have that $\mathcal{Z}(\mathcal{K})=\mathcal{Z}$ . We say that K is a perfect map for $P$ if $\mathcal{Z}(\mathcal{K})=\mathcal{Z}(P)$ .
> 如果我们有 $\mathcal I (\mathcal K) = \mathcal I$ ，则称图 $\mathcal K$ 是独立性集合 $\mathcal I$ 的完美 I-map，或者称为 p-map

If we obtain a grap $\mathcal{G}$ that is a P-m p for a distribution $P$ , then we can ( nition) read the independencies in P directly from G . By construction, our original graph $G_{s t u d e n t}$ is a P-map for $P_{\mathcal{B}^{s t u d e n t}}$ . 
> 如果我们得到一个分布 $P$ 的一个 p-map $\mathcal G$，则我们可以从 $\mathcal G$ 中读出 $P$ 中的所有的独立性

If our goal is to ﬁnd a perfect map for a distribution, an immediate question is whether every distribution has a perfect map. Unfortunately, the answer is no, and for several reasons. The ﬁrst type of counterexample involves regularity in the parameter iz ation of the distribution that cannot be captured in the graph structure. 
> 并不是每个分布都有 p-map，一个例子就是分布的参数化中的 regularity 有时不能被图结构捕获

Example 3.6 
Consider a joint distribution $P$ over 3 random variables $X, Y, Z$ such that: 

$$
P (x, y, z)=\left\{\begin{array}{l l}{{1/12\qquad}}&{{x\oplus y\oplus z=f a l s e}}\\ {{1/6\qquad}}&{{x\oplus y\oplus z=t r u e}}\end{array}\right.
$$ 
where $\oplus$ the XOR (exclusive OR unctio A sim e calc tion shows that $(X\bot Y)\in{\mathcal{Z}}(P)$ , and that Z is not independent of X given Y $Y$ or of $Y$ given X . Hence, one minimal $I_{\cdot}$ -map for this distribution is the network $X\rightarrow Z\leftarrow Y$ , using a deterministic XOR for the CPD of $Z$ . However, this network is not a perfect map; a preci ly analogous calculation shows that $(X\perp Z)\in{\mathcal{Z}}(P)$ , but this conclusion is not supported by a d-separation analysis. 

Thus, we see that deterministic relationships can lead to distributions that do not have a P-map. Additional examples arise as a consequence of other regularities in the CPD. 
> 确定性关系会让分布不存在 p-map
> 其他的 CPD 中的 regularity 也是如此

Example 3.7 
Consider a slight elaboration of our Student example. During his academic career, our student George has taken both Econ101 and CS102. The professors of both classes have written him letters, but the recruiter at Acme Consulting asks for only a single recommendation. George’s chance of getting the job depends on the quality of the letter he gives the recruiter. We thus have four random variables: $L1$ and $L2$ , corresponding to the quality of the recommendation letters for Econ101 and CS102 respectively; $C$ , whose value represents George’s choice of which letter to use; and $J$ , representing the event that George is hired by Acme Consulting. 

The obvious minimal $I^{,}$ -map for this distribution is shown in ﬁgure 3.9. Is this a perfect map? Clearly, it does not reﬂect independencies that are not at the variable level. In particular, we have that $(L1\perp J\mid C=2)$ . However, this limitation is not surprising; by deﬁnition, a BN structure makes independence assertions only at the level of variables. (We return to this issue in section 5.2.2.) However, our problems are not limited to these ﬁner-grained independencies. Some thought reveals that, in our arget distribution, we also have tha $(L1\;\bot\; L2\;\vert\; C, J)$ ! This independence is not implied by d-separation, because the $\nu$ -structure L $L1\,\rightarrow\, J\,\leftarrow\, L2$ → ← is enabl wever, we can convince ourselves that the independence holds using reasoning by cases. If $C=1$ , then there is no dependence of $J$ on $L2$ . Intuitively, the edge from $L2$ to $J$ disappears, eliminating the trail between $L1$ and $L2$ , so that $L1$ and $L2$ are independent in this case. A symmetric analysis applies in the case that $C\,=\, 2$ . Thus, in both cases, we have that $L1$ and $L2$ are independent. This independence assertion is not captured by our minimal I-map, which is therefore not a $P\cdot$ -map. 

A second class of distributions that do not have a perfect map are those for which the independence assumptions imposed by the structure of Bayesian networks is simply not appropriate. 
> 有时，贝叶斯网络 impose 的独立性假设仅仅是不适合于特定分布的，因此这类分布也不存在 p-map

Example 3.8
Consider a scenario where we have four students who get together in pairs to work on the homework for a class. For various reasons, only the following pairs meet: Alice and Bob; Bob and Charles; Charles and Debbie; and Debbie and Alice. (Alice and Charles just can’t stand each other, and Bob and Debbie had a relationship that ended badly.) The study pairs are shown in ﬁgure 3.10a. 

In this example, the professor accidentally misspoke in class, giving rise to a possible miscon- ception among the students in the class. Each of the students in the class may subsequently have ﬁgured out the problem, perhaps by thinking about the issue or reading the textbook. In subsequent study pairs, he or she may transmit this newfound understanding to his or her study partners. We therefore have four binary random variables, representing whether the student has the misconcep- tion or not. We assume tha or each $X\,\in\,\{A, B, C, D\}$ , $x^{1}$ denotes the case where the student has the misconception, and x $x^{0}$ denotes the case where he or she does not. 

Because Alice and Charles never speak to each other directly, we have that $A$ and $C$ are con- ditionally independent given $B$ and $D$ . Similarly, $B$ and $D$ are conditionally independent given $A$ and $C$ . Can we represent this distribution (with these independence properties) using a BN? One attempt is shown in ﬁgure 3.10b. Indeed, it encodes the independence assumption that $(A\ \perp\ C\ |\ \{B, D\})$ . However, it also implies that $B$ and $D$ are independent given only $A$ , but dependent given both $A$ and $C$ . Hence, it fails to provide a perfect map for our target dis- tribution. A second attempt, shown in ﬁgure 3.10c, is equally unsuccessful. It also implies that $(A\perp C\mid\{B, D\})$ , but it also implies that $B$ and $D$ are marginally independent. It is clear that all other candidate BN structures are also ﬂawed, so that this distribution does not have a perfect map. 
### 3.4.3 Finding Perfect Maps\*
Earlier we discussed an algorithm for ﬁnding minimal I-maps. We now consider an algorithm for ﬁnding a perfect map (P-map) of a distribution. Because the requirements from a P-map are stronger than the ones we require from an I-map, the algorithm will be more involved. 

Throughout the discussion in this section, we assume that $P$ has a P-map. In other words, there is an unknown DAG $\mathcal{G}^{*}$ that is map of $P$ . Since $\mathcal{G}^{*}$ is a P-map, we will interchangeably refer to independencies in P and in G $\mathcal{G}^{*}$ (since these are the same). We note that the algorithms we describe do fail when they are given a distribution that does not have a P-map. We discuss this issue in more detail later. 

Thus, our goal is to identify $\mathcal{G}^{\ast}$ from $P$ . One obvious difculty hat arises when we consider this goal is that $\mathcal{G}^{*}$ is, in general, not uniquely identiﬁable from P . A P-map of a distribution, if one exists, is generally not unique: As we saw, for example, in ﬁgure 3.5, multiple graphs can encode precisely the same independence assumptions. However, the P-map of a distribution is unique up to I-equivalence between networks. That is, a distribution $P$ can have many P-maps, but all of them are I-equivalent. 

If we require that a P-map construction algorithm return a single network, the output we get may be some arbitrary member of the I-equivalence class of $\mathcal{G}^{\ast}$ . A more correct answer would be to return the entire equivalence class, thus avoiding an arbitrary commitment to a possibly incorrect structure. Of course, we do not want our algorithm to return a (possibly very large) set of distinct networks as output. Thus, one of our tasks in this section is to develop a compact representation of an entire equivalence class of DAGs. As we will see later in the book, this representation plays a useful role in other contexts as well. 

This formulation of the problem points us toward a solution. Recall that, according to theorem 3.8, two DAGs are I-equivalent if they share the same skeleton and the same set of immoralities. Thus, we can construct the I-equivalence class for $\mathcal{G}^{\ast}$ by determi ng its skeleton and its immoralities from the independence properties of the given distribution P . We then use both of these components to build a representation of the equivalence class. 

#### 3.4.3.1 Identifying the Undirected Skeleton 

At this stage we want to construct an undirected graph $S$ that contains an edge $X{-}Y$ if $X$ and $Y$ are adjacent in $\mathcal{G}^{\ast}$ ; that is, if either $X\rightarrow Y$ or $Y\rightarrow X$ is an edge in $\mathcal{G}^{\ast}$ . 

The basic ea is to use independence queries of the form $(X\perp Y\mid U)$ for difer sets of variables U . This idea is based on the observation that if X and Y are adjacent in G $\mathcal{G}^{\ast}$ , we cannot separate them with any set of variables. 

Lemma 3.1 

$\mathcal{G}^{*}$ be a $P\cdot$ -map of a distribution $\mathcal{P}$ , an let $X$ and $Y$ be two v ables ch that $X\rightarrow Y$ is in $\mathcal{G}^{*}$ . Then, $P\not\models (X\bot Y\mid U)$ for any set U that does not include X and Y . 

Proof Assume that $X\,\rightarrow\, Y\,\in\,{\mathcal{G}}^{*}$ , and let $U$ be a set of riables. According to d- separation the il $X\rightarrow Y$ → cannot be blo ed by the evid $U$ $X$ and $Y$ are not d-separated by U . Since G $\mathcal{G}^{*}$ is a P-map of P , we have that $P\not\models (X\bot Y\mid U)$ ̸| ⊥ | . 

This lemma implies that if $X$ and $Y$ are adjacent in $\mathcal{G}^{*}$ that involve both of them would fail. Conversely, if X and $Y$ are not adjacent in $\mathcal{G}$ , we would hope to be able to ﬁnd a set of variables that makes these two variables conditionally independent. Indeed, as we now show, we can provide a precise characterization of such a set: 

Lemma 3.2 The proof is left as an exercise (exercise 3.19). Thus, if $X$ an $Y$ are not adjacent in $\mathcal{G}^{*}$ , then we can ﬁnd a set $U$ so that ${\mathcal{P}}\models (X\ \bot\ Y\ |\ U)$ ⊥ | . We call this set U a witness of their independence. Moreover, the lemma shows that we can ﬁnd a witn s of bounded size. Thus, if we assume that $\mathcal{G}^{\ast}$ has bounded indegree, say less than or equal to d , then we do not need to consider witness sets larger than d . 

 

With these tools in hand, we can now construct an algorithm for building a skeleton of $\mathcal{G}^{*}$ , shown in algorithm 3.3. For each pair of variables, we consider all potential witness sets and test for independence. If we ﬁnd a witness that separates the two variables, we record it (we will soon see why) and move on to the next pair of variables. If we do not ﬁnd a witness, then we conclu o variables are adjacent in $\mathcal{G}^{*}$ and add them to the skeleton. The list Witnesses $(X_{i}, X_{j},{\mathcal{H}}, d)$ H in line 4 speciﬁes the set of possible witness sets t t we consider for separating $X_{i}$ and $X_{j}$ . From our earlier discussion, if we assume a bound d on the indegree, then we can restrict attention to sets $U$ of size at most $d$ . Moreover, using the same analysis, we saw that we have a witness that consists either of the parents of $X_{i}$ or of the parents of $X_{j}$ . In the ﬁ case, we can restrict at ntion to sets $U\subseteq\dot{\mathrm{Nb}}_{X_{i}}^{\mathcal{H}}-\{X_{j}\}$ −{ } , where $\operatorname{Nb}_{X_{i}}^{\hat{\mathcal{H}}}$ are the neighbors of $X_{i}$ in the current graph H ; in the s ond, we c similarly restrict attention to ts $U\subseteq\mathrm{Nb}_{X_{j}}^{\mathcal{H}}-\{X_{i}\}$ . ally, w note that if U separates $X_{i}$ and $X_{j}$ , then also many of $U$ ’s supersets will separate $X_{i}$ and $X_{j}$ . Thus, we search the set of possible witnesses in order of increasing size. 

This algorithm ill over the correct skeleton given that $\mathcal{G}^{\ast}$ is a P-map of $P$ and has bounded indegree d . If P does not have a P-map, then the algorithm can fail; see exercise 3.22. This algorithm has complexity of $O (n^{d+2})$ since we consider $O (n^{2})$ pairs, and for each we perform $O ((n-2)^{d})$ independence tests. We greatly reduce the number of independence tests by ordering potential witnesses accordingly, and by aborting the inner loop once we ﬁnd a witness for a pair (after line 9). However, for pairs of variables that are directly connected in the skeleton, we still need to evaluate all potential witnesses. 
#### 3.4.3.2 Identifying Immoralities 

potential immorality 

Proposition 3.1 

At this stage we have reconstructed the undirected skeleton $S$ using Build-PMap-Skeleton . Now, we want to reconstruct edge direction. The main cue for learning about edge dir ions in $\mathcal{G}^{\ast}$ are immoralities. As shown in theorem 3.8, all DAGs in the equivalence class of G $\mathcal{G}^{\ast}$ share the same set of immoralities. Thus, our goal is to consider potential immoralities in the skeleton and for each one determine whether it is indeed an immorality. A triplet of variables $X, Z, Y$ is a potential immorality if the skeleton contains $X{-}Z{-}Y$ but does not contain an edge between $X$ an $Y$ . If such a triplet is indeed an immorality $\mathcal{G}^{\ast}$ , then $X$ an $Y$ cannot be independent given Z . Nor will they be independent given a set U that contains Z . More precisely, 

$\mathcal{G}^{\ast}$ $P\cdot$ map of a distribution $P$ , and let $X, Y$ nd $Z$ be variab s that form an immorality $X\rightarrow Z\leftarrow Y$ → ← . Then, $P\not\models (X\bot Y\mid U)$ ⊥ | for any set U that contains Z . 

Proof Let $U$ be set of ariables t at contains $Z$ nce $Z$ i bserved, the tr $X\rightarrow Z\leftarrow Y$ is active, and so X and Y $Y$ are not d-separated in G $\mathcal{G}^{\ast}$ . Since G $\mathcal{G}^{*}$ is a P-map of P , we have that $P^{*}\not\models (X\bot Y\mid U)$ ⊥ | . 

What happens in the complementary situation? Suppose $X{-}Z{-}Y$ in the skeleton, but is lity. at one of the followi three ases is in $\mathcal{G}^{*}$ : $X\rightarrow Z\rightarrow Y$ , $Y\,\rightarrow\, Z\,\rightarrow\, X$ → → , or $X\leftarrow Z\rightarrow Y$ ← → . In all three cases, X and Y are d-separated only if Z is observed. 

Proposition 3.2 

Let $\mathcal{G}^{*}$ be a $P\!\!\cdot\!\!$ map of a d $P$ nd let the triplet $X, Y, Z$ be a potential immorality in the n of G $\mathcal{G}^{\ast}$ , such that $X\rightarrow Z\leftarrow Y$ → ← is not in $\mathcal{G}^{*}$ . If U is such that $P\models (X\ \bot\ Y\ |\ U)$ ⊥ | , then $Z\in U$ . 

Proof Consider all three conﬁgurations of the trail $X\,\rightleftharpoons\, Z\,\rightleftharpoons\, Y$ . In all three, $Z$ must be served r to block the trail. Since $\mathcal{G}^{\ast}$ is a P-map of $P$ , we have that if $P\models (X\bot Y\mid$ ⊥ | $U$ ) , then $Z\in U$ ∈ . 

Combining these two results, we see that a potential immorality $X{-}Z{-}Y$ is an immorality if and only if $Z$ is not in the witness set (s) for $X$ and $Y$ . That is, if $X{-}Z{-}Y$ is an immorality, then proposition 3.1 shows that $Z$ is not in any witness set $U$ ; conversely, if $X{-}Z{-}Y$ is not an immorality, the $Z$ must be in every witness set $U$ . Thus, we can use the speciﬁc witness set $U_{X, Y}$ that we recorded for $X, Y$ in order to determine whether this triplet is an immorality or not: we simply check whether $Z\in U_{X, Y}$ . If $Z\notin U_{X, Y}$ , then we declare the triplet an immorality. Otherwise, we declare that it is not an immorality. The Mark-Immoralities procedure shown in algorithm 3.4 summarizes this process. 

#### 3.4.3.3 Representing Equivalence Classes 

Once we have the skeleton and identiﬁed the immoralities, we have a speciﬁcation of the equivalence class of $\mathcal{G}^{\ast}$ . example, to test if $\mathcal{G}$ is equivalent to $\mathcal{G}^{*}$ we can check whether it has the same skeleton as $\mathcal{G}^{\ast}$ and whether it agrees on the location of the immoralities. 

The description of an equivalence class using only the skeleton and the set of immoralities is somewhat unsatisfying. For example, we might want to know whether the fact that our network is in the equivalence class implies that there an a $X\rightarrow Y$ . Although the deﬁnition does tell us whether there is some edge between X and Y , it leaves the direction unresolved. In other cases, however, the direction of an edge is fully determined, for example, by the presence of an immorality. To encode both of these cases, we use a graph that allows both directed and undirected edges, as deﬁned in section 2.2. Indeed, as we show, the chain graph, or PDAG, representation (deﬁnition 2.21) provides precisely the right framework. 

Deﬁnition 3.15 class PDAG 

Let $\mathcal{G}$ be a AG. A chain graph $\mathcal{K}$ is $^a$ class f the equivalence ss of $\mathcal{G}$ i shares the sa skeleton as G , an ns a directed edge $X\rightarrow Y$ → if and only if all G ${\mathcal{G}}^{\prime}$ that are I-equivalent to G contain the edge $X\rightarrow Y$ . 

In other words, a class PDAG represents potential edge orientations in the equivalence classes. If the edge is directed, then all the members of the equivalence class agree on the orientation of the edge. If the edge is undirected, there are two DAGs in the equivalence class that disagree on the orientation of the edge. 

For example, the networks in ﬁgure 3.5a–(c) are I-equivalent. The class PDAG of this equiva- lence class is the graph $X{-}Z{-}Y$ , since both edges can be oriented in either direction in some member of the equivalence class. Note that, although both edges in this PDAG are undirected, not all joint orientations of these edges are in the equivalence class. As discussed earlier, setting the orientations $X\rightarrow Z\leftarrow Y$ results in the network of ﬁ re $3.5\mathrm{d}$ , which does not belong this equivalence class. More generally, if the class PDAG has k undirected edges, the equivalence class can contain at most $2^{k}$ networks, but the actual number can be much smaller. 

Can we efectively construct the class PDAG $\mathcal{K}$ for $\mathcal{G}^{\ast}$ from the reconstru d skeleton and immoralities? Clea , edges involved in immoralities must be directed in K . The obvious question is whether K can contain directed edges that are not involved in immoralities. In other words, can there be additional edges whose direction is necessarily the same in every member of the equivalence class? To understand this issue better, consider the following example: 

Example 3.9 

Consider the DAG of ﬁgure 3.11a. This DAG has a single immor $A\rightarrow C\leftarrow B$ s immorality implies that the class PDAG of this DAG must have the arcs A $A\,\rightarrow\, C$ and B $B\,\rightarrow\, C$ directed, as 

![](images/1a02c0c0fccb74370210e12e9cea0befa0aa0e6c62c4ef574dba3b71002430ec.jpg) 
Figure 3.11 Simple example of compelled edges in the representation of an equivalence class. (a) Original DAG $\mathcal{G}^{\ast}$ . (b) Skeleton of $\mathcal{G}^{*}$ annotated with immoralities. (c) a DAG that is not equivalent to $\mathcal{G}^{\ast}$ . 

shown in ﬁgure 3.11b. This PDAG representation suggests that the edge $C{-}D$ can assume either orientation. Note, however, that the DAG of ﬁgure 3.11c, where we orient the edge between $C$ and $D$ as $D\rightarrow C$ , contains additional immoralities (that is, $A\rightarrow C\leftarrow D$ and $B\rightarrow C\leftarrow D_{z}$ ). Thus, this DAG is not equivalent to our original DAG. 

In this example, there is only one possible orientation of $C{-}D$ that is consistent with the ﬁnding that $A{-}C{-}D$ is not an immorality. Thus, we conclude that the class PDAG for the DAG of ﬁgure 3.11a is simply the DAG itself. In other words, the equivalence class of this DAG is a singleton. 

As this example shows, a negative result in an immorality test also provides information about edge orientation. In particu , in y case where the PDAG $\mathcal{K}$ contai cture $X\rightarrow Y{-}Z$ and there is no edge from $X$ $Z$ we must orient the edge Y $Y\,\rightarrow\, Z$ → , for otherwise we would create an immorality X $X\rightarrow Y\leftarrow Z$ . 

Some thought reveals that there are other local conﬁgurations of edges where some ways of orienting edges are inconsistent, forcing a particular direction for an edge. Each such conﬁgu- ration can be viewed as a local constraint on edge orientation, give rise to a rule that can be used to orient more edges in the PDAG. Three such rules are shown in ﬁgure 3.12. 

Let us understand the intuition behind these rules. Rule R1 is precisely the one we discussed earlier. Rule R2 is derived from the standard acyclicity constraint: If we have the directed path $X\,\rightarrow\, Y\,\rightarrow\, Z$ , and an undirected edge $X{-}Z$ , we cannot direct the ed $X\leftarrow Z$ without creating a cycle. Hence, we can conclude that the edge must be directed X $X\rightarrow Z$ → . The third rule seems a little more complex, but it is also easily motivated. Assume, by contradiction, that we direct the edge $Z\rightarrow X$ . In this ca nnot direct the edge $X{-}Y_{1}$ $X\rightarrow Y_{1}$ without creati we must have $Y_{1}\rightarrow X$ → . Similarly, we must have $Y_{2}\rightarrow X$ . , in this case, Y $Y_{1}\,\rightarrow\, X\,\leftarrow\, Y_{2}$ → ← forms an i ity (a is no edge between $Y_{1}$ and $Y_{2}$ ), which contradicts the fact that the edges $X{-}Y_{1}$ and $X{-}Y_{2}$ are undirected in the original PDAG. 

These three rules can be applied constructively in an obvious way: A rule applies to a PDAG whenever the induced subgraph on a subset of variables exactly matches the graph on the left-hand side of the rule. In that case, we modify this subgraph to match the subgraph on the right-hand side of the rule. Note that, by applying one rule and orienting a previously undirected edge, we create a new graph. This might create a subgraph that matches the antecedent of a rule, enforcing the orientation of additional edges. This process, however, must terminate at 

 
Figure 3.12 Rules for orienting edges in PDAG. Each rule lists a conﬁguration of edges before and after an application of the rule. 

some point (since we are only adding orientations at each step, and the number of edges is ﬁnite). This implies that iterated application of this local constraint to the graph (a process known as constraint propagation ) is guaranteed to converge. 

 

Algorithm 3.5 implements this process. It builds an initial graph using Build-PMap-Skeleton and Mark-Immoralities , and then iteratively applies the three rules until convergence, that is, until we cannot ﬁnd a subgraph that matches a left-hand side of any of the rules. 

 
Figure 3.13 More complex example of compelled edges in the representation of an equivalence class. (a) Original DAG $\mathcal{G}^{\ast}$ . (b) S ton of $\mathcal{G}^{\ast}$ annotated with immoralities. (c) Complete PDAG represen- tation of the equivalence class of G $\mathcal{G}^{*}$ . 

Example 3.10 Consider the DAG shown in ﬁgure 3.13a. After checking for immoralities, we ﬁnd the graph shown in ﬁgure $3. l3b.$ Now, we can start applying the preceding rules. For example, consider the variables $B,\, E,$ , and $F$ . They induce a subgraph that matches the left-hand side of rule R1. Thus, we orient the edge between $E$ and $F$ to $E\rightarrow F$ . No onsider the variables $C,\,E.$ , and $F$ . T ir ind ced subgraph matches the left-hand side of rule R2, so we now orient the edge between C and F to $C\rightarrow F$ . stage, if we consider the variables $E,\, F,\,G.$ , we can apply the rule R1, orient e edge $F\rightarrow G$ → . (Alternatively, we could have arrived at the same orientation using $C,\, F$ , and $G$ .) The resulting PDAG is shown in ﬁgure 3.13c. 

It seems fairly obvious that this algorithm is guaranteed to be sound: Any edge that is oriented by this procedure is, indeed, directed in exactly the same way in all of the members of the equivalence class. Much more surprising is the fact that it is also complete: Repeated application of these three local rules is guaranteed to capture all edge orientations in the equivalence class, without the need for additional global constraints. More precisely, we can prove that this algorithm produces the correct class PDAG for the distribution $P$ : 

Theorem 3.10 Let P be a distribution that s a $P\!\!\cdot\!\!$ -map $\mathcal{G}^{\ast}$ , and let K be the PDAG returned by Build-PDAG $(\mathcal{X}, P)$ . Then, K is a class PDAG of G $\mathcal{G}^{\ast}$ . The proof of this theorem can be decomposed into several aspects of correctness. We have already established the correctness of the skeleton found by Build-PMap-Skeleton . Thus, it remains to show that the directionality of the edges is correct. Speciﬁcally, we need to establish three basic facts: 

• Acyclicity: The graph returned by Build-PDAG $\scriptstyle (\mathcal{X}, P)$ is acyclic. 

• Soundness: If $X\rightarrow Y\in{\mathcal{K}}$ , then $X\rightarrow Y$ appears in all DAGs in $\mathcal{G}^{*}$ ’s I-equivalence class.

 • Com f $X{-}Y\in{\mathcal{K}}$ , then we can ﬁnd a DAG $\mathcal{G}$ that is I-equivalent to $\mathcal{G}^{*}$ such that $X\rightarrow Y\in{\mathcal{G}}$ . 

The last condition establishes completeness, since there is no constraint on the direction of the arc. In other words, the same condition can be used to prove the existence of a graph with $X\rightarrow Y$ and of a graph with $Y\rightarrow X$ . Hence, it shows that either direction is possible within the equivalence class. We begin with the soundness of the procedure. 

Proposition 3.3 

Let $P$ b n that $P\!\!\cdot\!\!$ map $\mathcal{G}^{\ast}$ , and let K be the graph returned by Build- AG $(\mathcal{X}, P)$ . Then, if $X\rightarrow Y\in{\mathcal{K}}$ , then $X\rightarrow Y$ appears in all DAGs in the I-equivalence class of $\mathcal{G}^{\ast}$ . 

The proof is left as an exercise (exercise 3.23). Next, we consider the acyclicity of the graph. We start by proving a property of graphs returned by the procedure. (Note that, once we prove that the graph returned by the procedure is the correct PDAG, it will follow that this property also holds for class PDAGs in general.) 

Proposition 3.4 

Proposition 3.5 

$\mathcal{K}$ graph returned by Build-PDAG . Then, if $X\,\rightarrow\, Y\,\in\,{\mathcal{K}}$ and $Y{-}Z\;\in\;{\mathcal{K}},$ , then $X\rightarrow Z\in{\mathcal{K}}$ . 

The proof is left as an exercise (exercise 3.24). 

Let $\mathcal{K}$ be the chain graph returned by Build-PDAG . Then $\mathcal{K}$ is acyclic. 

Proof Suppose, by way of contradiction, that $\mathcal{K}$ contains a cycle. That is, there is a (partially) directed path $X_{1}\rightleftharpoons X_{2}\rightleftharpoons\ldots\rightleftharpoons X_{n}\rightleftharpoons X_{1}$ . Without loss of generality, assume that this path is the shortest cycle in $\mathcal{K}$ . We claim that the path c directed edge. To see that, suppose that the the triplet $X_{i}\,\rightarrow\, X_{i+1}{-}X_{i+2}$ → . Then, invoking proposit 4, we have that $X_{i}\,\rightarrow\, X_{i+2}\,\in\,{\mathcal{K}}$ → nd thus, we can construct a shorter path $X_{i+1}$ the edge $X_{i}\,\rightarrow\, X_{i+2}$ → . At this stage, we have a directed cycle $X_{1}\,\rightarrow\, X_{2}\,\rightarrow\,.\,.\,.\, X_{n}\,\rightarrow\, X_{1}$ → → → . Using proposition 3.3, we nclude that this cycle appears in any DAG in the quivalence class, and in rticular in G $\mathcal{G}^{\ast}$ . This conclusion contradicts the assumption that G $\mathcal{G}^{\ast}$ is acyclic. It follows that K is acyclic. 

The ﬁnal step is the completeness proof. Again, we start by examining a property of the graph $\mathcal{K}$ . 

Proposition 3.6 

The PDAG $\mathcal{K}$ returned by Build-PDAG is necessarily chordal. 

The proof is left as an exercise (exercise 3.25). 

This property a ws us to characterize the structure of the PDAG $\mathcal{K}$ returned by Build-PDAG . Recall that, since K is an undirected chain graph, we can partition X into chain components $K_{1},\dots, K_{\ell}$ , where each chain component contains variables that are connected by undirected edges (see deﬁnition 2.21). It turns out that, in an undirected chordal graph, we can orient any edge in any direction without creating an immorality. 

Let $\mathcal{K}$ be a undirected chordal graph over $\mathcal{X}$ , and let $X, Y\in{\mathcal{X}}$ . Then, there is a DAG $\mathcal{G}$ such that 

(a) The skeleton of $\mathcal{G}$ is $\mathcal{K}$ . (b) $\mathcal{G}$ does not contain immoralities. (c) $X\rightarrow Y\in{\mathcal{G}}$ . 

The proof of this proposition requires some additional machinery that we introduce in chapter 4, so we defer the proof to that chapter. 

Using this proposition, we see that we can orient edges in the chain component $K_{j}$ without introducing immoralities within the component. We still need to ensure that orienting an edge $X{-}Y$ within a component cannot introduce an immorality involving edges from outside the component. To s tuation cannot occur, suppose we orient the edge $X\,\rightarrow\, Y$ , and suppose that Z $Z\,\rightarrow\, Y\,\in\,{\mathcal{K}}$ → ∈K . eems poten l immorality. Ho pplying oposition 3.4, we se that s ce $Z\rightarrow Y$ → an $Y{-}X$ n K , then so must be $Z\rightarrow X$ → . Since $Z$ is a parent of both X and $Y$ , we have that $X\rightarrow Y\leftarrow Z$ → ← is not an immorality. This argument applies to any edge we orient within an undirected component, and thus no new immoralities are introduced. 

With these tools, we can complete the completeness proof of Build-PDAG . 

Le $P$ ribution that has a $P\!\!\cdot\!\!$ -map $\mathcal{G}^{\ast}$ , and t $\mathcal{K}$ be the gra returned by $(\mathcal{X}, P)$ . If $X{-}Y\in{\mathcal{K}}$ ∈K , then we can ﬁnd a DAG G that is I-equivalent to G $\mathcal{G}^{\ast}$ such that $X\rightarrow Y\in{\mathcal{G}}$ → ∈G . 

roof Suppose we have an undirected edge $X{-}Y\in{\mathcal{K}}$ . We w that there is a DAG $\mathcal{G}$ G that has the same skeleton and immoralities as K such that $X\rightarrow Y\in{\mathcal{G}}$ → ∈G . If can build such a graph $\mathcal{G}$ , then clearly it is in the I-equivalence class of $\mathcal{G}^{\ast}$ . 

The construction is simple. We start with the chain component that contains $X{-}Y$ , and use proposition 3.7 to orient the edges in the component so that $X\rightarrow Y$ is in the resulting DAG. Then, we use the same construction to orient all other chain components. Since the chain components are ordered and acyclic, and our orientation of each chain component is acyclic, the resulting directed graph is acyclic. Moreover, as shown, the new orientation in each component does not introduce immoralities. Thus, the resulting DAG has exactly the same skeleton and immoralities as $\mathcal{K}$ . 

## 3.5 Summary 
In this chapter, we discussed the issue of specifying a high-dimensional joint distribution com- pactly by exploiting its independence properties. We provided two complementary deﬁnitions of a Bayesian network. The ﬁrst is as a directed graph $\mathcal{G}$ , annotated with a set of conditional probability distributions $P (X_{i}\mid\mathrm{Pa}_{X_{i}})$ . The network together with the PDs deﬁne a di ribu- tion via the chain rule for Bayesian networks. In this case, we say that P factorizes over G . We also deﬁned the independence assumptions associated with the graph: the local independencies, the set of basic independence assumptions induced by the network structure; and the larger set of global independencies that are derived from the d-separation criterion. We showed the equivalence of these three f ndamental notions: $P$ facto es over $\mathcal{G}$ if and only if $P$ satisﬁes the local independencies of G , which holds if and only if P satisﬁes the global independencies derived from d-separation. This result shows the equivalence of our two views of a Bayesian network: as a scafolding for factoring a probability distribution $P$ , and as a representation of a set of independence assumptions that hold for $P$ . We also showed that the set of independen- cies derived from d-separation is a complete characterization of the independence properties that are implied by the graph structure alone, rather than by properties of a speciﬁc distribution over $\mathcal{G}$ . 
> 本章讨论了利用独立性质表示高维度联合分布的问题
> 我们提供了两个关于贝叶斯网络的互补定义，一个是有向图 $\mathcal G$ 和一系列条件概率分布 $P (X_i \mid \text{Pa}_{X_i})$，另一个是和网络相关的一系列独立性假设：局部独立性，即直接由网络结构推出的独立性假设集合；全局独立性，即由网络的 d-seperation 条件推出的独立性集合
> 我们说明了三个基本概念的等价：$P$ 根据 $\mathcal G$ 分解当且仅当 $P$ 满足 $\mathcal G$ 的局部独立性，which holds if and only if $P$ 满足从 d-seperation 中推导出的全局独立性
> 因此贝叶斯网络的两个定义是等价的：作为分解 $P$ 的 scaffolding 和作为 $P$ 中保持的一系列独立性假设的表示

We deﬁned a set of basic notions that use the characterization of a graph as a set of indepen- dencies. We deﬁned the notion of a minimal I-map and showed that almost every distribution has multiple minimal I-maps, but that a minimal I-map for $P$ does not necessarily capture all of the independence properties in $P$ . We then deﬁned a more stringent notion of a perfect map , and showed that not every distribution has a perfect map. We deﬁned $I^{,}$ -equivalence , which captures an independence-equivalence relationship between two graphs, one where they specify precisely the same set of independencies. 

Finally, we deﬁned the notion of a class PDAG , a partially directed graph that provides a compact representation for an entire I-equivalence class, and we provided an algorithm for constructing this graph. 

These deﬁnitions and results are fundamental properties of the Bayesian network represen- tation and its semantics. Some of the algorithms that we discussed are never used as is; for example, we never directly use the procedure to ﬁnd a minimal I-map given an explicit rep- resentation of the distribution. However, these results are crucial to understanding the cases where we can construct a Bayesian network that reﬂects our understanding of a given domain, and what the resulting network means. 

# 4 Undirected Graphical Models 
So far, we have dealt only with directed graphical models, or Bayesian networks. These models are useful because both the structure and the parameters provide a natural representation for many types of real-world domains. In this chapter, we turn our attention to another important class of graphical models, deﬁned on the basis of undirected graphs. 

As we will see, these models are useful in modeling a variety of phenomena where one cannot naturally ascribe a directionality to the interaction between variables. Furthermore, the undirected models also ofer a diferent and often simpler perspective on directed models, in terms of both the independence structure and the inference task. We also introduce a combined framework that allows both directed and undirected edges. We note that, unlike our results in the previous chapter, some of the results in this chapter require that we restrict attention to distributions over discrete state spaces. 

## 4.1 The Misconception Example 
To motivate our discussion of an alternative graphical representation, let us reexamine the Misconception example of section 3.4.2 (example 3.8). In this example, we have four students who get together in pairs to work on their homework for a class. The pairs that meet are shown via the edges in the undirected graph of ﬁgure 3.10a. 

As we discussed, we intuitively want to model a distribution that satisﬁes $(A\perp C\mid\{B,D\})$ and $(B\,\perp\,D\,\mid\,\{A,C\})$ , but no other independencies. As we showed, these independencies cannot be naturally captured in a Bayesian network: any Bayesian network I-map of such a distribution would necessarily have extraneous edges, and it would not capture at least one of the desired independence statements. More broadly, a Bayesian network requires that we ascribe a directionality to each inﬂuence. In this case, the interactions between the variables seem symmetrical, and we would like a model that allows us to represent these correlations without forcing a speciﬁc direction to the inﬂuence. 
> 变量之间的交互是对称的情况下，我们考虑用无向边建模，以不指定变量之间影响的特定方向

A representation that implements this intuition is an undirected graph. As in a Bayesian network, the nodes in the graph of a Markov network represent the variables, and the edges correspond to a notion of direct probabilistic interaction between the neighboring variables — an interaction that is not mediated by any other variable in the network. In this case, the graph of ﬁgure 3.10, which captures the interacting pairs, is precisely the Markov network structure that captures our intuitions for this example. As we will see, this similarity is not an accident. 
> 这对应的网络就是 Markov 网络，它和贝叶斯网络类似，节点表示变量，边表示变量之间的直接交互

The remaining question is how to parameterize this undirected graph. Because the interaction is not directed, there is no reason to use a standard CPD, where we represent the distribution over one node given others. Rather, we need a more symmetric parameter iz ation. Intuitively, what we want to capture is the afnities between related variables. For example, we might want to represent the fact that Alice and Bob are more likely to agree than to disagree. We associate with $A,B$ a general-purpose function, also called a factor : 
> 无向图中，影响是无向的，因此标准 CPD 的语义不符合
> 我们需要对称的参数化，我们需要捕获相关变量的亲近性
> 我们使用一种通用目的的函数来关联变量 $A, B$，称其为因子：

**Definition 4.1**
Let $D$ be a set of random variables. We deﬁne $a$ factor $\phi$ to be a function from $V a l(D)$ to I R . A factor is nonnegative if all its entries are nonnegative. The set of variables $_D$ is called the scope of the factor and denoted $Scope[φ]$. 
> 定义：
> $D$ 为随机变量集合，定义因子 $\phi$ 是从 $Val (D)$ 到 $\mathbb R$ 的函数，如果因子的所有项都非负，因子就是非负的
> 变量集合 $D$ 称为因子的作用域，记作 $Scope[\phi]$

Unless stated otherwise, we restrict attention to nonnegative factors. 
> 除非明确说明，我们都关注非负因子

In our example, we e a factor $\phi_{1}(A,B):V a l(A,B)\mapsto I\!\!R^{+}$ . The value associated with a particular assignment a, b denotes the afnity between these two values: the higher the value $\phi_{1}(a,b)$ , the more compatible these two values are. 
> 例如，我们有因子 $\phi_1 (A, B): Val (A, B)\mapsto \mathbb R^+$，$\mathbb R^+$ 中的值和特定的 $a, b$ 赋值关联，表示这两个值之间的亲密性，值越高，二者越相容

Figure 4.1a shows one possible compatibility factor for these variables. Note that this factor is not normalized; indeed, the entries are not even in $[0,1]$ . Roughly speaking, $\phi_{1}(A,B)$ asserts that it is more likely that Alice and Bob agree. It also adds more weight for the case where they are both right than for the case where they are both wrong. This factor function also has the property that $\phi_{1}(a^{1},b^{0})\,<\,\phi_{1}(a^{0},b^{1})$ . Thus, if they disagree, there is less weight for the case where Alice has the misconception but Bob does not than for the converse case. 

In a similar way, we deﬁne a compatibility factor for each other interacting pair: $\{B,C\}$ , $\{C,D\}$ , and $\{A,D\}$ . Figure 4.1 shows one possible choice of factors for all four pairs. For example, the factor over $C,D$ represents the compatibility of Charles and Debbie. It indicates that Charles and Debbie argue all the time, so that the most likely instantiations are those where they end up disagreeing. 
> 我们为每一个直接交互的变量对都定义相容因子

As in a Bayesian network, the parameter iz ation of the Markov network deﬁnes the local interactions between directly related variables. To deﬁne a global model, we need to combine these interactions. As in Bayesian networks, we combine the local models by multiplying them. Thus, we want $P(a,b,c,d)$ to be $\phi_{1}(a,b)\cdot\phi_{2}(b,c)\cdot\phi_{3}(c,d)\cdot\phi_{4}(d,a)$ .  In this case, however, we have no guarantees that the result of this process is a normalized joint distribution. Indeed, in this example, it deﬁnitely is not. Thus, we deﬁne the distribution by taking the product of  the local factors, and then normalizing it to deﬁne a legal distribution. Speciﬁcally, we deﬁne 

$$
P(a,b,c,d)=\frac{1}{Z}\phi_{1}(a,b)\cdot\phi_{2}(b,c)\cdot\phi_{3}(c,d)\cdot\phi_{4}(d,a),
$$ 
where 

$$
Z=\sum_{a,b,c,d}\phi_{1}(a,b)\cdot\phi_{2}(b,c)\cdot\phi_{3}(c,d)\cdot\phi_{4}(d,a)
$$ 
is a normalizing constant known as the partition function . 
> 定义了局部的交互之后，我们需要将这些交互结合以定义全局的模型
> 和贝叶斯网络中一样，我们通过将局部模型相乘来对它们进行结合，即 $P (a, b, c, d) = \phi_1 (a, b) \cdot \phi_2 (b, c)\cdot \phi_3 (c, d)\cdot \phi_4 (d, a)$
> 但我们不能保证得到规范化的联合分布，为此需要进行规范化，因此我们定义
> $P (a, b, c, d) = \frac 1 Z\phi_1 (a, b) \cdot \phi_2 (b, c)\cdot \phi_3 (c, d)\cdot \phi_4 (d, a)$，
> 其中 $Z$ 是规范化常数，$Z = \sum_{a, b, c, d}\phi_1 (a, b)\cdot\phi_2 (b, c)\cdot \phi_3 (c, d)\cdot \phi_4 (d, a)$，称其为划分函数

The term “partition” originates from the early history of Markov networks, which originated from the concept of Markov random ﬁeld (or MRF ) in statistical physics (see box 4.C); the “function” is because the value of $Z$ is a function of the parameters, a dependence that will play a signiﬁcant role in our discussion of learning. 
> $Z$ 也是其参数的函数，因此称为划分函数

In our example, the unnormalized measure (the simple product of the four factors) is shown in the next-to-last column in ﬁgure 4.2. For example, the entry corresponding to $a^{1},b^{1},c^{0},d^{1}$ is obtained by multiplying: 

$$
\begin{array}{r}{\phi_{1}(a^{1},b^{1})\cdot\phi_{2}(b^{1},c^{0})\cdot\phi_{3}(c^{0},d^{1})\cdot\phi_{4}(d^{1},a^{1})=10\cdot1\cdot100\cdot100=100,000.}\end{array}
$$ 
The last column shows the normalized distribution. 

We can use this joint distribution to answer queries, as usual. For example, by summing out $A,\,C$ , and $D$ , we obtain $P(b^{1})\approx0.732$ and $P(b^{0})\approx0.268$ ; that is, Bob is 26 percent likely to have the misconception. On the other hand, if we now observe that Charles does not have the misconception $(c^{0})$ , we obtain $P(b^{1}\mid c^{0})\approx0.06$ . 

The beneﬁt of this representation is that it allows us great ﬂexibility in representing inter- actions between variables. For example, if we want to change the nature of the interaction between $A$ and $B$ , we can simply modify the entries in that factor, without having to deal with normalization constraints and the interaction with other factors. The ﬂip side of this ﬂexibility, as we will see later, is that the efects of these changes are not always intuitively understandable. 
> 该表示使得我们在表示变量之间的交互时具有极大的灵活性，变量之间的因子可以任意定义，并且如果有两个变量的交互方式变化，可以直接修改其因子，不会影响其他因子

As in Bayesian networks, there is a tight connection between the factorization of the distribution and its independence properties. The key result here is stated in exercise 2.5: $P\models(X\ \bot\ Y\ |\ Z)$ if and only if we can write $P$ in the form $P(\mathcal{X})=\phi_{1}(\boldsymbol{X},Z)\phi_{2}(\boldsymbol{Y},Z)$ . In our example, the structure of the factors allows us to decompose the distribution in several ways; for example: 

$$
P(A,B,C,D)=\left[\frac{1}{Z}\phi_{1}(A,B)\phi_{2}(B,C)\right]\phi_{3}(C,D)\phi_{4}(A,D).
$$ 
From this decomposition, we can infer that $P\models(B\ \bot\ D\ |\ A,C)$  . We can similarly infer that $P\models(A\bot C\mid B,D)$ . 
> Markov 网络的分解和其表示的分布中的独立性同样存在联系：
> $P\vDash (\pmb X \perp \pmb Y\mid \pmb Z)$ 当且仅当我们可以将 $P$ 写为 $P (\mathcal X) = P(\pmb X, \pmb Y, \pmb Z) = \phi_1 (\pmb X, \pmb Z)\phi_2 (\pmb Y , \pmb Z)$

These are precisely the two independencies that we tried, unsuccessfully, to achieve using a Bayesian network, in example 3.8. Moreover, these properties correspond to our intuition of “paths of inﬂuence” in the graph, where we have that $B$ and $D$ are separated given $A,C$ , and that $A$ and $C$ are separated given $B,D$ . Indeed, as in a Bayesian network, independence properties of the distribution $P$ correspond directly to separation properties in the graph over which $P$ factorizes. 
> 和贝叶斯网络一样，分布 $P$ 分解的 Markov 网络中的分离性质直接对应了 $P$ 中的独立性质

## 4.2 Parameterization 
We begin our formal discussion by describing the parameter iz ation used in the class of undi- rected graphical models that are the focus of this chapter. In the next section, we make the connection to the graph structure and demonstrate how it captures the independence properties of the distribution. 

To represent a distribution, we need to associate the graph structure with a set of parameters, in the same way that CPDs were used to parameterize the directed graph structure. However, the parameter iz ation of Markov networks is not as intuitive as that of Bayesian networks, since the factors do not correspond either to probabilities or to conditional probabilities. As a con- sequence, the parameters are not intuitively understandable, making them hard to elicit from people. As we will see in chapter 20, they are also signiﬁcantly harder to estimate from data. 
> 要表示分布，我们需要将图结构和一组参数关联
> 有向图中，我们使用 CPDs 进行参数化
> 无向图中，因子并不直接对应概率分布或条件概率分布，因此参数并不是直觉上可以理解的

### 4.2.1 Factors 
A key issue in parameterizing a Markov network is that the representation is undirected, so that the parameteriz ation cannot be directed in nature. We therefore use factors, as deﬁned in deﬁnition 4.1. Note that a factor subsumes both the notion of a joint distribution and the notion of a CPD. A joint distribution over $_D$ is a factor over $_D$ : it speciﬁes a real number for every assignment of values of $_D$ . A conditional distribution $P(X\mid U)$ is a factor over $\{X\}\cup U$ . However, both CPDs and joint distributions must satisfy certain normalization constraints (for example, in a joint distribution the numbers must sum to 1), whereas there are no constraints on the parameters in a factor. 
>  Markov 网络表示是无向的，故参数化本质也应该无向，即因子
>  因子实际包含了联合分布和 CPD 的概念，$\pmb D$ 上的联合分布也是 $\pmb D$ 上的因子，条件概率分布 $P (X\mid \pmb U)$ 也是 $\{X\} \cup \pmb U$ 上的因子，区别仅在于规范化

As we discussed, we can view a factor as roughly describing the “compatibilities” between diferent values of the variables in its scope. We can now parameterize the graph by associating a set of a factors with it. One obvious idea might be to associate parameters directly with the edges in the graph. However, a simple calculation will convince us that this approach is insufcient to parameterize a full distribution. 

Example 4.1 
Consider a fully connected graph over $\mathcal{X}$ ; in this case, the graph speciﬁes no conditional ind en- dence assumptions, so that we should be able to specify an arbitrary joint distribution over X . If all of the variables are binary, each factor over an edge would have 4 parameters, and the total number of parameters in the graph would be $4{\binom{n}{2}}$ . However, the number of parameters required to specify a joint distribution over $n$ binary variables is $2^{n}-1$ . Thus, pairwise factors simply do not have enough parameters to encompass the space of joint distributions. More intuitively, such factors capture only the pairwise interactions, and not interactions that involve combinations of values of larger subsets of variables. 
> 考虑 $\mathcal X$ 上的完全连接图，该图没有指定条件独立性假设，因此我们可以指定 $\mathcal X$ 上的任意联合分布
> 如果变量都为2元，两个变量之间的因子需要的参数量是4，指定全部的因子的参数量是 $4\binom {n} {2}$，而制定 $n$ 个二元变量的任意联合分布的参数量实际应该为 $2^n - 1$
> 故仅仅使用成对的因子是不足以有足够的参数覆盖整个联合分布的空间的
> 直观上，这类因子仅捕获了成对的交互，不能表示更大子集的变量之间的组合交互

A more general representation can be obtained by allowing factors over arbitrary subsets of variables. To provide a formal deﬁnition, we ﬁrst introduce the following important operation on factors. 
> 通过允许因子包含任意数量的变量，可以得到更通用的表示

**Deﬁnition 4.2** factor product 
Let $X$ , $Y$ , and $Z$ be three disjoint sets of variables, and let $\phi_{1}(X,Y)$ and $\phi_{2}(Y,Z)$ be two factors. We deﬁne the factor product $\phi_{1}\times\phi_{2}$ to be a factor $\psi:V a l(X,Y,Z)\mapsto I\!\!R$ as follows: 

$$
\psi(\pmb X,\pmb Y,\pmb Z)=\phi_{1}(\pmb X,\pmb Y)\cdot\phi_{2}(\pmb Y,\pmb Z).
$$

> 定义：
> $\pmb X, \pmb Y, \pmb Z$ 是三个不相交变量集合，令 $\phi_1 (\pmb X, \pmb Y), \phi_2 (\pmb Y, \pmb Z)$ 为两个因子
> 定义因子积 $\phi_1 \times \phi_2$ 为因子 $\psi: Val (\pmb X, \pmb Y, \pmb Z)\to \mathbb R$ 如上

![[Probabilistic Graph Theory-Fig4.3.png]]

The key aspect to note about this deﬁnition is the fact that the two factors $\phi_{1}$ and $\phi_{2}$ are multiplied in a way that “matches up” the common part $Y$ . Figure 4.3 shows an example of the product of two factors. We have deliberately chosen factors that do not correspond either to probabilities or to conditional probabilities, in order to emphasize the generality of this operation. 
> 两个因子 $\phi_1, \phi_2$ 通过“匹配”共同部分 $\pmb Y$ 而相乘

As we have already observed, both CPDs and joint distributions are factors. Indeed, the chain rule for Bayesian networks deﬁnes the joint distribution factor as the product of the CPD factors. For example, when computing $P(A,B)=P(A)P(B\mid A)$ , e always multiply entries in the $P(A)$ and $P(B\mid A)$ tables that have the same value for A . 
> 在定义上，CPDs 和联合分布都是因子，贝叶斯网络的链式规则就是定义了联合分布因子可以作为 CPD 因子的乘积
> 例如，计算 $P (A, B) = P (A) P (B\mid A)$ 时，我们总是将 $P (A), P (B\mid A)$ 表中具有相同 $A$ 值的项相乘

Thus, letting $\phi_{X_{i}}(X_{i},\mathrm{Pa}_{X_{i}})$ represent $P(X_{i}\mid\mathrm{Pa}_{X_{i}})$ , we have that 
> 令 $\phi_{X_i}(X_i, \text{Pa}_{X_i})$ 表示 $P (X_i \mid \text{Pa}_{X_i})$，我们有

$$
P(X_{1},.\,.\,.\,,X_{n})=\prod_{i}\phi_{X_{i}}.
$$ 
### 4.2.2 Gibbs Distributions and Markov Networks 
We can now use the more general notion of factor product to deﬁne an undirected parameterization of a distribution. 
> 我们使用因子乘积的更通用概念来定义一个分布的无向参数化

**Deﬁnition 4.3**  Gibbs distribution 
A distribution $P_{\Phi}$ is $a$ Gibbs distribution parameterized by a set of factors $\Phi=\{\phi_{1}(D_{1}),.\,.\,.\,,\phi_{K}(D_{K})\}$ if it is deﬁned as follows: 
> 定义：
> 定义吉布斯分布 $P_{\Phi}$，由因子集合 $\Phi = \{\phi_1(\pmb D_1), \dots, \phi_K(\pmb D_K)\}$ 参数化，满足：

$$
P_{\Phi}(X_{1},.\,.\,.\,,X_{n})=\frac{1}{{Z}}\tilde{P}_{\Phi}(X_{1},.\,.\,.\,,X_{n}),
$$

where 

$$
\tilde{P}_{\Phi}(X_{1},\dots,X_{n})=\phi_{1}(\pmb D_{1})\times\phi_{2}(\pmb D_{2})\times\cdot\cdot\times\phi_{m}(\pmb D_{m})
$$ 
is an unnormalized measure and 

$$
Z=\sum_{X_{1},\ldots,X_{n}}\tilde{P}_{\Phi}(X_{1},\ldots,X_{n})
$$ 
is a normalizing constant called the partition function . 
> 即 $\tilde P_{\Phi}(X_1,\dots, X_n)$ 定义为 $\phi_1 (\pmb D_1), \dots, \phi_m (\pmb D_m)$ 的乘积，$Z$ 定义为 $\tilde P_{\Phi}$ 对于所有情况的求和（称为划分函数），$P_{\Phi}$ 定义为 $\tilde P_\Phi$ 规范化后得到的分布

It is tempting to think of the factors as representing the marginal probabilities of the variables in their scope. Thus, looking at any individual factor, we might be led to believe that the behavior of the distribution deﬁned by the Markov network as a whole corresponds to the behavior deﬁned by the factor. However, this intuition is overly simplistic. **A factor is only one contribution to the overall joint distribution. The distribution as a whole has to take into consideration the contributions from all of the factors involved.** 
> 概率图模型中，因子通常代表变量之间相互作用的方式，它们共同定义了一个联合概率分布
> 马尔可夫网络中的单个因子只是对整体联合分布的一个贡献，整个分布必须考虑到所有相关因子的贡献
> 虽然单个因子可能反映了其作用域内变量的某种形式的概率分布，但是整个马尔可夫网络的联合概率分布是由所有因子共同决定的，没有一个单独的因子可以完全描述整个系统的概率行为，因为每个因子都只描述了部分变量之间的交互作用，而忽略了其他因子的影响

Example 4.2
Consider the distribution of ﬁgure 4.2. The marginal distribution over $A,B$ , is  The most likely conﬁguration is the one where Alice and Bob disagree. By contrast, the highest entry in the factor $\phi_{1}(A,B)$ in ﬁgure 4.1 corresponds to the assignment $a^{0},b^{0}$ . The reason for the discrepancy is the inﬂuence of the other factors on the distribution. In particular, $\phi_{3}(C,D)$ asserts that Charles and Debbie disagree, whereas $\phi_{2}(B,C)$ and $\phi_{4}(D,A)$ assert that Bob and Charles agree and that Debbie and Alice agree. Taking just these factors into consideration, we would conclude that Alice and Bob are likely to disagree. In this case, the “strength” of these other factors is much stronger than that of the $\phi_{1}(A,B)$ factor, so that the inﬂuence of the latter is overwhelmed. 

We now want to relate the parameter iz ation of a Gibbs distribution to a graph structure. If our parameter iz ation contains a factor whose scope contains both $X$ and $Y$ , we are introducing a direct interaction between them. Intuitively, we would like these direct interactions to be represented in the graph structure. Thus, if our parameter iz ation contains such a factor, we would like the associated Markov network structure $\mathcal{H}$ to contain an edge between $X$ and $Y$ . 
> 我们准备将 Gibbs 分布的参数化和一个图结构联系
> 如果 Gibbs 分布的参数化中包含了一个因子，其作用域包含了 $X, Y$，说明二者是有直接联系的，直观上，就是在图中有一条边相连

**Deﬁnition 4.4** Markov network factorization
We say that a distribution $P_{\Phi}$ with $\Phi\:=\:\{\phi_{1}(D_{1}),.\,.\,.\,,\phi_{K}(D_{K})\}$ factorizes over a Markov network $\mathcal H$ if each $D_{k}$ $(k=1,.\,.\,.\,,K)$ is a complete subgraph of $\mathcal H$.
> 定义：
> 对于分布 $P_\Phi$，其因子集合为 $\{\phi_1(\pmb D_1), \dots, \phi_K(\pmb D_K)\}$，如果满足每个 $\pmb D_k(k = 1,\dots, K)$ 都是 $\mathcal H$ 的一个完全子图，则称 $P_\Phi$ 在 Markov 网络 $\mathcal H$ 上分解

The factors that parameterize a Markov network are often called clique potentials . 
> 参数化了 Markov 网络的因子 $\phi_i$ 常常称为团势函数（团即完全子图的节点构成的集合）

As we will see, if we associate factors only with complete subgraphs, as in this deﬁnition, we are not violating the independence assumptions induced by the network structure, as deﬁned later in this chapter. 
> 在定义4.4中，我们将因子仅和完全子图关联，这不会违反由网络结构推导出的独立性假设

Note that, because every complete subgraph is a subset of some (maximal) clique, we can reduce the number of factors in our parameter iz ation by allowing factors only for maximal cliques. 
> 每个完全子图都是某个最大团/最大完全子图的子集，可以通过仅留下表示最大团的因子来减少参数化中的因子数量

More precisely, let $C_{1},\ldots,C_{k}$ be the cliques in $\mathcal{H}$ . We can parameterize $P$ using a set of factors $\phi_{1}(C_{1}),.\,.\,.\,,\phi_{l}(C_{l})$ . Any factorization in terms of complete subgraphs can be converted into this form simply by assigning each factor to a clique that encompasses its scope and multiplying all of the factors assigned to each clique to produce a clique potential. In our Misconception example, we have four cliques: $\{A,B\},\,\{B,C\},\,\{C,D\}$ , and $\{A,D\}$ . Each of these cliques can have its own clique potential. One possible setting of the parameters in these clique potential is shown in ﬁgure 4.1. Figure 4.4 shows two examples of a Markov network and the (maximal) cliques in that network. 
> 具体地说，令 $\pmb C_1, \dots, \pmb C_k$ 为 $\mathcal H$ 中的团，我们使用因子集合 $\phi_1 (\pmb C_1), \dots, \phi_l (\pmb C_l)$ 来参数化 $P$
> 我们只需要将每个因子和包含了它的作用域内的变量的团关联，然后将各个团得到的因子相乘，就得到了这个团的团势函数

**Although it can be used without loss of generality, the parameter iz ation using maximal clique potentials generally obscures structure that is present in the original set of factors.** For example, consider the Gibbs distribution described in example 4.1. Here, we have a potential for every pair of variables, so the Markov network associated with this distribution is a single large clique containing all variables. If we associate a factor with this single clique, it would be exponentially large in the number of variables, whereas the original parameter iz ation in terms of edges requires only a quadratic number of parameters. See section 4.4.1.1 for further discussion. 
> 使用最大团势函数的参数化虽然不会失去一般性，但这通常容易掩盖原始因子集合中的结构
> 例如 example 4.1中，对于每一对变量，它们都可以有一个团势函数/因子，而完整的 Markov 网络则关联了完整的分布，网络本身也是一个团，故完整的分布也可以由单个团势函数/因子表示
> 但要用单个因子表示整个网络，其参数数量和变量数量呈指数关系，而原来的按照边的参数化仅和变量数量呈二次关系

Box 4.A — Concept: Pairwise Markov Networks. 
A subclass of Markov networks that arises in many contexts is that of pairwise Markov networks , representing distributions where all of the factors are over single variables or pairs of variables. More precisely, $a$ pairwise Markov network over $a$ graph $\mathcal{H}$ is associated with a set of node potentials $\{\phi(X_{i}):i=1,.\,.\,.\,,n\}$ and a set of edge potentials $\{\phi(X_{i},X_{j})\,:\,(X_{i},X_{j})\,\in\,\mathcal{H}\}$ . The overall distribution is (as always) the normalized product of all of the potentials (both node and edge). Pairwise MRFs are attractive because of their simplicity, and because interactions on edges are an important special case that often arises in practice (see, for example, box 4.C and box 4.B). 

A class of pairwise Markov networks that often arises, and that is commonly used as a benchmark for inference, is the class of networks structured in the form of a grid, as shown in ﬁgure 4.A.1. As we discuss in the inference chapters of this book, although these networks have a simple and compact representation, they pose a signiﬁcant challenge for inference algorithms. 

### 4.2.3 Reduced Markov Networks 
We end this section with one ﬁnal concept that will prove very useful in later sections. Consider the process of conditioning a distribution on some assignment $\mathbfit{u}$ to some subset of variables $U$ . Conditioning a distribution corresponds to eliminating all entries in the joint distribution that are inconsistent with the event $\pmb{U}=\pmb{u}$ , and renormalizing the remaining entries to sum to 1. Now, consider the case where our distribution has the form $P_{\Phi}$ for some set of factors $\Phi$ . Each entry in the unnormalized measure $\tilde{P}_{\Phi}$ is a product of entries from the factors $\Phi$ , one entry from each factor. If, in some factor, we have an entry that is inconsistent with $\pmb{U}=\pmb{u}$ , it will only contribute to entries in $\tilde{P}_{\Phi}$ that are also inconsistent with this event. Thus, we can eliminate all such entries from every factor in $\Phi$ . 
> 考虑将一个分布条件于对于某个变量子集 $\pmb U$ 的某个赋值 $\pmb u$，这对应于消除联合分布中所有和事件 $\pmb U = \pmb u$ 不一致的项，然后将剩余的项重新规范化到和为1
> 现在，考虑我们分布形式为 $P_\Phi$，因子集合为 $\Phi$，我们知道未规范化的度量 $\tilde P_\Phi$ 中的每个项都是因子集合 $\Phi$ 中每个因子的各个项的乘积，每个因子共享一个项
> 此时，如果在某个因子中，我们有某个项和事件 $\pmb U = \pmb u$ 不一致，显然它只会对于 $\tilde P_\Phi$ 中和该事件不一致的项做出贡献，因此，我们应该消除 $\Phi$ 中所有因子中所有和该事件不一致的项

More generally, we can deﬁne: 

**Deﬁnition 4.5** factor reduction 
Let $\phi(Y)$ be a factor, $\pmb{U}={\pmb u}$ an assignment for $U\subseteq Y$ . We deﬁne the reduction of the factor $\phi$ to the context $\pmb{U}=\pmb{u}$ , denoted $\phi[U=u]$ (and abbreviated $\phi[{\pmb u}])$ , to be a factor over scope $Y^{\prime}=Y-U$ , such that 

$$
\phi[{\pmb u}]({\pmb y}^{\prime})=\phi({\pmb y}^{\prime},{\pmb u}).
$$ 
For $U\not\subset Y$ , we deﬁne $\phi[{\pmb u}]$ to be $\phi[U^{\prime}=\pmb{u}^{\prime}]$ , wher $U^{\prime}=U\cap Y$ , and ${\pmb u}^{\prime}\,=\,{\pmb u}\langle{\pmb U}^{\prime}\rangle$ , where $u\langle U^{\prime}\rangle$ denotes the assignment in u to the variables in $U^{\prime}$ . 
> 定义：
> $\phi (\pmb Y)$ 为因子，$\pmb U = \pmb u$ 是对于 $\pmb U\subseteq \pmb Y$ 的赋值，定义因子 $\phi$ 对于上下文 $\pmb U = \pmb u$ 的简化为一个在作用域 $\pmb Y' = \pmb Y - \pmb U$ 上的因子，记作 $\phi[\pmb U = \pmb u]$ 或者 $\phi[\pmb u]$，满足 $\phi [\pmb u](\pmb y') = \phi (\pmb y', \pmb u)$
> 对于 $\pmb U \not\subset \pmb Y$，定义 $\phi[\pmb u]$ 为 $\phi[\pmb U' = \pmb u']$，其中 $\pmb U' = \pmb U \cap \pmb Y$，$\pmb u' = \pmb u \langle \pmb U' \rangle$（$\pmb u\langle \pmb U' \rangle$ 表示 $\pmb u$ 中对于 $\pmb U'$ 中的变量中的赋值）

Figure 4.5 illustrates this operation, reducing the of ﬁgure 4.3 to the context $C=c^{1}$ . 
> 直观上看，缩减就是把上下文对应的因子中的项都筛选了出来

Now, consider a product of factors. An entry in the product is consistent with $\mathbfit{u}$ if and only if it is a product of entries that are all consistent with $\mathbfit{u}$ . We can therefore deﬁne: 
> 考虑因子的积：积中的一项要和 $\pmb u$ 一致当且仅当求积的所有项都和 $\pmb u$ 一致

**Deﬁnition 4.6** reduced Gibbs distribution 
Let $P_{\Phi}$ be a Gibbs distribution parameterized by $\Phi=\{\phi_{1},.\,.\,.\,,\phi_{K}\}$ and let $\mathbfit{u}$ be a context. The reduced Gibbs distribution $P_{\Phi}[{\pmb u}]$ is the Gibbs distribution deﬁned by the set of factors $\Phi[{\pmb u}]=$ $\{\phi_{1}[{\pmb u}],.\,.\,.\,,\phi_{K}[{\pmb u}]\}$ . 
> 定义：
> $P_\Phi$ 为 Gibbs 分布，由 $\Phi = \{\phi_1, \dots, \phi_K\}$ 参数化，令 $\pmb u$ 为上下文
> 简化的 Gibbs 分布 $P_\Phi[\pmb u]$ 为由因子集合 $\Phi[\pmb u] = \{\phi_1[\pmb u], \dots, \phi_K[\pmb u]\}$ 参数化的 Gibbs 分布
> 直观上看，就是把每个因子都根据上下文筛选了一遍

Reducing the set of factors deﬁning $P_{\Phi}$ to some context $\mathbfit{u}$ corresponds directly to the opera- tion of conditioning $P_{\Phi}$ on the observation $\mathbfit{u}$ . More formally: 
> 将定义 $P_\Phi$ 的因子集合根据上下文 $\pmb u$ 简化对应于将 $P_\Phi$ 条件于观测 $\pmb u$

**Proposition 4.1** Let $P_{\Phi}(X)$ be a Gibbs distribution. Then $P_{\Phi}[{\pmb u}]=P_{\Phi}({\pmb W}\mid{\pmb u})$ where $W=X-U$ . 
> 命题：
> 令 $P_{\Phi}(\pmb X)$ 为 Gibbs 分布，则 $P_{\Phi}[{\pmb u}]=P_{\Phi}({\pmb W}\mid{\pmb u})$ where $\pmb W=\pmb X-\pmb U$ . 
> 也就是简化的分布等于条件概率分布

Thus, to condition a Gibbs distribution on a context $\mathbfit{u}$ , we simply reduce every one of its factors to that context. Intuitively, the renormalization step needed to account for $\mathbfit{u}$ is simply folded into the standard renormalization of any Gibbs distribution. 
> 因此，要将 Gibbs 分布条件于某上下文，我们只需要将其因子简化到该上下文
> 直观上，缩减后需要的重规范化实际上在缩减 Gibbs 分布的标准规范化过程中完成了

This result immediately provides us with a construction for the Markov network that we obtain when we condition the associated distribution on some observation $\mathbfit{u}$ . 
> 我们接着考虑为条件于 $\pmb u$ 得到的简化的 Gibbs 分布构造 Markov 网络

**Deﬁnition 4.7** reduced Markov network 
Let $\mathcal{H}$ be a Markov network over $X$ $\pmb{U}=\pmb{u}$ a context. The reduced M kov network $\mathcal{H}[\pmb{u}]$ is a ne ork over the nodes $W=X-U$ , where we have an edge $X{-}Y$ if there is an edge $X{-}Y$ in $\mathcal H$
> 定义：
> $\mathcal H$ 为 $\pmb X$ 上的 Markov 网络，$\pmb U = \pmb u$ 为上下文，定义缩减的 Markov 网络 $\mathcal H[\pmb u]$ 为节点 $\pmb W = \pmb X - \pmb U$ 上的 Markov 网络，如果 $\mathcal H$ 中边 $X-Y$ 存在，则缩减的 Markov 网络中该边也存在

**Proposition 4.2** 
Let PΦ(X) be a Gibbs distribution that factorizes over H, and U = u a context. Then PΦ[u] factorizes over H[u].
> 命题：
> $P_\Phi (\pmb X)$ 为在 $\mathcal H$ 上分解的 Gibbs 分布，$\pmb U = \pmb u$ 为上下文，则 $P_\Phi[\pmb u]$ 在 $\mathcal H[\pmb u]$ 上分解
> （$P_\Phi(\pmb X)$ 在 $\mathcal H$ 上分解说明 $\mathcal H$ 包含了 $\Phi$ 中所有因子对应的完全子图，给定上下文，$\Phi$ 中所有因子关于上下文的项被移除，对应于完全子图中关于上下文的边、节点被移除，但是根据定义4.7，$\mathcal H[\pmb u]$ 中的其他边都保留于 $\mathcal H$，因此对于 $P_\Phi[\pmb u]$ 的分解性质显然保留

Note the contrast to the efect of conditioning in a Bayesian network: Here, conditioning on a context $\mathbfit{u}$ only eliminates edges from the graph; in a Bayesian network, conditioning on evidence can activate a $\mathrm{v}$ -structure, creating new dependencies. We return to this issue in section 4.5.1.1. 
> Markov 网络和 Bayesian 网络在条件下略有不同，Markov 网路中，条件的影响就是移除边和点，Bayesian 网络中，条件可能会激活 v-structure，创建新的依赖

Example 4.3 
Consider, for example, the Markov network shown in ﬁgure 4.6a; as we will see, this network is the Markov network required to capture the distribution encoded by an extended version of our Student Bayesian network (see ﬁgure 9.8). Figure 4.6b shows the same Markov network reduced over a context of the form $G=g$ , and (c) shows the network reduced over a context of the form $G=g,S=s$ . As we can see, the network structures are considerably simpliﬁed. 

Box 4.B — Case Study: Markov Networks for Computer Vision. 
One important application area for Markov networks is computer vision. Markov networks, typically called MRFs in this vision com- munity, have been used for a wide variety of visual processing tasks, such as image segmentation, removal of blur or noise, stereo reconstruction, object recognition, and many more. 

In most of these applications, the network takes the structure of a pairwise MRF, where the variables correspond to pixels and the edges (factors) to interactions between adjacent pixels in the grid that represents the image; thus, each (interior) pixel has exactly four neighbors. The value space of the variables and the exact form of factors depend on the task. These models are usually formulated in terms of energies (negative log-potentials), so that values represent “penalties,” and a lower value corresponds to a higher-probability conﬁguration. 

In image denoising , for example, the task is to restore the “true” value of all of the pixels given possibly noisy pixel values. Here, we have a node potential for each pixel $X_{i}$ that penalizes large discrepancies from the observed pixel value $y_{i}$ . The edge potential encodes a preference for continuity between adjacent pixel values, penalizing cases where the inferred value for $X_{i}$ is too 

far from the inferred pixel value for one of its neighbors $X_{j}$ . However, it is important not to overpenalize true disparities (such as edges between objects or regions), leading to oversmoothing of the image. Thus, we bound the penalty, using, for example, some truncated norm, as described in box $4.D$ : $\epsilon(x_{i},x_{j})=\operatorname*{min}(c\|x_{i}-x_{j}\|_{p},\mathrm{dist}_{\mathrm{max}})$ (for $p\in\{1,2\}$ ). 

Slight variants of the same model are used in many other applications. For example, in stereo reconstruction , the goal is to reconstruct the depth disparity of each pixel in the image. Here, the values of the variables represent some discretized version of the depth dimension (usually more ﬁnely discretized for distances close to the camera and more coarsely discretized as the distance from the camera increases). The individual node potential for each pixel $X_{i}$ uses standard techniques from computer vision to estimate, from a pair of stereo images, the individual depth disparity of this pixel. The edge potentials, precisely as before, often use a truncated metric to enforce continuity of the depth estimates, with the truncation avoiding an over pen aliz ation of true depth disparities (for example, when one object is partially in front of the other). Here, it is also quite common to make the penalty inversely proportional to the image gradient between the two pixels, allowing a smaller penalty to be applied in cases where a large image gradient suggests an edge between the pixels, possibly corresponding to an occlusion boundary. 

In image segmentation , the task is to partition the image pixels into regions corresponding to distinct parts of the scene. There are diferent variants of the segmentation task, many of which can be formulated as a Markov network. In one formulation, known as multiclass segmentation, each var ble $X_{i}$ has a domain $\{1,\cdot\cdot\cdot,K\}$ , where the value of $X_{i}$ represents a region assignment for pixel i (for example, grass, water, sky, car). Since classifying every pixel can be computationally expensive, some state-of-the-art methods for image segmentation and other tasks ﬁrst oversegment the image into superpixels (or small coherent regions) and classify each region — all pixels within a region are assigned the same label. The oversegmented image induces a graph in which there is one node for each superpixel and an edge between two nodes if the superpixels are adjacent (share a boundary) in the underlying image. We can now deﬁne our distribution in terms of this graph. 

Features are extracted from the image for each pixel or superpixel. The appearance features depend on the speciﬁc task. In image segmentation, for example, features typically include statistics over color, texture, and location. Often the features are clustered or provided as input to local classiﬁers to reduce dimensionality. The features used in the model are then the soft cluster assign- ments or local classiﬁer outputs for each superpixel. The node potential for a pixel or superpixel is then a function of these features. We note that the factors used in deﬁning this model depend on the speciﬁc values of the pixels in the image, so that each image deﬁnes a diferent probability distribution over the segment labels for the pixels or superpixels. In efect, the model used here is $^a$ conditional random ﬁeld , a concept that we deﬁne more formally in section 4.6.1. 

The model contains an edge potential between every pair of neighboring superpixels $X_{i},X_{j}$ . Most simply, this potential encodes a contiguity preference, with a penalty of λ whenever $X_{i}\neq X_{j}$ . Again, we can improve the model by making the penalty depend on the presence of an image gradient between the two pixels. An even better model does more than penalize discontinuities. We can have nondefault values for other class pairs, allowing us to encode the fact that we more often ﬁnd tigers adjacent to vegetation than adjacent to water; we can even make the model depend on the relative pixel location, allowing us to encode the fact that we usually ﬁnd water below vegetation, cars over roads, and sky above everything. 

Figure 4.B.1 shows segmentation results in a model containing only potentials on single pixels (thereby labeling each of them independently) versus results obtained from a model also containing pairwise potentials. The diference in the quality of the results clearly illustrates the importance of modeling the correlations between the superpixels. 

## 4.3 Markov Network Independencies 
In section 4.1, we gave an intuitive justiﬁcation of why an undirected graph seemed to capture the types of interactions in the Misconception example. We now provide a formal presentation of the undirected graph as a representation of independence assertions. 

### 4.3.1 Basic Independencies 
As in the case of Bayesian networks, the graph structure in a Markov network can be viewed as encoding a set of independence assumptions. Intuitively, in Markov networks, probabilistic inﬂuence “ﬂows” along the undirected paths in the graph, but it is blocked if we condition on the intervening nodes. 
> 和 Bayesian 网络类似，Markov 网络结构也可以视为编码了独立性假设，直观上，Markov 网络中，概率影响通过无向路径流动，如果观察到了中间节点，则被堵塞

**Deﬁnition 4.8** active path 
Let $\mathcal{H}$ be a Markov network stru $X_{1}\!-\!\ldots\!-\!X_{k}$ a path in $\mathcal{H}$ . L $Z\subseteq\mathcal{X}$ of observed variables . The path $X_{1}\!-\!\ldots\!-\!X_{k}$ is active given Z if none of the $X_{i}{\mathit{\Sigma}}_{\mathit{S}},$ $i=1,\ldots,k,$ is in $Z$ . 
> 定义：
> $\mathcal H$ 为 Markov 网络，$X_1-\dots-X_k$ 为 $\mathcal H$ 中的路径，令 $\pmb Z\subseteq \mathcal X$ 为观察到的变量，给定 $\pmb Z$，如果 $X_i, i=1,\dots, k$ 都不在 $\pmb Z$ 中，则路径 $X_1-\dots-X_k$ 是活跃的

Using this notion, we define a notion of seperation in the graph.

**Deﬁnition 4.9** separation
We say that a set of nodes $Z$ s $X$ $Y$ in $\mathcal{H}$ , noted $\mathrm{sep}_{\mathcal{H}}(X;Y\mid Z).$ , if there is no active path betw n any node $X\in X$ and $Y\in Y$  given Z . We deﬁne the global independencies associated with H to be: 

$$
{\mathcal{I}}({\mathcal{H}})=\{(\pmb X\perp \pmb Y\mid \pmb Z)\;:\;\mathrm{sep}_{{\mathcal{H}}}(\pmb X;\pmb Y\mid \pmb Z)\}.
$$ 
> 定义：
> 节点集合 $\pmb Z, \pmb X, \pmb Y$，如果在给定 $\pmb Z$ 下，在 $X\in \pmb X$ 和 $Y\in\pmb Y$ 之间没有活跃路径，则称 $\pmb Z$ 分离 $\pmb X, \pmb Y$，记作 $\text{sep}_{\mathcal H}(\pmb X;\pmb Y\mid \pmb Z)$
> 定义 $\mathcal H$ 相关的全局独立性如上
> 直观上看，节点集 $\pmb X, \pmb Y$ 被 $\pmb Z$ 分离，就是在给定 $\pmb Z$ 下条件独立

As we will discuss, the i epend cies in $\mathcal{T}(\mathcal{H})$ are precisely those that are guaranteed to hold for every distribution P over H . In other word the separation criterion is sound for detecting independence properties in distributions over H 
> $\mathcal I (\mathcal H)$ 中的独立性保证对于所有在 $\mathcal H$ 上分解的 $P$ 成立，也就是图中的分离准则对于检测图上的分布中的独立性是可靠的

Note that the deﬁnition ation is monotonic in Z , that is, if $s e p_{\mathcal{H}}(X;Y\mid Z)$ , then $s e p_{\mathcal{H}}(X;Y\mid Z^{\prime})$ for any $Z^{\prime}\supset Z$ ⊃ . Thus, if we take separation as our deﬁnition of the inde- pendencies induced by the network structure, we are efectively restricting our ability to encode nonmonotonic independence relations. Recall that in the context of intercausal reasoning in Bayesian networks, nonmonotonic reasoning patterns are quite useful in many situations — for example, when two diseases are independent, but dependent given some common symp- tom. The nature of the separation property implies that such independence patterns cannot be expressed in the structure of a Markov network. We return to this issue in section 4.5. 
> 注意分离的定义是单调的，也就是如果 $sep_{\mathcal H}(\pmb X; \pmb Y\mid \pmb Z)$ 成立，则 $sep_{\mathcal H}(\pmb X; \pmb Y\mid \pmb Z')$ 对于任意 $\pmb Z' \supset \pmb Z$ 成立

As for Bayesian networks, we can show a connection between the independence properties implied by the Markov network structure, and the possibility of factorizing a distribution over the graph. As before, we can now state the analogue to both of our representation theorems for Bayesian networks, which assert the equivalence between the Gibbs factorization of a distribution $P$ over a graph $\mathcal{H}$ and the assertion that $\mathcal{H}$ is an I-map for $P$ , that is, that $P$ satisﬁes the Markov assumptions $\mathcal{Z}(\mathcal{H})$ . 

#### 4.3.1.1 Soundness 
We ﬁrst consider the analogue to theorem 3.2, which asserts that a Gibbs distribution satisﬁes the independencies associated with the graph. In other words, this result states the soundness of the separation criterion. 
> 首先讨论分离准则的可靠性，也就是在图上分解的分布会满足由图导出的独立性推断

**Theorem 4.1** 
Let $P$ be a distribution over $\mathcal{X}$ and $\mathcal{H}$ a Ma kov netw structure over $\mathcal{X}$ . If $P$ is a Gibbs distribution that factorizes over $\mathcal H$, then $\mathcal H$ is an I-map for $P$ .
> 定理：
> $P$ 为 $\mathcal X$ 上的分布，$\mathcal H$ 为 $\mathcal X$ 上的 Markov 网络，若 $P$ 是在 $\mathcal H$ 上分解的 Gibbs 分布，则 $\mathcal H$ 是 $P$ 的 I-map

Proof Let $X,Y,Z$ be any three disjoint subsets in $\mathcal{X}$ such that $Z$ separates $X$ and $Y$ in $\mathcal{H}$ . We want to show that $P\models(X\ \bot\ Y\ |\ Z)$，We start by considering the se w e $X\cup Y\cup Z=\mathcal{X}$ . As $Z$ $X$ from $Y$ , there direct betw $X$ and Y . Hence, any clique in H is fully contained e $X\cup Z$ ∪ in Y $Y\cup Z$ ∪ . Let I $\mathcal{T}_{X}$ be the indexes of the set of cliques that are contained in $X\cup Z$ ∪ , and let I $\mathcal{T}_{Y}$ be the indexes of the remaining cliques. We know that 
> 证明：
> $\pmb X, \pmb Y, \pmb Z$ 为 $\mathcal X$ 中任意三个不相交子集，满足 $\pmb Z$ 在 $\mathcal H$ 中分离 $\pmb X, \pmb Y$，我们希望得到 $P$ 满足 $(\pmb X\perp \pmb Y \mid \pmb Z)$，也就是 $P \vDash (\pmb X\perp \pmb Y \mid \pmb Z)$
> 考虑 $\pmb X \cup \pmb Y \cup \pmb Z = \mathcal X$，因为 $\pmb Z$ 分离了 $\pmb X, \pmb Y$，故 $\pmb X, \pmb Y$ 之间是不直接相连的，因此，$\mathcal H$ 中的任意团都完全包含在 $\pmb X \cup \pmb Z$ 或 $\pmb Y \cup \pmb Z$ 中，令 $\mathcal I_{\pmb X}$ 表示包含在 $\pmb X \cup \pmb Z$ 中的团的索引，令 $\mathcal I_{\pmb Y}$ 表示包含在 $\pmb Y \cup \pmb Z$ 中的团的索引
> 因为 $P$ 在 $\mathcal H$ 上分解，因此 $P$ 的因子都在 $\mathcal H$ 中有对应的团，故 $P$ 可以写为：

$$
P(X_{1},\ldots,X_{n})={\frac{1}{Z}}\prod_{i\in{\mathcal{I}}_{\pmb X}}\phi_{i}(\pmb D_{i})\cdot\prod_{i\in{\mathcal{I}}_{\pmb Y}}\phi_{i}(\pmb D_{i}).
$$ 
As we discussed, none of the factors in the ﬁrst product involve any variable in $Y$ , and none in the second product involve any variable in $X$ . Hence, we can rewrite this product in the form: 
> 显然，第一个乘积中，没有因子包含 $\pmb Y$ 中的任意变量（因为都是对应 $\pmb X \cup \pmb Z$ 中的团），同理，第二个乘积中，没有因子包含 $\pmb X$ 中的任意变量，因此我们将该乘积重写为：

$$
P(X_{1},\dots,X_{n})=\frac{1}{Z}f(\pmb X,\pmb Z)g(\pmb Y,\pmb Z).
$$ 
From this decomposition, the desired independence follows immediately (exercise 2.5). 
> 根据 exercise2.5 的结论，可以知道 $P\vDash (\pmb X\perp \pmb Y\mid \pmb Z)$

Now consider the case where $X\cup Y\cup Z\subset\mathcal{X}$ . L $U\,=\,\mathcal{X}\,-\,(X\cup Y\cup Z)$ . We can rtition U into two disjoint sets $U_{1}$ and $U_{2}$ such th $Z$ $X\cup U_{1}$ $Y\cup U_{2}$ in H . Using the preceding argument, we conclude that $P\models(X,U_{1}\bot Y,U_{2}\mid Z)$ . Using the decomposition property (equation (2.8)), we conclude that $P\models(X\bot Y\mid Z)$.
> 再考虑 $\pmb X \cup \pmb Y \cup \pmb Z \subset \mathcal X$，令 $\pmb U = \mathcal X - (\pmb X \cup \pmb Y \cup \pmb Z)$
> 我们可以将 $\pmb U$ 划分为两个不相交子集 $\pmb U_1, \pmb U_2$，使得 $\pmb Z$ 在 $\mathcal H$ 中分离 $\pmb X \cup \pmb U_1, \pmb Y \cup \pmb U_2$，因此有 $P \vDash (\pmb X, \pmb U_1 \perp \pmb Y, \pmb U_2 \mid \pmb Z )$
> 根据条件独立性的分解形式 (eq2.8)，我们容易得到 $P\vDash (\pmb X\perp \pmb Y\mid \pmb Z)$

The other direction (the analogue to theorem 3.1), which goes from the independence properties of a distribution to its factorization, is known as the Hammersley-Cliford theorem . Unlike for Bayesian networks, this direction does not hold in general. As we will show, it holds only under the additional assumption that $P$ is a positive distribution (see deﬁnition 2.5). 
> 我们证明了如果 $P$ 在 $\mathcal H$ 上分解，则 $\mathcal H$ 是 $P$ 的 I-map，但另一个方向，即如果 $\mathcal H$ 是 $P$ 的 I-map，则 $P$ 在 $\mathcal H$ 上分解，则在 Markov 网络中不一定成立
> 它仅在 $P$ 是正分布的假设下（即只要事件 $A$ 不为空，则 $P(A) > 0$）成立，该定理称为 Hammersley-Cliford theorem

**Theorem 4.2** Hammersley-Cliford theorem
Let $P$ be a sitive distribution over $\mathcal{X}$ , and $\mathcal{H}$ a Marko etwork graph over $\mathcal{X}$ . If $\mathcal{H}$ is an I-map for P , then P is a Gibbs distribution that factorizes over . 
> 定理：
>  $P$ 是 $\mathcal X$ 上的正分布，$\mathcal H$ 是 $\mathcal X$ 上的 Markov 网络，如果 $\mathcal H$ 是 $P$ 的 I-map，则 $P$ 就是可以在 $\mathcal H$ 上分解的 Gibbs 分布

To prove this result, we would need to use the independence assumptions to construct a set of factors over $\mathcal{H}$ that give rise to the distribution $P$ . In the ca of Bayesian networks, these factors were simply CPDs, which we could derive directly from P . As we have discussed, the correspondence between the factors in a Gibbs distribution and the distribution $P$ is much more indirect. The construction required here is therefore signiﬁcantly more subtle, and relies on concepts that we develop later in this chapter; hence, we defer the proof to section 4.4 (theorem 4.8). 
> 要证明该定理，我们需要使用 $\mathcal H$ 中的独立性假设来在 $\mathcal H$ 上构造出一个因子集合，通过该集合可以得到 $P$
> 在 Bayesian 网络中，这些因子就是 CPDs，CPDs 可以直接从图中读出，但 GIbbs 分布和分布 $P$ 中的因子的对应关系更不直接，该定理的证明推迟到 Theorem 4.8

This result shows that, **for positive distributions, the global independencies imply that the distribution factorizes according the network structure. Thus, for this class of distritions, we have at a distribution $P$ factorizes over a Markov network $\mathcal{H}$ if and only if H is an I-map of P** . The positivity assumption is necessary for this result to hold: 
> 定理4.2表明：对于正分布，其全局独立性表明了分布根据网络结构分解，对于这类分布，我们有分布 $P$ 在 Markov 网络 $\mathcal H$ 上分解当且仅当 $\mathcal H$ 是 $P$ 的 I-map

Example 4.4 
Consider a distribution $P$ over four binary random variables $X_{1},X_{2},X_{3},X_{4},$ which gives proba- bility $1/8$ to each of the following eight conﬁgurations, and probability zero to all others:  Let $\mathcal{H}$ be e graph $X_{1}{-}X_{2}{-}X_{3}{-}X_{4}{-}X_{1}$ . Then $P$ satisﬁes the global independencies with $\mathcal{H}$ ample, consider the independence $(X_{1}\perp X_{3}\mid X_{2},X_{4})$ . For the assignment $X_{2}=x_{2}^{1},X_{4}=x_{4}^{0}$ , we have that only assignments where $X_{1}=x_{1}^{1}$ receive positive probability. Thus, $P(x_{1}^{1}\mid x_{2}^{1},x_{4}^{0})=1$ | , and $X_{1}$ is trivially independent of $X_{3}$ in this conditional distribution. A similar analysis applies to all other cases, so that the global independencies hold. However, the distribution $P$ does not factorize according to $\mathcal{H}$ . The proof of this fact is left as an exercise (see exercise 4.1). 

#### 4.3.1.2 Completeness 
The preceding discussion shows the soundness of the separation condition as a criterion for detecting independencies in Markov networks: any distribution that factorizes over $\mathcal{G}$ satisﬁes the independence assertions implied by separation. The next obvious issue is the completeness of this criterion.
> 上一小节讨论了用分离条件作为准则检测 Markov 网络中的独立性的可靠性：**任意**在 $\mathcal G$ 上分解的分布都满足 $\mathcal G$ 中通过分离条件表明的独立性
> （可靠性：图中分离 --> 分布中独立，对于任意分解于图的分布成立）
> 本节讨论该准则的完备性

As for Bayesian networks, the strong version of completeness does not hold in this setting. In other words, it is not the case that ery pair of nodes $X$ d $Y$ that are not separated in $\mathcal{H}$ are dependent in every distribution $P$ which factorizes over H . However, as in theorem 3.3, we can use a weaker deﬁnition of completeness that does hold: 
> 对于贝叶斯网络，强完备性成立，也就是说，$\mathcal G$ 没有被分离 (d-seperation) 的两个节点 $X, Y$ 在分解于 $\mathcal G$ 上的分布 $P$ 中一定是依赖的，也就是图中的分离可以检测到分布中所有的独立性
> （完备性：图中不分离 --> 分布中不独立，对于任意分解于图的分布成立）
> 但 Markov 网络中，强完备性不成立，也就是说，$\mathcal H$ 中没有被分离的两个节点 $X, Y$ 在分解于 $\mathcal H$ 上的分布 $P$ 中不一定是依赖的，可能是独立的，也就是图中的分离不能检测出分布中所有的独立性
> Markov 网络中，我们可以定义一种较弱的完备性

**Theorem 4.3**
Let $\mathcal{H}$ be a Markov net rk structure. If $X$ an $Y$ are not separated iven $Z$ in $\mathcal{H}$ , then $X$ and $Y$ are dependent given Z in some distribution P that factorizes over H . 
> 定理：
> $\mathcal H$ 为 Markov 网络，如果在 $\mathcal H$ 中，$X, Y$ 在给定 $\pmb Z$ 时不分离，则 $X, Y$ 在**某个**分解于 $\mathcal H$ 上的分布 $P$ 中给定 $\pmb Z$ 时是依赖的

Proof e pro is a constructive one: we construct a distribution $P$ that factorizes over $\mathcal{H}$ where X and Y $Y$ are dependent. We assume, without loss of generality, that all variables are binary-valued. If this is not the case, we can treat them as binary-valued by restricting attention to two distinguished values for each variable. 
> 证明：
> 构造性证明，我们构造一个分解于 $\mathcal H$ 的分布 $P$，使得 $X, Y$ 在 $P$ 中给定 $\pmb Z$ 是依赖的
> 不失一般性，假设都为二值变量，如果变量不是二值变量，我们可以将注意限制到每个变量的两个不同值，将其视为二值变量

By assumption, $X$ and $Y$ $Z$ $\mathcal{H}$ ; hence, they must be connected by some unblocked trail. Let $X=U_{1}-U_{2}-\ldots-U_{k}=Y$ be some minimal trail in the graph such that, for all $i$ , $U_{i}\notin Z$ here we deﬁne a m trail in $\mathcal{H}$ to be a path with no shortcuts: thus, for any i and $j\neq i\pm1$ ̸ ± , there is no edge $U_{i}{-}U_{j}$ . We can always ﬁnd such a path: If we have a nonminimal path where we have $U_{i}{-}U_{j}$ for $j\,>\,i+1$ , we can always “shortcut” the original trail, converting it to one that goes directly from $U_{i}$ to $U_{j}$ . 
> $X, Y$ 在给定 $\pmb Z$ 时在 $\mathcal H$ 中不分离，因此二者必须通过某个未被堵塞的迹相连
> 不妨设 $X=U_1-U_2-\dots-U_k = Y$ 为图中满足该性质的极小的迹，满足 for all $i$, $U_i \not\in \pmb Z$
> 我们定义 $\mathcal H$ 中极小的迹为没有捷径的迹，即对于任意 $i$ 和 $j\ne i\pm 1$，不存在边 $U_i-U_j$
> 我们总可以找到这样一条极小的迹，如果有一条非极小的迹存在 $U_i - U_j,\quad j > i+1$，我们可以对这个迹取捷径，以此得到极小的迹

For any $i=1,\ldots,k-1$ , as there is an edge $U_{i}{-}U_{i+1}$ , i ollows that $U_{i},U_{i+1}$ must both appear in some clique $C_{i}$ . We pick some very large weight W , and for each i we deﬁne the clique potential $\phi_{i}(\boldsymbol{C}_{i})$ to assign weight $W$ if $U_{i}=U_{i+1}$ and weight 1 otherwise, regardless of the values of the other variables in the clique. Note that the cliques $C_{i}$ for $U_{i},U_{i+1}$ and $C_{j}$ for $U_{j},U_{j+1}$ must be diferent cliques: If $C_{i}=C_{j}$ , then $U_{j}$ is in the same clique as $U_{i}$ , and we have an edge $U_{i}{-}U_{j}$ , contradicting the minimality of the trail. Hence, we can deﬁne the clique potential for each clique $C_{i}$ separately. We deﬁne the clique potential for any other clique to be uniformly 1 . 
> 对于任意 $i=1,\dots, k-1$，存在边 $U_i - U_{i+1}$，因此 $U_i, U_{i+1}$ 一定都出现在某个团 $\pmb C_i$ 中
> 我们选定一个很大的权重 $W$，对于每个 $i$，我们定义团势能函数 $\phi_i (\pmb C_i)$，如果 $U_i = U_{i+1}$，赋予它权重 $W$，否则赋予权重 1，权重的赋予和团中其他的变量的值无关
> 注意 $U_i, U_{i+1}$ 的团 $\pmb C_i$ 和 $U_j, U_{j+1}$ 的团 $\pmb C_j$ 必须是不同的团，因为迹是极小的，因此 $U_i$ 和 $U_j$ 之间显然不可能有边，这允许我们为每个 $\pmb C_i$ 分别定义势能函数
> 我们将其他的团的势能都定义为 1

We now consider the distribution $P$ resulting from multiplying all of these clique potentials. Intuitively, the distribution $P(U_{1},.\,.\,,U_{k})$ is simply the distribution deﬁned by multiplying the pairwise factors for the pairs $U_{i},U_{i+1}$ , regardless of the other variables (including the ones in $Z)$ ). One can verify that, in $P(U_{1},\cdot\cdot\,,U_{k})$ , we have that $X=U_{1}$ and $Y=U_{k}$ are dependent. We leave the conclusion of this argument as an exercise (exercise 4.5). 
> 现在考虑将这些团势能函数全部相乘得到的分布 $P$
> 直观上，$P (U_1,\dots, U_k)$ 就是通过由 $U_i, U_{i+1}$ 定义的因子相乘得到的分布，与其他变量无关，包括 $\pmb Z$
> 可以验证 $P (U_1,\dots, U_k)$ 中，$X = U_1, Y = U_k$ 是相互依赖的

We can use the same argument as theorem 3.5 to conclude that, for almost all distributions $P$ that factorize over $\mathcal{H}$ (that is, for all distributions except for a set of measure zero in the space of factor parameter iz at ions) we have that $\mathcal{I}(P)=\mathcal{I}(\mathcal{H})$ . 
> 事实上，对于几乎所有在 $\mathcal H$ 上分解的分布 $P$（也就是除了在因子参数化空间中测度为零的一组分布），都有 $\mathcal I (P) = \mathcal I (\mathcal H)$

Once again, we can view this result as telling us that our deﬁnition of $\mathcal{T}(\mathcal{H})$ is t maximal one. For any independence assertion tha is not a consequen of separation in H , we can always ﬁnd a counterexample distribution P that factorizes over . 
> 可以认为该结果说明了我们对 $\mathcal I (\mathcal H)$ 的定义就是极大的，对于任意不是 $\mathcal H$ 分离的结果的独立性断言，我们总能在分解于 $\mathcal H$ 的分布中找到一个反例

### 4.3.2 Independencies Revisited 
When characterizing the independencies in a Bayesian network, we provided two deﬁnitions: the local independencies (each node is independent of its nondescendants given its parents), and the global independencies induced by d-separation. As we showed, these two sets of independencies are equivalent, in that one implies the other. 
> Bayesian 网络中的独立性被我们分为了两类：
> - 局部独立性：给定父变量，变量和所有非后继条件独立
> - 全局独立性：由 d-seperation 推导出的独立性
> 这两类独立性实际上等价，也就是可以根据局部独立性的定义推导出网络中的全局独立性，也可以根据全局独立性的定义推导出网络中的局部独立性

So far, our discussion for Markov networks provides only a global criterion. While the global criterion characterizes the entire set of independencies induced by the network structure, a local criterion is also valuable, since it allows us to focus on a smaller set of properties when examining the distribution, signiﬁcantly simplifying the process of ﬁnding an I-map for a distribution $P$ . 
> 我们目前仅讨论了 Markov 网络中的全局独立性

Thus, it is natural to ask whether we can provide a local deﬁnition of the independencies induced by a Markov network, analogously to the local independencies of Bayesian networks. Surprisingly, as we now show, in the context of Markov networks, there are three diferent possible deﬁnitions of the independencies associated with the network structure — two local ones and the global one in deﬁnition 4.9. While these deﬁnitions are related, they are equivalent only for positive distributions. 
> 类似贝叶斯网络，我们希望提供一个 Markov 网络中独立性的局部定义
> 对于 Markov 网络，存在三种可能的独立性定义，两个局部一个全局
> 这三个定义仅在分布是正分布时才等价

As we will see, nonpositive distributions allow for deterministic dependencies between the variables. Such deterministic interactions can “fool” local indepen- dence tests, allowing us to construct networks that are not I-maps of the distribution, yet the local independencies hold. 
> 非正分布允许变量间的确定性依赖，这会扰乱局部独立性测试，也就是可以借由它构建不是分布的 I-map 的网络 （不满足 $\mathcal I(\mathcal H) \subseteq \mathcal I(P)$），但分布的局部独立性在网络中成立

#### 4.3.2.1 Local Markov Assumptions 
The ﬁrst, and weakest, deﬁnition is based on the following intuition: Whenever two variables are directly connected, they have the potential of being directly correlated in a way that is not mediated by other variables. Conversely, when two variables are not directly linked, there must be some way of rendering them conditionally independent. Speciﬁcally, we can require that $X$ and $Y$ be independent given all other nodes in the graph. 
> 先介绍Markov 网络的第一个，也是最弱的独立性定义
> 它的直觉为：当两个变量直接相连，它们就有可能相互直接影响，而不经过中间变量；并且，如果两个变量没有直接相连，一定可以找到条件使二者条件独立，例如，我们可以要求给定图中所有其他节点，$X, Y$ 条件独立

**Deﬁnition 4.10** pairwise independencies 
Let $\mathcal{H}$ be a Markov network. We deﬁne the pairwise independencies associated with $\mathcal{H}$ to be: ${\mathcal{I}}_{p}({\mathcal{H}})=\{(X\perp Y\mid{\mathcal{X}}-\{X,Y\})\;:\;X{\mathrm{-}}Y\notin{\mathcal{H}}\}.$ 
> 定义：
> 令 $\mathcal H$ 为 Markov 网络，定义和 $\mathcal H$ 相关的成对独立性为：
> $\mathcal I_p (\mathcal H) = \{(X\perp Y \mid \mathcal X - \{X, Y\}): X-Y\not\in \mathcal H\}$

Using this deﬁnition, we can easily represent the independencies in our Misconception example using a Markov network: We simply connect the nodes up in exactly the same way as the interaction structure between the students. 

The second local deﬁnition is an undirected analogue to the local independencies associated with a Bayesian network. It is based on the intuition that we can block all inﬂuences on a node by conditioning on its immediate neighbors. 
> 第二个独立性定义类似贝叶斯网络中的局部独立性定义
> 它的直觉为：通过条件于某个节点的所有直接邻居，可以阻塞所有对该节点的影响

**Deﬁnition 4.11** Markov blanket 
For a given aph $\mathcal{H}$ , we deﬁne the Markov blanket of $X$ in $\mathcal{H}$ , denote $\mathrm{MB}_{\mathcal{H}}(X)$ , to be the neighbors of X in . We deﬁne the local independencies associated with to be: 
> 定义：
> 对于图 $\mathcal H$，定义 $\mathcal H$ 中 $X$ 的 Markov 毯为 $X$ 所有的邻居变量，记作 $\text{MB}_{\mathcal H}(X)$
> 然后，定义和 $\mathcal H$ 相关的局部独立性为：

$$
{\mathcal{I}}_{\ell}({\mathcal{H}})=\{(X\perp{\mathcal{X}}-\{X\}-\mathrm{MB}_{{\mathcal{H}}}(X)\mid\mathrm{MB}_{{\mathcal{H}}}(X))\;:\;X\in{\mathcal{X}}\}.
$$ 
In other words, the local independencies state that $X$ is independent of the rest of the nodes in the graph given its immediate neighbors. 
> 局部独立性的含义即 $X$ 在给定它的直接邻居节点/Markov 毯时，对 $\mathcal H$ 中剩余的所有节点都条件独立

We will show that these local independence assumptions hold for any distribution that factorizes over $\mathcal{H}$ , so that $X$ ’s Markov blanket in $\mathcal{H}$ truly does separate it from all other variables. 
> 这两个局部独立性假设对于任意分解于 $\mathcal H$ 的分布 $P$ 都成立
> 故 $X$ 在 $\mathcal H$ 中的 Markov blanket 确实在分布中将 $X$ 和其他变量分离
#### 4.3.2.2 Relationships between Markov Properties 
We have now presented three sets of independence assertions associated with a network struc- ture $\mathcal{H}$ . For general distributions, $\mathcal{T}_{p}(\mathcal{H})$ is strictly weaker than $\mathcal{T}_{\ell}(\mathcal{H})$ , which in turn is strictly weaker than $\mathcal{T}(\mathcal{H})$ . However, all three deﬁnitions are equivalent for positive distributions. 
> 目前为止我们讨论了三组和 $\mathcal H$ 相关的独立性断言
> 对于一般的分布，$\mathcal I_p(\mathcal H)$ 严格弱于 $\mathcal I_{\mathscr l}(\mathcal H)$，而 $\mathcal I_{\mathscr l}(\mathcal H)$ 严格弱于 $\mathcal I (\mathcal H)$
> 但对于正分布，这三个定义等价

**Proposition 4.3** 
For any Markov network H , and any distribution $P$ , we have that if $P\vDash\mathcal{Z}_{\ell}(\mathcal{H})$ I H then $P\vDash\mathcal{Z}_{p}(\mathcal{H})$ I H . The proof of this result is left as an exercise (exercise 4.8). 
> 命题：
> 对于任意 Markov 网络 $\mathcal H$ 和任意分布 $P$，如果 $P\vDash \mathcal I_{\mathscr l} (\mathcal H)$，则 $P\vDash \mathcal I_p (\mathcal H)$
>（如果在 $P$ 中存在独立性 $\mathcal I_{\mathscr l}(\mathcal H)$，则 $P$ 中存在独立性 $\mathcal I_p(\mathcal H)$）

**Proposition 4.4**
For any Markov network H , and any distribution $P$ , we have that if $P\vDash\mathcal{Z}(\mathcal{H})$ I H then $P\vDash\mathcal{Z}_{\ell}(\mathcal{H})$ I H . 
> 命题：
> 对于任意 Markov 网络 $\mathcal H$ 和任意分布 $P$，如果 $P\vDash \mathcal I (\mathcal H)$，则 $P\vDash \mathcal I_{\mathscr l}(\mathcal H)$
>（如果在 $P$ 中存在独立性 $\mathcal I(\mathcal H)$，则 $P$ 中存在独立性 $\mathcal I_{\mathscr l}(\mathcal H)$）

The proof of this result follows directly from the fact that if $X$ and $Y$ are not connected by an edge, then they are necessarily separated by all of the remaining nodes in the graph. 

The converse of these inclusion results holds only for positive distributions (see deﬁnition 2.5). More speciﬁcally, if we assume the intersection property (equation (2.11)), all three of the Markov conditions are equivalent. 
> 命题4.3和命题4.4的包含关系的反过来仅对于正分布成立，或者说对于具有 intersection 性质的分布成立

**Theorem 4.4** 
Let $P$ be a positive distribution. If $P$ satisﬁes $\mathcal{I}_{p}(\mathcal{H})$ , then $P$ satisﬁes $\mathcal{I}(\mathcal{H})$ . 
> 定理：
> $P$ 为正分布，如果 $P$ 满足 $\mathcal I_p (\mathcal H)$，则 $P$ 满足 $\mathcal I (\mathcal H)$

Proof We want to prove that for all disjoint sets $X,Y,Z$ : 

$$
s e p_{\mathcal{H}}(X;Y\mid Z)\Longrightarrow P\vDash(X\perp Y\mid Z).\tag{4.1}
$$ 
The proof proceeds by descending induction on the size of $Z$ . 
> 证明：
> 回忆一下 $\mathcal I (\mathcal H)$ 的定义为 ${\mathcal{I}}({\mathcal{H}})=\{(\pmb X\perp \pmb Y\mid \pmb Z)\;:\;\mathrm{sep}_{{\mathcal{H}}}(\pmb X;\pmb Y\mid \pmb Z)\}$，故我们需要证明在 $P$ 满足 $\mathcal I_p (\mathcal H)$ 的条件下，对于所有图中的 $sep_\mathcal H (\pmb X; \pmb Y \mid \pmb Z)$，在 $P$ 中成立 $(\pmb X \perp \pmb Y \mid \pmb Z)$
> 证明的思路是用归纳法，逐渐减少 $\pmb Z$ 的大小

The base case is $|Z|=n-2;$ ; equation (4.1) follows immediately fr the deﬁnition of $\mathcal{I}_{p}(\mathcal{H})$ . 
> 基例是 $|\pmb Z| = n-2$，此时 $\pmb X, \pmb Y, \pmb Z$ 满足 $sep_\mathcal H (\pmb X; \pmb Y\mid \pmb Z)$，显然，$\pmb X, \pmb Y$ 此时仅是两个节点，并且它们没有直接相连的边，满足 $(\pmb X \perp \pmb Y \mid \pmb Z)\in \mathcal I_p (\mathcal H)$，而 $P$ 满足 $\mathcal I_p (\mathcal H)$ ，故 $P\vDash (\pmb X \perp \pmb Y \mid \pmb Z)$

For the inductive step, assume that equation (4.1) holds for every $Z^{\prime}$ with size $|Z^{\prime}|=\dot{k}$ , and let Z be any set su $|Z|=k-1$ . We distinguish between two case 
> 归纳推理：
> 假设对于大小为 $k$ 的 $\pmb Z'$，式4.1成立，令 $\pmb Z$ 为任意满足 $|\pmb Z| = k-1$ 的集合，分为两类讨论

In the ﬁrst case, $X\cup Z\cup Y=\mathcal{X}$ ∪ ∪ X . As $|Z|<n-2$ , we hat either $|X|\geq2$ or $|Y|\geq2$ . Without loss of generality, assume that the latter holds; let $A\in Y$ ∈ and $Y^{\prime}=Y\!-\!\{A\}$ . From the t that $s e p_{\mathcal{H}}(X;Y\mid Z)$ , we also have that $s e p_{\mathcal{H}}(X;Y^{\prime}\mid Z)$ on one hand and $s e p_{\mathcal{H}}(\boldsymbol{X};A\mid$ $Z)$ on the other hand. As separation is monotonic, we also have $s e p_{\mathcal H}(X;Y^{\prime}\mid Z\cup\{A\})$ and $s e p_{\mathcal{H}}(X;A\mid Z\cup Y^{\prime})$ . The separating sets $Z\cup\{A\}$ and $Z\cup Y^{\prime}$ ∪ are ach at least size $|Z|+1=k$ in size, so that equation (4.1) applies, and we can conclude that P satisﬁes: 

$$
(X\bot Y^{\prime}\mid Z\cup\{A\})\quad\&\quad\ (X\bot A\mid Z\cup Y^{\prime}).
$$ 
Because $P$ is positive, we can apply the intersection property (equation (2.11)) and conclude that $P\models(X\ \bot\ Y^{\prime}\cup\{A\}\mid Z)$ , that is, $(X\perp Y\mid Z)$ . 
> 第一例：
> 当 $\pmb X \cup \pmb Z \cup \pmb Y = \mathcal X$，因为 $|\pmb Z | <n-2$，故 $|\pmb X | \ge 2$ 或 $|\pmb Y| \ge 2$
> 假设 $|\pmb Y | \ge 2$，令 $A\in \pmb Y$，$\pmb Y' = \pmb Y - \{A\}$，
> 因为 $sep_{\mathcal H}(\pmb X; \pmb Y \mid \pmb Z)$，故显然 $sep_{\mathcal H}(\pmb X; \pmb Y' \mid \pmb Z)$ 和 $sep_{\mathcal H}(\pmb X; A\mid \pmb Z)$ 也成立
> 因为 seperation 是单调的，故显然 $sep_{\mathcal H}(\pmb X; \pmb Y' \mid \pmb Z\cup \{A\})$ 和 $sep_{\mathcal H}(\pmb X; A\mid \pmb Z\cup \pmb Y')$ 也成立
> 而其中的分离集合 $\pmb Z \cup \{A\}$ 和 $\pmb Z \cup \pmb Y$ 的大小都至少是 $|\pmb Z| + 1 =k$，故式4.1成立，也就是  $P\vDash(\pmb X\perp \pmb Y' \mid \pmb Z\cup \{A\})$ 和 $P\vDash(\pmb X\perp A\mid \pmb Z\cup \pmb Y')$ 成立
> 根据正分布的 intersection 性质，我们可以得到
> $P\vDash (\pmb X\perp \pmb Y' \cup \{A\} \mid \pmb Z)$，也就是 $P\vDash (\pmb X\perp \pmb Y \mid \pmb Z)$

The second case is where $X\cup Y\cup Z\neq X$ . Here, we might have that both $X$ and $Y$ are singletons. This case requires a similar argument that uses the induction hypothesis and properties of independence. We leave it as an exercise (exercise 4.9). 
> 第二例的情况是 $\pmb X \cup \pmb Y\cup \pmb Z \ne \mathcal X$，但归纳推理是类似的，如果 $\pmb X, \pmb Y$ 都仅含单个节点，则二者的条件独立性直接包含在了 $\mathcal I_p (\mathcal H)$ 中，如果 $\pmb X, \pmb Y$ 包含多个节点，则推导和上面完全类似，同样满足 $\pmb Z \cup \{A\}$ 和 $\pmb Z \cup \pmb A$ 的大小至少是 $|\pmb Z | + 1 = k$

Our previous results entail that, for positive distributions, the three conditions are equivalent. 
> 根据定理4.4，对于正分布，三个独立性定义是等价的

**Corollary 4.1** 
The following three statements are equivalent for a positive distribution $P$ : 
> 引理：
> 以下三个表述对于正分布 $P$ 等价

1. $P\vDash\mathcal{I}_{\ell}(\mathcal{H})$ .
2. $P\vDash\mathcal{I}_{p}(\mathcal{H})$ .
3. $P\vDash\mathcal{I}(\mathcal{H})$ . 

This equivalence relies on the positivity assumption. In particular, for nonpositive distributions, we can provide examples of a distribution $P$ that satisﬁes one of these properties, but not the stronger one. 

Example 4.5 
Let $P$ be any distribution over ${\mathcal{X}}=\{X_{1},.\,.\,.\,,X_{n}\}.$ ; let $\mathcal{X}^{\prime}=\{X_{1}^{\prime},\cdot\cdot\cdot,X_{n}^{\prime}\}$ } . We now construct a distribution $P^{\prime}(\mathcal{X},\mathcal{X}^{\prime})$ whose arginal over $X_{1},\ldots,X_{n}$ is the same as $P$ , and where $X_{i}^{\prime}$ is deterministically equal to $X_{i}$ . t H be a Markov network over $\mathcal{X},\mathcal{X}^{\prime}$ that contains no edges other than $X_{i}{-}X_{i}^{\prime}$ . Then, in P $P^{\prime}$ ′ , $X_{i}$ is independent of the rest of the variables in the network given its neighbor $X_{i}^{\prime}$ , and similarly for $X_{i}^{\prime}$ thus, $\mathcal{H}$ isﬁes th local independencies for every node in the network. clearly $\mathcal{H}$ is not an I ap for $P^{\prime}$ ′ , since H makes many independence assertions regarding the $X_{i}$ ’s that do not hold in P (or in P $P^{\prime}$ ). 

Thus, for nonpositive distributions, the local independencies do not imply the global ones. A similar construction can be used to show that, for nonpositive distributions, the pairwise independencies do necessarily imply the local independencies. 
> 对于非负分布，满足图中的局部独立性并不表明它满足图中的全局独立性；满足图中的成对独立性并不表明它满足图中的局部独立性

Example 4.6
Let $P$ any tribution ove $\mathcal{X}=\{X_{1},\ldots,X_{n}\}$ , and now consider two auxiliary sets of vari- ables X $\mathcal{X}^{\prime}$ and X $\mathcal{X}^{\prime\prime}$ , and deﬁne X $\mathcal{X}^{\ast}=\mathcal{X}\cup\mathcal{X}^{\prime}\cup\mathcal{X}^{\prime\prime}$ X ∪X X . We now construct a distribution $P^{\prime}(\mathcal{X}^{*})$ whose marginal over $X_{1},\dots,X_{n}$ is the same as P , and where $X_{i}^{\prime}$ and $X_{i}^{\prime\prime}$ are both deterministic ally equal to $X_{i}$ . Let $\mathcal{H}$ be the empty Markov network over $\mathcal{X}^{*}$ . We argue that this empty network satisﬁes the pairwise assumptions for every pair of nodes in the network. For example, $X_{i}$ and $X_{i}^{\prime}$ are rendered independent ecause $\mathcal{X}^{*}\,-\,\{X_{i},X_{i}^{\prime}\}$ } contains $X_{i}^{\prime\prime}$ . Similarly, $X_{i}$ and $X_{j}$ are independent given $X_{i}^{\prime}$ . Thus, H satisﬁes the pairwise independencies, but not the local or global independencies. 
> Example 4.5和4.6都用了同一个 trick，就是引入确定性关系，使得构建出的分布出现在给定 $X'$ 时 $X$ 独立于所有其他变量这样的独立性，而这样的独立性在目前的 Markov 网络语义中显然是不包含的（目前的 Markov 网络仅用节点阻塞这样的方式表示独立性语义），故分布中的独立性超出了网络语义的表示范围，网络无法检测到分布中的全部独立性，导致完备性不成立
> 如果是正分布，分布中不存在概率为0的事件，也就是不存在确定性事件（概率为0的事件就是确定性事件取反），就不会有这样的问题

### 4.3.3 From Distributions to Graphs 
Based on our deeper understanding of the independence properties associated with a Markov network, we can now turn to the question of encoding the independencies in a given distribution $P$ using a graph structure. As for Bayesian networks, the notion of an I-map is not sufcient by itself: The complete graph implies no independence assumptions and is hence an I-map for any distribution. We therefore return to the notion of a minimal I-map, deﬁned in deﬁnition 3.13, which was deﬁned broadly enough to apply to Markov networks as well. 
> 本节讨论从分布到图，也就是用图编码给定分布中的独立性
> 对于贝叶斯网络，构建 I-map 本身也是不充分的，例如构建一个完全图，图结构不编码任何独立性信息，该图也是任何分布 I-map
> 我们故而考虑极小 I-map 的概念，也就是图中任意删除一条边都会导致图不再是 I-map，即多出额外的独立性

How can we construct a minimal I-map for a distribution $P$ ? Our discussion in section 4.3.2 immediately suggests two approaches for constructing a minimal I-map: one based on the pairwise Markov independencies, and the other based on the local independencies. 
> 考虑基于成对 Markov 独立性和局部独立性构建极小 I-map

In the ﬁrst approach, we consider the pairwise independencies. They assert that, if the edge $\{X,Y\}$ is not in $\mathcal{H}$ , then $X$ and $Y$ must be independent given all other nodes in the grap regardless of which other edges the graph contains. Thus, at the very least to guarantee that H is an I-map, we must add direct edges between all pairs of nodes X and Y such that 

$$
P\not\vDash(X\perp Y\mid{\mathcal{X}}-\{X,Y\}).\tag{4.2}
$$ 
We can now deﬁne $\mathcal{H}$ to include an edge $X{-}Y$ for all $X,Y$ for which equation (4.2) holds. 
> 对于成对独立性：
> 成对独立性即如果 $X-Y$ 不在 $\mathcal H$ 中，则 $X, Y$ 必须在给定图中所有其他节点的情况下条件独立，为此，在完全图的基础上，我们首先移除分布中存在成对独立性的变量之间直接相连的边
> 反过来说，就是在满足 $P\not \vDash (X\perp Y\mid \mathcal X - \{X, Y\})$ (即不存在成对独立性) 的变量 $X, Y$ 之间添加边

In the second approach, we use the local independencies and the notion of minimality. For each variable $X$ , we deﬁne the neighbors of $X$ to be a minimal set of nodes $Y$ that render $X$ independent of the rest of the nodes. More precisely, deﬁne: 
> 关于局部独立性：
> 我们首先为每个变量 $X$ 定义它的 Markov 毯，也就是可以让变量 $X$ 和其余节点条件独立的极小的节点集合

**Deﬁnition 4.12** Markov blanket 
A set $U$ is $a$ Markov blanket of $X$ in a distribution $P$ if $X\not\in U$ and if $U$ is a minimal set of nodes such that 

$$
(X\perp{\mathcal{X}}-\{X\}-U\mid U)\in{\mathcal{I}}(P).\tag{4.3}
$$ 
> 定义：
> 对于分布 $P$ 中的变量 $X$，它的 Markov 毯定义为一个节点集合 $\pmb U$，满足 $X\not\in \pmb U$，并且 $\pmb U$ 是满足 $(X\perp \mathcal X - \{X\} - \pmb U \mid \pmb U) \in \mathcal I (P)$ 的极小的节点集合

We then deﬁne a graph $\mathcal{H}$ by introducing an edge $\{X,Y\}$ for all $X$ and $Y\in{\mathrm{MB}}_{P}(X)$ . As deﬁned, this construction is not unique, since there may be several sets U satisfying equation (4.3). However, theorem 4.6 will show that there is only one such minimal set. 
> 因此，在图 $\mathcal H$ 中，对于所有的 $X$，我们需要为 $Y \in \text{MB}_P (X)$ 引入边 $X-Y$

In fact, we now show that any positive distribution $P$ has a unique minimal I-map, and that both of these constructions produce this I-map. 

We begin with the proof for the pairwise deﬁnition: 

**Theorem 4.5** 
Let $P$ be a positive distribution, and let $\mathcal{H}$ be deﬁned by roducing an edge $\{X,Y\}$ for all $X,Y$ for which equation (4.2) holds. Then the Markov network H is the unique minimal I-map for P . 
> 定理：
> $P$ 为正分布，$\mathcal H$ 通过为满足式4.2的所有 $\{X, Y\}$ 引入边得到，则 $\mathcal H$ 是 $P$ 的唯一极小 I-map

Proof The fact that $\mathcal{H}$ is an I-map for $P$ follows immediately from fact that $P$ , by construction, satisﬁes $\mathcal{T}_{p}(\mathcal{H})$ , and, therefore, by corollary 4.1, also s $\mathcal{Z}(\mathcal{H})$ The fact that it is minimal follows from the fact that if we eliminate some edge { $\{X,Y\}$ } from H , the graph wo d imply the pairwise independence $(X\perp Y\mid{\mathcal{X}}-\{X,Y\})$ , which w know to be false for P (otherwise, the edge would have been omitted in the construction of H ). The un enes f the minimal I-map also follows tr ally: By the same argument, an other I-map H for P must contain at least the edges in H and is therefore either equal to H or contains additional edges and is therefore not minimal. 
> 证明：
> 显然，在构造上，我们可以知道 $\mathcal H$ 是 $P$ 的 I-map，$P$ 满足 $\mathcal I_p (\mathcal H)$，故根据引理4.1，可以知道 $P$ 满足 $\mathcal I (\mathcal H)$
> 极小性：如果从图中移除某个边 $\{X, Y\}$，则 $\mathcal H$ 就会编码成对独立性 $(X\perp Y \mid \mathcal X - \{X, Y\})$，而在 $P$ 中该成对独立性是不成立的，否则 $X-Y$ 一开始就不会被加入，因此，$\mathcal H$ 是极小的 I-map
> 独立性：$P$ 的其他 I-map $\mathcal H'$ 至少需要包含和 $\mathcal H$ 同样的边，如果没有额外的边，就等于 $\mathcal H$，因此要保持极小性，就需要等于 $\mathcal H$

It remains to show that the second deﬁnition results in the same minimal I-map. 

**Theorem 4.6** 
Let $P$ be a positive distribution. For each node $X$ , let ${\mathrm{MB}}_{P}(X)$ be a minimal set of nodes $U$ satisfying equation (4.3). We deﬁne a grap $\mathcal{H}$ by introducing an ed e $\{X,Y\}$ for all $X$ and all $Y\in{\mathrm{MB}}_{P}(X)$ . Then the Markov network H is the unique minimal I-map for P . 
> 定理：
> $P$ 为正分布，$\mathcal H$ 通过为满足式4.3的所有 $\{X, Y\}\quad (Y \in \text{MB}_P(X))$ 引入边得到，则 $\mathcal H$ 是 $P$ 的唯一极小 I-map

The proof is left as an exercise (exercise 4.11). 

Both of the techniques for constructing a minimal I-map make the assumption that the distribution $P$ is positive. As we have shown, for nonpositive distributions, neither the pairwise independencies nor the local independencies imply the global one. Hence, for a nonpositive distribution $P$ , construc g a graph $\mathcal{H}$ su that $P$ satisﬁes the pairwise assumptions for $\mathcal{H}$ does not guarantee that H is an I-map for P . Indeed, we can easily demonstrate that both of these constructions break down for nonpositive distributions. 
> 定理4.5和4.6都基于前提条件：$P$ 为正分布
> 对于非负分布，成对独立性和局部独立性都不包含全局独立性，因此构造使得 $P$ 满足 $\mathcal H$ 编码的成对独立性和局部独立性的 Markov 网络不一定是 $P$ 的 I-map，也就是 $P$ 满足 $\mathcal I_p (\mathcal H)$ 和满足 $\mathcal I_{\mathscr l}(\mathcal H)$ 不一定表明 $P$ 满足 $\mathcal I (\mathcal H)$

Example 4.7
Consider a nonpositive distribution P over four binary variables A; B; C; D that assigns nonzero probability only to cases where all four variables take on exactly the same value; for example, we might have P (a1; b1; c1; d1) = 0:5 and P (a0; b0; c0; d0) = 0:5. The graph H shown in figure 4.7 is one possible output of applying the local independence I-map construction algorithm to P : For example, P j= (A ? C; D j B), and hence fBg is a legal choice for MBP (A). A similar analysis shows that this network satisfies the Markov blanket condition for all nodes. However, it is not an I-map for the distribution.

If we use the pairwise independence I-map construction algorithm for this distribution, the network constructed is the empty network. For example, the algorithm would not place an edge between $A$ and $B$ , because $P\models(A\bot B\mid C,D)$ ⊥ | . Exactly the same a alysis sh s that no edges will be placed into the graph. However, the resulting network is not an I-map for P . 
> Example 4.7同样用了确定性依赖的 trick，分布 $P$ 中存在确定性依赖，使得通过局部独立性算法和成对独立性算法构建出来的 Markov 网络 $\mathcal H$ 都会编码额外的独立性，使得 $\mathcal H$ 不是 $P$ 的 I-map

Both these examples show that deterministic relations between variables can lead to failure in the construction based on local and pairwise independence. Suppose that $A$ and $B$ are two variables that are identical to each other and that both $C$ and $D$ are variables that correlated to both $A$ and $B$ so that $(C\;\bot\;D\;|\;A,B)$ holds. Since $A$ is identical to $B$ , we have that both $(A,D\perp C\mid B)$ and $(B,D\perp C\mid A)$ hold. In other words, it su ces to observe ne of these two variables to capture the relevant information both have about C and separate C from $D$ . In this case the Markov blanket of $C$ is not uniquely deﬁned. This ambiguity leads to the failure of both local and pairwise constructions. Clearly, identical variables are only one way of getting such ambiguities in local independencies. Once we allow nonpositive distribution, other distributions can have similar problems. 
> 变量之间的确定性关系会使得通过分布中的局部或者成对独立性构造 I-map 是失败的
> 例如，假设 $A, B$ 确定性相等，$C, D$ 与它们都相关，满足 $(C\perp D \mid A, B)$，因为 $A = B$，因此有 $(A, D \perp C \mid B)$ 和 $(B, C\perp C \mid A)$，此时，只需观测到 $A, B$ 中的一个，就足以分离 $C, D$，此时 $C$ 的 Markov 毯就不是唯一的，这会导致通过局部独立性构建 I-map 失败

Having deﬁned the notion of a minimal I-map for a distribution $P$ , we can now ask to what extent it represents the independencies in $P$ . More formally, we can ask whether every distribution has a perfect map. Clearly, the answer is no, even for positive distributions: 
> 即便正分布也不一定有完备 I-map

Consider a distribution arising from a three-node Bayesian network with a $\nu$ -structure, for example, the distribution induced in the Student example over the nodes Intelligence, Difculty, and Grade (ﬁgure 3.3). In the Markov network for this distribution, we must clearly have an edge between $I$ and $G$ and between $D$ and $G$ . Can we omit the edge between $I$ and $D$ ? No, because we do not have that $(I\perp D\mid G)$ holds for the distribut on; rather, we ve the opposite: $I$ and $D$ are dependent given G . Therefore, the only minimal I-map for this P is the fully connected graph, which does not capture the marginal independence $(I\perp D)$ that holds in $P$ . 
> 考虑 v-structure $I\rightarrow G \leftarrow D$，将其建模为 Markov 网络时，我们需要添加边 $I-G$ 和 $G-D$，但也要添加边 $I-D$，因为 $(I\perp D\mid G)$ 在分布中不成立，因此 $I-D$ 不是成对独立的，反而是在给定 $G$ 之后，$I, D$ 相互依赖
> 因此，$P$ 的唯一极小 I-map 就是完全图，而完全图没有编码 $P$ 中的边际独立性 $(I\perp D)$

This example provides another counterexample to the strong version of completeness men- tioned earlier. The only distributions for which separation is a sound and complete criterion for determining conditional independence are those for which $\mathcal{H}$ is a perfect map. 
> 只有存在 perfect map 的分布，其 $\mathcal H$ 的分离准则才是可靠且完备的

## 4.4 Parameterization Revisited 
Now that we understand the semantics and independence properties of Markov networks, we revisit some alternative representations for the parameterization of a Markov network. 
### 4.4.1 Finer-Grained Parameterization 
#### 4.4.1.1 Factor Graphs 
A Markov network structure does not generally reveal all of the structure in a Gibbs parameterization. In particular, one cannot tell from the graph structure whether the factors in the parameterization involve maximal cliques or subsets thereof. Consider, for example, a Gibbs distribution $P$ over a fully connected pairwise Markov network; that is, $P$ is parameterized by a factor for each pair of variables $X,Y\in{\mathcal{X}}$ . The clique potential parameterization would utilize a factor whose scope is the entire graph, and which therefore uses an exponential number of parameters. On the other hand, as we discussed in section 4.2.1, the number of parameters in the pairwise parameterization is quadratic in the number of variables. Note that the com- plete Markov network is not redundant in terms of conditional independencies $\mathrm{~-~}P$ does not factorize over any smaller network. Thus, although the ﬁner-grained structure does not imply additional independencies in the distribution (see exercise 4.6), it is still very signiﬁcant. 
>马尔可夫网络结构一般并不能完全揭示 Gibbs 参数化中的所有结构
>特别地，从图结构中我们无法判断参数化中的因子是否涉及最大团或是这些最大团的子集。
>例如，考虑一个完全连通的成对马尔可夫网络上的 Gibbs 分布 $P$；也就是说，$P$ 由每一对变量 $X, Y\in \mathcal X$ 的因子参数化。最大团势参数化将利用一个作用在整个图上的因子，因此需要用到指数数量的参数。另一方面，成对参数化的参数数量与变量的数量呈二次关系。（也就是完全连通的 Markov 网络有两种参数化方式，这在 Markov 图中不能直接看出来）
>值得注意的是，完全连通的马尔可夫网络在条件独立性方面并不是冗余的 —— $P$ 不会在任何更小的网络上分解。因此，尽管更精细的结构并不会在分布中暗示额外的独立性（参见练习题4.6），但它仍然非常重要。

An alternative representation that makes this structure explicit is a factor graph . A factor graph is a graph containing two types of nodes: one type corresponds, as usual, to random variables; the other corresponds to factors over the variables. Formally: 
> 因子图包含两类节点：一类节点对应于随机变量，另一类节点对应于随机变量上的因子

**Deﬁnition 4.13** factor graph 
A factor graph $\mathcal{F}$ is an undirected graph containing two types of nodes: variable nodes (denoted as ovals) and factor nodes (denoted as squares). The graph only contains edges between variable nodes and factor nodes. A factor graph $\mathcal{F}$ is parameterized by a set of factors, where each factor node $V_{\phi}$ is associated with precisely one factor $\phi.$ , whose scope is the set of variables that are neighbors of $V_{\phi}$ in the graph. A distribution $P$ factorizes over $\mathcal{F}$ if it can be represented as a set of factors of this form. 
> 定义：
> 因子图为无向图 $\mathcal F$，包含两类节点：变量节点（椭圆形）、因子节点（方形）
> 图仅包含变量节点和因子节点之间的边
> 因子图 $\mathcal F$ 由一组因子参数化，每个因子节点 $V_\phi$ 正好和一个因子 $\phi$ 关联，其作用域就是和 $V_\phi$ 相邻的变量节点
> 分解于 $\mathcal f$ 的分布 $P$ 可以被图中的一组因子表示

Factor graphs make explicit the structure of the factors in the network. For example, in a fully connected pairwise Markov network, the factor graph would contain a factor node for each of the $\textstyle{\binom{n}{2}}$ pairs of nodes; the factor node for a pair $X_{i},X_{j}$ would be connected to $X_{i}$ and $X_{j}$ ; by contrast, a factor graph for a distribution with a single factor over $X_{1},\dots,X_{n}$ would have a single factor node connected to all of $X_{1},\ldots,X_{n}$ (see ﬁgure 4.8). Thus, although the Markov networks for these two distributions are identical, their factor graphs make explicit the diference in their factorization. 
> 因子图显式在网络中表示了因子的结构
> 两个分布的 Markov 网络相同时，其因子图可以不同，可以根据其因子图确认其因子分解的形式

#### 4.4.1.2 Log-Linear Models 
Although factor graphs make certain types of structure more explicit, they still encode factors as complete tables over the scope of the factor. As in Bayesian networks, factors can also exhibit a type of context-speciﬁc structure — patterns that involve particular values of the variables. These patterns are often more easily seen in terms of an alternative parameteriz ation of the factors that converts them into log-space. 
> 因子图依旧用表格的形式表示因子
> 在贝叶斯网络中，我们讨论了 CPD 的特定上下文中的结构，也就是在特定变量有特定赋值时的情况
> 在因子的另一种参数化形式：将其转化到对数空间时，这类模式也非常常见

More precisely, we can rewrite a factor $\phi(D)$ as 

$$\phi({\cal D})=\exp(-\epsilon({\cal D})),$$

where $\epsilon(D)\,=\,-\ln\phi(D)$ is often called an energy function . 
> 将因子 $\phi (\pmb D)$ 重写为 $\phi (\pmb D) = \exp (-\epsilon (\pmb D))$
> 其中 $\epsilon (\pmb D) = -\ln \phi (\pmb D)$ 称为能量函数（注意这要求 $\phi(\pmb D) \ne 0$）

The use of the word “energy” derives from statistical physics, where the probability of a physical state (for example, a conﬁguration of a set of electrons), depends inversely on its energy. 
> 在统计物理学中，一个物理状态（例如一组电子的一个配置）的概率与它的能量成反比（能量越低概率越高）
> 因此也就和能量函数的反成正比

In this logarithmic representation, we have 

$$
P(X_{1},.\,.\,,X_{n})\propto\exp\left[-\sum_{i=1}^{m}\epsilon_{i}(\pmb D_{i})\right].
$$ 
The logarithmic representation ensures that the probability distribution is positive. Moreover, the logarithmic parameters can take any value along the real line. 
> 对数表示：
> $P (X_1, \dots, X_n)\propto \prod_{i=1}^m \phi (\pmb D_i) = \prod_{i=1}^m \exp (-\epsilon_i (\pmb D_i)) = \exp\left[-\epsilon_i(\pmb D_i)\right]$
> 对数表示下，概率一定是正数

Any Markov network parameterized using positive factors can be converted to a logarithmic representation. 
> 任意使用正因子参数化的 Markov 网络都可以被转化为对数表示

Fig4.9:

$$
\epsilon_{1}(A,B)\qquad\qquad\epsilon_{2}(B,C)\qquad\qquad\epsilon_{3}(C,D)\qquad\qquad\epsilon_{4}(D,A)
$$ 

$$
\begin{array}{r}{\left|\begin{array}{l l l}{0}&{b^{0}}&{-3.4}\\ {0}&{b^{1}}&{-1.61}\\ {1}&{b^{0}}&{0}\\ {1}&{b^{1}}&{-2.3}\end{array}\right|\left|\begin{array}{c c c c}{b^{0}}&{c^{0}}&{-4.61}\\ {b^{0}}&{c^{1}}&{0}\\ {b^{1}}&{c^{0}}&{0}\\ {b^{1}}&{c^{1}}&{-4.61}\end{array}\right|\left|\begin{array}{c c c c}{c^{0}}&{d^{0}}&{0}\\ {c^{0}}&{d^{1}}&{-4.61}\\ {c^{1}}&{d^{0}}&{-4.61}\\ {c^{1}}&{d^{1}}&{0}\end{array}\right|\left|\begin{array}{c c c c}{d^{0}}&{a^{0}}&{-4.61}\\ {d^{0}}&{a^{1}}&{0}\\ {d^{1}}&{a^{0}}&{0}\\ {d^{1}}&{a^{1}}&{-4.61}\end{array}\right|}\end{array}
$$ 

Example 4.9 
Figure 4.9 shows the logarithmic representation of the clique potential parameters in ﬁgure 4.1. We can see that the “1” entries in the clique potentials translate into $"0"$ entries in the energy function. 

This representation makes certain types of structure in the potentials more apparent. For example, we can see that both $\epsilon_{2}(B,C)$ and $\epsilon_{4}(D,A)$ are constant multiples of an energy function that ascribes 1 to instantiations where the values of the two variables agree, and 0 to the instantiations where they do not. 

We can provide a general framework for capturing such structure using the following notion: 

**Deﬁnition 4.14** feature 
Let D be a subset of variables. We define a feature f(D) to be a function from Val(D) to R.
> 定义：
> $\pmb D$ 为变量子集，定义特征 $f (\pmb D)$ 为从 $Val (\pmb D)$ 到 $R$ 的函数 

A feature is simply a factor without the nonnegativity requirement. One type of feature of particular interest is the indicator feature that takes on value 1 for some values $\pmb{y}\in V a l(\pmb{D})$ and 0 otherwise. 
> 特征简单来说就是没有非负要求的因子
> 一类常用的特征是指示器特征，它对于特定的 $\pmb y \in Val (\pmb D)$ 取值为 1，否则取值为 0

Features provide us with an easy mechanism for specifying certain types of interactions more compactly. 

Example 4.10 
Consider a situation where $A_{1}$ and $A_{2}$ each have $\ell$ values $a^{1},\ldots,a^{\ell}$ . Assume that our distribution is such that we prefer situations where $A_{1}$ and $A_{2}$ take on the same value, but otherwise have no preference. Thus, our energy function might have the following form: 

$$
\epsilon(A_{1},A_{2})=\left\{\begin{array}{l l}{{-3\qquad\qquad A_{1}=A_{2}}}\\ {{0\qquad\qquad o t h e r w i s e}}\end{array}\right.
$$ 
Represented as a full factor, this clique potential requires $\ell^{2}$ values. However, it can also be represented as a log-linear function in terms of $a$ feature $f(A_{1},A_{2})$ that is an indicator function for the event $A_{1}=A_{2}$ . The energy function is then simply a constant multiple $-3$ of this feature. 

Thus, we can provide a more general deﬁnition for our notion of log-linear models: 

**Deﬁnition 4.15** log-linear model 
A distribution $P$ is a log-linear model over a Markov network $\mathcal{H}$ if it is associated with: 
- a set of features $\mathcal{F}=\{f_{1}(D_{1}),.\,.\,.\,,f_{k}(D_{k})\}$ , where each $D_{i}$ is a complete subgraph in $\mathcal{H}$ , 
- a set of weights $w_{1},\dots,w_{k}$ , 

such that 

$$
P(X_{1},\dots,X_{n})=\frac{1}{Z}\exp\left[-\sum_{i=1}^{k}w_{i}f_{i}(D_{i})\right].
$$ 
Note that we can have several features over the same scope, so that we can, in fact, represent a standard set of table potentials. (See exercise 4.13.) 
> 定义：
> Markov 网络 $\mathcal H$ 上的对数线性模型是一个分布 $P$，它满足：
> - 和一组特征 $\mathcal F = \{f_1 (\pmb D_1), \dots, f_k (\pmb D_k)\}$ 关联，其中每个 $\pmb D_i$ 都是 $\mathcal H$ 中的完全子图
> - 和一组权重 $w_1,\dots, w_k$ 关联
> 使得 $P (X_1, \dots, X_n) = \frac 1 Z \exp \left[ -\sum_{i=1}^k w_i f_i(\pmb D_i)\right]$

> $P (X_1, \dots, X_n) \propto \exp\left[-\sum_{i=1}^k w_if_i(\pmb D_i)\right] = \prod_{i=1}^k \exp \left[-w_if_i(\pmb D_i)\right]$
> 因此可以认为对数线性模型就对应于把因子设定为了 $\phi_i (\pmb D_i) = \exp \left[-w_i f_i (\pmb D_i)\right]$ 的形式，也就是将能量函数的形式设定为了 $w_i f_i (\pmb D_i)$

The log-linear model provides a much more compact representation for many distributions, especially in situations where variables have large domains such as text (such as box 4.E). 

#### 4.4.1.3 Discussion 
We now have three representations of the parameterization of a Markov network. The Markov network denotes a product over potentials on cliques. A factor graph denotes a product of factors. And a set of features denotes a product over feature weights. Clearly, each representation is ﬁner-grained than the previous one and as rich. A factor graph can describe the Gibbs distribution, and a set of features can describe all the entries in each of the factors of a factor graph. 
> 目前我们对 Markov 网络的参数化有了三种表示：Markov 网络表示多个团势能函数的乘积、因子图表示多个因子的乘积、一组特征表示特征的加权乘积
> 每个表示都比上一个更精细：因子图可以表示 Gibbs 分布、一组特征可以描述因子图中因子的所有项

Depending on the question of interest, diferent representations may be more appropriate. For example, a Markov network provides the right level of abstraction for discussing independence queries: The ﬁner-grained representations of factor graphs or log-linear models do not change the independence assertions made by the model. On the other hand, as we will see in later chapters, factor graphs are useful when we discuss inference, and features are useful when we discuss parameterizations, both for hand-coded models and for learning. 

**Box 4.C — Concept: Ising Models and Boltzmann Machines.** 
One of the earliest types of Markov network models is the Ising model , which ﬁrst arose in statistical physics as a model for the energy of a physical system involving a system of interacting atoms. In these systems, each atom is associ- ated with a binary-valued random variable $X_{i}\in\{+1,-1\}$ , whose value deﬁnes the direction of the atom’s spin. The energy function associated with the edges is deﬁned by a particularly simple parametric form: 
> 最早的一类 Markov 网络模型为 lsing 模型，出现在统计物理学，建模包含了交互的原子的物理系统的能量
> 系统中，每个原子和一个二值变量 $X_i \in \{+1, -1\}$，其值定义了原子的自旋方向
> 和其相关的能量函数和边关联，其参数化形式很简单：

$$
\epsilon_{i,j}(x_{i},x_{j})=w_{i,j}x_{i}x_{j}
$$ 
This energy is symmetric in $X_{i},X_{j}.$ ; it makes a contribution of $w_{i,j}$ to the energy function when $X_{i}=X_{j}$ (so both atoms have the same spin) and a contribution of $-w_{i,j}$ otherwise. 
> 能量在 $X_i, X_j$ 之间是对称的，如果两个原子的自旋方向相同，就贡献 $w_{ij}$ 的能量，否则就贡献 $-w_{ij}$ 的能量

Our model also contains a set of parameters $u_{i}$ that encode individual node potentials; these bias individual variables to have one spin or another. 
> 模型还包含一组参数 $u_i$ 来编码独立的节点势能，使得对应原子向某个特定的自旋方向偏置

As usual, the energy function deﬁnes the following distribution: 

$$
P(\xi)=\frac{1}{Z}\exp\left(-\sum_{i<j}w_{i,j}x_{i}x_{j}-\sum_{i}u_{i}x_{i}\right).\tag{4.4}
$$ 
As we can see, when $w_{i,j}\,>\,0$ the model prefers to align the spins of the two atoms; in this case, the interaction is called ferromagnetic . When $w_{i,j}<0$ the interaction is called anti ferromagnetic . When $w_{i,j}=0$ the atoms are non-interacting . 
> 能量函数定义的分布如上，仍然遵循能量越小，概率越大的形式
> 显然，模型中，$w_{ij} > 0$ 促使原子 $i, j$ 的自旋方向相同，$w_{ij} < 0$ 促使原子 $i, j$ 的自旋方向相反，$w_{ij} = 0$ 表示两个原子并未交互，二者的方向关系无所谓

Much work has gone into studying particular types of Ising models, attempting to answer a variety of questions, usually as the number of atoms goes to inﬁnity. For example, we might ask the probability of a conﬁguration in which a majority of the spins are $+1$ or $-1$ , versus the probability of more mixed conﬁgurations. The answer to this question depends heavily on the strength of the interaction between the variables; so, we can consider adapting this strength (by multiplying all weights by a temperature parameter ) and asking whether this change causes a phase transition in the probability of skewed versus mixed conﬁgurations. These questions, and many others, have been investigated extensively by physicists, and the answers are known (in some cases even analytically) for several cases. 
> 调节原子之间交互的强度：为权重乘上温度参数

Related to the Ising model is the Boltzmann distribution ; here, the variables are usually taken to have values $\{0,1\}$ , but still with the energy form of equation (4.4). Here, we get a nonzero contribution to the model from an edge $(X_{i},X_{j})$ only when $X_{i}=X_{j}=1,$ ; however, the resulting energy can still be reformulated in terms of an Ising model (exercise 4.12). 
> Blotzmann 分布中，变量一般取值 $\{0, 1\}$，但能量形式和式4.4仍相同
> 此时，只有在 $X_i = X_j = 1$ 时，才对系统能量有贡献，但该形式仍然可以重构为 Ising 模型

The popularity of the Boltzmann machine was primarily driven by its similarity to an activation model for neurons. To understand the relationship, we note that the probability distribution over each variable $X_{i}$ given an assignment to is neighbors is $\text{sigmoid}(z)$ where 

$$
z=-(\sum_{j}w_{i,j}x_{j})-w_{i}.
$$ 
This function is a sigmoid of a weighted combination of $X_{i}$ ’s neighbors, weighted by the strength and direction of the connections between them. This is the simplest but also most popular mathematical approximation of the function employed by a neuron in the brain. Thus, if we imagine a process by which the network continuously adapts its assignment by resampling the value of each variable as a stochastic function of its neighbors, then the “activation” probability of each variable resembles a neuron’s activity. This model is a very simple variant of a stochastic, recurrent neural network. 
> Boltzmann 机和神经元的激活模型很相似，考虑 Boltzmann 机中每个变量 $X_i$ 在给定它的邻居的赋值的情况下的分布，它实际为一个 sigmoid 函数 $\text{sigmoid}(z)$，其中 $z = -(\sum_j w_{ij}x_j)- w_i$ ( $X_i = 1$ 时相关的能量)，$z$ 实际上就是对和 $X_i$ 相连的邻居的加权求和再加上偏置
> ($X_i = 0$ 时，$P (X_i \mid \text{Pa}_{X_i}) = \frac 1 Z\exp (0) = 1$
> $X_i = 1$ 时，$P (X_i \mid \text{Pa}_{X_i}) = \frac 1 Z\exp \left(-\sum_jw_{ij}x_j -w_i\right)= \exp(z)$
> 显然，$Z = 1 + \exp (z)$，故 $P (X_i = 1\mid \text{Pa}_{X_i}) = \frac {\exp (z)}{1 + \exp (z)} = \text{sigmoid}(z)$)
> 该函数是对神经网络的最常用的近似数学形式

Box 4.D — Concept: Metric MRFs. 
One important class of MRFs comprises those used for la- beling . Here, we ve a graph of nodes $X_{1},\dots,X_{n}$ related by a set of edges $\mathcal{E}$ , and we wish to assign to each $X_{i}$ a label in the space $\mathcal{V}=\{v_{1},.\,.\,.\,,v_{K}\}$ . Each node, taken in isolation, has its preferences among the possible labels. However, we also want to impose a soft “smoothness” constraint over the graph, in that neighboring nodes should take “similar” values. 

We encode the individual node preferences as node potentials in a pairwise MRF and the smooth- ness preferences as edge potentials. For reasons that will become clear, it is traditional to encode these models in negative log-space, using energy functions. As our objective in these models is inevitably the MAP objective, we can also ignore the partition function, and simply consider the energy function: 

$$
E(x_{1},.\,.\,,x_{n})=\sum_{i}\epsilon_{i}(x_{i})+\sum_{(i,j)\in{\mathcal{E}}}\epsilon_{i,j}(x_{i},x_{j}).
$$ 
Our goal is then to minimize the energy: 

$$
\arg\operatorname*{min}_{x_{1},\ldots,x_{n}}E(x_{1},\ldots,x_{n}).
$$ 
We now need to provide a formal deﬁnition for the intuition of “smoothness” described earlier. There are many diferent types of conditions that we can impose; diferent conditions allow diferent methods to be applied. 

One of the simplest in this class of models is a slight variant of the Ising model , where we have that, for any $i,j$ : 

$$
\epsilon_{i,j}(x_{i},x_{j})=\left\{\begin{array}{l l}{{0}}&{{\qquad\qquad x_{i}=x_{j}}}\\ {{\lambda_{i,j}}}&{{\qquad\qquad x_{i}\neq x_{j},}}\end{array}\right.
$$ 
for $\lambda_{i,j}\geq0$ In this model, we obtain the lowest possible rwise energy (0) when two neighboring nodes $X_{i},X_{j}$ take the same value, and a higher energy $\lambda_{i,j}$ when they do not. 

This simple model has been generalized in many ways. The Potts model extends it to the setting of more than two labels. An even broader class contains models where we have a distance function on the labels, and where we prefer neighboring nodes to have labels that are a smaller distance apart. More precisely, a function $\mu\ :\ \mathcal{V}\times\mathcal{V}\mapsto[0,\infty)$ is $^a$ metric if it satisﬁes: 

- Reﬂexivity: $\mu(v_{k},v_{l})=0$ if and only if $k=l$ ;

- Symmetry: $\mu(v_{k},v_{l})=\mu(v_{l},v_{k}).$ ; 

$$
\begin{array}{c c c}{{a^{0}}}&{{b^{0}}}&{{-4.4}}\\ {{a^{0}}}&{{b^{1}}}&{{-1.61}}\\ {{a^{1}}}&{{b^{0}}}&{{-1}}\\ {{a^{1}}}&{{b^{1}}}&{{-2.3}}\end{array}\left|\begin{array}{c c c}{{b^{0}}}&{{c^{0}}}&{{-3.61}}\\ {{b^{0}}}&{{c^{1}}}&{{+1}}\\ {{b^{1}}}&{{c^{0}}}&{{0}}\\ {{b^{1}}}&{{c^{1}}}&{{-4.61}}\end{array}\right|
$$ 
We say that $\mu$ is a semimetric if it satisﬁes reﬂexivity and symmetry. We can now deﬁne a metric MRF (or a semimetric MRF ) by deﬁning $\epsilon_{i,j}(v_{k},v_{l})=\mu(v_{k},v_{l})$ for all $i,j$ , where $\mu$ is a metric (semimetric). We note that, as deﬁned, this model assumes that the distance metric used is the same for all pairs of variables. This assumption is made because it simpliﬁes notation, it often holds in practice, and it reduces the number of parameters that must be acquired. It is not required for the inference algorithms that we present in later chapters. Metric interactions arise in many applications, and play a particularly important role in computer vision (see box 4.B and box 13.B). For example, one common metric used is some form of truncated $p$ -norm (usually $p=1$ or $p=2.$ ): 

$$
\epsilon(x_{i},x_{j})=\operatorname*{min}(c\|x_{i}-x_{j}\|_{p},{\mathrm{dist}}_{\mathrm{max}}).
$$ 
### 4.4.2 Over parameterization 
Even if we use ﬁner-grained factors, and in some cases, even features, the Markov network parameterization is generally over parameterized. That is, for any given distribution, there are multiple choices of parameters to describe it in the model. Most obviously, if our graph is a single clique over $n$ binary variables $X_{1},\dots,X_{n}$ , then the network is associated with a clique potential that has $2^{n}$ parameters, whereas the joint distribution only has $2^{n}-1$ independent parameters. 
> 对于任意给定的分布，存在多个参数选择来在模型中描述这个分布

A more subtle point arises in the context of a nontrivial clique structure. Consider a pair of cliques $\{A,B\}$ and $\{B,C\}$ . The energy function $\epsilon_{1}(A,B)$ (or its corresponding clique potential) contains information not only about the interaction between $A$ and $B$ , but also about the distribution of the individual variables $A$ and $B$ . Similarly, $\epsilon_{2}(B,C)$ gives us information about the individual variables $B$ and $C$ . The information about $B$ can be placed in either of the two cliques, or its contribution can be split between them in arbitrary ways, resulting in many diferent ways of specifying the same distribution. 
> 例如考虑两对团 $\{A, B\}$ 和 $\{B, C\}$，能量函数 $\epsilon_1 (A, B)$ 包含了二者的交互，同时也包含了 $A, B$ 各自独立的分布，$\epsilon_2 (B, C)$ 也是如此，故关于 $B$ 的信息可以放在两个团其中之一，或者以任意的方式在二者之间分离，因此指定相同的分布就有了不同的方式

Example 4.11
Consider the energy functions 1(A; B) and 2(B; C) in figure 4.9. The pair of energy functions shown in figure 4.10 result in an equivalent distribution: Here, we have simply subtracted 1 from 1(A; B) and added 1 to 2(B; C) for all instantiations where B = b0. It is straightforward to check that this results in an identical distribution to that of ﬁgure 4.9. In inst ere $B\neq b^{0}$ the energy function returns exactly the same value as before. In cases where $B\,=\,b^{0}$ , the actual values of the energy functions have changed. However, because the sum of the energy functions on each instance is identical to the original sum, the probability of the instance will not change. 

Intuitively, the standard Markov network representation gives us too many places to account for the inﬂuence of variables in shared cliques. Thus, the same distribution can be represented as a Markov network (of a given structure) in inﬁnitely many ways. It is often useful to pick one of this inﬁnite set as our chosen parameterization for the distribution. 
> 直观上看，标准的 Markov 网络表示有近乎无限的方式表示在团中变量的影响，我们需要选择以一种对分布进行参数化

#### 4.4.2.1 Canonical Parameterization 
The canonical parameterization provides one very natural approach to avoiding this ambiguity in the parameterization of a Gibbs distribution $P$ . This canonical parameterization requires that the distribution $P$ be positive. It is most convenient to describe this parameterization using energy functions rather then clique potentials. For this reason, it is also useful to consider a log- transform of $P$ : For any assignment $\xi$ to $\mathcal{X}$ , we use $\ell(\xi)$ to denote $\ln P(\xi)$ . This transformation is well deﬁned because of our positivity assumption. 
> 规范参数化方式要求 $P$ 为正分布
> 一般使用能量函数而不是团势能来描述该参数化，故一般考虑 $P$ 的对数形式：
> 对于任意对 $\mathcal X$ 的赋值 $\xi$，我们使用 $\mathscr l (\xi)$ 来表示 $\ln P (\xi)$ (因为考虑对数形式，因此要求是正分布)

The canonical parameterization of a Gibbs distribution over $\mathcal{H}$ is deﬁned via set of energy functions over all cliques. Thus, for example, the Markov network in ﬁgure 4.4b would have energy functions for the two cliques $\{A,B,D\}$ and $\{B,C,D\}$ , energy functions for all possible pairs of variables except the pair $\{A,C\}$ (a total of ﬁve pairs), energy functions for all four singleton sets, and a constant energy function for the empty clique. 
> Gibbs 分布在 $\mathcal H$ 上的规范参数化通过在所有团上的一组能量函数定义（所有的团包括了：所有的完全子图、每个节点自己、以及为空团也定义一个常数能量函数）

At ﬁrst glance, it appears that we have only increased the number of parameters in the speciﬁcation. However, as we will see, this approach uniquely associates the interaction parameters for a subset of variables with that subset, avoiding the ambiguity described earlier. As a consequence, many of the parameters in this canonical parameterization are often zero. 
> 该方法唯一地将一个变量子集的交互参数和该变量子集关联
> 该方法中，许多参数常常是0

The canonical parameterization is deﬁned relative to a particular ﬁxed assignment $\xi^{*}\ =$ $\left(x_{1}^{*},\cdot\cdot\cdot,x_{n}^{*}\right)$ to the network variables $\mathcal{X}$ . is assignment can chosen arbitraly. For any subset of variables Z , and any assignment x to some subset of X that contain $Z$ , we deﬁne the assignment $\pmb{x}_{Z}$ to be $_{x\langle Z\rangle}$ , that is, the assignment in $_{_{x}}$ the variables in Z . Conversely, we deﬁne $\xi_{-Z}^{*}$ to be $\xi^{*}\langle\mathcal{X}-Z\rangle$ , that is, the assignment in $\xi^{*}$ to the variables outside Z . − can now construct an assignment $(x_{Z},\xi_{-Z}^{\ast})$ that keeps the assignments to the variables in Z − as speciﬁed in $_{_{x}}$ , and augments it using the default values in $\xi^{*}$ . 
> 规范参数化相对于对于网络变量 $\mathcal X$ 的特定赋值 $\xi^* = (x_1^*, \cdots, x_n^*)$ 定义，该赋值可以任意选择
> 对于任意的变量子集 $\pmb Z$，以及任意对某个包含 $\pmb Z$ 的 $\mathcal X$ 子集的赋值 $\pmb x$，定义赋值 $\pmb x_{\pmb Z}$ 为 $\pmb x\langle \pmb Z\rangle$，也就是 $\pmb x$ 中对于 $\pmb Z$ 的赋值，还定义 $\xi_{-\pmb Z}^*$ 为 $\xi^*\langle\mathcal X-\pmb Z\rangle$，即 $\xi^*$ 中对于 $\pmb Z$ 以外的变量的赋值
> 由此，我们构造赋值 $(\pmb x_{\pmb Z}, \xi^*_{-\pmb Z})$，即对于 $\pmb Z$ 中的变量，使用 $\pmb x$ 中指定的赋值，对于 $\pmb Z$ 外的变量，保持 $\xi^*$ 中的默认赋值

The canonical energy function for a clique $\pmb D$ is now deﬁned as follows: 

$$
\epsilon_{\pmb D}^{*}(\pmb d)=\sum_{\pmb Z\subseteq \pmb D}(-1)^{|\pmb D-\pmb Z|}\ell(\pmb d_{\pmb Z},\xi_{-\pmb Z}^{*}),\tag{4.8}
$$ 
where the sum is over all subsets of $D$ , including $D$ itself and the empty set $\varnothing$ . Note that all of the terms in the summation have a scope that is contained in D , which in turn is part of a clique, so that these energy functions are legal relative to our Markov network structure. 
> 团 $\pmb D$ 的规范能量函数定义为式4.8，其中的求和 $\sum_{\pmb Z\subseteq \pmb D}$ 是在 $\pmb D$ 的所有子集上求和，包括 $\pmb D$ 自己和空集

This formula performs an inclusion-exclusion computation. For a set $\{A,B,C\}$ , it ﬁrst subtracts out the inﬂuence of all of the pairs: $\{A,B\},\ \{B,C\}$ , and $\{C,A\}$ . However, this process oversubtracts the inﬂuence of the individual variables. Thus, their inﬂuence is added back in, to compensate. More generally, consider any subset of variables $\pmb Z\subseteq \pmb D$ . Intuitively, it make a “contribution” once for every subset $\pmb{U}\supseteq\pmb{Z}$ . Except for $\pmb U = \pmb D$ , the number of times that $\pmb Z$ appears is even — there is an even number of subsets $\pmb U \supseteq \pmb Z$ — and the number of times it appears with a positive sign is equal to the number of times it appears with a negative sign. Thus, we have effectively eliminated the net contribution of the subsets from the canonical energy function.
> 该公式执行的是包含-排斥计算（通过系数 $(-1)^{|\pmb D - \pmb Z|}$）
> 考虑任意变量子集 $\pmb Z \subseteq \pmb D$，直观上，它在每个 $\pmb U \supseteq \pmb Z$ 都对总能量函数做出一次贡献，除去 $\pmb U = \pmb D$，$\pmb Z$ 的超集的数量是偶数，其中一半的时候其系数是-1，另一半的时候其系数是+1，故 $\pmb Z$ 的净影响是在规范参数化中被完全消除的

Let us consider the effect of the canonical transformation on our Misconception network.

Let us choose $(a^{0},b^{0},c^{0},d^{0})$ as our arbitrary assignment on which to base the canonical parameterization. The resulting energy functions are shown in ﬁgure 4.11. For example, the energy value $\epsilon_{1}^{*}(a^{1},b^{1})$ was computed as follows:

$\begin{array}{r l}&{\ell(a^{1},b^{1},c^{0},d^{0})-\ell(a^{1},b^{0},c^{0},d^{0})-\ell(a^{0},b^{1},c^{0},d^{0})+\ell(a^{0},b^{0},c^{0},d^{0})=}\\ &{\phantom{a^{1}}-13.49--11.18--9.58+-3.18=4.09}\end{array}$ 

Note that many of the entries in the energy functions are zero. As discussed earlier, this phenomenon is fairly general, and occurs because we have accounted for the inﬂuence of small subsets of variables separately, leaving the larger factors to deal only with higher-order inﬂuences. We also note that these canonical parameters are not very intuitive, highlighting yet again the diffculties of constructing $^a$ reasonable parameter iz ation of a Markov network by hand. 

This canonical parameterization deﬁnes the same distribution as our original distribution $P$ : 
> 规范参数化定义了和原来的分布 $P$ 同样的分布

**Theorem 4.7**
Let $P$ be a positive Gibbs distribution over $\mathcal{H}$ , and let $\epsilon^{*}(\mathsf{D}_{i})$ for each clique $\mathsf{D}_{i}$ be deﬁned as speciﬁed in equation (4.8). Then 
> 定理：
> $P$ 为 $\mathcal H$ 上的正 Gibbs 分布，$\epsilon^*(\pmb D_i)$ 为每个团的规范能量函数 $\pmb D_i$，其定义如 (4.8)，则 $P$ 可以写为

$$
P(\xi)=\exp\left[\sum_{i}\epsilon_{\pmb{D}i}^{*}(\xi\langle\pmb{D}_{i}\rangle)\right].
$$ 
The proof for the case where $\mathcal{H}$ consists of a single clique is fairly simple, and it is left as an exercise (exercise 4.4). The general case follows from results in the next section. 
> 证明：
> 当 $\mathcal H$ 仅由一个团构成时，有

$$
\begin{align}
P(\xi) &=\exp\left[\epsilon_{\mathcal X}^*(\xi)\right]\\
&=\exp\left[\sum_{\pmb Z\subseteq \mathcal X}(-1)^{|\mathcal X-\pmb Z|}\ell(\xi_{\pmb Z},\xi_{-\pmb Z}^{*})\right]\\
&=\prod_{\pmb Z \subseteq \mathcal X}\exp\left[(-1)^{|\mathcal X-\pmb Z|}\ell(\xi_{\pmb Z},\xi_{-\pmb Z}^{*})\right]\\
&=\prod_{\pmb Z \subseteq \mathcal X}\exp\left[(-1)^{|\mathcal X-\pmb Z|}\ln P(\xi_{\pmb Z},\xi_{-\pmb Z}^{*})\right]\\
&=\prod_{\pmb Z \subseteq \mathcal X}\exp\left[\ln P(\xi_{\pmb Z},\xi_{-\pmb Z}^{*})^{(-1)^{{|\mathcal X - \pmb Z|}}}\right]\\
&=\prod_{\pmb Z \subseteq \mathcal X}\left[ P(\xi_{\pmb Z},\xi_{-\pmb Z}^{*})\right]^{(-1)^{{|\mathcal X - \pmb Z|}}}\\
&=P(\xi_{\mathcal X}, \xi^*_{-\mathcal X})\\
&=P(\xi)
\end{align}
$$

The canonical parameterization gives us the tools to prove the Hammersley-Clifford theorem, which we restate for convenience.

**Theorem 4.8**
Let $P$ be a positive distribution over $\mathcal{X}$ , a $\mathcal{H}$ a Markov network graph over $\mathcal{X}$ . If $\mathcal{H}$ is an $I\cdot$ -map for P , then P is a Gibbs distribution over . 
> 定理：
> $P$ 为 $\mathcal X$ 上的正分布，$\mathcal H$ 为 $\mathcal X$ 上的 Markov 网络，如果 $\mathcal H$ 是 $P$ 的 I-map，则 $P$ 是 $\mathcal X$ 上的 Gibbs 分布

Proof To prove this result, we need to show the existence of a Gibbs parameter iz ation for any distribution $P$ that satisﬁes the Markov assumptions associated with $\mathcal{H}$ . The proof is constructive, and simply uses the canonical parameterization shown earlier in this section. Given $P$ , we deﬁne an energy function for all subsets $\pmb D$ of nodes in the graph, regardless of whether they are cliques in the graph. This energy function is deﬁned exactly as in equation (4.8), relative to some speciﬁc ﬁxed assignment $\xi^{*}$ used to deﬁne the canonical parameterization. The distribution deﬁned using this set of energy functions is $P$ : the argument is identical to the proof of theorem 4.7, for the case where the graph consists of a single clique (see exercise 4.4). 
> 证明：
> 我们需要证明对于任意满足和 $\mathcal H$ 相关的 Markov 假设的分布 $P$ 都存在 Gibbs 参数化，故证明是构造性的，使用了规范参数化
> 给定 $P$，我们为所有图中的节点子集 $\pmb D$ 定义能量函数，无论该节点子集是否构成团，能量函数的定义遵循 (4.8)，也就是相对于某个特定的固定赋值 $\xi^*$ 来定义规范参数化
> 使用这一组能量函数定义的分布就是 $P$，该结论来源于定理4.7

It remains only to show that the resulting distribution is a Gibbs distribution over $\mathcal{H}$ . To show that, we need to show that the factors $\epsilon^{*}(\pmb D)$ are identically 0 whenever $\pmb D$ is not a clique in the graph, that is, whenever the nodes in $\pmb D$ do not form a fully connected subgraph.
> 接下来，我们需要证明这一组能量函数定义的分布也是 $\mathcal H$ 上的 Gibbs 分布
> 为此，我们需要证明因子 $\epsilon^*(\pmb D)$ 在 $\pmb D$ 不是图中的团的时候等于0

Assume that we have $X,Y\in \pmb D$ such that there is no edge between $X$ and $Y$ . 
> 假设 $\pmb D$ 中存在两个节点 $X, Y$ 之间没有边

For this proof, it helps to introduce the notation

$$
\sigma_{\pmb Z}[\pmb x]=(\pmb x_{\pmb Z},\xi_{-\pmb Z}^{\ast}).
$$ 
Plugging this notation into equation (4.8), we have that: 

$$
\epsilon_{\pmb D}^{*}(\pmb d)=\sum_{\pmb Z\subseteq \pmb D}(-1)^{|\pmb D-\pmb Z|}\ell(\sigma_{\pmb Z}[\pmb d]).
$$ 
> 引入记号 $\sigma_{\pmb Z}[\pmb d] = (\pmb d_{\pmb Z}, \xi^*_{-\pmb Z})$ ，以方便表示

We now rearrange the sum over subsets $\pmb Z$ into a sum over groups of subsets.  $\pmb W\subseteq \pmb D-\{X,Y\}$ ; then $\pmb W,\,\pmb W\cup\{X\},\,\pmb W\cup\{Y\}$ and $\pmb W\cup\{X,Y\}$ are all subsets of $\pmb Z$ . 
> 令 $\pmb W \subseteq \pmb D - \{X, Y\}$，然后将 $\pmb D$ 的全部子集 $\pmb Z$ 表示为四个组：$\pmb W,\pmb W \cup \{X\}, \pmb W\cup \{Y\}, \pmb W\cup \{X, Y\}$

Hence, we can rewrite the summation over subsets of $\pmb D$ as a summation over subsets of $\pmb D-\{X,Y\}$ : 
> 将和式重写为：

$$
\begin{align}
\epsilon_{\pmb D}^*(\pmb d) &= \sum_{\pmb W \subseteq \pmb D- \{X, Y\}}(-1)^{|\pmb D - \{X, Y\} - \pmb W|}\tag{4.9}\\
&\quad (\ell(\sigma_{\pmb W}[\pmb d]) - \ell(\sigma_{\pmb W \cup \{X\}}[\pmb d]) - \ell(\sigma_{\pmb W\cup \{Y\}}[\pmb d]) + \ell(\sigma_{\pmb W\cup \{X, Y\}}[\pmb d]))
\end{align}
$$

Now consider a speciﬁc subset $W$ in this sum, and let $u^{*}$ be $\xi^{*}\langle\mathcal{X}-D\rangle$ — the assignment to $\mathcal{X}-D$ in $\xi$ . We now have that: 
> 考虑某个特定的子集 $\pmb W$，令 $\pmb u^*$ 表示 $\xi^*\langle\mathcal X - \pmb D\rangle$，即 $\xi$ 中对于 $\mathcal X - \pmb D$ 的赋值，我们有：

$$
\begin{align}
\ell(\sigma_{\pmb W \cup \{X, Y\}}[\pmb d]) - \ell(\sigma_{\pmb W \cup \{X\}}[\pmb d]) &= \ln \frac {P(x, y, \pmb w, \pmb u^*)}{P(x, y^*,\pmb w, \pmb u^*)}\\
&=\ln\frac {P(y\mid x, \pmb w, \pmb u^*)P(x, \pmb w, \pmb u^*)}{P(y^*\mid x, \pmb w, \pmb u^*)P(x, \pmb w, \pmb u^*)}\\
&=\ln\frac {P(y\mid x^*, \pmb w, \pmb u^*)P(x, \pmb w, \pmb u^*)}{P(y^*\mid x^*, \pmb w, \pmb u^*)P(x, \pmb w, \pmb u^*)}\\
&=\ln\frac {P(y\mid x^*, \pmb w, \pmb u^*)P(x^*, \pmb w, \pmb u^*)}{P(y^*\mid x^*, \pmb w, \pmb u^*)P(x^*, \pmb w, \pmb u^*)}\\
&=\ln\frac {P(x^*, y,\pmb w, \pmb u^*)}{P(x^*,y^*, \pmb w, \pmb u^*)}\\
&=\ell(\sigma_{\pmb W \cup \{Y\}}[\pmb d])-\ell(\sigma_{\pmb W}[\pmb d])
\end{align}
$$

where the third equality is a consequence of the fact that $X$ and $Y$ are not connected directly by an edge, and hence we have that $P\models(X\ \bot\ Y\ |\ \mathcal{X}-\{X,Y\})$ . 
> 其中，第三个等式是因为 $X, Y$ 并没有直接相连，因此有 $P\vDash (X \perp Y \mid \mathcal X - \{X, Y\})$，故 $X$ 在条件中取任何值都无所谓；第四个等式是将 $\frac {P (x, \pmb w, \pmb u^*)}{P (x, \pmb w, \pmb u^*)}$ 替换为了 $\frac {P (x^*, \pmb w, \pmb u^*)}{P (x^*, \pmb w, \pmb u^*)}$，也就是用1替换1

Thus, we have that each term in the outside summation in equation (4.9) adds to zero, and hence the summation as a whole is also zero, as required. 
> 因此，我们可以得到式 (4.9) 中，对于存在不直接相连的两个点的团 $\pmb D$，和式中的四个项求和得到0，因此整个和式的结果也是0

For positive distributions, we have already shown that all three sets of Markov assumptions are equivalent; putting these results together with theorem 4.1 and theorem 4.2, we obtain that, **for positive distributions, all four conditions — factorization and the three types of Markov assumptions — are all equivalent.** 
> 我们已经知道，对于正分布，三个 Markov 假设是等价的，此时再加上定理4.8，我们可以知道对于正分布 $P$：$P$ 在 $\mathcal H$ 上分解和 $P$ 满足 $\mathcal H$ 上的三个 Markov 独立性假设这四个条件都是等价的

#### 4.4.2.2 Eliminating Redundancy 
An alternative approach to the issue of overparameterization is to try to eliminate it entirely. We can do so in the context of a feature-based representation, which is sufciently ﬁne-grained to allow us to eliminate redundancies without losing expressive power. The tools for detecting and eliminating redundancies come from linear algebra. 
> 在基于特征的表示下，可以借由线性代数移除多余的表示

We say that a set of features $f_{1},\ldots,f_{k}$ is linearly dependent if there are constants $\alpha_{0},\alpha_{1},.\cdot\cdot,\alpha_{k}$ , not all of which are 0 , so that for all $\xi$ 

$$
\alpha_{0}+\sum_{i}\alpha_{i}f_{i}(\xi)=0.
$$ 
> 对于一组特征 $f_1, \dots, f_k$ ，如果存在一组不全为0的常数 $\alpha_0, \alpha_1, \dots, \alpha_k$，满足对于所有的 $\xi$，都有 $\alpha_0 + \sum_i \alpha_i f_i (\xi) = 0$，则称这一组特征是线性相关的

This is the usual deﬁnition of linear dependencies in linear algebra, where we view each feature as a vector whose entries are the value of the feature in each of the possible instantiations. 

Example 4.13 
Consider again the Misconception example. We can encode the log-factors in example 4.9 as a set of features by introducing indicator features of the form: 

$$
f_{a,b}(A,B)=\left\{\begin{array}{l l}{{1\qquad}}&{{A=a,B=b}}\\ {{0\qquad}}&{{o t h e r w i s e.}}\end{array}\right.
$$ 
Thus, to represent $\epsilon_{1}(A,B)$ , we introduce four features that correspond to the four entries in the energy function. Since $A,B$ take on exactly one of these possible four values, we have that 

$$
f_{a^{0},b^{0}}(A,B)+f_{a^{0},b^{1}}(A,B)+f_{a^{1},b^{0}}(A,B)+f_{a^{1},b^{1}}(A,B)=1.
$$ 
Thus, this set of features is linearly dependent. 

Example 4.14 
Now consider also the features that capture $\epsilon_{2}(B,C)$ and their interplay with the features that capture $\epsilon_{1}(A,B)$ . We start by noting that the sum $f_{a^{0},b^{0}}(A,B)+f_{a^{1},b^{0}}(A,B)$ is equal to 1 when $B=b^{0}$ and 0 otherwise. Similarly, $f_{b^{0},c^{0}}(B,C)+f_{b^{0},c^{1}}(B,C)$ is also an indicator for $B=b^{0}$ . Thus we get that 

$$
f_{a^{0},b^{0}}(A,B)+f_{a^{1},b^{0}}(A,B)-f_{b^{0},c^{0}}(B,C)-f_{b^{0},c^{1}}(B,C)=0.
$$ 
And so these four features are linearly dependent. As we now show, linear dependencies imply non-unique parameterization. 

As we now show, linear dependencies imply non-unique parameterization.
> 线性相关性质暗示了不唯一的参数化

**Proposition 4.5** 
Let $f_{1},\ldots,f_{k}$ be a set of atures with weights $\pmb{w}=\{w_{1},.\,.\,.\,,w_{k}\}$ that form a log-linear representation of a distribution P . If there are coefcients $\alpha_{0},\alpha_{1},.\cdot\cdot,\alpha_{k}$ such that for all $\xi$ 

$$
\alpha_{0}+\sum_{i}\alpha_{i}f_{i}({\xi})=0\tag{4.10}
$$ 
then the log-linear model with weights $\pmb w^{\prime}=\{w_{1}+\alpha_{1},.\,.\,.\,,w_{k}+\alpha_{k}\}$ also represents $P$ . 
> 命题：
> 特征集合 $f_1, \dots, f_k$，权重 $\pmb w = \{w_1, \dots, w_k\}$，构成了分布 $P$ 的对数线性表示
> 如果存在系数 $\alpha_0, \alpha_1, \dots, \alpha_k$，使得对于所有的 $\xi$，有 $\alpha_0 + \sum_i \alpha_i f_i (\xi) = 0$，也就是特征之间存在线性相关，则权重为 $\pmb w' = \{w_1 + \alpha_1, \dots, w_k + \alpha_k\}$ 的对数线性模型也表示了 $P$

Proof Consider the distribution 
> 证明：
> 考虑权重 $\pmb w'$ 定义的对数线性模型，如下所示：

$$
P_{\pmb w^{\prime}}(\xi)\propto\exp\left\{-\sum_{i}(w_{i}+\alpha_{i})f_{i}(\xi)\right\}.
$$ 
Using equation (4.10) we see that
> 根据式 (4.10) ，我们有
> $-\sum_i (w_i + \alpha_i) f_i (\xi) = -\sum_iw_i f_i (\xi) - \sum_i \alpha_i f_i (\xi)$
> 即 $-\sum_i (w_i + \alpha_i) f_i (\xi) = \alpha_0 - \sum_i w_i f_i (\xi)$

$$
-\sum_{i}(w_{i}+\alpha_{i})f_{i}(\xi)=\alpha_{0}-\sum_{i}w_{i}f_{i}(\xi).
$$ 
Thus, 
> 带入即可得到 $P_{\pmb w'}(\xi)\propto \exp\left\{-\sum_i w_i f_i(\xi)\right\}\propto P_{\pmb w}(\xi)$

$$
P_{\pmb w^{\prime}}(\xi)\propto e^{\alpha_{0}}\exp\left\{-\sum_{i}w_{i}f_{i}(\xi)\right\}\propto P(\xi).
$$ 
We conclude that $P_{\pmb w^{\prime}}(\xi)=P(\xi)$ . 
> 因此 $\pmb w'$ 表示的分布和 $\pmb w$ 表示的分布实际上等价

Motivated by this result, we say that a set of linearly dependent features is redundant . A nonredundant set of features is one where the features are not linearly dependent on each other. In fact, if the set of features is nonredundant, then each set of weights describes a unique distribution. 
> 我们称一组线性相关的特征是冗余的，不冗余即一组特征线性无关，此时每一组权重都描述一个唯一的分布

**Proposition 4.6** 
Let $f_{1},\ldots,f_{k}$ be a set of nonredundant features, and let $v,w^{\prime}\in\mathit{I\!R}^{k}$ . If $\mathbf{\omega}\neq\mathbf{\omega}\mathbf{w}^{\prime}$ then $P_{w}\neq P_{w^{\prime}}$ . 
> 命题：
> $f_1, \dots, f_k$ 为一组不冗余的特征，令 $\pmb w, \pmb w' \in R^k$，如果 $\pmb w \ne \pmb w'$，则 $P_{\pmb w}\ne P_{\pmb w'}$

Example 4.15
Can we construct a nonredundant set of features for the Misconception example? We can determine the number of nonredundant features by building the $16\times16$ matrix of the values of the 16 features (four factors with four features each) in the 16 instances of the joint distribution. This matrix has rank of 9, which implies that a subset of 8 features will be a nonredundant subset. In fact, there are several such subsets. In particular, the canonical parameter iz ation shown in ﬁgure 4.11 has nine features of nonzero weight, which form a nonredundant parameter iz ation. The equivalence of the canonical parameter iz ation (theorem 4.7) implies that this set of features has the same expressive power as the original set of features. To verify this, we can show that adding any other feature will lead to a linear dependency. Consider, for example, the feature $f_{a^{1},b^{0}}$ . We can verify that 

$$
f_{a^{1},b^{0}}+f_{a^{1},b^{1}}-f_{a^{1}}=0.
$$ 
Similarly, consider the feature $f_{a^{0},b^{0}}$ . Again we can ﬁnd a linear dependency on other features: 

$$
f_{a^{0},b^{0}}+f_{a^{1}}+f_{b^{1}}-f_{a^{1},b^{1}}=1.
$$ 
Using similar arguments, we can show that adding any of the original features will lead to redundancy. Thus, this set of features can represent any parameter iz ation in the original model. 

## 4.5 Bayesian Networks and Markov Networks 
We have now described two graphical representation languages: Bayesian networks and Markov networks. Example 3.8 and example 4.8 show that these two representations are incomparable as a language for representing independencies: each can represent independence constraints that the other cannot. In this section, we strive to provide more insight about the relationship between these two representations. 

### 4.5.1 From Bayesian Networks to Markov Networks 
Let us begin by examining how we might take a distribution represented using one of these frameworks, and represent it in the other. One can view this endeavor from two diferent perspectives: Given a Bayesian network $\mathcal{B}$ , we can ask how to represent the distribution $P_{\mathcal{B}}$ as a parameterize Markov network; or, given graph G , we can ask how to represent the independencies in $\mathcal G$ using an undirected graph $\mathcal H$ . In other words, we might be interested in ﬁnding a minimal I-map for a distribution $P_{\mathcal{B}}$ , or a minimal I-map for the independencies $\mathcal{I}(\mathcal{G})$ . B We can see that these two questions are related, but each perspective ofers its own insights. 
> 考虑给定一个 Bayesian/Markov 网络表示的分布，如何将它用 Markov/Bayesian 网络表示
> 这个问题可以从两个角度考虑：给定一个贝叶斯网络 $\mathcal B$，如何将分布 $P_{\mathcal B}$ 表示为参数化的 Markov 网络，或给定图 $\mathcal G$，如何使用一个无向图 $\mathcal H$ 表示 $\mathcal G$ 中的独立性
> 换句话说，我们希望找到分布 $P_{\mathcal B}$ 的极小 I-map，或者找到 $\mathcal I (\mathcal G)$ 的极小 I-map

Let us begin by considering a distribution $P_{\mathcal{B}}$ , where $\mathcal{B}$ is a parameterized Bayesian network over a graph $\mathcal G$ . Importantly, the parameterization of B can also be viewed as a parameterization for a ribution: We simply take each CPD $P(X_{i}\mid\mathrm{Pa}_{X_{i}})$ and view it as a factor of scope $X_{i},\mathrm{Pa}_{X_{i}}$ . This factor satisﬁes additional normalization properties that are not generally true of all factors, but it is still a legal factor. This set of factors deﬁnes a Gibbs distribution, one whose partition function happens to be 1. 
> 考虑一个贝叶斯网络定义的分布 $P_{\mathcal B}$，其中 $\mathcal B$ 是在图 $\mathcal G$ 上参数化的贝叶斯网络
> 贝叶斯网络 $\mathcal B$ 的参数化也可以视作对一个 Gibbs 分布的参数化：将每个 CPD $P(X_i \mid \text{Pa}_{X_i})$ 视作在作用域 $X_i, \text{Pa}_{X_i}$ 上的因子
> 由此得到的一组因子定义了 Gibbs 分布，其划分函数恰好为 1 
> (因此实际上就是将 $P_{\mathcal B}$ 视为了一个 Gibbs 分布)
> (因为各个因子就是 CPD，故所有因子乘起来正好就是联合分布 $P_{\mathcal B}$ ，故 $\sum_{X_1,\dots, X_n} (\prod_{X_i} P (X_i\mid \text{Pa}_{X_i})) = \sum_{X_1,\dots, X_n} P_{\mathcal B}(X_1,\dots, X_n) = 1$，即划分函数为 1)

What is more important, **a Bayesian network conditioned on evidence $E=e$ also induces a Gibbs distribution: the one deﬁned by the original factors reduced to the context $E=e$ .** 
>更重要的是，一个条件于证据 $E=e$ 的贝叶斯网络也会诱导出一个吉布斯分布：这个分布即由原始因子定义的分布在上下文 $E=e$ 下简化后得到的分布

**Proposition 4.7** 
Let $\mathcal{B}$ be a Bayesian network over $\mathcal{X}$ and $\boldsymbol E\,=\,\boldsymbol e$ an observation. Let $W\,=\,\mathcal{X}\,-\,E$ . Then $P_{\mathcal{B}}(W\mid e)$ is a Gibbs distribution deﬁned by the factors $\Phi=\{\phi_{X_{i}}\}_{\mathcal{X}_{i}\in\mathcal{X}}$ , where 

$$
\phi_{X_{i}}=P_{\mathcal{B}}(X_{i}\mid\mathrm{Pa}_{X_{i}})[\boldsymbol{E}=\boldsymbol{e}].
$$ 
The partition function for this Giabbs distribution is $P(e)$ . 
> 命题：
> $\mathcal B$ 为 $\mathcal X$ 上的贝叶斯网络，$\pmb E = \pmb e$ 为观测
> 令 $\pmb W = \mathcal  X - \pmb E$，则分布 $P_{\mathcal B}(\pmb W \mid \pmb e)$ 是由因子 $\Phi = \{\phi_{X_i}\}_{\mathcal X_i \in \mathcal X}$ 定义的分布，其中的因子 $\phi_{X_i}$ 即 CPD 定义的因子（形式不改变，直接把 CPD 看作为因子）在上下文 $[\pmb E = \pmb e]$ 下简化后得到的因子

(证明：

$$
\begin{align}
P_{\Phi}[\pmb e] &= \frac 1 {Z'}\prod_{i=1}^K{\phi_{X_i}[\pmb e]}\\
&=\frac 1 {Z'}\prod_{i=1}^K P_{\mathcal B}(X_i \mid \text{Pa}_{X_i})[\pmb e]\\
&=\frac 1 {Z'} P_{\mathcal B}[\pmb e]\\
&=\frac 1 {Z'} P_{\mathcal B}(\pmb W, \pmb e)
\end{align}
$$

其中第三个等号来源于将每个因子根据上下文筛选和直接相乘再对乘积进行上下文筛选是一致的，其结果都等价于将原 Gibbs 分布在上下文下简化得到的 Gibbs 分布，因为因子乘积结果中和上下文一致的一项来源于每个因子中和上下文一致的各项相乘得到
其中第四个等号直接来源于定义，即简化的因子应该满足 $\phi' (\pmb w)[\pmb e] = \phi (\pmb w, \pmb e)$
因此，划分函数 $Z' = \sum_{\pmb w} P_{\mathcal B}(\pmb W, \pmb e) = P_{\mathcal B}(\pmb e)$
证毕 )

The proof follows directly from the deﬁnitions. This result allows us to view any Bayesian network conditioned as evidence as a Gibbs distribution, and to bring to bear techniques developed for analysis of Markov networks. 

What is the structure of the undirected graph that can serve as an I-map for a set of factors in a Bayesian network? In other words, what is the I-map for the Bayesian network structure $\mathcal{G}\mathrm{:}$ Going back to our construction, we see that we have created a factor for each family of $X_{i}$ , containing all the variables in the family. Thus, in the undirected I-map, we need to have an edge between $X_{i}$ and each of its parents, as well as between all of the parents of $X_{i}$ . This observation motivates the following deﬁnition: 
> 通过贝叶斯网络转化得到的无向图中，其变量之间的依赖性需要保持（不然就会引入额外的独立性），故显然，贝叶斯网络中直接相连的边应该在无向图中也保持，也就是 $X_i$ 和它的父变量之间的边

**Definition 4.16** moralized graph
The moral graph $\mathcal M[\mathcal G]$ of a Bayesian network structure $\mathcal G$ over $\mathcal X$ is the undirected graph over X that contains an undirected edge between X and Y if: (a) there is a directed edge between them (in either direction), or (b) X and Y are both parents of the same node.
> 定义：
> 贝叶斯网络结构 $\mathcal{G}$ 在变量集 $\mathcal{X}$ 上的道德图 $\mathcal{M}[\mathcal{G}]$ 是一个无向图，如果满足：
> (a) $\mathcal G$ 中存在从 $X$ 到 $Y$ 或从 $Y$ 到 $X$ 的有向边；或 
> (b) $\mathcal G$ 中 $X$ 和 $Y$ 都是同一个节点的父母节点
> 则 $\mathcal M[\mathcal G]$ 中 包含一条无向边连接变量 $X$ 和 $Y$ 

For example, ﬁgure 4.6a shows the moralized graph for the extended $\mathcal{B}^{s t u d e n t}$ network of ﬁgure 9.8. 

The preceding discussion shows the following result: 

**Corollary 4.2** 
Let $\mathcal{G}$ be Bayesian network structure Then for y distribution $P_{\mathcal{B}}$ such that $\mathcal{B}$ is a parameterization of G , we have that $\mathcal{M}[\mathcal{G}]$ is an I-map for $P_{\mathcal{B}}$ . 
> 引理：
> 有贝叶斯网络结构 $\mathcal G$，对于 $\mathcal G$ 的任意参数化 $\mathcal B$ 对应的分布 $P_{\mathcal B}$，$\mathcal M[\mathcal G]$ 是 $P_{\mathcal B}$ 的 I-map

One can also view the moralized graph construction purely from the perspective of the independencies encoded by a graph, avoiding completely the discussion of parameterizations of the network. 

**Proposition 4.9** 
Let $\mathcal{G}$ be any Bayesian network graph. The moralized graph $\mathcal{M}[\mathcal{G}]$ is a minimal $I_{\cdot}$ -map for $\mathcal{G}$ . 
> 命题：
> 有贝叶斯网络结构 $\mathcal G$，其道德图 $\mathcal M[\mathcal G]$ 是 $\mathcal G$ 的极小 I-map

Proof We ant to build a Markov network $\mathcal{H}$ such that ${\mathcal{I}}({\mathcal{H}})\subseteq{\mathcal{I}}({\mathcal{G}})$ , that is, that $\mathcal{H}$ is an I-map for G (see deﬁnition 3.3). We use the algorithm or constructing minimal maps based on the Markov independeies. Consider a node X in $\mathcal{X}$ : our task is to select as X ’s neighbors the smallest set of nodes $U$ that are needed to render X independent of all other nodes in the network. We deﬁne the arkov blank of $X$ in a Bayesian network $\mathcal{G}$ , denoted ${\mathrm{MB}}_{\mathcal{G}}(X)$ , to be the nodes consisting of X ’s parents, X ’s children, and other parents of X ’s children. We now need to show that ${\mathrm{MB}}_{\mathcal{G}}(X)$ d-separates $X$ from all other variables in $\mathcal{G}$ ; and that no subset of ${\mathrm{MB}}_{\mathcal{G}}(X)$ has that property. The proof uses straightforward graph-theoretic properties of trails, and it is left as an exercise (exercise 4.14). 
> 证明：
> 我们想要构建一个 Markov 网络 $\mathcal H$，$\mathcal H$ 是 $\mathcal G$ 的 I-map，也就是 $\mathcal I (\mathcal H) \subseteq \mathcal I (\mathcal G)$ (definition 3.3)
> 我们使用基于 Markov 独立性构建极小 I-map 的算法 (4.3.3节，主要思想就是从完全图开始，根据 Markov 独立性删去变量之间的边，这样构造可以保证图中不会有分布中没有的独立性)：
> 考虑 $\mathcal X$ 中的节点 $X$，在我们构建的 $\mathcal H$ 中，$X$ 的所有邻居节点应该构成它的 Markov 毯，即给定 $X$ 的 Markov 毯，$X$ 应该与其余所有节点独立，那么在有向图 $\mathcal G$ 中满足这一点的节点集应该包括 $X$ 的所有父节点、$X$ 的所有子节点以及 $X$ 的子节点的其他父节点，给定该节点集，$X$ 独立于所有其他节点，该节点集被我们定义为 $X$ 在贝叶斯网络 $\mathcal G$ 中的 Markov 毯，记作 $\text{MB}_{\mathcal G}(X)$
> 容易证明 $\text{MB}_{\mathcal G}(X)$ 将 $X$ 和 $\mathcal G$ 中的所有其他变量 d-seperate，并且没有 $\text{MB}_{\mathcal G}(X)$ 的子集满足这一性质
> 显然，$\mathcal M[\mathcal G]$ 就可以视作是根据 $\mathcal G$ 中的 Markov 独立性构建出的 Markov 网络 $\mathcal H$，显然有 $\mathcal I (\mathcal M[\mathcal G])\subseteq \mathcal I (\mathcal G)$，并且 $\mathcal M[\mathcal G]$ 删去任意一边，都会引入在 $\mathcal G$ 中不成立的独立性，故 $\mathcal M[\mathcal G]$ 是 $\mathcal G$ 的极小 I-map

Now, let us consider how “close” the moralized graph is to th original graph $\mathcal{G}$ . **Intuitively, the addition of the moralizing edges to the Markov network H leads to the loss of independence information implied by the graph structure.** For example, if our Bayesian network $\mathcal{G}$ has the form $X\rightarrow Z\leftarrow Y$ with no edge between $X$ and $Y$ , the Markov network $\mathcal{M}[\mathcal{G}]$ loses the information that X and Y are marginally independent (not given Z ). However, information is not always lost. Intuitively, moralization causes loss of information about independencies only when it introduces new edges into the graph. 
> $\mathcal M[\mathcal G]$ 通过保留 $\mathcal G$ 中的边，以及为 $\mathcal G$ 中的 v-structure 添加边得到，直观上，为 v-structure 添加的边会让 $\mathcal M[\mathcal G]$ 相较于 $\mathcal G$ 多了一些依赖性信息，也就是失去了一些独立性信息
> 考虑 v-structure $X\rightarrow Z \leftarrow Y$，在 $\mathcal M[\mathcal G]$ 中，$X, Y$ 会被相连，因此 $\mathcal M[\mathcal G]$ 失去了 $X, Y$ 边际独立的独立性信息
> 显然，$\mathcal M[\mathcal G]$ 只有在 $\mathcal G$ 中存在 v-structure 时，会引入新的边，导致失去部分独立性信息

We say that a Bayesian network $\mathcal{G}$ moral if it contains no immoralities (as in deﬁnition 3.11); that is, for any pair of variables $X,Y$ that share a child, there is a covering edge between $X$ and $Y$ . It is not difcult to show that: 
>如果一个贝叶斯网络 $\mathcal{G}$ 不包含任何 immorality (definition 3.11，也就是 v-structure)，我们称它是 moral
>也就是说，在 $\mathcal G$ 中，对于任意一对共享一个孩子的变量 $X$ 和 $Y$，在 $X$ 和 $Y$ 之间存在一条覆盖边

**Proposition 4.9**
If the directed graph G is moral, then its moralized graph M[G] is a perfect map of G.
> 命题：
> 如果有向图 $\mathcal G$ 是 moral，它的 moralized graph $\mathcal M[\mathcal G]$ 就是 $\mathcal G$ 的 perfect-map

Proof Let $\mathcal{H}=\mathcal{M}[\mathcal{G}]$ . We have already shown that ${\mathcal{I}}({\mathcal{H}})\subseteq{\mathcal{I}}({\mathcal{G}})$ , so it re opposite inclusion. Assume by contradiction that there is an independence $(X\ \bot\ Y\ |\ Z)\in$ $\mathcal{I}(\mathcal{G})$ which not in $\mathcal{I}(\mathcal{H})$ . Thus, there must exist some trail from X to $Y$ in H which is active given Z . Consider some such trail that is minimal, in the sense that it has no shortcuts. As $\mathcal{H}$ and $\mathcal{G}$ have precisely the same edges, the same trail must exist in $\mathcal{G}$ . As t cannot be active in $\mathcal{G}$ given Z , we conclude that it must contain a v-structure $X_{1}\rightarrow X_{2}\leftarrow X_{3}$ . However, because G is moralized, we also have some edge between $X_{1}$ and $X_{3}$ , contradicting the assumption that the trail is minimal. 
> 证明：
> 令 $\mathcal H = \mathcal M[\mathcal G]$，我们已知 $\mathcal I (\mathcal H) \subseteq \mathcal I (\mathcal G)$，故我们需要证明 $\mathcal I (\mathcal G) \subseteq \mathcal I (\mathcal H)$ 
> 使用反证法，假设存在 $(\pmb X\perp \pmb Y \mid \pmb Z) \in \mathcal I (\mathcal G)$ 且 $(\pmb X\perp \pmb Y \mid \pmb Z) \not \in \mathcal I (\mathcal H)$，也就是 $\mathcal H$ 中在给定 $\pmb Z$ 的条件下，在 $\pmb X, \pmb Y$ 之间存在活跃的迹
> 考虑这些活跃的迹中极小的一条，也就是没有捷径的一条迹，因为 $\mathcal H, \mathcal G$ 有完全相同的边，故该迹也存在于 $\mathcal G$ 中，因为 $(\pmb X \perp \pmb Y \mid \pmb Z) \in \mathcal I (\mathcal G)$，故该迹在给定 $\pmb Z$ 时一定不活跃，因此它一定包含 v-structure，这和 $\mathcal G$ 是 moral 的事实矛盾

Thus, a moral graph $\mathcal{G}$ can be converted to a Markov network without losing indepe ence assumptions. This conclusion is fairly intuitive, inasmuch as the only independencies in $\mathcal G$ that are not present in an undirected graph containing the same edges are those corresponding to v-structures. But if any v-structure can be short-cut, it induces no independencies that are not represented in the undirected graph. 
> 因此，moral graph $\mathcal G$ 可以在不损失独立性信息的情况下被转化为 Markov 网络
> 实际上，有向图 $\mathcal G$ 中不能在和它边相同的无向图中包含的唯一的独立性就是 v-structure 带来的独立性

We note, however, that very few directed graphs are moral. For example, assume that we have a v-structure $X\rightarrow Y\leftarrow Z$ , wh h is mora stence of an arc $X\rightarrow Z$ . If $Z$ has another paren $W$ , it o has a v-structure $X\rightarrow Z\leftarrow W$ → ← , which, to be moral, requires some edge between X and W . We return to this issue in section 4.5.3. 
> 但一般很少的有向图是 moral，例如一个包含 $X\rightarrow Y \leftarrow Z$ 的有向图，我们引入边 $X \rightarrow Z$ 使其 moral，但如果 $Z$ 有另一个父变量，则 v-structure $X \rightarrow Z \leftarrow W$ 又会存在

#### 4.5.1.1 Soundness of d-Separation 
The connection between Bayesian networks and Markov networks provides us with the tools for proving the soundness of the d-separation criterion in Bayesian networks. 
> 本节证明贝叶斯网络中 d-seperation 准则的可靠性

The idea behind the proof is to leverage the soundness of separation in undirected graphs, a result which (as we showed) is much easier to prove. Thus, we want to construct an undirected graph $\mathcal{H}$ such that active paths in $\mathcal{H}$ correspond to active paths in $\mathcal{G}$ . A moment of thought shows that the moralized graph is not the right construct, because there are paths in the undirected graph that orr ructu s in $\mathcal{G}$ that may or may not be active. For exa le, if our graph G is $X\rightarrow Z\leftarrow Y$ and Z is ot observed, d-separation tells us that $X$ and $Y$ are independent; but the moralized graph for G is the complete undirected graph, which does not have the same independence. 
> 证明的思路是利用无向图中 seperation 准则的可靠性
> 我们希望构造一个无向图 $\mathcal H$，使得 $\mathcal H$ 中的活跃路径和 $\mathcal G$ 中的活跃路径对应
> 直接使用 moralized graph $\mathcal M[\mathcal G]$ 并不合适，它无法建模 $\mathcal G$ 中的 v-structure $X\rightarrow Z \leftarrow Y$ 中 $Z$ 未被观察到时 $X, Y$ 的边际独立性 (该独立性是可以被 d-seperation 检测到的)

Therefore, to show the result, we ﬁrst want to eliminate v-structures that are not active, so as to remove such cases. To do so, we ﬁrst construct a subgraph where remove all barren nodes from the graph, thereby also removing all v-structures that do not have an observed descendant. The elimination of the barren nodes does not change the independence properties of the distribution over the remaining variables, but does eliminate paths in the graph involving v-structures that are not active. If we now consider only the subgraph, we can reduce d-separation to separation and utilize the soundness of separation to show the desired result. 
>我们首先希望消除那些不活跃的 v-structure
>为此，我们首先构建一个子图，该子图移除了所有空节点，这也会移除所有没有观测到的后代的 v-structure，移除空节点不会改变剩余变量上的分布的独立性质，但会消除涉及不活跃 v-structure 的路径
>现在只考虑这个子图，我们可以将有向图中的 d-seperation 简化为无向图中的 seperation 并利用 seperation 的可靠性来证明 d-seperation 的可靠性

>[!barren node]
>   
>在概率图模型（如贝叶斯网络或马尔可夫网络）中， “barren node”通常指的是那些在模型中不起作用或没有贡献的节点，具体来说：
>
>1. 没有下游节点：如果一个节点没有任何下游节点，那么它不会影响其他节点的状态。
>2. 状态不传播：如果一个节点的状态不能通过模型传播到其他节点，那么它也不会对整体的概率分布产生影响。
>
>在一个有效的概率图模型中，每个节点都应当对其周围的节点或者整个系统的概率分布有所贡献，如果某个节点完全孤立，或者它的状态无法通过模型传播到其他节点，那么这个节点就可以被认为是“barren node”
>
>对于一个 v-structure $A \rightarrow B \leftarrow C$，如果 $B$ 没有后续的下游节点，则节点 $B$ 的状态完全由节点 $A, C$ 决定，并且 $B$ 没有被观测到时，$B$ 不传递任何额外的信息给其他节点，故 $B$ 此时就是 barren node
>
>移除 barren node 不会对其他节点的分布产生影响，因为 barren node 本身就不影响任何其他节点

We ﬁrst use these intuitions to provide an alternative formulation for d-separation. Recall that in deﬁnition 2.14 we deﬁned the upward closure of a set of nodes $U$ in a graph to be $U\cup$ Ancestors U . Letting $U^{*}$ be the closure of a s $U$ , we can de the network induced over $U^{*}$ ; importantly, as all parents of every node in $U^{*}$ are also in U $U^{*}$ , we have all the variables mentioned in every CPD, so that the induced graph deﬁnes a coherent probability distribution. We let ${\mathcal{G}}^{+}[U]$ be the induced Bayesian network over $U$ and its ancestors. 
> 首先考虑为 d-seperation 提供另一种表述方式
> 在 definition 2.14 中，我们将节点集 $\pmb U$ 的上闭包定义为 $\pmb U \cup Ancestors_{\pmb U}$，我们将其记作 $\pmb U^*$
> 随后，我们定义在 $\pmb U^*$ 导出的贝叶斯网络，记作 $\mathcal G^+[\pmb U]$ 
> 因为 $\pmb U^*$ 中的任意节点的所有父节点都在 $\pmb U^*$ 中，因此导出的贝叶斯网络中任意 CPD 相关的变量都在 $\pmb U^*$ 中，导出的贝叶斯网络 $\mathcal G^+[\pmb U]$ 定义了一个一致的概率分布

**Proposition 4.10** 
Let $X,Y,Z$ three disjoint sets of nodes in a Bayesia twork $\mathcal{G}$ . Let $U=X\cup Y\cup Z,$ , and let G $\mathcal{G}^{\prime}=\mathcal{G}^{+}[\boldsymbol{U}]$ G be the induced Bayesian network over $U\cup$ ∪ Ancestors $U$ . Let H be the moralized graph $\mathcal{M}[\mathcal{G}^{\prime}]$ . Then $\operatorname{d-sep}_{\mathcal{G}}(X;Y\mid Z)$ if and only if $\mathrm{sep}_{\mathcal{H}}(X;Y\mid Z)$ . 
> 命题：
> $\pmb X, \pmb Y, \pmb Z$ 为 $\mathcal G$ 中三个不相交的节点集，记 $\pmb U = \pmb X \cup \pmb Y \cup \pmb Z$，记 $\pmb U$ 导出的贝叶斯网络为 $\mathcal G' = \mathcal G^+[\pmb U]$，记导出贝叶斯网络的 moral graph 为 $\mathcal H = \mathcal M[\mathcal G']$，则 $\text{d-sep}_{\mathcal G}(\pmb X; \pmb Y\mid \pmb Z)$ 存在当且仅当 $\text{sep}_{\mathcal H}(\pmb X; \pmb Y\mid \pmb Z)$ 存在
> 也就是贝叶斯网络 $\mathcal G$ 中的 d-seperation 和对应的导出的贝叶斯网络的 moral graph 的 seperation 等价

> 该思想类似于分类讨论
> 我们不能直接将 $\mathcal G$ 中的 d-seperation 和 $\mathcal M[\mathcal G]$ 中的 seperation 等价，因为 $\mathcal G$ 中的 v-structure 在不给定子节点时编码了条件独立，而 $\mathcal M[\mathcal G]$ 不能表示这一点，因为 $\mathcal M[\mathcal G]$ 无论子节点给定不给定都会将 v-structure 的父节点相连
> 因此，我们针对每个具体的 d-seperation 定义导出的网络 $\mathcal G'$，如果某个 v-structure 的子节点不给定，则 $\mathcal G'$ 中不会包含该子节点，也就是直接消除了 v-structure，故 $\mathcal M[\mathcal G']$ 自然对应编码了边际独立性

![[Probabilistic Graph Theory-Fig4.12.png]]

Example 4.16 
To gain some intuition for this result, consider the Bayesian network G of figure 4.12a (which extends our Student network). Consider the d-separation query d-sepG (D; I j L). In this case, U = fD; I; Lg, and hence the moralized graph M[G+[U]] is the graph shown in figure 4.12b, where we have introduced an undirected moralizing edge between D and I. In the resulting graph, D and I are not separated given L, exactly as we would have concluded using the d-separation procedure on the original graph. On the other hand, consider the d-separation query d-sepG (D; I j S; A). In this case, U = fD; I; S; Ag. Because D and I are not spouses in G+[U], the moralization process does not add an edge between them. The resulting moralized graph is shown in ﬁgure 4.12c. As we can see, we have that $\mathsf{s e p}_{\mathcal{M}[\mathcal{G}^{+}[\boldsymbol{U}]]}(D;I\mid S,A)$ , as desired. 

The proof for the general case is similar and is left as an exercise (exercise 4.15). 
With this result, the soundness of $\mathrm{d}$ -separation follows easily. We repeat the statement of theorem 3.3: 

**Theorem 4.9**
If a distribution $P_{\mathcal{B}}$ factorizes according to $\mathcal{G}$ , then $\mathcal{G}$ is an $I\cdot$ -map for $P$ . 
> 定理：
> 如果分布 $P_{\mathcal B}$ 根据 $\mathcal G$ 分解，则 $\mathcal G$ 是 $P$ 的 I-map
>(可靠性：$\mathcal G$ 中 d-seperation 导出的独立性在 $P$ 中一定存在)

Proof As in proposition 4.1 t $U=X\cup Y\cup Z$ , let $U^{*}=U\cup A n c e s t o r s_{U}$ , let $\mathcal{G}_{U^{\ast}}=\mathcal{G}^{+}[U]$ be the induced graph over $U^{*}$ , and let H be the moralized graph $\mathcal{M}[\mathcal{G}_{U^{*}}]$ . Let $P_{U^{*}}$ be the Ba an network distrib tion deﬁn ver ${\mathcal{G}}_{U^{*}}$ in the obvious way: the CPD for any variable in $U^{*}$ is the same as in B . Because $U^{*}$ is upwardly closed, all variables used in these CPDs are in $U^{*}$ . 

Now, consider an independence assertion $(X\,\perp\,Y\,\mid\,Z)\,\in\,{\mathcal{I}}({\mathcal{G}})$ ; we want to prove that $P_{\mathcal{B}}=(X\perp Y\mid Z)$ . By deﬁnition 3.7, if $(X\perp Y\mid Z)\in{\mathcal{I}}(\mathcal{G})$ , we have that $\mathit{d-s e p}_{\mathcal{G}}(X;Y\mid$ $Z)$ . It follows th $s e p_{\mathcal{H}}(X;Y\mid Z)$ , and hence that $(X\perp Y\mid Z)\in{\mathcal{Z}}({\mathcal{H}})$ . P $P_{U^{*}}$ is a Gibbs distribution over H , and hence, from theorem 4.1, $P_{U^{*}}\mid=(X\perp Y\mid Z)$ ⊥ | . e distribution $P_{U^{*}}(U^{*})$ is the same as $P_{\mathcal{B}}(U^{*})$ . Hence, it follows also that $P_{\mathcal{B}}\models(X\ \bot\ Y\ |\ Z)$  , B proving the desired result. 
> 证明：
> 令 $\pmb U = \pmb X \cup \pmb Y \cup \pmb Z$，记 $\pmb U^* = \pmb U \cup Ancestors_{\pmb U}$ ，记 $\mathcal G_{\pmb U^*} = \mathcal G^+[\pmb U]$，记 $\mathcal H = \mathcal M[\mathcal G_{\pmb U^*}]$，记 $P_{\pmb U^*}$ 为 $\mathcal G_{\pmb U^*}$ 上定义的分布，定义方式就是 $\mathcal G_{\pmb U^*}$ 中的 CPD 相乘得到，注意 $\mathcal G_{\pmb U^*}$ 中的 CPD 和 $\mathcal B$ 中的对应 CPD 都是一致的 ($\pmb U^*$ 向上封闭，因此包含了所有 CPD 相关的变量)
> 考虑独立性 $(\pmb X \perp \pmb Y \mid \pmb Z)\in \mathcal I (\mathcal G)$，我们要证明 $P_{\mathcal B}\vDash(\pmb X \perp \pmb Y \mid \pmb Z)$
> 根据 defintion 3.7，$(\pmb X \perp \pmb Y \mid \pmb Z)\in \mathcal I (\mathcal G)$ 等价于 $\text{d-sep}_{\mathcal G}(\pmb X; \pmb Y \mid \pmb Z)$ ，进而等价于 $\text{sep}_{\mathcal H}(\pmb X; \pmb Y \mid \pmb Z)$，进而等价于 $(\pmb X \perp \pmb Y \mid \pmb Z)\in \mathcal I (\mathcal H)$；而 $P_{\pmb U^*}$ 可以视作分解于 $\mathcal H$ 上的 Gibbs 分布，根据 Markov 网络的 seperation 的可靠性，有 $P_{\pmb U^*}\vDash (\pmb X \perp \pmb Y \mid \pmb Z)$，by exercise 3.8，导出网络中定义的联合分布和原网络中的联合分布是相同的，也就是 $P_{\pmb U^*}(\pmb U^*) = P_{\mathcal B}(\pmb U^*)$，因此 $P_{\pmb U^*}$ 中成立的独立性显然在 $P_{\mathcal B}$ 中也成立，即 $P_{\mathcal B}\vDash(\pmb X \perp \pmb Y \mid \pmb Z)$

> 该证明的思路：独立性在 $\mathcal G$ 中成立 -> 独立性在 $\mathcal H = \mathcal M[\mathcal G^+[\pmb U]]$ 中成立 -> 独立性在 $P_{\pmb U^*}$ 中成立 -> 独立性在 $P_{\mathcal B}$ 中成立

### 4.5.2 From Markov Networks to Bayesian Networks 
The previous section dealt with the conversion from a Bayesian network to a Markov network. We now consider the converse transformation: ﬁnding a Bayesian network that is a minimal I-map for a Markov network. It turns out that the transformation in this direction is signiﬁcantly more difcult, both conceptually and computationally. Indeed, the Bayesian network that is a minimal I-map for a Markov network might be considerably larger than the Markov network. 
> 上一节讨论了 Bayesian 网络到 Markov 网络的转换
> 本节讨论 Markov 网络到 Bayesian 网络的转换：对于给定的 Markov 网络，找到一个 Bayesian 网络是它的极小 I-map
> 事实上，作为一个 Markov 网络的极小 I-map 的 Bayesian 网络会比原网络大许多

![[Probabilistic Graph Theory-Fig4.13.png]]


Example 4.17 
Consider the Mark v networ ructure $\mathcal{H}_{\ell}$ of ﬁgure 4.13a, and assume that we want to ﬁnd a Bayesian network I-map for $\mathcal{H}_{\ell}$ . As we discussed in section 3.4.1, we can ﬁnd such an I-map by enumerating the nodes in X in some ordering, and deﬁne the parent set for each one in turn according to the independencies in the distribution. Assume we enumerate the nodes in the order $A,B,C,D,E,F$ . The process for $A$ and $B$ is obvious. Consider what happens when we add $C$ . We must, of course, introduce $A$ as a parent for $C$ . More interestingly, however, $C$ is not independent of $B$ given $A$ ; hence, we must also add $B$ as a parent for $C$ . Now, consider the node $D$ . One of its parents must be $B$ . As $D$ is not independent of $C$ given $B$ , we must add $C$ as a parent for $B$ . We do not need to add $A$ , as $D$ is independent of $A$ given $B$ and $C$ . Similarly, $E$ ’s parents must be $C$ and $D$ . Overall, the minimal Bayesian network $I_{\cdot}$ -map according to this ordering has the structure $\mathcal{G}_{\ell}$ shown in ﬁgure 4.13b. 

A quick examination of the structure $\mathcal{G}_{\ell}$ shows that we have added sever edges to the graph, resulting in a set of triangles crisscrossing the loop. In fact, the graph G $\mathcal{G}_{\ell}$ in ﬁgure 4.13b is chordal: all loops have been partitioned into triangles. 

One might hope that a diferent ordering might lead to fewer edges being introduced. Un- fortunately, this phenomenon is a general one: any Bayesian network I-map for this Markov network must add triangulating edges into the graph, so that the resulting graph is chordal (see deﬁnition 2.24). In fact, we can show the following property, which is even stronger: 
>人们可能会希望不同的节点顺序能够引入更少的边，不幸的是，这种现象是普遍存在的：对于这个马尔可夫网络的任何 I-map 贝叶斯网络都必须向图中添加三角化边，从而使所得的图成为弦图（参见定义2.24）
>事实上，我们可以证明以下性质，这一性质甚至更强：

**Theorem 4.10** 
Let H be a Markov network structure, and let G be any Bayesian network minimal I-map for H. Then G can have no immoralities (see definition 3.11).
> 定理：
> $\mathcal H$ 为 Markov 网络结构，$\mathcal G$ 为任意是 $\mathcal H$ 的极小 I-map 的贝叶斯网络结构，则 $\mathcal G$ 中不可能存在 v-structure (immorality)

Proof Let $X_1, \dots, X_n$ be a topological ordering for G. Assume, by contradiction, that there is some immorality Xi ! Xj Xk in G such that there is no edge between Xi and Xk; assume (without loss of generality) that i < k < j. 
Owing to minimality of the I-map G, if Xi is a parent of Xj, then Xi and Xj are not separated by Xj’s other parents. Thus, H necessarily contains one or more paths between Xi and $X_{j}$ that are not cut by $X_{k}$ (or by $X_{j}$ ’s other parents). Similarly, $\mathcal{H}$ necessarily contains one or more paths between $X_{k}$ and $X_{j}$ that are not cut by $X_{i}$ (or by $X_{j}$ ’s other parents). 
> 证明：
> 令 $X_1, \dots, X_n$ 为 $\mathcal G$ 的一个拓扑排序
> 假设 $\mathcal G$ 中存在 v-structure $X_i \rightarrow X_j \leftarrow X_k$，其中 $X_i , X_k$ 之间没有边，不失一般性，我们还假设 $i < k < j$
> 因为 $\mathcal G$ 是 $\mathcal H$ 的极小 I-map，故如果 $X_j$ 在 $\mathcal G$ 中是 $X_i$ 的父节点，则 $X_j$ 就不会被 $X_j$ 的其他父节点分离 (极小性意味着没有边是多余的，如果断开边 $X_j - X_i$，就会引入 $X_j, X_i$ 之间的独立性，因此边 $X_j - X_i$ 不能断开)；因此，在 $\mathcal H$ 中，$X_i, X_j$ 之间会有一条或者多条不被 $X_k$ 或者 $X_j$ 其他父变量的路径，类似地，对于 $X_k$ 也可以这样推理

Consider the parent set $U$ that was chosen for $X_{k}$ . By our previous argument, there are one or more paths in $\mathcal{H}$ between $X_{i}$ and $X_{k}$ via $X_{j}$ . As $i<k$ , and $X_{i}$ is ot a parent of $X_{k}$ (by our assumption), we have that $U$ must cut all of those paths. To do so, U must cut either all of the paths between $X_{i}$ and $X_{j}$ , or all of the paths between $X_{j}$ and $X_{k}$ : As long as there is at least one active path from $X_{i}$ to $X_{j}$ and one from $X_{j}$ to $X_{k}$ , there is an active path between $X_{i}$ and $X_{k}$ that is not cut by $U$ . Assume, without loss of generality, that $U$ cuts all paths between $X_{j}$ and $X_{k}$ (the other case is symmetrical). Now, consider the choice of parent set for $X_{j}$ , and recall that it is the (unique) minimal subset among $X_{1},\dots,X_{j-1}$ that separates $X_{j}$ from the others. In a Markov network, this set consists of all nodes in $X_{1},.\ldots,X_{j-1}$ that are the ﬁrst on some uncut path from $X_{j}$ . As $U$ separates $X_{k}$ from $X_{j}$ , it follows that $X_{k}$ cannot be the ﬁrst on any uncut path from $X_{j}$ , and therefore $X_{k}$ cannot be a parent of $X_{j}$ . This result provides the desired contradiction. 
>考虑为 $X_k$ 选择的父节点集合 $\pmb U$，根据我们之前的论点，在 $\mathcal{H}$ 中有从 $X_i$ 到 $X_k$ 经过 $X_j$ 的一条或多条路径，由于 $i < k$，且 $X_i$ 不是 $X_k$ 的父节点（根据我们的假设），那么 $\pmb U$ 必须切断所有这些路径，也就是说，$\pmb U$ 必须切断 $X_i$ 和 $X_j$ 之间或 $X_j$ 和 $X_k$ 之间的所有路径：只要从 $X_i$ 到 $X_j$ 和从 $X_j$ 到 $X_k$ 至少有一条激活路径，则存在一条未被 $\pmb U$ 切断的从 $X_i$ 到 $X_k$ 的激活路径
>不失一般性，假设 $\pmb U$ 切断了 $X_j$ 和 $X_k$ 之间的所有路径（另一情况是对称的）
>
>现在，考虑 $X_j$ 的父节点集合的选择，回想起来，它是 $X_1,\dots,X_{j-1}$ 中将 $X_j$ 与其他节点分开的唯一最小的子集
>在马尔可夫网络中，这个集合包含在 $X_1,\dots,X_{j-1}$ 中满足是某个未被切断路径上的第一个节点的所有节点，由于 $\pmb U$ 将 $X_k$ 与 $X_j$ 分隔开，因此可以得出结论 $X_k$ 不可能是任何未被切断路径上的 $X_j$ 的第一个节点，因此 $X_k$ 不可能是 $X_j$ 的父节点，这个结果提供了所需的矛盾

Because any nontriangulated loop of length at least 4 in a Bayesian network graph necessarily contains an immorality, we conclude: 
>因为在贝叶斯网络图中，任何长度至少为4的非三角环必然包含一个 immorality，我们得出结论：

**Corollary 4.3** 
Let H be a Markov network structure, and let $\mathcal{G}$ be any minimal I-map for $\mathcal{H}$ . Then $\mathcal{G}$ is necessarily chordal. 
> 引理：
> 令 $\mathcal{H}$ 是一个马尔可夫网络结构，并且令 $\mathcal{G}$ 是 $\mathcal{H}$ 的任何一个极小 I-map，那么 $\mathcal{G}$ 必然是弦图 ($\mathcal G$ 中的任意长度至少为4的环一定会被三角化，也就是包含 covering edge，否则就会构成 immorality/v-structure)

Thus, the process of turning a Markov network into a Bayesian network requires that we add enough edges to a graph to make it chordal. This process is called *triangulation* . As in the transformation from Bayesian networks to Markov networks, the addition of edges leads to the loss of independence information. For instance, in example 4.17, the Bayesian network $\mathcal{G}_{\ell}$ in ﬁgure 4.13b loses the information that $C$ and $D$ are independent given $A$ and $F$ . In the transformation from directed to undirected models, however, the edges added are only the ones that are, in some sense, implicitly there — the edges required by the fact that each factor in a Bayesian network involves an entire family (a node and its parents). By contrast, the transformation from Markov networks to Bayesian networks can lead to the introduction of a large number of edges, and, in many cases, to the creation of very large families (exercise 4.16). 
>因此，将马尔可夫网络转换为贝叶斯网络的过程需要向图中添加足够的边，使其成为弦图，这一过程称为三角化；正如从贝叶斯网络到马尔可夫网络的转换一样，添加边会导致独立性信息的丢失
>例如，在示例4.17中，图4.13b中的贝叶斯网络 $\mathcal{G}_{\ell}$ 失去了 $C$ 和 $D$ 在给定 $A$ 和 $F$ 时是独立的信息
>然而，在从有向模型到无向模型的转换过程中，添加的边只是那些在某种意义上原本就存在的边——即由贝叶斯网络中的每个因子涉及整个家族（一个节点及其父节点）的事实所要求的边；相比之下，从马尔可夫网络到贝叶斯网络的转换可能导致大量边的引入，并且在许多情况下，可能导致非常大的一族节点

### 4.5.3 Chordal Graphs 
We have seen that the conversion in either direction between Bayesian networks to Markov networks can lead to the addition of edges to the graph and to the loss of independence information implied by the graph structure. It is interesting to ask when a set of independence assumptions can be represented perfectly by both a Bayesian network and a Markov network. It turns out that this class is precisely the class of undirected chordal graphs. 
> 无向弦图中的独立性可以被贝叶斯网络完美表示，同时也可以被 Markov 网络完美表示

The proof of one direction is fairly straightforward, based on our earlier results. 

**Theorem 4.11** 
Let $\mathcal{H}$ b nonchordal Markov network. Then there is no Bayesian network $\mathcal{G}$ which is a perfect map for H (that is, such that $\mathcal{I}(\mathcal{H})=\mathcal{I}(\mathcal{G})$ ). 
> 定理：
> $\mathcal H$ 为 nonchordal Markov 网络，则不存在是 $\mathcal H$ 的 perfect map 的贝叶斯网络 $\mathcal G$，也就是不存在满足 $\mathcal I (\mathcal H) = \mathcal I (\mathcal G)$ 的贝叶斯网络 $\mathcal G$

Proof The roof s from the fact that the minimal I-map for $\mathcal{G}$ any I-map G for I $\mathcal{Z}(\mathcal{H})$ H must include edges that are not present in H . Because any additional edge eliminates independence assumptions, it is not possible for any Bayesian network $\mathcal{G}$ to precisely encode $\mathcal{I}(\mathcal{H})$ . 
> 证明：
> 首先 $\mathcal G$ 需要是 $\mathcal H$ 的极小 I-map，故 $\mathcal G$ 必须是弦图，因此 $\mathcal G$ 一定包含 $\mathcal H$ 中没有出现的边；因为任意额外的边会消除独立性，故 $\mathcal G$ 不可能完全编码 $\mathcal I (\mathcal H)$

To prove the other direction of this equivalence, we ﬁrst prove some important properties of chordal graphs. As we will see, chordal graphs and the properties we now show play a central role in the derivation of exact inference algorithms for graphical models. For the remainder of this discussion, we restrict attention to connected graphs; the extension to the general case is straightforward. The basic result we show is that we can decompose any connected chordal graph $\mathcal{H}$ into a *tree of cliques* — a tree whose nodes are the maximal cliques in ${\mathcal{H}}-s{\boldsymbol{0}}$ that the structure of the tree precisely encodes the independencies in H . (In the case of disconnected graphs, we obtain a forest of cliques, rather than a tree.) 
>为了证明这一等价关系的另一方向，我们首先证明弦图的一些重要性质，弦图及其现在展示的性质在精确推理算法的推导中起着核心作用
>在接下来的讨论中，我们将注意力限制在连通图上，推广到一般情况是直接的
>我们要展示的基本结果是：我们可以将任意连通弦图 $\mathcal{H}$ 分解成一个由团组成的树——这棵树的节点是由 ${\mathcal{H}}$ 中的极大团组成，而树的结构精确地编码了 $\mathcal{H}$ 中的独立性（在非连通图的情况下，我们得到的是一个由团组成的森林，而不是一棵树）

We begin by introducing some notation. Le $\mathcal{H}$ be connected undirected graph, and let $C_{1},\ldots,C_{k}$ be the set of maximal cliques in H . Let T $\mathcal{T}$ be any tree-structured graph whose nodes correspond to the maximal cliques $C_{1},\ldots,C_{k}$ . Let $C_{i},C_{j}$ be two cliques in the tree that direct edge; we deﬁne $S_{i,j}=C_{i}\cap C_{j}$ to be a sepset between $C_{i}$ and $C_{j}$ . Let $W_{<(i,j)}\ (W_{<(j,i)})$ ) be all of the variables that appear in any clique on the $C_{i}$ $(C_{j})$ edge us, each edge decomposes $\mathcal{X}$ into three disjoint sets: $W_{<(i,j)}-S_{i,j}$ , $W_{<(j,i)}-S_{i,j}$ − , and $\boldsymbol{S}_{i,j}$ . 
> 引入一些记号：
> $\mathcal H$ 表示连通无向图；
> $\pmb C_1, \dots, \pmb C_k$ 为 $\mathcal H$ 中的一系列极大团；
> $\mathcal T$ 为任意树，其节点对应于极大团；
> $\pmb C_i, \pmb C_j$ 为树中直接相连的两个团；
> 定义 $\pmb S_{i, j} =  \pmb C_i \cap \pmb C_j$ 为 $\pmb C_i, \pmb C_j$ 之间的分离集 sepset；
> $\pmb W_{<(i,j)}\ (\pmb W_{<(j,i)})$  表示出现在 $\pmb C_i(\pmb C_j)$ 那一边的团中的任意节点，因此，一条边将 $\mathcal X$ 分为三个不相交集合 $\pmb W_{<(i, j)} - \pmb S_{ij} , \pmb W_{<(j, i)} - \pmb S_{ij}, \pmb S_{ij}$

**Deﬁnition 4.17** clique tree 
We say that a tree $\mathcal{T}$ s a clique tree for $\mathcal{H}$ if: 

-  each node corresponds to a clique in $\mathcal{H}$ , and each maximal clique in $\mathcal{H}$ is a node in $\mathcal{T}$ ; 
- each sepset $\boldsymbol{S}_{i,j}$ separates $W_{<(i,j)}$ and $W_{<(j,i)}$ in $\mathcal{H}$ . 

> 定义：
> 如果树 $\mathcal T$ 满足：
> 每个节点对应于 $\mathcal H$ 中的一个团，且 $\mathcal H$ 中的每个极大团都对应于 $\mathcal T$ 中的一个节点
> 每个分离集 $\pmb S_{ij}$ 在 $\mathcal H$ 中分离 $\pmb W_{<(i, j)}, \pmb W_{<(j, i)}$
> 称 $\mathcal T$ 是 $\mathcal H$ 的团树

Note that this deﬁnition implies that each separator $\boldsymbol{S}_{i,j}$ renders its two sides conditionally independent in $\mathcal{H}$ . 
>该定义要求每个分离集 $\pmb S_{ij}$ 在 $\mathcal H$ 中分离 $\pmb W_{<(i, j)}, \pmb W_{<(j, i)}$，也就是二者要在 $\mathcal H$ 中条件独立

Example 4.18 
Consider the Bayesian network graph $\mathcal{G}_{\ell}$ in ﬁgure 4.13b. Since it contains no immoralities, its mo ized graph $\mathcal{H}_{\ell}^{\prime}$ is simply the same graph, but where all edges have been made undirected. As $\mathcal{G}_{\ell}$ is chordal, so is $\mathcal{H}_{\ell}^{\prime}$ . The clique tree for $\mathcal{H}_{\ell}^{\prime}$ is simply a chain $\{A,B,C\}\rightarrow\{B,C,D\}\rightarrow$ $\{C,D,E\}\,\rightarrow\,\{D,E,F\}$ , which clearly satisﬁes the separation requirements of the clique tree deﬁnition. 

**Theorem 4.12** 
Every undirected chordal graph $\mathcal{H}$ has a clique tree $\mathcal{T}$ . 
> 定理：
> 每个无向弦图都有团树

Proof We prove the theorem by induction on the number of nodes in the graph. The base case of a single node is trivial. Now, consider a chordal graph $\mathcal{H}$ of size $>1$ . If $\mathcal{H}$ consists of a single clique, then the theorem holds trivially. Therefore, consider the case where we have at least two nodes $X_{1},X_{2}$ that are not connected directly by an edge. Assume that $X_{1}$ and $X_{2}$ are connected, otherwise the inductive step holds trivially. Let $S$ be a minimal subset of nodes that separates $X_{1}$ and $X_{2}$ . 
> 证明跳过

The removal of the set $S$ breaks up the graph into at least two disconnected components — one containing $X_{1}$ , another containing $X_{2}$ , and perhaps additional ones. Let $W_{1},W_{2}$ be some partition of the variables in ${\mathcal{X}}-S$ into two disjoint components, such that $W_{i}$ encompasses the connected component containing $X_{i}$ . (The other connected components can be assigned to $W_{1}$ or $W_{2}$ arbitrarily.) We ﬁrst show that $S$ must be a complete subgraph. Let $Z_{1},Z_{2}$ be any two variables in $S$ . Due to the minimality of $S$ , each $Z_{i}$ must lie on a path between $X_{1}$ and $X_{2}$ that does not go through any other node in $S$ . (Otherwise, we could eliminate $Z_{i}$ from $S$ while still maintaining separation.) We can therefore construct a minimal path from $Z_{1}$ to $Z_{2}$ that goes only through nodes in $W_{1}$ by constructing a path from $Z_{1}$ to $X_{1}$ to $Z_{2}$ that goes only through $W_{1}$ , and by eliminating any shortcuts. We can similarly construct a minimal path from $Z_{1}$ to $Z_{2}$ that goes only through nodes in $W_{2}$ . The two paths together form a cycle of length $\geq4$ . Because of chordality, the cycle must have a chord, which, by construction, must be the edge $Z_{1}{-}Z_{2}$ . 

Now cons r the induc graph ${\mathcal{H}}_{1}\,=\,{\mathcal{H}}[W_{1}\cup S]$ . As $X_{2}\notin\mathcal{H}_{1}$ , this induced gr h is smaller than H . M over, H $\mathcal{H}_{1}$ is hordal, so we can apply the inductive hypothesis. Let T $\mathcal{T}_{1}$ be the clique tree for H $\mathcal{H}_{1}$ . Because S is a compl conne subgraph, it is eit a maximal ique or a subset of some maximal clique in H $\mathcal{H}_{1}$ . Let $C_{1}$ be some ue in $\mathcal{T}_{1}$ con ing $S$ (ther ay b ore than on such clique). We can imilarly deﬁne H $\mathcal{H}_{2}$ and $C_{2}$ for $X_{2}$ . If ne er $C_{1}$ or $C_{2}$ is equal $S$ , we nstruct a tree T $\mathcal{T}$ that contains the union of the cliques $\mathcal{T}_{1}$ d T $\mathcal{T}_{2}$ , and nnects $C_{1}$ a $C_{2}$ by edge therwise, with loss of generality, let $C_{1}=S$ ; we create T $\mathcal{T}$ by merging T $\mathcal{T}_{1}$ minus $C_{1}$ into T $\mathcal{T}_{2}$ , making all of $C_{1}$ ’s neighbors adjacent to $C_{2}$ instead. 

It remains to sho hat the resulting structure is a clique tree for $\mathcal{H}$ . First, we note at there is no clique in H that intersects both $W_{1}$ and $W_{2}$ ; hence, any aximal clique in H is a maximal ique in eith $\mathcal{H}_{1}$ or $\mathcal{H}_{2}$ (or both in e possible case of S ), so that all maxi l cliques in H appear in T . Thus, the nodes in T $\mathcal{T}$ are precisely the maximal cliques in H . Second, we need to show that any $\boldsymbol{S}_{i,j}$ separates $W_{<(i,j)}$ and $W_{<(j,i)}$ . Consider two variables $X\in W_{<(i,j)}$ and $Y\in W_{<(j,i)}$ . First, assume that $X,Y\in{\mathcal{H}}_{1}$ ; as all the nodes in $\mathcal{H}_{1}$ re on the T $\mathcal{T}_{1}$ side o tree, we also have that $S_{i,j}\subset\mathcal{H}_{1}$ . path bet two node $\mathcal{H}_{1}$ H t goes through $W_{2}$ can be shor t to go only through H $\mathcal{H}_{1}$ . Thus, if $\boldsymbol{S}_{i,j}$ separates $X,Y$ in H $\mathcal{H}_{1}$ , so separates them in H . The same argument applies for $X,Y\in{\mathcal{H}}_{2}$ . Now, sider $X\in W_{1}$ and $Y\in W_{2}$ . If $\boldsymbol{S}_{i,j}=\boldsymbol{S}$ , th esult follows from th fact at S separates $W_{1}$ and $W_{2}$ . Otherwise, a me th $\boldsymbol{S}_{i,j}$ in T $\mathcal{T}_{1}$ , on the path f m X to $C_{1}$ 1 . In this case, we have that $\boldsymbol{S}_{i,j}$ separates X from S , and S separates $\boldsymbol{S}_{i,j}$ from Y . The conclusion now follows from the transitivity of graph separation. 

We have therefore constructed a clique tree for $\mathcal{H}$ , proving the inductive claim. 

Using this result, we can show that the independe es in an undirected graph $\mathcal{H}$ can be captured perfectly in a Bayesian network if and only if is chordal. 

**Theorem 4.13**
Let H be a chordal Markov network. Then there is a Bayesian network G such that I(H) = I(G) 
> 定理：
> $\mathcal H$ 为弦图 Makrov 网络，则存在贝叶斯网络 $\mathcal G$ 使得 $\mathcal I (\mathcal H) = \mathcal I (\mathcal G)$

Proof let clique $C_{1}$ to be the root of the clique tree, and then order the cliques $C_{1},\ldots,C_{k}$ using any topological ordering, that is, where cliques closer to the root are ordered ﬁrst. We now order the nodes in the network in any ordering consistent with the clique ordering: if $X_{l}$ ﬁrst appears in $C_{i}$ and $X_{m}$ ﬁrst appears in $C_{j}$ , for $i<j$ , then $X_{l}$ must precede $X_{m}$ in the ordering. We now construct a Bayesian network using the procedure Build-Minimal-I-Map of algorithm 3.2 applied to the resulting node ordering $X_{1},\dots,X_{n}$ and to $\mathcal{I}(\mathcal{H})$ . 
> 证明跳过

Let $\mathcal{G}$ be the result , when $X_{i}$ is added to the graph, then $X_{i}$ pa nts are precisely $U_{i}=\mathrm{Nb}_{X_{i}}\cap\{X_{1},.\,.\,.\,,X_{i-1}\}$ ∩{ − } , where $\operatorname{Nb}_{X_{i}}$ of $X_{i}$ H . In er words, we want to show that $X_{i}$ is independent o $\{X_{1},.\,.\,.\,,X_{i-1}\}-U_{i}$ − } $U_{i}$ . et $C_{k}$ be the ﬁrs que in the clique ordering to which X i belongs. Then $U_{i}\subset C_{k}$ ⊂ . Let $C_{l}$ be the parent of $C_{k}$ in the rooted clique tree. According to our selected ordering, all of the variables in $C_{l}$ are ordered before any variab $C_{k}-C_{l}$ $S_{l,k}\subset\{X_{1},.\.\,.\,,X_{i-1}\}$ . M ver, from our choice of ordering, none of { $\{X_{1},.\,.\,.\,,X_{i-1}\}-U_{i}$ − } − are in any descendants of $C_{k}$ in the clique tree. Thus, they are all in $W_{<(l,k)}$ . From theorem 4.12, it follows that $S_{l,k}$ separates $X_{i}$ from all of $\{X_{1},.\,.\,.\,,X_{i-1}\}-U_{i}$ , and ence that $X_{i}$ is independent of all of $\{X_{1},.\,.\,.\,,X_{i-1}\}-U_{i}$ given $U_{i}$ . It follows that $\mathcal{G}$ and H have e same set of edges. Moreover, e note that all of $U_{i}$ are in $C_{k}$ , and h nce are connected in G . Therefore, $\mathcal{G}$ is moralized. As H is the moralized undirected graph of G , the result now follows from proposition 4.9. 

For example, the graph $\mathcal{G}_{\ell}$ of ﬁgure 4.13b, and its moralized network $\mathcal{H}_{\ell}^{\prime}$ encode precisely the same independencies. By contrast, as we discussed, there exists no Bayesian network that encodes precisely the independencies in the nonchordal network $\mathcal{H}_{\ell}$ of ﬁgure 4.13a. 

Thus, we have shown that chordal graphs are precisely the intersection between Markov networks and Bayesian networks, in that the independencies in a graph can be represented exactly in both types of models if and only if the graph is chordal. 
> 弦图就是 Markov 网络和 Bayesian 网络之间的交集，当且仅当图是弦图，其独立性才可以精确被两种网络表示

## 4.6 Partially Directed Models 
So far, we have presented two distinct types of graphical models, based on directed and undirected graphs. We can unify both representations by allowing models that incorporate both directed and undirected dependencies. We begin by describing the notion of conditional random ﬁeld , a Markov network with a directed dependency on some subset of variables. We then present a generalization of this framework to the class of chain graphs , an entire network in which undirected components depend on each other in a directed fashion. 
> 考虑部分有向图，即有的边有向，有的边无向
> 例如条件随机场，CRF 是一个 Markov 网络，其中部分节点带有有向的依赖

### 4.6.1 Conditional Random Fields 
So far, we have described the Markov network representation as encoding a joint distribution over $\mathcal{X}$ . The same undirected gra tation a parameter iz ation can also be us to encode a conditional distribution $P(Y\mid X)$ | , where $Y$ is a set of target variables and X is a (disjoint) set of observed variables . We will also see a directed analogue of this concept in section 5.6. In the case of Markov networks, this representation is generally called a conditional random ﬁeld (CRF). 
> Markov 网络表示可以用于编码 $\mathcal X$ 上的联合分布，实际上相同的图表示和参数化也可以用于编码条件分布 $P (\pmb Y \mid \pmb X)$，其中 $\pmb Y$ 表示目标变量，$\pmb X$ 表示观察变量，这类 Markov 网络称为 CRF

#### 4.6.1.1 CRF Representation and Semantics 
More formally, a CRF is an undirected graph whose nodes correspond to $Y\cup X$ . At a high level, this graph is parameterized in the same way as an ordinary Markov network, as a set of factors $\phi_{1}(D_{1}),.\,.\,.\,,\phi_{m}(D_{m})$ . (As before, these factors can also be encoded more compactly as a log-linear model; for uniformity of presentation, we view the log-linear model as encoding a set of factors.) However, rather than encoding the distribution $P(Y,X)$ , we view it as representing the conditional distribution $P(Y\mid X)$ . To have the network structure and parameteriztion correspond naturally to a conditional distribution, we want to avoid representing a probabilistic model over $X$ . We therefore disallow potentials that involve only variables in $X$ . 
> CRF 是一个节点和 $\pmb Y \cup \pmb X$ 相对的无向图，该图参数化的方式和普通 Markov 网络相同，即用一组因子 $\phi_1 (\pmb D_1), \dots, \phi_m (\pmb D_m)$ 参数化，这组因子也可以用 log-linear 模型更紧凑地表示
> 差异在于我们将 CRF 编码的分布视作条件分布 $P (\pmb Y \mid \pmb X)$ 而不是联合分布 $P (\pmb Y, \pmb X)$，为此，我们需要避免在 $\pmb X$ 上表示概率模型，故不允许仅包含 $\pmb X$ 中的变量的势能函数

**Deﬁnition 4.18** conditional random ﬁeld 
$A$ conditional random ﬁeld is an undirected graph $\mathcal{H}$ whose nodes correspond to $X\cup Y$ ; the network is annotated with a set of factors $\phi_{1}(D_{1}),.\,.\,.\,,\phi_{m}(D_{m})$ such that each $D_{i}\nsubseteq X$ . The network encodes a conditional distribution as follows: 

$$
\begin{align}
P(\pmb Y \mid \pmb X) &= \frac {1}{Z(\pmb X)} \tilde P(\pmb Y, \pmb X)\\
\tilde P(\pmb Y, \pmb X)&= \prod_{i=1}^m \phi_i(\pmb D_i)\\
 Z(\pmb X)&=\sum_{\pmb Y}\tilde P(\pmb Y, \pmb X)
\end{align}\tag{4.11}
$$

Two variables in $\mathcal{H}$ are connected by an (undirected) edge whenever they appear together in the scope of some factor. 
> 定义：
> CRF 为一个无向图 $\mathcal H$，其节点对应于 $\pmb X \cup \pmb Y$，该网络由一组因子 $\phi_1 (\pmb D_1), \dots, \phi_m (\pmb D_m)$ 标注，其中 $\pmb D_i \not \subseteq \pmb X$
> 该网络如式 (4.11) 编码了条件概率分布 $P (\pmb Y \mid \pmb X)$
> 在 $\mathcal{H}$ 中，当且仅当两个变量同时出现在某个因子的作用范围内时，它们由一条无向边连接

> CRF 中，计算 $\tilde P (\pmb Y, \pmb X)$ 时不需要考虑仅属于 $\pmb X$ 的团，这些团可以视作编码了 $\pmb X$ 的先验，而因为 $\pmb X$ 总是给定，故不需要考虑 $\pmb X$ 自己的先验，或者可以认为 $\pmb X$ 取任意 $\pmb x$ 值的概率是相等的，此时只需要考虑它和 $\pmb Y$ 的交互的影响，因为就算考虑了，当 $\pmb X$ 取任意 $\pmb x$ 值的先验概率相等时， $\tilde P (\pmb Y, \pmb X)$ 和 $Z (\pmb X)$ 在任意的 $\pmb x$ 值下也只是同时乘上一个固定的常数，最后还是被消掉
> 事实上将 $\tilde P (\pmb Y, \pmb X)$ 直接视为一个未规范化的 $\pmb Y$ 条件于 $\pmb X$ 的条件分布会更加直观 (也就是写为 $\tilde P (\pmb Y \mid \pmb X)$)，因为它只考虑了 $\pmb Y$ 自己内部的交互和 $\pmb Y$ 和 $\pmb X$ 之间的交互，规范化常数是 $Z (\pmb X)$ 是条件于 $\pmb X$ 下 $\pmb Y$ 的所有可能取值的分数总和，因此它是和 $\pmb Y$ 无关的常数，因为 $\pmb X$ 是前提条件，故和 $\pmb X$ 有关，就写为 $Z (\pmb X)$

The only diference between equation (4.11) and the (unconditional) Gibbs distribution of deﬁnition 4.3 is the diferent normalization used in the partition function $Z(X)$ . The deﬁnition of a CRF induces a diferent value for the partition function for every assignment ${x}$ to $X$ . This diference is denoted graphically by having the feature variables grayed out. 
> 式 (4.11) 和定义4.3中对于 (无条件) Gibbs 分布的定义的差异仅在于在分区函数 $Z (\pmb X)$ 计算的不同，CRF 的定义为 $\pmb X$ 的不同赋值 $\pmb x$ 都引入一个不同的分区函数值
> 在图中，我们会将特征变量 $\pmb X$ 涂灰

![[Probabilistic Graph Theory-Fig4.14.png]]

Example 4.19 
Consider a CRF over $\pmb Y = \{Y_1, \dots, Y_k\}$ and $\pmb X = \{X_1, \dots, X_k\}$, with an edge $Y_i - Y_{i+1}$ ($i = 1, \dots, k-1$) and an edge $Y_i - X_i$  ($i = 1,\dots, k$), as shown in figure 4.14a. The distribution represented by this network has the form: 

$$
\begin{array}{r c l}{P(\boldsymbol{Y}\mid\boldsymbol{X})}&{=}&{\displaystyle\frac{1}{Z(\boldsymbol{X})}\tilde{P}(\boldsymbol{Y},\boldsymbol{X})}\\ {\tilde{P}(\boldsymbol{Y},\boldsymbol{X})}&{=}&{\displaystyle\prod_{i=1}^{k-1}\phi(Y_{i},Y_{i+1})\prod_{i=1}^{k}\phi(Y_{i},X_{i})}\\ {Z(\boldsymbol{X})}&{=}&{\displaystyle\sum_{\boldsymbol{Y}}\tilde{P}(\boldsymbol{Y},\boldsymbol{X}).}\end{array}
$$

Note that, unlike the deﬁnition of a conditional Bayesian network, the structure of a CRF may still contain edges between variables in $X$ , which arise when two such variables appear together in a factor that also contains a target variable. However, these edges do not encode the structure of any distribution over $X$ , since the network explicitly does not encode any such distribution. 
> 条件随机场和条件贝叶斯网络在图表示中不同的地方在于 CRF 的图表示中允许出现 $\pmb X$ 中的变量相连，这种情况在相连的 $X_i, X_j$ 都和 $\pmb Y$ 中的某个变量相连，并且一同处于一个团中
> 因为 CRF 的分布在定义上是不考虑 $\pmb X$ 之内的任意分布的，因此这条边并没有编码 $\pmb X$ 上的分布中的任意结构

**The fact that we avoid encoding the distribution over the variables in $X$ is one of the main strengths of the CRF representation. This ﬂexibility allows us to incorporate into the model a rich set of observed variables whose dependencies may be quite complex or even poorly understood. It also allows us to include continuous variables whose distribution may not have a simple parametric form. This ﬂexibility allows us to use domain knowledge in order to deﬁne a rich set of features characterizing our domain, without worrying about modeling their joint distribution.** 
>避免对变量集 $\pmb X$ 上的分布进行编码，这是 CRF 表示的主要优点之一，这种灵活性使我们能够将一组丰富的观测变量纳入模型，无论这些变量之间的依赖关系有多复杂和难以理解。此外，这也使我们能够引入那些分布可能没有简单参数形式的连续变量。这种灵活性允许我们利用领域知识来定义丰富的特征集，而不必担心对其联合分布进行建模

For example, returning to the vision MRFs of box 4.B, rather than deﬁning a joint distribution over pixel values and their region assignment, we can deﬁne a conditional distribution over segment assignments given the pixel values. The use of a conditional distribution here allows us to avoid making a parametric assumption over the (continuous) pixel values. Even more important, we can use image-processing routines to deﬁne rich features, such as the presence or direction of an image gradient at a pixel. Such features can be highly informative in determining the region assignment of a pixel. However, the deﬁnition of such features usually relies on multiple pixels, and deﬁning a correct joint distribution or a set of independence assumptions over these features is far from trivial. The fact that we can condition on these features and avoid this whole issue allows us the ﬂexibility to include them in the model. See box 4.E for another example. 
>举例来说，回到第4.B节中的视觉马尔可夫随机场（MRF），我们不必定义像素值及其区域分配的联合分布，而是可以在给定像素值的情况下定义区域分配的条件分布。这里使用条件分布使我们能够避免对（连续）像素值做出参数假设
>更重要的是，我们可以使用图像处理技术来定义丰富的特征，例如像素处的图像梯度的存在或方向。这些特征在确定像素的区域分配时是非常有信息量的，但定义这样的特征通常依赖于多个像素，在这些特征上定义正确的联合分布或一套独立性假设远非易事。而我们能够通过条件于这些特征来避免这一问题，从而赋予我们在模型中包含它们的灵活性。参见第4.E节中的另一个示例
>( 直观地说，就是建模 $P (\pmb Y \mid \pmb X)$ 比建模 $P (\pmb Y, \pmb X)$ 更简单，条件分布考虑了 $\pmb Y$ 和 $\pmb X$ 之间的交互、$\pmb Y$ 内部的交互，而不需要考虑 $\pmb X$ 内部的交互；条件分布的语义就是如此，$\pmb X$ 是给定的，我们仅关心它对 $\pmb Y$ 的影响如何，这或许和 $\pmb X$ 内部的交互有关，但它对外的表现就是 $\pmb X$ 和 $\pmb Y$ 之间的交互，故我们只需要关注后者就行 )

#### 4.6.1.2 Directed and Undirected Dependencies 
A CRF deﬁnes a conditional distribution of $Y$ on $X$ ; thus, it can be viewed as a partially directed graph, where we have an undirected component over $Y$ , which has the variables in $X$ as parents. 
> CRF 定义了 $\pmb Y$ 条件于 $\pmb X$ 的条件分布，因为引入了有向的条件依赖，因此它可以被视作部分有向图，图中对应 $\pmb Y$ 的成分是无向的，但它们有 $\pmb X$ 对应的节点作为父变量

Example 4.20 naive Markov 
Consider a CRF over the binary-valued variables $\pmb X\;=\;\{X_{1},.\,.\,.\,,X_{k}\}$ and ${\pmb Y}\,=\,\{{Y}\}$ , and a pairwise potential between $Y$ and each $X_{i}$ ; this model is sometimes known as a naive Markov model, due to its similarity to the naive Bayes model. Assume that the pairwise potentials deﬁned via the following log-linear model 

$$
\phi_{i}(X_{i},Y)=\exp\left\{w_{i}{\pmb 1}\{X_{i}=1,Y=1\}\right\}.
$$ 
We also introduce a single-node potential $\phi_{0}(Y)=\exp\left\{w_{0}{\pmb 1}\{Y=1\}\right\}$ . 
> 根据 HC 定理，朴素 Markov 模型的团势能函数定义如上

Following equation (4.11), we now have: 

$$
\begin{array}{c c l}{{\displaystyle\tilde{P}(Y=1\mid x_{1},\ldots,x_{k})}}&{{=}}&{{\displaystyle\exp\left\{w_{0}+\sum_{i=1}^{k}w_{i}x_{i}\right\}}}\\ {{\displaystyle\tilde{P}(Y=0\mid x_{1},\ldots,x_{k})}}&{{=}}&{{\displaystyle\exp\left\{0\right\}=1.}}\end{array}
$$ 


In this case, we can show (exercise 5.16) that 

$$
P(Y=1\mid x_{1},\dots,x_{k})=\mathrm{sigmoid}\left(w_{0}+\sum_{i=1}^{k}w_{i}x_{i}\right),
$$ 
where 

$$
\mathrm{sigmoid}(z)=\frac{e^{z}}{1+e^{z}}
$$ 
> 推导：
> 根据式 (4.11)

$$
\begin{align}
\tilde P(Y\mid \pmb X) &={\phi_0(Y)\prod_i\phi_i(X_i,Y)}\\
\end{align}
$$

> 因此

$$\begin{align}\tilde P(Y=1\mid \pmb X) &={\exp \{w_0\}\prod_i\exp\{w_ix_i\}}\\
&=\exp\left\{w_0 + \sum_i w_i x_i\right\}\\

\tilde P(Y=0\mid \pmb X) &= {\exp \{0\}\prod_i\exp\{0\}}=1
\end{align}
$$

> 再将具体的值代入 $P (Y=y \mid \pmb X) = P (Y = y \mid \pmb X) / \sum_Y P (Y \mid \pmb X)$ 计算即可

is the sigmoid function. This conditional distribution $P(Y\mid X)$ is of great practical interest: It deﬁnes a CPD that is not structured as a table, but that is induced by a small set of parameters $w_{0},\ldots,w_{k}$ — parameters whose number is linear, rather than exponential, in the number of parents. This type of CPD, often called a logistic CPD , is a natural model for many real-world applications, inasmuch as it naturally aggregates the inﬂuence of diferent parents. We discuss this CPD in greater detail in section 5.4.2 as part of our general presentation of structured CPDs. 

The partially directed model for the CRF of example 4.19 is shown in ﬁgure 4.14b. We may be tempted to believe that we can construct an equivalent model that is fully directed, such as the one in ﬁgure $4.14\mathrm{c}$ . In particular, conditioned on any assignment $\mathbf {x}$ , the posterior distributions over $Y$ in the two models satisfy the same independence assignments (the ones deﬁned by the chain structure). However, the two models are not equivalent: In the Bayesian network, we have that $Y_{1}$ is independent of $X_{2}$ if we are not given $Y_{2}$ . By contrast, in the original CRF, the unnormalized marginal measure of $Y$ depends on the entire parameter iz ation of the chain, and speciﬁcally the values of all of the variables in $X$ . A sound conditional Bayesian network for this distribution would require edges from all of the variables in $X$ to each of the variables $Y_{i}$ , thereby losing much of the structure in the distribution. See also box 20.A for further discussion. 
> Fig 4.14 中 b 图和 c 图的独立性语义不是等价的，c 图中的贝叶斯网络中，$Y_1$ 在不给定 $Y_2$ 时是和 $X_2$ 相互独立的，而 b 图的 CRF 中 $Y_i$ 是依赖于所有的 $X_i$ 的，因此如果要建模为一个条件贝叶斯网络，我们需要将所有的 $X_i$ 向 $Y_i$ 引入一条有向边，这会导致失去所有 CRF 编码的 $Y_i$ 之间的独立性关系

Box 4.E — Case Study: CRFs for Text Analysis. One important use for the CRF framework is in the domain of text analysis. Various models have been proposed for diferent tasks, including part-of-speech labeling, identifying named entities (people, places, organizations, and so forth), and extracting structured information from the text (for example, extracting from a reference list the publication titles, authors, journals, years, and the like). Most of these models share a similar structure: We have a target variable for each word (or perhaps short phrase) in the document, which encodes the possible labels for that word. Each target variable is connected to a set of feature variables that capture properties relevant to the target distinction. These methods are very popular in text analysis, both because the structure of the networks is a good ﬁt for this domain, and because they produce state-of-the-art results for a broad range of natural-language processing problems. 

As a concrete example, consider the named entity recognition task, as described by Sutton and McCallum (2004, 2007). Entities often span multiple words, and the type of an entity may not be apparent from individual words; for example, “New York” is a location, but “New York Times” is an organization. The problem of extracting entities from a word sequence of length $T$ can be cast as a graphical model by introducing for each word, $X_{t},1\leq t\leq T$ , a target variable, $Y_{t}$ , which indicates the entity type of the word. The outcomes of $Y_{t}$ include B-Person, I-Person, B-Location, I-Location, B-Organization, I-Organization , and Other . In this so-called “BIO notation,” Other indicates that the word is not part of an entity, the B- outcomes indicate the beginning of a named entity phrase, and the I- outcomes indicate the inside or end of the named entity phrase. Having a distinguishing label for the beginning versus inside of an entity phrase allows the model to segment adjacent entities of the same type. 

linear-chain CRF 

hidden Markov model 

A common structure for this problem is $a$ linear-chain CRF often having two factors for each word: one factor $\phi_{t}^{1}(Y_{t},Y_{t+1})$ to represent the dependency between neighboring target variables, and another factor $\phi_{t}^{2}(Y_{t},X_{1},\dots.,X_{T})$ that represents the dependency between a target and its context in the word sequence. Note that the second factor can depend on arbitrary features of the entire input word sequence. We generally do not encode this model using table factors, but using a log-linear model. Thus, the factors are derived from a number of feature functions, such as . We note that, just as logistic CPDs are the conditional analog of the naive Bayes classiﬁer (example 4.20), the linear-chain CRF is the conditional analog of the hidden Markov model (HMM) that we present in section 6.2.3.1. 

A large number of features of the word $X_{t}$ and neighboring words are relevant to the named entity decision. These include features of the word itself: is it capitalized; does it appear in a list of common person names; does it appear in an atlas of location names; does it end with the character string “ton”; is it exactly the string “York”; is the following word “Times.” Also relevant are aggregate features of the entire word sequence, such as whether it contains more than two sports-related words, which might be an indicator that “New York” is an organization (sports team) rather than a location. In addition, including features that are conjunctions of all these features often increases accuracy. The total number of features can be quite large, often in the hundreds of thousands or more if conjunctions of word pairs are used as features. However, the features are sparse, meaning that most features are zero for most words. 

Note that the same feature variable can be connected to multiple target variables, so that $Y_{t}$ would typically be dependent on the identity of several words in a window around position t . These contextual features are often highly indicative: for example, “Mrs.” before a word and “spoke” after a word are both strong indicators that the word is a person. These context words would generally be used as a feature for multiple target variables. Thus, if we were using a simple naive-Bayes-style generative model, where each target variable is a parent of its associated feature, we either would have to deal with the fact that a context word has multiple parents or we would have to duplicate its occurrences (with one copy for each target variable for which it is in the context), and thereby overcount its contribution. 

Linear-chain CRFs frequently provide per-token accuracies in the high 90 percent range on many natural data sets. Per-ﬁeld precision and recall (where the entire phrase category and boundaries must be correct) are more often around 80–95 percent, depending on the data set. 

skip-chain CRF 

Although the linear-chain model is often efective, additional information can be incorporated into the model by augmenting the graphical structure. For example, often when a word occurs multiple times in the same document, it often has the same label. This knowledge can be incor- porated by including factors that connect identical words, resulting in a skip-chain CRF , as shown in ﬁgure 4.E.1a. The ﬁrst occurrence of the word “Green” has neighboring words that provide strong  KEY  B-PER Begin person name I-LOC Within location name I-PER Within person name OTH Not an entitiy B-LOC Begin location name  (a)  $B$ Begin noun phrase V Verb $I$ Within noun phrase IN Preposition $O$ Not a noun phrase PRP Possesive pronoun $N$ Noun DT Determiner (e.g., a, an, the) $A D J$ Adjective 

Figure 4.E.1 — Two models for text analysis based on a linear chain CRF Gray nodes indicate $_{X}$ and clear nodes $\mathbf{Y}$ . The annotations inside the $\mathbf{Y}$ are the true labels. (a) A skip chain CRF for named entity recognition, with connections between adjacent words and long-range connections between multiple occurrences of the same word. (b) A pair of coupled linear-chain CRFs that performs joint part-of-speech labeling and noun-phrase segmentation. Here, B indicates the beginning of a noun phrase, I other words in the noun phrase, and O words not in a noun phrase. The labels for the second chain are parts of speech. 

evidence that it is a Person ’s name; however, the second occurrence is much more ambiguous. By augmenting the original linear-chain CRF with an additional long-range factor that prefers its connected target variables to have the same value, the model is more likely to predict correctly that the second occurrence is also a Person . This example demonstrates another ﬂexibility of conditional models, which is that the graphical structure over $Y$ can easily depend on the value of the $X{\dot{s}}.$ . 

coupled HMM 

CRFs having a wide variety of model structures have been successfully applied to many difer- ent tasks. Joint inference of both part-of-speech labels and noun-phrase segmentation has been performed with two connected linear chains (somewhat analogous to a coupled hidden Markov mode , shown in ﬁgure 6.3). This structure is illustrated in ﬁgure 4.E.1b. 

### 4.6.2 Chain Graph Models\* 
We now present a more general framework that builds on the CRF representation and can be used to provide a general treatment of the independence assumptions made in these partially directed models. Recall from deﬁnition 2.21 that, in a partially directed acyclic graph (PDAG), the nodes can be disjointly partitioned into several chain components . An edge between two nodes in the same chain component must be undirected, while an edge between two nodes in diferent chain components must be directed. Thus, PDAGs are also called chain graphs . 

#### 4.6.2.1 Factorization 

chain graph model 

As in our other graphical representations, the str ture of a PDAG $\mathcal{K}$ can be used to deﬁne a factorization for a probability distribution over K . Intuitively, the factorization for PDAGs represents the distribution as a product of each of the chain components given its parents. Thus, we call such a representation a chain graph model . 

Intuitively, each chain component $K_{i}$ in the chain graph model is associated with a CRF that deﬁnes $\mathcal{P}(\boldsymbol{K}_{i}\mid\mathrm{Pa}_{\boldsymbol{K}_{i}})$ — the conditional distribution of $K_{i}$ given its parents the graph. More precisely, each is deﬁned via a set of factors that involve the variables in $K_{i}$ and their parents; the distribution $\mathcal{P}(K_{i}\mid\mathrm{Pa}_{K_{i}})$ deﬁned by using the factors associa ith $K_{i}$ to deﬁne a CRF whose target variables are $K_{i}$ and whose observable variables are $\mathrm{Pa}_{K_{i}}$ . 

To provide a formal deﬁnition, it helps to introduce the concept of a moralized PDAG. 

Deﬁnition 4.19 moralized graph Let $\mathcal{K}$ be DAG and $K_{1},\dots,K_{\ell}$ b its chain components. We deﬁne $\mathrm{Pa}_{K_{i}}$ to be the parents of nodes in $K_{i}$ i . The moralized graph of K is an undirected graph $\mathcal M[\mathcal K]$ ed by ﬁrst connecting, using undirected edges, any pair of nodes $X,Y\in\mathrm{Pa}_{K_{i}}$ for all $i=1,\ldots,\ell,$ , and then converting all directed edges into undirected edges. 

This deﬁnition generalizes our earlier notion of a moralized directed graph. In the case of directed graphs, each node is its own chain component, and hence we are simply adding undirected edges between the parents of each node. 

Example 4.21 

Figure 4.15 A chain graph $\mathcal{K}$ and its moralized version 

$D$ and $H$ (even though $D$ and $C,E$ are in the same chain component), since $D$ is not a parent of $I$ . 

We can now deﬁne the factorization of a chain graph: 

Deﬁnition 4.20 chain graph distribution 

Let $\mathcal{K}$ be a PDAG, and $K_{1},\dots,K_{\ell}$ be its chain components. $A$ chain graph distribution is deﬁned via a set of factors $\phi_{i}(D_{i})$ $(i=1,.\ldots,m),$ , such that each $D_{i}$ is a complete subgraph in the moralized graph $\mathcal{M}[\mathcal{K}^{+}[D_{i}]]$ . We associate $\phi_{i}(D_{i})$ with a single chain component $K_{j}$ , $D_{i}\subseteq K_{i}\cup\mathrm{Pa}_{K_{i}}$ and deﬁne $P(K_{i}\mid\mathrm{Pa}_{K_{i}})$ | as a CRF with these factors, and with $Y_{i}=K_{i}$ and $X_{i}=\mathrm{Pa}_{K_{i}}$ . We now deﬁne 

$$
P(\mathcal{X})=\prod_{i=1}^{\ell}P(K_{i}\mid\mathrm{Pa}_{K_{i}}).
$$ 

We say that a d tribution $P$ factorizes over $\mathcal{K}$ if it can be represented as a chain graph distribution over . 

 Example 4.22 

In the chain graph model deﬁned by the graph of ﬁgure 4.15, we require that the conditional distribution $P(C,D,E\mid A,B)$ factorize according to the graph of ﬁgure 4.16a. Speciﬁcally, we would have to deﬁne the conditional probability as a normalized product of factors: 

$$
{\frac{1}{Z(A,B)}}\phi_{1}(A,C)\phi_{2}(B,E)\phi_{3}(C,D)\phi_{4}(D,E).
$$ 

#### 4.6.2.2 Independencies in Chain Graphs 
boundary As for undirected graphs, there are three distinct interpretations for the independence properties induced by a PDAG. Recall that in a PDAG, we have both the notion of parents of $X$ (variables $Y$ such that $Y\rightarrow X$ is in the graph) and neighbors of $X$ (variable $Y$ such that $Y{-}X$ is in the graph). Recall that the union of these two sets is the boundary of X , denoted Boundary $X$ . Also recall, from deﬁnition 2.15, that the descendants of $X$ are those nodes $Y$ that can be reached using any directed path, where a directed path can involve both directed and undirected edges but must contain at least one edge directed from $X$ to $Y$ , and no edges directed from $Y$ to 

 
Figure 4.16 Example for deﬁnition of c-separation in a chain graph. (a) The Markov network $\mathcal{M}[\mathcal{K}^{+}[C,D,E]]$ . (b) The Markov network $\mathcal{M}[\mathcal{K}^{+}[C,D,E,I]]$ . 

$X$ . Thus, in the case of PDAGs, it follows that if $Y$ is a descendant of $X$ , then $Y$ must be in a “lower” chain component. 

Deﬁnition 4.21 

pairwise independencies For a PDAG $\mathcal{K}$ , we deﬁne the pairwise independencies associated with $\mathcal{K}$ to be: 

$$
\begin{array}{c}{{\mathcal{Z}_{p}(\mathcal{K})=\{(X\perp Y\mid\mathrm{(NonDSedant3}_{X}-\{X,Y\}))\;:}}\\ {{X,Y\ n o n{\cdot}a d j a c e n t,Y\in\mathrm{NonDSedant3}_{X}\}.}}\end{array}
$$ 

This deﬁnition generalizes the pairwise independencies for undirected graphs: in an undirected graph, nodes have no descendants, so NonDescendants $\v{x}=\v{x}$ . Similarly, it is not too hard to show that these independencies also hold in a directed graph. 

Deﬁnition 4.22 

local independencies For a PDAG $\mathcal{K}$ , we deﬁne the local independencies associated with $\mathcal{K}$ to be: 

$$
{\mathcal{Z}}_{\ell}(X)=\{(X\;\bot\;{\mathrm{NonDS}}_{X}-{\mathrm{Boundary}}_{X}\;|\;{\mathrm{boundary}}_{X})\;:\;X\in{\mathcal{X}}\}.
$$ 

This deﬁnition generalizes the deﬁnition of local independencies for both directed and undi- rected graphs. For directed graphs, NonDescendants $X$ is precisely the set of nondescendants, whereas Boundary $X$ is the set of parents. For undirected graphs, NonDescendants $X$ is $\mathcal{X}$ , whereas Boundary $\v{U}_{X}=\mathrm{Nb}_{X}$ . 

We deﬁne the global independencies in a PDAG using the deﬁnition of moral graph. Our deﬁnition follows the lines of proposition 4.10. 

Deﬁnition 4.23 c-separation 

Example 4.23 

Let $X,Y,Z\subset\mathcal{X}$ three disjoint set nd let $U=X\cup Y\cup Z$ . We sa $X$ from $Y$ given Z if $X$ is separated from $Y$ given Z in the undirected graph M ${\mathcal{M}}[{\mathcal{K}}^{+}[X\cup Y\cup Z]].$ K ∪ ∪ 

Consider again the PDAG of ﬁgure 4.15. Then $C$ is $c$ -separated from $E$ given $D,A$ , because $C$ and $E$ are s arated ven $D,A$ in the ndirected gr h $\mathcal{M}[\mathcal{K}^{+}[\{C,D,E\}]]$ , shown ﬁgure .16a. - ever, C is not c-separated from E given only D , since there is $^a$ path between C and E via $A,B$ . On the other hand, $C$ is not separated from $E$ given $D,A,I$ . The graph $\mathcal{M}[\mathcal{K}^{+}[\{C,D,E,I\}]]$ is shown in ﬁgure 4.16b. As we can see, the introduction of I into the set $U$ causes us to introduce a direct edge between $C$ and $E$ in order to moralize the graph. Thus, we cannot block the path between $C$ and $E$ using $D,A,I$ . 

This notion of c-separation clearly generalizes the notion of separation in undirected graphs, since the ancestors of a set $U$ n an undirected graph are simply the entire set of nodes $\mathcal{X}$ . It also generalizes the notion of d-separation in directed graphs, using the equivalent deﬁnition provided in proposition 4.10. Using the deﬁnition of c-separation, we can ﬁnally deﬁne the notion of global Markov independencies: 

Deﬁnition 4.24 global independencies Let $\mathcal{K}$ be a PDAG. We deﬁne the global independencies associated with $\mathcal{K}$ to be: ${\mathcal{Z}}(\mathcal{K})=\{(X\perp Y\mid Z)\;:\;X,Y,Z\subset{\mathcal{X}},X$ is $c$ -separated from $Y$ given $Z\}$ . 

As in the case of undirected models, these three criteria for independence are not equivalent for nonpositive distributions. The inclusions are the same: the global independencies imply the local independencies, which in turn imply the pairwise independencies. Because undirected models are a subclass of PDAGs, the same counterexamples used in section 4.3.3 show that the inclusions are strict for nonpositive distributions. For positive distributions, we again have that the three deﬁnitions are equivalent. 

We note that, as in the case of Bayesian networks, the parents of a chain component are always fully connected in $\mathcal{M}[\mathcal{K}[K_{i}\cup\mathrm{Pa}_{K_{i}}]]$ . Thus, while the structure over the parents helps factorize the distribution over the chain components containing the parents, it does not give rise to independence assertions in the conditional distribution over the child chain component. Importantly, however, it does give rise to structure in the form of the parameter iz ation of $P(K_{i}\mid\mathrm{Pa}_{K_{i}})$ , as we saw in example 4.20. 

As in the case of directed and undirected models, we have an equivalence between the requirement of factorization of a distribution and the requirement that it satisfy the indepen- dencies associated with the graph. Not surprisingly, since PDAGs generalize undirected graphs, this equivalence only holds for positive distributions: 

Theorem 4.14 A positive distribution $P$ factorizes over a PDAG K if and only if $P\vDash\mathcal{Z}(\mathcal{K})$ I K . We omit the proof. 

## 4.7 Summary and Discussion 
In this chapter, we introduced Markov networks , an alternative graphical modeling language for probability distributions, based on undirected graphs. 
> 本章介绍了描述概率分布的无向图语言，也就是 Markov 网络

We showed that Markov networks, like Bayesian networks, can be viewed as deﬁning a set of independence assumptions determined by the graph structure. In the case of undirected models, there are several possible deﬁnitions for the independence assumptions induced by the graph, which are equivalent for positive distributions. As in the case of Bayesian network, we also showed that the graph can be viewed as a data structure for specifying a probability distribution in a factored form. The factorization is deﬁned as a product of factors (general nonnegative functions) over cliques in the graph. We showed that, for positive distributions, the two characterizations of undirected graphs — as specifying a set of independence assumptions and as deﬁning a factorization — are equivalent. 
> Markov 网络也可以被视为编码了由图结构定义的一组独立性假设，这和贝叶斯网络是类似的
> Markov 网络的多个对独立性的定义对于正分布是等价的
> Markov 网络也可以视为以分解的形式指定了一个概率分布的数据结构，这和贝叶斯网络也是类似的。Markov 网络的分解定义为在图中的 cliques 上的 (非负) 因子的乘积
>对于正分布，无向图对于分布的两种表征——指定一组独立性假设和指定一种分布形式——是等价的

Markov networks also provide useful insight on Bayesian networks. In particular, we showed how a Bayesian network can be viewed as a Gibbs distribution. More importantly, the unnormalized measure we obtain by introducing evidence into a Bayesian network is also a Gibbs distribution, whose partition function is the probability of the evidence. This observation will play a critical role in providing a uniﬁed view of inference in graphical models. 
> Markov 网络也帮助我们理解贝叶斯网络
> 例如，我们可以将贝叶斯网络也视为定义了 Gibbs 分布，向贝叶斯网络引入 evidence 得到的未规范化的分布也是 Gibbs 分布，其划分函数就是 evidence 的概率
> 这会帮助我们为图模型的推理提供一个统一视角

We investigated the relationship between Bayesian networks and Markov networks and showed that the two represent diferent families of independence assumptions. The diference in these independence assumptions is a key factor in deciding which of the two representations to use in encoding a particular domain. There are domains where interactions have a natural directionality, often derived from causal intuitions. In this case, the independencies derived from the network structure directly reﬂect patterns such as intercausal reasoning. Markov networks represent only monotonic independence patterns: observing a variable can only serve to remove dependencies, not to activate them. Of course, we can encode a distribution with “causal” connections as a Gibbs distribution, and it will exhibit the same nonmonotonic independencies. However, these independencies will not be manifest in the network structure. 
> 贝叶斯网络和 Markov 网络各自表示不同的一族独立性假设，这些独立性假设的差异是我们决定具体要使用哪种模型解决问题的关键
> 一些领域中，交互具有自然的有向性，例如因果推理，此时网络结构中的独立性就表示了因果关系；Markov 网络则仅表示单调的独立性模式：观察到一个变量仅能移除依赖性，而不能激活依赖性 (对比于贝叶斯网络中的 v-structure)
> 我们当然可以将带有因果关系的分布编码为 Gibbs 分布，该分布也会展示出相同的非单调独立性，但对应的网络结构就无法完美反映这一关系

In other domains, the interactions are more symmetrical, and attempts to force a directionality give rise to models that are unintuitive and that often are incapable of capturing the independencies in the domain (see, for example, section 6.6). As a consequence, the use of undirected models has increased steadily, most notably in ﬁelds such as computer vision and natural language processing, where the acyclicity requirements of directed graphical models are often at odds with the nature of the model. The ﬂexibility of the undirected model also allows the distribution to be decomposed into factors over multiple overlapping “features” without having to worry about deﬁning a single normalized generating distribution for each variable. Conversely, this very ﬂexibility and the associated lack of clear semantics for the model parameters often make it difcult to elicit models from experts. Therefore, many recent applications use learning techniques to estimate parameters from data, avoiding the need to provide a precise semantic meaning for each of them. 
> 一些领域中，交互是对称的，添加有向性会使得图结构无法捕获领域中的一些独立性 (菱形结构)
> 在 CV 和 NLP 领域使用较多的是无向图，这些领域中，有向图的无环要求有时会和模型的自然性质冲突，而无向图的灵活性较高
> 使用无向图建模时，我们可以定义多个交叉的“特征”作为因子，进而表示分布，使用有向图则需要为每个变量定义一个规范化的条件分布
> 但灵活性意味着模型的参数缺乏清晰语义，使得专家不便于定义模型参数，因此我们开始使用学习方法，从数据中学习和估计参数，不再为它们确定详细的语义含义

Finally, the question of which class of models better encodes the properties of the distribution is only one factor in the selection of a representation. There are other important distinctions between these two classes of models, especially when it comes to learning from data. We return to these topics later in the book (see, for example, box 20.A). 

# 5 Local Probabilistic Models 
In chapter 3 and chapter 4, we discussed the representation of global properties of independence by graphs. These properties of independence allowed us to factorize a high-dimensional joint distribution into a product of lower-dimensional CPDs or factors. So far, we have mostly ignored the representation of these factors. In this chapter, we examine CPDs in more detail. We describe a range of representations and consider their implications in terms of additional regularities we can exploit. We have chosen to phrase our discussion in terms of CPDs, since they are more constrained than factors (because of the local normalization constraints). However, many of the representations we discuss in the context of CPDs can also be applied to factors. 
> 我们之前讨论了将高维的联合分布分解为低维的 CPDs 或因子的乘积，本章讨论 CPDs 的细节
## 5.1 Tabular CPDs 
When dealing with spaces composed solely of discrete-valued random variables, we can always resort to a tabular representation of CPDs, w re w code $P(X~\mid\operatorname{Pa}_{X})$ as a table that contains an entry for each joint assignment to X and $\mathrm{Pa}_{X}$ . For this table to be a proper CPD, we require that all the values are nonnegative, and that, for each value $\operatorname{pa}_{X}$ , we have 
> 当处理仅仅由离散值随机变量构成的空间时，我们总是可以用表格表示 CPDs，表格中的各个项包含了 $P (X\mid \text{Pa}_X)$ 的所有可能值，同时，我们要求

$$
\sum_{x\in V a l(X)}{\cal P}(x\mid\mathrm{pa}_{X})=1.\tag{5.1}
$$ 
It is clear that this representation is as general as possible. We can represent every possible discrete CPD using such a table. 

As we will also see, table-CPDs can be used in a natural way in inference algorithms that we discuss in chapter 9. These advantages often lead to the perception that table-CPDs , also known as conditional probability tables (CPTs), are an inherent part of the Bayesian network representation. 
> table-CPDs，也可以称为条件概率表格 CPTs

However, the tabular representation also has several signiﬁcant disadvantages. First, it is clear that if we consider random variables with inﬁnite domains (for example, random variables with continuous values), we cannot store each possible conditional probability in a table. But even in the discrete setting, we encounter difculties. The number of parameters needed to describe a table-CPD is the number of joint assignments to $X$ and $\mathrm{Pa}_{X}$ , that is, $|\,V a l(\mathrm{Pa}_{X})|\cdot|\,V a l(X)|$ . This number grows exponentially in the number of parents. Thus, for example, if we have 5 binary parents of a binary variable $X$ , we need specify $2^{5}=32$ values; if we have 10 parents, we need to specify $2^{10}=1,024$ values. 
> table-CPDs 需要的参数数量是  $|\,V a l(\mathrm{Pa}_{X})|\cdot|\,V a l(X)|$，这个值是随着 parents 的数量而指数增长的（$|Val(\text{Pa}_X)|$ 的值随着 $\text{Pa}_X$ 的数量指数增长）

Clearly, the tabular representation rapidly becomes large and unwieldy as the number of parents grows. This problem is a serious one in many settings. Consider a medical domain where a symptom, Fever , depends on 10 diseases. It would be quite tiresome to ask our expert 1,024 questions of the format: “What is the probability of high fever when the patient has disease $A$ , does not have disease $B$ , . . . ?” Clearly, our expert will lose patience with us at some point!  This example illustrates another problem with the tabular representation: it ignores structure within the CPD. If the CPD is such that there are no similarity between the various cases, that is, each combination of disease has drastically diferent probability of high fever, then the expert might be more patient. However, in this example, like many others, there is some regularity in the parameters for diferent values of the parents of $X$ . For example, it might be the case that, if the patient sufers from disease $A$ , then sh is certain to have high fever and thus $P(X\mid\mathrm{{pa}}_{X})$ is the same for all values $\operatorname{pa}_{X}$ in which A is true. Indeed, many of the representations we consider in this chapter attempt to describe such regularities explicitly and to exploit them in order to reduce the number of parameters needed to specify a CPD. 
> table-CPDs 忽视了 CPD 内的结构
> 本章我们考虑的许多表示都尝试显式描述 CPD 内的 regularity 以减少表示 CPS 所需要的参数

The key insight that allows us to avoid these problems is the following observation: **A CPD needs to specify a conditional probability $P(x\mid\mathrm{pa}_{X})$ for every assignment of values $\mathrm{pa}_{X}$ and $x$ , but it does not have to do so by listing each such value explicitly. We should view CPDs not as tables listing all of the conditional probabilities, but rather as functions that given $\mathrm{pa}_{x}$ and $x$ , return the conditional probability $P(x\mid\mathrm{pa}_{X})$ .** This implicit representation sufces in order to specify a well-deﬁned joint distribution as a BN. In the remainder of the chapter, we will explore some of the possible representations of such functions. 
> CPD 需要为所有的 $\text{pa}_X$ 和 $x$ 的赋值指定出 $P (x \mid \text{pa}_X)$，但并不一定要列举出它们的所有值，我们可以将 CPD 看作一个函数，给定 $\text{pa}_x, x$，返回条件概率 $P (x\mid \text{pa}_X)$
## 5.2 Deterministic CPDs 
### 5.2.1 Representation 
Perhaps the simplest type of nontabular CPD arises when a variable $X$ is a deterministic function of its parents $\mathrm{Pa}_{X}$ . That is, there is a function $f:V a l(\mathrm{Pa}_{X})\mapsto V a l(X)$ , such that 

$$
P(x\mid\operatorname{pa}_{X})={\left\{\begin{array}{l l}{1}&{\qquad x=f(\operatorname{pa}_{X})}\\ {0}&{\qquad{\mathrm{otherwise.}}}\end{array}\right.}
$$ 
> 当变量 $X$ 是 $\text{Pa}_X$ 的确定性函数时，我们得到最简单的非表格 CPD，也就是当 $x = f (\text{pa}_x)$ 时，$P (x\mid \text{pa}_X)$ 为1，否则都为0

For example, in the case of binary-valued variables, $X$ might be the “or” of its parents. In a continuous domain, we might want to assert in $P(X\mid Y,Z)$ that $X$ is equal to $Y+Z$ . 
> 例如，对于二值变量，$X$ 是它的父变量的 “或”，对于连续变量，$X = Y + Z$

Of course, the extent to which this representation is more compact than a table (that is, takes less space in the computer) depends on the expressive power that our BN modeling language offers us for specifying deterministic functions. For example, some languages might allow a vocabulary that includes only logical OR and AND of the parents, so that all other functions must be speciﬁed explicitly as a table. In a domain with continuous variables, a language might choose to allow only linear dependencies of the form $X=2Y+-3Z+1$ , and not arbitrary functions such as $X=\sin(y+e^{z})$ . 
> 但这类表示的 compact 程序取决于 BN 建模语言可以为我们提供的用于指定确定性函数的表示能力
> 例如可能只能建模逻辑的 OR/AND，因此其他的函数就必须用表格指定；例如只能建模线性关系，而不能是三角函数

Deterministic relations are useful in modeling many domains. In some cases, they occur naturally. Most obviously, when modeling constructed artifacts such as machines or electronic circuits, deterministic dependencies are often part of the device speciﬁcation. For example, the behavior of an OR gate in an electronic circuit (in the case of no faults) is that the gate output is a deterministic OR of the gate inputs. However, we can also ﬁnd deterministic dependencies in “natural” domains. 
> 确定性依赖有时本身就是规格指定的一部分，例如电路的 OR 门，有时我们也可以在 “natural” domains 中找到确定性依赖

Example 5.1 
Recall that the genotype of a person is determined by two copies of each gene, called alleles . Each allele can take on one of several values corresponding to diferent genetic tendencies. The person’s phenotype is often a deterministic function of these values. For example, the gene responsible for determining blood type has three values: $a,\,b,$ , and o . Letting $G_{1}$ and $G_{2}$ be variables representing the two alleles, and $T$ the variable representing the phenotypical blood type, then we have that: 

$$
T=\left\{\begin{array}{l l}{a l}\\ {a}\\ {}\\ {b}\\ {}\\ {o}\end{array}\right.
$$ 

Deterministic variables can also help simplify the dependencies in a complex model. 
> 确定性变量也可以帮助简化复杂模型中的依赖

Example 5.2 
When modeling a car, we might have four variables $T_{1},\dots,T_{4}$ , each corresponding to a ﬂat in one of the four tires. When one or more of these tires is ﬂat, there are several efects; for example, the steering may be afected, the ride can be rougher, and so forth. Naively, we can make all of the $T_{i}$ ’s parents of all of the afected variables — Steering, Ride, and so on. However, it can signiﬁcantly simplify the model to introduce a new variable Flat-Tire, which is the deterministic OR of $T_{1},\dots,T_{4}$ . We can then replace a complex dependency of Steering and Ride on $T_{1},\dots,T_{4}$ with a dependency on a single parent Flat-Tire, signiﬁcantly reducing their indegree. If these variables have other parents, the savings can be considerable. 
> good example
### 5.2.2 Independencies 
Aside from a more compact representation, we get an additional advantage from making the structure explicit.
Recall that conditional independence is a numerical property — it is deﬁned using equality of probabilities. However, the graphical structure in a BN makes certain properties of a distribution explicit, allowing us to deduce that some independencies hold without looking at the numbers. 
By making structure explicit in the CPD, we can do even more of the same. 
> 除了表示会更 compact，结构也会更加清晰


![[Probabilistic Graph Theory-Fig5.1.png]]

Example 5.3 
Consider the simple network structure in ﬁgure 5.1. If $C$ is a deterministic function of $A$ and $B$ , what new conditional independencies do we have? Suppose that we are given the values of $A$ and $B$ . Then, since $C$ is deterministic, we also know the value of $C$ . As a consequence, we have that $D$ and $E$ a independent. Thus, we conclude tha $(D\perp E\mid A,B)$ holds in the distribution. Note that, had C not been a deterministic function of A and B , this independence would not necessarily hold. Indeed, $d$ -separation would not deduce that $D$ and $E$ are independent given $A$ and $B$ . 
> 例如，当 $C$ 是 $A, B$ 的确定性函数时，知道了 $A, B$，实际上也知道了 $C$，因此就知道了 $D\perp E$，也就是对于该分布，$(D\perp E\mid A, B)$ 成立

Can we augment the d-separation procedure to discover independencies in cases such as this? Consider an independence assertion $(X\ \bot\ Y\ |\ Z)$ in our example, we are interested in the case where $Z=\{A,B\}$ . The variable C is not in Z and is therefore not considered observed. 

But when $A$ and $B$ are observed, then the value of $C$ is also known with certainty, so we can consider it as part of our observed set $Z$ . In our example, this simple modiﬁcation would sufce for inferring that $D$ and $E$ are independent given $A$ and $B$ . 

![[Probabilistic Graph Theory-Algorithm5.1.png]]

In other examples, however, we might need to continue this process. For example, if we had another variable $F$ that was a deterministic function of $C$ , then $F$ is also de facto observed when $C$ is observed, and hence when $A$ and $B$ are observed. Thus, $F$ should also be introduced into $Z$ . Thus, we have to extend $Z$ iteratively to contain all the variables that are determined by it. This discussion suggests the simple procedure shown in algorithm 5.1. 
> 我们可以稍微修改我们的 d-seperation 算法，例如 $F$ 是 $C$ 的确定性函数时，在 $C$ 被观测到时，$F$ 也应该认为被观测到，因此需要被加入被观测到的变量集合 $\pmb Z$，那么 d-seperation 算法就可以在存在确定性函数的情况下也可以检测到变量之间的条件独立性了

This algorithm provides a procedural deﬁnition for *deterministic separation* of $X$ from $Y$ given $Z$ . This deﬁnition is sound, in the same sense that d-separation is sound. 
> 该算法提供了 $\pmb X, \pmb Y$ 在给定 $\pmb Z$ 的情况下的 deterministic seperation 的一个过程定义，这个定义是可靠的，和 d-seperation 的可靠性含义一样，也就是 deterministic seperated 的变量保证是条件独立的

**Theorem 5.1** 
Let $\mathcal{G}$ be a net k stru re, and let $D,X,Y,Z$ les. If $X$ is deterministica separated $Y$ en Z (as deﬁned by $\cdot_{\mathrm{SEP}}(\mathcal{G},D,X,Y,Z))$ ), then for all distributions P such that $P\vDash\mathcal{Z}_{\ell}(\mathcal{G})$ | I G and where, for each $X\in D$ ∈ , $P(X\mid\mathrm{Pa}_{X})$ | is a deterministic CPD, we have that $P\models(X\bot Y\mid Z)$ ⊥ | . 
> 定理：
> $\mathcal G$ 是图，$\pmb D, \pmb X, \pmb Y, \pmb Z$ 是变量集合，如果 $\pmb X$ 在给定 $\pmb Z$ 时是和 $\pmb Y$ determinstic seperated（记为 $\text{DET-SEP}(\mathcal G, \pmb D, \pmb X, \pmb Y, \pmb Z)$），
> 则对于所有满足 $P\vDash \mathcal I_{\mathscr l}(\mathcal G)$ ，并且满足对于所有 $X\in \pmb D$，$P (X\mid \text{Pa}_X)$ 是一个确定性的 CPD 的分布 $P$ (保证分布的 $\pmb Z^+$ 至少包含图的 $\pmb Z^+$ )，则我们有 $P\vDash (\pmb X \perp \pmb Y \mid \pmb Z)$

The proof is straightforward and is left as an exercise (exercise 5.1). 

Does this procedure capture all of the independencies implied by the deterministic functions? As with d-separation, the answer must be qualiﬁed: Given only the graph structure and the set  of deterministic CPDs, we cannot ﬁnd additional independencies. 
> 事实上，仅仅给定图结构和确定性 CPDs 的集合，我们是找不到额外的独立性的

**Theorem 5.2** 
Let $\mathcal{G}$ be a network structure, and let $D,X,Y,Z$ be sets of variables. I $\boldsymbol{\mathbf{\ell}}_{\mathrm{DFT-SEP}}(\mathcal{G},D,X,Y,Z)$ returns false, then there is a distribution P such that $P\,\vDash\mathcal{Z}_{\ell}(\mathcal{G})$ I G and where, for each $X\in D$ ∈ , $P(X\mid\mathrm{Pa}_{X})$ is deterministic CPD, but we have that $P\not\models(X\bot Y\mid Z)$ ⊥ | . 
> 定理：
> $\mathcal G$ 是图，$\pmb D, \pmb X, \pmb Y, \pmb Z$ 是变量集合，如果 $\text{DET-SEP}(\mathcal G, \pmb D, \pmb X, \pmb Y, \pmb Z)$ 返回 false（也就是给定 $\pmb Z$ 时，$\pmb X, \pmb Y$ 不是 determinstic seperated），则存在满足 $P\vDash \mathcal I_{\mathscr l}(\mathcal G)$ ，并且满足对于所有 $X\in \pmb D$，$P (X\mid \text{Pa}_X)$ 是一个确定性的 CPD 的分布 $P$，满足 $P\not\models (\pmb X \perp \pmb Y \mid \pmb Z)$ 

Of course, the det-sep procedure detects independencies that are derived purely from the fact that a variable is a deterministic function of its parents. However, particular deterministic functions can imply additional independencies. 
> 对于一个变量是其父变量的确定性函数这一事实，det-sep 算法可以检测到从中（朴素地）推演出的独立性，但一些特定的确定性函数会暗示额外的独立性，例如 XOR

Example 5.4 
Consider the network of ﬁgure 5.2, where $C$ is the exclusive or of $A$ and $B$ . What additional independencies do we have here? In the case of XOR (although not for all other deterministic functions), the values of $C$ and $B$ fully determine that of $A$ . Therefore, we have that $(D\perp E\mid$ $B,C)$ holds in the distribution. 
Speciﬁc deterministic functions can also induce other independencies, ones that are more reﬁned than the variable-level independencies discussed in chapter 3. 
> 特定的确定性函数还会引导出其他的比变量级独立性更加精细的独立性，例如 OR

Example 5.5 
Consider the Bayesian network of ﬁgure 5.1, but where we also know that the deterministic function at $C$ is an OR. Assume we are given the evidence $A=a^{1}$ . Because $C$ is an OR of its parents, we immediately know that $C=c^{1}$ , regardless of the value of $B$ . Thus, we can conclude that $B$ and $D$ are now independent: In other words, we have that 

$$
P(D\mid B,a^{1})=P(D\mid a^{1}).
$$ 
On the other hand, if we are given $A=a^{0}$ , the value of $C$ is not determined, and it does depend on the value of $B$ . Hence, the corresponding statement conditioned on $a^{0}$ is false. 

Thus, deterministic variables can induce a form of independence that is diferent from the standard notion on which we have focused so far. Up to now, we have restricted attention to independence properties of the form $(X\ \bot\ Y\ |\ Z)$ which represent the assumption that $P(X\mid Y,Z)=P(X\mid Z)$ for all values of X , Y $Y$ and Z . Deterministic functions can imply a type of independence that only holds for particular values of some variables. 
> 因此，确定性函数实际上可以引导出我们之前讨论的标准概念之外的独立性，我们一直讨论的是形式为 $P (X\perp Y \mid Z)$ 的独立性，其表示了 $P (X\mid Y, Z) = P (X\mid Z)$ 对于 $X, Y, Z$ 的所有值都是成立的
> 但确定性函数可以暗示这类独立性仅对一些变量的特定值成立

***Definition 5.1*** context-speciﬁc independence 
Let $X,Y,Z$ be pairwise disjoint sets of variables, let $C$ be a set of variables (that might overlap with $X\cup Y\cup Z)$ , and let $c\in\mathit{V a l}(C)$ . We say that $X$ and $Y$ are contextually independent given Z and the context $^c$ denoted $(X\perp_{c}Y\mid Z,c)$ , if 
> 定义：
> 令 $\pmb X, \pmb Y, \pmb Z$ 是成对的不相交变量集合，令 $\pmb C$ 是变量集合（可以和 $\pmb X\cup \pmb Y\cup \pmb Z$ 相交），令 $c \in Val (\pmb C)$，如果

$$
P(\pmb X\mid \pmb Y,\pmb Z,c) = P(\pmb X \mid \pmb Z,c)\;\text{whenever}\;P(\pmb Y, \pmb Z, c)>0
$$

> 则我们称 $\pmb X, \pmb Y$ 在给定 $\pmb Z$ 和上下文 $c$ 时，contextually independent，记作 $(\pmb X \perp_c \pmb Y\mid \pmb Z, c)$

Independence statements of this form are called context-speciﬁc independencies (CSI). They arise in many forms in the context of deterministic dependencies. 
> 这种类型的独立性声明称为针对上下文的独立性 CSI，它们在确定性函数的上下文中会以多种形式出现

Example 5.6 
As we saw in example 5.5, we can have that some value of one parent $A$ can be enough to determine the value of the child $C$ . Thus, we have that $(C\perp_{c}B\mid a^{1})$ , and hence also that $(D\perp_{c}B\mid a^{1})$ . We can make additional conclusions if we use properties of the OR function. For example, if we know that $C=c^{0}$ , we can conclude that both $A=a^{0}$ and $B=b^{0}$ . Thus, in particular, we can conc th that $(A\ \bot_{c}\ B\ |\ c^{0})$ at $(D\perp_{c}E\mid c^{0})$ . Similarly, if we know that $C=c^{1}$ and $B=b^{0}$ , we can conclude that $A=a^{1}$ , and hence we have that $(D\perp_{c}E\mid b^{0},c^{1})$ . 

It is important to note that **context-speciﬁc independencies can also arise when we have tabular CPDs. However, in the case of tabular CPDs, the independencies would only become apparent if we examine the network parameters. By making the structure of the CPD explicit, we can use qualitative arguments to deduce these independencies.** 
> 但注意针对上下文的独立性在表格式 CPDs 中也会出现，并且仅会在我们检查网络参数时才会 become apparent，而通过让 CPD 的结构清晰，我们就可以用定性的论证来推导这些独立性
## 5.3 Context-Specific CPDs 
### 5.3.1 Representation 
Structure in CPDs does not arise only in the case of deterministic dependencies. A very common type of regularity arises when we have precisely the same efect in several contexts. 
> CPDs 中的结构不仅仅在确定性依赖中出现
> 当我们在多个上下文中有完全相同的影响时，一个非常常见的 regularity 类型就会出现

![[Probabilistic Graph Theory-Fig5.3.png]]

Example 5.7 
We augment our Student example to model the event that the student will be oered a job at Acme Consulting. Thus, we have a binary-valued variable J, whose value is j1 if the student is oered this job, and j0 otherwise. The probability of this event depends on the student’s SAT scores and the strength of his recommendation letter. We also have to represent the fact that our student might choose not to apply for a job at Acme Consulting. Thus, we have a binary variable Applied, whose value $(a^{1}$ or $a^{0}$ ) indicates whether the student applied or not. The structure of the augmented network is shown in ﬁgure 5.3. 

Now, we need to describe the CPD $P(J\mid A,S,L)$ . In our domain, even if the student does not apply, there is still a chance that Acme Consulting is sufciently desperate for employees to ofer him a job anyway. (This phenomenon was quite common during the days of the Internet Gold Rush.) In this case, however, the recruiter has no access to the student’s SAT scores or recommendation letters, and therefore the decision to make an ofer cannot depend on these variables. Thus, among the 8 values of the parents $A,S,L,$ , the four that have $A=a^{0}$ must induce an identical distribution over the variable $J$ . 

We can elaborate this model even further. Assume that our recruiter, knowing that SAT scores are a far more reliable indicator of the student’s intelligence than a recommendation letter, ﬁrst considers the SAT score. If it is high, he generates an ofer immediately. (As we said, Acme Consulting is somewhat desperate for employees.) If, on the other hand, the SAT score is low, he goes to the efort of obtaining the professor’s letter of recommendation, and makes his decision accordingly. In this case, we have yet more regularity in the CPD: $P(J\mid a^{1},s^{1},l^{1})=P(J\mid a^{1},s^{1},l^{0})$ . 

In this simple example, we have a CPD in which several values of $\mathrm{Pa}_{J}$ specify the same conditional probability over $J$ . In general, we often have CPDs where, for certain partial assign- ments $\mathbfit{u}$ to subsets $U\subset\mathrm{Pa}_{X}$ , the values of the remaining parents are not relevant. In such cases, several diferent distributions $P(X\mid\mathrm{{pa}}_{X})$ are identical. In this section, we discuss how we might capture this regularity in our CPD representation and what implications this structure has on conditional independence. There are many possible approaches for capturing functions over a scope $X$ that are constant over certain subsets of instantiations to $X$ . In this section, we present two common and useful choices: trees and rules. 
> 本例中，$\text{Pa}_J$ 的多个赋值都指向了在 $J$ 上的相同的条件概率分布
> 一般情况下，对于 $U\subset \text{Pa}_j$ 的部分赋值 $\pmb u$ 可以让 $\text{Pa}_J$ 中剩余的父变量的值和 CPDs 的形式无关，此时，多个分布 $P (X\mid \text{Pa}_X)$ 实质上是完全相同的
> 本节讨论如何捕获这类 regularity
> 在集合 $\pmb X$ 上捕获对于 $\pmb X$ 的特定子集实例保持为常数的函数有两种通用的方法：树和规则
#### 5.3.1.1 Tree-CPDs 
A very natural representation for capturing common elements in a CPD is via a tree , where the leaves of the tree represent diferent possible (conditional) distributions over $J$ , and where the path to each leaf dictates the contexts in which this distribution is used. 
> 在 CPDs 捕获共同元素的非常自然的一个方法就是树
> 树中的叶子表示 $J$ 上不同的可能条件概率分布，而到每一个叶子的路径表示分布被使用时所在的上下文

![[Probabilistic Graph Theory-Fig5.4.png]]

Example 5.8
Figure 5.4 shows a tree for the CPD of the variable $J$ in example 5.7. Given this tree, we ﬁnd $P(J\mid A,S,L)$ by traversing the tree from the root downward. At each internal node, we see a test on one of the attributes. For example, in the root node of our tree we see a test on the value $A$ . We then follow the arc that is labeled with the value $a$ , which is given in the current setting of th s. Assume, for example, that we are interested in $P(J\mid a^{1},s^{1},l^{0})$ . Thus, w have that $A\,=\,a^{1}$ , and we would follow the right-hand arc labeled a $a^{1}$ . The next test is over S . We have $S=s^{1}$ , and we would also follow the right-hand arc. We have now reached a leaf, which is annotated with $a$ particular distribution over $J$ : $P(j^{1})=0.9$ , and $P(j^{0})=0.1$ . This distribution is the one we use for $P(J\mid a^{1},s^{1},l^{0})$ . 
> Fig5.4展示了 $J$ 的 CPD 的一个树表示，给定这个树表示，我们通过从根向下的遍历得到一个 $P (J\mid A, S, L)$
> 在每一个内部节点，我们对一个属性进行测试

Formally, we use the following recursive deﬁnition of trees. 

**Deﬁnition 5.2** tree-CPD 
A tree-CPD representing a CPD for variable $X$ is a rooted tree; each t-node in the tree is either $^a$ leaf t-node or an interior t-node . Each leaf is labeled with a distribution $P(X)$ . Each interior t-node is labeled with some variable $Z\in\mathrm{Pa}_{X}$ . Each interior t-node set of outgoing arcs to its children, ch one associated with a unique variable assignment $Z=z_{i}$ for $z_{i}\in V a l(Z)$ . A branch $\beta$ through a tree-CPD is a path beginning at the root and proceeding to a leaf node. We assume that no branch contains two interior nodes labeled by the same variable. The parent context induced by branch $\beta$ is the set of variable assignments $Z=z_{i}$ encountered on the arcs along the branch. 
> 定义：
> 一个用于表示变量 $X$ 的 CPD 的 tree-CPD 是一个有根的树，树中的每个 t-node 可以是叶子 t-node 或者是内部 t-node
> 树的每一个叶子都被一个 $P (X)$ 分布 labeled，树的每一个内部节点都被 $Z\in \text{Pa}_X$ 的某一个变量 labeled
> 每一个内部节点都有一系列向外的通向子节点的边，每个边都和一个独立的变量赋值 $Z = z_i\; \text{for}\; z_i \in Val (Z)$
>
> tree-CPD 的一个分支 $\beta$ 是一个从根节点开始，到叶节点结束的路径，我们假设没有分支包含两个由相同的变量 label 的内部节点
> 由分支 $\beta$ 导出的 parent context 是一个包含了分支中路径经过的边的变量赋值 $Z=z_i$ 的集合

Note that, to avoid confusion, we use t-nodes and arcs for a tree-CPD, as opposed to our use of nodes and edges as the terminology in a BN. 

Example 5.9 
Consider again the tree in ﬁgure 5.4. There are four branches in this tree. One induces the parent context $\langle a^{0}\rangle$ , corresponding to the situation where the student did not apply for the job. A second induces the parent context $\langle a^{1},s^{1}\rangle$ , corresponding to an application with a gh . The two branches induce complete assignments to all the parents of J : ⟨ $\langle a^{1},s^{0},l^{1}\rangle$ ⟩ and $\langle a^{1},s^{0},l^{0}\rangle$ ⟨ ⟩ . Thus, this representation breaks down the conditional distribution of J given its parents into four parent contexts by grouping the possible assignments in $V a l(\mathrm{Pa}_{J})$ into subsets that have the same efect on $J$ . Note that now we need only 4 parameters to describe the behavior of $J$ , instead of 8 in the table representation. 
> Fig5.4一共有4个 branch，每个 branch 导出/代表了一个 parent context
> 树表示通过将 $Val(\text{Pa}_j$) 中的可能赋值根据对 $J$ 的相同影响划分为了多个子集，以此将 $J$ 在给定它的父变量时的条件概率分布分解为了4个 parent context
> 此时我们只需要4个参数就可以表示 $J$ 的所有条件概率分布，而不是8个参数

Regularities of this type occur in many domains. Some events can occur only in certain situations. For example, we can have a Wet variable, denoting whether we get wet; that variable would depend on the Raining variable, but only in the context where we are outside. Another type of example arises in cases where we have a sensor, for example, a thermometer; in general, the thermometer depends on the temperature, but not if it is broken. 

This type of regularity is very common in cases where a variable can depend on one of a large set of variables: it depends only on one, but we have uncertainty about the choice of variable on which it depends. 
> 这种类型的 regularity 在许多领域都会出现
> 这种类型的 regularity 在一个变量依赖于许多个变量的情况中非常常见：该变量仅依赖于其中的一个变量，但我们不知道是哪一个

Example 5.10  

![[Probabilistic Graph Theory-Fig5.5.png]]


Let us revisit example 3.7, where George had to decide whether to give the recruiter at Acme Consulting the letter from his professor in Computer Science 101 or his professor in Computer Science 102. George’s chances of getting a job can depend on the quality of both letters L1 and L2, and hence both are parents. However, depending on which choice $C$ George makes, the dependence will only be on one of the two. Figure 5.5a shows the network fragment, and $b$ shows the tree-CPD for the variable $J$ . (For simplicity, we have eliminated the dependence on $S$ and $A$ that we had in ﬁgure 5.4.) 

More formally, we deﬁne the following: 

**Deﬁnition 5.3** multiplexer CPD 
A CPD $P(Y\mid A,Z_{1},.\,.\,.\,,Z_{k})$ is said to be $a$ multiplexer CPD if $V a l(A)=\{1,.\,.\,.\,,k\}$ , and 

$$
P(Y\mid a,Z_{1},.\,.\,.\,,Z_{k})=\pmb 1\{Y=Z_{a}\},
$$ 
where $a$ is the value of $A$ . The variable $A$ is called the selector variable for the CPD. 
> 定义：
> 一个 CPD $P (Y\mid A, Z_1,\dots, Z_k)$ 如果满足 $Val (A) = \{1,\dots, k\}$，并且

$$
P(Y\mid a, Z_1,\dots, Z_k) = \pmb 1\{Y = Z_a\}
$$

> 其中 $a$ 是 $A$ 的一个值，则称该 CPD 为 multiplexer CPD，变量 $A$ 称为 CPD 的选择器变量 (条件概率分布 $P (Y\mid a, Z_1,\dots, Z_k)$ 中不存在不确定性，随机变量 $Y$ 取值为 $Z_a$ 的概率是1，其他都为0)

In other words, the value of the multiplexer variable is a copy of the value of one of its parents, $Z_{1},\ldots,Z_{k}$ . The role of $A$ is to select the parent who is being copied. Thus, we can think of a multiplexer CPD as a switch. 
> 换句话说，一个 multiplexer 变量的值是其中一个它的父变量 $Z_1,\dots, Z_k$ 的值的拷贝，$A$ 的作用是选择出需要拷贝的是哪一个父变量，因此，我们可以将 multiplerxer CPD 看作是一个开关

We can apply this deﬁnition to example 5.10 by introducing a new variable $L_{i}$ , which is a multiplexer of $L_{1}$ and $L_{2}$ , using $C$ as the selector. The variable $J$ now depends directly only on $L$ . The modiﬁed network is shown in ﬁgure 5.5c. 
> 我们对于 example 5.10 应用该定义，引入一个 multiplexer 变量 $L$，$C$ 作为它的选择器，此时变量 $J$ 直接依赖于 $L$

This type of model arises in many settings. For example, it can arise when we have diferent actions in our model; it is often the case that the set of parents for a variables varies considerably based on the action taken. Conﬁguration variables also result in such situations: depending on the speciﬁc conﬁguration of a physical system, the interactions between variables might difer (see box 5.A). 
> 这类模型在许多设定下会出现，一般地来说，就是取决于特定的配置/动作，变量之间的交互关系会变化

Another setting where this type of model is particularly useful is in dealing with uncertainty about correspondence between diferent objects. This problem arises, for example, in data association , where we obtain sensor measurements about real-world objects, but we are uncertain about which object gave rise to which sensor measurement. For example, we might get a blip on a radar screen without knowing which of of several airplanes the source of the signal. Such cases also arise in robotics (see box 15.A and box 19.D). Similar situations also arise in other applications, such as identity resolution : associating names mentioned in text to the real- correspondence variable world objects to which they refer (see box 6.D). We can model this type of situation using a correspondence variable $U$ that associates, with each sensor measurement, the identity $u$ of the object that gave rise to the measurement. The actual sensor measurement is then deﬁned using a multiplexer CPD that depends on the correspondence variable $U$ (which plays the role of the selector variable), and on the value of $A(u)$ for all $u$ from which the measurement could have been derived. The value of the measurement will be the value of $A(u)$ for $U=u$ , usually with some added noise due to measurement error. Box 12.D describes this problem in more detail and presents algorithms for dealing with the difcult inference problem it entails. 
> 这类模型的另一个设定在处理不同对象之间的对应性 correspondence 的不确定性时非常有用
> 这类问题可以出现在例如数据关联 data association 的场景中、标识识别 identity resolution 的场景中
> 对于这类场景，我们可以使用一个 correspondence 变量 $U$ 表示导致观察到的现象的父变量（原因），然后定义一个 multiplexer CPD，以 $U$ 作为选择器变量

Trees provide a very natural framework for representing context-speciﬁcity in the CPD. In particular, it turns out that people ﬁnd it very convenient to represent this type of structure using trees. Furthermore, the tree representation lends itself very well to automated learning algorithms that construct a tree automatically from a data set. 
> 树提供了一个表示 CPD 中的 contexte-specificity 的一个很自然的框架

Box 5.A — Case Study: Context-Speciﬁcity in Diagnostic Networks. A common setting where context-speciﬁc CPDs arise is in troubleshooting of physical systems, as described, for example, by Heckerman, Breese, and Rommelse (1995). In such networks, the context speciﬁcity is due to the presence of alternative conﬁgurations. For example, consider a network for diagnosis of faults in $^a$ printer, developed as part of a suite of troubleshooting networks for Microsoft’s Windows $95^{T M}$ op- erating system. This network, shown in ﬁgure 5.A.1a, models the fact that the printer can be hooked up either to the network via an Ethernet cable or to a local computer via a cable, and therefore depends on both the status of the local transport medium and the network transport medium. However, the status of the Ethernet cable only afects the printer’s output if the printer is hooked up to the network. The tree-CPD for the variable Printer-Output is shown in ﬁgure 5.A.1b. Even in this very simple network, this use of local structure in the CPD reduced the number of parameters required from 145 to 55. 

We return to the topic of Bayesian networks for troubleshooting in box 21.C and box 23.C. 
#### 5.3.1.2 Rule CPDs 
As we seen, trees are appealing for several reasons. However, trees are a global representation that captures the entire CPD in a single data structure. In many cases, it is easier to reason using a CPD if we break down the dependency structure into ﬁner-grained elements. A ﬁner- grained representation of context-speciﬁc dependencies is via rules . Roughly speaking, each rule corresponds to a single entry in the CPD of the variable. It speciﬁes a context in which the CPD entry applies and its numerical value. 
> 树是一个在单个数据结构内捕获整个 CPD 的全局表示
> 在许多情况下，我们需要将 CPD 的依赖结构分解为更细粒度的元素，而要对 context-specific 的依赖进行更细粒度的表示，就需要借助规则
> 每个规则都对应于变量的 CPD 中的单个 entry，它指定了 CPD 的上下文，以及它的数值

**Deﬁnition 5.4** rule scope 
$A$ rule $\rho$ i $a$ pair $\langle c;p\rangle$ where $^c$ is an assignment to some subset of variables $C$ , and $p\in[0,1]$ . We deﬁne C to be the scope of $\rho,$ , denoted Scope [ ρ ] . 
> 定义：
> 规则 $\rho$ 定义为一个 pair $\langle \pmb c; p\rangle$，其中 $\pmb c$ 是一个对某个变量子集 $\pmb C$ 的一个赋值，$p\in [0,1]$ ，我们定义 $\pmb C$ 是规则 $\rho$ 的作用域，记为 $\text{Scope}[\rho]$

This representation decomposes a tree-CPD into its most basic elements. 

Example 5.11 
Consider the tree of ﬁgure 5.4. There are eight entries in the CPD tree, such that each one corresponds to a branch in the tree and an assignment to the variable $J$ itself. Thus, the CPD deﬁnes eight rules: 

$$
\left\{\begin{array}{l l}{\rho_{1}\colon\langle a^{0},j^{0};0,8\rangle}\\ {\rho_{2}\colon\langle a^{0},j^{1};0,2\rangle}\\ {\rho_{3}\colon\langle a^{1},s^{0},j^{0};0,9\rangle}\\ {\rho_{4}\colon\langle a^{1},s^{0},l^{0},j^{1};0,1\rangle}\\ {\rho_{5}\colon\langle a^{1},s^{0},l^{1},j^{0};0,4\rangle}\\ {\rho_{6}\colon\langle a^{1},s^{0},l^{1},j^{1};0,6\rangle}\\ {\rho_{7}\colon\langle a^{1},s^{1},j^{0};0,1\rangle}\\ {\rho_{8}\colon\langle a^{1},s^{1},j^{1};0,9\rangle}\end{array}\right\}
$$ 

For example, the rule $\rho_{4}$ is derived by following the branch $a^{1},s^{0},l^{0}$ and then selecting the probability associated with the assignment $J=j^{1}$ . 

Although we can decompose any tree-CPD into its constituent rules, we wish to deﬁne rule- based CPDs as an independent notion. To deﬁne a coherent CPD from a set of rules, we need to make sure that each conditional distribution of the form $P(X\mid\mathrm{{pa}}_{X})$ is speciﬁed by precisely one rule. Thus, the rules in a CPD must be mutually exclusive and exhaustive. 
> 我们在上个例子中展示了我们可以将 tree-CPD 分解为组成它的多个规则
> 我们希望直接定义 rule-based CPD，为了从一个规则集合中定义 CPD，我们需要保证每个形式为 $P (X\mid \text{pa}_X)$ 都可以刚刚好被一个规则指定，因此，CPD 中的规则需要是互斥且完备的

**Definition 5.5**  rule-based CPD 
A rule-based CPD $P(X\mid\mathrm{Pa}_{X})$ is a set of rules $\mathcal{R}$ such that: 

• For each rule $\rho\in\mathcal R$ , we have that $Scope [ ρ |\subseteq\{X\}\cup\mathrm{Pa}_{X}$ . 
• For each assignment $(x,u)$ to $\{X\}\cup\mathrm{Pa}_{X}$ , we have precisely one rule $\langle c;p\rangle\in{\mathcal{R}}$ such that c is compatible with $(x,u)$ . In this case, we say that $P(X=x\mid\mathrm{Pa}_{X}={\pmb u})=p.$ . 
• The resulting CPD $P(X\mid U)$ is a legal CPD, in that $\sum_{x}P(x\mid u)=1.$ 

> 一个基于规则的 CPD $P (X\mid \text{Pa}_X)$ 是一个规则集合 $\mathcal R$，满足：
> - 对于每个规则 $\rho \in \mathcal R$，我们有 $\text{Scpoe}[\rho]\subseteq \{X\}\cup \text{Pa}_X$
> - 对于每个对 $\{X\}\cup \text{Pa}_X$ 的赋值 $(x, u)$，我们会恰好有一个规则 $\langle \pmb c, p\rangle \in \mathcal R$ ，满足 $\pmb c$ 和 $(x, \pmb u)$ 一致，此时，我们称 $P (X = x\mid \text{Pa}_X = \pmb u) = p$
> - 得到的 CPD $P (X\mid \pmb U)$ 满足 $\sum_x P (x\mid \pmb u) = 1$ 

The rule set in example 5.11 satisﬁes these conditions. Consider the following, more complex, example. 

Example 5.12 
Let $X$ be a variable with $\mathrm{Pa}_{X}=\{A,B,C\}$ , and assume that X ’s CPD is deﬁned via the following set of rules: 

This set of rules deﬁnes the following CPD: 

For example, the CPD entry $P(x^{0}\mid a^{0},b^{1},c^{1})$ is determined by the rule $\rho_{3}$ the CPD entry 0 . 2 ; we can verify that no other rule is compatible with the context $a^{0},b^{1},c^{1},x^{1}$ . We can also verify that each of the CPD entries is also compatible with precisely one context, and hence that the diferent contexts are mutually exclusive and exhaustive. 

Note that both CPD entries $P(x^{1}\mid a^{0},b^{1},c^{0})$ and $P(x^{0}\mid a^{0},b^{1},c^{0})$ are determined by $^a$ single rule $\rho_{9}$ . As the probabilities for the diferent contexts in this case must sum up to 1, this phenomenon is only possible when the rule deﬁnes a uniform distribution, as it does in this case. 

This perspective views rules as a decomposition of a CPD. We can also view a rule as a ﬁner-grained factorization of an entire distribution. 

**Proposition 5.1**
Let $\mathcal{B}$ be a Bayesian ne ork, and assume that each CPD $P(X\mid\mathrm{Pa}_{X})$ n $\mathcal{B}$ is represented as a set of rules $\mathcal{R}_{X}$ . Let R be the multiset deﬁned as $\uplus_{X\in\mathcal{X}}\mathcal{R}_{X}$ , where ⊎ denotes multiset join, which puts together all of the rule instances (including duplicates). Then, the probability of any instantiation $\xi$ to the network variables $\mathcal{X}$ can be computed as 

$$
P(\xi)=\prod_{\langle \pmb c;p\rangle\in{\mathcal R},\xi\sim \pmb c}p.
$$ 
The proof is left as an exercise (exercise 5.3). 
> 定理：
> 令 $\mathcal B$ 表示一个贝叶斯网络，假设 $\mathcal B$ 中的每一个 CPD 都可以被一个规则集合 $\mathcal R_X$ 表示
> 令 $\mathcal R$ 表示定义为 $\uplus_{X\in\mathcal{X}}\mathcal{R}_{X}$ 的多重集合，其中 $\uplus$ 表示多重集合的并集运算，因此 $\mathcal R$ 就是将所有的规则都放在一起的多重集合（包括重复）
> 则对于网络变量 $\mathcal X$ 的任意实例化 $\xi$ 的概率可以计算为 $P (\xi)=\prod_{\langle c; p\rangle\in{\mathcal R},\xi\sim c}p.$

The rule representation is more than a simple transformation of tree-CPDs. In particular, although every tree-CPDs can be represented compactly as a set of rules, the converse does not necessarily hold: not every rule-based CPD can be represented compactly as a tree. 
> 规则表示并不是 tree-CPDs 的简单转换，特别地，即便每个 tree-CPDs 都可以被表示为一个紧凑的规则集合，但不是每个规则集合都可以被表示为一个紧凑的 tree-CPD

Example 5.13 
Consider the rule-based CPD of example 5.12. In any rule set that is derived from a tree, one variable — the one at the root — appears in all rules. In the rule set $\mathcal{R}$ , none of the parent variables $A,B,C$ appears in all rules, and hence the rule set is not derived from a tree. If we try to represent it as a tree-CPD, we would have to select one of $A$ , $B$ , or $C$ to be the root. Say, for example, that we select A to be the root. In this case, rules that do not contain $A$ would necessarily correspond to more than one branch (one for $a^{1}$ and one for $a^{0}$ ). Thus, the transformation would result in more branches than rules. For example, ﬁgure 5.6 shows a minimal tree-CPD that represents the rule-based CPD of example 5.12. 
#### 5.3.1.3 Other Representations 
The tree and rule representations provide two possibilities for representing context-speciﬁc structure. We have focused on these two approaches as they have been demonstrated to be useful for representation, for inference, or for learning. However, other representations are also possible, and can also be used for these tasks. In general, if we abstract away from the details of these representations, we see that they both simply induce partitions of $V a l(\{X\}\cup\mathrm{Pa}_{X})$ , deﬁned by the branches in the tree on one hand or the rule contexts on the other. Each partition is associated with a diferent entry in $X$ ’s CPD. 
> 如果我们对这些表示进行抽象，我们可以认为这些表示都是简单地引入了对 $Val (\{X\}\cup \text{Pa}_X$) 的划分，在树中，它被定义为分支，在规则集中，它被定义为规则，每一个划分都对应于 $X$ 的 CPD 的一个 entry

This perspective allows us to understand the strengths and limitations of the diferent rep- resentations. In both trees and rules, all the partitions are described via an assignment to a subset of the variables. Thus, for example, we cannot represent the partition that contains only $a^{1},s^{1},l^{0}$ and $a^{1},s^{0},l^{1}$ , a partition that we might obtain if the recruiter lumped together candidates that had a high SAT score or a strong recommendation letter, but not both. As deﬁned, these representations also require that we either split on a variable (within a branch of the tree or within a rule) or ignore it entirely. In particular, this restriction does not allow us to capture dependencies that utilize a taxonomic hierarchy on some parent attribute, as described in box 5.B 
> 在树和规则中，划分都是通过对所有变量的一个子集的赋值得到的
> 定义上，这类表示要求我们要么在一个变量上做出划分 split，要么完全忽略它，这个限制导致我们不能捕获在一些 parent attribute 中使用了 taxonomic 层次的依赖

Of course, we can still represent distributions with these properties by simply having multiple tree branches or multiple rules that are associated with the same parameter iz ation. However, this solution both is less compact and fails to capture some aspects of the structure of the CPD. A very ﬂexible representation, that allows these structures, might use general logical formulas to describe partitions. This representation is very ﬂexible, and it can precisely capture any partition we might consider; however, the formulas might get fairly complex. Somewhat more restrictive is the use of a decision diagram , which allows diferent t-nodes in a tree to share children, avoiding duplication of subtrees where possible. This representation is more general than trees, in that any structure that can be represented compactly as a tree can be represented compactly as a decision diagram, but the converse does not hold. Decision diagrams are incomparable to rules, in that there are examples where each is more compact than the other. In general, diferent representations ofer diferent trade-ofs and might be appropriate for diferent applications. 
> 但是对于具有这些性质的分布，我们当然也可以用具有多个有相同参数的分支的树或者规则来表示，但显然不够 compact，并且不能捕获 CPD 中的一些结构

Box 5.B — Concept: Multinets and Similarity Networks. The multinet representation provides a more global approach to capturing context-speciﬁc independence. In its simple form, a multinet is a network centered on a single distinguished class variable $C$ , which is a root of the network. The multinet deﬁnes a separate network $\mathcal{B}_{c}$ for each value of $C$ , where the structure as well as the parameters can difer for these diferent networks. In most cases, a multinet deﬁnes a single network where every variable $X$ has as its parents $C$ , and all variables $Y$ in any of the networks $\mathcal{B}_{c}$ . However, the CPD of $X$ is such th in context $C\,=\,c_{i}$ , it depends y on $\dot{\mathrm{Pa}}_{X}^{\mathcal{B}_{c}}$ . In some cases, however, a subtlety arises, where Y $Y$ is a parent of X in $\mathcal{B}_{c^{1}}$ , and X is a parent of $Y$ in $\mathcal{B}_{c^{2}}$ . In this case, the Bayesian network induced by the multinet is cyclic; nevertheless, because of the context-speciﬁc independence properties of this network, it speciﬁes a coherent distribution. (See also exercise 5.2.) Although, in most cases, a multinet can be represented as a standard BN with context-speciﬁc CPDs, it is nevertheless useful, since it explicitly shows the independencies in $^a$ graphical form, making them easier to understand and elicit. 

A related representation, the similarity network , was developed as part of the Pathﬁnder system (see box $3.D$ ). In a similarity network, we deﬁne a network $\mathcal{B}_{S}$ for certain subsets of values $S\subset V a l(C)$ , which contains only those attributes re ant for distinguishing between the values in S . The underlying assumption is th variable X d not appear in e network $\mathcal{B}_{S}$ , then $P(X\mid C=c)$ is the same for all c $c\in S$ ∈ . More er, if X not ha $Y$ as a parent in this network, then $X$ is contextually independent of Y given C $C\,\in\,S$ ∈ and $X\mathit{\dot{s}}$ ’s ot pa nts in this network. A similarity network easily captures structure where the dependence of X on C is deﬁned in terms of a taxonomic hierarchy on $C$ . For example, we might have that our class variable is Disease. While Sore-throat depends on Disease, it does not have a diferent conditional distribution for every value $d$ of Disease. For example, we might partition diseases into diseases that do not cause sore throat and those that do, and the latter might be further split into difuse disease (causing soreness throughout the throat) and localized diseases (such as abscesses). Using this partition, we might have only three diferent conditional distributions for $P$ ( Sore-Throat $\mid D i s e a s e=d_{\mid}$ ) . Multinets facilitate elicitation both by focusing the expert’s attention on attributes that matter, and by reducing the number of distinct probabilities that must be elicited. 
### 5.3.2 Independencies 
In many of our preceding examples, we used phrases such as “In the case $a^{0}$ , where the student does not apply, the recruiter’s decision cannot depend on the variables $S$ and $L$ .” These phrases suggest that context-speciﬁc CPDs induce context-speciﬁc independence. In this section, we analyze the independencies induced by context-speciﬁc dependency models. 
> 针对上下文的 CPDs 会引入针对上下文的独立性，本节对其进行讨论

Consider a CPD $P(X\mid\mathrm{Pa}_{X})$ , where certain distributions over $X$ are shared across diferent instantiations of $\mathrm{Pa}_{X}$ . The structure of such a CPD allows us to infer certain independencies *locally* without having to consider any global aspects of the network. 
> 考虑一个 CPD $P (X\mid \text{Pa}_X)$，其中在 $X$ 上的特定分布被多个不同的 $\text{Pa}_X$ 实例共享
> 这类 CPD 的结构允许我们在不需要考虑网络全局的情况下局部地推理出特定独立性

Example 5.14 
Returning to example 5.7, we can see tha $(J\ \perp_{c}\ S,L\ \vert\ \ a^{0})$ : By the deﬁnition of the CPD, $P(J\mid a^{0},s,l)$ is the same for all values of s and l . Note that this equality holds regardless of the structure or the parameters of the network in which this CPD is embedded. Similarly, we have that $\left(J\perp_{c}L\mid a^{1},s^{1}\right)$ . 

In general, if we deﬁne $c$ to be the context associated with a branch in the tree-CPD for $X$ , then $X$ is independent of the remaining parents $(\mathrm{Pa}_{X}-S c o p e[c])$ given the context $c$ . However, there might be additional CSI statements that we can determine locally, conditioned on contexts that are not induced by complete branches. 
> 一般地，如果我们将 $\pmb c$ 定义为和 $X$ 的 tree-CPD 的一个分支相关的上下文，则 $X$ 在给定上下文 $\pmb c$ 的情况下，就和其余的 parents: $(\text{Pa}_X - Scope[\pmb c])$ 独立
> 但也存在我们根据局部的上下文就可以决定的 CSI statement，也就是不需要条件于完整的分支导出的上下文

Example 5.15 
Consider, the tree-CPD of ﬁgure $5.5b$ . Here, once George chooses to request a letter from one professor, his job prospects still depend on the quality of that professor’s letter, but not on that of the other. More precisely, we have that $(J\perp_{c}L_{2}\mid c^{1})$ ; note that $c^{1}$ is not the full assignment associated with a branch. 

Example 5.16 
More interestingly, consider again the tree of figure 5.4, and suppose we are given the context s1. Clearly, we should only consider branches that are consistent with this value. There are two such branches. One associated with the assignment $a^{0}$ and the other with the assignment $a^{1},s^{1}$ . We can immediately see that the choice between these two branches does not depend on the value of $L$ . Thus, we conclude that $(J\perp_{c}L\mid s^{1})$ holds in this case. 

We can generalize this line of reasoning by considering the rules compatible with a particular context $^c$ . Intuitively, if none of these rules mentions a particular parent $Y$ of $X$ , then $X$ is conditionally independent of $Y$ given $^c$ . More generally, we can deﬁne the notion of conditioning a rule on a context: 
> 考虑和某个特定的上下文 $\pmb c$ 相兼容的规则，直观地，如果其中没有规则提到 $X$ 的一个特定的 $Y$，则 $X$ 在给定 $\pmb c$ 的情况下条件独立于 $Y$

**Deﬁnition 5.6**  reduced rule 
Let $\rho\;=\;\left\langle c^{\prime};p\right\rangle$ be a rule and $C\,=\,c$ be a context. If $c^{\prime}$ is compatible ith $c$ , we say that $\rho\sim c$ . In this case, let $c^{\prime\prime}=c^{\prime}\langle S c o p e[c^{\prime}]-S c o p e[c]\rangle$ be the assignment in $c^{\prime}$ to the variables in $S c o p e[c^{\prime}]\mathrm{~-~}S c o p e[c]$ . We then deﬁne the reduced rule $\rho[{\pmb c}]\,=\,\langle{\pmb c}^{\prime\prime};p\rangle$ . For R a set of rules, we deﬁne the reduced rule set 

$$
{\mathcal{R}}[{\pmb{c}}]=\{\rho[{\pmb{c}}]\ :\ \rho\in{\mathcal{R}},\rho\sim{\pmb{c}}\}.
$$ 

> 定义：
> 令 $\rho = \langle \pmb c'; p\rangle$ 为一个规则，$\pmb C = \pmb c$ 为上下文，如果 $\pmb c'$ 和 $\pmb c$ 兼容，则我们说 $\rho \sim \pmb c$，此时，令 $\pmb c'' = \pmb c'\langle \text{Scope}[\pmb c']-\text{Scpoe}[\pmb c]\rangle$，也就是 $\pmb c'$ 对于处于 $\text{Scope}[\pmb c']-\text{Scope}[\pmb c]$ 中的变量的赋值
> 对于规则集合 $\mathcal R$，我们定义简化的规则 $\rho[\pmb c]=\langle \pmb c''; p\rangle$ 集合如上

Example 5.17 
In the rule set $\mathcal{R}$ of example 5.12, $\mathcal{R}[a^{1}]$ is the set 

$$
\begin{array}{r l}{\rho_{1}^{\prime}\colon\langle b^{1},x^{0};0.1\rangle}&{\qquad\rho_{2}\colon\langle b^{1},x^{1};0.9\rangle}\\ {\rho_{5}\colon\langle b^{0},c^{0},x^{0};0.3\rangle}&{\qquad\rho_{6}\colon\langle b^{0},c^{0},x^{1};0.7\rangle}\\ {\rho_{7}^{\prime}\colon\langle b^{0},c^{1},x^{0};0.4\rangle}&{\qquad\rho_{8}^{\prime}\colon\langle b^{0},c^{1},x^{1};0.6\rangle.}\end{array}
$$ 

Thus, we have left only the rules compatible with $a^{1}$ , and eliminated $a^{1}$ from the context in the rules where it appeared. 
> 我们从全部规则集合中过滤出仅和 $a^1$ 兼容的规则，并且将 $a^1$ 从过滤出的规则中的上下文中去除

**Proposition 5.2** 
Let $\mathcal{R}$ be the rul s in t ed CPD for a variable $X$ , and t $\mathcal{R}_{c}$ be the rules in $\mathcal{R}$ that are compatible with c . Let $Y\subseteq\operatorname{Pa}_{X}$ ⊆ be some subset of parents of X such that $Y\cap S c o p e[c]=\emptyset$ . If for every $\rho\in\mathcal{R}[c]$ , we have that $Y\cap S c o p e[\rho]=\emptyset$ , then $(X\perp_{c}Y\mid\mathrm{Pa}_{X}-Y,c)$ . 
> 引理：
> 令 $\mathcal R$ 是变量 $X$ 的 rule-based CPD 的规则集合，令 $\mathcal R_{\pmb c}$ 表示其中和 $\pmb c$ 兼容的规则，令 $Y\subseteq \text{Pa}_X$ 是 $X$ 的父变量的子集，满足 $Y\cap Scope[\pmb c] = \emptyset$
> 如果对于所有 $\rho \in \mathcal R[\pmb c]$，都满足 $Y\cap Scope[\rho] = \emptyset$，则满足 $(X\perp_{\pmb c} \pmb Y \mid \text{Pa}_X - \pmb Y, \pmb c)$

The proof is left as an exercise (exercise 5.4). 

This proposition speciﬁes a computational tool for deducing “local” CSI relations from the rule representation. We can check whether a variable $Y$ is being tested in the reduced rule set given a context in linear time in the number of rules. (See also exercise 5.6 for a similar procedure for trees.) 

This procedure, however, is incomplete in two ways. First, since the procedure does not examine the actual parameter values, it can miss additional independencies that are true for the speciﬁc parameter assignments. However, as in the case of completeness for d-separation in BNs, this violation only occurs in degenerate cases. (See exercise 5.7.) 

The more severe limitation of this procedure is that it only tests for independencies between $X$ and some of its parents given a context and the other parents. Are there are other, more global, implications of such CSI relations? 

Example 5.18 
Can we capture this intuition formally? Consider the dependence structure in the context $A=a^{0}$ . Intuitively, in this context, the edges $S\rightarrow J$ and $L\rightarrow J$ are both redundant, since we know that $(J\perp_{c}S,L\mid a^{0})$ . Thus, our intuition is that we should check for d-separation in the graph without this edge. Indeed, we can show that this is a sound check for CSI conditions. 

Deﬁnition 5.7 spurious edge 
$P(X~\mid\mathrm{Pa}_{X})$ be a CPD, let $Y\,\,\in\,\,\mathrm{Pa}_{X}$ , and let $^c$ be a context. We say that the edge $Y\rightarrow X$ urious in the contex $^c$ if $P(X\mid\mathrm{Pa}_{X})$ $(X\perp_{c}Y\mid\mathrm{Pa}_{X}-\{Y\},c^{\prime})$ , where $c^{\prime}=c\langle\mathrm{Pa}_{X}\rangle$ ⟨ ⟩ is the restriction of c to variables in $\mathrm{Pa}_{X}$ . 

If we represent CPDs with rules, then we can determine whether an edge is spurious by examin- ing the reduced rule s . L $\mathcal{R}$ be the rule-based CPD for $P(X\mid\mathrm{Pa}_{X})$ , then the edge $Y\rightarrow X$ is spurious in context c if Y does not appear in the reduced rule set $\mathcal{R}[c]$ . 


CSI-separation 
Now we can deﬁne CSI-separation , a variant of $\mathrm{d}$ -separation that takes CSI into account. This notion, deﬁned procedurally in algorithm 5.2, is straightforward: we use local considerations to remove spurious edges and then apply standard d-separation to the resulting graph. We say that 

$X$ is CSI-separated from $Y$ given $Z$ in the context $^c$ if CSI- $\cdot_{\mathrm{SEP}}(\mathcal{G},c,X,Y,Z)$ returns true . As an example, consider the network of example 5.7, in the context $A=a^{0}$ . In this case, we get that the arcs $S\rightarrow J$ and $L\rightarrow J$ are spurious, leading to the reduc grap in ﬁgure $5.7\mathrm{a}$ . As we can see, J and I are d-separated in the reduced graph, as are J and D . Thus, using CSI-sep , we get that $I$ and $J$ are $\mathrm{d}$ -separated given the context $a^{0}$ . Figure 5.7b shows the reduced graph in the context $s^{1}$ . 

It is not hard to show that CSI-separation provides a sound test for determining context- speciﬁc independence. 

Let $\mathcal{G}$ etwork structure, let $P$ b distribution such that $P\vDash\mathcal{Z}_{\ell}(\mathcal{G})$ I let $^c$ be a cont t, and let $X,Y,Z$ be sets of variables. If X is CSI-separated from Y given Z in the context c , then $P\models(X\ \bot_{c}Y\mid Z,c)$ . 

The proof is left as an exercise (exercise 5.8). 

Of course, we also want to know if CSI-separation is complete — that is, whether it discovers all the context-speciﬁc independencies in the distribution. At best, we can hope for the same type of qualiﬁed completeness that we had before: discovering all CSI assertions that are a direct consequence of the structural properties of the model, regardless of the particular choice of parameters. In this case, the structural properties consist of the graph structure (as usual) and the structure of the rule sets or trees. Unfortunately, even this weak notion of completeness does not hold in this case. 

Example 5.19 Consider the example of ﬁgure $5.5b$ and e con $C=c^{1}$ . I this context, the arc $L_{2}\,\rightarrow\,J$ is spurious. Thus, there is no path between $L_{1}$ and $L_{2}$ , even given J . Hence, CSI-sep will report that $L_{1}$ and $L_{2}$ are $d$ -separated given $J$ and the context $C\,=\,c^{1}$ . This case is shown in ﬁgure $5.8a$ . , we conclude that $\left(L_{1}\;\;\perp_{c}\;\;L_{2}\;\;|\;\;J,c^{1}\right)$ . Similarly, in the context $C\ =\ c^{2}$ , the arc $L_{1}\to J$ s, and we have that $L_{1}$ and $L_{2}$ are $d$ -separated given $J$ and $c^{2}$ , and hen that $(L_{1}\perp_{c}L_{2}\mid J,c^{2})$ ⊥ Thus, reason ng by cases, we nclude that once e of C , we have that $L_{1}$ and $L_{2}$ are always d-separated given J , and hence that $\left(L_{1}\perp L_{2}\mid J,C\right)$ ⊥ | . Can we get this conclusion using CSI-separation? Unfortunately, the answer is no. If we invoke CSI-separation with the empty context, then no edges are spurious and CSI-separation reduces to $d$ -separation. Since both $L_{1}$ and $L_{2}$ are parents of $J$ , we conclude that they are not separated given $J$ and $C$ . 

The problem here is that CSI-separation does not perform reasoning by cases. Of course, if we want to determine whether $X$ and $Y$ are independent given $Z$ and a context $^{c,}$ we can invoke CSI-separation on the context $c,z$ for each possible value of $Z$ , and see if $X$ and $Y$ are separated in all of these contexts. This procedure, however, is exponential in the number of variables of $Z$ . Thus, it is practical only for small evidence sets. Can we do better than reasoning by cases? The answer is that sometimes we cannot. See exercise 5.10 for a more detailed examination of this issue. 
## 5.4 Independence of Causal Inﬂuence 
In this section, we describe a very diferent type of structure in the local probability model. Consider a variable $Y$ whose distribution depends on some set of causes $X_{1},\ldots,X_{k}$ . In general, $Y$ can depend on its parents in arbitrary ways — the $X_{i}$ can interact with each other in complex ways, making the efect of each combination of values unrelated to any other combination. However, in many cases, the combined inﬂuence of the $X_{i}$ ’s on $Y$ is a simple combination of the inﬂuence of each of the $X_{i}$ ’s on $Y$ in isolation. In other words, each of the $X_{i}$ ’s inﬂuences $Y$ independently, and the inﬂuence of several of them is simply combined in some way. 
> 本节描述一个局部概率模型中一个非常不同的结构
> 考虑一个变量 $Y$ ，其分布依赖于某个集合 $X_1,\dots, X_k$，一般地 $Y$ 可以以任意方式依赖于其父变量，$X_i$ 之间可以以复杂的方式交互，使得不同的值组合的效果互不相同
> 但在许多情况下，$X_i$ 的结合对 $Y$ 的影响只是单独每个 $X_i$ 对于 $Y$ 的影响的结合

We begin by describing two very useful models of this type — the noisy-or model, and the class of generalized linear models . We then provide a general deﬁnition for this type of interaction. 
### 5.4.1 The Noisy-Or Model 
Let us begin by considering an example in which a diferent professor writes a recommendation letter for a student. Unlike our earlier example, this professor teaches a small seminar class, where she gets to know every student. The quality of her letter depends on two things: whether the student participated in class, for example, by asking good questions $(Q)$ ; and whether he wrote a good ﬁnal paper $(F)$ . Roughly speaking, each of these events is enough to cause the professor to write a good letter. However, the professor might fail to remember the student’s participation. On the other hand, she might not have been able to read the student’s handwriting, and hence may not appreciate the quality of his ﬁnal paper. Thus, there is some noise in the process. 

Let us consider each of the two causes in isolation. Assume that $P(l^{1}\;\mid\;q^{1},f^{0})\:=\:0.8,$ , that is, the professor is 80 percent likely to remember class participation. On the other hand, $P(l^{1}\mid q^{0},\bar{f^{1}})=0.9$ , that is, the student’s handwriting is readable in 90 percent of the cases. What happens if both occur: the student participates in class and writes a good ﬁnal paper? The key assumption is that these are two independent *causal mechanisms* for causing a strong letter, and that the letter is weak only if neither of them succeeded. The ﬁrst causal mechanism — class participation $q^{1}$ — fails with probability 0 . 2 . The second mechanism — a good ﬁnal paper $f^{1}\gets$ fails with probability 0 . 1 . If both $q^{1}$ and $f^{1}$ occurred, the probability that both mechanisms fail (independently) is $0.2\cdot0.1=0.02$ . Thus, we have that $\bar{P}(l^{0}\mid q^{1},f^{1})=0.02$  and $P(l^{1}\mid q^{1},f^{1})=0.98$ . In other words, our CPD for $P(L\mid Q,F)$ is: 
> 本例中，考虑了两个独立的因果机制，第一个因果机制 fail 的概率是0.2，第二个因果机制 fail 的概率是0.1，如果二者同时发生，则 fail 的概率应该是 $0.2*0.1 = 0.02$

This type of interaction between causes is called the noisy-or model. Note that we assumed that a student cannot end up with a strong letter if he neither participated in class nor wrote a good ﬁnal paper. We relax this assumption later on. 
> 这类 cause 之间的交互称为噪声或模型

![[Probabilistic Graph Theory-Fig5.9.png]]

An alternative way of understanding this interaction is by assuming that the letter-writing process can be represented by a more elaborate probabilistic model, as shown in ﬁgure 5.9. This ﬁgure represents the conditional distribution for the Letter variable given Questions and FinalPaper . It also uses two intermediate variables that reveal the associated causal mechanisms. The variable $Q^{\prime}$ is true if the professor remembers the student’s participation; the variable $F^{\prime}$ is true if the professor could read and appreciate the student’s high-quality ﬁnal paper. The letter is strong if and only if one of these events holds. We can verify that the conditional distribution $P(L\mid Q,F)$ induced by this netw k is precisely the one shown before. 

The probability that $Q$ causes L ( 0.8 in this example) is called the noise parameter , and denoted $\lambda_{Q}$ $\lambda_{Q}={\bar{P}}(q^{\prime1}\mid q^{1})$ . Similarly, we have a noise parameter $\lambda_{F}$ , which in this context is $\lambda_{F}=P(f^{\prime1}\mid f^{1})$ . 
> Fig5.9中，$Q$ 能够导致 $L$ 的概率称为噪声参数，记作 $\lambda_Q$，在 Fig5.9的分解下，我们可以知道 $\lambda_Q = P (q'^1 \mid q^1)$
> 类似地，我们有噪声参数 $\lambda_F = P (f'^1 \mid f^1)$
> 噪声参数编码了单独影响的情况下， cause 能导致结果的实际概率

We can also incorporate a *leak probabilit*y that represents the probability — say $0.0001\mathrm{~-~}$ that the professor would write a good recommendation letter for no good reason, simply because she is having a good day. We simply introduce another variable into the network to represent this event. This variable has no parents, and is true with probability $\lambda_{0}=0.0001$ . It is also a parent of the Letter variable, which remains a deterministic or. 
> 我们还引入泄露概率的概念，它表示无论如何 $L$ 都会成功的概率，例如 $0.00001$
> 我们为网络引入另一个变量来表示这一事件，该变量没有父变量，并且为真的概率是 $0.00001$，同时该变量是变量 $L$ 的父变量，关系是 determinstic or

The decomposition of this CPD clearly shows why this local probability model is called a noisy-or. The basic interaction of the efect with its causes is that of an OR, but there is some noise in the “efective value” of each cause. We can deﬁne this model in the more general setting: 
> 在 noisy-or 局部概率模型中，causes 之间的基本的交互就是或关系，并且在每个 cause 的“有效值”中存在一些噪音

Deﬁnition 5.8 noisy-or CPD
Let Y be a binary-valued random variable with k binary-valued parents $X_1,\dots ,X_k$. The CPD $P (Y \ X_1,\dots, X_k)$ is a noisy-or if there are k + 1 noise parameters $\lambda_0,\dots,\lambda_k$ such that noisy-or CPD
> 定义：
> 令 $Y$ 是二值随机变量，且有 $k$ 个二值的父变量 $X_1,\dots, X_k$
> 如果存在 $k+1$ 个噪声参数 $\lambda_0,\dots,\lambda_k$，使得

$$
\begin{align}
P(y^0\mid X_1,\dots,X_k) &= (1-\lambda_0)\prod_{i:X_i = x_i^1}(1-\lambda_i)\tag{5.2}\\
P(y^1\mid X_1,\dots,X_k)&= 1-[(1-\lambda_0)\prod_{i:X_i = x_i^1}(1-\lambda_i)]
\end{align}
$$

> 则CPD $P (Y\mid X_1,\dots, X_k)$ 称为一个噪声或

We note that, if we interpret $x_{i}^{1}$ as $1$ and $x_{i}^{0}$ as $0$ , we can rewrite equation (5.2) somewhat more compactly as: 

$$
P(y^{0}\mid x_{1},.\,.\,.,x_{k})=(1-\lambda_{0})\prod_{i=1}^{k}(1-\lambda_{i})^{x_{i}}.\tag{5.3}
$$ 
Although this transformation might seem cumbersome, it will turn out to be very useful in a variety of settings. 

![[Probabilistic Graph Theory-Fig5.10.png]]

Figure 5.10 shows a graph of the behavior of a special-case noisy or model, where all the variables have the same noise parameter $\lambda$ . The graph shows the probability of the child $Y$ in terms of the parameter $\lambda$ and the number of $X_{i}$ ’s that have the value true . 
> Fig5.10的情况对应于所有的变量都有相同的噪声参数 $\lambda$

The noisy-or model is applicable in a wide variety of settings, but perhaps the most obvious is in the medical domain. For example, as we discussed earlier, a symptom variable such as Fever usually has a very large number of parents, corresponding to diferent diseases that can cause the symptom. However, it is often a reasonable approximation to assume that the diferent diseases use diferent causal mechanisms, and that if any disease succeeds in activating its mechanism, the symptom is present. Hence, the noisy-or model is a reasonable approximation. 

Box 5.C — Concept: BN2O Networks. A class of networks that has received some attention in BN2O network the domain of medical diagnosis is the class of BN2O networks. 
> 在医疗诊断领域，BN2O 网络也十分常用

A BN2O network, illustrated in figure 5.C.1, is a two-layer Bayesian network, where the top layer corresponds to a set of causes, such as diseases, and the second to findings that might indicate these causes, such as symptoms or test results. All variables are binary-valued, and the variables in the second layer all have noisy-or models. 
> BN2O 网络是一个两层的贝叶斯网络，其中顶层对应于 causes，下一层则表示结果，所有的变量都是二值变量，且第二层的变量都有噪声或模型

Speciﬁcally, the CPD of $F_{i}$ is given by: 

$$
P(f_{i}^{0}\mid\mathrm{Pa}_{F_{i}})\;\;=\;\;(1-\lambda_{i,0})\prod_{D_{j}\in\mathrm{Pa}_{F_{i}}}(1-\lambda_{i,j})^{d_{j}}.
$$ 
These networks are conceptually very simple and require a small number of easy-to-understand parameters: Each edge denotes a causal association between a cause $d_{i}$ and a ﬁnding $f_{j}$ ; each is associated with a parameter $\lambda_{i,j}$ that encodes the probability that $d_{i}$ , in isolation, causes $f_{j}$ to manifest. Thus, these networks resemble a simple set of noisy rules, a similarity that greatly facilitates the knowledge-elicitation task. Although simple, BN2O networks are a reasonable ﬁrst approximation for a medical diagnosis network. 

BN2O networks also have another useful property. In Bayesian networks, observing a variable generally induces a correlation between all of its parents. In medical diagnosis networks, where ﬁndings can be caused by a large number of diseases, this phenomenon might lead to signiﬁcant complexity, both cognitively and in terms of inference. However, in medical diagnosis, most of the ﬁndings in any speciﬁc case are false — a patient generally only has a small handful of symptoms. As discussed in section 5.4.4, the parents of a noisy-or variable $F$ are conditionally independent given that we observe that $F$ is false. As a consequence, a BN2O network where we observe $F=f^{0}$ is equivalent to a network where $F$ disappears from the network entirely (see exercise 5.13). This observation can greatly reduce the cost of inference. 
> 贝叶斯网络中，观察到一个变量通常会引入该变量和它所有的父变量之间的关联
> 但噪声或变量 $F$ 的父变量在给定我们观察到 $F$ 为 false 时是条件独立的，因此在 BN2O 网络中，观察到 $F=f^0$ 等价于 $F$ 完全从网络中消失的网络，因此可以大大减少推理开销
### 5.4.2 Generalized Linear Models 
An apparently very diferent class of models that also satisfy independence of causal inﬂuence are the generalized linear models . Although there are many models of this type, in this section we focus on models that deﬁne probability distributions $P(Y\mid X_{1},.\,.\,,X_{k})$ where $Y$ takes on values in some discrete ﬁnite space. We ﬁrst discuss the case where Y and all of the $X_{i}$ ’s are binary-valued. We then extend the model to deal with the multinomial case. 
> 另一类也满足因果影响之间的独立性的模型是广义线性模型
> 我们讨论定义了概率分布 $P (Y\mid X_1,\dots, X_k)$ 的广义线性模型，其中 $Y$ 取离散值
#### 5.4.2.1 Binary-Valued Variables
Roughly speaking, our models in this case are a soft version of a linear threshold function. As a motivating example, we can think of applying this model in a medical setting: In practice, our body’s immune system is constantly ﬁghting of multiple invaders. Each of them adds to the burden, with some adding more than others. We can imagine that when the total burden passes some threshold, we begin to exhibit a fever and other symptoms of infection. That is, as the total burden increases, the probability of fever increases. This requires us to clarify two terms in this discussion. The ﬁrst is the “total burden” value and how it depends on the particular possible disease causes. The second is a speciﬁcation of how the probability of fever depends on the total burden. 

More generally, we examine a CPD of $Y$ given $X_{1},\ldots,X_{k}$ . We assume that the efect of the $X_{i}$ ’s on $Y$ can summari via a linear function $\begin{array}{r}{f(X_{1},\dots,X_{k})=\sum_{i=1}^{k}w_{i}X_{i}}\end{array}$ , where we again interpret $x_{i}^{1}$ as 1 and $x_{i}^{0}$ as 0 . In our example, this function will be the total burden on the immune system, and the $w_{i}$ coefcient describes how much burden is contributed by each disease cause. 
> 考虑给定 $X_1,\dots, X_k$ 时 $Y$ 的条件概率分布，我们假设 $X_i$  对于 $Y$ 的影响可以通过一个线性函数总结 $f (X_1,\dots, X_k) = \sum_{i=1}^k w_i X_i$，其中我们用 $x_i^1$ 表示 1，用 $x_i^0$ 表示 0
> 这可以认为是 $X_1,\dots, X_k$ 对于 $Y$ 的总贡献值，其中 $w_i$ 表示了各个 cause 贡献值的权重

The next question is how the probability of $Y~=~y^{1}$ depends on $f(X_{1},\cdot\cdot\cdot,X_{k})$ . In general, this probability undergoes a phase transition around some threshold value $\tau$ : when $f(X_{1},.\,.\,.\,,X_{k})\,\geq\,\tau$ , then $Y$ is very likely to be 1; when $f(X_{1},.\,.\,.\,,X_{k})\,<\tau$ , then $Y$ is very likely to 0. It is easier to inate $\tau$ by simply deﬁning $\begin{array}{r}{f(X_{1},\dots,X_{k})=w_{0}+\sum_{i=1}^{k}w_{i}X_{i}}\end{array}$ , so that $w_{0}$ takes the role of $-\tau$ . 
> 下一步我们要将贡献值转化为概率，即 $Y=y^1$ 的概率是如何依赖于 $f (X_1,\dots, X_k)$ 的
> 一般情况下，如果 $f (X_1,\dots, X_k )$ 超过某个阈值 $\tau$，则 $Y$ 就非常有可能为 1，当 $f (X_1,\dots, X_k) < \tau$，则 $Y$ 就非常有可能为 0
> 我们可以定义 $f (X_1,\dots, X_k) = w_0 + \sum_{i=1}^k w_i X_i$，其中 $w_0 = -\tau$，则此时 $f (X_1,\dots, X_k)$ 就可以直接与0比较

To provide a realistic model for immune system example and others, we do not use a hard threshold function to deﬁne the probability of $Y$ , but rather a smoother transition function. One common choice (although not the only one) is the sigmoid or logit function: 
> 为了提供一个更实际的模型，我们不会使用硬阈值函数来定义 $Y$ 的概率，而是使用更平滑的转换函数

$$
\mathrm{sigmoid}(z)=\frac{e^{z}}{1+e^{z}}.
$$ 

![[Probabilistic Graph Theory-Fig5.11.png]]

Figure 5.11a shows the sigmoid function. This function implies that the probability saturates to 1 when $f(X_{1},\dots,X_{k})$ is large, and saturates to 0 when $f(X_{1},\dots,X_{k})$ is small. 
And so, activation of another disease cause for a sick patient will not change the probability of fever by much, since it is already close to 1 . Similarly, if the patient is healthy, a minor burden on the immune system will not increase the probability of fever, since $f(X_{1},\cdot\cdot\cdot,X_{k})$ is far from the threshold. In the area of the phase transition, the behavior is close to linear. We can now deﬁne: 

**Deﬁnition 5.9** logistic CPD
Let $Y$ be a binary-valued random variable with $k$ parents $X_{1},\ldots,X_{k}$ that take on numerical values. The CPD $P(Y\mid X_{1},.\,.\,,X_{k})$ is a logistic CPD if there are $k+1$ weights $w_{0},w_{1},.\cdot\cdot,w_{k}$ such that: 

$$
P(y^{1}\mid X_{1},\ldots,X_{k})\ \ =\ \ \mathrm{sigmoid}(w_{0}+\sum_{i=1}^{k}w_{i}X_{i}).
$$ 

> 定义：
> 令 $Y$ 是二值随机变量，有 $k$ 个值为数值的父变量 $X_1,\dots, X_k$，如果存在 $k+1$ 个权重，满足

$$
P(y^1\mid X_1,\dots,X_k) = \text{sigmoid}(w_0 + \sum_{i=1}^k w_i X_i)
$$

> 则称 CPD $P (Y\mid X_1,\dots, X_k)$ 是逻辑 CPD

We have already encountered this CPD in example 4.20, where we saw that it can be derived by taking a naive Markov network and reformulating it as a conditional distribution. 

We can interpret the parameter $w_{i}$ in terms of its efect on the log-odds of $Y$ . In general, the odds ratio for a binary variable is the ratio of the probability of $y^{1}$ and the probability of $y^{0}$ . It is the same concept used when we say that the odds of some event (for example, a sports team winning the Super Bowl) are $^{u}2$ to 1.” Consider the odds ratio for the variable $Y$ , where we use $Z$ to represent $\begin{array}{r}{w_{0}+\sum_{i}w_{i}X_{i}}\end{array}$ : 
> 可以从 $Y$ 的对数几率的角度解释参数 $w_i$
> 二值变量的几率是 $y^1$ 的概率和 $y^0$ 的概率的比值
> 广义线性模型中，$Y$ 的几率如下，其中 $Z  = w_0 + \sum_i w_i X_i$

$$
O(X)={\frac{P(y^{1}\mid X_{1},\ldots,X_{k})}{P(y^{0}\mid X_{1},\ldots,X_{k})}}={\frac{e^{Z}/(1+e^{Z})}{1/(1+e^{Z})}}=e^{Z}.
$$

Now, consider the effect on this odds ratio as some variable $X_{j}$ changes its value from false to true . Let $X_{-j}$ be the variables in $X_{1},\ldots,X_{k}$ except for $X_{j}$ . Then: 

$$
\frac {O(X_{-j},x_j^1)}{O(X_{-j},x_j^0)} = \frac{\exp(w_0 + \sum_{i=\ne j}w_i X_i + w_j)}{\exp(w_0 + \sum_{i\ne j}w_i X_i)} = e^{w_j}
$$

Thus, $X_{j}=t r u e$ changes the odds ratio by a multiplicative factor of $e^{w_{j}}$ . A positive coefcient $w_{j}\,>\,0$ implies that $e^{w_{j}}\,>\,1$ so that the odds ratio increases, hence making $y^{1}$ more likely. Conversely, a negative coefcient $w_{j}\,<\,0$ implies that $e^{w_{j}}\,<\,1$ and hence the odds ratio decreases, making $y^{1}$ less likely. 
> 考虑当某个变量 $X_j$ 从 false 变为 true，前后几率的比值为 $e^{w_j}$
> 如果系数 $w_j > 0$，则 $e^{w_j} > 1$，说明几率增大，也就是 $y^1$ 更可能了
> 如果系数 $w_j < 0$，则 $e^{w_j} < 1$，说明几率减小，也就是 $y^0$ 更可能

Figure 5.11b shows a graph of the behavior of a special case of the logistic CPD model, where all the variables have the me weight $w$ . The graph shows $P(Y\mid X_{1},.\,.\,,X_{k})$ as a function of $w$ and the number of $X_{i}$ ’s that take the value true . The graph shows two cases: one where $w_{0}\,=\,0$ and the other where $w_{0}~=~-5$ . In the ﬁrst case, the probability starts out at 0.5, when none of the causes are in efect, and rapidly goes up to 1; the rate of increase is, as expected, much higher for high values of $w$ . 
> 可以看到 $w$ 越大，概率的增长速率也越大

It is interesting to compare this graph to the graph of ﬁgure 5.10b that shows the behavior of the noisy-or model with $\lambda_{0}=0.5$ . The graphs exhibit very similar behavior for $\lambda=w$ , showing that the incremental efect of a new cause is similar in both. However, the logistic CPD also allows for a negative inﬂuence of some $X_{i}$ on $Y$ by making $w_{i}$ negative. Furthermore, the parameterization of the logistic model also provides substantially more ﬂexibility in generating qualitatively diferent distributions. For example, as shown in ﬁgure 5.11c, setting $w_{0}$ to a diferent value allows us to obtain the threshold effect discussed earlier. Furthermore, as shown in ﬁgure 5.11d, we can adapt the scale of the parameters to obtain a sharper transition. However, the noisy-or model is cognitively very plausible in many settings. Furthermore, as we discuss, it has certain beneﬁts both in reasoning with the models and in learning the models from data. 
> 噪声或 CPD 模型在 $\lambda = w$ 时和逻辑 CPD 模型的行为非常相似
> 但逻辑 CPD 允许某个 $X_i$ 对 $Y$ 产生的是负面影响，只要让 $w_i$ 为负即可
> 逻辑 CPD 的参数化为生成性质上不同的分布也提供了更多灵活性，例如设定 $w_0$ 以改变阈值，以及 scale 参数以得到更陡峭的转变
#### 5.4.2.2 Multivalued Variables 
We can extend the logistic CPD to the case where $Y$ takes on multiple values $y^{1},\cdot\cdot\cdot,y^{m}$ . In this case, we can imagine that the diferent values of $Y$ are each supported in a diferent way by the $X_{i}$ ’s, where the support is again deﬁned via a linear function. The choice of $Y$ can be viewed as a soft version of “winner takes all,” where the $y^{i}$ that has the most support gets probability 1 and the others get probability 0. More precisely, we have: 
> 我们将逻辑 CPD 中的 $Y$ 拓展到多个值 $y^1,\dots, y^m$
> 可以认为每个 $X_i$ 都有不同的方式支持 $Y$ 的不同值，“支持”通过一个线性函数定义
> $Y$ 的取值就是具有支持最高的 $y^i$ 值

**Deﬁnition 5.10** multinomial logistic CPD 
Let $Y$ be an $m$ -valued random variable with $k$ parents $X_{1},\ldots,X_{k}$ that take on numerical values. The CPD $P(Y\mid X_{1},.\,.\,,X_{k})$ is $a$ multinomial logistic if for each $j=1,\dots,m,$ , there are $k+1$  weights $w_{j,0},w_{j,1},\cdot\cdot\cdot,w_{j,k}$ such that: 

$$
\begin{array}{r c l}{\ell_{j}(X_{1},\ldots,X_{k})}&{=}&{\displaystyle w_{j,0}+\sum_{i=1}^{k}w_{j,i}X_{i}}\\ {P(y^{j}\mid X_{1},\ldots,X_{k})}&{=}&{\displaystyle\frac{\exp\left(\ell_{j}(X_{1},\ldots,X_{k})\right)}{\sum_{j^{\prime}=1}^{m}\exp\left(\ell_{j^{\prime}}(X_{1},\ldots,X_{k})\right)}.}\end{array}
$$ 
> 定义：
> $Y$ 是一个 $m$ 值随机变量，有 $k$ 个父变量 $X_1,\dots, X_k$
> 如果对于每个 $j = 1,\dots, m$，都存在 $k+1$ 个权重 $w_{j, 0}, \dots, w_{j, k}$，使得

$$
\begin{array}{r c l}{\ell_{j}(X_{1},\ldots,X_{k})}&{=}&{\displaystyle w_{j,0}+\sum_{i=1}^{k}w_{j,i}X_{i}}\\ {P(y^{j}\mid X_{1},\ldots,X_{k})}&{=}&{\displaystyle\frac{\exp\left(\ell_{j}(X_{1},\ldots,X_{k})\right)}{\sum_{j^{\prime}=1}^{m}\exp\left(\ell_{j^{\prime}}(X_{1},\ldots,X_{k})\right)}.}\end{array}
$$

> 则称条件概率分布 $P (Y\mid X_1,\dots, X_k)$ 是一个多项式逻辑分布

Figure 5.12 shows one example of this model for the case of two parents and a three-valued child $Y$ . We note that one of the weights $w_{j,1},\cdot\cdot\cdot,w_{j,k}$ is redundant, as it can be folded into the bias term $w_{j,0}$ . 

We can also deal with the case where the parent variables $X_{i}$ take on more than two values. The approach taken is usually straightforward. If $X_{i}^{=}x_{i}^{1},\ldots,x_{i}^{m}$ , we deﬁne a new set of binary- valued variables $X_{i,1},.\cdot\cdot\cdot,X_{i,m},$ , where $X_{i,j}=x_{i,j}^{1}$ precisely when $X_{i}=j$ . 
> 如果父变量取多个值时，我们可以定义多个二值变量

Each of these new variables gets its own coefcient (or set of coefcients) in the logistic function. For example, if we have a binary-valued child $Y$ with an $m$ -valued parent $X$ , our logistic function would be parameterized using $m+1$ weights, $w_{0},w_{1},\dots,w_{m}$ , such that 

$$
P(y^{1}\mid X)\;\;=\;\;\mathrm{sigmoid}(w_{0}+\sum_{j=1}^{m}w_{j}I\{X=x^{j}\}).\tag{5.4}
$$ 
We note that, for any assignment to $X_{i}$ , precisely one of the weights $w_{1},.\,.\,.\,,w_{m}$ will make a contribution to the linear function. As a consequence, one of the weights is redundant, since it can be folded into the bias weight $w_{0}$
> 对于对 $X_i$ 的每一个赋值，仅仅只会有1个权重 $w_1,\dots, w_m$ 会对线性函数做出贡献

We noted before that we can view a binary-valued logistic CPD as a conditional version of a naive Markov model. We can generalize this observation to the nonbinary case, and show that the multinomial logit CPD is also a particular type of pairwise CRF (see exercise 5.16). 
### 5.4.3 The General Formulation 
Both of these models are specials case of a general class of local probability models, which satisfy a property called causal independence or independence of causal inﬂuence ( ICI ). These models all share the property that the inﬂuence of multiple causes can be decomposed into separate inﬂuences. We can deﬁne the resulting class of models more precisely as follows: 
> 之前介绍的两个广义线性模型都是局部概率模型的特例
> 局部概率模型满足一个称为因果独立性/因果影响独立性的特性，也就是多个 cause 的影响可以被分解为它们分别的影响

![[Probabilistic Graphical Models-Fig5.13.png]]

**Deﬁnition 5.11** 
Let $Y$ be a random variable with parents $X_{1},\ldots,X_{k}$ . The CPD $P(Y\mid X_{1},.\,.\,,X_{k})$ exhibits independence of causal inﬂuence if it is described via a network fragment of the structure shown in ﬁgure 5.13, where the CPD of $Z$ is a deterministic function $f$ . 
> 定义：
> 令 $Y$ 为父变量是 $X_1,\dots, X_k$ 的随机变量，如果CPD $P (Y\mid X_1,\dots, X_k)$ 可以通过5.13中的网络结构描述（其中 $Z$ 的 CPD 是一个确定性函数 $f$），则 $P (Y\mid X_1,\dots, X_k)$ 即展现了因果影响之间的独立性

Intuitively, each variable $X_{i}$ can be transformed separately using its own individual noise model. The resulting variables $Z_{i}$ are combined using some deterministic combination function. Finally, an additional stochastic choice can be applied to the result $Z$ , so that the ﬁnal value of $Y$ is not necessarily a deterministic function of the variables $Z_{i}$ ’s. The key here is that any stochastic parts of the model are applied independently to each of the $X_{i}$ ’s, so that there can be no interactions between them. The only interaction between the $X_{i}$ ’s occurs in the context of the function $f$ . 
> 每个变量 $X_i$ 通过自己独立的噪声模型被分别转换为变量 $Z_i$ ，然后经过某个确定性函数被结合为 $Z$，最终对 $Z$ 应用随机选择
> 故 $Y$ 的最终值并不必要是变量 $Z_i$ 的确定性函数
> 关键在于模型中每个随机部分都是独立应用于各个 $X_i$ 的，因此它们之间不存在交互，$X_i$ 之间唯一的交互存在于函数 $f$ 中

As stated, this deﬁnition is not particularly meaningful. Given an arbitrarily complex function $f$ , we can represent any CPD using the representation of ﬁgure 5.13. (See exercise 5.15.) It is possible to place various restrictions on the form of the function $f$ that would make the deﬁnition more meaningful. For our purposes, we provide a fairly stringent deﬁnition that fortunately turns out to capture the standard uses of ICI models. 

**Deﬁnition 5.12** 
We say that a deterministic binary function $x\diamond y$ is commutative if $x\diamond y=y\diamond x$ , and associative $i f\left(x\diamond y\right)\diamond z=x\diamond\left(y\diamond z\right)$ . We say that a function $f(x_{1},\dots,x_{k})$ is $a$ symmetric decomposable if there is a commutative associative function $x\diamond y$ such that $f(x_{1},.\,.\,.\,,x_{k})=x_{1}\diamond x_{2}\diamond$ $\cdot\cdot\cdot x_{k}$ . 
> 定义：
> 如果存在一个可交换且可结合的函数 $x\diamond y$ ，使得函数 $f (x_1,\dots, x_k) = x_1\diamond x_2\diamond  \cdots \diamond x_k$，则称 $f (x_1,\dots, x_k)$ 是对称可分解函数

**Deﬁnition 5.13** symmetric ICI 
We say that the CPD $P(Y\mid X_{1},.\,.\,,X_{k})$ exhibits symmetric ICI it is described via a network fragment of the structure shown in ﬁgure 5.13, where the CPD of Z is a deterministic symmetric decomposable function $f$ . The CPD exhibits fully symmetric ICI if the CPDs of the diferent $Z_{i}$ variables are identical. 
> 定义：
> 如果 CPD $P (Y\mid X_1,\dots, X_k)$ 可以被 fig5.13的结构描述，且其中 $Z$ 的 CPD 是一个对称可分解函数，则称 CPD 展现了对称的因果影响间的独立性
> 如果不同的 $Z_i$ 的 CPDs 都相同，则 CPD 展现了完全的对称 ICI

There are many instantiations of the symmetric ICI model, with diferent noise models — $P(Z_{i}\mid X_{i})$ — and diferent combination functions. Our noisy-or model uses the combination function OR and a simple noise model with binary variables. The generalized linear models use the $Z_{i}$ to produce $w_{i}X_{i}$ , and then summation as the combination function $f$ . The ﬁnal soft thresholding efect is accomplished in the distribution of $Y$ given $Z$ . 
> 不同的 ICI 模型的区别：不同的噪声模型 $P (Z_i\mid X_i)$ 和不同的结合函数

These types of models turn out to be very useful in practice, both because of their cognitive plausibility and because they provide a signiﬁcant reduction in the number of parameters required to represent the distribution. The number of parameters in the CPD is linear in the number of parents, as opposed to the usual exponential. 

Box 5.D — Case Study: Noisy Rule Models for Medical Diagnosis. As discussed in box 5.C, noisy rule interactions such as noisy-or are a simple yet plausible first approximation of models for noisy-max medical diagnosis. A generalization that is also useful in this setting is the noisy-max model. Like
noisy-max the application of the noisy-or model for diagnosis, the parents $X_{i}$ correspond to diferent diseases that the patient might have. In this case, however, the value space of the symptom variable $Y$ can be more reﬁned than simply $\{p r e s e n t,a b s e n t\}$ ; it can encode the severit f the symptom. Each $Z_{i}$ corresponds (intuitively) to the efect of the disease $X_{i}$ on the symptom Y in isolation, that is, the severity of the symptom in case only the disease $X_{i}$ is present. The value of $Z$ is the maximum of the diferent $Z_{i}$ ’s. 

Both noisy-or and noisy-max models have been used in several medical diagnosis networks. Two of the largest are the QMR-DT (Shwe et al. 1991) and CPCS (Pradhan et al. 1994) networks, both based on various versions of a knowledge-based system called QMR (Quick Medical Reference), compiled for diagnosis of internal medicine. QMR-DT is a BN2O network (see box 5.C) that contains more than ﬁve hundred signiﬁcant diseases, about four thousand associated ﬁndings, and more than forty thousand disease-ﬁnding associations. 

CPCS is a somewhat smaller network, containing close to ﬁve hundred variables and more than nine hundred edges. Unlike QMR-DT, the network contains not only diseases and ﬁndings but also variables for predisposing factors and intermediate physiological states. Thus, CPCS has at least four distinct layers. All variables representing diseases and intermediate states take on one of four values. A speciﬁcation of the network using full conditional probability tables would require close to 134 million parameters. However, the network is constructed using only noisy-or and noisy-max interactions, so that the number of actual parameters is only 8,254. Furthermore, most of the parameters were generated automatically from “frequency weights” in the original knowledge base. Thus, the number of parameters that were, in fact, elicited during the construction of the network is around 560. 

Finally, the symmetric ICI models allow certain decompositions of the CPD that can be exploited by probabilistic inference algorithms for computational gain, when the domain of the variables $Z_{i}$ and the variable $Z$ are reasonably small. 
### 5.4.4 Independencies 
As we have seen, structured CPDs often induce independence properties that go beyond those represented explicitly in the Bayesian network structure. Understanding these independencies can be useful for gaining insight into the properties of our distribution. Also, as we will see, the additional structure can be exploited for improving the performance of various probabilistic inference algorithms. 
> 我们知道有结构的 CPDs 可以引入贝叶斯网络结构中不能准确表示的独立性

The additional independence properties that arise in general ICI models $P(Y\mid X_{1},.\,.\,,X_{k})$ are more indirect than those we have seen in the context of deterministic CPDs or tree-CPDs. In particular, they do not manifest directly in terms of the original variables, but only if we decompose it by adding auxiliary variables. In particular, as we can easily see from ﬁgure 5.13, each $X_{i}$ is conditionally independent of $Y$ , and of the other $X_{j}$ ’s, given $Z_{i}$ . 
> 在通用 ICI 模型 $P (Y\mid X_1,\dots, X_k)$ 中出现的额外独立性相较于 determinstic CPDs 和 tree-CPDs 中的独立性要更不直接
> 它们并不关于原始变量展现出来，需要我们通过添加辅助变量进行分解，例如 fig5.13中，每个 $X_i$ 在给定 $Z_i$ 的情况下，条件独立于 $Y$ 和其他 $X_j$

We can obtain even more independencies by decomposing the CPD of $Z$ in various ways. For example, assume that $k=4$ , so that our CPD has the form $P(Y\mid X_{1},X_{2},X_{3},X_{4})$ . We can introduce two new variables $W_{1}$ and $W_{2}$ , such that: 

$$
\begin{array}{r c l}{{W_{1}}}&{{=}}&{{Z_{0}\diamond Z_{1}\diamond Z_{2}}}\\ {{W_{2}}}&{{=}}&{{Z_{3}\diamond Z_{4}}}\\ {{Z}}&{{=}}&{{W_{1}\diamond W_{2}}}\end{array}
$$ 
By the associativity of $\diamondsuit$ , the decomposed CPD is precisely equivalent to th riginal one. In this CPD, we can use the results of section 5.2 to conclude, for example, that $X_{4}$ is independent of $Y$ given $W_{2}$ . 
> 通过对 $Z$ 的 CPD 以多种方式分解，可以引入更多独立性

Although these independencies might appear somewhat artiﬁcial, it turns out that the associated decomposition of the network can be exploited by inference algorithms (see section 9.6.1). However, as we will see, they are only useful when the domain of the intermediate variables $(W_{1}$ and $W_{2}$ in our example) are small. This restriction should not be surprising given our earlier observation that any CPD can be decomposed in this way if we allow the $Z_{i}$ ’s and $Z$ to be arbitrarily complex. 

The independencies that we just saw are derived simply from the fact that the CPD of $Z$ is deterministic and symmetric. As in section 5.2, there are often additional independencies that are associated with the particular choice of deterministic function. 
> 上例中我们仅利用了 $Z$ 的 CPD 是确定且对称的性质推导出了新的独立性，我们可以看到之后会存在和特定的确定性函数相关的独立性

The best-known independence of this type is the one arising for noisy-or models: 

**Proposition 5.3**
$P(Y\mid X_{1},.\,.\,,X_{k})$ be a noisy-or CPD. Then for each $i\neq j$ , $X_{i}$ is independent of $X_{j}$ given $Y=y^{0}$ . 
> 引理：
> 对于噪声或 CPD $P (Y\mid X_1,\dots, X_k)$，当 $i\ne j$，$X_i$ 在给定 $Y = y^0$ 的情况下就独立于 $X_j$

The proof is left as an exercise (exercise 5.11). Note that this independence is not derived from the network structure via d-separation: Instantiating $Y$ enables the v-structure between $X_{i}$ and $X_{j}$ , and hence potentially renders them correlated. Furthermore, this independence is context- speciﬁc: it holds only for the speciﬁc value $Y=y^{0}$ . Other deterministic functions are associated with other context-speciﬁc independencies. 
> 该独立性不能从 d-seperation 中推导出来，而是和上下文 $Y = y^0$ 相关的
## 5.5 Continuous Variables 
So far, we have restricted attention to discrete variables with ﬁnitely many values. In many situations, some variables are best modeled as taking values in some continuous space. Examples include variables such as position, velocity, temperature, and pressure. Clearly, we cannot use a table representation in this case. One common solution is to circumvent the entire issue by discretizing all continuous variables. Unfortunately, this solution can be problematic in many cases. In order to get a reasonably accurate model, we often have to use a fairly ﬁne discretization, with tens or even hundreds of values. For example, when applying probabilistic models to a robot navigation task, a typical discretization granularity might be 15 centimeters for the $x$ and $y$ coordinates of the robot location. For a reasonably sized environment, each of these variables might have more than a thousand values, leading to more than a million discretized values for the robot’s position. CPDs of this magnitude are outside the range of most systems. 

**Furthermore, when we discretize a continuous variable we often lose much of the structure that characterizes it. It is not generally the case that each of the million values that deﬁnes a robot position can be associated with an arbitrary probability. Basic continuity assumptions that hold in almost all domains imply certain relationships that hold between probabilities associated with “nearby” discretized values of a continuous variable. However, such constraints are very hard to capture in a discrete distribution, where there is no notion that two values of the variable are “close” to each other.** 

Fortunately, nothing in our formulation of a Bayesian network requires that we restrict attention to discrete variables. Our only requirement is that the CPD $P(X\mid\mathrm{Pa}_{X})$ represent, for every assignment of values $\operatorname{pa}_{X}$ to $\mathrm{Pa}_{X}$ , a distribution over $X$. In this case, $X$ might be continuous, in which case the CPD would need to represent distributions over a continuum of values; we might also have some of $X$ ’s parents be continuous, so that the CPD would also need to represent a continuum of diferent probability distributions. However, as we now show, we can provide implicit representations for CPDs of this type, allowing us to apply all of the machinery we developed for the continuous case as well as for hybrid networks involving both discrete and continuous variables. 
> 我们的贝叶斯网络公式并不限定在离散变量，唯一的要求就是 CPD $P (X\mid \text{Pa}_X)$ 对于 $\text{Pa}_X$ 的每一个赋值 $\text{pa}_X$ 都定义了一个在 $X$ 上的分布

In this section, we describe how continuous variables can be integrated into the BN framework. We ﬁrst describe the purely continuous case, where the CPDs involve only continuous variables, both as parents and as children. We then examine the case of hybrid networks, which involve both discrete and continuous variables. 
> 本节讨论贝叶斯网络包含连续变量的情况，包括了仅仅包含连续变量以及既包含了连续变量也包含了离散变量

There are many possible models one could use for any of these cases; we brieﬂy describe only one prototypical example for each of them, focusing on the models that are most commonly used. Of course, there is an unlimited range of representations that we can use: any parametric representation for a CPD is eligible in principle. The only difculty, as far as representation is concerned, is in creating a language that allows for it. Other tasks, such as inference and learning, are a diferent issue. As we will see, these tasks can be difcult even for very simple hybrid models. 

The most commonly used parametric form for continuous density functions is the Gaussian distribution. We have already described the univariate Gaussian distribution in chapter 2. We now describe how it can be used within the context of a Bayesian network representation. 
> 连续密度函数最常用的参数化形式就是高斯分布

First, let us consider the problem of representing a dependency of a continuous variable $Y$ on a continuous parent $X$ . One simple solution is to decide to model the distribution of $Y$ as a Gaussian, whose parameters depend on the value of $X$ . In this case, we need to have a set of parameters for every one of the inﬁnitely many values $x\in V a l(X)$ . A commo olution is to decide that the mean of Y is a linear function of X , and that the variance of Y does not depend on $X$ . For example, we might have that 
> 考虑如何表示连续变量 $Y$ 依赖于连续变量 $X$
> 一个简单方法是将 $Y$ 的分布建模为高斯分布，其参数依赖于 $X$ 的值，例如 $Y$ 的均值是 $X$ 的线性函数，$Y$ 的方差不依赖于 $X$

$$
p(Y\mid x)=\!{\mathcal{N}}\left(-2x+0.9;1\right).
$$ 
Example 5.20 
Consider a vehicle (for example, a car) moving over time. For simplicity, assume that the vehicle is moving along a straight line, so that its position (measured in meters) at the t ’th second is described using a single variable $X^{(t)}$ . Let $V^{(t)}$ represent the velocity of the car at the k th second, measured in meters per second. Then, under ideal motion, we would have that $X^{(t+1)}=X^{(t)}+V^{(t)}-i f$ the car is at meter #510 along the road, and its current velocity is 15 meters/second, then we expect its position at the next second to be meter #525. However, there is invariably some stochasticity in the motion. Hence, it is much more realistic to assert that the car’s position $X^{(t+1)}$ is described using a Gaussian distribution whose mean is 525 and whose variance is 5 meters. 

This type of dependence is called a linear Gaussian model. It extends to multiple continuous parents in a straightforward way: 
> 这类依赖被称为线性高斯模型

**Deﬁnition 5.14** linear Gaussian CPD 
Let $Y$ be a continuous variable with continuous parents $X_{1},\ldots,X_{k}$ . We say that $Y$ has a linear Gaussian model if there are parameters $\beta_{0},\ldots,\beta_{k}$ and $\sigma^{2}$ such that 

$$
p(Y\mid x_{1},\ldots,x_{k})=\mathcal{N}\left(\beta_{0}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k};\sigma^{2}\right).
$$ 
In vector notation, 

$$
p(Y\mid\mathbf{\boldsymbol{x}})=\mathcal{N}\left(\beta_{0}+\beta^{T}\mathbf{\boldsymbol{x}};\sigma^{2}\right).
$$ 

> 定义：
> $Y$ 是连续变量，其父变量包括 $X_1,\dots, X_k$，如果存在参数 $\beta_0,\dots, \beta_k$ 以及 $\sigma^2$，使得

$$
p(Y\mid x_{1},\ldots,x_{k})=\mathcal{N}\left(\beta_{0}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k};\sigma^{2}\right).
$$

> 则 $Y$ 具有线性高斯模型

Viewed slightly diferently, this formulation says that $Y$ is a linear function of the variables $X_{1},\ldots,X_{k}$ , with the addition of Gaussian noise with mean 0 and variance $\sigma^{2}$ : 

$$
Y=\beta_{0}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+\epsilon,
$$ 
where $\epsilon$ is a Gaussian random variable with mean 0 and variance $\sigma^{2}$ , representing the noise in the system. 
> 该表示也可以理解为 $Y$ 是有关于 $X_1,\dots, X_k$ 的线性函数再加上均值为0，方差为 $\sigma^2$ 的高斯噪声 $\epsilon$

This simple model captures many interesting dependencies. However, there are certain facets of the situation that it might not capture. For example, the variance of the child variable $Y$ cannot depend on the actual values of the parents. In example 5.20, we might wish to construct a model in which there is more variance about a car’s future position if it is currently moving very quickly. The linear Gaussian model cannot capture this type of interaction. 

Of course, we can easily extend this model to have the mean and variance of $Y$ depend on the values of its parents in arbitrary way. For example, we can easily construct a richer representation where we allow the mean of $Y$ to be $\sin(x_{1})^{x_{2}}$ and its variance to be $(x_{3}/x_{4})^{2}$ . However, the linear Gaussian model is a very natural one, which is a useful approximation in many practical applications. Furthermore, as we will see in section 7.2, networks based on the linear Gaussian model provide us with an alternative representation for multivariate Gaussian distributions, one that directly reveals more of the underlying structure. 

Box 5.E — Case Study: Robot Motion and Sensors. 
One interesting application of hybrid mod- els is in the domain of robot localization . In this application, the robot must keep track of its location as it moves in an environment, and obtains sensor readings that depend on its location. This application is an example of a temporal model, a topic that will be discussed in detail in section 6.2; we also return to the robot example speciﬁcally in box 15.A. There are two main local probability models associated with this application. The ﬁrst speciﬁes the robot dynamics — the distribution over its position at the next time step $L^{\prime}$ given its current position $L$ and the action taken $A$ ; the second speciﬁes the robot sensor model — the distribution over its observed sensor reading $S$ at the current time given its current location $L$ . 

We describe one model for this application, as proposed by Fox et al. (1999) and Thrun et al. (2000). Here, the robot location $L$ is a three-dimensional vector containing its $X,Y$ coordinates and an angular orientation $\theta$ . The action $A$ speciﬁes a distance to travel and a rotation (ofset from the current $\theta_{-}$ ). The model uses the assumption that the errors in both translation and rotation are normally distributed with zero mean. Speciﬁcally, $P(L^{\prime}\mid L,A)$ is deﬁned as a product of two independent Gaussians with cut of tails, $P(\theta^{\prime}\mid\theta,A)$ and $P(X^{\prime},Y^{\prime}\mid X,Y,A)$ , whose variances 

are proportional to the length of the motion. The robot’s conditional distribution over $(X^{\prime},Y^{\prime})$ is $a$ banana-shaped cloud (see ﬁgure 5.E.1a, where the banana shape is due to the noise in the rotation. 

The sensor is generally some type of range sensor, either a sonar or a laser, which provides $a$ reading $D$ of the distance between the robot and the nearest obstacle along the direction of the sensor. There are two distinct cases to consider. If the sensor signal results from an obstacle in the map, then the resulting distribution is modeled by $a$ Gaussian distribution with mean at the distance to this obstacle. Letting $O_{L}$ be the distance to the closest obstacle to the position $L$ (along the sensor beam), we can deﬁne $P_{m}(D\mid L)=\mathcal{N}\left(o_{L};\sigma^{2}\right)$  , where the variance $\sigma^{2}$ represents the uncertainty of the measured distance, based on the accuracy of the world model and the accuracy of the sensor. Figure 5.E.1b shows an example of such a distribution for an ultrasound sensor and $a$ laser range ﬁnder. The laser sensor has a higher accuracy than the ultrasound sensor, as indicated by the smaller variance. 

The second case arises when the sensor beam is reﬂected by an obstacle not represented in the world model (for example, a dynamic obstacle, such as a person or a chair, which is not in the robot’s map). Assuming that these objects are equally distributed in the environment, the probability $P_{u}(D)$ of detecting an unknown obstacle at distance $D$ is independent of the location of the robot and can be modeled by an exponential distribution. This distribution results from the observation that a distance $d$ is measured if the sensor is not reﬂected by an obstacle at a shorter distance and is reﬂected at distance $d$ . An example exponential distribution is shown in ﬁgure 5.E.1c. 

Only one of these two cases can hold r a given measurement. Thus, $P(D\mid L)$ is a combi- nation of the tw distributions $P_{m}$ and $P_{u}$ . The combined probability $P(D\mid L)$ is based on the observation that d is measured in one of two cases: 

• The sensor beam in not reﬂected by an unknown obstacle before reaching distance $d$ , and is reﬂected by the known obstacle at distance $d$ (an event that happens only with some probability). 
• The beam is reﬂected neither by an unknown obstacle nor by the known obstacle before reaching distance $d$ , and it is reﬂected by an unknown obstacle at distance $d$ . 

Overall, the probability of sensor measurements is computed incrementally for the diferent distances starting at 0cm ; for each distance, we consider the probability that the sensor beam reaches the corresponding distance and is reﬂected either by the closest obstacle in the map (along the sensor beam) or by an unknown obstacle. Putting these diferent cases together, we obtain a single distribu- tion for $P(D\mid L)$ . This distribution is shown in ﬁgure 5.E.1d, along with an empirical distribution obtained from data pairs consisting of the distance $O_{L}$ to the closest obstacle on the map and the measured distance d during the typical operation of the robot. 
### 5.5.1 Hybrid Models 
We now turn our attention to models incorporating both discrete and continuous variables. We have to address two types of dependencies: a continuous variable with continuous and discrete parents, and a discrete variable with continuous and discrete parents. 
> 我们考虑同时包含离散和连续变量的模型
> 我们需要解决两类依赖：连续变量具有连续和离散的父变量、离散变量具有连续和离散的父变量

Let us ﬁrst consider the case of a continuous child $X$ . If we ignore the discrete parents of $X$ , we can simply represent the CPD of $X$ as a linear Gaussian of $X$ ’s continuous parents. 
The simplest way of making the continuous variable $X$ depend on a discrete variable $U$ is to deﬁne a diferent set of parameters for every value of the discrete parent. More precisely: 
> 考虑连续变量为子变量，最简单的方式是为离散父变量的每一个取值定义一组参数

**Deﬁnition 5.15**  conditional linear Gaussian CPD 
$X$ ontinuous variable, and let $U\,=\,\{U_{1},.\,.\,.\,,U_{m}\}$ be its discrete parents and $\textbf{\textit{Y}}=$ $\{Y_{1},\ldots,Y_{k}\}$ be its continuous parents. We say that X has $a$ conditional linear Gaussian (CLG) CPD if, for every value $\pmb u\in\mathit{V a l}(\pmb{U})$ , we have $a$ set of $k+1$ coefcients $a_{{\pmb u},0},\cdot\cdot\cdot,a_{{\pmb u},k}$ and $^a$ variance $\sigma_{u}^{2}$ such that 

$$
p(X\mid\mathbf{\boldsymbol{u}},\mathbf{\boldsymbol{y}})=\mathcal{N}\left(a_{\mathbf{\boldsymbol{u}},0}+\sum_{i=1}^{k}a_{\mathbf{\boldsymbol{u}},i}y_{i};\sigma_{\mathbf{\boldsymbol{u}}}^{2}\right)
$$ 
> 定义：
> $X$ 为连续变量，$\pmb U = \{U_1,\dots, U_m\}$ 为其离散父变量，$\pmb Y=\{Y_1,\dots, Y_k\}$ 为其连续父变量，如果对于每个值 $\pmb u \in Val (\pmb U)$，我们都存在 $k+1$ 个系数 $a_{\pmb u, 0},\dots, a_{\pmb u, k}$，以及一个方差 $\sigma_{\pmb u}^2$，使得

$$
p(X\mid\mathbf{\boldsymbol{u}},\mathbf{\boldsymbol{y}})=\mathcal{N}\left(a_{\mathbf{\boldsymbol{u}},0}+\sum_{i=1}^{k}a_{\mathbf{\boldsymbol{u}},i}y_{i};\sigma_{\mathbf{\boldsymbol{u}}}^{2}\right)
$$
> 则称 $X$ 具有条件线性高斯 CPD

If we restrict attention to this type of CPD, we get an interesting class of models. More precisely, we have: 

**Deﬁnition 5.16** CLG network 
A Bayesian network is called a CLG network if every discrete variable has only discrete parents and every continuous variable has a CLG CPD. 
> 定义：
> 如果贝叶斯网络中每个离散变量仅有离散父变量，且每个连续变量都有一个 CLG (continuous linear gaussian) CPD，则该网络称为一个 CLG 网络

Note that the conditional linear Gaussian model does not allow for continuous variables to have discrete children. A CLG model induces a joint distribution that has the form of a mixture — a weighted average — of Gaussians. The mixture contains one Gaussian component for each instantiation of the discrete network variables; the weight of the component is the probability of that instantiation. Thus, the number of mixture components is (in the worst case) exponential in the number of discrete network variables. 
> 根据定义，条件线性高斯模型不允许连续变量有离散的子变量
> CLG 模型将联合分布定义为了一个加权混合高斯分布，each instatiation of the discrete network variables 都贡献一个高斯成分，该高斯成分的权重就是该 instantiation 的概率
> 因此高斯成分的数量实际上会与离散变量的数量成指数比例

Finally, we address the case of a discrete child with a continuous parent. The simplest model is a threshold model. Assume we have a binary discrete variable $U$ with a continuous parent $Y$ . We may want to deﬁne: 
> 对于具有连续父变量的离散变量，最简单的方式就是 threshold 模型

$$
P(u^{1})=\left\{\begin{array}{l l}{0.9\qquad\qquad y\leq65}\\ {0.05\qquad\qquad\mathrm{otherwise}.}\end{array}\right.
$$ 
Such a model may be appropriate, for example, if $Y$ is the temperature (in Fahrenheit) and $U$ is the thermostat turning the heater on. 

The problem with the threshold model is that the change in probability is discontinuous as a function of $Y$ , which is both inconvenient from a mathematical perspective and implausible in many settings. However, we can address this problem by simply using the logistic model or its multinomial extension, as deﬁned in deﬁnition 5.9 or deﬁnition 5.10. 
> threshold 模型的问题在于概率的变化是连续父变量的不连续函数，一种办法是 logistic model 或者其多项式拓展

Figure 5.14 shows how a multinomial CPD can be used to model a simple sensor that has three values: low , medium and high . The probability of each of these values depends on the value of the continuous parent $Y$ . As discussed in section 5.4.2, we can easily accommodate a variety of noise models for the sensor: we can make it less reliable in borderline situations by making the transitions between regions more moderate. It is also fairly straightforward to generalize the model to allow the probabilities of the diferent values in each of the regions to be values other than 0 or 1. 

As for the conditional linear Gaussian CPD, we address the existence of discrete parents for $Y$ by simply introducing a separate set of parameters for each instantiation of the discrete parents. 
## 5.6 Conditional Bayesian Networks 
The previous sections all describe various compact representations of a CPD. Another very useful way of compactly representing a conditional probability distribution is via a Bayesian network fragment. We have already seen one very simple example of this idea: Our decomposition of the noisy-or CPD for the Letter variable, shown in ﬁgure 5.9. There, our decomposition used a Bayesian network to represent the internal model of the Letter variable. The network included explicit variables for the parents of the variable, as well as auxiliary variables that are not in the original network. This entire network represented the CPD for Letter . In this section, we generalize this idea to a much wider setting. 
> 之前的部分描述了 CPD 的多种紧凑表示
> 另一种紧凑表示 CPD 的方式是 Bayesian network fragment，例如我们的对噪声或模型的分解
> 即 fig5.9中，我们用 Bayesian network 表示变量 $L$ 的 internal model，这个网络包含了 $L$ 变量的直接父变量，以及原来网络中没有的辅助变量
> 本节我们将该思想拓展到更一般的设定下

Note that the network fragment in this example is not a full Bayesian network. In particular, it does not specify a probabilistic model — parents and a CPD — for the parent variables Questions and FinalPaper . This network fragment speciﬁes not a joint distribution over the variables in the fragment, but a conditional distribution of Letter given Questions and FinalPaper . More generally, we can deﬁne the following: 
> fig5.9中的 network fragment 并不是一个完全的 Bayesian network，network fragment 中的信息并没有帮助指定其中的父变量的条件概率分布
> network fragment 并没有指定 fragment 中所有变量的联合分布，而是仅仅指定了关于一个变量在给定其 parent variables 时的条件概率分布

**Definition 5.17** conditional Bayesian network
$A$ conditional network $\mathcal{B}$ over $Y$ given $X$ is deﬁned as a direc acyclic graph $\mathcal{G}$ whose nodes are $X\cup Y\cup Z$ , where $X,Y,Z$ ar disjoint. The variables in X are alled inputs, the variables in $Y$ outputs , and the variables in Z encapsulated . The variables in $X$ have no parents in $\mathcal{G}$ . The variables in $Z\cup Y$ are associated with a conditional probability distribution. The network deﬁnes a conditional distribution using a chain rule: 

$$
P_{\mathcal{B}}(Y,Z\mid X)=\prod_{X\in Y\cup Z}P(X\mid\mathrm{Pa}_{X}^{\mathcal{G}}).
$$ 
The distribution $P_{\mathcal{B}}(Y\mid X)$ is deﬁned as the marginal of $P_{\mathcal{B}}(Y,Z\mid X)$ : 

$$
P_{\mathcal{B}}(Y\mid X)=\sum_{Z}P_{\mathcal{B}}(Y,Z\mid X).
$$ 

> 定义：
> 给定 $\pmb X$，在 $\pmb Y$ 上的条件贝叶斯网络定义为一个有向无环图 $\mathcal G$，节点为 $\pmb X\cup \pmb Y \cup \pmb Z$，其中 $\pmb X, \pmb Y, \pmb Z$ 不相交
> $\pmb X$ 中的变量称为输入，$\pmb Y$ 中的称为输出
> 输入变量在图中没有父节点，$\pmb Z\cup \pmb Y$ 中的变量则都和一个条件概率分布相关联，即：

$$
P_{\mathcal{B}}(\pmb Y,\pmb Z\mid \pmb X)=\prod_{X\in \pmb Y\cup \pmb Z}P(X\mid\mathrm{Pa}_{X}^{\mathcal{G}}).
$$
> 分布 $P_{\mathcal B}(\pmb Y \mid \pmb X)$ 定义为 $P_{\mathcal B}(\pmb Y, \pmb Z\mid \pmb X)$ 的边际分布：

$$
P_{\mathcal{B}}(\pmb Y\mid \pmb X)=\sum_{\pmb Z}P_{\mathcal{B}}(\pmb Y,\pmb Z\mid \pmb X).
$$


The conditional random ﬁeld of section 4.6.1 is the undirected analogue of this deﬁnition. 
> 条件随机场就是该概念的无向图版本

The notion of a conditional BN turns out to be useful in many settings. In particular, we can use it to deﬁne an encapsulated CPD. 
> 条件贝叶斯网络可以用于定义 encapsulated CPD

**Definition 5.18**
Let $Y$ be a random variable with $k$ parents $X_{1},\ldots,X_{k}$ . The CPD $P(Y\mid X_{1},.\,.\,.\,,X_{k})$ is an encapsulated CPD if it is represented using a conditional Bayesian network over Y given $X_{1},\ldots,X_{k}$ . 
> 定义：
> 令 $Y$ 是具有 $k$ 个父变量 $X_1,\dots, X_k$ 的随机变量，如果条件概率分布可以使用一个在给定 $X_1,\dots, X_k$ 时在 $Y$ 上的条件贝叶斯网络表示，则 CPD $P (Y\mid X_1,\dots, X_k)$ 就是一个 encapsulated CPD

At some level, it is clear that the representation of an individual CPD for a variable $Y$ as a conditional Bayesian network ${\mathcal{B}}_{Y}$ es not add expressive power to the model. After we could simply take the network ${\mathcal{B}}_{Y}$ and “substitute it in” for the atomic CPD $P(Y\mid$ $\mathrm{Pa}_{Y}$ ) . 
One key advantage of the encapsulated representation over a more explicit model is that the encapsulation can simplify the model signiﬁcantly from a cognitive perspective. Consider again our noisy-or model. Externally, to the rest of the network, we can still view Letter as a single variable with its two parents: Questions and FinalPaper . All of the internal structure is encapsulated, so that, to the rest of the network, the variable can be viewed as any other variable. In particular, a knowledge engineer specifying the network does not have to ascribe meaning to the encapsulated variables. 
> 封装的 CPD 隐藏了内部的细节，使得我们可以直接将其视作一个原子 CPD 看待，外部的成分只需要关心它的输入和输出变量

The encapsulation advantage can be even more signiﬁcant when we want to describe a complex system where components are composed of other, lower-level, subsystems. When specifying a model for such a system, we would like to model each subsystem separately, without having to consider the internal model of its lower level components. 

In particular, consider a model for a physical device such as a computer; we might construct such a model for fault diagnosis purposes. When modeling the computer, we would like to avoid thinking about the detailed structure and fault models of its individual components, such as the hard drive, and within the hard drive the disk surfaces, the controller, and more, each of which has yet other components. By using an encapsulated CPD, we can decouple the model of the computer from the detailed model of the hard drive. We need only specify which global aspects of the computer state the hard drive behavior depends on, and which it inﬂuences. Furthermore, we can hierarchically compose encapsulated CPDs, modeling, in turn, the hard drive’s behavior in terms of its yet-lower-level components. 

In ﬁgure 5.15 we show a simple hierarchical model for a computer system. This high-level model for a computer, ﬁgure 5.15a, uses encapsulated CPDs for Power-Source , Motherboard , Hard- Drive , Printer , and more. The Hard-Drive CPD has inputs Temperature , Age and OS-Status , and the outputs Status and Full . Although the hard drive has a rich internal state, the only aspects of its state that inﬂuence objects outside the hard drive are whether it is working properly and whether it is full. The Temperature input of the hard drive in a computer is outside the probabilistic model and will be mapped to the Temperature parent of the Hard-Drive variable in the computer model. A similar mapping happens for other inputs. 

The Hard-Drive encapsulated network, ﬁgure 5.15b, in turn uses encapsulated CPDs for Controller , Surface , Drive-Mechanism , and more. The hierarchy can continue as necessary. In this case, the model for the variable Motor (in the Drive-Mechanism ) is “simple,” in that none of its CPDs are encapsulated. 

One obvious observation that can be derived from looking at this example is that an encapsulated CPD is often appropriate for more than one variable in the model. For example, the encapsulated CPD for the variable Surface1 in the hard drive is almost certainly the same as the CPDs for the variables Surface2 , Surface2 , and Surface4 . Thus, we can imagine creating a template of an encapsulated CPD, and reusing it multiple times, for several variables in the model. This idea forms the basis for a framework known as object-oriented Bayesian networks . 
## 5.7 Summary 
In this chapter, we have shown that our ability to represent structure in the distribution does not end at the level of the graph. In many cases, here is important structure within the CPDs that we wish to make explicit. 

In particular, we discussed several important types of discrete structured CPDs. 

- deterministic functions;
- asymmetric, or context speciﬁc, dependencies;
- cases where diferent inﬂuences combine independently within the CPD, including noisy-or, logistic functions, and more. 

In many cases, we showed that the additional structure provides not only a more compact parameter iz ation, but also additional independencies that are not visible at the level of the original graph. 
> 本章讨论了离散结构 CPD 的重要类型：
> - 确定性函数
> - 不对称 or 针对上下文
> - 不同影响独立地在 CDP 中结合，包括噪声或、logistic function

As we discussed, the idea of structured CPDs is critical in the case of continuous variables, where a table-based representation is clearly irrelevant. We discussed various representations for CPDs in hybrid (discrete/continuous) networks, of which the most common is the linear Gaussian representation. For this case, we showed some important connections between the linear Gaussian representation and multivariate Gaussian distributions. 
> 还讨论了混合网络中的 CPD 表示，最常见的是线性高斯表示

Finally, we discussed the notion of a conditional Bayesian network, which allows us to decompose a conditional probability distribution recursively, into another Bayesian network. 
> 条件贝叶斯网络：允许我们将条件概率分布递归地分解为另一个贝叶斯网络
# 6 Template-Based Representations 
## 6.1 Introduction 
A probabilistic graphical model (whether a Bayesian network or a Markov network) specifies a joint distribution over a fixed set $\mathcal{X}$ of random variables. This fixed distribution is then used in a variety of diferent situations. For example, a network for medical diagnosis can be applied to multiple patients, each with diferent symptoms and diseases. However, in this example, the diferent situations to which the network is applied all share the same general structure — all patients can be described by the same set of attributes, only the attributes’ values difer across patients. We call this type of model variable-based , since the focus of the representation is a set of random variables. 
> 一个概率图模型指定了随机变量集合 $\mathcal X$ 上的一个联合分布，该分布的对象也限制在随机变量集合 $\mathcal X$ 上，我们称这种模型是基于变量的，因为表示聚焦于随机变量集合

In many domains, however, the probabilistic model relates to a much more complex space than can be encoded as a fixed set of variables. In a temporal setting, we wish to represent distributions over systems whose state changes over time. For example, we may be monitoring a patient in an intensive care unit. In this setting, we obtain sensor readings at regular intervals — heart rate, blood pressure, EKG — and are interested in tracking the patient’s state over time. As another example, we may be interested in tracking a robot’s location as it moves in the world and gathers observations. Here, we want a single model to apply to trajectories of diferent lengths, or perhaps even infinite trajectories. 
> 在很多领域，我们往往不能将概率模型与一个固定的随机变量集合相关
> 例如在时序设定下，我们在一个状态随着时间变化的系统上表示分布，
> 又或者我们希望追踪一个机器人移动时的位置和它的观察
> 此时，我们希望对于不同的长度的轨迹，甚至无限长度的轨迹应用相同的一个模型

An even more complex setting arises in our Genetics example; here, each pedigree (family tree) consists of an entire set of individuals, all with their own properties. Our probabilistic model should encode a joint distribution over the properties of all of the family members. Clearly, we cannot define a single variable-based model that applies universally to this application: each family has a diferent family tree; the networks that represent the genetic inheritance process within the tree have diferent random variables, and diferent connectivities. Yet the mechanism by which genes are transmitted from parent to child is identical both for diferent individuals within a pedigree and across diferent pedigrees. 

In both of these examples, and in many others, we might hope to construct a single, com- pact model that provides a template for an entire class of distributions from the same type: trajectories of diferent lengths, or diferent pedigrees. In this chapter, we define representations that allow us to define distributions over richly structured spaces, consisting of multiple objects, interrelated in a variety of ways. These template-based representations have been used in two main settings. The first is temporal modeling, where the language of dynamic Bayesian networks allows us to construct a single compact model that captures the properties of the system dy- namics, and to produce distributions over diferent trajectories. The second involves domains such as the Genetics example, where we have multiple objects that are somehow related to each other. Here, various languages have been proposed that allow us to produce distributions over diferent worlds, each with its own set of individuals and set of relations between them. 
> 我们希望构建单个紧凑的模型，为一整类相同类型的分布（不同类型的轨迹、不同的血统）提供一个模板
> 本章介绍基于模板的表示，它允许我们在更丰富的，由多个相关的对象构成的结构空间之上定义分布，这类表示主要用于：时序建模（使用动态贝叶斯网络捕获系统动态，在不同的轨迹上生成不同分布）；基因分析

Once we consider higher-level representations that allow us to model objects, relations, and probabilistic statements about those entities, we open the door to very rich and expressive languages and to queries about concepts that are not even within the scope of a variable-based framework. For example, in the Genetics example, our space consists of multiple people with diferent types of relationships such as Mother , Father-of , and perhaps Married . In this type probability space, we can also express uncertainty about the identity of Michael’s father, or how many children Great-aunt Ethel had. Thus, we may wish to construct a probability distribution over a space consisting of distinct pedigree structures, which may even contain a varying set of objects. As we will see, this richer modeling language will allow us both to answer new types of queries, and to provide more informed answers to “traditional” queries. 

## 6.2 Temporal Models 
Our focus in this section is on modeling dynamic settings, where we are interested in reasoning about the state of the world as it evolves over time. We can model such settings in terms of a system state , whose value at time $t$ is a snapshot of the relevant attributes (hidden or observed) of the system at time $t$ . We assume that the system state is represented, as usual, as an assignment of value some s of random riables $\mathcal{X}$ . We use $X_{i}^{(t)}$ to represent the instantiation of the variable $X_{i}$ at time t . Note that $X_{i}$ itself is no longer a variable that takes a value; rather, it is a template variable . This template is instantiated at diferent points in time $t_{;}$ , and each $X_{i}^{(t)}$ is a variable that takes a value in $V a l(X_{i})$ . For a set of variables $X\subseteq\mathcal{X}$ , we use $X^{(t_{1}:t_{2})}\ (t_{1}<t_{2})$ to denote the set of variables $\{X^{(t)}:t\in[t_{1},t_{2}]\}$ . As usual, we use the notation $\mathbf{\boldsymbol{x}}^{(t:t^{\prime})}$ for an assignment of values to this set of variables. 
> 在动态设定下，我们对随着时间变化的状态进行分析，我们将其建模为一个系统状态，它在时间 $t$ 的值是系统此时相关属性（观察到的或隐藏的）的一个快照
> 我们假设系统状态通过对于一个随机变量集合 $\mathcal X$ 的一个赋值表示，我们使用 $X_i^{(t)}$ 表示随机变量 $X_i$ 在 $t$ 时的实例
> 注意，$X_i$ 本身已经不是一个直接取某个值的随机变量，而是一个模板变量，该模板在特定的时间点 $t$ 实例化，而每个实例 $X_i^{(t)}$ 是一个取 $Val (X_i)$ 中的某个值的随机变量
> 我们使用 $X^{(t_1:t_2)}$ 表示随机变量集合 $\{X^{(t)}: t\in [t_1, t_2]\}$，使用 $\pmb x^{(t: t')}$ 表示赋值

Each “possible world” in our probability space is now a trajectory : an assignment of values to each variable $X_{i}^{(t)}$ for each relevant time $t$ . Our goal therefore is to represent a joint distribution over such trajectories. Clearly, the space of possible trajectories is a very complex probability space, so representing such a distribution can be very difcult. We therefore make a series of simplifying assumptions that help make this representational problem more tractable. 
> 在我们的概率空间中，此时对于一个系统的观测实际是一个轨迹，包含了每个相关时刻 $t$ 上对于每个变量 $X_i^{(t)}$ 的赋值
> 我们需要表达该轨迹上的联合分布（概率空间是所有可能的轨迹），为此，需要做出一系列简化的假设

Example 6.1
Consider a vehicle localization task, where a moving car tries to track its current location using the data obtained from a, possibly faulty, sensor. The system state can be encoded (very simply) using the: Location — the car’s current location, Velocity — the car’s current velocity, Weather — the current weather, Failure — the failure status of the sensor, and Obs — the current observation. We have one such set of variables for every point t . 
A joint probability distribution over all of these sets defines a probability distribution over trajectories of the car. Using this distribution, we can answer a variety queries, such as: Given a sequence of observations about the car, where is it now? Where is it likely to be in ten minutes? Did it stop at the red light? 

### 6.2.1 Basic Assumptions 
Our first simplification is to discretize the timeline into a set of time slices : measurements of the system state taken at intervals that are regularly spaced with a predetermined time granularity $\Delta$ . Thus, we can now restrict our set of random variables to $\mathcal{X}^{(\bar{0})},\mathcal{X}^{(1)},...,$ where $\mathcal{X}^{(t)}$ are the ground random variables that represent the system state at time $t\!\cdot\!\Delta$ . For example, in the patient monitoring example, we might be interested in monitoring the patient’s state every second, so that $\Delta\,=\,1s e c$ . This assumption simplifies our problem from representing distributions over a continuum of random variables to representing distributions over countably many random variables, sampled at discrete intervals. 
> 第一个简化：将时间线离散化为时间片段集合
> 此时对于系统状态的度量按照规律的间隔观测，间隔是预定义的时间粒度 $\Delta$
> 此时的随机变量集合记为 $\mathcal X^{(0)},\dots, \mathcal X^{(t)}$，其中 $\mathcal X^{(t)}$ 即系统在时间 $t\cdot \Delta$ 时的基础随机变量集

Consider a distribution over trajectories sampled over a prefix of time $t\;=\;0,.\,.\,.\,,T\;-$ $P(\mathcal{X}^{(0)},\mathcal{X}^{(1)},\ldots,\mathcal{X}^{(T)})$ , often abbreviated $P(\bar{\mathcal{X}}^{(0:T)})$ . We can reparameterize the distribution using the chain rule for probabilities, in a direction consistent with time: 

$$
P(\mathcal{X}^{(0:T)})=P(\mathcal{X}^{(0)})\prod_{t=0}^{T-1}P(\mathcal{X}^{(t+1)}\mid\mathcal{X}^{(0:t)}).
$$

Thus, the distribution over trajectories is the product of conditional distributions, for the variables in each time slice given the preceding ones. 
> 考虑时间 $t=0,\dots, T$ 上的轨迹上的分布 $P (\mathcal X^{(0)}, \mathcal X^{(1)}, \dots, \mathcal X^{(T)})$，一般简写为 $P (\mathcal X^{(0:T)})$
> 我们使用链式法则，将该联合分布重参数化，方向沿着时间方向：

$$
P(\mathcal X^{(0:T)}) = P(\mathcal X^{(0)})\prod_{t=0}^{T-1}P(\mathcal X^{(t+1)}\mid \mathcal X^{(0:t)}).
$$

> 此时，我们将轨迹上的分布分解为了条件概率分布（给定先前的时间片的变量，当前时间片的变量从属于的分布）的乘积

We can considerably simplify this formulation by using our usual tool — conditional independence assumptions.  One very natural approach is to assume that the future is conditionally independent of the past given the present: 
> 显然，我们可以利用条件独立性假设简化该乘积，一种非常自然的方法就是假设在给定当下的条件下，未来条件独立于过去

**Definition 6.1** Markov assumption 
We hat a dynamic system over the template variables $\mathcal{X}$ satisfies the Markov assumption if, for all $t\geq0$ , 

$$
(\mathcal{X}^{(t+1)}\perp\mathcal{X}^{(0:(t-1))}\mid\mathcal{X}^{(t)}).
$$ 
Markov chain system Such systems are called Markov chain . 
> 定义：Markov assumption
> 对于模板变量 $\mathcal X$ 上的一个动态系统，如果对于所有的 $t\ge 0$，有

$$
(\mathcal X^{(t+1)}\perp \mathcal X^{(0:(t-1))}\mid \mathcal X^{(t)})
$$

> 我们称该系统满足 Markov 假设，并且该系统称为 Markov chain

Th kov a ions states that the variables in $\mathcal{X}^{(t+1)}$ cannot depend directly on variables in $\mathcal{X}^{(t^{\prime})}$ for t $t^{\prime}~<~t$ . If we were to draw our dependency model as an (infinite) Bayesian network, the Markov assumption would correspond to the constraint on the graph that there are no edges into $\mathcal{X}^{(t+1)}$ from variables in time slices $t\mathrm{~-~}1$ or earlier. 
> Markov 假设表明 $\mathcal X^{(t+1)}$ 中的变量不能直接依赖于 $\mathcal X^{(t')} (t' < t)$ 中的变量
> 如果要将我们的依赖模型表现为一个无限的 Bayesian 网络，Markov 假设对应于图中时间片 $t-1$ 以及以前的变量没有直接到 $\mathcal X^{(t+1)}$ 的边

Like many other conditional independence assumptions, the Markov assumption allows us to define a more compact representation of the distribution: 
> Markov 假设帮助我们简化了轨迹的联合分布表示

$$
P(\mathcal{X}^{(0)},\mathcal{X}^{(1)},\ldots,\mathcal{X}^{(T)})=P(\mathcal{X}^{(0)})\prod_{t=0}^{T-1}P(\mathcal{X}^{(t+1)}\mid\mathcal{X}^{(t)}).\tag{6.1}
$$

Like any conditional independence assumption, the Markov assumption may or may not be reasonable in a particular setting. 
> 在特定的设置下，Markov 假设可以是合理的，也可以是不合理的

Example 6.2 
time $t$ , because the previous locations give us information about the object’s direction of motion and speed. By adding Velocity, we make the Markov assumption closer to being satisfied. If, however, the driver is more likely to accelerate and decelerate sharply in certain types of weather (say heavy winds), then our $V,L$ model does not satisfy the Markov assumption relative to $V$ ; we can, again, make the model more Markovian by adding the Weather variable. Finally, in many cases, a sensor failure at one point is usually accompanied with a sensor failure at nearby time points, rendering nearby Obs variables correlated. By adding all of these variables into our state model, we define a state space whereby the Markov assumption is arguably a reasonable approximation. 
> 该例讲述了如何通过添加变量使得状态空间中 Markov 假设是近似成立的

Philosophically, one might argue whether, given a sufciently rich description of the world state, the past is independent of the future given the present. However, that question is not central to the use of the Markov assumption in practice. Rather, **we need only consider whether the Markov assumption is a sufciently reasonable approximation to the dependencies in our distribution. In most cases, if we use a reasonably rich state description, the approximation is quite reasonable.** In other cases, we can also define models that are semi-Markov , where the independence assumption is relaxed (see exercise 6.1). 
> 不考虑哲学，在实践中，我们仅仅需要考虑 Markov 假设是否对于我们分布中的依赖是一个合理的近似，在大多数情况下，如果我们使用丰富的状态描述，该近似是十分合理的
> 我们也可以定义半 Markov 的模型，松弛独立性假设

Because the process can continue indefinitely, equation (6.1) still leaves us with the task of acquiring an infinite set of conditional distributions, or a very large one, in the case of finite-horizon processes. Therefore, we usually make one last simplifying assumption: 
> 因为该过程会随着时间无限前进，(6.1) 中将会包含无限个条件概率分布，因此，我们需要再做出一个简化假设

**Definition 6.2**  stationary dynamical system
We say that a Markovian dynamic system is stationary (also called time invariant or homogeneous ) if $P(\dot{\mathcal{X}}^{(t+1)}\mid\mathcal{X}^{(t)})$ is the same for all $t$ . is case, we can represent the process using a transition model $P(\mathcal{X}^{\prime}\mid\mathcal{X})$ , so that, for any $t\geq0$ , 

$$
P(\mathcal{X}^{(t+1)}=\xi^{\prime}\mid\mathcal{X}^{(t)}=\xi)=\ P(\mathcal{X}^{\prime}=\xi^{\prime}\mid\mathcal{X}=\xi).
$$

> 定义：
> 如果 $P (\mathcal X^{(t+1)}\mid \mathcal X^{(t)})$ 对于全部的 $t$ 都相同，我们称 Markov chain 动态系统是平稳的/时间不变的/同质的
> 此时，我们用一个转移模型 $P (\mathcal X'\mid \mathcal X)$ 就可以表示 Markov 过程

### 6.2.2 Dynamic Bayesian Networks 
The Markov and stationarity assumptions described in the previous section allow us to represent the probability distribution over infinite trajectories very compactly: We need only represent the initial state distribution and the transition model $P(\mathcal{X}^{\prime}\mid\mathcal{X})$ . This transition model is a conditional probability distribution, which we can represent using a conditional Bayesian network, as described in section 5.6. 
> Markov 和平稳假设允许我们紧凑表示无限轨迹上的概率分布：我们仅需要表示初始分布和转移模型 $P (\mathcal X' \mid \mathcal X)$
> 转移模型是 CPD，我们可以用条件贝叶斯网络表示

Example 6.3 
Let us return to the setting of example 6.1. Here, we might want to represent the system dynamics using the model shown in figure 6.1a, the current observation depends on the car’s location (and the map, which is not explicitly modeled) and on the error status of the sensor. Bad weather makes the sensor more likely to fail. And the car’s location depends on the previous position and the velocity. All of the variables are interface variables except for Obs, since we assume that the sensor observation is generated at each time point independently given the other variables. 

This type of conditional Bayesian network is called a 2-time-slice Bayesian network (2-TBN) . 

![[Graphical Probabilistic Theory-Fig6.1.png]]


**Definition 6.3** 2-TBN interface variable 
A 2-time-slice Bayesian network (2-TBN) for a process over X is a conditional Bayesian network 2-TBN over X0 given XI, where XI ⊆ X is a set of interface variables.
> 定义： 2-时间片贝叶斯网络
> 对于 $\mathcal X$ 上的过程的 2时间片贝叶斯网络定义为在 $\mathcal X'$ 上的给定 $\mathcal X_I$ 的条件贝叶斯网络，其中 $\mathcal X_I \subseteq \mathcal X$ 是接口变量的集合（接口变量在不同的时间片上有不同的值，它们在时间上的延续使得我们可以从一个时间点推断到另一个时间点，因此它们是不同时间片之间的桥梁）
 
As a reminder, in a co tional Bayesian network, only the variabl $\mathcal{X}^{\prime}$ have parents or CPDs. 
> 注意条件贝叶斯网络仅定义了 $\mathcal X'$ 的条件概率分布，以及也仅 $\mathcal X'$ 有父变量
 
The interface var $\mathcal{X}_{I}$ are those variables whos alues at time t have a direct ef t on the variables at time $t+1$ . Thus nly the variables in X $\mathcal{X}_{I}$ can be parents of variables in X $\mathcal{X}^{\prime}$ . In our example, all variables except O are in the interface. 
> 接口变量即它们在时间 $t$ 的值会影响时间 $t+1$ 的变量值的变量，因此它们会作为2-TBN 中的父变量，非接口变量不会影响到 $\mathcal X'$

Overall, the 2-TBN represents the conditional distribution: 

$$
P(\mathcal{X}^{\prime}\mid\mathcal{X})=P(\mathcal{X}^{\prime}\mid\mathcal{X}_{I})=\prod_{i=1}^{n}P(X_{i}^{\prime}\mid\mathrm{Pa}_{X_{i}^{\prime}}).
$$ 
For each template variable $X_{i}$ , the CPD $P(X_{i}^{\prime}\mid\mathrm{Pa}_{X_{i}^{\prime}})$ is a template factor : it will be instantiated multiple times within the model, for multiple variables $X_{i}^{(t)}$ (and their parents). 

> 2-TBN 表示了 CPD：

$$
P(\mathcal X'\mid \mathcal X) = P(\mathcal X'\mid \mathcal X_I) = \prod_{i=1}^n P(X_i'\mid \text{Pa}_{X_i'})
$$

> 对于 $\mathcal X'$ 中的每个模板变量 $X_i'$，CPD $P (X_i' \mid \text{Pa}_{X_i'})$ 都是一个模板因子，在不同的时间点上，它会被实例化多次，实例化为 $X_i^{(t)}$ 和它们的父变量

Perhaps the simplest nontrivial example of a temporal model of this kind is the hidden Markov model (see section 6.2.3.1). It has a single state variable $S$ and a single observation variable $O$ . Viewed as a DBN, an HMM has the structure shown in figure 6.2. 
> 这类时序模型最简单的例子是隐 Markov 模型，HMM 有单个状态变量 $S$ 和单个观察变量 $O$

Example 6.4 
Consider a robot moving around in a grid. Most simply, the robot is the only aspect of the world that is changing, so that the state of the system $S$ is simply the robot’s position. Our transition model $P(S^{\prime}\mid S)$ then repre nts the probability that, if the robot is in some state (position) $s$ , it will move to another state s $s^{\prime}$ . Our task is to keep track of the robot’s location, using a noisy sensor (for example, a sonar) whose value depends on the robot’s location. The observation model $P(O\mid S)$ tells us the probability of making a particular sensor reading o given that the robot’s current position is $s$ . (See box 5. E for more details on the state transition and observation models in a real robot localization task.) 

In a 2-TBN, some of the edges are inter-time-slice edges , going between time slices, whereas others are intra-time-slice edges , connecting variables in the same time slice. Intuitively, our decision of how to relate two variables depends on how tight the coupling is between them. If the efect of one variable on the other is immediate — much shorter than the time granularity in the model — the inﬂuence would manifest (roughly) within a time slice. If the efect is slightly longer-term, the inﬂuence manifests from one time slice to the next. In our simple examples, the efect on the observations is almost immediate, and hence is modeled as an intra-time-slice edge, whereas other dependencies are inter-time-slice. In other examples, when time slices have a coarser granularity, more efects might be short relative to the length of the time slice, and so we might have other dependencies that are intra-time-slice. 
> 二时间片贝叶斯网络中，一些边属于时间片间边，即连接了两个时间片的节点，一些边属于时间片内边，连接了相同时间片内的节点
> 我们构建网络时，如何关联两个变量应取决于它们之间的联系有多紧密，如果二者之间的影响是直接的——比时间粒度更短，则影响可以在时间片内表示；如果影响是长期的，则可以在时间片之间表示

Many of the inter-time-slice edges are of the form $X\rightarrow X^{\prime}$ . Such edges are called persistence edges , and they represent the tendency of the variable X (for example, sensor failure) to persist over time with high probability. A variable $X$ for which we have an edge $X\rightarrow X^{\prime}$ in the 2-TBN is called a persistent variable . 
> 许多时间片间的边的形式是 $X\rightarrow X'$，这类边被称为持续边，表示变量 $X$ 以较高的概率随着时间持续的趋势
> 在 2-TBN 中，我们称具有形式为 $X\rightarrow X'$ 的变量为持续变量

Based on the stationarity property, a 2-TBN defines the probability distribution $P(\mathcal{X}^{(t+1)}\mid$ $\mathcal{X}^{(t)}$ ) for any $t$ . Given a distribution over the initial states, we can unroll the network over sequences of any length, to define a Bayesian network that induces a distribution over trajectories of that length. In these networks, all the copies of the variable $X_{i}^{(t)}$ for $t>0$ have the same dependency structure and the same CPD. Figure 6.1 demonstrates a transition model, initial state network, and a resulting unrolled DBN, for our car example. 
> 如果平稳性质成立，一个 2-TBN 实际上为任意 $t$ 定义了概率分布 $P (\mathcal X^{(t+1)}\mid \mathcal X^{(t)})$，此时，给定初始状态上的分布，我们可以展开任意长度的序列上的网络，得到针对该长度的轨迹上的贝叶斯网络
> 该网络中，所有 $X_i^{(t)}, t> 0$ 具有相同的依赖结构和相同的 CPD

**Definition 6.4** dynamic Bayesian network
$A$ dynamic Bayesian network (DBN) is a pair $\langle\mathcal{B}_{0},\mathcal{B}_{\rightarrow}\rangle$ , where ${\mathcal B}_{0}$ is a Bayesian network over $\mathcal{X}^{(0)}$ , representing the initial distribution over and $\mathcal{B}_{\rightarrow}$ is a 2-TBN for the process. For any desired time span $T\geq0$ , the distribution over X $\mathcal{X}^{(0:T)}$ is defined as $a$ unrolled Bayesian network , where, for any $i=1,\dots,n$ : 

• the structure and CPDs of X(0) i are the same as those for Xi in B0,
• the structure and CPD of X(t) i for t > 0 are the same as those for Xi0 in B!.

> 定义：
> 动态贝叶斯网络定义为 $\langle \mathcal B_0, \mathcal B_{\rightarrow} \rangle$，其中 $\mathcal B_0$ 是 $\mathcal X^{(0)}$ 上的贝叶斯网络，表示状态的初始分布，$\mathcal B_{\rightarrow}$ 是一个 2时间片贝叶斯网络
> 对于任意时间片 $T\ge 0$，$\mathcal X^{(0:T)}$ 上的分布定义为一个展开的贝叶斯网络，其中，对于任意 $i = 1,\dots , n$，满足：
> - $X_i^{(0)}$ 的结构和 CPDs 和 $X_i$ 在 $\mathcal B_0$ 中的结构和 CPDs 相同
> - $X_i^{(t)}, t> 0$ 的结构和 CPD 和 $X_i'$ 在 $\mathcal B_{\rightarrow}$ 中的结构和 CPD 相同

Thus, we can view a DBN as a compact representation from which we can generate an infinite set of Bayesian networks (one for every $T>0$ ). 
> 因此，我们可以将 DBN 视为可以生成无限个贝叶斯网络的紧凑表示

![[Graphical Probabilistic Theory-Fig6.3.png]]

Figure 6.3 shows two useful classes of DBNs that are constructed from HMMs. A factorial left, is a DBN whose 2-TBN has the s cture of a set of chains $X_{i}\ \rightarrow\ X_{i}^{\prime}$ $(i=1,.\,.\,.\,,n)$ ), with a single (always) observed variable Y $Y^{\prime}$ , which is a child of all the variables $X_{i}^{\prime}$ . This type of model is very useful in a variety of applications, for example, when several sources of sound are being heard simultaneously through a single microphone.
A coupled HMM ,  on the right, is also constructed from a set of chains $X_{i}$ , but now, each chain is an HMM with its private own observation variable $Y_{i}$ . The chains now interact directly via their state variables, with each chain afecting its adjacent chains. These models are also useful in a variety of applications. For example, consider monitoring the temperature in a building over time (for example, for fire alarms). Here, $X_{i}$ might be the true (hidden) state of the i th room, and $Y_{i}$ the value returned by the room’s own temperature sensor. In this case, we would expect to have interactions between the hidden states of adjacent rooms. 
> 两类常见的动态贝叶斯网络见 Fig6.3
> 左边是因子隐马尔可夫模型，该模型中，其 2-TBN 的结构是一系列的 $X_i \rightarrow X_i' ( i = 1, \dots, n)$ ，以及单个被观察到的变量 $Y'$，$Y'$ 是所有 $X_i'$ 的子变量
> 建模 factorial HMM 的情况例如：多个声源通过单个麦克风传出
> 右边是耦合隐马尔可夫模型，其结构同样包含一系列的 $X_i \rightarrow X_i' (i = 1,\dots , n)$，不同的是此时每个 $X_i \rightarrow X_i'$ 都构成一个隐马尔可夫模型，即都具有各自的观察变量 $Y_i$；并且，此时各条链还会直接通过状态变量交互，也就是每一条链都会影响其相邻的链

In DBNs, it is often the case that our observation pattern is constant over time. That is, we can partition the varia $\mathcal{X}$ into disjoint subsets $X$ and $^o$ , such that the variables in $X^{(t)}$ are always hidden and $O^{(t)}$ are always observed. For uniformity of presentation, we generally make this assumption; however, the algorithms we present also apply to the more general case. 
> DBN 建模中，我们的观察模式往往对于时间是不变的，也就是说，我们可以将 $\mathcal X$ 划分为两个不相交的集合 $X, O$，其中变量 $X^{(t)}$ 总是隐变量，变量 $O^{(t)}$ 总是被观察的变量
> 这是我们常常会做的假设

A DBN can enable fairly sophisticated reasoning patterns. 

Example 6.5
By explicitly encoding sensor failure, we allow the agent to reach the conclusion that the sensor has failed. Thus, for example, if we suddenly get a reading that tells us something unexpected, for example, the car is suddenly 15 feet to the left of where we thought it was 0.1 seconds ago, then in addition to considering the option that the car has suddenly teleported, we will also consider the option that the sensor has simply failed. Note that the model only considers options that are built into it. If we had no “sensor failure” variable, and had the sensor reading depend only on the current location, then the diferent sensor readings would be independent given the car’s trajectory, so that there would be no way to explain correlations of unexpected sensor readings except via the trajectory. Similarly, if the system knows (perhaps from a weather report or from prior observations) that it is raining, it will expect the sensor to be less accurate, and therefore be less likely to believe that the car is out of position. 

Box 6. A — Case Study: HMMs and Phylo-HMMs for Gene Finding. HMMs are a primary tool in algorithms that extract information from biological sequences. Key applications (among many) include: modeling families of related proteins within and between organisms, finding genes in DNA sequences, and modeling the correlation structure of the genetic variation between individuals in a population. We describe the second of these applications, as an illustration of the methods used. 

The DNA of an organism is composed of two paired helical strands consisting of a long sequence of nucleotides, each of which can take on one of four values — A, C, G, T; in the double helix structure, A is paired with T and C is paired with G, to form a base pair. The DNA sequence consists of multiple regions that can play diferent roles. Some regions are genes, whose DNA is transcribed into mRNA, some of which is subsequently translated into protein. In the translation process, triplets of base pairs, known as codons, are converted into amino acids. There are $4^{3}\,=\,64$ diferent codons, but only 20 diferent amino acids, so that the code is redundant. Not all transcribed regions are necessarily translated. Genes can contain exons, which are translated, and introns, which are spliced out during the translation process. The DNA thus consists of multiple genes that are separated by intergenic regions; and genes are themselves structured, consisting of multiple exons separated by introns. The sequences in each of these regions is characterized by certain statistical properties; for example, a region that produces protein has a very regular codon structure, where the codon triplets exhibit the usage statistics of the amino acids they produce. Moreover, boundaries between these regions are also often demarcated with sequence elements that help the cell determine where transcription should begin and end, and where translation ought to begin and end. Nevertheless, the signals in the sequence are not always clear, and therefore identifying the relevant sequence units (genes, exons, and more) is a difcult task. 

HMMs are a critical tool in this analysis. Here, we have a hidden state variable for each base pair, which denotes the type of region to which this base pair belongs. To satisfy the Markov assumption, one generally needs to refine the state space. For example, to capture the codon structure, we generally include diferent hidden states for the first, second, and third base pairs within a codon. This larger state space allows us to encode the fact that coding regions are sequences of triplets of base pairs, as well as encode the diferent statistical properties of these three positions. We can further refine the state space to include diferent statistics for codons in the first exon and in the last exon in the gene, which can exhibit diferent characteristics than exons in the middle of the gene. The observed state of the HMM naturally includes the base pair itself, with the observation model reﬂecting the diferent statistics of the nucleotide composition of the diferent regions. It can also include other forms of evidence, such as the extent to which measurements of mRNA taken from the cell have suggested that a particular region is transcribed. And, very importantly, it can contain evidence regarding the conservation of a base pair across other species. This last key piece of evidence derives from the fact that base pairs that play a functional role in the cell, such as those that code for protein, are much more likely to be conserved across related species; base pairs that are nonfunctional, such as most of those in the intergenic regions, evolve much more rapidly, since they are not subject to selective pressure. Thus, we can use conservation as evidence regarding the role of a particular base pair. 

One way of incorporating the evolutionary model more explicitly into the model is via a phylo- genetic HMM (of which we now present a simplified version). Here, we encode not a single DNA sequence, but the sequences of an entire phylogeny (or evolutionary tree) of related species. We let $X_{k,i}$ be the i th nucleotide for species $s_{k}$ . We also introduce a species-independent variable $Y_{i}$ denoting the functional role of the i th base pair (intergenic, intron, and so on). The base pair $X_{k,i}$ will depend on the corresponding base pair $X_{\ell,i}$ where $s_{\ell}$ is the ancestral species from which $s_{k}$ evolved. The parameters of this dependency will depend on the evolutionary distance between $s_{k}$ and $s_{l}$ (the extent to which $s_{k}$ has diverged) and on the rate at which a base pair playing a particular role evolves. For example, as we mentioned, a base pair in an intergenic region generally evolves much faster than one in a coding region. Moreover, the base pair in the third position in a codon also often evolves more rapidly, since this position encodes most of the redundancy between codons and amino acids, and so allows evolution without changing the amino acid composition. Thus, overall, we define $X_{k,i}\,\acute{s}$ parents in the model to be $Y_{i}$ (the type of region in which $X_{k,i}$ resides), $X_{k,i-1}$ (the previous nucleotide in species $s$ ) and $X_{\ell,i}$ (the i th nucleotide in the parent species $s_{\ell}$ ). This model captures both the correlations in the functional roles (as in the simple gene finding model) and the fact that evolution of a particular base pair can depend on the adjacent base pairs. This model allows us to combine information from multiple species in order to infer which are the regions that are functional, and to suggest a segmentation of the sequence into its constituent units. 

Overall, the structure of this model is roughly a set of trees connected by chains: For each i we have a tree over the variables $\{X_{k,i}\}_{s_{k}}$ , where the structure of the tree is that of the evolutionary tree; in addition, all of the $X_{k,i}$ are connected by chains to $X_{k,i+1}$ ; finally, we also have the variables $Y_{i}$ , which also form a chain and are parents of all of the $X_{k,i}$ . Unfortunately, the structure of this model is highly intractable for inference, and requires the use of approximate inference methods; see exercise 11.29. 

### 6.2.3 State-Observation Models 
An alternative way of thinking about a temporal process is as a state-observation model . In a state- observation model, we view the system as evolving naturally on its own, with our observations of it occurring in a separate process. This view separates out the system dynamics from our observation model, allowing us to consider each of them separately. It is particularly useful when our observations are obtained from a (usually noisy) sensor, so that it makes sense to model separately the dynamics of the system and our ability to sense it. 
> 另一种思考时序过程的方式是状态-观察模型
> 在状态-观察模型中，我们将模型视为自己自然演变，我们的观察发生在另一个过程中，因此，我们的观察模型和系统动态被分离开了，故我们可以分别独立考虑系统动态和观察
> 该模型在我们的观察具有噪声的情况下非常有用，此时分别建模系统动态和我们观察它的能力就十分合理

A state-observation model utilizes two independence assumptions: that the state variables evolve in a Markovian way, so that 

$$
(X^{(t+1)}\perp X^{(0:(t-1))}\mid X^{(t)});
$$

and that the observation variables at time $t$ are conditionally independent of the entire state sequence given the state variables at time $t$ : 

$$
(O^{(t)}\perp X^{(0:(t-1))},X^{(t+1:\infty)}\mid X^{(t)}).
$$

> 状态-观察模型采用了两个独立性假设：
> 1. 状态变量马尔可夫式演变，即 $(X^{(t+1)}\perp X^{(0: (t-1))}\mid X^{(t)})$
> 2. 给定 $t$ 时刻的状态变量，$t$ 时刻的观察变量条件独立于整个状态序列，即 $(O^{(t)}\perp X^{(0: (t-1))}, X^{(t+1:\infty)}\mid X^{(t)})$

We now view our probabilistic model as consisting of two components: the transition model , $P(X^{\prime}\mid X)$ , and the observation model , $P(O\mid X)$ . 
> 现在，我们将我们的概率模型视为由两个成分构成：转移模型 $P (X'\mid X)$ 和观察模型 $P (O\mid X)$

From the perspective of DBNs, this type of model corresponds to a 2-TBN structure where the observation variables $O^{\prime}$ are all leaves, and have parents only in $X^{\prime}$ . This type of situation arises quite naturally in a variety of real-world systems, where we do not have direct observations of the system state, but only access to a set of (generally noisy) sensors that depend on the state. The sensor observations do not directly efect the system dynamics, and therefore are naturally viewed as leaves. 
> 从 DBN 的角度来看，状态-观察模型对应于 2-TBN 结构中观察变量 $O'$ 都是叶子，且仅有在 $X'$ 的父变量
> 这类情况在现实系统中十分常见，我们往往不能直接观察系统状态，而是只能依访问依赖于系统状态（通常带噪声的）的传感器
> 而传感器的观察并不会直接影响系统动态，因此很自然地可以视为叶子

We note that we can convert any 2-TBN to a state-observation representation as follows: For any observed variable $Y$ that does not already satisfy the structural restrictions, we introduce a new variable $\tilde{Y}$ whose only parent is $Y$ , and that is deterministic ally equal to $Y$ . Then, we view $Y$ as being hidden, and we interpret our observations of $Y$ as observations on $\tilde{Y}$ . In efect, we construct $\tilde{Y}$ to be a perfectly reliable sensor of $Y$ . Note, however, that, while the resulting transformed network is probabilistic ally equivalent to the original, it does obscure structural independence properties of the network (for example, various independencies given that $Y$ is observed), which are now apparent only if we account for the deterministic dependency between $Y$ and $\tilde{Y}$ . 
> 我们可以将任意 2-TBN 转化为状态-观察模型：
> 对于任意不满足假设地观察到的变量 $Y$，我们引入新变量 $\tilde Y$ 作为其子变量，$\tilde Y$ 确定性地等于 $Y$
> 此时，我们可以将 $Y$ 视为隐变量，将对于 $Y$ 的观察解释为对于 $\tilde Y$ 的观察
> 这样得到的模型和原模型在概率上等价，但考虑独立性关系时需要更加小心，例如原模型中可能在 $Y$ 被观察到时存在条件独立性，现在我们应该将其解释为 $\tilde Y$ 被观察到，且考虑 $Y,\tilde Y$ 之间的确定性关系时，条件独立性存在

It is often convenient to view a temporal system as a state-observation model, both because it lends a certain uniformity of notation to a range of diferent systems, and because the state transition and observation models often induce diferent computational operations, and it is convenient to consider them separately. 

State-observation models encompass two important architectures that have been used in a wide variety of applications: hidden Markov models and linear dynamical systems. We now brieﬂy describe each of them. 
> 时序系统常常可以视为状态-观察模型，以保持符号一致性，并方便分别考虑对于状态转移模型和观察模型的计算
> 状态-观察模型有两个广泛使用的结构：HMM 和线性动态系统

#### 6.2.3.1 Hidden Markov Models 
A hidden Markov model , which we illustrated in figure 6.2, is the simplest example of a state- observation model. While an HMM is a special case of a simple DBN, it is often used to encode structure that is left implicit in the DBN representation. 
Specifically, the transition model $P(S^{\prime}\mid S)$ in an HMM is often assumed to be sparse, with many of the possible transitions having zero probability. In such cases, HMMs are often represented using a diferent graphical notation, which visualizes this sparse transition model. In this representation, the HMM transition model is encoded using a directed (generally cyclic) graph, whose nodes represent the diferent states of the system, that is, the values in $V a l(S)$ . We have a directed arc from $s$ to $s^{\prime}$ if it is possible to transition from $s$ to $s^{\prime}$ — that is, $P(s^{\prime}\mid s)>0$ . The edge from $s$ to $s^{\prime}$ can also be annotated with its associated transition probability $P(s^{\prime}\mid s)$ . 
> HMM 中的转移模型 $P (S'\mid S)$ 常常假设为稀疏的，即许多转移的概率是0
> 此时，HMM 可以采用另一类图表示，以可视化该稀疏转移模型
> 我们使用有向（通常有环）的图表示 HMM 转移模型，其中的节点表示系统的不同状态，也就是 $Val (S)$ 中的值，如果有从 $P (s' \mid s) > 0$，状态 $s$ 和 $s'$ 相连

Example 6.6
Consider an HMM with a state variable S that takes 4 values s1; s2; s3; s4, and with a transition where the rows correspond to states s and the columns to successor states $s^{\prime}$ (so that each row must sum to 1 ). The transition graph for this model is shown in figure 6.4. 

Importantly, the transition graph for an HMM is a very diferent entity from the graph encoding a graphical model. Here, the nodes in the graph are state , or possible values of the state variable; the directed edges represent possible transitions between the states, or entries in the CPD that have nonzero probability. Thus, the weights of the edges leaving a node must sum to 1. This graph representation can also be viewed as probabilistic finite-state automaton . Note that this graph-based representation does not encode the observation model of the HMM. In some cases, the observation model is deterministic, in that, for each $s$ , there is a single observation $O$ for which $P(o\mid s)=1$ (although the same observation can arise in multiple states). In this case, the observation is often annotated on the node associated with the state. 
> HMM 的状态转移模型的图表示可以视作一个概率有限状态自动机，其中每个节点的出边上的概率和应该是 1
> 该图表示没有编码 HMM 的观察模型
> 一些情况下，观察模型是确定的，即给定状态 $s$，具有确定的观察 $o$ ($P (o \mid s) = 1$)，此时可以直接在图上节点旁边标出观察 $o$

It turns out that HMMs, despite their simplicity, are an extremely useful architecture. For example, they are the primary architecture for speech recognition systems (see box 6. B) and for many problems related to analysis of biological sequences (see, for example, box 6. A). Moreover, these applications and others have inspired a variety of valuable generalizations of the basic HMM framework (see, for example, Exercises 6.2–6.5).  
> HMM 是语音识别系统的首选

Box 6. B — Case Study: HMMs for Speech Recognition. Hidden Markov models are currently the key technology in all speech- recognition systems. The HMM for speech is composed of three distinct layers: the language model , which generates sentences as sequences of words; the word model , where words are described as a sequence of phonemes; and the acoustic model , which shows the progression of the acoustic signal through a phoneme.  At the highest level, the language model represents a probability distribution over sequences of words in the language. Most simply, one can use $^a$ bigram model , which is a Markov model over words, defined via a probability distribution $P(W_{i}\mid W_{i-1})$ for each position i in the sentence. We can view this model as a Markov model where the state is the current word in the sequence. (Note that this model does not take into account the actual position in the sentence, so that $P(W_{i}\mid W_{i-1})$ is the same for all $i>1.$ .) $A$ somewhat richer model is the trigram model , where the states correspond to pairs of successive words in the sentence, so that our model defines a probability distribution $P(W_{i}\mid W_{i-1},W_{i-2})$ . Both of these distributions define a ridiculously naive model of language, since they only capture local correlations between neighboring words, with no attempt at modeling global coherence. Nevertheless, these models prove surprisingly hard to beat, probably because they are quite easy to train robustly from the (virtually unlimited amounts of) available training data, without the need for any manual labeling. 

The middle layer describes the composition of individual words in terms of phonemes — basic phonetic units corresponding to distinct sounds. These units vary not just on the basic sound uttered (“p” versus “b”), but also on whether the sound is breathy, aspirated, nasalized, and more. There is an international agreement on an International Phonetic Alphabet , which contains about 100 phonemes. Each word is modeled as a sequence of phonemes. Of course, a word can have multiple diferent pronunciations, in which case it corresponds to several such sequences. 

At the acoustic level, the acoustic signal is segmented into short time frames (around 10–25ms). A given phoneme lasts over a sequence of these partitions. The phoneme is also not homogenous. Diferent acoustics are associated with its beginning, its middle, and its end. We thus create an HMM for each phoneme, with its hidden variable corresponding to stages in the expression of the phoneme. HMMs for phonemes are usually quite simple, with three states, but can get more complicated, as in figure 6.B.1. The observation represents some set of features extracted from the acoustic signal; the feature vector is generally either discretized into a set of bins or treated as $^a$ continuous observation with a Gaussian or mixture of Gaussian distribution. 

Given these three models, we can put them all together to form a single huge hierarchical HMM that defines a joint probability distribution over a state space encompassing words, phonemes, and basic acoustic units. In a bigram model, the states in the space have the form $(w,i,j)$ , where $w$ is the current word, $i$ is a phoneme within that word, and $j$ is an acoustic position within that phoneme. The sequence of states corresponding to a word $w$ is governed by a word-HMM representing the distribution over pronunciations of $w$ . This word-HMM has a start-state and an end-state. When we exit from the end-state of the HMM for one word $w$ , we branch to the start-state of another word $w^{\prime}$ with probability $P(w^{\prime}\mid w)$ . Each sequence is thus a trajectory through acoustic HMMs of individual phonemes, transitioning from the end-state of one phoneme’s HMM to the start-state of the next phoneme’s HMM. 

A hierarchical HMM can be converted into a DBN, whose variables represent the states of the diferent levels of the hierarchy (word, phoneme, and intraphone state), along with some auxiliary variables to capture the “control architecture” of the hierarchical HMM; see exercise 6.5. The DBN formulation has the benefit of being a much more ﬂexible framework in which to introduce exten- sions to the model. One extension addresses the coarticulation problem , where the proximity of one phoneme changes the pronunciation of another. Thus, for example, the last phoneme in the word “don’t” sounds very diferent if the word after it is “go” or if it is “you.” Similarly, we often pronounce “going to” as “gonna.” The reason for coarticulation is the fact that a person’s speech articulators (such as the tongue or the lips) have some inertia and therefore do not always move all the way to where they are supposed to be. Within the DBN framework, we can easily solve this problem by introducing a dependency of the pronunciation model for one phoneme on the value of the preceding phoneme and the next one. Note that “previous” and “next” need to be interpreted with care: These are not the values of the phoneme variable at the previous or next states in the HMM, which are generally exactly the same as the current phoneme; rather, these are the values of the variables prior to the previous phoneme change, and following the next phoneme change. This extension gives rise to a non-Markovian model, which is more easily represented as a structured graphical model. Another extension that is facilitated by a DBN structure is the introduction of variables that denote states at which a transition between phonemes occurs. These variables can then be connected to observations that are indicative of such a transition, such as a significant change in the spectrum. Such features can also be incorporated into the standard HMM model, but it is diffcult to restrict the model so that these features affect only our beliefs in phoneme transitions. 

Finally, graphical model structure has also been used to model the structure in the Gaussian distribution over the acoustic signal features given the state. Here, two “traditional” models are: a diagonal Gaussian over the features, a model that generally loses many important correlations between the features; and a full covariance Gaussian, a model that requires many parameters and is hard to estimate from data (especially since the Gaussian is different for every state in the HMM). As we discuss in chapter 7, graphical models provide an intermediate point along the spectrum: we can use a Gaussian graphical model that captures the most important of the correlations between the features. The structure of this Gaussian can be learned from data, allowing a ﬂexible trade-off to be determined based on the available data. 

#### 6.2.3.2 Linear Dynamical Systems 
Another very useful temporal model is a linear dynamical system , which represents a system of one or more real-valued variables that evolve linearly over time, with some Gaussian noise. Such systems are also often called Kalman filters , after the algorithm used to perform tracking. 
> 另一类常用的时序模型是线性动态系统，它用于表示一个或者多个随着时间线性演化的实值变量（带有一点高斯噪声）
> 这类系统也常被称为 Kalman 滤波器

**A linear dynamical system can be viewed as a dynamic Bayesian network where the variables are all continuous and all of the dependencies are linear Gaussian.** 
> 线性动态系统可以视为一个 DBN，其中的变量都是连续变量，其中的依赖都是线性高斯

Linear dynamical systems are often used to model the dynamics of moving objects and to track their current positions given noisy measurements. (See also box 15.A.) 
> 线性动态系统常用于建模移动物体的动态，给定 noisy 的度量，追踪它们的当前位置

Example 6.7
Recall example 5.20, where we have a (vector) variable $X$ denoting a vehicle’s current position (in each relevant dimension) and $^a$ variable $V$ denoting its velocity (also in each dimension). As we discussed earlier, a first level approximation may a model where ${\cal P}(X^{\prime}\mid X,V)\;=\;$ ${\mathcal{N}}\left(X+V\Delta;\sigma_{X}^{2}\right)$  and $P(V^{\prime}\mid V)=\mathcal{N}\left(V;\sigma_{V}^{2}\right)$  (where ∆ , as before, is the length of our time slice). The observation — for example, a GPS signal measured from the car — is a noisy Gaussian measurement of $X$ . 

These systems and their extensions are at the heart of most target tracking systems, for example, tracking airplanes in an air traffc control system using radar data. 

Traditionally, linear dynamical systems have not been viewed from the perspective of factor- ized representations of the distribution. They are traditionally represented as a state-observation model, where the state and observation are both vector-valued random variables, and the transi- tion and observation models are encoded using matrices. More precisely, the model is generally defined via the following set of equations: 

$$
\begin{align}
{{P(X^{(t)}\mid X^{(t-1)})}}&={{\mathcal{N}\left(A X^{(t-1)};Q\right),}}\tag{6.3}\\ {{P(O^{(t)}\mid X^{(t)})}}&={{\mathcal{N}\left(H X^{(t)};R\right),}}\tag{6.4}\end{align}
$$

where: $X$ is an $n$ -vector of state variables, $O$ is an $m$ -vector of observation variables, $A$ is an $n\times n$ matrix defining the linear transition odel, $Q$ n $n\times n$ matrix defining the Gaussian noise associ ed with the system dynamics, H is an $n\times m$ × matrix defining the linear observation model, and R is an $m\times m$ matrix defining the Gaussian noise associated with the observations. 
> 线性动态系统通过 (6.3), (6.3) 定义，其中 $X$ 是长度为 $n$ 的状态向量，$O$ 是长度为 $n$ 的观察向量，矩阵 $A (n\times n)$ 定义了线性转移模型，矩阵 $Q (n\times n)$ 定义了和系统动态相关的高斯噪声，矩阵 $H (n\times n)$ 定义了线性观察模型，矩阵 $R (m\times m)$ 定义了和观察相关的高斯噪声

This type of model encodes independence structure implicitly, in the parameters of the matrices (see exercise 7.5). 
> 这类模型在矩阵的参数中隐式编码了独立性结构

There are many interesting generalizations of the basic linear dynamical system, which can also be placed within the DBN framework. For example, a nonlinear variant, often called an extended Kalman filter , is a system where the state and observation variables are still vectors of real numbers, but where the state transition and observation models can be nonlinear functions rather than linear matrix multiplications as in equation (6.3) and equation (6.4). Specifically, we usually write: 

$$
\begin{array}{r c l}{{P(X^{(t)}\mid X^{(t-1)})}}&{{=}}&{{f(X^{(t-1)},U^{(t-1)})}}\\ {{P(O^{(t)}\mid X^{(t)})}}&{{=}}&{{g(X^{(t)},W^{(t)}),}}\end{array}
$$

where $f$ and $g$ are deterministic nonlinear functions, and $U^{(t)},W^{(t)}$ are Gaussian random variables that explicitly encode the noise in the transition and observation models, respectively. 
> 线性动态系统可以进行拓展，例如非线性的变体，即拓展的 Kalman 滤波器
> 其中的状态和观察变量仍是实数向量，但状态转移模型和观察模型不再是简单矩阵乘法，而是非线性函数

In other words, rather than model the system in terms of stochastic CPDs, we use an equivalent representation that partitions the model into a deterministic function and a noise component. 
> 需要注意的是，非线性情况下，非线性函数是确定性函数，系统的不确定性来源于作为输入的高斯噪声的不确定性，这和直接之前用高斯分布建模条件概率分布是不同的
> 因此，我们没有用随机 CPDs 建模系统，而是将模型划分为一个确定性函数和一个噪声成分，这两个表示实际上是等价的

Another interesting class of models are systems where the continuous dynamics are linear, but that also include discrete variables. For example, in our tracking example, we might introduce a discrete variable that denotes the driver’s target lane in the freeway: the driver can stay in her current lane, or she can switch to a lane on the right or a lane on the left. Each of these discrete settings leads to diferent dynamics for the vehicle velocity, in both the lateral (across the road) and frontal velocity. 
> 系统还可以引入离散变量，系统的连续动态是线性的，但是不同的离散变量可以对应不同的连续动态

Systems that model such phenomena are called switching linear dynamical system (SLDS) . In such models, we system can switch between a set of discrete modes . While within a fixed mode, the system evolves using standard linear (or nonlinear) Gaussian dynamics, but the equations governing the dynamics are diferent in diferent modes. 
> 这类系统称为切换线性动态系统，系统在一系列离散模式中切换，在某个模式下，系统使用标准线性/非线性高斯动态来演化，不同的模式中，具体的动态（参数/形式）是不同的

We can view this type of system as a DBN including a discrete variable $D$ that encodes the mode, where $\mathrm{Pa}_{D^{\prime}}\,=\,\{D\}$ , and allowing D to be the parent of (some of) the continuous variables in the model, so that they use a conditional linear Gaussian CPDs. 
> 很显然，这就是条件线性高斯 CPDs

## 6.3 Template Variables and Template Factors 
Having seen one concrete example of a template-based model, we now describe a more general framework that provides the fundamental building blocks for a template-based model. This framework provides a more formal perspective on the temporal models of the previous section, and a sound foundation for the richer models in the remaining sections of this chapter. 

**Template Attributes** The key concept in the definition of the models we describe in this chapter is that of a *template* that is instantiated many times within the model. A template for a random variable allows us to encode models that contain multiple random variables with the same value space and the same semantics. For example, we can have a Blood-Type template, which has a particular value space (say, A , B , AB , or $O_{-}$ ) and is reused for multiple individuals within a pedigree. That is, when reasoning about a pedigree, as in box 3. B, we would want to have multiple instances of blood-type variables, such as Blood-Type ( Bart ) or Blood-Type ( Homer ) . We use the word attribute or template variable to distinguish a template, such as Blood-Type , from a specific random variable, such as Blood-Type ( Bart ) . 
> 一个随机变量的模板允许我们编码包含多个具有相同值空间和相同语义的随机变量的模型
> 随机变量模板实例化即得到随机变量
> 模板变量也可以称为属性

In a very diferent type of example, we can have a template attribute Location , which can be instantiated to produce random variables Location $(t)$ for a set of diferent time points $t$ . This type of model allows us to represent a joint distribution over a vehicle’s position at diferent points in time, as in the previous section. 

In these example, the template was a property of a single object — a person. More broadly, attributes may be properties of entire tuples of objects. For example, a student’s grade in a course is associated with a student-course pair; a person’s opinion of a book is associated with the person-book pair; the afnity between a regulatory protein in the cell and one of its gene targets is also associated with the pair. More specifically, in our Student example, we want to have a Grade template, which we can instantiate for diferent (student, course) pairs $s,c$ to produce multiple random variables $G r a d e(s,c)$ , such as $G r a d e(G e o r g e,C S l O I)$ .

Because many domains involve heterogeneous objects, such as courses and students, it is convenient to view the world as being composed of a set of objects . Most simply, objects can be divided into a set of mutually exclusive and exhaustive classes $\mathcal{Q}=\mathsf{Q}_{1},.\ldots,\mathsf{Q}_{k}$ . In the Student scenario, we might have a Student class and a Course class. 

Attributes have a tuple of arguments , each of which is associated with a particular class of objects. This class defines the set of objects that can be used to instantiate the argument in a given domain. For example, in our Grade template, we have one argument $S$ that can be instantiated with a “student” object, and another argument $C$ that can be instantiated with a “course” object. Template attributes thus provide us with a “generator” for random variables in a given probability space. 
> 属性具有一系列参数，每个参数都和特定类型的对象相关，类就是对象集合，它定义了参数在给定领域可以被实例化成什么对象
> 模板属性可以视为为我们在给定的概率空间提供了一个随机变量的“生成器”

**Definition 6.5** 
An attribute $A$ is a function ${\overline{{A(U_{1}}}},.\,.\,.\,,U_{k})$ , whose range is some set $V a l(A)$ and where each argu- ment $U_{i}$ is a typed logical variable associated with a particular class $\mathsf{Q}[U_{i}]$ . The tuple $U_{1},\dots,U_{k}$ is called the argument signature of the attribute $A$ , and denoted $\alpha(A)$ . 
> 定义：
> 属性 $A$ 是一个函数 $A (U_1,\dots, U_k)$，其值域是集合 $Val (A)$，其每个参数 $U_i$ 都是一个有类型的逻辑变量，和特定的类 $Q[U_i]$ 相关
> $U_1,\dots , U_k$ 称为属性 $A$ 的参数签名，记作 $\alpha (A)$

From here on, we assume without loss of generality that each logical variable $U_{i}$ is uniquely associated with a particular class $\mathsf{Q}[U_{i}]$ ; thus, any mention of a logical variable $U_{i}$ uniquely specifies the class over which it ranges. 
> 我们假设每个逻辑变量和唯一地和一个特定类 $Q[U_i]$ 关联

For example, the argument signature of the Grade attribute would have two logical variables $S,C$ , where $S$ is of class Student and $C$ is of class Course . We note that the classes associated with an attribute’s argument signature are not necessarily distinct. For example, we might have a binary-valued Cited attribute with argument signature $A_{1},A_{2}$ , where both are of type Article . We assume, for simplicity of presentation, that attribute names are uniquely defined; thus, for example, the attribute denoting the age for a person will be named diferently from the attribute denoting the age for a car. 

This last example demonstrates a basic concept in this framework: that of a relation . A relation is a property of a tuple of objects, which tells us whether the objects in this tuple satisfy a certain relationship with each other. For example, Took-Course is a relation over student-course object pairs $s,c,$ which is true is student $s$ took the course $c$ . As another example Mother is a relation between person-person pairs $p_{1},p_{2}$ , which is true if $p_{1}$ is the mother of $p_{2}$ . Relations are not restricted to involving only pairs of objects. For example, we can have a Go relation, which takes triples of objects — a person, a source location, and a destination location. 

At some level, a relation is simply a binary-valued attribute, as in our Cited example. However, this perspective obscures the fundamental property of a relation — that it relates a tuple of objects to each other. Thus, when we introduce the notion of probabilistic dependencies that can occur between related objects, the presence or absence of a relation between a pair of objects will play a central role in defining the probabilistic dependency model. 

**Instantiations** Given a set of template attributes, we can instantiate them in diferent ways, to produce probability spaces with multiple random variables of the same type. For example, we can consider a particular university, with a set of students and a set of courses, and use the notion of a template attribute to define a probability space that contains a random variable $G r a d e(s,c)$ for diferent (student, course) pairs $s,c$ . The resulting model encodes a joint distribution over the grades of multiple students in multiple courses. Similarly, in a temporal model, we can have the template attribute Location $(T)$ ; we can then select a set of relevant time points and generate a trajectory with specific random variables Location $.(t)$ 

To instantiate a set of template attributes to a particular setting, we need to define a set of objects for each class in our domain. For example, we may want to take a particular set of students and set of courses and define a model that contains a ground random variable Intelligence $(s)$ and $S A T(s)$ for every student object $s$ , a ground random variable Difculty $\cdot(c)$ for every course object $c,$ , and ground random variables $G r a d e(s,c)$ and Satisfaction $(s,c)$ for every valid pair of (student, course) objects. 

More formally, we now show how a set of template attributes can be used to generate an infinite set of probability spaces, each involving instantiations of the template attributes induced by some set of objects. We begin with a simple definition, deferring discussion of some of the more complicated extensions to section 6.6. 

**Definition 6.6** object skeleton
Let $\mathcal{Q}$ be a set of classes, and $\aleph$ a set of tem ttributes over $\mathcal{Q}$ . An object skeleton $\kappa$ specifies a fixed, finite set of objects ${\mathcal{O}}^{\kappa}[\mathsf{Q}]$ for every $\mathsf Q\in\mathcal Q$ . We also define ${\mathcal O}^{\kappa}[U_{1},.\,.\,,U_{k}]={\mathcal O}^{\kappa}[\mathsf{Q}[U_{1}]]\times.\,.\,.\times{\mathcal O}^{\kappa}[\mathsf{Q}[U_{k}]].$ By default, we define $\Gamma_{\kappa}[A]\,=\,{\mathcal{O}}^{\kappa}[\alpha(A)]$ to be the set of possible assignments to the logical variables in the argument signature of A . However, an object skeleton may also specify a subset of legal assignments. $\Gamma_{\kappa}[A]\subset\mathcal{O}^{\kappa}[\alpha(A)]$ . 
> 定义：
> $\mathcal Q$ 为类的集合，$\aleph$ 是 $\mathcal Q$ 上的属性的集合，一个对象框架 $\kappa$ 为每个类 $Q\in \mathcal Q$ 指定了一个固定的、有限的对象集合 $\mathcal O^k[Q]$
> 我们还定义 ${\mathcal O}^{\kappa}[U_{1},.\,.\,,U_{k}]={\mathcal O}^{\kappa}[\mathsf{Q}[U_{1}]]\times.\,.\,.\times{\mathcal O}^{\kappa}[\mathsf{Q}[U_{k}]].$ ，也就是 $\mathcal O^{\kappa}[U_1,\dots , U_k]$ 的成员是 $\mathcal O^{\kappa}[Q[U_i]]$ 的各自成员的所有可能组合
> 我们还定义 $\Gamma_{\kappa}[A] = \mathcal O^{\kappa}[\alpha (A)]$，即定义了 $A$ 的参数签名的逻辑变量集合的所有可能的赋值构成的集合，可以人为定义一些赋值不合法，此时 $\Gamma_{\kappa}[A] \subset \mathcal O^{\kappa}[\alpha (A)]$

We can now define the set of instantiations of the attributes: 

**Definition 6.7** ground random variable 
Let $\kappa$ be an object skeleton over $\mathcal{Q},\aleph$ . We define sets of ground random variables: 

$$
\begin{array}{r c l}{\mathcal{X}_{\kappa}[A]}&{=}&{\{A(\gamma)\ :\ \gamma\in\Gamma_{\kappa}[A]\}}\\ {\mathcal{X}_{\kappa}[\aleph]}&{=}&{\cup_{A\in\aleph}\mathcal{X}_{\kappa}[A].}\end{array}\tag{6.5}
$$ 
Note that we ar tation here, identifying an assignment $\gamma=\left\langle U_{1}\mapsto u_{1},.\,.\,.\,,U_{k}\mapsto u_{k}\right\rangle$ with the tuple ⟨ $\langle u_{1}\cdot\cdot\cdot,u_{k}\rangle$ ⟩ ; this abuse of notation is unambiguous in this context due to the ordering of the tuples. 
> 定义：
> 令 $\kappa$ 为 $\mathcal Q, \aleph$ 上的对象框架，定义 ground random variable 的集合
> $\mathcal X_{\kappa}[A] = \{A(\gamma): \gamma \in \Gamma_{\kappa}[A]\}$
> ${\mathcal{X}_{\kappa}[\aleph]}={\cup_{A\in\aleph}\mathcal{X}_{\kappa}[A].}$

The ability to specify a subset of ${\mathcal{O}}^{\kappa}[\alpha(A)]$ is useful in eliminating the need to consider random variables that do not really appear in the model. For example, in most cases, not every student takes every course, and so we would not want to include a Grade variable for every possible (student, course) pair at our university. See figure 6.5 as an example. 

Clearly, the set of random variables is diferent for diferent skeletons; hence the model is a template for an infinite set of probability distributions, each spanning a diferent set of objects that induces a diferent set of random variables. In a sense, this is similar to the situation we had in DBNs, where the same 2TBN could induce a distribution over diferent numbers of time slices. Here, however, the variation between the diferent instantiations of the template is significantly greater. 

Our discussion so far makes several important simplifying assumptions. First, we portrayed the skeleton as defining a set of objects for each of the classes. As we discuss in later sections, it can be important to allow the skeleton to provide additional background information about the set of possible worlds, such as some relationships that hold between objects (such as the structure of a family tree). Conversely, we may also want the skeleton to provide less information: In particular, the premise underlying equation (6.5) is that the set of objects is predefined by the skeleton. As we brieﬂy discuss in section 6.6.2, we may also want to deal with settings in which we have uncertainty over the number of objects in the domain. In this case, diferent possible worlds may have diferent sets of objects, so that a random variable such as $A(u)$ may be defined in some worlds (those that contain the object $u$ ) but not in others. Settings like this pose significant challenges, a discussion of which is outside the scope of this book. 
> (6.5) 的前提是对象的集合由对象框架 $\kappa$ 预定义

**Template Factors** The final component in a template-based probabilistic model is one that defines the actual probability distribution over a set of a ground random variables generated from a set of template attributes. Clearly, we want the specification of the model to be defined in a template-based way. Specifically, we would like to take a factor — whether an undirected factor or a CPD — and instantiate it to apply to multiple scopes in the domain. We have already seen one simple example of this notion: in a 2-TBN, we had a template CPD $P(X_{i}^{\prime}\mid\mathrm{Pa}_{X_{i}^{\prime}})$ , which we instantiated to apply to diferent scopes $X_{i}^{(t)},\mathrm{Pa}_{X_{i}^{(t)}}$ , by instantiating any occurrence $X_{j}^{\prime}$ to $X_{j}^{(t)}$ , and any occurrence $X_{j}$ to $X_{j}^{(t-1)}$ . In efect, there we had template variables of the form $X_{j}$ and $X_{j}^{\prime}$ as arguments to the CPD, and we instantiated them in diferent ways for diferent time points. 
> 一个基于模板的概率模型定义了在由模板属性集合生成的 ground random variable 集合之上的实际概率分布

We can now generalize this notion by defining a factor with arguments. Recall that a factor $\phi$ is a function from a tuple of random variables $X=S c o p e[\phi]$ to the reals; this function returns a number for each assignment ${x}$ to the variables $X$ . We can now define 
> 一个因子是 $\phi$ 是将随机变量 tuple $X = Scope[\phi]$ 的映射到实数的函数，该函数为每个对 $X$ 的赋值 $\pmb x$ 返回一个实数

**Definition 6.8** instantiated factor 
$A$ template factor is a function $\xi$ defined over a tuple of template attributes $A_{1},\ldots,A_{l},$ where each $A_{j}$ has a range $V a l(A)$ . It defines a mapping from $V a l(A_{1})\times.-.\times V a l(A_{l})$ to $I\!\!R$ . Given a tuple of random variables $X_{1},\ldots,X_{l}.$ , such that $V a l(X_{j})=V a l(A_{j})$ for all $j\,=\,1,\ldots,l.$ , we define $\xi(X_{1},.\,.\,.\,,X_{l})$ to be the instantiated factor from $X$ to $I\!\!R$ . 
> 定义：
> 模板因子是一个函数 $\xi$，定义于模板属性 tuple $A_1,\dots, A_l$，其中每个 $A_j$ 的值域为 $Val (A_j)$，该函数定义了从 $Val (A_1)\times Val (A_2)\times \dots \times Val (A_l)$ 到 $R$ 的映射
> 给定一个随机变量 tuple $X_1,\dots, X_l$，使得 $Val (X_j) = Val (A_j)$，我们定义 $\xi (X_1, \dots, X_l)$ 为从 $\pmb X$ 到 $R$ 的实例化因子

In the subsequent sections of this chapter, we use these notions to define various languages for encoding template-based probabilistic models. As we will see, some of these representational frameworks subsume and generalize on the DBN framework defined earlier. 

## 6.4 Directed Probabilistic Models for Object-Relational Domains 

Based on the framework described in the previous section, we now describe template-based representation languages that can encode directed probabilistic models. 

### 6.4.1 Plate Models 

plate model We begin our discussion by presenting the plate model , the simplest and best-established of the object-relational frameworks. Although restricted in several important ways, the plate modeling framework is perhaps the approach that has been most commonly used in practice, notably for encoding the assumptions made in various learning tasks. This framework also provides an excellent starting point for describing the key ideas of template-based languages and for motivating some of the extensions that have been pursued in richer languages. 

In the plate formalism, object types are called plates . The fact that multiple objects in the class share the same set of attributes and same probabilistic model is the basis for the use of the term “plate,” which suggests a stack of identical objects. We begin with some motivating examples and then describe the formal framework. 

#### 6.4.1.1 Examples 

Example 6.8 

Figure 6.6 Plate model for a set of coin tosses sampled from a single coin 

plate that all have the same domain $V a l(X)$ and are sampled from the same distribution. In a plate representation, we encode the fact that these variables are all generated from the same template by drawing only a single node $X(d)$ and enclosing it in a box denoting that $d$ ranges over $\mathcal{D}$ , so that we know that the box represents an entire “stack” of these identically distributed variables. This box is called a plate , with the analogy that it represents a stack of identical plates. 

In many cases, we want to explicitly encode the fact that these variables have an identical distribution. We therefore often explicitly add to the model the variable $\theta_{X}$ , which denotes the parameter iz ation of the CPD from which the variables $X(d)$ are sampled. Most simply, if the $X\mathit{\dot{s}}$ are coin tosses of a single (possibly biased) coin, $\theta_{X}$ would take on values in the range $[0,1]$ , and its value would denote the bias of the coin (the probability with which it comes up “Heads”). 

The idea of including the CPD parameters directly in the probabilistic model plays a central role in our discussion of learning later in this book (see section 17.3). For the moment, we note only that including the parameters directly in the model allows us to make explicit the fact that all of the variables $X(d)$ are sampled from the same CPD. By contrast, we could also have used a model where a variable $\theta_{X}(d)$ is included inside the plate, allowing us to encode the setting where each of the coin tosses was sampled from a diferent coin. We note that this transformation is equivalent to adding the coin ID $d$ as a parent to $X$ ; however, the explicit placement of $\theta_{X}$ within the plate makes the nature of the dependence more explicit. In this chapter, to reduce clutter, we use the convention that parameters not explicitly included in the model (or in the figure) are outside of all plates. 

Example 6.9 

ground Bayesian network 

Let us return to our Student example. We can have a Student plate that includes the attributes $I(S),G(S)$ . As shown in figure 6.7a, we can have $G(S)$ depend on $I(s)$ . In this model we have a set of (Intelligence, Grade) pairs, one for each student. The figure also shows the ground Bayesian network that would result from instantiating this model for two students. As we discussed, this model implicitly makes the assumption that the CPDs for $P(I(s))$ and for $P(G(s)\mid I(s))$ is the same for all students $s$ . Clearly, we can further enrich the model by introducing additional variables, such as an SAT-score variable for each student. 

Our examples thus far have included only a single type of object, and do not significantly expand the expressive power of our language beyond that of plain graphical models. The key benefit of the plate framework is that it allows for multiple plates that can overlap with each other in various ways. 

Example 6.10 

nested plate Assume we want to capture the fact that a course has multiple students in it, each with his or her own grade, and that this grade depends on the difculty of the course. Thus, we can introduce a second type of plate, labeled Course , where the Grade attribute is now associated with a (student, course) pair. There are several ways in which we can modify the model to include courses. In figure $6.7b,$ the Student plate is nested within the Course plate. The Difculty variable is enclosed within the Course plate, whereas Intelligence and Grade are enclosed within both plates. We thus have that Grade $(s,c)$ for a particular (student, course) pair $(s,c)$ depends on Difculty $(c)$ and on Intelligence $(s,c)$ . 

This formulation ignores an important facet of this problem. As illustrated in figure $6.7\mathrm{b}$ , it induces networks where the Intelligence variable is associated not with a student, but rather with a (student, course) pair. Thus, if we have the same student in two diferent courses, we would have two diferent variables corresponding to his intelligence, and these could take on diferent values. This formulation may make sense in some settings, where diferent notions of “intelligence” may be appropriate to diferent topics (for example, math versus art); however, it is clearly not a suitable model for all settings. Fortunately, the plate framework allows us to come up with a diferent formulation. 

Example 6.11 plate intersection 

Figure 6.7c shows a construction that avoids this limitation. Here, the Student plate and the Course plates intersect , so that the Intelligence attribute is now associated only with the Student plate, and Difculty with the Course plate; the Grade attribute is associated with the pair (comprising the intersection between the plates). The interpretation of this dependence is that, for any pair of (student, course) objects $s,c,$ the attribute Grade $(s,c)$ depends on Intelligence ( s ) and on Difculty $(c)$ . The figure also shows the network that results for two students both taking two courses. 

In these examples, we see that even simple plate representations can induce fairly complex ground Bayesian networks. Such networks model a rich network of interdependencies between diferent variables, allowing for paths of inﬂuence that one may not anticipate. 

Example 6.12 Consider the plate model of figure 6.7c, where we know that a student Jane took CS101 and got an A. This fact changes our belief about Jane’s intelligence and increases our belief that CS101 is an easy class. If we now observe that Jane got a C in Math 101, it decreases our beliefs that she is intelligent, and therefore should increase our beliefs that CS101 is an easy class. If we now observe that George got a C in CS101, our probability that George has high intelligence is significantly lower. Thus, our beliefs about George’s intelligence can be afected by the grades of other students in other classes. 

Figure 6.8 shows a ground Bayesian network induced by a more complex skeleton involving fifteen students and four courses. Somewhat surprisingly, the additional pieces of “weak” evidence regarding other students in other courses can accumulate to change our conclusions fairly radically: Considering only the evidence that relates directly to George’s grades in the two classes that he took, our posterior probability that George has high intelligence is 0 . 8 . If we consider our entire body of evidence about all students in all classes, this probability decreases from 0.8 to 0.25. When we examine the evidence more closely, this conclusion is quite intuitive. We note, for example, that of the students who took CS101, only George got a C. In fact, even Alice, who got a C in both of her other classes, got an A in CS101. This evidence suggests strongly that CS101 is not a difcult class, so that George’s grade of a C in CS101 is a very strong indicator that he does not have high intelligence. 

![](images/766ffe8ec48820114c23511ee14c1346ff86fadfddab2499b0f444ed282cc8b8.jpg) 
Figure 6.7 Plate models and induced ground Bayesian networks for a simplified Student example. (a) Single plate: Multiple independent samples from the same distribution. (b) Nested plates: Multiple courses, each with a separate set of students. (c) Intersecting plates: Multiple courses with overlapping sets of students. 

Thus, we obtain much more informed conclusions by defining probabilistic models that encompass all of the relevant evidence. 

As we can see, a plate model provides a language for encoding models with repeated  structure and shared parameters. As in the case of DBNs, the models are represented at the template level ; given a particular set of objects, they can then be instantiated to induce a ground Bayesian network over the random variables induced by these objects. Because there are infinitely many sets of objects, this template can induce an infinite set of ground networks. 

![](images/3c0885c0e2f292f6a2965711eed8b485ad88a0872be97ed433c701e388fa507d.jpg) 
Figure 6.8 Illustration of probabilistic interactions in the University domain. The ground network contains random variables for the Intelligence of fifteen students (right ovals), including George (denoted by the white oval), and the Difculty for four courses (left ovals), including CS101 and Econ101. There are also observed random variables for some subset of the (student, course) pairs. For clarity, these observed grades are not denoted as variables in the network, but rather as edges relating the relevant (student, course) pairs. Thus, for example George received an A in Econ101 but a C in CS101. Also shown are the final probabilities obtained by running inference over the resulting network. 

#### 6.4.1.2 Plate Models: Formal Framework 

We now provide a more formal description of the plate modeling language: its representation and its semantics. The plate formalism uses the basic object-relational framework described in section 6.4. As we mentioned earlier, plates correspond to object types. 

Each template attribute in the model is embedded in zero, one, or more plates (when plates intersect). If an attribute $A$ is embedded in a set of plates $\mathsf{Q}_{1},\ldots,\mathsf{Q}_{k}$ , we can view it as being associated with the argument signature $U_{1},\dots,U_{k}$ , where each logical variable $U_{i}$ ranges over the objects in the plate (class) $\mathsf{Q}_{i}$ . Recall that a plate model can also have attributes that are external to any plate; these are attributes for which there is always a single copy in the model. We can view this attribute as being associated with an argument signature of arity zero. 

In a plate model, the set of random variables induced by a template attribute $A$ is defined by the complete set of assignments: $\Gamma_{\kappa}[A]\,=\,{\mathcal{O}}^{\kappa}[\alpha(A)]$ . Thus, for example, we would have a Grade random variable for every (student, course) pair, whereas, intuitively, these variables are only defined in cases where the student has taken the course. We can take the values of such variables to be unobserved, and, if the model is well designed, its descendants in the probabilistic dependency graph will also be unobserved. In this case, the resulting random variables in the network will be barren, and can be dropped from the network without afecting the marginal distribution over the others. This solution, however, while providing the right semantics, is not particularly elegant. 

We now define the probabilistic dependency structures allowed in the plate framework. To provide a simple, well-specified dependency structure, plate models place strong restrictions on the types of dependencies allowed. For example, in example 6.11, if we define Intelligence to be a parent of Difculty (reﬂecting, say, our intuition that intelligent students may choose to take harder classes), the semantics of the ground model is not clear: for a ground random variable $D(c)$ , the model does not specify which specific $I(s)$ is the parent. To avoid this problem, plate models require that an attribute can only depend on attributes in the same plate. This requirement is precisely the intuition behind the notion of plate intersection: Attributes in the intersection of plates can depend on other attributes in any of the plates to which they belong. Formally, we have the following definition: 

Definition 6.9 plate model 

template parent 

parent argument signature 

$A$ plate model $\mathcal{M}_{P l a t e}$ defines, for each template attribute $A~\in~\aleph$ with argument signature $U_{1},\dots,U_{k}$ : • a set of template parents $\mathrm{Pa}_{A}=\{B_{1}(\pmb{U}_{1}),.\,.\,.\,,B_{l}(\pmb{U}_{l})\}$ such that for each $B_{i}(\pmb{U}_{i})$ , we e that $U_{i}\;\subseteq\;\{U_{1},.\,.\,.\,,U_{k}\}$ . The variables $U_{i}$ are the argument signature of the parent $B_{i}$ . • a template CPD $P(A\mid\mathrm{Pa}_{A})$ . 

This definition allows the Grade attribute $G r a d e (S, C)$ to depend on Intelligence $\cdot (S)$ , but not vice versa. Note that, as in Bayesian networks, this definition allows any form of CPD, with or without local structure. 

Note that the straightforward graphical representation of plates fails to make certain distinc- tions that are clear in the symbolic representation. 

Example 6.13 

Assume that our model contains an attribute $\mathit{C i t e d}(U_{1}, U_{2})$ , where $U_{1}, U_{2}$ are both in the Paper class. We might want the dependency model of this attribute to depend on properties of both papers, for example, $T o p i c (U_{1})$ , $T o p i c (U_{2})$ , or Review-Paper $\left (U_{1}\right)$ . To encode this dependency graphically, we first need to have two sets of attributes from the Paper class, one for $U_{1}$ and the other for $U_{2}$ . Moreover, we need to denote somehow which attributes of which of the two arguments are the parents. The symbolic representation makes these distinctions unambiguously. 

To instantiate the template parents and template CPDs, it helps to introduce some shorthand notation. $\gamma=\left\langle U_{1}\mapsto u_{1},.\,.\,.\,, U_{k}\mapsto u_{k}\right\rangle$ be some assignment to some set of logical vari- ables, and $B (U_{i_{1}},.\,.\,.\,, U_{i_{l}})$ be an attribute whose argument signature involves only a subset of these variables. We define $B (\gamma)$ to be the ground random variable $B (u_{i_{1}},.\,.\,.\,, u_{i_{l}})$ . 

The template-level plate model, when applied to a particular skeleton, defines a ground probabilistic model, in the form of a Bayesian network: 

Definition 6.10 ground Bayesian network 

lows. Let $A (U_{1},\dots, U_{k})$ be any template attribute in $\aleph$ . Then, for any assignment $\gamma\_=$ $\left\langle U_{1}\mapsto u_{1},.\,.\,, U_{k}\mapsto u_{k}\right\rangle\in\Gamma_{\kappa}[A].$ , we have a va $A (\gamma)$ in the ground network, with parents $B (\gamma)$ for all $B\in\mathrm{Pa}_{A}$ ∈ , and the instantiated CPD $P (A (\gamma)\mid\mathrm{Pa}_{A}(\gamma))$ | . 

Thus, in our example, we have that the network contains a set of ground random variables $G r a d e (s, c)$ , one for every student $s$ and every course $c$ . Each such variable depends on Intelligence $\cdot (s)$ and on Difculty $\left (c\right)$ . 

The ground net $\mathcal{B}_{\kappa}^{\mathcal{M}_{P l a t e}}$ specifies a well-defined joint distribution over $\mathcal{X}_{\kappa}[\aleph]$ , as required. The BN in figure 6.7b is precisely the network structure we would obtain from this defini- tion, using the plate model of figure 6.7 and the object skeleton $\mathcal{O}^{\kappa}[\mathsf{S t u d e n t}]=\{s_{1}, s_{2}\}$ and $\mathcal{O}^{\kappa}[\mathsf{C o u r s e}]=\{c_{1}, c_{2}\}$ . In general, despite the compact parameter iz ation (only one local prob- abilistic model for every attribute in the model), the resulting ground Bayesian network can be quite complex, and models a rich set of interactions. As we saw in example 6.12, the ability to incorporate all of the relevant evidence into the single network shown in the figure can significantly improve our ability to obtain meaningful conclusions even from weak indicators. 

The plate model is simple and easy to understand, but it is also highly limited in several ways. Most important is the first condition of definition 6.9, whereby $A (U_{1},\dots, U_{k})$ can only depend on attributes of the form $B (U_{i_{1}},.\,.\,.\,, U_{i_{l}})$ , where $U_{i_{1}},\ldots, U_{i_{l}}$ is a subtuple of $U_{1},\dots, U_{k}$ . This restriction significantly constrains our ability to encode a rich network of probabilistic dependencies between the objects in the domain. For example, in the Genetics domain, we cannot encode a dependence of Genotype $\left (U_{1}\right)$ on Genotype ( $\left (U_{2}\right)$ , where $U_{2}$ is (say) the mother of $U_{1}$ . Similarly, we cannot encode temporal models such as those described in section 6.2, where the car’s position at a point in time depends on its position at the previous time point. In the next section, we describe a more expressive representation that addresses these limitations. 

### 6.4.2 Probabilistic Relational Models 

As we discussed, the greatest limitation of the plate formalism is its restriction on the argument signature of an attribute’s parents. In particular, in our genetic inheritance example, we would like to have a model where Genotype $\prime (u)$ depends on Genotype $(u^{\prime})$ , where $u^{\prime}$ is the mother of $u$ . This type of dependency is not encodable within plate models, because it uses a logical variable in the attribute’s parent that is not used within the attribute itself. To allow such models, we must relax this restriction on plate models. However, relaxing this assumption without care can lead to nonsensical models. In particular, if we simply allow Genotype $(U)$ to depend on Genotype $\left (U^{\prime}\right)$ , we end up with a dependency model where every ground variable Genotype $(u)$ depends on every other such variable. Such models are intractably dense, and (more importantly) cyclic. What we really want is to allow a dependence of Genotype $(U)$ on Genotype $\left (U^{\prime}\right)$ , but only for those assignments to $U^{\prime}$ that correspond to $U$ ’s mother. We now describe one representation that allows such dependencies, and then discuss some of the subtleties that arise when we introduce this significant extension to our expressive power. 

#### 6.4.2.1 Contingent Dependencies 

To capture such situations, we introduce the notion of a contingent dependency , which specifies the context in which a particular dependency holds. A contingent dependency is defined in terms of a guard — a formula that must hold for the dependency to be applicable. 

Example 6.14 

Consider again our University example. As usual, we can define Grade $(S, C)$ for a student $S$ and a course $C$ to have the parents Difculty $\prime (C)$ and Intelligence $\cdot (S)$ . Now, however, we can make this dependency contingent on the guard Registered $\iota (S, C)$ . Here, the parent’s argument signature is the same as the child’s. More interestingly, contingent dependencies allow us to model the dependency of the student’s satisfaction in a course, Satisfaction $(S, C)$ , on the teaching ability of the professor who teaches the course. In this setting, we can make Teaching-Ability $\cdot (P)$ the parent faction $. (S, C)$ , where the dependency is contingent on the guard Registered $\I (S, C)\wedge$ Teaches $\cdot (P, C)$ . Note that here, we have more logical variables in the parents of Satisfaction $(S, C)$ than in the attribute itself: the attribute’s argument signature is $S, C,$ , whereas its parent argument signature is the tuple $S, C, P$ . 

We can also represent chains of dependencies within objects in the same class. 

Example 6.15 

For example, to encode temporal models, we could have Location $(U)$ depend on Location $(V)$ , contingent on the guard Precedes $\cdot (V, U)$ . In our Genetics example, for the attribute Genotype $(U)$ , we would define the template parents G $\cdot (V)$ and Genotype $(W)$ , the guard Mother $\cdot (V, U)\wedge$ Father $\mathopen{}\mathclose\bgroup\left (W, U\aftergroup\egroup\right)$ , and the parent signature $U, V, W$ . 

We now provide the formal definition underlying these examples: 

Definition 6.11 

contingent dependency model parent argument signature guard 

probabilistic relational model For a template attribute $A$ , we define $^a$ contingent dependency model as a tuple consisting of: 

• $A$ parent argument signature $\alpha (\operatorname{Pa}_{A})$ , which is a tuple of typed logical variables $U_{i}$ such that $\alpha (\mathrm{Pa}_{A})\supseteq\alpha (A)$ . • $A$ guard $\Gamma$ , which is a binary-valued formula defined in terms of a set of template attributes $\mathrm{Pa}_{A}^{\bar{\Gamma}}$ over the argument signature $\alpha (\operatorname{Pa}_{A})$ . • a set of template parents $\mathrm{Pa}_{A}=\{B_{1}(\pmb{U}_{1}),.\,.\,.\,, B_{l}(\pmb{U}_{l})\}$ such that for each $B_{i}(\pmb{U}_{i})$ , we have that $U_{i}\subseteq\alpha (\mathrm{Pa}_{A})$ . 

A probabilistic relational model (PRM) $\mathcal{M}_{P R M}$ defines, for each $A\in\mathbb{N}$ a contingent dependency model, as in definition 6.11, and a template CPD. The structure of the template CPD in this case is more complex, and we discuss it in detail in section 6.4.2.2. 

Intuitively, the template parents in a PRM, as in the plate model, define a template for the parent assignments in the ground network, which will correspond to specific assignments of the logical variables to objects of the appropriate type. In this setting, however, the set of logical variables in the parents is not necessarily a subset of the logical variables in the child. 

The ability to introduce new logical variables into the specification of an attribute’s par- ents gives us significant expressive power, but introduces some significant challenges. These challenges clearly manifest in the construction of the ground network. 

A PRM $\mathcal{M}_{P R M}$ and object skeleton $\kappa$ define a ground Bayesian network $\mathcal{B}_{\kappa}^{\mathcal{M}_{P R M}}$ as follows. Let $A (U_{1},\dots, U_{k})$ be any template attribute in ℵ . Then, for any assig $\gamma\,\in\,\Gamma_{\kappa}[A]$ we have $a$ variable $A (\gamma)$ in the ground network. This variable has, for any $B\,\in\,\mathrm{Pa}_{A}^{\Gamma}\cup\mathrm{Pa}_{A}$ ∈ and any assignment $\gamma^{\prime}$ to $\alpha (\mathrm{Pa}_{A})-\alpha (A).$ , the parent that is the instantiated variable $B (\gamma,\gamma^{\prime})$ . 

An important subtlety in this definition is that the attributes that appear in the guard are also parents of the ground variable. This requirement is necessary, because the values of the guard attributes determine whether there is a dependency on the parents or not, and hence they afect the probabilistic model. 

Using this definition for the model of example 6.14, we have that Satisfaction $\iota (s, c)$ has the parents: Teaching-Ability $\mathbf{\sigma}(p)$ , Registered $(s, c)$ , and Teaches $\cdot (p, c)$ for every professor $p$ . The guard in the contingent dependency is intended to encode the fact that the dependency on Teaching-Ability $(p)$ is only present for a subset of individuals $p$ , but it is not obvious how that fact afects the construction of our model. The situation is even more complex in example 6.15, where we have as parents of Genotype $(u)$ all of the variables of the form Father $\cdot (v, u)$ and Genotype ( v ) , for all person objects $v$ , and similarly for $M o t h e r (v, u)$ and Genotype $(v)$ . In both cases, the resulting ground network is very densely connected. In the Genetics network, it is also obviously cyclic. We will describe how to encode such dependencies correctly within the CPD of the ground network, and how to deal with the issues of potential cyclicity. 

#### 6.4.2.2 CPDs in the Ground Network 

As we just discussed, the ground network induced by a PRM can introduce a dependency of a variable on a set of parents that is not fixed in advance, and which may be arbitrarily large. How do we encode a probabilistic dependency model for such dependencies? 

Exploiting the Guard Structure The first key observation is that the notion of a contin- gent dependency is intended to specifically capture context-specific independence: In defini- tion 6.12, if the guard for a parent $B$ of $A$ is false for a particular assignment $(\gamma,\gamma^{\prime})$ , then there is no dependency of $A (\gamma)$ on $B (\gamma,\gamma^{\prime})$ . For example, unless Mother $\cdot (v, u)$ is true for a particular pair $(u, v)$ , we have no dependence of Genotype ( u ) on Genotype $(v)$ . Similarly, unless Registered $(s, c)\ \wedge$ Teaches $\!:\! (p, c)$ is true, there is no dependence of Satisfaction $(s, c)$ on Teaching-Ability $(p)$ . We can easily capture this type of context-specific independence in the CPD using a variant of the multiplexer CPD of definition 5.3. 

While this approach helps us specify the CPD in these networks of potentially unbounded indegree, it does not address the fundamental problem: the dense, and often cyclic, connectivity structure. A common solution to this problem is to assume that the guard predicates are properties of the basic relational structure in the domain, and are often fixed in advance. For example, in the tem oral setting, the Precedes relation is always fixed: time point $t-1$ always precedes time point t . Somewhat less obviously, in our Genetics example, it may be reasonable to assume that the pedigree is known in advance. 

relational skeleton 

We can encode this assumption by defining a relational skeleton $\kappa_{r}$ , which defines a certain set of facts (usually relationships between objects) that are given in advance, and are not part of the probabilistic model. In cases where the values of the attributes in the guards are specified as part of the relational skeleton, we can simply use that information to determine the set of parents that are active in the model, usually a very limited set. Thus, for example, if Registered and Teaches are part of the relational skeleton, then Satisfaction $(s, c)$ has the parent Teaching-Ability $\mathbf{\sigma}(p)$ only when Registered $\left (s, c\right)$ and $T e a c h e s (p, c)$ both hold. Similarly, in the Genetics example, if the pedigree structure is given in the skeleton, we would have that Genotype $(v)$ is a parent of Genotype ( u ) only if $M o t h e r (v, u)$ or Father $\cdot (v, u)$ are present in the skeleton. Moreover, we see that, assuming a legal pedigree, the resulting ground network in the Genetics domain is guaranteed to be acyclic. Indeed, the resulting model produces ground networks that are precisely of the same type demonstrated in box 3.B. The use of contingent dependencies allows us to exploit relations that are determined by our skeleton to produce greatly simplified models, and to make explicit the fact that the model is acyclic. 

relational uncertainty 

The situation becomes more complex, however, if the guard predicates are associated with a probabilistic model, and therefore are random variables in the domain. Because the guards are typically associated with relational structure, we refer to this type of uncertainty as relational uncertainty . Relational uncertainty introduces significant complexity into our model, as we now cannot use background knowledge (from our skeleton) to simplify the dependency structure in contingent dependencies. In this case, when the family tree is uncertain, we may indeed have that Genotype ( u ) can depend on every other variable Genotype ( v ) , a model that is cyclic and ill defined. However, if we restrict the distribution over the Mother and Father relations so as to ensure that only “reasonable” pedigrees get positive probability, we can still guarantee that our probabilistic model defines a coherent probability distribution. However, defining a probability distribution over the Mother and Father relations that is guaranteed to have this property is far from trivial; we return to this issue in section 6.6. 

Aggregating Dependencies By itself, the use of guards may not fully address the problem of defining a parameter iz ation for the CPDs in a PRM. Consider again a dependency of $A$ on an attribute $B$ that involves some set of logical variables $U^{\prime}$ that are not in $\alpha (A)$ . Even if we assume that we have a relational skeleton that fully determines the values of all the guards, there may be multiple assignments $\gamma^{\prime}$ to $U^{\prime}$ for which the guard holds, and hence multiple diferent ground parents $B (\gamma,\gamma^{\prime})$ — one for each distinct assignment $\gamma^{\prime}$ to $U^{\prime}$ . Even in our simple University example, there may be multiple instructors for a course $c_{i}$ , and therefore multiple ground variables Teaching-Ability $\mathbf{\sigma}(p)$ that are parents of a ground variable Satisfaction $(s, c)$ . In general, the number of possible instantiations of a given parent $B$ is not known in advance, and may not even be bounded. Thus, we need to define a mechanism for specifying a template-level local dependency model that allows a variable number of parents. Moreover, because the parents corresponding to diferent instantiations are interchangeable, the local dependency model must be symmetric. 

aggregator CPD 

There are many possible ways of specifying such a model. One approach is to use one of the symmetric local probability models that we saw in chapter 5. For example, we can use a noisy-or (section 5.4.1) or logistic model (see section 5.4.2), where all parents have the same parameter. An alternative approach is to define an aggregator CPD that uses certain aggregate statistics or summaries of the set of parents of a variable. (See exercise 6.7 for some analysis of the expressive power of such CPDs.) 

Ability are binary-valued, we might use a noisy-or model: Given a parameter iz ation for Satisfac- tion given a single Teaching-Ability, we can use the noisy-or model to define a general CPD for Satisfaction $(s, c)$ given any set of parents Teaching-Ability ( $p_{1}$ ) , . . . , Teaching-Ability $(p_{m})$ . Alter- natively, we can assume that the student’s satisfaction depends on the worst instructor and best instructor in the course. In this case, we might aggregate the teaching abilities using the min and max functions, and then use a CPD of our choosing to denote the student’s satisfaction as a function of the resulting pair of values. As another example, a student’s job prospects can depend on the average grade in all the courses she has taken. 

When designing such a combination rule, it is important to consider any possible boundary cases. On one side, in many settings the set of parents can be empty: a course may have no instructors (if it is a seminar composed entirely of invited lecturers); or a person may not have a parent in the pedigree. On the other side, we may also need to consider cases where the number of parents is large, in which case noisy-or and logistic models often become degenerate (see figure 5.10 and figure 5.11). 

The situation becomes even more complex when there are multiple distinct parents at the template level, each of which may result in a set of ground parents. For example, a student’s satisfaction in a course may depend both on the teaching ability of multiple instructors and on the quality of the design of the diferent problem sets in the course. We therefore need to address both the aggregation of each type of parent set (instructors or problem sets) as well as combining them into a single CPD. Thus, we need to define some way of combining a set of CPDs $\{P (X\mid Y_{i, 1},.\,.\,.\,, Y_{i, j_{i}})\quad:\quad i\,=\, 1,.\,.\,.\,, l\}$ , to a single joint CPD $\{P (X~|~}$ $Y_{1,1},.\,.\,.\,, Y_{1, j_{1}},.\,.\,.\,, Y_{l, 1},.\,.\,.\,, Y_{l, j_{l}})$ . Here, as before, there is no single right answer, and the particular choice is likely to depend heavily on the properties of the application. 

We note that the issue of multiple parents is distinct from the multiple parents that arise when we have relational uncertainty. In the case of relational uncertainty, we also have multiple parents for a variable in the ground network; yet, it may well be the case that, in any situation, at most one assignment to the logical variables in the parent signature will satisfy the guard condition. For example, even if we are uncertain about John’s paternity, we would like it to be the case that, in any world, there is a unique object $v$ for which Father $\cdot (v, J o h n)$ holds. 

(As we discuss in section 6.6, however, defining a probabilistic model that ensures this type of constraint can be far from trivial.) In this type of situation, the concept of a guard is again useful, since it allows us to avoid defining local dependency models with a variable number of parents in domains (such as genetic inheritance) where such situations do not actually arise. 

#### 6.4.2.3 Checking Acyclicity 

One important issue in relational dependency models is that the dependency structure of the ground network is, in general, not determined in advance, but rather induced from the model structure and the skeleton. How, then, can we guarantee that we obtain a coherent probability distribution? Most obviously, we can simply check, post hoc , that any particular ground network resulting from this process is acyclic. However, this approach is unsatisfying from a model design perspective. When constructing a model, whether by hand or using learning methods, we would like to have some guarantees that it will lead to coherent probability distributions. 

Thus, we would like to provide a test that we can execute on a mod $\mathcal{M}_{P R M}$ at the template level , and which will guarantee that ground distributions induced from M $\mathcal{M}_{P R M}$ will be coherent. 

![](images/aea016ac9cf2d4c81c74eab2678c07ca2c96ead2838bb25f607d051aa3d027da.jpg) 
Figure 6.9 Examples of dependency graphs: (a) Dependency graph for the University example. (b) Dependency graph for the Genetics example. 

One approach for doing so is to construct a template-level graph that encodes a set of potential dependencies that may happen at the ground level. The nodes in the graph are the template- level attributes; there is an edge from $B$ to $A$ if there is any possibility that a ground variable of type $B$ will inﬂuence one of type $A$ . 

Definition 6.13 template dependency graph 

$A$ template dependenc raph for a template depen ncy model $\mathcal{M}_{P R M}$ contains a node for each template-level attribute A , and a directed edge from B to A whenever there is an attribute of type $B$ in $\mathrm{Pa}_{A}^{\Gamma}\cup\mathrm{Pa}_{A}$ . 

This graph can easily be constructed from the definition of the dependency model. For example, the template dependency graph of our University model (example 6.14) is shown in figure $6.9\mathbf{a}$ . 

It is not difcult to show that if template dependency graph for el $\mathcal{M}_{P R M}$ is acyclic (as in this case), it is clear that any ground network generated from M $\mathcal{M}_{P R M}$ mus be acyclic (see exercise 6.8). However, a cycle in the t e dependency graph for M $\mathcal{M}_{P R M}$ does not imply that every ground network induced by M $\mathcal{M}_{P R M}$ is cyclic. (This is the case, however, for any nondegenerate instantiation of a plate model; see exercise 6.9.) Indeed, there are template models that, although cyclic at the template levels, reﬂect a natural dependency structure whose ground networks are guaranteed to be acyclic in practice. 

Example 6.17 Consider the template dependency graph of our Genetics domain, shown in figure $6.9b.$ . The template-level self-loop involving Genotype ( Person ) reﬂects a ground-level dependency of a person’s genotype on that of his or her parents. This type of dependency can only lead to cycles in the ground network if the pedigree is cyclic, that is, a person is his/her own ancestor. Because such cases (time travel aside) are impossible, this template model cannot result in cyclic ground networks for the skeletons that arise in practice . Intuitively, in this e, we have an (acyclic) ordering $\prec$ e objects (people) in the domain, which implies u $u^{\prime}$ can be nt of u only when u $u^{\prime}\prec u_{\downarrow}$ ≺ ; therefore, Genotype ( u ) can depend on Genotype $(u^{\prime})$ only when u $u^{\prime}\prec u$ ≺ . This ordering on objects is acyclic, and therefore so is the resulting dependency structure. 

The template dependency graph does not account for these constraints on the skeleton, and therefore we cannot conclude by examining the graph whether cycles can occur in ground networks for such ordered skeletons. However, exercise 6.10 discusses a richer form of the template dependency network that explicitly incorporates such constraints, and it is therefore able to determine that our Genetics model results in acyclic ground networks for any skeleton representing an acyclic pedigree. 

So far in our discussion of acyclicity, we have largely sidestepped the issue of relational uncertainty. As we discussed, in the case of relational uncertainty, the ground network contains many “potential” edges, only a few of which will ever be “active” simultaneously. In such cases, the resulting ground network may not even be acyclic, even though it may well define a coherent (and sparse) probability distribution for every relational structure. Indeed, as we discussed in example 6.17, there are models that are potentially cyclic but are guaranteed to be acyclic by virtue of specific constraints on the dependency structure. It is thus possible to guarantee the coherence of a cyclic model of this type by ensuring that there is no positive-probability assignment to the guards that actually induces a cyclic dependence between the attributes. 

## 6.5 Undirected Representation 

The previous sections describe template-based formalisms that use a directed graphical model as their foundation. One can define similar extensions for undirected graphical models. Many of the ideas underlying this extension are fairly similar to the directed case. However, the greater ﬂexibility of the undirected representation in avoiding local normalization requirements and acyclicity constraints can be particularly helpful in the context of these richer representations. Eliminating these requirements allows us to easily encode a much richer set of patterns about the relationships between objects in the domain; see, for example, box 6.C. In particular, as we discuss in section 6.6, these benefits can be very significant when we wish to define distributions over complex relational structures. 

The basic component in a template-based undirected model is some expression, written in terms of template-level attributes with logical variables as arguments, and associated with a template factor. For a given object skeleton, each possible assignment $\gamma$ to the logical variables in the expression induces a factor in the ground undirected network, all sharing the same parameters. As for variable-based undirected representations, one can parameterize a template- based undirected probabilistic model using full factors, or using features, as in a log-linear model. This decision is largely orthogonal to other issues. We choose to use log-linear features, which are the finest-grained representation and subsume table factors. 

Let us begin by revisiting our Misconception example in section 4.1. Now, assume that we are interested in defining a probabilistic model over an entire set of students, where some number of pairs study together. We define a binary predicate (relation) Study-Pair $\!\left (S_{1}, S_{2}\right)$ , which is true when two students $S_{1}, S_{2}$ study together, and a predicate (attribute) Misconception $(S)$ that encodes the level of understanding of a student $S$ . We can now define $a$ template feature $f_{M}$ , over pairs Misconception $\left (S_{1}\right)$ , Misconception $. (S_{2})$ , which takes value 1 whenever 

$$
[S t u d y–P a i r (S_{1}, S_{2})=t r u e\wedge M i s c o n c e p t i o n (S_{1})=M i s c o n c e p t i o n (S_{2})]=t r u e
$$ 

and has value $O$ otherwise. 

Definition 6.14 relational Markov network feature argument 

object skeleton 

Example 6.19 

Definition 6.15 ground Gibbs distribution 

$A$ ional Markov network $\mathcal{M}_{R M N}$ is defined in terms of a set $\Lambda$ of template features , where each $\lambda\in\Lambda$ ∈ comprises: 

• a real-valued template feature $f_{\lambda}$ whose arguments are $\aleph (\lambda)=\{A_{1}(\pmb{U}_{1}),.\,.\,.\,, A_{l}(U_{l})\},$ ; • a weight $w_{\lambda}\in\mathbb{R}$ . We define $\alpha (\lambda)$ so that for all $i$ , $U_{i}\subseteq\alpha (\lambda)$ . In example 6.18, we have that $\alpha (\lambda_{M})=\{S_{1}, S_{2}\}$ , both of type Student ; and $\aleph (\lambda_{M})=\{S t u d y{\mathrm{-}}P a i r (S_{1}, S_{2}), M i s c o n c e p t i o n (S_{1}), M i s c o n c e p t i o n (S_{2})\}.$ 

To specify a ground network using an RMN, we must provide an object skeleton $\kappa$ that defines ts ${\mathcal{O}}^{\kappa}[\mathsf{Q}]$ for each class $\mathsf Q$ . As before, we can also define a restricted set $\Gamma_{\kappa}[A]\,\subset\,\mathcal{O}^{\kappa}[\alpha (A)]$ ⊂O . Given a skeleton, we can now define a ground Gibbs distribution in the natural way: 

Continuing example 6.18, assume we are given a skeleton containing a particular set of students and the set of study pairs within this set. This model induces a Markov network where the ground random variables have the form Misconception $(s)$ for every student s in the domain. In this model, we have a feature $f_{M}$ for every triple of variables Misconception $\left (s_{1}\right)$ , Misconception $\left (s_{2}\right)$ , Study-Pair $\left (s_{1}, s_{2}\right)$ . As usual in log-linear models, features can be associated with a weight; in this example, we might choose $w_{M}\,=\, 10$ . In this case, the unnormalized measure for a given assignment to the ground variables would be $\exp (10K)$ , where $K$ is the number of pairs $s_{1}, s_{2}$ for which equation (6.6) holds. 

More formally: 

Given an RMN M and an object skeleton $\kappa$ , we can define a ground Gibbs distribution $P_{\kappa}^{\mathcal{M}_{R M N}}$ RMN as follows: 

• The variables in the network are $\mathcal{X}_{\kappa}[\aleph]$ (as in definition 6.7); • $P_{\kappa}^{\mathcal{M}_{R M N}}$ contains a term $\exp (w_{\lambda}\cdot f_{\lambda}(\gamma))$ for each feature template $\lambda\in\Lambda$ and each assignment $\gamma\in\Gamma_{\kappa}[\alpha (\lambda)]$ . 

As always, a (ground) Gibbs distribution defines a Markov network, where we connect every pair of variables that appear together in some factor. 

In the directed setting, the dense connectivity arising in ground networks raised several concerns: acyclicity, aggregation of dependencies, and computational cost of the resulting model. The first of these is obviously not a concern in undirected models. The other two, however, deserve some discussion. 

Although better hidden, the issue of aggregating the contribution of multiple assignments of a feature also arises in undirected model. Here, the definition of the Gibbs distribution dictates the form of the aggregation we use. In this case, each grounding of the feature defines a factor in the unnormalized measure, and they are combined by a product operation, or an addition in log-space. In other words, each occurrence of the feature has a log-linear contribution to the unnormalized density. Importantly, however, this type of aggregation may not be appropriate for every application. 

Example 6.20 Consider a model for “viral marketing” — a social network of individuals related by the Friends ${\bf\mathcal{(}}P, P^{\prime})$ relation, where the attribute of interest Gadget $\mathopen{}\mathclose\bgroup\left (P\aftergroup\egroup\right)$ is the purchase of some cool new gadget $G$ . We may want to construct a model where it is more likely that two friends either both own or both do not own $G$ . That is, we have a feature similar to $\lambda_{M}$ in example 6.18. In the log-linear model, the unnormalized probability that a person $p$ purchases $G$ grows log-linearly with the number $k_{p}$ of his friends who own $G$ . However, a more realistic model may involve a saturation efect, where the impact diminishes as $k_{p}$ grows; that is, the increase in probability of Gadget $\cdot (P)$ between $k_{p}=0$ and $k_{p}=1$ is greater than the increase in probability between $k_{p}=20$ and $k_{p}=21$ . 

Thus, in concrete applications, we may wish to extend the framework to allow for other forms of combination, or even simply to define auxiliary variables corresponding to relevant aggregates (for example, the value of $k_{p}$ in our example). 

The issue of dense connectivity is as much an issue in the undirected case as in the directed case. The typical solution is similar: If we have background knowledge in the form of a relational skeleton, we can significantly simplify the resulting model. Here, the operation is very simple: we simply reduce every one of the factors in our model using the evidence contained in the relational skeleton, producing a reduced Markov network, as in definition 4.7. In this network, we would eliminate any ground variables whose values are observed in the skeleton and instantiate their (fixed) values in any ground factors containing them. In many cases, this process can greatly simplify the resulting features, often making them degenerate. 

Returning to example 6.18, assume now that our skeleton specifies the instantiation of the relation Study-Pair, so that we know exactly which pairs of students study together and which do not. Now, consider the reduced Markov network obtained by conditioning on the skeleton. As all the variables Study-Pai $r (s_{1}, s_{2})$ are observed, they are all eliminated from the network. Moreover, for any pair of students $s_{1}, s_{2}$ for which Study-Pair $\cdot (s_{1}, s_{2})=f a l s o$ e, the feature $\lambda_{M}(s_{1}, s_{2})$ necessarily takes the value 0 , regardless of the values of the other variables in the feature. Because this ground feature is vacuous and has no impact on the distribution, it can be eliminated from the model. The resulting Markov network is much simpler, containing edges only between pairs of students who study together (according to the information in the relational skeleton). 

We note that we could have introduced the notion of a guarded dependency, as we did for PRMs. However, this component is far less useful here than it was in the directed case, where it also served a role in eliminating the need to aggregate parents that are not actually present in the network and in helping clarify the acyclicity in the model. Neither of these issues arises in the undirected framework, obviating the need for the additional notational complexity. 

Finally, we mention one subtlety that is specific to the undirected setting. An undirected model uses nonlocal factors, which can have a dramatic inﬂuence on the global probability measure of the model. Thus, the probability distribution defined by an undirected relational model is not modular: Introducing a new object into the domain can drastically change the distribution over the properties of existing objects, even when the newly introduced object seems to have no meaningful interactions with the previous objects. 

Let us return to our example 6.19, and assume that any pair of students study together with some probability $p_{\cdot}$ ; that is, we have an additional template feature over Study-Pair $\!\left (S_{1}, S_{2}\right)$ that takes the value $\log{p}$ when this binary attribute is true and $\log (1-p)$ otherwise. 

Assume that we have a probability distribution over the properties of some set of students $\mathcal{O}^{\kappa}[\mathsf{S t u d e n t}]\,=\,\{s_{1},.\,.\,.\,, s_{n}\}$ , and let us study how this distribution changes if we add a new student $s_{n+1}$ . Consider an assignment to the properties of $s_{1},\ldots, s_{n}$ in which m of the n students $s_{i}$ have Misconception $(s_{i})=1$ , whereas the remaining $n-m$ have Misconc tion $(s_{i})=0$ . We can now consider the following situations with respect to $s_{n+1}$ : he studies with k of the m students for whom Misconception $(s_{i})=1$ , with $\ell$ of the $n-m$ students for whom Misconception $\left (s_{i}\right)=0$ , and himself has Misconception $\iota (s_{n+1})=c$ (for $c\in\{0,1\}.$ ). The probability of each such event is 

$$
\binom{m}{k}\binom{(n-m)}{\ell}p^{\ell}(1-p)^{(n-m-\ell)})(10^{k c}\cdot10^{\ell (1-c)}),
$$ 

where the first two terms come from the factors over the Study-Pair $\cdot (S_{1}, S_{2})$ structure, and the final term comes from the template feature $\lambda_{M}$ . We want to compute the marginal distribution over our original variables (not involving $s_{n+1}$ ), to see whether introducing $s_{n+1}$ changes this distribution. Thus, we sum out over all of the preceding events, which (using simple algebra) is $(10p+(1-p))^{m}+(10p+(1-p))^{n-m}$ . 

This analysis shows that the assignments to our original variables are multiplied by very diferent terms, depending on the value $m$ . In particular, the probability of joint assignments where $m=0$ , so that all students agree, are multiplied by a factor of $(10p+(1-p))^{n}$ , whereas the probability of joint assignments where the students are equally divided in their opinion are multiplied by $(10p+(1-p))^{n/2}$ , an exponentially smaller factor. Thus, adding a new student, even one about whom we appear to know nothing, can drastically change the properties of our probability distribution. 

Thus, for undirected models, it can be problematic to construct (by learning or by hand) a template-based model for domains of a certain size, and apply it to models of a very diferent size. The impact of the domain size on the probability distribution varies, and therefore the implications regarding our ability to apply learning in this setting need to be evaluated per application. 

Box 6. C — Case Study: Collective Classification of Web Pages. One application that calls for interesting models of interobject relationships is the classification of a network of interlinked web- pages. One example is that of a university website, where webpages can be associated with students, faculty members, courses, projects, and more. We can associate each webpage w with a hidden variable $T (w)$ whose value denotes the type of the entity to which the webpage belongs. In a standard classification setting, we would use some learned classifier to label each webpage based on its features, such as the words on the page. However, we can obtain more information by also considering the interactions between the entities and the correlations they induce over their labels. For example, an examination of the data reveals that student pages are more likely to link to faculty webpages than to other student pages. 

One can capture this type of interaction both in a directed and in an undirected model. In a directed model, we might have a binary attribute $L i n k s (W_{1}, W_{2})$ that takes the value true if $W_{1}$ links to $W_{2}$ and false otherwise. We can then have $L i n k s (W_{1}, W_{2})$ depend on $T (W_{1})$ and $T (W_{2})$ , capturing the dependence of the link probability on the classes of the two linked pages. An alternative approach is to use an undirected model, where we directly introduce a pairwise template feature over $T (W_{1}), T (W_{2})$ for pairs $W_{1}, W_{2}$ such that $L i n k s (W_{1}, W_{2})$ . Here, we can give higher potentials to pairs of types that tend to link, for example, student-faculty, faculty-course, faculty-project, project-student, and more. 

A priori, both models appear to capture the basic structure of the domain. However, the directed model has some significant disadvantages in this setting. First, since the link structure of the webpages is known, the $L i n k s (W_{1}, W_{2})$ is always observed. Thus, we have an active $\nu$ -structure connecting every pair of webpages, whether they are linked or not. The computational disadvantages of this requirement are obvious. Less obvious but equally important is the fact that there are many more non-links than links, and so the signal from the absent links tends to overwhelm the signal that could derived from the links that are present. In an undirected model, the absent links are simply omitted from the model; we simply introduce a potential that correlates the topics of two webpages only if they are linked. Therefore, an undirected model generally achieves much better performance on this task. 

Another important advantage of the undirected model for this task is its ﬂexibility in incorporating a much richer set of interactions. For example, it is often the case that a faculty member has a section in her webpage where she lists courses that she teaches, and another section that lists students whom she advises. Thus, another useful correlation that we may wish to model is one between the types of two webpages that are both linked from a third, and whose links are in close proximity on the page. We can model this type of interaction using features of the form $\begin{array}{r}{C l o s e–L i n k s (W, W_{1}, W_{2})\wedge T (W_{1})=t_{1}\wedge T (W_{2})=t_{2}}\end{array}$ , where Close-Links $\mathfrak{s}(W, W_{1}, W_{2})$ is derived directly from the structure of the page. 

Finally, an extension of the same model can be used to label not only the entities (webpages) but also the links between them. For example, we might want to determine whether a student- professor $(s, p)$ pair with a link from s to $p$ represents an advising relationship, or whether a linked professor-course pair represents an instructor relationship. Once again, a standard classifier would make use of features such as words in the vicinity of the hyperlink. At the next level, we can use an extension of the model described earlier to classify jointly both the types of the entities and the types of the links that relate them. In a more interesting extension, a relational model can also utilize higher-level patterns; for example, using a template feature over triplets of template attributes $T (W)$ , we can encode the fact that students and their advisors belong to the same research group, or that students often serve as teaching assistants in courses that their advisors teach. 

## 6.6 Structural Uncertainty $\star$ 

structural uncertainty The object-relational probabilistic models we described allow us to encode a very rich family of distributions over possible worlds. In addition to encoding distributions over the attributes of objects, these approaches can allow us to encode structural uncertainty — a probabilistic model over the actual structure of the worlds, both the set of objects they contain and the relations 

between them. The diferent models we presented exhibit significant diferences in the types of structural uncertainty that they naturally encompass. In this section, we discuss some of the major issues that arise when representing structural uncertainty, and how these issues are handled by the diferent models. 

relational uncertainty object uncertainty 

There are two main types of structural uncertainty that we can consider: relational uncertainty , which models a distribution over the presence or absence of relations between objects; and object uncertainty , which models a distribution over the existence or number of actual objects in the domain. We discuss each in turn. 

### 6.6.1 Relational Uncertainty 

The template representations we have already developed already allow us to encode uncertainty about the relational structure. As in example 6.22, we can simply make the existence of a relationship a stochastic event. What types of probability distributions over relational structure can we encode using these representational tools? In example 6.22, each possible relation Study-Pair $\left (s_{1}, s_{2}\right)$ is selected independently, at random, with probability $p$ . Unfortunately, such graphs are not representative of most relational structures that we observe in real-world settings. 

Example 6.23 

Let us select an even simpler example, where the graph we are constructing is bipartite. Consider the relation Teaches $\cdot (P, C)$ , and assume that it takes the value true with probability 0 . 1 . Consider a skeleton that contains $l0$ professors and 20 courses. Then the expected number of courses per professor is 2, and the expected number of professors per course is 1. So far, everything appears quite reasonable. However, the probability that, in the resulting graph, a particular professor teaches $\ell$ courses is distributed binomially: ${\binom{20}{\ell}}0.1^{\ell}0.9^{20-\ell}$  . For example, the probability that any single professor teaches 5 or more courses is 4.3 percent, and the probability that at least one of them does is around 29 percent. This is much higher than is realistic in real-world graphs. The situation becomes much worse if we increase the number of professors and courses in our skeleton. 

Of course, we can add parents to this attribute. For example, we can let the presence of an edge depend on the research area of the professor and the topic of the course, so that this attribute is more likely to take the value true if the area and topic match. However, this solution does not address the fundamental problem: it is still the case that, given all of the research areas and topics, the relationship status for diferent pairs of objects (the edges in the relational graph) are chosen independently. 

In this example, we wish to model certain global constraints on the distribution over the graph: the fact that each faculty member tends to teach only a small subset of courses. Unfortunately, it is far from clear how to incorporate this constraint into a template-level generative (directed) model over attributes corresponding to the presence of individual relations. Indeed, consider even the simpler case where we wish to encode the prior knowledge that each course has exactly one instructor. This model induces a correlation among all of the binary random variables corresponding to diferent instantiations $T e a c h e s (p, c)$ for diferent professors $p$ and the same course $c$ : once we have $T e a c h e s (p, c)\:=\: t r u e,$ we must have Teaches $(p^{\prime}, c)=f a l s e$ for all $p^{\prime}\neq p$ . In order to incorporate this correlation, we would have to define a generative process that “selects” the relation variables $T e a c h e s (p, c)$ in some sequence, in a way that allows each Teaches $\cdot (p^{\prime}, c)$ to depend on all of the preceding variables Teaches $(p, c)$ . This induces dependency models with dense connectivity, an arbitrary number of parents per variable (in the ground network), and a fairly complex dependency structure. 

object-valued attribute 

An alternative approach is to use a diferent encoding for the course-instructor relationship. In logical languages, an alternative mechanism for relating objects to each other is via functions. A function , or object-valued attribute takes as argument a tuple of objects from a given set of classes, and returns a set of objects in another class. Thus, for example, rather than having a relation $M o t h e r (P_{1}, P_{2})$ , we might use a function Mother- $\mathsf{o f f}(P_{1})\mapsto$ Person that takes, as argument, a person-object $p_{1}$ , and returns the person object $p_{2}$ , which is $p_{1}$ ’s mother. In this case, the return-value of the function is just a single object, but, in general, we can define functions that return an entire set of objects. In our University example, the relation Teaches defines the function Courses-Of , which maps from professors to the courses they teach, and the function Instructor , which maps from courses to the professors that teach them. We note that these functions are inverses of each other: We have that a professor $p$ is in Instructor $\cdot (c)$ if and only if $c$ is in Courses- $\cdot O f (p)$ . 

As we can see, we can easily convert between set-valued functions and relations. Indeed, as long as the relational structure is fixed, the decision on which representation to use is largely a matter of convenience (or convention). However, once we introduce probabilistic models over the relational structure, the two representations lend themselves more naturally to quite diferent types of model. Thus, for example, if we encode the course-instructor relationship as a function from professors to courses, then rather than select pairwise relations at random, we might select, for any professor $p$ , the set of courses Courses- $O f (p)$ . We can define a distribution over sets in two components: a distribution over the size $\ell$ of the set, and a distribution that then selects $\ell$ distinct objects that will make up the set. 

Assume we want to define a probability distribution over the set Courses- $\cdot O f (p)$ of courses taught by a professor $p$ . We may first define a distribution over the number $\ell$ of courses c in Courses- $O f (p)$ . This distribution may depend on properties of the professor, such as her department or her level of seniority. Given the size $\ell.$ , we now have to select the actual set of $\ell$ courses taught by $p$ . We can define a model that selects $\ell$ courses independently from among the set of courses at the university. This choice can depend on properties of both the professor and the course. For example, if the professor’s specialization is in artificial intelligence, she is more likely to teach a course in that area than in operating systems. Thus, the probability of selecting c to be in Courses- $O f (p)$ depends both on Topic $\cdot (c)$ and on Research-Area $(p)$ . Importantly, since we have already chosen $\ell=|C o u r s e s{\cdot}O f (p)|$ , we need to ensure that we actuall select $\ell$ distinct courses, that is, we must sample from the courses without replacement. Thus, our ℓ sampling events for the diferent courses cannot be completely independent. 

While useful in certain settings, this model does not solve the fundamental problem. For example, although it allows us to enforce that every professor teaches between two and four courses, it still leaves open the possibility that a single course is taught by ten professors. We can, of course, consider a model that reverses the direction of the function, encoding a distribution over the instructors of each course rather than the courses taught by a professor, but this solution would simply raise the converse problem of the possibility that a single professor teaches a large number of classes. 

It follows from this discussion that it is difcult, in generative (directed) representations, to define distributions over relational structures that guarantee (or prefer) certain structural properties of the relation. For example, there is no natural way in which we can construct a probabilistic model that exhibits (a preference for) transitivity, that is, one satisfying that if $R (u, v)$ and $R (v, w)$ then (it is more likely that) $R (u, w)$ . 

These problems have a natural solution within the undirected framework. For example, a preference for transitivity can be encoded simply as a template feature that ascribes a high value to the (template) event 

$$
R (U, V)=t r u e,R (V, W)=t r u e,R (U, W)=t r u e.
$$ 

A (soft) constraint enforcing at most one instructor per course can be encoded similarly as a (very) low potential on the template event 

$$
T e a c h e s (P, C)=t r u e, T e a c h e s (P^{\prime}, C)=t r u e.
$$ 

A constraint enforcing at least one instructor per course cannot be encoded in the framework of relational Markov networks, which allow only features with a bounded set of arguments. However, it is not difcult to extend the language to include potentials over unbounded sets of variables, as long as these potentials have a compact, finite-size representation. For example, we could incorporate an aggregator feature that counts the number $t_{p}$ of objects $c$ such that Teaches $\cdot (p, c)$ , and introduce a potential over the value of $t_{p}$ . This extension would allow us to incorporate arbitrary preferences about the number of courses taught by a professor. At the same time, the model could also include potentials over the aggregator $i_{c}$ that counts the number of instructors $p$ for a course $c$ . Thus, we can simultaneously include global preferences on both sides of the relation between courses and professors. 

However, while this approach addresses the issue of expressing such constraints, it leaves unresolved the problem of the complexity of the resulting ground network. In all of these examples, the induced ground network is very densely connected, with a ground variable for every potential edge in the relational graph (for example, $R (u, v))$ , and a factor relating every pair or even every triple of these variables. In the latter examples, involving the aggregator, we have potentials whose scope is unbounded, containing all of the ground variables $R (u, v)$ . 

### 6.6.2 Object Uncertainty 

So far, we have focused our discussion on representing probabilistic models about the presence or absence of certain relations, given a set of base objects. One can also consider settings in which even the set of objects in the world is not predetermined, and so we wish to define a probability distribution over this set. 

Perhaps the most common setting in which this type of reasoning arises is in situations where diferent objects in our domain may be equal to each other. This situation arises quite often. For example, a single person can be student $\#34$ in CS101, student $\#57$ in Econ203, the eldest daughter of John and Mary, the girlfriend of Tom, and so on. 

One solution is to allow objects in the domain to correspond to diferent “names,” or ways of referring to an object, but explicitly reason about the probability that some of these names refer to the same object. But how do we model a distribution over equality relationships between the objects playing diferent roles in the model? 

The key insight is to introduce explicitly into the model the notion of a “reference” to an object, where the same object can be referred to in several diferent ways. That is, we include in the model objects that correspond to the diferent “references” to the object. Thus, for example, we could have a class of “person objects” and another class for “person reference objects.” We can use a relation-based representation in this setting, using a relation Refers- $t o (r, p)$ that is true whenever the reference $r$ refers to a person $p$ . However, we must also introduce uniqueness constraints to ensure that a reference $r$ refers to precisely a single person $p$ . Alternatively, a more natural approach is to use a function, or object-valued attribute, Referent $\cdot (r)$ , which designates the person to whom $r$ refers. This approach automatically enforces the uniqueness constraints, and it is thus perhaps more appropriate to this application. 

In either case, the relationship between references and the objects to which they refer is generally probabilistic and interacts probabilistic ally with other attributes in the domain. In particular, we would generally introduce factors that model the similarity of the properties of a “reference object” $r$ and those of the true object $p$ to which it refers. These attribute similarity potentials can be constructed to allow for noise and variation. For example, we can model the fact that a person whose name is “John Franklin Adams” may decide to go by “J.F. Adams” in one setting and “Frank Adams” in another, but is unlikely to go by the name “Peggy Smith.” We can also model the fact that a person may decide to “round down” his or her reported age in some settings (for example, social interactions) but not in others (for example, tax forms). The problem of determining the correspondence between references and the entities to which they refer is an instance of the correspondence problem, which is described in detail in box 12.D. $\mathrm{Box}\; 6.\mathrm{D}$ describes an application of this type of model to the problem of matching bibliographical citations. 

In an alternative approach, we might go one step further, we can eliminate any mention of the true underlying objects, and restrict the model only to object references. In this solution, the domain contains only “reference objects” (at least for some classes). Now, rather than mapping references to the object to which they refer, we simply allow for diferent references to “correspond” to each other. Specifically, we might include a binary predicate Same- $a s (\boldsymbol{r},\boldsymbol{r}^{\prime})$ , which asserts that $r$ and $r^{\prime}$ both refer to the same underlying object (not included as an object in the domain). 

To ensure that Same-As is consistent with the semantics of an equality relation, we need to introduce various constraints on its properties. (Because these constraints are standard axioms of equality, we can include them as part of the formalism rather than require each user to specify them.) First, using the ideas described in section 6.6.1, we can introduce undirected

 (hard) potentials to constrain the relation to satisfy: 

• Reﬂexivity — Same- $\cdot A s (r, r)$ ;

 • Symmetry — Same $\cdot A s (r, r^{\prime})$ if and only if Same- $\mathit{A s}(\mathit{r}^{\prime},\mathit{r})$ ;

 • Transitivity — Same- $\cdot A s (r, r^{\prime})$ and Same- $4s (r^{\prime}, r^{\prime\prime})$ implies Same- $\mathbf{\nabla}_{: A s}(r, r^{\prime\prime})$ . 

These conditions imply that the Same-As relation defines an equivalence relation on reference objects, and thus partitions them into mutually exclusive and exhaustive equivalence classes. Importantly, however, these constraints can only be encoded in an undirected model, and therefore this approach to dealing with equality only applies in that setting. In addition, we include in the model attribute similarity potentials, as before, which indicate the extent to which we expect attributes or predicates for two Same-As reference objects $r$ and $r^{\prime}$ to be similar to each other. This approach, applied to a set of named objects, tends to cluster them together into groups whose attributes are similar and that participate in relations with objects that are also in equivalent groups. 

There are, however, several problems with the reference-only solution. First, there is no natural place to put factors that should apply once per underlying entity. 

Example 6.25 

Suppose we are interested in inferring people’s gender from their names. We might have a potential saying that someone named “Alex” is more likely to be male than female. But if we make this a template factor on $\{N a m e (R)$ , Gender $\cdot (R)\}$ where $R$ ranges over references, then the factor will apply many times to people with many references. Thus, the probability that a person named “Alex” is male will increase exponentially with the number of references to that person. 

A related but more subtle problem is the dependence of the outcome of our inference on the number of references. 

Example 6.26 

Consider a very simple example where we have only references to one type of object and only the at- tribute $A$ , which takes values $1,2,3$ . For each pair of object references $r, r^{\prime}$ such that Same $\cdot A s (r, r^{\prime})$ holds, we have an attribute similarity potential relating $A (r)$ and $A (r^{\prime})$ : the cases of $A (r)=A (r^{\prime})$ have the highest weight $w$ ; $A (r)=1$ , $A (r^{\prime})=3$ has very low weight; and $A (r)=2$ , $A (r^{\prime})=1$ and $A (r)=2,\ A (r^{\prime})=3$ both have the same medium potential $q$ . Now, consider the graph of people related by the Same-As relation: since Same-As is an equivalence relation, the graph is $^a$ set of mutually exclusive and exhaustive partitions, each corresponding to a set of references that correspond to the same object. Now, assume we have a configuration of evidence where we observe $k_{i}$ references with $A (r)=i$ , for $i={1,2,3}$ . The most likely assignment relative to this model will have one cluster with all the $A (r)=1$ references, and another with all the $A (r)=3$ references. What about the references with $A (r)=2?$ 

Somewhat surprisingly, their disposition depends on the relative sizes of the clusters. To under- stand why, we first note that (assuming $w>1.$ ) there are only three solutions with reasonably high probability: three separate clusters; a $^{u}\!{\boldsymbol{I}}\!\!+\!\boldsymbol{Z}^{\prime}$ and a “3” cluster; and a “1” and a $^{u}{}{{+}}3^{\prime\prime}$ cluster. All other solutions have much lower probability, and the discrepancy decays exponentially with the size of the domain. Now, consider the case where $k_{2}=1$ , so that there only one $r^{*}$ with $A (r)=2$ . If we add $r^{*}$ to the “1” cluster, we introduce an attribute similarity potential between $A (r^{*})$ and all of the $A (r)$ ’s in the “1” cluster. This multiplies the overall probability of the configuration by $q^{k_{1}}$ . Similarly, if we add $r^{*}$ to the $"3"$ cluster, the probability of the configuration is multiplied by $q^{k_{3}}$ . Thus, if $q<1$ , the reference $r^{*}$ is more likely to be placed in the smaller of the two clusters; if $q>1$ , it is more likely to be placed in the larger cluster. As $k_{2}$ grows, the optimal solution may now be one where we put the $2s$ into their own, separate cluster; the benefit of doing so depends on the relative sizes of the diferent parameters $q, w, k_{1}, k_{2}, k_{3}$ . 

Thus, in this type of model, the resulting posterior is often highly peaked, and the probabilities of the diferent high-probability outcomes very sensitive to the parameters. By contrast, a model where each equivalence cluster is associated with a single actual object is a lot “smoother,” for the number of attribute similarity potentials induced by a cluster of references grows linearly, not quadratically, in the size of the cluster. 

Box 6. D — Case Study: Object Uncertainty and Citation Matching. Being able to browse the network of citations between academic works is a valuable tool for research. For instance, given one citation to a relevant publication, one might want a list of other papers that cite the same work. There are several services that attempt to construct such lists automatically by extracting citations from online papers. This task is difcult because the citations come in a wide variety of formats, and often contain errors — owing both to the original author and to the vagaries of the extraction process. For example, consider the two citations: 

Elston R, Stewart A. A General Model for the Genetic Analysis of Pedigree Data. Hum. Hered. 1971;21:523–542. Elston RC, Stewart J (1971): A general model for the analysis of pedigree data. Hum Hered 21523–542. 

These citations refer to the same paper, but the first one gives the wrong first initial for J. Stewart, and the second one omits the word “genetic” in the title. The colon between the journal volume and page numbers has also been lost in the second citation. A citation matching system must handle this kind of variation, but must also avoid lumping together distinct papers that have similar titles and author lists. 

Probabilistic object-relational models have proven to be an efective approach to this problem. One way to handle the inherent object uncertainty is to use a directed model with a Citation class, as well as Publication and Author classes. The set of observed Citation objects can be included in the object skeleton, but the number of Publication and Author objects is unknown. 

A directed object-relational model for this problem (based roughly on the model of Milch et al. (2004)) is shown in figure 6.D.1a. The model includes random variables for the sizes of the Author and Publication classes. The Citation class has an object-valued attribute PubCited (C), whose value is the Publication object that the citation refers to. The Publication class has a set-valued attribute Authors (P), indicating the set of authors on the publication. These attributes are given very simple CPDs: for PubCited (C), we use a uniform distribution over the set of Publication objects, and for Authors (P) we use a prior for the number of contributors along with a uniform selection distribution. 

To complete this model, we include string-valued attributes Name (A) and Title (P), whose CPDs encode prior distributions over name and title strings (for now, we ignore other attributes such as date and journal name). Finally, the Citation class has an attribute Text (C), containing the observed text of the citation. The citation text attribute depends on the title and author names of the publication it refers to; its CPD encodes the way citation strings are formatted, and the probabilities of various errors and abbreviations. 

Thus, given observed values for all the $T e x t (c_{i})$ attributes, our goal is to infer an assignment of values to the PubCited attributes — which induces a partition of the citations into coreferring groups. To get a sense of how this process works, consider the two preceding citations. One hypothesis, $H_{1}$ , is that the two citations $c_{1}$ and $c_{2}$ refer to a single publication $p_{1}$ , which has “genetic” in its title. An alternative, $H_{2}$ , is that there is an additional publication $p_{2}$ whose title is identical except for the omission of “genetic,” and $c_{2}$ refers to $p_{2}$ instead. $H_{1}$ obviously involves an unlikely event — a word being left out of a citation; this is reﬂected in the probability of $T e x t (c_{2})$ given $T i t l e (p_{1})$ . But the probability of $H_{2}$ involves an additional factor for $T i t l e (p_{2})$ , reﬂecting the prior probability of the string $^ Ḋ A Ḍ$ general model for the analysis of pedigree data” under our model of academic paper titles. Since there are so many possible titles, this probability will be extremely small, allowing $H_{1}$ to win out. As this example shows, probabilistic models of this form exhibit 

![](images/a889a20ae0773f2c02713db6882988b4fd0ab7b2b55184a9fd1292c414f89dfd.jpg) 
Figure 6.D.1 — Two template models for citation-matching (a) A directed model. (b) An undirected model instantiated for three citations. 

a built-in Ockham’s razor efect: the highest probability goes to hypotheses that do not include any more objects — and hence any more instantiated attributes — than necessary to explain the observed data. 

Another line of work (for example, Wellner et al. (2004)) tackle the citation-matching problem using undirected template models, whose ground instantiation is a CRF (as in section 4.6.1). As we saw in the main text, one approach is to eliminate the Author and Publication classes and simply reason about a relation $S a m e (C, C^{\prime})$ between citations (constrained to be an equivalence relation). Figure 6.D.1b shows an instantiation of such a model for three citations. For each pair of citations $C, C^{\prime}$ , there is an array of factors $\phi_{1},\ldots,\phi_{k}$ that look at various features of $T e x t (C)$ and $T e x t (C^{\prime})$ — whether they have same surname for the first author, whether their titles are within an edit distance of two, and so on — and relate these features to $S a m e (C_{1}, C_{2})$ . These factors encode preferences for and against coreference more explicitly than the factors in the directed model. 

However, as we have discussed, a reference-only model produces overly peaked posteriors that are very sensitive to parameters and to the number of mentions. Moreover, there are some examples where pairwise compatibility factors are insufcient for finding the right partition. For instance, suppose we have three references to people: “Jane,” which is clearly $a$ female’s given name; “Smith,” which is clearly a surname; and “Stanley,” which could be a surname or a male’s given name. Any pair of these references could refer to the same person: there could easily be a Jane Smith, a Stanley Smith, or a Jane Stanley. But it is unlikely that all three names corefer. Thus, a reasonable approach uses an undirected model that has explicit (hidden) variables for each entity and its attributes. The same potentials can be used as in the reference-only model. However, due to the use of undirected dependencies, we can allow the use of a much richer feature set, as described in box 4.E. 

Systems that use template-based probabilistic models can now achieve accuracies in the high 90s for identifying coreferent citations. Identifying multiple mentions of the same author is harder; accuracies vary considerably depending on the data set, but tend to be around 70 percent. These models are also useful for segmenting citations into fields such as the title, author names, journal, and date. This is done by treating the citation text not as a single attribute but as a sequence of tokens (words and punctuation marks), each of which has an associated variable indicating which field it belongs to. These “field” variables can be thought of as the state variables in a hidden Markov model in the directed setting, or a conditional random field in the undirected setting (as in box 4. E). The resulting model can segment ambiguous citations more accurately than one that treats each citation in isolation, because it prefers for segmentations of coreferring citations to be consistent. 

## 6.7 Summary 
The representation languages discussed in earlier chapters — Bayesian networks and Markov networks — allow us to write down a model that encodes a specific probability distribution over a fixed, finite set of random variables. In this chapter, we have provided a general frame- work for defining templates for fragments of the probabilistic model. These templates can be reused both within a single model, and across multiple models of diferent structures. Thus, a template-based representation language allows us to encode a potentially infinite set of distributions, over arbitrarily large probability spaces. The rich models that one can produce from such a representation can capture complex interactions between many interrelated objects, and thus utilize many pieces of evidence that we may otherwise ignore; as we have seen, these pieces of evidence can provide substantial improvements in the quality of our predictions. 
> 本章提供了定义概率模型模板的通用框架
> 这些模板可以在单个模型中复用，也可以跨模型复用
> 基于模板的表示允许我们在任意大的概率空间编码无限的分布

We described several diferent representation languages: one specialized to temporal representations, and several that allow the specification of models over general object-relational domains. In the latter category, we first described two directed representations: plate models, and probabilistic relational models. The latter allow a considerably richer set of dependencies to be encoded, but at the cost of both conceptual and computational complexity. We also described an undirected representation, which, by avoiding the need to guarantee acyclicity and coherent local probability models, avoids some of the complexities of the directed models. As we discussed, the ﬂexibility of undirected models is particularly valuable when we want to encode a probability distribution over richer representations, such as the structure of the relational graph. 

There are, of course, other ways to produce these large, richly structured models. Most obviously, for any given application, we can define a procedural method that can take a skeleton, and produce a concrete model for that specific set of objects (and possibly relations). For example, we can easily build a program that takes a pedigree and produces a Bayesian network for genetic inheritance over that pedigree. The benefit of the template-based representations that we have described here is that they provide a uniform, modular, declarative language for models of this type. Unlike specialized representations, such a language allows the template-based model to be modified easily, whether by hand or as part of an automated learning algorithm. Indeed, learning is perhaps one of the key advantages of the template-based representations. In particular, as we will discuss, the model is learned at the template level, allowing a model to be learned from a domain with one set of objects, and applied seamlessly to a domain with a completely diferent set of objects (see section 17.5.1.2 and section 18.6.2). 

In addition, by making objects and relations first-class citizens in the model, we have laid a foundation for the option of allowing probability distributions over probability spaces that are significantly richer than simply properties of objects. For example, as we saw, we can consider modeling uncertainty about the network of interrelationships between objects, and even about the actual set of objects included in our domain. These extensions raise many important and difcult questions regarding the appropriate type of distribution that one should use for such richly structured probability spaces. These questions become even more complex as we introduce more of the expressive power of relational languages, such as function symbols, quantifiers, and more. These issues are an active area of research. 

These representations also raise important questions regarding inference. At first glance, the problem appears straightforward: The semantics for each of our representation languages depends on instantiating the template-based model to produce a specific ground network; clearly, we can simply run standard inference algorithms on the resulting network. This approach is has been called knowledge-based model construction , because a knowledge-base (or skeleton) is used to construct a model. However, this approach is problematic, because the models produced by this process can pose a significant challenge to inference algorithms. First, the network produced by this process is often quite large — much larger than models that one can reasonably construct by hand. Second, such models are often quite densely connected, due to the multiple interactions between variables. Finally, structural uncertainty, both about the relations and about the presence of objects, also makes for densely connected models. On the other side, such models often have unique characteristics, such as multiple similar fragments across the network, or large amounts of context-specific independence, which could, perhaps, be exploited by an appropriate choice of inference algorithm. Chapter 15 presents some techniques for addressing the inference problems in temporal models. The question of inference in the models defined by the object-relational frameworks — and specifically of inference algorithms that exploit their special structure — is very much a topic of current work. 
# 7 Gaussian Network Models 
Although much of our presentation focuses on discrete variables, we mentioned in chapter 5 that the Bayesian network framework, and the associated results relating independencies to factorization of the distribution, also apply to continuous variables. The same statement holds for Markov networks. However, whereas table CPDs provide a general-purpose mechanism for describing any discrete distribution (albeit potentially not very compactly), the space of possible parameterizations in the case of continuous variables is essentially unbounded. 
> 贝叶斯网络和 Markov 网络对于分布的分解定理对于连续变量也是成立的
> 连续变量的可能的参数化空间是无界的

In this chapter, we focus on a type of continuous distribution that is of particular interest: the class of multivariate Gaussian distributions. Gaussians are a particularly simple subclass of distributions that make very strong assumptions, such as the exponential decay of the distribution away from its mean, and the linearity of interactions between variables. While these assumptions are often invalid, Gaussians are nevertheless a surprisingly good approximation for many real-world distributions. Moreover, the Gaussian distribution has been generalized in many ways, to nonlinear interactions, or mixtures of Gaussians; many of the tools developed for Gaussians can be extended to that setting, so that the study of Gaussian provides a good foundation for dealing with a broad class of distributions. 
> 本章介绍多元高斯分布
> 高斯分布对分布的结构做出了很强的假设，例如从分布均值向其他方向的指数衰减、变量交互的线性性质等
> 这些假设在现实问题不一定成立，但高斯分布同样存在拓展，例如拓展到非线性交互、混合高斯等

In the remainder of this chapter, we ﬁrst review the class of multivariate Gaussian distributions and some of its properties. We then discuss how a multivariate Gaussian can be encoded using probabilistic graphical models, both directed and undirected. 

## 7.1 Multivariate Gaussians 
### 7.1.1 Basic Parameterization 
We have already described the univariate Gaussian distribution in chapter 2. We now describe its generalization to the multivariate case. As we discuss, there are two diferent parameterizations for a joint Gaussian density, with quite diferent properties. 
> 对于联合高斯密度有两种参数化，二者的性质不同

The univariate Gaussian is deﬁned in terms of two parameters: a mean and a variance. In its most common representation, a multivariate Gaussian distribution over $X_{1},\dots,X_{n}$ is characterized by an $n$ -dimensional mean vector $\mu$ , and a symmetric $n\times n$ covariance matrix $\Sigma$ ; the density function is most often deﬁned as: 

$$
p({\pmb x})=\frac{1}{(2\pi)^{n/2}|{\Sigma}|^{1/2}}\exp\left[-\frac{1}{2}({\pmb x}-{\pmb\mu})^{T}{\Sigma}^{-1}({\pmb x}-{\pmb\mu})\right]\tag{7.1}
$$

where $|\Sigma|$ is the determinant of $\Sigma$ . 
>  $X_, \dots, X_n$ 上的多元高斯分布相关的参数是 $n$ 维均值向量 $\pmb \mu$ 和对称的协方差矩阵 $\Sigma$ ($\Sigma_{ij} = \text {cov} (X_i, X_j)$)，密度函数定义如上
>  其中 $|\Sigma|$ 为 $\Sigma$ 的行列式

We extend the notion of a standard Gaussian to the multidimensional case, deﬁning it to be a Gaussian whose mean is the all-zero vector 0 and whose covariance matrix is the identity matrix $I$ , which has 1 ’s on the diagonal and zeros elsewhere. The multidimensional standard Gaussian is simply a product of independent standard Gaussians for each of the dimensions. 
> 标准的多元高斯分布其均值向量为全零向量，协方差矩阵为单位矩阵 $I$
> 标准的多元高斯分布本质是由每一个维度的相互独立的标准单元高斯分布相乘得到的

In order for this equation to induce a well-deﬁned density (that integrates to 1), the matrix $\Sigma$ must be positive deﬁnite : for any $\mathbf{\Psi}^{\mathcal{X}}\in I\!\!R^{n}$ such that $\pmb{x}\neq0$ , we have that ${\pmb x}^{T}\Sigma{\pmb x}>0$ . Positive deﬁnite matrices are guaranteed to be nonsingular, and hence have nonzero determinant, a necessary requirement for the coherence of this deﬁnition. A somewhat more complex deﬁnition can be used to generalize the multivariate Gaussian to the case of a positive semi-deﬁnite covariance matrix: for any $\pmb{x}\in\mathbb{R}^{n}$ , we have that ${\pmb x}^{T}\Sigma{\pmb x}\geq0$ . This extension is useful, since it allows for singular covariance matrices, which arise in several applications. For the remainder of our discussion, we focus our attention on Gaussians with positive deﬁnite covariance matrices. 
>为了让式 7.1 是良定义的(积分值为1)，矩阵 $\Sigma$ 必须是正定的：对于任何 $\pmb x\in \mathbb{R}^{n}, \pmb{x}\neq0$，我们有 ${\pmb x}^{T}\Sigma{\pmb x}>0$ 。
>正定矩阵保证是非奇异的，因此具有非零行列式 (正定矩阵所有特征值为正数)，这是这个定义一致性的必要条件 (正定矩阵保证线性变化不会降维) 
>一个更复杂的定义将多元高斯分布推广到协方差矩阵半正定的情况：对于任何 $\pmb{x}\in\mathbb{R}^{n}$，我们有 ${\pmb x}^{T}\Sigma{\pmb x}\geq0$。这种扩展是有用的，因为它允许奇异的协方差矩阵 (半正定矩阵允许零特征值)，在许多应用中都会出现这种情况
>在我们讨论的剩余部分，我们将重点关注协方差矩阵为正定的高斯分布

Because positive deﬁnite matrices are invertible, one can also utilize an alternative parameterization, where the Gaussian is deﬁned in terms of its inverse covariance matrix $J=\Sigma^{-1}$ , called information matrix (or precision matrix). This representation induces an alternative form for the Gaussian density. Consider the expression in the exponent of equation (7.1): 

$$
\begin{array}{r c l}{{-\displaystyle\frac{1}{2}({\boldsymbol x}-{\boldsymbol\mu})^{T}\Sigma^{-1}({\boldsymbol x}-{\boldsymbol\mu})}}&{{=}}&{{-\displaystyle\frac{1}{2}({\boldsymbol x}-{\boldsymbol\mu})^{T}J({\boldsymbol x}-{\boldsymbol\mu})}}\\ {{}}&{{=}}&{{-\displaystyle\frac{1}{2}\left[{\boldsymbol x}^{T}J{\boldsymbol x}-2{\boldsymbol x}^{T}J{\boldsymbol\mu}+{\boldsymbol\mu}^{T}J{\boldsymbol\mu}\right].}}\end{array}
$$ 
The last term is constant, so we obtain:

$$
p(\pmb{x})\propto\exp\left[-\frac12\pmb{x}^{T}J\pmb{x}+(J\pmb{\mu})^{T}\pmb{x}\right].\tag{7.2}
$$ 
This formulation of the Gaussian density is generally called the information form , and the vector $h=J\mu$ is called the potential vector . The information form deﬁnes a valid Gaussian density if and only if the information matrix is symmetric and positive deﬁnite, since $\Sigma$ is positive deﬁnite if and only if $\Sigma^{-1}$ is positive deﬁnite. The information form is useful in several settings, some of which are described here. 
> 另一种参数化利用了协方差矩阵的正定性质
> 因为正定矩阵一定有逆，我们定义信息/精度矩阵 $J = \Sigma^{-1}$ 为协方差矩阵的逆
> 我们将式 7.1 中的指数项中的 $\Sigma^{-1}$ 用 $J$ 替代，经过化简，就得到式 7.2
> 该形式的高斯密度称为信息形式，向量 $\pmb h = J\pmb \mu$ 称为势能向量
> 信息形式当且仅当信息矩阵 $J$ 是对称且正定时才定义有效的高斯分布 (正定的协方差矩阵就满足这一要求，$\Sigma$ 和 $\Sigma^{-1}$ 的正定性是一致的)

Intuitively, a multivariate Gaussian distribution speciﬁes a set of ellipsoidal contours around the mean vector $\mu$ . The contours are parallel, and each corresponds to some particular value of the density function. The shape of the ellipsoid, as well as the “steepness” of the contours, are determined by the covariance matrix $\Sigma$ . 
> 直观上，多元高斯分布指定了围绕均值向量 $\pmb \mu$ 的一组平行的椭圆等高线，每一条对应于密度函数的特定值
> 椭圆的形状和等高线的陡峭程度由协方差矩阵决定

Figure 7.1 shows two multivariate Gaussians, one where the covariances are zero, and one where they are positive. 
>图7.1展示了两个多元高斯分布，一个是协方差为零的情况，另一个是协方差为正的情况

As in the univariate case, the mean vector and covariance matrix correspond to the ﬁrst two moments of the normal distribution. In matrix notation, $\pmb{\mu}=\pmb{{\cal E}}[\pmb{X}]$ and $\Sigma=E[X X^{T}]-E[X]E[X]^{T}$ − . Breaking this expression down to the level of individual variables, we have that $\mu_{i}$ is the mean of $X_{i}$ , $\Sigma_{i,i}$ is the variance of $X_{i}$ , and $\Sigma_{i,j}\;=\;\Sigma_{j,i}$ (for $i\ne j)$ ) is the covariance between $X_{i}$ and $X_{j}$ : $\mathbf{C}o v[X_{i};X_{j}]=\mathbf{E}[X_{i}X_{j}]-\mathbf{E}[X_{i}]\mathbf{E}[X_{j}]$ . 
>正如单变量情况一样，均值向量和协方差矩阵对应于正态分布的前两阶矩，即 $\pmb{\mu}=\pmb{E}[\pmb{X}]$ 和 $\Sigma=\pmb E[\pmb X\pmb X^{T}]-\pmb E[\pmb X]\pmb E[\pmb X]^{T}$。将这个表达式分解到单个变量的层面，我们有：
>- $\mu_{i}$ 是 $X_{i}$ 的均值，
>- $\Sigma_{i,i}$ 是 $X_{i}$ 的方差，
>- $\Sigma_{i,j} = \Sigma_{j,i}$ （对于 $i \neq j$） 是 $X_{i}$ 和 $X_{j}$ 之间的协方差：$\mathbf{Cov}[X_{i};X_{j}]=\pmb{E}[X_{i}X_{j}]-\pmb{E}[X_{i}]\pmb{E}[X_{j}]$。
>
>简而言之，多元高斯分布的均值向量和协方差矩阵分别对应于各个变量的均值、方差以及不同变量之间的协方差

Example 7.1 
Consider a particular joint distribution p(X1; X2; X3) over three random variables. We can parameterize it via a mean vector $\mu$ and a covariance matrix $\Sigma$ : 

$$
\mu=\left(\begin{array}{r}{{1}}\\ {{-3}}\\ {{4}}\end{array}\right)\qquad\quad\Sigma=\left(\begin{array}{r r r}{{4}}&{{2}}&{{-2}}\\ {{2}}&{{5}}&{{-5}}\\ {{-2}}&{{-5}}&{{8}}\end{array}\right)
$$ 
As we can see, the covariances $\pmb{C}o\nu[X_{1};X_{3}]$ and $C o v[X_{2};X_{3}]$ are both negative. Thus, $X_{3}$ is negatively correlated with $X_{1}$ : when $X_{1}$ goes up, $X_{3}$ goes down (and similarly for $X_{3}$ and $X_{2}$ ). 
> $X_1, X_3$ 协方差为 0 -> $X_1, X_3$ 负相关 -> 总体趋势上，$X_1$ 上升，$X_3$ 下降，或者说二者远离各自均值的方向不同

> 协方差计算公式：

$$
\begin{align}
\text{cov}(X_1, X_2)&=E[(X_1 - E[X_1])(X_2 - E[X_2])]\\
&=E[X_1X_2 - X_1E[X_2] - X_2 E[X_1] + E[X_1]E[X_2]]\\
&=E[X_1X_2] - E[X_1]E[X_2]
\end{align}
$$

### 7.1.2 Operations on Gaussians 
There are two main operations that we wish to perform on a distribution: compute the marginal distribution over some subset of the variables $Y$ , and conditioning the distribution on some assignment of values $Z=z$ . It turns out that each of these operations is very easy to perform in one of the two ways of encoding a Gaussian, and not so easy in the other. 
> 对于分布我们常执行的两种计算：在某个变量子集 $\pmb Y$ 上计算边际分布、将分布条件于某个变量赋值 $\pmb Z = \mathbf z$ ，对于之前的两种参数化的高斯分布都容易完成

Marginalization is trivial to perform in the covariance form. Speciﬁcally, the marginal Gaussian distribution over any subset of the variables can simply be read from the mean and covariance matrix. For instance, in example 7.1, we can obtain the marginal Gaussian distribution over $X_{2}$ and $X_{3}$ by simply considering only the relevant entries in both the mean vector the covariance matrix. 
> 对于协方差的参数化形式，某个变量子集的边际分布可以直接从均值向量和协方差矩阵中读出来，只需要其中查看相关的 entries 即可

More generally, assume that we have a joint normal distribution over $\{X,Y\}$ where $X\,\in\,I\!\!R^{n}$ and $\pmb{Y}\,\in\,I\!\!R^{m}$ . Then we can decompose the mean and covariance of this joint distribution as follows: 

$$
p(\pmb X,\pmb Y)=\mathcal{N}\left({\left(\begin{array}{l}{\pmb \mu_{\pmb X}}\\ {\pmb \mu_{\pmb Y}}\end{array}\right)};\left[\begin{array}{l l}{\Sigma_{\pmb X \pmb X}}&{\Sigma_{\pmb X \pmb Y}}\\ {\Sigma_{\pmb Y \pmb X}}&{\Sigma_{\pmb Y \pmb Y}}\end{array}\right]\right)\tag{7.3}
$$ 
where  $\pmb \mu_{X}\in{I\!\!R}^{n},\,\pmb \mu_{Y}\in{I\!\!R}^{m},\,\Sigma_{X X}$ is a matrix of size $n\times n,\,\Sigma_{X Y}$ is a matrix of size $n\times m$ , $\Sigma_{Y X}=\Sigma_{X T}^{T}$ is a matrix of size $m\times n$ and $\Sigma_{Y Y}$ is a matrix of size $m\times m$   
> 一般地，如果我们有 $\pmb X \in \mathbb R^n, \pmb Y \in \mathbb R^m$ 上的联合高斯分布，我们可以按照式 7.3 分解该联合分布的均值和协方差

**Lemma 7.1** Let $\{X,Y\}$ ave a joint normal distribution deﬁned in equation (7.3). Then the marginal distri- bution over $Y$ is a normal distribution $\mathcal{N}\left(\mu_{Y};\Sigma_{Y Y}\right)$ . 
> 引理：
> $\pmb X, \pmb Y$ 服从式 7.3 定义的联合高斯分布，则 $\pmb Y$ 上的边际分布是高斯分布 $\mathcal N (\pmb \mu_{\pmb Y}, \Sigma_{\pmb Y\pmb Y}$)
> (高斯分布的一个基本性质就是高斯分布的边缘分布仍然是高斯分布)

The proof follows directly from the deﬁnitions (see exercise 7.1). 

On the other hand, conditioning a Gaussian on an observation $Z=z$ is very easy to perform in the information form. We simply assign the values $Z=z$ in equation (7.2). This process turns some of the quadratic terms into linear terms or even constant terms, and some of the linear terms into constant terms. The resulting expression, however, is still in the same form as in equation (7.2), albeit over a smaller subset of variables. 
> 将高斯分布条件于某个观测 $\pmb Z = \mathbf z$ 则在信息形式容易执行，我们在式 7.2 中直接赋值 $\pmb Z = \mathbf z$，这会将部二次项转化为线性项，将部分线性项转化为常数项
> 最后得到的表达式形式仍然是式 7.2，此时仅基于一个更小的变量子集

In summary, although the two representations both encode the same information, they have diferent computational properties. To marginalize a Gaussian over a subset of the variables, one essentially needs to compute their pairwise covariances, which is precisely generating the distribution in its covariance form. Similarly, to condition a Gaussian on an observation, one essentially needs to invert the covariance matrix to obtain the information form. For small matrices, inverting a matrix may be feasible, but in high-dimensional spaces, matrix inversion may be far too costly. 
> 小结：两种表示编码了相同信息，但计算性质不同
> 信息形式需要转置协方差矩阵，这在高维空间会过于昂贵

### 7.1.3 Independencies in Gaussians 
For multivariate Gaussians, independence is easy to determine directly from the parameters of the distribution. 
> 多元高斯分布可以直接从分布的参数中决定变量之间的独立性

**Theorem 7.1** 
Let $X=X_{1},...,X_{n}$ have a joint normal distribution $\mathcal{N}\left(\boldsymbol{\mu};\boldsymbol{\Sigma}\right)$ . Then $X_{i}$ and $X_{j}$ are independent if and only if $\Sigma_{i,j}=0$ . 
> 定理：
> $\pmb X  = X_1, \dots, X_2$ 有联合高斯分布 $\mathcal N (\pmb \mu; \Sigma)$，则当且仅当 $\Sigma_{ij} = 0$ ，$X_i, X_j$ 相互独立

The proof is left as an exercise (exercise 7.2). 

>证明：
>**必要性：** 如果 $X_i, X_j$ 相互独立，则 $\Sigma_{ij} = 0$
>假设 $X_i, X_j$ 相互独立。根据独立性的定义，随机变量 $X_i$ 和 $X_j$ 的联合概率密度函数等于它们各自边缘概率密度函数的乘积，即：

$$ p(X_i, X_j) = p(X_i) \cdot p(X_j) $$

>联合高斯分布中，协方差矩阵的非对角线元素表示了不同随机变量之间的线性相关性，如果两个随机变量相互独立，它们之间的线性相关性为零，故其协方差矩阵 $\Sigma$ 中对应于 $X_i$ 和 $X_j$ 之间的协方差项 $\Sigma_{ij}$ 必须为零。
>
>**充分性**：如果 $\Sigma_{ij} = 0$，则 $X_i, X_j$ 相互独立。
>假设 $\Sigma_{ij} = 0$。我们需要证明 $X_i$ 和 $X_j$ 相互独立。根据高斯分布的性质，如果 $\Sigma_{ij} = 0$，则 $X_i$ 和 $X_j$ 之间的线性相关性为零。
>在高斯分布的情况下，从两个随机变量之间没有线性关系是可以推出它们是统计上独立的。
>
>具体来说，对于联合高斯分布 $\mathcal{N}(\pmb \mu; \Sigma)$，如果协方差矩阵 $\Sigma$ 中的某个非对角线元素 $\Sigma_{ij} = 0$，那么 $X_i$ 和 $X_j$ 的联合分布可以分解为两个独立的边缘分布的乘积：

$$ p(X_i, X_j) = p(X_i) \cdot p(X_j) $$

>这是因为高斯分布的联合概率密度函数可以写成：

$$ p(\pmb X) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\pmb X - \pmb \mu)^T \Sigma^{-1} (\pmb X - \pmb \mu)\right) $$

>当 $\Sigma_{ij} = 0$ 时，协方差矩阵 $\Sigma$ 的逆矩阵 $\Sigma^{-1}$ 中对应的元素也为零，这使得联合概率密度函数可以分解为：

$$ p(X_i, X_j) = \frac{1}{(2\pi)^{1/2} \sqrt{\sigma_{ii}}} \exp\left(-\frac{1}{2} \frac{(X_i - \mu_i)^2}{\sigma_{ii}}\right) \cdot \frac{1}{(2\pi)^{1/2} \sqrt{\sigma_{jj}}} \exp\left(-\frac{1}{2} \frac{(X_j - \mu_j)^2}{\sigma_{jj}}\right) $$

>这表明 $X_i$ 和 $X_j$ 的联合分布确实可以分解为它们各自边缘分布的乘积，从而证明了它们的独立性。

Note that this property does not hold in general. In other words, if $p(X,Y)$ is not Gaussian, then it is possible that $C o v[X;Y]=0$ while $X$ and $Y$ are still dependent in $p$ . (See exercise 7.2.) 
> 在高斯分布中，线性不相关可以直接表示统计上的不相关，但在其他分布中不一定

At ﬁrst glance, it seems that conditional independencies are not quite as apparent as marginal independencies. However, it turns out that the independence structure in the distribution is apparent not in the covariance matrix, but in the information matrix. 
> 协方差矩阵容易看出边际独立性，信息矩阵容易看出条件独立性

**Theorem 7.2** 
Consider a Gaussian distribution $p(X_{1},.\,.\,.\,,X_{n})=\mathcal{N}\left(\pmb{\mu};\Sigma\right)$ , and let $J=\Sigma^{-1}$ be the information matrix. Then ${{J}_{i,j}}=0$ if and only if $\cdot p=(X_{i}\perp X_{j}\mid\mathcal{X}-\{X_{i},X_{j}\})$  . 
> 定理：
> 考虑高斯分布 $p (X_1, \dots, X_n) = \mathcal N (\pmb \mu; \Sigma)$，$J = \Sigma^{-1}$ 为信息矩阵，则 $J_{ij} = 0$ 当且仅当 $p \vDash (X_1 \perp X_j \mid \mathcal X - \{X_1, X_j\}$)
> ($J_{ij} \ne 0$ 意味着 $X_j, X_i$ 之间存在直接的相互作用，因此给定其他所有变量后，二者仍然相互依赖；$J_{ij} = 0$ 意味着 $X_j, X_i$ 之间没有直接的相互作用，二者的相关性是通过其他变量间接产生的，因此二者在给定其他变量时条件独立)

The proof is left as an exercise (exercise 7.3). 

Example 7.2
Consider the covariance matrix of example 7.1. Simple algebraic operations allow us to compute its inverse: 

$$
J=\left(\begin{array}{c c c}{{0.3125}}&{{-0.125}}&{{0}}\\ {{-0.125}}&{{0.5833}}&{{0.3333}}\\ {{0}}&{{0.3333}}&{{0.3333}}\end{array}\right)
$$ 
As we can see, the entry in the matrix corresponding to $X_{1},X_{3}$ is zero, reﬂecting the fact that they are conditionally independent given $X_{2}$ . 

Theorem 7.2 asserts the fact that the information matrix captures independencies between pairs of variables, conditioned on all of the remaining variables in the model. These are precisely the same independencies as the pairwise Markov independencies of deﬁnition 4.10. Thus, we can view the information matrix $J$ for a Gaussian density $p$ as precisely capturing the pairwise Markov independencies in a Markov network representing $p$ . Because a Gaussian density is a positive distribution, we can now use theorem 4.5 to construct a Markov network that is a unique minimal I-map for $p$ : As stated in this theorem, the construction simply introduces an edge between $X_{i}$ and $X_{j}$ whenever $\left(X_{i}\perp X_{j}\mid{\mathcal{X}}-\{X_{i},X_{j}\}\right)$ does not hold in $p$ . But this latter condition holds precisely when $J_{i,j}\,\ne\,0$ ̸ . **Thus, we can ew the information matrix as directly deﬁning a minimal I-map Markov network for p , whereby nonzero entries correspond to edges in the network.** 
> 定理 7.2 表明信息矩阵 $J$ 捕获了成对变量之间的给定其他所有变量时的条件独立性，这恰好也就是成对 Markov 独立性的定义
> 因此，我们可以将一个高斯密度 $p$ 的信息矩阵 $J$ 视作精确地捕获了表示 $p$ 的 Markov 网络中的成对 Markov 独立性
> 因为高斯密度是正分布，因此我们可以根据定理 4.5 构造是 $p$ 的唯一极小 I-map 的 Markov 网络，构造时，我们为 $(X_i \perp X_j \mid \mathcal X - \{X_i, X_j\}$ 不成立的 $X_i, X_j$ 之间引入一条边，也就是等价于 $J_{ij} \ne 0$ 时为 $X_i, X_j$ 引入一条边。因此 $J$ 也可以视为直接定义了 $p$ 的极小 I-mapMarkov 网络，$J$ 中的非零项就对应于网络中的一条边

## 7.2 Gaussian Bayesian Networks 
We now show how we can deﬁne a continuous joint distribution using a Bayesian network. This representation is based on the linear Gaussian model , which we deﬁned in deﬁnition 5.14. Although this model can be used as a CPD within any network, it turns out that continuous networks deﬁned solely in terms of linear Gaussian CPDs are of particular interest: 
> 本小节展示使用贝叶斯网络定义连续联合分布

**Deﬁnition 7.1**  Gaussian Bayesian network 
We define a Gaussian Bayesian network to be a Bayesian network all of whose variables are continuous, and where all of the CPDs are linear Gaussians.
> 定义：
> 对于一个贝叶斯网络，如果它的所有变量都是连续变量，并且所有的 CPD 都是线性高斯模型，则该网络就是高斯贝叶斯网络

An important and surprising result is that linear Gaussian Bayesian networks are an alternative representation for the class of multivariate Gaussian distributions. . 
> 线性高斯贝叶斯网络实际上是一类多元高斯分布的替代性表示

This result has two parts. The ﬁrst is that a linear Gaussian network always deﬁnes a joint multivariate Gaussian distribution
> 线性高斯网络总是定义一个联合多元高斯分布：

**Theorem 7.3** 
Let $Y$ be the linear Gaussian of its parents $X_1, \dots, X_k$

$$
p(Y\mid\mathbf{\boldsymbol{x}})=\mathcal{N}\left(\beta_{0}+\pmb \beta^{T}\mathbf{\boldsymbol{x}};\sigma^{2}\right).
$$ 
Assume that $X_{1},\ldots,X_{k}$ are jointly Gaussian with distribution $\mathcal{N}\left(\boldsymbol{\mu};\boldsymbol{\Sigma}\right)$ . Then: 

- The distribution of $Y$ is a normal distribution $p(Y)=\mathcal{N}\left(\mu_{Y};\sigma_{Y}^{2}\right)$ where: 

$$
\begin{array}{r c l}{{\mu_{Y}}}&{{=}}&{{\beta_{0}+\pmb \beta^{T}\pmb \mu}}\\ {{\sigma_{Y}^{2}}}&{{=}}&{{\sigma^{2}+\pmb \beta^{T}\Sigma\pmb \beta.}}\end{array}
$$ 
- The joint distribution over $\{X,Y\}$ is a normal distribution where: 

$$
\pmb{C}o v[X_{i};Y]=\sum_{j=1}^{k}\beta_{j}\Sigma_{i,j}.
$$ 
> 定理：
> $Y$ 是其父变量 $X_1,\dots, X_k$ 的线性高斯模型，假设 $X_1, \dots, X_k$ 服从联合高斯分布 $\mathcal N (\pmb \mu , \Sigma)$，则：
>  $Y$ 也服从高斯分布，其均值和方差分别和 $\pmb \mu, \Sigma$ 相关
>  $\pmb X, Y$ 上的联合分布也是高斯分布，其中 $X_i, Y$ 的协方差等于 $\beta_j \Sigma_{ij}$ 对所有 $j$ 求和 ($\Sigma_{ij}$ 表示 $X_i, X_j$ 的协方差，$\beta_j$ 是 $X_j$ 和 $Y$ 相关的系数)

From this theorem, it follows easily by induction that if $\mathcal{B}$ is a linear Gaussian Bayesian network, then it deﬁnes a joint distribution that is jointly Gaussian. 
> 线性高斯网络的所有 CPD 都是线性高斯模型，因此根据该定理，线性高斯网络 $\mathcal B$ 就定义了一个联合的多元高斯分布

Example 7.3 Consider the linear Gaussian network $X_{1}\rightarrow X_{2}\rightarrow X_{3}$ , where 

$$
\begin{array}{r c l}{p(X_{1})}&{=}&{\mathcal{N}\left(1;4\right)}\\ {p(X_{2}\mid X_{1})}&{=}&{\mathcal{N}\left(0.5X_{1}-3.5;4\right)}\\ {p(X_{3}\mid X_{2})}&{=}&{\mathcal{N}\left(-X_{2}+1;3\right).}\end{array}
$$ 
Using the equations in theorem 7.3, we can compute the joint Gaussian distribution $p(X_{1},X_{2},X_{3})$ . For the mean, we have that: 

$$
\begin{array}{l l l}{{\mu_{2}}}&{{=}}&{{0.5\mu_{1}-3.5=0.5\cdot1-3.5=-3}}\\ {{\mu_{3}}}&{{=}}&{{(-1)\mu_{2}+1=(-1)\cdot(-3)+1=4.}}\end{array}
$$ 
The variance of $X_{2}$ and $X_{3}$ can be computed as: 

$$
\begin{array}{r c l}{{\Sigma_{22}}}&{{=}}&{{4+(1/2)^{2}\cdot4=5}}\\ {{\Sigma_{33}}}&{{=}}&{{3+(-1)^{2}\cdot5=8.}}\end{array}
$$

We see that the variance of the variable is a sum of two terms: the variance arising from its own Gaussian noise parameter, and the variance of its parent variables weighted by the strength of the dependence. 
> 可以看到，本例中，变量的方差是两项的和：一项是自己的高斯噪声参数，一项是其父变量的方差乘上依赖性计算得到的权重

Finally, we can compute the covariances as follows: 

$$
\begin{array}{l c l}{{\Sigma_{12}}}&{{=}}&{{(1/2)\cdot4=2}}\\ {{\Sigma_{23}}}&{{=}}&{{(-1)\cdot\Sigma_{22}=-5}}\\ {{\Sigma_{13}}}&{{=}}&{{(-1)\cdot\Sigma_{12}=-2.}}\end{array}
$$ 
The third equation shows that, although $X_{3}$ does not depend directly on $X_{1}$ , they have a nonzero covariance. Intuitively, this is clear: $X_{3}$ depends on $X_{2}$ , which depends on $X_{1}$ ; hence, we expect $X_{1}$ and $X_{3}$ to be correlated, a fact that is reﬂected in their covariance. As we can see, the covariance between $X_{1}$ and $X_{3}$ is the covariance between $X_{1}$ and $X_{2}$ , weighted by the strength of the dependence of $X_{3}$ on $X_{2}$ . 
> 本例中，可以看到 $X_1, X_3$ 之间的协方差等于 $X_1, X_2$ 之间的协方差乘上 $X_3, X_2$ 之间的依赖性作为权重

In general, putting these results together, we can see that the mean and covariance matrix for $p(X_{1},X_{2},X_{3})$ is precisely our covariance matrix of example 7.1. 

The converse to this theorem also holds: the result of conditioning is a normal distribution where there is a linear dependency on the conditioning variables. The expressions for converting a multivariate Gaussian to a linear Gaussian network appear complex, but they are based on simple algebra. They can be derived by taking the linear equations speciﬁed in theorem 7.3, and reformulating them as deﬁning the parameters $\beta_{i}$ in terms of the means and covariance matrix entries. 
> 该定理的逆命题同样成立：联合高斯分布下的条件分布也是高斯分布，并且是线性高斯模型
> 将多元高斯分布转换为线性高斯网络的公式看起来很复杂，但实际上它们基于简单的代数运算。这些公式可以通过定理7.3中指定的线性方程推导出来，并重新表述为通过均值和协方差矩阵元素来定义参数$\beta_{i}$。 

**Theorem 7.4** 
Let $\{X,Y\}$ have a joint normal distribution deﬁned in equation (7.3). Then the conditional density 

$$
p({Y}\mid\boldsymbol{X})=\mathcal{N}\left(\beta_{0}+\pmb \beta^{T}\boldsymbol{X};\sigma^{2}\right),
$$

is such that: 

$$
\begin{array}{r c l}{{\beta_{0}}}&{{=}}&{{\mu_{Y}-\Sigma_{Y \pmb X}\Sigma_{\pmb X \pmb X}^{-1}\mu_{\pmb X}}}\\ {{\pmb \beta}}&{{=}}&{{\Sigma_{\pmb X \pmb X}^{-1}\Sigma_{Y \pmb X}}}\\ {{\sigma^{2}}}&{{=}}&{{\Sigma_{Y Y}-\Sigma_{Y \pmb X}\Sigma_{\pmb X \pmb X}^{-1}\Sigma_{\pmb X Y}.}}\end{array}
$$ 
> 定理：
> $\{\pmb X, Y\}$ 服从式 7.3 定义的联合正态分布，则 $Y$ 条件于 $\pmb X$ 的条件分布也是正态分布，并且是线性高斯模型

This result allows us to take a joint Gaussian distribution and produce a Bayesian network, using an identical process to our construction of a minimal I-map in section 3.4.1. 
> 根据该定理，我们可以根据联合高斯分布写出分布中的各个条件概率分布，因此可以根据给定的独立性构建分布的极小 I-map

**Theorem 7.5** 
Let $\mathcal{X}=\{X_{1},\ldots,X_{n}\}$ , and let $p$ be a joint Gaussian distributio over $\mathcal{X}$ . Given any orderi $X_{1},\dots,X_{n}$ over X , we can construct a Bayesian network graph G and a Bayesian network B over G such that: 

1. $\mathrm{Pa}_{X_{i}}^{\mathcal{G}}\subseteq\{X_{1},\ldots,X_{i-1}\};$ ;
2. the CPD of $X_{i}$ in $\mathcal{B}$ is a linear Gaussian of its parents;
3. $\mathcal{G}$ is a minimal $I^{,}$ -map for $p$ . 

> 定理：
> $p$ 为 $\mathcal X = \{X_1, \dots, X_n\}$ 上的联合高斯分布，给定 $\mathcal X$ 上的任意顺序 $X_1, \dots, X_n$，我们可以构建贝叶斯网络图 $\mathcal G$ 和 $\mathcal G$ 上的贝叶斯网络 $\mathcal B$，满足：图中任意变量的父变量都在 $\mathcal X$ 中，$X_i$ 在 $\mathcal B$ 中的 CPD 是关于它的父变量的线性高斯模型，$\mathcal G$ 是 $p$ 的 minimal I-map

The proof is left as an exercise (exercise 7.4). 

As for the case of discrete networks, the minimal I-map is not unique: diferent choices of orderings over the variables will lead to diferent network structures. For example, the distribution in ﬁgure 7.1b can be represented either as the network where $X\rightarrow Y$ or as the network where $Y\rightarrow X$ . 
>至于离散网络的情况，最小的 I-map 不是唯一的：变量的不同排序将导致不同的网络结构。例如，图7.1b 中的分布可以用 $X\rightarrow Y$ 的网络或 $Y\rightarrow X$ 的网络来表示。

This equivalence between Gaussian distributions and linear Gaussian networks has important practical ramiﬁcations. On one hand, we can conclude that, for linear Gaussian networks, the joint distribution has a compact representation (one that is quadratic in the number of variables). Furthermore, the transformations from the network to the joint and back have a fairly simple and efciently computable closed form. Thus, we can easily convert one representation to another, using whichever is more convenient for the current task. Conversely, while the two representations are equivalent in their expressive power, there is not a one-to-one correspondence between their parameter iz at ions. In particular, although in the worst case, the linear Gaussian representation and the Gaussian representation have the same number of parameters (exercise 7.6), there are cases where one representation can be signiﬁcantly more compact than the other. 
>高斯分布与线性高斯网络之间的这种等价性具有重要的实际意义。一方面，我们可以得出结论，对于线性高斯网络，联合分布有一个紧凑的表示形式（即在变量数量上是二次的）
>此外，从网络到联合分布及其逆变换都有相对简单且可有效计算的闭式形式。因此，我们可以轻松地在这两种表示形式之间进行转换，使用对当前任务更方便的一种。另一方面，尽管这两种表示在表达能力上是等价的，但它们的参数化之间并没有一对一的对应关系
>特别是，虽然最坏情况下，线性高斯表示和高斯表示具有相同数量的参数（习题7.6），但在某些情况下，一种表示形式可以比另一种显著更紧凑。

Example 7.4 
Consider a linear Gaussian network structured as a chain: 

$$
X_{1}\rightarrow X_{2}\rightarrow\cdot\cdot\cdot\rightarrow X_{n}.
$$

Assuming the network parameterization is not degenerate (that is, the network is a minimal I-map of its distribution), we have that each pair of variables $X_{i},X_{j}$ are correlated. In this case, as shown in theorem 7.1, the covariance matrix would be dense — none of the entries would be zero. Thus, the representation of the covariance matrix would require a quadratic number of parameters. In the information matrix, however, for all $X_{i},X_{j}$ that are not neighbors in the chain, we have that $X_{i}$ and $X_{j}$ are conditionally independent given the rest of the variables in the network; hence, by theorem 7.2, ${{J}_{i,j}}\mathrm{~=~}0$ . Thus, the information matrix has most of the entries being zero; the only nonzero entries are on the tridiagonal (the entries $i,j$ for $j=i-1,i,i+1)$ . 

However, not all structure in a linear Gaussian network is represented in the information matrix. 

Example 7.5 In a v-structure $X\rightarrow Z\leftarrow Y$ , we have that $X$ and $Y$ are marginally independent, but not conditionally independent given Z . Thus, according to theorem 7.2, the $X,Y$ entry in the information matrix would not be 0 . Conversely, because the variables are marginally independent, the $X,Y$ entry in the covariance entry would be zero. 

Complicating the example somewhat, assume that $X$ and $Y$ also have a joint parent $W$ ; that is, the network is structured as a diamond. In this case, $X$ and $Y$ are still not independent given the remaining network variables $Z,W$ , and hence the $X,Y$ entry in the information matrix is nonzero. Conversely, they are also not marginally independent, and thus the $X,Y$ entry in the covariance matrix is also nonzero. 

These examples simply recapitulate, in the context of Gaussian networks, the fundamental diference in expressive power between Bayesian networks and Markov networks. 

## 7.3 Gaussian Markov Random Fields 
We now turn to the representation of multivariate Gaussian distributions via an undirected graphical model.  
> 本节讨论使用无向图模型表示多元高斯分布

We ﬁrst show how a Gaussian distribution can be viewed as an MRF. This formulation is derived almost immediately from the information form of the Gaussian. Consider again equation (7.2). We can break up the expression in the exponent into two types of terms: those that involve single variables $X_{i}$ and those that involve pairs of variables $X_{i},X_{j}$ . 
> 高斯分布可以被视作一个 MRF/CRF
> 考虑 eq7.2 ，我们指数中的表达式分为两项，一项仅和单个变量 $X_i$ 相关，另一项和一对变量 $X_i, X_j$ 相关

The terms that involve only the variable $X_{i}$ are:

$$
-\frac{1}{2}J_{i,i}x_{i}^{2}+h_{i}x_{i},
$$ 
where we recall that the potential vector $\pmb h=J\pmb \mu$ . 
> 仅和 $X_i$ 相关的项如上，其中 $\pmb h = J \pmb \mu$ 称为势能向量

The terms that involve the pair $X_{i},X_{j}$ are: 

$$
-\frac{1}{2}[J_{i,j}x_{i}x_{j}+J_{j,i}x_{j}x_{i}]=-J_{i,j}x_{i}x_{j},
$$ 
due to the symmetry of the information matrix. 
> 和 $X_i, X_j$ 相关的项如上

Thus, the information form immediately induces a pairwise Markov network, whose node potentials are derived from the potential vector and the diagonal elements of the information matrix, and whose edge potentials are derived from the of-diagonal entries of the information matrix. We also note that, when ${{J}_{i,j}}\mathrm{~=~}0$ , there is no edge between $X_{i}$ and $X_{j}$ in the model, corresponding directly to the independence assumption of the Markov network. 
> 因此，Gaussian 分布的信息形式直接导出了一个成对 Markov 网络，其节点势能来自于势能向量和信息矩阵的对角线元素，其边势能来自于信息矩阵的非对角线元素
> 当 $J_{i, j} = 0$，模型中 $X_i, X_j$ 之间没有边，直接对应于 Markov 网络的独立性假设

Thus, any Gaussian distribution can be represented as a pairwise Markov network with quadratic node and edge potentials. This Markov network is generally called a Gaussian Markov random ﬁeld (GMRF) .
> 因此，任意高斯分布可以被表示为带有二次节点和边势能的成对 Markov 网络，该 Markov 网络一般称为高斯 Markov 随机场

Conversely, consider any pairwise Markov network with quadratic node and edge potentials. Ignoring constant factors, which can be assimilated into the partition function, we can write the node and edge energy functions (log-potentials) as: 

$$
\begin{array}{r l}&{\ \ \ \epsilon_{i}(x_{i})=d_{0}^{i}+d_{1}^{i}x_{i}+d_{2}^{i}x_{i}^{2}}\\ &{\epsilon_{i,j}(x_{i},x_{j})=a_{00}^{i,j}+a_{01}^{i,j}x_{i}+a_{10}^{i,j}x_{j}+a_{11}^{i,j}x_{i}x_{j}+a_{02}^{i,j}x_{i}^{2}+a_{20}^{i,j}x_{j}^{2},}\end{array}\tag{7.6}
$$ 
where we used the log-linear notation of section 4.4.1.2. 
> 反过来说，考虑任意带有二次节点和边势能的成对 Markov 网络，忽略常数因子，我们可以将节点和边的能量函数 (势能函数的对数形式) 写为式 7.6 的形式

By aggregating like terms, we can reformulate any such set of potentials in the log-quadratic form: 

$$
p^{\prime}(\pmb{x})=\exp(-\frac{1}{2}\pmb{x}^{T}J\pmb{x}+\pmb h^{T}\pmb{x}),\tag{7.7}
$$

where we can assume without loss of generality that $J$ is symmetric. 
> 将能量函数中的常数消去 (这些常数本身也会被划分函数吸收)，再将系数统合，我们可以将该成对 Markov 网络的定义的联合分布写为式 7.7 的形式，其中 $J$ 是一个对称矩阵

This Markov network deﬁnes a valid Gaussian density if and only if $J$ is a positive deﬁnite matrix. If so, then $J$ is a legal information matrix, and we can take $\pmb h$ to be a potential vector, resulting in a distribution in the form of equation (7.2). 
> 因此，当且仅当 $J$ 是正定矩阵时，该 Markov 网络定义了一个有效的高斯分布，此时 $J$ 就是一个合法的信息矩阵，$\pmb h$ 就作为势能向量

However, unlike the case of Gaussian Bayesian networks, it is not the case that every set of quadratic node and edge potentials induces a legal Gaussian distribution. Indeed, the decom- position of equation (7.4) and equation (7.5) can be performed for any quadratic form, including one not corresponding to a positive deﬁnite matrix. For such matrices, the resulting function $\exp({\pmb x}^{T}A{\pmb x}+\bar{\pmb b}^{T}{\pmb x})$ will have an inﬁnite integral, and cannot be normalized to produce a valid density. 
> 虽然任意带有二次节点和边势能的 Markov 网络都可以写为式 7.7 的形式，但当 $J$ 不是正定矩阵时，$\exp (\pmb x^T J \pmb x + \pmb h^T \pmb x)$ 的积分将趋于无穷，因此不能被规范化以产生有效的概率分布

Unfortunately, **other than generating the entire information matrix and testing whether it is positive deﬁnite, there is no simple way to check whether the MRF is valid. In particular, there is no local test that can be applied to the network parameters that precisely characterizes valid Gaussian densities.** However, there are simple tests that are sufficient to induce a valid density. While these conditions are not necessary, they appear to cover many of the cases that occur in practice. 
>除了生成整个信息矩阵并测试其是否为正定矩阵外，没有简单的方法可以检查MRF是否有效，特别地，没有局部测试可以应用于网络参数以精确表征这些参数是否定义有效的高斯密度
>然而，存在一些简单的测试条件，这些条件对于引出一个有效的密度是充分的，虽然这些条件不是必要的，但它们涵盖了实际中出现的许多情况

We ﬁrst provide one very simple test that can be veriﬁed by direct examination of the information matrix. 

**Deﬁnition 7.2** diagonally dominant 
A quadratic MRF parameterized by J is said to be diagonally dominant if, for all i
> 定义：
> 一个由 $J$ 参数化的 MRF 如果满足对于所有 $i$ 都有 $\sum_{j\ne i} |J_{i, j}| < J_{i, i}$，则称 $J$ 是对角主导的

$$
\sum_{j\neq i}|J_{i,j}|<J_{i,i}.
$$ 
For example, the information matrix in example 7.2 is diagonally dominant; for instance, for $i=2$ we have: 

$$
|-0.125|+0.3333<0.5833.
$$ 
One can now show the following result: 

**Proposition 7.1** 
Let $\begin{array}{r}{p^{\prime}(x)=\exp(-\frac{1}{2}x^{T}J x+\overline{{h}}^{T}x)}\end{array}$ be a quadratic pairwise MRF. If $J$ is diagonally dominant, then $p^{\prime}$ deﬁnes a valid Gaussian MRF. 
> 命题：
> $p' (x) = \exp (-\frac 1 2 \pmb x^T J \pmb x + \pmb h^T \pmb x)$ 为二次成对 Markov 随机场，如果 $J$ 是对角主导的，则 $p'$ 定义了一个有效的高斯 Markov 随机场

The proof is straightforward algebra and is left as an exercise (exercise 7.8). 

The following condition is less easily veriﬁed, since it cannot be tested by simple examination of the information matrix. Rather, it checks whether the distribution can be written as a quadratic pairwise MRF whose node and edge potentials satisfy certain conditions. Speciﬁcally, recall that a Gaussian MRF consists of a set of node potentials, which are log-quadratic forms in $x_{i}$ , and a set of edge potentials, which are log-quadratic forms in $x_{i},x_{j}$ . We can state a condition in terms of the coeffcients for the nonlinear components of this parameterization: 

**Deﬁnition 7.3** pairwise normalizable 
A quadratic MRF parameterized as in equation (7.6) is said to be pairwise normalizable if: 

- for all i , $d_{2}^{i}>0$ ; 
- for all $i,j$ , the $2\times2$ matrix $\left(\begin{array}{c c}{{a_{02}^{i,j}}}&{{a_{11}^{i,j}/2}}\\ {{a_{11}^{i,j}/2}}&{{a_{20}^{i,j}}}\end{array}\right)$ is positive semideﬁnite. 

> 定义：
> 按照 eq 7.6 参数化的二次 Markov 随机场如果满足
> - $d_{2}^{i}>0$ 对于所有的 $i$ 成立，也就是节点势能中二次项的系数是正数
>- 对于所有的 $i, j$，矩阵 $\left(\begin{array}{c c}{{a_{02}^{i,j}}}&{{a_{11}^{i,j}/2}}\\ {{a_{11}^{i,j}/2}}&{{a_{20}^{i,j}}}\end{array}\right)$ 是半正定的
> 则称该 MRF 是成对可规范化的

Intuitively, this deﬁnition states that each edge potential, considered in isolation, is normalizable (hence the name “pairwise-normalizable”). 
> 该定义即声明了每个边势能在单独考虑时都是可规范化的

We can show the following result: 

**Proposition 7.2** 
Let $p^{\prime}(x)$ be a quadratic pairwise MRF, parameterized as in equation (7.6). If $p^{\prime}$ is pairwise normalizable, then it deﬁnes a valid Gaussian distribution. 
> 命题：
> $p' (x)$ 为二次成对 MRF，按照 eq 7.6 参数化，如果 $p'$ 是成对可规范化的，则它定义了有效的高斯分布

Once again, the proof follows from standard algebraic manipulations, and is left as an exercise (exercise 7.9). 

We note that, like the preceding conditions, this condition is sufficient but not necessary: 
> 上述两个条件都是充分条件而不是必要条件

Example 7.6 Consider the following information matrix: 

$$
\left(\begin{array}{l l l}{1}&{0.6}&{0.6}\\ {0.6}&{1}&{0.6}\\ {0.6}&{0.6}&{1}\end{array}\right)
$$ 
It is not difcult to show that this information matrix is positive deﬁnite, and hence deﬁnes a legal Gaussian distribution. However, it turns out that it is not possible to decompose this matrix into a set of three edge potentials, each of which is positive deﬁnite. 

Unfortunately, evaluating whether pairwise normalizability holds for a given MRF is not always trivial, since it can be the case that one parameterization is not pairwise normalizable, yet a diferent parameterization that induces precisely the same density function is pairwise normalizable. 

Example 7.7
Consider the information matrix of example 7.2, with a mean vector 0 . We can deﬁne this distribution using an MRF by simply choosing the node potential for $X_{i}$ to be $J_{i,i}x_{i}^{2}$ and the edge potential for $X_{i},X_{j}$ to be $2J_{i,j}x_{i}x_{j}$ . Clearly, the $X_{1},X_{2}$ edge does not deﬁne a nor- malizable density over $X_{1},X_{2}$ , and hence this MRF is not pairwise normalizable. However, as we discussed in the context of discrete MRFs, the MRF parameter iz ation is nonunique, and the same density can be induced using a continuum of diferent parameter iz at ions. In this case, one alternative parameter iz ation of the same density is to deﬁne all node potentials as $\epsilon_{i}(x_{i})=0.05x_{i}^{2}$ , and the edge potentials to be $\epsilon_{1,2}(x_{1},x_{2})=0.2625x_{1}^{2}+0.0033x_{2}^{2}-0.25x_{1}x_{2}.$ − , and $\epsilon_{2,3}(x_{2},x_{3})=0.53x_{2}^{2}+0.2833x_{3}^{2}+0.6666x_{2}x_{3}$ . Straightforward arithmetic shows that this set of potentials induces the information matrix of example 7.2. Moreover, we can show that this formulation is pairwise normalizable: The three node potentials are all positive, and the two edge potentials are both positive deﬁnite. (This latter fact can be shown either directly or as a conse- quence of the fact that each of the edge potentials is diagonally dominant, and hence also positive deﬁnite.) 

This example illustrates that the pairwise normalizability condition is easily checked for a speciﬁc MRF parameter iz ation. However, if our aim is to encode a particular Gaussian density as an MRF, we may have to actively search for a decomposition that satisﬁes the relevant constraints. If the information matrix is small enough to manipulate directly, this process is not difcult, but if the information matrix is large, ﬁnding an appropriate parameter ization may incur a nontrivial computational cost. 
>这个例子说明，对于特定的MRF参数化，成对归一化条件很容易进行检查。然而，如果我们希望将特定的高斯密度编码为MRF，则可能需要积极寻找满足相关约束的分解。如果信息矩阵足够小，可以直接操作，这个过程并不困难，但如果信息矩阵很大，找到适当的参数化可能会带来非 trivial 的计算成本。

## 7.4 Summary 
This chapter focused on the representation and independence properties of Gaussian networks. 
> 本章聚焦于高斯网络的表示和独立性质

We showed an equivalence of expressive power between three representational classes: multivariate Gaussians, linear Gaussian Bayesian networks, and Gaussian MRFs. In particular, any distribution that can be represented in one of those forms can also be represented in another. We provided closed-form formulas that allow us convert between the multivariate Gaussian representation and the linear Gaussian Bayesian network. The conversion for Markov networks is simpler in some sense, inasmuch as there is a direct mapping between the entries in the infor- mation (inverse covariance) matrix of the Gaussian and the quadratic forms that parameterize the edge potentials in the Markov network. 
> 本章介绍了三种表示类型的等价性：多元高斯分布、线性高斯贝叶斯网络、高斯 Markov 随机场，任意可以以其中一种形式表示的分布都可以用另一种形式表示
> 我们提供了在多元高斯表示和线性高斯贝叶斯网络之间转化的闭式公式，高斯分布的信息矩阵中的 entries 和 Markov 网络中参数化边势能的二次形式之间则存在直接映射

However, unlike the case of Bayesian networks, here we must take care, since not every quadratic parameterization of a pairwise Markov network induces a legal Gaussian distribution: The quadratic form that arises when we combine all the pairwise potentials may not have a ﬁnite integral, and therefore may not be normalizable. In general, there is no local way of determining whether a pairwise MRF with quadratic potentials is normalizable; however, we provided some easily checkable sufcient conditions that are often sufcient in practice. 
>然而，与贝叶斯网络不同的是，我们必须小心并不是每个二元马尔可夫随机场的二次参数化都能诱导出合法的高斯分布：我们结合所有成对势能时得到的二次形式可能没有有限的积分，因此可能无法归一化
>一般来说，没有局部方法可以确定一个具有二次势能的二元马尔可夫随机场是否可归一化；然而，我们提供了一些易于检查的充分条件，这些条件在实践中通常是足够的

The equivalence between the diferent representations is analogous to the equivalence of Bayesian networks, Markov networks, and discrete distributions: any discrete distribution can be encoded both as a Bayesian network and as a Markov network, and vice versa. However, as in the discrete case, this equivalence does not imply equivalence of expressive power with respect to independence assumptions. **In particular, the expressive power of the directed and undirected representations in terms of independence assumptions is exactly the same as in the discrete case: Directed models can encode the independencies associated with immoralities, whereas undirected models cannot; conversely, undirected models can encode a symmetric diamond, whereas directed models cannot.**
> 高斯分布的不同表示之间的等价性类似于贝叶斯网络、Markov 网络、离散分布之间的等价性：任意离散分布都可以被编码为贝叶斯网络和 Markov 网络，反之也成立
> 无论是离散情况还是连续情况，有向图和无向图之间关于独立性假设的表示能力都存在差异，有向模型可以编码和 immorality 相关的独立性，而无向模型不行，无向模型可以编码和对称菱形相关的独立性，而有向模型不行

 As we saw, the undirected models have a particularly elegant connection to the natural representation of the Gaussian distribution in terms of the information matrix; in particular, zeros in the information matrix for $p$ correspond precisely to missing edges in the minimal I-map Markov network for $p$ . 
> 无向模型和高斯分布的信息矩阵表示之间存在直接的联系：$p$ 的信息矩阵中为零的项直接对应于 $p$ 的极小 I-map 的 Markov 网络中缺少边

Finally, we note that the class of Gaussian distributions is highly restrictive, making strong assumptions that often do not hold in practice. Nevertheless, it is a very useful class, due to its compact representation and computational tractability (see section 14.2). Thus, in many cases, we may be willing to make the assumption that a distribution is Gaussian even when that is only a rough approximation. This approximation may happen a priori, in encoding a distribution as a Gaussian even when it is not. Or, in many cases, we perform the approximation as part of our inference process, representing intermediate results as a Gaussian, in order to keep the computation tractable. Indeed, as we will see, the Gaussian representation is ubiquitous in methods that perform inference in a broad range of continuous models. 
>最后，我们注意到高斯分布类是非常严格的，它常常做出在实际中不成立的强假设。尽管如此，由于其紧凑的表示和计算上的可处理性（见第14.2节），它是一个非常有用的类。因此，在许多情况下，即使这种假设只是一个粗略的近似，我们也可能愿意假设一个分布是高斯的。
>这种近似可以在先验知识中发生，即使在编码分布时它实际上并不是高斯分布 （也就是先验服从高斯，但实际分布不服从高斯）。或者，在许多情况下，我们在推理过程中进行近似，将中间结果表示为高斯分布，以便保持计算的可处理性。
>事实上，正如我们将看到的，高斯表示在广泛连续模型的推理方法中无处不在。

# Part 2 Inference
# 9 Exact Inference: Variable Elimination 
In this chapter, we discuss the problem of performing inference in graphical models. We show that the structure of the network, both the conditional independence assertions it makes and the associated factorization of the joint distribution, is critical to our ability to perform inference effectively, allowing tractable inference even in complex networks. 
> 本章讨论图模型中的推理

Our focus in this chapter is on the most common query type: the conditional probability query , $P(Y\mid E=e)$ (see section 2.1.5). We have already seen several examples of conditional probability queries in chapter 3 and chapter 4; as we saw, such queries allow for many useful reasoning patterns, including explanation, prediction, intercausal reasoning, and many more. 
> 本章聚焦于条件概率查询 $P (\pmb Y \mid \pmb E = \pmb e)$

By the deﬁnition of conditional probability, we know that 

$$
P(\pmb Y\mid \pmb E=\pmb e)=\frac{P(\pmb Y,\pmb e)}{P(\pmb e)}.\tag{9.1}
$$ 
Each of the instantiations of the numerator is a probability expression $P(\pmb{y},\pmb e)$ , which can be computed by summing out all entries in the joint that correspond to assignments consistent with $\pmb y, \pmb e$ . More precisely, let $\pmb W=\mathcal{X}-\pmb Y-\pmb E$ be the random variables that are neither query nor evidence. Then

$$
P(\pmb{y},\pmb e)=\sum_{\pmb{w}}P(\pmb{y},\pmb e,\pmb{w}).\tag{9.2}
$$ 
Because $\pmb Y,\pmb E,\pmb W$ are all of the network variables, each term $P(\pmb{y},\pmb{e},\pmb{w})$ in the summation is simply an entry in the joint distribution. 

The probability $P(\pmb e)$ can also be computed directly by summing out the joint. However, it can also be computed as 

$$
P(\pmb e)=\sum_{\pmb y}P(\pmb y,\pmb e),\tag{9.3}
$$ 
which allows us to reuse our computation for equation (9.2). If we compute both equation (9.2) and equation (9.3), we can then divide each $P(\pmb{y},\pmb e)$ by $P(\pmb e)$ , to get the desired conditional probability $P(\pmb{y}\mid\pmb{e})$ . Note that this process corresponds to taking the vector of marginal probabilities $P(\pmb{y}^{1},\pmb{e}),\dots,P(\pmb{y}^{k},\pmb{e})$ (where $k\,=\,|\mathit{V a l}(\pmb Y)|)$ and renormalizing the entries to sum to 1 . 

## 9.1 Analysis of Complexity 
In principle, a graphical model can be used to answer all of the query types described earlier. We simply generate the joint distribution and exhaustively sum out the joint (in the case of a conditional probability query), search for the most likely entry (in the case of a MAP query), or both (in the case of a marginal MAP query). However, this approach to the inference problem is not very satisfactory, since it returns us to the exponential blowup of the joint distribution that the graphical model representation was precisely designed to avoid. 

Unfortunately, we now show that exponential blowup of the inference task is (almost certainly) unavoidable in the worst case: The problem of inference in graphical models is $\mathcal{N P}$ -hard, and therefore ly requires exponential time in the worst ca except in the unlikely event that P $\mathcal{P}=\mathcal{N P}$ NP ). Even worse, approximate inference is also NP -hard. Importantly, however, the story does not end with this negative result. In general, we care not about the worst case, but about the cases that we encounter in practice. As we show in the remainder of this part of the book, many real-world applications can be tackled very efectively using exact or approximate inference algorithms for graphical models. 

In our theoretical analysis, we focus our discussion on Bayesian networks. Because any Bayesian network can be encoded as a Markov network with no increase in its representation size, a hardness proof for inference in Bayesian networks immediately implies hardness of inference in Markov networks. 

### 9.1.1 Analysis of Exact Inference 
To address the question of the complexity of BN inference, we need to address the question of how we encode a Bayesian network. Without going into too much detail, we can assume that the encoding speciﬁes the DAG structure and the CPDs. For the following results, we assume the worst-case representation of a CPD as a full table of size $|V a l(\{X_{i}\}\cup\mathrm{Pa}_{X_{i}})|$ . 

As we discuss in appendix A.3.4, most analyses of complexity are stated in terms of decision problems. We therefore begin with a formulation of the inference problem as a decision prob- lem, and then discuss the numerical version. One natural decision version of the conditional probability task is the problem $B N–P r–D P,$ , deﬁned as follows: 

Given a $\mathcal{B}$ over $\mathcal{X}$ , a variable $X\in{\mathcal{X}}$ , and a value $x\in V a l(X)$ , decide whether $P_{\mathcal{B}}(X=x)>0$ . 

Theorem 9.1 
Proof It is straightforward to prove that $B N–P r–D P$ is in $\mathcal{N P}$ : In the guessing phase, we full assignment $\xi$ to the network variables. In the veriﬁcation phase, we check whether X $X=x$ in $\xi;$ , and whether $P(\xi)\,>\,0$ . One of these guesses succeeds if and only if $P(X\,=\,x)\,>\,0$ . Computing $P(\xi)$ for a full assignment of the network variables requires only that we multiply the relevant entries in the factors, as per the chain rule for Bayesian networks, and hence can be done in linear time. 

To prove $\mathcal{N P}$ -hardness, we need to show that, if we can answer instances in BN-Pr-DP , we can use that as a subroutine to answer questions in a class of problems that is known to be $\mathcal{N P}$ -hard. We will use a reduction from the 3-SAT problem deﬁned in deﬁnition A.8. 

 
Figure 9.1 An outline of the network structure used in the reduction of 3-SAT to Bayesian network inference. 

To show the reduction, we show the following: Given any 3-SAT formula $\phi$ , we can create a Bayesian network $\mathcal{B}_{\phi}$ with some distinguished variable $X$ , such that $\phi$ is satisﬁable if and only if $P_{\mathcal{B}_{\phi}}(X=x^{1})>0$ . Thus, if we can solve the Bayesian network inference problem in polynomial time, we can also solve the 3-SAT problem in polynomial time. To enable this conclusion, our BN $\mathcal{B}_{\phi}$ has to be constructible in time that is polynomial in the length of the formula $\phi$ . 

Consider a 3-SAT instance $\phi$ over the propositional variables $q_{1},\ldots,q_{n}$ . Figure 9.1 illustrates the structure of the network constr ted in this reduction. Our Bayesian network $\mathcal{B}_{\phi}$ has a node $Q_{k}$ for each propositional variable $q_{k}$ ; these variables are roots, with $P(q_{k}^{1})=0.5$ . It also has a no $C_{i}$ for each cl e $C_{i}$ . There is an edge from $Q_{k}$ to $C_{i}$ if $q_{k}$ or $\neg q_{k}$ is one of the literals in $C_{i}$ . The CPD for $C_{i}$ is deterministic, and chosen such that it exactly duplicates the behavior of the clause. Note that, because $C_{i}$ contains at most three variables, the CPD has at most eight distributions, and at most sixteen entries. 

We want to introduce a variable $X$ that has the value 1 if and only if all the $C_{i}$ ’s have the value 1 . We can achieve this requirement by having $C_{1},\ldots,C_{m}$ be parents of $X$ . This construction, however, has the property that $P(X\mid C_{1},.\,.\,,C_{m})$ is exponentially large when written as a table. To avoid this difculty, we introduce intermediate “AND” gates $A_{1},\dots,A_{m-2}$ , so that $A_{1}$ is the “AND” of $C_{1}$ and $C_{2}$ , $A_{2}$ is the “AND” of $A_{1}$ and $C_{3}$ , and so on. The last variable $X$ is the “AND” of $A_{m-2}$ and $C_{m}$ . This construction achieves the desired efect: $X$ has value 1 if and only if all the clauses are satisﬁed. Furthermore, in this construction, all variables have at most three (binary-valued) parents, so that the size of $\mathcal{B}_{\phi}$ is polynomial in the size of $\phi$ . It follows that $P_{\mathcal{B}_{\phi}}(x^{1}\mid q_{1},.\,.\,.\,,q_{n})=1$ if and only if $q_{1},\ldots,q_{n}$ is a satisfying assignment for $\phi$ . Because the prior probability of each possible assignment is $1/2^{n}$ , we get that the overall probability $P_{\mathcal{B}_{\phi}}(x^{1})$ is the number of satisfying assignments to $\phi$ , divided by $2^{n}$ . We can therefore test whether $\phi$ has a satisfying assignment simply by checking whether $P(x^{1})>0$ . 

This analysis shows that the decision problem associated with Bayesian network inference is $\mathcal{N P}$ -complete. However, the problem is originally a numerical problem. Precisely the same construction allows us to provide an analysis for the original problem formulation. We deﬁne the problem $B N!P r$ as follows: 

Given: a Bayesian network $\mathcal{B}$ over $\mathcal{X}$ , a variable $X\,\in\,{\mathcal{X}}$ , and a value $x\;\in\;V a l(X)$ , compute $P_{\mathcal{B}}(X=x)$ . 

Our task here is to compute the total probability of network instantiations that are consistent with $X=x$ . Or, in other words, to do a weighted count of instantiations, with the weight being the probability. An appropriate complexity class for counting problems is $\#\mathcal{P}$ : Whereas $\mathcal{N P}$ represents problems of deciding “are there any solutions that satisfy certain requirements,” $\#\mathcal{P}$ P represents problems that ask “how many solutions are there that satisfy certain requirements.” It is not surprising that we can relate the complexity of the BN inference problem to the counting class $\#\mathcal{P}$ : 

The problem BN-Pr is $\#\mathcal{P}$ -complete. We leave the proof as an exercise (exercise 9.1). 

### 9.1.2 Analysis of Approximate Inference 
Upon noting the hardness of exact inference, a natural question is whether we can circumvent the difculties by compromising, to some extent, on the accuracies of our answers. Indeed, in many applications we can tolerate some imprecision in the ﬁnal probabilities: it is often unlikely that a change in probability from 0 . 87 to 0 . 92 will change our course of action. Thus, we now explore the computational complexity of approximate inference. 

To analyze the approximate inference task formally, we must ﬁrst deﬁne a metric for evaluating the quality of our approximation. We can consider two perspectives on this issue, depending on how we choose to deﬁne our query. Consider ﬁrst our previous formulation of the conditional probabilit query task, wh e our goal is to compute the probability $P(Y\mid e)$ for some set of variables Y $Y$ and evidence e . The result of this type of query is a probability distribution over $Y$ . Given an approximate answer to this query, we can evaluate its quality using any of the distance metrics we deﬁne for probability distributions in appendix A.1.3.3. 

There is, however, another way of looking at this task, one that is somewhat simpler and will be very useful for analyzing its complexity. Consider a speciﬁc query $P(\pmb{y}\mid\pmb{e})$ , where we are focusing on one particular assignment $_{_y}$ . The approximate answer to this query is a number $\rho$ , whose accuracy we wish to evaluate relative to the correct probability. One way of evaluating the accuracy of an estimate is as simple as the diference between the approximate answer and the right one. 

$$
|P(\pmb{y}\mid e)-\rho|\leq\epsilon.
$$ 

This deﬁnition, although plausible, is somewhat weak. Consider, for example, a situation in which we are trying to compute the probability of a really rare disease, one whose true probability is, say, 0 . 00001 . In this case, an absolute error of 0 . 0001 is unacceptable, even though such an error may be an excellent approximation for an event whose probability is 0 . 3 . A stronger deﬁnition of accuracy takes into consideration the value of the probability that we are trying to estimate: 

Deﬁnition 9.2 relative error 

An estimate $\rho$ has relative error $\epsilon$ for $P(\pmb{y}\mid\pmb{e})$ if: 

$$
\frac{\rho}{1+\epsilon}\le P(\pmb{y}\mid e)\le\rho(1+\epsilon).
$$ 

Note that, unlike absolute error, relative error makes sense even for $\epsilon>1$ . For example, $\epsilon=4$ means that $P(\pmb{y}\mid e)$ is at least 20 percent of $\rho$ and at most 600 percent of $\rho$ . For probabilities, where low values are often very important, relative error appears much more relevant than absolute error. 

With these deﬁnitions, we can turn to answering the question of whether approximate in- ference is actually an easier problem. A priori, it seems as if the extra slack provided by the approximation might help. Unfortunately, this hope turns out to be unfounded. As we now show, approximate inference in Bayesian networks is also $\mathcal{N P}$ -hard. 

This result is straightforward for the case of relative error. 

Theorem 9.3 The following problem is -hard: 

Given a Bayesian network $\mathcal{B}$ ove $\mathcal{X}$ , a variable $X\in{\mathcal{X}}$ , and a value $x\in V a l(X)$ , ﬁnd a number $\rho$ that has relative error ϵ for $P_{\mathcal{B}}(X=x)$ . 

Proof The proof is obvious based on the original $\mathcal{N P}$ k inference (theorem 9.1). There, we proved that it is NP -hard to decide whethe $P_{\mathcal{B}}(x^{1})>0$ . Now, assume that we have an algorithm that returns an estimate $\rho$ to the same $P_{\mathcal{B}}(x^{1})$ , which B is guaranteed to have relative error $\epsilon$ for some $\epsilon>0$ . Then $\rho>0$ if and only if $P_{\mathcal{B}}(x^{1})>0$ . Thus, achieving this relative error is as $\mathcal{N P}$ -hard as the original problem. 

We can generalize this result to make $\epsilon(n)$ a function that grows with the input size $n$ . Thus, for example, we can deﬁne $\epsilon(n)=2^{2^{n}}$ and the theorem still holds. Thus, in a sense, this result is not so interesting as a statement about hardness of approximation. Rather, it tells us that relative error is too strong a notion of approximation to use in this context. 

What about absolute error? As we will see in section 12.1.2, the problem of just approximating $P(X\,=\,x)$ up to some ﬁxed absolute error $\epsilon$ has a randomized polynomial time algorithm. Therefore, the problem cannot be $\mathcal{N P}$ -hard unless $\mathcal{N P}=\mathcal{R P}$ . T sult is an improvement on the exact case, where even the task of computing $P(X=x)$ is -hard. 

Unfortunately, the good news is very limited in scope, in that it disappears once we introduce e. Speciﬁcally, it is $\mathcal{N P}$ -hard to ﬁnd an absolute approximation to $P(x\mid e)$ for any $\epsilon<1/2$ . 

Theorem 9.4 

Given a Ba netw $\mathcal{B}$ over $\mathcal{X}$ e $X\,\in\,{\mathcal{X}}$ , a value $x\,\in\,V a l(X)$ , and a observation $E=e$ for $E\subset{\mathcal{X}}$ ⊂X and $e\in V a l(E)$ ∈ , ﬁnd a number $\rho$ that has absolute error ϵ for $P_{\mathcal{B}}(X=x\mid e)$ . 

Proof The proof uses the same construction that we used before. Consider a formula $\phi$ , and nsider the analogous BN $\mathcal{B}$ , as described in theorem 9.1. Recall that our BN had a variable $Q_{i}$ for each propositional variable $q_{i}$ in our Boolean formula, a bunch of other intermediate variables, and then a variable $X$ whose value, given any assignment of values $q_{1}^{1},q_{1}^{0}$ to the $Q_{i}$ ’s, was the associated truth value of the formula. We now show that, given such an approximation algorithm, we can cide ether the formula is satis ble. We begin by computing $P(Q_{1}\mid x^{1})$ . We pick the value $v_{1}$ for $Q_{1}$ that is most likely given $x^{1}$ , and we instantiate it to this value. That , we generate a network $\mathcal{B}_{2}$ that does not contain $Q_{1}$ , and that represents the distribution $\mathcal{B}$ B conditioned on $Q_{1}\,=\,v_{1}$ . We repeat this process for $Q_{2},\ldots,Q_{n}$ . This results in some assignment $v_{1},\dots,v_{n}$ to the $Q_{i}$ ’s. We now prove that this is a satisfying assignment if and only if the original formula $\phi$ was satisﬁable. 

We begin with the easy case. If $\phi$ is not satisﬁable, then $v_{1},\dots,v_{n}$ can hardly be a satisfying assignment for it. Now, assume that $\phi$ is satisﬁable. We show that it also has a satisfying assignment with $Q_{1}\,=\,v_{1}$ . If $\phi$ is satisﬁable with both $Q_{1}\,=\,q_{1}^{1}$ and $Q_{1}\,=\,q_{1}^{0}$ , then this is obvious. Assume, however, that $\phi$ is satisﬁable, but not when $Q_{1}\,=\,v$ . Then necessarily, we will have that $P(Q_{1}=v\mid x^{1})$ is 0, and the probability of the complementary event i 1. If we have an approximation $\rho$ whose error is guaranteed to be $<1/2$ , then choosing the v that maximizes this probability is guaranteed to pick the $v$ whose probability is 1. Thus, in either case the formula has a satisfying assignment where $Q_{1}=v$ . 

We can continue in this fashion, proving by induction on $k$ that $\phi$ has a satisfying assignment with $Q_{1}=v_{1},.\,.\,.\,,Q_{k}=v_{k}$ . In the case where $\phi$ is satisﬁable, this process will terminate with a satisfying assignment. In the case where $\phi$ is not, it clearly will not terminate with a satisfying assignment. We can determine which is the case simply by checking whether the resulting assignment satisﬁes $\phi$ . This gives us a polynomial time process for deciding satisﬁability. 

Because $\epsilon=1/2$ corresponds to random guessing, this result is quite discouraging. It tells us that, in the case where we have evidence, approximate inference is no easier than exact inference, in the worst case. 

## 9.2 Variable Elimination: The Basic Ideas 
We begin our discussion of inference by discussing the principles underlying exact inference in graphical models. As we show, the same graphical structure that allows a compact representation of complex distributions also help support inference. In particular, we can use dynamic programming techniques (as discussed in appendix A.3.3) to perform inference even for certain large and complex networks in a very reasonable time. We now provide the intuition underlying these algorithms, an intuition that is presented more formally in the remainder of this chapter. 

We begin by considering the inference task in a very simple network $A\,\rightarrow\,B\,\rightarrow\,C\,\rightarrow$ $D$ . We ﬁrst provide a phased computation, which uses results from the previous phase for the computation in the next phase. We then reformulate this process in terms of a global computation on the joint distribution. 
> 我们考虑网络 $A \rightarrow B \rightarrow C \rightarrow D$ 中的推理任务
> 我们首先考虑一个分阶段的计算方法，每个阶段使用上个阶段的计算结果
> 然后我们将该过程重构为在联合分布上的全局计算

Assume that our ﬁrst goal is to compute the probability $P(B)$ , that is, the distribution over values $b$ of $B$ . Basic probabilistic reasoning (with no assumptions) tells us that 

$$
P(B)=\sum_{a}P(a)P(B\mid a).\tag{9.4}
$$ 
Fortunately, we have all the required numbers in our Bayesian network representation: each number $P(a)$ is in the CPD for $A$ , and each number $P(b\mid a)$ is in the CPD for $B$ . Note that if $A$ has $k$ values and $B$ has $m$ values, the number of basic arithmetic operations required is $O(k\times m)$ : to compute $P(b)$ , we must multiply $P(b\mid a)$ $P(a)$ for each of the $k$ values of $A$ , and then add them up, that is, $k$ multiplications and $k-1$ additions; this process must be repeated for each of the m values b . 
> 考虑计算 $B$ 的边际分布，将 $P (B)$ 根据图结构写为式 9.4 的形式
> 式 9.4 中，$P (a)$ 为 $a$ 的 CPD，$P (B\mid a)$ 为 $B$ 的 CPD，二者在贝叶斯网络表示中已知，故可以直接计算
> 假设 $|Val (A)| = k, |Val (B)| = m$，则式 9.4 的计算复杂度为 $O (k\times m)$，因为 $P (b)$ 的计算需要遍历 $A$ 的所有 $k$ 个取值，而 $P (B)$ 的计算需要遍历 $B$ 的所有 $m$ 个取值

Now, assume we want to compute $P(C)$ . Using the same analysis, we have that 

$$
P(C)=\sum_{b}P(b)P(C\mid b).\tag{9.5}
$$ 
Again, the conditional probabilities $P(c\mid b)$ are known: they constitute the CPD for $C$ . The probability of B is not speciﬁed as part of the network parameters, but equation (9.4) shows us how it can be computed. Thus, we can compute $P(C)$ . We can continue the process in an analogous way, in order to compute $P(D)$ . 
> 根据式 9.5，依赖于式 9.4 计算的结果和贝叶斯网络中 $C$ 的 CPD，我们可以进一步推理 $C$ 的边际分布，以此类推

Note that the structure of the network, and its efect on the parameter iz ation of the CPDs, is critical for our ability to perform this computation as described. Speciﬁcally, assume that $A$ had been a parent of $C$ . In this case, the CPD for $C$ would have included $A$ , and our computation of $P(B)$ would not have sufced for equation (9.5). 

Also note that this algorithm does not compute single values, but rather sets of values at a time. In particular equation (9.4) computes an entire distribution over all of the possible values of $B$ . All of these are then used in equation (9.5) to compute $P(C)$ . This property turns out to be critical for the performance of the general algorithm. 
> 该算法并没有仅计算单个取值的概率，而是计算所有取值的概率
> 并且，链式计算的下一个变量的边际概率依赖于上一个变量计算得到的所有取值的概率

Let us analyze the complexity of this process on a general chain. Assume that we have a chain with $n$ variables $X_{1}\,\rightarrow\,.\,.\,\rightarrow\,X_{n},$ each e in $k$ values. As described, the algorithm would compute $P(X_{i+1})$ from $P(X_{i})$ , for $i=1,\dots,n-1$  . Each such step would consist of the following computation: 

$$
P(X_{i+1})=\sum_{x_{i}}{P(X_{i+1}\mid x_{i})P(x_{i})},
$$ 
where $P(X_{i})$ is computed in the previous step. 
> 将该算法推广到 $X_1 \rightarrow \dots \rightarrow X_n$，算法的每一步都会从 $P (X_i)$ 计算 $P (X_{i+1})$ ($i = 1, \dots, n-1$)，计算公式如上所示

The cost of each such step is $O(k^{2})$ : The distributi er $X_{i}$ has $k$ va s, and the CPD $P(X_{i+1}\mid X_{i})$ has $k^{2}$ values; we need to multiply $P(x_{i})$ , for value x , with each CPD entry $P(x_{i+1}\mid x_{i})$ $\,\!\,k^{2}$ multiplications), and then, for each value $x_{i+1}$ , sum up the co entries ( $(k\times(k-1)$ additions). We need to perform this process for every variable $X_{2},\ldots,X_{n}$ ; hence, the total cost is $O(n k^{2})$ . 
> 假设每个 $X_i$ 有 $k$ 个取值，则每一步的开销就是 $O (k^2)$，算法中，$X_2, \dots, X_n$ 各执行一步，因此总开销为 $O (nk^2)$，该开销和 $n$ 呈线性关系

By comparison, consider the process of generating the entire joint and summing it out, which requires that we generate $k^{n}$ probabilities for the diferent events $x_{1},\dots,x_{n}$ . Hence, we have at least one example where, despite the exponential size of the joint distribution, we can do inference in linear time. 
> 如果直接逐个计算 $(x_1, \dots, x_n)$ 每个赋值的概率来直接得到联合分布，我们需要计算 $k^n$ 个概率，该复杂度和 $n$ 呈指数关系

Using this process, we have managed to do inference over the joint distribution without ever generating it explicitly. 
> 该过程让我们在不显示生成联合分布的情况下可以在其上进行推理

What is the basic insight that allows us to avoid the exhaustive enumeration? Let us reexamine this process in terms of the joint $P(A,B,C,D)$ . By the chain rule for Bayesian networks, the joint decomposes as 

$$
P(A)P(B\mid A)P(C\mid B)P(D\mid C)
$$ 
To compute $P(D)$ , we need to sum together all of the entries where $D=d^{1}$ , and to (separately) sum together all of the entries where $D\ =\ d^{2}$ . The exact computation that needs to be performed, for binary-valued variables $A,B,C,D$ , is shown in ﬁgure 9.2. 

Examining this summation, we see that it has a lot of structure. For example, the third and fourth terms in the ﬁrst two entries are both $P(c^{1}\mid b^{1})P(d^{1}\mid c^{1})$ . We can therefore modify the computation to ﬁrst compute 

$$
P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2})
$$ 
and only then multiply by the common term. The same structure is repeated throughout the table. If we perform the same transformation, we get a new expression, as shown in ﬁgure 9.3. 

We now observe that certain terms are repeated several times in this expression. Speciﬁcally, $P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2})$ and $P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2})$ are each repeated four times. Thus, it seems clear that we can gain signiﬁcant computational savings by computing them once and then storing them. There are two such expressions, one for each value of $B$ . Thus, we e a function $\tau_{1}\ :\ \,V a l(B)\mapsto I\!\!R,$ , where $\tau_{1}(b^{1})$ is the ﬁrst of these two expressions, and $\tau_{1}(b^{2})$ is the second. Note that $\tau_{1}(B)$ corresponds exactly to $P(B)$ . 

The resulting expression, assuming $\tau_{1}(B)$ has been computed, is shown in ﬁgure 9.4. Examin- ing this new expression, we see that we once again can reverse the order of a sum and a product, resulting in the expression of ﬁgure 9.5. And, once again, we notice some shared expressions, that are better computed once and used multiple times. We deﬁne $\tau_{2}~:~V a l(C)\mapsto I\!\!R.$ . 

$$
\begin{array}{l l l}{{\tau_{2}(c^{1})}}&{{=}}&{{\tau_{1}(b^{1})P(c^{1}\mid b^{1})+\tau_{1}(b^{2})P(c^{1}\mid b^{2})}}\\ {{\tau_{2}(c^{2})}}&{{=}}&{{\tau_{1}(b^{1})P(c^{2}\mid b^{1})+\tau_{1}(b^{2})P(c^{2}\mid b^{2})}}\end{array}
$$ 
1. When $D$ is binary-valued, we can get away with doing only the ﬁrst of these computations. However, this trick does not carry over to the case of variables with more than two values or to the case where we have evidence. Therefore, our example will show the computation in its generality. 

$$
\begin{array}{c c}{{}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{1}\mid b^{1})~~~P(d^{1}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{1}\mid b^{2})~~~P(d^{1}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{2}\mid b^{1})~~~P(d^{1}\mid c^{2})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{2}\mid b^{2})~~~P(d^{1}\mid c^{2})}}\\ {{}}&{{}}&{{}}\\ {{}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{1}\mid b^{1})~~~P(d^{2}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{1}\mid b^{2})~~~P(d^{2}\mid c^{1})}}\\ {{+}}&{{(P(a^{1})P(b^{1}\mid a^{1})+P(a^{2})P(b^{1}\mid a^{2}))~~~P(c^{2}\mid b^{1})~~~P(d^{2}\mid c^{2})}}\\ {{+}}&{{(P(a^{1})P(b^{2}\mid a^{1})+P(a^{2})P(b^{2}\mid a^{2}))~~~P(c^{2}\mid b^{2})~~~P(d^{2}\mid c^{2})}}\end{array}
$$ 

Figure 9.3 The ﬁrst transformation on the sum of ﬁgure 9.2 

$$
\begin{array}{c c c}{\tau_{1}(b^{1})}&{P(c^{1}\mid b^{1})}&{P(d^{1}\mid c^{1})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{1}\mid b^{2})}&{P(d^{1}\mid c^{1})}\\ {+}&{\tau_{1}(b^{1})}&{P(c^{2}\mid b^{1})}&{P(d^{1}\mid c^{2})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{2}\mid b^{2})}&{P(d^{1}\mid c^{2})}\\ {}&{}&{}&{}\\ {\tau_{1}(b^{1})}&{P(c^{1}\mid b^{1})}&{P(d^{2}\mid c^{1})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{1}\mid b^{2})}&{P(d^{2}\mid c^{1})}\\ {+}&{\tau_{1}(b^{1})}&{P(c^{2}\mid b^{1})}&{P(d^{2}\mid c^{2})}\\ {+}&{\tau_{1}(b^{2})}&{P(c^{2}\mid b^{2})}&{P(d^{2}\mid c^{2})}\end{array}
$$ 

$$
{\begin{array}{l l l}{}&{(\tau_{1}(b^{1})P(c^{1}\mid b^{1})+\tau_{1}(b^{2})P(c^{1}\mid b^{2}))}&{P(d^{1}\mid c^{1})}\\ {+}&{(\tau_{1}(b^{1})P(c^{2}\mid b^{1})+\tau_{1}(b^{2})P(c^{2}\mid b^{2}))}&{P(d^{1}\mid c^{2})}\\ {}&{}&{}\\ {+}&{(\tau_{1}(b^{1})P(c^{1}\mid b^{1})+\tau_{1}(b^{2})P(c^{1}\mid b^{2}))}&{P(d^{2}\mid c^{1})}\\ {+}&{(\tau_{1}(b^{1})P(c^{2}\mid b^{1})+\tau_{1}(b^{2})P(c^{2}\mid b^{2}))}&{P(d^{2}\mid c^{2})}\end{array}}
$$ 

Figure 9.5 The third transformation on the sum of ﬁgure 9.2 

$$
{\begin{array}{r l}{\tau_{2}(c^{1})}&{P(d^{1}\mid c^{1})}\\ {+}&{\tau_{2}(c^{2})}&{P(d^{1}\mid c^{2})}\\ {\,}&{}\\ {\tau_{2}(c^{1})}&{P(d^{2}\mid c^{1})}\\ {+}&{\tau_{2}(c^{2})}&{P(d^{2}\mid c^{2})}\end{array}}
$$ 

Figure 9.6 The fourth transformation on the sum of ﬁgure 9.2 

The ﬁnal expression is shown in ﬁgure 9.6. 

Summarizing, we begin by computing $\tau_{1}(B)$ , which requires four multiplications and two additions. Using it, we can compute $\tau_{2}(C)$ , which also requires four multiplications and two additions. Finally, we can compute $P(D)$ , again, at the same cost. The total number of operations is therefore 18. By comparison, generating the joint distribution requires $16\cdot3=48$ multiplications (three for each of the 16 entries in the joint), and 14 additions (7 for each of $P(d^{1})$ and $P(d^{2}))$ . 

Written somewhat more compactly, the transformation we have performed takes the following steps: We want to compute 

$$
P(D)=\sum_{C}\sum_{B}\sum_{A}P(A)P(B\mid A)P(C\mid B)P(D\mid C).
$$ 
We push in the ﬁrst summation, resulting in 

$$
\sum_{C}P(D\mid C)\sum_{B}P(C\mid B)\sum_{A}P(A)P(B\mid A).
$$ 
We compute the product $\psi_{1}(A,B)=P(A)P(B\mid A)$ a d then sum out $A$ to obtain the func- $\begin{array}{r}{\tau_{1}(B)=\sum_{A}\psi_{1}(A,B)}\end{array}$ . Speciﬁcally, for each value b , we compute $\begin{array}{r l r}{\tau_{1}(b)=\sum_{A}\psi_{1}(A,b)=}\end{array}$ $\textstyle\sum_{A}P(A)P(b\mid A)$ . We then continue by computing: 

$$
\begin{array}{r c l}{{\psi_{2}(B,C)}}&{{=}}&{{\tau_{1}(B)P(C\mid B)}}\\ {{\tau_{2}(C)}}&{{=}}&{{\displaystyle\sum_{B}\psi_{2}(B,C).}}\end{array}
$$ 
This computation results in a new vector $\tau_{2}(C)$ , which we then proceed to use in the ﬁnal phase of computing $P(D)$ . 

This procedure is performing dynamic programming (see appendix A.3.3); doing this sum- mation the naive way w uld h us compute every $\begin{array}{r}{P(b)=\sum_{A}P(A)P(b\mid A)}\end{array}$ many times, once for every value of C and D . In general, in a chain of length $n$ , this internal summation would be computed exponentially many times. Dynamic programming “inverts” the order of computation — performing it inside out instead of outside in. Speciﬁcally, we perform the innermost summation ﬁrst, computing once and for all the values in $\tau_{1}(B)$ ; that allows us to compute $\tau_{2}(C)$ once and for all, and so on. 
> 链式算法将朴素算法的指数级复杂度降到了线性级别，该算法本质是一种动态规划，例如计算一个变量的边际分布 $P (B) = \sum_a P (a) P (B\mid a)$ ，我们将所有的 $P (a)$ 提前计算好并存储，避免每次计算 $P (b)$ 都需要重新计算 $P (A)$，导致指数级别的重复计算

**To summarize, the two ideas that help us address the exponential blowup of the joint distribution are:** 

- **Because of the structure of the Bayesian network, some subexpressions in the joint depend only on a small number of variables.**
- **By computing these expressions once and caching the results, we can avoid generating them exponentially many times.** 

 >总结来说，帮助我们应对联合分布的指数爆炸的两个想法是：
>- 由于贝叶斯网络的结构，联合分布中的一些子表达式只依赖于少数几个变量
>- 通过一次性计算这些表达式并缓存结果，我们可以避免多次（指数次）生成它们

## 9.3 Variable Elimination 
To formalize the algorithm demonstrated in the previous section, we need to introduce some basic concepts. In chapter 4, we introduced the notion of a factor $\phi$ over a scope $S c o p e[\phi]=X$ , which is a function $\phi:V a l(X)\mapsto I\!\!R$ . The main steps in the algorithm described here can be viewed as a manipulation of factors. Importantly, by using the factor-based view, we can deﬁne the algorithm in a general form that applies equally to Bayesian networks and Markov networks. 
> 我们知道一个作用域为 $X$ 的因子 $\phi$ 定义为一个函数 $\phi: Val (X) \mapsto \mathbb R$
> 上一节介绍的算法的主要步骤可以被视为对因子的操作，通过因子视角，我们将该算法推广到同时适用于贝叶斯网络和 Markov 网络

### 9.3.1 Basic Elimination 
#### 9.3.1.1 Factor Marginalization 
The key operation that we are performing when computing the probability of some subset of variables is that of marginalizing out variables from a distribution. That is, we have a distribution over a of variables $\mathcal{X}$ , and we want to compute the marginal of that distribution over some subset $\pmb X$ . We can view this computation as an operation on a factor: 
>计算某些变量子集的概率的关键操作是从分布中消去变量
>也就是说，我们有一个关于变量集合 $\mathcal{X}$ 的分布，我们想要计算该分布关于某个子集 $\pmb{X}$ 的边缘分布，我们可以将此计算视为对一个因子的操作：

**Deﬁnition 9.3** factor marginalization 
Let $X$ be a set of v iab s, and $Y\notin X$ a variable. Let $\phi(X,Y)$ e a factor. We deﬁne the factor marginalization of Y $Y$ in φ , denoted $\textstyle\sum_{Y}\phi$ , to be a factor $\psi$ over X such that: 

$$
\psi(\pmb X)=\sum_{Y}\phi(\pmb X,Y).
$$

This operation is also called summing out of $Y$ in $\psi$ . 
>定义
> $\pmb X$ 为变量集合，变量 $Y \not\in \pmb X$，$\phi (\pmb X, Y)$ 为因子，定义 $\phi$ 中对 $Y$ 的因子边际化 $\sum_Y \phi$ 为一个 $\pmb X$ 上的因子 $\psi$，满足 $\psi (\pmb X) = \sum_Y\phi (\pmb X, Y)$
> 该运算也称为在 $\psi$ 中求和消去 $Y$

The key point in this deﬁnition is that we only sum up entries in the table where the values of $X$ match up. Figure 9.7 illustrates this process. 
>这个定义的关键在于我们只对表格中 $X$ 的取值匹配的部分进行求和

The process of marginalizing a joint distribution $P(X,Y)$ onto $X$ in a Bayesian network is simply summing out the variables $Y$ in the factor corresponding to $P$ . If we sum out all variables, we get a factor consisting of a single number whose value is 1 . If we sum out all of the variables in the unnormalized distribution $\tilde{P}_{\Phi}$ deﬁned by the product of factors in a Markov network, we get the partition function. 
>在贝叶斯网络中，将联合分布 $P(X,Y)$ 边际化到 $X$ 的过程就是对对应于 $P$ 的因子中的变量 $Y$ 求和。如果我们消去所有变量，我们得到一个只包含单个数值的因子，其值为1。如果我们消去马尔可夫随机场中未归一化的分布 $\tilde{P}_{\Phi}$（由因子的乘积定义）中的所有变量，我们得到分区函数。

A key observation used in performing inference in graphical models is that the operations of factor product and summation behave precisely as do product and summation over numbers. , both operations are commutative, s $\phi_{1}\cdot\phi_{2}\,=\,\phi_{2}\,\cdot\,\phi_{1}$ and $\begin{array}{r l}{\sum_{\boldsymbol{X}}\sum_{\boldsymbol{Y}}\phi\;=}\end{array}$ $\textstyle\sum_{Y}\sum_{X}\phi$ . Products are also associative, so that $\left(\phi_{1}\cdot\phi_{2}\right)\cdot\phi_{3}=\phi_{1}\cdot\left(\phi_{2}\cdot\phi_{3}\right)$ · · · ·  . 
> 因为因子本质是从作用域映射到标量的函数，因此我们允许在边际化中任意交换求和顺序，以及因子之间相乘也和标量一样满足结合律和交换律

Most importantly, we have a simple rule allowing us to exchange summation and product: If $X\not\in S c o p e[\phi_{1}]$ , then 
> 并且，如果 $X$ 不属于因子 $\phi_1$ 的作用域，$\phi_1$ 相对于对 $X$ 的求和就仅仅是常数，可以直接提取到求和符号外面，如式 9.6 所示

$$
\sum_{X}(\phi_{1}\cdot\phi_{2})=\phi_{1}\cdot\sum_{X}\phi_{2}.\tag{9.6}
$$ 
#### 9.3.1.2 The Variable Elimination Algorithm 
The key to both of our examples in the last section is the application of equation (9.6). Speciﬁcally, in our chain example of section 9.2, we can write: 

$$
P(A,B,C,D)=\phi_{A}\cdot\phi_{B}\cdot\phi_{C}\cdot\phi_{D}.
$$ 
On the other hand, the marginal distribution over $D$ is 

$$
P(D)=\sum_{C}\sum_{B}\sum_{A}P(A,B,C,D).
$$ 
Applying equation (9.6), we can now conclude: 

$$
\begin{array}{r c l}{{P(D)}}&{{=}}&{{\displaystyle\sum_{C}\displaystyle\sum_{B}\displaystyle\sum_{A}\phi_{A}\cdot\phi_{B}\cdot\phi_{C}\cdot\phi_{D}}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\displaystyle\sum_{B}\phi_{C}\cdot\phi_{D}\cdot\left(\displaystyle\sum_{A}\phi_{A}\cdot\phi_{B}\right)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\phi_{D}\cdot\left(\displaystyle\sum_{B}\phi_{C}\cdot\left(\displaystyle\sum_{A}\phi_{A}\cdot\phi_{B}\right)\right),}}\end{array}
$$ 
where the diferent transformations are justiﬁed by the limited scope of the CPD factors; for example, the second equality is justiﬁed by the fact that the scope of $\phi_{C}$ and $\phi_{D}$ does not contain $A$ . In general, any marginal probability computation involves taking the product of all the CPDs, and doing a summation on all the variables except the query variables. We can do these steps in any order we want, as long as we only do a summation on a variable $X$ after multiplying in all of the factors that involve $X$ . 
>其中不同的变换的合理性来自于 CPD 因子各自的有限范围；例如，第二个等式是因为 $\phi_{C}$ 和 $\phi_{D}$ 的作用范围不包含 $A$
>一般来说，任何边际概率计算都涉及对所有 CPD 的乘积运算，并对除了查询变量之外的所有变量进行求和，就如以上的推导所示
>只要我们在乘以所有包含变量 $X$ 的因子之后再对变量 $X$ 进行求和，也就是在和式中需要包含所有相关的因子，我们就可以任意地将不相关的因子移出和式，然后分别进行相应的求和

In general, we can view the task at hand as that of computing the value of an expression of the form: 

$$
\sum_{\pmb Z}\prod_{\phi\in\Phi}\phi.
$$ 
We call this task the sum-product inference task. The key insight that allows the efective computation of this expression is the fact that the scope of the factors is limited, allowing us to “push in” some of the summations, performing them over the product of only a subset of factors. 
> 一般地说，我们可以将计算边际分布的任务视为计算形式为 $\sum_{\pmb Z} \prod_{\phi \in \Phi} \phi$ 的表达式的值，该任务也被称为和-积推理任务
> 该表达式可以高效计算根本原因在于每个因子的作用域是有限制的，这使得我们可以将一些求和仅在相关的因子子集的积上进行 (而不是所有因子的乘积)

One simple instantiation of this algorithm is a procedure called sum-product variable elimination (VE), shown in algorithm 9.1. The basic idea in the algorithm is that we sum out variables one at a time. When we sum out any variable, we multiply all the factors that mention that variable, generating a product factor. Now, we sum out the variable from this combined factor, generating a new factor that we enter into our set of factors to be dealt with. 
> 这类算法的一个简单应用实例就是和-积变量消除，算法流程见 Algorithm 9.1，该算法的基本思路就是一次求和消去一个变量，要求和消去某个变量时，我们将所有和该变量有关的因子从因子集合移除，然后相乘，并对相乘得到的该因子求和消除该变量，得到的新因子再加入我们的因子集合

![[PGM-Algorithm9.1.png]]

Based on equation (9.6), the following result follows easily: 

**Theorem 9.5** 
Let $X$ e set of variables, and let $\Phi$ be a set o hat for each $\phi\in\Phi$ , $S c o p e[\phi]\subseteq X$ . Let $Y\subset X$ be a set of query variables, and let $Z=X\mathrm{~-~}Y$ . Then for any ordering ≺ over Z , Sum-Product $\textstyle\mathcal{\mathrm{NE}}(\Phi,Z,\prec)$ returns a factor $\phi^{*}(Y)$ such that 
> 定理：
> $\pmb X$ 为变量集合，$\Phi$ 为一组因子，满足对于所有 $\phi \in \Phi$，$Scope[\phi]\subseteq \pmb X$
> $\pmb Y \subset \pmb X$ 为一组查询变量，$\pmb Z = \pmb X - \pmb Y$，则对于 $\pmb Z$ 上的任意排序 $\prec$，$\text{Sum-Product-VE}(\Phi, \pmb Z, \prec)$ 返回的因子 $\phi^*(\pmb Y)$ 满足：

$$
\phi^{*}(\pmb Y)=\sum_{\pmb Z}\prod_{\phi\in\Phi}\phi.
$$

We can apply this algorithm to the task of computing the probability distribution $P_{\mathcal{B}}(Y)$ for a Bayesian network $\mathcal{B}$ . We simply instantiate $\Phi$ to consist of all of the CPDs: 

$$
\Phi=\{\phi_{X_{i}}\}_{i=1}^{n}
$$ 
where $\phi_{X_{i}}\;=\;P(X_{i}\;\mid\;\mathrm{Pa}_{X_{i}})$ . We then apply the variable elimination algorithm to the set $\left\{Z_{1},.\,.\,.\,,Z_{m}\right\}=\mathcal{X}-Y$ (that is, we eliminate all the nonquery variables). 
> 我们可以应用该算法在贝叶斯网络 $\mathcal B$ 中计算概率分布 $P_{\mathcal B}(\pmb Y)$
> 我们将所有的 CPD 构成的集合作为 $\Phi = \{\phi_{X_i}\}_{i=1}^n$ ($\phi_{X_i} = P (X_i \mid \text{Pa}_{X_i})$)，然后对变量集合 $\{Z_1, \dots, Z_m\} = \mathcal X - \pmb Y$ 执行变量消除算法，也就是消除所有的非查询变量

We can also apply precisely the same algorithm to the task of computing conditional prob- abilities in a Markov network. We simply initialize the factors to be the clique potentials and run the elimination algorithm. As for Bayesian networks, we then apply the variable elimination algorithm the set $Z=\mathcal{X}-Y$ . T procedure returns an unnormalized factor over the query variables Y . The distribution over $Y$ can be obtained by normalizing the factor; the partition function is simply the normalizing constant.
> 在 Markov 网络中计算边际分布同样也可以应用该算法
> 我们将所有的团势能函数作为最初的因子集合，然后为变量集合 $\pmb Z = \mathcal X - \pmb Y$ 执行变量消除算法，该过程最后返回查询变量 $\pmb Y$ 上一个未规范化的因子，将其规范化我们就得到了 $\pmb Y$ 上的边际分布

Example 9.1
Let us demonstrate the procedure on a nontrivial example. Consider the network demonstrated in ﬁgure 9.8, which is an extension of our Student network. The chain rule for this network asserts that 

$$
\begin{array}{r c l}{{P(C,D,I,G,S,L,J,H)}}&{{=}}&{{P(C)P(D\mid C)P(I)P(G\mid I,D)P(S\mid I)}}\\ {{}}&{{}}&{{P(L\mid G)P(J\mid L,S)P(H\mid G,J)}}\\ {{}}&{{=}}&{{\phi_{C}(C)\phi_{D}(D,C)\phi_{I}(I)\phi_{G}(G,I,D)\phi_{S}(S,I)}}\\ {{}}&{{}}&{{\phi_{L}(L,G)\phi_{J}(J,L,S)\phi_{H}(H,G,J).}}\end{array}
$$ 
We will now apply the $V E$ algorithm to compute $P(J)$ . We will use the elimination ordering: $C,D,I,H,G,S,L$ : 

1. Eliminating $C$ : We compute the factors 

$$
\begin{array}{r c l}{\psi_{1}(C,D)}&{=}&{\phi_{C}(C)\cdot\phi_{D}(D,C)}\\ {\tau_{1}(D)}&{=}&{\displaystyle\sum_{C}\psi_{1}.}\end{array}
$$ 
2. Eliminating $D$ : Note that we have already eliminated one of the original factors that involve $D$ — $\phi_{D}(D,C)=P(D\mid C)$ . On the other hand, we introduced the factor $\tau_{1}(D)$ that involves $D$ . Hence, we now compute: 

$$
\begin{array}{r c l}{{\psi_{2}(G,I,D)}}&{{=}}&{{\phi_{G}(G,I,D)\cdot\tau_{1}(D)}}\\ {{\tau_{2}(G,I)}}&{{=}}&{{\displaystyle\sum_{D}\psi_{2}(G,I,D).}}\end{array}
$$ 
3. Eliminating $I$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{3}(G,I,S)}}&{{=}}&{{\phi_{I}(I)\cdot\phi_{S}(S,I)\cdot\tau_{2}(G,I)}}\\ {{\tau_{3}(G,S)}}&{{=}}&{{\displaystyle\sum_{I}\psi_{3}(G,I,S).}}\end{array}
$$ 

4. Eliminating $H$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{4}(G,J,H)}}&{{=}}&{{\phi_{H}(H,G,J)}}\\ {{\tau_{4}(G,J)}}&{{=}}&{{\displaystyle\sum_{H}\psi_{4}(G,J,H).}}\end{array}
$$ 

Note that $\tau_{4}\equiv1$ (all of its entries are exac : we are simply computing $\textstyle\sum_{H}P(H\mid G,J)$ | , which is a probability distribution for every $G,J$ , and hence sums to 1. A naive execution of this algorithm will end up generating this factor, which has no value. Generating it has no impact on the ﬁnal answer, but it does complicate the algorithm. In particular, the existence of this factor complicates our computation in the next step. 

5. Eliminating $G$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{5}(G,J,L,S)}}&{{=}}&{{\tau_{4}(G,J)\cdot\tau_{3}(G,S)\cdot\phi_{L}(L,G)}}\\ {{\tau_{5}(J,L,S)}}&{{=}}&{{\displaystyle\sum_{G}\psi_{5}(G,J,L,S).}}\end{array}
$$ 

Note that, without the factor $\tau_{4}(G,J)$ , the results of this step would not have involved $J$ .

 6. Eliminating $S$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{6}(J,L,S)}}&{{=}}&{{\tau_{5}(J,L,S)\cdot\phi_{J}(J,L,S)}}\\ {{\tau_{6}(J,L)}}&{{=}}&{{\displaystyle\sum_{S}\psi_{6}(J,L,S).}}\end{array}
$$ 

7. Eliminating $L$ : We compute the factors 

$$
\begin{array}{r c l}{{\psi_{7}(J,L)}}&{{=}}&{{\tau_{6}(J,L)}}\\ {{\tau_{7}(J)}}&{{=}}&{{\displaystyle\sum_{L}\psi_{7}(J,L).}}\end{array}
$$ 

We summarize these steps in table 9.1. 

Note that we can use any elimination ordering. For example, consider eliminating variables in the order $G,\,I,\,S,\,L,\,H,\,C,\,D$ . We would then get the behavior of table 9.2. The result, as before, is precisely $P(J)$ . However, note that this elimination ordering introduces factors with much larger scope. We return to this point later on. 

#### 9.3.1.3 Semantics of Factors 
It is interesting to consider the semantics of the intermediate factors generated as part of this computation. In many of the examples we have given, they correspond to marginal or conditional probabilities in the network. However, although these factors often correspond to such probabilities, this is not always the case.
> 在变量消除计算的步骤中生成的中间因子有时会直接对应于网络中的某个边际或者条件分布，但这不是一定的

 Consider, for example, the network of ﬁgure $9.9\mathrm{a}$ . The result of eliminating the variable $X$ is a factor 
>以图 9.9(a) 的网络为例。消除变量 $X$ 后的结果因子如下

$$
\tau(A,B,C)=\sum_{X}P(X)\cdot P(A\mid X)\cdot P(C\mid B,X).
$$ 
This factor does not correspond to any probability or conditional probability in this network. To understand why, consider the various options for the meaning of this factor. Clearly, it cannot be a conditional distribution where $B$ is on the left hand side of the conditioning bar (for example, $P(A,B,C))$ , as $P(B\mid A)$ has not yet been multiplied in. The mo candidate is $P(A,C\mid B)$ . However, this conjecture is also false. The obability $P(A\mid B)$ | relies he ily on the properties of the CPD $P(B\mid A)$ for example, if B is determi stically equal to A , $P(A\mid B)$ has a very diferent form than if B depends only very weakly on A . Since the CPD $P(B\mid A)$ was not taken into consideration when computing $\tau(A,B,C)$ , it cannot represent the conditional probability $P(A,C\mid B)$ . In general, we can verify that this factor does not correspond to any conditional probability expression in this network. 
>这个因子并不对应于该网络中的任何概率或条件概率
>为了理解为什么，考虑这个因子的各种可能含义：显然，它不能是 $B$ 在条件符号的左侧的条件分布，其中（例如，$P(A,B,C)$），因为还没有乘以 $P(B \mid A)$；剩下的候选者是 $P(A,C \mid B)$ ($B$ 在条件符号右侧)。然而，这个假设也是错误的。$P(A \mid B)$ 的概率严重依赖于条件概率分布 $P(B \mid A)$ 的属性。例如，如果 $B$ 确定等于 $A$，则 $P(A \mid B)$ 的形式与 $B$ 对 $A$ 只有非常微弱的依赖性时完全不同，由于在计算 $\tau(A,B,C)$ 时没有考虑到条件概率分布 $P(B \mid A)$，因此它不能代表条件概率 $P(A,C \mid B)$。
>一般而言，我们可以验证这个因子并不对应于该网络中的任何条件概率表达式。

It is interesting to note, however, that the resulting factor does, in fact, correspond to a conditional probability $P(A,C\mid B)$ , but in a *diferent network* : the one shown in ﬁgure 9.9b, where all CPDs except for B are the same. In fact, this phenomenon is a general one (see exercise 9.2). 
>有趣的是，得到的结果因子确实对应于条件概率 $P(A,C\mid B)$，但是是在一个不同的网络中：如图 9.9b 所示的网络，在该网络中除了 $B$ 之外的所有 CPD 都是相同的。事实上，这种现象是一个普遍的现象（参见练习 9.2）。

### 9.3.2 Dealing with Evidence 
It remains only to consider how we would introduce evidence. 
>剩下要做的就是考虑如何引入证据

For example, assume we observe the value $i^{1}$ (the student is intelligent) and $h^{0}$ (the student is unhappy). Our goal is to compute $P(J\mid i^{1},h^{0})$ . First, we reduce this problem to computing the unnormalized distribution $P(J,i^{1},h^{0})$ . From this intermediate result, we can compute the conditional probability as in equation (9.1), by renormalizing by the probability of the evidence $P(i^{1},h^{0})$ . 
>例如，假设我们观察到值 $i^{1}$和 $h^{0}$，我们的目标是计算 $P(J\mid i^{1},h^{0})$
>首先，我们将这个问题简化为先计算未归一化的分布 $P(J,i^{1},h^{0})$，然后像方程（9.1）那样通过重新归一化证据的概率 $P(i^{1},h^{0})$ 来计算条件概率。

How do we compute $P(J,i^{1},h^{0})$ ? The key observation is proposition 4.7, which shows us how to view, as a Gibbs distribution, an unnormalized measure derived from introducing evidence into a Bayesian network. Thus, we can view this computation as summing out all of the entries in the reduced factor : $P[i^{1}h^{0}]$ whose scope is $\{C,D,G,L,S,J\}$ . This factor is no longer normalized, but it is still a valid factor. 
>那么如何计算 $P(J,i^{1},h^{0})$ 呢？
>关键的观察在于命题 4.7，命题 4.7 表明了分布 $P (\pmb W,\pmb e)$ 可以视为由一组因子定义的 Gibbs 分布，这些因子通过贝叶斯网络中的 CPD 在 $\pmb E = \pmb e$ 上简化得到

Based on this observation, we can now apply precisely the same sum-product variable elimination algorithm to the task of computing $P(\pmb{Y},\pmb{e})$ . We simply apply the algorithm to the set of factors in the network, reduced by $E=e$ , and eliminate the variables in $\mathcal{X}-Y-E$ . The returned factor $\phi^{*}(Y)$ is precisely $P(\pmb{Y},\pmb{e})$ . To obtain $P(Y\mid e)$ we simply renormalize $\phi^{*}(Y)$ by multiplying it by $\textstyle{\frac{1}{\alpha}}$ to obtain a legal distribution, where $\alpha$ is the sum over the entries in our unnormalized distribution, which represents the probability of the evidence. To summarize, the algorithm for computing conditional probabilities in a Bayesian or Markov network is shown in algorithm 9.2. 
>基于这一观察，我们现在可以应用相同的和-积变量消去算法来计算 $P(\pmb{Y},\pmb{e})$
>我们首先用 $\pmb E = \pmb e$ 将网络中的因子集合简化，然后对新的因子集合消除 $\mathcal{X}-\pmb Y-\pmb E$ 中的变量，返回的 $\phi^{*}(\pmb Y)$ 就是 $P(\pmb{Y},\pmb{e})$ 
>为了获得 $P(\pmb Y\mid \pmb e)$，我们只需通过将其乘以 $\textstyle{\frac{1}{\alpha}}$ 来重新归一化 $\phi^{*}(Y)$，其中 $\alpha$ 是我们未归一化分布中条目的总和，表示证据的概率
>总之，用于计算贝叶斯或马尔可夫网络中条件概率的算法如算法 9.2 所示。

![[PGM-Algorithm9.2.png]]

We demonstrate this process on the example of computing $P(J,i^{1},h^{0})$ . We use the same elimination ordering that we used in table 9.1. The results are shown in table 9.3; the step num- bers correspond to the steps in table 9.1. It is interesting to note the diferences between the two runs of the algorithm. First, we notice that steps (3) and (4) disappear in the computation with evidence, since $I$ and $H$ do not need to be eliminated. More interestingly, by not eliminating $I$ , we avoid the step that correlates $G$ and $S$ . In this execution, $G$ and $S$ never appear together in the same factor; they are both eliminated, and only their end results are combined. Intuitively, $G$ and $S$ are conditionally independent given $I$ ; hence, observing $I$ renders them independent, so that we do not have to consider their joint distribution explicitly. Finally, we notice that $\phi_{I}[I=i^{1}]\,=\,P(i^{1})$ is a factor over an empty scope, which is simply a number. It can be multiplied into any factor at any point in the computation. We chose arbitrarily to incorporate it into step $(2^{\prime})$ . Note that if our goal is to compute a conditional probability given the evidence, and not the probability of the evidence itself, we can avoid multiplying in this factor entirely, since its efect will disappear in the renormalization step at the end.

Box 9.A — Concept: The Network Polynomial. 
The network polynomial provides an interestpolynomial ing and useful alternative view of variable elimination. We begin with describing the concept for the case of a Gibbs distribution parameterized via a set of full table factors Φ. The polynomial fΦ
is deﬁned over the following set of variables: 

• For each factor $\phi_{c}\in\Phi$ with scope $X_{c},$ , we have a variable $\theta_{\pmb{x}_{c}}$ for every $\pmb{x}_{c}\in V a l(\pmb{X}_{c})$ . • For each variable $X_{i}$ and every value $x_{i}\in V a l(X_{i})$ , we have a binary-valued variable $\lambda_{x_{i}}$ 

In other words, the polynomial has one argument for each of the network parameters and for each possible assignment to a network variable. The polynomial $f_{\Phi}$ is now deﬁned as follows: 

$$
f_{\Phi}(\pmb\theta,\pmb\lambda)=\sum_{x_{1},...,x_{n}}\left(\prod_{\phi_{c}\in\Phi}\theta_{\pmb x_{c}}\cdot\prod_{i=1}^{n}\lambda_{x_{i}}\right).
$$ 

Evaluating the network polynomial is equivalent to the inference task. In particular, let $Y=y$ be an assignment to some subset of network variables; deﬁne an assignment $\lambda^{y}$ as follows: 

With this deﬁnition, we can now show (exercise 9.4a) that: 

$$
f_{\Phi}(\pmb\theta,\lambda^{y})=\tilde{P}_{\Phi}(Y=y\mid\pmb\theta).
$$ 

The derivatives of the network polynomial are also of signiﬁcant interest. We can show (exer- cise 9.4b) that 

$$
\frac{\partial f_{\Phi}(\pmb\theta,\pmb\lambda^{\pmb y})}{\partial\lambda_{x_{i}}}=\tilde{P}_{\Phi}(x_{i},\pmb y_{-i}\mid\pmb\theta),
$$ 

where $\pmb{y}_{-i}$ is the assignment in $_{_y}$ to all variables other than $X_{i}$ . We can also show that 

$$
{\frac{\partial f_{\Phi}(\pmb\theta,\lambda^{y})}{\partial\theta_{x_{c}}}}={\frac{{\tilde{P}}_{\Phi}(\pmb y,\pmb x_{c}\mid\pmb\theta)}{\theta_{x_{c}}}}~;
$$ 

sensitivity analysis 

this fact is proved in lemma 19.1. These derivatives can be used for various purposes, including retracting or modifying evidence in the network (exercise 9.4c), and sensitivity analysis — comput- ing the efect of changes in a network parameter on the answer to a particular probabilistic query (exercise 9.5). 

Of course, as deﬁned, the representation of the network polynomial is exponentially large in the number of variables in the network. However, we can use the algebraic operations performed in a run of variable elimination to deﬁne a network polynomial that has precisely the same complexity as the VE run. More interesting, we can also use the same structure to compute efciently all of the derivatives of the network polynomial, relative both to the $\lambda_{i}$ and the $\theta_{\pmb{x}_{c}}$ (see exercise 9.6). 

## 9.7 Summary and Discussion 
In this chapter, we described the basic algorithms for exact inference in graphical models. As we saw, probability queries essentially require that we sum out an exponentially large joint distribution. The fundamental idea that allows us to avoid the exponential blowup in this task is the use of dynamic programming, where we perform the summation of the joint distribution from the inside out rather than from the outside in, and cache the intermediate results, thereby avoiding repeated computation. 
> 本章描述了图模型中进行精确推理的基本算法
> 使用动态规划的思想，我们避免了指数级别的计算复杂度

We presented an algorithm based on this insight, called variable elimination. The algorithm works using two fundamental operations over factors — multiplying factors and summing out variables in factors. We analyzed the computational complexity of this algorithm using the structural properties of the graph, showing that the key computational metric was the induced width of the graph. 
> 变量消除算法对因子进行两种基本的运算：因子间相乘和求和消去因子中的变量

We also presented another algorithm, called conditioning, which performs some of the sum-mation operations from the outside in rather than from the inside out, and then uses variable elimination for the rest of the computation. Although the conditioning algorithm is never less expensive than variable elimination in terms of running time, it requires less storage space and hence provides a time-space trade-of for variable elimination. 

We showed that both variable elimination and conditioning can take advantage of local structure within the CPDs. Speciﬁcally, we presented methods for making use of CPDs with independence of causal inﬂuence, and of CPDs with context-speciﬁc independence. In both cases, techniques tend to fall into two categories: In one class of methods, we modify the network structure, adding auxiliary variables that reveal some of the structure inside the CPD and break up large factors. In the other, we modify the variable elimination algorithm directly to use structured factors rather than tables. 

Although exact inference is tractable for surprisingly many real-world graphical models, it is still limited by its worst-case exponential performance. There are many models that are simply too complex for exact inference. As one example, consider the $n\times n$ grid-structured pairwise Markov networks of box 4.A. It is not difcult to show that the minimal tree-width of this network is $n$ . Because these networks are often used to model pixels in an image, where $n\,=\,1,000$ is quite common, it is clear that exact inference is intractable for such networks. Another example is the family of networks that we obtain from the template model of example 6.11. Here, the moralized network, given the evidence, is a fully connected bipartite graph; if we have $n$ variables on one side and $m$ on the other, the minimal tree-width is $\operatorname*{min}(n,m)$ , which can be very large for many practical models. Although this example is obviously a toy domain, examples of similar structure arise often in practice. In later chapters, we will see many other examples where exact inference fails to scale up. Therefore, in chapter 11 and chapter 12 we discuss approximate inference methods that trade of the accuracy of the results for the ability to scale up to much larger models. 

One class of networks that poses great challenges to inference is the class of networks induced by template-based representations. These languages allow us to specify (or learn) very small, compact models, yet use them to construct arbitrarily large, and often densely connected, networks. Chapter 15 discusses some of the techniques that have been used to deal with dynamic Bayesian networks. 

Our focus in this chapter has been on inference in networks involving only discrete variables. The introduction of continuous variables into the network also adds a signiﬁcant challenge. Although the ideas that we described here are instrumental in constructing algorithms for this richer class of models, many additional ideas are required. We discuss the problems and the solutions in chapter 14. 

# 10 Exact Inference: Clique Trees 
In the previous chapter, we showed how we can exploit the structure of a graphical model to perform exact inference efectively. The fundamental insight in this process is that the factorization of the distribution allows us to perform local operations on the factors deﬁning the distribution, rather than simply generate the entire joint distribution. We implemented this insight in the context of the variable elimination algorithm, which sums out variables one at a time, multiplying the factors necessary for that operation. 
>在前一章中，我们展示了如何利用图形模型的结构来进行有效的精确推理，这一过程的基本见解在于，分布的分解允许我们对定义分布的因子进行局部操作，而不是生成整个联合分布。
>这一见解引导我们实现变量消去算法，该算法每次将和变量相关的所有因子相乘，然后进行求和消去该变量

In this chapter, we present an alternative implementation of the same insight. As in the case of variable elimination, the algorithm uses manipulation of factors as its basic computational step. However, the algorithm uses a more global data structure for scheduling these operations, with surprising computational beneﬁts. 
> 本章介绍该见解的另一种实现
> 变量消去算法使用因子作为基本的计算步骤，本章介绍的算法将使用更全局的数据结构来调度这些运算

Throughout this chapter, we will assume that we are dealing with a set of factors $\Phi$ over a set of variables $\mathcal{X}$ , where each factor $\phi_{i}$ has a scope $X_{i}$ . This set of factors deﬁnes a (usually) unnormalized measure 
> 本章中，我们将假设我们处理一组变量 $\mathcal X$ 上的一组因子 $\Phi$，其中每个因子 $\phi_i$ 的作用域为 $\pmb X_i$
> 这组因子定义了一个非规范化的度量如下：

$$
\tilde{P}_{\Phi}(\mathcal{X})=\prod_{\phi_{i}\in\Phi}\phi_{i}(X_{i}).\tag{10.1}
$$ 
For a Bayesian network without evidence, the factors are simply the CPDs, and the measure $\tilde{P}_{\Phi}$ is a normalized distrib ion. rk $\mathcal{B}$ with evidence $E=e$ , the factors are the CPDs restricted to e , and $\tilde{P}_{\Phi}(\mathcal{X})\overset{\cdot}{=}P_{\mathcal{B}}(\mathcal{X},e)$ . Fo Gibbs distribution (with or without B evidence), the factors are the (restricted) potentials, and $\tilde{P}_{\Phi}$ is the unnormalized Gibbs measure. 
> 对于没有 evidence 的贝叶斯网络，这些因子就是 CPDs，度量 $\tilde P_{\Phi}$ 就是规范化的联合分布
> 对于带有 evidence $\pmb E = \pmb e$ 的贝叶斯网络 $\mathcal B$，这些因子就是限制到 $\pmb e$ 的 CPDs，同时 $\tilde P_{\Phi}(\mathcal X) = P_{\mathcal B}(\mathcal X, \pmb e)$
> 对于 Gibbs 分布 (有或者没有 evidence)，这些因子就是 (受限制的) 势能函数，$\tilde P_{\Phi}$ 就是未规范化的 Gibbs 度量

It is important to note that all of the operations that one can perform on a normalized distri- bution can also be performed on an unnormalized measure. In particular, we can marginalize $\tilde{P}_{\Phi}$ on a subset of the variables by summing out the others. We can also consider a conditional measure, ${\tilde{P}}_{\Phi}(X\mid Y)={\tilde{P}}_{\Phi}(X,{\dot{Y}})/{\tilde{P}}_{\Phi}(Y)$ (which, in fact, is the same as $P_{\Phi}(X\mid Y))$ ). 
> 注意所有可以在规范化的分布上做的操作也可以在未规范化的度量上进行
> 例如，我们可以通过求和消去变量将 $\tilde P_\Phi$ 在某个变量子集上进行边际化
> 我们也可以定义条件度量 $\tilde P_\Phi (\pmb X \mid \pmb Y) = \tilde P_\Phi (\pmb X, \pmb Y) / \tilde P_\Phi (\pmb Y)$ (实际上它就等于 $P_\Phi (\pmb X \mid \pmb Y)$)

## 10.1 Variable Elimination and Clique Trees 
Recall that the basic operation of the variable elimination algorithm is the manipulation of factors. Each step in the computation creates a factor $\psi_{i}$ by multiplying existing factors. A variable is then eliminated in $\psi_{i}$ to generate a new factor $\tau_{i}$ , which is then used to create another factor.
>变量消去算法的基本操作是对因子进行操作，计算中的每一步通过乘以现有因子生成一个新的因子 $\psi_{i}$，然后在 $\psi_{i}$ 中消去一个变量以生成一个新的因子 $\tau_{i}$，这个新的因子随后被用来创建另一个因子

 In this section, we present another view of this computation. We consider a factor $\psi_{i}$ to be a computational data structure, which takes “messages” $\tau_{j}$ generated by other factors $\psi_{j}$ , and generates a message $\tau_{i}$ that is used by another factor $\psi_{l}$ . 
>在本节中，我们提出了这种计算的另一种视角，我们将一个因子 $\psi_{i}$ 视为一种计算数据结构，它接收由其他因子 $\psi_{j}$ 生成的消息 $\tau_{j}$，并生成一个消息 $\tau_{i}$，这个消息被另一个因子 $\psi_{l}$ 使用。

### 10.1.1 Cluster Graphs 
We begin by deﬁning a cluster graph — a data structure that provides a graphical ﬂowchart of the factor-manipulation process. Each node in the cluster graph is a cluster , which is associated with a subset of variables; the graph contains undirected edges that connect clusters whose scopes have some non-empty intersection. We note that this deﬁnition is more general than the data structures we use in this chapter, but this generality will be important in the next chapter, where we signiﬁcantly extend the algorithms of this chapter. 
> 首先定义簇图，簇图是一类数据结构，提供了因子操纵过程的流程表示
> 簇图中的每个节点都是一个簇，簇和一组变量关联
> 如果簇之间的作用域存在非空的交集，则簇由一条无向边相连

**Definition 10.1** cluster graph
A cluster graph $\mathcal{U}$ for a ctors $\Phi$ over $\mathcal{X}$ is an undirected graph, each of whose no $i$ associated with a subset $C_{i}\subseteq\mathcal X$ cluster gra ust be family-preserving — each factor $\phi\in\Phi$ must be associate ith a cluster $C_{i:}$ , denoted $\alpha(\phi)$ , such that Scope $S c o p e[\phi]\subseteq C_{i}$ . Each edge between a pair of clusters $C_{i}$ and $C_{j}$ is associated with $a$ sepset $S_{i,j}\subseteq C_{i}\cap C_{j}$ . 
> 定义：
> $\mathcal X$ 上的一组因子 $\Phi$ 定义的簇图 $\mathcal U$ 是一个无向图，其每个节点 $i$ 都和一个变量子集 $\pmb C_i \subseteq \mathcal X$ 相关联
> 簇图必须是族保持的——每个因子 $\phi \in \Phi$ 必须和一个簇 $\pmb C_i$ 相关联，满足 $Scope[\phi]\subseteq \pmb C_i$ (因子的作用域是簇关联的变量集合的子集)，该簇记作 $\alpha (\phi)$
> 一对簇 $\pmb C_i, \pmb C_j$ 之间的每条边都和一个分离集 $\pmb S_{i, j} \subseteq \pmb C_i \cap \pmb C_j$ 关联

An execution of variable elimination deﬁnes a cluster graph: We have a cluster for each factor $\psi_{i}$ used in the computation, which is associated with the set of variables $C_{i}=S c o p e[\psi_{i}]$ . We draw an edge between two clusters $C_{i}$ and $C_{j}$ if the message $\tau_{i}$ , produced by eliminating a variable in $\psi_{i}$ , is used in the computation of $\tau_{j}$ . 
> 变量消除算法的一次执行定义了一个簇图：计算过程中使用的每个因子 $\psi_i$ 都对应一个簇，该簇和变量集合 $\pmb C_i = Scope[\psi_i]$ 关联
> 如果在消除 $\psi_i$ 中的一个变量中产生的信息 $\tau_i$ 被用在了 $\tau_j$ 中的计算，我们就在簇 $\pmb C_i, \pmb C_j$ 之间画一条边

Example 10.1 
Consider the elimination process of table 9.1. In this case, we have seven factors $\psi_{1},.\,.\,.\,,\psi_{7}$ , whose scope is shown in the table. The message $\tau_{1}(D)$ , generated from $\psi_{1}(C,D)$ , participates in the computation of $\psi_{2}$ . Thus, we would have an edge from $C_{1}$ to $C_{2}$ . Similarly, the message $\tau_{3}(G,S)$ is generated from $\psi_{3}$ and used in the computation of $\psi_{5}$ . Hence, we introduce an edge between $C_{3}$ and $C_{5}$ . The entire graph is shown in ﬁgure 10.1. The edges in the graph are annotated with directions, indicating the ﬂow of messages between clusters in the execution of the variable elimination algorithm. Each of the factors in the initial set of factors $\Phi$ is also associated with $^a$ clust $C_{i}$ . For example, the cluster $\phi_{D}(D,C)$ (corresponding to the CPD $P(D\mid C),$ ) is associated with $C_{1}$ , and the cluster $\phi_{H}(H,G,J)$ (corresponding to the CPD $P(H\mid G,J))$ is associated with $C_{4}$ . 

### 10.1.2 Clique Trees 
The cluster graph associated with an execution of variable elimination is guaranteed to have certain properties that turn out to be very important. 
> 由变量消除算法的执行所生成的簇图会存在一些重要性质

First, recall that the variable elimination algorithm uses each intermediate factor $\tau_{i}$ at most once: when $\phi_{i}$ is used in Sum-Product-Eliminate-Var to create $\psi_{j}$ , it is removed from the set of factors $\Phi$ , and thus cannot be used again. Hence, the cluster graph induced by an execution of variable elimination is necessarily a tree. 
> 变量消除算法仅会使用每个中间因子 $\tau_i$ 最多一次，因为对于任意因子 $\phi_i$，当它在 Sum-Produt-Eliminate-Var 中被用于创建 $\psi_j$ 时，它首先会从 $\Phi$ 中被移除，因此就不会再被使用一次
> 因此，由变量消除算法导出的簇图是一颗树 
> (树即没有环的单连通无向图，即每个节点到其他节点的路径是唯一的，本例中，每个 $\tau_i$ 显然只能有一个父节点)

We note that although a cluster graph is deﬁned to be an undirected graph, an execution of variable elimination does deﬁne a direction for the edges, as induced by the ﬂow of messages between the clusters. The directed graph induced by the messages is a directed tree, with all the messages ﬂowing toward a single cluster where the ﬁnal result is computed. This cluster is called the root of the directed tree. Using standard conventions in computer science, we assume that the root of the tree is “up,” so that messages sent toward the root are sent upward. If $C_{i}$ is on the path from $C_{j}$ to the root we say that $C_{i}$ is upstream from $C_{j}$ , and $C_{j}$ is downstream from $C_{i}$ . 
> 虽然簇图被定义为无向图，但变量消除算法的执行实际定义了边的方向为簇之间的信息流动方向
> 添加信息流动方向后，我们得到的就是有向树，其中所有的信息都流向最后计算得到的簇，此即该有向树的根
> 我们称往根方向为上游，相反方向为下游

We note that, for reasons that will become clear later on, the directions of the edges and the root are not part of the deﬁnition of a cluster graph. 

The cluster tree deﬁned by variable elimination satisﬁes an important structural constraint: 

**Deﬁnition 10.2** running intersection property 
Let $\mathcal{T}$ be a cluster ee over a set of factors $\Phi$ . We denote by $\nu_{\tau}$ the vertices of $\mathcal{T}$ and $\mathcal{E}_{\mathcal{T}}$ its edge that $\mathcal{T}$ has the running intersection property $i f,$ whenever there is a v iable X s that $X\in C_{i}$ and $X\in C_{j}$ , then X is also in every cluster in the (unique) path in $\mathcal{T}$ between $C_{i}$ and $C_{j}$ . 
> 定义：
> $\mathcal T$ 为一组因子 $\Phi$ 上的簇树，记 $\mathcal V_{\mathcal T}$ 为 $\mathcal T$ 的节点，$\mathcal E_{\mathcal T}$ 为边
> 对于任意变量 $X$，只要 $X$ 满足 $X \in \pmb C_i$ 且 $X\in \pmb C_j$ ，$X$ 就会存在于 $\mathcal T$ 中 $\pmb C_i$ 到 $\pmb C_j$ 中的 (唯一) 路径上的每个簇中，我们就称 $\mathcal T$ 具有运行相交性质，

Note that the running intersection property implies that $S_{i,j}=C_{i}\cap C_{j}$ . 
> 运行相交性质暗示了 $\pmb S_{i, j} = \pmb C_i \cap \pmb C_j$

Example 10.2 
We can easily check that the running intersection property holds for the cluster tree of ﬁgure 10.1. For example, $G$ is present in $C_{2}$ and in $C_{4}$ , so it is also present in the cliques on the path between them: $C_{3}$ and $C_{5}$ . 

Intuitively, the running intersection property must hold for cluster trees induced by variable elimination because a variable appears in every factor from the moment it is introduced (by multiplying in a factor that mentions it) until it is summed out. We now prove that this property holds in general. 
> 直观上，运行相交性质在由变量消除算法导出的簇树中一定成立，因为一个变量从被引入 (通过将所有提及该变量的因子相乘) 直到它被求和消除，它都会出现在所有的路径上的因子中

**Theorem 10.1** 
Let $\mathcal{T}$ be a cluster tree induced by a variable elimination algorithm over some set of factors $\Phi$ . Then $\mathcal{T}$ T satisﬁes the running intersection property. 
> 定理：
> $\mathcal T$ 为变量消除算法在一组因子 $\Phi$ 上导出的簇树，则 $\mathcal T$ 满足运行相交性质

Proof Let $C$ and $C^{\prime}$ be two clusters that contain $X$ . Let $C_{X}$ be the cluster where $X$ is eliminated. (If $X$ is a query variable, we assume that it is eliminated in the last cluster.) We will prove that $X$ must be present in every cluster on the path between $C$ and $C_{X}$ , and analogously for $C^{\prime}$ , thereby proving the result.
> 证明：
> 令 $\pmb C, \pmb C'$ 为两个包含 $X$ 的簇，$\pmb C_X$ 为 $X$ 被消除的簇 (如果 $X$ 为 query 变量，则假定它在最后一个簇被消除)
> 我们需要证明 $X$ 必须出现在 $\pmb C$ 和 $\pmb C_X$ 之间的路径上的所有簇中，以及证明 $X$ 必须出现在 $\pmb C'$ 和 $\pmb C_X$ 之间的路径上的所有簇中，则就证明了 $\pmb C, \pmb C'$ 之间的路径中都包含 $X$，因此就证明了运行相交性质

First, we observe that the computation at $C_{X}$ must take place later in the algorithm’s execution than the computation at $C$ : When $X$ is eliminated in $C_{X}$ , all of the factors involving $X$ are multiplied into $C_{X}$ ; the result of the summation does not have $X$ in its domain. Hence, after this elimination, $\Phi$ no longer has any factors containing $X$ , so no factor generated afterward will contain $X$ in its domain. 
> 首先观察到 $\pmb C_X$ 的计算在算法中的执行顺序一定在 $\pmb C$ 的计算之后，因为 $X$ 被消除时，所有和 $X$ 有关的因子都会乘入 $\pmb C_X$，求和之后的结构就是 $X$ 不会再存在于定义域中，即 $\Phi$ 不再有任何包含 $X$ 的因子

By assumption, $X$ is in the domain of the factor in $C$ . We also know that $X$ is not eliminated in $C$ . Therefore, the message computed in $C$ must have $X$ in its domain. By deﬁnition, the recipient of X’s message, which is C’s upstream neighbor in the tree, multiplies in the message the from $C$ . Hence, it will also have $X$ in its scope. The same argument applies to show that all cliques upstream from $C$ will have $X$ in their scope, until $X$ is eliminated, which happens only in $C_{X}$ . Thus, $X$ must appear in all cliques between $C$ and $C_{X}$ , as required. 
> $X$ 处在 $\pmb C$ 的作用域中，因此在 $\pmb C$ 中，$X$ 还未被消除，因此由 $\pmb C$ 计算得到的消息的作用域中一定也有 $X$，而 $\pmb C$ 在树上的上游父节点，也就是消息的接收者会乘上该消息，因此其作用域中也会包含 $X$
> 这样的推理适用于 $\pmb C$ 的所有上游祖先节点，因此它们的作用域都会包括 $X$，直到 $\pmb C_X$ 将 $X$ 消除，因此 $X$ 会出现在 $\pmb C, \pmb C_X$ 之间的所有簇中

A very similar proof can be used to show the following result: 

**Proposition 10.1** 
$\mathcal{T}$ be a cluster tree induced by $a$ variable elimination algorithm over some set of factors $\Phi$ . Let $C_{i}$ and $C_{j}$ be two neighboring clusters, such that $C_{i}$ passes the message $\tau_{i}$ to $C_{j}$ . Then the scope of the message $\tau_{i}$ is precisely $C_{i}\cap C_{j}$ . 
> 命题：
> $\mathcal T$ 为变量消除算法在一组因子 $\Phi$ 上导出的簇树，$\pmb C_i, \pmb C_j$ 为两个相邻簇，$\pmb C_i$ 向 $\pmb C_j$ 传递消息 $\tau_i$，则消息 $\tau_i$ 的作用域就是 $\pmb C_i \cap \pmb C_j$

The proof is left as an exercise (exercise 10.1). 

It turns out that a cluster tree that satisﬁes the running intersection property is an extremely useful data structure for exact inference in graphical models. We therefore deﬁne: 

**Deﬁnition 10.3** clique tree
Let $\Phi$ be a set of factors over $\mathcal{X}$ . A cluster tree ver $\Phi$ that satisﬁ the running intersection property is called a clique tree (sometimes also called a junction tree or a join tree ). In the case of a clique tree, the clusters are also called cliques . 
> 定义：
> $\Phi$ 为 $\mathcal X$ 上的一组因子，$\Phi$ 上的满足运行相交性质的簇树称为团树，在团树中，簇也被称为团

Note that we have already deﬁned one notion of a clique tree in deﬁnition 4.17. This double deﬁnition is not an overload of terminology, because the two deﬁnitions are actually equivalent: It follows from the results of this chapter that $\mathcal{T}$ is a clique tree for $\Phi$ (in sense of deﬁnition 10.3) if and only if it is a clique tree for a chordal graph containing ${\mathcal{H}}_{\Phi}$ (in the sense of deﬁnition 4.17), and these properties are true if and only if the clique-tree data structure admits variable elimination by passing messages over the tree. 
> 该定义和 definition 4.17 中对于团树的定义是等价的：$\mathcal T$ 是 $\Phi$ 的团树当且仅当 $\mathcal T$ 是包含 $\mathcal H_{\Phi}$ 的弦图的团树，并且这些性质成立当且仅当团树数据结构通过在树上传递消息来支持变量消去

We ﬁrst show that the running intersection property implies the independence statement, which is the heart of our ﬁrst deﬁnition of clique trees. Let $\mathcal{T}$ be a cluster tree over $\Phi$ , and let ${\mathcal{H}}_{\Phi}$ be the undirected graph associated with this set of factors. For any sepset $\boldsymbol{S}_{i,j}$ , let $W_{<(i,j)}$ be the set of all variables in the scope of clusters in the $C_{i}$ side of the tree, and $W_{<(j,i)}$ be the set of all variables in the scope of clusters in the $C_{j}$ side of the tree. 
> 运行相交性质意味着独立性陈述
> $\mathcal T$ 为 $\Phi$ 上的团树，$\mathcal H_\Phi$ 为和因子集合 $\Phi$ 相关的无向图，对于任意分离集 $\pmb S_{i, j}$，令 $\pmb W_{<(i, j)}$ 为树的 $\pmb C_i$ 边的所有簇的作用域包含的变量，$\pmb W_{<(j, i)}$ 为数的 $\pmb C_j$ 边的所有簇的作用域包含的变量

**Theorem 10.2** 
$\mathcal{T}$ satisﬁes the running intersection property if and only if, for every sepset $\boldsymbol{S}_{i,j}$ , we have that $W_{<(i,j)}$ and $W_{<(j,i)}$ are separated in ${\mathcal{H}}_{\Phi}$ given $\boldsymbol{S}_{i,j}$ . 
> 定理：
> $\mathcal T$ 满足运行相交性质当且仅当 $\mathcal T$ 中的任意分离集 $\pmb S_{i, j}$ 在 $\mathcal H_\Phi$ 中分离 $\pmb W_{<(i, j)}$ 和 $\pmb W_{<(j, i)}$

The proof is left as an exercise (exercise 10.2). 

To conclude the proof of the equivalence of the two deﬁnitions, it remains only to show that the running intersection operty for a ee $\mathcal{T}$ implies that each node in $\mathcal{T}$ corresponds to a ique in a chordal graph H containing H , and that each maximal clique in H is represented in $\mathcal{T}$ . This result follows from our ability to use any clique tree satisfying the running intersection property to perform inference, as shown in this chapter. 
> 此时，要证明两个定义的等价性，我们需要证明树 $\mathcal T$ 的运行相交性质意味着 $\mathcal T$ 中的每个节点都对应于包含 $\mathcal H_\Phi$ 的弦图 $\mathcal H'$ 中的一个团，并且 $\mathcal H'$ 中的每个极大团都对应于 $\mathcal T$ 中的一个节点

## 10.2 Message Passing: Sum Product 
In the previous section, we started out with an execution of the variable elimination algorithm, and showed that it induces a clique tree. In this section, we go in the opposite direction. We assume that we are given a clique tree as a starting point, and we will show how this data structure can be used to perform variable elimination. As we will see, the clique tree is a very useful and versatile data structure. For one, the same clique tree can be used as the basis for many diferent executions of variable elimination. More importantly, the clique tree provides a data structure for caching computations, allowing multiple executions of variable elimination to be performed much more efciently than simply performing each one separately. 
> 上一节中，我们知道了变量消除算法的执行可以导出满足运行相交性质的簇树，也就是团树
> 本节将展示给定一颗团树，我们可以使用该数据结构执行变量消除，一颗团树可以用于多次变量消除的执行，并且团树可以缓存计算结构，使得在同一颗团树上的多次变量消除的执行更高效

Consider some set of factors $\Phi$ over $\mathcal{X}$ , and assume that we are given a clique tree $\mathcal{T}$ over $\Phi$ , as deﬁned in deﬁnition 4.17. In particular, T is guaranteed to satisfy the family preservation and running intersection properties. As we now show, we can use the clique tree in several diferent ways to perform exact inference in graphical models. 
> 考虑 $\mathcal X$ 上的一组因子 $\Phi$，假设给定 $\Phi$ 上的如 definition 4.17 的一颗团树 $\mathcal T$，这颗团树一定会满足族保持和运行相交性质

### 10.2.1 Variable Elimination in a Clique Tree 
One way of using a clique tree is simply as guidance for the operations of variable elimination. The factors $\psi$ are computed in the cliques, and messages are sent along the edges. Each clique takes the incoming messages (factors), multiplies them, sums out one or more variables, and sends an outgoing message to another clique. As we will see, the clique-tree data structure dictates the operations that are performed on factors in the clique tree and a partial order over these operations. In particular, if clique $C^{\prime}$ requires a message from $C$ , then $C^{\prime}$ must wait with its computation until $C$ performs its computation and sends the appropriate message to $C^{\prime}$ . We begin with an example and then describe the general algorithm. 
> 使用团树的一种方式就是引导变量消除的操作
> 因子 $\psi$ 在团中计算，然后通过边传递消息，每个团接受输入的消息/因子，乘上它，然后求和消去一个或者多个变量，然后传递消息给另一个团
> 正如我们将看到的，团树数据结构决定了在团树中的因子上执行的操作以及这些操作上的偏序关系，特别地，如果团 $\pmb C'$ 需要从团 $\pmb C$ 获取消息，则 $\pmb C'$ 必须在 $\pmb C$ 完成其计算并将适当的消息发送给 $\pmb C'$ 之后才能进行它的计算
> 我们将先举一个例子，然后描述一般算法

#### 10.2.1.1 An Example 
Figure 10.2 shows one possible clique tree $\mathcal{T}$ for the Student ne rk. N that it is diferent from the clique tree of ﬁgure 10.1, in that nonmaximal cliques ( $(C_{6}$ and $C_{7}$ ) are absent. Nevertheless, it is straightforward to verify that $\mathcal{T}$ satisﬁes both the family preservation and the running intersection property. The ﬁgure also speciﬁes the assignment $\alpha$ of the initial factors (CPDs) to cliques. Note that, in some cases (for example, the CPD $P(I))$ ), we have more than one possible clique into which the factor can legally be assigned; as we will see, the algorithm applies for any legal choice. 
>图10.2展示了一个可能的 Student 网络的团树 $\mathcal{T}$。请注意，它与图10.1中的团树不同，因为在该团树中非极大团 $(C_{6}$ 和 $C_{7})$ 被省略了。尽管如此，很容易验证 $\mathcal{T}$ 满足族保持和运行相交性质
>图中还指定了将因子 (CPDs)  到团的初始分配 $\alpha$ ，请注意，在某些情况下（例如，CPD $P(I)$），我们有多个合法的团可以将因子分配进去；正如我们将看到的，算法适用于任何合法的选择。

Our ﬁrst step is to generate a set of initial potentials associated with the diferent cliques. The initial potential $\psi_{i}(C_{i})$ is computed by multiplying the initial factors assigned to the clique $C_{i}$ . For example, $\psi_{5}(J,L,G,S)=\phi_{L}(L,G)\cdot\phi_{J}(J,L,S)$ . 
> 第一步是为不同的团生成和它相关的初始势能函数
> 初始势能函数 $\psi_i(\pmb C_i)$ 的计算就是将所有分配给团 $\pmb C_i$ 的因子相乘，例如 $\psi_5 (J, L, G, S) = \phi_L (L, G)\cdot \phi_J (J, L, S)$

Now, assume that our task is to compute the probability $P(J)$ . We want to do the variable elimination process so that $J$ is not eliminated. Thus, we select as our root clique some clique that contains $J$ , for example, $C_{5}$ . We then execute the following steps: 
> 假设任务是计算边际概率 $P (J)$，因此需要执行变量消除算法，最后留下 $J$，故我们将团树的根团选定为某个包含 $J$ 的团，例如 $\pmb C_5$，然后执行：

1. In $C_{1}$ : We eliminate $C$ for $\textstyle\sum_{C}\psi_{1}(C,D)$ . The resulting factor has scope $D$ . We send it as a message $\delta_{1\rightarrow2}(D)$ to $C_{2}$ . →
 2. In $C_{2}$ : W ne $\beta_{2}(G,I,D)=\,\delta_{1\rightarrow2}(D)\cdot\psi_{2}(G,I,D)$ . We the liminate $D$ to get a factor over $G,I$ . The resulting factor is $\delta_{2\to3}(G,I)$ , which is sent to $C_{3}$ .
 3. In $C_{3}$ : We deﬁne $\beta_{3}(G,S,I)=\delta_{2\rightarrow3}(G,I)\cdot\psi_{3}(G,S,I)$ and eliminate $I$ to get a factor over $G,S_{i}$ , which is $\delta_{3\rightarrow5}(G,S)$ .
 4. In $C_{4}$ min $H$ by performing $\textstyle\sum_{H}\psi_{4}(H,G,J)$ and send out the resulting factor as $\delta_{4\to5}(G,J)$ to $C_{5}$ . →
 5. In $C_{5}$ : We deﬁne $\beta_{5}(G,J,S,L)=\delta_{3\rightarrow5}(G,S)\cdot\delta_{4\rightarrow5}(G,J)\cdot\psi_{5}(G,J,S,L).$ 

> 在 $\pmb C_1$ 中，执行 $\sum_C \psi_1 (C, D)$ 消去 $C$，得到的因子作用域为 $D$，将它作为消息 $\delta_{1\rightarrow 2}(D)$ 传递给 $\pmb C_2$
> 在 $\pmb C_2$ 中，先计算 $\beta_2 (G, I, D) = \delta_{1\rightarrow 2}(D)\cdot \psi_2 (G, I, D)$，然后求和消去 $D$，将消息 $\delta_{2 \rightarrow 3}(G, I)$ 传递给 $\pmb C_3$
> 在 $\pmb C_3$ 中，先计算 $\beta_3 (G, S, I) = \delta_{2\rightarrow 3} (G, I) \cdot \psi_3 (G, S, I)$，消去 $I$，将消息 $\delta_{3 \rightarrow 5}(G, S)$ 传递给 $\pmb C_5$
> 在 $\pmb C_4$ 中，指定 $\sum_H \psi_4 (H, G J)$，将消息 $\delta_{4 \rightarrow 5}(G, J)$ 传递给 $\pmb C_5$
> 在 $\pmb C_5$ 中，计算 $\beta_5 (G, J, S, L) = \delta_{3\rightarrow 5}(G, S) \cdot \delta_{4 \rightarrow 5}(G, J)\cdot \psi (G, J, S, L)$

The factor $\beta_{5}$ is a factor over $G,J,S,L$ that encodes the joint distribution $P(G,J,L,S)$ : all the CPDs have been multiplied in, and all the other variables have been eliminated. If we now want to obtain $P(J)$ , we simply sum out $G,\,L,$ , and $S$ . 
> 因子 $\beta_5$ 定义了边际分布 $P (G, J, L, S)$ (网络中所有的 CPDs 都被乘入，然后通过加法消去了其余的变量，故得到的就是 $G, J, L, S$ 上的边际分布)
> 此时通过求和消去 $G, L, S$ 就得到 $P (J)$，

We note that the operations in the elimination process could also have been done in another order. The only constraint is that a clique get all of its incoming messages from its downstream neighbors before it sends its outgoing message toward its upstream neighbor. We say that a clique is ready when it has received all of its incoming messages. 
> 上述消除算法的过程也可以按另一种顺序执行，唯一的约束是一个团在向它的上游邻居发送消息之前必须从它的所有下游邻居收到消息
> 一个团收到所有的消息后，我们称它是准备好的

Thus, for example, $C_{4}$ is ready at the very start of the algorithm, and the computation associated with it can be performed at any point in the execution. However, $C_{2}$ is ready only after it receives its message from $C_{1}$ . Thus, $C_{1},C_{4},C_{2},C_{3},C_{5}$ is a legal execution ordering for a tree rooted at $C_{5}$ , whereas $C_{2},C_{1},C_{4},C_{3},C_{5}$ is not. Overall, the set of messages transmitted throughout the execution of the algorithm is shown in ﬁgure $10.3\mathrm{a}$ . 
> 例如，$\pmb C_4$ 在算法开始就是准备好的，$\pmb C_2$ 则需要再接收到 $\pmb C_1$ 的消息才准备好

As we mentioned, the choice of root clique is not fully determined. To derive $P(J)$ , we could have chosen $C_{4}$ as the root. Let us see how the algorithm would have changed in that case: 
> 根团的选择也不是唯一的，我们可以选择 $\pmb C_4$ 为根团，则算法流程对应如下：

1. In $C_{1}$ : The computation and message are unchanged.
2. In $C_{2}$ : The computation and message are unchanged.
3. In $C_{3}$ : The computation and message are unchanged.
4. In $C_{5}$ : We deﬁne $\beta_{5}(G,J,S,L)=\delta_{3\rightarrow5}(G,S)\cdot\psi_{5}(G,J,S,L)$ and eliminate $S$ and $L$ . We send out the resulting factor as $\delta_{5\to4}(G,J)$ to $C_{4}$ .
5. In $C_{4}$ : We deﬁne β $\beta_{4}(H,G,J)=\delta_{5\rightarrow4}(G,S)\cdot\psi_{4}(H,G,J).$ 

We can now extract $P(J)$ by eliminating $H$ and $G$ from $\beta_{4}(H,G,J)$ . 

In a similar way, we can apply exactly the same process to computing the distribution over any other variable. For example, if we want to compute the probability $P(G)$ , we could choose any of the cliques where it appears. If we use $C_{3}$ , for example, the computation in $C_{1}$ and $C_{2}$ is identical. The computation in $C_{4}$ is the same as in the ﬁrst of our two executions: a message is computed and s nt to $C_{5}$ . In $C_{5}$ , we compute $\beta_{5}(G,J,S,L)=\delta_{4\rightarrow5}(G,J)\cdot\psi_{5}(G,J,S,L)$ , and we eliminate J and L to produce a message $\delta_{5\to3}(G,S)$ , which can then be sent to $C_{3}$ and used in the operation: 

$$
\beta_{3}(G,S,I)=\delta_{2\rightarrow3}(G,I)\cdot\delta_{5\rightarrow3}(G,S)\cdot\psi_{3}(G,S,I).
$$ 

Overall, the set of messages transmitted throughout this execution of the algorithm is shown in ﬁgure 10.3b. 
> 计算其他变量的边际分布也是类似的，我们选择一个包含该变量的团作为根，然后定义一个合法的顺序，然后按照顺序逐个求和消去变量 -> 传递消息，最后再对得到的结果因子求和消去多余的变量即可

#### 10.2.1.2 Clique-Tree Message Passing 
We can now specify a general variable elimination algorithm that can be implemented via message passing in a clique tree. 
> 我们在本节说明通过团树中的消息传递实现的通用变量消除算法

Let $\mathcal{T}$ be a clique tree with the cliques $C_{1},\ldots,C_{k}$ . We begin by multiplying the factors assigned to each clique, resulting in our initial potentials. We then use the clique-tree data structure to pass messages between neighboring cliques, sending all messages toward the root clique. We describe the algorithm in abstract terms; box 10.A provides some important tips for efcient implementation. 
> $\mathcal T$ 是团 $\pmb C_1, \dots, \pmb C_k$ 构成的团树，我们首先为每个团将分配给它的因子相乘，得到它们的初始势能函数，然后根据团树结构在邻居团之间传递消息，最终将所有消息传递到根团

Recall that each factor $\phi\in\Phi$ is assigned to some clique $\alpha(\phi)$ . We deﬁne the initial potential of $C_{j}$ to be: 
> 根据团树的定义，每个因子 $\phi \in \Phi$ 都会分配给/关联某个团，记作 $\alpha (\phi)$
> 定义团 $\pmb C_j$ 的初始势能为所有关联的因子的乘积

$$
\psi_{j}(\pmb C_{j})=\prod_{\phi\;\;:\;\alpha(\phi)=j}\phi.
$$ 
Because each factor is assigned to exactly one clique, we have that 
> 因为每个因子仅和一个团关联/仅分配给一个团，故所有因子的乘积就是所有团的初始势能的乘积

$$
\prod_{\phi}\phi=\prod_{j}\psi_{j}.
$$

Let $C_{r}$ be the selected root clique. We now perform sum-product variable elimination over the cliques, starting from the leaves of the clique tree and moving inward. More precisely, for each clique $C_{i}$ , we deﬁne $\mathrm{Nb}_{i}$ to be the set of indexes of cliques that are neighbors of $C_{i}$ . Let $p_{r}(i)$ be the upstream neighbor of $i$ (the one on the path to the root clique $r$ ). Each clique $C_{i}$ , except for the root, performs a message passing computation and sends a message to its upstream neighbor $C_{p_{r}(i)}$ . 
> 令 $\pmb C_r$ 为选择的根团，我们在团上执行和-积变量消除，从团树的叶子开始，逐渐向根移动
> 具体地说，对于每个团 $\pmb C_i$，定义 $\text{Nb}_i$ 为其临近团的下标，定义 $p_r (i)$ 为团 $i$ 的上游邻居 (处在前往根团 $r$ 的路径上的团)，则除了根团的所有团执行消息传递计算，然后将消息传递给 $\pmb C_{p_r (i)}$

The message from $C_{i}$ to another clique $C_{j}$ is computed using the following sum-product message passing computation: 

$$
\delta_{i\rightarrow j}=\sum_{\pmb C_{i}-\pmb S_{i,j}}\psi_{i}\cdot\prod_{k\in(\mathrm{Nb}_{i}-\{j\})}\delta_{k\rightarrow i}.\tag{10.2}
$$

In words, the clique $C_{i}$ multiplies all incoming messages from its other neighbors with its initial clique potential, resulting in a factor $\psi$ whose scope is the clique. It then sums out all variables except those in the sepset between $C_{i}$ and $C_{j}$ , and sends the resulting factor as a message to $C_{j}$ . 
> 从 $\pmb C_i$ 到 $\pmb C_j$ 传递的消息的计算公式如上，其含义是先将团势能乘上所有传入的消息，然后求和消去所有 $\pmb C_i - \pmb S_{i, j}$ 中的变量，就得到了需要传递的消息 (消息就是一个仅和 $\pmb S_{i, j}$ 相关的因子)

This message passing process proceeds up the tree, culminating at the root clique. When the root clique has received all messages, it multiplies them with its own initial potential. The result is a factor called the beliefs , denoted $\beta_{r}(C_{r})$ . It represents, as we show, 

$$
\tilde{P}_{\Phi}(C_{r})=\sum_{\mathcal X-\pmb C_{r}}\prod_{\phi}\phi.
$$

> 该消息传递的过程不断沿着树向上，最后都累积到根团
> 根团将所有传入的消息乘上它的初始势能，得到的因子称为信念，记作 $\beta_r (\pmb C_r)$，它实际上表示将所有因子相乘然后求和消去 $\mathcal X - \pmb C_r$ 中所有变量得到的因子

The complete algorithm is shown in algorithm 10.1. 

![[PGM-Algorithm10.1.png]]

Example 10.3 
Consider the abstract clique tree of ﬁgure 10.4, and assume that we have selected $C_{6}$ as our root clique. The numbering of the cliques denotes one possible ordering of the operations, with $C_{1}$ being the ﬁrst to compute its message. However, multiple other orderings are legitimate, for example, $2,5,1,3,4,6$ ; in general, any ordering that respects the ordering constraints $\{(2~\prec~3),(3~\prec$ $4),(1\prec4),(4\prec6),(5\prec6)\}$ is a legal ordering for the message passing process. 

We can use this algorithm to compute the marginal probability of any set of query nodes $Y$ which is fully contained in some clique. We select one such clique $C_{r}$ to be the root, and perform the clique-tree message passing toward that root. We then extract $\tilde{P}_{\Phi}(Y)$ from the ﬁnal potential at $C_{r}$ by summing out the other variables $C_{r}-Y$ . 
> 该算法可以用于计算任意一组 query 节点 $\pmb Y$ 的边际分布，只要它们完全包含在某个团中
> 我们选择某个包含 $\pmb Y$ 中所有节点的团作为根，然后朝着根执行团树消息传递，最后在 $\pmb C_r$ 上的信念求和消去 $\pmb C_r - \pmb Y$ 中的变量，就得到了 $\tilde P_\Phi (\pmb Y)$

#### 10.2.1.3 Correctness 
We now prove that this algorithm, when applied to a clique tree that satisﬁes the family preservation and running intersection property, computes the desired expressions over the messages and the cliques. 
> 上一节介绍了团树消息传递算法
> 本节证明该算法应用在具有族保持和运行交集性质的团树中，将在团和消息上计算出我们所想要的表达式

In our algorithm, a variable $X$ is eliminated only when a message is sent from $C_{i}$ to a neighboring $C_{j}$ such that $X\in C_{i}$ and $X\notin C_{j}$ . We ﬁrst prove the following result: 
>在我们的算法中，变量 $X$ 只有在从 $\pmb C_{i}$ 向相邻的 $\pmb C_{j}$ 发送消息时才会被消除，且此时需要满足 $X\in \pmb C_{i}$ 和 $X\notin \pmb C_{j}$
>我们首先证明以下结果：

**Proposition 10.2** 
Assume that $X$ is eliminated when a message is sent from $C_{i}$ to $C_{j}$ . Then $X$ does not appear anywhere in the tree on the $C_{j}$ side of the edge $(i{-}j)$ . 
> 命题：
> 假设 $X$ 在 $\pmb C_i$ 向 $\pmb C_j$ 传递消息时被消除，则在团树中， $X$ 将不会再出现在边 $(i-j)$ 的靠 $\pmb C_j$ 的一边

Proof The proof is a simple consequence of the running intersection property. Assume by contradiction that $X$ appears in some other clique $C_{k}$ that is on the $C_{j}$ side of the tree. Then $C_{j}$ is on the path from $C_{i}$ to $C_{k}$ . But we know that $X$ appears in both $C_{i}$ and $C_{k}$ but not in $C_{j}$ , violating the running intersection property. 
> 证明：
> 假设 $X$ 出现在了某个团 $\pmb C_k$ 中，$\pmb C_k$ 处于 $\pmb C_j$ 的那一边，则 $\pmb C_j$ 就处于 $\pmb C_i$ 到 $\pmb C_k$ 的路径中，那么根据运行相交性质，$X$ 就应该在 $\pmb C_i, \pmb C_k, \pmb C_j$ 中都出现，但 $X$ 在 $\pmb C_i$ 向 $\pmb C_j$ 传递消息时已经被消除，因此不可能再出现在 $\pmb C_j$ 中，故推出了矛盾

Based on this result, we can provide a semantic interpretation for the messages used in the clique tree. Let $(i{-}j)$ be some edge in the clique tree. We use $\mathcal{F}_{\prec(i\rightarrow j)}$ to denote the set of factors in the cliques on the $C_{i}$ -side of the edge and $\mathcal{V}_{\prec(i\rightarrow j)}$ to denote the set of variables that appear on the $C_{i}$ -side but are not in the sepset. For example, in the clique tree of ﬁgure 10.2, we have that $\mathcal{F}_{\prec(3\rightarrow5)}\,=\,\{P(C),P(D\mid C),P(G\mid I,D),P(I),P(S\mid I)\}$ and $\mathcal{V}_{\prec(3\rightarrow5)}=\{C,D,I\}$ . Intuitively, the message passed between the cliques $C_{i}$ and $C_{j}$ is the product of all the factors in $\mathcal{F}_{\prec(i\rightarrow j)}$ , marginalized over the variables in the sepset (that is, summing out all the others). 
> 基于该命题，我们可以为团树中使用的消息提供一个语义上的解释
> 设 $(i{-}j)$ 是团树中的某条边，我们用 $\mathcal{F}_{\prec(i\rightarrow j)}$ 表示边的 $\pmb C_{i}$ 一侧团中的因子集合，用 $\mathcal{V}_{\prec(i\rightarrow j)}$ 表示出现在 $\pmb C_{i}$ 一侧但不在分离集中的变量集合。例如，在图10.2的团树中，我们有 $\mathcal{F}_{\prec(3\rightarrow5)}=\{P(C),P(D\mid C),P(G\mid I,D),P(I),P(S\mid I)\}$ 和 $\mathcal{V}_{\prec(3\rightarrow5)}=\{C,D,I\}$
> 直观地说，团 $\pmb C_{i}$ 和 $\pmb C_{j}$ 之间传递的消息是 $\mathcal{F}_{\prec(i\rightarrow j)}$ 中所有因子的乘积在分离集上变量进行边际化 (即求和消去所有其他所有变量) 得到的结果 

**Theorem 10.3**
Let $\delta_{i\to j}$ be a message from $\pmb C_{i}$ to $\pmb C_{j}$ . Then: 
> 定理： 
> 令 $\delta_{i\rightarrow j}$ 为从 $\pmb C_i$ 到 $\pmb C_j$ 的消息，则该消息满足：

$$
\delta_{i\rightarrow j}\big(\pmb{S}_{i,j}\big)=\sum_{\mathcal{V}_{\prec(i\rightarrow j)}}\prod_{\phi\in\mathcal{F}_{\prec(i\rightarrow j)}}\phi.
$$ 
Proof The proof proceeds by induction on the length of the path from the leaves. For the base case, the clique $C_{i}$ is a leaf in the tree. In this case, the result follows from a simple examination of the operations executed at the clique. 
>证明
>证明过程通过对路径长度进行归纳来完成
>归纳的基本情况是团 $\pmb C_{i}$ 是树的叶子节点，此时 $\mathcal{F}_{\prec (i\rightarrow j)}$ 为空，$\mathcal{V}_{\prec (i\rightarrow j)}$ 就是 $\pmb C_i - \pmb S_{i, j}$，故 $\delta_{i\rightarrow j} = \sum_{\pmb C_i - \pmb S_{i, j}}\phi_{i}$ ，这符合消息的定义

Now, consider a clique $\pmb C_{i}$ that is not a leaf, and consider the expression 

$$
\sum_{\mathcal{V}_{\prec(i\rightarrow j)}}\prod_{\phi\in\mathcal{F}_{\prec(i\rightarrow j)}}\phi.\tag{10.3}
$$ 
Let $\displaystyle i_{1},.\cdot\cdot\cdot,i_{m}$ be the neighboring cliques of $C_{i}$ other than $C_{j}$ . It follows immediately from proposition 10.2 that $\mathcal{V}_{\prec(i\rightarrow j)}$ is the disjoint union of $\mathcal{V}_{\prec(i_{k}\rightarrow i)}$ for $k\,=\,1,\cdots,m$ and the variables $Y_{i}$ eliminated at $C_{i}$ itself. Similarly, $\mathcal{F}_{\prec(i\rightarrow j)}$ is the disjoint union of the $\mathcal{F}_{\prec(i_{k}\rightarrow i)}$ and the factors $\mathcal F_i$ from which $\psi_{i}$ was computed. 
> 当 $\pmb C_i$ 不是叶子节点，令 $i_1, \dots, i_m$ 为 $\pmb C_i$ 除了 $\pmb C_j$ 以外的邻居团
> 根据命题 10.2， $\mathcal V_{\prec (i \rightarrow j)}$ 就是集合 $\mathcal V_{\prec (i_k \rightarrow i)}$ ($k = 1, \dots, m$) 和 $\pmb C_i$ 自身中消除的变量 $\pmb Y_i$ 的不交并集；类似地，$\mathcal F_{\prec (i\rightarrow j)}$ 是 $\mathcal{F}_{\prec(i_{k}\rightarrow i)}$ ($k=1,\dots, m$) 和计算 $\psi_i$ 本身的因子的集合 $\mathcal F_i$ 的不交并集

Thus equation (10.3) is equal to 
> 因此，我们将式 10.3 中的 $\mathcal V_{\prec (i \rightarrow j)}$ 和 $\mathcal F_{\prec (i\rightarrow j)}$ 拆开，写为式 10.4 的形式

$$
\sum_{\pmb Y_{i}}\sum_{\mathcal{V}_{\prec(i_{1}\rightarrow i)}}\cdots\sum_{\mathcal{V}_{\prec(i_{m}\rightarrow i)}}\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{1}\rightarrow i)}}\phi\right)\cdot\cdot\cdot\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{m}\rightarrow i)}}\phi\right)\cdot\left(\prod_{\phi\in\mathcal{F}_{i}}\phi\right).\tag{10.4}
$$

As we just showed, for each $k$ , none of the variables in $\mathcal{V}_{\prec(i_{k}\rightarrow i)}$ appear in any of the other factors. Thus, we can use equation (9.6) and push in the summation over $\mathcal{V}_{\prec(i_{k}\rightarrow i)}$ in equation (10.4), and obtain:
> 对于每个 $k$，$\mathcal{V}_{\prec (i_{k}\rightarrow i)}$ 是各不相交的 ( $\mathcal{F}_{\prec (i_{k}\rightarrow i)}$ 中的因子各自的作用域的交集是 $\pmb Y_i$，因此作用域中去除 $\pmb Y_i$ 得到的变量集合 $\mathcal{V}_{\prec (i_{k}\rightarrow i)}$ 就是互不相交的)，因此每个关于 $\mathcal{V}_{\prec (i_{k}\rightarrow i)}$ 的求和就仅和 $\mathcal{F}_{\prec (i_{k}\rightarrow i)}$ 中的因子有关，故我们可以将这些求和符号 “推入”，得到式 10.5

$$
\sum_{\pmb Y_{i}}\left(\prod_{\phi\in\mathcal{F}_{i}}\phi\right)\cdot\sum_{\mathcal{V}_{\prec(i_{1}\to i)}}\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{1}\to i)}}\phi\right)\cdot\cdot\cdot\sum_{\mathcal{V}_{\prec(i_{m}\to i)}}\left(\prod_{\phi\in\mathcal{F}_{\prec(i_{m}\to i)}}\phi\right).\tag{10.5}
$$ 
Using the inductive hypothesis and the deﬁnition of $\psi_{i}$ , this expression is equal to 
> 使用归纳假设，以及 $\psi_i$ 的定义，我们得到式 10.6

$$
\sum_{\pmb Y_{i}}\psi_{i}\cdot\delta_{i_{1}\rightarrow i}\cdot\cdot\cdot\cdot\delta_{i_{m}\rightarrow i},\tag{10.6}
$$

which is precisely the operation used to compute the message $\delta_{i\to j}$ . 
> 显然式 10.6 就是对消息 $\delta_{i\rightarrow j}$ 进行计算，证毕

This theorem is closely related to theorem 10.2, which tells us that a sepset divides the graph into conditionally independent pieces. It is this conditional independence property that allows the message over the sepset to summarize completely the information in one side of the clique tree that is necessary for the computation in the other. 
> 该定理和定理 10.2 密切相关
> 定理 10.2 告诉我们分离集将图分为了条件独立的两个部分，而正是这种条件独立性 (对于团树的一侧，给定分离集，团树的另一侧和这一侧条件独立) 使得我们传递的仅仅定义于分离集上的信息就能够完全总结团树一侧的信息，这些信息是在另一侧进行计算所需的

Based on this analysis, we can show that: 

**Corollary 10.1**
Let $\pmb C_{r}$ be the root clique in a clique tree, and assume that $\beta_{r}$ is computed as in the algorithm of algorithm 10.1. Then 
> 引理：
> $\pmb C_r$ 为团树的根，假设 $\beta_r$ 是 algorithm 10.1 计算得到的结果，则 $\beta_r$ 实际上就是 $\mathcal X$ 上的未规范化的度量求和消去 $\mathcal X - \pmb C_r$ 中的所有变量得到的因子

$$
\beta_{r}(\pmb C_{r})=\sum_{\mathcal{X}-\pmb C_{r}}\tilde{P}_{\Phi}(\mathcal{X}).
$$ 
As we discussed earlier, this algorithm applies both to Bayesian network and Markov network inference. For a Bayesian network $\mathcal{B}$ , if $\Phi$ consists of the C Ds in $\mathcal{B}$ , reduced with some evidence $e$ , then $\beta_{r}(\boldsymbol{C}_{r})\,=\,P_{\mathcal{B}}(\boldsymbol{C}_{r},e)$ . For a $\mathcal{H}$ , if Φ consists of the compatibility functions deﬁning the network, then $\beta_{r}(C_{r})\,=\,\tilde{P}_{\Phi}(C_{r})$ . In both cases, we can obtain the probability over the variables in $C_{r}$ as usual, by normalizing the resulting factor to sum to 1. In the Markov network, we can also obtain the value of the partition function simply by summing up all of the entries in the potential of the root clique $\beta_{r}(C_{r})$ . 
>如我们之前讨论的，该算法既适用于贝叶斯网络也适用于马尔可夫网络中的推断
>对于贝叶斯网络 $\mathcal{B}$ ，如果 $\Phi$ 由 $\mathcal{B}$ 中的 CPDs 构成，并在某些证据 $\pmb e$ 下进行了简化，那么 $\beta_{r}(\boldsymbol{C}_{r}) = P_{\mathcal{B}}(\boldsymbol{C}_{r}, \pmb e)$ 
>对于马尔可夫网络 $\mathcal{H}$ ，如果 $\Phi$ 由网络中的相容性函数构成，则 $\beta_{r}(\pmb C_{r}) = \tilde{P}_{\Phi}(\pmb C_{r})$
>在这两种情况下，我们都可以像通常一样通过归一化最后得到的因子来得到 $\pmb C_{r}$ 中变量的概率分布；在马尔可夫网络中，我们也可以通过将根团 $\beta_{r}(\pmb C_{r})$ 的势函数中的所有条目相加来直接得到划分函数的值

### 10.2.2 Clique Tree Calibration 
We have shown that we can use the same clique tree to compute the probability of any variable in $\mathcal{X}$ . In many applications, we often wish to estimate the probability of a large number of variables. For example, in a medical-diagnosis setting, we generally want the probability of several possible diseases. Furthermore, as we will see, when learning Bayesian networks from partially observed data, we always want the probability distributions over each of the unobserved variables in the domain (and their parents). 
> 上一节说明了我们可以使用相同的团树计算 $\mathcal X$ 中任意变量的后验概率

Therefore, let us consider the task of computing the posterior distribution over every random variable in the network. The most naive approach is to do inference separately for each variable. Letting $c$ be the cost of a single execution of clique tree inference, the total cost of this algorithm is nc . An approach that is slightly less naive is to run the algorithm once for every clique, making it the root. The total cost of this variant is $K c,$ where $K$ is the number of cliques. However, it turns out that we can do substantially better than either of these approaches. 
> 考虑为网络中每一个随机变量都计算后验分布
> 最朴素的方法就是分别为每个变量进行推理，令 $c$ 为团树推理单次执行的开销，则总开销就是 $nc$
> 还有一个较朴素的方法是为每个团运行依次算法，令 $K$ 为团的数量，则总开销就是 $Kc$

Let us revisit our clique tree of ﬁgure 10.2 and consider the three diferent executions of the clique tree algorithm that we described: one where $C_{5}$ is the root, one where $C_{4}$ is the root, and one where $C_{3}$ is the root. As we pointed out, the messages sent from $C_{1}$ to $C_{2}$ and from $C_{2}$ to $C_{3}$ are the same in all three executions. The message sent from $C_{4}$ to $C_{5}$ is the same in both of the executions where it appears. In the second of the three executions, there simply is no message from $C_{4}$ to $C_{5}$ — the message goes the other way, from $C_{5}$ to $C_{4}$ . 
> 让我们回顾一下图10.2中的团树，并考虑我们描述的三种不同的团树算法执行方式：一种是以 $C_{5}$ 为根节点，一种是以 $C_{4}$ 为根节点，另一种是以 $C_{3}$ 为根节点
> 在这三种执行方式中，从 $C_{1}$ 到 $C_{2}$ 和从 $C_{2}$ 到 $C_{3}$ 的消息都是相同的
> 从 $C_{4}$ 到 $C_{5}$ 的消息在出现该消息的两种执行方式中也是相同的，另一种则根本没有从 $C_{4}$ 到 $C_{5}$ 的消息——消息的方向相反，从 $C_{5}$ 到 $C_{4}$

More generally, consider two neighboring cliques $C_{i}$ and $C_{j}$ in some clique tree. It follows from theorem 10.3 that the value of the message sent from $C_{i}$ to $C_{j}$ does not depend on speciﬁc choice of root clique: As long as the root clique is on the $C_{j}$ -side, exactly the same message is sent from $C_{i}$ to $C_{j}$ . The same argument applies if the root is on the $C_{i}$ -side. Thus, in all executions of the clique tree algorithm, whenever a message is sent between two cliques in the same direction, it is necessarily the same. Thus, for any given clique tree, each edge has two messages associated with it: one for each direction of the edge. If we have a total of $c$ cliques, there are $c-1$ edges in the tree; therefore, we have $2(c-1)$ messages to compute. 
> 更一般地说，考虑团树中的相邻团 $C_i, C_j$
> 根据定理 10.3，只要根团在 $C_j$ 这一边，从 $C_i$ 到 $C_j$ 的消息的值就不依赖于对根团的选择
> 类似地，只要根团在 $C_i$ 这一边，从 $C_j$ 到 $C_i$ 的消息的值就不依赖于对根团的选择
> 因此，对于团树算法的所有执行，只要两个相邻团之间的消息方向一致，消息本身就一致
> 进而，对于任意给定团树，其中的每一条边实际仅关联两个消息，对应于两个方向，如果一共有 $c$ 个团，则树中就一共有 $c-1$ 条边，需要计算的消息总数就是 $2 (c-1)$

We can compute both messages for each edge by the following simple asynchronous algorithm. Recall that a clique can transmit a message upstream toward the root when it has all of the messages from its downstream neighbors. We can generalize this concept as follows: 
>我们可以通过以下简单的异步算法 (Algorithm 10.2) 计算每条边的两个消息
>回想一下，当一个团从其下游邻居收到所有消息后，它才可以向上游向根节点发送消息，我们可以将这一概念概括如下：

**Deﬁnition 10.4** ready clique 
Let $\tau$ be a clique tree. We say that $C_{i}$ is ready to transmit to a neighbor $C_{j}$ when $C_{i}$ has messages from all of its neighbors except from $C_{j}$ . 
> 定义
> $\mathcal T$ 为团树，如果 $C_i$ 具有它除了 $C_j$ 以外的所有邻居的消息，则称 $C_i$ 准备好向 $C_j$ 传输

When $C_{i}$ is ready to transmit to $C_{j}$ , it can compute the message $\delta_{i\to j}(S_{i,j})$ by multiplying its initial potential with all of its incoming messages except the one from $C_{j}$ , and then eliminate the variables in $C_{i}-S_{i,j}$ . In eﬀect, this algorithm uses yet another layer of dynamic programming to avoid recomputing the same message multiple times. 
> 当 $C_{i}$ 准备好向 $C_{j}$ 发送消息时，它就可以计算消息 $\delta_{i\to j}(\pmb S_{i,j})$
> 计算方法就是将其 $C_i$ 的初始势能与除来自 $C_{j}$ 之外的所有传入消息相乘，然后消去 $\pmb C_{i}-\pmb S_{i,j}$ 中的变量
> 实际上，该算法利用了另一层动态规划来避免多次重新计算相同的消息

Algorithm 10.2 shows the full procedure, often called *sum-product belief propagation* . As written, the algorithm is deﬁned asynchronously , with each clique sending a message as soon as it is ready. One might wonder why this process is guaranteed to terminate, that is, why there is always a clique that is ready to transmit to some other clique. In fact, the message passing process performed by the algorithm is equivalent to a much more systematic process that consists of an upward pass and a downward pass . In the upward pass, we ﬁrst pick a root and send all messages toward the root. When this process is complete, the root has all messages. Therefore, it can now send the appropriate message to all of its children. This algorithm continues until the leaves of the tree are reached, at which point no more messages need to be sent. This second phase is called the downward pass. The asynchronous algorithm is equivalent to this systematic algorithm, except that the root is simply the ﬁrst clique that happens to obtain messages from all of its neighbors. In an actual implementation, we might want to schedule this process more explicitly. (At the very least, the algorithm would check in line 2 that a message is not computed more than once.) 
> 算法10.2展示了完整的流程，通常称为“和积信念传播”
> 该算法被定义为异步执行的，每个团在其准备好的时候就发送消息
> 有人可能会疑惑这个过程为什么一定能终止，也就是为什么总有一个团准备好向其他某个团发送消息。实际上，该算法执行的消息传递过程等同于一个更加系统的过程，该过程由向上传递和向下传递组成。
> 在向上传递过程中，我们选择一个根节点，然后将所有消息发送到根节点。当此过程完成时，根节点将拥有所有消息。
> 因此，它现在可以将适当的消息发送给它的所有子节点。该算法将继续运行直到树的叶节点被到达，此时不再需要发送更多消息。这个第二阶段被称为向下传递。
> 异步算法实际上等价于以上描述的这个系统化的算法，只是异步算法中，根节点仅仅是恰好从其所有邻居处获得消息的第一个团 (根节点没有父节点)
> 在实际实现中，我们可能希望更明确地安排这个过程（至少，Algorithm 10.2 会在第2行检查消息不会被重复计算）

![[PGM-Algorithm10.2.png]]

Example 10.4
Figure 10.3a shows the upward pass of the clique tree algorithm when $C_{5}$ is the root. Figure $\it{10.5a}$ shows a possible ﬁrst step in a downward pass, where $C_{5}$ sends a message to its child $C_{3}$ , based on the message from $C_{4}$ and its initial potential. As soon as a child of the root receives a message, it has all of the information it needs to send a message to its own children. Figure $l0.5b$ shows $C_{3}$ sending the downward message to $C_{2}$ . 
> 在向下传递过程中，只要根团的某个子节点收到信息，它就有了需要向它自己的子节点传输消息所需要的全部信息

At the end of this process, we compute the *beliefs* for all cliques in the tree by multiplying the initial potential with each of the incoming messages. The key is to note that the messages used in the computation of $\beta_{i}$ are precisely the same messages that would have been used in a standard upward pass of the algorithm with $C_{i}$ as the root. Thus, we conclude: 
> 在 Algorithm 10.2 的整个过程结束时，对于每一个团 $C_i$，我们通过将其初始势能与每个传入消息相乘以得到它的“信念”
> 注意到用于计算 $\beta_{i}$ 的消息正是在以 $C_{i}$ 为根的算法标准向上传递过程中会使用到的消息
> 因此，我们得出结论：

**Corollary 10.2**
Assume that, for each clique $i$ , $\beta_{i}$ is computed as in the algorithm of algorithm 10.2. Then 

$$
\beta_{i}(C_{i})=\sum_{\mathcal{X}-C_{i}}\tilde{P}_{\Phi}(\mathcal{X}).
$$ 
> 引理
> Algorithm 10.2 计算得到的 $\beta_i (C_i)$ 就是 $\tilde P_\Phi (\mathcal X)$ 中求和消去除 $\pmb C_i$ 以外的所有变量得到的因子

Note that it is important that $C_{i}$ compute the message to a neighboring clique $C_{j}$ based on its initial potential $\psi_{i}$ and not its modiﬁed potential $\beta_{i}$ . The latter already integrates information from $j$ . If the message were computed based on this latter potential, we would be double-counting the factors assigned to $C_{j}$ (multiplying them twice into the joint). 
>要注意 $C_{i}$ 应该基于其初始势能 $\psi_{i}$ 而不是修改后的势能 $\beta_{i}$ 来计算发往相邻团 $C_{j}$ 的消息
> $\beta_i$ 已经集成了来自 $j$ 的信息。如果消息是基于后者的势能计算的，我们将对分配给 $C_{j}$ 的因子进行重复计数（将其乘两次进入联合概率）

When this process concludes, each clique contains the marginal (unnormalized) probability over the variables in its scope. As we discussed, we can compute the marginal probability over a particular variable $X$ by selecting a clique whose scope contains $X$ , and eliminating the redundant variables in the clique. A key point is that the result of this process does not depend on the clique we selected. That is, if $X$ appears in two cliques, they must agree on its marginal. 
> 当这个过程结束时，每个团上得到的就是包含其作用域内的变量的边际（未归一化）概率
> 我们可以通过选择一个包含特定变量 $X$ 的团，并消去团中的冗余变量来计算特定变量 $X$ 的边际概率。注意这一过程的结果不依赖于我们选择的团。也就是说，如果 $X$ 出现在两个团中，它们必须在 $X$ 的边际概率上达成一致 (得到的结果都将是 $\sum_{\mathcal X - X}\tilde P_\Phi (\mathcal X)$)

**Deﬁnition 10.5** calibrated 
Two adjacent cliques $C_{i}$ and $C_{j}$ are said to be calibrated if 

$$
\sum_{C_{i}-S_{i,j}}\beta_{i}(C_{i})=\sum_{C_{j}-S_{i,j}}\beta_{j}(C_{j}).
$$ 
A clique tree $\mathcal{T}$ is calibrated if all pairs of adjacent cliques are calibrated. For a calibrated clique tree, we use the term clique beliefs for $\beta_{i}(C_{i})$ and sepset beliefs for 

$$
\mu_{i,j}(\pmb S_{i,j})=\sum_{C_{i}-\pmb S_{i,j}}\beta_{i}(C_{i})=\sum_{C_{j}-\pmb S_{i,j}}\beta_{j}(C_{j}).\tag{10.7}
$$ 
> 定义
> 如果两个相邻的团 $C_i, C_j$ 满足 $C_i$ 上的信念求和消去所有 $C_i -  S_{i, j}$ 中的变量等于 $C_j$ 上的信念求和消去所有 $C_j - S_{i, j}$ 中的变量 (都得到 $S_{i, j}$ 上的边际概率，称其为分离集信念)，称二者是校准的
> 如果团树 $\mathcal T$ 中所有的相邻团都是校准的，称该团树是校准的

The main advantage of the clique tree algorithm is that it computes the posterior probability of all variables in a graphical model using only twice the computation of the upward pass in the same tree. Letting $c$ once again be the execution cost of message passing in a clique tree to one root, the cost of this algorithm is $2c$ . By comparison, recall that the cost of doing a separate computation for each variable is $n c$ and a separate computation for each root clique is $K c,$ where $K$ is the number of cliques. In most cases, the savings are considerable, making the clique tree algorithm the algorithm of choice in situations where we want to compute the posterior of multiple query variables. 
> 团树算法的主要优势在于，它只使用相当于在同一棵树中向上传递消息计算量的两倍就能计算图形模型中**所有**变量的后验概率：假设 $c$ 再次表示在团树中传递消息的执行成本（以一个根节点为目的地），该算法的成本就为 $2c$
> 相比之下，分别对每个变量进行单独计算的成本是 $nc$，而对每个根团进行单独计算的成本是 $Kc$，其中 $K$ 是团的数量。在大多数情况下，这种节省是相当可观的，这使得团树算法成为在需要计算多个查询变量后验概率的情况下首选的算法
> 团树算法本质上还是利用动态规划节约了重复的消息计算

Box 10.A — Skill: Efcient Implementation of Factor Manipulation Algorithms. While sim- ple conceptually, the implementation of algorithms involving manipulation of factors can be sur- prisingly subtle. In particular, diferent design decisions can lead to orders-of-magnitude diferences in performance, as well as diferences in the accuracy of the results. We now discuss some of the key design decisions in these algorithms. We note that the methods we describe here are equally applicable to the algorithms in many of the other chapters in the book, including the variable elimination algorithm of chapter 9, the exact and approximate sum-product message passing algorithms of chapters 10 and 11, and many of the MAP algorithms of chapter 13. 

The ﬁrst key decision is the representation of our basic data structure: a factor, or a multidimen- sional table, with an entry for each possible assignment to the variables. One standard technique for storing multidimensional tables is to ﬂatten them into a single array in computer memory. For each variable, we also store its cardinality, and its stride , or step size in the factor. For example, given a factor $\phi(A,B,C)$ over variables $A$ , $B$ , and $C$ , with cardinalities 2, 2, and 3, respectively, we can represent the factor in memory by the array 

$$
p h i[0.\ldots11]=\left\{\phi(a^{1},b^{1},c^{1}),\phi(a^{2},b^{1},c^{1}),\phi(a^{1},b^{2},c^{1}),.\ldots,\phi(a^{2},b^{2},c^{3})\right\}.
$$ 

Here the stride for variable $A$ is $l,$ for $B$ is 2 and for $C$ is 4. If we add a fourth variable, $D$ , its stride would be $l2,$ , since we would need to step over twelve entries before reaching the next assignment to $D$ . Notice how, using each variable’s stride, we can easily go from a variable assignment to $^a$ corresponding index into the factor array 

$$
i n d e x=\sum_{i}a s s i g n m e n t[i]\cdot p h i\,.s t r i d e[i]
$$ 
and vice versa 

$$
a s s i g n m e n t[i]=\lfloor i n d e x/p h i\,.s t r i d e[i]\rfloor\mod{c a r d[i]}
$$ 
With this factor representation, we can now design a library of operations: product , marginal- ization , maximization , reduction , and so forth. Since many inference algorithms involve multiple iterations over a series of factor operations, it is important that these be high-performance. One of the key design decisions is indexing the appropriate entries in each factor for the operations that we wish to perform. (In fact, when one uses a naive implementation of index computations, one often discovers that 90 percent of the running time is spent on that task.) 

Algorithm 10.A.1 provides an example for the product between two arbitrary factors. Here we deﬁne phi.stride $\mathbf{\nabla}![X]\ =\ 0$ if $X\ \notin\ S c o p e[\phi]$ . The inner loop (over l ) advances to the next assignment to the variables in $\psi$ and calculates indexes into each other factor array on the ﬂy. It can be understood by considering the equation for computing index shown earlier. Similar on- the-ﬂy index calculations can be applied for other factor operations. We leave these as an exercise (exercise 10.3). 

For iterative algorithms or multiple queries, where the same operation (on diferent data) is performed a large number of times, it may be beneﬁcial to cache these index mappings for later use. Note, however, that the index mappings require the same amount of storage as the factors themselves, that is, are exponential in the number of variables. Thus, this design choice ofers a direct trade-of between memory usage and speed, especially in view of the fact that the index computations require approximately the same amount of work as the factor operation itself. Since performance of main memory is orders of magnitudes faster than secondary storage (disk), when memory limitations are an issue, it is better not to cache index mappings for large problems. One exception is template models, where savings can be made by reusing the same indexes for diferent instantiations of the factor templates. 

An additional trick in reducing the computational burden is to preallocate and reuse memory for factor storage. Allocating memory is a relatively expensive procedure, and one does not want to waste time on this task inside a tight loop. To illustrate this point, we consider the example of variable elimination for computing $\psi(A,D)$ as 

$$
\psi(A,D)=\sum_{B,C}\phi_{1}(A,B)\phi_{2}(B,C)\phi_{3}(C,D)=\sum_{B}\phi_{1}(A,B)\sum_{C}\phi_{2}(B,C)\phi_{3}(C,D).
$$ 
Here we need to compute three intermediate factors: $\tau_{1}(B,C,D)=\phi_{2}(B,C)\phi_{3}(C,D);\tau_{2}(B,D)=$ $\textstyle\sum_{C}\tau_{1}(B,C,D)$ ; and $\tau_{3}(A,B,D)=\phi_{1}(A,B)\tau_{2}(B,D)$ . Notice that, once $\tau_{2}(B,D)$ has been calculated, we no longer need the values in $\tau_{1}(B,C,D)$ . By initially allocating memory large enough to hold the larger of $\tau_{1}(B,C,D)$ and $\tau_{3}(A,B,D)$ , we can use the same memory for both. Because every operation in a variable elimination or message passing algorithm requires the computation of one or more intermediate factors, some of which are much larger than the desired end product, the savings in both time (preallocation) and memory (reusage) can be signiﬁcant. 

log-space factor marginalization 

We now turn our attention to numerical considerations. Operations such as factor product involve multiplying many small numbers together, which can lead to underﬂow problems due to ﬁnite precision arithmetic. The problem can be alleviated somewhat by renormalizing the factor after each operation (so that the maximum entry in the factor is one); this operation leaves the results to most queries unchanged (see exercise 9.3). However, if each entry in the factor is computed as the product of many terms, underﬂow can still occur. An alternative solution is to perform the computation in log-space , replacing multiplications with additions; this transformation allows for greater machine precision to be utilized. Note that marginalization , which requires that we sum entries, cannot be performed in log-space; it requires exponentiating each entry in the factor, performing the marginalization, and taking the log of the result. Since moving from log-space to probability-space incurs a signiﬁcant decrease in dynamic range, factors should be normalized before applying this transform. One standard trick is to shift every entry by the maximum entry 

$$
\begin{array}{r}{p h i[i]\leftarrow\exp\left\{\mathit{l o g P h i}[i]-c\right\},}\end{array}
$$ 
where $c=\operatorname*{max}_{i}\mathcal{I}o g P h i[i]$ ; this transformation ensures that the resulting factor has a maximum entry of one and prevents overﬂow. 

We note that there are some caveats to operating in log-space. First, one may incur a performance hit: Floating point multiplication is no slower than ﬂoating point addition, but the transformation to and from log-space, as required for marginalization, can take a signiﬁcant proportion of the total processing time. This caveat does not apply to algorithms such as max-product, where maximization can be performed in log-space; indeed, these algorithms are almost always implemented as max- sum. Moreover, log-space operations require care in handling nonpositive factors (that is, factors with some zero entries). 

Finally, at a higher level, as with any software implementation, there is always a trade-of between speed, memory consumption, and reusability of the code. For example, software specialized for the case of pairwise potentials over a grid will almost certainly outperform code written for general graphs with arbitrary potentials. However, the small performance hit in using well designed general purpose code often outweighs the development efort required to reimplement algorithms for each specialized application. However, as always, it is also important not to try to optimize code too early. It is more beneﬁcial to write and proﬁle the code, on real examples, to determine what operations are causing bottlenecks. This allows the development efort to be targeted to areas that can yield the most gain. 

### 10.2.3 A Calibrated Clique Tree as a Distribution 
A calibrated clique tree is more than simply a data structure that stores the results of probabilistic inference for all of the cliques in the tree. As we now show, it can also be viewed as an alternative representation of the measure $\tilde{P}_{\Phi}$ . 
> 一个校准的团树不仅仅是一个存储树中所有团的概率推理结果的数据结构
> 如本节将介绍的，它还可以被视为度量 $\tilde{P}_{\Phi}$ 的另一种表示方式

At calibration, we have that: 

$$
\beta_{i}=\psi_{i}\cdot\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}.\tag{10.8}
$$

We also have that:

$$
\begin{align}
\mu_{i,j}(\pmb S_{i,j}) 
&= \sum_{\pmb C_i - \pmb S_{i,j}} \beta_i(\pmb C_i)\\
&= \sum_{\pmb C_i - \pmb S_{i,j}} \psi_i \cdot \prod_{k\in \text{Nb}_i}\delta_{k\rightarrow i}\\
&= \sum_{\pmb C_i - \pmb S_{i,j}} \psi_i \cdot \delta_{j\rightarrow i}\cdot \prod_{k\in (\text{Nb}_i-\{j\})}\delta_{k\rightarrow i}\\
&= \sum_{\pmb C_i - \pmb S_{i,j}} \psi_i \cdot \delta_{j\rightarrow i}\cdot \prod_{k\in (\text{Nb}_i-\{j\})}\delta_{k\rightarrow i}\\
&=\delta_{j\rightarrow i}\sum_{\pmb C_i - \pmb S_{i,j}} \psi_i \cdot \prod_{k\in (\text{Nb}_i-\{j\})}\delta_{k\rightarrow i}\\
&=\delta_{j\rightarrow i}\delta_{i\rightarrow j}\tag{10.9}
\end{align}
$$

where the fourth equality holds because no variable in the scope of $\delta_{j\rightarrow i}$ is involved in the summation. 

> (10.9) 表明了 $\pmb C_i, \pmb C_j$ 之间的分离集信念等于边 $i-j$ 上的两个消息的乘积

We can now show the following important result: 

**Proposition 10.3** 
At convergence of the clique tree calibration algorithm, we have that: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=\frac{\prod_{i\in\mathcal{V}_{\mathcal{T}}}\beta_{i}(\pmb C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(\pmb S_{i,j})}.\tag{10.10}
$$

> 命题
> 在团树算法收敛时，$\mathcal X$ 上的非规范化的 Gibbs 分布等于所有团的信念的乘积除去所有边上的分离集信念的乘积

Proof Using equation (10.8), the numerator in the right-hand side of equation (10.10) can be rewritten as: 

$$
\prod_{i\in\mathcal{V}_{T}}\psi_{i}(\pmb C_{i})\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}.
$$

Using equation (10.9), the denominator can be rewritten as: 

$$
\prod_{(i-j)\in\mathcal{E}\tau}\delta_{i\to j}\delta_{j\to i}.
$$

Each message $\delta_{i\to j}$ appears exactly once in the numerator and exactly once in the denominator, so that all messages cancel. The remaining expression is simply: 

$$
\prod_{i\in\mathcal{V}_{T}}\psi_{i}(C_{i})=\tilde{P}_{\Phi}.
$$

> 证明
> 将团和分离集的信念展开，发现分母和分子中，每个消息 $\delta_{i\rightarrow j}$ 正好都出现一次，因此直接消除，留下的就是所有团的势能函数的乘积，即未规范化的 Gibbs 分布

Thus, via equation (10.10), the clique and sepsets beliefs provide a re parameterization of the unnormalized measure. This property is called the clique tree invariant , for reasons which will become clear later on in this chapter. 
> 因此，通过(10.10)，团和分离集的信念提供了对未归一化度量的一种重参数化，这个性质被称为团树不变性

Another intuition for this result can be obtained from the following example: 

Example 10.5
Consider a clique tree obtained from Markov network $A-B-C-D,$ , with an appropriate set of factors $\Phi$ . Our clique tree in this case would have three cliques $C_{1}\,=\,\{A,B\}$ , $C_{2}\,=\,\{B,C\}$ , and $C_{3}=\{C,D\}$ . When the clique tree is calibrated, we have that $\beta_{1}(A,B)=\tilde{P}_{\Phi}(A,B)$ and $\beta_{2}(B,C)\,=\,\tilde{P}_{\Phi}(B,C)$ . From the conditional independence properties of this distribution, we have that 

$$
{\tilde{P}}_{\Phi}(A,B,C)={\tilde{P}}_{\Phi}(A,B){\tilde{P}}_{\Phi}(C\mid B),
$$ 
and 

$$
\tilde{P}_{\Phi}(C\mid B)=\frac{\beta_{2}(B,C)}{\tilde{P}_{\Phi}(B)}.
$$ 
As $\beta_{2}(B,C)=\tilde{P}_{\Phi}(B,C)$ , we can obtain $\tilde{P}_{\Phi}(B)$ by marginalizing $\beta_{2}(B,C)$ . Thus, we can write: 

$$
\begin{array}{r c l}{{\tilde{P}_{\Phi}(A,B,C)}}&{{=}}&{{\beta_{1}(A,B)\displaystyle\frac{\beta_{2}(B,C)}{\sum_{C}\beta_{2}(B,C)}}}\\ {{}}&{{=}}&{{\displaystyle\frac{\beta_{1}(A,B)\beta_{2}(B,C)}{\sum_{C}\beta_{2}(B,C)}.}}\end{array}
$$ 
In fact, when the two cliques are calibrated, they must agree on the marginal of $B$ . Thus, the expression in the denominator can equivalently be replaced by $\textstyle\sum_{A}\beta_{1}(A,B)$ . 

> 团树被校准后，团上的信念就等于团上的边际未规范化度量，例如 $\beta_1 (A, B) = \tilde P_\Phi (A, B)$

Based on this analysis, we now formally deﬁne the distribution represented by a clique tree: 

**Deﬁnition 10.6** clique tree measure 
We deﬁne the measure induced by a calibrated tree $\mathcal{T}$ to be: 

$$
Q_{\mathcal{T}}=\frac{\prod_{i\in\nu_{\mathcal{T}}}\beta_{i}(\pmb C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(\pmb S_{i,j})},\tag{10.11}
$$ 
where 

$$
\mu_{i,j}=\sum_{\boldsymbol{C}_{i}-\boldsymbol{S}_{i,j}}\beta_{i}(\boldsymbol{C}_{i})=\sum_{\boldsymbol{C}_{j}-\boldsymbol{S}_{i,j}}\beta_{j}(\boldsymbol{C}_{j}).
$$ 
> 定义：
> 我们将由校准的团树 $\mathcal T$ 导出的度量定义为 (10.11) ，即所有团上信念的乘积除去所有边上分离集信念的乘积

Example 10.6 
Consider, for example, the Markov network of example 3.8, whose joint distribution is shown in ﬁgure 4.2. One clique tree for this network consists of the two cliques $\{A,B,D\}$ and $\{B,C,D\}$ , with the sepset $\{B,D\}$ . The ﬁnal potentials and sepset for this example are shown in ﬁgure 10.6. It is straightforward to conﬁrm that the clique tree is indeed calibrated. One can also verify that this clique tree provides a re parameter iz ation of the original distribution. For example, consider the entry $\tilde{P}_{\Phi}(a^{1},b^{0},c^{1},d^{0})=100$ . According to equation (10.10), the clique tree measure is: 

$$
\frac{\beta_{1}(a^{1},b^{0},d^{0})\beta_{2}(b^{0},c^{1},d^{0})}{\mu_{1,2}(b^{0},d^{0})}=\frac{200\cdot300,100}{600,200}=100,
$$ 
as required. 

Our analysis so far shows that for a set of calibrated potentials derived from clique tree inference, we have two properties: the clique tree measure is $\tilde{P}_{\Phi}$ and the ﬁnal beliefs are the marginals of $\tilde{P}_{\Phi}$ . As we now show, these two properties coincide for any calibrated clique tree. 
> 到目前为止，我们的分析表明，对于从团树推理导出的一组校准的势函数，我们有两个性质：团树测度是 $\tilde{P}_{\Phi}$、团的信念是 $\tilde{P}_{\Phi}$ 的边缘分布
> 正如我们现在所展示的，这两个性质对于任何校准的团树都是等价的

**Theorem 10.4** 
Let $\mathcal{T}$ ique tree over $\Phi$ , and $\beta_{i}(C_{i})$ be a set tials for $\mathcal{T}$ . Then, $\tilde{P}_{\Phi}(\mathcal{X})\propto Q_{\mathcal{T}}$ if and only if, for each $i\in\mathcal{V}_{T}$  , we have that $\dot{\beta}_{i}(\mathbf{\cal{C}}_{i})\propto\tilde{P_{\Phi}}(\mathbf{\cal{C}}_{i})$ .
> 定理
> 令 $\mathcal T$ 为 $\Phi$ 上的团树，$\beta_i (\pmb C_i)$ 为 $\mathcal T$ 的一组校准的势能函数
> 则 $\tilde P_{\Phi}(\mathcal X)\propto \mathcal Q_{\mathcal T}$ 当且仅当对于所有 $i \in \mathcal V_{\mathcal T}$，$\beta_i (\pmb C_i) \propto \tilde P_\Phi (\pmb C_i)$

Proof Let $r$ e any clique in $\mathcal{T}$ , which we choose to be th oot. Deﬁne e descendant cliques of a clique $C_{i}$ to be the cliques that are downstream from $C_{i}$ relative to $C_{r}$ ; the nondescendant cliques are then the remaining cliques (other than $C_{i.}$ ). Let $X$ be the variables in the scope of the nondescendant cliques. It follows immediately from theorem 10.2 that 
> 证明
> 将 $\mathcal T$ 的任意一个团 $\pmb C_r$ 选择为根团 $r$
> 定义团 $\pmb C_{i}$ 的后代团为相对于 $\pmb C_{r}$ 而言在 $\pmb C_{i}$ 下游的那些团，非后代团就是剩余的团 (除去 $\pmb C_i$)
> 令 $\pmb X$ 是 $\pmb C_i$ 的非后代团的作用域中变量的集合，根据定理 10.2，我们有：

$$
{\tilde{P}}_{\Phi}\models(\pmb C_{i}\ \bot\ \pmb X\ |\ \pmb S_{i,p_{r}(i)}).
$$ 
From this, we obtain, using the standard chain-rule argument, that: 
> 由此，使用标准链式法则论证，我们可以得到：

$$
\tilde{P}_{\Phi}(\mathcal{X})=\tilde{P}_{\Phi}(\pmb C_{r})\cdot\prod_{i\neq r}\tilde{P}_{\Phi}(\pmb C_{i}\mid \pmb S_{i,p_{r}(i)}).
$$ 
We can rewrite equation (10.11) as a similar product, using the same root: 
> 我们可以将方程 (10.11) 重写为类似的形式，同样以相同的根表示：

$$
Q_{\mathcal{T}}(\mathcal{X})=\beta_{r}(\pmb C_{r})\cdot\prod_{i\neq r}\beta_{i}(\pmb C_{i}\mid \pmb S_{i,p_{r}(i)}).
$$ 
The “if” direction now follows from direct substitution of $\beta_{i}$ for each $\tilde{P}_{\Phi}(C_{i})$ . 
> “if”的方向现在可以通过把每个 $\beta_{i}$ 替换为 $\tilde{P}_{\Phi}(\pmb C_{i})$ 直接得到。

To prove the “only if” direction, we note that each of the terms $\beta_{i}(C_{i}\ \mid\ S_{i,p_{r}(i)})$ is a conditional distribution; hence, if we marginalize out the variables not in $C_{r}$ in the distribution $Q_{\mathcal{T}}$ , each of these conditional distributions marginalizes to 1 , and so we are left with $Q_{\mathcal{T}}(C_{r})=$ $\beta_{r}(C_{r})$ . It now follows that if $\tilde{P}_{\Phi}\propto Q_{\mathcal{T}}$ , then $\tilde{P}_{\Phi}(C_{r})\propto Q_{\mathcal{T}}(C_{r})=\beta_{r}(C_{r})$  . Because this argument applies to any choice of root clique, we have proved that this equality holds for every clique. 
> 为了证明“only if”的方向，我们注意到 $\beta_{i}(\pmb C_{i} \ | \ \pmb S_{i, p_{r}(i)})$ 中的每一项都是一个条件分布；因此，如果我们从分布 $Q_{\mathcal{T}}$ 中边际化掉不在 $\pmb C_{r}$ 中的变量，则这些条件分布会边际化为 1，所以我们只剩下 $Q_{\mathcal{T}}(\pmb C_{r}) = \beta_{r}(\pmb C_{r})$
> 于是，如果 $\tilde{P}_{\Phi} \propto Q_{\mathcal{T}}$，则 $\tilde{P}_{\Phi}(\pmb C_{r}) \propto Q_{\mathcal{T}}(C_{r}) = \beta_{r}(\pmb C_{r})$ 
> 由于此论证适用于任何根团的选择，我们已经证明了该等式对每个团都成立

**Thus, we can view the clique tree as an alternative representation of the joint measure, one that directly reveals the clique marginals.** As we will see, this view turns out to be very useful, both in the next section and in chapter 11. 
> 因此，我们可以将团树视为联合度量的另一种表示，它直接揭示了团的边际分布

## 10.3 Message Passing: Belief Update 
The previous section showed one approach to message passing in clique trees, based on the same ideas of variable elimination that we discussed in chapter 9. In this section, we present a related approach, but one that is based on very diferent intuitions. We begin by describing an alternative message passing scheme that is diferent from but mathematically equivalent to that of the previous section. We then show how this new approach can be viewed as operations on the reparameterization of the distribution in terms of the clique and sepset beliefs $\{\beta_{i}(C_{i})\}_{i\in\mathcal{V}_{\mathcal{T}}}$ and $\{\mu_{i,j}(S_{i,j})\}_{(i-j)\in\mathcal{E}_{T}}$ . Each message passing step will change this representation while leaving it a re parameterization of $\tilde{P}_{\Phi}$ . 
> 上一节展示了一种基于变量消去思想的团树中的消息传递方法
> 在本节中，我们将介绍一种相关的但基于非常不同的直觉的方法
> 我们首先描述另一种与前一节的方法不同但数学上等价的消息传递方案，然后展示如何将这种方法视为对在团和分离集信念 $\{\beta_{i}(C_{i})\}_{i\in\mathcal{V}_{\mathcal{T}}}$ 和 $\{\mu_{i,j}(S_{i,j})\}_{(i-j)\in\mathcal{E}_{T}}$ 的分布进行重新参数化的操作，每次消息传递步骤都会改变它的表示形式，但同时仍保持其为 $\tilde{P}_{\Phi}$ 的重新参数化

### 10.3.1 Message Passing with Division 
Consider again the message passing process used in CTree-SP-Calibrate (algorithm 10.2). There, two messages are passed along each link $(i{-}j)$ . Assume, without loss of generality, that the ﬁrst message is passed from $C_{j}$ to $C_{i}$ . A return message from $C_{i}$ to $C_{j}$ is passed when $C_{i}$ has received messages from all of its other neighbors. 
> 考虑 Algorithm 10.2 中的消息传递过程，其中每条边 $(i-j)$ 上都会经过两条消息
> 不失一般性，假设第一条消息是从 $\pmb C_j$ 到 $\pmb C_i$，第二条消息则是 $\pmb C_i$ 在收到所有其邻居的消息之后返回给 $\pmb C_j$ 的消息

At this point, $C_{i}$ has all of the necessary information to compute its ﬁnal potential. It multiplies the initial potential with the incoming messages from all of its neighbors: 

$$
\beta_{i}=\psi_{i}\cdot\prod_{k\in\mathrm{Nb}_{i}}\delta_{k\rightarrow i}.\tag{10-12}
$$ 
> 向回传递消息时，$\pmb C_i$ 有了用于计算其最终势能所有必要的消息，它将自己的初始势能和其所有邻居的消息相乘得到它的信念

As we discussed, this ﬁnal potential is not used in computing the message to $C_{j}$ : this potential already incorporates the information (message) passed from $C_{j}$ ; if we used it when computing the message to $C_{j}$ , this information would be double-counted. Thus, the message from $C_{i}$ to $C_{j}$ is computed in a way that omits the information obtained from $C_{j}$ : we multiply the initial potential with all of the messages except for the message from $C_{i}$ , and then marginalize over the sepset (equation (10.2)). 
> 但信念并不会用于计算 $\pmb C_i$ 向 $\pmb C_j$ 回传的消息，因为信念已经包含了 $\pmb C_j$ 到 $\pmb C_i$ 的消息，如果我们在这个消息计算过程中使用它，就会导致信息的重复计数
> 从 $\pmb C_i$ 发往 $\pmb C_j$ 的消息的计算需要忽略来自 $\pmb C_j$ 的信息，也就是将 $\pmb C_i$ 的初始势函数与除了来自 $\pmb C_j$ 的消息之外的所有消息相乘，然后对分离集进行边际化 (求和消去所有分离集之外的变量)

A diferent approach to computing the same expression is to multiply in all of the messages, and then divide the resulting factor by $\delta_{j\rightarrow i}$ . To make this notion precise, we must deﬁne a factor-division operation: 
> 另一种方法是将所有的传入消息都乘进来，然后将得到的因子除去 $\delta_{j\rightarrow i}$

**Deﬁnition 10.7** factor division 
Let $X$ and $Y$ be disjoint sets of variables, and let $\phi_{1}(X,Y)$ and $\phi_{2}(Y)$ be two factors. We deﬁne the division $\frac{\phi_{1}}{\phi_{2}}$ to be a factor $\psi$ of scope $X,Y$ deﬁned as follows: 

$$
\psi(\pmb X,\pmb Y)=\frac{\phi_{1}(\pmb X,\pmb Y)}{\phi_{2}(\pmb Y)},
$$

where we deﬁne $0/0=0$ . 

> 定义
> $\pmb X, \pmb Y$ 是不相交的变量集合，$\phi_1 (\pmb X, \pmb Y)$ 和 $\phi_2 (\pmb Y)$ 为两个因子
> 将除法 $\frac {\phi_1}{\phi_2}$ 的结果定义为作用域为 $\pmb X, \pmb Y$ 的因子 $\psi(\pmb X, \pmb Y)$，同时在该除法计算中，将 $0/0$ 定义为 $0$

Note that, as in the case of other factor operations, factor division is done component by component. Figure 10.7 shows an example. Also note that the operation is not well deﬁned if the denominator is zero and the numerator is not. 
> 和其他的因子运算相同，因子除法是逐个分量进行的 (详见 Figure 10.7)
> 注意当分母为零且分子不为零时，该运算是未明确定义的

We now see that we can compute the expression of equation (10.2) by computing the beliefs as in equation (10.12), and then dividing by the remaining message: 
> 定义好因子除法后，我们可以用信念先除去来自 $\pmb C_j$ 的消息，再用得到的因子求和消去其他变量得到发往 $\pmb C_i$ 的消息，如下所示

$$
\begin{align}
\delta_{i\rightarrow j}&=\sum_{\pmb C_i - \pmb S_{i,j}}\frac {\beta _i}{\delta_{j\rightarrow i}}\\
&=\frac{\sum_{\pmb C_{i}-\pmb S_{i,j}}\beta_{i}}{\delta_{j\rightarrow i}}.\tag{10.13}\\
\end{align}
$$

Example 10.7 Let us return to the simple clique tree in example 10.5, and assume that $C_{2}$ serves as the (de facto) root, so that we ﬁrst pass messages from $C_{1}$ to $C_{2}$ and from $C_{3}$ to $C_{2}$ . The message $\delta_{1\rightarrow2}$ $\textstyle\sum_{A}\psi_{1}(A,B)$ . Using the variable elimination message ( CTree-SP-Calibrate ), we pass a return message $\begin{array}{r}{\delta_{2\to1}(B)\,=\,\sum_{C}\psi_{2}(B,C)\delta_{3\to2}(C)}\end{array}$ . Alternatively, we can compute → $\beta_{2}(B,C)=\delta_{1\rightarrow2}(B)\cdot\delta_{3\rightarrow2}(C)\cdot\psi_{2}(B,C)$ → · → · , and then send a message 

$$
{\frac{\sum_{C}\beta_{2}(B,C)}{\delta_{1\rightarrow2}(B)}}=\sum_{C}{\frac{\beta_{2}(B,C)}{\delta_{1\rightarrow2}(B)}}=\sum_{C}\psi_{2}(B,C)\cdot\delta_{3\rightarrow2}(C).
$$ 

Thus, the two approaches are equivalent. 

Based on this insight, we can deﬁne the sum-product-divide message passing scheme, where each clique $C_{i}$ maintains its fully updated current beliefs $\beta_{i}$ , which are deﬁned as in equa- tion (10.8). Each sepset also maintains its beliefs $\mu_{i,j}$ deﬁned as the product of the messages in both directions, as in equation (10.9). We now show that the entire message passing process can be executed in an equivalent way in terms of the clique and sepset beliefs, without having to remember the initial potentials $\psi_{i}$ or to compute explicitly the messages $\delta_{i\to j}$ . 
> 基于这一见解，我们可以定义一个和-积-商的消息传递方法
> 该方法中，每个团 $\pmb C_{i}$ 维护其完全更新的当前信念 $\beta_{i}$ ，这些信念如公式 (10.8)所定义；每个分离集也维护其信念 $\mu_{i,j}$，这些信念被定义为两个方向消息的乘积，如公式 (10.9) 所示
> 我们现在表明，整个消息传递过程可以通过团和分离集的信念执行，而不需要记住初始势 $\psi_{i}$ 或显式计算消息 $\delta_{i\to j}$ 以等价的方式进行

The message passing process follows the lines of example 10.7. Each clique $C_{i}$ initializes $\beta_{i}$ as $\psi_{i}$ and then updates it by multiplying with message updates received from its neighbors. Each sepset $\boldsymbol{S}_{i,j}$ maintains $\mu_{i,j}$ as the previous message passed along the edge $(i{-}j)$ , regardless of the direction. This message is used to ensure that we do not double-count: Whenever a new message is passed along the edge, it is divided by the old message, eliminating the previous message from the update to the clique. Somewhat surprisingly, as we will show, the message passing operation is correct regardless of the clique that sent the last message on the edge. Intuitively, once the message is passed, its information is incorporated into both cliques; thus, each needs to divide by it when passing a message to the other. We can view this algorithm as maintaining a set of belief over the cliques in the tree. The message passing operation takes the beliefs of one clique and uses them to update the beliefs of a neighbor. Thus, we call this algorithm *belief-update message passing*; it is also known as the Lauritzen-Spiegelhalter algorithm . 
> 消息传递过程遵循示例10.7的思路
> 每个团 $\pmb C_{i}$ 将其信念 $\beta_{i}$ 的初始值设定为其初始势能 $\psi_{i}$，然后通过将其与从邻居接收到的消息更新相乘来更新它；每个分离集 $\boldsymbol{S}_{i,j}$ 将 $\mu_{i,j}$ 维护为沿着边 $(i{-}j)$ 传递的上一条消息，不考虑其方向
> 边上维护的这条消息用于确保我们不会重复计算：每当新消息沿该边传递时，它会除去上一条消息，从而消除了上一条消息对团更新的影响
> 无论是哪个团在边上发送了最后一条消息，消息传递操作都是正确的，直观地讲，当一条消息被传递，其信息将被包含到两个团中；因此，当一个团向另一个团传递消息时，都需要除以该消息
> 我们可以将此算法视为维护树中团的一组信念，消息传递操作利用一个团的信念来更新其邻居团的信念，我们称之为信念更新消息传递算法，它也被称为 Lauritzen-Spiegelhalter 算法

Example 10.8
Continuing with example 10.7, assume that $C_{2}$ initially passes an uninformed message to $C_{3}$ : $\begin{array}{r}{\sigma_{2\rightarrow3}=\sum_{B}\psi_{2}(B,C)}\end{array}$ . This message multiplies the beliefs about $C_{3}$ , so that, at this point: 

$$
\beta_{3}(C,D)=\psi_{3}(C,D)\sum_{B}\psi_{2}(B,C).
$$

This message is also stored in the sepset as $\mu_{2,3}$ . Now, assume that $C_{3}$ sends a message to $C_{2}$ : $\begin{array}{r}{\sigma_{3\rightarrow2}(C)=\sum_{D}\beta_{3}(C,D)}\end{array}$ . This message is divided by $\mu_{2,3}$ , so the actual update for $C_{2}$ is: 

$$
\begin{array}{c c l}{\displaystyle\frac{\sigma_{3\to2}(C)}{\mu_{2,3}(C)}}&{=}&{\displaystyle\frac{\sum_{D}\beta_{3}(C,D)}{\mu_{2,3}(C)}}\\ &{=}&{\displaystyle\frac{\sum_{D}\psi_{3}(C,D)\mu_{2,3}(C)}{\mu_{2,3}(C)}}\\ &{=}&{\displaystyle\sum_{D}\psi_{3}(C,D).}\end{array}
$$

This expression is precisely the update that $C_{2}$ would have received from $C_{3}$ in the case where $C_{2}$ does not ﬁrst send an uninformed message. At this point, the message stored in the sepset is 

$$
\sum_{D}\beta_{3}(C,D)=\sum_{D}\left(\psi_{3}(C,D)\cdot\sum_{B}\psi_{2}(B,C)\right).
$$

Assu t at the next step $C_{2}$ receives a message from $C_{1}$ , containing $\textstyle\sum_{A}\psi_{1}(A,B)$ . The sepset $S_{\mathrm{1,2}}$ contains a message that is identically 1 , so that this message is transmitted as is. $A t$ this point, $C_{2}$ has received informed messages from both sides and is therefore informed. Indeed, we have shown that: 

$$
\beta_{2}(B,C)=\psi_{2}(B,C)\cdot\sum_{A}\psi_{1}(A,B)\cdot\sum_{D}\psi_{3}(C,D).
$$

as required. 

![[PGM-Algorithm10.3.png]]

The precise algorithm is shown in algorithm 10.3. Note that, as written, the message passing algorithm is underspeciﬁed: in line 3, we can select any pair of cliques $C_{i}$ and $C_{j}$ between which we will pass a message. Interestingly, we can make this choice arbitrarily, without damaging the correctness of the algorithm. For example, if $C_{i}$ (for some reason) passes the same message to $C_{j}$ a second time, the process of dividing out by the stored message reduces the message actually passed to 1 , so that it has no inﬂuence. Furthermore, if $C_{i}$ passes a message to $C_{j}$ based on partial information (that is, without taking into consideration all of its incoming messages), and then resends a more updated message later on, the efect is identical to simply sending the updated message once. 
> 精确的算法如算法10.3所示。请注意，按照描述，消息传递算法是未充分规定的：在第3行，我们可以选择任何一对团 $\pmb C_{i}$ 和 $\pmb C_{j}$ 之间传递消息
> 有趣的是，我们可以随意做出这种选择，而不损害算法的正确性，例如，如果 $C_{i}$ （出于某种原因）再次向 $C_{j}$ 发送相同的 messages，通过从存储的消息中除以该消息，实际传递的消息将减少到 1，从而使其无影响；此外，如果 $C_{i}$ 根据部分信息（即，不考虑所有传入消息）向 $C_{j}$ 发送消息，然后再发送更更新的消息，其效果等同于仅发送一次更新后的消息

Moreover, at convergence, regardless of the message passing steps used, we necessarily have a calibrated clique tree. This property follows from the fact that, in order for all message updates to have no effect, we need to have $\sigma_{i\rightarrow j} = \mu_{i, j} = \sigma_{j\rightarrow i}$ for all $i, j$, and so:
> 此外，在收敛时，无论使用何种消息传递步骤，我们必然有一个校准良好的团树，这一性质来源于以下事实：在算法收敛时，为了使所有消息更新都再没有影响，我们需要满足对所有的 $i, j$ 都有 $\sigma_{i\rightarrow j} = \mu_{i, j} = \sigma_{j\rightarrow i}$，因此：

$$
\sum_{\pmb C_{i}-\pmb S_{i,j}}\beta_{i}=\mu_{i,j}=\sum_{\pmb C_{j}-\pmb S_{i,j}}\beta_{j}.
$$

Thus, at convergence, each pair of neighboring cliques $i,j$ must agree on the variables in sepset, and the message $\mu_{i,j}$ is precisely the sepset marginal. These properties also follow from the equivalence between belief-update message passing and sum-product message passing, which we show next. 
> 因此，在收敛时，每对相邻的团 $i,j$ 必须在分离集中的变量上达成一致，并且消息 $\mu_{i,j}$ 恰好是分离集的边缘分布
> 这些性质也来自于信念更新消息传递算法和和积消息传递算法之间的等价性，我们将在接下来展示这一点

### 10.3.2 Equivalence of Sum-Product and Belief Update Messages 
So far, although we used sum-product message propagation to motivate the deﬁnition of the belief update steps, we have not shown a direct connection between them. We now show a simple and elegant equivalence between the two types of message passing operations. From this result, it immediately follows that belief-update message passing is guaranteed to converge to the correct marginals. 
> 本节将展示 sum-product 消息传递和 belief-update 消息传递的等价性，从该等价性可以知道 belief-update 消息传递保证会收敛到正确的边际分布

Our proof is based on equation (10.8) and equation (10.9), which provide a mapping between the sum-product and belief-update representations. We consider corresponding runs of the two algorithms in which an identical sequence of message passing steps is executed. We show that these two properties hold as an invariant between the data structures maintained by the two algorithms. The invariant holds initially, and it is maintained throughout the corresponding runs. 
> 证明基于 eq (10.8), eq (10. 9)，它们提供了 sum-product 和 belief-update 表示之间的映射
> 我们考虑两种算法的相应运行，在这些运行中执行相同的一系列消息传递步骤，我们证明这两个性质 (eq 10.8 和 eq 10.9) 将作为两个算法维持的数据结构之间的不变量成立，这一不变量最初成立，并在整个相应的运行过程中得以保持

**Theorem 10.5** 
Consider a set of sum-product initial potentials $\{\psi_{i}\ \ :\ \ i\,\in\,\mathcal{V}_{\mathcal{T}}\}$ and me $\{\delta_{i\to j},\delta_{j\to i}\ \ :$ $(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}\}$ , and a set of belief-update beliefs $\{\beta_{i}\,:\,i\in\mathcal{V}_{\mathcal{T}}\}$ and messages { $\{\mu_{i,j}\,:\,(i{-}j)\in\mathcal{E}_{T}\}$ , for which equation (10.8) and equation (10.9) hold. For any pair of neighboring cliques $C_{i},C_{j}$ , let $\{\delta_{i\to j}^{\prime},\delta_{j\to i}^{\prime}\ :\ (i\!-\!j)\in\mathcal{E}_{T}\}$ → ssages following an application of SP-Message $(i,j)$ , and { $\{\beta_{i}^{\prime}\ :\ C_{i}\in\mathcal{T}\}$ $\{\mu_{i,j}^{\prime}\ :\ (i{-}j)\in\mathcal{E}_{T}\}$ , be the set of belief-update beliefs following an application of BU-Message $(i,j)$ . Then equation (10.8) and equation (10.9) also hold for the new beliefs $\delta_{i\to j}^{\prime},\,\beta_{i}^{\prime},\,\mu_{i,j}^{\prime}$ . 

> 定理：
> 考虑一组 sum-product 算法的初始势能 $\{\psi_i: i\in \mathcal V_{\mathcal T}\}$ 和消息 $\{\delta_{i\rightarrow j}, \delta_{j\rightarrow i}: (i-j)\in \mathcal E_{\mathcal T}\}$，以及一组 belief-update 算法的信念 $\{\beta_i : i \in \mathcal V_{\mathcal T}\}$ 和消息 $\{\mu_{i, j}: (i-j) \in \mathcal E_{\mathcal T}\}$ 它们满足 eq (10.8), eq (10.9)
> 对于任意一对邻居团 $\pmb C_i, \pmb C_j$，令 $\{\delta'_{i\rightarrow j}, \delta'_{j\rightarrow i}: (i-j)\in\mathcal E_{\mathcal T}\}$ 为应用 $\text{SP-Message} (i, j)$ 得到的 sum-product 算法中的一组消息，$\{\beta_i': \pmb C_i \in \mathcal T\}$ 和 $\{\mu'_{i, j}: (i-j)\in \mathcal E_{\mathcal T}\}$ 为应用 $\text{BU-Message}(i, j)$ 得到的 belief-update 算法中的一组信念，则 eq 10.8, eq 10.9 对于 $\delta_{i\to j}^{\prime},\,\beta_{i}^{\prime},\,\mu_{i,j}^{\prime}$ 仍然成立

The proof uses simple algebraic manipulation, and it is left as an exercise (exercise 10.4). 

This equivalence implies another result that will prove important in subsequent developments: 

**Corollary 10.3** 
In an execution of belief-update message passing, the clique tree invariant equation (10.10) holds initially and after every message passing step.
> 引理
> 在 belief-update 消息传递算法的执行过程中，团树不变式 (10.10) 在每一步消息传递之后都保持成立

Proof The proof of proposition 10.3 relied only on equation (10.8) and equation (10.9). Because these two equalities hold in every step of the belief-update message passing algorithm, we have that the clique tree invariant also holds continuously. 
> 证明：
> 命题 10.3 的证明仅依赖于 eq (10.8) 和 eq (10.9)，而这两个等式在 belief-update 消息传递算法中的每一步都成立，因此团树不变式也一直保持成立

This equivalence also allows us to deﬁne a message schedule that guarantees convergence to the correct clique marginals in two passes: We simply follow the same upward-downward-pass schedule used in CTree-SP-Calibrate , using any (arbitrarily chosen) root clique $\pmb C_{r}$ . 
> 这一等价性还允许我们定义一个消息调度方案，保证在两次传递中收敛到正确的团边际：我们只需遵循与 CTree-SP-Calibrate 中使用的相同向上向下传递调度方案，使用任意（随意选择的）根团 $\pmb C_{r}$

### 10.3.3 Answering Queries 
As we have seen, a calibrated clique tree contains the answer to multiple queries at once: the posterior probability of any set of variables that are present together in a single clique. A particular type of query that turns out to be important in this setting is the computation of the posterior for families of variables in a probabilistic network: a node and its parents in the context of Bayesian networks, or a clique in a Markov network. The family preservation property for cluster graphs (and hence for clique trees) implies that a family must be a subset of some cluster in the cluster graph. 
> 正如我们所见，一个校准后的团树同时包含了多个查询的答案：单个团中存在的任何一组变量的后验概率
> 在这种设置下，一种特别重要的查询类型是在概率网络中计算变量族的后验概率：贝叶斯网络中的节点及其父节点，或者马尔可夫网络中的团
> 簇图（以及因此团树）的族保留性质意味着一个变量族必须是簇图中某个簇的子集

In addition to these queries, which we get immediately as a by-product of calibration, we can also use a clique tree for other queries. We describe the algorithm for these queries in terms of a calibrated clique tree that satisﬁes the clique tree invariant. Due to the equivalence of sum-product and belief-update message passing, we can obtain such a clique tree using either method. 
> 除了这些可以直接通过校准获得结果的查询之外，我们还可以使用团树来进行其他查询
> 我们用满足团树不变式的校准团树来描述这些查询的算法，由于和积消息传递与信念更新消息传递是等价的，我们可以使用任一方法来获得这样的团树

#### 10.3.3.1 Incremental Updates 
Consider a situation where, at some point in time, we have a certain set of observations, which we use to condition our distribution and reach conclusions. At some later time, we obtain additional evidence, and want to update our conclusions accordingly. This type of situation, where we want to perform incremental update is very common in a wide variety of settings. For example, in a medical setting, we often perform diagnosis on the basis of limited evidence; the initial diagnosis helps us decide which tests to perform, and the results need to be incorporated into our diagnosis. 
> 考虑一个情况：开始有一组观察，我们用于 condition 我们的分布，获得结论，之后又获得了新的观察，需要对应更新我们的结论
> 这类情况下需要执行增量更新

The most naive approach to dealing with this task is simply to condition the initial factors (for example, the CPDs) on all of the evidence, and then redo the calibration process from the beginning, starting from these factors. A somewhat more efcient approach is based on the view of the clique tree as representing the distribution $\tilde{P}_{\Phi}$ . 
> 朴素方法是将所有初始因子 (例如 CPDs) 在 condition 于所有观察，然后重新执行校准过程
> 存在更高效的办法，这基于我们将团树视为分布 $\tilde P_\Phi$ 的表示

Assume that our initial distribution $\tilde{P}_{\Phi}$ (prior to the new information) is represented via a set of factors $\Phi$ , as in equation (10.1). Given some evidence $Z=z$ , we can obtain $\tilde{P}_{\Phi}(\mathcal{X},Z=z)$ X by zeroing out the entries in the unnormalized distribution that are inconsistent with the evidence $Z=z$ . We can accomplish this efect by multiplying $\tilde{P}_{\Phi}$ with an additional factor which is the indicator function $I\{Z=z\}$ . 
> 假设我们有分布 $\tilde P_\Phi$，给定观察 $Z = z$，我们可以通过将所有和 $Z = z$ 不一致的项归零得到 $\tilde P_\Phi(\mathcal X, Z = z)$
> 通过将 $\tilde P_\Phi$ 乘上指示函数 $\mathbf 1\{ Z = z\}$ (将它视为一个额外的因子)，可以达到相同的效果

More precisely, assume that our current distribution over $\mathcal{X}$ is deﬁned by a set of factors $\Phi$ , so that 

$$
{\tilde{P}}_{\Phi}({\mathcal{X}})=\prod_{\phi\in\Phi}\phi.
$$ 
Then, 

$$
\tilde{P}_{\Phi}(\mathcal{X},Z=z)=\mathbf{1}\{Z=z\}\cdot\prod_{\phi\in\Phi}\phi.
$$ 
Let $\tilde{P}_{\Phi}^{\prime}(\mathcal{X})=\tilde{P}_{\Phi}(\mathcal{X},Z=z)$ . 
> 用公式描述即如上所示
> 我们令 $\tilde P'_\Phi = \tilde P_\Phi(\mathcal X, Z = z)$

Now, assume that we have a clique tree (calibrated or not) that represents this distribution using the clique tree invariant. That is: 

$$
\tilde{P}_{\Phi}(\mathcal{X})=Q_{\mathcal{T}}=\frac{\prod_{i\in\nu_{\mathcal{T}}}\beta_{i}(\pmb C_{i})}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{T}}}\mu_{i,j}(\pmb S_{i,j})}.
$$ 
We can represent the distribution $\tilde{P}_{\Phi}^{\prime}(\mathcal{X})$ as 

$$
\tilde{P}_{\Phi}^{\prime}(\mathcal{X})=\mathbfcal{1}\{Z=z\}\cdot\frac{\prod_{i\in\mathcal{V}_{T}}\beta_{i}(\pmb C_{i})}{\prod_{(i-j)\in\mathcal{E}_{T}}\mu_{i,j}(\pmb S_{i,j})}.
$$ 
Thus, we obtain a representation of $\tilde{P}_{\Phi}^{\prime}$ in the clique tree simply by multiplying in the new factor $\mathbf 1\{Z=z\}$ into some clique $\pmb C_{i}$ containing the variable $Z$ . 
> 我们将 $\tilde P_\Phi(\mathcal X)$ 用团树不变式表示，就得到了上述的式子，也就是通过将新因子 $\mathbf 1\{Z = z\}$ 乘入某个包含变量 $Z$ 的团 $\pmb C_i$ 中就得到了 $\tilde P'_\Phi$

If the clique tree is calibrated before this new factor is introduced, then the clique $\pmb C_{i}$ has already assimilated all of the other information in the graph. Thus, the clique $\pmb C_{i}$ itself is now fully informed, and no additional message passing is required in order to obtain $\tilde{P}_{\Phi}^{\prime}(\pmb C_{i})$ . Other cliques, however, still need to be updated with the new information. To obtain $\check{P}_{\Phi}^{\prime}(\pmb C_{j})$ for another clique $\pmb C_{j}$ , we need only transmit messages from $\pmb C_{i}$ to $\pmb C_{j}$ , via the intervening cliques on the path between them. (See exercise 10.10.) As a consequence, the entire tree can be recalibrated to account for the new evidence using a single pass. 
> 如果在引入新因子之前团树已经校准，对于 $\pmb C_i$ 来说，它乘上了新因子，不需要额外的信息再更新自己，其信念已经等于 $\tilde P'_\Phi(\pmb C_i)$
> 对于其他团则需要额外的更新，我们需要从 $\pmb C_i$ 到 $\pmb C_j$ 再传递一次信息，因此整个团树需要额外一次 pass 以针对新的 evidence 进行重校准

Note that retracting evidence is not as simple: Once we multiply parts of the distribution by zero, these parts are lost, and they cannot be recovered. Thus, if we want to reserve the ability to retract evidence, we must store the beliefs prior to the conditioning step (see exercise 10.12). 
> 注意从重校准后的团树中重新提取 evidence 的信息则不容易，因为我们将分布中和 evidence 相关的部分变为了 0，其信息丢失了
> 如果想要保持重新提取 evidence 的能力，需要在 conditioning 先存储当前的 beliefs

Interestingly, the same incremental-update approach applies to other forms of updating the distribution. In particular, we can multiply the distribution with a factor that is not an indicator function for some variable, an operation that is useful in various applications. The same analysis holds unchanged. 
> 对于其他形式的分布更新，这一增量更新方法同样适用
> 具体地说，即便新因子不是指示函数的形式，我们仍然仅仅将它和整个分布相乘即可，分析的步骤是一致的

#### 10.3.3.2 Queries Outside a Clique 
Consider a query $P(Y\mid e)$ where the variables $Y$ are not present together in a single cliq One naive approach is to construct a clique tree where we force one of the cliques to contain $Y$ (see exercise 10.13). However, this approach forces us to tailor our clique tree to diferent queries, negating many of its advantages. An alternative approach is to perform variable elimination over a calibrated clique tree. 
> 考虑一个查询 $P(\pmb Y \mid e)$，其中 $\pmb Y$ 不完全出现在单个团中
> 朴素方法是构造一个团树，我们强制让某个团包含 $\pmb Y$，这让我们不得不根据查询调整团树
> 另一种方法是在校准的团树上执行变量消除

Example 10.9
Consider the simple clique tree of example 10.7, and assume that we have calibrated the clique tree, so that the beliefs represent the joint distribution as in equation (10.10). Assume that we now want to compute the probability $\tilde{P}_{\Phi}(B,D)$ . If the entire clique tree is calibrated, so is any (connected) subtree $\mathcal{T}^{\prime}$ . Letting $\mathcal{T}^{\prime}$ consist of the two cliques $C_{2}$ and $C_{3}$ , it follows from theorem 10.4 that: 

$$
\tilde{P}_{\Phi}(B,C,D)=Q_{\cal T}{}^{\prime}.
$$ 
By the clique tree invariant (equation (10.10)), we have that: 

$$
\begin{array}{c c l}{{\tilde{P}_{\Phi}(B,D)}}&{{=}}&{{\displaystyle\sum_{C}\tilde{P}_{\Phi}(B,C,D)}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\frac{\beta_{2}(B,C)\beta_{3}(C,D)}{\mu_{2,3}(C)}}}\\ {{}}&{{=}}&{{\displaystyle\sum_{C}\tilde{P}_{\Phi}(B\mid C)\tilde{P}_{\Phi}(C,D),}}\end{array}
$$ 
where the last equality follows from calibration. Each of these probability expressions corresponds to a set of clique beliefs divided by a message. We can now perform variable elimination, using these factors in the usual way. 

More generally, we can compute the joint probability $\tilde{P}_{\Phi}(\pmb Y)$ for an arbitrary subset $\pmb Y$ by using the beliefs in a calibrated clique tree to deﬁne factors corresponding to conditional probabilities in $\tilde{P}_{\Phi}$ , and then performing variable elimination over the resulting set of factors. The precise algorithm is shown in algorithm 10.4. The savings over simple variable elimination arise because we do not have to perform inference over the entire clique tree, but only over a portion of the tree that contains the variables $Y$ that constitute our query. In cases where we have a very large clique tree, the savings can be signiﬁcant. 
> 我们可以使用在校准的团树中的信念计算任意变量集合 $\pmb Y$ 的联合概率 $\tilde P_\Phi(\pmb Y)$，算法描述于 Algorithm 10.4
> 在 Algorithm 10.4 中，我们根据一个包含 $\pmb Y$ 的子树进行变量消除，不需要在整个团树中执行推理

![[PGM-Algorithm10.4.png]]

#### 10.3.3.3 Multiple Queries 
Now, assume that we want to compute the probabilities of an entire set of queries where the variables are not together in a clique. For example, we might wish to compute $\tilde{P}_{\Phi}(X,Y)$ for every pair of variables $X,Y\,\in\,{\mathcal{X}}\,-\,\pmb E$ . Clearly, the approach of constructing a clique tree to ensure that our query variables are present in a single clique breaks down in this case: If every pair of variables is present in some clique, there must be some clique that contains all of the variables (see exercise 10.14). 
> 现在，假设我们要计算一组查询的概率，其中变量并不都在同一个团中
> 例如，我们可能希望对每一个变量对 $X,Y \in \mathcal{X} - \pmb E$ 计算 $\tilde{P}_{\Phi}(X,Y)$ 
> 显然，在这种情况下，构建一个团树以确保查询变量存在于单个团中的方法就失效了：如果每对变量都存在于某个团中，那么必须存在一个包含所有这些变量的团（参见练习10.14）

A somewhat less naive approach is simply to run the variable elimination algorithm of algorithm 10.4 $\textstyle{\binom{n}{2}}$ times, once for each pair of variables $X,Y$ . However, because pairs of variables, on average, are fairly far from each other in the clique tree, this approach requires fairly substantial running time (see exercise 10.15). An even better approach can be obtained by using dynamic programming . 
> 一个较不朴素的方法是执行 algorithm 10.4 $\binom{n}{2}$ 次，每次执行针对一对变量 $X, Y$
> 但因为在团树中，变量对之间的平均距离一般是较远的，因此该方法也需要较多的运行时间
> 使用动态规划可以得到更好的方法

Consider a calibrated clique tree $\mathcal{T}$ over $\Phi$ , and assume we want to compute the probability $\tilde{P}_{\Phi}(X,Y)$ for every pair of variables $X,Y$ . We execute this process by gradually constructing a table for each $\pmb C_{i},\pmb C_{j}$ that contains $\tilde{P}_{\Phi}(\pmb C_{i},\pmb C_{j})$ . We construct the table for $i,j$ in order of the distance between $\pmb C_{i}$ and $\pmb C_{j}$ in the tree. 
> 考虑 $\Phi$ 上的校准的树 $\mathcal T$，我们想要为每个变量对 $X, Y$ 计算 $\tilde P_\Phi(X, Y)$
> 我们逐渐为每个 $\pmb C_i, \pmb C_j$ 对计算 $\tilde P_\Phi(\pmb C_i, \pmb C_j)$ ( 按照 $\pmb C_i,\pmb C_j$ 在树种的距离排序按照 $\pmb C_i,\pmb C_j$ 在树种的距离 )，记录在表格中，

The base case is when $i,j$ are neighboring cliques. In this case, we simply extract $\tilde{P}_{\Phi}(\pmb C_{i})$ from its clique beliefs, and compute 

$$
\tilde{P}_{\Phi}(\pmb C_{j}\mid \pmb C_{i})=\frac{\beta_{j}(\pmb C_{j})}{\mu_{i,j}(\pmb C_{i}\cap \pmb C_{j})}.
$$

From these, we can compute $\tilde{P}_{\Phi}(\pmb C_{i},\pmb C_{j})$ . 
> 基例是 $i, j$ 相邻，我们根据上式计算 $\tilde P_\Phi(\pmb C_j \mid \pmb C_i)$ ，进而计算 $\tilde P_\Phi(\pmb C_i, \pmb C_j)$

Now, consider a pair of cliques $C_{i},C_{j}$ that are not neighbors, and let $C_{l}$ be the neighbor of $C_{j}$ that is one step closer in the clique tree to $C_{i}$ . By construction, we have already computed $\tilde{P}_{\Phi}(C_{i},C_{l})$ and $\tilde{P}_{\Phi}(C_{l},C_{j})$ . The key now, is to observe that 
> 考虑 $\pmb C_i, \pmb C_j$ 不是邻居，令 $\pmb C_l$ 为 $\pmb C_j$ 向着 $\pmb C_i$ 方向的邻居
> 根据归纳假设，我们已知 $\tilde P_\Phi(\pmb C_i, \pmb C_l)$ ，并且容易计算出 $\tilde P_\Phi(\pmb C_l, \pmb C_j)$，此时观察到

$$
{\tilde{P}}_{\Phi}\models(\pmb C_{i}\ \bot\ \pmb C_{j}\ |\ \pmb C_{l}).
$$ 
Thus, we can compute 

$$
\begin{align}
\tilde{P}_{\Phi}(\pmb C_{i},\pmb C_{j})
&=\sum_{\pmb C_l - \pmb C_j}\tilde P_\Phi(\pmb C_i, \pmb C_l, \pmb C_j),\\
&=\sum_{\pmb C_{l}-\pmb C_{j}}\tilde{P}_{\Phi}(\pmb C_{i},\pmb C_{l})\tilde{P}_{\Phi}(\pmb C_{j}\mid \pmb C_{l}, \pmb C_i),\\
&=\sum_{\pmb C_{l}-\pmb C_{j}}\tilde{P}_{\Phi}(\pmb C_{i},\pmb C_{l})\tilde{P}_{\Phi}(\pmb C_{j}\mid \pmb C_{l}),\\
\end{align}
$$ 
where $\tilde{P}_{\Phi}(\pmb C_{j}\mid \pmb C_{l})$ can be easily computed from the marginal $\tilde{P}_{\Phi}(\pmb C_{j},\pmb C_{l})$ . 
> 因此可以直接从 $\tilde P_\Phi(\pmb C_i, \pmb C_l)$ 和 $\tilde P_\Phi(\pmb C_j, \pmb C_l)$ 中计算出 $\tilde P_\Phi(\pmb C_i, \pmb C_j)$

The cost of this computation is signiﬁcantly lower than that of running variable elimination in the clique tree $\textstyle{\binom{n}{2}}$ times (see exercise 10.15). 
> 该算法的开销经低于执行 $\binom{n}{2}$ 次 algorithm 10.4

## 10.5 Summary 
In this chapter, we have described a somewhat diferent perspective on the basic task of exact inference. This approach uses a preconstructed clique tree as a data structure for exact inference. Messages are passed between the cliques in the clique tree, with the end result that the cliques are calibrated — all cliques agree on the same marginal beliefs of any variable they share. We showed two diferent approaches to message passing in clique trees. The ﬁrst uses the same operations as variable elimination, using dynamic programming to cache messages in order to avoid repeated computation. The second uses belief propagation messages, which propagate marginal beliefs between cliques in an attempt to make them agree with each other. Both approaches allow calibration of the entire clique tree within two passes over the tree. 
> 本章从另一个角度描述了精确推理任务
> 本章的方法基于用预购建的团树作为推理的数据结构，消息在团树中的团之间传递，最后得到校准的团——所有的团在它们共享的变量上的边际概率分布上一致
> 本章讨论了团树中的两个消息传递方法，第一个方法使用变量消除的操作，同时利用动态规划缓存消息以避免重复计算；第二个方法使用信念传播，即在团之间传递信念，使得它们最后达成一致
> 两种方法都可以在对团树的两次 pass 之后校准整个树

It is instructive to compare the standard variable elimination algorithm of chapter 9 and the algorithm obtained by variable elimination in a clique tree. In principle, they are equivalent, in that they both use the same basic operations of multiplying factors and summing out variables. Furthermore, the cliques in the clique tree are basically the factors in variable elimination. Thus, we can use any variable elimination algorithm to ﬁnd a clique tree, and any clique tree to deﬁne an elimination ordering. It follows that the two approaches have basically the same computational complexity. 
> 比较第九章的标准变量消除算法和本章的团树中的变量消除算法，原则上二者是等价的，因为它们都仅使用了基本的运算：将因子相乘、求和消去变量
> 另外，团树中的团本质上就是执行了变量消除中使用的因子
> 因此我们可以使用任意变量消除算法找到团树，以及使用任意团树定义变量消除顺序，这两种方法有相同的计算复杂度

In practice, however, the two algorithms ofer diferent trade-ofs. On one hand, clique trees have several advantages. Most importantly, through the use of dynamic programming, the clique tree provides answers to multiple cliques using a single computation. Additional layers of dynamic programming allow the same data structure to answer an even broader range of queries, and to dynamically introduce and retract evidence. Moreover, the clique tree approach executes a nontrivial number of the required operations in advance, including the construction of the basic data structures, the choice of elimination ordering (which is almost determined), and the product of the CPDs assigned to a single clique. 
> 实践中，两种算法则有不同的 trade-off
> 团树有较多的优势，借助动态规划，团树可以用单次计算回答对多个团的查询，甚至回答更广范围的查询，并且可以动态引入和提取 evidence
> 团树方法会预先执行大量必要操作，包括构建基本数据结构、选择变量消除顺序、将 CPDs 的乘积分配给团

On the other hand, clique trees, as typically implemented, also have disadvantages. First, clique trees are more expensive in terms of space. In a clique tree, we keep all intermediate factors, whereas in variable elimination we can throw them out. If there are $c$ cliques, the cost of the clique tree algorithm can be as much as $2c$ times as expensive. More importantly, in a clique tree, the structure of the computation is ﬁxed and predetermined. We therefore have less ﬂexibility to take advantage of computational efciencies that arise because of speciﬁc features of the evidence and query. For example, in the Student network with evidence $i^{1}$ , the variable elimination algorithm could avoid introducing a dependence between $G$ and $S$ , resulting in substantially smaller factors. In the clique tree algorithm, the clique structure is usually predetermined, precluding these online optimizations. The diference in cost can be quite dramatic in situations where there is a lot of evidence. This type of situation-speciﬁc simpliﬁcation occurs even more often in networks that exhibit context-speciﬁc independence. Finally, in standard implementaitons, the cliques in a clique tree are typically the maximal cliques in a triangulated graph. Furthermore, the operations performed in the clique tree computation are typically implemented in a fairly standard way, where the incoming messages are multiplied with the clique beliefs, and the outgoing message is generated. This approach is not always optimal (see exercise 10.7). 
> 团树也存在劣势，首先在空间上更昂贵，我们在团树中会保存所有中间因子
> 如果有 c 个团，空间开销就要达到 2c
> 另外，团树中计算的结构是固定的且预定义的，因此不容易利用来自于 evidence 或 query 的特定特征灵活提高计算效率 (不能 online optimization)，这种特定情况下的简化在表现出上下文特定独立性的网络中更为常见
> 最后，在标准实现中，团树中的团通常是三角化图的最大团，此外，团树计算中执行的操作通常是采用相对标准的方式，即将传入的消息与团的信念相乘，然后生成传出的消息。这种方法并不总是最优的（参见练习10.7）

We can modify each of these algorithms to have some of the advantages of the other. For example, we can choose to deﬁne a clique tree online, after the evidence is obtained. In this case, the clique tree structure can take advantage of simpliﬁcations resulting from the evidence. However, we lose the advantage of precomputing the clique tree ofine. As another example, we can store intermediate results in a variable elimination execution, and then do a downward pass to obtain the marginal posteriors of all variables. Here, we gain the advantage of reusing computation, at the cost of additional space. In general, we can view these two algorithms as two examples in a space of variable elimination algorithms. There are many other variants that make somewhat diferent trade-ofs, but, fundamentally, they are performing essentially equivalent computations. 
> 我们可以分别修改这些算法，使其具备对方的一些优点
> 例如，我们可以在获得证据之后在线定义团树。在这种情况下，团树结构可以利用因证据而产生的简化。然而，我们会失去预先计算团树的优势
> 再举一个例子，我们可以在变量消去执行过程中存储中间结果，然后进行下推传递以获得所有变量的边缘后验概率。在这里，我们获得了重用计算的优势，但要付出额外的空间成本
> 总的来说，我们可以将这两种算法视为变量消去算法空间中的两个示例。还有许多其他变体，它们做出稍微不同的权衡，但从根本上说，它们执行的是本质上等效的计算

# 11 Inference as Optimization 
## 11.1 Introduction
In the previous chapters we examined exact inference. We have seen that for many networks we can perform exact inference efciently. As we have seen, the computational and space complexity of the clique tree is exponential in the tree-width of the network. This means that the exact algorithms we examined become infeasible for networks with a large tree-width. In many real-life applications, we encounter such networks. This motivates examination of approximate inference methods that are applicable to networks where exact inference is intractable.
> 团树中，精确推理算法的时间和空间复杂度和树的宽度成指数关系
> 对于很宽的树，精确推理算法不可解，我们需要进行近似推理
 
**In this chapter we consider a class of approximate inference methods, where the approximation arises from constructing an approximation to the target distribution $P_{\Phi}$ .** This approximation takes a simpler form that allows for inference. In general, the simpler approximating form exploits a local factorization structure that is similar in nature to the structure exploited by graphical models.
> 本章介绍一类近似推理算法
> 近似来自于为目标分布 $P_\Phi$ 构建的近似，该近似分布形式更简单，易于推理
> 一般地，这一更简单的近似形式采用了一个局部分解结构，该结构在本质上类似于图模型采用的结构
 
The specific algorithms we consider difer in many details, and yet they share some common conceptual principles. We now review these principles to provide a common framework for the remaining presentation. In each method, we define a target class $\mathcal{Q}$ of “easy” dis utions $Q$ and then search for an instance within that class that is the “best” approximation to $P_{\Phi}$ . Queries can then be answered using inference on $Q$ rather than on $P_{\Phi}$ . All of the methods we describe optimize (roughly) the same target function for measuring the similarity between $Q$ and $P_{\Phi}$ .
> 本章介绍的算法存在共同的概念上的原则：
> 在每个方法中，我们定义一个“简单”分布构成的目标族 $\mathcal Q$，在该族中搜索一个最为近似 $P_\Phi$ 的分布实例 $Q$，然后使用 $Q$ 回答对于 $P_\Phi$ 的查询
> 我们描述的所有方法都 (近似地) 优化一个衡量 $Q$ 和 $P_\Phi$ 之间相似度的目标函数
 
This approach reformulates the inference task as one of optimizing an objective function over the class $\mathcal{Q}$ . This problem falls into the category of constrained optimization . Such problems can be solved using a variety of diferent methods. Thus, the formulation of inference from this perspective opens the door to the application of a range of techniques developed in the optimization literature. Currently, the technique most often used in the setting of graphical models is one based on the use of Lagrange multipliers , which we review in appendix A.5.3. This method produce a set of equations that characterize the optima of the objective. In our setting, this characterization takes the form of a set of fixed-point equations that define each variable in terms of others. A particularly compelling and elegant result is that the fixed-point equations derived from the constrained energy optimization, for any of the methods we describe, can be viewed as passing messages over a graph object. Indeed, as we will show, even the standard sum-product algorithm for clique trees (algorithm 10.2) can be rederived from this perspective. Moreover, many other message passing algorithms follow from the same derivation. 
> 这类方法将推理任务重构为在一类分布 $\mathcal Q$ 上优化目标函数的任务，问题被转化为约束优化问题，可以用大量优化技巧求解
> 目前在图模型的设定下使用最多的方法基于拉格朗日乘子法，该方法产生一组表征目标的最优值的方程
> 在我们的设定中，该表征的形式为一组固定点方程，通过这些方程，每个变量都可以用其他变量来定义
> 一个较为优雅的结果是从约束能量优化导出的固定点方程可以被视为在一个图对象上发送消息，因此甚至标准的团树 sum-product 方法也可以从这一角度推导，许多其他的消息传递算法也亦如此

Methods in this class fall into three main categories. The first category includes methods that use clique-tree message passing schemes on structures other than trees. This class of methods, which includes the famous loopy belief propagation algorithm, can be understood as optimizing approximate versions of the energy functional. The second category includes methods that use message propagation on clique trees with approximate messages. This class of methods, often known as the expectation propagation algorithm, maximize the exact energy functional, but with relaxed consistency constraints on the representation $Q$ . Finally, in the third category there are methods that generalize the mean field method originating in statistical physics. These methods use the exact energy functional, but they restrict attention to a class $\mathcal{Q}$ consisting of distributions $Q$ that have a particular simple factorization. This factorization is chosen to be simple enough to ensure that we can perform inference with $Q$ .
> 这类方法可以分为三类
> 第一类在非树的结构上使用团树消息传递方法，例如 loopy belief propagation 算法，这类方法可以理解为优化能量泛函的近似形式
> 第二类在团树结构上使用近似消息进行消息传递，这类方法称为期望传播算法，它们最大化精确的能量泛函，但用 $Q$ 的表示松弛了一致性约束
> 第三类方法推广了统计物理学中的平均场方法，这类方法使用精确的能量泛函，但仅关注具有特定的简单分解形式的一类分布 $\mathcal Q$
 
More broadly, each of these algorithms can be described from two perspectives: as a procedural description of a message passing algorithm, or as an optimization problem consisting of an objective and a constraint space. Historically, the message passing algorithm generally originated first, sometimes long before the optimization interpretation was understood. However, the optimization perspective provides a much deeper understanding of these methods, and it shows that message passing is only one way of performing the optimization; it also helps point the way toward useful generalizations. In the ensuing discussion, we usually begin the presentation of each class of methods by describing a simple variant of the algorithm, providing a concrete manifestation to ground the concepts. We then present the optimization perspective on the algorithm, allowing a deeper understanding of the algorithm. Finally, we discuss generalizations of the simple algorithm, often ones that are derived directly from the optimization perspective.
> 更广泛地说，这些算法都可以从两个角度描述：作为一个消息传递算法的过程性描述，或描述为由一个目标和一个限制空间组成的优化问题
 
### 11.1.1 Exact Inference Revisited\* 
Before considering approximate inference methods, we start by casting exact inference as an optimization problem. The concepts we introduce here will serve in the discussion of the following approximate inference methods.
> 我们先将精确推理转化为一个优化问题
 
Assume we have a factorized distribution of the form
 
$$
P_{\Phi}(\mathcal{X})=\frac{1}{Z}\prod_{\phi\in\Phi}\phi(\pmb U_{\phi}),\tag{11.1}
$$

where the factors $\phi$ in $\Phi$ comprise the distribution, and the variables $\pmb U_{\phi}\,=\,S c o p e[\phi]\,\subseteq\,\mathcal{X}$ are the scope of each factor. 
> 假设我们由一个分解化的分布，形式如上
> 其中 $\pmb U_\phi = Scope[\phi]$

For example, the factors might be CPDs in a Bayesian network, generally restricted by an evidence set, or they might be potentials in a Markov network. We are interested in answering queries about the distribution $P_{\Phi}$ . These include queries about marginal probabilities of variables and queries about the partition function $Z$ . As we discussed, if $P_{\Phi}$ is a Bayesian network with instantiated evidence on some variables, then the partition function $Z$ is the probability of the evidence.
> 其中的因子可以为贝叶斯网络的 CPDs，一般会限制到某个 evidence 集合；或者可以是 Markov 网络的 potentials
> 我们需要回答关于分布 $P_\phi$ 的 query，例如变量的边际，以及关于划分函数 $Z$ 的查询
> 如果 $P_\phi$ 是在某个 evidence 下的贝叶斯网络，划分函数 $Z$ 就是 evidence 的概率
 
Recall that the end product of belief propagation is a calibrated cluster tree. Also recall that a calibrated set of beliefs for the cluster tree represents a distribution. In exact inference we find a set of calibrated beliefs that represent $P_{\Phi}(\mathcal X)$ . That is, we find beliefs that match the distribution represented by given set of initial potentials. Thus, we can view exact inference as searching over the set of d ibutions $\mathcal{Q}$ that are representable by the cluster tree to find a distribution $Q^{*}$ that matches $P_{\Phi}$ .
> 我们知道信念传播的结果是一个校准的簇树，而簇树的一组校准的信念也表示一个分布
> 精确推理中，我们找到表示 $P_\phi(\mathcal X)$ 的一组校准的信念，也就是找到一组匹配初始势能集合表示的分布的一组信念
> 因此，我们将精确推理视为在能被簇树表示的一组分布 $\mathcal Q$ 中搜索能匹配 $P_\phi$ 的分布 $Q^*$
 
Intuitively, we can rephrase this question as searching for a calibrated distribution that is as close as possible to $P_{\Phi}$ . There are many possible ways of measuring the distance between two distributions, such as the Euclidean distance $\mathrm{(L_{2})}$ , or the $\mathrm{L_{1}}$ distance and the related variational distance (see appendix A.1.3.3). As we will see, our main challenge, however, is our aim to avoid performing inference with the distribution $P_{\Phi}$ ; in particular, we cannot efectively compute marginal distributions in $P_{\Phi}$ . Hence, we need methods that allow us to optimize the distance between $Q$ and $P_{\Phi}$ without answering hard queries about $P_{\Phi}$ . A priori, this requirement may seem impossible to satisfy. However, it turns out that there exists a distance measure — the relative entropy (or KL-divergence) — that allows us to exploit the structure of $P_{\Phi}$ without performing reasoning with it.
> 直观上，我们可以将该问题表述为搜索到一个尽可能接近 $P_\phi$ 的校准分布
> 衡量两个分布的距离的方法包括欧式距离 $L_2$，或 $L_1$ 距离以及与它相关的变分距离等
> 但因为我们需要避免在 $P_\phi$ 上执行推理，特别地，我们不能在 $P_\phi$ 中高效计算边际，因子我们需要能在不准确回答关于 $P_\phi$ 的 query 的情况下优化 $Q$ 和 $P_\phi$ 之间距离的方法
> 因此我们考虑相对熵 (KL 散度)，它允许我们在不在 $P_\phi$ 上执行分析的情况下利用 $P_\phi$ 的结构 
 
Recall that the relative entropy between $P_{1}$ and $P_{2}$ is defined as
 
$$
D(P_{1}\|P_{2})=E_{P_{1}}\biggl[\ln\frac{P_{1}(\mathcal{X})}{P_{2}(\mathcal{X})}\biggr].
$$

Also recall that the relative entropy is always nonnegative, and equal to 0 if and only if $P_{1}=P_{2}$ . Thus, we can use it as a distance measure, and choose to find an approximation $Q$ to $P_{\Phi}$ that minimizes the relative entropy.
> $P_1, P_2$ 的相对熵定义如上，相对熵总是非负的，当且仅当 $P_1 = P_2$ 时等于零
> 我们将相对熵作为距离度量，希望找到一个最小化 $P_\phi$ 和 $Q$ 之间相对熵的近似分布 $Q$
 
However, as we discussed, the relative entropy is not symmetric $-\;D(P_{1}\|P_{2})\neq D(P_{2}\|P_{1})$ | | ̸ | | . In section 8.5, we discussed the use of relative entropy for projecting a distribution into a restricted class; this projection can aim to minimize either $D(P_{\Phi}\|Q)$ | | , via the $M\cdot$ -projection , or $D(Q\|P_{\Phi})$ | | , via the I-projection . A priori, it might appear that the M-projection is more appropriate, since one of the main information-theoretic justifications for the relative entropy $D(P_{\Phi}\|Q)$ | | is the numb of bits lost when coding a true message distribution $P_{\Phi}$ using an (approximate) estimate Q . However, as the discussion of section 8.5.2 shows, computing the M-projection $Q\,-\,\arg\operatorname*{min}_{Q}D(P_{\Phi}\|Q)\,-$ | | — requires that we compute marginals of $P_{\Phi}$ and is therefore equivalent to running inference in $P_{\Phi}$ . Somewhat surprisingly, as we show in the subsequent discussion, this does not apply to I-projection: we can exploit the structure of $P_{\Phi}$ to optimize arg min Q $D(Q\|P_{\Phi})$ efciently, without running inference in $P_{\Phi}$
> 我们知道，相对熵不是对称的，即 $D(P_1 || P_2) \ne D(P_2 || P_1)$
> 在第8.5节中，我们讨论了使用相对熵将一个分布投影到受限类别中的方法；这种投影可以旨在最小化 $D(P_{\Phi}\|Q)$ 或 $D(Q\|P_{\Phi})$，分别通过 M-投影和 I-投影实现
> 从表面上看，M-投影更合适一些，因为相对熵 $D(P_{\Phi}\|Q)$ 的一个重要信息论依据是，当使用（近似）估计 $Q$ 对真实消息分布 $P_{\Phi}$ 进行编码时会丢失的位数
> 但是，如第 8.5.2 节的讨论所示，计算 M-投影 $Q = \arg\min_{Q}D(P_{\Phi}\|Q)$ 需要计算 $P_{\Phi}$ 的边缘分布，因此等同于在 $P_{\Phi}$ 中运行推理过程
> 而 I-投影可以让我们我们利用 $P_{\Phi}$ 的结构，高效地优化 $\arg \min_{Q} D(Q\|P_{\Phi})$，同时而无需在 $P_{\Phi}$ 中运行推理过程
 
To summarize this discussion, we want to search for a distribution Q that minimizes $D(Q\|P_{\Phi})$ . To define and analyze this optimization problem formally, we also eed specify he objects we optimize over. Suppose we are given a clique tree structure T $\mathcal{T}$ for $P_{\Phi}$ ; that is, T $\mathcal{T}$ satisfies the running intersection property and the family preservation property. Moreover, suppose we are given a set of beliefs
> 因此，我们希望找到可以最小化 $D(Q||P_\Phi)$ 的 $Q$
> 为了形式化定义并且分析该优化问题，我们需要指定我们优化的对象
> 假设我们已经获得了 $P_{\Phi}$ 的一个团树结构 $\mathcal{T}$，也就是说，$\mathcal{T}$ 满足运行交集性质和族保持性质
> 此外，假设我们已经获得了一组信念：
 
$$
\pmb Q=\{\beta_{i}:i\in\mathcal{V}_{\mathcal{T}}\}\cup\{\mu_{i,j}:(i\mathrm{-}j)\in\mathcal{E}_{\mathcal{T}},\}
$$
 
where $C_{i}$ d notes clusters in $\mathcal{T},\,\beta_{i}$ denotes beliefs over $C_{i}$ , and $\mu_{i,j}$ denotes beliefs over $\boldsymbol{S}_{i,j}$ of edges in T .
> 我们用 $\pmb C_i$ 表示 $\mathcal T$ 中的簇，$\beta_i$ 表示 $\pmb C_i$ 上的信念，$\mu_{i, j}$ 表示 $\pmb S_{i, j}$ 上的信念
 
As in definition 10.6, the set of beliefs in $\mathcal{T}$ defines a distribution $Q$ by the formula
 
$$
Q(\mathcal X)=\frac{\prod_{i\in\mathcal V_{T}}\beta_{i}}{\prod_{(i-j)\in\mathcal E_{T}}\mu_{i,j}}.\tag{11.2}
$$

(See section 10.2.3.) Due to the calibration requirement, the set of beliefs $Q$ satisfies the marginal consistency constraints if, for each $(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}$ , the beliefs on $\boldsymbol{S}_{i,j}$ are the marginal of $\beta_{i}$ (and $\beta_{j})$ . 
> 根据定义 10.6，$\mathcal T$ 中的这一组信念按照 (11.2) 定义了一个分布 $Q(\mathcal X)$
> 根据校准要求，信念集合 $\pmb Q$ 需要满足边际一致约束，即对于每条边 $(i-j)\in \mathcal E_{\mathcal T}$，$\pmb S_{i, j}$ 上的信念需要是 $\beta_i$ 和 $\beta_j$ 的边际

Recall that theorem 10.4 shows that if $\pmb Q$ is a set of calibrated beliefs for  $\mathcal{T}$ and $Q$ is the distribution defined by equation (11.2), then

$$
\begin{array}{r c l}{{\beta_{i}[{\pmb c}_{i}]}}&{{=}}&{{Q({\pmb c}_{i})}}\\ {{\mu_{i,j}[{\pmb s}_{i,j}]}}&{{=}}&{{Q({\pmb s}_{i,j}).}}\end{array}
$$
 
Thus, the beliefs correspond to marginals of the distribution $Q$ defined by equation (11.2).
> 根据定理 10.4，如果 $\pmb Q$ 对于 $\mathcal T$ 是一组校准的信念，则 (11.2) 定义的分布 $Q(\mathcal X)$ 将满足信念就对应于 $Q$ 中的边际
 
Thus, we are now searching over a set of distributions $Q$ that are representable by a set of beliefs $Q$ over the cliques and sepsets in a particular clique tree structure $\mathcal{T}$ . Note that when deciding on the representation of $Q$ we are actually making two decisions: We are deciding both on the space of distributions that we are considering (all distributions for which $\mathcal{T}$ is an I-map), and on the representation of these distributions (as a set of calibrated clique beliefs). Both of these decisions are significant components in the specification of our optimization problem.
> 因此，我们现在需要寻找一组可以通过特定团树结构 $\mathcal{T}$ 中的团和分离集上的信念集 $\pmb Q$ 表示的分布 $Q$ 
> 请注意，在决定 $\pmb Q$ 的表示时，我们实际上做出了两个决定：我们不仅决定了要考虑的分布空间 (所有对于 $\mathcal{T}$ 是 I-映射的分布)，还决定了这些分布的表示方式 (作为一组校准的团信念)，这两个决定都是我们优化问题定义中的重要组成部分
 
With these definitions in hand, we can now view exact inference as maximizing $-D(Q\|P_{\Phi})$ over the space of calibrated sets $\pmb Q$ .
> 根据这些定义，我们将精确推理视作在校准集合 $\pmb Q$ 的空间中最大化 $-D(Q|| P_\Phi)$

CTree-Optimize-KL
**Find**    $\pmb Q = \{\beta_i : i \in \mathcal V_{\mathcal T}\}\cup \{\mu_{i, j}: (i-j)\in \mathcal E_{\mathcal T}\}$
**maximizing**  $-D(Q||P_{\Phi})$
**subject to**
 
$$
\begin{array}{r c l}{\mu_{i,j}[\pmb{s}_{i,j}]}&{=}&{\displaystyle\sum_{\pmb{C}_{i}-\pmb{S}_{i,j}}\beta_{i}(\pmb{c}_{i})\quad\forall(i\!-\!j)\in\mathcal{E}_{T},\forall\pmb{s}_{i,j}\in V a l(\pmb S_{i,j})}\\ {\displaystyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})}&{=}&{1\qquad\forall i\in\mathcal{V}_{T}.}\end{array}
$$
 
In solving this optimization problem, we conceptually examine diferent configurations of beliefs that satisfy the marginal consistency constraints, and we select the configuration that maximizes the objective. Such an exhaustive examination, of course, is impossible to perform in practice. However, there are efective solutions to this problem that find the maximum point. We have already seen that, if $\mathcal{T}$ is a proper cluster tree for the set of original potentials $\Phi$ , we know that there is a set $Q$ that induces, via equation (11.2), a distribution $Q=P_{\Phi}$ . Because this solution achieves a relative entropy of 0 , which is the highest value possible, it is the unique global optimum of this optimization.
> 在解决这个优化问题时，我们在概念上检查满足边际一致性约束的不同信念配置，并选择其中使目标函数最大化的配置
> 当然，这种穷尽的检查在实践中是不可能实现的，然而，有一些有效的方法可以找到最大点
> 我们已经知道，如果 $\mathcal{T}$ 是原始势函数集合 $\Phi$ 的适当簇树，则存在一个集合 $\pmb Q$，通过公式 (11.2)，它诱导出 $Q = P_{\Phi}$ ，由于这个解实现了相对熵为0，这是可能达到的最高值，因此它是该优化问题的唯一全局最优解

**Theorem 11.1**
If $\mathcal T$ is an I-map of $P_\Phi$, then there is a unique solution to CTree-Optimize-KL
> 定理
> 如果 $\mathcal T$ 是 $P_\Phi$ 的 I-map，则 CTree-Optimize-KL 存在唯一解

This optimum can be found using the exact inference algorithms we developed in chapter 10.
> 该最优解可以使用第十章的精确推理算法找到
 
### 11.1.2 The Energy Functional
The preceding discussion suggests a strategy for constructing approximations of $P_{\Phi}$ . Instead of searching over the space of all calibrated cluster trees, we can search over a space of “simpler” distributions. In this search we will not find a distribution equivalent to $P_{\Phi}$ , yet we might find one that is reasonably close to $P_{\Phi}$ . Moreover, as part of the design of the target set of distributions, we can ensure that these distributions are ones in which we can perform inference efciently.
> 之前的讨论提出了一种构造 $P_{\Phi}$ 近似的方法
> 但与其在整个校准簇树的空间内搜索，我们可以搜索“更简单”的分布空间，在这个搜索过程中，我们不会找到与 $P_{\Phi}$ 等价的分布，但我们可能会找到一个与 $P_{\Phi}$ 相对接近的分布
> 此外，在设计目标分布集合时，我们可以确保这些分布是我们能够高效进行推理的分布
 
One problem that we will face is that the target of the optimization $D(Q\|P_{\Phi})$ is unwieldy for direct optimization. The relative entropy term contains an explicit summation over all possible instantiations of $\mathcal{X}$ , an operation that is infeasible in practice. However, since we know the form of $\ln{P_{\Phi}}(\xi)$ from equation (11.1), we can exploit its structure to rewrite the relative entropy in a simpler form, as shown in the following theorem.
> 我们将面临的一个问题是，优化目标 $D(Q\|P_{\Phi})$ 在直接优化时难以处理，相对熵项包含对 $\mathcal{X}$ 所有可能实例的显式求知道和，而在实际操作中这是不可行的
> 然而，由于我们从公式 (11.1) 中了 $\ln{P_{\Phi}}(\xi)$ 的形式，我们可以利用其结构将其重写为一个更简单的形式，如下定理所示

**Theorem 11.2**
$D(Q||P_\Phi) = \ln Z  - F[\tilde P_\Phi, Q]$
where $F[\tilde P_\Phi, Q]$ is the energy functional
 
$$
F[\tilde{P}_{\Phi},Q]=E_{Q}\Big[\ln\tilde{P}(\mathcal{X})\Big]+H_{Q}(\mathcal{X})=\sum_{\phi\in\Phi}E_{Q}[\ln\phi]+H_{Q}(\mathcal{X}).\tag{11.3}
$$
 
Proof
 
$$
D(Q\|P_{\Phi})=E_{Q}[\ln Q(\mathcal{X})]-E_{Q}[\ln P_{\Phi}(\mathcal{X})].\tag{11.4}
$$
 
Using the product form of $P_{\Phi}$ , we have that
 
$$
\ln P_{\Phi}(\mathcal{X})=\sum_{\phi\in\Phi}\ln\phi(\pmb{U}_{\phi})-\ln Z.
$$
 
Moreover, recall that $H_{Q}(\mathcal{X})=-E_{Q}[\ln Q(\mathcal{X})]$ . Plugging these into equation (11.4), we get
 
$$
\begin{array}{r c l}{{\displaystyle D(Q\|P_{\Phi})}}&{{=}}&{{\displaystyle-H_{Q}({\mathcal X})-E_{Q}\left[\sum_{\phi\in\Phi}\ln\phi({\pmb U}_{\phi})\right]+E_{Q}[\ln Z]}}\\ {{}}&{{=}}&{{\displaystyle-F[\tilde{P}_{\Phi},Q]+\ln Z.}}\end{array}
$$
 
Importantly, the term $\ln{Z}$ does not depend on $Q$ . Hence, minimizing the relative entropy $D(Q\|P_{\Phi})$ is equivalent to maximizing the energy functional $F[\tilde{P}_{\Phi},Q]$ .
> 根据该定理，最小化相对熵 $D(Q||P_\Phi)$ 等价于最大化能量泛函 $F[\tilde P_\Phi, Q]$
 
This latter term relates to concepts from statistical physics, and it is the negative of what is referred to in that field as the (Helmholtz) free energy . While explaining the physics-based motivation for this term is out of the scope of this book, we continue to use the standard terminology of energy functional.
> $F[\tilde P_\Phi, Q]$ 与统计物理中的概念有关，它是该领域所说的 (Helmholtz) 自由能的负值
> 虽然解释这一项基于物理学的动机超出了本书的范围，但我们会继续使用"能量泛函"这一标准术语
 
The energy functional contains two terms. The first, called the energy term , involves expectations of the logarithms of factors in $\Phi$ . Here, each factor in $\Phi$ appears as a separate term. Thus, if the factors that comprise $\Phi$ are small, each expectation deals with relatively few variables. The difculties in dealing with these expectations depends on the properties of the distribution $Q$ . Assuming that inference is “easy” in $Q$ , we should be able to evaluate such expectations relatively easily. The second term, called the entropy term , is the entropy of $Q$ . Again, the choice of $Q$ determines whether we can evaluate this term. However, we will see that, for the choices we make, this term will also be tractable.
> 能量泛函包含两项。第一项称为能量项，涉及 $\Phi$ 中各因子的对数的期望
> 在这里，$\Phi$ 中的每个因子都作为一个单独的项出现，因此，如果组成 $\Phi$ 的因子较小，则每个期望处理的变量相对较少。处理这些期望的难度取决于分布 $Q$ 的属性。假设在 $Q$ 中推断是“容易”的，我们应该能够相对容易地评估这些期望
> 第二项称为熵项，是 $Q$ 的熵。同样，$Q$ 的选择决定了我们是否能够评估这一项
> 我们会看到，我们将做出的选择会使得这一项也将是可处理的

### 11.1.3 Optimizing the Energy Functional
In the remainder of this chapter, **we pose the problem of finding a good approximation $Q$ as one of maximizing the energy functional, or, equivalently, minimizing the relative entropy.** Importantly, the energy functional involves expectations in $Q$ . As we show, by choosing approximations $Q$ that allow for efcient inference, we can both evaluate the energy functional and optimize it efectively.
> 在本章的剩余部分，我们将找到好的近似 $Q$ 的问题转化为最大化能量泛函，或者等价地，最小化相对熵的问题
 
Moreover, since $D(Q\|P_{\Phi})\ge0$ , we have that
 
$$
\ln Z\geq F[\tilde{P}_{\Phi},Q].\tag{11.5}
$$
 
That is, the energy functional is a lower bound on the logarithm of the partition function $Z$ , for any choice of $Q$ . Why is this fact significant? Recall that, in directed models, the partition function $Z$ is the probability of the evidence. Computing the partition function is often the hardest part of inference. And so, this theorem shows that if we have a good approximation (that is, $D(Q\|P_{\Phi})$ is small), then we can get a good lower-bound approximation to $Z$ . The fact that this approximation is a lower bound will play an important role in later chapters on learning.
> 对于任何 $Q$ 的选择，能量泛函都是划分函数 $Z$ 的对数的一个下界
> ( $\ln Z = F[\tilde P_\Phi, Q] + D(Q||P_\Phi)$)
> 这个事实为什么很重要？回想一下，在有向模型中，划分函数 $Z$ 是证据的概率，计算划分函数通常是推断中最难的部分
> 因此，这个定理表明，如果我们有一个良好的近似 (即 $D(Q\|P_{\Phi})$ 很小)，那么我们可以得到 $Z$ 的一个好的下界近似
> 这个近似的下界性质将在后面关于学习的章节中发挥重要作用。
 
In this chapter, we explore inference methods that can be viewed as strategies for optimizing the energy functional. These kinds of methods are often referred to as variational methods . The name refers to a general strategy in which we want to solve a problem by introducing new variational parameters that increase the degrees of freedom over which we optimize. Each choice of these parameters gives an approximate answer. We then attempt to optimize the variational parameters to get the best approximation. In our case, the task is to answer queries about $P_{\Phi}$ , and the variational parameters describe the distribution $Q$ . In the methods we consider, we vary these parameters to try to find a good approximation to the target query.
> 在本章中，我们将探索可以被视为优化能量泛函的策略的推断方法
> 这类方法通常被称为变分方法，这个名字指的是一个通用策略，在这种方法中，我们希望通过引入新的变分参数来增加优化的自由度，这些参数的每一个选择都会给出一个近似答案，然后，我们尝试优化这些变分参数以获得最佳近似
> 在我们的案例中，任务是回答关于 $P_{\Phi}$ 的查询，而变分参数描述了分布 $Q$，我们调整这些参数以试图找到对目标查询的良好近似

## 11.2 Exact Inference as Optimization 
Before considering approximate inference methods, we illustrate the the use of a variational approach to rederive an exact inference procedure. The concepts we introduce here will serve in discussion of the following approximate inference methods. 
> 在讨论近似推理方法之前，我们阐述使用变分方法重新推导精确推理过程

As we have already seen, the optimization problem CTree-Optimize-KL has a unique solution. We start by reformulating the optimization problem in terms of the energy functional. As we have seen, maximizing the energy functional is equivalent to minimizing the relative entropy between $Q$ and $P_{\Phi}$ . 
> 我们知道优化问题 CTree-Optimize-KL 有唯一解
> 我们首先用能量泛函来重新表述该问题
> 我们已经知道，最大化能量泛函等价于最小化 $Q$ 和 $P_\Phi$ 之间的相对熵

Once we restrict attention to calibrated cluster trees, we can further simplify the objective function. More precisely, we can rewrite the energy functional in a factored form as a sum of terms each of which depends directly only on one of the beliefs in $Q$ . This form reveals the structure in the distribution, and it is therefore a much better starting point for further analysis. As we will see, this form is also the basis for our approximations in subsequent sections. 
> 一旦我们将注意力限制在校准簇树上，我们可以进一步简化目标函数
> 更确切地说，我们可以将能量泛函重写为一个各项之和的形式，每一项仅直接依赖于 $Q$ 中的一个信念
> 这种形式揭示了分布中的结构，因此是进一步分析的一个更好的起点
> 正如我们将看到的，这种形式也是我们在后续部分中进行近似的基础

**Definition 11.1**
Given a cluster tree $\mathcal T$ with a set of beliefs $\pmb Q$ and an assignment $\alpha$ that maps factors in $P_\Phi$ to clusters in $\mathcal T$, we define the factored energy functional:

$$
\tilde{F}[\tilde{P}_{\Phi},\pmb Q]=\sum_{i\in\mathcal{V}_{T}}E_{\pmb C_{i}\sim\beta_{i}}[\ln\psi_{i}]+\sum_{i\in\mathcal{V}_{T}}H_{\beta_{i}}(\pmb C_{i})-\sum_{(i-j)\in\mathcal{E}_{T}}H_{\mu_{i,j}}(\pmb S_{i,j}),\tag{11.6}
$$ 
where $\psi_{i}$ is the initial potential assigned to $\pmb C_{i}$ : 

$$
\psi_{i}=\prod_{\phi,\alpha(\phi)=i}\phi,
$$ 
and $E_{\pmb C_{i}\sim\beta_{i}}[\cdot]$ · denotes expectation on the value $\pmb C_{i}$ given the beliefs $\beta_{i}$ 
> 定义：
> 给定簇树 $\mathcal T$ 和它相关的一组因子 $\pmb Q$，以及一个赋值 $\alpha$ 将 $P_\Phi$ 中的因子映射到 $\mathcal T$ 中的因子，我们定义分解的能量泛函如上
> 其中 $\psi_i$ 是赋值给 $\pmb C_i$ 的初始势能，$E_{\pmb C_i \sim \beta_i}[\cdot]$ 表示给定信念 $\beta_i$ 时，$\pmb C_i$ 上的值的期望

Before we prove that the energy functional is equivalent to its factored variant, let us first study its components. The first term is a sum of terms of the form ${\cal E}_{C_{i}\sim\beta_{i}}[\ln\psi_{i}]$ . Recall that $\psi_{i}$ is a factor (not necessarily a distribution) over the scope $C_{i}$ , that is, a function from $V a l(C_{i})$ to $I\!R^{+}$ . Its logarithm is therefore a function from $V a l(C_{i})$ to $I\!\!R$ . The beliefs $\beta_{i}$ are a distribution over $V a l(C_{i})$ . We can therefore compute the expectation $\textstyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})\ln\psi_{i}$ . The last two terms are entropies of the beliefs associated with the clusters and sepsets in the tree. The important benefit of this reformulation is that all the terms are *local* , in the sense that they refer to a specific belief factor. As we will see, this will make our tasks much simpler. 
> 在我们证明能量泛函与其因式分解变体等价之前，让我们先研究其组成部分
> 第一项是由形式为 ${E}_{\pmb C_{i}\sim\beta_{i}}[\ln\psi_{i}]$ 的项组成的和式，回顾一下，$\psi_{i}$ 是 $\pmb C_{i}$ 作用域上的一个因子 (不一定必要是分布)，即从 $V a l(C_{i})$ 到 $R^{+}$ 的一个函数，因此，它的对数是从 $V a l(\pmb C_{i})$ 到 $R$ 的一个函数。而信念 $\beta_{i}$ 是在 $V a l(\pmb C_{i})$ 上的一个**分布**，因此，我们可以计算期望 $\textstyle\sum_{\pmb{c}_{i}}\beta_{i}(\pmb{c}_{i})\ln\psi_{i}$
> 后两项是与树中簇和分离集相关的信念的熵
> 这种重新表述的重要好处是所有项都是局部的，这意味着它们只涉及到特定的信念因子，正如我们将看到的，这将使我们的任务变得更加简单

**Proposition 11.1**
If $\pmb Q$ is a set of calibrated beliefs for $\mathcal{T}$ , and $Q$ is defined by equation (11.2), then 
$$
\tilde{F}[\tilde{P}_{\Phi},\pmb Q]=F[\tilde{P}_{\Phi},Q].
$$ 
> 命题
> 若 $\pmb Q$ 为 $\mathcal T$ 的一组校准的信念，$Q$ 是 $\pmb Q$ 由 (11.2) 定义的分布，则 $\tilde P_{\Phi} , \pmb Q$ 定义的分解的能量泛函等于 $\tilde P_\Phi, Q$ 定义的能量泛函

Proof Note that $\begin{array}{r}{\ln\psi_{i}=\sum_{\phi,\alpha(\phi)=i}\ln\phi}\end{array}$ . Moreover, since $\beta_{i}(\pmb c_{i})=Q(\pmb c_{i})$ , we conclude that 

$$
\sum_{i}E_{\pmb C_{i}\sim\beta_{i}}[\ln\psi_{i}]=\sum_{\phi}E_{\pmb C_{i}\sim Q}[\ln\phi].
$$ 
It remains to show that 

$$
H_{Q}(\mathcal{X})=\sum_{i\in\mathcal{V}_{T}}H_{\beta_{i}}(\pmb C_{i})-\sum_{(i-j)\in\mathcal{E}_{T}}H_{\mu_{i,j}}(\pmb S_{i,j}).
$$ 
This equality follows directly from equation (11.2) and theorem 10.4. 
> 证明
> $\ln \psi_i$ 中 $\psi_i$ 可以写为多个 $\phi$ 的乘积，因此 $\ln\psi_i$ 可以写为多个 $\ln\phi$ 的求和
> 根据之前的讨论，因子表示的边际分布 $\beta_i$ 等于我们构造的 $Q$ 在 $Val(\pmb C_i)$ 上的边际，即 $\beta_i(\pmb c_i) = Q(\pmb c_i)$
> 因此，我们将关于信念 $\beta_i$ 和团势能 $\psi_i$ 的和式 $\sum_{i}E_{\pmb C_{i}\sim\beta_{i}}[\ln\psi_{i}]$ 拆分并重写为关于分布 $Q$ 和因子的和式 $\sum_{\phi}E_{\pmb C_{i}\sim Q}[\ln\phi]$
> 现在，我们仅需要证明 $Q$ 的熵可以拆分为关于团和分离集各自信念的形式，我们直接根据 (11.2) 将 $Q(\mathcal X)$ 拆分，并根据定理 10.4 将 $Q(\mathcal X)$ 的边际替换为信念即可

Using this form of the energy, we can now define the optimization problem. We first need to define the space over which we are optimizing. If $Q$ is factorized according to $\mathcal{T}$ , we can represent it by a set of calibrated beliefs. Marginal consistency is a constraint on the beliefs that requires neighboring beliefs to agree on the marginal distribution on their joint subset. It is equivalent to requiring that the beliefs be calibrated. Thus, we pose the following constrained optimization procedure: 
> 利用这种分解形式的能量泛函，我们现在可以定义优化问题
> 首先，我们需要定义优化的空间，如果 $Q$ 按照 $\mathcal{T}$ 进行分解，我们可以通过一组校准的信念来表示它
> 边缘一致性是对信念的一种约束，要求相邻的信念在它们的联合子集上具有相同的边缘分布，这等价于要求信念是校准的
> 因此，我们提出了以下约束优化过程：

CTree-Optimize:
**Find**    $\pmb Q = \{\beta_i : i \in \mathcal V_{\mathcal T}\}\cup \{\mu_{I, j}: (i-j) \in \mathcal E_{\mathcal T}\}$
**maximizing**    $\tilde F[\tilde P_\Phi, \pmb Q]$
**subject to**

$$
\begin{align}
\mu_{i,j}[\pmb s_{i,j}] &= \sum_{\pmb C_i - \pmb S_{i,j}} \beta_i(\pmb c_i)\quad \forall(i-j)\in \mathcal E_{\mathcal T}, \forall \pmb s_{i,j} \in Val(\pmb S_{i,j})\tag{11.7}\\
\sum_{\pmb c_i}\beta_i(\pmb c_i) & = 1\quad \forall i \in \mathcal V_{\mathcal T}\tag{11.8}\\
\beta_i(\pmb c_i) & \ge 0\quad \forall i \in \mathcal V_{\mathcal T}, \pmb c_i \in Val(\pmb C_i)\tag{11.9}
\end{align}
$$

The constraints equation (11.7), equation (11.8), and equation (11.9) ensure that the beliefs in $Q$ are calibrated and represent legal distributions (exercise 11.2). 
> 约束方程 (11.7-11.9) 保证了 $\pmb Q$ 中的信念是校准的，并且表示合法的分布

### 11.2.1 Fixed-Point Characterization 
We can now prove that the *stationary points* of this constrained optimization function — the points at which the gradient is orthogonal to all the constraints — can be characterized by a set of fixed-point equations . As we show, these equations turn out to be the update equations in the sum-product belief-propagation procedure ( CTree-SP-calibrate in algorithm 10.2). Thus, if we turn these equations into an iterative algorithm, as we will describe, we obtain precisely the belief propagation algorithm in clique trees. We note that for this derivation and other similar ones later in the chapter, we restrict attention to models where all of the potentials are strictly positive (contain no zero entries). Although the results generally hold also for the case of deterministic potentials (zero entries), the proofs are considerably more complex and are outside the scope of this book. 
> 现在我们证明，这个约束优化函数的驻点——即梯度与所有约束正交的点——可以用一组固定点方程来刻画
> 我们将展示这些方程实际上是和积信念传播过程中的更新方程 (算法10.2中的CTree-SP-calibrate)，因此，如果我们把这些方程转化为迭代算法，我们得到的就是在团树中的信念传播算法
> 注意对于本推导和本章稍后部分其他类似推导，我们仅关注所有势能严格为正 (没有零条目) 的模型上，尽管结果通常也适用于确定性势能 (包含零条目) 的情况，但这些情况的证明会复杂得多，并超出了本书的范围

Recall that a stationary point of a function is either a local maximum, a local minimum, or a saddle point. In the optimization problem CTree-Optimize , there is a single global maximum (see theorem 11.1). Although we do not show it here, one can show that it is also the only stationary point (see exercise 11.3), and thus once we find a stationary point, we know that we have found the maximum. 
> 回想一下，函数的驻点要么是局部最大值，要么是局部最小值，要么是鞍点
> 在优化问题 CTree-Optimize 中，存在一个全局最大值 (参见定理11.1)，可以证明它也是唯一的驻点 (参见练习11.3)
> 因此，一旦找到一个驻点，我们就找到了最大值

We want to characterize this stationary point by a set of equations that must hold when the choice of beliefs in $Q$ is at the stationary point. Recall that our aim is to maximize the function $\tilde{F}[\tilde{P}_{\Phi},Q]$ under the consistency constraints. The method of Lagrange multipliers , reviewed in appendix A.5.3, provides us with tools for dealing with constrained optimization. Because the characterization of the stationary point is of central importance to later developments, we examine how to construct such a characterization using the method of Lagrange multipliers. 
> 因此，我们希望通过一组方程来刻画在 $\pmb Q$ 的信念处于驻点时必须满足的条件，以及我们的目的是在一致性的约束下最大化函数 $\tilde{F}[\tilde{P}_{\Phi},\pmb Q]$
> 而拉格朗日乘数法为我们提供了解决约束优化问题的工具
> 由于刻画驻点对于后续的发展至关重要，我们将考察如何使用拉格朗日乘数法构建这样的刻画

When using the method of Lagrange multipliers, we start by defining a Lagrangian with a Lagrange multiplier for each of the constraints on the function we want to optimize. In our case, we have the constraints in equation (11.7) and equation (11.8). We note that, in principle, we also need to introduce a Lagrange multiplier for the inequality constraint that ensures that all beliefs are nonnegative. However, as we will see, the assumption that factors are strictly positive implies that the beliefs we construct in the solution to the optimization problem will be nonnegative, and thus we do not need to enforce these constraints actively. 
> 使用拉格朗日乘数法时，我们为每个约束定义一个拉格朗日乘子，然后定义拉格朗日函数
> 我们因此为 (11.7), (11.8) 引入拉格朗日乘子，注意在原则上也需要为 (11.9) 的不等式引入拉格朗日乘子，但我们事先假设了所有的因子都是严格正的，这其实暗示了我们在优化问题的解中构建的信念也都是非负的，因此就不需要显式施加这一约束

We therefore obtain the following Lagrangian: 
> 我们得到以下拉格朗日函数

$$
\begin{array}{r c l}{\mathcal{J}}&{=}&{\tilde{F}[\tilde{{P}}_{\Phi},\pmb Q]}\\ &&{-\displaystyle\sum_{i\in\mathcal{V}_{T}}\lambda_{i}\left(\displaystyle\sum_{\pmb c_{i}}\beta_{i}(\pmb c_{i})-1\right)}\\ &&{-\displaystyle\sum_{i}\displaystyle\sum_{j\in\mathrm{Nb}_{i}}\displaystyle\sum_{\pmb s_{i,j}}\lambda_{j\to i}[\pmb s_{i,j}]\left(\displaystyle\sum_{\pmb c_{i}\sim \pmb s_{i,j}}\beta_{i}(\pmb c_{i})-\mu_{i,j}[\pmb s_{i,j}]\right),}\end{array}
$$ 
where $\mathrm{Nb}_{i}$ is the neighbors of $C_{i}$ in the clique tree. 
> 其中 $\text{Nb}_i$ 是 $\pmb C_i$ 在团树中的邻居集合

We introduce Lagrange multipliers $\lambda_{i}$ for each beliefs factor $\beta_{i}$ to ensure that it sums to 1 . We also introduce, for each pair of neighboring cliques $i$ and $j$ and assignment to their sepset $\pmb{s}_{i,j}$ , a Lagrange multiplier $\lambda_{j\rightarrow i}[\pmb{s}_{i,j}]$ to ensure that the marginal distribution of $\pmb{s}_{i,j}$ in $\beta_{j}$ is consistent with its value in the sepset beliefs $\mu_{i,j}$ . (Note that we also introduce another Lagrange multiplier for the direction $i\rightarrow j$ .) 
> 我们为每个信念因子 $\beta_i$ 引入了拉格朗日乘子 $\lambda_i$ 确保其在作用域上求和得到 1
> 我们也为每对邻居团 $i, j$ 和对它们的分离集 $\pmb s_{i, j}$ 的赋值引入拉格朗日乘子 $\lambda_{j\rightarrow i}[\pmb s_{i, j}]$，确保 $\beta_j$ 中 $\pmb s_{i, j}$ 的边际分布和分离集信念 $\mu_{i, j}$ 一致

Remember that $\mathcal{J}$ is a function of the clique beliefs $\{\beta_{i}\}$ , the sepset beliefs $\left\{\mu_{i,j}\right\}$ , and the Lagrange multipliers. To find the maximum of the Lagrangian, we take its partial derivatives with respect to $\beta_{i}(\pmb {c}_{i}),\,\mu_{i,j}[\mathbf{\boldsymbol{s}}_{i,j}]$ , and the Lagrange multipliers. These last derivatives reconstruct the original constraints. The first two types of derivatives require some work. Diferentiating the Lagrangian (see exercise 11.1), we get that 
> 拉格朗日函数 $\mathcal J$ 是关于团信念 $\{\beta_i\}$、分离集信念 $\{\mu_{i, j}\}$ 和拉格朗日乘子的函数
> 要找到拉格朗日函数的极大值，我们取 $\mathcal J$ 相对于 $\beta_i(\pmb c_i), \mu_{i, j}[\pmb s_{i, j}]$ 和拉格朗日乘子的偏导数
   因此得到 (拉格朗日乘子的偏导数会重构约束，因此略去)：

$$
\begin{array}{r c l}{\displaystyle\frac{\partial}{\partial\beta_{i}(\pmb{c}_{i})}\mathcal{J}}&{=}&{\ln\psi_{i}[\pmb{c}_{i}]-\ln\beta_{i}(\pmb{c}_{i})-1-\lambda_{i}-\displaystyle\sum_{j\in\mathrm{Nb}_{i}}\lambda_{j\rightarrow i}[\pmb{s}_{i,j}]}\\ {\displaystyle\frac{\partial}{\partial\mu_{i,j}[\pmb{s}_{i,j}]}\mathcal{J}}&{=}&{\ln\mu_{i,j}[\pmb{s}_{i,j}]+1+\lambda_{i\rightarrow j}[\pmb{s}_{i,j}]+\lambda_{j\rightarrow i}[\pmb{s}_{i,j}].}\end{array}
$$ 
At the stationary point, these derivatives are zero. Equating each derivative to 0 , rearranging terms, and exponentiating, we get 
> 在驻点上，这些导数都为零，因此令偏导数为零，整理得到：

$$
\begin{array}{r c l}{{\beta_{i}(\pmb c_{i})}}&{{=}}&{{\displaystyle\exp\left\{-1-\lambda_{i}\right\}\psi_{i}[\pmb c_{i}]\prod_{j\in{\mathrm{Nb}}_{i}}\exp\left\{-\lambda_{j\to i}[\pmb{s}_{i,j}]\right\}}}\\ {{\ }}&{{\ }}&{{\ }}\\ {{\mu_{i,j}[\pmb{s}_{i,j}]}}&{{=}}&{{\displaystyle\exp\left\{-1\right\}\exp\left\{-\lambda_{i\to j}[\pmb{s}_{i,j}]\right\}\exp\left\{-\lambda_{j\to i}[\pmb{s}_{i,j}]\right\}.}}\end{array}
$$ 
These equations describe beliefs as functions of terms of the form $\exp\left\{-\lambda_{i\rightarrow j}[\pmb{s}_{i,j}]\right\}$ . In fact, $\mu_{i,j}$ is a product of two such terms (and a constant). This suggests that these terms play the role of a message $\delta_{i\to j}$ . 
> 我们将信念写为了关于形如 $\exp\{-\lambda_{i\rightarrow j}[\pmb s_{i, j}]\}$ 的形式，可以看到 $\mu_{i, j}$ 是两个这样的项的乘积，这暗示了这些项实际上扮演的角色就是信息 $\delta_{i\rightarrow j}$

To make this more explicit, we define 
> 定义 $\delta_{i\rightarrow j}[\pmb s_{i, j}]$ 如下

$$
\delta_{i\rightarrow j}[\pmb {s}_{i,j}]=\exp\left\{-\lambda_{i\rightarrow j}[\pmb {s}_{i,j}]-\frac{1}{2}\right\}.
$$ 
(We add the term $-\frac{1}{2}$ to deal with the additional $\exp\left\{-1\right\}$ term, but since this is a multiplicative constant, it is not that crucial.) We can now rewrite the resulting system of equations as 
> 进而将方程组写为

$$
\begin{array}{r c l}{{\beta_{i}(\pmb c_{i})}}&{{=}}&{{\displaystyle\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}\psi_{i}(\pmb c_{i})\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\to i}[{\pmb s}_{i,j}]}}\\ {{}}&{{}}&{{}}\\ {{\mu_{i,j}[{\pmb s}_{i,j}]}}&{{=}}&{{\delta_{i\to j}[{\pmb s}_{i,j}]\delta_{j\to i}[{\pmb s}_{i,j}].}}\end{array}
$$ 
Combining these equations with equation (11.7), we now rewrite the message $\delta_{i\to j}$ as a function of other messages: 
> 根据 (11.7)，我们可以将 $\delta_{i\rightarrow j}$ 表示为其他信息的函数

$$
\begin{array}{c c l}{\delta_{i\to j}[\pmb{s}_{i,j}]}&{=}&{\displaystyle\frac{\mu_{i,j}[\pmb{s}_{i,j}]}{\delta_{j\to i}[\pmb{s}_{i,j}]}}\\ &{=}&{\displaystyle\frac{\sum_{\pmb{c}_{i}\sim\pmb{s}_{i,j}}\beta_{i}(\pmb{c}_{i})}{\delta_{j\to i}[\pmb{s}_{i,j}]}}\\ &{=}&{\displaystyle\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}\sum_{\pmb{c}_{i}\sim\pmb{s}_{i,j}}\psi_{i}(\pmb{c}_{i})\prod_{{k}\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\to i}[\pmb{s}_{i,k}].}\end{array}
$$

Note that the term $\begin{array}{r l}{\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}}\end{array}$ is a constant (since it does not depend on $\pmb c_{i}$ ), and when we combine these equations with equation (11.8), we can solve for $\lambda_{i}$ to ensure that this constant normalizes the clique beliefs $\beta_{i}$ . We note that if the original factors define a distribution that sums to 1 , then the solution for $\lambda_{i}$ that satisfies equation (11.8) will be one where $\begin{array}{r}{\lambda_{i}=\frac{1}{2}(|\mathrm{Nb}_{i}|-1)}\end{array}$  , that is, the normalizing constant is 1 . 
> 注意，项 $\exp\left\{-\lambda_{i}-1+\frac{1}{2}|\mathrm{Nb}_{i}|\right\}$ 是一个用于规范化 $\beta_i$ 的常数 (因为它不依赖于 $\pmb c_{i}$)
> 结合这些方程和 (11.8) ，我们可以求解出 $\lambda_{i}$ ，以确保这个常数规范化团信念 $\beta_{i}$
> 注意如果原始因子定义了一个总和为1的分布，则满足 (11.8) 的 $\lambda_{i}$ 的解将是 $\lambda_{i}=\frac{1}{2}(|\mathrm{Nb}_{i}|-1)$，也就是说，归一化常数为1

This derivation proves the following result:
> 该推导过程证明了以下结果

**Theorem 11.3**
A set of beliefs $\pmb Q$ is a stationary point of CTree-Optimize if and only if there exists a set of factors $\{\delta_{i\rightarrow j}[\pmb S_{i,j}]:(i\!-\!j)\in\mathcal{E}_{\mathcal{T}}\}$ such that 

$$
\delta_{i\rightarrow j}\propto\sum_{\pmb C_{i}-\pmb S_{i,j}}\psi_{i}\left(\prod_{k\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\rightarrow i}\right)\tag{11.10}
$$

and moreover, we have that

$$
\begin{array}{r c l}{\beta_{i}}&{\propto}&{\psi_{i}\left(\displaystyle\prod_{j\in{\mathrm{Nb}}_{i}}\delta_{j\to i}\right)}\\ {\mu_{i,j}}&{=}&{\delta_{j\to i}\cdot\delta_{i\to j}.}\end{array}
$$ 
> 定理
> 一组信念 $\pmb Q$ 是 CTree-Optimize 的驻点当且仅当存在一组作用域在各个分离集上的因子 $\{\delta_{i\rightarrow j}[\pmb S_{i, j}]: (i-j) \in \mathcal E_{\mathcal T}\}$，这些因子满足 (11.10)，即 $\delta_{i\rightarrow j}$ 正比于 $\psi_i$ 和除去来自 $j$ 的所有消息的乘积在 $\pmb S_{i, j}$ 上的边际，并且还要满足信念 $\beta_i$ 正比于 $\psi_i$ 乘上所有的消息，以及信念 $\mu_{i, j}$ 等于 $\delta_{i\rightarrow j}$ 和 $\delta_{j\rightarrow i}$ 的乘积

**This theorem characterizes the solution of the optimization problem in terms of fixed-point equations that must hold when we find a maximal $Q$ . These fixed-point equations define the relationships that must hold between the different parameters involved in the optimization problem. Most importantly, equation (11.10) defines each message in terms of other messages, allowing an easy iterative approach to solving the fixed point equations. These same themes appear in all the approaches we will discuss later in this chapter.** 
> 这个定理通过固定点方程刻画了优化问题的解，这些固定点方程在找到最大化的 $Q$ 时必须成立
> 这些固定点方程定义了优化问题中涉及的不同参数之间必须满足的关系，最重要的是，方程 (11.10) 通过其他消息定义了每个消息，这允许我们采用简单的迭代方法来解决固定点方程
> 这些相同的主题将在本章后续讨论的所有方法中出现

### 11.2.2 Inference as Optimization 
The fixed-point characterization of theorem 11.3 focuses on the relationships that hold at the maximum point (or points). However, they also hint at a way of achieving these relationships. Intuitively, a change in $\pmb Q$ that reduces the diferences between the left-hand and right-hand side of these equations will get us closer to a maximum point. The most direct way of reducing such discrepancies is to apply the equations as assignments and iteratively apply equations to the current values of the right-hand side to define a new value for the left-hand side. 
> 定理 11.3 的固定点表示聚焦于在最大点处的所保持的关系
> 然而，它们还暗示了一种实现这些关系的方法，直观地说，$\pmb Q$ 的变化如果能减少这些方程左右两边的差异，就会使我们更接近最大点
> 减少此类差异的最直接方法是将方程作为赋值，并迭代地将方程应用于右侧的当前值，以定义左侧的新值

More precisely, we initialize all of the $\delta_{i\to j}$ ’s to 1 and then iteratively apply equation (11.10), computing the left-hand side $\delta_{i\to j}$ of each equality in terms of the right-hand side (essentially converting each equality sign to an assignment). Clearly, a single iteration of this process does not usually suffice to make the equalities hold; however, under certain conditions (which hold in a clique tree), we can guarantee that this process converges to a solution satisfying all of the equations in equation (11.10); the other equations are now easy to satisfy. 
> 更具体地说，我们将全部的 $\delta_{i\rightarrow j}$ 初始化为 1，然后迭代式应用方程 (11.10)，用方程的右边计算方程的左边 (也就是将等于号转化为赋值号)
> 该过程仅进行单次迭代显然不足以让等式成立，而在特定条件下 (团树满足这些条件)，我们可以保证该过程收敛到一个满足 (11.10) 中的所有方程的解

Each assignment step defined by a fixed-point equation corresponds to a message passing step, where an outgoing message $\delta_{i\to j}$ is defined in terms of incoming messages $\delta_{k\to i}$ . The fact that the process requires multiple assignments to converge corresponds to the fact that inference requires multiple message passing steps. In this specific example, a particular order of applying the fixed-point equation reconstructs the sum-product message passing algorithm in cluster trees shown in algorithm 10.2. As we will see, however, when we consider other variants of the optimization problem, the associated fixed-point equations result in new algorithms. 
> 每一次赋值步骤都由对应于一次消息传递步骤的固定点方程定义，其中发出的 $\delta_{i\rightarrow j}$ 由传入的信息 $\delta_{k\rightarrow i}$ 定义
> 而该过程需要多次赋值才可以收敛的事实实际上对应于推理需要多次的信息传递步骤
> 在该特定的例子中，应用固定点方程的一个特定顺序就会重构 sum-product 消息传递算法 (algorithm 10.22)
> 而当我们考虑优化问题的变体时，这些固定点方程将生成新的算法

## 11.3 Propagation-Based Approximation 
In this section, we consider approximation methods that use exactly the same message propagation as in exact inference. However, these propagation schemes use a general-purpose cluster graph, as in definition 10.1, rather than a clique tree. Since the constraints defining a clique tree were crucial in ensuring exact inference, the message-propagation schemes that use cluster graphs will generally not provide the correct answers. 
> 本节讨论使用和精确推理完全相同的消息传递的近似方法
> 这些传递方法使用通用目的的簇图 (definition 10.1) 而不是团树，而因为定义了团树的约束 (是一颗树，且满足运行相交性质) 对于确保准确推理是不可缺少的，因此使用通用目的的簇图的消息传递方法一般不会提供正确的答案

We begin by defining the general message passing algorithm in a cluster graph. We then show that it can be derived, using the same process as in the previous section, from a set of fixed-point equations induced by the stationary points of an approximate energy functional. 
> 我们首先在簇图上定义广义的消息传递算法，然后，我们将展示它可以使用与前一节相同的过程，从由近似能量泛函的驻点导出的的固定点方程中推导出来

### 11.3.1 A Simple Example 
Consider the simple Markov network of figure 11.1a. Recall that, to perform exact inference within this network, we must first reduce it to a tree, such as the tree of figure 11.1b. Inference in this simple tree involves passing messages over the sepset, which consists of the variables $\{B,D\}$ . 
> 考虑 figure 11.1a 的 Markov 网络，回忆以下，要在该网络上执行精确推理，我们首先需要将它化简为一颗树，例如 figure 11.1b 的树，在该树上的推理包括了在由变量 $\{B, C\}$ 构成的分离集上传递消息

Now suppose that, instead, we perform inference as follows. We set up four clusters, which correspond to the four initial potentials: $C_{1}=\{A,B\}$ , $C_{2}=\{B,C\}$ , $C_{3}=\{C,D\}$ , $C_{4}=$ $\{A,D\}$ . We connect these clusters to each other as shown in the cluster graph of figure 11.1c. Note that this cluster graph contains loops (undirected cycles), and is therefore not a tree; such graphs are often called loopy . Nevertheless, we can apply the belief-update propagation algorithm CTree-BU-calibrate (algorithm 10.3). Although in our discussion of that algorithm we assumed that the input is a tree, there is nothing in the algorithm itself that relies on that fact. In each step of the algorithm we propagate a message between neighboring clusters. Thus, it is perfectly applicable to a general cluster graph that may not necessarily be a tree. 
> 现在假设我们执行推理如下：
> 我们设定四个簇 $\pmb C_1, \pmb C_2, \pmb C_3, \pmb C_4$，分别对应于四个初始势能
> 我们将这些簇相互连接，形成 figure 11.1c 中的簇图，注意该簇图包含了回路 (无向的环)，因此它不是一棵树
> 这种图一般称为有圈图
> 我们可以在该图上应用信念更新传播算法 CTree-BU-Clibrate (algorithm 10.3)，虽然在讨论该算法时，我们假定了输入是一棵树，但算法本身并不依赖于这一事实，在算法的每一步，我们只是在相邻的簇之间传播消息，因此，该算法完全可以应用在一个通用的簇图上

The clusters in this cluster graph are smaller than those in the clique tree of figure 11.1b; therefore, the message passing steps are less expensive. But what is the result of this procedure? Suppose we propagate messages in the following order $\mu_{1,2},\,\mu_{2,3},\,\mu_{3,4}$ , and then $\mu_{4,1}$ . In the first message, the $\{A,B\}$ cluster passes information to the $\{B,C\}$ cluster through a marginal distribution on B . This information is then propagated to next cluster, and so on. However, in the final message $\mu_{4,1}$ , this information reaches the original cluster, but this time as observation about the values of $A$ . As an example, suppose all clusters favor consensus joint assignments; that is, $\beta_{1}(a^{0},b^{0})$ and $\beta_{1}(a^{1},b^{1})$ are much larger than $\beta_{1}(a^{1},b^{0})$ and $\beta_{1}(a^{0},b^{1})$ , and similarly for the other beliefs. Thus, if the message $\mu_{1,2}$ strengthens the belief that $B\,=\,b^{1}$ , then the message $\mu_{2,3}$ will increase the belief in $C=c^{1}$ and so on. Once we get around the loop, the message $\mu_{4,1}$ will strengthen the support in $A=a^{1}$ . This message will be incorporated into the cluster as though it were independent evidence that did not depend on the initial propagation. Now, if we continue to apply the same sequence of propagations again, we will keep increasing the beliefs in the assignment of $A=a^{1}$ . This behavior is illustrated in figure 11.2. As we can see, in later iterations the procedure overestimates the marginal probability of $A$ . However, the efect of the “feedback” decays until the iterations converge.  
> figure 11.1c 中的簇比 figure11.1b 的团树中的簇要小，因此消息传递步骤的成本更低
> 但是，这个过程的结果是什么呢？假设我们按以下顺序传递消息：$\mu_{1,2}$、$\mu_{2,3}$、$\mu_{3,4}$，然后再传递 $\mu_{4,1}$。在第一个消息传递中，$\{A,B\}$ 簇通过关于 $B$ 的边缘分布向 $\{B,C\}$ 簇传递信息。这些信息随后被传递到下一个簇，依此类推，然而，在最终的消息传递 $\mu_{4,1}$ 中，这些信息又回到了原始簇，但这次是以关于 $A$ 取值的观察形式
> 举个例子说明：
> 假设所有簇都偏好共识联合分配；即，$\beta_{1}(a^{0},b^{0})$ 和 $\beta_{1}(a^{1},b^{1})$ 远大于 $\beta_{1}(a^{1},b^{0})$ 和 $\beta_{1}(a^{0},b^{1})$，并且其他信念也是如此，因此，如果消息 $\mu_{1,2}$ 加强了 $B=b^{1}$ 的信念，则消息 $\mu_{2,3}$ 将增加 $C=c^{1}$ 的信念，依此类推
> 一旦我们经过一次循环，消息 $\mu_{4,1}$ 将加强 $A=a^{1}$ 的支持，这条消息将被纳入簇中，就好像它是独立证据，不依赖于最初的传播一样
> 现在，如果我们继续应用相同的传播序列，我们将不断加强 $A=a^{1}$ 分配的信念，这种行为如 figure 11.2 所示
> 正如我们可以看到的，在后续的迭代中，该过程会高估 $A$ 的边缘概率，然而，“反馈”的影响会逐渐衰减直至迭代收敛

This simple experiment already suggests several important issues we need to consider: 
> 这个简单的实验已经提出了几个我们需要考虑的重要问题：

- In the case of cluster trees, we described a sequence of message propagations that calibrate the tree in two passes. Once the tree is calibrated, additional message propagations do not change any of the beliefs. Thus, we can say that the propagation process has converged . When we consider our example, it seems clear that **the process may not converge in two passes, since information from one pass will circulate and afect the next round. Indeed, it is far from clear that the propagation of beliefs necessarily converges at all.**
> 在簇树的情况下，我们知道一个消息传递的序列可以在两次传递中校准树
> 一旦树被校准，任何额外的消息传递都不会改变任何信念，因此，我们可以认为传播过程已经收敛
> 而我们刚才考虑的例子中，很明显该过程可能不会在两次传递中收敛，因为来自一次传递的信息会在下一轮中循环并影响结果，实际上，信念传播是否一定会收敛远非明确

- In the case of cluster trees, we saw that, in a calibrated tree, each cluster of beliefs is the joint marginal of the cluster variables. As our example suggests, for cluster graph propagation, the beliefs on $A$ are not necessarily the marginal probability in $P_{\Phi}$ . Thus, the question is the relationship between the calibrated cluster graph and the actual probability distribution. 
> 在簇树的情况下，我们发现，在校准的树中，每个信念簇是簇变量的联合边缘分布
> 在我们的刚才考虑的例子中，在簇图传播过程中 $A$ 上的信念不一定是在 $P_{\Phi}$ 中的边缘概率，因此，问题在于校准簇图与实际概率分布之间的关系

Before we address these questions, we present the algorithm in more general terms. 
> 在我们探讨这些问题之前，我们先用更广义的方式呈现算法

Box 11. A — Case Study: Turbocodes and loopy belief propagation. The idea of propagating messages in loopy graphs was first proposed in the early days of the field, in parallel with the introduction of the first exact inference algorithms. As we discussed in box 9. B, one of the first inference algorithms was Pearl’s message passing for singly connected Bayesian networks (polytrees). In his 1988 book, Pearl says: 
When loops are present, the network is no longer singly connected and local propagation schemes will invariably run into trouble . . . If we ignore the existence of loops and permit the nodes to continue communicating with each other as if the network were singly connected, messages may circulate indefinitely around the loops and the process may not converge to a stable equilibrium . . . Such oscillations do not normally occur in probabilistic networks . . . which tend to bring all messages to some stable equilibrium as time goes on. However, this asymptotic equilibrium is not coherent, in the sense that it does not represent the posterior probabilities of all nodes of the networks. 

As a consequence of these problems, the idea of loopy belief propagation was largely abandoned for many years. 
Surprisingly, the revival of loopy belief propagation is due to a seemingly unrelated advance in coding theory. The area of coding addresses the problem of sending messages over a noisy channel, and recovering it from the garbled result. Formally, the coding task can be defined as follows. We wish to send a $k$ -bit message $u_{1},\ldots,u_{k}$ . We code the message using a number of bits $x_{1},.\ldots,x_{n},$ , which are then sent over the noisy channel, resulting in a set of (possibly corrupted) outputs $y_{1},\dotsc,y_{n},$ which can be either discrete or continuous. Diferent channels introduce noise in diferent ways: In a simple Gaussian noise model, each bit sent is corrupted independently by the addition of some Gaussian noise; another simple model ﬂips each bit independently with some probability; more complex channel models, where noise is added in a correlated way to consecutive bits, are also used. The message decoding task is to recover an estimate $\hat{u_{1}},\dots,\hat{u_{k}}$ from $y_{1},\dotsc,y_{n}$ . The bit error rate is the probability that a bit is ultimately decoded incorrectly. This error rate depends on the code and decoding algorithm used and on the amount of noise in the channel. The rate of a code is $k/n$ — the ratio between the number of bits in the message and the number of bits used to transmit it. 
For example, a very simple repetition code takes each bit and transmits it three times, then decodes the bit by majority voting on the three (noisy) copies received. If the channel corrupts each bit with probability $p$ , the bit error rate of this algorithm is $p^{3}+3p^{2}$ , which, for reasonable values of p , is much lower than $p$ . The rate of this code is $1/3;$ , because for every message bit, three bits are transmitted. In general, we can get better bit error rates by increasing the redundancy of the code, so we want to compare the bit error rate of diferent codes that have the same rate. Repetition codes are some of the least efcient codes designed. Figure 11.A.1a shows a simple rate 4/7 parity check code, where every four message bits are sent along with three bits that encode parity checks (exclusive ORs) of diferent subsets of the four bits.  In 1948, Claude Shannon provided a theoretical analysis of the coding problem (Shannon 1948). For a given rate, Shannon provided an upper bound on the maximum noise level that can be tolerated while still achieving a certain bit error rate, no matter which code is used. Shannon also showed that there exist channel codes that achieve this limit, but his proof was nonconstructive — he did not present practical encoders and decoders that achieve this limit. 

Since Shannon’s landmark result, multiple codes were suggested. However, despite a gradual improvement in the quality of the code (bit-error rate for a given noise level), none of the codes even came close to the Shannon limit. The big breakthrough came in the early 1990s, when Berrou et al. (1993) came up with a new scheme that they called a turbocode , which, empirically, came much closer to achieving the Shannon limit than any other code proposed up to that point. However, their decoding algorithm had no theoretical justification, and, while it seemed to work well in real examples, could be made to diverge or converge to the wrong answer. The second big breakthrough was the subsequent realization that turbocodes were simply performing belief propagation on $a$ Bayesian network representing the probability model for the code and the channel noise. 

To understand this, we first observe that message decoding can easily be reformulated as a probabilistic inference task: We have a prior over the message bits $U=\left<U_{1},.\,.\,.\,,U_{k}\right>$ , a (usually deterministic) function that defines how a message is converted into a sequence of transmitted bits $X_{1},\dots,X_{n},$ and another (stochastic) model that defines how the channel randomly corrupts the $X_{i}\,{\stackrel{\prime}{s}}$ to produce $Y_{i}$ ’s. The decoding task can then be viewed as finding the most likely joint assignment to $U$ given the observed message bits ${\pmb y}\,=\,\langle y_{1},.\,.\,.\,,y_{n}\rangle$ , or (alternatively) as finding the posterior $P(U_{i}\mid\pmb{y})$ for each bit $U_{i}$ . The first task is a MAP inference task, and the second task one of computing posterior probabilities. Unfortunately, the probability distribution is of high dimension, and the network structure of the associated graphical model is quite densely connected and with many loops. 
The turbocode approach, as first proposed, comprised both a particular coding scheme, and the use of a message passing algorithm to decode it. The coding scheme transmits two sets of bits: one set comprises the original message bits $X^{a}\;=\;\left\langle X_{1}^{a},.\,.\,.\,,X_{k}^{a}\right\rangle\;=\;{\pmb u}$ ⟩ , and the second some set $X^{b}\,=\,\langle X_{1}^{b},.\,.\,.\,,X_{k}^{b}\rangle$ ⟩ of transformed bits (like the parity c its, but more complicated). The received bits then can also be partitioned into the noisy $\boldsymbol{y}^{a},\boldsymbol{y}^{b}$ . Importantly, the code is designed so that the message can be decoded (albeit with errors) using either $\boldsymbol{y}^{a}$ or $\boldsymbol{y}^{b}$ . The turbocoding algorithm then works as follows: It uses the model of $X^{a}$ (trivial in this case) and of the channel noise to compute a posterior probability over $U$ given $\boldsymbol{y}^{a}$ . It then uses that posterior $\pi_{a}(U_{1}),.\,.\,.\,,\pi_{a}(U_{k})$ as a prior over $U$ and computes a new posterior over $U$ , using the model for $X^{b}$ and the channel, and $\boldsymbol{y}^{b}$ as the evidence, to compute a new posterior $\pi_{b}(U_{1}),.\,.\,.\,,\pi_{b}(U_{k})$ . The “new information,” which is $\pi_{b}(U_{i})/\pi_{a}(U_{i})$ , is then transmitted back to the first decoder, and the process repeats until a stopping criterion is reached. In efect, the turbocoding idea was to use two weak coding schemes, but to “turbocharge” them using a feedback loop. Each decoder is used to decode one subset of received bits, generating a more informed distribution over the message bits to be subsequently updated by the other. The specific method proposed used particular coding scheme for the $X^{b}$ bits, illustrated in figure 11.A.1b. 
This process looked a lot like black magic, and in the beginning, many people did not even believe that the algorithm worked. However, when the empirical success of these properties was demonstrated conclusively, an attempt was made to understand its theoretical properties. McEliece et al. (1998) subsequently showed that the specific message passing procedure proposed by Berrou et al. is precisely an application of belief propagation (with a particular message passing schedule) to the Bayesian network representing the turbocode (as in figure 11.A.1b). 
This revelation had a tremendous impact on both the coding theory community and the graphical models community. For the former, loopy belief propagation provides a general-purpose algorithm for decoding a large family of codes. By separating the algorithmic question of decoding from the question of the code design, it allowed the development of many new coding schemes with improved properties. These codes have come much, much closer to the Shannon limit than any previous codes, and they have revolutionized both the theory and the practice of coding. For the graphical models community, it was the astounding success of loopy belief propagation for this application that led to the resurgence of interest in these approaches, and subsequently to much of the work described in this chapter. 

### 11.3.2 Cluster-Graph Belief Propagation 
The basis for our message passing algorithm is the cluster graph of definition 10.1, first defined in section 10.1.1. In that section, we required that cluster graphs be trees and that they respect the running intersection property. Those requirements led us to the definition of a clique tree. Here, we remove the first of these two assumptions, allowing inference to be performed on a loopy cluster graph. However, we still wish to require a variant of the running intersection property that is generalized to this case: for any two clusters containing $X$ , there is precisely one path between them over which information about $X$ can be propagated. 
> 我们的消息传递算法基于 definition 10.1 定义的簇图，而在那一节中，我们要求簇图是树，并且满足运行相交性质
> 我们将满足这两个要求使得的簇图定义为团树
> 本节中，我们将其中树的要求移除，允许在环状簇图中执行推理，但我们仍要求一个运行相交性质的变体，我们将该性质推广到：对于任意两个包含 $X$ 的簇，它们之间正好存在一条 $X$ 可以经由传播的路径

**Definition 11.2** running intersection property 
We say that $\mathcal{U}$ satisfies the running intersection property if, w ever there is a variable $X$ such that $X\in C_{i}$ and $X\in C_{j},$ , then there is a single path between $C_{i}$ and $C_{j}$ for which $X\in S_{e}$ for all edges $e$ in the path. 

> 定义
> 对于一个簇图 $\mathcal U$，如果一个变量 $X$ 满足 $X\in \pmb C_i$ 和 $X\in \pmb C_j$，则 $\pmb C_i$ 和 $\pmb C_j$ 之间会有恰好一条路径，满足对于路径中所有的边 $e$ 都有 $X\in \pmb S_e$，则称 $\mathcal U$ 满足运行相交性质

This generalized running intersection property implies that all edges associated with $X$ form a tree that spans all the clusters that contain $X$ . Thus, intuitively, there is only a single path by which information that is directly about $X$ can ﬂow in the graph. Both parts of this assumption are significant. The fact that some path must exist forces information about $X$ to ﬂow between all clusters that contain it, so that, in a calibrated cluster graph, all clusters must agree about the marginal distribution of $X$ . The fact that there is at most one path prevents information about $X$ from cycling endlessly in a loop, making our beliefs more extreme due to “cyclic arguments.” 
> 这一广义的运行相交性质意味着和 $X$ 相关的所有边构成了一颗树，树中包含了所有包含 $X$ 的簇，因此，直观上图中仅存在一条路径使得和 $X$ 直接相关的信息可流过
> 这个假设的两个方面都是重要的，必须存在某个路径这一事实迫使关于 $X$ 的信息在所有包含它的簇之间传递，所以，在校准的簇图中，所有簇都必须就 $X$ 的边缘分布达成一致；而最多只有一条路径的事实则防止了关于 $X$ 的信息在循环中无休止地循环，从而避免由于“循环论证”使我们的信念变得极端

Importantly, however, since the graph is not necessarily a tree, the same pair of clusters might also be connected by other paths. For example, in the cluster graph of figure 11.3a, we see that the edges labeled with $B$ form a subtree that spans all the clusters that contain $B$ . However, there are loops in the graph. For example, there are two paths fro ${C_{3}}=\left\{{B,D,F}\right\}$ to $C_{2}=\{B,C,D\}$ . The first, through $C_{4}$ propagates information about B , and the second, through $C_{5}$ , propagates information about D . Thus, we can still get circular reasoning, albeit less directly than we would in a graph that did not satisfy the running intersection property; we return to this point in section 11.3.8. Note that while in the case of trees the definition of running intersection implied that $S_{i,j}=C_{i}\cap C_{j}$ , in a gra this e lity is no longer enf ed by the running intersection property. For example, cliques $C_{1}$ and $C_{2}$ in figure 11.3a have B in common, but $S_{1,2}=\{C\}$ . 
> 重要的是，然而，由于图不一定是树结构，同一对簇也可能通过其他路径相连
> 例如，在 figure11.3a 的簇图中，我们可以看到标记为 $B$ 的边形成了一个子树，该子树覆盖了所有包含 $B$ 的簇；然而，图中存在回路
> 例如，从 ${\pmb C_{3}}=\{B,D,F\}$ 到 $\pmb C_{2}=\{B,C,D\}$ 有两个路径。第一个路径通过 $\pmb C_{4}$ 传播关于 $B$ 的信息，第二个路径通过 $\pmb C_{5}$ 传播关于 $D$ 的信息。因此，我们仍然可以得到循环推理，尽管不像在一个不满足运行交集性质的图中那样直接，我们在 11.3.8 节中会回到这一点
> 需要注意的是，在树的情况下，运行交集的定义暗示 $\pmb S_{i,j}=\pmb C_{i}\cap \pmb C_{j}$，但在一般图中，这种关系不再由运行交集性质强制。例如，在图11.3a中，簇 $\pmb C_{1}$ 和 $\pmb C_{2}$ 有 $B$ 共同存在，但 $\pmb S_{1,2}=\{C\}$

In clique trees, inference is performed by calibrating beliefs. In a cluster graph, we can also associate cluster $C_{i}$ with beliefs $\beta_{i}$ . We now say that a cluster graph is calibrated if for each calibrated cluster graph edge $(i{-}j)$ , connecting the clusters $C_{i}$ and $C_{j}$ , we have that 

$$
\sum_{\pmb C_{i}-\pmb S_{i,j}}\beta_{i}=\sum_{\pmb C_{j}-\pmb S_{i,j}}\beta_{j};
$$ 
that is, the two clusters agree on the marginal of variables in $\boldsymbol{S}_{i,j}$ . Note that this definition is weaker than cluster tree calibration, since the clusters do not necessarily agree on the joint marginal of all the variables they have in common, but only on those variables in the sepset. However, if a calibrated cluster graph satisfies the running intersection property, then the marginal of a variable $X$ is identical in all the clusters that contain it. 
> 团树中，推理是通过校准信念执行的；在簇图中，我们同样将簇 $\pmb C_i$ 和信念 $\beta_i$ 关联
> 一个簇图如果满足图中每个由边 $(i-j)$ 相连的簇 $\pmb C_i, \pmb C_j$ 在的信念其分离集上的边际是相同的，就称该簇图是校准的
> 该定义要弱于簇树校准，因为相邻的簇并不需要在它们所公有的所有变量上的边际保持一致，仅仅是在分离集中的变量
   如果校准的簇图满足运行相交性质，那么变量 $X$ 的边际在所有包含它的簇中就是一致的
 
How do we calibrate a cluster graph? Because calibration is a local property that relates adjoining clusters, we want to try to ensure that each cluster is sharing information with its neighbors. From the perspective of a single cluster $C_{i}$ , there is not much diference between a cluster graph and a cluster tree. The cluster is related to each neighboring cluster through an edge that conveys information on variables in the sepset. Thus, we can transmit information by simply having one cluster pass a message to the other. 
> 如何校准簇图？
> 校准性是一个局部性质，仅关联相邻的簇，故我们需要尝试保证每个簇都与其邻居共享信息
> 从单个簇 $\pmb C_i$ 的角度来看，簇图和簇树没有不同，簇都和每个邻居簇通过边相连，边表达了分离集中变量的信息，因此我们同样可以经由一个个簇传递消息

However, a priori, it is not clear how we can execute a message passing algorithm over a loopy cluster graph. In particular, the sum-product calibration of algorithm 10.2 sends a message only when the sending clique is ready to transmit, that is, when all other incoming messages have been received. In the loopy cluster graph, initially, there is no cluster that has received any incoming messages. Thus, no cluster is ready to transmit, and the algorithm is deadlocked. However, in section 10.3, we showed that the two algorithms are actually equivalent; that is, any sequence of sum-product propagation steps can be emulated by the same sequence of belief-update propagation steps and leads to the same beliefs. In this transformation, we have that $\mu_{i,j}=\delta_{i\to j}\delta_{j\to i}$ . Thus, we can construct a “deadlock-free” variant of the sum-product message passing algorithm simply by initializing all messages $\delta_{i\to j}=\mathbf{1}$ . This initialization of the sum-product algorithm is equivalent to the standard initialization of the belief update algorithm, in which $\mu_{i,j}=\mathbf{1}$ . Importantly, in this variant of the sum-product algorithm, each cluster begins with all of the incoming messages initialized, and therefore it can send any of the outgoing messages at any time, without waiting for any other cluster. 
> 然而，从先验的角度来看，不清楚我们如何可以在一个循环簇图上执行消息传递算法，特别是，algorithm 10.2 的和积校准方法仅在发送簇准备好传输时才发送消息，即当所有其他传入消息均已收到时。在循环簇图中，最初没有任何簇接收到传入消息，因此，没有簇准备好传输，算法陷入死锁状态
> 在10.3节中，我们展示了这和积校准方法和信念更新方法这两个算法实际上是等价的；也就是说，任何和积传播步骤序列都可以通过相同的一系列信念更新传播步骤来模拟，并且结果相同。在这个转换中，我们有 $\mu_{i,j}=\delta_{i\to j}\delta_{j\to i}$
> 因此，我们可以通过将所有消息初始化为 1，即 $\delta_{i\to j}=1$ ，来构造一个“无死锁”的和积消息传递算法变体。这种和积算法的初始化等同于信念更新算法的标准初始化，其中 $\mu_{i,j}={1}$。重要的是，在这种和积算法的变体中，每个簇开始时所有传入的消息都已经初始化，因此它可以随时发送任何传出消息，而不必等待其他簇

![[PGM-Algorithm11.1.png]]

Algorithm 11.1 shows the sum-product message passing algorithm for cluster graphs; other than the fact that the algorithm is applied to graphs rather than trees, the algorithm is identical to CTree-SP-Calibrate . In much the same manner, we can adapt CTree-BU-Calibrate to define a procedure CGraph-BU-Calibrate that operates over cluster graphs using belief-update message passing steps. Both of these algorithms are instances of a general class of algorithms called cluster-graph belief propagation , which passes messages over cluster graphs. 
> Algorithm 11.1 展示了簇图上的和积消息传递算法；除了该算法应用于图形而非树之外，该算法与CTree-SP-Calibrate算法相同
> 同样地，我们可以将 CTree-BU-Calibrate 调整，以定义一个称为 CGraph-BU-Calibrate 的过程，它使用信念更新消息传递步骤在簇图上操作
> 这两种算法都是一个称为簇图信念传播的通用算法类的实例，该算法类在簇图上传递消息

Before we continue, we note that cluster-graph belief propagation can be significantly cheaper than performing exact inference. A canonical example of a class of networks that is compactly representable yet hard for inference is the class of grid-structured Markov networks (such as the ones used in image analysis; see box 4. B). In these networks, each variable $A_{i,j}$ corresponds to a point on a two-dimensional grid. Each edge in this network corresponds to a potential between adjacent points on the grid, with $A_{i,j}$ connected to the four nodes $A_{i-1,j}$ , $A_{i+1,j}$ ,  $A_{i,j-1}$ , $A_{i,j+1}$ (except for nodes $A_{i,j}$ on the boundary of the grid); see figure 11.4. Such a network has only pairwise potentials, and hence it is very compactly represented. Yet, exact inference requires separating sets, which are as large as cutsets in the grid. Hence, in an $n\times n$ grid, exact computation is exponential in $n$ . 
> 在继续之前，要注意簇图信念传播可以显著比执行精确推断更经济
> 一类网络的例子是紧凑表示但对推断困难的网络，例如网格结构的马尔可夫网人(如用于图像分析的网络；参见框4. B)，在这些网络中，每个变量 $A_{i,j}$ 对应于二维网格上的一个点。该网络中的每条边对应于网格上相邻点之间的势能函数，其中 $A_{i,j}$ 连接到四个节点 $A_{i-1,j}$、$A_{i+1,j}$、$A_{i,j-1}$、$A_{i,j+1}$  (边界上的节点 $A_{i,j}$ 除外)；参见 figure 11.4。这样的网络只有成对的二元势函数，因此表示十分紧凑
> 然而，精确推断需要分离集合，其大小与网格中的割集一样大，因此，在 $n \times n$ 网格中，精确计算的时间复杂度是关于 $n$ 的指数级

However, we can easily create a generalized cluster graph for grid networks that directly corresponds to the factors in the network. In this cluster graph, each cluster represents beliefs over two neighboring grid variables, and each cluster has a small number of adjoining edges that connect it to other clusters that share one of the two variables. See figure 11.5 for an example for a small $3\times3$ grid. (Note that there are several ways of constructing such a cluster graph; this figure represents one reasonable choice.) A round of propagations in the generalized cluster graph is linear in the size of the grid (quadratic in $n$ ). 
> 然而，我们可以很容易地为网格网络创建一个广义的簇图，该簇图直接对应于网络中的因子。在这个簇图中，每个簇代表两个相邻网格变量的上的信念，并且每个簇有少量相邻的边将其连接到共享这两个变量之一的其他簇
> 图11.5展示了一个小型 $3 \times 3$ 网格的示例。（注意，构建这样的簇图有多种方法；此图代表一种合理的选择），广义簇图中一轮传播的计算量与网格的大小线性相关（在 $n$ 的平方级别）

### 11.3.3 Properties of Cluster-Graph Belief Propagation 
What can we say about the properties and guarantees provided by cluster-graph belief propagation? We now consider some of the ramifications of the “mechanical” operation of message passing in the graph. Later, when we discuss cluster-graph belief propagation as an optimization procedure, we will revisit this question from a diferent perspective. 

#### 11.3.3.1 Reparameterization 
Recall that in section 10.2.3 we showed that belief propagation maintains an invariant property. This allowed us to show that the convergence point represents a reparameterization of the original distribution. We can directly extend this property to cluster graphs, resulting in a cluster graph invariant . 
> section 10.2.3 中，我们证明了信念传播维护了一个不变性，该不变性最后引出的结论是算法的收敛点本质是原分布的一个重参数化
> 该性质可以延伸到簇图，即簇图不变性

**Theorem 11.4**
Let $\mathcal{U}$ be a generalized cluster graph over a set of factors $\Phi$ . Consider the set of beliefs $\{\beta_{i}\}$ and sepsets $\{\mu_{i,j}\}$ at any iteration of CGraph-BU-Calibrate ; then

$$
\tilde{P}_{\Phi}(\mathcal{X})=\frac{\prod_{i\in\mathcal{V}_{\mathcal{U}}}\beta_{i}[\pmb C_{i}]}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{U}}}\mu_{i,j}[\pmb S_{i,j}]}.
$$ 
where $\begin{array}{r}{\tilde{P}_{\Phi}(\mathcal{X})=\prod_{\phi\in\Phi}\phi}\end{array}$ is the unnormalized distribution defined by $\Phi$ . 
> 定理
> $\mathcal U$ 为因子集合 $\Phi$ 上的推广的簇图，算法 CGraph-BU-Calibrate 中的任意迭代中，由 $\Phi$ 定义的未规范化的分布 $\tilde P(\mathcal X) = \prod_{\phi \in \Phi}\phi$ 都满足以上的式子

Proof Recall that $\begin{array}{r}{\beta_{i}=\psi_{i}\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\rightarrow i}}\end{array}$ and that $\mu_{i,j}=\delta_{j\to i}\delta_{i\to j}$ . We now have

$$\begin{array}{r c l}{\displaystyle\frac{\prod_{i\in\mathcal{V}_{\mathcal{U}}}\beta_{i}[\pmb C_{i}]}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{U}}}\mu_{i,j}[\pmb S_{i,j}]}}&{=}&{\displaystyle\frac{\prod_{i\in\mathcal{V}_{\mathcal{U}}}\psi_{i}[\pmb C_{i}]\prod_{j\in\mathrm{Nb}_{i}}\delta_{j\to i}[\pmb S_{i,j}]}{\prod_{(i-j)\in\mathcal{E}_{\mathcal{U}}}\delta_{j\to i}[\pmb S_{i,j}]\delta_{i\to j}[\pmb S_{i,j}]}}\\ &{=}&{\displaystyle\prod_{i\in\mathcal{V}_{\mathcal{U}}}\psi_{i}[\pmb C_{i}]}\\ &{=}&{\displaystyle\prod_{\phi\in\Phi}\phi(\boldsymbol{U}_{\phi})=\tilde{P}_{\Phi}(\mathcal{X}).}\end{array}$$

Note that the second step is based on the fact that each message $\delta_{i\to j}$ appears exactly once in the numerator and the denominator and thus can be canceled. 

**This property shows that cluster-graph belief propagation preserves all of the information about the original distribution. In particular, it does not “dilute” the original factors by performing propagation along loops. Hence, we can view the process as trying to represent the original factors anew in a more useful form.** 
> 该性质说明了簇图信念传播保持了原分布的所有信息，特别地，它不会通过在环路上进行传播而“稀释”原始因子
> 因此，我们可以将这个过程视为试图以更有用的形式重新表示原始因子

#### 11.3.3.2 Tree Consistency 
Recall that theorem 10.4 implies that, in a calibrated cluster tree, the belief over a cluster is the marginal of the distribution. Thus, in a calibrated cluster tree, we can “read of” the marginals of $P_{\Phi}$ locally from clusters that contain them. More precisely, by normalizing the beliefs factor $\beta_{i}$ (so that it sums to 1), we get the marginal distribution over $C_{i}$ . An obvious question is whether a corresponding property holds for cluster-graph belief propagation. Suppose we manage to calibrate a generalized cluster graph and normalize the resulting beliefs; do we have an interpretation for the beliefs in each cluster? 
> 在校准的簇树中，每个簇上的(规范化的)信念 $\beta_i$ 都是一个 $\pmb C_i$ 上的边际 
> 我们考虑簇图是否有类似的性质

As we saw in our simple example (figure 11.2), the beliefs we compute by BU-message are not necessarily marginals of $P_{\Phi}$ , but rather an approximation. Can we say anything about the quality of this approximation? To characterize the beliefs we get at the end of the process, we can use the cluster tree invariant property applied to subtrees of a cluster graph. 
> 我们知道 BU-message 计算的信念并不是 $P_\Phi$ 的边际，而是对其的近似
> 能否确定该近似的质量？
> 为了确定我们最后得到的信念的性质，我们将簇树不变性应用到簇图中的一个子树

Consider a subtree $\mathcal{T}$ of $\mathcal{U}$ ; that is, a subset of clusters and edges that together form a tree that satisfies the running intersection property. For example, consider the cluster graph of figure 11.1c. If we remove one of the clusters and its incident edges, we are left with a proper cluster tree. Note that the running intersection property is not necessarily as easy to achieve in general, since removing some edges from the cluster graph may result in a graph that violates the running intersection property relative to a variable, necessitating the removal of additional edges, and so on. 
> 考虑 $\mathcal U$ 中的一个簇树 $\mathcal T$，它满足运行相交性质

Once we select a tree $\mathcal{T}$ , we can think of it as defining a distribution 
> 可以认为 $\mathcal T$ 定义了一个分布如下

$$
P_{\mathcal T}(\mathcal X)=\frac{\prod_{i\in\mathcal\nu_{\mathcal T}}\beta_{i}(\pmb C_{i})}{\prod_{(i-j)\in\mathcal E_{\mathcal T}}\mu_{i,j}[\pmb S_{i,j}]}.
$$ 
If the cluster graph is calibrated, then by definition so is $\mathcal{T}$ . And so, because $\mathcal{T}$ is a tree that satisfies the running intersection property, we can apply theorem 10.4, and we conclude that 
> 如果簇图是校准的，则树也是校准的，则根据定理 10.4，有

$$
\beta_{i}(\pmb C_{i})=P_{\mathcal{T}}(\pmb C_{i}).\tag{11.11}
$$  
That is, the beliefs over $C_{i}$ in the tree are the marginal of $P_{\mathcal{T}}$ , a property called tree consistency . 
> 即 $\pmb C_i$ 上的信念是 $P_{\mathcal T}$ 的边际，该性质称为树一致性

As a concrete example, consider the cluster graph of figure 11.1c. Removing the cluster $C_{4}=\{A,D\}$ , we are left with a proper cluster tree $\mathcal{T}$ . The preceding argument implies that once we have calibrated the cluster graph, we have $\beta_{1}(A,B)=P\tau(A,B)$ . This result suggests that $\beta_{1}(A,B)\,\neq\,P_{\Phi}(A,B)$ ; to show this formally, contrast equation (11.11) with theorem 11.4. We see that the tree distribution involves some of the terms that define the joint distribution. Thus, we can conclude that 

$$
P_{\mathcal{T}}(A,B,C,D)=P_{\Phi}(A,B,C,D)\frac{\mu_{3,4}[D]\mu_{1,4}[A]}{\beta_{4}(A,D)}.
$$ 
We see that unless $\beta_{4}(A,D)=\mu_{3,4}[D]\mu_{1,4}[A]$ , $P_{\mathcal{T}}$ will be diferent from $P_{\Phi}$ . This conclusion suggests that, in this example, the beliefs $\beta_{1}(A,B)$ in the calibrated cluster graph are not the marginal $P_{\Phi}(A,B)$ . 
> 但 $P_{\mathcal T}$ 并不等于 $P_\Phi$，$\mathcal T$ 往往由 $\mathcal U$ 移除某个点以及和它相关的边得到，因此根据定理 11.4，$P_\Phi$ 和 $P_{\mathcal T}$ 之间往往还存在类似上式的计算关系
> 因此，在校准的簇图中，信念 $\beta_i$ 是 $P_{\mathcal T}$ 的边际，却不是 $P_\Phi$ 的边际

Clearly, we can apply the same type of reasoning using other subtrees of $\mathcal{U}$ . And so we reach the surprising conclusion that equation (11.11) must hold with respect to every cluster tree embedded in $\mathcal{U}$ . In our example, we n see that by removing a single cluster, we can construct three diferent trees that contain $C_{1}$ . The same beliefs $\beta_{1}(A,B)$ are the marginal of the three distributions defined by each of these trees. While these three distributions agree on the joint marginal of $A$ and $B$ , they can difer on the joint marginal distributions of other pairs of variables. 
> 对于 $\mathcal U$ 中的每个子树，eq (11.11) 都会成立，也就是 $P_{\mathcal T}$ 在 $\pmb C_i$ 上的边际会等于 $\pmb C_i$ 的信念，尽管不同的 $\mathcal T$ 会定义不同的 $P_{\mathcal T}$，但它们在边际上达成一致

Moreover, these subtrees allow us to get insight about the quality of the marginal distributions we read from the calibrated cluster graph. Consider our example again: we can use the residual term $\frac{\mu_{3,4}[D]\mu_{1,4}[A]}{\beta_{4}(A,D)}$ to analyze the error in the marginal distribution. In this simple example, this analysis is fairly straightforward (see exercise 11.4). 
> 从校准的簇图中得到的信念相对于 $P_\Phi$ 中的边际有差异，其差异就类似于上个例子中的残差项 $\frac{\mu_{3,4}[D]\mu_{1,4}[A]}{\beta_{4}(A,D)}$ ，故我们使用残差项分析该误差

In other cases, the analysis can be more complex. For example, suppose we want to find a subtree in the cluster graph for a grid (e.g., figure 11.5). To construct a tree, we must remove a nontrivial number of clusters. More precisely, because each cluster corresponds to an edge in the grid, a cl ee corresponds to a subtree of the grid. For an $n\times n$ grid, such a tree will have at most $n^{2}-1$ − edges of the $2n(n-1)$ edges in the grid. Thus, each cluster tree contains about half of the clusters in the original cluster graph. In such a situation the residual term is more complex, and we cannot necessarily evaluate it. 
> 在其他情况下，分析可能会更加复杂。例如，假设我们要在簇图中找到一个子树（例如，图11.5）。为了构建一棵树，我们必须移除一定数量的簇。更精确地说，因为每个簇对应于网格中的一条边，所以一个簇树对应于网格的一个子树。对于一个 $n \times n$ 的网格，这样的树最多将包含 $2n(n-1)$ 条边中的 $n^2-1$ 条边。因此，每个簇树大约包含原簇图中一半的簇。在这种情况下，残差项更为复杂，我们不能总是对其进行评估。

### 11.3.4 Analyzing Convergence\*
A key question regarding the belief propagation algorithm is whether and when it converges. Indeed, there are many networks for which belief propagation does not converge; see box 11.C. 
Although we cannot hope for convergence in all cases, it is important to understand when this algorithm does converge. We know that if the cluster graph is a tree then the algorithm will converge. Can we find other classes of cluster graphs for which we can prove convergence? 

One method of analyzing convergence is based on the following important perspective on belief propagation. This analysis is easier to perform on a variant of BP called synchronous $B P$ that performs all of the message updates simultaneously. Consider the update step that takes all of the messages $\delta^{t}$ at a particular iteration $t$ and produces a new set of messages $\delta^{t+1}$ for the next step. Letting $\Delta$ be the space of all possible messages in the cluster graph, we can view the belief-propagation update operator as a function $G_{B P}\ :\ \Delta\mapsto\Delta$ 7→ . Consider the standard sum-product message update: 

$$
\delta_{i\rightarrow j}^{\prime}\propto\sum_{C_{i}-S_{i,j}}\psi_{i}\cdot\prod_{k\in(\mathrm{Nb}_{i}-\{j\})}\delta_{k\rightarrow i},
$$ 
where we normalize each message to sum to 1; this renormalization step is essential to avoid a degenerate convergence to the 0 message. We can now define the $B P$ operator as the function that simultaneously takes one set of messages and computes a new one: 

$$
G_{B P}(\{\delta_{i\to j}\})=\{\delta_{i\to j}^{\prime}\}.
$$ 
The question of convergence of the algorithm now reduces to one of asking whether repeated applications of the operator $G_{B P}$ are guaranteed to converge. 
One interesting, albeit strong, condition that guarantees convergence is the contraction prop- erty : 
Definition 11.3 contraction 
For a number $\alpha\in[0,1)$ operator $G$ ric space $(\Delta,D(;))$ is an $\alpha$ - contraction relative to the distance function I $D(;)$ if, for any δ $\delta,\delta^{\prime}\in\Delta$ , we have that: 

$$
D(G(\delta);G(\delta^{\prime}))\leq\alpha D(\delta;\delta^{\prime}).
$$ 
In other words, an operator is a contraction if its application to two points in the space is guaranteed to decrease the distance between them by at least some constant factor $\alpha<1$ . 
A basic result in analysis shows that, under fairly weak conditions, if an operator $G$ is a contraction, we have that repeated applications of $G$ are guaranteed to converge to a unique fixed point: 
Proposition 11.2 fixed-point 
Let $G$ be an $\alpha$ -contraction of a complete metric space $(\Delta,D(;))$ . Then there is a unique fixed-point $\delta^{*}$ for which $G(\delta^{*})=\delta^{*}$ . Moreover, for any $\delta$ , we have that 

$$
\operatorname*{lim}_{n\to\infty}G^{n}(\delta)=\delta^{*}.
$$ 
The proof is left as an exercise (exercise 11.5). 
Indeed, the contraction rate $\alpha$ can be used to provide bounds on the rate of convergence of the algorithm to its unique fixed point: To reach a point that is guaranteed to be within $\epsilon$ of $\delta^{*}$ , it sufces to apply $G$ the following number of times: 

$$
\log_{\alpha}{\frac{\epsilon}{\mathrm{diameter}(\Delta)}},
$$ 
where diameter $\begin{array}{r}{\cdot(\Delta)=\operatorname*{max}_{\delta,\delta^{\prime}\in\Delta}D(\delta;\delta^{\prime})}\end{array}$ . 
Applying this analysis to the operator $G$ induced by the belief-propagation message update is far from trivial. This operator is complex and nonlinear, because it involves both multiplying messages and a renormalization step. A review of these analyses is outside the scope of this book. At a high level, these results show that if the factors in the network are fairly “smooth,” one can guarantee that the synchronous BP operator is a contraction and hence converges to a unique fixed point. We describe one of the simplest of these results, in order to give a ﬂavor for this type of analysis. 
This analysis applies to synchronous loopy belief propagation over a pairwise Markov network with two-valued random variables $X_{i}\,\in\,\{-1,+1\}\quad$ . Specifically, we assume that the network model is parameterized as follows:

$$
P(x_{1},.\,.\,,x_{n})=\frac{1}{Z}\exp\left(\sum_{(i,j)}\epsilon_{i,j}(x_{i},x_{j})+\sum_{i}\epsilon_{i}(x_{i}),\right),
$$ 
where we assume for simplicity of notation that $\epsilon_{i,j}=0$ when $X_{i}$ and $X_{j}$ are not neighbors in the network. 
hyperbolic tangent 
We begin by introducing some notation. The hyperbolic tangent function is defined as: 

$$
\operatorname{tanh}(w)={\frac{e^{w}-e^{-w}}{e^{w}+e^{-w}}}={\frac{e^{2w}-1}{e^{2w}+1}}.
$$ 
The hyperbolic tangent has a shape very similar to the sigmoid function of figure 5.11a. The following condition can be shown to sufce for $G_{B P}$ to be a contraction, and hence for the convergence of belief propagation to a unique fixed point: 

$$
\operatorname*{max}_{i}\operatorname*{max}_{j\in\mathrm{Nb}_{i}}\sum_{k\in\mathrm{Nb}_{i}-\{j\}}\operatorname{tanh}{|\epsilon_{k,i}|}<1.
$$ 
Intuitively, this expression measures the total extent to which $i$ ’s neighbors other than $j$ can inﬂuence the message from $i$ to $j$ . The larger the magnitude of the parameters in the network, the larger this sum. 
The analysis of the more general case is significantly more complex but shares the same intuitions. At a very high level, if we can place strong bounds on the skew of the parameters in a factor: 

$$
\operatorname*{max}_{\pmb{x},\pmb{x}^{\prime}}\phi(\pmb{x})/\phi(\pmb{x}^{\prime}),
$$ 
we can guarantee convergence of belief propagation. Intuitively, the lower the skew of the factors in our network, the more each message update “smoothes out” diferences between entries in the messages, and therefore also makes diferent messages more similar to each other. 

While the conditions that underlie these theorems are usually too stringent to hold in practice, this analysis does provide useful insight. First, it suggests that networks with potentials that are closer to deterministic are more likely to have problems with convergence, an observation that certainly holds in practice. Second, although global contraction throughout the space is a very strong assumption, a contraction property in a region of the space may be plausible, guaranteeing convergence of the algorithm if it winds up (or is initialized) in this region. These results and their ramifications are only now being explored. 

### 11.3.5 Constructing Cluster Graphs 
So far, we have taken the cluster graph to be given. However, the choice of cluster graph is generally far from obvious, and it can make a significant diference to the algorithm. Recall that, even in exact inference, more than one clique tree can be used to perform inference for a given distribution. However, while these diferent trees can vary in their computational cost, they all give rise to the same answers. **In the case of cluster graph approximations, diferent graphs can lead to very diferent answers. Thus, when selecting a cluster graph, we have to consider trade-ofs between cost and accuracy, since cluster graphs that allow fast propagation might result in a poor approximation.** 
> 精确推理中，对于给定的分布，我们可以使用不同的团树进行推理，其计算开销不同，但最后结果相同
> 近似推理中，因为使用簇图推理也只是得到近似解，因此不同的图会导致不同的答案，我们在选择时需要考虑开销和准确率之间的 trade-off，可以快速传播的图可能近似效果很差

It is important to keep in mind that the structure of the cluster graph determines the propagation steps the algorithm can perform, and thus dictate what type of information is passed during the propagations. These choices directly inﬂuence the quality of the results. 
> 簇图的结构决定了算法执行传播的步骤，因此决定了在传播中哪些类型的信息会被传递，这会直接影响结果的质量

Example 11.1 
Consider, for examp the cluster graphs $\mathcal{U}_{1}$ $\mathcal{U}_{2}$ of figure 11.3a and figure 11.3b. Both a fairl imilar, yet in $\mathcal{U}_{2}$ the ed between $C_{1}$ and $C_{2}$ involves the mar al distribution over B and C . On the other hand, in $\mathcal{U}_{1}$ , we propagate the margin only er C . Intuitively, we expect inference in $\mathcal{U}_{2}$ to better capture the dependencies between $B$ and $C$ . For , assu that the potential of $C_{1}$ intro es strong cor ations between B d C (say $B\,=\,C.$ ). In U $\mathcal{U}_{2}$ , this correlation is conveyed $C_{2}$ directly. In $\mathcal{U}_{1}$ , t marginal on C is conveyed on the edge (1 – 2) , while the marginal on B is conveyed through $C_{4}$ . In this case, the strong dependency between the two variables is lost. In particular, if the marginal on $C$ is difuse (close to uniform), then the message $C_{1}$ sends to $C_{4}$ will also have a uniform distribution on $B$ , and from $C_{2}\mathit{\dot{s}}$ perspective the messages on $B$ and $C$ will appear as two independent variables. 

On the other hand, if we introduce many messages between clusters or increase the scope of these messages, we run the risk of constructing a tree that violates the running intersection property. And so, we have to worry about methods that ensure that the resulting structure is a proper cluster graph. We now consider several approaches for constructing cluster graphs.
> 另一方面，如果我们引入许多簇之间的消息或者增加这些消息的范围，我们就有可能构建一个违反运行相交属性的树
> 因此，我们需要关注确保最终结构是一个合适的簇图的方法，我们现在考虑几种构建簇图的方法

#### 11.3.5.1 Pairwise Markov Networks 
We start with the class of pairwise Markov networks . In these networks, we have a univariate potential $\phi_{i}[X_{i}]$ over each variable $X_{i}$ , and in addition a pairwise potential $\phi_{(i, j)}[X_{i}, X_{j}]$ over some pairs of variables. These pairwise potentials correspond to edges in the Markov network. Many problems are naturally formulated as pairwise Markov networks, including the grid networks we discussed earlier and Boltzmann distributions (see box 4. C). Indeed, if we are willing to transform our variables, any distribution can be reformulated as a pairwise Markov network (see exercise 11.10). 
> 考虑成对 Markov 网络，这类网络在每个变量 $X$ 上有单变量势能 $\phi_i[X_i]$，在某些变量上有成对势能 $\phi_{(i, j)}[X_i, X_j]$，这些成对势能对应于 Markov 网络中的边
> 许多问题自然构成了成对 Markov 网络，包括网格网络和 Boltzmann 分布
> 如果我们愿意转化我们的变量，任意分布都可以重构为一个成对 Markov 网络

One straightforward transformation of such a network into a cluster graph is as follows: For each potential, we introduce a corresponding cluster, and put edges between the clusters that have overlapping scope. In other words, there is an edge between the cluster $\pmb C_{(i, j)}$ that corresponds to the edge $X_{i}{-}X_{j}$ and the clusters $\pmb C_{i}$ and $\pmb C_{j}$ that correspond to the univariate factors over $X_{i}$ and $X_{j}$ . Figure 11.6 illustrates this construction in the case of a 3 by 3 grid network. 
> 将这类网络转化为簇图的一个直接方式就是将为每个势能引入一个对应的簇，然后在具有相交作用域的簇之间添加边
> 换句话说，簇 $\pmb C_{(i, j)}$ 对应于边 $X_i - X_j$，簇 $\pmb C_i$ 和 $\pmb C_j$ 对应于 $X_i, X_j$ 上的单变量势能，$\pmb C_{(i, j)}$ 和 $\pmb C_i, \pmb C_j$ 相连

Because there is a direct correspondence between the clusters in the cluster graphs and variables or edges in the original Markov network, it is often convenient to think of the propagation steps as operations on the original network. Moreover, since each pairwise cluster has only two neighbors, we consider two propagation steps along the path $C_{i}{-}C_{(i, j)}{-}C_{j}$ as propagating information between $X_{i}$ and $X_{j}$ . (See exercise 11.9.) Indeed, early versions of cluster-graph belief propagation were stated in these terms. This algorithm is known as loopy belief propagation , since it uses propagation steps used by algorithms for Markov trees, except that it was applied to networks with loops. 
> 这样构建的簇图中的簇和原来的 Markov 网络中的变量或边具有直接的对应关系，因此可以将传播步骤直接认为是在原来网络中的操作
> 并且，因为每个成对簇仅有两个邻居，我们可以将沿着路径 $\pmb C_i - \pmb C_{(i, j)} - \pmb C_j$ 之间的两次传播步骤视为在 $X_i$ 和 $X_j$ 之间传播信息
> 事实上，早期的簇图信念传播算法就是以这种方式表述的，该算法被称为循环信念传播，因为它将用于马尔可夫树算法的传播步骤应用于存在循环的网络中

#### 11.3.5.2 Bethe Cluster Graph 
A natural question is how we can extend this idea to networks that are more complex than pairwise Markov networks. Once we have larger potentials, they may overlap in ways that result in complex interactions among them. 
Bethe cluster graph 
> 考虑如何将从成对 Markov 网络构建簇图的思想拓展到更复杂的网络

One simple construction, called the Bethe cluster graph , uses a bipartite graph. The first layer consists of “large” clusters, with one cluster for each factor $\phi$ in $\Phi$ , whose scope is $S c o p e[\phi]$ . These clusters ensure that we satisfy the family-preservation property. The second layer consists of “small” univariate clusters, one for each random variable. Finally, we place an edge between each univariate cluster $X$ on the second layer and each cluster in the first layer that includes $X$ ; the scope of this edge is $X$ itself. For a concrete example, see figure 11.7a. 
> Bethe 簇图使用二部图，其第一层由 “大” 簇组成，每个簇对应于 $\Phi$ 中的一个因子 $\phi$，其作用域为 ${Scope}[\phi]$，这些簇保证我们构建的图满足族保持性质
> 二部图的第二层由 “小” 的单变量簇构成，每个簇对应于一个随机变量，最后，如果第一层的簇关联的因子包含了第二层的簇关联的变量，就将二者相连，相连的边的作用域就是变量 $X$ 本身

We can easily verify that this cluster graph is a proper one. First, by construction, it satisfies the family preservation property. Second, the edges that mention a variable $X$ form a star-shaped subgraph with edges from the univariate cluster for $X$ to all the large clusters that contain $X$ . It is also easy to check that, if we apply this procedure to a pairwise Markov network, it results in the “natural” cluster graph for the pairwise network that we discussed. The construction of this cluster graph is simple and can easily be automated. 
> 我们可以轻松验证这个簇图是有效的。首先，根据构造，它满足族保持性质。其次，提到变量 $X$ 的边形成一个星形子图，该子图由 $X$ 的单变量簇到所有包含 $X$ 的大簇之间的边组成
> 同样容易检查的是，如果我们将此过程应用于成对的马尔可夫网络，它将产生我们之前讨论的成对网络的“自然”簇图
> 这种簇图的构造方式简单，可以轻松自动化实现
 
#### 11.3.5.3 Beyond Marginal Probabilities 
The main limitation of using the Bethe cluster graph is that information between different clusters in the top level is passed through univariate marginal distributions. Thus, interactions between variables are lost during propagations. Consider the example of figure 11.7a. Suppose that $C_{1}$ creates a strong dependency between $B$ and $C$ . These two variables are shared with $C_{2}$ . However, the messages between two clusters are mediated through the univariate factors. And thus, interactions introduced by one cluster are not directly propagated to the other. 
> Bethe 簇图的主要限制在于不同簇之间的信息在顶层是通过单变量的边际分布(间接)传递的，故变量之间的交互在传播时丢失

One possible solution is to merge some of the large clusters. For example, if we want to capture the interactions between $C_{1}$ and $C_{2}$ in figure 11.7a, we can replace both of them by a cluster with the score $A, B, C, D$ . This new cluster will allow us to capture the interactions between the factors involved in these two clusters. This modification, however, comes at a price, since the cost of manipulating a cluster grows exponentially with this scope. Moreover, this approach seems excessive in this case, since we can summarize these interactions simply using a distribution over $B$ and $C$ . This intuition suggests the construction of figure 11.7b. Note that this cluster graph is equivalent to figure 11.3b; see exercise 11.6. 
> 一个可能的解决方法是合并一些 “大” 的簇，合并后的簇可以捕获其包含的簇相关的因子之间的交互
> 但合并会增大簇的作用域，而操纵簇的开销是随着簇作用域大小指数增长的

Can we generalize this construction? A reasonable goal might be to capture all pairwise interactions. We can try to use a construction similar to the Bethe approximation, but introducing an intermediate level that includes pairwise clusters. In the same manner as we introduced $C_{12}$ in figure 11.7b, we can introduce other pairs that are shared by more than two clusters. As a concrete example, consider the factors $C_{1}=\{A, B, C\}$ , $C_{2}=\{B, C, D\}$ , and ${C_{3}}=\left\{{A, C, D}\right\}$ . The relevant pairwise factors that capture interactions among these clusters are $\{B, C\}=C_{1}\cap C_{2}$ , $\{C, D\}=C_{2}\cap C_{3}$ , and $\{A, C\}=C_{1}\cap C_{3}$ . The resulting cluster graph appears in figure 11.8a. Unfortunately, a quick check shows that this cluster graph does not satisfy the running intersection property — all the edges in this graph are labeled by $C$ , and together they form a loop. As a result, information concerning $C$ can propagate indefinitely around the loop, “overcounting” the efect of $C$ in the result. 
> 我们可以推广这种构造吗？一个合理的目标可能是捕捉所有的成对交互作用
> 我们可以尝试使用类似于 Bethe 近似的构造，但引入一个包含成对簇的中间层级。就像我们在图11.7b 中引入 $C_{12}$ 一样，我们也可以引入由超过两个簇共享的其他成对簇
> 以具体的例子来说，考虑因子 $C_{1}=\{A, B, C\}$ ，$C_{2}=\{B, C, D\}$ 和 $C_{3}=\{A, C, D\}$。这些簇之间相关联的成对因子是 $\{B, C\}=C_{1}\cap C_{2}$，$\{C, D\}=C_{2}\cap C_{3}$ 和 $\{A, C\}=C_{1}\cap C_{3}$。所得到的聚类图如图11.8a 所示
> 不幸的是，快速检查显示该聚类图并不满足运行相交性质——图中的所有边都标记为 $C$，并且它们共同形成一个环路。因此，关于 $C$ 的信息可以在环路上无限传播，“在结果中过度计算” $C$ 的影响

How do we avoid this problem? In this specific example, we can consider a weaker approximation by removing $C$ from one of the intersection sets. For example, if we remove $C$ from $C_{5}$ , we get the cluster graph of figure 11.8b. This cluster graph satisfies the running intersection property. An alternative approach tries to “compensate” somehow for the violation of the run- ning intersection property using a more complex message passing algorithm; see section 11.3.7.3. 
> 我们如何避免这个问题？在这个特定的例子中，我们可以通过从一个交集中移除 $C$ 来考虑一个较弱的近似方法。例如，如果我们从 $C_{5}$ 中移除 $C$，我们得到图11.8b中的簇图。此簇图满足运行相交性质。另一种方法试图通过更复杂的传信算法来“补偿”违反运行相交性质的问题；参见第11.3.7.3节。

Box 11. B — Skill: Making loopy belief propagation work in practice. One of the main prob- lems with loopy belief propagation is nonconvergence . This problem is particularly serious when we build systems that use inference as a subroutine within other tasks, for example, as the inner loop of a learning algorithm (see, for example, section 20.5.1). Several approaches have been used for addressing this nonconvergence issue. Some are fairly simple heuristics. Others are more so- phisticated, and typically are based on the characterization of cluster-graph belief propagation as optimizing the approximate free-energy functional. 
A first observation is that, often, nonconvergence is a local problem. In many practical cases, most of the beliefs in the network do converge, and only a small portion of the network remains problematic. In such cases, it is often quite reasonable simply to stop the algorithm at some point (for example, when some predetermined amount of time has elapsed) and use the beliefs at that point, or a running average of the beliefs over some time window. This heuristic is particularly reasonable when we are not interested in individual beliefs, but rather in some aggregate over the entire network, for example, in a learning setting. 
A second observation is that nonconvergence is often due to oscillations in the beliefs (see sec- tion 11.3.1). This observation suggests that we dampen the oscillations by reducing the diference between two subsequent updates. Consider the belief-propagation update rule in SP-Message $(i, j)$ : 

$$
\delta_{i\rightarrow j}\leftarrow\sum_{C_{i}-S_{i, j}}\psi_{i}\prod_{k\neq j}\delta_{k\rightarrow i}.
$$ 
damping We can replace this line by $^a$ damped (or smoothed ) version that averages the update $\delta_{i\to j}$ with the previous message between the two cliques: 

$$
\delta_{i\to j}\leftarrow\lambda\left (\sum_{C_{i}-S_{i, j}}\psi_{i}\prod_{k\neq j}\delta_{k\to i}\right)+(1-\lambda)\delta_{i\to j}^{\mathrm{old}},
$$ 
where $\lambda$ is the damping weight and $\delta_{i\to j}^{\mathrm{old}}$ is the previous value of the message. When $\lambda\,=\, 1$ , → this update is equivalent to standard belief propagation. For $0\,<\,\lambda\,<\, 1$ , the update is partial and although it shifts $\beta_{j}$ toward agreement with $\beta_{i}$ , it leaves some momentum for the old value of the belief, a dampening efect that in turn reduces the ﬂuctuations in the beliefs. It turns out that this damped update rule is “equivalent” to the original update rule, in that a set of beliefs is a convergence point of the damped update if and only if it is a convergence point of standard updates (see exercise 11.13). Moreover, one can show that, if run from a point close enough to a stable convergence point of the algorithm, with a sufciently small $\lambda$ , this damped update rule is guaranteed to converge. Of course, this guarantee is not very useful in practice, but there are indeed many cases where the damped update rule is convergent, whereas the original update rule oscillates indefinitely. 

A broader-spectrum heuristic, which plays an important role not only in ensuring convergence but also in speeding it up considerably, is intelligent message scheduling . It is tempting to implement BP message passing as a synchronous algorithm, where all messages are updated at once. It turns out that, in most cases, this schedule is far from optimal, both in terms of reaching convergence, and in the number of messages required for convergence. The latter problem is easy to understand: In a cluster graph with m edges, and diameter $d$ , synchronous message passing requires $m (d-1)$ messages to pass information from one side of the graph to the other. By contrast, asynchronous message passing, appropriately scheduled, can pass information between two clusters at opposite ends of the graph using $d-1$ messages. Moreover, the fact that, in synchronous message passing, each cluster uses messages from its neighbors that are based on their previous beliefs appears to increase the chances of oscillatory behavior and nonconvergence in general. 

In practice, an asynchronous message passing schedule works significantly better than the synchronous approach. Moreover, even greater improvements can be obtained by scheduling messages in a guided way. One approach, called tree re parameter iz ation (TRP) , selects a set of trees, each of which spans a large number of the clusters, and whose union covers all of the edges in the network. The TRP algorithm then iteratively selects a tree and does an upward-downward calibration of the tree, keeping all other messages fixed. Of course, calibrating this tree has the efect of “uncalibrating” other trees, and so this process repeats. This approach has the advantage of passing information more globally within the graph. It therefore converges more often, and more quickly, than other asynchronous schedules, particularly if the trees are selected using a careful design that accounts for the properties of the problem. 

An even more ﬂexible approach attempts to detect dynamically in which parts of the network messages would be most useful. Specifically, as we observed, often some parts of the network converge fairly quickly, whereas others require more messages. We can schedule messages in a way that accounts for their potential usefulness; for example, we can pass a message between clusters where the beliefs disagree most strongly on the sepset. This approach, called residual belief propagation is convenient, since it is fully general and does not require a deep understanding of the properties of the network. It also works well across a range of diferent real-world networks. 

An alternative general-purpose approach to avoiding nonconvergence is to directly optimize the energy functional. Here, several methods have been proposed. The simplest is to use standard optimization methods such as gradient ascent to optimize $\tilde{F}[\tilde{P}_{\Phi}, Q]$ (see appendix A.5.2 and exercise 11.12). Other methods are more specialized to the form of the energy functional, and they often turn out to be more efcient (see section 11.7). Although these methods do improve convergence, they are somewhat complex to implement, and have not (at this time) been used extensively in practice. 
It turns out that many of the parameter settings encountered during a learning algorithm are problematic, and cause cluster-graph belief propagation to diverge. Intuitively, in many real-world problems, “appropriate” parameters encode strong constraints that tend to drive the algorithm toward well-behaved regions of the space. However, the parameters encountered during an iterative learning procedure have no such properties, and often allow the algorithm to end up in difcult regions. One approach is to train some parameters of the model separately, using a simpler network. We then use these parameters as our starting point in the general learning procedure. The use of “reasonable” parameters in the model can stabilize BP, allowing it to converge within the context of the general learning algorithm. 

A final problem with cluster-graph belief propagation is the fact that the energy functional objective is multimodal, and so there are many local maxima to which a cluster-graph belief propagation algorithm might converge (if it converges). One can, of course, apply any of the standard approaches for addressing optimization of multimodal functions, such as initializing the algorithm heuristically, or using multiple restarts with diferent initializations. In the setting of $B P,$ initialization must be done with care, so as not to lose the connection to the correct underlying distribution $P_{\Phi}$ , as reﬂected by the invariant of theorem 11.4. In sum-product belief propagation, we can simply initialize the messages to something other than 1 . In belief update propagation, care must be taken to initialize messages and beliefs in a coordinate way, to preserve $P_{\Phi}$ . 

Box 11. C — Case Study: BP in practice. To convey the behavior of belief propagation in practice, we demonstrate its performance on an $11\times11$ (121 binary variables) Ising grid (see box 4. C). The potentials of the network were randomly sampled as follows: Each univariate potential was sampled uniformly in the interval $[0,1]$ ; for each pair of variables $X_{i}, Z_{j},\, w_{i, j}$ is sampled uniformly in the range $[-C, C]$ (recall that in an Ising model, we define the negative log potential $\epsilon_{i, j}(x_{i}, x_{j})= -w_{i, j}x_{i}x_{j}).$ . This sampling process creates an energy function where some potentials are attractive
 $(w_{i, j}\,>\, 0)$ and some are repulsive $(w_{i, j}\,<\, 0)$ , resulting in a nontrivial inference problem. The magnitude of $C$ (11 in this example) controls the magnitude of the energy forces and higher values correspond, on average, to more challenging inference problems. 
 
Figure 11.C.1 illustrates the convergence behavior on this problem. Panel (a) shows the percentage of messages converged as a function of time for three variants of the belief propagation algorithm: synchronous BP with damping (dashed line), where only a small fraction of the messages ever converge; asynchronous BP with damping (smoothing) that converges (solid line); asynchronous $B P$ with no damping (dash-dot line) that does not fully converge. The benefit of using asynchronous propagation over synchronous updating is obvious. At first, it appears as if smoothing messages is not beneficial. This is because some percentage of messages can converge quickly when updates are not slowed down by smoothing. However, the overall benefit of damping is evident, and without it the algorithm never converges. 

The remaining panels illustrate the progression of the marginal beliefs over the course of the algorithm. (b) shows a marginal where both the synchronous and asynchronous updates converge quite rapidly and are close to the true marginal (thin solid black). Such behavior is atypical, and it comprises only around 10 percent of the marginals in this example. In the vast majority of the cases (almost 80 percent in this example), the synchronous beliefs oscillate around the asynchronous ones ((c)–(e)). In many cases, such as the ones shown in (e), the entropy of the synchronous beliefs is quite significant. For about 10 percent of the marginals (for example (f)), both the asynchronous and synchronous marginals are inaccurate. In these cases, using more informed message schedules can significantly improve the algorithms performance. 

These qualitative diferences between the BP variants are quite consistent across many random and real-life models. Typically, the more complex the inference problem, the larger the gaps in performance. For very complex real-life networks involving tens of thousands of variables and multiple cycles, even asynchronous BP is not very useful and more elaborate propagation methods or convergent alternatives must be adopted. 

### 11.3.6 Variational Analysis 
So far, our discussion of cluster-graph belief propagation has been procedural, motivated purely by similarity to message passing algorithms for cluster trees. Is there any formal justification for this approach? Is there a sense in which we can view this algorithm as providing an approximation to the exact inference task? In this section, we show that cluster-graph belief propagation can be justified using the energy functional formulation of section 11.1. Specifically, the messages passed by cluster-graph belief propagation can be derived from fixed-point equations for the stationary points of an approximate version of the energy functional of equation (11.3). As we will see, this formulation provides significant insight into the generalized belief propagation algorithm. It allows us to understand better the convergence properties of cluster-graph belief propagation and to characterize its convergence points. It also suggests generalizations of the algorithm that have better convergence properties, or that optimize a better approximation to the energy functional. 
>  我们目前对于簇图信念传播算法的讨论都是过程性的，是直接效仿簇树中的消息传递算法而来
>  本节展示簇图信念传播算法可以由能量泛函推导而来，具体地说，可以从能量泛函的近似形式的驻点所满足的固定点方程推导而来

Our construction will be similar to the one in section 11.2 for exact inference. However, there are important diferences that underlie the fact that this algorithm is only an approximate inference algorithm. 
factored energy functional 

First, the exact energy functional $F[\tilde{P}_{\Phi}, Q]$ has terms involving the entropy of an entire joint distribution; thus, it cannot be tractably optimized. However, the factored energy functional $\tilde{F}[\tilde{P}_{\Phi}, Q].$ is defined in terms of entropies of clusters and sepsets, each of which can be computed efciently based purely on local information at the clusters. Importantly, however, unlike for clique trees, $\tilde{F}[\tilde{P_{\Phi}}, Q]$ is no longer simply a reformulation of the energy functional, but rather an approximation of it. 
>  精确能量泛函 $F[\tilde P_\Phi, Q]$ 其中一项是整个联合分布的熵，因此无法直接优化
>  分解形式的能量泛函 $\tilde F[\tilde P_\Phi, \pmb Q]$ 则根据簇和分离集的熵定义，这些熵可以基于簇上的局部信息高效计算
>  但和团树中的情况不同的是，在簇图中，$\tilde F[\tilde P_\Phi, \pmb Q]$ 将不再和 $F[\tilde P_\Phi, Q]$ 等价，而是对后者的一个近似

However, even the factored energy functional cannot be optimized over the space of all marginals $Q$ that correspond to some actual distribution $P_{\Phi}$ . More precisely, consider some cluster graph $\mathcal{U}$ ; for a distribution $P$ e define $Q_{P}=\{P (C_{i})\}_{i\in\mathcal{V}_{\mathcal{U}}}\cup\{P (S_{i, j})\}_{(i-j)\in\mathcal{E}_{\mathcal{U}}}$ . We now define the marginal polytope of U to be 

$$
M a r g[\mathcal{U}]=\{\pmb Q_{P}: P\mathrm{~is~a~distribution~over~}\mathcal{X}\}
$$ 
>  并且，即便是分解形式的能量泛函，也难以在关于 $\pmb Q$ 的空间 ($\pmb Q$ 对应于可以构造出目标分布 $P_\Phi$ 的全部边际集合) 上优化
>  对于簇图 $\mathcal U$ 和一个分布 $P$，我们定义 $\pmb Q_{P}=\{P (\pmb C_{i})\}_{i\in\mathcal{V}_{\mathcal{U}}}\cup\{P (\pmb S_{i, j})\}_{(i-j)\in\mathcal{E}_{\mathcal{U}}}$，进而定义 $\mathcal U$ 的边际多面体如上

That is, the marginal polytope is the set of all cluster (and sepset) beliefs that can be obtained from marginalizing an actual distribution $P$ . It is called the marginal polytope because it is the set of marginals obtained from the polytope of all pro bility distributions over $\mathcal{X}$ . Unfortunately, not every set of beliefs that correspond to clusters in U is in the marginal polytope; that is, there are calibrated cluster graph beliefs that do not represent the marginals of any single coherent joint distribution over $\mathcal{X}$ (see exercise 11.2). However, the marginal polytope is a complex object with exponentially many facets. (In fact, the problem of determining whether a set of beliefs is in the marginal polytope can be shown to be $\mathcal{N P}$ -hard.) Thus, optimizing a function over the local consistency polytope marginal polytope is a computationally difcult task that is generally as hard as exact inference over the cluster graph. To circumvent these problems, we perform our optimization over the local consistency polytope : 
>  也就是说，边际多面体是从实际分布 $P$ 的边际化中获得的所有簇（和分割集）信念的集合
>  之所以称为边际多面体，是因为它是从 $\mathcal{X}$ 上所有概率分布的多面体中得到的边际分布的集合
>  不幸的是，并不是每个对应于 $\mathcal{U}$ 中簇的信念集合都属于边际多面体；也就是说，存在校准后的簇图信念集合，它们并不表示 $\mathcal{X}$ 上任何单一一致联合分布的边际（参见练习 11.2）
>  然而，边际多面体是一个具有指数多个面的复杂对象（事实上，判断一组信念是否在边际多面体中的问题可以证明是 NP 难的），因此，在局部一致性多面体上优化函数是一项计算上困难的任务，通常与簇图上的精确推理一样难
>  为了解决这些问题，我们在局部一致性多面体上进行优化：
 
pseudo-marginals 
We can think of the local consistency polytope as defining a set of pseudo-marginal distri- butions , each one over the variables in one cluster. The constraints imply that these pseudo- marginals must be calibrated and therefore locally consistent with each other. However, they are not necessarily marginals of a single underlying joint distribution. 
Overall, we can write down an optimization problem as follows: 
CGraph-Optimize : Find Q maximizing F ˜ [ P ˜ Φ , Q ] subject to $Q\in L o c a l[\mathcal{U}]$ 

Thus, our optimization problem contains two approximations: We are using an approx- imation, rather than an exact, energy functional; and we are optimizing it over the space of pseudo-marginals, which is a relaxation (a superspace) of the space of all coherent probability distributions that factorize over the cluster graph. 
In section 11.1, we noted that the energy functional is a lower bound on the log-partition function; thus, by maximizing it, we get better approximations of $P_{\Phi}$ . Unfortunately, the factored energy functional, which is only an approximation to the true energy functional, is not necessarily also a lower bound. Nonetheless, it is still a reasonable strategy to maximize the approximate energy functional, since it may lead to a good approximation of the log-partition function. 
fixed-point equations 
Theorem 11.5 
This maximization problem directly generalizes CTree-Optimize to the case of cluster graphs. Not surprisingly, we can derive a similar analogue to theorem 11.3, where we characterize the stationary points of this optimization problem as solutions to a set of fixed-point equations . 
$$
\delta_{i\rightarrow j}\propto\sum_{C_{i}-S_{i, j}}\psi_{i}\cdot\prod_{k\in\mathrm{Nb}_{i}-\{j\}}\delta_{k\rightarrow i}.
$$ 
and moreover, we have that 
$$
\begin{array}{r c l}{\beta_{i}}&{\propto}&{\psi_{i}\cdot\displaystyle\prod_{j\in\ensuremath{\mathrm{Nb}}_{i}}\delta_{j\to i}}\\ {\mu_{i, j}}&{=}&{\delta_{j\to i}\cdot\delta_{i\to j}.}\end{array}
$$ 
The proof is identical to the proof of theorem 11.3. 
This theorem shows that we can characterize convergence points of the energy function in terms of the original potentials and messages between clusters. We can, once again, define a procedural variant, in which we initialize $\delta_{i\to j}$ , and then iteratively use equation (11.18) to redefine each $\delta_{i\to j}$ in terms of the current values of other $\delta_{k\to i}$ . This process is identical (up to a renormalization step) to the update formula we use in CTree-SP-calibrate (algorithm 10.2). Indeed, we defined CGraph-SP-Calibrate , a cluster graph version of CTree-SP-Calibrate , the mes- sage passing steps are simply executing this iterative process using the fixed-point equation. Theorem 11.5 shows that convergence points of this procedure are related to stationary points of $\tilde{F}[\tilde{P}_{\Phi}, Q]$ . 
Corollary 11.1 $Q$ is the ce point of applying CGraph-SP-Calibrate $(\Phi,{\mathcal{U}})$ if and only if $Q$ is a stationary point of $\tilde{F}[\tilde{P}_{\Phi},\bar{Q}]$ . 
Due to the equivalence between sum-product and belief update messages, it follows that convergence points of CGraph-BU-Calibrate are also convergence points of CGraph-SP-Calibrate . 
Corollary 11.2 At convergence of CGraph-BU-Calibrate , the set of beliefs is a stationary point of $\tilde{F}[\tilde{P}_{\Phi}, Q]$ 
It is tempting to interpret this result as stating that the convergence points of belief propa- gation are maxima of the factored energy functional. However, there are several gaps between the theorem and this idealized interpretation, which it is important to understand. First, we note that maxima of a function are not necessarily fixed points. In this case, we can verify that $\tilde{F}[\tilde{P}_{\Phi}, Q]$ is bounded from above, and thus must have a maximum. However, if the maximum is a boundary point (where some of the probabilities in $Q$ are 0), it may not be a fixed point. Fortunately, this situation is rare in practice, and it can be guaranteed not to arise under fairly benign assumptions. 
stable convergence point 
Second, we note that maxima are not the only fixed points of the belief propagation algorithm; minima and saddle points are also fixed points. Intuitively, however, such solutions are not likely to be stable, in the sense that slight perturbations to the messages will drive the process away from them. Indeed, it is possible to show (although this result is outside the scope of this book) that stable convergence points of belief propagation are always local maxima of the function. 
The most important limitation of this result, however, is that it does not show that we can reach these maxima by applying belief propagation steps. There is no guarantee that the message passing steps of cluster-graph belief propagation necessarily improve the energy functional: a message passing step may increase or decrease the energy functional. Indeed, as we showed, there are examples where the belief propagation procedure oscillates indefinitely and fails to converge. Even more surprisingly, this problem is not simply a matter of the algorithm being unable to “find” the maximum. One can show examples where the global maximum is not a stable convergence point of belief propagation. That is, while it is, in principle, a fixed point of the algorithm, it will never be reached in practice, since even a slight perturbation will give rise to oscillatory behavior. 
Nevertheless, this result is of significant importance in several ways. First, it provides us with a declarative semantics for cluster-graph belief propagation in terms of optimization of a target functional. The success of the belief propagation algorithm, when it converges, leads us to hope that the development of new, possibly more convergent, methods to solve the optimization problem may give rise to good solutions. Second, the declarative view defines the problem in terms of an objective — the factored energy functional — and a set of constraints — the set of locally consistent pseudo-marginals. Both of these are approximations to the ones used in the optimization problem for exact inference. When we view the task from this perspective, some potential directions for improvements become obvious: We can perhaps achieve a better approximation by making our objective a better approximation to the true energy functional, or by tightening our constraints so as to make the constraint space closer to the exact marginal polytope. We will describe some of the extensions based on these ideas; others are mentioned in section 11.7. 

### 11.3.7 Other Entropy Approximations\*
The variational analysis of the previous section provides us with a framework for understanding the properties of this type of approximation, and for providing significant generalizations. 
#### 11.3.7.1 Motivation 
To understand this general framework, consider first the form of the factored energy functional when our cluster graph $\mathcal{U}$ has the form of the Bethe approximation. Recall that in the Bethe approximation graph there are two layers: one consisting of clusters that correspond to factors in $\Phi$ , and the other consisting of univariate clusters. When the cluster graph is calibrated, these univariate clusters have the same distribution as the sepsets between them and the factors in the first layer. As such, we can combine together the entropy terms for all the sepsets labeled by $X$ and the associated univariate cluster and rewrite the energy functional, as follows: 
Proposition 11.3 If $Q=\{\beta_{\phi}:\phi\in\Phi\}\cup\{\beta_{i}(X_{i})\}$ is a calibrated set of beliefs for a Bethe cluster graph $\mathcal{U}$ with clusters $\left\{C_{\phi}\ :\ \phi\in\Phi\right\}\cup\left\{X_{i}\ :\ X_{i}\in\mathcal{X}\right\}$ , then 
$$
\tilde{F}[\tilde{P}_{\Phi}, Q]=\sum_{\phi\in\Phi}\pmb{E}_{S c o p e[\phi]\sim\beta_{\phi}}[\ln\phi]+\sum_{\phi\in\Phi}\pmb{H}_{\beta_{\phi}}(C_{\phi})-\sum_{i}(d_{i}-1)\pmb{H}_{\beta_{i}}(X_{i}),
$$ 
where $d_{i}=|\{\phi: X_{i}\in S c o p e[\phi]\}|$ is the number of factors that contain $X_{i}$ . 
Bethe free energy Note that equation (11.19) is equivalent to the factored energy functional only when $Q$ is cali- brated. However, because we are interested only in such cases, we can freely alternate between the two forms for the purpose of finding fixed points of the factored energy functional. Equa- tion (11.19) is the negation of a term known as the Bethe free energy in statistical mechanics. The Bethe cluster graph we discussed earlier is a construction that is designed to match the Bethe free energy functional. 
Why is this reformulation useful? Recall that, in our discussion of generalized cluster graphs, we required the running intersection property. This property has two important implications. First is that the set of clusters that contain some variable $X$ are connected; hence, the marginal over $X$ will be the same in all of these clusters at the calibration point. Second is that there is no cycle of clusters and sepsets all of which contain $X$ . We motivated this assumption by noting that it prevents us from allowing information about $X$ to cycle endlessly through a loop. This new formulation provides a more formal justification. As we can see, if the variable $X_{i}$ appears in $d_{i}$ clusters in the cluster graph, then it appears in an entropy term with a positive sign exactly $d_{i}$ times. Owing to the running intersection property, the number of sepsets that contain $X_{i}$ is $d_{i}-1$ (the number of edges in a tr $k$ vertices is $k-1)$ , so that $X_{i}$ appe in an entropy term with a egative sign exactly $d_{i}-1$ − ti this case, the entropy of $X_{i}$ appears with positive sign $d_{i}$ times, and with negative sign $d_{i}-1$ − times, so overall it is counted exactly once. 
counting number weighted approximate entropy 
Example 11.2 Bethe cluster graph 
This reformulation suggests a much more general class of entropy approximations. We can construct define a set of regions $\mathbf{R}$ , each with its own scope $C_{r}$ and its own counting number $\kappa_{r}$ . We can now define the following weighted approximate entropy : 
$$
\tilde{H}_{Q}^{\kappa}(\mathcal{X})=\sum_{r}\kappa_{r}H_{\beta_{r}}(C_{r}).
$$ 
The simple Bethe cluster graph of section 11.3.5.2 fits easily into this new framework. The construc- tion has two levels of regions: a set of “large” $\mathbf{R}^{+}$ , where ea $r\in\mathbf{R}^{+}$ contains multiple variables, and singleton regio con the i vidu bles $X_{i}\,\in\,{\mathcal{X}}$ ∈X . oth types of regions have counting numb $\kappa_{r}$ for r $r\in\mathbf{R}^{+}$ ∈ and $\kappa_{i}$ r $X_{i}\in\mathcal{X}$ ∈X . All fa $\Phi$ re assi only to large reg $\psi_{i}=1$ for all $i$ . We use $\mathrm{Nb}_{r}$ to denote the set { $\{X_{i}\in C_{r}\}$ ∈ } , and $\mathrm{Nb}_{i}$ to denote the set { $\{r\ :\ X_{i}\in C_{r}\}$ } . 
To capture exactly the Bethe free energy, we set each large region to have a counting number of 1 , and each singleton region corresponding to $X_{i}$ to have a counting number of $1-d_{i}$ where $d_{i}$ is the number of large regions that contain $X_{i}$ in their scope. We see that in this construction the region graph energy functional is identical to the Bethe free energy of equation (11.19). 
However, this framework also allows us to capture much richer constructions. 
Example 11.3 Consider again the example of figure 11.8a. As we discussed in section 11.3.5.3, this cluster graph has the benefit of maintaining the pairwise correlations between all pairs of variables when passing messages between clusters. Unfortunately, it is not a legal cluster graph, since it does not satisfy the running intersection property. We can obtain another perspective on the problem with this cluster graph by examining the energy functional associated with it: 
$$
\begin{array}{r c l}{{\tilde{F}[\tilde{P}_{\Phi}, Q]}}&{{=}}&{{{\pmb E}_{\beta_{1}}[\ln\phi_{1}({\cal A},{\cal B},{\cal C})]+{\pmb E}_{\beta_{2}}[\ln\phi_{2}({\cal B},{\cal C},{\cal D})]+{\pmb E}_{\beta_{3}}[\ln\phi_{3}({\cal A},{\cal C},{\cal D})}}\\ {{}}&{{}}&{{+{\pmb H}_{\beta_{1}}({\cal A},{\cal B},{\cal C})+{\pmb H}_{\beta_{2}}({\cal B},{\cal C},{\cal D})+{\pmb H}_{\beta_{3}}({\cal A},{\cal C},{\cal D})}}\\ {{}}&{{}}&{{-{\pmb H}_{\beta_{4}}({\cal B},{\cal C})-{\pmb H}_{\beta_{5}}({\cal A},{\cal C})-{\pmb H}_{\beta_{6}}({\cal C},{\cal D}).}}\end{array}
$$ 
As we can see, the variable $C$ appears in three clusters and three sepsets. As a consequence, the counting number of $C$ in the energy functional is 0 . This means that we are undercounting the entropy of $C$ in the approximation. Indeed, as we discussed, this cluster graph does not satisfy the running intersection property. Thus, we considered modifying the graph by removing $C$ from one of the sepsets. However, if we consider this problem from the perspective of the energy functional, we can deal with the problem by adding another factor $\beta_{7}$ that has $C$ as its scope. If we add $H_{\beta_{7}}(C)$ to the energy functional we solve the undercounting problem. This results in a modified energy functional 
$$
\begin{array}{r c l}{{\tilde{P}_{\Phi}, Q]}}&{{=}}&{{{\cal E}_{\beta_{1}}[\ln\phi_{1}(A, B, C)]+{\cal E}_{\beta_{2}}[\ln\phi_{2}(B, C, D)]+{\cal E}_{\beta_{3}}[\ln\phi_{3}(A, C, D)]}}\\ {{}}&{{}}&{{+{\cal H}_{\beta_{1}}(A, B, C)+{\cal H}_{\beta_{2}}(B, C, D)+{\cal H}_{\beta_{3}}(A, C, D)+{\cal H}_{\beta_{7}}(C)}}\\ {{}}&{{}}&{{-{\cal H}_{\beta_{4}}(B, C)-{\cal H}_{\beta_{5}}(A, C)-{\cal H}_{\beta_{6}}(C, D).}}\end{array}
$$ 
This is simply an instance of our weighted entropy approximation, with seven regions: the three triplets, the three pairs, and the singleton $C$ . 
This perspective provides a clean and simple framework for proposing generalizations to the class of approximations defined by the cluster graph framework. Of course, to formulate our optimization problem fully, we need to define the constraints and construct algorithms that solve the resulting optimization problems. We now address these issues in the context of two diferent classes of weighted entropy approximations. 
#### 11.3.7.2 Convex Approximations 
One of the biggest problems with the objective used in standard loopy BP is that it gives rise to a nonconvex optimization problem. In fact, the objective often has multiple local optima. These properties make the optimization hard and the answers nonrobust. However, a diferent choice of counting numbers can lead to a concave optimization objective, and hence to a convex optimization problem. Such problems are much easier to solve using a range of algorithms, and the solutions ofer a satisfying guarantee of optimality. We first define the class of convex BP objectives and then describe one solution algorithm. 
We focus our discussion on energy functionals whose structure uses the two-layer Bethe cluster graph structure of example 11.2, but where the counting numbers are diferent. To preserve the desired semantics of the counting numbers, we require: 
$$
\kappa_{i}=1-\sum_{r\in\mathrm{Nb}_{i}}\kappa_{r},
$$ 
ensuring that the total counting number of terms involving the entropy of $X_{i}$ is precisely 1 . When we define $\kappa_{r}\,=\, 1$ for all $r\,\in\,\mathbf{R}$ , this constraint implies the counting numbers in the Bethe free energy. 
We now introduce the following condition on the counting numbers: 
Definition 11.4 convex counting numbers 
We say that a vector of counting numbers $\kappa_{r}$ is convex if there exist nonnegative numbers $\nu_{r},\,\nu_{i}$ , and $\nu_{r, i}$ such that: 
$$
\begin{array}{r l r}{\kappa_{r}}&{=}&{\nu_{r}+\sum_{i\,:\, X_{i}\in C_{r}}\nu_{r, i}\quad{\it f o r\; a l l\; r}}\\ {\kappa_{i}}&{=}&{\nu_{i}-\sum_{r\,:\, X_{i}\in C_{r}}\nu_{r, i}\quad{\it f o r\; a l l\; i}}\end{array}
$$ 
Assuming that we have a set of convex counting numbers, we can rewrite the weighted approximate entropy of equation (11.20) as: 
$$
\begin{array}{l}{{\displaystyle\sum_{r}\kappa_{r}{\pmb H}_{\beta_{r}}({\pmb C}_{r})+\sum_{i}\kappa_{i}{\pmb H}_{\beta_{i}}({\pmb X}_{i})=}}\\ {{\displaystyle\sum_{r}\nu_{r}{\pmb H}_{\beta_{r}}({\pmb C}_{r})+\sum_{r, X_{i}\in{\pmb C}_{r}}\nu_{r, i}\big ({\pmb H}_{\beta_{r}}({\pmb C}_{r})-{\pmb H}_{\beta_{i}}({\pmb X}_{i})\big)+\sum_{i}\nu_{i}{\pmb H}_{\beta_{i}}({\pmb X}_{i}).}}\end{array}
$$ 
Importantly, when the beliefs satisfy the marginal-consistency constraints, the terms in the second summation can be rewritten as conditional entropies: 
$$
H_{\beta_{r}}(C_{r})-H_{\beta_{i}}(X_{i})=H_{\beta_{r}}(C_{r}\mid X_{i}).
$$ 
Plugging this result back into equation (11.23), we obtain an objective that is a summation of terms each of which is either an entropy or a conditional entropy, all with positive coefcients. Because both entropies and conditional entropies are convex, we obtain the following result: 

Proposition 11.4 
concave over constraints convex entropy The function in equation (11.23) is a concave function for any set of beliefs $Q$ that satisfies the marginal consistency constraints. 
This type of objective function is called concave over the constraints , since it is not generally concave, but it is concave over the subspace that satisfies the constraints of our optimization problem. An entropy as in equation (11.23) that uses convex counting numbers is called a convex entropy . 
Assuming that the potentials are all strictly positive, we can now conclude that the optimiza- tion problem CGraph-Optimize with convex counting numbers is a convex optimization problem that has a unique global optimum. 
Convex optimization problems can, in principle, be solved by a range of diferent algorithms, all of which guarantee convergence to the unique global optimum. However, the basic optimiza- tion problem can easily get intractably large. Recall that to formulate our optimization space, we need to introduce an optimization variable for each assignment of values to each cluster in our cluster graph, and a constraint for each assignment of values to each sepset in the graph. 
Example 11.4 Consider a grid-structured network corresponding to a modestly sized $500\times500$ image, where each pixel can take 100 values. The structure of the graph is a pairwise network, with approximately $2\times250,000$ clusters (pairwise edges), e $100\times100=10,000$ values. The total number of variables is therefore 500 $\vert, 000\times10,000=5\times10^{9}$ × × , an unmanageable number for most optimizers. 
Fortunately, due to the convexity of this problem, we have that strong duality holds (see appendix A.5.4), and therefore we can find a solution to this problem by solving its dual. The message passing algorithms that we derive from the Lagrange multipliers are one method that we can use for solving the dual. (For example, exercise 11.17 provides one message passing algorithm for a Bethe cluster graph with general counting numbers.) However, the message passing algorithms are not directly optimizing the objective. Rather, they characterize the optimum using a set of fixed-point equations, and attempt to converge to the optimum by iterating through these equations. This process is generally not guaranteed to achieve the optimum, even when the problem is convex. Again, we can consider using other optimization algorithms over the dual problem. However, a message passing approach has some important advantages, such as modularity and efciency. 
Fortunately, a careful reformulation of the message passing scheme can be shown to guar- antee convergence to the global optimum. This reformulation is diferent for synchronous and asynchronous message passing. We present the asynchronous version, which is simpler and also likely to be more efcient in practice. 
The algorithm, shown in algorithm 11.2, uses the following quantities in its computations: 
$$
\hat{\nu}_{i}=\nu_{i}+\sum_{r\in\ensuremath{\mathrm{Nb}}_{i}}\nu_{r};\qquad\qquad\qquad\qquad\hat{\nu}_{i, r}=\nu_{r}+\nu_{i, r}.
$$ 
In each message passing iteration, it traverses the variables $X_{i}$ in a round-robin fashion; for each $X_{i}$ , it computes two sets of messages: incoming messages $\delta_{r\rightarrow i}(X_{i})$ from regions to variables, 
Procedure Convex-BP-Msg ( $\psi_{r}(C_{r})$ // set of initial potentials $\sigma_{i\rightarrow r}(C_{r})$ // Current node-to-region messages ) 1 for $i=1,\dots, n$ 2 $//$ Compute incoming messages from neighboring regions to X i 3 for $r\in\mathrm{Nb}_{i}$ 4 $\begin{array}{r}{\delta_{\boldsymbol{r}\rightarrow\boldsymbol{i}}(X_{i})\gets\;\sum_{\boldsymbol{C}_{\boldsymbol{r}}-X_{i}}\Big (\psi_{\boldsymbol{r}}(\boldsymbol{C}_{\boldsymbol{r}})\prod_{\boldsymbol{j}\in\mathrm{Nb}_{\boldsymbol{r}}-\{\boldsymbol{i}\}}\sigma_{\boldsymbol{j}\rightarrow\boldsymbol{r}}(\boldsymbol{C}_{\boldsymbol{r}})\Big)^{\frac{1}{\hat{\nu}_{i,\boldsymbol{r}}}}}\end{array}$ 5 $//$ Compute beliefs for $X_{i}$ , renormalizing to avoid numerical underﬂows 6 $\begin{array}{r}{\beta_{i}(\overbrace{X_{i}}^{\mathrm{Yukawa}\cdots\cdots}\overbrace{\frac{1}{Z_{X_{i}}}}^{\cdots}\prod_{r\in\mathrm{Nb}_{i}}(\delta_{r\rightarrow i}(X_{i}))^{\hat{\nu}_{i, r}/\hat{\nu}_{i}}}\end{array}$ Q 7 // Compute outgoing messages from $X_{i}$ to neighboring re- gions 8 for $\bar{r}\in\mathrm{Nb}_{i}$ 10 $\begin{array}{r l}{\dot{\mathrm{\boldmath{~\sigma~}}}_{i\rightarrow r}({\cal C}_{r})\gets}&{\left (\psi_{r}({\cal C}_{r})\prod_{j\in\mathrm{Nb}_{r}-\{i\}}\sigma_{j\rightarrow r}({\cal C}_{r})\right)^{-\frac{\nu_{i, r}}{\tilde{\nu}_{i, r}}}\left (\frac{\beta_{i}(X_{i})}{\delta_{r\rightarrow i}(X_{i})}\right)^{\nu_{r}}}\\ {{\mathrm{\bfreturn}~}\{\sigma_{i\rightarrow r}({\cal C}_{r})\}_{i, r\in\mathrm{Nb}_{i}}}\end{array}$ factor graph 
and outgoing messages $\sigma_{i\rightarrow r}(C_{r})$ from variables to regions (essentially passing messages over the factor graph ). The overall process is initialized (in the first message passing iteration) by setting $\sigma_{i\to r}\,=\, 1$ . This algorithm is guaranteed to converge to the global maximum of our convex energy functional. 
This derivation applies to any set of convex counting numbers, leaving open the question of which counting numbers are likely to be give the best approximation. Although there is currently no theoretical analysis answering this question, intuitively, we might argue that we want the counting numbers for diferent regions to be as close as possible to uniform. This intuition is also supported by the fact that the Bethe approximation, which sets all $\kappa_{r}=1$ , obtains very high- quality approximations when it converges. Thus, we can try to select nonnegative coefcients $\nu_{i},~\nu_{r}$ , and $\nu_{i, r}$ for which $\kappa_{r}$ and $\kappa_{i}$ , defined via equation (11.22), satisfy equation (11.21) and minimize 
$$
\sum_{r\in\mathbf{R}^{+}}(\kappa_{r}-1)^{2}.
$$ 
TRW 
Other choices are also possible. For example, the tree-reweighted belief propagation (TRW) algorithm computes convex counting numbers for a pairwise Markov network using the following process: We first define a probability distribution $\rho$ over trees $\mathcal{T}$ in the network, such that each edge in the pairwise network is present in at least one tree. This distribution defines a set of weights: 
$$
\begin{array}{r l}{\kappa_{i}}&{{}=-\sum_{\mathcal T\ni X_{i}}\rho (\mathcal T)}\\ {\kappa_{i, j}}&{{}=\sum_{\mathcal T\ni (X_{i}, X_{j})}\rho (\mathcal T)}\end{array}
$$ 
This computation results in a set of convex counting numbers (see exercise 11.18). Preliminary results suggest that the TRW counting numbers and the ones derived from optimizing equa- tion (11.25) appear to achieve similar performance in practice. 
However, the comparison to standard (Bethe-approximation) BP is less clear. When standard BP converges, it generally tends to produce better results than the convex counterparts, and almost universally it converges much faster. Conversely, when standard BP does not converge, the convex algorithms have an advantage; but, as we discuss in box 11. B, there are many tricks we can use to improve the convergence of BP, so it is not clear how often nonconvergence is a problem. One setting where a convergent algorithm can have important benefits is in those settings (chapter 19 and chapter 20) where we generally learn the model using iterative, hill-climbing methods that use inference in the inner loop for tasks such as gradient computations. There, the use of a nonconvergent algorithm for computing the gradient can severely destabilize the learning algorithm. In other settings, however, the decision of whether to use standard or convex BP is one of approximate optimization of a pretty good (although still approximate) objective, versus exact optimization of an objective that is generally not as good. The right decision in this trade-of is not clear, and needs to be made specifically for the target application. 
#### 11.3.7.3 Region Graph Approximations 
As illustrated in example 11.3, a very diferent motivation for using an objective based on diferent counting numbers is to improve the quality of the approximation by better capturing interactions between variables. As we showed in this example, we can use the notion of a weighted entropy approximation to define a (hopefully) better approximation to the entropy. Of course, to specify the optimization problem fully, we also need to specify the constraints. In this example, it is fairly straightforward to do so: we want $\beta_{7}(C)$ to be consistent with the marginal probability of $C$ in one of the other beliefs that mention $C$ . Now, we have an optimization problem that seems to solve the problem we set out to solve: It can compute beliefs on each of the original factors while maintaining consistency at the level of each pairwise marginal shared among these factors. 
However, the new optimization problem we defined is not one that corresponds to a cluster graph. To see this, notice that $\beta_{7}$ appears in the role of a cluster. But, if it is a cluster, it would have to be connected to one of the other factors by a sepset with scope $C$ , which would require an additional term in the energy functional associated with this cluster graph. Thus, it is not immediately clear how we would go about optimizing the new modified functional. 
We now discuss a general framework that defines the form of the optimization objective and the constraints for constructions that capture higher-level interactions between the variables. We also describe a message passing algorithm that can be used to find fixed points of this optimization problem. 
Region Graphs The basic structure we consider is similar to a cluster graph, but unlike cluster graphs we no longer distinguish two types of vertices (clusters and sepsets). Rather, we can have a more deeply nested hierarchy of regions, related by containment. 
![](images/fb0601e9d08519a16cac90eb6dec042d0cefc3bf915a3ecb949c0b62fedbcac5.jpg) 
Figure 11.9 An example of simple region graph 
is associated with a distinct set of random variables $C_{r}$ . Whenever there is an arc from a region $r_{i}$ to a region $r_{j}$ we require that $S c o p e[r_{i}]\supset S c o p e[r_{j}]$ . Regions that have no incoming edges are called top regions . 
counting numbers 
Each region is associated with a counting number $\kappa_{r}\in\mathbb{R}$ . Note that the counting number may be negative, positive, or zero. 
Because containment is transitive, we have that if there is a directed path from $r$ to $r^{\prime}$ , then $S c o p e[r^{\prime}]\subset S c o p e[r]$ . Thus, a region graph is acyclic. 
family preservation 
To define the energy term in the free energy, we must assign our original factors in $\Phi$ to regions in the region graph. Here, because diferent regions are counted to diferent extents, it is useful to allow a fa assigned t e region. Thus, for each $\phi\in\Phi$ we have a set of regions $\alpha (\phi)\subset\mathbf{R}$ ⊂ such that $S c o p e[\phi]\subseteq C_{r}$ ⊆ . This property is analogous to the family-preservation property for cluster graphs. Throughout this book, we assume without loss of generality that any $r\in\alpha (\phi)$ is a top region. 
We are now ready to define the energy functional associated with a region graph: 
$$
\tilde{F}[\tilde{P}_{\Phi}, Q]=\sum_{r}\kappa_{r}E_{C_{r}\sim\beta_{r}}[\ln\psi_{r}]+\tilde{H}_{Q}^{k}(\mathcal{X}),
$$ 
where $\psi_{r}$ is defined as: 
$$
\psi_{r}=\prod_{\phi\in\Phi\ :\ r\in\alpha (\phi)}\phi.
$$ 
As with cluster graphs, a region graph defines a set of beliefs, one per region. We use the notation $\beta_{r}(C_{r})$ to denote the belief associated with the region $r$ over the set of variables $C_{r}=S c o p e[r]$ . 
Figure 11.9 demonstrates the region graph construction for the approximation of example 11.3. This example contains three regions that correspond to the initial factors in the distribution we want to approximate. The lower set of regions are the pairwise intersections between the three factors. The lowest region is associated with the variable $C$ . 
Whereas the counting numbers specify the energy functional, the graph structure specifies the constraints over the beliefs that we wish to associate with the regions. In particular, we 
![](images/ae20deef87ea84848612e9f5dff21a811205e459b20095b6200782a3a23c5885.jpg) 
Figure 11.10 The region graph corresponding to the Bethe cluster graph of figure 11.7a 
want the beliefs to satisfy the calibration constraints that are implied by the edges in the region graph structure. 
Definition 11.6 region graph calibration 
Given a region graph, a set of region beliefs is calibrated if whenever $r\to r^{\prime}$ appears in the region graph then 
$$
\sum_{C_{r}-C_{r^{\prime}}}\beta_{r}(C_{r})=\beta_{r^{\prime}}(C_{r}^{\prime})
$$ 
The region graph structure provides a very general framework for specifying energy-functional optimization problems. The set of regions and their counting numbers tell us which components appear in the energy functional, and with what weight. The arcs in the region graph tell us which consistency constraints we wish to enforce. We can choose which consistency constraints we want to enforce by adding or removing regions or edges between them; we note that this can be done without afecting the energy functional by simply giving the regions introduced a counting number of 0 . The more edges we have, the more constraints we require regarding the calibration of diferent beliefs. 
The region graph framework is very general, and it can encode a broad spectrum of opti- mization objectives and constraints. However, not all such formulations are equally reasonable as approximations to our true objective, which is the exact energy functional of equation (11.3). The following requirement captures some of the essential properties that make a region graph construction suitable for this purpose. 
Definition 11.7 
e $X_{i}$ , let $\overline{{\mathbf{R}_{i}\;=\;}}\left\{r\;:\; X_{i}\;\in\; S c o p e[r]\right\}$ ; for each factor $\phi\,\in\,\Phi$ , let $\mathbf{R}_{\phi}\,=\,\{r\,:$ $S c o p e[\phi]\subseteq C_{r}\}$ ⊆ } . We now define the following conditions for a region graph: • variable connectedness : for every variable $X_{i}$ , the set $\mathbf{R}_{i}$ forms a single connected component; • factor connectedness : for every factor $\phi.$ , the set $\mathbf{R}_{\phi}$ forms a single connected component; • factor preservation : $\sum_{r\in\alpha (\phi)}\kappa_{r}=1.$ • running intersection : for every variable $X_{i}$ , 
The connectedness requirement for variables ensures that the beliefs about any individual vari- able $X_{i}$ in any calibrated region graph will be consistent for all beliefs that contain $X_{i}$ . The counting number condition for variables ensures that we are not not overcounting or under- counting the entropy of $X_{i}$ . Together, these conditions extend the running intersection property, ensuring that the subgraph containing $X_{i}$ is connected and that $X_{i}$ is “counted” once in total. We note that, like the running intersection property, this requirement does not address the ex- tent to which we count the contribution associated with the interactions between pairs or larger subsets of variables. 
The factor-preservation condition for factors ensures that, when we sum up the energy terms of the diferent regions, each factor is counted exactly once in total. As we will see, this ensures that a calibrated region graph will still encode our original distribution $P_{\Phi}$ . Finally, the connectedness condition for factors ensures that we cannot double-count contributions of our initial factors: that is, a factor cannot “ﬂow” around a loop. 
An examination confirms that all parts of the region graph condition hold both for the Bethe region graph and for the region graph of figure 11.9. 
How do we construct a valid region graph? One approach is based on the following simple recursive construction for the counting numbers. We first define, for a region $r$ , 
$$
\mathbf{Up}(r)=\{r^{\prime}\ :\ (r^{\prime}\to r)\in\mathcal{E}_{\mathcal{R}}\}
$$ 
to be the set of regions that are directly upwards of $r$ ; similarly, we define 
$$
\mathbf{Dewn}(r)=\{r^{\prime}\ :\ (r\to r^{\prime})\in\mathcal{E}_{\mathcal{R}}\}.
$$ 
We also define the upward closure of $r$ to be the set $\mathbf{Up}^{*}(r)$ of all the regions from which there is a directed path to $r$ , and the downward closure $\mathbf{D o w n}^{*}(r)$ to be all the regions that can be reached by a directed path from $r$ ; finally, we define $\mathbf{Dofwr}^{+}(r)=\{r\}\cup\mathbf{Dofwr}^{*}(r)$ . 
We can now define the counting numbers recursively, using the following formula: 
$$
\kappa_{r}=1-\sum_{r^{\prime}\in\mathbf{U}\mathbf{p}^{*}(r)}\kappa_{r^{\prime}}.
$$ 
This condition ensures that the sum of the counting numbers of $r$ and all of the regions above it will be 1 . Intuitively, we can think of the counting number of the region $r$ as correcting for overcounting or undercounting of the weight of the scope of $r$ by regions above it. Now, assume that our region graph is structured so that, for each variable $X_{i}$ , there is a unique region $r_{i}$ such that every other region whose scope contains $X_{i}$ is an ancestor of $r_{i}$ . Then, we are guaranteed that both the connectedness and the counting number condition for $X_{i}$ hold. We can similarly require that for any factor $\phi$ , there is a unique region $r_{\phi}$ such that any other region whose scope contains $S c o p e[\phi]$ is an ancestor of $r_{\phi}$ . This construction guarantees the requirements for factor connectedness and counting numbers. 
It is easy to see that the Bethe region graph of example 11.2 satisfies both of these conditions. Moreover, this process of guaranteeing that a unique minimal region exists for each $X_{i}$ is essentially what we did in example 11.3 to produce a valid region graph. 
These conditions provide us with a simple strategy for constructing a saturated region graph . 
![](images/e95f218fcee7baf3d52011fd69ef631b1803af5e9a6536fc3c397d785b08a264.jpg) 
We start with initial set of regions. Often, these regions will be the initial factors in $P_{\Phi}$ , although we can decide to work with bigger regions that capture some more global interactions. We then extend this set of regions into a valid region graph, where our goal is to represent appropriately any subset of variables that is shared by some of the regions. We therefore expand the set of regions to be closed under intersections. We connect these regions so that the upward closure of each region contains all of its supersets. The full procedure is shown in algorithm 11.3. Unlike the Bethe approximation, this region graph maintains the consistency of higher-order marginals. The example of figure 11.9 is an example of running this procedure on the original set of regions $\{A, B, C\}$ , $\{B, C, D\}$ , and $\{A, C, D\}$ . As our previous discussion suggests, this procedure guarantees a region graph that satisfies the region graph condition. 
Belief Propagation in Region Graphs Given a region graph, we are faced with the task of optimizing the free energy associated with its structure: 
RegionGraph-Optimize : 
$$
\begin{array}{l}{Q=\{\beta_{r}: i\in\mathcal{V}_{\mathcal{R}}\}}\\ {\tilde{F}[\tilde{P}_{\Phi}, Q]}\end{array}
$$ 
$$
\begin{array}{r l r}{\displaystyle\sum_{\boldsymbol{C}_{r^{\prime}}-\boldsymbol{C}_{r}}\beta_{r^{\prime}}(\boldsymbol{c}_{r^{\prime}})}&{=}&{\beta_{r}(\boldsymbol{c}_{r})\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad (11.30)}\\ &{}&{\quad\quad\quad\quad\quad\quad\quad\quad\forall r\in\mathcal{V}_{\mathcal{R}},\forall r^{\prime}\in\mathbf{U}\mathbf{p}(r),\forall\boldsymbol{c}_{r}\in V a l (\boldsymbol{C}_{r})}\\ {\displaystyle\sum_{\boldsymbol{C}_{r}}\beta_{r}(\boldsymbol{c}_{r})}&{=}&{1\quad\quad\quad\quad\forall r\in\mathcal{V}_{\mathcal{R}}\quad\quad\quad\quad\quad\quad\quad (11.31)}\\ {\displaystyle\beta_{r}(\boldsymbol{c}_{r})}&{\geq}&{0\quad\quad\quad\quad\forall r\in\mathcal{V}_{\mathcal{R}},\boldsymbol{c}_{r}\in V a l (\boldsymbol{C}_{r}).\quad (11.32)}\end{array}
$$ 
Our strategy for devising algorithms for solving this optimization problem is similar to the approach we took in section 11.3.6. Using the method of Lagrange multipliers, we characterize the stationary points of the target function (given the constraints) as a set of fixed-point equations. We then find an iterative algorithm that attempts to reach such a stationary point. 
We first characterize the fixed point via the Lagrange multipliers. As before, we form a Lagrangian by introducing terms for each of the constraints: from equation (11.30), we obtain a Lagrange multiplier $\lambda_{r, r^{\prime}}(c_{r})$ for every pair $r^{\prime}\,\in\,\mathbf{Up}(r)$ a d every $c_{r}\ \in\ V a l (C_{r})$ rom equation (11.31), we obtain a Lagrange multiplier $\lambda_{r}$ for every r and every $c_{r}\,\in\, V a l (C_{r})$ ∈ ; as before, we assume that we are dealing with interior fixed points only, and so do not have to worry about the inequality constraint. We diferentiate the Lagrangian relative to each of the region beliefs $\beta_{r}(c_{r})$ , and obtain the following set of fixed-point equations : 
$$
\kappa_{r}\ln\beta_{r}(\pmb{c}_{r})=\lambda_{r}+\kappa_{r}\ln\psi_{r}(\pmb{c}_{r})-\sum_{r^{\prime}\in\mathbf{Lip}(r)}\lambda_{r, r^{\prime}}(\pmb{c}_{r})+\sum_{r^{\prime}\in\mathbf{Lip}(r)}\lambda_{r^{\prime}, r}(\pmb{c}_{r^{\prime}})-\kappa_{r}\ln\psi_{r}(\pmb{c}_{r})
$$ 
For regions for which $\kappa_{r}\neq0$ , we can rewrite this equation to conclude that: 
$$
\beta_{r}(\pmb{c}_{r})=\frac{1}{Z_{r}}\psi_{r}(\pmb{c}_{r})\left (\frac{\prod_{r^{\prime}\in\mathbf{D o w n}(r)}\delta_{r\rightarrow r^{\prime}}(\pmb{c}_{r^{\prime}})}{\prod_{r^{\prime}\in\mathbf{U p}(r)}\delta_{r^{\prime}\rightarrow r}(\pmb{c}_{r})}\right)^{1/\kappa_{r}}.
$$ 
From this equality, one can conclude the following result: 
Theorem 11.6 Assume that our region graph satisfies the family preservation property. Then, at fixed points of the RegionGraph-Optimize optimization problem, we have that: 
$$
P_{\Phi}(\mathcal{X})\propto\prod_{r}(\beta_{r})^{\kappa_{r}}.
$$ 
The proof is derived from a simple cancellation of messages in the diferent terms (see exer- cise 11.16). 
This result tells us that we can reparameterize the initial distribution $P_{\Phi}$ in terms of the final beliefs obtained as fixed points of the region graph optimization problem. It tells us that we can represent the distribution in terms of a calibrated set of beliefs for the individual regions. This 
![](images/283353daf637649d98c3dc51f10bfe5acbbd3eb50d0277dead3a5afa03f1c7ae.jpg) 
Figure 11.11 The messages participating in diferent region graph computations. Participating mes- sages are marked with thicker arrows. (a) The computation of the beliefs $\beta_{r}(C_{r})$ ; (b) The set $N (r_{u}, r_{d})$ participating in the computation of the message $\delta_{r_{u}\rightarrow r_{d}}$ ; (c) The set $D (r_{u}, r_{d})$ participating in the com- putation of the message $\delta_{r_{u}\rightarrow r_{d}}$ . 
result is a very powerful one, because it holds for any set of counting numbers that satisfies the family preservation property — a very large class. Of course, this result only shows that any fixed point is a re parameter iz ation of the distribution, but not that such a re parameter iz ation exists. However, under our assumption that all of the initial factors in $\Phi$ are strictly positive, one can show that such a fixed point, and hence a corresponding re parameter iz ation, necessarily exists. 
message passing algorithm 
As we can see, unlike the case of cluster graphs, the fixed-point equations for region graphs are more involved, and do not lead directly to an elegant message passing algorithm . Indeed, the derivation of the update rules from the Lagrange multipliers often involves multiple steps of algebraic manipulation. (Although such derivations are possible in restricted cases; see exercise 11.17 and exercise 11.19.) In the remainder of this section, we present, without derivation, one set of update rules that can be derived from equation (11.34), in the specific case where the counting numbers are as presented in equation (11.29). 
The basic idea is similar to the message update used in cluster graphs. There, each sepset carried messages sent by one of the clusters through the sepset to the neighboring cluster. We can think of that as a message from the cluster to the sepset. The analog of this in region graph is a message from a region t ion below it. Thus, for each pair $r_{1}\,\rightarrow\, r_{2}$ in the region graph we will have a message $\delta_{r_{1}\to r_{2}}$ whose scope is Scope [ r ] . All messages are associated with → 2 “downward” edges of the form $r_{1}\rightarrow r_{2}$ , but they are used to define the beliefs and messages of regions that contain them. 
The definitions of the messages and the beliefs are somewhat involved. We begin by defining the beliefs of a region as a function of these messages, which is somewhat simpler: 
$$
\begin{array}{r l r}{\beta_{r}(\mathbf{\cal{C}}_{r})}&{=}&{\psi_{r}(\mathbf{\cal{C}}_{r})\left (\displaystyle\prod_{r_{u}\in{\bf U p}(r)}\delta_{r_{u}\to r}(\mathbf{\cal{C}}_{r})\right)}\\ &{}&{\left (\displaystyle\prod_{r_{d}\in{\bf D o w n}^{*}(r)}\prod_{r_{u}\in{\bf U p}(r_{d})-{\bf D o w n}^{+}(r)}\delta_{r_{u}\to r_{d}}(\mathbf{\cal{C}}_{r_{d}})\right).}\end{array}
$$ 
In other words, the belief of a region is the product of three groups of terms. The first two are very natural: the initial beliefs $\psi_{r}$ ( 1 for all regions except the top ones) and the messages from its upward regions. The last group contain all messages sent to regions below the region $r$ from regions other than the region $r$ itself and regions below it. In other words, these are messages from “external” sources to regions below the region $r$ ; see figure 11.11a. Thus, the beliefs of the region are not inﬂuenced by messages it sends down, but only by messages sent to it or its subsets from other regions. 
Again, it is instructive to compare this definition to our definition of beliefs in cluster graphs. In cluster graphs, the belief over a sepset is the product of messages from the neighboring clusters. These messages correspond in our case to messages from upward regions. The belief over a cluster $C$ is the product of its initial potential and messages sent from neighboring clusters to the sepsets adjacent to $C$ . The sepsets correspond to the regions in $\mathbf{D o w n}^{+}(C)$ ; the message sent by another clique $C^{\prime}$ to this sepset corresponds to messages sent by an “external” source to a region in $\mathbf{D o w n}^{+}(C)$ . 
We now move to defining the computation of a message from $r_{u}$ to $r_{d}$ , also illustrated in figure 11.11b, c: 
$$
\delta_{r_{u}\rightarrow r_{d}}(C_{r_{d}})=\frac{\sum_{C_{r_{u}}-C_{r_{d}}}\psi_{r_{u}}(C_{r_{u}})\prod_{r_{1}\rightarrow r_{2}\in N (r_{u}, r_{d})}\delta_{r_{1}\rightarrow r_{2}}(C_{r_{2}})}{\prod_{r_{1}\rightarrow r_{2}\in D (r_{u}, r_{d})}\delta_{r_{1}\rightarrow r_{2}}(C_{r_{2}}).}
$$ 
The numerator involves the initial factor assigned to the region, and a product of messages associated with the set of edges 
$$
N (r_{u}, r_{d})=\{(r_{1}\to r_{2})\in{\mathscr{E}}_{\mathcal{R}}: r_{1}\notin\mathbf{D o w n}^{+}(r_{u}), r_{2}\in\mathbf{D o w n}^{+}(r_{u})-\mathbf{D o w n}^{+}(r_{u})\}
$$ 
This set contains edges from sources “external” to $r_{u}$ that are outside the scope of inﬂuence of $r_{d};$ that is, they either enter $r_{u}$ directly, or enter regions below $r_{u}$ that are not below $r_{d}$ . The denominator involves a product of the messages in the set: 
$$
D (r_{u}, r_{d})=\{(r_{1}\to r_{2})\in{\mathscr{E}}_{\mathcal{R}}: r_{1}\in\mathbf{D o w n}^{+}(r_{u})-\mathbf{D o w n}^{+}(r_{d}), r_{2}\in\mathbf{D o w n}^{+}(r_{2})\}
$$ 
This set counts information that would be passed from $r_{u}$ to regions below $r_{d}$ indirectly — not through $r_{d}$ . We want to divide by these messages, since otherwise the same information would be incorporated multiple times into the beliefs and the messages. 
Applying equation (11.36) to this example, we get a set of equation representing the potentials as function of the initial factors and the messages: 
$$
\begin{array}{l c l}{{\beta_{1}}}&{{=}}&{{\psi_{1}\delta_{2\rightarrow4}\delta_{3\rightarrow5}\delta_{6\rightarrow7}}}\\ {{\beta_{2}}}&{{=}}&{{\psi_{2}\delta_{1\rightarrow4}\delta_{3\rightarrow6}\delta_{5\rightarrow7}}}\\ {{\beta_{3}}}&{{=}}&{{\psi_{3}\delta_{1\rightarrow5}\delta_{2\rightarrow6}\delta_{4\rightarrow7}}}\\ {{\beta_{4}}}&{{=}}&{{\delta_{1\rightarrow4}\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}}}\\ {{\beta_{5}}}&{{=}}&{{\delta_{1\rightarrow5}\delta_{3\rightarrow5}\delta_{4\rightarrow7}\delta_{6\rightarrow7}}}\\ {{\beta_{6}}}&{{=}}&{{\delta_{2\rightarrow6}\delta_{3\rightarrow6}\delta_{4\rightarrow7}\delta_{5\rightarrow7}}}\\ {{\beta_{7}}}&{{=}}&{{\delta_{4\rightarrow7}\delta_{5\rightarrow7}\delta_{6\rightarrow7}.}}\end{array}
$$ 
Applying equation (11.37), we can construct the messages. For example, 
$$
\delta_{4\rightarrow7}=\sum_{B}\delta_{1\rightarrow4}\delta_{2\rightarrow4}.
$$ 
One easy way to derive this message directly is to use the marginal consistency constraint: 
$$
\beta_{7}=\sum_{B}\beta_{4}.
$$ 
Plugging in the expanded form of the two beliefs, we get 
$$
\delta_{4\rightarrow7}\delta_{5\rightarrow7}\delta_{6\rightarrow7}=\sum_{B}\delta_{1\rightarrow4}\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}.
$$ 
If we now isolate $\delta_{4\rightarrow7}\ w e$ get 
$$
\delta_{4\rightarrow7}=\frac{\sum_{b}\delta_{1\rightarrow4}\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}}{\delta_{5\rightarrow7}\delta_{6\rightarrow7}}.
$$ 
After we cancel out the common terms $\delta_{5\rightarrow7}$ and $\delta_{6\rightarrow7}$ , we get the desired form. 
This message is essentially identical to the message in a cluster graph where we marginalize the other incoming messages in the cluster to send a message to a particular sepset. Here region 4 behaves as a cluster and region 7 as a sepset. The other messages incoming to region 7 have a similar form. 
Messages into the middle layer regions have more complex form. For example 
$$
\delta_{1\rightarrow4}=\frac{\sum_{A}\psi_{1}\delta_{3\rightarrow5}}{\delta_{5\rightarrow7}}.
$$ 
Again, we can use the marginal consistency constraint 
$$
\beta_{4}=\sum_{A}\beta_{1}
$$ 
to reconstruct the message. Plugging in the expanded form of the two beliefs, and isolating $\delta_{1\rightarrow4}$ we get 
$$
\delta_{1\rightarrow4}=\frac{\sum_{A}\psi_{1}\delta_{2\rightarrow4}\delta_{3\rightarrow5}\delta_{6\rightarrow7}}{\delta_{2\rightarrow4}\delta_{5\rightarrow7}\delta_{6\rightarrow7}}.
$$ 
After we cancel out $\delta_{2\rightarrow4}$ and $\delta_{6\rightarrow7}$ , we get the desired form. 
These definitions set up a message passing algorithm similar to CGraph-SP-Calibrate , except that we use the messages as formulated in equation (11.37). As with belief propagation on cluster graphs, we can prove that convergence points of such propagations are stationary points of the RegionGraph-Optimize optimization problem. 
A set of beliefs $Q$ is a stationary point of RegionGraph-Optimize for region graph $\mathcal{R}$ if and only if for every edge $(i\!-\! j)\in\mathcal{E}_{\mathcal{R}}$ there are auxiliary factors $\delta_{u\rightarrow d}(C_{d})$ that satisfy equation (11.36) and equation (11.37). 
This result is a direct generalization of theorem 11.5, and is proved in a similar way. We leave the detail as an exercise (see exercise 11.14). Much of the discussion following theorem 11.5 applies here. In particular, we do not have guarantees that iterations of message passing will converge. However, if they do, we have reached a stationary point of the energy functional. In practice, the experience is that when we consider moving from the Bethe approximation to “richer” region graphs that contain intermediate regions with larger subsets, problems of nonconverging runs are less common. For example, a region graph construction for grids is much more convergent than the corresponding cluster graph (see exercise 11.15). However, except for special cases (for example, region graphs that correspond to cluster trees), we do not know how to characterize region graphs where belief propagation converges. 
### 11.3.8 Discussion 
Cluster-graph belief propagation methods such as the ones we have described in this chapter pro- vide a general-purpose mechanism for approximate inference in graphical models. In principle, they apply to any network, including networks with high tree-width, for which exact inference is intractable. They have been applied successfully to a large number of dramatically diferent applications, including (among many others) message decoding in communication over a noisy channel (see box 11. A), predicting protein structure (see box 20. B), and image segmentation (see box 4. B). 
However, it is important to keep in mind that cluster-graph belief propagation is not a global panacea to the problem of inference in graphical models. The algorithm may not converge, and when it does converge, there may be multiple diferent convergence points. Although there are currently no conditions characterizing precisely when cluster-graph belief propagation converges, several factors seem to play a role. 
The first is the topology of the network: A network containing a large number of short loops is more likely to be nonconvergent. Although this notion has been elusive to characterize in practice, it has been shown that cluster-graph belief propagation is guaranteed to converge on networks with a single loop. 
An even more significant factor is the extent to which the factors parameterizing the network are skewed, or close to deterministic. Intuitively, deterministic factors can cause difculties in several ways. First, they often induce strong correlations between variables, which cluster-graph belief propagation (depending on the approximation chosen) can lose. This error can have an efect not only for the correlated variables, but also for marginals of variables that interact with both. Second, close-to-deterministic factors allow information to be propagated reliably through long paths in the network. Recall that part of our motivation for the running intersection property was to prevent information about some variable to be propagated infinitely through a loop. While the running intersection property prevents such loops from occurring structurally, deterministic potentials allow us to recreate them using an appropriate choice of parameters. For example, if $A$ is deterministic ally equal to $B$ , then we can have a cycle of clusters where $A$ appears in some of the clusters and $B$ in others. Although this cluster graph may satisfy the running intersection property relative to $A$ , efectively there is a cycle in which the same variable appears in all clusters. Finally, as we discussed in section 11.3.4, factors that are less skewed provide smoothing of the messages, reducing oscillations; indeed, one can even prove that, if the skew of the factors in the network is sufciently bounded, it can give rise to a contraction property that guarantees convergence. 
In summary, the key factor relating to convergence of belief propagation appears to be the extent to which the network contains strong inﬂuences that “pull” a variable in diferent directions. Owing to its local nature, the algorithm is incapable of reconciling these diferent constraints, and it can therefore oscillate as diferent messages arrive that pull it in one direction or another. 
A second problem relates to the quality of the results obtained. Despite the appeal and im- portance of the energy-based analysis, it does not (except in a few rare cases — see section 11.7) provide any guarantees on the accuracy of the marginals obtained by cluster-graph belief prop- agation. This is in contrast to the sampling-based methods of chapter 12, where we are at least assured that, if we run the algorithm for long enough, we will obtain accurate estimates of the posteriors. (Of course, the key question of “how long is long enough” does not usually have an answer, so it is not clear how important this distinction is in practice.) Empirical results show that, in the settings where cluster-graph belief propagation convergence is more likely (not too many tight loops, no highly skewed factors), one also often obtains reasonable answers. 
Importantly, these answers are often good but overconfident: The value $x\ \in\ V a l (X)$ to which cluster-graph belief propagation gives the highest probability is often the value for which $P_{\Phi}(X=x)$ is indeed the highest, but the probability assigned to $x$ by the approximation is often too high. This phenomenon arises (partly) from the fact the cluster-graph belief propagation ignores correlations between messages and can therefore count the same piece of evidence multiple times as it arrives along diferent paths, leading to overly strong conclusions. In other cases, however, the answers obtained by cluster-graph belief propagation are simply wrong (see section 11.3.1); unfortunately, there is currently no way of determining when the answers returned by a run of cluster-graph belief propagation are reasonable approximations to the true marginals. 
The intuitions described previously do help us, however, to design approximations that are more likely to produce good answers. In general, we cannot construct a cluster graph that pre- serves all of the higher-order interactions among the factors. Hence, we need to decide which factors to include in the cluster graph and how to relate them. As the preceding discussion suggests, we do better if we construct approximations that incorporate tight loops and maintain the strongest factors within clusters as much as possible. While these intuitions provide reason- able rules of thumb on how to construct approximations, it is not obvious how to capture them within a general-purpose automated cluster-graph construction procedure. 

## 11.5 Structured Variational Approximations 
In the previous two sections, we examined approximations based on belief propagation. As we saw, both methods can be viewed as optimizing an approximate energy functional over the structured variational class of pseudo-marginals. These pseudo-marginals generally do not correspond to a globally coherent joint distribution $Q$ . **The structured variational approach aims to optimize the energy functional over a family $\mathcal{Q}$ of coherent distributions $Q$ . This family is chosen to be computationally tractable, and hence it is generally not sufciently expressive to capture all of the information in $P_{\Phi}$ .** 
>  之前的部分中，我们介绍了基于信念传播的近似方法，它可以视作在结构化的伪边际上的变分类中优化近似的能量泛函，这些伪边际一般不对应于全局一致的联合分布 $Q$
>  本节介绍的结构化变分方法意在一致分布 $Q$ 的一族 $\mathcal Q$ 中优化能量泛函，这一族一般选择为计算可解的，因此它们的表示能力一般不足以充分捕获 $P_\Phi$ 中的全部信息

More precisely, we aim to address the following maximization problem: 

$\begin{array}{l l}{{\mathrm{Find}}}&{{Q\in{\mathcal Q}}}\\ {{\mathrm{maximize}}}&{{F[\tilde{P}_{\Phi},Q]}}\end{array}$ 

where $\mathcal{Q}$ is a given family of distributions. In these methods, we are using the exact energy functional $F[\tilde{P}_{\Phi},\bar{Q}]$ , which satisfies theorem 11.2. Thus, maximizing the energy functional corresponds directly to obtaining a better approximation to $P_{\Phi}$ (in terms of $D(Q\|P_{\Phi}))$ . 
>  我们意在解决以上优化问题，其中 $\mathcal Q$ 是给定的一族分布
>  我们使用满足定理 11.2 的精确能量函数，因此最大化能量泛函直接对应于在 $D(Q||P_\Phi)$ 的维度上获得对 $P_\Phi$ 的在 $\mathcal Q$ 中的最优近似 $Q$

The main parameter in this maximization problem is the choice of family Q . This choice induces a trade-of. On one hand, families that are “simpler,” that is, that can be described by a Bayesian network or a Markov network with small tree-width, allow more efcient inference. As we will see, simpler families also allow us to solve the maximization problem efciently. On the other hand, if the fa y $\mathcal{Q}$ is too restrictive, then it cannot r resent distributions that are good approximations of $P_{\Phi}$ , giving rise to a poor approximation Q . In either case, this family is generally chosen to have enough structure that allows inference to be tractable, giving rise to the name structured variational approximation. 
>  对于分布族 $\mathcal Q$ 的选择是该最大化问题的主要参数，该选择存在 trade-off
>  更简单的分布可以在其中高效推理，同时求解最大化问题也更快，但表示性更弱
>  但在这类方法中，我们应该保证 $\mathcal Q$ 的结构性充足使得在其中推理是可解的，因此这类方法被称为结构化变分近似

As we will see, the methods of this type difer from generalized belief propagation in several ways. They are guaranteed to lower-bound the log-partition function, and they also are guaranteed to converge. 
>  这类方法和通用的信念传播不同，它们由确定的下界和对数划分函数，它们确定收敛

### 11.5.1 The Mean Field Approximation 
The first approach we consider is called the mean field approximation. As we will see, in many respects, it resembles the algorithm obtained using the Bethe approximation to the energy functional. In particular, the resulting algorithm performs message passing where the messages are distributions over single variables. As we will see, however, the form of the updates is somewhat diferent. 
>  首先考虑平均场近似，它在许多方面和使用 Bethe 近似方法近似能量泛函得到的算法类似，例如，平均场近似得到的算法执行的消息传递中的消息也是单个变量上的分布

#### 11.5.1.1 The Mean Field Energy 
Unlike our presentation in earlier sections, we begin our discussion with the energy functional, and we derive the algorithm directly from analyzing it. The mean field algorithm finds the distribution $Q$ , which is closest to $P_{\Phi}$ in terms of $D(Q\|P_{\Phi})$ within the class of distributions representable as a product of independent marginals: 

$$
Q({\mathcal{X}})=\prod_{i}Q(X_{i}).\tag{11.48}
$$
On the one hand, the approximation of $P_{\Phi}$ as a fully factored distribution is likely to lose a lot of information in the distribution. On the other hand, this approximation is computationally attractive, since we can easily evaluate any query on $Q$ by a product over terms that involve the variables in the scope of the query. Moreover, to represent $Q$ , we need only to describe the marginal probabilities of each of the variables. 

>  平均场算法考虑的分布族 $\mathcal Q$ 满足其中的分布可以完全分解为所有独立边际乘积的形式，即 $\mathcal X$ 中的所有随机变量相互独立
>  这样完全分解的近似形式可能会丢失 $P_\Phi$ 中的许多信息，但计算上是简单可解的，我们可以将任意对 $Q$ 的查询完全分解，要表示 $Q$ 时，我们也仅需要考虑各个独立边际

As in previous sections, the mean field algorithm is derived by considering fixed points of the energy functional . We thus begin by considering the form of the energy functional in equation (11.3) when $Q$ has the form of a product distribution as in equation (11.48). We can then characterize its fixed points and thereby derive an iterative algorithm to find such fixed points. 
>  确定了 $Q$ 的形式，我们开始推导平均场算法的迭代式优化算法

The functional contains two terms. The first is a sum of terms of the form $E_{U_{\phi}\sim Q}[\ln\phi]$ , where we need to evaluate 

$$
\begin{array}{r c l}{{E_{\pmb U_{\phi}\sim Q}[\ln\phi]}}&{{=}}&{{\displaystyle\sum_{\pmb{u}_{\phi}}Q(\pmb{u}_{\phi})\ln\phi(\pmb{u}_{\phi})}}\\ {{}}&{{=}}&{{\displaystyle\sum_{\pmb{u}_{\phi}}\left(\prod_{X_{i}\in \pmb U_{\phi}}Q(x_{i})\right)\ln\phi(\pmb{u}_{\phi}).}}\end{array}
$$ 
As shown, we can use the form of $Q$ to compute $Q(\pmb{u}_{\phi})$ as a product of marginals, allowing the evaluation of this term to be performed in time linear in the number of values of $U_{\phi}$ . Because this cost is linear in the description size of the factors of $P_{\Phi}$ , we cannot expect to do much better. 

>  能量泛函的第一项是形式为 $E_{\pmb U_\phi \sim Q}[\ln \phi]$ 的乘积，利用 $Q$ 的结构，我们可以将 $Q(\pmb u_\phi)$ 完全分解，因此仅需要线性于 $\pmb U_\phi$ 的取值数量的时间就可以评估 $Q(\pmb u_\phi)$

As we saw in section 8.4.1, the term $H_{Q}(\mathcal X)$ also decomposes in this case. 

Corollary 11.3 

$$
H_{Q}({\mathcal{X}})=\sum_{i}H_{Q}(X_{i}).\tag{11.49}
$$ 
Thus, the energy functional for a fully factored distribution $Q$ can be rewritten simply as a sum of expectations, each one over a small set of variables. Importantly, the complexity of this expression depends on the size of the factors in $P_{\Phi}$ , and not on the topology of the network. Thus, the energy functional in this case can be represented and manipulated efectively, even in networks that would require exponential time for exact inference. 
>  同理，能量泛函中的第二项 $Q$ 的熵也可以完全分解
>  因此整个能量泛函可以重写为期望的和，其中每个期望仅关于一小组变量，该表示的复杂度仅依赖于 $P_\phi$ 中因子的大小，不依赖于网络拓扑 (因为 $Q$ 没有拓扑结构)

Example 11.10 
Continuing our running example, consider the form of the mean field energy for a $4\times4$ grid network. Based on our discussion, we see that it has the form 

$$
{\begin{array}{r c l}{F[{\tilde{P}}_{\Phi},Q]}&{=}&{\displaystyle\sum_{i\in\{1,2,3\},j\in\{1,2,3,4\}}E_{Q}[\ln\phi(A_{i,j},A_{i+1,j})]}\\ &&{+\displaystyle\sum_{i\in\{1,2,3,4\},j\in\{1,2,3\}}E_{Q}[\ln\phi(A_{i,j},A_{i,j+1})]}\\ &&{+\displaystyle\sum_{i\in\{1,2,3,4\},j\in\{1,2,3,4\}}H_{Q}(A_{i,j}).}\end{array}}
$$ 
We see that the energy functional involves only expectations over single variables and pairs of neighboring variable expression has the same general form for an $n\times n$ grid. Thus, although the tree-width of an $n\times n$ × grid is exponential in $n$ , the energy functional can be represented and computed in cost $O(n^{2})$ ; that is, in a time linear in the number of variables. 

#### 11.5.1.2 Maximizing the Energy Functional: Fixed-point Characterization 
The next step is to consider the task of optimizing the energy function: finding the distribution $Q$ for which this energy functional is maximized: 
>  将能量泛函分解后，考虑对它的优化

Mean-Field
Find $\{Q(X_i)\}$
maximizing $F[\tilde P_\Phi, Q]$
subject to 

$$
\begin{align}
Q(\mathcal X)  &= \prod_i Q(X_i)\tag{11.50}\\
\sum_{x_i}Q(x_i) &=1,\quad\forall i\tag{11.51}
\end{align}
$$

To simplify notation, from now on we use $X_{-i}$ to denote $\mathcal{X}-\{X_{i}\}$ 

Note that, unlike the cluster-graph belief propagation algorithms of section 11.3 and the expectation propagation algorithm of section 11.4, here we are not approximating the objective. We are approximating only the optimization space by selecting a space of distributions $\mathcal{Q}$ that generally does not contain our original distribution $P_{\Phi}$ . 
>  注意和簇图信念传播算法和期望传播算法不同，这里我们没有对目标进行近似，我们仅近似了分布选择的空间 $\mathcal Q$，近似后的空间一般不会包含原始 $P_\Phi$

As with the previous optimization problems in this chapter, we use the method of Lagrange multipliers to derive a characterization of the stationary points of $F[\tilde{P}_{\Phi},Q]$ . However, the structure of $Q$ allows us to consider the optimal value of each component (that is, marginal distribution) given the rest. (This iterative optimization procedure was not feasible in cluster trees and graphs due to constraints that relate diferent beliefs.) 
>  我们用拉格朗日乘子法描述 $F[\tilde P_\Phi, Q]$ 的驻点
>  此时 $Q$ 的结构允许我们在给定其他的情况下，考虑每个成分 (边际分布) 单独的最优值 (这种迭代优化过程在簇树和簇图中是无效的，因为其中的信念/边际存在相关性)

We now provide a set of fixed-point equations that characterize the stationary points of the mean field optimization problem: 

**Theorem 11.9** 
The distribution $Q(X_{i})$ is a local maximum of Mean-Field given $\{Q(X_{j})\}_{j\neq i}$ if and only if 

$$
Q(x_{i})=\frac{1}{Z_{i}}\exp\left\{\sum_{\phi\in\Phi}E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]\right\},\tag{11.52}
$$ 
where $Z_{i}$ is a local normalizing constant and $E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]$ is the conditional expectation given the value $x_{i}$ 

$$
E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]=\sum_{\pmb{u}_{\phi}}Q(\pmb{u}_{\phi}\mid x_{i})\ln\phi(\pmb{u}_{\phi}).
$$

>  定理
>  平均场优化问题中，给定 $\{Q(X_j)\}_{j\ne i}$，分布 $Q(X_i)$ 当且仅当满足 (11.52) 时，它使得平均场优化问题达到局部最大值
>  (11.52) 中，$Z_i$ 是规范化常数， $E_{\mathcal X \sim Q}[\ln \phi \mid x_i]$ 是给定值 $x_i$ 时的条件期望

Proof 
The proof of this theorem relies on proving the fixed-point characterization of the individual marginal $Q(X_{i})$ in terms of the other components, $Q(X_{1}),\dots,Q(X_{i-1}),\,Q(X_{i+1}),\dots,$ , $Q(X_{n})$ , as specified in equation (11.52). 

We first consider the restriction of our objective $F[\tilde{P}_{\Phi},Q]$ to those terms that involve $Q(X_{i})$ : 
>  先将能量泛函中和 $Q(X_i)$ 有关的部分写出，记作 $F_i[Q]$

$$
F_{i}[Q]=\sum_{\phi\in\Phi}{\pmb E}_{{\pmb U}_{\phi}\sim Q}[\ln\phi]+{\pmb H}_{Q}(X_{i}).\tag{11.53}
$$ 
To optimize $Q(X_{i})$ , we define the Lagrangian that consists of all terms in $F[\tilde{P}_{\Phi},Q]$ that involve $Q(X_{i})$ 
>  我们为 $F_i[Q]$ 和约束 $\sum_{x_i} Q(x_i) = 1$ 写出拉格朗日函数
$$
L_{i}[Q]=\sum_{\phi\in\Phi}{\pmb E}_{{\pmb U}_{\phi}\sim Q}[\ln\phi]+{\pmb H}_{Q}(X_{i})+\lambda\left(\sum_{x_{i}}{Q}(x_{i})-1\right).
$$ 
The Lagrange multiplier $\lambda$ corresponds to the constraint that $Q(X_{i})$ is a distribution. We now take derivatives with respect to $Q(x_{i})$ . The following result plays an important role in the remainder of the derivation: 
>  我们将拉格朗日函数相对于 $Q(x_i)$ 求导

**Lemma 11.1** 
If $\begin{array}{r}{Q(\mathcal{X})=\prod_{i}Q(X_{i})}\end{array}$ then, for any function $f$ with scope $U$ , 

$$
{\frac{\partial}{\partial Q(x_{i})}}E_{U\sim Q}[f(U)]=E_{U\sim Q}[f(U)\mid x_{i}].
$$ 
The proof of this lemma is left as an exercise (see exercise 11.24). 

Using this lemma, and standard derivatives of entropies, we see that 

$$
{\frac{\partial}{\partial Q(x_{i})}}L_{i}=\sum_{\phi\in\Phi}{\pmb E}_{{\pmb X}\sim Q}[\ln\phi\mid x_{i}]-\ln Q(x_{i})-1+{\lambda}.
$$ 
Setting the derivative to 0 , and rearranging terms, we get that 

$$
\ln Q(x_{i})=\lambda-1+\sum_{\phi\in\Phi}E_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}].
$$

We take exponents of both sides and renormalize; because $\lambda$ is constant relative to $x_{i}$ , it drops out in the renormalization, so that we obtain the formula in equation (11.52). 
>  将导数设为零，两边取指数并且重规范化，就得到了 (11.52)

This derivation, by itself, shows only that the solution of equation (11.52) is a stationary point of equation (11.53). To prove that it is a maximum, we note that equation (11.53) is a sum of two terms: $\begin{array}{r}{\sum_{\phi\in\Phi_{\circ}}E_{U_{\phi}\sim Q}[\ln\phi]}\end{array}$ is linear in $Q(X_{i})$ , given all the other components $Q(X_{j});H_{Q}(X_{i})$ is a concave function in $Q(X_{i})$ . As a whole, given the other components of $Q$ , the function $F_{i}$ is concave in $Q(X_{i})$ , and therefore has a unique global optimum, which is easily verified to be equation (11.52) rather than any of the extremal points. 
>  上述的论证证明了 (11.52) 的解是 (11.53) 的驻点，要进一步证明它是最大值
>  注意到 (11.53) 中， $\begin{array}{r}{\sum_{\phi\in\Phi_{\circ}}E_{U_{\phi}\sim Q}[\ln\phi]}\end{array}$ 在给定其他 $Q(X_j)$ 是线性于 $Q(X_i)$，以及 $H_Q(X_i)$ 是关于 $Q(X_i)$ 的凹函数，因此整个函数 $F_i$ 在给定其他 $Q(X_j)$ 的情况下是关于 $Q(X_i)$ 的凹函数，故存在唯一的全局最大值
>  容易验证在驻点取得最大值，而不是其他极限点

From this it follows that: 

**Corollary 11.4** 
The distribution $Q$ is a stationary point of Mean-Field if and only if, for each $X_{i}.$ , equation (11.52) holds. 
>  引理
>  分布 $Q$ 当且仅当对于每个 $X_i$ ，都有 (11.52) 成立时，它是 Mean-Field 问题的驻点

In contrast to theorem 11.9, this result only provides a characterization of stationary points of the objective, and not necessarily of its optima. The stationary points include local maxima, local minima, and saddle points. The reason for the diference is that, although each “coordinate” $Q(X_{i})$ is guaranteed to be locally maximal given the others, the direction that locally improves the objective may require a coordinated change in several components. We return to this point in section 11.5.1.3. 

We now move to interpreting this characterization. The key term in equation (11.52) is the argument in the expectation. We can prove the following property. 

**Corollary 11.5**
In the mean field approximation, $Q(X_i)$ is locally optimal only if

$$
Q(x_{i})={\frac{1}{Z_{i}}}\exp\left\{E_{\pmb X_{-i}\sim Q}[\ln P_{\Phi}(x_{i}\mid \pmb X_{-i})]\right\}\tag{11.54}
$$ 
where $Z_{i}$ is a normalizing constant. 
>  引理
>  平均场近似中，$Q(X_i)$ 仅在满足 (11.54) 时构成局部最优，其中 $Z_i$ 是规范化常数

Proof 
Recall that $\begin{array}{r}{\tilde{P}_{\Phi}\;=\;\prod_{\phi\in\Phi}\phi}\end{array}$ is the unnormalized measure defined by $\Phi$ . Due to the linearity of expectation: 

$$
\sum_{\phi\in\Phi}{\pmb E}_{\mathcal{X}\sim Q}[\ln\phi\mid x_{i}]={\pmb E}_{\mathcal{X}\sim Q}\Bigl[\ln\tilde{P}_{\Phi}(X_{i},{\pmb X}_{-i})\mid x_{i}\Bigr].
$$

Because $Q$ is a product of marginals, we can rewrite $Q(X_{-i}\mid x_{i})=Q(X_{-i})$ , and get that: 
>  将 $Q(X_{-i} \mid x_i)$ 重写为 $Q(X_{-i})$ (因为 $X_i$ 之间完全相互独立)

$$
{\pmb E}_{\mathcal{X}\sim Q}\Bigl[\ln\tilde{P}_{\Phi}(X_{i},{\pmb X}_{-i})\mid x_{i}\Bigr]={\pmb E}_{{\pmb X}_{-i}\sim Q}\Bigl[\ln\tilde{P}_{\Phi}(x_{i},{\pmb X}_{-i})\Bigr].
$$ 
Using properties of conditional distributions, it follows that: 

$$
{\tilde{P}}_{\Phi}(x_{i},\pmb X_{-i})=Z P_{\Phi}(x_{i},\pmb X_{-i})=Z P_{\Phi}(\pmb X_{-i})P_{\Phi}(x_{i}\mid \pmb X_{-i}).
$$ 
We conclude that 

$$
\sum_{\phi\in\Phi}{\pmb E}_{\mathcal X\sim Q}[\ln\phi\mid x_{i}]={\pmb E}_{\pmb X_{-i}\sim Q}[\ln P_{\Phi}(x_{i}\mid \pmb X_{-i})]+{\pmb E}_{\pmb X_{-i}\sim Q}[\ln P_{\Phi}(\pmb X_{-i})Z].
$$ 
Plugging this equality into the update equation equation (11.52), we get that 

$$
{\cal Q}(x_{i})=\frac{1}{Z_{i}}\exp\left\{E_{\pmb X_{-i}\sim{\cal Q}}[\ln{ P}_{\Phi}(x_{i}\mid \pmb X_{-i})]\right\}\exp\left\{E_{\pmb X_{-i}\sim{ Q}}[\ln{P}_{\Phi}(\pmb X_{-i})Z]\right\}.
$$ 
The term $\ln P_{\Phi}(\pmb X_{-i})Z$ does not depend on the value of $x_{i}$ . Recall that when we multiply a belief by a constant factor, it does not change the distribution $Q$ ; in fact, as we renormalize the distribution at the end to sum to 1, this constant will be “absorbed” into the normalizing function, to achieve normalization. Thus, we can simply ignore this term, thereby achieving the desired conclusion. We note that this type of algebraic manipulation will prove useful multiple times throughout this section. 

This corollary shows that $Q(x_{i})$ is the geometric average of the conditional probability of $x_{i}$ given all other variables in the domain. The average is based on the probability that $Q$ assigns to all possible assignments to the variables in the domain. In this sense, the mean field approximation requires that the marginal of $X_{i}$ be “consistent” with the marginals of other variables. 
>  该引理表明了 $Q(x_i)$ 是给定作用域中其他所有变量时 $x_i$ 的条件概率的几何平均 ($\ln P_\Phi(x_i \mid \pmb X_{-i})$ 在 $Q$ 中的期望)

Note that, in $P_{\Phi}$ , we can also represent the marginal of $X_{i}$ as an average: 

$$
P_{\Phi}(x_{i})=\sum_{x_{-i}}P_{\Phi}(x_{-i})P_{\Phi}(x_{i}\mid x_{-i})=E_{\pmb X_{-i}\sim P_{\Phi}}[P_{\Phi}(x_{i}\mid \pmb X_{-i})].
$$ 
This average is an arithmetic average, whereas the one used in the mean field approximation is a geometric average. In general, the latter tends to lead to marginals that are more sharply peaked than the original marginals in $P_{\Phi}$ . More significant, however, is the fact that the expectations in equation (11.55) are taken relative to $P_{\Phi}$ , whereas the ones in equation (11.54) are taken relative to the approximation $Q$ . Thus, this similarity does not imply as a consequence that our approximation in $Q$ to the marginals in $P_{\Phi}$ is a good one. 
>  $P_\Phi$ 中，$P_\Phi(x_i)$ 则可以写为 $x_i$ 的条件概率的算数平均 ($P_\Phi(x_i \mid \pmb X_{-i})$ 在 $P_\Phi$ 中的期望)
>  一般来说，几何平均会使得我们得到的分布比 $P_\Phi$ 中的分布更加锐利，同时注意二者一个是在 $Q$ 中期望，一个是在 $P_\Phi$ 中取期望，故虽然形式上相似，这不表示 $Q$ 中的对于 $P_\Phi$ 中的边际的近似是好的

#### 11.5.1.3 Maximizing the Energy Functional: The Mean Field Algorithm 
How do we convert the fixed-point equation of equation (11.52) into an update algorithm? We start by observing that if $X_{i}\,\notin\,S c o p e[\phi]$ then $E_{U_{\phi}\sim Q}[\ln\phi\mid x_{i}]=E_{U_{\phi}\sim Q}[\ln\phi]$ . Thus, expectation terms on such factors are independent of the value of $X_{i}$ . Consequently, we can absorb them into the normalization constant $Z_{i}$ and get the following simplification. 
>  进一步观察 (11.52)，发现 $E_{\pmb U_{\phi}\sim Q}[\ln\phi\mid x_{i}]=E_{\pmb U_{\phi}\sim Q}[\ln\phi]$ 在 $X_i \not\in Scope[\phi]$ 时成立
>  因此，我们进一步缩小范围，简化到仅和包含 $X_i$ 的因子有关的形式，其他无关项融合到规范化常数 $Z_i$ 中

**Corollary 11.6**
In the mean field approximation, $Q(X_{i})$ is locally optimal only if 

$$
Q(x_{i})=\frac{1}{Z_{i}}\exp\left\{\sum_{\phi:X_{i}\in S c o p e[\phi]}E_{(U_{\phi}-\{X_{i}\})\sim Q}[\ln\phi(U_{\phi},x_{i})]\right\}.\tag{11.56}
$$ 
where $Z_{i}$ is a normalizing constant. 
> 引理
> 平均场近似中，$Q(X_i)$ 仅在满足 (11.56) 时为局部最优

This representation shows that $Q(X_{i})$ has to be consistent with the expectation of the potentials in which it appears. In our grid network example, this characterization implies that $Q(A_{i,j})$ is a product of four terms measuring its interaction with each of its four neighbors: 

$$
Q(a_{i,j})=\frac{1}{Z_{i,j}}\exp\left\{\begin{array}{l l}{\sum_{a_{i-1,j}}Q(a_{i-1,j})\ln(\phi(a_{i-1,j},a_{i,j}))+}\\ {\sum_{a_{i,j-1}}Q(a_{i,j-1})\ln(\phi(a_{i,j-1},a_{i,j}))+}\\ {\sum_{a_{i+1,j}}Q(a_{i+1,j})\ln(\phi(a_{i,j},a_{i+1,j}))+}\\ {\sum_{a_{i,j+1}}Q(a_{i,j+1})\ln(\phi(a_{i,j},a_{i,j+1}))}\end{array}\right\}.\tag{11.57}
$$ 
Each term is a (geometric) average of one of the potentials involving $A_{i,j}$ . For example, the final term in the exponent represents a geometric average of the potential between $A_{i,j}$ and $A_{i,j+1}$ , averaged using the distribution $Q(A_{i,j+1})$ . 

>  因此，$Q(X_i)$ 的最优值需要和它相关的因子上的势能的期望的和一致
>  在网格网络中，$a_{i, j}$ 相关的因子就是 $a_{i, j}$ 分别和它的上下左右变量组成的二变量因子
>  (11.56) 中的求和中，每一项都是包含了 $X_i$ 的势能的几何平均 ( $\ln \phi(U_\phi, x_i)$ 在 $Q$ 上的期望)

The characterization of corollary 11.6 provides tools for developing an algorithm to maximize $F[\tilde{P}_{\Phi},Q]$  . For example, examining equation (11.57), we see that we can easily evaluate the term within the exponential by considering each of $A_{i,j}$ ’s neighbors and computing the interaction between the values that neighbor can take and possible values of $A_{i,j}$ . Moreover, in this example, we see that $Q(A_{i,j})$ does not appear on the right-hand side of the update rule. Thus, we can choose $Q(A_{i,j})$ , which satisfies the required equality by assigning it to the term denoted by the right-hand side of the equation. 
>  我们根据引理 11.6 进行对 $Q(x_i)$ 的更新，每次取一个 $X_i$ 的可能值 $x_i$ ，计算和 $X_i$ 相关的因子上的期望值，得到结果后赋值给 $Q(x_i)$

This last observation is true in general. All the terms on the right-hand side of equation (11.56) involve expectations of variables other than $X_{i}$ , and do not depend on the choice of $Q(X_{i})$ . We can achieve equality simply by evaluating the exponential terms for each value $x_{i}$ , normalizing the results to sum to 1 , and then assigning them to $Q(X_{i})$ . As a consequence, we reach the optimal value of $Q(X_{i})$ in one easy step. 
>  (11.56) 中 RHS 的所有项涉及的是 $X_i$ 以外的变量上的期望，不依赖于 $Q(X_i)$ 的值，因此我们为每个可能的 $x_i$ 值直接计算 $Q(x_i)$ 即可，最后再进行归一化，得到完整的 $Q(X_i)$，得到的就是 (固定其他 $Q(X_j)$ 情况下的) 最优的 $Q(X_i)$ 

This last statement must be interpreted with some care. The resulting value for $Q(X_{i})$ is its optimal value given the choice of all other marginals. Thus, this step optimizes our function relative only to a single coordinate in the space — the marginal of $Q(X_{i})$ . To optimize the function in its entirety, we need to optimize relative to all of the coordinates. We can embed this step in an iterated coordinate ascent algorithm, which repeatedly optimizes a single marginal at a time, given fixed choices to all of the others. The resulting algorithm is shown in algorithm 11.7. Importantly, a single optimization of $Q(X_{i})$ does not usually sufce: a subsequent modification to another marginal $Q(X_{j})$ may result in a diferent optimal parameterization for $Q(X_{i})$ . Thus, the algorithm repeats these steps until convergence. Note that, in practice, we do not test for equality in line 9, but rather for equality up to some fixed small-error tolerance. 
>  注意我们在推导时假定其他所有边际是给定的，因此得到的 $Q(X_i)$ 的最优解也是该情况下的最优解
>  因此，这一步优化实际上仅在空间的单个坐标轴上 ($Q(X_i)$ 的边际) 优化了我们的函数 $Q$
>  要完整优化 $Q$，我们需要对全部坐标轴优化，因此我们考虑迭代式坐标上升算法，也就是固定其他所有选择，一次优化一个坐标轴 (边际)，迭代式进行，这样得到的算法就是 algorithm 11.7
>  注意仅优化 $Q(X_i)$ 一次往往不够，其他边际 $Q(X_j)$ 的更新也会为 $Q(X_i)$ 带来更优的解，因此我们需要重复迭代直到收敛
>  实践中测试是否收敛时，我们在 line9 不会要求完全相等，而是有一定容忍误差

![[PGM-Algorithm11.7.png]]

A key property of the coordinate ascent procedure is that each step leads to an increase in the energy functional. **Thus, each iteration of Mean-Field results in a better approximation $Q$ to the target density $P_{\Phi}$ , guaranteeing convergence.** 
>  坐标上升过程是保证收敛的，因为每一步都保证能量泛函是不减的，并且能量泛函存在上界，当且仅当 $P_\Phi$ 和 $Q$ 之间的 KL 散度为零时达到
>  也就是说每一次迭代得到的都是对目标分布 $P_\Phi$ 更好的近似 $Q$

**Theorem 11.10**
The Mean-Field iterations are guaranteed to converge. Moreover, the distribution $Q^{*}$ returned by Mean-Field is a stationary point of $F[\tilde{P}_{\Phi},Q]$ , subject to the constraint that $\begin{array}{r}{Q(\mathcal{X})=\prod_{i}Q(X_{i})}\end{array}$ is a distribution. 
>  定理
>  算法 11.7 保证收敛，并且返回的分布 $Q^*$ 是 $F[\tilde P_\Phi, Q]$ 的驻点，且满足约束 (11.50) 和 (11.51)，即 $Q(\mathcal X) = \prod_i Q(X_i)$ 是一个分布

Proof 
We showed earlier that each iteration of Mean-Field is monotonically nondecreasing in $F[\tilde{P}_{\Phi},Q]$ . Because the energy functional is bounded, the sequence of distributions represented by successive iterations of Mean-Field must converge. At the convergence point the fixed-point equations of theorem 11.9 hold for all the variables in the domain. As a consequence, the convergence point is a stationary point of the energy functional. 
>  证明
>  我们直到算法的每次迭代在能量泛函 $F[\tilde P_\Phi, Q]$ 的值上都是不减的，而能量泛函有界，因此多次迭代得到的连续的分布序列一定收敛
>  根据上面的推导，收敛点上，定理 11.9 的固定点方程显然对于定义域中的所有变量都是满足的，因此收敛点是能量泛函的驻点 (也就是满足约束)

As we discussed, the distribution $Q^{*}$ returned by Mean-Field is not necessarily a local optimum of the algorithm. However, local minima and saddle points are not stable convergence points of the algorithm, in the sense that a small perturbation of $Q$ followed by optimization will lead to a better convergence point. Because the algorithm is unlikely to accidentally land precisely on the unstable point and get stuck there, in practice, the convergence points of the algorithm are local maxima. 
>  算法返回的分布 $Q^*$ 保证是驻点，但不一定是算法的局部最优，但是，局部最小值和鞍点一般不会是算法稳定的收敛点，因为在这些点上，对 $Q$ 的小的扰动就可以让它继续前往更好的收敛点
>  因为算法恰好完全落在一个不稳定的鞍点的概率极低，故我们可以认为在实践中认为算法的收敛点就是局部最大值

In general, however, the result of the mean field approximation is a local maximum, and not necessarily a global one. 
>  总的来说，该近似算法的结果就是局部最大值，但不一定是全局最大值

Example 11.11
Consider a distribution $P_{\Phi}$ that is an approximate XOR (exclusive or) of two variables $A$ and $B$ , that ${P_{\Phi}}(a,b)=0.5-\epsilon$ if $a\neq b$ and ${\cal P}_{\Phi}(a,b)=\epsilon$ if $a=b$ . Clearly, we cannot approxima $P_{\Phi}$ by a product of marginals, since such a product cannot capture the relationship between A and $B$ . It turns out that if $\epsilon$ is sufciently small, say 0.01 , then the energy potential surface has two local maxima that correspond to the two cases where $a\neq b$ . See figure 11.16. (For sufciently large ϵ , such as 0.1 , the mean field approximation has a single maximum point at the uniform distribution.) 

We can use standard strategies, such as multiple random restarts, to try to avoid getting stuck in local maxima. However, these do not overcome the basic shortcoming of the mean field approximation, which is apparent in this example. The approximation cannot describe complex posteriors, such as the XOR posterior we discussed. And thus, we cannot expect it to give satisfactory approximations in these situations. To provide better approximations, we must use a richer class of distributions $\mathcal{Q}_{i}$ , which has greater expressive power. 
>  可以用一些常规策略，例如多次随机启动，以尝试避免卡在局部最大值
>  但平均场近似的最大限制仍然在于它无法描述带有交互的复杂分布

### 11.5.2 Structured Approximations 
The mean field algorithm provides an easy approximation method. However, it is limited by forcing $Q$ to be a very simple distribution. As we just saw, the fact that all variables are independent of each other in $Q$ can lead to very poor approximations. Intuitively, if we use a distribution $Q$ that can capture some of the dependencies in $P_{\Phi}$ , we can get a better approximation. Thus, we would like to explore the spectrum of approximations between the mean field approximation and exact inference. 

A natural approach to get richer approximations that capture some of the dependencies in $P_{\Phi}$ is to use network structures of diferent complexity. By adding and removing edges from the network we can control the cost of inference in the approximating distribution and how well it captures dependencies in the target distribution. We can achieve this type of ﬂexibility by using either Bayesian networks or Markov networks. Both types of networks lead to similar approximations, and so we focus on the undirected case, parameterized as Gibbs distributions (so that we are not restricted to factors over maximal cliques). Exercise 11.34 develops similar ideas using a Bayesian network approximation. 

#### 11.5.2.1 Fixed-Point Characterization 
We now consider the form of the variational approximation when we are given a general form of $Q$ as a Gibbs parametric family. Formally, we assume we are given a set of potential scopes $\{C_{j}\subseteq\mathcal{X}:j=1,.\,.\,.\,,J\}$ . We can then choose an approximation $Q$ that has the form: 

$$
Q(\mathcal X)=\frac{1}{Z_{Q}}\prod_{j=1}^{J}\psi_{j},
$$ 
where $\psi_{j}$ is a factor with $S c o p e[\psi_{j}]=C_{j}$ . 

Example 11.12 Consider again the grid network example. There are many possible approximating network struc- tures we can choose that allow for efcient inference. As a concrete example, we might choose potential scopes $\{A_{1,1},A_{1,2}\}$ , $\{A_{1,2},A_{1,3}\},.\,.\,.\,,\{A_{2,1},A_{2,2}\}$ , $\{A_{2,2},A_{2,3}\},.\,.\,.$ . That is, we pre- serve the dependencies between variables in the same row, but ignore the ones that relate difer- ent columns. Alternatively, we can consider an approximation that preserves dependencies along columns and ignores the dependencies between rows. As we can see in figure 11.17, in both cases, 

the structure we use is a collection of independent chain structures. Exact inference with such structures is linear, and so the cost of inference is not much worse than in the mean field approx- imation. Clearly, we can also consider many other structures for the approximating distributions. These might introduce additional dependencies and can have higher cost in terms of inference. We will return to the question of what structure to use. 

Assume that we decide on the form of the potentials for the approxi ating family $\mathcal{Q}$ . As before, we consider the form of the energy functional for a distribution Q in this family. We then characterize the stationary points of the functional, and we use those to derive an iterative optimization algorithm. 

As before, evaluating the terms that involve $E_{U_{\phi}\sim Q}[\ln\phi]$ requires performing expectations ∼ with respect to the variables in Scope [ φ ] . Unlike the case of mean field approximation, the complexity of computing this expectation depends on the structure of the approximating distri- bution. However, we assume that we can solve this problem by exact inference (in the network corresponding to $Q.$ ), using the methods we discussed in previous chapters. 

As discussed in section 8.4.1, the entropy term in the energy functional also reduces to computing a similar set of expectation terms: 

Proposition 11.5 

$$
{H_{Q}}(\mathcal{X})=-\sum_{j=1}^{J}{E_{{C_{j}}\sim Q}}[\ln{{\psi_{j}}({C_{j}})}]+\ln{Z_{Q}}.
$$ 
Overall, we obtain the following form for the energy functional, for distributions $Q$ in the family $\mathcal{Q}$ : 

$$
{\cal F}[\tilde{P}_{\Phi},Q]=\sum_{k=1}^{K}{\pmb E}_{Q}[\ln\phi_{k}]-\sum_{j=1}^{J}{\pmb E}_{Q}[\ln\psi_{j}]+\ln Z_{Q}.
$$ 
As before, the hard question is how to optimize the potential to get the best approximation. We solve this problem using the same general strategy we discussed in the context of the mean field approximation. First, we derive the fixed-point equations that hold when the approximation is a local maximum (or, more precisely, a stationary point) of the energy functional. We then use these fixed-point equations to help derive an optimization algorithm. 

fixed-point equations 

Theorem 11.11 

We derive the fixed-point equations by taking derivatives of $F[\tilde{P}_{\Phi},Q]$ with respect to param- eters of the distribution $Q$ . In our case, the parameters will be an entry $\psi_{j}(c_{j})$ in each of the factors that define the distribution. We then set those equations to zero, obtaining the following result: 

If $\begin{array}{r}{Q(\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}}\end{array}$ Q , then the potential $\psi_{j}$ is a stationary point of the energy functional if and only if 

$$
\psi_{j}(c_{j})\propto\exp\left\{E_{Q}\Big[\ln\tilde{P}_{\Phi}\ |\ c_{j}\Big]-\sum_{k\neq j}E_{Q}[\ln\psi_{k}\ |\ c_{j}]-F[\tilde{P}_{\Phi},Q]\right\}.
$$ 
The proof is straightforward algebraic manipulation and is left as an exercise (exercise 11.26). 

This theorem establishes a characterization of the fixed point as the diference between the expected value of logarithm of the original potentials and the expected value of the logarithm of the approximating potentials. The last term in equation (11.60) is the energy functional $\bar{F}[\tilde{P}_{\Phi},Q]$ , which is independent of the assignment $c_{j}$ ; thus, as we discussed in the proof of corollary 11.5, we can absorb this term into the normalization constant of the distribution and ignore it. 

If $\begin{array}{r}{Q(\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}}\end{array}$ Q , then the potential $\psi_{j}$ is a stationary point of the energy functional if and only if: 

$$
\psi_{j}(\pmb{c}_{j})\propto\exp\left\{E_{Q}\left[\ln\tilde{P}_{\Phi}\mid\pmb{c}_{j}\right]-\sum_{k\neq j}\pmb{E}_{Q}[\ln\psi_{k}\mid\pmb{c}_{j}]\right\}.
$$ 
As we show in section 11.5.2.3 and section 11.5.2.4, we can often exploit additional structure in $Q$ to reduce further the complexity of the fixed-point equations, and hence of the resulting update steps. The following discussion, which describes the procedure of applying the fixed- point equations to find a stationary point of the energy functional, is orthogonal to these simplifications. 

#### 11.5.2.2 Optimization 
Given a set of fixed-point equations as in equation (11.61), our task is to find a distribution $Q$ that satisfies them. As in section 11.5.1, our strategy is based on the key observation that the factor $\psi_{j}$ does not afect the right-hand side of the fixed-point equations defining its value: The first expectation, I $\pmb{{\cal E}}_{Q}\left[\ln\tilde{P}_{\Phi}\mid\pmb{c}_{j}\right]$ i , is conditioned on $c_{j}$ and therefore does not depend on the parameter iz ation of $\psi_{j}$ . The same observation holds for the second expectation, $E_{Q}[\ln\psi_{k}\mid c_{j}]$ | , for any $k\neq j$ . (Importantly, there is no such term for $k\,=\,j$ in the right-hand side.) Thus, we can use the same general approach as in Mean-Field : We can optimize each potential $\psi_{j}$ , given values for the other potentials , by simply selecting $\psi_{j}$ to satisfy the fixed-point equation. As for the case of mean field, this step is guaranteed to increase (or not decrease) the value of the objective; thus, the overall process is guaranteed to converge to a stationary point of the objective. 

This last step requires that we perform inference in the approximating distribution $Q$ to compute the requisite expectations. Although this step was also present (implicitly) in the mean field approximation, there the structure of the approximating distribution was trivial, and so the inference step involved only individual marginals. Here, we need to collect the expectation of several factors, and each of these requires that we compute expectations given diferent assignments to the factor of interest. (See exercise 11.27 for a discussion of how these expectations can be computed efciently.) For a general distribution $Q$ , even one with tractable structure, running inference in the corresponding network $\mathcal{H}_{Q}$ can be costly, and we may want to reduce the number of calls to the inference subroutine. 

This observation leads to a question of how best to perform updates for several factors in $Q$ . We can consider two strategies. The sequential update strategy is similar to our strategy in the mean field algorithm: We choose a factor $\psi_{j}$ , apply the fixed-point equation to that factor by running inference in $\mathcal{H}_{Q}$ , update the distribution, and then repeat this process with another factor until convergence. The problematic aspect of this approach is that we need to perform inference after each update step. For example, if we are using cluster tree inference in the network $\mathcal{H}_{Q}$ , the network parameter iz ation changes after each update step, so we need to recalibrate the clique tree every time. Some of these steps can be made more efcient by selecting an appropriate order of updates and using dynamic programming (see exercise 11.27), but the process can still be quite expensive. 

An alternative approach is the parallel update strategy, where we compute the right-hand side of our fixed-point equations (for example, equation (11.61)) simultaneously for each of the factors in $Q$ . If we are using a cluster tree for inference, this process involves multiple queries from the same calibrated cluster tree. Thus, we can perform a single calibration step and use the resulting tree to reestimate all of our potentials. However, the diferent queries required all have diferent evidence; hence, it is not easy to obtain significant computational savings, and the algorithms needed are fairly tricky. Nevertheless, this approach might be less costly than recalibrating the clique tree $J$ times. 

On the other hand, the guarantees provided by these two update steps are diferent. For the sequential update strategy, we can prove that each update step is monotonic in the energy functional: each step maximizes the value of one potential given the values of all the others, and therefore is guaranteed not to decrease (and generally to increase) the energy functional. This monotonic improvement implies that iterations of sequential updates necessarily converge, generally to a local maximum. The issue of convergence is more complicated in the parallel update strategy. Because we update all the potentials at once, we have no guarantees that any fixed-point equation holds after the update; a value that was optimal for $\psi_{j}$ with respect to the values of all other factors before the parallel update step is not necessarily optimal given their new values. As such, it is conceivable that parallel updates will not converge (for example, oscillate between two sets of values for the potentials). Such oscillations can generally be avoided using damped update steps, similar to these we discussed in the case of cluster- graph belief propagation (see box 11. B), but this modified procedure still does not guarantee convergence. 

At this point, there is no generally accepted procedure for scheduling updates in variational methods, and diferent approaches are likely to be best for diferent applications. 

#### 11.5.2.3 Simplifying the Update Equations 
Equation (11.61) provides a general characterization of the fixed points of the energy functional, for any approximating class of distributions $\mathcal{Q}$ obeying a particular factorization, as in equ tion (11.58). In many cases, we can exploit additional structure of the approximating class Q and of the distribution $P_{\Phi}$ to simplify significantly the form of these fixed-point equations and thereby make the update step more efcient. 

The simplifications we describe take two forms. The first utilizes marginal independencies in $\mathcal{Q}$ to simplify the right-hand side of the fixed-point equation, eq tion (11.61), elimi ing irrelevant terms. The second xploits interactions between the form of Q and the form of $P_{\Phi}$ to simplify the factorization of Q , without loss in expressive power. Both simplifications allow the fixed-point updates to be performed more efciently. We motivate each of the simplifications using an example, and then we present the general result. 

Example 11.13 Once again, consider the $4\!\times\! 4$ grid network. Assume that we approximate it by a “row” network that has the structure shown in figure 11.17a. This approximating network consists of four independent chains. Now we can apply the general form of the fixed-point equation equation (11.61) for a specific entry in our approximation, say: 

$$
\begin{array}{r}{{}_{1}(a_{1,1}, a_{1,2})\propto\exp\left\{\begin{array}{l}{\pmb{E}_{Q}\left[\ln\tilde{P}_{\Phi}\mid a_{1,1}, a_{1,2}\right]}\\ {-\sum_{\scriptstyle\begin{array}{c}{i=1,\dots, 4}\\ {j=1,\dots, 3}\\ {(i, j)\neq (1,1)}\end{array}}\pmb{E}_{Q}\left[\ln\psi_{(i, j)}(A_{i, j}, A_{i, j+1})\mid a_{1,1}, a_{1,2}\right]}\\ {}\end{array}\right\}}\end{array}
$$ 
As in the proof of corollary 11.5, the expectation of $\ \ln{\tilde{P}_{\Phi}}$ is the sum of expectations of the logarithm of each of the potentials in $\Phi$ . Some of these terms, however, do not depend on the choice of value of $A_{1,1}, A_{1,2}$ we are evaluating. For example, because $A_{2,1}$ and $A_{2,2}$ are independent of $A_{1,1}, A_{1,2}$ in $Q$ , we conclude that 

$$
E_{\{A_{2,1}, A_{2,2}\}\sim Q}[\ln\phi (A_{2,1}, A_{2,2})\mid a_{1,1}, a_{1,2}]=E_{\{A_{2,1}, A_{2,2}\}\sim Q}[\ln\phi (A_{2,1}, A_{2,2})].
$$ 
Thus, this term will contribute the same value to each of the entries of $\psi (A_{1,1}, A_{1,2})$ , and can therefore be absorbed into the corresponding normalizing term. We can continue in this manner and remove all terms that are not dependent on the context of the factor we are in- terested in. Overall, we can remove any term $E_{Q}[\ln\phi (A_{i, j}, A_{i, j+1})\mid a_{1,1}, a_{1,2}]$ | and any term $E_{Q}[\ln\phi (A_{i, j}, A_{i+1, j})\mid a_{1,1}, a_{1,2}]$ | except those where i $i=1$ ilarly, we can remove any term $E_{Q}\left[\ln\psi_{(i, j)}{\left (A_{i, j}, A_{i, j+1}\right)}\mid a_{1,1}, a_{1,2}\right]$ except those where i $i\,=\, 1$ . These simplifications result in the following update rule: 

$$
\begin{array}{r l}&{\psi_{1,1}(a_{1,1}, a_{1,2})\propto}\\ &{\qquad\exp\left\{\begin{array}{l}{\sum_{j=1,\dots, 3}\pmb{E}_{\{A_{1, j}, A_{1, j+1}\}\sim Q}\left[\ln\phi_{(1, j)}(A_{1, j}, A_{1, j+1})\mid a_{1,1}, a_{1,2}\right]}\\ {+\sum_{j=1,\dots, 4}\pmb{E}_{\{A_{1, j}, A_{2, j}\}\sim Q}\left[\ln\phi_{(1, j)}(A_{1, j}, A_{2, j})\mid a_{1,1}, a_{1,2}\right]}\\ {-\sum_{j=2,3}\pmb{E}_{\{A_{1, j}, A_{1, j+1}\}\sim Q}\left[\ln\psi_{(1, j)}(A_{1, j}, A_{1, j+1})\mid a_{1,1}, a_{1,2}\right]}\end{array}\right\}.}\end{array}
$$ 

We can generalize this analysis to arbitrary sets of factors: 

Theorem 11.12 

$$
\psi_{j}(\pmb{c}_{j})\propto\exp\left\{\sum_{\phi\in A_{j}}\pmb{E}_{\mathcal{X}\sim Q}[\ln\phi\ |\ \pmb{c}_{j}]-\sum_{\psi_{k}\in B_{j}}\pmb{E}_{\mathcal{X}\sim Q}[\ln\psi_{k}\ |\ \pmb{c}_{j}]\right\},
$$ 

where 

$$
A_{j}=\{\phi\in\Phi: Q\neq (U_{\phi}\perp C_{j})\}
$$ 

and 

$$
B_{j}=\{\psi_{k}: Q\neq (C_{k}\perp C_{j})\}-\{C_{j}\}.
$$ 
Stated in words, this result shows that the parameter iz ation of a factor $\psi_{j}(C_{j})$ depends only on factors in $P_{\Phi}$ and in $Q$ whose scopes are not independent of $C_{j}$ in $\mathcal{Q}$ . This result, applied to example 11.13, provides us precisely with the simplification shown: only factors whose scopes intersect with the first row are relevant to $\psi_{1,1}(A_{1,1}, A_{1,2})$ . Thus, we can use independence properties of the approximating family $\mathcal{Q}$ to simplify the right-hand side of equation (11.61) by removing irrelevant terms. 

#### 11.5.2.4 Simplifying the Family $\mathcal{Q}$ 
It turns out that a similar analysis allows us to simplify the form of the approximating family $\mathcal{Q}$ without loss in the quality of the approximation. We start by considering a simple example. 

Example 11.14 Consider again the four-variable pairwise Markov network of figure 11.18a, which is parameterized by the pairwise factors: 

$$
{\cal P}_{\Phi}(A, B, C, D)\propto\phi_{A B}(A, B)\cdot\phi_{B C}(B, C)\cdot\phi_{C D}(C, D)\cdot\phi_{A D}(A, D).
$$ 
Consider applying the variational approximation with the distribution 

$$
Q (A, B, C, D)=\frac{1}{Z_{Q}}\psi_{1}(A, B)\cdot\psi_{2}(C, D)
$$ 
that has the structure shown in figure 11.18b. Using equation (11.62), we conclude that the fixed-point characterization of $\psi_{1}$ is 

$$
\psi_{1}(a, b)\propto\exp{\{E_{Q}[\ln\phi_{A B}(A, B)\mid a, b]+E_{Q}[\ln\phi_{B C}(B, C)\mid a, b]+E_{Q}[\ln\phi_{A D}(A, B)\mid a, b]\}}
$$ 

Can we further simplify this equation? Consider the first term. Clearly, ${\cal E}_{Q}[\ln\phi_{A B}(A, B)\mid a, b]=$ | $\ln\phi_{A B}(a, b)$ . What about the second term, $E_{Q}[\ln\phi_{B C}(B, C)\mid a, b].$ | ? To compute this expectation, we need to compute $Q (B, C\mid a, b)$ . According to the structure of $Q$ , we can see that $Q (B, C\mid a, b)={\left\{\begin{array}{l l}{Q (C)}&{I\! f\, B=b}\\ {0}&{o t h e r w i s e.}\end{array}\right.}$ 

Thus, we conclude that 

$$
\pmb{{\cal E}}_{A, B, C\sim Q}[\ln\phi_{B C}(B, C)\mid a, b]=\pmb{{\cal E}}_{C\sim Q}[\ln\phi_{B C}(b, C)].
$$ 
We can simplify the third term in exactly the same way, concluding that: 

$$
\psi_{1}(a, b)\propto\exp\left\{\ln\phi_{A B}(a, b)+E_{C\sim Q}[\ln\phi_{B C}(b, C)]+E_{D\sim Q}[\ln\phi_{A D}(a, D)]\right\}.
$$ 
Setting $\psi_{1}^{\prime}(a)=\exp\{E_{D\sim Q}[\ln\phi_{A D}(a, D)]\}$ { } and $\psi_{1}^{\prime\prime}(b)=\exp\{E_{C\sim Q}[\ln\phi_{B C}(b, C)]\}$ { } , we con- ∼ ∼ clude that the optimal $\psi_{1}$ factorizes as a product of three factors: 

$$
\psi_{1}(A, B)=\phi_{A B}(A, B)\cdot\psi_{1}^{\prime}(A)\cdot\psi_{1}^{\prime\prime}(B).
$$ 
Have we gained anything from this decomposition? First, we see that $Q$ preserves the original pairwise interaction term $\phi (A, B)$ from $P_{\Phi}$ . Moreover, the efect of the interactions between these variables and the rest of the network ( $\mathcal{C}$ and $D$ in this example) is summarized by $^a$ univariate factor for each of the variables. Thus, $Q$ does not change the interaction between $A$ and $B$ . 

Applying the same set of arguments to $\psi_{2}$ , we conclude that we can rewrite $Q$ as 

$$
Q^{\prime}(A, B, C, D)=\frac{1}{Z_{Q}}\phi_{A B}(A, B)\cdot\phi_{C D}(C, D)\cdot\psi_{1}^{\prime}(A)\cdot\psi_{1}^{\prime\prime}(B)\cdot\psi_{2}^{\prime}(C)\cdot\psi_{2}^{\prime\prime}(D)
$$ 
The preceding discussion shows that the best approximation to $P_{\Phi}$ within $\mathcal{Q}$ can be rewritten in the form of $Q^{\prime}$ . Thus, there is no point in using the more complicated form of the approximating family of equation (11.63); we may as well use the form of $Q^{\prime}$ in equation (11.64). Note that the form of $Q^{\prime}$ involves a product of a subset of the original factors, which we keep intact without change, and $a$ set of new factors, which we need to optimize. In this example, instead of estimating two pairwise potentials, we estimate four univariate potentials, which utilize a smaller number of parameters. 

Moreover, the update equations for $Q^{\prime}$ are simpler. Consider, for example, applying equa- tion (11.62) for $\psi_{1}^{\prime}$ : 

$$
\begin{array}{r l}&{)\propto\pmb{E}_{B\sim Q^{\prime}}[\ln\phi_{A B}(a, B)\mid a]+\pmb{E}_{D\sim Q^{\prime}}[\ln\phi_{A D}(a, D)\mid a]+\pmb{E}_{B, C\sim Q^{\prime}}[\ln\phi_{B C}(B, B)\mid a]}\\ &{\quad-\pmb{E}_{B\sim Q^{\prime}}[\ln\phi_{A B}(a, B)\mid a]-\pmb{E}_{B\sim Q^{\prime}}[\ln\psi_{1}^{\prime\prime}(B)\mid a]}\\ &{\quad=\pmb{E}_{D\sim Q^{\prime}}[\ln\phi_{A D}(a, D)\mid a]+\pmb{E}_{B, C\sim Q^{\prime}}[\ln\phi_{B C}(B, C)\mid a]-\pmb{E}_{B\sim Q^{\prime}}[\ln\psi_{1}^{\prime\prime}(B)\mid a].}\end{array}
$$ 
The terms involving $E_{Q^{\prime}}[\ln\phi_{A B}\mid a]$ | ] appear twice, once as a factor in $P_{\Phi}$ and once as a factor in $Q^{\prime}$ . These two terms cancel out, and we are left with the simpler update equation. Although this equation does not explicitly mention $\phi_{A B}$ , this factor participates in the computation of $Q^{\prime}(B\mid a)$ that implicitly appears in $E_{B\sim Q^{\prime}}[\ln\psi_{1}^{\prime\prime}(B)\mid a]$ | . 

Note that this result is somewhat counter intuitive, since it shows that the interactions between $A$ and $B$ are captured by the original potential in $P_{\Phi}$ . Intuitively, we would expect the chain of inﬂuence $\scriptstyle A-D-C-B$ to introduce additional interactions between $A$ and $B$ that should be represented in $Q$ . This is not the only counter intuitive result. 

Example 11.15 

Consider another approximating family for the same network, using the network structure shown in figure 11.18c. In this approximation, we have two pairwise factors, $\psi_{1}(A, C)$ , and $\psi_{2}(B, D)$ . Applying the same set of arguments as before, we can show that the update equation can be written as 

$$
\begin{array}{r c l}{\ln\psi_{1}(a, c)}&{\propto}&{{\cal E}_{B\sim Q}[\ln\phi_{A B}(a, B)]+{\cal E}_{D\sim Q}[\ln\phi_{A D}(a, D)]}\\ &&{+{\cal E}_{B\sim Q}[\ln\phi_{B C}(B, c)]+{\cal E}_{D\sim Q}[\ln\phi_{C D}(c, D)].}\end{array}
$$ 

Thus, we can factorize $\psi_{1}$ into two factors, one with a scope of $A$ and the other with $C$ 

$$
\psi_{1}(A, C)=\psi_{1}^{\prime}(A)\cdot\psi_{1}^{\prime\prime}(C).
$$ 

In other words, the approximation in this case is equivalent to the mean field approximation. This result shows that, in some cases, we can remove spurious dependencies in the approximating distribution. However, this result is surprising, since it holds regardless of the actual values of the potentials in $P_{\Phi}$ . And so, we can imagine a network where there are very strong interactions between $A$ and $C$ and between $B$ and $D$ in $P_{\Phi}$ , and yet the variational approximation with a network structure of figure 11.18c will not capture these dependencies. This is a consequence of using $I^{,}$ -projections. Had we used an $M\cdot$ -projection that minimizes $D (P_{\Phi}\|Q)$ | | , then we would have represented the dependencies between $A$ and $C$ ; see exercise 11.30. 

These two examples suggest that we can use the fixed-point characterization to refine an initial approximating network by factorizing its factors into a product of, possibly smaller, factors and potentials from $P_{\Phi}$ . We now consider the general theory of such factorizations and then discuss its implications. 

We start with a simple definition and a proposition that form the basis of the simplifications we consider. 

Definition 11.8 interface 

Example 11.16 

Let $\mathcal{H}$ be a Markov network structure and let $X, Y\subseteq\mathcal{X}$ . We define the $Y$ - interface of $X$ , denoted Interfac $z e_{\mathcal{H}}(X; Y)$ , to be the minimal subset of X such that $\mathrm{sep}_{\mathcal{H}}(X; Y\mid$ Interface $_{\mathcal{U}}(X; Y)_{,}$ ) . That is, the $Y$ -interface of $X$ is the subset of $X$ that sufces to separate it from $Y$ . 

The $\{A, D\}$ -in rface of $\{A, B\}$ in $\mathcal{H}_{P_{\Phi}}$ of figure 11.18 is $\{A, B\}$ , since neither $A$ is parated from $\{A, D\}$ given B , no is B is separated from $\{A, D\}$ given A . In $\mathcal{H}_{\mathcal{Q}_{1}}$ , we have that B is separated from $\{A, D\}$ given A , so that Interfa $\iota c e_{{\mathcal{H}_{\mathcal{Q}_{1}}}}(\{A, B\};\{A, D\})$ is $\{A\}$ . The same holds in ${\mathcal{H}}_{{\mathcal{Q}}_{3}}$ . In $\mathcal{H}_{\mathcal{Q}_{2}}$ , we have that, again, neither $A$ nor $B$ sufces to separate the other from $\{A, D\}$ , and hence, Interfa $\iota c e_{{\mathcal{H}_{\mathcal{Q}_{2}}}}(\{A, B\};\{A, D\})=\{A, B\}$ . 

The definition of interface can be used to reduce the scope of the conditional expectations in the fixed-point equations: 

Proposition 11.6 

$$
E_{U_{\phi}\sim Q}[\phi\mid c_{j}]=E_{U_{\phi}\sim Q}[\phi\mid c_{j}\langle I n t e r f a c e_{\mathcal{H}}(C_{j}; U_{\phi})\rangle].
$$ 

The proof follows immediately from the definition of conditional independence. 

This proposition provides a principled approach for reformulating terms on the right-hand side of the fixed-point equation. 

We can use this simplification result to define a two-phase strategy for designing approximation. First, we define a “rough” outline for approximation by defining $Q$ over factors with a fairly large scope. We use this outline to obtain a set of update equations, as implied by equation (11.62) on $Q$ . We then derive a finer-grained representation by factorizing each of these factors using proposition 11.6. This process results in a finer- grained approximation that is provably equivalent to the one with which we started. 

Theorem 11.13 (Factorization) Let $\mathcal{Q}$ be an approximating family defined in terms of factors $\{\psi_{j}(C_{k})\}$ , which Markov network structure $\mathcal{H}_{\mathcal{Q}}$ . Let $Q\in{\mathcal{Q}}$ be a stationary point of the energy functional $F[\tilde{P}_{\Phi}, Q]$ subject to the given factorization. Then, factors in $Q$ are factorized as 

$$
\psi_{j}(C_{j})=\prod_{\phi\in\Phi_{j}}\phi\prod_{D_{l}\in\mathcal{D}_{j}}\psi_{j, l}(D_{l}),
$$ 

where 

$$
\Phi_{j}=\{\phi\in\Phi: S c o p e[\phi]\subseteq C_{j}\}
$$ 

and 

$$
D_{j}=\{I n t e r f a c e_{\mathcal{H}_{\emptyset}}(C_{j}; X): X\in\{S c o p e[\phi]:\phi\in\Phi-\Phi_{j}\}\cup\{S c o p e[\psi_{k}]: k\neq j\},
$$ 

This theorem states that $\psi_{j}$ can be written as the product of two sets of factors. The first set contains factors in the original distribution $P_{\Phi}$ whose scope is a subset of the scope of $\psi_{j}$ . The factors in the second set are the interfaces of $\psi_{j}$ with other factors that appear in the update equation. These include factors in $P_{\Phi}$ that are partially “covered” by the scope of $\psi_{k}$ , and other factors in $Q$ . The set $\mathcal{D}_{k}$ defines the set of interfaces between $\psi_{k}$ and these factors. 

To gain a better understanding of this theorem, let us consider various approximations in two concrete examples. The first example serves to demonstrate the ease with which this theorem allows us to determine the form of the factorization of $Q$ . 

Let us return to example 11.14. In example 11.16, we have already shown the interfaces of $\{A, B\}$ with $\{A, D\}$ in $\mathcal{H}_{1}$ . This analysis, togeth rem 11.13, direct mply the reduced factor- example 11.14 In particular, for $\psi_{1}(\{A, B\})$ { } , we have that $\Phi_{1}$ contains only the factor $\phi (\{A, B\})$ { } in $P_{\Phi}$ , which therefore constitutes the first term in the factorization of equation (11.65). The second set of terms in the equation corresponds to the interfaces of $\{A, B\}$ with other factors in bo $\mathcal{H}_{P_{\Phi}}$ and in $\mathcal{H}_{\mathcal{Q}_{1}}$ . We get two such interfaces: one with scope $\{A\}$ from the factor $\phi\big (\{A, D\}\big)$ in $P_{\Phi}$ , and one with scope $\{B\}$ f e factor $\phi (\{B, C\})$ . 

Assume that we add the edge $A{-}C$ , as in figure 11.18d. Now, Interfa $c e_{\mathcal{H}_{\mathcal{Q}_{3}}}(\{A, B\};\{B, C\})$ is the entire set $\{A, B\}$ , since $B$ no longer separates $C$ from $A$ . Thus, in this case, the second set of terms in the factorization of $\psi$ also contains a new pairwise interaction factor $\psi_{1,\{A, B\}}$ . As $^a$ consequence, the pairwise interaction of $A, B$ is no longer the same in $Q$ and in $P_{\Phi}$ . This result is somewhat counterintuit n the simpler ork $\mathcal{H}_{\mathcal{Q}_{1}}$ , h contained no factors allowing any interaction between the $A, B$ pair and the $C, D$ pair, the $A, B$ interaction was the same in $P_{\Phi}$ and in $Q$ . But if we enrich our approximation (presumably allowing a better fit via the introduction of the $A, C$ factor), the pairwise interaction term does change. 

Finally, $\mathcal{H}_{\mathcal{Q}_{2}}$ does not contain an $\{A, B\}$ factor. Here, $\Phi_{j}\,=\,\emptyset$ for both factors in $\mathcal{H}_{\mathcal{Q}_{2}}$ , and each $\mathcal{D}_{j}$ consists solely of singleton scopes; for example, Interfa $c e_{\mathcal{H}_{\mathcal{Q}_{2}}}(\{A, C\};\{A, D\})=\{A\}$ . 

Our second example serves to illustrate the two-phase strategy described earlier, where we first select a “rough” approximation containing a few large factors and then use the theorem to refine them. 

Consider again our running example of the $4\times4$ grid. Suppose we select an approximation where each factor consists of the variables in a single row in the grid. Thus, for example, $C_{1}=$ $\{A_{1,1},.\,.\,.\,, A_{1,4}\}$ . Note that this approximation is not the one shown in figure 11.17a, since the structure in our approximation here is a full clique over each row. We now apply theorem 11.13. What is the factorization of $C_{1}$ ? First, we search for factors in $\Phi_{1}$ . We see that the factors $\phi (A_{1,1}, A_{1,2})$ , $\phi (A_{1,2}, A_{1,3})$ , and $\phi (A_{1,3}, A_{1,4})$ have a scope that is a subset of $C_{1}$ . Next, we consider the interfaces between $C_{1}$ and other factors in $P_{\Phi}$ and $Q$ . For example, the interface with $\phi (A_{1,1}, A_{2,1})$ is $\{A_{1,1}\}$ . Similarly, $\{A_{1,2}\},\,\{A_{1,3}\}$ , and $\{A_{1,4}\}$ are interfaces with other factors in $P_{\Phi}$ . It is easy to convince ourselves that these are the only non-empty interfaces in $\mathcal{L}_{1}$ . Thus, by applying theorem 11.13, we get the following factorization: 

$$
\begin{array}{r c l}{\psi_{1}(A_{1,1},\dots, A_{1,4})}&{=}&{\phi (A_{1,1}, A_{1,2})\cdot\phi (A_{1,2}, A_{1,3})\cdot\phi (A_{1,3}, A_{1,4})}\\ &&{\psi_{1,1}(A_{1,1})\cdot\psi_{1,2}(A_{1,2})\cdot\psi_{1,3}(A_{1,3})\cdot\psi_{1,4}(A_{1,4}).}\end{array}
$$ 

We conclude that, once we decide that the approximation should decouple the rows in the group, we might as well work with an approximation where we keep all original potentials along each row and introduce univariate potentials only to capture interactions along columns. Additional potentials, such as a potential between $A_{1,1}$ and $A_{1,3}$ , would not improve the approximation. Thus, while we started with an approximation containing full cliques on each of the rows, we ended up with an approximation whose structure is that of figure 11.17a, and where we have only the original factors and new factors over single variables. 

We can work directly with this new factorized form of $Q$ , ignoring our original factorization entirely. More precisely, we define $Q^{\prime}$ to be 

$$
\begin{array}{r c l}{{Q^{\prime}(\mathcal{X})}}&{{=}}&{{\phi (A_{1,1}, A_{1,2})\cdot\phi (A_{1,2}, A_{1,3})\cdot\phi (A_{1,3}, A_{1,4})}}\\ {{}}&{{}}&{{\cdot\cdot\cdot}}\\ {{}}&{{}}&{{\phi (A_{4,1}, A_{4,2})\cdot\phi (A_{4,2}, A_{4,3})\cdot\phi (A_{4,3}, A_{4,4})}}\\ {{}}&{{}}&{{\psi_{1,1}(A_{1,1})\cdot\,\cdot\psi_{4,4}(A_{4,4}).}}\end{array}
$$ 

In this new form, we fix the value of all the pairwise potentials, and so we have to define an update rule only for the new singleton potentials. For example, consider the fixed-point equation for $\psi_{1,1}(A_{1,1})$ . Applying theorem 11.12 we get that 

$$
\begin{array}{r l}&{\ln\psi_{1,1}(a_{1,1})\propto}\\ &{\qquad+E_{Q^{\prime}}[\ln\phi (A_{1,1}, A_{2,1})\mid a_{1,1}]+E_{Q^{\prime}}[\ln\phi (A_{1,2}, A_{2,2})\mid a_{1,1}]}\\ &{\qquad+E_{Q^{\prime}}[\ln\phi (A_{1,3}, A_{2,3})\mid a_{1,1}]+E_{Q^{\prime}}[\ln\phi (A_{1,4}, A_{2,4})\mid a_{1,1}]}\\ &{\qquad-E_{Q^{\prime}}[\ln\psi_{1,2}(A_{1,2})\mid a_{1,1}]-E_{Q^{\prime}}[\ln\psi_{1,3}(A_{1,3})\mid a_{1,1}]-E_{Q^{\prime}}[\ln\psi_{1,4}(A_{1,4})\mid a_{1,1}]}\end{array}
$$ 

where we have exploited the fact that the terms involving factors such as $\phi (A_{1,1}, A_{1,2})$ appear in both $P_{\Phi}$ and $Q$ , and so cancel out of the equation. Note that to compute terms such as $E_{Q^{\prime}}[\ln\phi (A_{1,2}, A_{2,2})\mid a_{1,1}]$ | we need to evaluate $Q^{\prime}(A_{1,2}, A_{2,2}\,\,\mid\,\, a_{1,1})\;=\; Q^{\prime}(A_{1,2}\,\,\mid\,\, a_{1,1})$ · $Q^{\prime}(A_{2,2})$ (where we used the independencies in $Q^{\prime}$ to simplify the joint marginal). Note that $Q^{\prime}(A_{2,2})$ does not change when we update factors in the first row, such as $\psi_{1,1}(A_{1,1})$ . Thus, we can cache the computation of this marginal when updating the factors $\psi_{1,1}(A_{1,1}),.\,.\,.\,,\psi_{1,4}(A_{1,4})$ . When performing inference in a large model this can result in dramatic efect. 

cluster mean field 

This example is a special case of an approximation approach called cluster mean field . In this case, our initial approximation has the form 

$$
Q (\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}(C_{j}),
$$ 

where the scopes $C_{1},\ldots, C_{K}$ are partition of $\mathcal{X}$ . That is, each pair of factors have disjoint scopes, and each variable in X appears in one factor. This approximation resembles the mean field approximation, except that it is clusters, rather than individual variables, that are marginally independent. We can now apply theorem 11.13 to refine the approximation. Because the factors are all disjoint, there are no chains of inﬂuence, and so the interfaces take a particularly simple form: 

Proposition 11.7 Let $\begin{array}{r}{Q (\mathcal{X})=\frac{1}{Z_{Q}}\prod_{j}\psi_{j}(C_{j})}\end{array}$ be a cluster mean field approximation to a set of factors $P_{\Phi}$ , and let $\psi_{j}$ be a factor of Q . Then, the set $\mathcal{D}_{j}$ of theorem 11.13 can be written as 

$$
\begin{array}{r}{\mathcal{D}_{j}=\{C_{j}\cap S c o p e[\phi]:\phi\in\Phi-\Phi_{j}\}-\{\emptyset\}.}\end{array}
$$ 

The proof follows directly from the independence properties in $Q$ , and is left as an exercise (exercise 11.31). 

In words, this result states that the interfaces of a cluster are simply the places where the cluster scope intersects potentials in $\Phi$ that are not fully contained in the cluster. In our grid example, when we choose the clusters to be the individual columns, the interfaces are the intersections with the row potentials, which are precisely the singleton variables that we discussed in example 11.18. 

We conclude this discussion with a slightly more elaborate example, demonstrating again the strength of this result: 

Example 11.19 Consider again our $4\times4$ grid, and the “comb” approximation whose structure is shown in fig- ure 11.19a. In this structure, we have a fully connected clique over each of the columns, and a “backbone” connecting the columns to each other. Consider again the factorization of the potential over $C_{1}=\{A_{1,1},.\,.\,.\,, A_{4,1}\}$ . As in the previous example, the first term in the new factorization contains the pairwise factors $\phi (A_{1,1}, A_{2,1})$ , $\phi (A_{2,1}, A_{3,1})$ , and $\phi (A_{3,1}, A_{4,1})$ . The second set of terms contains the interfaces with other factors in $P_{\Phi}$ and $Q$ . Due to the structure of the approxi- mation, the $Q$ interfaces introduce only singleton potentials. The factors in $P_{\Phi}$ , however, are more interesting. Consider, for example, the factor $\phi (A_{4,1}, A_{4,2})$ . The interface of $C_{1}$ with $\{A_{4,1}, A_{4,2}\}$ is $A_{1,1}, A_{4,1}$ — the variable $A_{4,1}$ separates $C_{1}$ from itself, and the variable $A_{1,1}$ from $A_{4,2}$ . Now, consider $a$ factor $\phi (A_{2,3}, A_{3,3}).$ ; in this case, the interface is simply $A_{1,1}$ , which separates the first column from both of these variables. Continuing this argument, it follows that all other factors in 

$P_{\Phi}$ induce an interface containing a variable at the head of the column and (possibly) another variable in the column. Thus, we can eliminate any (new) pairwise interaction terms between any other pair of variables. For $a$ general $n\times n$ grid, this reduces the overall number of (new) pairwise potentials from $\textstyle n\cdot{\binom{n}{2}}$ to $n\times (n-1)$ . 

#### 11.5.2.5 Selecting the Approximation 
In general, both the quality and the computational complexity of the variational approximation depend on the structure of $P_{\Phi}$ and the structure of the approximating family $\mathcal{Q}$ . There are several guiding intuitions. First, we want to be able to perform efcient inference in the approximating network. In example 11.18, the approximating structure was a chain of variables, where we can perform inference in linear time (as a function of the number of variables in the chain). In general, we often select our network so that the resulting factorization leads to a tractable network (that is, one of low tree-width). 

It is important to note, however, that the structure of the original distribution is not the only aspect in determining the complexity of inference in $Q$ . We also need to take into account factors that correspond to the interfaces of the cluster. In our grid example, these interfaces involved a single variable at time, and so they did not add to the network complexity. However, in more complex networks, these factors can have a significant efect. 

Another consideration besides computational complexity is the quality of our approximation. Intuitively, we should design $\mathcal{Q}$ so as to preserve the strong dependencies in $P_{\Phi}$ . By preserving such dependencies we maintain the main efects in the distribution we want to apply. 

These intuitions provide some guidelines in choosing the approximating distribution. How- ever, these choices are far from an exact science at this stage. The theory we described here allows to automate two parts of the process: defining the form of the approximation given some initial rough set of (disjoint or overlapping) clusters; and defining the fixed-point iterations to 

optimize such an approximation. The current tools do not provide for an automated way for determining what are reasonable sets of clusters to achieve a desired degree of approximation. 

### 11.5.3 Local Variational Methods\* 
Lemma 11.2 
The general method that we used throughout this chapter is an instance of a general class of methods known as variational methods . In this class of methods, we take a complex objective function $f_{\mathrm{obj}}(\pmb{x})$ , and lower or upper bound it using a parameterized family of functions ${\pmb g}({\pmb x},{\pmb\lambda})$ . Focusing, for concreteness, on the case of a lower bound , this family has the property that $f_{\mathrm{obj}}(\pmb{x})\,\geq\,\pmb{g}(\pmb{x},\pmb{\lambda})$ for any value of $\lambda$ , and that, for any $_{_{x}}$ , the bound is tight for some value of λ (a diferent one for every $_{_{x}}$ ). 

As an example, we can show the variational lower bound : 

For any choice of $\lambda$ and $x$ 

$$
-\ln (x)\geq-\lambda x+\ln (\lambda)+1,
$$ 

and, for any $x$ , this bound is tight for some value of $\lambda$ . 

Proof Consider the tangent of $\ln (x)$ at the point $x_{0}$ 

$$
f_{\mathrm{obj}}(x: x_{0})=\ln (x_{0})+(x-x_{0})\frac{1}{x_{0}}=\frac{x}{x_{0}}+\ln (x_{0})-1.
$$ 

Since $\ln (x)$ is a concave function, it is upper bounded by each of its tangents. And so, $-\ln (x)\geq-f_{\mathrm{obj}}(x: x_{0})$ for any choice of $x$ and $x_{0}$ . Setting $x_{0}=\lambda^{-1}$ leads to the desired result. 

convex duality 

variational parameter 

This result is illustrated in figure 11.20. It is a special case of a general result in the field of convex duality , which guarantees the existence of such bounds for a broad class of functions. 

This l wer bound allows to approximate a nonlinear function $-\ln (x)$ with a term that s linear in x . This simplification comes at the price of introducing a new variational parameter λ , whose value is undetermined. If we optimize $\lambda$ exactly for each value of $x$ , we obtain a tight lower bound, but a bound is obtained for any value of $\lambda$ . 

The techniques we have used in this chapter so far also fall into this category. Equation (11.5) shows that the energy functional is a lower bound on the log-partition function for any distribu- tion $Q$ . Thus, we can take $f_{\mathrm{obj}}$ to be the partition function, $_{_{x}}$ to correspond to the parameters of the true distribution $P_{\Phi}$ , and $\lambda$ to correspond to the parameters of the approximating dis- tribution $Q$ . Although the lower bound is tight when $Q$ precisely represents $P_{\Phi}$ , for reasons of efciency, we generally optimize $Q$ in a restricted space that provides a bound, but not a tight one, on the log-partition function. 

variational variable elimination 

This general approach of introducing auxiliary variational parameters that help in simplifying a complex objective function appears in many other domains. While it is beyond our scope to introduce a general theory of variational methods, we now brieﬂy describe one other application of variational methods that is relevant to probabilistic inference and does not fall directly within the scope of optimizing the energy functional. This application arises in the context of exact inference using an algorithm such as variable elimination. Here, we use variational bounds to avoid creating large factors that can lead to exponential complexity in the algorithm, giving rise to an approximate variational variable elimination algorithm. Such simplifications can be achieved in several ways; we describe two. 

#### 11.5.3.1 Variational Bounds 
Consider, for example, the diamond network of figure 11.18a. Assume that we run variable elimination to sum out the variable $B$ , which we assume for convenience is binary-valued. The elimination of $B$ introduces a new factor: 

$$
\phi_{B}(A, C)=\sum_{b}\phi_{1}(A, b)\phi_{2}(b, C)
$$ 
Coupling $A$ and $C$ in a single factor may be expensive, for example, if $A$ and $C$ have many values. In more complex networks, this type of coupling can induce complexity if an elimination step couples a larger set of variables, or if the local coupling leads to additional cost later in the computation, when we eliminate $A$ or $C$ . 

As we now show, we can use a variational bound to avoid this coupling. Consider the following bound: 

Proposition 11.8 

$$
\ln (1+e^{x})\geq\lambda x+H (\lambda),
$$ 

$$
1+e^{x}\geq e^{\lambda x+H (\lambda)}.
$$ 

Why is this useful? Using some algebraic manipulation, we can bound each of the entries in our newly generated factor: 

$$
\begin{array}{r c l}{{\phi_{B}(a, c)}}&{{=}}&{{\phi_{1}(b^{0}, a)\phi_{2}(b^{0}, c)+\phi_{1}(b^{1}, a)\phi_{2}(b^{1}, c)}}\\ {{}}&{{=}}&{{\phi_{1}(b^{0}, a)\phi_{2}(b^{0}, c)\left[1+\exp\left\{\ln\frac{\phi_{1}(b^{1}, a)\phi_{2}(b^{1}, c)}{\phi_{1}(b^{0}, a)\phi_{2}(b^{0}, c)}\right\}\right]}}\\ {{}}&{{\geq}}&{{\phi_{1}(b^{0}, a)\phi_{2}(b^{0}, c)\exp\left\{\lambda_{a, c}\ln\frac{\phi_{1}(b^{1}, a)\phi_{2}(b^{1}, c)}{\phi_{1}(b^{0}, a)\phi_{2}(b^{0}, c)}+H (\lambda_{a, c})\right\}}}\\ {{}}&{{=}}&{{\left (\phi_{1}(b^{0}, a)^{1-\lambda_{a, c}}\phi_{1}(b^{1}, a)^{\lambda_{a, c}}\right)\cdot}}\\ {{}}&{{}}&{{\left (\phi_{2}(b^{0}, c)^{1-\lambda_{a, c}}\phi_{2}(b^{1}, c)^{\lambda_{a, c}}\right)\cdot e^{H (\lambda_{a, c})}.}}\end{array}
$$ 

Thus, we can replace a factor that couples $A$ and $C$ by a product of three terms: an expression involving only factors of $A$ , an expression involving only factors of $C$ , and the final factor $e^{H (\lambda_{a, c})}$ . However, all three terms also involve the variational parameter $\lambda_{a, c}$ and therefore also depend on both $A$ and $C$ . At this point, it is unclear what we gain from the transformation. 

However, we can choose the same $\lambda$ for all joint assignments to $A, C$ . In doing so, we replace four variational parameters by a single parameter $\lambda$ . This operation relaxes the bound, which is no longer tight. On the other hand, it also decouples $A$ and $C$ , leading to a product of terms none of which depends on both variables: 

$$
\phi_{1}(b^{0}, a)^{1-\lambda}\phi_{1}(b^{1}, a)^{\lambda}\bigr)\cdot\bigl (\phi_{2}(b^{0}, c)^{1-\lambda}\phi_{2}(b^{1}, c)^{\lambda}\bigr)\cdot e^{\pmb{H}(\lambda)}=\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{2}(c,\lambda) e^{\pmb{H}(\lambda)}
$$ 

Thus, if we use this approximation, we have efectively eliminated $B$ without coupling $A$ and $C$ . As we saw in chapter 9, this type of simplification can circumvent the need for coupling yet more variables in later stages in variable elimination, potentially leading to significant savings. 

It is interesting to observe how $\lambda$ decouples the two factors. Each factor is replaced by a geometric average over the values of $B$ . The variational parameter specifies the weight we assign to each of the two cases. Note that the original bound in equation (11.66) is tight; thus, if we pick the “right” variational parameter $\lambda_{a, c}$ for each assignment $a, c,$ we reproduce the correct factor $\phi_{B}(A, C)$ . However, these variational parameters are generally diferent for each assignment $a, c,$ and hence, a single variational parameter cannot optimize all of the terms. Our choice of $\lambda$ efectively determines the quality of our approximation for each of the terms $\phi_{B}(a, c)$ . Thus, the overall quality of our approximation for a particular choice of $\lambda$ depends on the importance of these diferent terms in the variable elimination computation as a whole. 

Other variational approximations exploit specific parametric forms of CPDs in the network. For example, consider networks with sigmoid CPDs (see section 5.4.2). Recall that a logistic CPD $P (X\mid U)$ has the parametric form: 

$$
P (x^{1}\mid\mathbf{u})=\mathrm{sigmoid}(\sum_{i}w_{i}u_{i}+w_{0}),
$$ 

where $\begin{array}{r}{\mathrm{sigmoid}(x)=\frac{1}{1+e^{-x}}}\end{array}$ . The observation of $X$ couples the parents $U$ . Can we decouple these parents using an approximation? Using proposition 11.8 we can find an upper bound: 

$$
\ln\operatorname{sigmoid}(\sum_{i}w_{i}u_{i}+w_{0})\leq\lambda\left (\sum_{i}w_{i}u_{i}+w_{0}\right)-H (\lambda).
$$ 

Similarly to our earlier example, such an approximation allows us to replace a factor over several variables by a product of smaller factors. In this case, all the parents of $X$ are decoupled by the approximate form. 

#### 11.5.3.2 Variational Variable Elimination 
How do we use this approximation in the course of inference? Note that equation (11.67) provides a lower bound to $\phi_{B}(a, c)$ for every value of $a, c$ . Assume that, in the course of running variable elimination, rather than generating $\phi_{B}(A, C)$ , we introduce the expression in equation (11.67) and continue the variable elimination process with these decoupled factors. From a graph- theoretic perspective, the result of this approximation when applied to a variable $B$ is analogous to the efect of conditioning on $B$ , as described in section 9.5.3: it removes from the graph $B$ and all its adjacent edges. However, unlike conditioning, we do not enumerate and perform inference for all values of $B$ (of course, at the cost of obtaining an approximate result). 

variational variable elimination 

More generally, in an execution of variable elimination, there may be some set of elimination steps that create large factors that couple many variables. This variational approximation can allow us to avoid this coupling. Like conditioning (see section 9.5.4.1), we can perform such an approximation step not only at the very beginning, but also in a way that is interleaved with variable elimination, allowing us to reuse computation. This class of algorithms is called variational variable elimination . 

What is the result of this approximation? Each of the entries in the approximated factor is replaced with a lower bound; thus, each entry in every subsequent factor produced by the algorithm is also a lower bound to the original entry. If we proceed to eliminate all variables, either exactly or using additional variational approximation steps for other intermediate factors, the outcome of this process is a lower bound to the partition function. If we do not eliminate all of the variables, the result is an approximate factor in which every entry is a lower bound to the original. Of course, once we renormalize the factor to produce a distribution, we can make no guarantees about the direction of the approximation for any given entry. Nevertheless, the resulting factor might be a reasonable approximation to the original. 

The quality of our approximation depends on the choice of variational parameters introduced during the course of the variable elimination algorithm. How do we select them? One approach is simply to select the variational parameter at each step to optimize the quality of our ap- proximation at that step. However, this high-level goal is not fully defined. For example, we can choose $\lambda$ so as to make $\tilde{\phi}_{1}(a^{1},\lambda)\tilde{\phi}_{2}(b^{1},\lambda) e^{H (\lambda)}$ as close as possible to $\phi_{B}(a^{1}, c^{1})$ ; or, we can focus on $a^{1}, c^{0}$ . The decision of where to focus our “approximation efort” depends on the impact of these components of the factor on the final outcome of the computation. Thus, a more correct approach is to identify the actual expression that we are trying to estimate — for example, the partition function — and to try to maximize our bound to that expression. In our simple example, we can write down the partition function as a function of the variational parameter $\lambda$ introduced when eliminating $B$ : 

$$
\tilde{Z}(\lambda)=e^{H (\lambda)}\sum_{a}\sum_{c}\phi_{3}(a, d)\phi_{4}(c, d)\sum_{d}\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{2}(c,\lambda).
$$ 

This expression is a function of $\lambda$ ; we can then try to identify the best bound by maximizing $\operatorname*{max}_{\lambda}\hat{\tilde{Z}}(\lambda)$ , say, using gradient ascent or another numerical optimization method. 

However, as we discussed, in most cases we would use several approximate elimination steps within the variable elimination algorithm. In our example, the elimination of $D$ also couples $A$ and $C$ , a situation we may wish to avoid. Thus, we could apply the same type of variational bound to the internal summation: 

$$
\phi_{D}(A, C)=\sum_{d}\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{2}(c,\lambda),
$$ 

giving rise to the bound $\tilde{\phi}_{3}(a,\lambda^{\prime})\tilde{\phi}_{4}(c,\lambda^{\prime}) e^{H (\lambda^{\prime})}$ . The resulting approximate partition function now has the form 

$$
\tilde{Z}(\lambda,\lambda^{\prime})=e^{H (\lambda)}e^{H (\lambda^{\prime})}\sum_{a}\tilde{\phi}_{1}(a,\lambda)\tilde{\phi}_{3}(a,\lambda^{\prime})\sum_{c}\tilde{\phi}_{2}(c,\lambda)\tilde{\phi}_{4}(c,\lambda^{\prime}).
$$ 

We can now maximize $\mathrm{max}_{\lambda,\lambda^{\prime}}\,\tilde{Z}(\lambda,\lambda^{\prime})$ ; the higher the value we find, the better our approxi- mation to the true partition function. 

In general, our approximate partition function will be a function $\tilde{Z}(\lambda)$ , where $\lambda$ is the vector of all variational parameters $\lambda$ produced during the diferent approximation steps in the algorithm. One approach is to reformulate the variable elimination algorithm so that it produces factors that are not purely numerical, but rather symbolic expressions in the variables $\lambda$ ; see exercise 11.32. Given the multivariate function $\tilde{Z}(\lambda)$ , we can optimize it numerically to produce the best possible bound. A similar principle applies when we use the variational bound for sigmoid CPDs; see exercise 11.36 for an example. While we only sketch the basic idea here, this approach can be used as the basis for an algorithm that interleaves variational approximation steps with exact elimination steps to form a variational variable elimination algorithm. 

## 11.6 Summary and Discussion
In this chapter, we have described a general class of methods for performing approximate inference in a distribution $P_{\Phi}$ defined by a graphical model. These methods all attempt to construct a representation $Q$ within some approximation class $\mathcal{Q}$ that best approximates $P_{\Phi}$ . The key issue that must be tackled in this task is the construction of such an approximation without performing inference on the (intractable) $P_{\Phi}$ . The methods that we described all take a similar approach of using the I-projection framework, whereby we minimize the KL- divergence $D (Q\|P_{\Phi})$ ; these methods all reformulate this problem as one of maximizing the energy functional. 
>  在本章中，我们描述了一类用于在由图形模型定义的分布 $P_{\Phi}$ 中执行近似推理的方法。所有这些方法都试图在某个近似类 $\mathcal{Q}$ 内构建一个表示 $Q$，该表示最能逼近 $P_{\Phi}$。
>  在这项任务中必须解决的关键问题是，在不对（难以处理的）$P_{\Phi}$ 进行推理的情况下构造这样的近似。我们所描述的方法都采用了类似的策略，即使用 I-投影框架，通过最小化KL散度 $D(Q \| P_{\Phi})$ 来实现；这些方法都将这个问题重新表述为最大化能量泛函的问题。

The different methods that we described all follow a similar template. We optimize the free energy, or an approximation thereof, over a class of representations $\mathcal{Q}$ . We provided an optimization-based view for four diferent methods, each of which makes a particular choice regarding the objective and the constraints. We now recap these choices and their repercussions. 
>  我们描述的不同方法都遵循类似的模板。
>  我们在一组表示 $\mathcal{Q}$ 上优化自由能或其近似。
>  我们提供了四种不同方法的基于优化的观点，每种方法对目标函数和约束条件进行了特定的选择。我们现在总结这些选择及其影响。

Clique tree calibration optimizes the factored energy functional, which is exact for clique trees. The optimization is performed over the space of calibrated clique potentials. For clique trees, any set of calibrated clique potentials must arise from a real distribution, and so this space is precisely the marginal polytope. As a consequence, both our objective and our constraint space are exact, so our solution represents the exact posterior. 
>  团树校准优化分解的能量泛函，这对于团树而言是精确的。
>  优化是在校准团势能的空间上进行的。
>  对于团树来说，任何一组校准的团势能都必须来源于实际的分布，因此这个空间正是边缘多胞形。因此，我们的目标和约束空间都是精确的，所以我们的解代表了精确的后验。

Cluster-graph (loopy) belief propagation optimizes the factored energy functional, which is approximate for loopy graphs. The optimization is performed over the space of locally consistent pseudo-marginals, which is a relaxation of the marginal polytope. Thus, both the objective and the constraint space are approximate. 
>  簇图（循环）置信传播优化因子能量泛函，这在循环图中是近似的。
>  优化是在局部一致伪边缘的空间上进行的，这是边缘多胞形的一个松弛。因此，无论是目标还是约束空间都是近似的。

Expectation propagation over clique trees optimizes the factored energy functional, which is the exact objective in this case. However, the constraints define a space of pseudo-marginals that are not even entirely consistent — only their moments are required to match. Thus, the constraint space here is a relaxation of our constraints, even when the structure is a tree. Expectation propagation over cluster graphs adds, on top of these, all of the approximations induced by belief propagation. 
>  在团树上的期望传播优化因子能量泛函，在这种情况下它是精确的目标函数。然而，约束定义了一个连贯伪边缘的空间，这些伪边缘甚至都不是完全一致的——只有它们的矩需要匹配。因此，即使结构是一棵树，这里的约束空间也比我们的原始约束空间更宽松。在簇图上的期望传播除了这些外，还包括信念传播所导致的所有近似。

Finally, structured variational methods optimize the exact factored energy functional, for a class of distributions that is (generally) less expressive than necessary to encode $P_{\Phi}$ . As a consequence, the constraint space is actually a tightening of our original constraint space. Because the objective function is exact, the result of this optimization provides a lower bound on the value of the exact optimization problem. As we will see, such bounds can play an important role in the context of learning. 
>  最后，有结构的变分方法优化了分解的能量泛函，但是对于一类通常不如编码 $P_{\Phi}$ 所需那样具有表现力的分布。
>  因此，约束空间实际上是对我们原始约束空间的紧缩。
>  由于目标函数是精确的，这种优化的结果会为精确优化问题的值提供一个下界。正如我们将看到的，这种界限在学习的背景下可以发挥重要作用。

In the approaches we discussed, the optimization method was based on the method of Lagrange multipliers. This derivation gave rise to a series of fixed-point equations for the solution, where one variable is defined in terms of the others. As we showed, an iterative solution for these fixed-point equations gives rise to a suite of message passing algorithms that generalize the clique-tree message passing algorithms of chapter 10. 
> 在我们讨论的方法中，优化方法基于拉格朗日乘数法。此推导产生了一系列固定的点方程来求解，在这些方程中一个变量是根据其他变量定义的。
> 正如我们所展示的，这些固定点方程式的迭代解产生了一系列消息传递算法，这些算法推广了第十章中的团树消息传递算法。

This general framework opens the door to the development of many other approaches. Each of the methods that we described here involves a design choice in three dimensions: the objective function that we aim to optimize, the space of (pseudo-) distributions over which we perform our optimization, and the algorithm that we choose to use in order to perform the optimization. Although these three decisions afect each other, these dimensions are sufciently independent that we can improve each one separately. Some recent work takes exactly this approach. For example, we have already seen some work that focuses on better approximations to the energy functional; other work (see section 11.7) focuses on identifying constraints over the space of pseudo-marginals that make it a tighter relaxation to the (exact) marginal polytope; yet other work aims to find better (for example, more convergent) algorithms for a general class of optimization problems. 

Many of these improvements aimed to address a fundamental problem arising in these algorithms: the possible lack of convergence to a stable solution. The issue of convergence is one on which some progress has been made. Some recently developed methods have better convergence properties in practice, and others are even guaranteed to converge. There are also theoretical analyses that can help determine when the algorithms converge. 

A second key question, and one on which relatively little progress has been made, is the quality of the approximation. There is very little work that provides guarantees on the error made in the answers (for example, individual marginals) by any of these methods. As a consequence, there is almost no work that provides help in choosing a low-error approximation for a particular problem. Thus, the problem of applying these methods in practice is still a combination of manual tuning on one hand, and luck on the other. The development of automated methods that can help guide the selection of an approximating class for a particular problem is an important direction for future work. 

# 12 Particle-Based Approximate Inference 
In the previous chapter, we discussed one class of approximate inference methods. The techniques used in that chapter gave rise to algorithms that were very similar in ﬂavor to the factor-manipulation methods underlying exact inference. In this chapter, we discuss a very different class of methods, ones that can be roughly classiﬁed as particle-based methods . In these methods, we approximate the joint distribution as a set of instantiations to all or some of the variables in the network. These instantiations, often called particles , are designed to provide a good representation of the overall probability distribution. 
>  前一章讨论了一类近似推理方法，本章讨论另一类近似推理方法，可以将它分类为基于粒子的方法
>  在这类方法中，我们用网络中的全部或部分变量的一组实例化来近似联合分布，这些实例化通常称为粒子，我们通过粒子对整体分布提供一个良好表示

Particle-based methods can be roughly characterized along two axes. On one axis, approaches vary on the process by which particles are generated. There is a wide variety of possible processes. At one extreme, we can generate particles using some deterministic process. At another, we can sample particles from some distribution. Within each category, there are many possible variations. 

On the other axis, techniques use different notions of particles. Most simply, we can consider full particles — complete assignments to all of the network variables $\mathcal{X}$ . The disadvantage of this approach is that each particle covers only a very small part of the space. A more effective notion is that of a collapsed particle . A collapsed particle speciﬁes an assignment $\mathbfit{w}$ only to some subset of the variables $W$ , associating with it the conditional distribution $P(\mathcal{X}\mid w)$ or some “summary” of it. 

>  基于粒子的方法可以分为两类
>  其中一类的方法在粒子生成的过程中加入变化，我们可以使用确定性的过程生成粒子，也可以从某个分布采样粒子
>  另一类方法使用不同概念的粒子，例如完全粒子指对网络所有变量 $\mathcal X$ 的完整赋值，该方法的缺陷在于每个粒子仅覆盖空间的一小部分

The general framework for most of the discussion in this chapter is as follows. Consider some distribution $P(\mathcal X)$ , and assume we want to estimate the probability of some event $Y=y$ relative to $P$ , for some $Y\subseteq\mathcal{X}$ and $\pmb{y}\in V a l(\pmb{Y})$ More generally, we might want to estimate the expectation of some function $f(\mathcal{X})$ relative to $P$ ; this task is a generalization, since we can choose $f(\xi)=I\!\!\left\{\xi\langle Y\rangle=y\right\}$  , where we recall that $\xi\langle Y\rangle$ is e assignment in $\xi$ to the variables $Y$ . We approximate this expectation by generating a set of M particles, estimating the value of the function or its expectation relative to each of the generated particles, and then aggregating the results. 
>  本章的讨论框架如下：
>  考虑某个分布 $P(\mathcal X)$，我们要估计其中某个事件 $\pmb Y = \pmb y$ 的概率 ($\pmb Y \subseteq \mathcal X, \pmb y \in Val(\pmb Y)$)，更一般地说，我们要估计某个函数 $f(\mathcal X)$ 相对于 $P$ 的期望
>  显然，第一种目标是第二种目标的特例，如果我们选择 $f(\xi) = \mathbf 1\{\xi\langle \pmb Y \rangle = \pmb y\}$ (其中 $\xi \langle \pmb Y \rangle$ 表示赋值 $\xi$ 中对 $\pmb Y$ 的赋值)，那么估计某个事件 $\pmb Y = \pmb y$ 的概率就是估计 $f(\xi)$ 相对于 $P$ 的期望
>  为了估计函数 $f(\mathcal X)$ 相对于 $P$ 的期望，我们通过生成一组粒子，计算函数相对于这些粒子的期望值，用该值作为近似

For most of this chapter, we focus on methods that generate particles using random sampling: In section 12.1, we consider the simplest possible method, which simply generates samples from the original network. In section 12.2, we present a signiﬁcantly improved method that generates samples from a distribution that is closer to the posterior distribution. In section 12.3, we discuss a method based on Markov chains that deﬁnes a sampling process that, as it converges, generates samples from distributions arbitrarily close to the posterior. In section 12.5, we consider a very different type of method, one that generates particles deterministic ally by searching for high-probability instantiations in the joint distribution. Finally, in section 12.4, we extend these methods to the case of collapsed particles. We note that, unlike our discussion of exact inference, some of the methods presented in this chapter — forward sampling and likelihood weighting — apply (at least in their simple form) only to Bayesian networks, and not to Markov networks or chain graphs. 
>  我们生成粒子的方法一般采用随机采样
>  12.1 节考虑最简单的方法，即从原网络中生成样本
>  12.2 节从一个接近后验分布的分布生成样本
>  12.3 节讨论基于 Markov 链定义采样过程的方法，随着 Markov 链收敛，从分布中生成的样本将无限接近于从原后验中生成的样本
>  12.5 节讨论从联合分布中的高概率实例中确定性生成样本
>  注意和之前对于精确推理的讨论不同，本章讨论的部分方法，例如前向采样和似然加权仅适用于贝叶斯网络，不适用于 Markov 网络或链图

## 12.1 Forward Sampling 
The simplest approach to the generation of particles is forward sampling . Here, we generate random samples $\xi[1],\cdot\cdot\cdot,\xi[M]$ from the distribution $P(\mathcal X)$ . We ﬁrst show how we can easily generate particles from $P_{\mathcal{B}}(\mathcal{X})$ by sampling from a Bayesian network. We then analyze the number of particles needed in order to get a good approximation of the expectation of some target function $f$ . We ﬁnally discuss the diﬃculties in generating samples from the posterior $P_{\mathcal{B}}(\mathcal{X}\mid e)$ . We note that, in undirected models, even generating a sample from the prior distribution is a diﬃcult task. 
>  生成粒子最简单的方法即前向采样，也就是从分布 $P(\mathcal X)$ 生成随机样本 $\xi[1],\cdots, \xi[M]$
>  本节先讨论如何从 $P_{\mathcal B}(\mathcal X)$ 中通过采样生成粒子，然后分析要得到对目标函数 $f$ 的期望的较好的近似时需要的粒子数量，最后讨论从后验分布 $P_{\mathcal B}(\mathcal X \mid e)$ 中生成样本的难度
>  注意，和有向模型不同，在无向模型中，从先验分布生成样本都是很困难的

### 12.1.1 Sampling from a Bayesian Network 
Sampling from a Bayesian network is a very simple process. 
>  从贝叶斯网络采样则是十分简单的过程

Example 12.1 
Consider the Student network, shown again in ﬁgure 12.1. We begin by sampling $D$ with the appropriate (unconditional) distribution; that is, we ﬁguratively toss a coin that lands heads $(d^{1}$ ) 40 percent of the time and tails $(d^{0})$ ) the remaining 60 percent. Let us assume that the coin landed heads, so that we pick the value $d^{1}$ for $D$ . Similarly, we sample $I$ from its distribution; say that the result is $i^{0}$ . en those, we know the r ht distribution from which to sample $G$ : $P(G\mid i^{0},d^{1})$ , as deﬁned by $G\acute{s}$ ’s CPD; we therefore pick G to be $g^{1}$ with probability 0 . 05 , $g^{2}$ with probability 0 . 25 , and $g^{3}$ with probability 0 . 7 . The process continues similarly for $S$ and $L$ . 

![[pics/PGM-Algorithm12.1.png]]

As shown in algorithm 12.1, we sample the nodes in some order consistent with the partial order of the BN, so that by the time we sample a node we have values for all of its parents. We can then sample from the distribution deﬁned by the CPD and by the chosen values for the node’s parents. Note that the algorithm requires that we have the ability to sample from the distributions underlying our CPD. Such sampling is straightforward in the discrete case (see box 12. A), but subtler when dealing with continuous measures (see section 14.5.1). 
>  在 algorithm 12.1 中，我们遵循 BN 定义的偏序对节点进行采样，因此当我们需要采样一个节点时，我们已经有了它所有父节点的值，此时我们直接从该节点的 CPD 中采样 (CPD 由其父节点当前的采样值定义)
>  该方法要求我们直接从 CPD 中采样，CPD 为离散分布时较为容易，CPD 为连续分布的情况下则略微困难

Box 12. A — Skill: Sampling from a Discrete Distribution. 
How do we generate a sample from a distribution? For a uniform distribution, we can use any pseudo-random number generator on our machine. Other distributions require more thought, and much work has been devoted in statistics to the problem of sampling from a variety of parametric distributions. Most obviously, consider a multinomial distribution $P(X)$ for $V a l(X)\,=\,\{x^{1},\cdot\,\cdot\,,x^{k}\}$ , which is deﬁned by parameters $\theta_{1},\ldots,\theta_{k}$ . This process can be done quite simply as follows: We generate a sample $s$ uniformly from the interval $[0,1]$ . We then partition the interval into $k$ subintervals: $[0,\theta_{1}),[\theta_{1},\theta_{1}+\theta_{2}),.\;.\;.,$ ; at is, the i th interval is $\textstyle[\sum_{j=1}^{i-1}\theta_{j},\sum_{j=1}^{i}\theta_{j})$ . If $s$ is in the i th interval, then the sampled value is $x^{i}$ . We can determine the interval for s using binary search in time $O(\log k)$ . 

This approach gives us a general-purpose solution for generating samples from the CPD of any discrete-valued variable: given a parent assignment $\mathbfit{u}$ , we can always generate the full conditional distribution $P(X\mid{\pmb u})$ and sample from it. (Of course, more efcient methods may exist if $X$ has $^a$ large value space or a CPD that requires an expensive computation.) As we discuss in section 14.5.1, the problem of sampling from continuous CPDs is considerably more complex. 

Using basic convergence bounds (see appendix A.2), we know that from a set of particles $\mathcal{D}=\{\xi[1],\dots,\xi[M]\}$ generated via this sampling process, we can estimate the expectation of any function $f$ as: 

$$
\hat{\pmb{{E}}}_{\mathcal{D}}(f)=\frac{1}{M}\sum_{m=1}^{M}f(\xi[m]).\tag{12.1}
$$

In the case where our task is to compute $P(\pmb{y})$ , this estimate is simply the fraction of particles where we have seen the event $\pmb y$ : 

$$
\hat{P}_{\mathcal{D}}(\pmb{y})=\frac{1}{M}\sum_{m=1}^{M}\pmb{1}\{\pmb{y}[m]=\pmb{y}\},\tag{12.2}
$$ 
where we use $\pmb{y}[m]$ to denote $\xi[m]\langle \pmb Y\rangle\mathrm{~-~}$ the assignment to $\pmb Y$ in the particle $\xi[m]$ . For example, our estimate for the probability of an event such as $i^{1},l^{0}$ (a smart student getting a bad letter) is the fraction of particles in which $I$ got the value $i^{1}$ and $L$ the value $l^{0}$ . Note that the same set of particles can be used to estimate the probabilities of multiple events. 

> 通过该采样过程，我们得到一组粒子 $\mathcal D = \{\xi[i]\}_{i=1,\cdots, M}$ (每个粒子都是对 $\mathcal X$ 的一组赋值)，我们根据采样得到的粒子计算 $f(\xi[i])$ 的平均值，作为 $f()$ 对 $P$ 的期望值的近似
> 如果目标是计算某个部分赋值 $\pmb y$ 的概率，我们简单对 $\pmb y$ 在 $\mathcal D$ 中出现的次数进行计数，并除去总采样次数 $M$ 计算它出现的频率，用频率近似它的概率
> 注意同一组粒子可以用于估计多个事件的概率

This sampling process requires one sampling operation for each random variable in the network. For each variable $X$ , we need to index into the CPD using the current partial instantiation of the parents of $X$ . Using an appropriate data structure, this indexing can be accomplishing in time $O(|\mathrm{Pa}_{X}|)$ . The actual sampling process, we discussed, requires time $O(\log|V a l(X)|)$ (assuming appropriate preprocessing). Letting M be the total number of particles generated, $n=|{\mathcal{X}}|$ , $p=\mathrm{max}_{i}\left|\mathrm{Pa}_{X_{i}}\right|$ , and $d=\operatorname*{max}_{i}{\left|V a l(X_{i})\right|}$ , the overall cost is $O(M n p\log d)$ . 
>  该采样过程需要对网络中的每个随机变量都进行一次采样操作
>  对于每个变量 $X$，要对它进行采样前，我们先通过 $Pa_X$ 的当前的采样值索引对应的 CPD，复杂度为 $O(|Pa_X|)$；进行采样时，采样过程需要 $O(\log |Val(X)|)$ (见 `Box12.A` )
>  $M$ 为总的粒子数量，$n = |\mathcal X|$，$p = \max_i|Pa_{X_i}|$，$d = \max_i|Val(X_i)|$，则总开销为 $O(M \cdot np\log d )$

### 12.1.2 Analysis of Error 
Of course, the quality of the estimate obtained depends heavily on the number of particles generated. We now analyze the question of the number of particles required to obtain certain performance guarantees. We focus on the analysis for the case where our goal is to estimate $P(\pmb{y})$ . 
>  上一小节介绍的采样估计的质量依赖于生成的粒子数量
>  本节讨论对 $P(\pmb y)$ 进行估计时需要多少粒子才能获得一定的性能保证

The techniques of appendix A.2 provide us with the necessary tools for this analysis. Consider the quality of our estimate for a particular event $Y\,=\,y$ . We deﬁne a new random variable over the probability space of $P$ , using the indicator function $I\{Y=y\}$  . This is a Bernoulli random variable, and hence our $M$ particles in $\mathcal{D}$ deﬁne $M$ independent Bernoulli trials, each with success probability $P(\pmb{y})$ . 
>  对于事件 $\pmb Y = \pmb y$，我们在 $P$ 的概率空间上将指示函数 $\mathbf 1\{\pmb Y = \pmb y\}$ 定义为新的随机变量，显然它服从伯努利分布 (二项分布)，因此我们得到的 $M$ 个粒子定义了 $M$ 个相互独立的伯努利试验，每次伯努利试验中，$\mathbf 1\{\pmb Y = \pmb y\}$ 等于 1 的概率为 $P(\pmb y)$

We can now apply the Hoefding bound (theorem A.3) to show that this estimate is close to the truth with high probability: 

$$
\begin{array}{r}{P_{\mathcal{D}}\big(\hat{P}_{\mathcal{D}}(\pmb{y})\not\in[P(\pmb{y})-\epsilon,P(\pmb{y})+\epsilon]\big)\leq2e^{-2M\epsilon^{2}}.}\end{array}\tag{12.3}
$$ 
This analysis provides us with an estimate of how many samples are required to achieve an estimate whose error is bounded by $\epsilon$ , with probability at least $1-\delta$ . Setting 

$$
2e^{-2M\epsilon^{2}}\le\delta
$$ 
and doing simple algebraic manipulations, we get that the required sample size to get an estimator with $(\epsilon,\delta)$ reliability is: 

$$
M\geq\frac{\ln(2/\delta)}{2\epsilon^{2}}.
$$ 
>  $\epsilon$ 是估计值 $\hat P_D(\pmb y)$ 相对于准确值 $P(\pmb y)$ 的期待误差绝对值，$\delta$ 是估计值和准确值之间的误差在 $\epsilon$ 以外的概率上界
>  可以看到，要使得期望误差 $\epsilon$ 越小，$M$ 就应该越大，以及要使得实际误差在期望误差以内的概率越大，$\delta$ 就应该越小，$M$ 就应该越大

We can similarly apply the Chernof bound (theorem A.4) to conclude that $\hat{P}_{\mathcal{D}}(y)$ is also within a relative error $\epsilon$ of the true value $P(\pmb{y})$ , with high probability. Speciﬁcally, we have that: 

$$
\begin{array}{r}{P_{\mathcal{D}}\big(\hat{P}_{\mathcal{D}}(\pmb{y})\notin P(\pmb{y})(1\pm\epsilon)\big)\le2e^{-M P(\pmb{y})\epsilon^{2}/3}.}\end{array}\tag{12.4}
$$ 
Note that in this analysis, unlike the one based on Hoefding’s bound, the error probability (the chance of getting an estimate that is more than $\epsilon$ away from the true value) depends on the actual target value $P(\pmb{y})$ . This dependence is not surprising for a relative error bound. Assume that we generate $M$ samples, but we generate none where $\pmb{y}[m]=\pmb{y}$ . Our estimate $\hat{P}_{\mathcal{D}}(y)$ is simply 0 . However, if $P(\pmb{y})$ is very small, it is fairly likely that we simply have not generated any samples where this event holds. In this case, our estimate of 0 is not going to be within any relative error of $P(\pmb{y})$ . Thus, for very small values of $P(\pmb{y})$ , we need many more samples in order to guarantee that our estimate is close with high probability. 
>  之前的分析基于 Hoefding 界，我们也可以使用 Chernof 界，得到 (12.4)
>  Chernof 界和 Hoefding 界的差异在于前者的误差概率 (估计值和真实值的误差在 $\epsilon$ 以外的概率，即 $\delta$ ) 依赖于真实的目标值 $P(\pmb y)$，这是因为此时 $\epsilon$ 表示的是相对误差，而不是绝对误差值
>  Chernof 是关于相对误差的界，因此这个依赖性并不奇怪
>  例如，假设真实值 $P(\pmb y)$ 本身就十分小，那么我们生成 $M$ 个样本，很有可能其中没有样本 $\xi[i]$ 满足 $\xi[i]\langle \pmb Y\rangle = \pmb y$，因此得到估计值为 0 ，而 0 值显然不会在 $P(\pmb y)$ 的任意相对误差界内，故由于 $P(\pmb y)$ 较小，我们也需要采样更多样本，使得估计值 $\hat P_{\mathcal D}(\pmb y)$ 在相对误差界内的概率更高

Examining equation (12.4), we can see that, for a given $\epsilon$ , the number of samples needed to guarantee a certain error probability $\delta$ is: 

$$
M\geq3\frac{\ln(2/\delta)}{P(\pmb{y})\epsilon^{2}}.\tag{12.5}
$$ 
Thus, the number of required samples grows inversely with the probability $P(\pmb{y})$ . 

>  因此，对于给定的相对误差值 $\epsilon$，要保证特定的某个较小的误差概率 $\delta$，我们可以计算得到所需的样本数量 $M$ 应大于特定阈值，如 (12.5) 所示
>  显然，$P(\pmb y)$ 越小，所要求的 $M$ 越大

In summary, to guarantee an absolute error of $\epsilon$ with probability at least $1-\delta$ , we need a number of samples that grows logarithmically in $1/\delta$ and quadratically in $1/\epsilon$ . To guarantee a relative error of $\epsilon$ with error probability at most $\delta$ , we need a number of samples that grows similarly in $\delta$ and $\epsilon$ , but that also grows linearly with $1/P(\pmb{y})$ . A signiﬁcant problem with using this latter bound is that we do not know $P(\pmb{y})$ . (If we did, we would not have to estimate it.) Thus, we cannot determine how many samples we need in order to ensure a good estimate. 
>  总结来说，要保证估计值至少有 $1-\delta$ 的概率在绝对误差 $\epsilon$ 内，需要的样本数量的增长速度和 $1 / \delta$ 呈对数关系，和 $1/\epsilon$ 呈二次关系；要保证至少有 $1-\delta$ 的概率在相对误差 $\epsilon$ 内的结论是类似的，不过同时 $M$ 的增长还和 $1/P(\pmb y)$ 呈线性关系
>  但我们一般不知道 $P(\pmb y)$，因此要估计确保特定相对误差的情况下所需的样本数量一般是不可行的

### 12.1.3 Conditional Probability Queries 
So far, we have focused on the problem of estimating marginal probabilities, that is, probabilities of events $Y=y$ relative to the original joint distribution. In general, however, we are interested in conditional probabilities of the form $P(\pmb{y}\mid\pmb{E}\,=\,e)$ . Unfortunately, it turns out that this estimation task is signiﬁcantly harder. 
>  除了事件 $\pmb Y = \pmb y$ 的边际概率以外，我们进一步考虑估计条件概率，其形式为 $P(\pmb y \mid \pmb E = \pmb e)$，该任务显著更难

One approach to this task is simply to generate samples from the posterior pr ability $P(\mathcal X\mid$ $e$ ) . We can do so by a process called rejection sampling : We generate samples x from $P(X)$ , as in section 12.1.1. We then reject any sample that is not compatible with $e$ . The resulting samples are sampled from the posterior $P(X\mid e)$ . The results of the analysis in section 12.1.2 now apply unchanged. 
>  一种方法是考虑从后验分布 $P(\mathcal X \mid \pmb e)$ 中生成样本
>  可以通过拒绝采样执行这一过程：先直接从 $P(\pmb X)$ 中生成样本 $\pmb x$，然后拒绝任意和条件 $\pmb e$ 不匹配的样本，留下来的样本就认为是从后验分布 $P(\pmb X \mid \pmb e)$ 中采样得到的样本

The problem, of course, is that the number of unrejected particles can be quite small. In general, the expected number of particles that are not rejected from an original sample set of size $M$ is $M P(e)$ . For example, if $P(e)=0.001$ , then even for $M\,=\,10,000$ samples, the expected number of unrejected particles is 10 . Conversely, to obtain at least $M^{*}$ unrejected particles, we need to generate on average $M=M^{*}/P(e)$ samples from $P(X)$ . 
>  该方法的问题在于会拒绝过多的样本
>  假设生成 $M$ 个样本，不被拒绝的样本数量的期望值应该是 $MP(\pmb e)$
>  例如当 $P(\pmb e) = 0.001$，则即便我们生成 $M = 10,000$ 个样本，则最后留下来的样本的期望值也仅为 10
>  反过来说，要得到至少 $M^*$ 个不被拒绝的样本，我们平均需要从 $P(\pmb X)$ 中生成 $M = M^*/P(\pmb e)$ 个样本

Unfortunately, in many applications, low-probability evidence is the rule rather than the exception. For example, in medical diagnosis, any set of symptoms typically has low probability. In general, as the number of observe variables $k=|E|$ grows, the probability of the evidence usually decreases exponentially with k . 
>  在许多应用中，evidence 的概率 $P(\pmb e)$ 低是常态，并且，随着 evidence 涉及到的变量数量 $k = |\pmb E|$ 增长，evidence 的概率会随着 $k$ 指数级下降

An alternative approach to the problem is to use a separate estimator for $P(e)$ and for $P(\pmb{y},e)$ and then compute the ratio. We can show that if we have estimators of low relative error for both of these quantities, then their ratio will also have a low relative error (exercise 12.2). Unfortunately, this approach only moves the problem from one place to the other. As we said, **the number of samples required to achieve a low relative error also grows linearly with $1/P(e)$ . The number of samples required to get low absolute error does not grow with $P(e)$ . However, it is not hard to verify (exercise 12.2) that a bound on the absolute error for $P(e)$ does not suffice to get any type of bound (relative or absolute) for the ratio $P(\pmb{y},\pmb{e})/P(\pmb{e})$ .** 
>  还有一种方法是分别估计 $P(\pmb e)$ 和 $P(\pmb y, \pmb e)$，然后计算它们的比值
>  可以证明，如果对于 $P(\pmb e)$ 和 $P(\pmb y, \pmb e)$ 的估计的相对误差都较小，则它们的比值和 $P(\pmb y \mid \pmb e)$ 的相对误差也会较小
>  但问题仍然存在，我们知道要达到特定相对误差所需要的采样样本数量随着 $1/P(\pmb e)$ 线性增长，因此对于 $P(\pmb e)$ 小的 evidence 我们仍需要大量的样本
>  并且转而考虑绝对误差也是不可行的，可以验证如果对于 $P(\pmb e)$ 和 $P(\pmb y, \pmb e)$ 的估计的绝对误差都较小，则它们的比值和 $P(\pmb y \mid \pmb e)$ 的绝对误差和相对误差却不能保证

## 12.2 Likelihood Weighting and Importance Sampling 
The rejection sampling process seems very wasteful in the way it handles evidence. We generate multiple samples that are inconsistent with our evidence and that are ultimately rejected without contributing to our estimator. In this section, we consider an approach that makes our samples more relevant to our evidence. 
>  拒绝采样过程中，我们会生成过多和估计值不相关的样本，本节考虑能使得样本和 evidence 更相关的方法

### 12.2.1 Likelihood Weighting: Intuition 
Consider the network in example 12.1, and assume that our evidence is $d^{1},s^{1}$ . Our forward sampling process might begin by generating a value of $d^{0}$ for $D$ . No matter how the sampling process proceeds, this sample will always be rejected as being incompatible with the evidence. 
>  考虑示例 12.1 中的网络，并假设我们的证据是 $d^{1}, s^{1}$
>  我们的前向采样过程可能会从为 $D$ 生成 $d^{0}$ 的值开始，得到 $d^0$ 之后，无论采样过程如何进行，这个样本总是会被拒绝，因为它与证据不兼容

It seems much more sensible to simply force the samples to take on the appropriate values at observed nodes. That is, when we come to sampling a node $X_{i}$ whose value has been observed, we simply set it to its observed value. 
>  似乎更有意义的做法是直接让样本在观测节点上取特定的值
>  也就是说，当我们采样一个节点 $X_i$ 时，如果它的值已经被观察到 ($X_i$ 属于 evidence)，我们只需将 $X_i$ 的采样值设置为其观察到的值

In general, however, this simple approach can generate incorrect results: 
>  但该简单的策略一般会导致错误的结果

Example 12.2
Consider the network of example 12.1, and assume that our evidence is $s^{1}-a$ student who received a high SAT score. Using the naive process, we sample $D$ and $I$ from their prior distribution, set $S\;=\;s^{1}$ , and then sample $G$ and $L$ appropriately. All of our samples will have $S\;=\;s^{1}$ , as desired. However, the expected number of samples that have $I=i^{1}-a n$ intelligent student — is 30 percent, the same as in the prior distribution. Thus, this approach fails to conclude that the posterior probability of $i^{1}$ is higher when we observe $s^{1}$ . 

The problem with this approach is that it fails to account for the fact that, in the standard forward sampling process, the node $S$ is more likely to take the value $s^{1}$ when its parent $I$ has the value $i^{1}$ than when $I$ has the value $i^{0}$ . In particular, consider an imaginary process where we run rejection sampling many times; samples where we generated the value ${\mathit{\bar{I}}}\,=\,i^{1}$ would have generated $S=s^{1}$ in 80 percent of the samples, whereas samples where we generated the value $I=i^{0}$ would have generated $S=s^{1}$ in only 5 percent of the samples. To simulate this long-run behavior within a single sample, we should conclude that a sample where we have $I=i^{1}$ and force $S=s^{1}$ should be worth 80 percent of a sample, whereas one where we have $I=i^{0}$ and force $S=s^{1}$ should only be worth 5 percent of a sample. 
>  上例使用的 BN 中存在一个事实：在标准前向采样过程中，节点 $S$ 在其父节点 $I$ 取 $i^0$ 时更容易取到 $s^1$ 值，而在任意情况下都将 $S$ 的值设定为 $s^1$ 就会导致我们无法从得到的样本序列中观察到这一相关性
>  更具体地说，假设我们进行很多次的标准前向采样，我们会发现，满足 $I = i^1$ 的样本中，有 80% 都满足 $S = s^1$，满足 $I = i^0$ 的样本中，仅有 5% 满足 $S = s^1$
>  而我们直接令 $S = s^1$ 时，就忽略了以上的这种相关性，因此，为了在单个样本中模拟这一多次采样出现的情况，当我们将 $I = i^1$ 的样本的 $S$ 赋值为 $s^1$ 时，我们应该认为该样本仅有 80% 的权重，将 $I = i^0$ 的样本的 $S$ 赋值为 $s^1$ 时，我们认为该样本仅有 5% 的权重

When we have multiple observations and we want our sampling process to set all of them to their observed values, we need to consider the probability that each of the observation nodes, had it been sampled using the standard forward sampling process, would have resulted in the observed values. The sampling events for each node in forward sampling are independent, and hence the weight for each sample should be the product of the weights induced by each evidence node separately. 
>  将该过程推广到多个观察变量，我们希望在采样过程中将这些变量的采样值直接设定为它们的观察值
>  此时，我们分别考虑每个观察节点，如果采用标准的前向采样过程，它得到观察值的概率是多少
>  在标准前向采样过程中，每个节点的采样事件是相互独立的 (都是给定已知，确定单个节点的 CPD 后对单个节点采样，而不是一次采样多个节点)，因此在标准前向采样中，观察出现的概率应该是每个观察节点得到各自观察值的概率的积
>  因此，我们在加权时，每个样本的权重应该是每个观察节点的权重的积

Example 12.3 
Consider the same network, where our evidence set now consists of $l^{0},s^{1}$ . Assume that we sample $D\,=\,d^{1}$ , $I\,=\,i^{0}$ , set $S\;=\;s^{1}$ , sample $G\,=\,g^{2}$ , and set $L\,=\,l^{0}$ . The probability that, given $I\,=\,i^{0}$ , forward sampling would have generated $S\;=\;s^{1}$ is 0 . 05 . The probability that, given $G\,=\,g^{2}$ , forward sampling would have generated $L\,=\,l^{0}$ is 0 . 4 . If we consider the standard forward sampling process, each of these events is the result of an independent coin toss. Hence, the probability that both would have occurred is simply the product of their probabilities. Thus, the weight required for this sample to compensate for the setting of the evidence is $0.05\cdot0.4=0.02.$ 

![[pics/PGM-Algorithm12.2.png]]

Generalizing this intuition results in an algorithm called likelihood weighting (LW) , shown in algorithm 12.2. The name indicates that the weights of different samples are derived from the likelihood of the evidence accumulated throughout the sampling process. 
>  我们将这一直觉推广为似然加权算法，见 algorithm 12.2
>  似然加权意味着不同样本的权重来自于 evidence 在采样过程中累计的似然

This process generates a weighted particle . We can now estimate a conditional probability $P(\pmb{y}\mid\pmb{e})$ by using LW-Sample $M$ times to generate a set $\mathcal{D}$ of weighted particles $\langle\xi[1],w[1]\rangle,.\,.\,.\,,\langle\xi[M],w[M]\rangle$ . We then estimate: 

$$
\hat{P}_{\mathcal{D}}(\pmb{y}\mid \pmb e)=\frac{\sum_{m=1}^{M}w[m]\pmb{1}\{\pmb{y}[m]=\pmb{y}\}}{\sum_{m=1}^{M}w[m]}.\tag{12.6}
$$ 
This estimator is an obvious generalization of the one we used for unweighted particles in equation (12.2). There, each particle had weight 1 ; hence, the terms in the numerator were unweighted, and the denominator, which is the sum of all the particle weights, was simply $M$ . It is also important to note that, as in forward sampling, the same set of samples can be used to estimate the probability of any event $\pmb y$ . 

>  通过似然加权采样，我们得到的就是一组加权的粒子，我们使用这组加权的粒子来估计条件概率 $P(\pmb y \mid \pmb e)$，如 (12.6) 所示
>  (12.6) 的估计器显然是对相对于未加权粒子的 (12.2) 的推广，(12.2) 可以认为每个粒子的权重是 1
>  和前向采样相同的是，同一组粒子可以用于估计任意事件 $\pmb y$ 的概率

Aside from some intuition, we have provided no formal justiﬁcation for the correctness of LW as yet. It turns out that LW is a special case of a very general approach called importance sampling , which also provides us the basis for an analysis. We begin by providing a general description and analysis of importance sampling, and then reformulate LW as a special case of this framework. 
>  似然加权采样实际上是重要性采样方法的特例，我们在下一节对其讨论

### 12.2.2 Importance Sampling 
Let $X$ be a variable (or set of variables) that takes on values in some space $V a l(X)$ . Importance sampling is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$ , typically called the target distribution . 
> 令 $\pmb X$ 为一组变量，取值空间为 $Val(\pmb X)$
> 重要性采样是用于估计函数 $f(\pmb x)$ 相对于某个 $\pmb X$ 上的分布 $P(\pmb X)$ (一般称为目标分布) 的期望的一种通用方法

As we discussed, we can estimate this expectation by generating samples ${\pmb x}[1],\cdot\cdot\cdot,{\pmb x}[M]$ from $P$ , and then estimating 

$$
E_{P}[f]\approx\frac{1}{M}\sum_{m=1}^{M}f(\pmb{x}[m]).
$$ 
>  我们从 $P$ 中生成 $M$ 个样本 $\pmb x[1], \dots, \pmb x[M]$ ，计算 $f(\pmb x)$ 在这 $M$ 个样本上的平均值，作为其期望的估计值

In some cases, however, we might prefer to generate samples from a diferent distribution $Q$ , known as the proposal distribution or sampling distribution . There are several reasons why we might wish to sample from a diferent distribution. Most importantly for our purposes, it might be impossible or computationally very expensive to generate samples from $P$ . For example, $P$ might be a posterior distribution for a Bayesian network, or even a prior distribution for a Markov network. 
>  在一些情况下，我们可能希望从另一个分布 $Q$ 进行采样，该分布称为提案分布或采样分布
>  这样做的一大原因在于有时从 $P$ 中生成样本是非常昂贵的，例如 $P$ 是一个 BN 的后验分布，或者 MN 的先验分布

In this section, we discuss how we might obtain estimates of an expectation relative to $P$ by generating samples from a different distribution $Q$ . In general, the proposal distribution $Q$ can be arbitrary; we require only that $Q({\pmb x})\,>\,0$ whenever $P(\pmb{x})\;>\;0$ , so that $Q$ does not “ignore” any states that have nonzero probability relative to $P$ . (More formally, the support of a distribution $P$ is the set of points ${x}$ for which $P({\pmb x})\,>\,0$ ; we require that the support of $Q$ contain the support of $P$ .) However, as we will see, the computational performance of this approach does depend strongly on the extent to which $Q$ is similar to $P$ . 
>  本节中，我们讨论用 $Q$ 中采样的样本对相对于 $P$ 的期望进行估计
>  提案分布 $Q$ 通常可以是任意的，我们仅要求当 $P(\pmb x) > 0$ 时 $Q(\pmb x)>0$，故 $Q$ 不会“忽略” 任何相对于 $P$ 具有非零概率的状态

#### 12.2.2.1 Unnormalized Importance Sampling 
If we generate samples from $Q$ instead of $P$ , we cannot simply average the $f$ -value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that 

$$
\pmb{{E}}_{P(\pmb X)}[f(\pmb X)]=\pmb{{E}}_{Q(\pmb X)}\bigg[f(\pmb X)\frac{P(\pmb X)}{Q(\pmb X)}\bigg].\tag{12.7}
$$ 
This equality follows directly:

$$
\begin{align}
\pmb E_{Q(\pmb X)} \left[f(\pmb X)\frac {P(\pmb X)}{Q(\pmb X)}\right]&=\sum_{\pmb x} Q(\pmb x) f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}\\
&=\sum_{\pmb x}f(\pmb x)P(\pmb x)\\
&=E_{P(\pmb X)}\left[f(\pmb X)\right]
\end{align}
$$ 
>  根据上面的推导，我们知道函数 $f(\pmb x)$ 相对于分布 $P(\pmb X)$ 的期望等价于函数 $f(\pmb x) \frac {P(\pmb x)}{Q(\pmb x)}$ 相对于分布 $Q(\pmb X)$ 的期望
>  上面的推导假设了离散情况，但该结论对于连续情况也是成立的

Based on this observation, we can use the standard estimator for expectations relative to $Q$ . We generate a set of samples ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ from $Q$ , and then estimate: 

$$
\hat{\pmb{{E}}}_{\cal D}(f)=\frac{1}{M}\sum_{m=1}^{M}f({\pmb x}[m])\frac{{P}({\pmb x}[m])}{{Q}({\pmb x}[m])}.\tag{12.8}
$$ 
>  因此，我们可以从 $Q$ 中采样，使用样本平均值估计函数 $f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}$ 相对于 $Q$ 的期望，该结果同时就是对函数 $f(\pmb x)$ 相对于 $P$ 的期望的估计，因为这两个期望值是相等的，因此估计的实际就是同一个值，只不过解释方式不同

We call this estimator the unnormalized importance sampling estimator; this method is also often called unweighted importance sampling (this terminology is confusing, inasmuch as the particles here are also associated with weights). The factor $P(\pmb{x}[m])/Q(\pmb{x}[m])$ can be viewed as a correction weight to the term $f({\boldsymbol{x}}[m])$ , which we would have used had $Q$ been our target distribution. We use $w(\pmb x)$ to denote $P(\pmb{x})/Q(\pmb{x})$ . 
>  我们称 (12.8) 的估计器为未规范化的重要性采样估计器，该方法也称为未加权的重要性采样 (虽然这里实际上每个粒子已经和一个权重相关)
>  每个粒子相关的 $P(\pmb x[i])/Q(\pmb x[i])$ 可以视作对 $f(\pmb x[i])$ 的校正权重
>  我们将 $P(\pmb x)/Q(\pmb x)$ 记作 $w(\pmb x)$

Our analysis immediately implies that this estimator is unbiased , that is, its mean for any data set is precisely the desired value: 
>  该估计是无偏的，即估计值的期望就是我们的目标期望，或者说，将所有可能数据集的估计值求平均得到的就是目标期望

**Proposition 12.1** 
For data sets $\mathcal{D}$ sampled from $Q$ , we have that: 

$$
\pmb{E}_{\mathcal{D}}\Big[\hat{\pmb{E}}_{\mathcal{D}}(f)\Big]=\pmb{E}_{Q(\pmb{X})}[f(\pmb{X})w(\pmb{X})]=\pmb{E}_{P(\pmb{X})}[f(\pmb{X})].
$$ 
We can also estimate the distribution of this estimator around its mean. Letting $\epsilon_{\mathcal{D}}~=$ $\hat{\pmb{{ E}}}_{\mathcal{D}}(f)-\pmb{{  E}}_{ P}[f(\pmb{x})]$  , we have that, since $M\rightarrow\infty$ : 

$$
\pmb E_{\mathcal{D}}[\epsilon_{\mathcal{D}}]\sim\mathcal{N}\left(0;\sigma_{Q}^{2}/M\right),
$$

where 

$$
\begin{array}{r c l}{\sigma_{Q}^{2}}&{=}&{{\pmb E}_{Q(\pmb X)}\big[(f(\pmb X)w(\pmb X))^{2}\big]-{\pmb E}_{Q(\pmb X)}[(f(\pmb X)w(\pmb X))]^{2}}\\ &{=}&{{\pmb E}_{Q(\pmb X)}\big[(f(\pmb X)w(\pmb X))^{2}\big]-({\pmb E}_{P(\pmb X)}[f(\pmb X)])^{2}.}\end{array}\tag{12.9}
$$ 
>  命题
>  对于 $Q$ 中采样得到的任意数据集，其估计值 $\hat {\pmb E}_{\mathcal D}(f)$ 的期望就是目标期望值
>  并且，随着采样数量 $M$ 趋近于正无穷，该估计器和其期望之间的误差将服从均值为零，方差为 $\sigma^2_Q/M$ 的高斯分布，其中 $\sigma_Q^2$ 的形式如 (12.9) 所示

As we discussed in appendix A.2, the variance of this type of estimator — an average of $M$ independent random samples from a distribution — decreases linearly with the number of samples. This point is important, since it allows us to provide a bound on the number of samples required to obtain a reliable estimate. 
>  对于形式为 $M$ 个独立同分布的随机样本的平均值的估计器，其方差会随着样本数量 $M$ 线性下降
>  因此，我们可以分析我们想要得到某个可靠估计时需要的样本数量的界

To understand the constant term in this expression, consider the (uninteresting) case where the function $f$ is the constant function $f(\xi)\equiv1$ . In this case, equation (12.9) simpliﬁes to: 

$$
\begin{array}{r c l}{\pmb E_{Q(\pmb X)}\left[w(\pmb X)^{2}\right]-\pmb E_{P(\pmb X)}[1]}&{=}&{\pmb E_{Q(\pmb X)}\Biggl[\left(\frac{P(\pmb X)}{Q(\pmb X)}\right)^{2}\Biggr]-\left(\pmb E_{Q(\pmb X)}\left[\frac{P(\pmb X)}{Q(\pmb X)}\right]\right)^{2},}\end{array}
$$ 
which is simply the variance of the weighting function $P(\pmb x)/Q(\pmb x)$ . Thus, the more different $Q$ is from $P$ , the higher the variance of this estimator. When $f$ is an indicator function over part of the space, we obtain an identical expression restricted to the relevant subspace. 
>  考虑 $f$ 为恒等函数 $f\equiv 1$ 时的情况，我们可以将 (12.9) 中的方差简化为上式
>  可以看到，此时该方差就是权重函数 $P(\pmb x)/Q(\pmb x)$ 相对于 $Q$ 的方差 (注意 $P(\pmb x)/ Q(\pmb x)$ 相对于 $Q$ 的期望是 1)
>  因此，如果 $Q$ 和 $P$ 的差异越大，则方差项 $\sigma_Q$ 就越大，也就是误差项 $\epsilon_{\mathcal D}$ 的方差越大，因此估计器的方差等于 $E[\epsilon_{\mathcal D}^2]  = Var[\epsilon_{\mathcal D}] + E[\epsilon_{\mathcal D}]^2 = Var[\epsilon_{\mathcal D}]$ 就越大
>  当 $f$ 是空间某部分的指示函数时，我们在相应的子空间内得到一个相同的表达式

In general, one can show that the lowest variance is achieved when 

$$
Q(\pmb X)\propto|f(\pmb X)|P(\pmb X);
$$ 
thus, for example, if $f$ is an indicator function over part of the space, we want our sampling distribution to be $P$ conditioned on the subspace. 
>  总的来说，我们可以证明最低的方差在 $Q(\pmb X)$ 正比于 $|f(\pmb X)|P(\pmb X)$ 时取到
>  因此，例如 $f$ 是空间某部分上的指示函数时，我们希望我们的采样分布最好是 $P$ 在相应子空间上的条件分布

Note that we should avoid cases where our sampling probability $Q(\pmb X)\ll P(\pmb X)f(\pmb X)$ in any part of the space, since these cases can lead to very large or even inﬁnite variance. Thus, care must be taken when using very skewed sampling distributions, to ensure that probabilities in $Q$ are close to zero only when $P(\pmb X)f(\pmb X)$ is also very small. 
>  注意，我们需要避免我们的采样分布 $Q(\pmb X)$ 在空间的任意部分出现 $Q(\pmb X) \ll P(\pmb X) f(\pmb X)$，这会导致估计器的方差非常大甚至趋近无穷
>  因此，在使用非常偏斜的采样分布时必须小心，$Q(\pmb X)$ 仅在确保 $P(\pmb X) f(\pmb X$) 非常小时，才可以接近于零

#### 12.2.2.2 Normalized Importance Sampling 
One problem with the preceding discussion is that it assumes that $P$ is known. A frequent situation, and one of the most common reasons why we must resort to sampling from a different distribution $Q$ , is that $P$ is known only up to a normalizing constant $Z$ . Speciﬁcally, what we have access to is a function ${\tilde{P}}(X)$ such that $\tilde{P}$ is not a normalized distribution, but ${\tilde{P}}(X)=Z P(X)$ . For example, in a Bayesian work $\mathcal{B}$ , we might have (for $X=\mathcal{X}$ ) $P(\mathcal X)$ be our posterior distribution $P_{\mathcal{B}}(\mathcal{X}\mid e)$ , and $\tilde{P}(\mathcal X)$ b the unnormalized distribution $P_{\mathcal{B}}(\mathcal{X},e)$ . In a Markov network, $P(\mathcal X)$ might be $P_{\mathcal H}(\mathcal X)$ , and $\tilde{P}$ might be the unnormalized distribution obtained by multiplying together the clique potentials, but without normalizing by the partition function. 
>  之前的讨论中假设了 $P$ 是已知的，而一般我们需要转而在 $Q$ 中采样的原因往往是距离知道 $P$ 还相差一个规范化常数 $Z$
>  具体来说，我们能够访问的是一个函数 ${\tilde{P}}(\pmb X)$, ${\tilde{P}}$ 不是一个规范化的分布，但 ${\tilde{P}}(\pmb X) = Z P(\pmb X)$
>  例如，在 BN $\mathcal B$ 中，我们可能有 $P(\mathcal X)$ 为后验分布 $P_{\mathcal B}(\mathcal X\mid \pmb e)$ ，而 $\tilde P(\mathcal X)$ 为未规范化的分布 $P_{\mathcal B}(\mathcal X, \pmb e)$；在 MN 中，$P(\mathcal X)$ 可以为 $P_{\mathcal H}(\mathcal X)$，而 $\tilde P$ 为将所有团势能相乘得到的未规范化的分布 (没有除以分区函数)

In this context, we cannot deﬁne the weights relative to $P$ , so we deﬁne: 
>  此时不能定义相对于 $P$ 的权重，故定义相对于 $\tilde P$ 的权重

$$
w(\pmb X)=\frac{\tilde{P}(\pmb X)}{Q(\pmb X)}.\tag{12.10}
$$ 
Unfortunately, with this deﬁnition of weights, the analysis justifying the use of equation (12.8) breaks down. However, we can use a slightly diferent estimator based on similar intuitions. As before, the weight $w(\pmb X)$ is a random variable. Its expected value is simply $Z$ : 

$$
E_{Q(\pmb X)}[w(\pmb X)]=\sum_{\pmb x}Q(\pmb x)\frac{\tilde{P}(\pmb x)}{Q(\pmb x)}=\sum_{\pmb x}\tilde{P}(\pmb x)=Z.\tag{12.11}
$$ 
This quantity is the normalizing constant of the distribution $\tilde{P}$ , which is itself often of considerable interest, as we will see in our discussion of learning algorithms. 

>  此时，$w(\pmb X)$ 相对于 $Q(\pmb X)$ 的期望将是 $Z$，即 $\tilde P$ 的规范化常数

We can now rewrite equation (12.7): 

$$
\begin{align}
E_{P(\pmb X)}[f(\pmb X)] &= \sum_{\pmb x} P(\pmb x)f(\pmb x)\\
&=\sum_{\pmb x}Q(\pmb x)f(\pmb x)\frac {P(\pmb x)}{Q(\pmb x)}\\
&=\frac 1 Z \sum_{\pmb x}Q(\pmb x)f(\pmb x)\frac {\tilde P(\pmb x)}{Q(\pmb x)}\\
&=\frac 1 ZE_{Q(\pmb X)}[f(\pmb X)w(\pmb X)]\\
&=\frac {E_{Q(\pmb X)}[f(\pmb X)w(\pmb X)]}{E_{Q(\pmb X)}[w(\pmb X)]}\tag{12.12}
\end{align}
$$

We can use an empirical estimator for both the numerator and denominator. Given $M$ samples ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ from $Q$ , we can estimate: 

$$
\hat{\pmb{{ E}}}_{\mathcal {D}}(f)=\frac{\sum_{m=1}^{M}f({\pmb x}[m])w({\pmb x}[m])}{\sum_{m=1}^{M}w({\pmb x}[m])}.\tag{12.13}
$$ 
>  因此 $f(\pmb X)$ 相对于 $P$ 的期望可以重写为 (12.12) 的形式
>  我们对其中的分母和分子都进行经验估计，具体地说，我们从 $Q$ 从采样 $M$ 个样本，然后分别用平均值替代分子分母中的期望值，计算得到 $f$ 相对于 $P$ 的期望的估计值

We call this estimator the normalized importance sampling estimator ; it is also known as the weighted importance sampling estimator. 
>  称该估计器为规范化的重要性采样估计器，也称为加权的重要性采样估计器

The normalized estimator involves a quotient, and it is therefore much more difficult to analyze theoretically. However, unlike the unnormalized estimator of equation (12.8), the normalized estimator is not unbiased. This bias is particularly immediate in the case $M=1$ . Here, the estimator reduces to: 

$$
\frac{f(\pmb{x}[1])w(\pmb{x}[1])}{w(\pmb{x}[1])}=f(\pmb{x}[1]).
$$ 
Because $\pmb{x}[1]$ is sampled from $Q$ , the mean of the estimator in this case is $E_{Q(\pmb X)}[f(\pmb X)]$ rather than the desired $\pmb{E}_{P(\pmb{X})}[f(\pmb{X})]$ . Conversely, when $M$ goes to inﬁnity, we have that each of the numerators and denominators converges to the expected value, and our analysis of the expectation applies. In general, for ﬁnite $M$ , the estimator is biased, and the bias goes down as $1/M$ . 

>  规范化的估计器不是无偏的，考虑 $M=1$ 时，估计器写为 $f(\pmb x[1])$，其期望为 $E_Q[f(\pmb X)]$ 而不是 $E_P[f(\pmb X)]$
>  如果 $M$ 趋近于无穷，则分母和分子各自收敛到其期望值，此时估计器也将收敛到 $E_P[f(\pmb X)]$
>  因此对于有限的 $M$，该估计器是有偏的，偏置随着 $1/M$ 下降

One can show that the variance of the importance sampling estimator with $M$ data instances is approximately: 

$$
{V a r}_{P}\Big[\hat{\pmb{E}}_{D}(f(\pmb{X}))\Big]\approx\frac{1}{M}{ V a r}_{P}[f(\pmb{X})](1+{V a r}_{Q}[w(\pmb{X})]),\tag{12.14}
$$ 
which also goes down as $1/M$ . Theoretically, this variance and the variance of the unnormalized estimator (equation (12.8)) are incomparable, and each of them can be larger than the other. Indeed, it is possible to construct examples where each of them performs better than the other. In practice, however, the variance of the normalized estimator is typically lower than that of the unnormalized estimator. This reduction in variance often outweighs the bias term, so that the normalized estimator is often used in place of the unnormalized estimator, even in cases where $P$ is known and we can sample from it effectively. 

>  可以证明 $M$ 个样本的重要性采样估计器的方差的形式为 (12.14)，该方差随着 $1/M$ 下降
>  理论上该方差和未规范化重要性采样估计器的方差是不可比的，$M$ 相同时，不能保证在所有情况下一者比另一者更好
>  实践中一般规范化估计器的方差更低，因此虽然存在偏置，我们还是常用规范化估计器替代未规范化的估计器，即便在 $P$ 已知的情况下

Note that equation (12.14) can be used to provide a rough estimate on the quality of a set of samples generated using normalized importance sampling. Assume that we were to estimate $\pmb{{E}}_{P}[f]$ using a standard sampling method, where we generate $M$ IID samples from $P(\pmb X)$ . (Obviously, this is generally intractable, but it provides a useful benchmark for comparison.) This approach would result in a variance ${{V}}a r_{P}[f(\pmb X)]/M$ . The ratio between these two variances is: 

$$
\frac{1}{1+{V}a r_{Q}[w(\pmb{x})]}.
$$

Thus, we would expect $M$ weighted samples generated by importance sampling to be “equivalent” to $M/(1+{V}a r_{Q}[w({\pmb x})])$ samples generated by IID sampling from $P$ . 

>  (12.14) 还可以用于评估规范化重要性采样生成的一组样本的质量
>  假设我们需要使用标准采样方法估计 $E_P[f]$，我们从 $P(\pmb X)$ 生成 $M$ 个独立同分布样本，用其均值估计期望，因此该估计器的方差就是 $Var_P[f(\pmb X)]/M$
>  该估计器的方差和规范化重要性采样估计器的方差的比值就是 $1/(1 + Var_Q[w(\pmb x)])$ (规范化重要性采样估计器乘上 $1/(1 + Var_Q[w(\pmb x)])$ 就等于从原分布 $P$ 进行采样得到的估计器的方差)
>  因此，我们期望重要性采样生成的 $M$ 个加权样本“等价于”从 $P$ 中生成的 $M/(1 + Var_Q[w(\pmb x)])$ 个独立同分布样本 (这样两个估计器的方差就相等)

We can use this observation to deﬁne a rule of thumb for the effective sample size of a particular set $\mathcal{D}$ of $M$ samples resulting from a particular run of importance sampling: 

$$
\begin{align}
M_{\text{eff}} &= \frac {M}{1 + Var[\mathcal D]}\tag{12.15}\\
Var[\mathcal D] &= \frac 1 M \sum_{m=1}^M w(\pmb x[m])^2 - \left(\frac 1 M\sum_{m=1}^M w(\pmb x[m])\right)^2
\end{align}
$$

This estimate can tell us whether we should continue generating additional samples. 

>  基于该观察，我们可以为重要性采样得到的包含 $M$ 个样本的集合 $\mathcal D$ 定义其有效样本数量如上
>  有效样本数量帮助我们决策是否应该继续生成额外的样本

### 12.2.3 Importance Sampling for Bayesian Networks 
With this theoretical foundation, we can now describe the application of importance sampling to Bayesian networks. We begin by providing the proposal distribution most commonly used for Bayesian networks. This distribution $Q$ uses the network structure and its CPDs to focus the sampling process on a particular part of the joint distribution — the one consistent with a particular event $Z=z$ . We show several ways in which this construction can be applied to the Bayesian network inference task, dealing with various types of probability queries. Finally, we brieﬂy discuss several other proposal distributions, which are somewhat more complicated to implement but may perform better in practice. 
>  本小节开始考虑将重要性采样应用到 BN 中
>  我们首先介绍最常用于 BN 的提案分布 $Q$，它利用 BN 的网络结构，聚焦于在联合分布中和特定事件 $\pmb Z = \pmb z$ 一致的部分进行采样

#### 12.2.3.1 The Mutilated Network Proposal Distribution 
Assume that we are interested in a particular event $Z=z$ , either because we wish to estimate its probability, or because we have observed it as evidence. We wish to focus our sampling process on the parts of the joint that are consistent with this event. In this section, we deﬁne an importance sampling process that achieves this goal. 
>  假定我们对特定事件 $\pmb Z = \pmb z$ 感兴趣，我们希望将采样过程聚焦在联合分布中和该事件一致的部分

To gain some intuition, consider the network of ﬁgure 12.1 and assume that we are interested in a particular event concerning a student’s grade: $G\,=\,g^{2}$ . We wish to bias our sampling toward parts of the space where this event holds. It is easy to take this event into consideration when sampling $L$ : we imply sample $L$ m $P(L\mid g^{2})$ . However, it is considerably more difficult to account for G ’s inﬂuence on $D,\,I$ , and S without doing inference in the network. 
>  考虑 figure 12.1 的例子，假设我们对事件 $G = g^2$ 感兴趣，故希望将我们的采样向该事件成立的空间中偏置
>  在采样 $L$ 时，要偏置到该事件很简单，我们直接从 $P(L\mid g^2)$ 中进行采样，因为 $L$ 的 CPD 是由 $G$ 的值决定的
>  但 $G$ 的值对网络中的其他变量 $D, I, S$ 的影响如果不执行推理则不明确

Our goal is to deﬁne a simple proposal distribution that allows for the efficient generation of particles. We therefore avoid the problem of accounting for the effect of the event on nondescendants; we deﬁne a proposal distribution that “sets” the value of a $Z\in \pmb Z$ to take the prespeciﬁed value in a way that inﬂuences the sampling process for its descendants, but not for the other nodes in the network. The proposal distribution is most easily described in terms of a Bayesian network: 
>  我们的目标是定义一个简单的可以高效采样我们感兴趣的粒子的提案分布，因此我们回避考虑该事件对于它的非后继节点的影响
>  我们定义提案分布的方式是通过将变量 $Z\in \pmb Z$ 的值设定为观察值来影响关于它的后继节点的采样过程，这不会影响网络中的其他节点
>  这样定义的提案分布可以描述为一个贝叶斯网络

**Deﬁnition 12.1** mutilated network 
Let $\mathcal{B}$ be a network, and ${Z}_{1}=z_{1},.\,.\,.\,,{Z}_{k}=z_{k}$ , abbreviated $Z=z$ , an instantiation of variables. We deﬁne the mutilated network $\mathcal{B}_{Z=z}$ as follows: 

- each node $Z_{i}\in Z$ has no parents in $\mathcal{B}_{Z=z}$  $Z_{i}$ in $\mathcal{B}_{Z=z}$ gives probability 1 to $Z_{i}=z_{i}$ and probability 0 to all other values $z_{i}^{\prime}\in V a l(Z_{i})$ . 
- The parents and CPDs of all other nodes $X\not\in Z$ are unchanged. 

>  定义
>  $\mathcal B$ 为贝叶斯网络，我们有一组变量实例 $\pmb Z = \pmb z$ ($Z_1 = z_1, \dots, Z_k = z_k$)，定义残缺化的网络 $\mathcal B_{\pmb Z = \pmb z}$ 如下：
>  - $\pmb Z$ 中的每个节点 $Z_i \in \pmb Z$ 在 $\mathcal B_{\pmb Z = \pmb z}$ 中没有父节点，并且 $Z_i$ 在 $\mathcal B_{\pmb Z = \pmb z}$ 中的 CPD 定义为在 $Z_i = z_i$ 时概率为 1，对于所有其他值 $z_i' \in Val(Z_i)$ 的概率都为 0
>  - 对于所有其他节点 $X\not \in\pmb Z$ ，其父节点和 CPD 保持不变

For example, the network $\mathcal{B}_{I=i^{1},G=g^{2}}^{s t u d e n t}$ is shown in ﬁgure 12.2. As we can see, the node $G$ is decoupled from its parents, eliminating its dependence on them (the node $I$ has no parents in the original network, so its parent set remains empty). Furthermore, both $I$ and $G$ have CPDs that are deterministic, ascribing probability 1 to their (respective) observed values. 
>  例如 figure 12.2 中，节点 $G$ 从其父节点脱离，消除了 $G$ 对它们的依赖，同时 $I, G$ 的 CPD 都是确定性分布，得到其观察值的概率为 1

Importance sampling with this proposal distribution is precisely equivalent to the LW algorithm shown in algorithm 12.2, with $\tilde{P(\mathcal{X})}=P_{\mathcal{B}}(\mathcal{X},z)$ and the proposal distribution $Q$ induced B by the mutilated network $\mathcal{B}_{Z=z}$ . More formally, we can show the following proposition: 
>  使用该提案分布进行重要性采样完全等价于 algorithm 12.2 的似然加权算法

**Proposition 12.2** 
Let $\xi$ be a sample generated by algorithm 12.2 and let $w$ be its weight. The the distribution of $\xi$ is as defined by the network $\mathcal B_{\pmb Z = \pmb z}$, and

$$
w(\xi)=\frac{P_{\mathcal{B}}(\xi)}{P_{\mathcal{B}_{\pmb Z=\pmb z}}(\xi)}.
$$ 
>  命题
>  algorithm 12.2 生成的样本 $\xi$ 服从由网络 $\mathcal B_{\pmb Z = \pmb z}$ 定义的分布，并且其权重 $w$ 等于 $P_{\mathcal B}(\xi)/P_{\mathcal B_{\pmb Z = \pmb z}}(\xi)$

The proof is not diffcult and is left as an exercise (exercise 12.4). It is important to note, however, that the algorithm does not require the explicit construction of the mutilated network. It simply traverses the original network, using the process shown in algorithm 12.2. 
>  因此该算法并不需要显式构建提案分布 $P_{\mathcal B_{\pmb Z = \pmb z}}$ 和残缺化的网络，只需要遍历原来的 BN，按照 algorithm 12.2 的过程采样即可

As we now show, this proposal distribution can be used for estimating a variety of Bayesian network queries. 

#### 12.2.3.2 Unconditional Probability of an Event\*
We begin by considering the simple problem of computing the unconditional probability of an event $Z=z$ . Although we can clearly use forward sampling for estimating this probability, we can also use unnormalized importance sampling, where the target distribution $P$ is simply our prior distribution $P_{\mathcal{B}}(\mathcal{X})$ , and the proposal distribution $Q$ is the one deﬁned by the mutilated network $\mathcal{B}_{Z=z}$ . Our al is to estimate the expectation of a function $f$ , which is the indicator function of the query z : $f(\xi)=I\!\!\left\{\xi\langle Z\rangle=z\right\}$ . 
>  考虑计算某个事件 $\pmb Z = \pmb z$ 的无条件概率
>  我们可以用前向采样估计该概率，也可以使用未规范化的重要性采样，其中目标分布 $P$ 为先验分布 $P_{\mathcal B}(\mathcal X)$，提案分布 $Q$ 为残缺化网络 $\mathcal B_{\pmb Z = \pmb z}$ 定义的分布，我们的目标是估计 $P(\pmb Z = \pmb z)$，等价于估计指示函数 $f(\xi) = \mathbf 1\{\xi\langle \pmb Z \rangle = \pmb z\}$ 的期望值

The unnormalized importance-sampling estimator for this case is simply: 

$$
\begin{array}{r c l}{{\hat{P}_{\mathcal D}(\pmb z)}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m=1}^{M}\mathbf 1\{\xi[m]\langle \pmb Z\rangle=\pmb z\}w(\xi[m])}}\\ {{}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m=1}^{M}w[m],}}\tag{12.16}\end{array}
$$ 
where the equality follows because, by deﬁnition of $Q$ , our sampling process generates samples $\xi[m]$ only where $\pmb z$ holds. 

>  我们的未规范化重要性采样估计器就是 (12.16)
>  注意我们将提案分布 $Q$ 定义为了 $\mathcal B_{\pmb Z = \pmb z}$ 定义的分布，因此 $Q$ 中生成的样本 $\xi[i]$ 总是满足 $\pmb Z = \pmb z$，因此指示函数总是成立，故估计器就是所有样本权重的平均值

When trying to bound the relative error of an estimator, a key quantity is the variance of the estimator relative to its mean . In the Chernof bound, when we are estimating the probability $p$ of a very low-probability event, the variance of the estimator, which is $p(1-p)$ , is very high relative to the mean $p$ . Importance sampling removes some of the variance associated with this sampling process, and it can therefore achieve better performance in certain cases. 

In this case, the samples are derived from our proposal distribution $Q$ , and the value of the function whose expectation we are computing is simply the weight. Thus, we need to bound the variance of the function $w(\mathcal{X})$ under our distribution $Q$ . Let us consider the sampling process in the algorithm. As we go through the variables in the network, we encounter the observed variables $Z_{1},.\,.\,.\,,Z_{k}$ . At each point, we multiply our current weight $w$ by some conditional probability number $P_{\mathcal{B}}(Z_{i}=z_{i}\mid\mathrm{Pa}_{Z_{i}})$ . 

One situation where we can bound the variance arises in a restricted class of networks, one where the entries in the CPD of the variables $Z_{i}$ are bounded away from the extremes of 0 and 1 . More precisely, we assume that there is some pair of numbers $\ell>0$ and $u<1$ such that: for each variable $Z\in Z$ , $z\,\in\,V a l(Z)$ , and $u\in{\it V a l}(\mathrm{Pa}_{Z})$ , we have that $P_{\mathcal{B}}(Z=z\mid\mathrm{Pa}_{Z}=$ $\pmb{u})\in[\ell,u]$ . Next, we assume that $|Z|=k$ for some small k . This assumption is not a trivial one; while queries often involve only a small number of variables, we often have a fairly large number of observations that we wish to incorporate. 

Under these assumptions, the weight $w$ generated through the LW process is necessarily in the interval $\ell^{k}$ and $u^{k}$ . We can now redeﬁne our weights by dividing each $w[m]$ by $u^{k}$ : 

$$
w^{\prime}[m]=w[m]/u^{k}.
$$ 
Each weight $w^{\prime}[m]$ is now a real-valued random variable in the range $[(\ell/u)^{k},1]$ . For a data set $\mathcal{D}$ of weights $w[1],\ldots,w[M]$ , we can now deﬁne: 

$$
\hat{p}_{\mathcal{D}}^{\prime}=\frac{1}{M}\sum_{m=1}^{M}w^{\prime}[m].
$$ 
The key point is that the mean of this random variable, which is $P_{\mathcal{B}}(z)/u^{k}$ , is therefore also in the range $[(\ell/u)^{k},1]$ , and its variance is, at worst, the variance of a Bernoulli random variable with the same mean. Thus, we now have a random variable whose variance is not that small relative to its mean. 

A simple generalization of Chernof’s bound (theorem A.4) to the case of real-valued variables can now be used to show that: 

$$
\begin{array}{r c l}{{P_{\mathcal{D}}(\hat{P}_{\mathcal{D}}(z)\not\in P_{\mathcal{B}}(z)(1\pm\epsilon))}}&{{=}}&{{P_{\mathcal{D}}(\hat{p}_{\mathcal{D}}^{\prime}\not\in\displaystyle\frac{1}{u^{k}}P_{\mathcal{B}}(z)(1\pm\epsilon))}}\\ {{}}&{{\le}}&{{2e^{-M\frac{1}{u^{k}}P_{\mathcal{B}}(z)\epsilon^{2}/3}.}}\end{array}
$$ 
sample size We can use this equation, as in the case of Bernoulli random variables, to derive a sufcient condition for the sample size that can guarantee that the estimator $\hat{P}_{\mathcal{D}}(z)$ of equation (12.16) D has error at most $\epsilon$ with probability at least $1-\delta$ : 

$$
M\geq\frac{3\ln(2/\delta)u^{k}}{P_{\mathcal{B}}(z)\epsilon^{2}}.
$$ 
Since $\mathcal{P}_{\mathcal{B}}(z)\geq\ell^{k}$ , a (stronger) sufcient condition is that: 

$$
M\geq\frac{3\ln(2/\delta)}{\epsilon^{2}}\left(\frac{u}{\ell}\right)^{k}.
$$ 

Chernof bound 

It is instructive to compare this bound to the one we obtain from the Chernof bound in equation (12.5). The bound in equation (12.18) makes a weaker assumption about the probability of the event $_z$ . Equation (12.5) requires that $P_{\mathcal{B}}(z)$ not be too low. By contrast, equation (12.17) assumes only that this probability is in a bounded range $\ell^{k},u^{k}$ ; the actual probability of the event $_z$ can still be very low — we have no guarantee on the actual magnitude of $\ell$ . Thus, for example, if our event $_z$ corresponds to a rare medical condition — one that has low probability given any instantiation of its parents — the estimator of equation (12.16) would give us a relative error bound, whereas standard sampling would not. 

We can use this bound to determine in advance the number of samples required for a certain desired accuracy. A disadvantage of this approach is that it does not take into consideration the speciﬁc samples we happened to generate during our sampling process. Intuitively, not all samples contribute equally to the quality of the estimate. A sample whose weight is high is more compatible with the evidence $e$ , and it arguably provides us with more information. Conversely, a low-weight sample is not as informative, and a data set that contains a large number of low-weight samples might not be representative and might lead to a poor estimate. A somewhat more sophisticated approach is to preselect not the number of particles, but a predeﬁned total weight. We then stop sampling when the total weight of the generated particles reaches our predeﬁned lower bound. 


Theorem 12.1 

expected sample size 

Theorem 12.2 

For this algorithm, we can provide a similar theoretical analysis with certain guarantees for this data-dependent likelihood weighting approach. Algorithm 12.3 shows an algorithm that uses a data-dependent stopping rule to terminate the sampling process when enough weight has been accumulated. We can show that: 

Data-Dependent-LW returns an estimate $\hat{p}$ for $P_{\mathcal{B}}(Z=z)$ which, with probability at least $1-\delta$ , has a relative error of ϵ . 

We can also place an upper bound on the expected sample size used by the algorithm: The expected number of samples used by Data-Dependent-LW is 

$\frac{u^{k}}{P_{\mathcal{B}}(z)}\gamma\leq\left(\frac{u}{\ell}\right)^{k}\gamma,$ where $\begin{array}{r}{\gamma=\frac{4(1+\epsilon)}{\epsilon^{2}}\ln\frac{2}{\delta}}\end{array}$ . 

The intuition behind this result is straightforward. The algorithm terminates when $W\geq\gamma u^{k}$ . The expected contribution of each sample is $E_{Q(\mathcal{X})}[w(\xi)]\,=\,P_{\mathcal{B}}(z)$ . Thus, the total number X B of samples required to achieve a total weight of $W\geq\gamma u^{k}$ is $M\geq\gamma u^{k}/P_{\mathcal{B}}(z)$ . Although this bound on the expected number of samples is no better than our bound in equation (12.17), the data-dependent bound allows us to stop early in cases where we were lucky in our random choice of samples, and to continue sampling in cases where we were unlucky. 

#### 12.2.3.3 Ratio Likelihood Weighting 
We now move to the problem of computing a conditional probability $P(\pmb{y}\mid\pmb{e})$ for a speciﬁc event $y$ . One obvious approach is ratio likelihood weighting : we compute the conditional probability as $P(\pmb{y},e)/P(e)$ , and use unnormalized importance sampling (equation (12.16)) for both the numerator and denominator. 

We can therefore estimat nditional probability $P(\pmb{y}\mid\pmb{e})$ in two phases: We use the algorithm of algorithm 12.2 M times with the argument $Y~=~y,E~=~e$ , to generate on et $\mathcal{D}$ of weighted samples $(\xi[1],w[1]),.\,.\,.\,,(\xi[M],w[M])$ . We use the same algorithm $M^{\prime}$ times with the argument $E=e$ , to generate another set ${\mathcal{D}}^{\prime}$ of weighted samples $(\xi^{\prime}[1],w^{\prime}[1]),.\,.\,.\,,(\xi^{\prime}[M^{\prime}],w^{\prime}[M^{\prime}])$ . We can then estimate: 

$$
\hat{P}_{\mathcal{D}}(\pmb{y}\mid \pmb e)=\frac{\hat{P}_{\mathcal{D}}(\pmb{y},\pmb e)}{\hat{P}_{\mathcal{D}^{\prime}}(\pmb e)}=\frac{1/M\sum_{m=1}^{M}w[m]}{1/M^{\prime}\sum_{m=1}^{M^{\prime}}w^{\prime}[m]}.\tag{12.19}
$$ 
In ratio LW, the numerator and denominator are both using unnormalized importance sampling, which admits a rigorous theoretical analysis. Thus, we can now provide bounds on the number of samples $M$ required to obtain a good estimate for both $P (\pmb{y},\pmb e)$ and $P (\pmb e)$ . 

>  考虑为特定的事件 $\pmb y$ 计算条件概率 $P (\pmb y \mid \pmb e)$，
>  一个直接的方法是比率似然加权，即我们用 $P (\pmb y, \pmb e)/ P (\pmb e)$ 计算条件概率，其中分母和分子分别使用未规范化的重要性采样 (等价于似然加权算法) 进行估计
>  具体地说，估计分子 $P (\pmb y, \pmb e)$ 时，我们将 $\pmb Y = \pmb y, \pmb E = \pmb e$ 一起作为 evidence 应用 algorithm 12.2 对 $P (\pmb y, \pmb e)$ 进行采样估计；估计分母 $P (\pmb e)$ 时，我们将 $\pmb e$ 作为 evidence 应用 algorithm 12.2 进行另一次采样估计，如 (12.19) 所示
>  在比率似然加权中，分母和分子都使用未规范化的重要性采样，因为未规范化的重要性采样有严格的理论分析界，故我们可以为估计好 $P (\pmb y, \pmb e), P (\pmb e)$ 给出关于样本数量 $M$ 的界

#### 12.2.3.4 Normalized Likelihood Weighting 
Ratio LW allows us to estimate the probability of a single query $P (\pmb{y}\mid\pmb{e})$ . In many cases, however, we are inter ted in estimating an entire joint distribution $P (\pmb Y\mid \pmb e)$ for some variable or subset of variables Y . We can answer such a query by running ratio LW for each $\pmb{y}\in V a l (\pmb{Y})$ , but this approach is typically too computationally expensive to be practical. 
>  比率似然加权可以用于估计单个查询 $P (\pmb y \mid \pmb e)$ 的概率
>  如果我们感兴趣整个后验分布 $P (\pmb Y \mid \pmb e)$，则为每个 $\pmb y \in Val (\pmb Y)$ 执行一次采样估计显然不现实

An alternative approach is to use normalized likelihood weighting , which is based on the normalized importance sampling estimator of equation (12.13). In this application, our target distribution is $P (\mathcal{X})=\operatorname{P_{\mathcal{B}}}(\mathcal{X}\mid e)$ . As we mentioned, we do not have access to $P$ directly; rather, we can evaluate $\tilde{P}(\mathcal{X})=P_{\mathcal{B}}(\mathcal{X}, e)$ , which is the probability of a full assignment and can be easily computed via the chain rule. In this case, we are trying to estimate the expectation of a function $f$ which is the indicator function of the query ${y}$ : $f (\xi)=I\!\!\left\{\xi\langle Y\rangle=y\right\}$ . Applying the normalized importance sampling estimator of equation (12.13) to this setting, we obtain precisely the estimator of equation (12.6). 
>  考虑采用规范化似然加权，比率似然加权基于未规范化的重要性采样，规范化似然加权则基于规范化的重要性采样
>  我们的目标分布是 $P_{\mathcal B}(\mathcal X \mid \pmb e)$，记为 $P (\mathcal X)$
>  我们难以直接访问目标分布 $P (\mathcal X)$，但可以访问其未规范化的分布 $\tilde P (\mathcal X) = P_{\mathcal B}(\mathcal X , \pmb e)$ (因为 $\mathcal X, \pmb e$ 是对 $\mathcal B$ 中全部变量的完整赋值，故其概率可以利用 $P_{\mathcal B}$ 的分解进行计算)
>  我们应用 (12.13) 的规范化采样估计器，估计函数 $f (\xi) = \mathbf 1 \{\xi\langle \pmb Y \rangle = \pmb y\}$ 相对于 $P_{\mathcal B}(\mathcal X\mid \pmb e)$ 的期望

>  该方法和上一种方法的差别就在于该方法的分子分母是用同一轮采样得到的同一组样本一起估计的，而不是分别为分母分子进行一轮采样

**The quality of the importance sampling estimator depends largely on how close the proposal distribution $Q$ is to the target distribution $P$ . We can gain intuition for this question by considering two extreme cases. If all of the evidence in our network is at the roots, the proposal distribution is precisely the posterior, and there is no need to compensate; indeed, no evidence is encountered along the way, and all samples will have the same weight $P (e)$ . On the other side of the spectrum, if all of the evidence is at the leaves, our proposal distribution $Q (\mathcal X)$ is the prior distribution $P_{\mathcal{B}}(\mathcal{X})$ , leaving the correction purely to the weights. In this situation, LW will work reasonably only if the prior is similar to the posterior. Otherwise, most of our samples will be irrelevant, a fact that will be reﬂected by their low weight.** For example, consider a medical-diagnosis setting, and assume that our evidence is a very unusual combination of symptoms generated by only one very rare disease. Most samples will not involve this disease and will give only very low probability to this combination of symptoms. Indeed, the combinations sampled are likely to be irrelevant and are not useful at all for understanding what disease the patient has. We return to this issue in section 12.2.4. 
>  重要性采样估计器的质量很大程度上取决于提案分布 $Q$ 与目标分布 $P$ 的接近程度。
>  我们可以通过考虑两种极端情况来获得对此问题的直观理解。第一个极端是如果网络中的所有证据都在根节点处，那么提案分布就是后验分布，不需要补偿；实际上，在采样过程中不会遇到任何证据，所有样本将具有相同的权重 $P (\pmb e)$。
>  另一极端是，如果所有证据都在叶节点处，我们的提案分布 $Q (\mathcal{X})$ 就是先验分布 $P_{\mathcal{B}}(\mathcal{X})$，此时完全依赖权重来进行修正。在这种情况下，如果先验分布与后验分布相似，LW 算法才能合理工作。否则，大多数样本将是不相关的，这一事实会通过它们的低权重反映出来。
>  例如，在医学诊断场景中，假设我们的证据是一组非常罕见的疾病导致的异常症状组合。大多数样本不会涉及这种疾病，并且只会给出非常低的症状组合概率。实际上，所采样的组合可能是不相关的，对理解患者患有什么疾病毫无帮助。我们在第 12.2.4 节中再回到这个问题。

To understand the relationship between the prior and the posterior, note that the prior is a weighted average of the posteriors, weighted over diferent instantiations of the evidence: 
>  为了理解先验分布和后验分布之间的关系，需要注意先验分布是通过对不同证据实例化的加权平均得到的：

$$
P (\mathcal{X})=\sum_{\pmb e}P (\pmb e) P (\mathcal{X}\mid \pmb e).
$$ 
If the evidence is very likely, then it is a major component in this summation, and it is probably not too far from the prior. For example, in the network $\mathcal{B}^{s t u d e n t}$ , the event $S=s^{1}$ is fairly likely, and the posterior distribution $P_{\mathcal{B}^{s t u d e n t}}(\mathcal{X}\mid s^{1})$ is fairly similar to the prior. However, for unlikely evidence, the weight of $P (\mathcal{X}\mid e)$ is negligible, and there is nothing constraining the posterior to be similar to the prior. Indeed, our distribution $P_{\mathcal{B}^{s t u d e n t}}(\mathcal{X}\mid l^{0})$ is very diferent from the prior. 
>  如果证据非常有可能出现，则它是此求和的主要组成部分，很可能与先验分布相差不大。
>  例如，在网络 $\mathcal{B}^{s t u d e n t}$ 中，事件 $S=s^{1}$ 发生的可能性相对较高，后验分布 $P_{\mathcal{B}^{s t u d e n t}}(\mathcal{X}\mid s^{1})$ 与先验分布相当接近。
>  然而，对于不太可能的证据，$P (\mathcal{X}\mid \pmb e)$ 的权重可以忽略不计，因此没有什么可以约束后验分布与先验分布相似。事实上，我们的分布 $P_{\mathcal{B}^{s t u d e n t}}(\mathcal{X}\mid l^{0})$ 与先验分布有很大的不同。

Unfortunately, there is currently no formal analysis for the number of particles required to achieve a certain quality of estimate using normalized importance sampling. In many cases, we simply preselect a number of particles that seems large enough, and we generate that number. Alternatively, we can use a heuristic approach that uses the total weight of the particles generated so far as guidance as to the extent to which they are representative. Thus, for example, we might decide to generate samples until a certain minimum bound on the total weight has been reached, as in Data-Dependent-LW . We note, however, that this approach is entirely heuristic in this case (as in all cases where we do not have bounds $[\ell, u]$ on our CPDs). Furthermore, there are cases where the evidence is simply unlikely in all conﬁgurations, and therefore all samples will have low weights. 
>  不幸的是，目前还没有关于使用归一化重要性采样达到一定质量估计所需的粒子数量的正式分析。
>  在许多情况下，我们通常预先选择一个看起来足够大的粒子数量，然后生成这么多粒子。
>  或者，我们可以使用一种启发式方法，根据迄今为止生成的所有粒子的总权重来确定它们代表性的程度。例如，我们可以决定一直生成样本直到总权重达到某个最小值，就像在 Data-Dependent-LW 方法中那样。
>  不过，需要注意的是，在这种情况下（以及在我们没有先验和后验的上下界 $[\ell, u]$ 的所有情况下），这种方法完全是启发式的。此外，有些情况下证据在所有配置中都是不可能的，因此所有样本都会具有较低的权重。

#### 12.2.3.5 Conditional Probabilities: Comparison 
We have seen two variants of likelihood weighting: normalized LW and ratio LW. Ratio LW has two related advantages. The normalized LW process samples an assignment of the variables $Y$ (those not in $E$ ), whereas ratio LW simply sets the values of these variables. The additional sampling step for $Y$ introduces additional variance into the overall process, leading to a reduction in the robustness of the estimate. Thus, in many cases, the variance of this estimator is lower than that of equation (12.6), leading to more robust estimates. 
>  我们已经看到了两种似然加权算法的变体：归一化似然加权 (normalized LW) 和比率似然加权 (ratio LW)。
>  比率似然加权有两个相关的优势。归一化似然加权过程对变量 $\pmb Y$  (那些不在 $\pmb E$ 中的变量) 进行采样，而比率似然加权则直接设置这些变量的值。
>  对 $\pmb Y$ 的额外采样步骤引入了额外的方差，从而降低了估计的鲁棒性。因此，在许多情况下，这个估计器的方差低于方程 (12.6) 中的方差，从而产生更稳健的估计。
>  (比率似然加权会采样两次，方差更低)

A second advantage of ratio LW is that it is much easier to analyze, and therefore it is associated with stronger guarantees regarding the number of samples required to get a good estimate. However, these bounds are useful only under very strong conditions: a small number of evidence variables, and a bound on the skew of the CPD entries in the network. 
>  比率似然加权的第二个优势在于它更容易分析，并因此与获取良好估计所需的样本数量有更强的保证。
>  然而，这些界限只有在非常强的条件下才有用：证据变量的数量较少，并且网络中条件概率分布 (CPD) 条目的偏斜有边界限制。

On the other hand, a signiﬁcant disadvantage of ratio LW is the fact that each query $_{_y}$ requires that we generate a new set of samples for the event $\pmb y, \pmb e$ . It is often the case that we want to evaluate the probability of multiple queries relative to the same set of evidence. The normalized LW approach allows these multiple computations to be executed relative to the same set of samples, whereas ratio LW requires a separate sample set for each query $_{_y}$ . This cost is particularly problematic when we are interested in computing the joint distribution over a subset of variables. Probably due to this last point, normalized LW is used more often in practice. 
>  另一方面，比率似然加权的一个显著缺点是，每个查询 $\pmb y$ 都需要我们为事件 $\pmb y, \pmb e$ 生成一组新的样本。
>  通常情况下，我们希望评估相对于同一组证据的多个查询的概率。归一化似然加权方法允许这些多次计算相对于同一组样本执行，而比率似然加权则需要为每个查询 $\pmb y$ 分别生成样本集。当我们要计算一组变量的联合分布时，这种成本尤其成问题。
>  可能由于这个最后一点，归一化似然加权在实践中使用得更为频繁。

### 12.2.4 Importance Sampling Revisited 
The likelihood weighting algorithm uses, as its proposal distribution, the very simple distribution obtained from mutilating the network by eliminating edges incoming to observed variables. However, this proposal distribution can be far from optimal. For example, if the CPDs associated with these evidence variables are skewed, the importance weights are likely to be quite large, resulting in estimators with high variance. Indeed, somewhat surprisingly, even in very simple cases, the obvious proposal distribution may not be optimal. For example, if $X$ is not a root node in the network, the optimal proposal distribution for computing $P (X=x)$ may not be the distribution $P$ , even without evidence! (See exercise 12.5.) 
>  似然加权算法使用了一种非常简单的提案分布，即通过消除观测变量的入射边来破坏网络原有的分布。然而，这种提案分布可能远非最优。
>  例如，如果这些证据变量的条件概率分布 (CPDs) 是偏斜的，则重要性权重可能会相当大，导致估计器的方差过高。事实上，即使在非常简单的情况下，显而易见的提案分布也可能不是最优的。例如，如果 $X$ 不是网络中的根节点，在没有证据的情况下，计算 $P (X=x)$ 的最优提案分布可能并不是分布 $P$ (参见习题 12.5)

The importance sampling framework is very general, however, and several other proposal distributions have been utilized. For example, backward importance sampling generates samples for parents of evidence variables using the likelihood of their children. Most simply, if $X$ is a variable whose child $Y$ is observed to be $Y=y$ , we might generate some samples for $X$ from renormalized distribution $Q (X)\propto P (Y=y\mid X)$ . We can continue this process, sampling $X$ ’s parents from the likelihood of $X$ ’s sampled value. We can also propose more complex schemes that sample the value of a variable given a combination of sampled or observed values for some of its parents and/or children. One can also consider hybrid approaches that use some global approximate inference algorithm (such as those in chapter 11) to construct a proposal distribution, which is then used as the basis for sampling. **As long as the importance weights are computed correctly, we are guaranteed that this process is correct.** (See exercise 12.7.) This process can lead to signiﬁcant improvements in theory, and it does lead to improvements in some cases in practice. 
>  然而，重要性采样框架是非常通用的，目前也存在许多常利用的提案分布
>  例如，逆向重要性采样使用证据变量的似然性来生成其父节点的样本。最简单地，如果 $X$ 是一个变量，其后代 $Y$ 被观察到为 $Y=y$，我们可以从重新归一化的分布 $Q (X) \propto P (Y=y \mid X)$ 中生成一些 $X$ 的样本。
>  我们可以继续这一过程，从 $X$ 的采样值的概率中采样其父节点。我们还可以提出更复杂的方案，根据其某些父节点和/或后代的采样值或观察值的组合来采样变量的值。
>  人们也可以考虑混合方法，使用某些全局近似推理算法（如第 11 章中的算法）来构建提案分布，然后基于此分布进行采样。只要正确计算了重要性权重，我们就能保证这一过程是正确的。（参见习题 12.7）
>  这一过程理论上可以带来显著改进，并且在实际中确实改善了一些情况下的结果。

## 12.3 Markov Chain Monte Carlo Methods 
One of the limitations of likelihood weighting is that an evidence node affects the sampling only for nodes that are its descendants. The effect on nodes that are nondescendants is accounted for only by the weights. As we discussed, in cases where much of the evidence is at the leaves of the network, we are essentially sampling from the prior distribution, which is often very far from the desired posterior. 
>  似然加权算法的一个限制在于采样过程中证据节点仅会影响其后代节点的采样，对于非后代节点的影响仅根据权重表示 (非后代节点的 CPD 不会受证据影响，因此其采样不受影响，但证据节点会根据其采样值影响粒子的权重，粒子的权重相应放缩了粒子的重要性，同时放缩了非后代节点的采样值，这个过程体现了证据节点对非后代节点的采样的影响)
>  当证据节点都是网络的叶子节点时，我们本质上是在先验中采样 (叶子节点没有后代，故不影响任何节点的 CPD，在对应的残缺化网络前向采样等价于在原网络中前向采样，证据无法通过影响 CPD 的方式引入后验信息)，而该先验往往和后验相距甚远 (因此在先验中采样和在后验中采样的结果往往是差异很大的，我们不能保证重要性权重可以补足这一差异)

We now present an alternative sampling approach that generates a sequence of samples. This sequence is constructed so that, although the ﬁrst sample may be generated from the prior, successive samples are generated from distributions that provably get closer and closer to the desired posterior. We note that, unlike forward sampling methods (including likelihood weighting), Markov chain methods apply equally well to directed and to undirected models. Indeed, the algorithm is easier to present in the context of a distribution $P_{\Phi}$ deﬁned in terms of a general set of factors $\Phi$ . 
>  本节展示另一类采样方法 (Markov 链方法)，它生成一个样本序列
>  该样本序列中，第一个样本生成自先验，而后续的样本则生成自越来越接近后验的分布
>  注意和前向采样方法 (包括似然加权方法) 不同，Markov 链方法同时适用于有向和无向模型，并且其算法更容易使用由因子集合 $\Phi$ 定义的 Gibbs 分布 $P_\Phi$ 表示

### 12.3.1 Gibbs Sampling Algorithm 
One idea for addressing the problem with forward sampling approaches is to try to “ﬁx” the sample we generated by resampling some of the variables we generated early in the process. Perhaps the simplest method for doing this is presented in algorithm 12.4. This method, called Gibbs sampling , starts out by generating a sample of the unobserved variables from some initial distribution; for example, we may use the mutilated network to generate a sample using forward sampling. Starting from that sample, we then iterate over each of the unobserved variables, sampling a new value for each variable given our current sample for all other variables. This process allows information to “ﬂow” across the network as we sample each variable. 
>  解决前向采样方法的问题的一个思路是对采样过程中早期采样的几个变量进行重采样，来“修复”样本
>  该思路对应的一个简单方法就是 Gibbs 采样，如 algorithm 12.4 所示
>  Gibbs 采样开始时从某个初始分布生成一个关于未被观察到的变量的样本，例如，我们可以用残缺化网络定义的分布作为初始分布，使用前向采样在其中生成一个样本
>  从该样本开始，我们在未被观察到的变量上迭代，在给定样本中其他所有变量采样值的情况下，为每个变量采样一个新值
>  随着我们对每个变量进行采样，信息相应地在网络中流动

![[pics/PGM-Algorithm12.4.png]]

To apply this algorithm to a network with evidence, we ﬁrst reduce all of the factors by the observations $\pmb e$ , so that the distribution $P_{\Phi}$ used in the algorithm corresponds to $P (\pmb X\mid \pmb e)$ . 
>  将该算法推广到带有证据的网络中，我们只需要首先将所有的因子根据观测 $\pmb e$ 简化即可，此时算法中使用的分布 $P_\Phi$ 实际上就是 $P (\pmb X\mid \pmb e)$

Example 12.4
Let us revisit example 12.3, recalling that we have the observations s1; l0. In this case, our algorithm will generate samples over the variables D; I; G. The set of reduced factors Φ is therefore: P (I); P (D); P (G j I; D); P (s1 j I); P (l0 j G). Our algorithm begins by generating one sample, say by forward sampling. Assume that this sample is d (0) = d1; i (0) = i0; g (0) = g2. In the first iteration, it would now resample all of the unobserved variables, one at a time, in some predetermined order, say G; I; D. Thus, we first sample g (1) from the distribution PΦ(G j d1; i0)

Note that because we are computing the distribution over a single variable given all the others, this computation can be performed very efficiently: 

$$
\begin{array}{l l l} {{P_{\Phi}(G\mid d^{1},i^{0})}} & {{=}} & {{\displaystyle\frac{P(i^{0})P(d^{1})P(G\mid i^{0},d^{1})P(l^{0}\mid G)P(s^{1}\mid i^{0})}{\sum_{g}P(i^{0})P(d^{1})P(g\mid i^{0},d^{1})P(l^{0}\mid g)P(s^{1}\mid i^{0})}} }\\ {{}} & {{=}} & {{\displaystyle\frac{P(G\mid i^{0},d^{1})P(l^{0}\mid G)}{\sum_{g}P(g\mid i^{0},d^{1})P(l^{0}\mid g)}.}} \end{array}
$$ 
Thus, we can compute the distribution simply by multiplying all factors that contain $G$ , with all other variables instantiated, and renormalizing to obtain a distribution over $G$ . 

Having sampled $g^{(1)}\,=\, g^{3}$ , w ntinue to resampling $i^{(1)}$ from he distribution $P_{\Phi}(I\mid$ $d^{1}, g^{3})$ , obtaining, for example, i $i^{(1)}\,=\, i^{1}$ ; note that the distribution for I is conditioned on the newly sampled value $g^{(1)}$ . Finally, we sample $d^{(1)}$ from $P_{\Phi}(D\mid g^{3}, i^{1})$ , obtaining $d^{1}$ . The result of the ﬁrst iteration of sampling is, then, the sample $(i^{1}, d^{1}, g^{3})$ . The process now repeats. 

Note that, unlike forward sampling, the sampling process for $G$ takes into consideration the downstream evidence at its child $L$ . Thus, its sampling distribution is arguably closer to the posterior. Of course, it is not the true posterior, since it still conditions on the originally sampled values for $I, D$ , which were sampled from the prior distribution. 
>  在上一个例子中，和前向采样不同的是，Gibbs 采样在对变量 $G$ 进行采样时，考虑到了它的子节点 $L$ (同时也是证据节点)，因此其采样分布可以认为更接近后验分布
>  当然这也不是 $G$ 条件于证据的真实的后验分布，因为它还条件于从先验分布中对 $I, D$ 的采样值

However, we now resample $I$ and $D$ from a distribution that conditions on the new value of $G$ , so one can imagine that their sampling distribution may also be closer to the posterior. Thus, perhaps the next sample of $G$ ,  which uses these new values for $I, D$ (and conditions on the evidence $l^{0}$ ), will be sampled from a distribution even closer to the posterior. 
>  我们采样完 $G$ 之后，又从条件于 $G$ 的新采样值的分布中重新采样 $I, D$，我们可以认为 $I, D$ 各自的采样分布也更接近于它们的后验分布
>  那么，对 $G$ 的下一次采样将条件于 $I, D$ 的新值，以及证据 $l^0$，可以认为这次的采样分布离后验分布更接近

Indeed, this intuition is correct. One can show that, as we repeat this sampling process, the distribution from which we generate each sample gets closer and closer to the posterior $P_{\Phi}(\pmb X)=P (\pmb X\mid \pmb e)$ . 
>  上述直觉是正确的，可以证明，随着我们重复这个采样过程，我们生成每个样本的分布将越来越接近后验分布 $P_\Phi (\pmb X) = P (\pmb X \mid \pmb e)$

In the subsequent sections, we formalize this intuitive argument using a framework called Markov chain Monte Carlo (MCMC) . This framework provides a general approach for generating samples from the posterior distribution, in cases where we cannot efciently sample from the posterior directly. In MCMC, we construct an iterative process that gradually samples from distributions that are closer and closer to the posterior. A key question is, of course, how many iterations we should perform before we can collect a sample as being (almost) generated from the posterior. In the following discussion, we provide the formal foundations for MCMC algorithms, and we try to address this and other important questions. We also present several valuable generalizations. 
>  本节的后续中，我们使用 MCMC 框架形式化这一直观的论点
>  当我们不能直接从后验中高效生成样本时，MCMC 框架提供了从后验分布生成样本的通用的方法
>  我们在 MCMC 中构造一个逐渐从越来越接近后验分布的分布序列中迭代采样的过程，其中一个关键问题是需要多少次迭代才可以得到一个几乎完全可以认为是从真实后验中生成的样本

### 12.3.2 Markov Chains 
#### 12.3.2.1 Basic Definition 
At a high level, a Markov chain is deﬁned in terms of a graph of states over which the sampling algorithm takes a random walk. In the case of graphical models, this graph is not the original graph, but rather a graph whose nodes are the possible assignments to our variables $\pmb X$ . 
>  Markov 链被定义为一个状态图，采样算法在该图上进行随机游走
>  从图模型的视角看，该图的节点是对变量 $\pmb X$ 的可能赋值 (每个赋值是 $\pmb X$ 的一个状态)

**Deﬁnition 12.2** Markov chain transition model 
A Markov chain is deﬁned via a state space $V a l (\pmb X)$ and a model that deﬁnes, for every state ${\pmb x}\in {{V a l}} (\pmb X)$ a next-state dis ion over $V a l (X)$ . More precisely, the transition del $\mathcal{T}$ speciﬁes for each pair of state ${\boldsymbol{x}},{\boldsymbol{x}}^{\prime}$ the probability $\tau (\pmb{x}\ \rightarrow\ \pmb{x}^{\prime})$ of going from ${x}$ to $\scriptstyle{\boldsymbol{x}}^{\prime}$ . This transition probability applies whenever the chain is in state ${x}$ . 
>  定义
>  一个状态空间 $Val (\pmb X)$ 和为其中每个状态 $\pmb x\in Val (\pmb X)$ 定义了其下一个状态在 $Val (\pmb X)$ 上的分布的模型 (转换模型) 定义了一个 Markov 链
>  具体地说，转换模型 $\mathcal T$ 为每对状态 $\pmb x, \pmb x'$ 指定了状态转移概率 $\tau (\pmb x \rightarrow \pmb x')$，表示从 $\pmb x$ 转移到 $\pmb x'$ 的概率，当 Markov 链处于状态 $\pmb x$ 时，该转移概率就适用

We note that, in this deﬁnition and in the subsequent discussion, we restrict attention to homogeneous , where the system dynamics do not change over time.  
>  我们进考虑同质的 Markov 链，即系统动态不会随时间而改变

We illustrate this concept with a simple example.

Example 12.5
Consider a Markov chain whose states consist of the nine integers -4, . . . , +4, arranged as points on a line. Assume that a drunken grasshopper starts out in position 0 on the line. At each point in time, it stays where it is with probability 0:5, or it jumps left and right with equal probability. Thus, T (i ! i) = 0:5, T (i ! i + 1) = 0:25, and T (i ! i - 1) = 0:25. However, the two end positions are blocked by walls; hence, if the grasshopper is in position +4 and tries to jump right, it remains in position $+4$ . Thus, for example, $\mathcal{T}(+4\rightarrow+4)=0.75$ . We can visualize the state space as a graph, with probability-weighted directed edges corresponding to transitions between different states. The graph for our example is shown in ﬁgure 12.3. 

We can imagine a random sampling process, that deﬁnes a random sequence of states ${\pmb x}^{(0)},{\pmb x}^{(1)},{\pmb x}^{(2)},\breve{~}.\cdot\cdot$ . Because the transition model is random, the state of the process at step $t$ can be viewed as a random variable $X^{(t)}$ . We assume that the initial state $X^ {{\bar{(0)}} }$ is distributed according to some initial state distribution $P^{(0)}(X^{(0)})$ . We can now deﬁne distributions over the subsequent states $P^{(1)}(X^{(1)}), P^{(2)}(X^{(2)}),.\,.\,.$ using the chain dynamics: 

$$
P^{(t+1)}(\pmb{X}^{(t+1)}=\pmb{x}^{\prime})=\sum_{\pmb{x}\in V a l (\pmb{X})}P^{(t)}(\pmb{X}^{(t)}=\pmb{x}){\mathcal{T}}(\pmb{x}\rightarrow\pmb{x}^{\prime}).\tag{12.20}
$$ 
>  考虑一个随机采样过程，该过程定义了一个关于状态的随机序列 $\pmb x^{(0)}, \pmb x^{(1)}, \pmb x^{(2)},\dots$
>  状态转移带有随机性，故我们可以将第 $t$ 步的状态视作一个随机变量 $\pmb X^{(t)}$
>  假设初始状态 $\pmb X^{(0)}$ 服从某个初始状态分布 $P^{(0)}(\pmb X^{(0)})$，我们进而可以用第 $t-1$ 步的状态分布 $P^{(t-1)}(\pmb X^{(t-1)})$ 和状态转移模型定义第 $t$ 步的状态分布 $P^{(t)}(\pmb X^{(t)})$，如 (12.20)

Intuitively, the probability of being at state ${\pmb x}^{\prime}$ at time $t+1$ is the sum over all possible states ${\pmb x}$ that the chain could have been in at time $t$ of the probability being in state ${\pmb x}$ times the probability that the chain took a transition from ${\pmb x}$ to $\pmb x^{\prime}$ . 

#### 12.3.2.2 Asymptotic Behavior 
For our purposes, the most important aspect of a Markov chain is its long-term behavior. 
>  我们最关心 Markov 链的长期行为，也就是随着 $t$ 增大，$X^{(t)}$ 的分布会如何变化

Example 12.6 
Because the grasshopper’s motion is random, we can consider its location at time $t$ to be a random variable, which we denote $X^{(t)}$ . Consider the distribution over $X^{(t)}$ . Initially, the grasshopper is at 0 , so that $P (X^{(0)}\,=\, 0)\,=\, 1$ . At time 1 , we have that $X^{(1)}$ is 0 with probability 0 . 5 , $+1$ $-1$ h probabil 0 . 25 . At time 2 , we have that $X^{(2)}$ is 0 with pr bility $0.5^{2}+2\cdot0.25^{2}=0.375$ · $+1$ $-1$ ch with probability $2 (0.5\cdot0.25)=0.25$ , and +2 and $-2$ − each with probability $0.25^{2}\,=\, 0.0625$ . As the proces nues, the probability gets spread out over more and more of the states. For example, at time t $t=10$ , the probabilities of the diferent states range from 0 . 1762 for the value 0 , and 0 . 0518 for the values $\pm4$ . At $t=50$ , the distribution is almost uniform, with a range of 0 . 1107–0 . 1116 . 

Thus, one approach for sampling from the uniform distribution over the set $-4,\cdot\cdot\cdot,+4$ is to start of at 0 and then randomly choose the next state from the transition model for this chain. After some number of such steps $t$ , our state $X^{(t)}$ would be sampled from a distribution that is very close to uniform over this space. We note that this approach is not a very good one for sampling from a uniform distribution; indeed, the expected time required for such a chain even to reach the boundaries of the interval $[-K, K]$ is $K^{2}$ steps. However, this general approach applies much more broadly, including in cases where our “long-term” distribution is not one from which we can easily sample. 
>  Example 12.6 中，该 Markov 链随着 $t$ 增长，$X^{(t)}$ 的分布将逐渐趋近于均匀分布
>  因此，一种从均匀分布采样的方式就是从该 Markov 链中逐渐采样，但该方法实际上并不是一个较优的方法，因为该 Markov 链达到 $[-K, K]$ 上的均匀分布的期望步数是 $K^2$

Markov chain Monte carlo (MCMC) sampling is a process that mirrors the dynamics of the Markov chain; the process of generating an MCMC trajectory is shown in algorithm 12.5. The sample $\mathbfit{x}^{(t)}$ is drawn from the distribution $P^{(t)}$ . We are interested in the limit of this process, that is, whether $P^{(t)}$ converges, and if so, to what limit. 
>  Markov Chain Monte Carlo 采样反映了 Markov 链的动态
>  生成 MCMC 轨迹的过程见 algorithm 12.5，其中样本 $\pmb x^{(t)}$ 生成自分布 $P^{(t)}$
>  我们关心该过程的极限，即 $P^{(t)}$ 什么时候收敛，收敛到什么程度

![[pics/PGM-Algorithm12.5.png]]

#### 12.3.2.3 Stationary Distributions 
Intuitively, as the process converges, we would expect $P^{(t+1)}$ to be close to $P^{(t)}$ . Using equation (12.20), we obtain: 

$$
P^{(t)}(\pmb{x}^{\prime})\approx P^{(t+1)}(\pmb{x}^{\prime})=\sum_{\pmb{x}\in V a l (\pmb{X})}P^{(t)}(\pmb{x})\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime}).
$$

>  直观上，随着该过程收敛，$t+1$ 时刻的分布 $P^{(t+1)}$ 应该和 $t$ 时刻的分布 $P^{(t)}$ 足够接近

At convergence, we would expect the resulting distribution $\pi (X)$ to be an equilibrium relative to the transition model; that is, the probability of being in a state is the same as the probability of transitioning into it from a randomly sampled predecessor. Formally: 
>  收敛时，我们期望得到的分布 $\pi (\pmb X)$ 是转移模型的一个稳态，即某个状态的概率等于从一个随机采样的上一个状态转移到该状态的概率

**Deﬁnition 12.3** 
A distribution $\pi (\pmb X)$ is $a$ stationary distribution for a Markov chain $\mathcal{T}$ if it satisﬁes: 

$$
\pi (\pmb X=\pmb x^{\prime})=\sum_{\pmb x\in V a l (\pmb X)}\pi (\pmb X=\pmb x)\mathcal{T}(\pmb x\to \pmb x^{\prime}).\tag{12.21}
$$ 
A stationary distribution is also called an invariant distribution . 

>  定义
>  满足 (12.21) 的分布称为 Markov 链 $\mathcal T$ 的稳态分布
>  如果我们将转移模型用矩阵 $A$ 表示，其中 $A_{ij} =  \mathcal T (\pmb x_i\rightarrow \pmb x_j)$ ，则稳态分布可以视作该矩阵特征值 1 对应的特征向量
>  显然，在该定义下，到达稳态分布之后，再转移得到的分布将还是稳态分布

As we have already discussed, the uniform distribution is a stationary distribution for the Markov chain of example 12.5. To take a slightly different example: 

Example 12.7 
Figure 12.4 shows an example of a different simple Markov chain where the transition probabilities are less uniform. By deﬁnition, the stationary distribution $\pi$ must satisfy the following three equations: 

$$
\begin{array}{l c l} {{\pi(x^{1})}} & {{=}} & {{0.25\pi(x^{1})+0.5\pi(x^{3})}} \\ {{\pi(x^{2})}} & {{=}} & {{0.7\pi(x^{2})+0.5\pi(x^{3})}} \\ {{\pi(x^{3})}} & {{=}} & {{0.75\pi(x^{1})+0.3\pi(x^{2}),}} \end{array}
$$ 
as well as the one asserting that it is a legal distribution: 

$$
\pi (x^{1})+\pi (x^{2})+\pi (x^{3})=1.
$$ 
$I t$ is straightforward to verify that this system has a unique solution: $\pi (x^{1})=0.2$ , $\pi (x^{2})=0.5$ , $\pi (x^{3})=0.3$ . For example, the ﬁrst equation asserts that 

$$
0.2=0.25\cdot0.2+0.5\cdot0.3,
$$ 
which clearly holds. 

In general, there is no guarantee that our MCMC sampling process converges to a stationary distribution. 
>  一般情况下，我们不能保证 MCMC 过程一定收敛到一个稳态分布

Example 12.8 
Consider th rkov chain ov $x^{1}$ and $x^{2}$ , suc that $\tau (x^{1}\rightarrow x^{2})=1$ and $\mathcal{T}(x^{2}\rightarrow$ $x^{1})=1$ . If P $P^{(0)}$ is such that $P^{(0)}(x^{1})=1$ , then the step t distribution $P^{(t)}$ has $P^{(t)}(x^{1})=1$ if $t$ is even, and $P^{(t)}(x^{2})=1$ if $t$ is odd. Thus, there is no convergence to a stationary distribution. 

Markov chains such as this, which exhibit a ﬁxed cyclic behavior, are called periodic Markov chains . 
>  不存在单个稳态分布，而是周期性地在一组分布中转换的 Markov 链被称为周期性 Markov 链

There is also no guarantee that the stationary distribution is unique: In some chains, the stationary distribution reached depends on our starting distribution $\bar{P}^{(0)}$ . Situations like this occur when the chain has several distinct regions that are not reachable from each other. Chains such as this are called reducible Markov chains . 
>  稳态分布也可能不是唯一的，一些 Markov 链中，不同的初始分布将导向不同的稳态分布
>  这类情况发生的原因在于 Markov 链存在多个独立的区域，这些区域互相不可达
>  这类 Markov 链称为可简化的 Markov 链

We wish to restrict attention to Markov chains that have a unique stationary distribution, which is reached from any starting distribution $P^{(0)}$ . There are various conditions that suffice to guarantee this property. The condition most commonly used is a fairly technical one: that the chain be ergodic . In the context of Markov chains where the state space $V a l (X)$ is ﬁnite, the following condition is equivalent to this requirement: 
>  我们关注仅存在可以从任意初始分布达到唯一稳态分布的 Markov 链
>  保证这一性质成立的一个条件称为各态遍历性
>  如果 Markov 链的状态空间 $Val (\pmb X)$ 有限，各态遍历性等价于以下表述：

**Definition 12.4** regular Markov chain
A Markov chain is said to be regular if there exists some number $k$ such that for every $\pmb x, \pmb x' \in Val (\pmb X)$, the probability of getting from $\pmb x$ to $\pmb x'$ in exactly $k$ steps is > 0

>  定义
>  如果 Markov 链满足对于状态空间 $Val (\pmb X)$ 中的任意两个状态 $\pmb x, \pmb x'$，存在一个数 $k$，使得正好通过 $k$ 步从 $\pmb x$ 转移到 $\pmb x'$ 的概率大于零，该 Markov 链就是规则的

In our Markov chain of example 12.5, the probability of getting from any state to any state in exactly 9 steps is greater than 0. Thus, this Markov chain is regular. Similarly, in the Markov chain of example 12.7, we can get from any state to any state in exactly two steps. 

The following result can be shown to hold: 

**Theorem 12.3** 
If a finite state Markov chain is regular, then it has a unique stationary distribution.

>  定理
>  如果 Markov chain 的状态空间有限，且 Markov chain 规则，则 Markov chain 有唯一的稳态分布

Ensuring regularity is usually straightforward. Two simple conditions that together guarantee regularity in ﬁnite-state Markov chains are as follows. First, it is possible to get from any state to any state using a positive probability path in the state graph. Second, for each state $_{_{x}}$ , there is a positive probability of transitioning from $_{_{x}}$ to $_{_{x}}$ in one step (a self-loop). These two conditions together are sufficient but not necessary to guarantee regularity (see exercise 12.12). However, they often hold in the chains used in practice. 
>  对于有限状态的 Markov 链，以下两个条件成立是该 Markov 链满足规则性的充分条件：
>  1. 状态图中，从任意一个状态转移到任意状态都存在一条概率为正的路径
>  2. 对于每个状态 $\pmb x$，存在一条一步从 $\pmb x$ 转移到 $\pmb x$ 的概率为正的路径 (即自环)
>  实践中使用的 Markov 链一般满足这两个性质

#### 12.3.2.4 Multiple Transition Models 
In the case of graphical models, our state space has a factorized structure — each state is an assignment to several variables. When deﬁning a transition model over this state space, we can consider a fully general case, where a transition can go from any state to any state. However, it is often convenient to decompose the transition model, considering transitions that update only a single component of the state vector at a time, that is, only a value for a single variable. 
>  在图模型中，整个状态空间应该是所有变量 $\mathcal X$ 的所有取值 $Val (\mathcal X)$
>  该状态空间往往具有可分解的结构，即我们将对 $\mathcal X$ 的赋值拆分为对多组变量的赋值的组合
>  在该状态空间上定义转移模型时，可以考虑分解转移模型，令每次转移仅更新状态变量的一个成分，即仅更新 $\mathcal X$ 中单个变量的取值

Example 12.9 
Consider an extension to our Grasshopper chain, where the grasshopper lives, not on a line, but in a two-dimensional plane. In this case, the state of the system is deﬁned via a pair of random variables $X, Y$ . Although we could deﬁne a joint transition model over both dimensions simultaneously, it might be easier to have separate transition models for the $X$ and $Y$ coordinate. 
>  例如，在一个二维坐标系中，可以为 $X, Y$ 分别定义各自的转移模型，而不是在 $X, Y$ 上定义联合的转移模型

In this case, as in several other settings, we often deﬁne a set of transition models, each with its own dynamics. Each such transition model $\mathcal{T}_{i}$ is called a kernel . In certain cases, the different kernels are necessary, because no single kernel on its own suffices to ensure regularity. This is the case in example 12.9. In other cases, having multiple kernels simply makes the state space more “connected” and therefore speeds the convergence to a stationary distribution. 
>  我们可以定义一组转移模型，其中每个转移模型 $\mathcal T_i$ 称为一个 kernel
>  一些情况下，多个转移模型是必要的，例如仅定义单个 kernel 无法保证 Markov chain 的规则性
>  一些情况下，定义多个 kernel 可以让状态空间更 “相连”，进而加速向稳态分布的收敛

There are several ways of constructing a single Markov chain from multiple kernels . One com- mon approach is simply to select randomly between them at each step, using any distribution. Thus, for example, at each step, we might select one of $\tau_{1},\dots,\tau_{k}$ , each with probability $1/k$ . Alternatively, we can simply cycle over the diferent kernels, taking each one in turn. Clearly, this approach does not deﬁne a homogeneous chain, since the kernel used in step $i$ is diferent from the one used in step $i+1$ . However, we can simply view the process as deﬁning a single chain $\mathcal{T}$ , each of whose steps is an aggregate step, consisting of ﬁrst taking $\mathcal{T}_{1}$ , then $\tau_{2},\,.\,.\,,$ , through $\mathcal{T}_{k}$ . 
>  用多个 kernel 定义 Markov chain 的方法也有多种
>  常见方法是在每一步使用任意分布随机选择一个 kernel 用于转移；
>  又或者可以循环使用各个 kernel，注意该方法没有定义一个同质的 Markov chain，因为第 $i$ 步使用的 kernel 和第 $i+1$ 步使用的 kernel 不同，但可以将该视为定义了另一个 Markov 链 $\mathcal T$，其每一步是原来的多步合并 (从 $\mathcal T_1$ 开始到 $\mathcal T_k$)

In the case of graphical models, one approach is to deﬁne a multikernel chain, where we have a el $\mathcal{T}_{i}$ for eac ariable $X_{i}\in X$ $X_{-i}=\mathcal{X}-\{X_{i}\}$ , and let $\mathbf{\Delta}x_{i}$ denote an tion to $X_{i}$ . The model T $\mathcal{T}_{i}$ takes a state ( $({\boldsymbol{x}}_{-i},{\boldsymbol{x}}_{i})$ and transitions to a state of the form ( $(\pmb{x}_{-i}, x_{i}^{\prime})$ . As − − we discussed, we can combine the diferent kernels into a single global model in various ways. 
>  具体在图模型中，我们会定义一个多 kernel 的 Markov chain，其中每个变量 $X_i \in \pmb X$ 都有一个 kernel
>  对于每个变量 $X_i$，我们令 $\pmb X_{-i} = \mathcal X - \{X_i\}$，同时令 $\pmb x_i$ 表示 $\pmb X_i$ 的实例
>  我们将每个 kernel $\mathcal T_i$ 定义为接受形式为 $(\pmb x_{-i}, x_i)$ 的状态，转移到形式为 $(\pmb x_{-i}, x_i')$ 的状态，即仅改变 $X_i$ 的实例
>  我们进而利用将用这些 kernels 定义 Markov chain，如之前所讨论的

Regardless of the structure of the different kernels, **we can prove that a distribution is a stationary distribution for the multiple kernel chain by proving that it is a stationary distribution (satisﬁes equation (12.21)) for each of individual kernels $\mathcal{T}_{i}$ . Note that each kernel by itself is generally not ergodic; but as long as each kernel satisﬁes certain conditions (speciﬁed in deﬁnition 12.5) that imply that it has the desired stationary distribution, we can combine them to produce a coherent chain, which may be ergodic as a whole. This ability to add new types of transitions to our chain is an important asset in dealing with the issue of local maxima, as we will discuss.**
>  无论不同 kernel 的结构如何，我们可以通过证明一个分布对于每个单独的 kernel $\mathcal{T}_{i}$ 都是稳态分布 (满足 eq 12.21) 来证明该分布是该多 kernel Markov chain 的稳态分布
>  需要注意的是，每个 kernel 本身通常不一定是各态遍历的；但是，只要每个 kernel 满足某些条件 (在定义 12.5 中指定)，这些条件就将表明该 kernel 具有所需的稳态分布，我们就可以将它们结合起来形成一个连贯的链，整体上这个链可能是各态遍历的
>  这种向链中添加新 kernel 的能力，在处理局部最大值问题时是一个重要的优势，这一点我们将在后续讨论

### 12.3.3 Gibbs Sampling Revisited 
The theory of Markov chains provides a general framework for generating samples from a target distribution $\pi$ . In this section, we discuss the application of this framework to the sampling tasks encountered in probabilistic graphical models. In this case, we typically wish to generate samples from the posterior n $P (X\mid E=e)$ , where $X=\mathcal{X}-E$ . Thus, we wish to deﬁne a chain for which $P (X\mid e)$ | is e st y distribution. Thus, we deﬁne the states of the Markov chain to be instantiations x to X − $\mathcal{X}-E$ . In order to deﬁne a Markov chain, we need to deﬁne a process that transitions from one state to the other, converging to a stationary distribution $\pi (X)$ , which terior distribution $P (X\mid e)$ . 
>  MCMC 方法提供了一种从目标分布 $\pi$ 生成样本的通用框架
>  本节讨论使用该框架完成图模型中的采样，我们希望从后验分布 $P (\pmb X \mid \pmb E = \pmb e)$ 中生成样本，其中 $\pmb X = \mathcal X - \pmb E$
>  故我们希望定义一个稳态分布为 $P (\pmb X \mid \pmb e)$ 的 Markov chain，其状态空间为 $Val (\pmb X)$
>  要定义 Markov chain，我们需要定义在状态空间中从一个状态转移到另一个状态的过程，该过程收敛到稳态分布 $\pi (\pmb X ) = P (\pmb X \mid \pmb e)$

As in our earlier example, we assume that P $P (X\mid e)\;=\; P_{\Phi}$ | for some set of factor $\Phi$ that are deﬁned by reducing the original factors in our graphical model by the evidence e . This reduction allows us to simplify notation and to discuss the methods in a way that applies both to directed and undirected graphical models. 
>  我们将目标分布 $P (\pmb X \mid \pmb e)$ 写为 Gibbs 分布 $P_\Phi$ 的形式，因子集合 $\Phi$ 通过将原来图模型中的因子在证据 $\pmb e$ 上简化得到

Gibbs sampling is based on one yet effective Markov chain for factored state spaces, which is particularly efficient for graphical models. 
>  Gibbs 采样实际上基于一类具有可因子化分解的状态空间的 Markov chain，称为 Gibbs chain, Gibbs chain 对于图模型十分高效

We deﬁne the kernel $\mathcal{T}_{i}$ as follows. Intuitively, we simply “forget” the value of $X_{i}$ in the current state and sample a new value for $X_{i}$ from its posterior given the rest of the current state. More precisely, let $({\boldsymbol{x}}_{-i}, {{x}} _{i})$ be a state in the chain. We deﬁne: 

$$
\begin{array}{r}{\mathcal{T}_{i}((\pmb{x}_{-i}, x_{i})\rightarrow (\pmb{x}_{-i}, x_{i}^{\prime}))=P (x_{i}^{\prime}\mid\pmb{x}_{-i}).}\end{array}\tag{12.22}
$$ 
Note that the transition probability does not depend on the current value $x_{i}$ of $X_{i}$ , but only on the remaining state ${\pmb x}_{-i}$ . It is not difcult to show that the posterior distribution $P_{\Phi}(X)=$ $P (\mathcal{X}\mid e)$ is a stationary distribution of this process. (See exercise 12.13.) 

>  Gibbs chain 中，直观上，在使用 $\mathcal T_i$ 转移时，我们遗忘 $X_i$ 的当前值，并从给定其他变量当前值的 $X_i$ 的后验分布中为 $X_i$ 采样一个新的值
>  $\mathcal T_i$ 的具体定义为 (12.22)
>  注意转移概率并不依赖于 $X_i$ 的当前值 $x_i$，但依赖于所有其他变量的当前值 $\pmb x_{-i}$
>  可以证明后验分布 $P_\Phi (\pmb X) = P (\mathcal X \mid \pmb e)$ 为该 Gibbs chain 的稳态分布

The sampling algorithm for a single trajectory of the Gibbs chain was shown earlier in this section, in algorithm 12.4. Recall that the Gibbs chain is deﬁned via a set of kernels; we use the multistep approach to combine them. Thus, the diferent local kernels are taken consecutively; having changed the value for a variable $X_{1}$ , the value for $X_{2}$ is sampled based on the new value. Note that a step in the aggregate chain occurs only once we have executed every local transition once. 
>  Gibbs chain 由多个 kernel 定义，在转移过程中轮流使用各个 kernel，执行一次局部转移
>  例如，使用 kernel $\mathcal T_1$ 为 $X_1$ 采样了新值以后，下一次就是使用 kernel $\mathcal T_2$ 为 $X_2$ 采样新值，注意 $X_2$ 的新值采样会基于 $X_1$ 的新值
>  我们认为在执行完一轮局部转移之后，在 chain 中前进了一步

Gibbs sampling is particularly easy to implement in the many graphical models where we can compute the transition probability $P (X_{i}\mid\pmb{x}_{-i})$ (in line 5 of the algorithm) very efficiently. In particular, as we now show, this distribution can be done based only on the Markov blanket of $X_{i}$ .
>  Gibbs 采样在许多图模型中都可以高效实现
>  Gibbs 采样的核心操作就是每一次的局部转移中在 $P (X_i \mid \pmb x_{-i})$ 中对 $X_i$ 进行采样
>  我们可以证明后验分布 $P (X_i \mid \pmb x_{-i})$ 仅仅基于 $X_i$ 的 Markov blanket

We show this analysis for a Markov network; the application to Bayesian networks is straightforward. Recalling deﬁnition 4.4, we have that: 

$$
\begin{array}{r c l}{P_{\Phi}(\pmb X)}&{=}&{\displaystyle\frac{1}{Z}\prod_{j}\phi_{j}(\pmb D_{j})}\\ &{=}&{\displaystyle\frac{1}{Z}\prod_{j\ :\ X_{i}\in \pmb D_{j}}\phi_{j}(\pmb D_{j})\prod_{j\ :\ X_{i}\not\in \pmb D_{j}}\phi_{j}(\pmb D_{j}).}\end{array}
$$ 
Let $\pmb x_{j,-i}$ denote the assignment in ${\pmb x}_{-i}$ to $\pmb D_{j}-\{X_{i}\}$ , noting that when $X_{i}\notin \pmb D_{j}$ , $\pmb x_{j,-i}$ is a full assignment to $\pmb D_{j}$ . We can now derive: 

$$
\begin{align}
P (x_i' \mid \pmb x_{-i}) &=\frac {P (x_i', \pmb x_{-i})}{\sum_{x_i''} P (x_i'', \pmb x_{-i})}\\
&=\frac {\frac 1 Z\prod_{\pmb D_j \ni X_i}\phi_j (x_i',\pmb x_{j,-i})\prod_{\pmb D_j \not \ni X_i}\phi_j (x_i',\pmb x_{j,-i})}{\frac 1 Z\sum_{x_i''}\prod_{\pmb D_j \ni X_i}\phi_j (x_i'',\pmb x_{j,-i})\prod_{\pmb D_j \not \ni X_i}\phi_j (x_i'',\pmb x_{j,-i})}\\
&=\frac {\prod_{\pmb D_j \ni X_i}\phi_j (x_i',\pmb x_{j,-i})\prod_{\pmb D_j \not \ni X_i}\phi_j (x_i',\pmb x_{j,-i})}{\sum_{x_i''}\prod_{\pmb D_j \ni X_i}\phi_j (x_i'',\pmb x_{j,-i})\prod_{\pmb D_j \not \ni X_i}\phi_j (x_i'',\pmb x_{j,-i})}\\
&=\frac {\prod_{\pmb D_j \ni X_i}\phi_j (x_i',\pmb x_{j,-i})\prod_{\pmb D_j \not \ni X_i}\phi_j (\pmb x_{j,-i})}{\sum_{x_i''}\prod_{\pmb D_j \ni X_i}\phi_j (x_i'',\pmb x_{j,-i})\prod_{\pmb D_j \not \ni X_i}\phi_j (\pmb x_{j,-i})}\\
&=\frac {\prod_{\pmb D_j \ni X_i}\phi_j (x_i',\pmb x_{j,-i})}{\sum_{x_i''}\prod_{\pmb D_j \ni X_i}\phi_j (x_i'',\pmb x_{j,-i})}\tag{12.23}\\
\end{align}
$$

This last expression uses only the factors involving $X_{i}$ , and depends only on the instantiation in ${\pmb x}_{-i}$ of $X_{i}$ ’s Markov blanket. In the case of Bayesian networks, this expression reduces to a formula involving only the CPDs of $X_{i}$ and its children, and its value, again, depends only on the assignment in ${\pmb x}_{-i}$ to the Markov blanket of $X_{i}$ . 

>  根据 (12.23) ，$P (x_i' \mid \pmb x_{-i})$ 仅依赖于包含了 $X_i$ 的因子，进而仅依赖于 $\pmb x_{-i}$ 中涉及到 $X_i$ 的 Markov blanket 的取值
>  在 BN 中，推导是类似的，最后我们将得到 $P (x_i' \mid \pmb x_{-i})$ 仅依赖于包含了 $X_i$ 和其子节点的 CPDs (只有这些 CPDs 和 $X_i$ 有关)，进而仅依赖于 $X_i$ 的 Markov blanket 的取值

Example 12.10
Consider again the Student network of ﬁgure 12.1, with the evidence $s^{1}, l^{0}$ . The kernel for the $G$ en a state $(i, d, g, s^{1}, l^{0})$ , we deﬁne ${\mathcal{T}}((i, g, d, s^{1}, l^{0})\ \rightarrow$ $(i, g^{\prime}, d, s^{1}, l^{0}))\,=\, P (g^{\prime}\mid i, d, s^{1}, l^{0})$ | . Th value can be computed locally, using only the CPDs that involve G , that is, the CPDs of G and L : 

$$
P (g^{\prime}\mid i, d, s^{1}, l^{0})=\frac{P (g^{\prime}\mid i, d) P (l^{0}\mid g^{\prime})}{\sum_{g^{\prime\prime}}P (g^{\prime\prime}\mid i, d) P (l^{0}\mid g^{\prime\prime})}.
$$ 
Similarly, the kernel for the variable $I$ is deﬁned to be ${\mathcal{T}}((i, g, d, s^{1}, l^{0})\,\rightarrow\, (i^{\prime}, g, d, s^{1}, l^{0}))\,=$ $P (i^{\prime}\mid g, d, s^{1}, l^{0})$ , which simpliﬁes as follows: 

$$
P (i^{\prime}\mid g, d, s^{1}, l^{0})=\frac{P (i^{\prime}) P (g\mid i^{\prime}, d) P (s^{1}\mid i^{\prime})}{\sum_{i^{\prime\prime}}P (i^{\prime\prime}) P (g\mid i^{\prime\prime}, d) P (s^{1}\mid i^{\prime\prime})}.
$$ 

As presented, the algorithm is deﬁned via a sequence of local kernels, where each samples a single variable conditioned on all the rest. The reason for this approach is computational. As we showed, we can easily compute the transition model for a single variable given the rest. However, there are cases where we can simultaneously sample several variables efciently. Speciﬁcally, assume we can partition the variables $X$ into several disjoint blocks of variables $X_{1},\dots, X_{k}$ , such that we can efciently sample $\pmb x_{i}$ from $P_{\Phi}(X_{i}\mid\mathbf{\mathcal{x}}_{1},.\,.\,,\mathbf{\mathcal{x}}_{i-1},\mathbf{\mathcal{x}}_{i+1},.\,.\,.\,,\mathbf{\mathcal{x}}_{k})$ . In this case, we can modify our Gibbs sampling algorithm to iteratively sample blocks of variables, rather than individual variables, thereby taking much “longer-range” transitions in the state space in a single sampling step. Here, like in Gibbs sampling, we deﬁne the algorithm to be producing a new sample only once all blocks have been resampled. This algorithm is called block Gibbs . Note that standard Gibbs sampling is a special case of block Gibbs sampling, with the blocks corresponding to individual variables. 
>  我们知道 Gibbs 采样算法的流程由一序列局部 kernels 定义，每个 kernel 基于所有其他变量采样一个变量
>  我们已经知道基于所有其他变量为单个变量采样的计算十分容易，但我们可以进一步同时采样多个变量
>  具体地说，我们将 $\pmb X$ 划分为多个不相交的变量组 $\pmb X_1, \dots, \pmb X_k$，同一个变量组内的变量可以同时采样，条件于其他所有变量组，即我们从 $P_{\Phi}(\pmb X_i\mid \pmb x_1, \dots, \pmb x_{i-1}, \pmb x_{i}, \dots, \pmb x_k)$ 中采样 $\pmb X_i$ 的实例 $\pmb x_i$
>  则 Gibbs 采样对应修改为迭代式地按组采样变量，而不是一次仅采样单个变量，因此在每一次采样我们实际上在状态空间中执行了一次更长范围的转换
>  算法在所有的变量组都经过重采样之后开始生成新的样本
>  该算法称为 block Gibbs，标准的 Gibbs 是 block size = 1 时的特例

Example 12.11
with a variable representing its difficulty. We also have a set of grades for students in classes (not necessarily $^a$ grade for each student in every class). Using an abbreviated notation, we have $^a$ set of variables $I_{1},\ldots, I_{n}$ for the students (where each $I_{j}=I (s_{j}))$ ), $D=\{D_{1},.\,.\,.\,, D_{\ell}\}$ for the courses, and $G=\{G_{j, k}\}$ for th s, wh h variable $G_{j, k}$ has the parents $I_{j}$ and $D_{k}$ . See ﬁgure 12.5 for an example with n $n=4$ and ℓ $\ell=2$ . Let us assume that we observe the grades, so that we have evidence $G=g$ . An examination of active paths shows that the diferent variables $I_{j}$ are conditionally independent given an assignment $^d$ to $_D$ . Thus, given $D=d, G=g$ , we can efciently sample all of the $\boldsymbol{I}$ variables as a block by sampling each $I_{j}$ independently of the others. Similarly, we can sample all of the $_D$ variables as a block given an assignment $I=i, G=g$ . Thus, we can alternate steps where in one we sample $\pmb{i}[m]$ given $_{g}$ and $d[m]$ , and in the other we sample $d[m+1]$ given $_{g}$ and $\pmb{i}[m]$ . 

In this example, we can easily apply block Gibbs because the variables in each block are marginally independent given the variables outside the block. This independence property allows us to compute efficiently the conditional distribution $P_{\Phi}(X_{i}\mid\mathbf{\mathcal{x}}_{1},.\,.\,,\mathbf{\mathcal{x}}_{i-1},\mathbf{\mathcal{x}}_{i+1},.\,.\,.\,,\mathbf{\mathcal{x}}_{k})$ , and to sample from it. Importantly, however, full independence is not essential: we need only have the property that the block-conditional distribution can be efficiently manipulated. For example, in a grid-structured network, we can easily deﬁne our blocks to consist of separate rows or of separate columns. In this case, the structure of each block is a simple chain-structured network; we can easily compute the conditional distribution of one row given all the others, and sample from it (see exercise 12.3). 
>  block Gibbs 要求同个 block 内的变量在给定 block 外的所有其他变量时都相互独立，因此我们可以将采样分布 $P_\Phi (\pmb X_i\mid \pmb x_1, \dots, \pmb x_{i-1}, \pmb x_{i}, \dots, \pmb x_k)$ 完全分解，转化为独立给每个 $\pmb X_i$ 中的变量采样
>  有时也不必要要求 block 内相互独立，只需要分布 $P_\Phi (\pmb X_i\mid \pmb x_1, \dots, \pmb x_{i-1}, \pmb x_{i}, \dots, \pmb x_k)$ 可以进行高效采样即可

We note that the Gibbs chain is not necessarily regular, and might not converge to a unique stationary distribution. 
>  Gibbs chain 不一定规则，也不一定收敛到唯一的稳态分布

Example 12.12
Consider a sim e net rk that consists of a single $\nu$ -structure $X\rightarrow Z\leftarrow Y$ , where the variables are all binary, X and $Y$ are both uniformly distributed, and Z is the deterministic exclusive or of $X$ and $Y$ (t is, $Z=z^{1}$ if $X\neq Y.$ ). Consider applying Gibbs sampling to thi with the evidence z $z^{1}$ . The true posterior assigns probability $1/2$ to each of the two states $x^{1}, y^{0}, z^{1}$ and $x^{0}, y^{1}, z^{1}$ . Assume that we start the ﬁrst of these two states. In is case, $P (X\mid y^{0}, z^{1})$ assig probability 1 to x $x^{1}$ , so that the X transition leaves the value of X unchanged. Similarly, the Y transition leaves the value of $Y$ unchanged. Therefore, the chain will simply stay at the initial state forever, and it will never sample from the other state. The analogous phenomenon occurs for the other starting state. This chain is an example of a reducible Markov chain. 

However, this chain is guaranteed to be regular whenever the distribution is positive, so that every value of $X_{i}$ has positive probability given an assignment ${\pmb x}_{-i}$ to the remaining variables. 

**Theorem 12.4** 
Let H be a Markov network such that all of the clique potentials are strictly positive. Then the Gibbs-sampling Markov chain is regular. 
>  定理
>  Markov 网络 $\mathcal H$ 的所有团势能都严格为正，则 Gibss chain 是规则的

The proof is not difcult, and is left as an exercise (exercise 12.20). 

Positivity is, however, not necessary; there are many examples of nonpositive distributions where the Gibbs chain is regular. 
>  Positivity 是充分条件不是必要条件

Importantly, however, even chains that are regular may require a long time to mix , that is, get close to the stationary distribution. In this case, instances generated from early in the sampling process will not be representative of the desired stationary distribution. 

### 12.3.4 A Broader Class of Markov Chains\*
As we discussed, the use of MCMC methods relies on the construction of a Markov chain that has the desired properties: regularity, and the target stationary distribution. In the previous section, we described the Gibbs chain, a simple Markov chain that is guaranteed to have these properties under certain assumptions. However, Gibbs sampling is applicable only in certain circumstances; in particular, we must be able to sample from the distribution $P (X_{i}\mid\mathbf{\sigma}\mathbf{x}_{-i})$ . Although this sampling step is easy for discrete graphical models, in continuous models, the conditional distribution may not be one that has a parametric form that allows sampling, so that Gibbs is not applicable. 
>  我们知道 MCMC 方法依赖于构建一个稳态分布为目标分布，且满足规范性的 Markov chain，为此，我们构造了 Gibbs chain，介绍了 Gibbs 采样算法
>  Gibbs 采样需要从 $P (X_i \mid \pmb x_{-i})$ 中采样，对于离散图模型不难，但在连续模型的情况下该条件分布并不一定会具有允许采样的参数化形式

Even more important, the Gibbs chain uses only very local moves over the state space: moves that change one variable at a time. In models where variables are tightly correlated, such moves often lead from states whose probability is high to states whose probability is very low. In this case, the high-probability states will form strong basins of attraction, and the chain will be very unlikely to move away from such a state; that is, the chain will mix very slowly. In this case, we often want to consider chains that allow a broader range of moves, including much larger steps in the space. The framework we develop in this section allows us to construct a broad family of chains in a way that guarantees the desired stationary distribution. 
>  Gibbs chain 一次仅改变一个变量，即在状态空间中的移动是局部的
>  在变量高度相关的模型中，这样的移动 (仅改变单个变量的值) 时常会导致从概率较高的状态移动到概率较低的状态
>  此时，高概率状态会形成吸引盆地，链将几乎不可能离开这个状态，故链的收敛速度会非常慢
>  此时我们希望考虑允许更大范围移动的链，包括在空间中移动更大步长

#### 12.3.4.1 Detailed Balance 
Before we address the question of how to construct a Markov chain with a particular stationary distribution, we address the question of how to verify easily that our Markov chain has the desired stationary distribution. Fortunately, we can deﬁne a test that is local and easy to check, and that sufces to characterize the stationary distribution. As we will see, this test also provides us with a simple method for constructing an appropriate chain. 
>  先考虑如何验证 Markov chain 有想要的稳态分布

**Deﬁnition 12.5**  reversible Markov chain 
A ﬁnite-state Markov chain $\mathcal{T}$ is reversible if there exists a unique distribution $\pi$ such that, for all $x, x^{\prime}\in V a l (X)$ : 

$$
\pi (\pmb x){\mathcal{T}}(\pmb x\to \pmb x^{\prime})=\pi (\pmb x^{\prime}){\mathcal{T}}(\pmb x^{\prime}\to \pmb x).\tag{12.24}
$$ 
This equation is called the detailed balance . 

>  定义
>  有限状态的 Markov chain $\mathcal T$ 如果满足存在唯一的分布 $\pi$，使得对于状态空间任意一对状态 $\pmb x, \pmb x'\in Val (\pmb X)$，从 $\pi (\pmb x)$ 移动到 $\pmb x'$ 的概率等于从 $\pi (\pmb x')$ 移动到 $\pmb x$ 的概率，称它是可逆的
>  (12.24) 称为细致平衡方程

The product $\pi ({\pmb x})\mathcal{T}({\pmb x}\,\rightarrow\,{\pmb x}^{\prime})$ represents a process where we pick a starting state at random according to $\pi$ , and then take a random transition from the chosen state according to the transition model. The detailed balance equation asserts that, using this process, the probability of a transition from $\pmb {x}$ to $\pmb x'$ is the same as the probability of a transition for $\pmb x'$ to $\pmb x$.

Reversibility implies that $\pi$ is a stationary distriion of $\mathcal{T}$ , but not necessarily that the chain will converge to $\pi$ (see example 12.8). However, if $\mathcal{T}$ is regular, then convergence is guaranteed, and the reversibility condition provides a simple characterization of its stationary distribution: 
>  可逆性说明了 $\pi$ 是 $\mathcal T$ 的一个稳态分布，但 Markov chain 不一定收敛到 $\pi$
>  如果 $\mathcal T$ 规范，则保证收敛到 $\pi$

**Proposition 12.3**
If $\mathcal{T}$ is regular and it sa ﬁes the detailed balance equation relative to $\pi$ , then $\pi$ is the unique stationary distribution of T . 
>  命题
>  如果 $\mathcal T$ 规范，且相对于 $\pi$ 满足细致平衡方程，则 $\pi$ 是 $\mathcal T$ 的唯一稳态分布

The proof is left as an exercise (exercise 12.14). 

>  证明

$$
\begin{align}
&\sum_{\pmb x} \pi (\pmb x)\mathcal T (\pmb x \rightarrow \pmb x')\\
=&\sum_{\pmb x} \pi (\pmb x')\mathcal T (\pmb x' \rightarrow \pmb x)\\
=& \pi (\pmb x')\sum_{\pmb x}\mathcal T (\pmb x' \rightarrow \pmb x)\\
=& \pi (\pmb x')
\end{align}
$$

>  因此 $\pi (\pmb x)$ 为 $\mathcal T$ 的稳态分布
>  又因为 $\mathcal T$ 规范，故其稳态分布是唯一的

Example 12.13 
We can test this proposition on the Markov chain of ﬁgure 12.4. Our detailed balance equation for the two states $x^{1}$ and $x^{3}$ asserts that 

$$
\pi (x^{1}){\mathcal{T}}(x^{1}\rightarrow x^{3})=\pi (x^{3}){\mathcal{T}}(x^{3}\rightarrow x^{1}).
$$ 
Testing this equation for the stationary distribution $\pi$ described in example 12.7, we have: 

$$
0.2\cdot0.75=0.3\cdot0.5=0.15.
$$ 
The detailed balance equation can also be applied to multiple kernels. If each kernel $\mathcal{T}_{i}$ satisﬁes the detailed balance equation relative to some stationary distribution $\pi$ , then so does the mi ure transition model $\mathcal{T}$ (see exercise 12.16). The application to the multistep transition model $\mathcal{T}$ is also possible, but requires some care (see exercise 12.17). 
>  细致平衡条件对于多个 kernel 也适用，如果每个 kernel $\mathcal T_i$ 都相对于分布 $\pi$ 满足细致平衡方程，则这些 kernels 定义的混合转移模型 $\mathcal T$ 也相对于 $\pi$ 满足细致平衡方程

#### 12.3.4.2 Metropolis-Hastings Algorithm 
The reversibility condition gives us a condition for verifying that our Markov chain has the desired stationary distribution. However, it does not provide us with a constructive approach for producing such a Markov chain. The Metropolis-Hastings algorithm is a general construction that allows us to build a reversible Markov chain with a particular stationary distribution. 
>  可逆条件方便我们验证 Marko chain 是否具有我们想要的稳态分布 (代入我们想要的稳态分布查看是否满足细致平衡方程)
>  Metropolis-Hastings 算法用于构建一个具有特定稳态分布的可逆 Markov chain

Unlike the Gibbs chain, the algorithm does not assume that we can generate next-state samples from a particular target distribution. Rather, it uses the idea of a proposal distribution that we have already seen in the case of importance sampling. 
>  Metropolis-Hasting 构建的 Markov chain 并不要求我们从特定的目标分布生成下一个状态样本 (和 Gibbs chain 不同)，而是可以使用提案分布

As for importance sampling, the proposal distribution in the Metropolis-Hastings algorithm is intended to deal with cases where we cannot sample directly from a desired distribution. In the case of a Markov chain, the target distribution is our next-state sampling distribution at a given state. We would like to deal with cases where we cannot sample directly from this target. Therefore, we sample from a different distribution — the proposal distribution — and then correct for the resulting error. However, unlike importance sampling, we do not want to keep track of importance weights, which are going to decay exponentially with the number of transitions, leading to a whole slew of problems. Therefore, we instead randomly choose whether to accept the proposed transition, with a probability that corrects for the discrepancy between the proposal distribution and the target. 
>  MH 中使用提案分布的目的也是解决不能直接从某个目标分布采样的情况
>  Markov chain 中，我们的目标分布就是在给定状态下对下一个状态的采样分布
>  此时的思路同样是从另一个提案分布采样，然后修正误差
>  和重要性采样不同，这里不追踪重要性权重，重要性权重会随着转移的数量增加而指数衰减
>  我们随机选择是否接受提案的转移，同时用一个概率修正提案分布和目标分布的差异

More precise, our proposal distribution $\mathcal{T}^{Q}$ deﬁnes a transition model over our state space: For each state x , $\mathcal{T}^{Q}$ deﬁnes a distribution over possible successor states in $V a l (X)$ , from which we select randomly a candidate next state $\scriptstyle{\boldsymbol{x}}^{\prime}$ . We can either accept the proposal and transition to $\scriptstyle{\boldsymbol{x}}^{\prime}$ , or reject it and stay at $_{_{x}}$ . Thus, for each pair of states ${\boldsymbol{x}},{\boldsymbol{x}}^{\prime}$ we have an acceptance probability $\mathcal{A}(\pmb{x}\rightarrow\pmb{x}^{\prime})$ . The actual transition model of the Markov chain is then: 
>  提案分布 $\mathcal T^Q$ 同样是一个定义在状态空间 $Val (\pmb X)$ 上的转移模型
>  $\mathcal T^Q$ 为每个状态 $\pmb x$ 定义它转移到 $Val (\pmb X)$ 中各个状态的概率，同时为每个转移额外定义了一个接收概率
>  我们将状态转移 $\pmb x \rightarrow \pmb x'$ 的接受概率记作 $\mathcal A (\pmb x \rightarrow \pmb x')$，则 Markov chain 的实际转移模型为

$$
\begin{align}
\mathcal T (\pmb x \rightarrow \pmb x') &= \mathcal T^Q (\pmb x \rightarrow \pmb x')\mathcal A (\pmb x\rightarrow \pmb x')\quad \pmb x\ne \pmb x'\\
\mathcal T (\pmb x \rightarrow \pmb x) &= \mathcal T^Q (\pmb x \rightarrow \pmb x)+\sum_{\pmb x' \ne \pmb x}\mathcal T^Q (\pmb x\rightarrow \pmb x')(1-\mathcal A (\pmb x\rightarrow \pmb x'))
\end{align}\tag{12.25}
$$

By using a proposal distribution, we allow the Metropolis-Hastings algorithm to be applied even in cases where we cannot directly sample from the desired next-state distribution; for example, where the distribution in equation (12.22) is too complex to represent. The choice of proposal distribution can be arbitrary, so long as it induces a regular chain. One simple choice in discrete factored state spaces is to use a multiple transition model, where $\mathcal{T}_{i}^{Q}$ is a uniform distribution over the values of the variable $X_{i}$ . 
>  当我们难以从目标分布直接采样时，我们可以使用提案分布
>  提案分布的选择可以任意，只要它能使得 chain 是规范的
>  对于离散可可分解状态空间的 Markov chain 来说，一种选择是使用多个转移模型定义提案分布，其中 $\mathcal T_i^Q$ 是 $X_i$ 上的均匀分布

Given a proposal distribution, we can use the detailed balance equation to select the acceptance probabilities so as to obtain the desired stationary distribution. For this Markov chain, the detailed balance equations assert that, for all $\pmb x \ne \pmb x'$

$$
\pi (\pmb x)\mathcal{T}^{Q}(\pmb x\to \pmb x^{\prime})\mathcal{A}(\pmb x\to \pmb x^{\prime})=\pi (\pmb x^{\prime})\mathcal{T}^{Q}(\pmb x^{\prime}\to \pmb x)\mathcal{A}(\pmb x^{\prime}\to \pmb x).
$$ 
We can verify that the following acceptance probabilities satisfy these equations: 

$$
\mathcal{A}(\pmb x\rightarrow \pmb x^{\prime})=\operatorname*{min}\left[1,\frac{\pi (\pmb x^{\prime})\mathcal{T}^{Q}(\pmb x^{\prime}\rightarrow \pmb x)}{\pi (\pmb x)\mathcal{T}^{Q}(\pmb x\rightarrow \pmb x^{\prime})}\right],\tag{12.26}
$$ 
and hence that the chain has the desired stationary distribution: 

>  确定提案分布 $\mathcal T^Q$ 后，我们根据细致平衡方程选择接受概率，条件是能够得到想要的稳态分布 (也就是要求提案分布 $\mathcal T^Q$ 和接受概率 $\mathcal A$ 定义的 Markov chain 的稳态分布是 $\pi$，根据 $\pi$ 和 $\mathcal T^Q$ 解出 $\mathcal A$)

**Theorem 12.5** 
Let $\mathcal{T}^{Q}$ be any proposal distribution, and consider the Markov chain deﬁned by equation (12.25) and equation (12.26). If this Markov chain is regular, then it has the stationary distribution $\pi$ . 
>  定理
>  $\mathcal T^Q$ 为任意提案分布，对于 (12.25) 和 (12.26) 定义的 Markov chain，如果 chain 规范，则它具有稳态分布 $\pi$

The proof is not difficult, and is left as an exercise (exercise 12.15). 

Let us see how this construction process works. 

Example 12.14 
Assume that our proposal distribution $\mathcal{T}^{Q}$ is given by the chain of ﬁgure 12.4, but that we want to sample from a stationary distribution π $\pi^{\prime}$ where: $\pi^{\prime}(x^{1})=0.6$ , $\pi^{\prime}(x^{2})=0.3,$ , and $\pi^{\prime}(x^{3})=0.1$ . To deﬁne the chain, we need to compute the acceptance probabilities. Applying equation (12.26), we obtain, for example, that: 

$$
\begin{array}{l l l} {{A(x^{1}\to x^{3})}} & {{=}} & {{\displaystyle\operatorname*{min}\left[1,\frac{\pi^{\prime}(x^{3})\mathcal{T}^{Q}(x^{3}\to x^{1})}{\pi^{\prime}(x^{1})\mathcal{T}^{Q}(x^{1}\to x^{3})}\right]=\operatorname*{min}\left[1,\frac{0.1\cdot0.5}{0.6\cdot0.75}\right]=0.11}} \\ {{A(x^{3}\to x^{1})}} & {{=}} & {{\displaystyle\operatorname*{min}\left[1,\frac{\pi^{\prime}(x^{1})\mathcal{T}^{Q}(x^{1}\to x^{3})}{\pi^{\prime}(x^{3})\mathcal{T}^{Q}(x^{3}\to x^{1})}\right]=\operatorname*{min}\left[1,\frac{0.6\cdot0.75}{0.1\cdot0.5}\right]=1.}} \end{array}
$$ 
We can now easily verify that the stationary distribution of the chain resulting from equation (12.25) and these acceptance probabilities gives the desired stationary distribution $\pi^{\prime}$ . 

The Metropolis-Hastings algorithm has a particularly natural implementation in the context of graphical models. Each local transition model $\mathcal{T}_{i}$ is deﬁned via an associated proposal distribution $\mathcal{T}_{i}^{Q_{i}}$ . The acceptance probability for this chain has the form 
>  在图模型中，为每个局部转移模型 $\mathcal T_i$ 定义其相关的提案分布 $\mathcal T_i^{Q_i}$
>  则 Markov chain 的接受概率形式如下

$$
\begin{align}
\mathcal A (\pmb x_{-i}, x_i \rightarrow \pmb x_{-i}, x_i') &= \min \left[1, \frac {\pi (\pmb x_{-i}, x_i')\mathcal T_i^{Q_i}(\pmb x_{-i}, x_i'\rightarrow \pmb x_{-i}, x_i)}{\pi (\pmb x_{-i}, x_i)\mathcal T_i^{Q_i}(\pmb x_{-i}, x_i \rightarrow \pmb x_{-i}, x_i')}\right]\\
&=\min\left[1, \frac {P_\Phi (x_i',\pmb x_{-i})}{P_\Phi (x_i,\pmb x_{-i})}\frac {\mathcal T_i^{Q_i}(\pmb x_{-i}, x_i'\rightarrow \pmb x_{-i}, x_i)}{\mathcal T_i^{Q_i}(\pmb x_{-i}, x_i\rightarrow \pmb x_{-i}, x_i')}\right]
\end{align}
$$

The proposal distributions are usually fairly simple, so it is easy to compute their ratios. In the case of graphical models, the ﬁrst ratio can also be computed easily: 

$$
\begin{array}{r c l}{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime},{\pmb x}_{-i})}{P_{\Phi}(x_{i},{\pmb x}_{-i})}}&{=}&{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid{\pmb x}_{-i}) P_{\Phi}({\pmb x}_{-i})}{P_{\Phi}(x_{i}\mid{\pmb x}_{-i}) P_{\Phi}({\pmb x}_{-i})}}\\ &{=}&{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid{\pmb x}_{-i})}{P_{\Phi}(x_{i}\mid{\pmb x}_{-i})}.}\end{array}
$$ 
As for Gibbs sampling, we can use the observation that each variable $X_{i}$ is conditionally independent of the remaining variables in the network given its Markov blanket. Letting $\pmb U_{i}$ denote ${\mathrm{MB}}_{\mathcal{K}}(X_{i})$ , and $\pmb{u}_{i}=(\pmb{x}_{-i})\langle\pmb{U}_{i}\rangle$ , we have that: 

$$
\begin{array}{r c l}{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid\mathbf{\boldsymbol{x}}_{-i})}{P_{\Phi}(x_{i}\mid\mathbf{\boldsymbol{x}}_{-i})}}&{=}&{\displaystyle\frac{P_{\Phi}(x_{i}^{\prime}\mid\mathbf{\boldsymbol{u}}_{i})}{P_{\Phi}(x_{i}\mid\mathbf{\boldsymbol{u}}_{i})}.}\end{array}
$$ 
This expression can be computed locally and efficiently, based only on the local parameterization of $X_{i}$ and its Markov blanket (exercise 12.18). 

>  其中提案分布一般会选择为简单的分布，因此是容易计算的
>  同时第一个比值也是容易计算的，根据上述推导，可以知道该比值仅和 $X_i$ 和其 Markov blanket 有关

The similarity to the derivation of Gibbs sampling is not accidental. Indeed, it is not difcult to show that Gibbs sampling is simply a special case of Metropolis-Hastings, one with a particular choice of proposal distribution (exercise 12.19). 
>  Gibbs 采样可以视为 MH 的一个特例，即选择了特定的提案分布的 MH 采样就是 Gibbs 采样

The Metropolis-Hastings construction allows us to produce a Markov chain for an arbitrary stationary distribution. Importantly, however, we point out that the key theorem still requires that the constructed chain be regular. This property does not follow directly from the construction. In particular, the exclusive-or network of example 12.12 induces a nonregular Markov chain for any Metropolis-Hastings construction that uses a local proposal distribution — one that proposes changes to only a single variable at a time. In order to obtain a regular chain for this example, we would need a proposal distribution that allows simultaneous changes to both $X$ and $Y$ at a single step. 
>  MH 构造方法允许我们为任意的一个稳态分布构造 Markov chain，但注意构造出的 chain 需要是规范的

### 12.3.5 Using a Markov Chain 
So far, we have discussed methods for deﬁning Markov chains that induce the desired stationary distribution. Assume that we have constructed a chain that has a unique stationary distribution $\pi$ , which is the one from which we wish to sample. How do we use this chain to answer queries? A naive answer is straightforward. We run the chain using the algorithm of algorithm 12.5 until it converges to the stationary distribution (or close to it). We then collect a sample from $\pi$ . We repeat this process once for each particle we want to collect. The result is a data set $\mathcal{D}$ consisting of independent particles, each of which is sampled (approximately) from the stationary distribution $\pi$ . The analysis of section 12.1 is applicable to this setting, so we can provide tight bounds on the number of samples required to get estimators of a certain quality. Unfortunately, matters are not so straightforward, as we now discuss. 

#### 12.3.5.1 Mixing Time 

burn-in time 

Deﬁnition 12.6 A critical gap in this description of the MCMC algorithm is a speciﬁcation of the burn-in time $T-$ the number of steps we take until we collect a sample from the chain. Clearly, we want to wait until the state distribution is reasonably close to $\pi$ . More precisely, we want to ﬁnd a $T$ that guarantees that, regardless of our starting distribution $P^{(0)},\,\bar{P^{(T)}}$ is within some small

 $\epsilon$ of $\pi$ . In this context, we usually use variational distance (see section A.1.3.3) as our notion of

 “within $\epsilon$ .” 

Let $\mathcal{T}$ be a Markov chain. Let $T_{\epsilon}$ be the minimal $T$ such that, for any starting distribution $P^{(0)}$ , we have that: 

$$
D_{\mathrm{var}}(P^{(T)};\pi)\leq\epsilon.
$$ 

mixing time Then $T_{\epsilon}$ is called the $\epsilon$ - mixing time of $\mathcal{T}$ . 

In certain cases, the mixing time can be extremely long. This situation arises in chains where the state space has several distinct regions each of which is well connected, but where transitions between regions are low probability. In particular, we can estimate the extent to which the chain allows mixing using the following quantity: 

Deﬁnition 12.7 conductance 

t $\mathcal{T}$ be a Markov chain transition model and $\pi$ its stationary distribution. The conductance of $\mathcal{T}$ is deﬁned as follows: 

$$
\begin{array}{c c}{\displaystyle\operatorname*{min}}&{\displaystyle\frac{P (\mathcal{S}\cap\mathcal{S}^{c})}{\pi (\mathcal{S})},}\\ {0<\pi (\mathcal{S})\le1/2}\end{array}
$$ 

where $\pi (S)$ is the probability assigned by the stationary distribution to the set of states $s$ , $S^{c}=$ $V a l (X)-S,$ , and 

$$
P (\mathcal{S}\sim\mathcal{S}^{c})=\sum_{\pmb{x}\in\mathcal{S},\pmb{x}^{\prime}\in\mathcal{S}^{c}}\mathcal{T}(\pmb{x}\rightarrow\pmb{x}^{\prime}).
$$ 

Intuitively, $P (S\curvearrowright S^{c})$ is the total “bandwidth” for transitioning f m $s$ to its comp ment. In cases where the conductance is low, there is some set of states S where, once in S , it is very difcult to transition out of it. Figure 12.6 visualizes this type of situation, where the only tran on between $S=\{x^{1}, x^{2}, x^{3}\}$ and its complement is the dashed transition between $x^{2}$ and $x^{4}$ , which has a very low probability. In cases such as this, if we start in a state within S , the chain is likely to stay in S and to take a very long time before exploring other regions of the state space. Indeed, it is possible to provide both upper and lower bounds on the mixing rate of a Markov chain in terms of its conductance. 

In the context of Markov chains corresponding to graphical models, chains with low conduc- tance are most common in networks that have deterministic or highly skewed parameter iz ation. 

In fact, as we saw in example 12.12, networks with deterministic CPDs might even lead to reducible chains, where diferent regions are entirely disconnected. However, even when the dis- tribution is positive, we might still have regions that are connected only by very low-probability transitions. (See exercise 12.21.) 

There are methods for providing tight bounds on the $\epsilon$ -mixing time of a given Markov chain. These methods are based on an analysis of the transition matrix between the states in the Markov chain. Unfortunately, in the case of graphical models, an exhaustive enumeration of the exponentially many states is precisely what we wish to avoid. (If this enumeration were feasible, we would not have to resort to approximate inference techniques in the ﬁrst place.) Alternatively, there is a suite of indirect techniques that allow us to provide bounds on the mixing time for some general class of chains. However, the application of these methods to each new class of chains requires a separate and usually quite sophisticated mathematical analysis. As of yet, there is no such analysis for the chains that are useful in the setting of graphical models. A more common approach is to use a variety of heuristics to try to evaluate the extent to which a sample trajectory has “mixed.” See box 12. B for some further discussion. 

#### 12.3.5.2 Collecting Samples 
The burn-in time for a large Markov chain is often quite large. Thus, the naive algorithm described above has to execute a large number of sampling steps for every usable sample. However, a key observation is that, if $\bar{\mathbf{\mathit{x}}^{(t)}}$ is sampled from $\pi$ , then $\cdot_{\pmb{x}}(t{+}1)$ is also sampled from $\pi$ . Thus, once we have run the chain long enough that we are sampling from the stationary distribution (or a distribution close to it), we can continue generating samples from the same trajectory and obtain a large number of samples from the stationary distribution. 

More formally, assume that we use $\mathbfit{\bar{x}}^{(0)},\hat{\mathbfit{\alpha}},\mathbfit{x}^{(T)}$ as our burn-in phase, and then collect $M$ mples ${\mathcal{D}}=\{{\pmb x}[1],.\,.\,.\,,{\pmb x}[M]\}$ from the stationary dis simp might collect $M$ consecutive samples, so that ${\pmb x}[m]\,=\,{\pmb x}^{(T+m)}$ , for $m\,=\, 1,\ldots, M$ . If $\pmb{x}^{(T+1)}$ is sampled from $\pi$ , then so are all of the samples in $\mathcal{D}$ . Thus, if our chain has mixed by the time we collect our ﬁrst sample, then for any function $f$ , 

$$
\hat{\boldsymbol E}_{\mathcal{D}}(\boldsymbol f)=\frac{1}{M}\sum_{m=1}^{M}\boldsymbol f (\boldsymbol x[m],\boldsymbol e)
$$ 

estimator is an unbiased estimator for $E_{\pi (X)}[f (X, e)]$ . 

How good is this estimator? As we discussed in appendix A.2.1, the quality of an unbiased estimator is measured by its variance: the lower the variance, the higher the probability that the estimator is close to its mean. In theorem A.2, we showed an analysis of the variance of an estimator obtained from $M$ independent samples. Unfortunately, we cannot apply that analysis in this setting. The key problem, of course, is that consecutive samples from the same trajectory are correlated. Thus, we cannot expect the same performance as we would from $M$ independent samples from $\pi$ . More formally, the variance of the estimator is signiﬁcantly higher than that of an estimator generated by $M$ independent samples from $\pi$ , as discussed before. 

Example 12.15 

central limit theorem 

Theorem 12.6 

Consider the Gibbs chain for the deterministic exclusive-or network of example 12.12, and assume we compute, for a given run of the chain, the fraction of states in which $x^{1}$ holds in the last 100 states traversed by the chain. A chain started in the state $x^{1}, y^{0}$ would have that $100/100$ of the states have $x^{1}$ , whereas a chain started in the state $x^{0}, y^{1}$ would have that $0/100$ of the states have $x^{1}$ . Thus, the variance of the estimator is very high in this case. 

One can formalize this intuition by the following generalization of the central limit theorem that applies to samples collected from a Markov chain: 

Let T be a rkov chain a ${\overline {{X}} }[1],\ldots, X[M]$ a set of samples collected from $\mathcal{T}$ at its stationary distribution P . Then, since $M\longrightarrow\infty$ : 

$$
\left (\hat{E}_{\mathcal{D}}(f)-E_{X\sim P}[f (X)]\right)\longrightarrow\mathcal{N}\left (0;\sigma_{f}^{2}\right)
$$ 

where 

$$
\sigma_{f}^{2}={\mathbb{V}\! a r_{X\sim\mathcal{T}}}[f (X)]+2\sum_{\ell=1}^{\infty}C o\nu_{\mathcal{T}}[f (X[m]); f (X[m+\ell])]<\infty.
$$ 

autocovariance The terms in the summation are called autocovariance terms, since they measure the covariance between samples from the chain, taken at diferent lags. The stronger the correlations between diferent samples, the larger the autocovariance terms, the higher the variance of our estimator. This result is consistent with the behavior we discussed in example 12.12. 

We want to use theorem 12.6 in order to assess the quality of our estimator. In order to do so, we need to estimate the quantity $\sigma_{f}^{2}$ . We can estimate the variance from our empirical data using the standard estimator: 

$$
\pmb{V a r}_{\pmb{X}\sim\mathcal{T}}[f (\pmb{X})]\approx\frac{1}{M-1}\left[\sum_{m=1}^{M}\left (f (\pmb{X})-\hat{\pmb{E}}_{\mathcal{D}}(f)\right)^{2}\right].
$$ 

To estimate the autocovariance terms from the empirical data, we compute: 

$$
\mathbb{C}o v_{\mathcal{T}}[f (X[m]); f (X[m+\ell])]\approx\frac{1}{M-\ell}\sum_{m=1}^{M-\ell}(f (X[m]-\hat{\mathbf{E}}_{\mathcal{D}}(f))(f (X[m+\ell]-\hat{\mathbf{E}}_{\mathcal{D}}(f))).
$$ 

At ﬁrst glance, theorem 12.6 suggests that the variance of the estimate could be reduced if the chain is allowed a sufcient number of iterations between sample collections. Thus, having collected a particle ${\pmb x}^{(T)}$ , we can let the chain run for a while, and collect a second particle $\pmb{x}^{(T+d)}$ for some appropriate choice of $d$ . For $d$ large enough, ${\pmb x}^{(T)}$ and $\pmb{x}^{(T+d)}$ are only slightly correlated, reducing the correlation in the preceding theorem. 

However, this approach is suboptimal for various reasons. First, the time $d$ required for “forgetting” the correlation is clearly related to the mixing time of the chain. Thus, chains that are slow to mix initially also require larger $d$ in order to produce close-to-independent particles. Nevertheless, the samples do come from the correct distribution for any value of $d$ , and hence it is often better to compromise and use a shorter $d$ than it is to use a shorter burn-in time $T$ . This method thus allows us to collect a larger number of usable particles with fewer transitions of the Markov chain. Indeed, although the samples between ${\pmb x}^{(\bar{T})}$ and $\pmb{x}^{(T+d)}$ are not independent samples, there is no reason to discard them. That is, one can show that using all of the samples $\mathbf{\boldsymbol{x}}^{(T)},\mathbf{\boldsymbol{x}}^{(T+1)},\ldots,\mathbf{\boldsymbol{x}}^{(T+d)}$ produces a provably better estimator than using just the two samples ${\pmb x}^{(T)}$ and $\pmb{x}^{(T+d)}$ : our variance is always no higher if we use all of the samples we generated rather than a subset. Thus, the strategy of picking only a subset of the samples is useful primarily in settings where there is a signiﬁcant cost associated with using each sample (for example, the evaluation of $f$ is costly), so that we might want to reduce the overall number of particles used. 

Box 12. B — Skill: MCMC in Practice. A key question when using a Markov chain is evaluating the time required for the chain to “mix” — that is, approach the stationary distribution. As we discussed, no general-purpose theoretical analysis exists for the mixing time of graphical models. However, we can still hope to estimate the extent to which a sample trajectory has “forgotten” its origin. Recall that, as we discussed, the most common problem with mixing arises when the state space consists of several regions that are connected only by low-probability transitions. If we start the chain in a state in one of these regions, it is likely to spend some amount of time in that same region before transitioning to another region. Intuitively, the states sampled in the initial phase are clearly not from the stationary distribution, since they are strongly correlated with our initial state, which is arbitrary. However, later in the trajectory, we might reach a state where the current state is as likely to have originated in any initial state. In this case, we might consider the chain to have mixed. 

Diagnosing convergence of a Markov chain Monte Carlo method is a notoriously hard problem. The chain may appear to have converged simply by spending a large number of iterations in a particular mode due to low conductance between modes. However, there are approaches that can tell us if a chain has not converged. 

One technique is based directly on theorem 12.6. In particular, we can compute the ratio ρ ℓ of the estimated autocovariance in equation (12.28) to the estimated variance in equation (12.27). This ratio is known as the autocorrelation of lag ℓ ; it provides a normalized estimate of the extent to which the chain has mixed in $\ell$ steps. In practice, the autocorrelation should drop of exponentially with the length of the lag, and one way to diagnose a poorly mixing chain is to observe high autocorrelation at distant lags. Note, however, that the number of samples available for computing autocorrelation decreases with lag, leading to large variance in the autocorrelation estimates at large lags. 

A diferent technique uses the observation that multiple chains sampling the same distribution should, upon convergence, all yield similar estimates. In addition, estimates based on a complete set of samples collected from all of the chains should have variance comparable to variance in each of the chains. More formally, assume that $K$ separate chains are each run for $T+M$ steps starting from a diverse set of starting points. After discarding the ﬁrst $T$ samples from each chain, let $X_{k}[m]$ denote $a$ sample from chain $k$ after iteration $T+m$ . We can now compute the $B$ (between-chains) and $W$ (within-chain) variances: 

$$
\begin{array}{r c l}{\bar{f}_{k}}&{=}&{\displaystyle\frac{1}{M}\sum_{m=1}^{M}f (X_{k}[m])}\\ {\bar{f}}&{=}&{\displaystyle\frac{1}{K}\sum_{k=1}^{K}\bar{f}_{k}}\\ {B}&{=}&{\displaystyle\frac{M}{K-1}\sum_{k=1}^{K}(\bar{f}_{k}-\bar{f})^{2}}\\ {W}&{=}&{\displaystyle\frac{1}{K}\frac{1}{M-1}\sum_{k=1}^{K}\sum_{m=1}^{M}\left (f (X_{k}[m])-\bar{f}_{k}\right)^{2}.}\end{array}
$$ 

The expression $\begin{array}{r}{V=\frac{M-1}{M}W+\frac{1}{M}B}\end{array}$ can now be shown to overestimate the variance of our estimate of $f$ based on the collected samples. In the limit of $M\longrightarrow\infty$ →∞ , both $W$ and $V$ converge to the true variance of the estimate. One measure of disagreement between chains is given by $\begin{array}{r}{\hat{R}=\sqrt{\frac{V}{W}}}\end{array}$ q . If the chains have not all converged to the stationary distribution, this estimate will be high. If this value is close to 1, either the chains have all converged to the true distribution, or the starting points were not sufciently dispersed and all of the chains have converged to the same mode or a set of modes. We can use this strategy with multiple diferent functions $f$ in order to increase our conﬁdence that our chain has mixed. We can, for example, use indicator functions of various events, as well as more complex functions of multiple variables. 

Overall, although the strategy of using only a single chain produces more viable particles using lower computational cost, there are still signiﬁcant advantages to the multichain approach. First, by starting out in very diferent regions of the space, we are more likely to explore a more representative subset of states. Second, the use of multiple chains allows us to evaluate the extent to which our chains are mixing. Thus, to summarize, a good strategy for using a Markov chain in practice is a hybrid approach, where we run a small number of chains in parallel for a reasonably long time, using their behavior to evaluate mixing. After the burn-in phase, we then use the existence of multiple chains to estimate convergence. If mixing appears to have occurred, we can use each of our chains to generate multiple particles, remembering that the particles generated in this fashion are not independent. 

#### 12.3.5.3 Discussion 

MCMC methods have many advantages over other methods. Unlike the global approximate inference methods of the previous chapter, they can, at least in principle, get arbitrarily close to the true posterior. Unlike forward sampling methods, these methods do not degrade when the probability of the evidence is low, or when the posterior is very diferent from the prior. Furthermore, unlike forward sampling, MCMC methods apply to undirected models as well as to directed models. As such, they are an important component in the suite of approximate inference techniques. 

However, MCMC methods are not generally an out-of-the-box solution for dealing with in- ference in complex models. First, the application of MCMC methods leaves many options that need to be speciﬁed: the proposal distribution, the number of chains to run, the metrics for evaluating mixing, techniques for determining the delay between samples that would allow them to be considered independent, and more. Unfortunately, at this point, there is little theoretical analysis that can help answer these questions for the chains that are of interest to us. Thus, the application of Markov chains is more of an art than a science, and it often requires signiﬁcant experimentation and hand-tuning of parameters. 

Second, MCMC methods are only viable if the chain we are using mixes reasonably quickly. Unfortunately, many of the chains derived from real-world graphical models frequently have multimodal posterior distributions, with slow mixing between the modes. For such chains, the straightforward MCMC methods described in this chapter are unlikely to work. In such cases, diagnostics such as the ones described in box 12. B can be used to determine that the chain is not mixing, and better methods must then be applied. The key to improving the convergence of a Markov chain is to introduce transitions that take larger steps in the space, allowing the chain to move more rapidly between modes, and thereby to better explore the space. The best strategy is often to analyze the properties of the posterior landscape of interest, and to construct moves that are tailored for this speciﬁc space. (See, for example, exercise 12.23.) Fortunately, the ability to mix diferent reversible kernels within a single chain (as discussed in section 12.3.4) allows us to introduce a variety of long-range moves while still maintaining the same target posterior. 

simulated annealing 

temperature parameter 

In addition to the use of long-range steps that are speciﬁcally designed for particular (classes of) chains, there are also some general-purpose methods that try to achieve that goal. The block Gibbs approach (section 12.3.3) is an instance of this general class of methods. Another strategy uses the same ideas in simulated annealing to improve convergence of local search to a better optimum. Here, we can deﬁne an intermediate distribution parameterized by a temperature parameter $T;\, T$ : 

$$
\tilde{P}_{T}(X)\propto\exp\{-\frac{1}{T}\log\tilde{P}(X)\}.
$$ 

This distribution is similar to our original target distribution $\tilde{P}$ . At a low temperature of $T=1$ , this equation yields the original target distribution. But as the temperature increases, modes become broader and merge, reducing the multimodality of the distribution and increasing its mixing rate. We can now deﬁne various methods that use a combination of related chains running at diferent temperatures. At a high level, the higher-temperature chain can be viewed as proposing a step, which we can accept or reject using the acceptance probability of our true target distribution. (See section 12.7 for references to some of these more advanced methods.) In efect, these approaches use the higher-temperature chains to deﬁne a set of larger steps in the space, thereby providing a general-purpose method for achieving more rapid movement between multiple modes. However, this generality comes at the computational cost of running parallel 

![](images/f63b7910a75024eda568b4d85c52fa131cf6d8a470e0d03b4112f44b6eb23ebc.jpg) 
Figure 12.C.1 — Example of bugs model speciﬁcation (a) A simple hybrid Bayesian network. (b) A bugs deﬁnition of a probabilistic model over this network. 

chains; thus, if we can understand our speciﬁc posterior well enough to construct specialized operators that move between modes, that often provides a more efective solution. 

Box 12. C — Case Study: The bugs System. One of the main advantages of MCMC methods is their broad applicability to a very general class of networks. Not only do they apply (at least in principle) to any discrete network, regardless of its complexity, they also generalize fairly simply to continuous variables (see section 14.5.3). One very useful system that exploits this generality is the bugs system, developed by Thomas et al. (1992). This system provides a general-purpose language for representing a broad range of probabilistic models and uses MCMC to run inference over these models. 

BUGS system 

The bugs system provides a programming-language-based representation of a probabilistic model. The model deﬁnes a joint distribution over a set of random variables. Variables can be deﬁned as functions of each other; these functions can be deterministic functions, or stochastic functions utiliz- ing a rich set of predeﬁned distributions. For example, consider the simple Bayesian network shown in ﬁgure 12.C.1a, where $A, B, C$ are discrete and $X, Y$ are continuous. One possible probabilistic model can be written in bugs using the commands shown in ﬁgure 12.C.1b. This model deﬁnes: A to be a binary-valued variable, with $P (a^{1})\,=\, 0.3$ ; $B$ is a 3-valued variable that depends on A , whose CPT is deﬁned in the trix $P$ ; $X$ is a Gaussian random variable with mea $-1$ a precision (inverse variance) $0.25; Y$ is a conditional Gaussian whose mean depends on X and B and whose precision also depends on $X$ ; and $C$ is a logistic function of $4X+2$ . Even in this very simple example, we can see that the bugs language provides a rich language for encoding diferent families of functional and stochastic dependencies between variables. 

Given a probabilistic model deﬁned in this way, the bugs system can instantiate evidence for some of the variables (for example, by reading their values from a ﬁle) and then perform inference over the model by running various MCMC algorithms. The system analyzes the parametric form specifying the distribution of the diferent variables, and it selects an appropriate sampling algorithm to use. The user speciﬁes the number of sampling iterations to perform, and which variables are to be monitored — their values are to be stored during the MCMC iterations. We can then compute such values as the mean and standard deviation of these monitored variables. The system also provides various methods to help detect convergence of the MCMC runs (see also box 12. B). 

Overall, the bugs tool provides a general-purpose and highly ﬂexible framework for specifying and reasoning with probabilistic models. Its ability to provide such a high level of expression power rests on the generality of MCMC as an inference method, and its applicability to a very broad range of distributions (broader than any other inference method currently available). 

## 12.6 Summary 
This chapter presents a series of methods that attempt to approximate a joint distribution using a set of particles. We discussed three main classes of techniques for generating particles. 
>  本章讨论了使用一组粒子近似联合分布的一系列方法，以及讨论了三类生成粒子的方法

Importance sampling, and speciﬁcally likelihood weighting, generates particles by random sampling from a distribution. Because we cannot, in general, sample from the posterior distribution, we generate particles from a different distribution, called the proposal distribution, and then adjust their weights so as to get an unbiased estimator. The proposal distribution is a mutilated Bayesian network, and the samples are generated using forward sampling. Owing to the use of forward sampling, likelihood weighting applies only to directed graphical models. However, if we somehow choose a proposal distribution, the more general framework of importance sampling can also be applied to undirected graphical models (see exercise 12.9). 
>  首先是重要性采样，以及似然加权算法
>  它们从一个分布中随机采样以生成粒子，但这类方法中我们一般不能从后验中采样，而是从另一个分布 (提案分布) 中采样，然后调节它们的权重，以得到无偏的估计器
>  提案分布可以由残缺化的贝叶斯网络定义，采样时只需要从残缺化贝叶斯网络中进行前向采样即可
>  因为依赖于前向采样，似然加权算法仅适用于有向模型，但可以通过选择其他的提案分布，将重要性采样的思想应用到无向模型

Markov chain Monte Carlo techniques attempt to generate samples from the posterior distribution. We deﬁne a Markov chain, or a stochastic sampling process, whose stationary distribution (the asymptotic result of the sampling process) is the correct posterior distribution. The Metropolis-Hastings algorithm is a general scheme for specifying a Markov chain that induces a particular posterior distribution. We showed how we can use the Metropolis-Hastings algorithm for graphical models. We also showed a particular instantiation, called Gibbs sampling, that is speciﬁcally designed for graphical models. 
> MCMC 方法尝试从后验分布中生成样本
> 在这类方法中，我们定义一个 Markov chain，其稳态分布 (采样过程的渐进结果) 是真实的后验分布
> MH 算法是构造这样的 Markov chain 的一个通用方法，MH 算法可以用于图模型
> Gibbs 采样算法是一类特定的 Markov chain 构造方法，特别适用于图模型

Finally, search methods take a different approach, where particles are generated deterministically, trying to focus on instantiations that have high probability. Unlike random sampling methods, deterministic search methods do not provide an unbiased estimator for the target query. However, they do provide sound upper and lower bounds. When the probability distribution is highly diffuse, so that many particles are necessary to cover most of the probability mass, these bounds will be very loose, and generally without value. However, when a small number of instantiations account for a large fraction of the probability mass, deterministic search techniques can obtain very accurate results with a very small number of particles, often much more accurate results than sampling-based methods with a comparable number of particles. There are several applications that have this property. For example, when performing fault diagnosis (see, for example, box 5. A), where faults are very rare, it can be very efficient to enumerate all hypotheses where the system has up to $K$ faults. Because multiple faults are highly unlikely, even a small value of $K$ (2 or 3) will likely suffice to cover most of the probability mass — even the mass consistent with our evidence. Another example is speech recognition (box 6. B), where only very few trajectories through the HMM are likely. In both of these applications, deterministic search methods have been successfully applied. 

From a high level, it appears that sampling methods are the ultimate general-purpose inference algorithm. They are the only method that can be applied to arbitrary probabilistic models and that is guaranteed to achieve the correct results at the large sample limit. Indeed, when faced with a complex probabilistic model that involves continuous variables or a nonparametric model, there are often very few other choices available to us. While optimization-based methods, such as those of chapter 11, can sometimes be applied, the application often requires a nontrivial derivation, speciﬁc to the problem at hand. Moreover, these methods provide no accuracy guarantee. Conversely, it seems that sampling-based methods can be applied easily, of-the- shelf, to virtually any model. 
>  从高层次来看，采样方法似乎就是终极的通用目的的推断算法，采样方法是唯一的可以应用于任意图模型的方法，并且在足够大的样本规模下可以保证达到正确的结果
>  但当概率模型非常复杂并且涉及了连续变量或非参数模型时，可选的采样方法是很少的；而基于优化的方法，例如 CH11 所介绍的则有时可以应用，但它们一般需要针对问题的复杂的推导，并且不提供正确性保证
>  因此反而是基于采样的方法可以简单直接应用于几乎所有模型

This impression, however, is somewhat misleading. While it is true that sampling methods provide asymptotic guarantees, their performance for reasonable sample sizes is very difficult to predict. In practice, a naive application of sampling methods to a complex probabilistic model often fails dismally, in that the estimates obtained from any rea- sonable number of samples are highly inaccurate. Thus, the success of these methods depends heavily on the properties of the distribution, and on a careful design of our sampling algorithm. Moreover, there is little theoretic basis for this design, so that the process of getting sampling methods to work is largely a matter of intuition and intensive experimentation. 
>  但该印象是有误导性的，采样方法有渐进的正确性保证，但具体需要多大的样本集合才可以有理想的性能则很难预测
>  实际上，对一个复杂的概率模型直接应用采样方法通常会失败，因为从任何合理的样本数量中获得的估计值往往非常不准确
>  因此，这些方法的成功很大程度上取决于分布的性质以及我们采样算法的精心设计，但这种设计几乎没有理论基础，因此让采样方法奏效的过程在很大程度上依赖于直觉和密集的实验

Nevertheless, the methods described in this chapter do provide an important component in our arsenal of methods for inference in complex models. Moreover, they are often used very successfully in combination with exact or approximate global inference methods. Standard combinations include the use of global inference for providing more informed proposal distributions, and for manipulating collapsed particles. Such combinations are highly successful in practice, and they often lead to much better results than any of the two types of inference methods in isolation. 
>  尽管如此，本章中描述的方法为我们处理复杂模型的推理提供了一个重要的工具
>  此外，它们经常与精确或近似的全局推理方法结合使用，并且通常能取得很好的效果，常见的组合包括使用全局推理来提供更有信息量的提案分布，以及操作塌缩后的粒子
>  这样的组合在实践中非常成功，通常比单独使用这两种类型的推理方法能得到更好的结果

Having described these basic methods, we showed how they can be extended to the case of collapsed particles, which consist of an assignment to a subset of network variables, associated with a closed-form distribution over the remaining ones. The answer to a query is then a (possibly weighted) sum over the particles, of the answer to the query within each associated distribution. This approach approximates part of the inference task via particles, while performing exact inference on a subnetwork, which may be simpler than the original network. 

# 17 Parameter Estimation 
In this chapter, we discuss the problem of estimating parameters for a Bayesian network. We assume that the network structure data set $\mathcal{D}$ consists of fully observed instances of the network variables: D ${\mathcal D}=\{\xi[1],\dots,\xi[M]\}$ { } . This problem arises fairly often in practice, since numerical parameters are harder to elicit from human experts than structure is. It also plays a key role as a building block for both structure learning and learning from incomplete data. As we will see, despite the apparent simplicity of our task deﬁnition, there is surprisingly much to say about it. 
>  本章讨论为 BN 估计参数的问题，问题设定中，我们有一个固定结构的 BN，和一个抱恨了一组网络变量实例的数据集 $\mathcal D = \{\xi[1], \dots, \xi[M]\}$

As we will see, there are two main approaches to dealing with the parameter-estimation task: one based on maximum likelihood estimation , and the other using Bayesian approaches. For each of these approaches, we ﬁrst discuss the general principles, demonstrating their application in the simplest context: a Bayesian network with a single random variable. We then show how the structure of the distribution allows the techniques developed in this very simple case to generalize to arbitrary network structures. Finally, we show how to deal with parameter estimation in the context of structured CPDs. 
>  参数估计有两类主要方法：极大似然估计、贝叶斯方法
>  我们先讨论这些方法对于包含单个随机变量 BN 的应用，然后推广到任意网络结构

## 17.1 Maximum Likelihood Estimation 
In this section, we describe the basic principles behind maximum likelihood estimation. 
>  本节描述 MLE 的基本原则

### 17.1.1 The Thumbtack Example 
We start with what may be considered the simplest learning problem: parameter learning for a single variable. This is a classical Statistics 101 problem that illustrates some of the issues that we will encounter in more complex learning problems. Surprisingly, this simple problem already contains some interesting issues that we need to tackle. 
>  考虑为单个变量学习参数

Imagine that we have a thumbtack, and we conduct an experiment whereby we ﬂip the thumbtack in the air. It comes to land as either heads or tails, as in ﬁgure 17.1. We toss the thumbtack several times, obtaining a data set consisting of heads or tails outcomes. Based on this data set, we want to estimate the probability with which the next ﬂip will land heads or tails. In this description, we already made the implicit assumption that the thumbtack tosses are controlled by an (unknown) parameter $\theta$ , which describes the frequency of heads in thumbtack tosses. In addition, we also assume that the data instances are independent and identically distributed (IID). 
>  假设做随机抛掷图钉的试验，我们假设抛掷的结构由一个未知参数 $\theta$ 控制，同时假设数据集 (试验结果集合) 是独立同分布的

Assume that we toss the thumbtack 100 times, of which 35 come up heads. What is our estimate for $\theta\colon$ Our intuition suggests that the best estimate is 0 . 35 . Had $\theta$ been 0 . 1 , for example, our chances of seeing $35/100$ heads would have been much lower. In fact, we examined a similar situation in our discussion of sampling methods in section 12.1, where we used samples from a distribution to estimate the probability of a query. As we discussed, the central limit theorem shows that, as the number of coin tosses grows, it is increasingly unlikely to sample a sequence of IID thumbtack ﬂips where the fraction of tosses that come out heads is very far from $\theta$ . Thus, for sufficiently large $M$ , the fraction of heads among the tosses is a good estimate with high probability. 
>  中心极限定理表明，随着抛次数的增加，抽样到一系列独立同分布的图钉翻转序列，其中朝上的比例与 $\theta$ 相差很远的情况变得越来越不可能，因此，对于足够大的 $M$，抛掷中正面的比例 (频率) 有很高的概率是一个很好的估计

To formalize this intuition, assume that we have a set of thumbtack tosses $x[1],\cdot\cdot\cdot,x[M]$ that are IID, that is, each is sampled independently from the same distribution in which $X[m]$ is equal to $H$ (heads) or $T$ (tails) th probability $\theta$ and $1-\theta$ , respectively. Our task is to ﬁnd a good value for the parameter θ . As in many formulations of learning tasks, we deﬁne a hypothesis space $\Theta$ — a set of possibilities that we are considering — and an objective function that tells us how good dif nt hypotheses in the space are relative to our data set $\mathcal{D}$ . In this case, our hypothesis space Θ is the set of all param ers $\theta\in[0,1]$ . 
>  假设有一组 IID 样本 $x[1], \dots, x[M]$ ，其服从的分布为 $P(X[m] = H) = \theta$, $P(X[m] = T) = 1-\theta$，我们的目标是估计 $\theta$
>  对于学习任务，我们定义假设空间 $\Theta$ ( 我们所考虑的可能值 )，和目标函数 (用于判断假设空间的假设相对于数据集 $\mathcal D$ 的优异程度)
>  本例的假设空间为 $\theta \in [0, 1]$ 中的所有参数

How do we score different possible parameters θ ? As we discussed in section 16.3.1, one way of evaluating $\theta$ is by how well it predicts the data. In other words, if the data are likely given the parameter, the parameter is a good predictor. 
>  评估可能参数 $\theta$ 的准则就是该参数能否很好地预测数据，即数据有可能给出该参数，该参数就是好的估计

For example, suppose we observe the sequence of outcomes $H,T,T,H,H$ . If we know $\theta$ , we could assign a probability to observing this particular sequence. The probability of the ﬁrst toss is $P(X[1]=H)=\theta$ . The probability of the second toss is $P(X[2]=T\mid X[1]=H)$ ) , but our assume es are independent allows us to conclude that this probability is simply $P(X[2]=T)=1-\theta$ − . This is also the probability of the third outcome, and so on. Thus, the probability of the sequence is 

$$
P(\langle H,T,T,H,H\rangle:\theta)=\theta(1-\theta)(1-\theta)\theta\theta=\theta^{3}(1-\theta)^{2}.
$$ 
As expected, this probability depends on the particular value $\theta$ . As we consider diferent values of $\theta$ , we get diferent probabilities for the sequence. Thus, we can examine how the probability of the data changes as a function of $\theta$ . We thus deﬁne the likelihood function to be 

$$
L(\theta:\langle H,T,T,H,H\rangle)=P(\langle H,T,T,H,H\rangle:\theta)=\theta^{3}(1-\theta)^{2}.
$$ 
>  假如我们知道参数，我们可以为某个特定的观测序列计算其概率
>  因此，对于不同的参数选择，我们的数据集就有不同的概率，故我们可以用一个关于 $\theta$ 的函数衡量数据集的出现概率随 $\theta$ 的变化关系
>  该函数就是似然函数，它是关于参数 $\theta$ 和数据集观测的函数

Figure 17.2 plots the likelihood function in our example.

Clearly, parameter values with higher likelihood are more likely to generate the observed sequences. Thus, we can use the likelihood function as our measure of quality for different parameter values and select the parameter value that maximizes the likelihood; this value is called the maximum likelihood estimator (MLE) . By viewing ﬁgure 17.2 we see that $\hat{\theta}=0.6=3/5$ maximizes the likelihood for the sequence $H,T,T,H,H$ . 
>  显然，给定数据集，似然更高的参数 $\theta$ 更有可能生成该观测序列，因此，我们使用似然函数度量不同参数值的质量，选择能最大化似然的参数值作为估计值
>  该估计值就称为极大似然估计器

Can we ﬁnd the MLE for the general case? Assume that r data set $\mathcal{D}$ of observations contains $M[1]$ heads and $M[0]$ tails. We want to ﬁnd the value $\hat{\theta}$ that maximizes the likelihood of $\theta$ relative to $\mathcal{D}$ . The likelihood function in this case is: 
>  $\theta$ 相对于数据集 $\mathcal D$ 的似然函数定义如下

$$
L(\theta:{\mathcal{D}})=\theta^{M[1]}(1-\theta)^{M[0]}.
$$ 
It turns out that it is easier to maximize the logarithm of the likelihood function. In our case, the log-likelihood function is: 
>  对似然函数取对数即得到对数似然函数

$$
\ell(\theta:{\mathcal{D}})=M[1]\log\theta+M[0]\log(1-\theta).
$$

Note that the log-likelihood is monotonically related to the likelihood. Therefore, maximizing the one is equivalent to maximizing the other. However, the log-likelihood is more convenient to work with, since products are converted to summations. 
>  对数似然函数的单调性和似然函数相同，因此最大化对数似然函数等价于最大化似然函数

Differentiating the log-likelihood, setting the derivative to 0 , and solving for $\theta$ , we get that the maximum likelihood parameter, which we denote $\hat{\theta}$ , is 

$$
\hat{\theta}=\frac{M[1]}{M[1]+M[0]},\tag{17.1}
$$ 
as expected (see exercise 17.1). 

>  对对数似然求导，令导数为零，得到极大似然参数 $\hat \theta$

As we will see, the maximum likelihood approach has many advantages. However, the approach also has some limitations. For example, if we get 3 heads out of 10 tosses, the MLE estimate is 0.3 . We get the same estimate if we get 300 heads out of 1,000 tosses. Clearly, the two experiments are not equivalent. Our intuition is that, in the second experiment, we should be more conﬁdent of our estimate. Indeed, statistical estimation theory deals with conﬁdence intervals . These are common in news reports, for example, when describing the results of election polls, where we often hear that $"61\pm2$ percent” plan to vote for a certain candidate. The 2 percent is a conﬁdence interval — the poll is designed to select enough people so that the MLE estimate will be within 0 . 02 of the true parameter, with high probability. 
>  极大似然方法存在问题，例如我们可以从 10 次试验中 3 次 Head 的结果得到 MLE 估计值为 0.3，而从 100 次试验中 30 次 Head 的结果得到 MLE 估计值也是 0.3
>  显然这两次试验应该是不等价的，我们应该对第二次估计的结果更自信
>  因此，我们要考虑置信区间，置信区间为 2% 表示我们的 MLE (极大似然估计) 将有很高概率在真实参数的 0.02 相对误差左右

### 17.1.2 The Maximum Likelihood Principle 
We now generalize the discussion of maximum likelihood estimation to a broader range of learning problems. We then consider how to apply it to the task of learning the parameters of a Bayesian network. 
>  考虑将 MLE 应用到 BN

We start by describing the setting of the learning problem. Assume that we observe several IID samples of a set of random variables $\mathcal{X}$ from an unknown distribution $P^{*}(\mathcal{X})$ . We assume we know in advance the sample space we are dealing with (that is, which random variables, and what values they can take). However, we do not make any additional assumptions about $P^{*}$ . We denote the training set of samples as $\mathcal{D}$ and assume that it consists of $M$ instances of $\mathcal{X}$ : $\xi[1],\cdot\cdot\cdot\xi[M]$ . 
>  假设我们有从未知分布 $P^*(\mathcal X)$ 观测到的 IID 样本集合 $\mathcal D$ (训练集)，我们知道样本空间 (即每个随机变量的所有取值可能)

Next, we need to consider what exactly we want to learn. We assume that we are given a parametric model for which we wish to estimate parameters . Formally, a parametric model (also known as a parametric family; see section 8.2) is deﬁned by a function $P(\xi:\theta)$ , speciﬁed in terms of a set of parameters . Given a particular set of parameter values $\theta$ and an instance $\xi$ of $\mathcal{X}$ , the model ss robability (or density) to $\xi$ . Of course, we require that for each choice of parameters θ , $P(\xi:\theta)$ is a legal distribution; that is, it is nonnegative and 
>  假设给定一个参数化模型 (参数化族) ，即函数 $P(\xi:\pmb \theta)$
>  该参数化模型在给定特定样本和参数 $\pmb \theta$ 时可以评估该样本的概率
>  要求参数化模型定义合法的分布，即非负并且和为 1

$$
\sum_{\xi}P(\xi:\pmb \theta)=1.
$$ 
In general, for each model, not all parameter values are legal. Thus, we need to deﬁne the parameter space $\Theta$ , which is the set of allowable parameters. 
>  注意对于各个模型，不是所有参数值都合法，我们需要定义合法参数组成的参数空间 $\Theta$

To get some intuition, we consider concrete examples. The model we examined in section 17.1.1 has parameter space $\Theta_{t h u m b t a c k}=[0,1]$ and is deﬁned as 

$$
P_{t h u m b t a c k}(x:\theta)={\left\{\begin{array}{l l}{\theta}&{{\mathrm{if~}}x=H}\\ {1-\theta}&{{\mathrm{if~}}x=T.}\end{array}\right.}
$$ 
There are many additional examples. 

Example 17.1 multinomial 
Suppose that $X$ is a multinomial variable that can take values $x^{1},\cdot\cdot\cdot,x^{K}$ . The simplest representation of a multinomial distribution is as a vector $\pmb{\theta}\in\mathbb{R}^{K}$ , such that 

$$
P_{m u l t i n o m i a l}(x:\theta)=\theta_{k}{\mathrm{~}}i f\,x=x^{k}.
$$ 
The parameter space of this model is 

$$
\Theta_{m u l t i n o m i a l}=\left\{\pmb{\theta}\in[0,1]^{K}:\sum_{i}\theta_{i}=1\right\}.
$$ 
Example 17.2 Gaussian 
Suppose that $X$ is a continuous variable that can take values in the real line. A Gaussian model for $X$ is 

$$
P_{G a u s s i a n}(x:\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}},
$$ 
where $\theta=\langle\mu,\sigma\rangle$ . The parameter space for this model is $\Theta_{G a u s s i a n}=I\!\!R\times I\!\!R^{+}$ × . That is, we allow any real value of $\mu$ and any positive real value for $\sigma$ . 

The next step in maximum likelihood estimation is deﬁning the likelihood function . As we saw in our example, the likelihood function for a given choice of parameters $\theta$ is the probability (or density) the model assigns the training data: 

$$
L(\pmb\theta:\mathcal D)=\prod_{m}P(\xi[m]:\pmb\theta).
$$ 
>  接着我们定义似然函数
>  似然函数是关于参数 $\pmb \theta$ 的函数，它给出模型在 $\pmb \theta$ 下赋予训练数据集 $\mathcal D$ 的概率

In the thumbtack example, we saw that we can write the likelihood function using simpler terms. That is, using the counts $M[1]$ and $M[0]$ , we managed to have a compact description of the likelihood. More precisely, once we knew the values of $M[1]$ and $M[0]$ , we did not need to consider other aspects of training data (for example, the order of tosses). These are the sufficient statistics for the thumbtack learning problem. In a more general setting, a sufficient statistic is a function of the data that summarizes the relevant information for computing the likelihood. 
>  在图钉的例子中，我们看到可以通过更简单的术语来表示似然函数，也就是说，利用计数 $M[1]$ 和 $M[0]$，我们能够简洁地描述似然性。更确切地说，一旦我们知道 $M[1]$ 和 $M[0]$ 的值，就不需要考虑训练数据的其他方面（例如，抛掷的顺序）
>  图钉每次抛掷的结果 (Head or Tail) 就是图钉学习问题的充分统计量
>  在更一般的设定下，充分统计量是数据的一个函数，它总结了计算似然所需的全部相关信息

**Deﬁnition 17.1** sufficient statistics 
A function $\tau(\xi)$ from in of $\mathcal{X}$ to $I\!\!R^{\ell}$ (for some $\ell$ ) is $a$ sufficient statistic $i f,$ for any two data sets and and any $\theta\in\Theta$ , we have that 

$$
\sum_{\xi[m]\in{\mathcal D}}\tau(\xi[m])=\sum_{\xi^{\prime}[m]\in{\mathcal D}^{\prime}}\tau(\xi^{\prime}[m])\quad\Longrightarrow\quad L(\pmb \theta:{\mathcal D})=L(\pmb \theta:{\mathcal D}^{\prime}).
$$ 
We often refer to the tuple $\begin{array}{r}{\sum_{\xi[m]\in\mathcal{D}}\tau\big(\xi[m]\big)}\end{array}$ as the sufficient statistics of the data set $\mathcal{D}$ . 

>  定义
>  函数 $\tau(\xi): \mathcal X \mapsto R^{\mathscr l}$ 如果对于任意两个数据集 $\mathcal D, \mathcal D'$ 和任意参数空间内的参数 $\pmb \theta \in \Theta$ 满足当 $\tau(\xi)$ 在 $\mathcal D$ 上求和的结果等于 $\tau(\xi)$ 在 $\mathcal D'$ 上求和的结果相等时，$\pmb \theta$ 相对于 $\mathcal D$ 和 $\mathcal D'$ 的似然函数就相等，$\tau(\xi)$ 就是一个充分统计量
>  我们经常将 $\sum_{\xi[m]\in \mathcal D}\tau(\xi[m])$ 称为数据集 $\mathcal D$ 的充分统计量
>  (直观上说，在同一参数下，如果任意两个数据集的某个统计量相等时，可以保证二者的似然相等，则该统计量就是充分统计量)

Example 17.3 
Let us reconsider the multinomial model of example 17.1. It is easy to see that a sufficient statistic for th ata set is the tuple of counts $\langle M[1],.\,.\,.\,,M[K]\rangle$ , such that $M[k]$ is number of times the value $x^{k}$ appears in the training data. To obtain these counts by summing instance-level statistics, we deﬁne $\tau(x)$ to be a tuple of dimension $K$ , such that $\tau(x)$ has $a\;0$ in every position, except at the position $k$ for which $x=x^{k}$ , where its value is 1 : 

$$
\tau(x^{k})=(\overbrace{0,.\\,.\,0}^{k-1},1,\overbrace{0,.\,.\,.\,0}^{n-k}).
$$ 
Given the vector of counts, we can write the likelihood function as 

$$
L(\pmb\theta:\mathcal{D})=\prod_{k}\theta_{k}^{M[k]}.
$$ 
Example 17.4 
Let us reconsider the Gaussian model of example 17.2. In this case, it is less obvious how to construct sufficient statistics. However, if we expand the term $(x-\mu)^{2}$ in the exponent, we can rewrite the model as 

$$
P_{G a u s s i a n}(x:\mu,\sigma)=e^{-x^{2}\frac{1}{2\sigma^{2}}+x\frac{\mu}{\sigma^{2}}-\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log(2\pi)-\log(\sigma)}.
$$ 
We then see that the function 

$$
s_{G a u s s i a n}(x)=\langle1,x,x^{2}\rangle
$$

is a sufficient statistic for this model. Note that the ﬁrst element in the sufficient statistics tuple is “1,” which does not depend on the value of the data item; it serves, as in the multinomial case, to count the number of data items. 

We venture several comments about the likelihood function. First, we stress that the likelihood function measures the effect of the choice of parameters on the training data. Thus, for example, if we have two sets of parameters $\theta$ and $\theta^{\prime}$ , so that $L(\pmb\theta:\mathcal D)=L(\pmb\theta^{\prime}:\mathcal D)$ , then we cannot , given only the data, distinguish between the two choices of parameters. Moreover, if $L(\theta\,:$ $\mathcal{D})=L(\boldsymbol{\theta}^{\prime}:\mathcal{D})$ for all possible choices of $\mathcal{D}$ , then the two parameters are indistinguishable for any outcome. In such a situation, we can say in advance (that is, before seeing the data) that some distinctions cannot be resolved based on the data alone. 
>  对似然函数的评价：
>  首先，似然函数度量了参数选择对训练数据的影响，例如我们有两组参数 $\pmb \theta, \pmb \theta'$，给定数据集 $\mathcal D$ 时，如果 $L(\pmb \theta: \mathcal D ) = L(\pmb \theta' : \mathcal D)$，则认为这两组参数无法区分
>  如果对于 $\mathcal D$ 的所有可能选择，这两组参数都无法区分，这两组参数对于任意结果都无法区分
>  此时我们可以说参数间的一些区别不能仅仅基于数据就明确

Second, since we are maximizing the likelihood function, we usually want it to be continuous (and preferably smooth) function of $\theta$ . To ensure these properties, most of the theory of statistical estimation requires that $P(\xi:\theta)$ is a continuous and differentiable function of $\theta$ , and moreover that $\Theta$ is a continuous set of points (which is often assumed to be convex). 
>  其次，因为需要最大化似然函数，我们一般要求似然函数连续 (最好光滑)
>  为了保证这些性质，大多数统计估计理论要求概率 $P(\xi: \pmb \theta)$ 是关于 $\pmb \theta$ 的连续可微函数，并且 $\Theta$ 是一个连续的点集 (并且常常假设为凸的)

Once we have deﬁned the likelihood function, we can use maximum likelihood estimation to choose the parameter values. Formally, we state this principle as follows. 

**Maximum Likelihood Estimation:** 
Given a data set $\mathcal{D}$ , choose parameters $\hat{\pmb\theta}$ that satisfy 

$$
L({\hat{\pmb \theta}}:{\mathcal{D}})=\operatorname*{max}_{\pmb \theta\in\Theta}L(\pmb \theta:{\mathcal{D}}).
$$ 
>  极大似然估计
>  给定数据集 $\mathcal D$，选择参数 $\hat {\pmb \theta}$ 使得似然函数可以最大

Example 17.5 
Consider estimating the parameters of the multinomial distribution of example 17.3. As one might guess, the maximum likelihood is attained when 

$$
{\hat{\theta}}_{k}={\frac{M[k]}{M}}
$$ 
(see exercise 17.2). That is, the probability of each value of $X$ corresponds to its frequency in the training data. 

Example 17.6 empirical mean, variance 
Consider estimating the parameters of a Gaussian distribution of example 17.4. It turns out that the maximum is attained when $\mu$ and $\sigma$ correspond to the empirical mean and variance of the training data: 

$$
\begin{array}{r c l}{{\hat{\mu}}}&{{=}}&{{\displaystyle\frac{1}{M}\sum_{m}x[m]}}\\ {{}}&{{}}&{{}}\\ {{\hat{\sigma}}}&{{=}}&{{\displaystyle\sqrt{\frac{1}{M}\sum_{m}(x[m]-\hat{\mu})^{2}}}}\end{array}
$$ 

## 17.2 MLE for Bayesian Networks 
We now move to the more general problem of estimating parameters for a Bayesian network. It turns out that the structure of the Bayesian network allows us to reduce the parameter estimation problem to a set of unrelated problems, each of which can be addressed using the techniques of the previous section. We begin by considering a simple example to clarify our intuition, and then generalize to more complicated networks. 
>  考虑为 BN 估计参数
>  我们可以利用 BN 的结构将参数估计问题化简为一组不相关的问题，每个问题都可以使用 (针对单个变量的) MLE 估计解决

### 17.2.1 A Simple Example 
The simplest example of a nontrivial network structure is a network consisting of two binary variables, say $X$ and $Y$ , with an arc $X\rightarrow Y$ . (A network without such an arc trivially reduces to the cases we already discussed.) 
>  考虑最简单的网络 $X\rightarrow Y$，二者都是二元变量

As for a single parameter, our goal in maximum likelihood estimation is to maximize the likelihood (or log-likelihood) function. In this case, our network is parameterized by a parameter vector $\theta$ , which deﬁnes the set of parameters for all the CPDs in the network. In this example, our parameterization would consist of the following parameters: $\theta_{x^{1}}$ , and $\theta_{x^{0}}$ specify the probability of the two values of $X$ ; $\theta_{y^{1}|x^{1}}$ , and $\theta_{y^{0}\mid x^{1}}$ specify the probability of $Y$ given that $X=x^{1}$ ; and $\theta_{y^{1}|x^{0}}$ , and $\theta_{y^{0}\mid x^{0}}$ describe the probability of $Y$ given that $X=x^{0}$ . For brevity, we also use the shorthand $\theta_{Y\mid x^{0}}$ to refer to the set $\{\theta_{y^{1}|x^{0}},\theta_{y^{0}|x^{0}}\}$ , and $\theta_{Y\mid X}$ to refer to $\pmb{\theta}_{Y\mid x^{1}}\cup\pmb{\theta}_{Y\mid x^{0}}$ . 
>  BN 由参数向量 $\pmb \theta$ 参数化，$\pmb \theta$ 包含了所有 CPDs 的参数
>  本例中，$\pmb \theta$ 包括了： $\theta_{x^1}, \theta_{x^0}$ 用于指定 $X$ 的两个值的概率；$\theta_{y^1 \mid x^1}, \theta_{y^0\mid x^1}$ 用于指定 $Y$ 在给定 $X = x^1$ 时两个值的概率 $\theta_{y^1\mid x^0}, \theta_{y^1\mid x^0}$ 用于指定 $Y$ 在 $X = x^0$ 时两个值的概率
>  我们用 $\theta_{Y\mid x^0}$ 表示集合 $\{\theta_{y^1\mid x^0}, \theta_{y^0\mid x^0}\}$，$\theta_{Y\mid x^1}$ 同理

In this exam, each training instance is a tuple $\langle x[m],y[m]\rangle$ that describes a particular assignment to X and Y . Our likelihood function is: 

$$
L(\pmb\theta:\mathcal D)=\prod_{m=1}^{M}P(x[m],y[m]:\pmb\theta).
$$ 
Our network model speciﬁes that $P(X,Y:\theta)$ has a product form. Thus, we can write 

$$
L(\pmb\theta:\mathcal D)=\prod_{m}P(x[m]:\pmb\theta)P(y[m]\mid x[m]:\pmb\theta).
$$

Exchanging the order of multiplication, we can equivalently write this term as 

$$
L(\pmb\theta:\mathcal D)=\left(\prod_{m}{P(x[m]:\pmb\theta)}\right)\left(\prod_{m}{P(y[m]\mid x[m]:\pmb\theta)}\right).
$$ 
That is, the likelihood decomposes into two separate terms, one for each variable. Moreover, each of these terms is a local likelihood function that measures how well the variable is predicted given its parents. 

>  数据集 $\mathcal D$ 中的每个实例 $\langle x[m], y[m]\rangle$ 都是对 $X, Y$ 的一次具体赋值，因此似然函数就写为所有实例的概率的乘积
>  每个实例的概率可以借由网络结构化简为两个 CPD 的乘积，因此似然函数可以进一步改写为两个分离的项的乘积，每个项针对一个变量
>  注意到其中的每一项都是一个局部的似然函数，描述了其对应变量在给定它的父变量时在 $\pmb \theta$ 被预测得多好

Now consider the two individual terms. Clearly, each one depends only on the parameters for that variable’s CPD. Thus, the ﬁrst is $\textstyle\prod_{m}{P(x[m]\,:\,\theta_{X})}$ . This term is identical to the multinomial likelihood function we discussed earlier. The second term is more interesting, since we can decompose it even further: 

$$
\begin{align}
&{{\prod_{m}P(y[m]\mid x[m]:\pmb \theta_{Y|X})}}\\ 
&{=}{\prod_{m:x[m]=x^{0}}P(y[m]\mid x[m]:\pmb \theta_{Y|X})\cdot\prod_{m:x[m]=x^{1}}P(y[m]\mid x[m]:\pmb \theta_{Y|X})}\\ 
&{=}{\prod_{m:x[m]=x^{0}}P(y[m]\mid x[m]:\pmb \theta_{Y|x^{0}})\cdot\prod_{m:x[m]=x^{1}}P(y[m]\mid x[m]:\pmb \theta_{Y|x^{1}}).}
\end{align}
$$

Thus, in this example, the likelihood function decomposes into a product of terms, one for each group of parameters in $\theta$ . This property is called the decomposability of the likelihood function. 

>  显然，每个项仅涉及该变量的 CPD，因此仅依赖于该 CPD 的对应参数
>  本例中，关于 $X$ 的项就仅依赖于 $\pmb \theta_X$，最大化 $\prod_m P(x[m] : \pmb \theta)$ 等价于最大化 $\prod_m P(x[m]:\pmb \theta_X)$
>  我们将关于 $Y$ 的项进一步分解，按照 $X$ 的取值将它划分为新的两项的乘积，其中每一项仅依赖于特定的一组参数 $\pmb \theta_{Y\mid x^0}$ 和 $\pmb \theta_{Y\mid x^1}$，我们仅需要根据这一子项优化其对应参数即可 (因为这些参数和其他的项无关)
>  该性质称为似然函数的可分解性

We can do one more simpliﬁcation by using the notion of sufficient statistics. Let us consider one term in this expression: 

$$
\prod_{m:x[m]=x^{0}}{ P}(y[m]\mid x[m]:\pmb \theta_{Y\mid x^{0}}).\tag{17.2}
$$ 
Each of the individual terms $P(y[m]\mid x[m]:\theta_{Y\mid x^{0}})$ can take one of two values, depending on the value of $y[m]$ . If $y[m]=y^{1}$ , it is equal to $\theta_{y^{1}|x^{0}}$ . If $y[m]=y^{0}$ , it is equal to $\theta_{y^{0}|x^{0}}$ . How many cases of each type do we get? First, we restrict attention only to those data cases where $x[m]=x^{0}$ . These, in turn, partition into the two categories. Thus, we get $\theta_{y^{1}|x^{0}}$ in those data cases where $x[m]\,=\,x^{0}$ and $y[m]\,=\,y^{1}$ ; we use $M[x^{0},y^{1}]$ to denote their number. We get $\theta_{y^{0}|x^{0}}$ in those data cases where $x[m]=x^{0}$ and $y[m]=x^{0}$ , and use $M[x^{0},y^{0}]$ to denote their number. Thus, the term in equation (17.2) is equal to: 

$$
\prod_{m:x[m]=x^{0}}{ P}(y[m]\mid x[m]:\theta_{Y|x^{0}})\;\;\;=\;\;\;\theta_{y^{1}|x^{0}}^{M[x^{0},y^{1}]}\cdot\theta_{y^{0}|x^{0}}^{M[x^{0},y^{0}]}.
$$ 
>  再从中选择一项 ($X$ 取 $x^0$ 时的情况)，我们根据 $y[m]$ 的取值将它进一步划分，$y[m]$ 取 $y^1$ 时，概率就是 $\theta_{y^1\mid x^0}$，$y[m]$ 取 $y^0$ 时，概率就是 $\theta_{y^0\mid x^0}$
>  我们用 $M[x^0, y^1]$ 表示 $x[m] = x^0,y[m] =  y^1$ 出现的次数，将 (17.2) 写为如上形式

Based on our discussion of the multinomial likelihood in example 17.5, we know that we maximize $\theta_{Y\mid x^{0}}$ by setting: 

$$
\theta_{y^{1}|x^{0}}=\frac{M[x^{0},y^{1}]}{M[x^{0},y^{1}]+M[x^{0},y^{0}]}=\frac{M[x^{0},y^{1}]}{M[x^{0}]},
$$ 
and similarly for $\theta_{y^{0}|x^{0}}$ . Thus, we can ﬁnd the maximum likelihood parameters in this CPD by simply counting how many times each of the possible assignments of $X$ and $Y$ appears in the training data. 

>  因此，为了最大化该项，根据多项式似然函数的推导结果，$\theta_{y^1 \mid x^0}$ 就等于 $M[x^0, y^1]/ (M[x^0, y^1] + M[x^0, y^0])$，即 $M[x^0, y^1]/M[x^0]$，其他的参数也类似
>  因此，在该 CPD 中，最大化似然的参数可以通过计数其对应的特定赋值的出现次数得到

It turns out that these counts of the various assignments for some set of variables are useful in general. We therefore deﬁne: 
>  显然对某一组变量的各个赋值出现次数进行计数的方法具有一般性，故我们首先定义：

**Definition 17.2**
Let $Z$ be some set of random variables, and $_z$ be some instantiation to these random variables. Let $\mathcal{D}$ be a data set. We deﬁne $M[z]$ to be the number of entries in $\mathcal{D}$ that have $Z[m]=z$ 

$$
M[\pmb z]=\sum_{m}1\{\pmb Z[m]=\pmb z\}.\tag{17.3}
$$

>  定义
>  $\pmb Z$ 为一组随机变量， $\pmb z$ 为其某个实例，$\mathcal D$ 为数据集
>  定义 $M[\pmb z]$ 为数据集 $\mathcal D$ 中满足 $\pmb Z[m] = \pmb z$ 的数据样本个数

### 17.2.2 Global Likelihood Decomposition 
As we can expect, the arguments we used for deriving the MLE of $\theta_{Y\mid x^{0}}$ apply for the parameters of other CPDs in that example and indeed for other networks as well. We now develop, in several steps, the formal machinery for proving such properties in Bayesian networks. 
>  上一小节的对参数 $\theta_{Y\mid x^0}$ 的 MLE 的推导在 BN 中具有普适性

We start by examining the likelihood function of a Bayesian network. Suppose we want to learn the parameters for a Bayesian network with structure $\mathcal{G}$ and parameters $\pmb \theta$ . This means that we agree in advance on the type of CPDs we want to learn (say table-CPDs, or noisy-ors). 
>  假定我们需要学习网络 $\mathcal G$ 的参数 $\pmb \theta$，网络的结构已知

As we discussed, we are also given a data set $\mathcal{D}$ consisting of samples $\xi[1],\cdot\cdot\cdot,\xi[M]$ . Writing the likelihood, and repeating the steps we performed in our example, we get 

$$
\begin{array}{r c l}{{{\cal L}(\pmb\theta:{\cal D})}}&{{=}}&{{\displaystyle\prod_{m}{ P}_{\mathcal G}(\xi[m]:\pmb\theta)}}\\ {{}}&{{=}}&{{\displaystyle\prod_{m}\prod_{i}{ P}(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m]:\pmb\theta)}}\\ {{}}&{{=}}&{{\displaystyle\prod_{i}\left[\displaystyle\prod_{m}{ P}(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m]:\pmb\theta)\right].}}\end{array}
$$

>  我们具有从网络中采样得到的数据集 $\mathcal D$，包含 $M$ 个样本
>  参数 $\pmb \theta$ 相对于 $\mathcal D$ 的似然如上，我们将每个样本 $\xi[m]$ 的概率都通过 BN 的网络结构展开

Note that each of the terms in the square brackets refers to the conditional likelihood of a particular variable given its parents in the network. 
>  注意到方括号中的每一项都是特定变量 $X_i$ 在给定其网络中的父变量时在 $\pmb \theta$ 下的条件似然，整体的似然函数是所有 $X_i$ 的似然的乘积

We use $\pmb \theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ to denote the subset of parameters that determines $P(X_{i}\mid\mathrm{Pa}_{X_{i}})$ in our model. Then, we can write 

$$
L(\pmb\theta:\mathcal{D})=\prod_{i}L_{i}(\pmb\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}:\mathcal{D}),
$$ 
where the local likelihood function for $X_{i}$ is: 

$$
L_{i}(\pmb{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}}:\mathcal{D})=\prod_{m}{P}(x_{i}[m]\mid\mathrm{pa}_{X_{i}}[m]:\pmb{\theta}_{X_{i}\mid\mathrm{Pa}_{X_{i}}}).
$$ 

>  我们用 $\pmb \theta_{X_i\mid \text{Pa}_{X_i}}$ 表示决定了条件概率 $P(X_i \mid \text{Pa}_{X_i})$ 的参数子集，也就是 $X_i$ 的似然仅由 $\pmb \theta_{X_i \mid \text{Pa}_{X_i}}$ 决定
>  我们将似然函数写为各个 $X_i$ 各自似然函数的乘积，其中 $X_i$ 的似然函数是关于 $\pmb \theta_{X_i \mid \text{Pa}_{X_i}}$ 的函数

This form is particularly useful when the parameter sets $\pmb \theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ are disjoint . 
That is, each CPD is parameterized by a separate set of parameters that do not overlap. This assumption is quite natural in all our examples so far. (Although, as we will see in section 17.5, parameter sharing can be handy in many domains.)
>  该分解形式在各个 $\pmb \theta_{X_i \mid \text{Pa}_{X_i}}$ 都不相交时十分有用，也就是每个 CPD 由单独的一组参数集合参数化，CPD 之间的参数不相交
>  目前为止，该假设在我们的例子中自然成立

 **This analysis shows that the likelihood decomposes as a product of independent terms, one for each CPD in the network. This important property is called the global decomposition of the likelihood function.** 
 >  该分析告诉我们似然函数可以分解为多个独立项的乘积，每个独立项仅和网络的一个 CPD 有关
 >  该性质称为似然函数的全局分解

We can now immediately derive the following result: 

**Proposition 17.1** 
Let $\mathcal{D}$ be a complete data set for $X_{1},\dots,X_{n},$ , let $\mathcal{G}$ be a network structure over these variables, and suppose that the parameters $\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}$ are disjoint from $\theta_{X_{j}|\mathrm{Pa}_{X_{j}}}$ for all j $j\neq i$ ̸ . Let $\hat{\pmb{\theta}}_{X_{i}|\mathrm{Pa}_{X_{i}}}$ be the parameters that maximize $L_{i}(\theta_{X_{i}|\mathrm{Pa}_{X_{i}}}:\mathcal{D})$ . Then, $\hat{\pmb\theta}=\langle\hat{\pmb\theta}_{X_{1}|\mathrm{Pa}_{1}},.\,.\,.\,,\hat{\pmb\theta}_{X_{n}|\mathrm{Pa}_{n}}\rangle$ ⟨ ⟩ maximizes $L(\theta:{\mathcal{D}})$ . 
>  命题
>  $\mathcal D$ 为 $X_1, \dots, X_n$ 的完整数据集，$\mathcal G$ 为这些变量上的网络结构
>  假设对于所有 $j\ne i$，参数集合 $\pmb \theta_{X_i\mid \text{Pa}_{X_i}}$ 和参数集合 $\pmb \theta_{X_j\mid \text{Pa}_{X_j}}$ 不相交
>  令 $\pmb {\hat \theta}_{{X_n\mid \text{Pa}_{n}}}$ 为最大化局部似然函数 $L_i(\pmb \theta_{X_i \mid \text{Pa}_{X_i}}:\mathcal D)$ 的参数，则 $\hat{\pmb\theta}=\langle\hat{\pmb\theta}_{X_{1}|\mathrm{Pa}_{1}},.\,.\,.\,,\hat{\pmb\theta}_{X_{n}|\mathrm{Pa}_{n}}\rangle$ 就是最大化整体似然函数 $L(\pmb \theta:\mathcal D)$ 的参数

In other words, we can maximize each local likelihood function independently of rest of the network, and then combine the solutions to get an MLE solution. This decomposition of the global problem to independent subproblems allows us to devise efficient solutions to the MLE problem. Moreover, this decomposition is an immediate consequence of the network structure and does not depend on any particular choice of parameterization for the CPDs. 
>  因此，我们可以独立于网络的其他部分，最大化每个局部似然函数，然后将它们各自的解结合得到整体的 MLE 解
>  这便将全局问题分解为了独立的子问题
>  该分解是网络结构的直接结果，并不依赖于 CPD 的参数化选择

### 17.2.3 Table-CPDs 
Based on the preceding discussion, we know that the likelihood of a Bayesian network decomposes into local terms that depend on the parameterization of CPDs. The choice of parameters determines how we can maximize each of the local likelihood functions. We now consider what is perhaps the simplest parameterization of the CPD: a table-CPD . 
>  我们知道 BN 的似然可以分解为多个仅依赖于 CPD 的参数化的局部项
>  CPD 的参数化决定了我们如何最大化局部似然函数
>  本节考虑最简单的 CPD 参数化 - Table CPD

Suppose we have a variable $X$ with parents $\pmb U$ . If we represent that CPD $P(X\mid \pmb U)$ as a table, then we will have a parameter $\theta_{x|u}$ for each combination of $x\in V a l(X)$ and $u\in V a l(U)$ . In this case, we can rewrite the local likelihood function as follows: 

$$
\begin{array}{r c l}{{{\cal L}_{X}(\pmb{\theta}_{X|\pmb U}:\mathcal{D})}}&{{=}}&{{\displaystyle\prod_{m}\theta_{x[m]|\pmb{u}[m]}}}\\ {{}}&{{}}&{{}}\\ {{}}&{{=}}&{{\displaystyle\prod_{\pmb{u}\in V a l(\pmb{U})}\left[\displaystyle\prod_{x\in V a l(X)}\theta_{x|\pmb{u}}^{M[\pmb{u},x]}\right],}}\end{array}\tag{17.4}
$$ 
where $M[{\boldsymbol{\mathbf{\mathit{u}}}},x]$ is the number of times $\xi[m]=x$ and ${\pmb u}[m]={\pmb u}$ in $\mathcal{D}$ . That is, we grouped together all the occurrences of $\theta_{x|\pmb u}$ in the product over all instances. This provides a further local decomposition of the likelihood function. 

>  考虑变量 $X$，其父变量为 $\pmb U$，我们将 $P(X\mid \pmb U)$ 用表格表示，则对于每个组合 $x\in Val(X), u \in Val(\pmb U)$，我们都有一个参数 $\theta_{x\mid \pmb u}$ 定义其概率
>  故我们将 $X$ 相对于数据集的似然函数写为 (17.4)
>  其中 $M[\pmb u, x]$ 为 $\xi[m] = x$ 且 $\pmb u[m] = \pmb u$ 在 $\mathcal D$ 中出现的次数，也就是我们通过 $\pmb U$ 的取值 $\pmb u$ 将局部似然函数进行进一步划分，将 $\pmb u$ 相关的 $\theta_{x\mid \pmb u}$ 聚集在一起

We need to maximize this term under the constraints that, for each choice of value for the parents $\pmb U$ , the conditional probability is legal, that is: 

$$
\sum\theta_{x\mid \pmb u}=1\quad{\mathrm{~for~all~}}\pmb u.
$$ 
These constraints imply that the choice of value for $\theta_{x|\pmb u}$ can impact choice of value for $\theta_{x^{\prime}|\pmb u}$ . However, the choice of parameters given different values $\mathbfit{u}$ of U are independent of each other. Thus, we can maximize each of the terms in square brackets in equation (17.4) independently. 

>  注意到该最大化优化问题还有一个约束，就是每个 $\pmb U$ 的取值 $\pmb u$ 都需要定义一个合法的 CPD，即所有的 $\theta_{x\mid \pmb u}$ 和为 1
>  同时注意到每个 $\pmb u$ 相关的参数仅和自己有关，因此可以针对各个 $\pmb u$ 分别优化

We can thus further decompose the local likelihood function for a tabular CPD into a product of simple likelihood functions. Each of these likelihood functions is a multinomial likelihood, of the type that we examined in example 17.3. The counts in the data for the different outcomes $x$ are simply $\{M[{\pmb u},{ x}]:{ x}\in V a l({ X})\}$ . We can then immediately use the maximum likelihood estimation for multinomial likelihood of example 17.5 and see that the MLE parameters are 

$$
{\hat{\theta}}_{x\mid \pmb u}={\frac{M[{\pmb u},x]}{M[{\pmb u}]}},\tag{17.5}
$$ 
where we use the fact that $\begin{array}{r}{M[\pmb{u}]=\sum_{x}M[\pmb{u},x]}\end{array}$ . 

>  因此我们将表格类 CPD 的局部似然函数进一步分解为了多个简单似然函数的乘积，其中每个简单似然函数都是一个多项式似然 ($\prod_{x\in Val(X)}\theta_{x\mid \pmb u}^{M[x, \pmb u]}$)
>  因此，$\theta_{x\mid \pmb u}$ 的 MLE 就是 $(x, \pmb u)$ 在数据集中出现的次数在所有 $\pmb u$ 出现中所占的比例是多少 ($M[\pmb u] = \sum_x M[\pmb u, x]$)

This simple formula reveals a key challenge when estimating parameters for a Bayesian networks. Note that the number of data points used to estimate the parameter ${\hat{\theta}}_{x\mid \pmb u}$ is $M[\pmb{u}]$ . Data points that do not agree with the parent assignment $\mathbfit{u}$ play no role in this computation. As the number of parents $U$ grows, the number of different parent assignments grows exponentially. Therefore, the number of data instances that we expect to have for a single parent assignment shrinks exponentially. This phenomenon is called data fragmentation , since the data set is partitioned into a large number of small subsets. Intuitively, when we have a very small number of data instances from which we estimate a parameter, the estimates we get can be very noisy (this intuition is formalized in section 17.6), leading to overﬁtting . We are also more likely to get a large number of zeros in the distribution, which can lead to very poor performance. **Our inability to estimate parameters reliably as the dimensionality of the parent set grows is one of the key limiting factors in learning Bayesian networks from data.** This problem is even more severe when the variables can take on a large number of values, for example, in text applications. 
>  可以看到和父变量赋值 $\pmb u$ 不相关的数据点对于参数 $\theta_{x\mid \pmb u}$ 的优化也不相关
>  而随着父变量 $\pmb U$ 的数量增长，其不同赋值 $\pmb u$ 的数量会指数增长，而每个赋值 $\pmb u$ 都会带来需要优化的参数，相应地，我们期望有对应的满足 $\pmb U = \pmb u$ 的数据点/样本进行统计/优化，故总体需要的数据点的数量实际也是指数增长
>  该现象称为数据划分，因为数据集被划分为了大量的小子集，直观上，我们只能有非常少的相关数据实例用于估计一个参数，因此得到的估计会带有大量噪声，导致过拟合
>  同时我们也很可能得到大量的零参数 (因为没有对应样本出现)，导致性能降低
>  BN 中，随着父变量集合维度增长，我们便难以评估参数，这是限制 BN 从数据中学习的关键因素，如果变量的取值空间也非常大，该问题会更加严重

Box 17.A — Concept: Naive Bayes Classiﬁer. One of the basic tasks of learning is classiﬁcation . In this task, our goal is build a classiﬁer — a procedure that assigns instances into two or more categories, for example, deciding whether an email message is junk mail that should be discarded or a relevant message that should be presented to the user. In the usual setting, we are given a training example of instances from each category, where instances are represented by various features. In our email classiﬁcation example, a message might be analyzed by multiple features: its length, the type of attachments it contains, the domain of the sender, whether that sender appears in the user’s address book, whether a particular word appears in the subject, and so on. 

One general approach to this problem, which is referred to as Bayesian classiﬁer , is to learn a probability distribution of the features of instances of each class. In the language of probabilistic models, we use the random variables $X$ to represent the instance, and the random variable $C$ to represent the category of the instance. The distribution $P(X\mid C)$ is the probability of a particular combination of features given the category. Using Bayes rule, we have that 

$$
P(C\mid X)\propto P(C)P(X\mid C).
$$ 
Thus, if we have a good model of how instances of each category behave (that is, of $P(X\mid C)),$ we can combine it with our prior estimate for the frequency of each category (that is, $P(C))$ to estimate the posterior probability of each of the categories (that is, $P(C\mid X))$ . We can then decide either to predict the most likely category or to perform a more complex decision based on the strength of likelihood of each option. For example, to reduce the number of erroneously removed messages, a junk-mail ﬁlter might remove email messages only when the probability that it is junk mail is higher than a strict threshold. 

This Bayesian classiﬁcation approach is quite intuitive. Loosely speaking, it states that to classify objects successfully, we need to recognize the characteristics of objects of each category. Then, we can classify a new object by considering whether it matches the characteristic of each of the classes. More formally, we use the language of probability to describe each category, assigning higher probability to objects that are typical for the category and low probability to ones that are not. 

The main hurdle in constructing a Bayesian classiﬁer is the question of representation of the multivariate distribution $p(X\mid C)$ . The naive Bayes classiﬁer is one where we use the simplest representation we can think of. That is, we assume that each feature $X_{i}$ is independent of all the other features given the class variable $C$ . That is, 

$$
P(X\mid C)=\prod_{i}{\cal P}(X_{i}\mid C).
$$

Learning the distribution $P(C)P(X\mid C)$ is thus reduced to learning the parameters in the naive Bayes structure, with the category variable C rendering all other features as conditionally independent of each other. 

As can be expected, learning this classiﬁer is a straightforward application of the parameter estimation that we consider in this chapter. Moreover, classifying new examples requires simple computation, evaluating $\textstyle P(c)\prod_{i}P(x_{i}\mid c)$ for each category $c$ . 

Although this simple classiﬁer is often dismissed as naive, in practice it is often surprisingly efective. From a training perspective, this classiﬁer is quite robust, since in most applications, even with relatively few training examples, we can learn the parameters of conditional distribution $P(X_{i}\mid C)$ . However, one might argue that robust learning does not compensate for oversimpliﬁed independence assumption. Indeed, the strong independence assumption usually results in poor representation of the distribution of instances. However, errors in estimating the probability of an instance do not necessarily lead to classiﬁcation errors. For classiﬁcation, we are interested in the relative size of the conditional distribution of the instances given diferent categories. The ranking of diferent labels may not be that sensitive to errors in estimating the actual probability of the instance. Empirically, one often ﬁnds that the naive Bayes classiﬁer correctly classiﬁes an example to the right category, yet its posterior probability is very skewed and quite far from the correct distribution. 

In practice, the naive Bayes classiﬁer is often a good baseline classiﬁer to try before considering more complex solutions. It is easy to implement, it is robust, and it can handle different choices of descriptions of instances (for example, box 17.E). 

### 17.2.4 Gaussian Bayesian Networks\* 
Our discussion until now has focused on learning discrete-state Bayesian networks with multi-nomial parameters. However, the concepts we have developed in this section carry through to a wide variety of other types of Bayesian networks. In particular, the global decomposition properties we proved for a Bayesian network apply, without any change, to any other type of CPD. That is, if the data are complete, the learning problem reduces to a set of local learning problems, one for each variable. The main diference is in applying the maximum likelihood estimation process to a CPD of a diferent type: how we deﬁne the sufficient statistics, and how we compute the maximum likelihood estimate from them. In this section, we demonstrate how MLE principles can be applied in the setting of linear Gaussian Bayesian networks. In section 17.2.5 we provide a general procedure for CPDs in the exponential family. 

Consider a variable $X$ with parents $U=\{U_{1},.\,.\,.\,,U_{k}\}$ with a linear Gaussian CPD: 

$$
P(X\mid\mathbf{\boldsymbol{u}})=\mathcal{N}\left(\beta_{0}+\beta_{1}u_{1}+\ldots,\beta_{k}u_{k};\sigma^{2}\right).
$$ 
Our task is to learn the parameters $\theta_{X|U}\;=\;\langle\beta_{0},.\,.\,.\,,\beta_{k},\sigma\rangle$ . To ﬁnd the MLE values of these parameters, we need to differentiate the likelihood and solve the equations that deﬁne a stationary point. As usual, it will be easier to work with the log-likelihood function. Using the deﬁnition of the Gaussian distribution, we have that 

$$
\begin{array}{l l l}{\lefteqn{\ell_{X}\big(\pmb{\theta}_{X|U}:\mathcal{D}\big)=\log L_{X}\big(\pmb{\theta}_{X|U}:\mathcal{D}\big)}}\\ &{=}&{\sum_{m}\left[-\frac{1}{2}\log(2\pi\sigma^{2})-\frac{1}{2}\frac{1}{\sigma^{2}}\left(\beta_{0}+\beta_{1}u_{1}[m]+\ldots+\beta_{k}u_{k}[m]-x[m]\right)^{2}\right].}\end{array}
$$ 

We start by considering the gradient of the log-likelihood with respect to $\beta_{0}$ : 

$$
\begin{array}{c c}{\displaystyle\frac{\partial}{\partial\beta_{0}}\ell_{X}(\pmb{\theta}_{X|U}:\mathcal{D})=\sum_{m}-\frac{1}{\sigma^{2}}\left(\beta_{0}+\beta_{1}u_{1}[m]+\ldots+\beta_{k}u_{k}[m]-x[m]\right)}\\ {=}&{\displaystyle-\frac{1}{\sigma^{2}}\left(M\beta_{0}+\beta_{1}\sum_{m}u_{1}[m]+\ldots+\beta_{k}\sum_{m}u_{k}[m]-\sum_{m}x[m]\right).}\end{array}
$$ 

Equating this gradient to 0 , and multiplying both sides with $\frac{\sigma^{2}}{M}$ , we get the equation 

$$
{\frac{1}{M}}\sum_{m}x[m]=\beta_{0}+\beta_{1}{\frac{1}{M}}\sum_{m}u_{1}[m]+\ldots+\beta_{k}{\frac{1}{M}}\sum_{m}u_{k}[m].
$$ 
Each of the terms is the average value of one of the variables in the data. We use the notation 

$$
E_{\mathcal{D}}[X]=\frac{1}{M}\sum_{m}x[m]
$$ 
to denote this expectation. Using this notation, we see that we get the following equation: 

$$
\begin{array}{r}{{\pmb E}_{\mathcal{D}}[X]=\beta_{0}+\beta_{1}{\pmb E}_{\mathcal{D}}[U_{1}]+.\,.+\beta_{k}{\pmb E}_{\mathcal{D}}[U_{k}].}\end{array}
$$ 
Recall that theorem 7.3 speciﬁes the mean of a linear Gaussian variable $X$ in terms of the means of its parents $U_{1},\dots,U_{k}$ , using an expression that has precisely this form. Thus, equation (17.6) tells us that the MLE parameters should be such that the mean of $X$ in the data is consistent with the predicted mean of $X$ according to the parameters. 

Next, consider the gradient with respect to one of the parameters $\beta_{i}$ . Using similar arithmetic manipulations, we see that the equation $\begin{array}{r}{0=\frac{\partial}{\partial\beta_{i}}\ell_{X}(\bar{\pmb\theta_{X|U}}:\mathcal{D})}\end{array}$ can be formulated as: 

$$
\begin{array}{r}{E_{\mathcal{D}}[X\cdot U_{i}]=\beta_{0}E_{\mathcal{D}}[U_{i}]+\beta_{1}E_{\mathcal{D}}[U_{1}\cdot U_{i}]+\ldots+\beta_{k}E_{\mathcal{D}}[U_{k}\cdot U_{i}].}\end{array}
$$ 
At this stage, we have $k+1$ linear equations with $k+1$ unknowns, and we can use standard linear algebra techniques for solving for the value of $\beta_{0},\beta_{1},.\cdot\cdot,\beta_{k}$ . We can get additional intuition, however, by doing additional manipulation of equation (17.7). Recall that the covariance $\mathbf{C}o v[X;Y]=E[X\cdot Y]-E[X]\cdot E[Y]$ · − · . Thus, if we subtract $\pmb{E}_{\mathcal{D}}[X]\cdot\pmb{E}_{\mathcal{D}}[U_{i}]$ D · D from the left-hand side of equation (17.7), we would get the empirical covariance of X and $U_{i}$ . Using equation (17.6), we have that this term can also be written as: 

$$
\begin{array}{r}{\pmb{E}_{\mathcal{D}}[X]\cdot\pmb{E}_{\mathcal{D}}[U_{i}]=\beta_{0}\pmb{E}_{\mathcal{D}}[U_{i}]+\beta_{1}\pmb{E}_{\mathcal{D}}[U_{1}]\cdot\pmb{E}_{\mathcal{D}}[U_{i}]+.+.+\beta_{k}\pmb{E}_{\mathcal{D}}[U_{k}]\cdot\pmb{E}_{\mathcal{D}}[U_{i}].}\end{array}
$$ 
Subtracting this equation from equation (17.7), we get: 

$$
\begin{array}{r c l}{{{\cal E}_{\mathcal{D}}[X\cdot U_{i}]-{\cal E}_{\mathcal{D}}[X]\cdot{\pmb E}_{\mathcal{D}}[U_{i}]}}&{{=}}&{{\beta_{1}\left({\pmb E}_{\mathcal{D}}[U_{1}\cdot U_{i}]-{\pmb E}_{\mathcal{D}}[U_{1}]\cdot{\pmb E}_{\mathcal{D}}[U_{i}]\right)+\ldots+}}\\ {{}}&{{}}&{{\beta_{k}\left({\pmb E}_{\mathcal{D}}[U_{k}\cdot U_{i}]-{\pmb E}_{\mathcal{D}}[U_{k}]\cdot{\pmb E}_{\mathcal{D}}[U_{i}]\right).}}\end{array}
$$ 
Using $\mathbf{\it{C}}o v_{\mathcal{D}}[X;U_{i}]$ to denote the observed covariance of $X$ and $U_{i}$ in the data, we get: 

$$
\begin{array}{r}{\pmb{C}o v_{\mathcal{D}}[X;U_{i}]=\beta_{1}\pmb{C}o v_{\mathcal{D}}[U_{1};U_{i}]+.\,.+\beta_{k}\pmb{C}o v_{\mathcal{D}}[U_{k};U_{i}].}\end{array}
$$ 
In other words, the observed covariance of $X$ with $U_{i}$ should be the one predicted by theorem 7.3 given the parameters and the observed covariances between the parents of $X$ . 

Finally, we need to ﬁnd the value of the $\sigma^{2}$ parameter. Taking the derivative of the likelihood and equating to 0 , we get an equation that, after suitable reformulation, can be written as 

$$
\sigma^{2}={\bf C}o v_{\mathcal{D}}[X;X]-\sum_{i}\sum_{j}\beta_{i}\beta_{j}{\bf C}o v_{\mathcal{D}}[U_{i};U_{j}]
$$ 
(see exercise 17.4). Again, we see that the MLE estimate has to match the constraints implied by theorem 7.3. 

e glob picture that emerges is as follows. To estimate $P(X\mid U)$ , we estimate the means of X and U and covariance matrix of $\{X\}\cup U$ from the vector of means and covariance matrix deﬁnes a joint Gaussian distribution over { $\{X\}\cup U$ } ∪ . (In fact, this is the MLE estimate of the joint Gaussian; see exercise 17.5.) We then solve for the (unique) linear Gaussian that matches the joint Gaussian with these parameters. For this purpose, we can use the formulas provided by theorem 7.4. While these equations seem somewhat complex, they are merely describing the solution to a system of linear equations. 

This discussion also identiﬁes the sufficient statistics we need to collect to estimate linear Gaussians. These are the un the f $\textstyle\sum_{m}x[m]$ and $\textstyle\sum_{m}u_{i}[m]$ , and the interaction terms of the form $\textstyle\sum_{m}x[m]\cdot u_{i}[m]$ P · and P $\textstyle\sum_{m}u_{i}[m]\cdot u_{j}[m]$ · ] . From these, we can estimate the mean and covariance matrix of the joint distribution. 

Box 17.B — Concept: Nonparametric Models. The discussion in this chapter has focused on estimating parameters for speciﬁc parametric models of CPDs: multinomials and linear Gaussians. However, a theory of maximum likelihood and Bayesian estimation exists for a wide variety of other parametric models. Moreover, in recent years, there has been a growing interest in the use of nonparametric Bayesian estimation methods, where a (conditional) distribution is not deﬁned to be in some particular parametric class with a ﬁxed number of parameters, but rather the complexity of the representation is allowed to grow as we get more data instances. In the case of discrete variables, any CPD can be described as a table, albeit perhaps a very large one; thus a nonparametric method is less essential (although see section 19.5.2.2 for a very useful example of a nonparametric method in the discrete case). In the case of continuous variables, we do not have a “universal” parametric distribution. While Gaussians are often the default, many distributions are not well ﬁt by them, and it is often difcult to determine which parametric family (if any) will be appropriate for a given variable. In such cases, nonparametric methods ofer a useful substitute. In such methods, we use the data points themselves as the basis for a probability distribution. Many nonparametric methods have been developed; we describe one simple variant that serves to illustrate this type of approach. 

Suppose we want to learn the distribution $P(X\mid U)$ from data. A reasonable assumption is that the CPD is smooth. Thus, if we observe $x,u$ in a training sample, it should increase the probability of seeing similar values of $X$ for similar values of $U$ . More precisely, we increase the density of $p(X=x+\epsilon\mid U=u+\delta)$ for small values of ϵ and $\delta$ . 

One simple approach that captures this intuition is the use of kernel density estimation (also known as Parzen windows ). The idea is fairly simple: given the data $\mathcal{D}$ , we estimate a “local” joint density ${\tilde{p}}_{X}(X,U)$ by spreading out density around each example $x[m],\pmb{u}[m]$ . Formally, we write 

$$
\tilde{p}_{X}(\boldsymbol{x},\boldsymbol{u})=\frac{1}{M}\sum_{m}K(\boldsymbol{x},\boldsymbol{u};\boldsymbol{x}[m],\boldsymbol{u}[m],\alpha),
$$ 
where $K$ is $^a$ kernel density function and $\alpha$ is a parameter (or vector of parameters) controlling $K$ . A common choice of kernel is $^a$ simple round Gaussian distribution with radius $\alpha$ around $x[m],\pmb{u}[m]$ : 

$$
K(\boldsymbol{x},\boldsymbol{\mathbf{\mathit{u}}};\boldsymbol{x}[m],\boldsymbol{\mathbf{\mathit{u}}}[m],\boldsymbol{\mathbf{\mathit{a}}})=\mathcal{N}\left(\left(\begin{array}{c}{\boldsymbol{x}[m]}\\ {\boldsymbol{\mathbf{\mathit{u}}}[m]}\end{array}\right);\alpha^{2}I\right),
$$ 
where $I$ is the identity matrix and $\alpha$ is the width of the window. Of course, many other choices for kernel function are possible; in fact, if $K$ deﬁnes a probability measure (nonnegative and integrates to ${\mathit{l}}),$ then $\tilde{p}_{X}(x,\pmb{u})$ is also a probability measure. Usually we choose kernel functions that are local, in that they put most of the mass in the vicinity of their argument. For such kernels, the resulting density $\tilde{p}_{X}(x,\pmb{u})$ will have high mass in regions where we have seen many data instances $(x[m],\mathbf{u}[m])$ and low mass in regions where we have seen none. 

We can now reformulate this local joint distribution to produce a conditional distribution: 

$$
p(x\mid\mathbf{\boldsymbol{u}})=\frac{\sum_{m}K(x,\mathbf{\boldsymbol{u}};x[m],\mathbf{\boldsymbol{u}}[m],\alpha)}{\sum_{m}K(\mathbf{\boldsymbol{u}};\mathbf{\boldsymbol{u}}[m],\alpha)}
$$ 
where $K(\pmb{u};\pmb{u}[m],\alpha)$ is $K(x,\pmb{u};x[m],\pmb{u}[m],\alpha)$ marginalized over $x$ . 

Note that this learning procedure estimates virtually no parameters: the CPD is derived directly from the training instances. The only free parameter is $\alpha$ , which is the width of the window. Importantly, this parameter cannot be estimated using maximum likelihood: The α that maximizes the likelihood of the training set is $\alpha=0$ , which gives maximum density to the training instances themselves. This, of course, will simply memorize the training instances without any generalization. Thus, this parameter is generally selected using cross-validation. 

The learned CPD here is essentially the list of training instances, which has both advantages and disadvantages. On the positive side, the estimates are very ﬂexible and tailor themselves to the observations; indeed, as we get more training data, we can produce arbitrarily expressive representations of our joint density. On the negative side, there is no “compression” of the original data, which has both computational and statistical ramiﬁcations. Computationally, when there are many training samples the learned CPDs can become unwieldy. Statistically, this learning procedure makes no attempt to generalize beyond the data instances that we have seen. In high-dimensional spaces with limited data, most points in the space will be “far” from data instances, and therefore the estimated density will tend to be quite poor in most parts of the space. Thus, this approach is primarily useful in cases where we have a large number of training instances relative to the dimension of the space. 

Finally, while these approaches help us avoid parametric assumptions on the learning side, we are left with the question of how to avoid them on the inference side. As we saw, most inference procedures are geared to working with parametric representations, mostly Gaussians. Thus, when performing inference with nonparametric CPDs, we must generally either use parametric approximations, or resort to sampling. 

### 17.2.5 Maximum Likelihood Estimation as M-Projection\*
The MLE principle is a general one, in that it gives a recipe how to construct estimators for diferent statistical models (for example, multinomials and Gaussians). As we have seen, for simple examples the resulting estimators are quite intuitive. However, the same principle can be applied in a much broader range of parametric models. Indeed, as we now show, we have already discussed the framework that forms the basis for this generalization. 

In section 8.5, we deﬁned the notion of projection : ﬁnding the distribution, within a speciﬁed class, that is closest to a given target distribution. Parameter estimation is similar in the sense that we select a distribution from a given class — all of those that can be described by the model — that is “closest” to our data. Indeed, we can show that maximum likelihood estimation aims to ﬁnd the distribution that is “closest” to the empirical distribution $\hat{P}_{\mathcal{D}}$ (see equation (16.4)). 

We start by rewriting the likelihood function in terms of the empirical distribution. 

Proposition 17.2 

Let $\mathcal{D}$ be a data set, then 

$$
\log L(\pmb\theta:\mathcal D)=M\cdot\pmb E_{\hat{P}_{\mathcal D}}[\log P(\mathcal X:\pmb\theta)].
$$ 

Proof We rewrite the likelihood by combining all identical instances in our training set and then writing the likelihood in terms of the empirical probability of each entry in our joint distribution: 

$$
\begin{align}
\log L(\pmb\theta:\mathcal{D}) &= \sum_{m} \log P(\xi[m] : \pmb\theta) \\
&= \sum_{\xi} \left[ \sum_{m} \pmb{I}\{\xi[m] = \xi\} \right] \log P(\xi : \pmb\theta) \\
&= \sum_{\xi} M \cdot \hat{P}_{\mathcal{D}}(\xi) \log P(\xi : \pmb\theta) \\
&= M \cdot \mathbb{E}_{\hat{P}_{\mathcal{D}}} [\log P(\mathcal{X} : \pmb\theta)].
\end{align}
$$

We can now apply proposition 16.1 to the empirical distribution to conclude that 

$$
\ell(\pmb\theta:{\mathcal D})=M\left(H_{\hat{P}_{\mathcal D}}(\mathcal X)-{\cal D}(\hat{P}_{\mathcal D}(\mathcal X)\|P(\mathcal X:\pmb\theta))\right).
$$ 
From this result, we immediately derive the following relationship between MLE and M-projections. 

Theorem 17.1 The MLE θ in a parametric family relative to a data set $\mathcal{D}$ is the $M\cdot$ -projection of $\hat{P}_{\mathcal{D}}$ onto the D parametric family 

$$
\hat{\pmb\theta}=\arg\operatorname*{min}_{\pmb\theta\in\Theta}\pmb D(\hat{P}_{\mathcal{D}}\|P_{\pmb\theta}).
$$ 
We see that MLE ﬁnds the distribution $P(\mathcal{X}:\theta)$ that is the M-projection of $\hat{P}_{\mathcal{D}}$ onto the set D of distributions representable in our parametric family. 

This result allows us to call upon our detailed analysis of M-projections in order to generalize MLE to other parametric classes in the exponential family. In particular, in section 8.5.2, we discussed the general notion of sufficient statistics and showed that the M-projection of a distribution $P$ into a class of distributions $\mathcal{Q}$ was deﬁned by e parameters $\theta$ such that $E_{Q_{\theta}}[\tau(\mathcal{X})]=E_{P}[\tau(\mathcal{X})]$ X X . In our setting, we seek the parameters θ whose expected sufficient statistics match those in $\hat{P}_{\mathcal{D}}$ , that is, the sufficient statistics in $\mathcal{D}$ . 

If our CPDs are in an exponential family where the mapping ess from parameters to sufficient statistics is invertible, we can simply take the sufficient statistic vector from $\hat{P}_{\mathcal{D}}$ , and invert this D mapping to produce the MLE. Indeed, this process is precisely the one that gave rise to our MLE for multinomials and for linear Gaussians, as described earlier. However, the same process can be applied to many other classes of distributions in the exponential family. 

This analysis provides us with a notion of sufficient statistics $\tau(\mathcal{X})$ and a clearly deﬁned path to deriving MLE parameters for any distribution in the exponential family. Somewhat more surprisingly, it turns out that a parametric family has a sufficient statistic only $i f$ it is in the exponential family. 

## 17.3 Bayesian Parameter Estimation 
### 17.3.1 The Thumbtack Example Revisited 
Although the MLE approach seems plausible, it can be overly simplistic in many cases. Assume again that we perform the thumbtack experiment and get 3 heads out of 10. It may be quite reasonable to conclude that the parameter $\theta$ is 0.3 . But what if we do the same experiment with a standard coin, and we also get 3 heads? We would be much less likely to jump to the conclusion that the parameter of the coin is 0.3 . Why? Because we have a lot more experience with tossing coins, so we have a lot more prior knowledge about their behavior. Note that we do not want our prior knowledge to be an absolute guide, but rather a reasonable starting assumption that allows us to counterbalance our current set of 10 tosses, under the assumption that they may not be typical. However, if we observe 1,000,000 tosses of the coin, of which 300,000 came out heads, then we may be more willing to conclude that this is a trick coin, one whose parameter is closer to 0.3 . 
>  MLE 方法在许多情况下会过度简单 (因为仅仅考虑频率)
>  MLE 方法不便于将先验指示纳入考虑，注意我们不会让先验知识成为绝对的引导，而是成为一个合理的起始推测，在该推测下，我们可能可以认为该试验结果的 MLE 估计的可信度不高/不低等等

Maximum likelihood allows us to make neither of these distinctions: between a thumbtack and a coin, and between 10 tosses and 1,000,000 tosses of the coin. There is, however, another approach, the one recommended by Bayesian statistics. 
>  本节介绍贝叶斯统计

#### 17.3.1.1 Joint Probabilistic Model 
In this approach, we encode our prior knowledge about $\theta$ with a probability distribution; this distribution represents how likely we are a priori to believe the different choices of parameters. Once we quantify our knowledge (or lack thereof) about possible values of $\theta$ , we can create a joint distribution over the parameter $\theta$ and the data cases that we are about to observe $X[1],\cdot\cdot\cdot,X[M]$ . This joint distribution captures our assumptions about the experiment. 
>  贝叶斯统计中，我们将关于 $\theta$ 的先验知识编码为一个概率分布
>  该分布表示了我们预先相信不同的参数选择的可能性
>  一旦我们量化了我们关于 $\theta$ 可能取值的知识，我们就可以为参数 $\theta$ 和我们观测到的数据实例 $X[1], \dots, X[M]$ 创建一个联合分布，该联合分布反映了我们对试验的假设

Let us reconsider these assumptions. Recall that we assumed that tosses are independent of each other. Note, however, that this assumption was made when $\theta$ was ﬁxed. If we do not know $\theta$ , then the tosses are not marginally independent: Each toss tells us something about the parameter $\theta$ , and thereby about the probability of the next toss. However, once $\theta$ is known, we cannot learn about the outcome of one toss from observing the results of others. Thus, we assume that the tosses are conditionally independent given $\theta$ . We can describe these assumptions using the probabilistic model of ﬁgure 17.3. 
>  我们在抛掷图钉的例子中，假设抛掷之间都是相互独立的，该假设建立在参数 $\theta$ 固定的基础上
>  如果我们不知道 $\theta$，则这些抛掷并不是边际独立的，因为每次抛掷都会告诉我们关于参数 $\theta$ 的一些信息，因此进而告诉我们关于下一次抛掷的概率的一些信息
>  但注意一旦 $\theta$ 已知，我们就不能通过观察其他抛掷的结果学习到关于某一次抛掷的结果的信息 ($\theta$ 已知，抛掷的结果就完全由 $\theta$ 决定)
>  因此，我们实际假设的是抛掷在给定 $\theta$ 时条件独立
>  我们使用概率模型描述该假设，如 figure 17.3 所示

![[pics/PGM-Fig17.3.png]]

Having determined the model structure, it remains to specify the local probability models in this network. We begin by considering the probability $P(X[m]\mid\theta)$ . Clearly, 

$$
P(x[m]\mid\theta)={\left\{\begin{array}{l l}{\theta}&{{\mathrm{if~}}x[m]=x^{1}}\\ {1-\theta}&{{\mathrm{if~}}x[m]=x^{0}.}\end{array}\right.}
$$

Note that since we now treat $\theta$ as a random variable, we use the conditioning bar, instead of $P(x[m]:\theta)$ . 

>  决定好模型结构以后，我们接着为该网络指定局部概率模型
>  考虑某个样本的分布 $P(X[m]\mid \theta)$，显然在给定 $\theta$ 时，它满足由 $\theta$ 决定的二项分布
>  注意此时我们将 $\theta$ 视为了随机变量，因此我们使用的是条件符号 $|$，而不是冒号 $:$

To ﬁnish the description of the joint distribution, we need to describe $P(\theta)$ . This is our prior distribution over the value of $\theta$ . In our case, this is a continuous density over the interval $[0,1]$ . Before we discuss particular choices for this distribution, let us consider how we use it. 
>  要完成这一联合分布，我们需要进一步描述 $P(\theta)$，也就是 $\theta$ 从属的先验分布
>  本例中，它应该是一个在 $[0, 1]$ 上的连续的密度
>  在考虑对它的特定选择之前，我们首先考虑如何使用它

The network structure implies that the joint distribution of a particular data set and $\theta$ factorizes as 

$$
\begin{array}{l l l}{P(x[1],\ldots,x[M],\theta)}&{=}&{P(x[1],\ldots,x[M]\mid\theta)P(\theta)}\\ &{=}&{P(\theta)\displaystyle\prod_{m=1}^{M}P(x[m]\mid\theta)}\\ &{=}&{P(\theta)\theta^{M[1]}(1-\theta)^{M[0]},}\end{array}
$$

where $M[1]$ is the number of heads in the data, and $M[0]$ is the number of tails. Note that the expression $P(x[1],.\,.\,.\,,x[M]\mid\theta)$ is simply the likelihood function $L(\theta:{\mathcal{D}})$ . 

>  根据网络结构，我们很容易将所有样本和 $\theta$ 的联合分布进行分解，进而将 $\prod_{m=1}^M P(x[m]\mid \theta)$ 根据数据集的实际取值替换为 $\theta^{M[1]}(1-\theta)^{M[0]}$
>  注意到后验概率 $P(x[1], \dots, x[M]\mid \theta)$ 实际上就是似然函数 $L(\theta : \mathcal D)$
>  因此，联合概率实际上就是数据集似然乘上先验概率

This network speciﬁes a joint probability model over parameters and data. There are several ways in which we can use this network. Most obviously, we can take an observed data set $\mathcal{D}$ of $M$ outcomes, and use it to instantiate the values of $x[1],\ldots,x[M]$ ; we can then compute the posterior distribution over $\theta$ : 

$$
P(\theta\mid x[1],\ldots,x[M])={\frac{P(x[1],\ldots,x[M]\mid\theta)P(\theta)}{P(x[1],\ldots,x[M])}}.
$$

In this posterior, the ﬁrst term in the numerator is the likelihood, the second is the prior over parameters, and the denominator is a normalizing factor that we will not expand on right now. We see that the posterior is (proportional to) a product of the likelihood and the prior. This product is normalized so that it will be a proper density function. In fact, if the prior is a uniform distribution (that is, $P(\theta)\,=\,1$ for all $\theta\,\in\,[0,1])$ , then the posterior is just the normalized likelihood function. 

>  我们还可以利用该联合分布计算 $\theta$ 在给定数据集 $\mathcal D$ 时的后验概率
>  后验概率中，分子中的第一项是似然，第二项是参数的先验概率，分母为规范化常数
>  可以看到，后验概率正比于似然和先验的乘积
>  实际上，如果先验是均匀分布，则后验就是一个规范化的似然函数
