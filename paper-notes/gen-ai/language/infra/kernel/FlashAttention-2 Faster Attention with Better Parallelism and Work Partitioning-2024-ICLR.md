# Abstract
Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. 
> æ‹“å±• Transformer åºåˆ—é•¿åº¦çš„éš¾ç‚¹åœ¨äº attention å±‚çš„è¿è¡Œæ—¶é—´å’Œå†…å­˜éƒ½éšåºåˆ—é•¿åº¦äºŒæ¬¡å¢é•¿

FlashAttention (Dao et al., 2022) exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup ( $2â€“4\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only $25.40\%$ of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. 
> FlashAttention ç›¸å¯¹äº GEMM ç®—å­ä»…èƒ½è¾¾åˆ° 25.40% çš„ç†è®ºæœ€å¤§ FLOPs/s
> åŸå› åœ¨äº GPU ä¸Š thread block å’Œ warp ä¹‹é—´çš„å·¥ä½œåˆ’åˆ†æ˜¯æ¬¡ä¼˜çš„ï¼Œå¯¼è‡´ä½ occupancy æˆ–ä¸å¿…è¦çš„ shared memory è¯»å†™

We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. 
> FlashAttention-2 æœ‰æ›´å¥½çš„å·¥ä½œåˆ’åˆ†æœºåˆ¶ï¼Œå…·ä½“ä¸ºï¼š
> 1. è°ƒæ•´äº†ç®—æ³•ï¼Œå‡å°‘äº† non-mamul FLOPs
> 2. åœ¨å¤šä¸ª thread block ä¸­å¹¶è¡ŒåŒ– attention è®¡ç®—ä»¥æé«˜ occupancy
> 3. åœ¨æ¯ä¸ª thread block å†…å°†å·¥ä½œåˆ†å¸ƒç»™ warps ä»¥å‡å°‘é€šè¿‡ shared memory çš„é€šè®¯

These yield around $2\times$ speedup compared to FlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU ( $72\%$ model FLOPs utilization). 
> FlashAttention è¾¾åˆ° 50-73% A100çš„ç†è®ºæœ€å¤§ FLOPs/sï¼Œæ•ˆç‡æ¥è¿‘ GEMM ç®—å­ (225 TFLOPs/s)

# 1 Introduction
Scaling up the context length of Transformers (Vaswani et al., 2017) is a challenge, since the attention layer at their heart has runtime and memory requirements quadratic in the input sequence length. Ideally, we would like to go beyond the standard 2k sequence length limit to train models to understand books, high resolution images, and long-form videos. Just within the last year, there have been several language models with much longer context than before: GPT-4 (OpenAI, 2023) with context length 32k, MosaicMLâ€™s MPT with context length 65k , and Anthropicâ€™s Claude with context length 100k. Emerging use cases such as long document querying and story writing have demonstrated a need for models with such long context. 
> ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæ ‡å‡† Transformer (2k), GPT-4 (32k), MPT (65k), Claude (100k)

To reduce the computational requirement of attention on such long context, there have been numerous methods proposed to approximate attention (Kitaev et al., 2020; Roy et al., 2021; Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Chen et al., 2021). Though these methods have seen some use cases, as far as we know, most large-scale training runs still use standard attention. Motivated by this, Dao et al. (2022) proposed to reorder the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. This yields $2â€“4\times$ wall-clock time speedup over optimized baselines, up to 10-20 Ã— memory saving, with no approximation, and as a result FlashAttention has seen wide adoption in large-scale training and inference of Transformers. 
> FlashAttention å°†å†…å­˜éœ€æ±‚é™ä¸ºçº¿æ€§

![[FlashAttention2-Fig6.png]]

![[FlashAttention2-Fig7.png]]

However, context length increases even more, FlashAttention is still not nearly as efficient as other primitives such as matrix-multiply (GEMM). In particular, while FlashAttention is already $2â€“4\times$ faster than a standard attention implementation, the forward pass only reaches $30.50\%$ of the theoretical maximum $\mathrm{FLOPs}/\mathrm{s}$ of the device (Fig. 6), while the backward pass is even more challenging, reaching only $25.35\%$ of maximum throughput on A100 GPU (Fig. 7). In contrast, optimized GEMM can reach up to $80â€“90\%$ of the theoretical maximum device throughput. Through careful profiling, we observe that FlashAttention still has suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. 
> ä½† FlashAttention å°šæœªåƒ GEMM è¿™ç±»åŸè¯­ç®—å­ä¸€æ ·é«˜æ•ˆï¼Œ
> ä¾‹å¦‚ FlashAttention å‰å‘ä»…è¾¾åˆ° A100 ç†è®ºæœ€å¤§ FLOPs/s çš„ 30.50%ï¼Œåå‘ä»…è¾¾åˆ° 23.35%ï¼Œè€Œ GEMM å¯ä»¥è¾¾åˆ° 80-90%
> æˆ‘ä»¬é€šè¿‡ profiling å‘ç° FlashAttention åœ¨ GPU ä¸Šä¸åŒçš„ thread block å’Œ warps ä¹‹é—´çš„å·¥ä½œåˆ’åˆ†æ˜¯æ¬¡ä¼˜çš„ï¼Œå¯¼è‡´ low-occupancy æˆ–ä¸å¿…è¦çš„ shared memory è¯»å†™

Building on FlashAttention, we propose FlashAttention-2 with better parallelism and work partitioning to address these challenges. 

1. In Section 3.1, we tweak the algorithms to reduce the number of non-matmul FLOPs while not changing the output. While the non-matmul FLOPs only account for a small fraction of the total FLOPs, they take longer to perform as GPUs have specialized units for matrix multiply, and as a result the matmul throughput can be up to $16\times$ higher than non-matmul throughput. It is thus important to reduce non-matmul FLOPs and spend as much time as possible doing matmul FLOPs.

 2. We propose to parallelize both the forward pass and backward pass along the sequence length dimension, in addition to the batch and number of heads dimension. This increases occupancy (utilization of GPU resources) in the case where the sequences are long (and hence batch size is often small).

 3. Even within one block of attention computation, we partition the work between different warps of a thread block to reduce communication and shared memory reads/writes. 

> FlashAttention-2 åŸºäº FlashAttentionï¼Œå…·æœ‰æ›´å¥½çš„å·¥ä½œåˆ’åˆ†å’Œå¹¶è¡Œæ€§
> å…·ä½“ä¸ºï¼š
> 1. FlashAttention çš„ç®—æ³•è¢«å¾®è°ƒï¼Œå‡å°‘äº† non-matmul FLOPs (ç†ç”±ï¼šnon-matmul FLOPs ä»…å æ€» FLOPs çš„ä¸€å°éƒ¨åˆ†ï¼Œä½† GPUs æœ‰é«˜åº¦ä¼˜åŒ–çš„ GEMM ç®—å­ï¼Œæ•… non-matmul çš„ååé‡ä»…ä¸º matmul çš„ 16åˆ†ä¹‹ä¸€)
> 2. åœ¨åºåˆ—é•¿åº¦ç»´åº¦ã€batch ç»´åº¦ã€å¤´æ•°é‡ç»´åº¦å¹¶è¡ŒåŒ–å‰å‘å’Œåå‘ä¼ æ’­ï¼Œè¿™åœ¨é•¿åºåˆ—é•¿åº¦ (batch size ç›¸åº”åœ°å°) çš„æƒ…å†µä¸‹æé«˜äº† occupancy (GPU èµ„æºçš„åˆ©ç”¨ç‡)
> 3. åœ¨å•ä¸ª attention block è®¡ç®—ä¸­ç»å·¥ä½œåˆ’åˆ†ç»™ thread block å†…ä¸åŒçš„ warpï¼Œå‡å°‘äº† shared memory çš„è¯»å†™

In Section 4, we empirically validate that FlashAttention-2 yields significant speedup compared to even FlashAttention . Benchmarks on different settings (with or without causal mask, different head dimensions) show that FlashAttention-2 achieves around $2\times$ speedup over FlashAttention , reaching up to 73% of the theoretical max throughput in the forward pass, and up to 63% of the theoretical max throughput in the backward pass. During LLM inference, FlashAttention-2â€™s kernel is up to $7\times$ faster than the attention kernel from Faster Transformer. When used end-to-end to train GPT-style models, we reach training speed of up to 225 TFLOPs/s per A100 GPU. 
> ä¸åŒè®¾å®š (æœ‰æ—  causal maskã€ä¸åŒå¤´ç»´åº¦) ä¸‹çš„ benchmark è¡¨æ˜ FlashAttention-2 é€Ÿåº¦ä¸º FlashAttention çš„ä¸¤å€ï¼Œå‰å‘ä¼ æ’­ä¸‹è¾¾åˆ° 73% çš„ç†è®ºæœ€å¤§ååï¼Œåå‘ä¼ æ’­ä¸‹è¾¾åˆ° 63%çš„ç†è®ºæœ€å¤§åå
> ç”¨äº LLM æ¨ç†æ—¶ï¼ŒFlashAttention 7 å€å¿«äº Faster Transformer
> ç«¯åˆ°ç«¯è®­ç»ƒ GPT-style æ¨¡å‹æ—¶ï¼ŒA100ä¸‹è¾¾åˆ° 225 TFLOPs/s

# 2 Background
We provide some background on the performance characteristics and execution model of GPUs. We also describe the standard implementation of attention, as well as FlashAttention. 

## 2.1 Hardware Characteristics
**GPU performance characteristics.** The GPU consists of compute elements (e.g., ï¬‚oating point arithmetic units) and a memory hierarchy. Most modern GPUs contain specialized units to accelerate matrix multiply in low-precision (e.g., Tensor Cores on Nvidia GPUs for FP16/BF16 matrix multiply). The memory hierarchy comprise of high bandwidth memory (HBM), and on-chip SRAM (aka shared memory). As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s (Jia et al., 2018; Jia and Van Sandt, 2021). As the L2 cache is not directly controllable by the programmer, we focus on the HBM and SRAM for the purpose of this discussion. 
> GPU çš„ memory å±‚æ¬¡åŒ…æ‹¬ HBM å’Œ SRAM
> A100 HBM å¤§å°ä¸º 40-80GBï¼Œå¸¦å®½ä¸º 1.5-2.0TB/sï¼ŒSRAM å¤§å°ä¸º 192KBï¼Œ108ä¸ª SM ä¸Šå„è‡ªæœ‰ä¸€ä¸ªï¼Œå¸¦å®½ä¸º 19TB/s

**Execution Model.** GPUs have a massive number of threads to execute an operation (called a kernel). Threads are organized into thread blocks, which are scheduled to run on streaming multiprocessors (SMs). Within each thread blocks, threads are grouped into warps (a group of 32 threads). Threads within a warp can communicate by fast shuffle instructions or cooperate to perform matrix multiply. Warps within a thread block can communicate by reading from / writing to shared memory. Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM. 
> thread block è¢«è°ƒåº¦äº SM ä¸Šè¿è¡Œï¼Œthread block å†…çš„è°ƒåº¦å•ä½æ˜¯ warp
> åŒ warp å†…çš„ thread å¯ä»¥é€šè¿‡å¿«é€Ÿ shuffle æŒ‡ä»¤é€šè®¯ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å‘ shared memory è¯»å†™é€šè®¯ï¼›åŒ warp å†…çš„ thread å¯ä»¥ååŒæ‰§è¡ŒçŸ©é˜µä¹˜æ³•

## 2.2 Standard Attention Implementation
Given input sequences $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ where $N$ is the sequence length and $d$ is the head dimension, we want to compute the attention output $\mathbf{O}\!\in\!\mathbb{R}^{N\times d}$ : 

$$
\mathbf{S}\!=\!\mathbf{Q}\mathbf{K}^{\top}\!\in\!\mathbb{R}^{N\times N},\quad\mathbf{P}\!=\!\operatorname{softmax}(\mathbf{S})\!\in\!\mathbb{R}^{N\times N},\quad\mathbf{O}\!=\!\mathbf{P}\mathbf{V}\!\in\!\mathbb{R}^{N\times d},
$$ 
where $\text{softmax}$ is applied row-wise. 

For multi-head attention (MHA), this same computation is performed in parallel across many heads, and parallel over the batch dimension (number of input sequences in a batch). 
> MHA ä¸‹ï¼Œattention è®¡ç®—åœ¨å¤šä¸ªå¤´ä¸­å¹¶è¡Œæ‰§è¡Œï¼Œè€Œ MHA åœ¨ batch ç»´åº¦ä¸‹å¹¶è¡Œæ‰§è¡Œ (ä¸º batch ä¸­çš„å¤šä¸ªè¾“å…¥åºåˆ—è®¡ç®— MHA)

The backward pass of attention proceeds as follows. Let $\mathbf{dO}\in\mathbb{R}^{N\times d}$ be the gradient of $\mathbf O$ with respect to some loss function. Then by the chain rule (aka backpropagation):  

$$
\begin{array}{r l r l}&{\mathbf{d}\mathbf{V}\!=\!\mathbf{P}^{\top}\mathbf{d}\mathbf{O}\!\in\!\mathbb{R}^{N\times d}}&&{\mathbf{d}\mathbf{P}\!=\!\mathbf{d}\mathbf{O}\mathbf{V}^{\top}\!\in\!\mathbb{R}^{N\times N}}\\ &{\mathbf{d}\mathbf{S}\!=\!\mathrm{d}\mathrm{softmax}(\mathbf{d}\mathbf{P})\!\in\!\mathbb{R}^{N\times N}}&&{\mathbf{d}\mathbf{Q}\!=\!\mathbf{d}\mathbf{S}\mathbf{K}\!\in\!\mathbb{R}^{N\times d}}&&{\mathbf{d}\mathbf{K}\!=\!\mathbf{d}\mathbf{S}^{\top}\mathbf{Q}\!\in\!\mathbb{R}^{N\times d},}\end{array}
$$ 
where $\text{dsoftmax}$ is the gradient (backward pass) of softmax applied row-wise. One can work out that if $p=\operatorname{softmax}(s)$ for some vector $s$ and $p$ , then with output gradient $d p$ , the input gradient $\begin{array}{r}{d s\!=\!(\mathrm{diag}(p)\!-\!p p^{\top})d p}\end{array}$ . 

Standard attention implementation materialize the matrices $\mathbf S$ and $\mathbf{P}$ to HBM, which takes $O(N^{2})$ memory. Often $N\!\gg\!d$ (typically ğ‘ is on the order of 1kâ€“8k and ğ‘‘ is around 64â€“128). The standard attention implementation (1) calls the matrix multiply (GEMM) subroutine to multiply $\begin{array}{r}{\mathbf{S}=\mathbf{Q}\mathbf{K}^{\top}}\end{array}$ , writes the result to HBM, then (2) loads $\mathbf S$ from HBM to compute softmax and write the result $\mathbf{P}$ to HBM, and finally (3) calls GEMM to get $\mathbf{O}\!=\!\mathbf{P}\mathbf{V}$ . As most of the operations are bounded by memory bandwidth, the large number of memory accesses translates to slow wall-clock time. Moreover, the required memory is $O(N^{2})$ due to having to materialize $\bf S$ and $\bf P$ . Moreover, one has to save ${\bf P}\!\in\!\mathbb{R}^{{N}\times N}$ for the backward pass to compute the gradients. 
> æ ‡å‡† attention å®ç°éœ€è¦å°† $\mathbf {S, P} \in \mathbb R^{N\times N}$ å†™å…¥ HBMï¼Œå ç”¨ $O (N^2)$ å†…å­˜
> ( $N$ çš„æ•°é‡çº§ä¸€èˆ¬åœ¨ 1k-8kï¼Œ$d$ çš„æ•°é‡çº§ä¸€èˆ¬åœ¨ 64-128ï¼Œæ•… $N\gg d$)
> æ ‡å‡† attention çš„è®¡ç®—æµç¨‹ä¸ºï¼š
> 1. GEMM è®¡ç®— $\mathbf {S = QK}^{\top}$ï¼Œ$\bf S$ å†™å› HBM
> 2. load $\mathbf S$ï¼Œè®¡ç®— $\bf P$ï¼Œ$\bf P$ å†™å› HBM
> 3. GEMM è®¡ç®— $\bf O = PV$
> æ ‡å‡† attention è®¡ç®—çš„åŠ£åŠ¿ï¼š
> 1. memory bound
> 2. éœ€è¦ $O (N^2)$ memory
> 3. éœ€è¦å­˜å‚¨ $\mathbf P \in \mathbb R^{N\times N}$ ç”¨äºåå‘

## 2.3 FlashAttention 
To speed up attention on hardware accelerators such as GPU, (Dao et al., 2022) proposes an algorithm to reduce the memory reads/writes while maintaining the same output (without approximation). 

### 2.3.1 Forward pass
FlashAttention applies the classical technique of tiling to reduce memory IOs, by (1) loading blocks of inputs from HBM to SRAM, (2) computing attention with respect to that block, and then (3) updating the output without writing the large intermediate matrices $\bf S$ and $\mathbf{P}$ to HBM. As the softmax couples entire rows or blocks of row, online softmax (Milakov and Gimelshein, 2018; Rabe and Staats, 2021) can split the attention computation into blocks, and rescale the output of each block to finally get the right result (with no approximation). By significantly reducing the amount of memory read/writes, FlashAttention yields $2â€“4\times$ wall-clock speedup over optimized baseline attention implementations. 
> FlashAttention å‡å°‘äº† memory IOï¼Œå…¶æµç¨‹ä¸ºï¼š
> 1. å°†è¾“å…¥çš„ block ä» HBM load åˆ° SRAM
> 2. è®¡ç®— block attention
> 3. ä¸å†™å›ä¸­é—´ç»“æœ $\bf {S, P}$ï¼Œç›´æ¥åœ¨ç‰‡ä¸Šæ›´æ–° $\bf O$ 
> FlashAttention åˆ©ç”¨äº† online softmax å°† attention è®¡ç®—åˆ’åˆ†ä¸ºå—ï¼Œé€šè¿‡ rescale ä¿æŒ block attention çš„è®¡ç®—ç»“æœæ˜¯æ­£ç¡®çš„
> FlashAttention å°†è®¡ç®—åŠ é€Ÿäº† 2-4 å€

We describe the online softmax technique (Milakov and Gimelshein, 2018) and how it is used in attention (Rabe and Staats, 2021). For simplicity, consider just one row block of the attention matrix $\bf S$ , of the form $\left[\mathbf{S}^{(1)}\quad\mathbf{S}^{(2)}\right]$ for some matrices ${\mathbf{S}}^{(1)},\mathbf{S}^{(2)}\in{\mathbb{R}}^{B_{r}\times B_{c}}$ , where $B_{r}$ and $B_{c}$ are the row and column block sizes. We want to compute softmax of this row block and multiply with the value, of $\bf V$ the form $\begin{bmatrix}\mathbf V^{(1)} \\ \mathbf V^{(2)}\end{bmatrix}$ for some matrices $\mathbf{V}^{(1)},\mathbf{V}^{(2)}\in\mathbb{R}^{B_{c}\times d}$ . Standard softmax would compute: 

$$\begin{align*} m &= \operatorname*{max}(\mathrm{rowmax}(\mathbf{S}^{(1)}), \mathrm{rowmax}(\mathbf{S}^{(2)})) \in \mathbb{R}^{B_{r}} \\ 
\ell &= \mathrm{rowsum}(e^{\mathbf S^{(1)} - m}) + \mathrm{rowsum}(e^{\mathbf S^{(2)} - m}) \in \mathbb{R}^{B_{r}} \\ 
\mathbf{P} &= \left[\mathbf{P}^{(1)} \quad \mathbf{P}^{(2)}\right] = \mathrm{diag}(\ell)^{-1} \left[e^{\mathbf S^{(1)} - m} \quad e^{\mathbf S^{(2)} - m}\right] \in \mathbb{R}^{B_{r} \times 2B_{c}} \\ 
\mathbf{O} &= \left[\mathbf{P}^{(1)} \quad \mathbf{P}^{(2)}\right] \begin{bmatrix}\mathbf{V}^{(1)} \\ \mathbf V^{(2)}\end{bmatrix} = \mathrm{diag}(\ell)^{-1} \left(e^{\mathbf S^{(1)} - m} \mathbf{V}^{(1)} + e^{\mathbf S^{(2)} - m} \mathbf{V}^{(2)}\right) \in \mathbb{R}^{B_{r} \times d}. \end{align*}$$

Online softmax instead computes "local" softmax with respect to each block and rescale to get the right out at the end:

$$
\begin{align}
m^{(1)} & = \text{rowmax}(\mathbf S^{(1)})\in \mathbb R^{B_r}\\
\ell^{(1)} &= \text{rowsum}(e^{\mathbf S^{(1)}-m^{(1)}}) \in \mathbb R^{B_r}\\
\tilde {\mathbf P}^{(1)} &= \text{diag}(\ell^{(1)})^{-1} e^{\mathbf S^{(1)}- m^{(1)}} \in \mathbb R^{B_r \times B_c}\\
\mathbf O^{(1)}&=\tilde {\mathbf P}^{(1)}\mathbf V^{(1)} =\text{diag}(\ell^{(1)})^{-1}e^{\mathbf S^{(1)}- m^{(1)}}\mathbf V^{(1)} \in \mathbb R^{B_r \times d}\\\\
m^{(2)} & = \max(m^{(1)},\text{rowmax}(\mathbf S^{(2)})) = m\\
\ell^{(2)} &= e^{m^{(1)} - m^{(2)}}\ell^{(1)} + \text{rowsum}(e^{\mathbf S^{(2)}-m^{(2)}}) = \text{rowsum}(e^{\mathbf S^{(1)}-m}) +\text{rowsum}(e^{\mathbf S^{(2)}-m}) = \ell\\
\tilde {\mathbf P}^{(2)} &= \text{diag}(\ell^{(2)})^{-1} e^{\mathbf S^{(2)}- m^{(2)}} \in \mathbb R^{B_r \times B_c}\\
\mathbf O^{(2)}&=\text{diag}(\ell^{(1)}/\ell^{(2)})e^{m^{(1)}-m}\mathbf O^{(1)} + \tilde {\mathbf P}^{(2)}\mathbf V^{(2)} =
\text{diag}(\ell^{(2)})^{-1}e^{\mathbf S^{(1)}- m}\mathbf V^{(1)}+
\text{diag}(\ell^{(2)})^{-1}e^{\mathbf S^{(2)}- m}\mathbf V^{(1)}=\mathbf O
\end{align}
$$

> å¸¸è§„çš„ softmax è€¦åˆäº†è¾“å…¥çŸ©é˜µçš„æ‰€æœ‰çš„åˆ—ï¼Œonline softmax å¯¹å…¶è¿›è¡Œè§£è€¦åˆ
> è€ƒè™‘æ¯ä¸€è¡Œï¼Œåœ¨åˆ—ç»´åº¦è¿›è¡Œåˆ†å—æ—¶ï¼Œonline softmax è®¡ç®—æ¯ä¸ªå—çš„å±€éƒ¨ softmaxï¼Œå¹¶éšç€åˆ—ç»´åº¦ä¸Šå—çš„éå†ä¸æ–­æ›´æ–° softmax ç»Ÿè®¡é‡ $\ell, m$ (è§„èŒƒåŒ–æŒ‡æ•°å’Œã€æœ€å¤§å€¼)ï¼Œç”¨æ›´æ–°çš„æœ€å¤§å€¼é‡ç¼©æ”¾ä¹‹å‰å—çš„å±€éƒ¨ softmax è®¡ç®—ç»“æœ

We show how FlashAttention uses online softmax to enable tiling (Fig. 1) to reduce memory reads/writes. 
> FlashAttention ä½¿ç”¨ online softmax å°† attention è®¡ç®— tile ä¸ºå¤šä¸ª block attention
> å…¶ä¸­ï¼Œæ¯ä¸ª block attention æ›´æ–° softmax ç»Ÿè®¡é‡ï¼Œé‡ç¼©æ”¾å½“å‰çš„ç´¯ç§¯ values åŠ æƒå’Œï¼Œè®¡ç®—è¯¥ block ç›¸å…³çš„ values åŠ æƒå’Œå¹¶å°†å…¶ç´¯ç§¯
> block attention ä»…è®¡ç®—ä¸€å— $\mathbf S, \mathbf P$ï¼Œå¹¶ä¸”ç”¨åå³å¼ƒï¼Œä¸å†™å…¥ HBMï¼Œæ•…èŠ‚çº¦äº†è¯»å†™ä¸­é—´ç»“æœ $\mathbf S, \mathbf P$ éœ€è¦çš„å¤§é‡ HBM è®¿é—®ï¼ŒåŒæ—¶èŠ‚çº¦äº† HBM ç©ºé—´
> FlashAttention å‡å°‘äº† $\mathbf {S, P}$ çš„ HBM è¯»å†™æ¬¡æ•°ï¼Œä½†å®é™…ä¸Šç›¸åº”å¢åŠ äº† $\mathbf {Q, O}$ çš„è¯»å†™æ¬¡æ•°ï¼Œä½†ç”±äº $N\gg d$ï¼Œæ•…æ€»ä½“çš„ HBM è¯»å†™çš„æ¬¡æ•°æ˜¯å¤§å¹…å‡å°‘çš„ï¼Œå› æ­¤ FlashAttention æœ¬è´¨ä¸Šåˆ©ç”¨äº† $\mathbf {QK}^\top$ çš„ä½ç§©æ€§è´¨

![[FlashAttention2-Fig1.png]]

### 2.3.2 Backward pass
In the backward pass, by re-computing the values of the attention matrices $\bf S$ and $\mathbf{P}$ once blocks of inputs $\mathbf{Q},\mathbf{K},\mathbf{V}$ are already loaded to SRAM, FlashAttention avoids having to store large intermediate values. By not having to save the large matrices $\bf S$ and $\mathbf{P}$ of size $N{\times}N$ , FlashAttention yields $10â€“20\times$ memory saving depending on sequence length (memory required in linear in sequence length ğ‘ instead of quadratic). The backward pass also achieves 2-4 Ã— wall-clock speedup due to reduce memory reads/writes. 
> FlashAttention åœ¨å‰å‘ä¸­æ²¡æœ‰ä¿å­˜ $\bf S, P$ï¼Œåœ¨åå‘è¿‡ç¨‹ä¼šæ ¹æ® SRAM ä¸Šçš„ $\bf Q, K, V$ é‡æ–°è®¡ç®— $\bf S, P$ï¼Œè¿™å°† memory éœ€æ±‚é™ä¸ºäº† $O (N)$ï¼ŒåŒæ—¶ä¹Ÿå‡å°‘äº† HBM è¯»å†™ï¼ŒåŸç†å’Œå‰å‘å®Œå…¨ç±»ä¼¼

The backward pass applies tiling to the equations in Section 2.2. Though the backward pass is simpler than the forward pass conceptually (there is no softmax rescaling), the implementation is significantly more involved. This is because there are more values to be kept in SRAM to perform 5 matrix multiples in the backward pass, compared to just 2 matrix multiples in the forward pass. 
> åå‘æ²¡æœ‰é‡å¤çš„ rescaleï¼Œæ•…åœ¨æ¦‚å¿µä¸Šç®€å•äºå‰å‘
> åå‘çš„å®ç°åˆ™æ¯”å‰å‘æ˜¾è‘—å¤æ‚ï¼Œå‰å‘ä»…éœ€è¦å®Œæˆä¸¤ä¸ªçŸ©é˜µä¹˜ ($\mathbf {QK}^\top = \mathbf S$, $\mathbf {PV} = \mathbf O$)ï¼Œåå‘éœ€è¦å®Œæˆäº”ä¸ªçŸ©é˜µä¹˜ ($\mathbf {QK}^\top = \mathbf S$, $\mathbf {dV} = \mathbf P^\top \mathbf {dO}$, $\mathbf {dP} = \mathbf {dO}\mathbf V^\top$, $\mathbf {dQ} = \mathbf {dS}\mathbf K$, $\mathbf {dK} = \mathbf {dS}^{\top}\mathbf {dQ}$)ï¼Œå› è€Œåœ¨ SRAM ä¸­éœ€è¦ä¿å­˜æ›´å¤šçš„çŸ©é˜µ

# 3 FlashAttention-2: Algorithm, Parallelism, and Work Partitioning
We describe the FlashAttention-2 algorithm, which includes several tweaks to FlashAttention to reduce the number of non-matmul FLOPs. We then describe how to parallelize the computation on different thread blocks to make full use the GPU resources. Finally we describe we partition the work between different warps within one thread block to reduce the amount of shared memory access. These improvements lead to $2â€“3\times$ speedup as validated in Section 4. 

## 3.1 Algorithm
We tweak the algorithm from FlashAttention to reduce the number of non-matmul FLOPs. This is because modern GPUs have specialized compute units (e.g., Tensor Cores on Nvidia GPUs) that makes matmul much faster. As an example, the A100 GPU has a max theoretical throughput of 312 TFLOPs/s of FP16/BF16 matmul, but only 19.5 TFLOPs/s of non-matmul FP32. Another way to think about this is that each non-matmul FLOP is $16\times$ more expensive than a matmul FLOP. To maintain high throughput (e.g., more than 50% of the maximum theoretical TFLOPs/s), we want to spend as much time on matmul FLOPs as possible. 
> A100çš„ Tensor core å¤„ç† FP16/BF16 çš„ matmul è¿ç®—çš„ç†è®ºå³°å€¼ååé‡ä¸º 312 TFLOPs/sï¼Œè€Œ Cuda core å¤„ç† FP32çš„ non-matmul è¿ç®—çš„ç†è®ºå³°å€¼ååé‡ä»…ä¸º 19.5 TFLOPs/s
> å¯ä»¥ç†è§£ä¸ºæ¯ä¸ªé matmul çš„æµ®ç‚¹è¿ç®—éƒ½16å€æ˜‚è´µäº matmul æµ®ç‚¹è¿ç®—ï¼Œæ•…éœ€è¦æé«˜ matmul è¿ç®—çš„æ¯”ä¾‹

### 3.1.1 Forward pass
We revisit the online softmax trick as shown in Section 2.3 and make two minor tweaks to reduce non-matmul FLOPs: 

(1) We do not have to rescale both terms of the output update by $\mathrm{diag}(\ell^{(2)})^{-1}$ : 

$$
{\bf O}^{(2)}\!=\!\mathrm{diag}(\ell^{(1)}/\ell^{(2)})e^{m^{(1)}-m^{(2)}}{\bf O}^{(1)}\!+\!\mathrm{diag}(\ell^{(2)})^{-1}e^{{\bf S}^{(2)}-m^{(2)}}{\bf V}^{(2)}.
$$ 
We can instead maintain an â€œun-scaledâ€ version of $\mathbf{O}^{(2)}$ and keep around the statistics $\ell^{(2)}$ : 

$$
\tilde {\mathbf O}^{(2)} = \text{diag}(\ell^{(1)})^{-1}e^{m^{(1)} - m^{(2)}}\mathbf O^{(1)} + e^{\mathbf S^{(2)}-m^{(2)}}\mathbf V^{(2)}
$$

Only at the every end of the loop do we scale the final $\tilde{\mathbf{O}}^{(\mathrm{last})}$ by $\mathrm{diag}(\ell^{(\mathrm{last})})^{-1}$ to get the right output. 

(2) We do not have to save both the max $m^{(j)}$ and the sum of exponentials $\ell^{(j)}$ for the backward pass. We only need to store the logsumexp $L^{(j)}{=}m^{(j)}{+}{\log}(\ell^{(j)})$ . 

> FlashAttention-2 å¯¹ FlashAttention åšçš„ä¸¤é¡¹å¾®è°ƒï¼š
> 1. ä¸å†ç”¨ $\text{diag}(\ell^{(i)})$ åœ¨æ¯ä¸€æ­¥ç¼©æ”¾ $\mathbf O^{(i)}$ï¼Œä»…åœ¨ç´¯ç§¯åˆ°æœ€åæ—¶ç”¨æœ€ç»ˆçš„ $\text{diag}(\ell )$ è¿›è¡Œç¼©æ”¾ï¼Œæ­¤æ—¶ $\ell^{(i)}$ åœ¨ç®—æ³•ä¸­ä»…éœ€è¦ä¿æŒæ›´æ–°ï¼Œä¸å‚ä¸è®¡ç®—ã€‚æ¢å¥è¯è¯´ï¼Œç®—æ³•ä¸­é™¤äº†æœ€åä¸€æ­¥ï¼Œå¯¹ values çš„åŠ æƒæ±‚å’Œéƒ½ä¸ä¼šå¯¹æƒé‡è¿›è¡Œå½’ä¸€åŒ– (ä½†æƒé‡æœ¬èº«ä¸€å®šåœ¨ $[0,1]$ ä¹‹é—´ï¼Œå› æ­¤å¯¹æ•°å€¼ç¨³å®šæ€§ä¸ä¼šæœ‰å¤ªå¤§å½±å“)
> 2. å°†åˆ†åˆ«å‚¨å­˜ $m^{(j)}, \ell^{(j)}$ (ç”¨äºåå‘ä¼ æ’­) æ”¹ä¸ºä»…å‚¨å­˜ $L^{(j)} = m^{(j)} + \log (\ell^{(j)})$

In the simple case of 2 blocks in Section 2.3, the online softmax trick now becomes:

$$\begin{align} 
m^{(1)} &= \mathrm{rowmax}(\mathbf{S}^{(1)}) \in \mathbb{R}^{B_{r}}\\
\ell^{(1)} &= \mathrm{rowsum}(e^{\mathbf{S}^{(1)} - m^{(1)}}) \in \mathbb{R}^{B_{r}} \\ 
\tilde {\mathbf{O}}^{(1)} &= e^{\mathbf{S}^{(1)} - m^{(1)}} \mathbf{V}^{(1)} \in \mathbb{R}^{B_{r} \times d}\\\\
m^{(2)} &= \operatorname*{max}(m^{(1)}, \mathrm{rowmax}(\mathbf{S}^{(2)})) = m \\
\ell^{(2)} &= e^{m^{(1)} - m^{(2)}} \ell^{(1)} + \mathrm{rowsum}(e^{\mathbf{S}^{(2)} - m^{(2)}}) = \mathrm{rowsum}(e^{\mathbf{S}^{(1)} - m}) + \mathrm{rowsum}(e^{\mathbf{S}^{(2)} - m}) = \ell \\
\tilde{\mathbf{P}}^{(2)} &= \mathrm{diag}(\ell^{(2)})^{-1} e^{\mathbf{S}^{(2)} - m^{(2)}} \\ 
\tilde{\mathbf{O}}^{(2)} &= \mathrm{diag}(e^{m^{(1)} - m^{(2)}}) \tilde{\mathbf{O}}^{(1)} + e^{\mathbf{S}^{(2)} - m^{(2)}} \mathbf{V}^{(2)} = e^{s^{(1)} - m} \mathbf{V}^{(1)} + e^{s^{(2)} - m} \mathbf{V}^{(2)} \\ 
\mathbf{O}^{(2)} &= \mathrm{diag}(\ell^{(2)})^{-1} \tilde{\mathbf{O}}^{(2)} = \mathbf{O}. 
\end{align}$$

We describe the full FlashAttention-2 forward pass in Algorithm 1. 

![[FlashAttention2-Algorithm 1.png]]

**Algorithm 1** FlashAttention-2 forward pass
**Require:** Matrices $\mathbf {Q,K,V}\in \mathbb R^{N\times d}$ in HBM, block sizes $B_c, B_r$.
  1: Divide $\mathbf Q$ into $T_r = \lceil \frac N {B_r} \rceil$ blocks $\mathbf Q_1, \dots ,\mathbf Q_{T_r}$ of size $B_r \times d$ each, and divide $\mathbf K, \mathbf V$ into $T_c = \lceil \frac N{B_c} \rceil$ blocks $\mathbf K_1, \dots, \mathbf K_{T_c}$ and $\mathbf V_1, \dots, \mathbf V_{T_c}$ of size $B_c\times d$ each.
> åˆ’åˆ† $\mathbf Q, \mathbf K, \mathbf V$ï¼Œåˆ’åˆ†æ—¶ä¿æŒåµŒå…¥ç»´åº¦ $d$ ä¸å˜ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $\mathbf Q$ åˆ’åˆ†å•ä½ä¸º $B_r \times d$ï¼Œ$\mathbf K, \mathbf V$ åˆ’åˆ†å•ä½ä¸º $B_c\times d$
> å¾—åˆ° $T_r$ ä¸ª $\mathbf Q$ å—ï¼Œå¾—åˆ° $T_c$ ä¸ª $\mathbf K, \mathbf V$ å—

  2: Divide the output $\mathbf O\in \mathbb R^{N\times d}$ into $T_r$ blocks $\mathbf O_i, \dots, \mathbf O_{T_r}$ of size $B_r \times d$ each, divide the logsumexp $L$ into $T_r$ blocks $L_i,\dots, L_{T_r}$ of size $B_r$ each.
> åˆ’åˆ† $\mathbf O$ï¼Œåˆ’åˆ†æ—¶ä¿æŒåµŒå…¥ç»´åº¦ $d$ ä¸å˜ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $\mathbf O$ åˆ’åˆ†å•ä½ä¸º $B_r \times d$
> å¾—åˆ° $T_r$ ä¸ª $\mathbf O$ å—
> åˆ’åˆ† $L$ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $L$ çš„åˆ’åˆ†å•ä½ä¸º $B_r$
> å¾—åˆ° $T_r$ ä¸ª $L$ å—

  3: **for** $1\le i \le T_r$ **do**
  4:     Load $\mathbf Q_i$ from HBM to on-chip SRAM.
  5:     On chip, initialize $\mathbf O_{i}^{(0)} = (0)_{B_r \times d}\in \mathbb R^{B_r \times d}$, $\ell^{(0)}_{i} = (0)_{B_r}\in \mathbb R^{B_r}$, $m_i^{(0)} = (\infty)_{B_r}\in \mathbb R^{B_r}$.
> å¤–å±‚å¾ªç¯ï¼š
> è£…è½½ $\mathbf Q$ å—åˆ° SRAM
> åœ¨ç‰‡ä¸Šåˆå§‹åŒ– $\mathbf O, \ell, m$ å—

  6:     **for** $1\le j \le T_c$ **do**
  7:         Load $\mathbf K_j, \mathbf V_j$ from HBM to on-chip SRAM.
  8:        On chip, computes $\mathbf S_{i}^{(j)} = \mathbf Q_i \mathbf K_j^\top \in \mathbb R^{B_r\times B_c}$.
  9:        On chip, compute $m_{i}^{(j)} = \max(m_i^{(j-1)},\text{rowmax}(\mathbf S_{i}^{(j)})) \in \mathbb R^{B_r}$ï¼Œ$\tilde {\mathbf P}_{i}^{(j)} = \exp(\mathbf S_{i}^{(j)}- {m}_{i}^{(j)})\in \mathbb R^{B_r\times B_c}$ (pointwise)ï¼Œ${\mathscr l}_{i}^{(j)} = e^{m_i^{(j-1)}-m_i^{(j)}}\ell_i^{(j-1)}+\text{rowsum}(\tilde {\mathbf  P}_{i}^{(j)}) \in \mathbb R^{B_r}$.
10:        Write $\mathbf O_i^{(j)} = \text{diag}(\mathscr e^{m_i^{(j-1)} - m_i^{(j)}})\mathbf O_i^{(j-1)}+\tilde {\mathbf P}_{i}^{(j)} \mathbf V_j$ to HBM.
 11:     **end for**
> å†…å±‚å¾ªç¯ï¼š
> è£…è½½ $\mathbf K, \mathbf V$ å—åˆ° SRAM
> 
> åœ¨ç‰‡ä¸Šè®¡ç®— $\mathbf S$ å—ï¼š$\mathbf S =  \mathbf Q\mathbf K^\top\in \mathbb R^{B_r\times B_c}$ (score æ˜¯ final çš„)
> æŒ‰è¡Œå–æœ€å¤§å€¼ï¼Œå¹¶æ›´æ–°è®°å½•çš„æ¯è¡Œæœ€å¤§å€¼: $m^{(j)} = \max(m^{(j-1)},\text{rowmax}(\mathbf S) )\in \mathbb R^{B_r}$
> æŒ‰è¡Œè§„èŒƒåŒ– $\mathbf S$ å¹¶å–æŒ‡æ•°: $\tilde {\mathbf P}= \exp(\mathbf S - m^{(j)}) \in \mathbb R^{B_r\times B_c}$
> æŒ‰è¡Œæ±‚å’Œï¼Œå¹¶æ›´æ–°å„è¡Œç´¯åŠ æŒ‡æ•°å’Œ: ${\ell}^{(j)} = e^{m^{(j-1)}-m^{(j)}}\ell^{(j-1)} + \text{rowsum}(\tilde {\mathbf P}) \in \mathbb R^{B_r}$
> (å…¶ä¸­ $e^{m^{(j-1)} - m^{(j)}}\ell^{(j-1)}$ æ˜¯ç”¨æ›´æ–°çš„å„è¡Œæœ€å¤§å€¼é‡æ”¾ç¼©ç›®å‰ä¸ºæ­¢çš„å„è¡Œç´¯åŠ æŒ‡æ•°å’Œ)
> 
> è®¡ç®— $\text{diag} (e^{m^{(j-1)} - m^{(j)}})\mathbf O^{(j-1)}$ï¼Œå³å¯¹äºæ¯ä¸€è¡Œ,ç”¨æ›´æ–°çš„æœ€å¤§å€¼é‡æ”¾ç¼©ç›®å‰ä¸ºæ­¢æ³¨æ„åˆ°çš„æ ·æœ¬çš„æŒ‡æ•°åˆ†æ•°ï¼Œç›®å‰ä¸ºæ­¢æ³¨æ„åˆ°çš„æ ·æœ¬æ•°é‡éšç€å†…å±‚å¾ªç¯å¢é•¿ï¼›
> è®¡ç®— $\tilde {\mathbf P}_{i}^{(j)} \mathbf V_j$ï¼Œå³å¯¹äºæ¯ä¸€è¡Œï¼ŒæŒ‰ç…§æŒ‡æ•°åˆ†æ•°å¯¹å½“å‰å—æ³¨æ„åˆ°çš„æ ·æœ¬åŠ æƒæ±‚å’Œï¼›
> è®¡ç®— $\mathbf O_i^{(j)} = \text{diag}(\mathscr e^{m_i^{(j-1)} - m_i^{(j)}})\mathbf O_i^{(j-1)}+\tilde {\mathbf P}_{i}^{(j)} \mathbf V_j$ï¼Œå³å¯¹äºæ¯ä¸€è¡Œï¼Œè¡¥å……æ³¨æ„åˆ°çš„ï¼ˆå½“å‰å—ï¼‰æ ·æœ¬çš„åŠ æƒå’Œï¼›

 12:     On chip, compute $\mathbf O_i = \text{diag}    (\ell_i^{(T_c)})^{-1}\mathbf O_i^{(T_c)}$.
 13:     On chip, compute $L_i = m_i^{(T_c)} + \log (\ell_i^{(T_c)})$.
 14:     Write $\mathbf O_i$ to HBM as the $i$ -th block of $\mathbf O$.
 15:     Write $L_i$ to HBM as the $i$ -th block of $L$.
 16: **end for**
> å¤–å±‚å¾ªç¯ï¼š
> å¯¹æœ€ç»ˆçš„ $\mathbf O$ å—ç”¨æœ€ç»ˆçš„ $\ell$ å—è¿›è¡Œæ”¾ç¼©ï¼Œå¹¶å°†ç»“æœå†™å›å¯¹åº”çš„ $\mathbf O$ å—
> ç”¨æœ€ç»ˆçš„ $m$ å—å’Œ $\ell$ å—è®¡ç®— $L$ å—ï¼š $L = m + \log (\ell)$ï¼Œå¹¶å°†ç»“æœå†™å›å¯¹åº”çš„ $L$ å—

 17: Return the output $\mathbf O$ and the logsumexp $L$.
> ç»“æŸï¼šè¿”å›å®Œæ•´çš„ $\mathbf O, L$

> FlashAttention-2 ç›¸è¾ƒäº FlashAttention
> 1. äº¤æ¢äº†å†…å¤–å±‚å¾ªç¯ï¼šFlashAttention-2 å¤–å±‚å¯¹ $\mathbf Q$ å—è¿›è¡Œå¾ªç¯ï¼Œæ¯æ¬¡å¤–å±‚å¾ªç¯å®Œæ•´è®¡ç®—ä¸€ä¸ª $\mathbf Q$ å—å¯¹åº”çš„ $\mathbf O$ å’Œ $m, \ell$ 
> 2. ç®€åŒ–äº†è®¡ç®—æµç¨‹ï¼šå†…å±‚å¾ªç¯ä¸­ï¼Œå…ˆæ›´æ–° $m$ï¼Œç”¨æ›´æ–°çš„ $m$ ç›´æ¥ç¼©æ”¾å½“å‰çš„ $\mathbf P$ å—ã€‚å› æ­¤ï¼Œè®¡ç®— $\ell$ æ—¶ï¼Œä»…éœ€è¦å¯¹ä¹‹å‰çš„ $\ell$ ç”¨æ–° $m$ é‡ç¼©æ”¾ï¼Œä¸éœ€è¦å¯¹å½“å‰å—çš„ $\ell$ é‡ç¼©æ”¾ï¼›è®¡ç®— $\mathbf O$ æ—¶ï¼Œä»…éœ€è¦å¯¹ä¹‹å‰çš„ $\mathbf O$ ç”¨æ–° $m$ é‡ç¼©æ”¾ï¼Œä¸éœ€è¦å¯¹å½“å‰å—çš„ $\mathbf O$ é‡ç¼©æ”¾ã€‚è¿™ä½¿å¾—æ¯æ¬¡å†…å±‚å¾ªç¯å°‘äº†ä¸¤æ¬¡é‡ç¼©æ”¾è®¡ç®—
> 3. ç§»é™¤äº†ç”¨ $\ell$ å¯¹ $\mathbf O$ çš„ç¼©æ”¾ï¼Œæ›´æ–° $\mathbf O$ æ—¶ä¸å†é™¤å»å½“å‰çš„å„è¡ŒæŒ‡æ•°åˆ†æ•°å’Œã€‚è¿™ä½¿å¾—æ¯æ¬¡å†…å±‚å¾ªç¯å°‘äº†ä¸¤æ¬¡é‡ç¼©æ”¾è®¡ç®—

> FlashAttention-2 çš„å›¾ç¤ºè§[[#Figure Illustration for FlashAttention-2 forward pass|é™„å½•]]

**Causal masking.** 
One common use case of attention is in auto-regressive language modeling, where we need to apply a causal mask to the attention matrix $\bf S$ (i.e., any entry $\mathbf{S}_{i j}$ with $j\!>\! i$ is set to $-\infty.$ ). 
> è‡ªå›å½’è¯­è¨€å»ºæ¨¡æ—¶ï¼Œattention çŸ©é˜µ $\mathbf S$ éœ€è¦è¿›è¡Œ causal mask
> causal mask ä»¤å½“å‰ token ä»…æ³¨æ„è‡ªå·±å’Œä¹‹å‰çš„ tokenï¼Œå…¶ç®€å•å®ç°å°±æ˜¯å°†æ‰€æœ‰ $\mathbf S_{ij}(j>i)$ èµ‹å€¼ä¸º $-\infty$

1. As FlashAttention and FlashAttention-2 already operate by blocks, for any blocks where all the column indices are more than the row indices (approximately half of the blocks for large sequence length), we can skip the computation of that block. This leads to around $1.7-1.8\times$ speedup compared to attention without the causal mask. 

2. We do not need to apply the causal mask for blocks whose row indices are guaranteed to be strictly less than the column indices. This means that for each row, we only need apply causal mask to 1 block (assuming square block). 

> å¯¹äº FlashAttention-2ï¼š
> è€ƒè™‘å¤–å±‚å¾ªç¯çš„ä¸€æ¬¡è¿­ä»£ï¼Œæœ¬æ¬¡è¿­ä»£è®¡ç®— $\mathbf Q_i$ å—å¯¹åº”çš„ attention ç»“æœ $\mathbf O_i$
> åœ¨å†…å±‚å¾ªç¯è£…è½½ $\mathbf K_j$ å¹¶è®¡ç®— $\mathbf Q_i \mathbf K_j^\top$ æ—¶ï¼š
> å¦‚æœ `j > i`ï¼Œç›´æ¥è·³è¿‡æ•´ä¸ªå†…å±‚å¾ªç¯ä»¥åŠåç»­çš„æ‰€æœ‰å†…å±‚å¾ªç¯ï¼Œä¹Ÿå°±æ˜¯æœ¬æ¬¡ $\mathbf O_i$ çš„è®¡ç®—ç›´æ¥ç»“æŸ
> å¦‚æœ `j < i`ï¼Œä¸éœ€è¦åº”ç”¨ causal mask
> å¦‚æœ `j == i`ï¼Œä¸º $\mathbf S_{ij}$ å—åº”ç”¨ causal mask

**Correctness, runtime, and memory requirement.** As with FlashAttention , Algorithm 1 returns the correct output $\mathbf{O}\!=\!\mathrm{softmax}(\mathbf{Q}\mathbf{K}^{\intercal})\mathbf{V}$ (with no approximation), using $O (N^{2}d)$ FLOPs and requires $O (N)$ additional memory beyond inputs and output (to store the logsumexp $L$ ). The proof is almost the same as the proof of Dao et al. (2022, Theorem 1), so we omit it here. 
> Algorithm 1 ç²¾ç¡®çš„ attention è®¡ç®—ç»“æœ $\mathbf O = \text{softmax}(\mathbf Q\mathbf K^\top) \mathbf V$ï¼ŒFLOPs ä¸º $O (N^2d)$ï¼Œé¢å¤– memory éœ€æ±‚ä¸º $O (N)$ 

### 3.1.2 Backward pass
The backward pass of FlashAttention-2 is almost the same as that of FlashAttention . We make a minor tweak to only use the row-wise logsumexp $L$ instead of both the row-wise max and row-wise sum of exponentials in the softmax. We include the backward pass description in Algorithm 2 for completeness. 
> FlashAttention-2 çš„åå‘å’Œ FlashAttention å‡ ä¹ä¸€è‡´
> å¾®å°çš„æ”¹åŠ¨åœ¨äºä½¿ç”¨ $L$ æ›¿ä»£äº† $\ell, m$

**Multi-query attention and grouped-query attention.** Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023) are variants of attention where multiple heads of query attend to the same head of key and value, in order to reduce the size of KV cache during inference. Instead of having to duplicate the key and value heads for the computation, we implicitly manipulate the indices into the head to perform the same computation. In the backward pass, we need to sum the gradients $\mathbf {dK}$ and $\mathbf {dV}$ across different heads that were implicitly duplicated. 
> Multi-quary attention/Grouped-query attention: 
> å¤šä¸ª query å¤´ï¼Œå•ä¸ª key å¤´å’Œ value å¤´ï¼Œä»¥å‡å°‘æ¨ç†æ—¶çš„ KV cache å¤§å° (ä»…éœ€ä¿å­˜å•ä¸ªå¤´çš„ KV å³å¯)
> FlashAttention-2 å¯¹å…¶çš„å®ç°ï¼š
> FlashAttention-2 æ²¡æœ‰é€‰æ‹©å°† key å¤´å’Œ value å¤´è¿›è¡Œ copyï¼Œè€Œæ˜¯éšå¼åœ°ä¿®æ”¹ç´¢å¼•ï¼Œæ³¨æ„åå‘ä¼ æ’­ä¸­ï¼Œéœ€è¦åœ¨ä¸åŒçš„å¤´ä¸­ç´¯ç§¯ $\mathbf {dK}, \mathbf {dV}$

## 3.2 Parallelism
The first version of FlashAttention parallelizes over batch size and number of heads. We use 1 thread block to process one attention head, and there are overall batch size $\cdot$ number of heads thread blocks. Each thread block is scheduled to run on a streaming multiprocessor (SM), and there are 108 of these SMs on an A100 GPU for example. This scheduling is efficient when this number is large $(\mathrm{say}\geq80)$, since we can effectively use almost all of the compute resources on the GPU. 
> FlashAttention åœ¨ batch size å’Œ number of heads ç»´åº¦å¹¶è¡Œï¼Œåœ¨æ¯ä¸ª batch ä¸­ï¼Œæ¯ä¸ª thread block å¤„ç†ä¸€ä¸ª attention headï¼Œå› æ­¤æ€»çš„ thread block æ•°é‡å°±æ˜¯ batch size ä¹˜ä¸Š number of heads
> GPU å°†æ¯ä¸ª thread block è°ƒåº¦åˆ°ä¸€ä¸ª SM ä¸Šï¼Œå½“ thread block æ•°é‡è¾ƒå¤§æ—¶ï¼Œå¯ä»¥æœ‰é«˜çš„ occupancyï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨å¤§éƒ¨åˆ† GPU è®¡ç®—èµ„æºã€‚ä¾‹å¦‚ A100 æœ‰108ä¸ª SMï¼Œå½“ thread block æ•°é‡å¤§äº 80 æ—¶ï¼Œoccupancy å°±è¾ƒé«˜

In the case of long sequences (which usually means small batch sizes or small number of heads), to make better use of the multiprocessors on the GPU, we now additionally parallelize over the sequence length dimension. This results in significant speedup for this regime. 
> å½“åºåˆ—è¾ƒé•¿ï¼Œbatch size å’Œ number of heads ä¸€èˆ¬å°±ä¼šè¾ƒå°‘ï¼Œå› æ­¤ï¼Œä¸ºäº†æé«˜è¿™ç§æƒ…å†µä¸‹çš„ occupancyï¼Œæˆ‘ä»¬å¯¹ FlashAttention åšå‡ºæ”¹è¿›ï¼Œä½¿å…¶é¢å¤–åœ¨åºåˆ—é•¿åº¦ç»´åº¦å¹¶è¡Œ
> è¿™å¯ä»¥å¸¦æ¥æ˜¾è‘—çš„åŠ é€Ÿ

**Forward pass.** We see that the outer loop (over sequence length) is embarrassingly parallel, and we schedule them on different thread blocks that do not need to communicate with each other. We also parallelize over the batch dimension and number of heads dimension, as done in FlashAttention . The increased parallelism over sequence length helps improve occupancy (fraction of GPU resources being used) when the batch size and number of heads are small, leading to speedup in this case. 
> Algorithm 1ä¸­ï¼Œå¤–å±‚å¾ªç¯æ˜¯å¯ä»¥å¹¶è¡Œçš„ ($\mathbf {Q}$ å—ä¹‹é—´ä¸ç›¸äº’ä¾èµ–)ï¼Œå› æ­¤ FlashAttention-2 å°†å¤šä¸ªå¤–å±‚è¿­ä»£è°ƒåº¦åˆ°ä¸åŒçš„ thread block ä¸Š (thread block ä¹‹é—´ä¸èƒ½ç›¸äº’é€šè®¯)ï¼Œæ¢å¥è¯è¯´å°±æ˜¯å±•å¼€äº†å¤–å±‚å¾ªç¯
> åŒæ—¶ï¼ŒFlashAttention-2 ä¿æŒäº†åœ¨ batch size ç»´åº¦å’Œ number of heads ç»´åº¦çš„å¹¶è¡Œ
> åœ¨åºåˆ—é•¿åº¦ ($\mathbf {Q}$ å—) ç»´åº¦ä¸Šé¢å¤–çš„å¹¶è¡Œå¸®åŠ©æé«˜äº†å½“ batch size å’Œ number of heads è¾ƒå°æ—¶çš„ occupancy (GPU èµ„æºè¢«ä½¿ç”¨çš„æ¯”ä¾‹)

**Backward pass.** Notice that the only shared computation between different column blocks is in update $\mathbf {dQ}$ in Algorithm 2, where we need to load $\mathbf{d}\mathbf{Q}_{i}$ from HBM to SRAM, then on chip, update $\mathbf{d}\mathbf{Q}_{i}\longleftarrow\mathbf{d}\mathbf{Q}_{i}\!+\!\mathbf{d}\mathbf{S}_{i}^{(j)}\mathbf{K}_{j}$ , and write back to HBM. We thus parallelize over the sequence length dimension as well, and schedule 1 thread block for each column block of the backward pass. We use atomic adds to communicate between different thread blocks to update $\mathbf {dQ}$ .
> Algorithm 2 ä¸­ï¼Œå¯¹äº $\mathbf {dQ}$ å—çš„æ›´æ–°æ˜¯å”¯ä¸€çš„åœ¨åˆ—å— ($\mathbf {K}$) ä¹‹é—´å…±äº«çš„è®¡ç®— ($\mathbf {dQ}_i \longleftarrow \mathbf {dQ}_i + \mathbf {dS}_i^{(j)}\mathbf K_j$)ï¼Œä¹Ÿå°±æ˜¯éœ€è¦å…¨éƒ¨å¤–å±‚å¾ªç¯æ‰èƒ½å®Œæˆçš„è®¡ç®—ï¼Œä½†è¯¥è®¡ç®—ä»…æ˜¯å…±äº«ï¼Œä¸å­˜åœ¨å¤–å±‚å¾ªç¯é—´çš„ä¾èµ–ï¼›è€Œå¯¹äº $\mathbf {dK, dV}$ å—çš„è®¡ç®—éƒ½æ˜¯å®Œå…¨åœ¨å†…å±‚å¾ªç¯ä¸­å®Œæˆçš„ï¼Œä¹Ÿæ²¡æœ‰å¤–å±‚å¾ªç¯é—´çš„ä¾èµ–ï¼Œå› æ­¤å¯ä»¥å°†å¤–å±‚å¾ªç¯å±•å¼€
> å› æ­¤æˆ‘ä»¬åŒæ ·è€ƒè™‘åœ¨åºåˆ—é•¿åº¦ç»´åº¦ ($\mathbf {K, V}$ å—) å¹¶è¡Œï¼Œä¸ºæ¯ä¸ªåˆ—å—è°ƒåº¦ä¸€ä¸ª thread blockï¼Œæ¯ä¸ª thread block è®¡ç®—å„è‡ªçš„ $\mathbf {dK, dV}$ æ—¶ä¸éœ€è¦äº’ç›¸é€šè®¯ï¼Œå„è‡ªå¹¶è¡Œè®¡ç®—ï¼Œè€Œå¯¹äº $\mathbf {dQ}$ çš„æ›´æ–°åˆ™é‡‡ç”¨ atomic add è¿›è¡Œé€šè®¯

> å¹¶è¡ŒåŒ–çš„å‰å‘å’Œåå‘çš„å›¾è§£è§[[#Figure Illustration for FlashAttention-2 Parallism|é™„å½•]]

We describe the parallelization scheme in Fig. 2. 

![[FlashAttention2-Fig2.png]]

**Decoding.** During LLM inference, most of the time is spent on iterative decoding, where one token is predicted at a time. The bottleneck for the attention operation during decoding is different from that during training or prefill (prompt processing), because the query length is very short (often query length is 1 since only the new extra token is attending to all the previous tokens, stored in the KV cache). As a result, the bottleneck is no longer the read/write of intermediate matrices (the scores $\mathbf{Q}\mathbf{K}^{\top}$ and attention probabilities $\text{softmax}(\mathbf Q\mathbf K^\top)$ ). Instead, the bottleneck is to load the KV cache as quickly as possible. 
> LLM æ¨ç†æ—¶ï¼Œå¤§å¤šæ•°æ—¶é—´æ˜¯åœ¨è¿›è¡Œè¿­ä»£å¼è§£ç ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡é¢„æµ‹ä¸€ä¸ª token
> è¿­ä»£å¼è§£ç æ—¶çš„ attention è¿ç®—çš„ç“¶é¢ˆå’Œè®­ç»ƒæˆ– prefill (å¤„ç† prompt) æ—¶çš„ç“¶é¢ˆæ˜¯ä¸åŒçš„ï¼Œè¿­ä»£å¼è§£ç æ—¶ query é•¿åº¦å¾ˆçŸ­ï¼Œå¸¸å¸¸ä»…ä¸º 1ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ª token attend to ä¹‹å‰çš„ tokensï¼Œä¹‹å‰çš„ tokens ä¹Ÿä¸éœ€è¦å†å‚ä¸è®¡ç®—ï¼Œå®ƒä»¬ä¸ä½œä¸º queryï¼Œkey å’Œ value å€¼ä¹Ÿå·²ç»å­˜å‚¨åœ¨äº† KV cache
> å› æ­¤ï¼Œè¿­ä»£å¼è§£ç æ—¶ç“¶é¢ˆå°±ä¸å†æ˜¯è¯»å†™ä¸­é—´çŸ©é˜µ (åˆ†æ•°çŸ©é˜µ $\mathbf {QK}^{\top}$ å’Œ attention æ¦‚ç‡çŸ©é˜µ $\text{softmax}(\mathbf {QK}^\top)$)ï¼Œå› ä¸ºä»…æœ‰ 1 ä¸ª queryï¼Œä¸­é—´çŸ©é˜µä»…æœ‰ 1 è¡Œã€‚æ­¤æ—¶çš„ç“¶é¢ˆåœ¨äºå°½å¿« load KV cache

To accommodate this setting, we split the KV cache loading among different thread blocks, to increase occupancy and saturate the HBM bandwidth. However, since the thread blocks cannot easily communicate with each other, we write intermediate results to HBM, then call a separate kernel to reduce the results and produce final output. 
> ä¸ºäº†æé«˜è¿­ä»£å¼è§£ç æ—¶çš„è¡¨ç°ï¼Œæˆ‘ä»¬å°† KV cache loading åˆ’åˆ†ç»™ä¸åŒçš„ thread blockï¼Œä»¥æé«˜ occupancyï¼Œé¥±å’Œ HBM å¸¦å®½ (å¤šçº¿ç¨‹è£…è½½æ•°æ®)
> thread block é—´æ— æ³•é€šè®¯ï¼Œæ•…æˆ‘ä»¬å°†ä¸­é—´ç»“æœå†™å› HBMï¼Œç„¶åè°ƒç”¨å¦ä¸€ä¸ª kernel å°†ç»“æœå½’çº¦

> å¹¶è¡Œæ€§æ–¹é¢ï¼ŒFlashAttention ä»…åœ¨ batch size å’Œ number of heads ç»´åº¦è¿›è¡Œäº†å¹¶è¡Œï¼ŒFlashAttention-2 çš„ç®—æ³•ç›¸è¾ƒäº FlashAttention çš„ä¸»è¦å·®å¼‚åœ¨äºäº¤æ¢äº†å†…å±‚å’Œå¤–å±‚å¾ªç¯ï¼Œç„¶åå°†å¤–å±‚å¾ªç¯åœ¨ thread block ä¹‹é—´å¹¶è¡Œ
> ä½†äº‹å®ä¸Šï¼Œå³ä¾¿æ²¡æœ‰äº¤æ¢å†…å±‚å’Œå¤–å±‚å¾ªç¯ï¼ŒFlashAttention çš„ç®—æ³•è®©å¤–å±‚å¾ªç¯åœ¨ thread block ä¹‹é—´å¹¶è¡Œï¼Œåªéœ€è¦æœ€åæ·»åŠ ä¸€ä¸ªå½’çº¦æ­¥éª¤å³å¯
> å› æ­¤ï¼Œé‡ç‚¹å®é™…ä¸Šåœ¨äº FlashAttention-2 é¢å¤–æ·»åŠ äº†ä¸€ä¸ªå¹¶è¡Œç»´åº¦ï¼Œå°† thread block è´Ÿè´£çš„åŒºåŸŸåˆ‡å¾—æ›´ç»†ç²’åº¦ (ä»å®Œæ•´çš„ $\mathbf O$ åˆ° $\mathbf O$ å—)
> å°†ä¸åŒçš„ $\mathbf {O}$ å—åˆ’åˆ†ç»™ä¸åŒçš„ thread block è´Ÿè´£åº”è¯¥æ˜¯å¾ˆè‡ªç„¶çš„æƒ³æ³•ï¼Œåœ¨ç®—æ³•å†™å‡ºæ¥åå°±åº”è¯¥å¯ä»¥æƒ³åˆ°å¯ä»¥æ ¹æ®å¤–å±‚å¾ªç¯è¿›è¡Œå¹¶è¡Œï¼Œæˆ–è€…äº¤æ¢å†…å¤–å±‚å¾ªç¯è¿›è¡Œå¹¶è¡Œï¼ŒFlashAttention-2 çš„ better parallelism æ˜¯è‡ªç„¶çš„å®ç°ç»“æœ
> åå‘ä¼ æ’­ä¹Ÿæ˜¯ç±»ä¼¼çš„æ€è·¯

## 3.3 Work Partitioning Between Warps
As Section 3.2 describe how we schedule thread blocks, even within each thread block, we also have to decide how to partition the work between different warps. We typically use 4 or 8 warps per thread block, and the partitioning is described in Fig. 3. 
> Section 3.2ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†å¦‚ä½•è°ƒåº¦ thread block (å°†å¤–å±‚å¾ªç¯åˆ’åˆ†ç»™ thread blocks)
> æœ¬èŠ‚é˜è¿°å¦‚ä½•åœ¨ thread block å°†å·¥ä½œè°ƒåº¦ç»™ä¸åŒçš„ warpï¼Œæˆ‘ä»¬æ¯ä¸ª thread block ä½¿ç”¨ 4-8 ä¸ª warps

> ä¸Šä¸€èŠ‚çš„ thread blocks è°ƒåº¦æœ¬è´¨å°±æ˜¯å¤–å±‚å¾ªç¯çš„åˆ’åˆ†
> æœ¬èŠ‚çš„ thread block å†…çš„ warps è°ƒåº¦æœ¬è´¨å°±æ˜¯å†…å±‚å¾ªç¯çš„åˆ’åˆ†

**Forward pass.** For each block, FlashAttention splits $\mathbf{K}$ and $\mathbf{V}$ across 4 warps while keeping $\mathbf{Q}$ accessible by all warps. Each warp multiplies to get a slice of $\mathbf{Q}\mathbf{K}^{\top}$ , then they need to multiply with a slice of $\mathbf{V}$ and communicate to add up the result. This is referred to as the â€œsplit-Kâ€ scheme. However, this is inefficient since all warps need to write their intermediate results out to shared memory, synchronize, then add up the intermediate results. These shared memory reads/writes slow down the forward pass in FlashAttention. 
> åœ¨æ¯ä¸ª block å†… (å®é™…ä¸Šæ˜¯æ¯ä¸ªå†…å±‚å¾ªç¯å†…)ï¼ŒFlashAttention å°† $\mathbf {K, V}$ åˆ’åˆ†ç»™ 4 ä¸ª warpï¼Œ$\mathbf Q$ ä¸è¢«åˆ’åˆ†ï¼Œå¯ä»¥è¢«æ‰€æœ‰ warp è®¿é—® (ç¤ºæ„å›¾è§[[#FlashAttention|é™„å½•]])
> æ¯ä¸ª warp è´Ÿè´£ä¸€ä¸ª slice çš„ $\mathbf {QK}^\top$ï¼Œç„¶åå’Œä¸€ä¸ª slice $\mathbf V$ ç›¸ä¹˜ï¼Œæœ€åç›¸äº’é€šè®¯ï¼Œç´¯åŠ ç»“æœï¼Œè¿™è¢«ç§°ä¸º split-K æ–¹æ³•
> è¯¥æ–¹æ³•ä¸­ï¼Œæ‰€æœ‰çš„ warp éœ€è¦å°†å®ƒè®¡ç®—çš„ä¸­é—´ç»“æœå†™å› shared memoryï¼Œç„¶ååŒæ­¥ç­‰å¾…å…¶ä»– warps å®Œæˆï¼Œæœ€åç´¯åŠ 
> å¯¹ shared memory çš„è¿™éƒ¨åˆ†è¯»å†™æ‹–æ…¢äº† FlashAttention çš„å‰å‘è¿‡ç¨‹

>  åœ¨ warps ä¸­åˆ’åˆ† $\mathbf {K, V}$ å—ä¼šä½¿å¾—(åˆ—)å®Œæ•´çš„ $\mathbf S$ å—éœ€è¦æ‰€æœ‰çš„ warp å®Œæˆè®¡ç®—æ‰èƒ½å¾—åˆ°ï¼Œè¿™ç±»åˆ’åˆ†å¯¹åº”çš„ç®—æ³•æµç¨‹å¯ä»¥æè¿°ä¸ºï¼š
>  1. warps æ ¹æ®å®Œæ•´çš„ $\mathbf Q$ å—å’Œå„è‡ªçš„ $\mathbf K$ slice è®¡ç®— $\mathbf S$ slice
>  2. warps å°†è®¡ç®—çš„ $\mathbf S$ slice å†™å› shared memory
>  3. sync warps
>  4. warps  å–å®Œæ•´çš„ $\mathbf S$ å—ï¼Œè®¡ç®— $m, \ell$ ç»Ÿè®¡é‡ï¼Œå¯¹ $\mathbf S$ slice æ”¾ç¼©ï¼Œå¾—åˆ° $\mathbf P$ slice
>  5. warps æ ¹æ® $\mathbf P$ slice å’Œ $\mathbf V$ slice è®¡ç®— $\mathbf O$ å—ç´¯åŠ å€¼
>  6. warps å°†è®¡ç®—çš„ $\mathbf O$ å—ç´¯åŠ å€¼ç´¯åŠ å› shared memory ä¸­çš„ $\mathbf O$ å— 
>  (è¿™éœ€è¦ warp å°† shared memory ä¸­çš„ $\mathbf O$ å— load åˆ°å¯„å­˜å™¨ï¼Œå¯¹å…¶ç´¯åŠ ï¼Œå†å°†ç»“æœå†™å›åˆ° shared memoryã€‚å¦‚æœä½¿ç”¨ shared memory ä¸­ç•™ç»™ $\mathbf O$ å—çš„ç©ºé—´ä»…æœ‰ä¸€å—ï¼Œè¯¥æ“ä½œè¿˜éœ€è¦ warp ä¹‹é—´æœ‰å¯¹è¯¥ç©ºé—´çš„äº’æ–¥é”ï¼Œå¦‚æœæ¯ä¸ª warp å„è‡ªæœ‰å¯¹åº”çš„ $\mathbf O$ å—çš„ç©ºé—´ï¼Œåˆ™éœ€è¦çš„æ˜¯å…ˆåŒæ­¥ï¼ŒåŒæ­¥åå†ç”±ä¸€ä¸ª warp è¿›è¡Œå½’çº¦)
>  8. sync warps

In FlashAttention-2, we instead split $\mathbf{Q}$ across 4 warps while keeping $\mathbf{K}$ and $\mathbf{V}$ accessible by all warps. After each warp performs matrix multiply to get a slice of $\mathbf{Q}\mathbf{K}^{\top}$ , they just need to multiply with their shared slice of $\mathbf V$ to get their corresponding slice of the output. There is no need for communication between warps. The reduction in shared memory reads/writes yields speedup (Section 4). 
> FlashAttention-2 å°† $\mathbf Q$ åˆ’åˆ†ç»™ 4 ä¸ª warpsï¼Œæ‰€æœ‰ warps éƒ½å¯ä»¥è®¿é—® $\mathbf {K, V}$
> æ¯ä¸ª warp è®¡ç®—å¾—åˆ°ä¸€ä¸ª $\mathbf {QK}^\top$ sliceï¼Œç„¶åç›´æ¥å’Œå®Œæ•´çš„ $\mathbf V$ ç›¸ä¹˜å¾—åˆ°ä¸€ä¸ª $\mathbf O$ sliceï¼Œä¸éœ€è¦ç´¯åŠ å½’çº¦ (ç¤ºæ„å›¾è§[[#FlashAttention-2|é™„å½•]])ï¼Œæ•… warps ä¹‹é—´ä¸éœ€è¦é€šè®¯ï¼Œè¿›è€Œå‡å°‘äº† shared memory çš„è¯»å†™

> åœ¨ warps ä¸­åˆ’åˆ† $\mathbf Q$ å—ä½¿å¾— $\mathbf S$ slice æ˜¯åˆ—å®Œæ•´çš„ï¼Œè¿™ç±»åˆ’åˆ†å¯¹åº”çš„ç®—æ³•æµç¨‹å¯ä»¥æè¿°ä¸ºï¼š
> 1. warps æ ¹æ®å„è‡ªçš„ $\mathbf Q$ slice å’Œå®Œæ•´çš„ $\mathbf K$ å—è®¡ç®— $\mathbf S$ slice
> 2. warps æ ¹æ® $\mathbf S$ slice è®¡ç®— $m, \ell$ ç»Ÿè®¡é‡ï¼Œå¯¹ $\mathbf S$ slice æ”¾ç¼©ï¼Œå¾—åˆ° $\mathbf P$ slice
> 3. warps æ ¹æ® $\mathbf P$ slice å’Œå®Œæ•´çš„ $\mathbf V$ å—è®¡ç®— $\mathbf O$ slice
> 4. warps å°† $\mathbf O$ slice å†™å› shared memory
> 5. sync warps

> ç¬¬ä¸€ä¸ªç®—æ³•æ¯ä¸€æ­¥å¯¹äº shared memory çš„è¯»å†™æœ‰ï¼š
> 1. è¯»å– $\mathbf Q$ å—å’Œ $\mathbf K$ slice (è®¡ç®— $\mathbf S$ slice)
> 2. å†™å› $\mathbf S$ slice
> 3. None
> 4. è¯»å– $\mathbf S$ å— (è®¡ç®— $\mathbf P$ slice)
> 5. è¯»å– $\mathbf V$ slice (è®¡ç®— $\mathbf O$ å—ç´¯åŠ å€¼)
> 6. å†™å› $\mathbf O$ å—ç´¯åŠ å€¼
> 7. None
> ç¬¬äºŒä¸ªç®—æ³•å…¨æ¯ä¸€æ­¥å¯¹äº shared memory çš„è¯»å†™æœ‰ï¼š
> 1. è¯»å– $\mathbf Q$ slice å’Œ $\mathbf K$ å— (è®¡ç®— $\mathbf S$ slice)
> 2. None (è®¡ç®— $\mathbf P$ slice)
> 3. è¯»å– $\mathbf V$ å— (è®¡ç®— $\mathbf O$ slice)
> 4. å†™å› $\mathbf O$ slice
> 5. None
> äºŒè€…ä¸»è¦çš„å·®å¼‚åœ¨äºï¼š
> 1. ç¬¬ä¸€ä¸ªç®—æ³•å¤šäº†ä¸­é—´å†™å› $\mathbf S$ sliceã€è¯»å– $\mathbf S$ ï¼Œè¿™æ˜¯ warps ä¹‹é—´ä¸ºäº†è®¡ç®— $\mathbf P$ è€Œå¿…è¦çš„é€šè®¯ï¼›å¦‚æœç®—æ³•å†™å¾—å¥½ï¼Œç¬¬ä¸€ä¸ªç®—æ³•å¯ä»¥ä¼˜åŒ–ä¸ºä»…å¯¹å„è‡ª $\mathbf S$ slice çš„ç»Ÿè®¡é‡è¿›è¡Œé€šè®¯ï¼Œæœ€å°åŒ–é€šè®¯å¼€é”€ï¼Œä½†æ˜¯æ€»ä½“ä¸Šä»å¤šå‡ºä¸€æ¬¡åŒæ­¥ä»¥åŠéƒ¨åˆ†é€šè®¯å¼€é”€
> 2. ç¬¬ä¸€ä¸ªç®—æ³•å¤šäº†æœ€åå¯¹ $\mathbf O$ çš„ç´¯åŠ æ‰€éœ€è¦çš„æ¯ä¸ª warp ä» shared memory å¯¹ $\mathbf O$ çš„è¯»å–å’Œè®¡ç®—åçš„å†™å›



![[FlashAttention2-Fig3.png]]

**Backward pass.** Similarly for the backward pass, we choose to partition the warps to avoid the â€œsplit-Kâ€ scheme. However, it still requires some synchronization due to the more complicated dependency between all the different inputs and gradients $\mathbf {Q , K , V , O , dO , dQ , dK , dV}$ . Nevertheless, avoiding â€œsplit-Kâ€ reduces shared memory reads/writes and again yields speedup (Section 4). 
> åå‘ä¼ æ’­ä¸­å¯¹äº warps çš„åˆ’åˆ†ä»ç„¶é€‰æ‹©é¿å… split-Kï¼Œå‡å°‘ shared memory è¯»å†™æ¬¡æ•°

**Tuning block sizes** Increasing block sizes generally reduces shared memory loads/stores, but increases the number of registers required and the total amount of shared memory. Past a certain block size, register spilling causes significant slowdown, or the amount of shared memory required is larger than what the GPU has available, and the kernel cannot run at all. Typically we choose blocks of size $\{64{,}128\}{\times}\{64{,}128\}$ , depending on the head dimension $d$ and the device shared memory size. 
> block size å¢å¤§ä¸€èˆ¬å¯ä»¥å‡å°‘ shared memory load/store æ¬¡æ•°ï¼Œä½†ä¼šæé«˜å¯„å­˜å™¨éœ€æ±‚é‡å’Œ shared memory éœ€æ±‚æ€»é‡
> å¯„å­˜å™¨éœ€æ±‚é‡è¿‡å¤§ä¼šå¯¼è‡´ register spillingï¼Œshared memory éœ€æ±‚é‡è¿‡å¤§ä¼šå¯¼è‡´ kernel æ— æ³•è¿è¡Œ
> FlashAttention-2 çš„ block size æœ‰ (64, 64), (64, 128), (128, 64), (128, 128) å››ç§é€‰æ‹©ï¼Œå–å†³äºç¡¬ä»¶æ¡ä»¶å’Œå¤´ç»´åº¦ $d$

We manually tune for each head dimensions since there are essentially only 4 choices for block sizes, but this could benefit from auto-tuning to avoid this manual labor. We leave this to future work. 
> å¯¹äºæ¯ä¸ªå¤´ç»´åº¦ï¼Œæˆ‘ä»¬æ‰‹åŠ¨è°ƒèŠ‚ block sizeï¼Œé€‰æ‹©æœ€ä¼˜çš„é…ç½®

# 4 Empirical Validation
We evaluate the impact of using FlashAttention-2 to train Transformer models. 

- **Benchmarking attention.** We measure the runtime of FlashAttention -2 across different sequence lengths and compare it to a standard implementation in PyTorch, FlashAttention , and FlashAttention in Triton. We confirm that FlashAttention-2 is $1.7{-}3.0\times$ faster than FlashAttention , 1.3-2.5 Ã— faster than FlashAttention in Triton, and 3-10 Ã— faster than a standard attention implementation. FlashAttention-2 reaches up to 230 TFLOPs/s, 73% of the theoretical maximum TFLOPs/s on A100 GPUs. 
> åœ¨ä¸åŒçš„åºåˆ—é•¿åº¦ä¸‹ï¼ŒFlashAttention-2 æ¯” FlashAttention å¿« 1-3å€ï¼ŒA100ä¸Šå¯ä»¥åˆ° 230 TFLOPs/s

- **End-to-end training speed.** When used end-to-end to train GPT-style models of size 1.3B and 2.7B on sequence lengths either 2k or 8k, FlashAttention-2 yields up to $1.3\times$ speedup compared to FlashAttention and $2.8\times$ speedup compared to a baseline without FlashAttention . FlashAttention-2 reaches up to 225 TFLOPs/s (72% model FLOPs utilization) per A100 GPU. 

## 4.1 Benchmarking Attention for Training
We measure the runtime of different attention methods on an A100 80GB SXM4 GPU for different settings (without / with causal mask, head dimension 64 or 128). We report the results in Fig. 4, Fig. 6 and Fig. 7, showing that FlashAttention-2 is around $2\times$ faster than FlashAttention and FlashAttention in `xformers` (the â€œcutlassâ€ implementation). FlashAttention-2 is around $1.3{\cdot}1.5\times$ faster than FlashAttention in Triton in the forward pass and around $2\times$ faster in the backward pass. Compared to a standard attention implementation in PyTorch, FlashAttention-2 can be up to $10\times$ faster. 

Benchmark setting: we vary the sequence length from 512, 1k, ..., 16k, and set batch size so that the total number of tokens is 16k. We set hidden dimension to 2048, and head dimension to be either 64 or 128 (i.e., 32 heads or 16 heads). 
> benchmark å…¶ä½™è®¾å®šï¼š
> sequence lenï¼š 512, 1k, ..., 16k
> batch sizeï¼š ä¿æŒ batch æ€» token æ•°ä¸º 16k
> hidden dimï¼š2048
> head dimï¼š64 æˆ– 128 (head æ•°é‡ä¸º 32 æˆ– 16)

To calculate the FLOPs of the forward pass, we use: 
> (FLOPs åŒ…æ‹¬æµ®ç‚¹ä¹˜å’Œæµ®ç‚¹åŠ )

$$
4\cdot \text{seqlen}^2\cdot\text{head dimension}\cdot \text{number of heads}
$$

With causal mask, we divide this number by 2 to account for the fact that approximately only half of the entries are calculated. To get the FLOPs of the backward pass, we multiply the forward pass FLOPs by 2.5 (since there are 2 matmuls in the forward pass and 5 matmuls in the backward pass, due to recomputation). 
> å‰å‘çš„ FLOPs å¦‚ä¸Šè®¡ç®—å¾—åˆ°
> causal mask ä¸‹ï¼Œè¯¥å€¼é™¤ä»¥2ï¼Œå› ä¸ºä»…æœ‰å¤§çº¦ä¸€åŠçš„ entry ä¼šè¢«è®¡ç®—
> åå‘çš„ FLOPs ç­‰äºå‰å‘çš„ FLOPs ä¹˜ä»¥ 2.5 (åå‘æœ‰5ä¸ª matmulï¼Œå‰å‘æœ‰2ä¸ª matmul)

![[FlashAttention2-Fig4.png]]

Just running the same implementation on H100 GPUs (using no special instructions to make use of new features such as TMA and 4th-gen Tensor Cores), we obtain up to 335 TFLOPs/s (Fig. 8). We expect that by using new instructions, we can obtain another $1.5\mathbf{x}â€“2\mathbf{x}$ speedup on H100 GPUs. We leave that to future work. 

## 4.2 Benchmarking Attention for Inference
We benchmark the attention kernel during decoding for the case of multi-query attention, where the bottleneck is loading the KV cache. In Fig. 5, we see that the attention kernel from FlashAttention-2 is up to $28\times$ faster than a naive implementation in PyTorch, and up to $7\times$ faster than an implementation from Faster Transformer. This is thanks to better work partitioning where multiple thread blocks are loading the KV cache at the same time to saturate HBM bandwidth. 
> å°† KV cache loading åˆ’åˆ†ç»™å¤šä¸ª thread blocks æ‰§è¡Œå……åˆ†åˆ©ç”¨äº† HBM å¸¦å®½ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦

![[FlashAttention2-Fig5.png]]

## 4.3 End-to-End Performance
We measure the training throughput of GPT-style models with either 1.3B or 2.7B parameters, on $8{\times}\mathrm{A}100$ 80GB SXM4. As shown in Table 1 FlashAttention-2 yields $2.8\times$ speedup compared to a baseline without FlashAttention and $1.3\times$ speedup compared to FlashAttention, reaching up to 225 TFLOPs/s per A100 GPU. 

Note that we calculate the FLOPs by the formula, following Megatron-LM (Shoeybi et al., 2019) (and many other papers and libraries): 

$$
6\cdot \text{seqlen}\cdot\text{number of params} + 12 \cdot\text{number of layers}\cdot\text{hidden dim}\cdot \text{seqlen}^2
$$

The first term accounts for the FLOPs due to weightâ€“input multiplication, and the second term accounts for the FLOPs due to attention. However, one can argue that the second term should be halved, as with causal mask we only need to compute approximately half the number of elements in attention. We choose to follow the formula from the literature (without dividing the attention FLOPs by 2) for consistency. 
> ä¸Šè¿°çš„ FLOPs è®¡ç®—ä¸­ï¼š
> ç¬¬ä¸€é¡¹è®¡ç®—çš„æ˜¯æƒé‡-è¾“å…¥çŸ©é˜µä¹˜çš„ FLOPs
> ç¬¬äºŒé¡¹è®¡ç®—çš„æ˜¯ attention çš„ FLOPsï¼Œåœ¨ causal mask ä¸‹å¯ä»¥é™¤ä»¥äºŒ

![[FlashAttention2-Table 1.png]]

# 5 Discussion and Future Directions
FlashAttention-2 is $2\times$ faster than FlashAttention, which means that we can train models with 16k longer context for the same price as previously training a 8k context model, for the same number of tokens. We are excited about how this can be used to understand long books and reports, high resolution images, audio and video. FlashAttention-2 will also speed up training, finetuning, and inference of existing models. 

In the near future, we plan to collaborate with researchers and engineers to make FlashAttention widely applicable in different kinds of devices (e.g., H100 GPUs, AMD GPUs), as well as new data types such as FP8. As an immediate next step, we plan to optimize FlashAttention-2 for H100 GPUs to use new hardware features (TMA, 4th-gen Tensor Cores, fp8). Combining the low-level optimizations in FlashAttention-2 with high-level algorithmic changes (e.g., local, dilated, block-sparse attention) could allow us to train AI models with much longer context. We are also excited to work with compiler researchers to make these optimization techniques easily programmable. 

# A FlashAttention-2 Backward Pass

![[FlashAttention2-Algorithm 2.png]]

**Algorithm 2** FlashAttention-2 Backward Pass
**Require**: Matrices $\mathbf {Q, K, V, O, dO} \in \mathbb R^{N\times d}$ in HBM, vectors $L \in \mathbb R^N$ in HBM,  block sizes $B_c, B_r$. 
 1: Divide $\bf Q$ into $T_r = \lceil \frac N {B_r} \rceil$ blocks $\bf Q_1, \dots, Q_{T_r}$ of size $B_r \times d$ each, and divide $\bf K, V$ into $T_c = \lceil \frac N {B_c} \rceil$ blocks $\bf K_1, \dots, K_{T_c}$ and $\bf V_1, \dots, V_{T_c}$ of size $B_c \times d$ each.
 2: Divide $\bf O$ into $T_r$ blocks $\bf O_1, \dots, O_{T_r}$ of size $B_r \times d$ each, divide $\bf dO$ into $T_r$ blocks $\bf dO_i, \dots, dO_{T_r}$ of size $B_r \times d$ each, divide $L$ into $T_r$ blocks $L_1, \dots, L_{T_r}$ of size $B_r$ each.
 3: Initialize $\mathbf {dQ} = (0)_{N\times d}$ in HBM and divide it into $T_r$ blocks $\bf dQ_1, \dots, dQ_{T_r}$ of size $B_r \times d$ each. Divide $\mathbf {dK, dV} \in \mathbb R^{N\times d}$ in HBM and divide it into $T_c$ blocks of size $B_c\times d$ each.
 4: Compute $D = \text{rowsum}(\mathbf {dO}\circ \mathbf O) \in \mathbb R^d$ (pointwise multiply), write $D$ to HBM and divide it into $T_r$ blocks $D_1, \dots, D_{T_r}$ of size $B_r$ each.
> FlashAttention-2 å°† $\text{rowsum}(\mathbf {dO}\circ \mathbf O)$ çš„è®¡ç®—æå‰åˆ°å¾ªç¯ä¹‹å¤–ï¼Œè®¡ç®—å¥½ååˆ†å—ï¼Œä¹‹åå¾ªç¯ä¸­ç›´æ¥ç”¨è®¡ç®—å¥½çš„ $D_i$
 
 4: **for** $1\le j \le T_c$ **do**
 5:   Load $\mathbf {K_j, V_j}$ from HBM to on-chip SRAM.
 6:   Initialize $\mathbf {dK}_j = (0)_{B_c \times d}, \mathbf {dV}_j = (0)_{B_c\times d}$ in SRAM
> å¤–å±‚å¾ªç¯è¿­ä»£ $\mathbf {K, V, dK, dV}$ å—

 7:   **for*** $1 \le i \le T_r$ **do**
 8:     Load $\mathbf {Q_i, O_i, dO_i, dQ_i}, L_i, D_i$ from HBM to on-chip SRAM.
> å†…å±‚å¾ªç¯è¿­ä»£ $\mathbf {Q, O, dO, dQ}, L, D$ å—

 9:     On chip, compute $\mathbf {S}_{i}^{(j)} = \mathbf Q_i \mathbf K_j^{\top} \in \mathbb R^{B_r \times B_c}$.
10:     On chip, compute $\mathbf {P}_{i}^{(j)} = \exp (\mathbf S_{i}^{(j)} - L_i)\in\mathbb R^{B_r \times B_c}$.
>  åœ¨ç‰‡ä¸Šæ ¹æ® $\mathbf {K_j, V_j, Q_i, O_i}$ é‡æ–°è®¡ç®—å¾—åˆ°æƒé‡çŸ©é˜µ $\mathbf P$ çš„å— $\mathbf P_{i}^{(j)}$
>  ($L_i = m_i -\ln (\ell_i)$)

11:     On chip, compute $\mathbf {{dV}}_j \leftarrow \mathbf {{dV}}_j + (\mathbf P_{i}^{(j)})^\top \mathbf {dO}_i  \in\mathbb R^{B_c \times d}$.
> æ›´æ–° $\mathbf {dV}_j$ï¼šæœ¬è´¨æ˜¯ $\mathbf {dV} = \mathbf P^\top \mathbf {dO}$ åˆ†å—å½¢å¼

12:     On chip, compute $\mathbf {dP}_{i}^{(j)} = \mathbf {dO}_{i}\mathbf V_j^{\top}\in \mathbb R^{B_r \times B_c}$.
> è®¡ç®— $\mathbf {dP}_{i}^{(j)}$ ï¼šæœ¬è´¨æ˜¯ $\mathbf {dP} = \mathbf {dOV}^\top$ çš„åˆ†å—å½¢å¼

13:    On chip, compute $\mathbf {dS}_{i}^{(j)} = \mathbf P_{i}^{(j)} \circ (\mathbf {dP}_{i}^{(j)}-D_i) \in \mathbb R^{B_r \times B_c}$.
> æ ¹æ® $\mathbf {dP}_{i}^{(j)}$ å’Œ $D_i$ è®¡ç®— $\mathbf {dS}_{i}^{(j)}$  ($d S_{i j}=P_{i j}(d P_{i j}-D_{i})$)

14:     Load $\mathbf {dQ}_i$ from HBM to SRAM, then on chip, update $\mathbf {dQ}_i \leftarrow \mathbf {dQ}_i + \mathbf {dS}_{i}^{(j)}\mathbf K_j\in \mathbb R^{B_r \times d}$ to HBM.
> æ›´æ–° $\mathbf {dQ}_i$ï¼š$\mathbf {dQ}_i$ æ¯æ¬¡å¤–å±‚å¾ªç¯æ›´æ–°ä¸€æ¬¡ï¼Œå› æ­¤ $\mathbf {dQ}_i$ å—åœ¨å†…å±‚å¾ªç¯ä¸€ç›´ä¿å­˜åœ¨ç‰‡ä¸Šä¸ç°å®ï¼Œæ•…éœ€è¦å†™å› HBM

15:     On chip, compute ${\mathbf {dK}}_j  \leftarrow {\mathbf {dK}_j} +  (\mathbf {dS}_{i}^{(j)})^\top\mathbf Q_i\in\mathbb R^{B_c \times d}$.
> æ›´æ–° $\mathbf {dK}_j$ï¼šæœ¬è´¨æ˜¯ $\mathbf {dK} = \mathbf {dS}^\top \mathbf {Q}$ çš„åˆ†å—å½¢å¼

16:   **end for**
17:   Write $\mathbf {dK}_j ,\mathbf {dV}_j$ to HBM.
> å†…å±‚å¾ªç¯ç»“æŸåï¼Œå¯ä»¥å¾—åˆ°è®¡ç®—å¥½çš„ $\mathbf {dK, dV}$ å—

18: **end for**
19: Return $\mathbf {dQ, dK, dV}$.
> å¤–å±‚å¾ªç¯ç»“æŸåï¼Œ$\mathbf {dQ}$ æ‰èƒ½å®Œæ•´ç®—å®Œ

> FlashAttention-2 çš„åå‘å’Œ FlashAttention å‡ ä¹æ²¡æœ‰å·®åˆ«
> å”¯ä¸€è¾ƒæ˜¾è‘—çš„å·®åˆ«æ˜¯å°† $m, \ell$ ç”¨ $L$ æ›¿ä»£
# B Benchmarking Attention on A100 and H100
# Appendix
## Figure Illustration for FlashAttention-2 forward pass
### $\mathbf {S}$
å¤–å±‚å¾ªç¯ + å†…å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig1.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig2.png]]

å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig3.png]]

### $\mathbf {O}$
å†…å±‚å¾ªç¯ + å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig4.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig5.png]]

å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig6.png]]

### Generalization for forward pass
å†…å±‚å¾ªç¯ + å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig7.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig8.png]]

å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention2-App-Fig9.png]]

## Figure Illustration for FlashAttention-2 Parallism
### Forward pass

![[FlashAttention2-App-Fig10.png]]

### Backward pass
#### $\mathbf {dV}$

![[FlashAttention2-App-Fig11.png]]

#### $\mathbf {dQ}$

![[FlashAttention2-App-Fig12.png]]

#### $\mathbf {dK}$

![[FlashAttention2-App-Fig13.png]]

## Figure Illustration for warp partitioning
### FlashAttention

![[FlashAttention2-App-Fig14.png]]

### FlashAttention-2

![[FlashAttention2-App-Fig15.png]]

