# Abstract 
High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. 
> LLM çš„é«˜ååæœåŠ¡è¦æ±‚åŒæ—¶æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
> ç°æœ‰ç³»ç»Ÿçš„é—®é¢˜åœ¨äºæ¯ä¸ªè¯·æ±‚çš„ KV cache å ç”¨å¾ˆå¤§çš„å†…å­˜ï¼Œå¹¶ä¸”ä¼šåŠ¨æ€åœ°å¢é•¿å’Œç¼©å°ï¼Œå¦‚æœç®¡ç†ä¸å½“ï¼ŒKV cache ç›¸å…³çš„å†…å­˜ä¼šå› ç¢ç‰‡åŒ–å’Œå†—ä½™å¤åˆ¶è€Œè¢«å¤§é‡æµªè´¹ï¼Œæ•…é™åˆ¶äº†èƒ½æ‰¹é‡å¤„ç†çš„è¯·æ±‚æ•°é‡

To address this problem, we propose Paged Attention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by $2â€“4\times$ with the same level of latency compared to the state-of-the-art systems, such as Faster Transformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLMâ€™s source code is publicly available at https://github.com/vllm-project/vllm . 
> æˆ‘ä»¬æå‡º Paged Attentionï¼Œè¯¥ç®—æ³•çµæ„Ÿæ¥æºäº OS ä¸­ç»å…¸çš„è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæŠ€æœ¯
> åŸºäº Paged Attentionï¼Œæˆ‘ä»¬æ„å»º LLM æœåŠ¡ç³»ç»Ÿ vLLMï¼Œå®ƒå®ç°äº†
> (1) KV cache å†…å­˜å‡ ä¹é›¶æµªè´¹
> (2) åœ¨è¯·æ±‚å†…éƒ¨å’Œè¯·æ±‚ä¹‹é—´çµæ´»å…±äº« KV cacheï¼Œè¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨
> è¯„ä¼°è¡¨æ˜å’Œ SOTA çš„ç³»ç»Ÿç›¸æ¯”ï¼Œåœ¨ç›¸åŒçš„å»¶è¿Ÿæ°´å¹³ä¸‹ï¼ŒvLLM å°†æµè¡Œ LLM çš„ååé‡æé«˜äº† 2-4 å€ï¼Œä¸”æ¨¡å‹è¶Šå¤§ã€åºåˆ—è¶Šé•¿ã€è§£ç ç®—æ³•è¶Šå¤æ‚ï¼Œæå‡è¶Šæ˜æ˜¾

# 1 Introduction 
The emergence of large language models ( LLMs ) like GPT [5 , 37] and PaLM [9] have enabled new applications such as programming assistants [6 , 18] and universal chatbots [19 , 35] that are starting to profoundly impact our work and daily routines. Many cloud companies [34 , 44] are racing to provide these applications as hosted services. However, running these applications is very expensive, requiring a large number of hardware accelerators such as GPUs. According to recent estimates, processing an LLM request can be $10\times$ more expensive than a traditional keyword query [43]. Given these high costs, increasing the throughputâ€”and hence reducing the cost per requestâ€”of *LLM serving systems* is becoming more important.  

At the core of LLMs lies an auto regressive Transformer model [53]. This model generates words (tokens), one at a time , based on the input (prompt) and the previous sequence of the outputâ€™s tokens it has generated so far. For each request, this expensive process is repeated until the model outputs a termination token. This sequential generation process makes the workload memory-bound , under utilizing the computation power of GPUs and limiting the serving throughput. 
> LLM åœ¨è¾“å‡ºæ—¶éœ€è¦åŸºäºè¾“å…¥ (prompt) å’Œä¹‹å‰çš„è¾“å‡º token åºåˆ—è‡ªå›å½’ç”Ÿæˆ tokens ç›´åˆ° termination token
> è¯¥åºåˆ—ç”Ÿæˆè¿‡ç¨‹æ˜¯ memory-boundï¼Œæœªèƒ½å®Œå…¨åˆ©ç”¨è®¡ç®—èµ„æºï¼Œå› æ­¤é™ä½äº†æœåŠ¡åå

Improving the throughput is possible by batching multiple requests together. However, to process many requests in a batch, the memory space for each request should be efficiently managed. For example, Fig. 1 (left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM. Approximately $65\%$ of the memory is allocated for the model weights, which remain static during serving. Close to $30\%$ of the memory is used to store the dynamic states of the requests. For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as KV cache [41], which represent the context from earlier tokens to generate new output tokens in sequence. The remaining small percentage of memory is used for other data, including activations â€“ the ephemeral tensors created when evaluating the LLM. Since the model weights are constant and the activations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size. When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig. 1 (right). 
> æé«˜ååçš„ä¸€ä¸ªæ–¹å¼æ˜¯æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œè¿™è¦æ±‚æˆ‘ä»¬é«˜æ•ˆç®¡ç†æ‰¹é‡å†…å„ä¸ªè¯·æ±‚çš„å†…å­˜ç©ºé—´
> Figure 1 left å±•ç¤ºäº† A100 ä¸Š 13B å‚æ•° LLM çš„å†…å­˜åˆ†é…ï¼Œçº¦65%çš„å†…å­˜ç”¨äºæ¨¡å‹æƒé‡ï¼Œè¿™éƒ¨åˆ†å†…å­˜åœ¨ LLM æœåŠ¡æ—¶ä¿æŒå›ºå®šï¼Œè¶Š30%çš„å†…å­˜ç”¨äºå­˜å‚¨è¯·æ±‚çš„åŠ¨æ€çŠ¶æ€ï¼Œä¹Ÿå°±æ˜¯ KV cacheï¼Œå®ƒä»¬è¡¨ç¤ºä¹‹å‰ tokens ç”¨äºç”Ÿæˆæ–°è¾“å‡º token çš„ä¸Šä¸‹æ–‡ï¼Œæœ€åä¸€å°éƒ¨åˆ†å†…å­˜ç”¨äºå…¶ä»–æ•°æ®ï¼ŒåŒ…æ‹¬æ¿€æ´» (è¯„ä¼° LLM æ—¶åˆ›å»ºçš„æš‚æ—¶çš„å¼ é‡)
> å‚æ•°çš„å†…å­˜å›ºå®šï¼Œæ¿€æ´»å ç”¨çš„å†…å­˜å°ï¼Œæ•… KV cache ç®¡ç†çš„æ–¹å¼å°†å†³å®šæˆ‘ä»¬å¯ä»¥è·å¾—çš„æœ€å¤§æ‰¹é‡å¤§å°
> KV cache ç®¡ç†ä¸å½“ï¼Œbatch size å°†è¢«æ˜æ˜¾é™åˆ¶ï¼Œæ•…è€Œé™åˆ¶ LLM çš„ååé‡

![[vLLM-Fig1.png]]

In this paper, we observe that existing LLM serving systems [31 , 60] fall short of managing the KV cache memory efficiently. This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks [33 , 39] require tensors to be stored in contiguous memory. However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique characteristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori. These characteristics make the existing systemsâ€™ approach significantly inefficient in two ways: 
> ç°å­˜çš„ LLM æœåŠ¡ç³»ç»Ÿå°†ä¸€ä¸ªè¯·æ±‚çš„ KV cache å­˜å‚¨åœ¨è¿ç»­çš„å†…å­˜ç©ºé—´ï¼Œå› ä¸ºå¤§å¤šæ•° DL æ¡†æ¶è¦æ±‚ tensor çš„å†…å­˜è¿ç»­
> ä½† KV cache å’Œä¼ ç»Ÿ DL å·¥ä½œè´Ÿè½½ä¸­çš„ tensor ä¸åŒçš„æ˜¯ï¼šéšç€æ¨¡å‹ç”Ÿæˆæ–°çš„ tokensï¼ŒKV cache ä¼šéšç€æ—¶é—´åŠ¨æ€åœ°å¢é•¿å’Œç¼©å°ï¼Œå¹¶ä¸”å…¶å£°æ˜å‘¨æœŸå’Œé•¿åº¦äº‹å…ˆæ˜¯æœªçŸ¥çš„
> è¯¥ç‰¹æ€§ä½¿å¾—ç°å­˜ç³»ç»Ÿåœ¨ä»¥ä¸‹ä¸¤æ–¹é¢æ˜¾è‘—ä½æ•ˆï¼š

First, the existing systems [31 , 60] suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, they pre-allocate a contiguous chunk of memory with the requestâ€™s maximum length (e.g., 2048 tokens). This can result in severe internal fragmentation, since the requestâ€™s actual length can be much shorter than its maximum length (e.g., Fig. 11). Moreover, even if the actual length is known a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the requestâ€™s lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused. Besides, external memory fragmentation can also be significant, since the preallocated size can be different for each request. Indeed, our profiling results in Fig. 2 show that only $20.4\%-38.2\%$ of the KV cache memory is used to store the actual token states in the existing systems. 
> é¦–å…ˆï¼šç°å­˜çš„ç³»ç»Ÿå­˜åœ¨å†…éƒ¨å’Œå¤–éƒ¨å†…å­˜ç¢ç‰‡çš„é—®é¢˜
> ä¸ºäº†å°†ä¸€ä¸ªè¯·æ±‚çš„ KV cache å­˜å‚¨åœ¨è¿ç»­çš„ç©ºé—´ï¼Œç°å­˜ç³»ç»Ÿä¼šé¢„åˆ†é…ä¸€ä¸ªè¿ç»­çš„å†…å­˜å—ï¼Œå¤§å°ä¸ºè¯·æ±‚çš„æœ€å¤§é•¿åº¦ (ä¾‹å¦‚ 2048 tokens)ï¼Œè¿™ä¼šå¯¼è‡´å†…éƒ¨ç¢ç‰‡ï¼Œå› ä¸ºè¯·æ±‚çš„å®é™…é•¿åº¦å¯èƒ½æ¯”æœ€å¤§é•¿åº¦çŸ­å¾ˆå¤š
> å¹¶ä¸”å³ä¾¿å®é™…é•¿åº¦é¢„å…ˆçŸ¥é“ï¼Œé¢„åˆ†é…ä¹Ÿæ˜¯ä½æ•ˆçš„ï¼Œå› ä¸ºæ•´ä¸ªå†…å­˜å—åœ¨è¯·æ±‚çš„å£°æ˜å‘¨æœŸä¸€ç›´è¢«é¢„ç•™ï¼Œå…¶ä»–æ›´çŸ­çš„è¯·æ±‚æ— æ³•åˆ©ç”¨è¯¥å—ä¸­å½“å‰æ²¡æœ‰ä½¿ç”¨çš„éƒ¨åˆ†
> å¤–éƒ¨ç¢ç‰‡çš„é—®é¢˜åŒæ ·å­˜åœ¨ï¼Œå› ä¸ºæ¯ä¸ªè¯·æ±‚é¢„åˆ†é…çš„å¤§å°å¯èƒ½å„ä¸ç›¸åŒ
> Figure 2 å±•ç¤ºäº†ç°å­˜ç³»ç»Ÿ KV cache ä½¿ç”¨çš„å†…å­˜ä¸­ä»…æœ‰ 20.4%-38.2% å®é™…ç”¨äºå­˜å‚¨ token çŠ¶æ€

![[vLLM-Fig2.png]]

Second, the existing systems cannot exploit the opportunities for memory sharing. LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request. In these scenarios, the request consists of multiple sequences that can partially share their KV cache. However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. 
> å…¶æ¬¡ï¼šç°å­˜ç³»ç»Ÿæ— æ³•åˆ©ç”¨å†…å­˜å…±äº«çš„æœºä¼š
> LLM æœåŠ¡ç»å¸¸ä½¿ç”¨é«˜çº§çš„è§£ç ç®—æ³•ï¼Œä¾‹å¦‚å¹¶è¡Œé‡‡æ ·å’ŒæŸæœç´¢ï¼Œè¿™äº›ç®—æ³•å¯¹äºæ¯æ¬¡è¯·æ±‚ä¼šç”Ÿæˆå¤šä¸ªè¾“å‡ºï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œç”±å¤šä¸ªåºåˆ—æ„æˆçš„è¯·æ±‚å¯ä»¥éƒ¨åˆ†åœ°å…±äº«å®ƒä»¬çš„ KV cache
> ç°å­˜ç³»ç»Ÿæ— æ³•å®ç°å†…å­˜å…±äº«ï¼Œå› ä¸ºå„ä¸ªåºåˆ—çš„ KV cache éƒ½å­˜å‚¨åœ¨åˆ†ç¦»çš„è¿ç»­ç©ºé—´

To address the above limitations, we propose PagedAttention , an attention algorithm inspired by the operating systemâ€™s (OS) solution to memory fragmentation and sharing: virtual memory with paging . Paged Attention divides the requestâ€™s KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens. In Paged Attention, the blocks for the KV cache are not necessarily stored in contiguous space. Therefore, we can manage the KV cache in a more flexible way as in OSâ€™s virtual memory: one can think of blocks as pages, tokens as bytes, and requests as processes. This design alleviates internal fragmentation by using relatively small blocks and allocating them on demand. Moreover, it eliminates external fragmentation as all blocks have the same size. Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. 
> PagedAttention å¯å‘è‡ª OS å¯¹äºå†…å­˜ç¢ç‰‡å’Œå…±äº«çš„è§£å†³æ–¹æ¡ˆï¼šåˆ†é¡µå¼è™šæ‹Ÿå†…å­˜
> PagedAttention å°†è¯·æ±‚çš„ KV cahe åˆ†å—ï¼Œæ¯ä¸ªå—åŒ…å«å›ºå®šæ•°é‡ token å¯¹åº”çš„keys å’Œ valuesï¼Œè¿™äº›å—ä¸å¿…è¦å­˜å‚¨åœ¨è¿ç»­çš„ç©ºé—´ï¼Œå› æ­¤ç”¨ç±»ä¼¼ OS ç®¡ç†è™šæ‹Ÿå†…å­˜çš„æ–¹å¼ç®¡ç† KV cacheï¼šå°† KV cache å—è§†ä½œ pageï¼Œå°† token è§†ä½œ byteï¼Œå°†è¯·æ±‚è§†ä½œ process
> é€šè¿‡ä½¿ç”¨è¾ƒå°çš„å—ï¼Œå¹¶ä¸”æŒ‰éœ€åˆ†é…å®ƒä»¬ï¼Œå°±å¯ä»¥å‡å°‘å†…éƒ¨ç¢ç‰‡ï¼Œå¹¶ä¸”å› ä¸ºæ‰€æœ‰çš„å—éƒ½æœ‰ç›¸åŒå¤§å°ï¼Œå®ƒä¹Ÿæ¶ˆé™¤äº†å¤–éƒ¨ç¢ç‰‡
> è¯¥æ–¹æ³•è¿˜ä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨å—çš„ç²’åº¦ä¸Šï¼Œåœ¨ç›¸åŒè¯·æ±‚çš„ä¸åŒåºåˆ—ä¹‹é—´ç”šè‡³ä¸åŒè¯·æ±‚çš„ä¸åŒåºåˆ—ä¹‹é—´è¿›è¡Œå†…å­˜å…±äº«

In this work, we build vLLM , a high-throughput distributed LLM serving engine on top of Paged Attention that achieves near-zero waste in KV cache memory. vLLM uses block-level memory management and preemptive request scheduling that are co-designed with Paged Attention. vLLM supports popular LLMs such as GPT [5], OPT [62], and LLaMA [52] with varying sizes, including the ones exceeding the memory capacity of a single GPU. Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by $2â€“4\times$ compared to the state-of-the-art systems [31 , 60], without affecting the model accuracy at all. The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms (Â§4.3). In summary, we make the following contributions: 
> æˆ‘ä»¬åŸºäº PagedAttention æœºåˆ¶æ„å»ºåˆ†å¸ƒå¼é«˜åå LLM æœåŠ¡å¼•æ“ vLLM
> vLLM å¯¹äº KV cache å†…å­˜å‡ ä¹é›¶æµªè´¹
> vLLM è¿˜ä½¿ç”¨äº†ä¸ PagedAttention å…±åŒè®¾è®¡çš„å—çº§å†…å­˜ç®¡ç†å’ŒæŠ¢å å¼è¯·æ±‚è°ƒåº¦
> vLLM å¯¹äºè¶…è¿‡å•ä¸ª GPU å†…å­˜å®¹é‡çš„æ¨¡å‹ä¹Ÿæä¾›æ”¯æŒ
> vLLM åœ¨å¤šä¸ªæ¨¡å‹å’Œå·¥ä½œè´Ÿè½½ä¸‹ç›¸è¾ƒäº SOTA ç³»ç»Ÿæé«˜äº† 2-4x çš„ååé‡ï¼Œä¸”ä¸å½±å“æ¨¡å‹ç²¾åº¦ï¼Œæ”¹è¿›åœ¨æ›´é•¿çš„åºåˆ—ã€æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤æ‚çš„è§£ç ç®—æ³•ä¸­æ›´ä¸ºæ˜æ˜¾
> è´¡çŒ®æ€»ç»“ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š

- We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance.
- We propose Paged Attention, an attention algorithm that operates on KV cache stored in non-contiguous paged memory, which is inspired by the virtual memory and paging in OS.
- We design and implement vLLM, a distributed LLM serving engine built on top of Paged Attention.
- We evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-theart solutions such as Faster Transformer [31] and Orca [60]. 

>- æˆ‘ä»¬è¯†åˆ«äº† LLM æœåŠ¡ä¸­çš„å†…å­˜åˆ†é…æŒ‘æˆ˜ï¼Œå¹¶é‡åŒ–äº†å®ƒä»¬å¯¹æœåŠ¡æ€§èƒ½çš„å½±å“
>- æˆ‘ä»¬æå‡ºäº† Paged Attentionï¼Œè¿™æ˜¯ä¸€ç§æ³¨æ„åŠ›ç®—æ³•ï¼Œå®ƒåœ¨å­˜å‚¨åœ¨éè¿ç»­åˆ†é¡µå†…å­˜çš„ KV cache ä¸Šè¿›è¡Œè¿ç®—ï¼Œå…¶çµæ„Ÿæ¥æºäºæ“ä½œç³»ç»Ÿä¸­çš„è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæŠ€æœ¯
>- æˆ‘ä»¬è®¾è®¡å¹¶å®ç°äº† vLLMï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Paged Attention æ„å»ºçš„åˆ†å¸ƒå¼ LLM æœåŠ¡å¼•æ“
>- æˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸­è¯„ä¼°äº† vLLMï¼Œè¯æ˜å…¶æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆï¼Œå¦‚ Faster Transformer [31] å’Œ Orca [60]

# 2 Background 
In this section, we describe the generation and serving procedures of typical LLMs and the iteration-level scheduling used in LLM serving. 

## 2.1 Transformer-Based Large Language Models 
The task of language modeling is to model the probability of a list of tokens $\left(x_{1},\ldots,x_{n}\right)$ . Since language has a natural sequential ordering, it is common to factorize the joint probability over the whole sequence as the product of conditional probabilities (a.k.a. auto regressive decomposition [3]): 
> è¯­è¨€æ˜¯è‡ªç„¶æœ‰åºçš„ï¼Œå› æ­¤å°†æ•´ä¸ªåºåˆ—ä¸Šçš„è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ¡ä»¶åˆ†å¸ƒçš„ä¹˜ç§¯æ˜¯åˆç†çš„ï¼Œå³è‡ªå›å½’åˆ†è§£ï¼š

$$
P(x)=P(x_{1})\cdot P(x_{2}\mid x_{1})\cdot\cdot\cdot P(x_{n}\mid x_{1},.\,.\,,x_{n-1}).\tag{1}
$$ 
Transformers [53] have become the de facto standard architecture for modeling the probability above at a large scale. The most important component of a Transformer-based language model is its self-attention layers. For an input hidden state sequence $\left(x_{1},\dots\right.,x_{n})\,\in\,\mathbb{R}^{n\times d}$ , a self-attention layer first applies linear transformations on each position ğ‘– to get the query, key, and value vectors: 

$$
q_{i}=W_{q}x_{i},\;k_{i}=W_{k}x_{i},\;v_{i}=W_{v}x_{i}.\tag{2}
$$ 
Then, the self-attention layer computes the attention score $a_{i j}$ by multiplying the query vector at one position with all the key vectors before it and compute the output $o_{i}$ as the weighted average over the value vectors: 

$$
a_{i j}=\frac{\exp({q_{i}^{\top}k_{j}}/{\sqrt{d}})}{\sum_{t=1}^{i}\exp({q_{i}^{\top}k_{t}}/{\sqrt{d}})},\ o_{i}=\sum_{j=1}^{i}a_{i j}v_{j}.\tag{3}
$$ 
Besides the computation in Eq. 3, all other components in the Transformer model, including the embedding layer, feed-forward layer, layer normalization [2], residual connection [22], output logit computation, and the query, key, and value transformation in Eq. 2, are all applied independently position-wise in a form of $y_{i}=f(x_{i})$ . 
> é™¤äº† eq 3 çš„ casual attention è®¡ç®—ï¼ŒTransformer ä¸­æ‰€æœ‰å…¶ä»–çš„è®¡ç®—ï¼ŒåŒ…æ‹¬ embedding layerã€FFPã€layer normalizationã€æ®‹å·®è¿æ¥ã€output logit computation éƒ½æ˜¯ position-wise ç‹¬ç«‹è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯ per token è®¡ç®—

## 2.2 LLM Service & Auto regressive Generation 
Once trained, LLMs are often deployed as a conditional generation service (e.g., completion API [34] or chatbot [19 , 35]). A request to an LLM service provides a list of input prompt tokens $\left(x_{1},\dots,x_{n}\right)$ , and the LLM service generates a list of output tokens $\left(x_{n+1},.\,.\,.\,,x_{n+T}\right)$ according to Eq. 1. We refer to the concatenation of the prompt and output lists as sequence . 
> LLM åœ¨éƒ¨ç½²åå¤„ç†çš„ä»»åŠ¡æ˜¯æ¡ä»¶ç”Ÿæˆä»»åŠ¡
> å¯¹äº LLM æœåŠ¡çš„è¯·æ±‚ä¼šæä¾›è¾“å…¥ prompt token åºåˆ— $(x_1, \dots, x_n)$ ï¼ŒLLM æ ¹æ® eq 1 ç”Ÿæˆè¾“å‡º token åºåˆ— $(x_{n+1}, \dots, x_{n+T})$

Due to the decomposition in Eq. 1, the LLM can only sample and generate new tokens one by one, and the generation process of each new token depends on all the previous tokens in that sequence, specifically their key and value vectors. In this sequential generation process, the key and value vectors of existing tokens are often cached for generating future tokens, known as KV cache . Note that the KV cache of one token depends on all its previous tokens. This means that the KV cache of the same token appearing at different positions in a sequence will be different. 
> æ ¹æ® eq 1 çš„åˆ†è§£ï¼Œå®¹æ˜“çŸ¥é“ LLM ä¸€æ¬¡ä»…èƒ½é‡‡æ ·å¹¶ä¸”ç”Ÿæˆä¸€ä¸ª tokenï¼Œæ–° token çš„ç”Ÿæˆä¾èµ–äºåºåˆ—ä¸­å‰é¢å…¨éƒ¨çš„ tokensï¼Œå…·ä½“åœ°è¯´å°±æ˜¯å®ƒä»¬çš„ keys å’Œ values
> æ•…åœ¨åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå‰é¢ tokens çš„ keys å’Œ values å¯ä»¥ç¼“å­˜ï¼Œå³ KV cache
> æ³¨æ„ä¸€ä¸ª tokens çš„ keys å’Œ values (KV cache) ä¾èµ–äºå®ƒä¹‹å‰çš„æ‰€æœ‰ tokensï¼Œå› æ­¤å‡ºç°åœ¨åŒä¸€åºåˆ—çš„ä¸åŒä½ç½®çš„ç›¸åŒ token çš„ KV cache å°†ä¸åŒ

Given a request prompt, the generation computation in the LLM service can be decomposed into two phases: 
> ç»™å®š request promptï¼ŒLLM æœåŠ¡ä¸­çš„ç”Ÿæˆå¼è®¡ç®—å¯ä»¥è¢«åˆ†è§£ä¸ºä»¥ä¸‹ä¸¤ä¸ªé˜¶æ®µï¼š

**The prompt phase** takes the whole user prompt $\left(x_{1},\ldots,x_{n}\right)$ as input and computes the probability of the first new token $P(x_{n+1}\mid x_{1},.\,.\,,x_{n})$ . During this process, also generates the key vectors $k_{1},\ldots,k_{n}$ and value vectors $v_{1},\dots,v_{n}$ . Since prompt tokens $x_{1},\ldots,x_{n}$ are all known, the computation of the prompt phase can be parallelized using matrix-matrix multiplication operations. Therefore, this phase can efficiently use the parallelism inherent in GPUs. 
> prompt é˜¶æ®µ
> å°†æ•´ä¸ªç”¨æˆ· prompt åºåˆ— $(x_1, \dots, x_n)$ ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—ç¬¬ä¸€ä¸ªæ–° token çš„æ¦‚ç‡ $P (x_{n+1}\mid x_1, \dots, x_n)$
> è¯¥è¿‡ç¨‹ä¸­ä¼šä¸º prompt tokens $(x_1, \dots, x_n)$ ç”Ÿæˆ key å‘é‡ $k_1, \dots, k_n$ å’Œ value å‘é‡ $v_1, \dots, v_n$ï¼Œå› ä¸º prompt tokens å…¨éƒ¨å·²çŸ¥ï¼Œè¯¥é˜¶æ®µçš„è®¡ç®—å¯ä»¥ä½¿ç”¨çŸ©é˜µ-çŸ©é˜µä¹˜æ³•ç®—å­å¹¶è¡Œ (æ‰€æœ‰çš„è¾“å…¥ tokens éƒ½è¦ä½œä¸º query è¢«ç¼–ç ï¼Œè¿›è¡Œ masked self attention è®¡ç®—)ï¼Œæ•…å¯ä»¥åˆ©ç”¨ GPU ä¸­å†…åœ¨çš„å¹¶è¡Œç‰¹æ€§

**The auto regressive generation phase** generates the remaining new tokens sequentially. At iteration $t$ , the model takes one token $x_{n+t}$ as input and computes the probability $P(x_{n+t+1}\mid x_{1},.\,.\,.\,,x_{n+t})$ with the key vectors $k_{1},.\,.\,.\,,k_{n+t}$  and value vectors $v_{1},.\,.\,.\,,v_{n+t}$ . Note that the key and value vectors at positions $1$ to $n+t-1$ are cached revious iterations, only the new key and value vector $k_{n+t}$ and $v_{n+t}$ are computed at this iteration. This phase completes either when the sequence reaches a maximum length (specified by users or limited by LLMs) or when an end-of-sequence $(<\!e o s\!>)$ token is emitted. The computation at different iterations cannot be parallelized due to the data dependency and often uses matrix-vector multiplication, which is less efficient. As a result, this phase severely under utilizes GPU computation and becomes memory-bound, being responsible for most portion of the latency of a single request. 
> è‡ªå›å½’ç”Ÿæˆé˜¶æ®µ
> è¯¥é˜¶æ®µé¡ºåºç”Ÿæˆæ–° tokens
> åœ¨è¿­ä»£ $t$ æ—¶ï¼Œæ¨¡å‹æ¥å—ä¸€ä¸ª token $x_{n+t}$ ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨ key å‘é‡ $k_1, \dots, k_{n+t}$ å’Œ value å‘é‡ $v_1, \dots, v_{n+t}$ è®¡ç®—æ¦‚ç‡ $P (x_{n+t+1}\mid x_1, \dots, x_{n+t})$
> æ³¨æ„ä½ç½® $1$ åˆ° $n+t-1$ çš„ keys å’Œ values åœ¨ä¹‹å‰çš„è¿­ä»£å·²ç»è¢«ç¼“å­˜ï¼Œå› æ­¤æœ¬æ¬¡è¿­ä»£ä»…éœ€è¦è®¡ç®—æ–°çš„ key å’Œ value å‘é‡ $k_{n+t}, v_{n+t}$
> è¯¥é˜¶æ®µåœ¨åºåˆ—è¾¾åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦æˆ–è€…ç”Ÿæˆäº† *\<eos\>* token åç»“æŸ
> è¯¥é˜¶æ®µä¸­ï¼Œä¸åŒè¿­ä»£ä¹‹é—´ä¸èƒ½å¹¶è¡Œï¼Œå› ä¸ºå­˜åœ¨é¡ºåºçš„æ•°æ®ä¾èµ–ï¼Œå¹¶ä¸”è¯¥é˜¶æ®µä¸€èˆ¬ä½¿ç”¨ç›¸å¯¹ä½æ•ˆçš„çŸ©é˜µ-å‘é‡ä¹˜æ³• (ä»…æœ‰æ–° token ä½œä¸º query éœ€è¦è¢«ç¼–ç )ï¼Œå› æ­¤è¯¥é˜¶æ®µæ²¡æœ‰å……åˆ†ä½¿ç”¨ GPU çš„è®¡ç®—èµ„æºï¼Œä¸º memory-boundï¼Œæ•…è¯¥é˜¶æ®µçš„è®¡ç®—ä¼šå æ®å•ä¸ª request çš„å¤§å¤šæ•°å»¶è¿Ÿæ—¶é—´

## 2.3 Batching Techniques for LLMs 
The compute utilization in serving LLMs can be improved by batching multiple requests. Because the requests share the same model weights, the overhead of moving weights is amortized across the requests in a batch, and can be overwhelmed by the computational overhead when the batch size is sufficiently large. However, batching the requests to an LLM service is non-trivial for two reasons. First, the requests may arrive at different times. A naive batching strategy would either make earlier requests wait for later ones or delay the incoming requests until earlier ones finish, leading to significant queueing delays. Second, the requests may have vastly different input and output lengths (Fig. 11). A straightforward batching technique would pad the inputs and outputs of the requests to equalize their lengths, wasting GPU computation and memory. 
> å¯ä»¥é€šè¿‡æ‰¹é‡å¤„ç†å¤šä¸ª request æé«˜ LLM æœåŠ¡ä¸­çš„è®¡ç®—èµ„æºåˆ©ç”¨ï¼Œå› ä¸ºå¤šä¸ª request å…±äº«æ¨¡å‹æƒé‡ï¼Œæ•…ç§»åŠ¨æƒé‡çš„å¼€é”€ä¼šåœ¨ batch ä¸­çš„ requests ä¹‹é—´æ‘Šé”€ï¼Œå¹¶ä¸”å¦‚æœ batch size è¶³å¤Ÿå¤§ï¼Œç§»åŠ¨æƒé‡çš„å¼€é”€åœ¨è¶³å¤Ÿçš„çš„è®¡ç®—å¼€é”€ä¸‹å°±ä¸æ˜¾å¾—é‡è¦
> åœ¨ LLM æœåŠ¡ä¸­æ‰¹å¤„ç†å¤šä¸ª request å­˜åœ¨ä¸¤ç‚¹å›°éš¾ï¼š
>  1. requests å¯èƒ½åœ¨ä¸åŒæ—¶åˆ»åˆ°è¾¾ï¼Œæœ´ç´ çš„æ‰¹å¤„ç†ç­–ç•¥è¦ä¹ˆè®©è¾ƒæ—©çš„ requests ç­‰å¾…è¾ƒæ™šçš„ requestsï¼Œè¦ä¹ˆå»¶è¿Ÿæ­£åœ¨ä¼ å…¥çš„ requests ç›´åˆ°è¾ƒæ—©çš„ requests å¤„ç†å®Œæˆï¼Œå› æ­¤å­˜åœ¨æ˜¾è‘—çš„æ’é˜Ÿå»¶è¿Ÿ
>  2. requests çš„è¾“å…¥å’Œè¾“å‡ºé•¿åº¦å¯èƒ½æ˜¾è‘—ä¸åŒï¼Œç›´æ¥çš„æ‰¹å¤„ç†ç­–ç•¥å°†å¡«å…… requests çš„è¾“å…¥å’Œè¾“å‡ºä½¿å…¶å…·æœ‰ç›¸åŒé•¿åº¦ï¼Œå¯¼è‡´æµªè´¹ GPU è®¡ç®—å’Œå†…å­˜

To address this problem, fine-grained batching mechanisms, such as cellular batching [16] and iteration-level scheduling [60], have been proposed. Unlike traditional methods that work at the request level, these techniques operate at the iteration level. After each iteration, completed requests are removed from the batch, and new ones are added. Therefore, a new request can be processed after waiting for a single iteration, not waiting for the entire batch to complete. Moreover, with special GPU kernels, these techniques eliminate the need to pad the inputs and outputs. By reducing the queueing delay and the inefficiencies from padding, the fine-grained batching mechanisms significantly increase the throughput of LLM serving. 
>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç»†ç²’åº¦çš„æ‰¹å¤„ç†æœºåˆ¶ï¼Œä¾‹å¦‚ç»†èƒæ‰¹å¤„ç†å’Œè¿­ä»£çº§è°ƒåº¦è¢«æå‡º
>ä¼ ç»Ÿæ–¹æ³•å·¥ä½œåœ¨ request çº§åˆ«ï¼Œè€Œè¿™ç±»æ–¹æ³•å·¥ä½œåœ¨ iteration çº§åˆ«ï¼Œåœ¨æ¯ä¸ª iterationï¼Œå®Œæˆçš„ request å°†ä»æ‰¹é‡ä¸­è¢«ç§»é™¤ï¼Œæ–°çš„ request ä¼šè¢«åŠ å…¥ï¼Œå› æ­¤æ–°çš„ request å¯ä»¥åœ¨ç­‰å¾…å•ä¸ª iteration ä¹‹åå°±è¢«å¤„ç†ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ•´ä¸ªæ‰¹é‡å®Œæˆ
>æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹æ®Šçš„GPU kernelï¼Œè¿™äº›æŠ€æœ¯æ¶ˆé™¤äº†å¡«å……è¾“å…¥å’Œè¾“å‡ºçš„éœ€æ±‚
>ç»†ç²’åº¦çš„æ‰¹å¤„ç†æœºåˆ¶é€šè¿‡å‡å°‘æ’é˜Ÿå»¶è¿Ÿå’Œå¡«å……æ‰€å¸¦æ¥çš„ä½æ•ˆç‡ï¼Œæ˜¾è‘—æé«˜äº†LLMæœåŠ¡çš„ååé‡

# 3 Memory Challenges in LLM Serving 
Although fine-grained batching reduces the waste of computing and enables requests to be batched in a more flexible way, the number of requests that can be batched together is still constrained by GPU memory capacity, particularly the space allocated to store the KV cache. In other words, the serving systemâ€™s throughput is memory-bound . Overcoming this memory-bound requires addressing the following challenges in the memory management: 
> ç»†ç²’åº¦çš„æ‰¹å¤„ç†å‡å°‘äº†è®¡ç®—æµªè´¹ (æ¶ˆé™¤äº†å¡«å……)ï¼Œå¹¶ä¸”ä½¿å¾— requests å¯ä»¥æ›´çµæ´»åœ°æ‰¹å¤„ç†ï¼Œä½†å¯ä»¥æ‰¹å¤„ç†çš„ requests æ•°é‡ä»ç„¶å—é™äº GPU æ˜¾å­˜å®¹é‡ï¼Œå°¤å…¶æ˜¯åˆ†é…äºå­˜å‚¨ KV cache çš„é‚£éƒ¨åˆ†æ˜¾å­˜ç©ºé—´å¤§å°
> æ¢å¥è¯è¯´ï¼ŒLLM æœåŠ¡ç³»ç»Ÿçš„ååé‡æ˜¯ memory-boundï¼Œè¦å…‹æœå®ƒï¼Œéœ€è¦æˆ‘ä»¬è§£å†³ä»¥ä¸‹æ˜¾å­˜ç®¡ç†çš„æŒ‘æˆ˜ï¼š

**Large KV cache.** The KV Cache size grows quickly with the number of requests. As an example, for the 13B parameter OPT model [62], the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) $\times5120$ (hidden state size) $\times\ 40$ (number of layers) $\times\;2$ (bytes per FP16). Since OPT can generate sequences up to 2048 tokens, the memory required to store the KV cache of one request can be as much as 1.6 GB. Concurrent GPUs have memory capacities in the tens of GBs. Even if all available memory was allocated to KV cache, only a few tens of requests could be accommodated. Moreover, inefficient memory management can further decrease the batch size, as shown in Fig. 2. Additionally, given the current trends, the GPUâ€™s computation speed grows faster than the memory capacity [17]. For example, from NVIDIA A100 to H100, The FLOPS increases by more than $2\mathrm{x}$ , but the GPU memory stays at 80GB maximum. Therefore, we believe the memory will become an increasingly significant bottleneck. 
> Large KV cache
> KV cache çš„å¤§å°ä¼šéšç€ requests æ•°é‡å¿«é€Ÿå¢é•¿
> ä¾‹å¦‚ï¼Œ13B OPT æ¨¡å‹ä¸­å•ä¸ª token çš„ FP16 KV cache éœ€è¦ 800KB çš„ç©ºé—´ 
> ($2\times 5120\times 40 \times 2\  \text{bytes} = 800\ \text{KB}$)ï¼ŒOPT ç”Ÿæˆåºåˆ—é•¿åº¦ä¸Šé™æ˜¯ 2048 tokensï¼Œæ•…å•ä¸ª request æ‰€éœ€çš„ KV cache ç©ºé—´å¯ä»¥è¾¾åˆ° 1.6 GB
> å½“å‰çš„ GPU è®¾å¤‡æ˜¾å­˜å®¹é‡åœ¨å‡ å GB çš„è§„æ¨¡ï¼Œå› æ­¤å³ä¾¿å…¨éƒ¨æ˜¾å­˜åˆ†é…ç»™ KV cacheï¼Œä¹Ÿä»…èƒ½å­˜ä¸‹åå‡ ä¸ª requests çš„ KV cache
> å¹¶ä¸”ï¼Œä½æ•ˆçš„å†…å­˜ç®¡ç†ä¼šè¿›ä¸€æ­¥å‡å°‘ batch size
> å½“å‰çš„å‘å±•è¶‹åŠ¿æ˜¯ GPU çš„è®¡ç®—é€Ÿåº¦å¢é•¿å¿«äºæ˜¾å­˜å®¹é‡ï¼Œä¾‹å¦‚ A100 åˆ° H100 FLOPs å¢é•¿1å€ï¼Œè€Œæ˜¾å­˜ä¿æŒ 80GB æœ€å¤§ä¸å˜
> å› æ­¤ï¼Œæ˜¾å­˜å°†é€æ¸æˆä¸ºè¶ŠåŠ æ˜¾è‘—çš„ç“¶é¢ˆ

**Complex decoding algorithms.** LLM services offer a range of decoding algorithms for users to select from, each with varying implications for memory management complexity. For example, when users request multiple random samples from a single input prompt, a typical use case in program suggestion [18], the KV cache of the prompt part, which accounts for $12\%$ of the total KV cache memory in our experiment (Â§6.3), can be shared to minimize memory usage. On the other hand, the KV cache during the auto regressive generation phase should remain unshared due to the different sample results and their dependence on context and position. The extent of KV cache sharing depends on the specific decoding algorithm employed. In more sophisticated algorithms like beam search [49], different request beams can share larger portions (up to $55\%$ memory saving, see $\S6.3)$ of their KV cache, and the sharing pattern evolves as the decoding process advances. 
> å¤æ‚è§£ç ç®—æ³•
> LLM æœåŠ¡æä¾›äº†ä¸€ç³»åˆ—è§£ç ç®—æ³•ä¾›ç”¨æˆ·é€‰æ‹©ï¼Œè¿™äº›ç®—æ³•å„è‡ªéƒ½å¯¹å†…å­˜ç®¡ç†çš„å¤æ‚æœ‰ä¸åŒçš„å½±å“
> ä¾‹å¦‚ï¼Œå½“ç”¨æˆ·ä»å•ä¸ªè¾“å…¥ prompt è¯·æ±‚å¤šä¸ªéšæœºæ ·æœ¬æ—¶ï¼Œprompt éƒ¨åˆ†çš„ KV cache (åœ¨æˆ‘ä»¬çš„å®éªŒä¸­å æ€» KV cache å†…å­˜çš„ $12\%$ )ï¼Œå¯ä»¥è¢«å…±äº«ä»¥æœ€å°åŒ–å†…å­˜ä½¿ç”¨ï¼Œå½“ç„¶è‡ªå›å½’ç”Ÿæˆé˜¶æ®µçš„ KV cache ä»ç„¶æ˜¯ä¸å…±äº«çš„ï¼Œå› ä¸ºæ¯ä¸ªé‡‡æ ·çš„ç”Ÿæˆç»“æœä¸åŒ
> KV cache çš„å…±äº«ç¨‹åº¦å–å†³äºé‡‡ç”¨çš„ç‰¹å®šè§£ç ç®—æ³•ï¼Œåœ¨æ›´ä¸ºå¤æ‚çš„ç®—æ³•ä¾‹å¦‚ beam search ä¸­ï¼Œä¸åŒçš„ request beam å¯ä»¥å…±äº«æ›´å¤§éƒ¨åˆ†çš„ KV cache (å› æ­¤èŠ‚çº¦æœ€å¤š 55% çš„å†…å­˜)ï¼Œå¹¶ä¸”éšç€è§£ç è¿‡ç¨‹çš„æ¨è¿›ï¼Œå®ƒä»¬çš„å…±äº«æ¨¡å¼ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–

**Scheduling for unknown input & output lengths.** The requests to an LLM service exhibit variability in their input and output lengths. This requires the memory management system to accommodate a wide range of prompt lengths. In addition, as the output length of a request grows at decoding, the memory required for its KV cache also expands and may exhaust available memory for incoming requests or ongoing generation for existing prompts. The system needs to make scheduling decisions, such as deleting or swapping out the KV cache of some requests from GPU memory. 
> å¯¹æœªçŸ¥è¾“å…¥ã€è¾“å‡ºé•¿åº¦çš„è°ƒåº¦
> å¯¹ LLM æœåŠ¡çš„ requests çš„è¾“å…¥å’Œè¾“å‡ºé•¿åº¦ä¸€èˆ¬éƒ½æ˜¯ä¸åŒçš„ï¼Œè¿™è¦æ±‚å†…å­˜ç®¡ç†ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”å„ç§é•¿åº¦çš„ prompt
> å¦å¤–ï¼Œéšç€ request çš„è¾“å‡ºé•¿åº¦åœ¨è§£ç ä¸­å¢é•¿ï¼Œå…¶ KV cache æ‰€éœ€çš„å†…å­˜ä¹Ÿå°†å¢é•¿ï¼Œè¿›è€Œæ¶ˆè€—æ‰ä¸ºæ–°çš„ request æˆ–è€…ä¸ºç°å­˜ prompt çš„ç”Ÿæˆè¿‡ç¨‹æ‰€å‡†å¤‡çš„å†…å­˜
> å› æ­¤ï¼Œç³»ç»Ÿéœ€è¦è¿›è¡Œè°ƒåº¦å†³ç­–ï¼Œä¾‹å¦‚ä» GPU æ˜¾å­˜ä¸­åˆ å»æˆ–è€…æ¢å‡ºä¸€äº› requests çš„ KV cache

## 3.1 Memory Management in Existing Systems
Since most operators in current deep learning frameworks [33 , 39] require tensors to be stored in contiguous memory, previous LLM serving systems [31 , 60] also store the KV cache of one request as a contiguous tensor across the different positions. Due to the unpredictable output lengths from the LLM, they statically allocate a chunk of memory for a request based on the requestâ€™s maximum possible sequence length, irrespective of the actual input or eventual output length of the request. 
> å½“å‰çš„ DL æ¡†æ¶çš„å¤§å¤šæ•°ç®—å­è¦æ±‚ tensor å­˜å‚¨åœ¨è¿ç»­å†…å­˜ä¸­ï¼Œæ•…ä¹‹å‰çš„ LLM æœåŠ¡ç³»ç»Ÿå°†ä¸€ä¸ª request çš„ KV cache ä¹Ÿä½œä¸ºè¿ç»­çš„ tensor å­˜å‚¨
> å› ä¸º request çš„è¾“å‡ºé•¿åº¦ä¸åŒï¼Œæ•…è¿™äº›ç³»ç»ŸåŸºäº request çš„æœ€å¤§å¯èƒ½åºåˆ—é•¿åº¦ä¸º request é™æ€åœ°åˆ†é…ä¸€ä¸ªå†…å­˜å—ï¼Œä¸å…³å¿ƒ request çš„å®é™…è¾“å…¥å’Œæœ€ç»ˆè¾“å‡ºé•¿åº¦

Fig. 3 illustrates two requests: request A with 2048 maximum possible sequence length and request B with a maximum of 512. The chunk pre-allocation scheme in existing systems has three primary sources of memory wastes: reserved slots for future tokens, internal fragmentation due to over-provisioning for potential maximum sequence lengths, and external fragmentation from the memory allocator like the buddy allocator. The external fragmentation will never be used for generated tokens, which is known before serving a request. Internal fragmentation also remains unused, but this is only realized after a request has finished sampling. They are both pure memory waste. Although the reserved memory is eventually used, reserving this space for the entire requestâ€™s duration, especially when the reserved space is large, occupies the space that could otherwise be used to process other requests. We visualize the average percentage of memory wastes in our experiments in Fig. 2, revealing that the actual effective memory in previous systems can be as low as $20.4\%$ . 
> å¦‚ Figure 3 æ‰€ç¤ºï¼Œrequest A çš„æœ€å¤§å¯èƒ½åºåˆ—é•¿åº¦ä¸º 2048ï¼Œrequest B çš„æœ€å¤§å¯èƒ½åºåˆ—é•¿åº¦æ˜¯ 512ï¼Œç°å­˜ç³»ç»Ÿçš„å†…å­˜å—é¢„åˆ†é…ç­–ç•¥å­˜åœ¨ä¸‰ç§ä¸»è¦çš„å†…å­˜æµªè´¹ï¼šä¸ºæœªæ¥çš„ token é¢„ç•™çš„ slotã€ä¸ºæœ€å¤§åºåˆ—é•¿åº¦è¿‡åº¦åˆ†é…çš„å†…å­˜å¯¼è‡´çš„å†…éƒ¨ç¢ç‰‡ã€æ¥è‡ªå†…å­˜åˆ†é…å™¨ (ä¾‹å¦‚ buddy åˆ†é…å™¨) çš„å¤–éƒ¨ç¢ç‰‡
> å…¶ä¸­ï¼Œå¤–éƒ¨ç¢ç‰‡åœ¨æœåŠ¡ request ä¹‹å‰å°±å·²çŸ¥ä¸ä¼šè¢«ä½¿ç”¨ï¼Œå†…éƒ¨ç¢ç‰‡ä»…åœ¨ request å®Œæˆé‡‡æ ·ä¹‹åæ‰èƒ½ç¡®å®šä¸ä¼šè¢«ä½¿ç”¨ï¼ŒäºŒè€…éƒ½æ˜¯å®Œå…¨çš„å†…å­˜æµªè´¹
> è€Œä¸ºæœªæ¥ token é¢„ç•™çš„å†…å­˜è™½ç„¶æœ€ç»ˆä¼šè¢«ä½¿ç”¨ï¼Œä½†è¯¥ç©ºé—´ä¹Ÿä¼šåœ¨ request çš„æ•´ä¸ªæŒç»­å‘¨æœŸè¢«é¢„ç•™ï¼Œå½“é¢„ç•™çš„ç©ºé—´è¾ƒå¤§æ—¶ï¼Œè¿™ä¹Ÿä¼šå ç”¨æœ¬å¯ä»¥ç”¨äºå¤„ç†å…¶ä»– request çš„ç©ºé—´
> Figure 2 å±•ç¤ºäº†ä¹‹å‰ç³»ç»Ÿçš„å®é™…æœ‰æ•ˆå†…å­˜ä½¿ç”¨ç‡å¯èƒ½ä½è‡³ 20.4%

![[vLLM-Fig3.png]]

Although compaction [54] has been proposed as a potential solution to fragmentation, performing compaction in a performance-sensitive LLM serving system is impractical due to the massive KV cache. Even with compaction, the pre-allocated chunk space for each request prevents memory sharing specific to decoding algorithms in existing memory management systems. 
> ä¸€ä¸ªè§£å†³ç¢ç‰‡çš„æ–¹æ³•æ˜¯ compactionï¼Œä½†åœ¨æ€§èƒ½æ•æ„Ÿçš„ LLM æœåŠ¡ç³»ç»Ÿä¸­æ‰§è¡Œ compaction æ˜¯ä¸ç°å®çš„ï¼Œå› ä¸ºå…¶ KV cache ååˆ†åºå¤§
> ä¸”å³ä¾¿ä½¿ç”¨ compactionï¼Œä¸ºæ¯ä¸ª request é¢„ç•™å—ç©ºé—´çš„æ–¹æ³•ä¹Ÿä¸èƒ½å®ç° request åœ¨ç‰¹å®šçš„è§£ç ç®—æ³•ä¸‹å…±äº«å†…å­˜

# 4 Method 
In this work, we develop a new attention algorithm, PagedAttention , and build an LLM serving engine, vLLM , to tackle the challenges outlined in $\S3$ . The architecture of vLLM is shown in Fig. 4. vLLM adopts a centralized scheduler to coordinate the execution of distributed GPU workers. The KV cache manager effectively manages the KV cache in a paged fashion, enabled by Paged Attention. Specifically, the KV cache manager manages the physical KV cache memory on the GPU workers through the instructions sent by the centralized scheduler. 
> æˆ‘ä»¬æå‡ºæ–°çš„ attention ç®—æ³• PagedAttentionï¼Œå¹¶åŸºäºæ­¤æ„å»º LLM æœåŠ¡å¼•æ“ vLLMï¼ŒvLLM æ¡†æ¶å¦‚ Figure 4 æ‰€ç¤º
> Figure 4 ä¸­ï¼Œä¸­å¿ƒåŒ–çš„è°ƒåº¦å™¨æ¥åè°ƒåˆ†å¸ƒå¼ GPU workers çš„æ‰§è¡Œï¼ŒKV cache ç®¡ç†å™¨é€šè¿‡ä¸­å¿ƒåŒ–çš„è°ƒåº¦å™¨å‘é€æŒ‡ä»¤æ¥ç®¡ç† GPU workers ä¸­çš„ç‰©ç† KV cache å†…å­˜

![[vLLM-Fig4.png]]

Next, We describe the Paged Attention algorithm in $\S4.1$ . With that, we show the design of the KV cache manager in $\S4.2$ and how it facilitates Paged Attention in $\S4.3$ , respectively. Then, we show how this design facilitates effective memory management for various decoding methods (Â§4.4) and handles the variable length input and output sequences (Â§4.5). Finally, we show how the system design of vLLM works in a distributed setting (Â§4.6). 

## 4.1 Paged Attention 
To address the memory challenges in $\S3$ , we introduce PagedAttention , an attention algorithm inspired by the classic idea of paging [25] in operating systems. Unlike the traditional attention algorithms, Paged Attention allows storing continuous keys and values in non-contiguous memory space. Specifically, Paged Attention partitions the KV cache of each sequence into KV blocks . Each block contains the key and value vectors for a fixed number of tokens, which we denote as $K V$ block size ( $B$ ). Denote the key block $K_{j}=(k_{(j-1)B+1},\dots,k_{j B})$ and value block $V_{j}=(v_{(j-1)B+1},\dots,v_{j B})$ . The attention computation in Eq. 4 can be transformed into the following blockwise computation: 

$$
A_{i j}=\frac{\exp(q_{i}^{\top}K_{j}/\sqrt{d})}{\sum_{t=1}^{\lceil i/B\rceil}\exp(q_{i}^{\top}K_{t}\mathbf 1/\sqrt{d})},\;o_{i}=\sum_{j=1}^{\lceil i/B\rceil}V_{j}A_{i j}^{\top},\tag{4}
$$

where $A_{i j}=\left(a_{i,(j-1)B+1},\dots,a_{i,j B}\right)$ is the row vector of attention score on $j$ -th KV block. 

> PagedAttention å…è®¸å°†è¿ç»­çš„ keys å’Œ values å­˜å‚¨åœ¨éè¿ç»­çš„å†…å­˜ç©ºé—´
> å…·ä½“åœ°è¯´ï¼ŒPagedAttention å°†æ¯ä¸ªåºåˆ—çš„ KV cache åˆ’åˆ†ä¸º KV  blocksï¼ŒåŒ…æ‹¬ key blocks å’Œ value blocksï¼Œæ¯ä¸ª key/value block åŒ…å«åºåˆ—ä¸­ block size ( $B$ ) ä¸ª tokens å¯¹åº”çš„ keys å’Œ valuesï¼Œåˆ†åˆ«è®°ä½œ $K_j = (k_{(j-1) B + 1}, \dots, k_{jB})$ å’Œ $V_j = (v_{(j-1)B + 1}, \dots, v_{jB})$
> PagedAttention è¿›è€Œå°† eq 3 çš„ attention è®¡ç®—è½¬åŒ–ä¸ºå¦‚ä¸Šçš„é€å—çš„è¿ç®—ï¼Œå…¶ä¸­ $A_{ij} = (a_{i, (j-1) B+1}, \dots, a_{i, jB})$ ä¸º $q_i$ ç›¸å¯¹äºç¬¬ $j$ ä¸ª K block çš„ attention score å‘é‡
> (æ³¨ï¼šå…¬å¼ (4) æ˜¾ç„¶å­˜åœ¨é”™è¯¯ï¼Œæ­£ç¡®çš„å…¬å¼åº”è¯¥å°† $\mathbf 1$ æ”¾åœ¨ $\exp$ å¤–ï¼Œå¹¶ä¸” $\mathbf 1$ åº”è¯¥åŒæ—¶ä½œä¸º indicator functionï¼Œæ»¡è¶³ç¬¬ $\lceil i / B \rceil$ çš„å—ä¸­ $j > i$ çš„ $k_j$ å¯¹åº”çš„ $\exp (q_i^\top k_j/\sqrt d)$ ä¹˜ä¸Šé›¶) 

During the attention computation, the Paged Attention kernel identifies and fetches different KV blocks separately. We show an example of Paged Attention in Fig. 5: The key and value vectors are spread across three blocks, and the three blocks are not contiguous on the physical memory. At each time, the kernel multiplies the query vector $q_{i}$ of the query token (*"forth"*) and the key vectors $K_{j}$ in a block (e.g., key vectors of *â€œFour score and sevenâ€* for block 0) to compute the attention score $A_{i j}$ , and later multiplies $A_{i j}$ with the value vectors $V_{j}$ in a block to derive the final attention output $o_{i}$ . 
> åœ¨ attention è®¡ç®—è¿‡ç¨‹ä¸­ï¼ŒPagedAttention kernel ä¼šåˆ†åˆ«è¯†åˆ«å¹¶è·å–ä¸åŒçš„ KV blocks
> PagenAttention çš„ç¤ºä¾‹è§ Fig5ï¼Œå¯ä»¥çœ‹åˆ°åºåˆ— "Four score and seven years ago our fathers brought forth" çš„å…¨éƒ¨ keys å’Œ values å‘é‡åˆ†ä¸ºä¸‰ä¸ªå—å­˜å‚¨ï¼Œå—ä¹‹é—´åœ¨ç‰©ç†å†…å­˜ä¸­æ˜¯ä¸å¿…è¿ç»­çš„
> åœ¨æ¯ä¸€æ¬¡è®¡ç®—ä¸­ï¼Œquery token ("forth") çš„æŸ¥è¯¢å‘é‡ $q_i$ ä»…å’Œä¸€ä¸ªå—çš„ key vectors $K_j$ ç›¸ä¹˜ (ä¾‹å¦‚ block 0 ä¸­ "Four score and seven" çš„ key vectors)ï¼Œè®¡ç®—å‡ºå¯¹åº”çš„ attention score $A_{ij}$ï¼Œä¹‹ååœ¨è®¡ç®—æœ€ç»ˆ attention è¾“å‡º $o_i$ æ—¶ï¼Œè¿˜ä¼šå°† $A_{ij}$ å’Œå—ä¸­çš„ value vectors $V_j$ ç›¸ä¹˜

![[vLLM-Fig5.png]]

In summary, the Paged Attention algorithm allows the KV blocks to be stored in non-contiguous physical memory, which enables more flexible paged memory management in vLLM. 
> æ€»ä¹‹ï¼ŒPagedAttention ç®—æ³•å…è®¸ KV blocks å­˜å‚¨åœ¨éè¿ç»­çš„ç‰©ç†å†…å­˜ä¸­ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨ vLLM å¯¹å†…å­˜è¿›è¡Œçµæ´»çš„åˆ†é¡µç®¡ç†

## 4.2 KV Cache Manager 
The key idea behind vLLMâ€™s memory manager is analogous to the virtual memory [25] in operating systems. OS partitions memory into fixed-sized pages and maps user programsâ€™ logical pages to physical pages. Contiguous logical pages can correspond to non-contiguous physical memory pages, allowing user programs to access memory as though it were contiguous. Moreover, physical memory space needs not to be fully reserved in advance, enabling the OS to dynamically allocate physical pages as needed. vLLM uses the ideas behind virtual memory to manage the KV cache in an LLM service. Enabled by Paged Attention, we organize the KV cache as fixed-size KV blocks, like pages in virtual memory. 
> vLLM è¿›è¡Œå†…å­˜ç®¡ç†çš„æ ¸å¿ƒæ€æƒ³ç±»ä¼¼äº OS çš„è™šæ‹Ÿå†…å­˜
> OS å°†å†…å­˜åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„é¡µï¼Œç„¶åå°†ç”¨æˆ·ç¨‹åºçš„é€»è¾‘é¡µæ˜ å°„åˆ°ç‰©ç†é¡µï¼Œè¿ç»­çš„é€»è¾‘é¡µå¯ä»¥å¯¹åº”äºä¸è¿ç»­çš„ç‰©ç†é¡µï¼Œè€Œç”¨æˆ·ç¨‹åºå¯ä»¥å°†å†…å­˜å½“ä½œè¿ç»­çš„æ¥è®¿é—®
> å¦å¤–ï¼Œç‰©ç†å†…å­˜ç©ºé—´å¹¶ä¸éœ€è¦å®Œå…¨é¢„å…ˆé¢„ç•™ï¼Œæ•… SO å¯ä»¥æŒ‰ç…§éœ€è¦åŠ¨æ€åœ°åˆ†é…ç‰©ç†é¡µ
> vLLM åˆ©ç”¨äº†è™šæ‹Ÿå†…å­˜çš„è¿™ç§æ€æƒ³æ¥ç®¡ç† LLM æœåŠ¡ä¸­çš„ KV cacheï¼Œé€šè¿‡ PagedAttention ç®—æ³•ï¼Œæˆ‘ä»¬å°† KV cache åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ KV blocks æ¥ç®¡ç†ï¼Œç±»ä¼¼äºè™šæ‹Ÿå†…å­˜ä¸­çš„é¡µ

A requestâ€™s KV cache is represented as a series of logical KV blocks , filled from left to right as new tokens and their KV cache are generated. The last KV blockâ€™s unfilled positions are reserved for future generations. On GPU workers, a block engine allocates a contiguous chunk of GPU DRAM and divides it into physical KV blocks (this is also done on CPU RAM for swapping; see $\S4.5)$ . The KV block manager also maintains block tables â€”the mapping between logical and physical KV blocks of each request. Each block table entry records the corresponding physical blocks of a logical block and the number of filled positions. Separating logical and physical KV blocks allows vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance, which eliminates most memory waste in existing systems, as in Fig. 2. 
> ä¸€ä¸ªè¯·æ±‚çš„ KV cache è¢«è¡¨ç¤ºä¸ºä¸€ç³»åˆ—é€»è¾‘ KV blocksï¼Œéšç€æ–°çš„ tokens å’Œå®ƒä»¬çš„ KV cache è¢«ç”Ÿæˆï¼ŒKV blocks ä¹Ÿä¼šä»å·¦åˆ°å³è¢«å¡«å……ï¼Œæœ€åä¸€ä¸ª KV block ä¸­æœªå¡«å……çš„ä½ç½®ä¸ºæœªæ¥çš„ç”Ÿæˆé¢„ç•™
> åœ¨ GPU worker ä¸Šï¼Œblock engine è´Ÿè´£åˆ†é…è¿ç»­çš„ GPU DRAM å—ï¼Œç„¶åå°†è¯¥ DRAM å—åˆ’åˆ†ä¸ºå¤šä¸ªç‰©ç† KV å— (åœ¨ swapping æ—¶ï¼ŒCPU RAM ä¹Ÿä¼šè¿›è¡Œè¿™æ ·çš„åˆ’åˆ†)
> KV block manager åŒæ—¶ç»´æŠ¤ block è¡¨ï¼Œblock è¡¨çš„æ¯ä¸ªè¡¨é¡¹è®°å½•äº†æ¯ä¸ª request çš„é€»è¾‘ KV å—åˆ°ç‰©ç† KV å—ä¹‹é—´çš„æ˜ å°„ï¼Œä»¥åŠå—ä¸­å·²ç»è¢«å¡«å……çš„ä½ç½®çš„æ•°é‡
> é€šè¿‡åˆ†ç¦»é€»è¾‘ KV å—å’Œç‰©ç† KV å—ï¼ŒvLLM å¾—ä»¥åœ¨ä¸æå‰ä¸ºæ‰€æœ‰çš„ä½ç½®é¢„ç•™å†…å­˜çš„æƒ…å†µä¸‹åŠ¨æ€å¢é•¿ KV cache å ç”¨çš„å†…å­˜ï¼Œè¿™æ¶ˆé™¤äº†ç°å­˜ç³»ç»Ÿä¸­å¤§å¤šæ•°çš„å†…å­˜æµªè´¹

## 4.3 Decoding with Paged Attention and vLLM 
Next, we walk through an example, as in Fig. 6, to demonstrate how vLLM executes Paged Attention and manages the memory during the decoding process of a single input sequence: 
1. As in OSâ€™s virtual memory, vLLM does not require reserving the memory for the maximum possible generated sequence length initially. Instead, it reserves only the necessary KV blocks to accommodate the KV cache generated during prompt computation. In this case, The prompt has 7 tokens, so vLLM maps the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively). In the prefill step, vLLM generates the KV cache of the prompts and the first output token with a conventional self-attention algorithm (e.g., [13]). vLLM then stores the KV cache of the first 4 tokens in logical block 0 and the following 3 tokens in logical block 1. The remaining slot is reserved for the subsequent auto regressive generation phase. 
2. In the first auto regressive decoding step, vLLM generates the new token with the Paged Attention algorithm on physical blocks 7 and 1. Since one slot remains available in the last logical block, the newly generated KV cache is stored there, and the block tableâ€™s \#filled record is updated. 
3. At the second decoding step, as the last logical block is full, vLLM stores the newly generated KV cache in a new logical block; vLLM allocates a new physical block (physical block 3) for it and stores this mapping in the block table.

> æœ¬èŠ‚å±•ç¤ºä¸€ä¸ªä¾‹å­ï¼Œæè¿° vLLM å¦‚ä½•æ‰§è¡Œ PagedAttention å¹¶åœ¨ä¸ºå•ä¸ªè¾“å…¥åºåˆ—è§£ç æ—¶ç®¡ç†å†…å­˜ï¼Œå›¾è§ Fig6
> 1. vLLM å¹¶ä¸ä¼šåœ¨æœ€åˆè¯·æ±‚é¢„ç•™å‡ºå®¹çº³æœ€å¤§å¯èƒ½ç”Ÿæˆçš„åºåˆ—é•¿åº¦çš„ KV cache å¯¹åº”çš„å†…å­˜ç©ºé—´ï¼Œè€Œæ˜¯ä»…é¢„ç•™å¿…è¦çš„ KV blocks ä»¥å®¹çº³ prompt è®¡ç®—æ—¶ç”Ÿæˆçš„ KV cacheï¼Œä¾‹å¦‚åœ¨æœ¬ä¾‹ä¸­ï¼Œprompt æœ‰ 7 ä¸ª tokensï¼ŒvLLM æ•…ä»…å°†å‰ä¸¤ä¸ªé€»è¾‘ KV å— (block 0, 1) æ˜ å°„åˆ°ç‰©ç† KV å— (block 7, 1)
>    åœ¨ prefill æ­¥éª¤ä¸­ï¼ŒvLLM ä½¿ç”¨å¸¸è§„çš„ self-attention ç®—æ³•ä¸º prompt å’Œç¬¬ä¸€ä¸ª output token ç”Ÿæˆ KV cacheï¼Œç„¶å vLLM å°†å‰å››ä¸ª tokens çš„ keys å’Œ values å­˜å‚¨åœ¨é€»è¾‘å—0ï¼Œå°†ä¹‹åä¸‰ä¸ª tokens çš„ keys å’Œ values å­˜å‚¨åœ¨é€»è¾‘å—1ï¼Œå‰©ä¸‹çš„ slot ä¸ºåé¢è‡ªåŠ¨å›å½’ç”Ÿæˆé˜¶æ®µçš„ token çš„ keys å’Œ values é¢„ç•™
> 2. åœ¨ç¬¬ä¸€ä¸ªè‡ªå›å½’è§£ç æ­¥éª¤ä¸­ï¼ŒvLLM ä½¿ç”¨ PagedAttention ç®—æ³•æ ¹æ®ç‰©ç†å— 7 å’Œ 1 ä¸­çš„ KV cache ç”Ÿæˆæ–° tokenï¼Œç”Ÿæˆæ–° token æ—¶ï¼Œå› ä¸ºä¸Šä¸€ä¸ªé€»è¾‘å—ä¸­è¿˜æœ‰ç©ºçš„ slotï¼Œæ•…æ–°ç”Ÿæˆçš„ KV cache ä¼šå…ˆå¡«å……åˆ°è¯¥ slot ä¸­ï¼Œå¹¶ä¸”æ›´æ–° block table ä¸­è¯¥é€»è¾‘å—çš„ `#filled` æ•°é‡æ¡ç›®
> 3. åœ¨ç¬¬äºŒä¸ªè‡ªå›å½’è§£ç æ­¥éª¤ä¸­ï¼Œå› ä¸ºä¸Šä¸€ä¸ªé€»è¾‘å—å·²ç»è£…æ»¡ï¼ŒvLLM å°†æ–°ç”Ÿæˆçš„ KV cache å­˜å‚¨åœ¨æ–°çš„é€»è¾‘å—ä¸­ï¼Œå¹¶ä¸”ä¸ºè¯¥é€»è¾‘å—åˆ†é…æ–°çš„ç‰©ç†å—ï¼Œåœ¨ block table ä¸­å­˜å‚¨è¯¥æ˜ å°„å…³ç³»
>    (è‡ªå›å½’è§£ç ä½¿ç”¨ FlashAttention æ²¡æœ‰æ„ä¹‰ï¼Œå› ä¸ºéœ€è¦ç¼–ç çš„åºåˆ—é•¿åº¦æ°¸è¿œæ»¡è¶³ $N=1$ï¼Œå½“ç„¶ FlashAttention2 æå‡ºåœ¨è‡ªå›å½’è§£ç æ—¶å¤šçº¿ç¨‹å¹¶è¡Œ load KV cacheï¼Œè¯¥æ€è·¯å¯ä»¥å’Œ PagedAttention ç»“åˆï¼Œä¹Ÿå°±æ˜¯å¤šçº¿ç¨‹å¹¶è¡Œ load ç‰©ç† KV cache å—ï¼ŒåŒæ—¶ tiling çš„æ€æƒ³ä»ç„¶å¯ä»¥åœ¨ PagedAttention çš„å®ç°ä¸­åº”ç”¨)

![[vLLM-Fig6.png]]

Globally, for each decoding iteration, vLLM first selects a set of candidate sequences for batching (more in $\S4.5$ ), and allocates the physical blocks for the newly required logical blocks. Then, vLLM concatenates all the input tokens of the current iteration (i.e., all tokens for prompt phase requests and the latest tokens for generation phase requests) as one sequence and feeds it into the LLM. During LLMâ€™s computation, vLLM uses the Paged Attention kernel to access the previous KV cache stored in the form of logical KV blocks and saves the newly generated KV cache into the physical KV blocks. Storing multiple tokens within a KV block (block size $>1$ ) enables the Paged Attention kernel to process the KV cache across more positions in parallel, thus increasing the hardware utilization and reducing latency. However, a larger block size also increases memory fragmentation. We study the effect of block size in $\S7.2$ . 
> å…¨å±€ä¸Šï¼Œåœ¨æ¯ä¸€æ¬¡è§£ç è¿­ä»£ä¸­ï¼ŒvLLM é¦–å…ˆé€‰æ‹©ä¸€ç»„å€™é€‰åºåˆ—è¿›è¡Œæ‰¹å¤„ç†ï¼Œå¹¶ä¸”ä¸ºæ–°éœ€è¦çš„é€»è¾‘å—åˆ†é…ç‰©ç†å—ï¼›ç„¶åï¼ŒvLLM å°†å½“å‰è¿­ä»£çš„æ‰€æœ‰è¾“å…¥ tokens (å¯¹äº prompt é˜¶æ®µçš„ requests å°±æ˜¯æ‰€æœ‰çš„ tokensï¼Œå¯¹äº generation é˜¶æ®µçš„ requests å°±æ˜¯æœ€åçš„ä¸€ä¸ª token) ä¸²è”æˆä¸€ä¸ªåºåˆ—ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ° LLM
> åœ¨ LLM è¿›è¡Œè®¡ç®—æ—¶ï¼ŒvLLM ä½¿ç”¨ PagedAttention kernel è®¿é—®ä¹‹å‰çš„ KV cache (ä»¥é€»è¾‘ KV å—çš„å½¢å¼å­˜å‚¨)ï¼Œå¹¶ä¸”å°†æ–°ç”Ÿæˆçš„ KV cache å‚¨å­˜åˆ°ç‰©ç† KV å—
> ä¸€ä¸ª KV å—å­˜å‚¨å¤šä¸ª tokens çš„ KV cache (å³ block size > 1) ä½¿å¾— PagedAttention å¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªä½ç½®çš„ KV cacheï¼Œè¿›è€Œæé«˜äº†ç¡¬ä»¶åˆ©ç”¨ç‡å¹¶é™ä½äº†å»¶è¿Ÿï¼Œä½†æ›´å¤§çš„ block size ä¹Ÿä¼šæé«˜å†…å­˜ç¢ç‰‡

Again, vLLM dynamically assigns new physical blocks to logical blocks as more tokens and their KV cache are generated. As all the blocks are filled from left to right and a new physical block is only allocated when all previous blocks are full, vLLM limits all the memory wastes for a request within one block, so it can effectively utilize all the memory, as shown in Fig. 2. This allows more requests to fit into memory for batchingâ€”hence improving the throughput. Once a request finishes its generation, its KV blocks can be freed to store the KV cache of other requests. 
> vLLM éšç€æ›´å¤šçš„ tokens å’Œå®ƒä»¬çš„ KV cache è¢«ç”Ÿæˆçš„æ—¶å€™ï¼ŒåŠ¨æ€åœ°å°†æ–°çš„ç‰©ç†å—åˆ†é…ç»™é€»è¾‘å—
> å› ä¸ºæ‰€æœ‰çš„å—éƒ½ä¼šä»å·¦åˆ°å³è¿›è¡Œå¡«å……ï¼Œå¹¶ä¸” vLLM ä»…åœ¨æ‰€æœ‰ä¹‹å‰çš„é€»è¾‘å—éƒ½å¡«æ»¡æ—¶æ‰åˆ†é…æ–°çš„ç‰©ç†å—ï¼Œæ•… vLLM å°†ä¸€ä¸ª request çš„å†…å­˜æµªè´¹å¤§å°é™åˆ¶åœ¨äº†å—å¤§å°ä»¥å†…ï¼Œæ•…å¯ä»¥é«˜æ•ˆåˆ©ç”¨å†…å­˜ï¼Œè¿™ä¹Ÿå…è®¸æ›´å¤šçš„ requests å¯ä»¥æ”¾å…¥å†…å­˜è¿›è¡Œæ‰¹å¤„ç†ï¼Œæ•…è¿›è€Œæé«˜äº†ååé‡
> ä¸€æ—¦ä¸€ä¸ª request å®Œæˆäº†å…¶ç”Ÿæˆï¼Œå®ƒçš„ KV å—å°±å¯ä»¥è¢«é‡Šæ”¾ï¼Œä»¥å­˜å‚¨å…¶ä»– requests çš„ KV cache

In Fig. 7, we show an example of vLLM managing the memory for two sequences. The logical blocks of the two sequences are mapped to different physical blocks within the space reserved by the block engine in GPU workers. The neighboring logical blocks of both sequences do not need to be contiguous in physical GPU memory and the space of physical blocks can be effectively utilized by both sequences. 
> Figure 7 å±•ç¤ºäº† vLLM ç®¡ç†ä¸¤ä¸ªåºåˆ—çš„å†…å­˜çš„ç¤ºä¾‹
> ä¸¤ä¸ªåºåˆ—çš„é€»è¾‘å—å„è‡ªè¢«æ˜ å°„åˆ°ä¸åŒçš„ç‰©ç†å— (ç‰©ç†å—çš„ç©ºé—´ç”± GPU worker ä¸Šçš„ block engine é¢„ç•™)ï¼Œå…¶ä¸­ç›¸é‚»çš„é€»è¾‘å—å¹¶ä¸è¦æ±‚å…¶ç‰©ç†å—è¿ç»­
> å¯ä»¥çœ‹åˆ°ç‰©ç†å—ç©ºé—´è¢«ä¸¤ä¸ªåºåˆ—åŒæ—¶é«˜æ•ˆåˆ©ç”¨

![[vLLM-Fig7.png]]

## 4.4 Application to Other Decoding Scenarios 
$\S4.3$ shows how Paged Attention and vLLM handle basic decoding algorithms, such as greedy decoding and sampling, that take one user prompt as input and generate a single output sequence. In many successful LLM applications [18 , 34], an LLM service must offer more complex decoding scenarios that exhibit complex accessing patterns and more opportunities for memory sharing. We show the general applicability of vLLM on them in this section. 
> ä¸Šä¸€èŠ‚ä»‹ç»äº† vLLM æ˜¯å¦‚ä½•å¤„ç†åŸºæœ¬çš„è§£ç ç®—æ³•çš„ (ä¾‹å¦‚è´ªå¿ƒè§£ç å’Œé‡‡æ ·ï¼Œå³æ¥å—ç”¨æˆ· prompt ä½œä¸ºè¾“å…¥ï¼Œç„¶åç”Ÿæˆå•ä¸ªè¾“å‡ºåºåˆ—)
> æœ¬èŠ‚ä»‹ç» vLLM å¯¹äºæ›´å¤æ‚è§£ç ç®—æ³•çš„å¤„ç†ï¼Œæ›´å¤æ‚çš„è§£ç ç®—æ³•ä¼šæœ‰æ›´å¤æ‚çš„å†…å­˜è®¿é—®æ¨¡å¼ï¼ŒåŒæ—¶ä¹Ÿæœ‰æ›´å¤šå†…å­˜å…±äº«çš„æœºä¼š

**Parallel sampling.** In LLM-based program assistants [6 , 18], an LLM generates multiple sampled outputs for a single input prompt; users can choose a favorite output from various candidates. So far we have implicitly assumed that a request generates a single sequence. In the remainder of this paper, we assume the more general case in which a request generates multiple sequences. In parallel sampling, one request includes multiple samples sharing the same input prompt, allowing the KV cache of the prompt to be shared as well. Via its Paged Attention and paged memory management, vLLM can realize this sharing easily and save memory. 
> å¹¶è¡Œé‡‡æ ·
> åœ¨åŸºäº LLM çš„ç¨‹åºåŠ©æ‰‹ä¸­ (ä¾‹å¦‚ copilot)ï¼ŒLLM ä¼šå¯¹å•ä¸ªè¾“å…¥ prompt ç”Ÿæˆå¤šä¸ªé‡‡æ ·çš„è¾“å‡ºï¼Œç”¨æˆ·ä»å¤šä¸ªè¾“å‡ºå€™é€‰ä¸­è¿›è¡Œé€‰æ‹©
> ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬éƒ½å‡è®¾ä¸€ä¸ª request ç”Ÿæˆå•ä¸ªåºåˆ—ï¼Œä¹‹åï¼Œæˆ‘ä»¬éƒ½è®¤ä¸ºä¸€ä¸ª request ç”Ÿæˆå¤šä¸ªåºåˆ—
> åœ¨å¹¶è¡Œé‡‡æ ·ä¸­ï¼Œå•ä¸ª request ä¼šå¯¹åº”å¤šä¸ªè¾“å‡ºåºåˆ— (å…±äº«ç›¸åŒçš„ prompt)ï¼Œå› æ­¤ prompt çš„ KV cache å°±å¯ä»¥è¢«å…±äº«ï¼ŒvLLM åŒæ ·å®ç°äº†è¿™ä¸€ç‚¹

Fig. 8 shows an example of parallel decoding for two outputs. Since both outputs share the same prompt, we only reserve space for one copy of the promptâ€™s state at the prompt phase; the logical blocks for the prompts of both sequences are mapped to the same physical blocks: the logical block 0 and 1 of both sequences are mapped to physical blocks 7 and 1, respectively. Since a single physical block can be mapped to multiple logical blocks, we introduce a reference count for each physical block. In this case, the reference counts for physical blocks 7 and 1 are both 2. At the generation phase, the two outputs sample different output tokens and need separate storage for KV cache. vLLM implements a copy-on-write mechanism at the block granularity for the physical blocks that need modification by multiple sequences, similar to the copy-on-write technique in OS virtual memory (e.g., when forking a process). Specifically, in Fig. 8, when sample A1 needs to write to its last logical block (logical block 1), vLLM recognizes that the reference count of the corresponding physical block (physical block 1) is greater than 1; it allocates a new physical block (physical block 3), instructs the block engine to copy the information from physical block 1, and decreases the reference count to 1. Next, when sample A2 writes to physical block 1, the reference count is already reduced to 1; thus A2 directly writes its newly generated KV cache to physical block 1. 
> å•ä¸ªè¾“å…¥å¤šä¸ªè¾“å‡ºçš„å¹¶è¡Œè§£ç è¿‡ç¨‹ç¤ºä¾‹è§ Figure 8
> å¯ä»¥çœ‹åˆ°ï¼Œå› ä¸ºä¸¤ä¸ªè¾“å‡ºå…±äº«ç›¸åŒçš„ promptï¼Œæˆ‘ä»¬åœ¨ prompt é˜¶æ®µä»…ä¿ç•™ä¸€ä»½ prompt çŠ¶æ€ï¼Œä¸¤ä¸ªåºåˆ—çš„é€»è¾‘å—è¢«æ˜ å°„åˆ°ç›¸åŒçš„ç‰©ç†å—
> å› ä¸ºå•ä¸ªç‰©ç†å—å¯ä»¥è¢«æ˜ å°„åˆ°å¤šä¸ªé€»è¾‘å—ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªç‰©ç†å—æ·»åŠ å¼•ç”¨è®¡æ•°ï¼Œæ­¤ä¾‹ä¸­ï¼Œç‰©ç†å— 1, 7 çš„å¼•ç”¨è®¡æ•°éƒ½æ˜¯ 2
> åœ¨è¾“å‡ºé˜¶æ®µï¼Œä¸åŒçš„è¾“å‡ºä¼šé‡‡æ ·ä¸åŒçš„ tokenï¼Œæ•…è¿™äº›æ–°ç”Ÿæˆçš„ tokens çš„ KV cache éœ€è¦åˆ†åˆ«å­˜å‚¨
> ä¸ºæ­¤ï¼ŒvLLM åœ¨å—çº§åˆ«çš„ç²’åº¦å®ç°å†™æ—¶æ‹·è´æœºåˆ¶ï¼Œå¯¹éœ€è¦è¢«å¤šä¸ªåºåˆ—ä¿®æ”¹çš„ç‰©ç†å—è¿›è¡Œå†™æ—¶æ‹·è´ï¼Œè¿™ä¹Ÿç±»ä¼¼äº OS è™šæ‹Ÿå†…å­˜ç®¡ç†ä¸­çš„å†™æ—¶æ‹·è´æŠ€æœ¯ (ä¾‹å¦‚ fork è¿›ç¨‹æ—¶)
> åœ¨ Figure8 ä¸­ï¼Œå½“æ ·æœ¬ A1 éœ€è¦å‘å®ƒæœ€æ–°çš„é€»è¾‘å—ä¸­**å†™å…¥**æ—¶ï¼ŒvLLM è¯†åˆ«åˆ°è¯¥é€»è¾‘å—å¯¹åº”çš„ç‰©ç†å—çš„å¼•ç”¨è®¡æ•°å¤§äº 1ï¼Œå› æ­¤åˆ†é…ä¸€ä¸ªæ–°çš„ç‰©ç†å—ï¼Œå¹¶è®© block engine å°†åŸæ¥å—çš„ä¿¡æ¯æ‹·è´åˆ°æ–°å—ï¼Œå¹¶ä¸”å°†åŸæ¥å—çš„å¼•ç”¨è®¡æ•°å‡ä¸€
> ä¹‹åï¼Œå½“æ ·æœ¬ A2 éœ€è¦å‘å®ƒçš„é€»è¾‘å—å†™å…¥æ—¶ï¼ŒvLLM è¯†åˆ«åˆ°è¯¥é€»è¾‘å—å¯¹åº”çš„ç‰©ç†å—å¼•ç”¨è®¡æ•°ä»…ä¸º 1ï¼Œå³æ²¡æœ‰è¢«å¤ç”¨ï¼Œç‹¬å±äº A2ï¼Œå› æ­¤ A2 ç›´æ¥å°†å…¶æ–° token çš„ KV cache å†™å…¥åˆ°åŸæ¥çš„ç‰©ç†å—ä¸­

![[vLLM-Fig8.png]]

In summary, vLLM enables the sharing of most of the space used to store the promptsâ€™ KV cache across multiple output samples, with the exception of the final logical block, which is managed by a copy-on-write mechanism. By sharing physical blocks across multiple samples, memory usage can be greatly reduced, especially for long input prompts . 
> æ€»ä¹‹ï¼ŒvLLM ä½¿å¾—å¤šä¸ªè¾“å‡ºæ ·æœ¬ä¹‹é—´å¯ä»¥å…±äº«å­˜å‚¨ prompt çš„ KV cache çš„å¤§å¤šæ•°ç©ºé—´ (åªæœ‰æœ€åä¸€ä¸ªé€»è¾‘å—ä¸­çš„éƒ¨åˆ† prompt tokens çš„ KV cache ä¸èƒ½å…±äº«)ï¼Œè¿™è¿›è€Œå‡å°‘äº†å¤šä¸ªè¾“å‡ºæ ·æœ¬æƒ…å†µä¸‹çš„å†…å­˜ä½¿ç”¨é‡ï¼Œå°¤å…¶æ˜¯å¯¹äºé•¿çš„è¾“å…¥ prompt

**Beam search.** In LLM tasks like machine translation [59], the users expect the top-k most appropriate translations output by the LLM. Beam search [49] is widely used to decode the most probable output sequence from an LLM, as it mitigates the computational complexity of fully traversing the sample space. The algorithm relies on the beam width parameter $k$ , which determines the number of top candidates retained at every step. During decoding, beam search expands each candidate sequence in the beam by considering all possible tokens, computes their respective probabilities using the LLM, and retains the top-k most probable sequences out of $k\cdot|V|$ candidates, where $|V|$ is the vocabulary size. 
> æŸæœç´¢
> åœ¨åƒæœºå™¨ç¿»è¯‘è¿™æ ·çš„ LLM ä»»åŠ¡ä¸­ï¼Œç”¨æˆ·æœŸå¾… LLM è¾“å‡º top-k ä¸ªæœ€åˆé€‚çš„ç¿»è¯‘ç»“æœ
> beam search è¢«å¹¿æ³›ç”¨äºä» LLM ä¸­è§£ç æœ€å¯èƒ½çš„è¾“å‡ºåºåˆ—ï¼Œè¯¥æ–¹æ³•ç¼“è§£äº†å®Œå…¨éå†æ ·æœ¬ç©ºé—´çš„è®¡ç®—å¤æ‚æ€§ (å¯¹äºé•¿åº¦ä¸º $n$ çš„åºåˆ—ï¼Œè¯è¢‹å¤§å°ä¸º $|V|$ï¼Œåˆ™å®Œæ•´æ ·æœ¬ç©ºé—´çš„å¤§å°ä¸º $|V|^n$)
> beam search ç®—æ³•ä¾èµ–äº beam width å‚æ•° $k$ï¼Œè¯¥å‚æ•°å†³å®šäº†æ¯ä¸€æ­¥éœ€è¦ä¿ç•™çš„å‰ $k$ ä¸ªå€™é€‰ tokenï¼Œåœ¨è§£ç æ—¶ï¼Œbeam search é€šè¿‡è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„ tokens å±•å¼€ beam ä¸­çš„æ¯ä¸ªå€™é€‰åºåˆ—ï¼Œä½¿ç”¨ LLM è®¡ç®—å®ƒä»¬å„è‡ªçš„æ¦‚ç‡ï¼Œç„¶åä» $k\cdot |V|$ ä¸ªå€™é€‰åºåˆ—ä¸­ä¿ç•™ top-k ä¸ªæœ€å¯èƒ½çš„åºåˆ—

Unlike parallel decoding, beam search facilities sharing not only the initial prompt blocks but also other blocks across different candidates, and the sharing patterns dynamically change as the decoding process advances, similar to the process tree in the OS created by compound forks. Fig. 9 shows how vLLM manages the KV blocks for a beam search example with $k\,=\,4$ . Prior to the iteration illustrated as the dotted line, each candidate sequence has used 4 full logical blocks. All beam candidates share the first block 0 (i.e., prompt). Candidate 3 digresses from others from the second block. Candidates 0-2 share the first 3 blocks and diverge at the fourth block. At subsequent iterations, the top-4 probable candidates all originate from candidates 1 and 2. As the original candidates 0 and 3 are no longer among the top candidates, their logical blocks are freed, and the reference counts of corresponding physical blocks are reduced. vLLM frees all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8). Then, vLLM allocates new physical blocks (blocks 9-12) to store the new KV cache from the new candidates. Now, all candidates share blocks 0, 1, 3; candidates 0 and 1 share block 6, and candidates 2 and 3 further share block 7. 
> å’Œå¹¶è¡Œè§£ç ä¸åŒï¼Œbeam search ä¸ä»…å…±äº«åˆå§‹çš„ prompt å—ï¼Œè¿˜å…±äº«ä¸åŒå€™é€‰ä¹‹é—´çš„ KV å—ï¼Œå¹¶ä¸”å…±äº«æ¨¡å¼éšç€è§£ç è¿‡ç¨‹åŠ¨æ€æ”¹å˜ï¼Œç±»ä¼¼äº OS é€šè¿‡å¤åˆ fork åˆ›å»ºçš„è¿›ç¨‹æ ‘
> Figure 9 å±•ç¤ºäº† $k=4$ æ—¶çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œåœ¨è™šçº¿ä¹‹å‰çš„è¿­ä»£ä¸­ï¼Œæ‰€æœ‰çš„å€™é€‰åºåˆ—éƒ½å„è‡ªä½¿ç”¨å››ä¸ªå®Œå…¨çš„é€»è¾‘å—ï¼Œæ‰€æœ‰çš„å€™é€‰åºåˆ—åœ¨ç‰©ç†ä¸Šå…±äº« block 0 (prompt)ï¼Œå€™é€‰3ä»ç¬¬äºŒä¸ªå—å¼€å§‹åˆ†ç¦»ï¼Œå€™é€‰0-2å…±äº«å‰3ä¸ªå—ï¼Œåœ¨ç¬¬å››ä¸ªå—åˆ†ç¦»
> åœ¨åç»­çš„è¿­ä»£ä¸­ï¼Œå‰ $k=4$ ä¸ªæœ€å¯èƒ½çš„å€™é€‰éƒ½æ¥è‡ªäºå€™é€‰ 1 å’Œ 2ï¼Œåˆ™åŸæ¥çš„å€™é€‰ 0 å’Œ 3 ä¸å†éœ€è¦ï¼Œåˆ™å®ƒä»¬çš„é€»è¾‘å—è¢«é‡Šæ”¾ï¼Œå¯¹åº”çš„ç‰©ç†å—çš„å¼•ç”¨è®¡æ•°å‡å°‘ï¼ŒvLLM ä¼šé‡Šæ”¾å¼•ç”¨è®¡æ•°å‡å°‘ä¸º 0 çš„ç‰©ç†å—
> ç„¶å vLLM åˆ†é…æ–°çš„ç‰©ç†å—æ¥å­˜å‚¨æ–°çš„å€™é€‰çš„ KV cacheï¼Œæ­¤æ—¶å€™é€‰ 0, 1 å…±äº« block 6ï¼Œå€™é€‰ 2, 3 å…±äº« block 7ï¼Œæ‰€æœ‰å€™é€‰å…±äº« block 0, 1, 3


![[vLLM-Fig9.png]]

Previous LLM serving systems require frequent memory copies of the KV cache across the beam candidates. For example, in the case shown in Fig. 9, after the dotted line, candidate 3 would need to copy a large portion of candidate 2â€™s KV cache to continue generation. This frequent memory copy overhead is significantly reduced by vLLMâ€™s physical block sharing. In vLLM, most blocks of different beam candidates can be shared. The copy-on-write mechanism is applied only when the newly generated tokens are within an old shared block, as in parallel decoding. This involves only copying one block of data. 
> ä¹‹å‰çš„ LLM æœåŠ¡ç³»ç»Ÿéœ€è¦åœ¨ beam å€™é€‰ä¸­é¢‘ç¹åœ°æ‹·è´ KV cache çš„å†…å­˜ï¼Œä¾‹å¦‚åœ¨ Figure 9 ä¸­ï¼Œå€™é€‰ 3 éœ€è¦æ‹·è´å€™é€‰ 2 çš„å¤§éƒ¨åˆ† KV cache ä»¥ç»§ç»­ç”Ÿæˆ
> vLLM çš„ç‰©ç†å—å…±äº«æ˜¾è‘—é™ä½äº†è¿™ç±»é¢‘ç¹å†…å­˜æ‹·è´çš„å¼€é”€ï¼Œåœ¨ vLLM ä¸­ï¼Œå¤šæ•° beam å€™é€‰çš„å—å¯ä»¥è¢«å…±äº«ï¼Œå†™æ—¶æ‹·è´æœºåˆ¶ä»…åœ¨æ–°ç”Ÿæˆçš„ token ä½äºæ—§çš„å…±äº«å—ä¸­æ‰æ‰§è¡Œ (å’Œå¹¶è¡Œè§£ç ä¸­çš„æƒ…å†µä¸€æ ·)ï¼Œè¿™ä»…æ¶‰åŠæ‹·è´ä¸€å—æ•°æ®

**Shared prefix.** Commonly, the LLM user provides a (long) description of the task including instructions and example inputs and outputs, also known as system prompt [36]. The description is concatenated with the actual task input to form the prompt of the request. The LLM generates outputs based  on the full prompt. Fig. 10 shows an example. Moreover, the shared prefix can be further tuned, via prompt engineering, to improve the accuracy of the downstream tasks [26, 27]. 
> å…±äº«å‰ç¼€
> ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒLLM ç”¨æˆ·ä¼šæä¾›å¯¹ä»»åŠ¡çš„æè¿° (åŒ…æ‹¬æŒ‡ä»¤ã€ç¤ºä¾‹è¾“å…¥è¾“å‡º)ï¼Œè¿™ç±» prompt ä¹Ÿç§°ä¸ºç³»ç»Ÿ prompt
> è¯¥æè¿°ä¼šå’Œå®é™…çš„ä»»åŠ¡è¾“å…¥è¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ° request çš„å®Œæ•´ promptï¼ŒLLM åŸºäºå®Œæ•´ prompt ç”Ÿæˆè¾“å‡º
> Figure 10 å±•ç¤ºäº†å…±äº«å‰ç¼€çš„ä¸€ä¸ªç¤ºä¾‹

![[vLLM-Fig10.png]]

For this type of application, many user prompts share a prefix, thus the LLM service provider can store the KV cache of the prefix in advance to reduce the redundant computation spent on the prefix. In vLLM, this can be conveniently achieved by reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider, as how OS handles shared library across processes. A user input prompt with the shared prefix can simply map its logical blocks to the cached physical blocks (with the last block marked copy-on-write). The prompt phase computation only needs to execute on the userâ€™s task input. 
> å¯¹äºè¿™ç±»åº”ç”¨ï¼Œè®¸å¤šç”¨æˆ· prompt ä¼šå…±äº«åŒä¸€ä¸ªå‰ç¼€ï¼Œå› æ­¤ LLM æœåŠ¡æä¾›è€…å¯ä»¥å°†è¯¥å‰ç¼€çš„ KV cache æå‰å­˜å‚¨ï¼Œä»¥å‡å°‘é‡å¤è®¡ç®—
> vLLM ä¸­ï¼Œå¯ä»¥ä¸ºé¢„å®šä¹‰çš„å‰ç¼€çš„ KV cache é¢„ç•™ä¸€ç»„ç‰©ç†å—ï¼Œç±»ä¼¼äº OS åœ¨å¤šä¸ªè¿›ç¨‹ä¹‹é—´å¤„ç†å…±äº«åº“ï¼Œä½¿ç”¨å…±äº«å‰ç¼€çš„ç”¨æˆ·è¾“å…¥ prompt å¯ä»¥ç›´æ¥å°†å…¶é€»è¾‘å—æ˜ å°„åˆ°è¿™äº›ç¼“å­˜çš„ç‰©ç†å— (æœ€åä¸€ä¸ªå—è¿›è¡Œå†™æ—¶æ‹·è´)ï¼Œprompt é˜¶æ®µçš„è®¡ç®—å°±åªéœ€è¦å¯¹ç”¨æˆ·çš„ä»»åŠ¡è¾“å…¥è¿›è¡Œ

**Mixed decoding methods.** The decoding methods discussed earlier exhibit diverse memory sharing and accessing patterns. Nonetheless, vLLM facilitates the simultaneous processing of requests with different decoding preferences, which existing systems cannot efficiently do. This is because vLLM conceals the complex memory sharing between different sequences via a common mapping layer that translates logical blocks to physical blocks. The LLM and its execution kernel only see a list of physical block IDs for each sequence and do not need to handle sharing patterns across sequences. Compared to existing systems, this approach broadens the batching opportunities for requests with different sampling requirements, ultimately increasing the systemâ€™s overall throughput. 
> æ··åˆè§£ç æ–¹æ³•
> è™½ç„¶ä¹‹å‰è®¨è®ºçš„è§£ç æ–¹æ³•å…·æœ‰ä¸åŒçš„å†…å­˜å…±äº«å’Œè®¿é—®æ¨¡å¼ï¼Œä½† vLLM ä¹Ÿå¯ä»¥åŒæ—¶å¤„ç†å…·æœ‰ä¸åŒè§£ç åå¥½çš„ requestsï¼Œè¿™æ˜¯ç°å­˜ç³»ç»Ÿæ— æ³•é«˜æ•ˆåšåˆ°çš„
> è¿™æ˜¯å› ä¸º vLLM é€šè¿‡ä¸€ä¸ªå°†é€»è¾‘å—è½¬åŒ–ä¸ºç‰©ç†å—çš„é€šç”¨çš„æ˜ å°„å±‚éšè—äº†ä¸åŒåºåˆ—ä¹‹é—´å¤æ‚çš„å†…å­˜å…±äº«æ¨¡å¼ï¼ŒLLM å’Œå…¶æ‰§è¡Œ kernel ä»…çœ‹åˆ°æ¯ä¸ªåºåˆ—çš„ç‰©ç†å— ID åˆ—è¡¨ï¼Œè€Œä¸éœ€è¦å¤„ç†åºåˆ—ä¹‹é—´çš„å…±äº«æ¨¡å¼
> ç›¸è¾ƒäºç°æœ‰ç³»ç»Ÿï¼Œè¯¥æ–¹æ³•æ‰©å¤§äº†å…·æœ‰ä¸åŒé‡‡æ ·/è§£ç è¯·æ±‚çš„ requests çš„æ‰¹å¤„ç†æœºä¼šï¼Œæœ€ç»ˆæé«˜äº†ç³»ç»Ÿçš„æ•´ä½“ååé‡

## 4.5 Scheduling and Preemption 
When the request traffic surpasses the systemâ€™s capacity, vLLM must prioritize a subset of requests. In vLLM, we adopt the first-come-first-serve (FCFS) scheduling policy for all requests, ensuring fairness and preventing starvation. When vLLM needs to preempt requests, it ensures that the earliest arrived requests are served first and the latest requests are preempted first. 
> å½“ request æµé‡è¶…è¿‡äº†ç³»ç»Ÿå¤„ç†èƒ½åŠ›ï¼ŒvLLM å¿…é¡»æœ‰é™å¤„ç†ä¸€éƒ¨åˆ† requests
> vLLM å¯¹äºæ‰€æœ‰çš„ requests é‡‡ç”¨å…ˆåˆ°å…ˆæœåŠ¡è°ƒåº¦ç­–ç•¥ï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§å¹¶é˜²æ­¢é¥¥é¥¿ç°è±¡çš„å‘ç”Ÿ
> å½“ vLLM éœ€è¦æŠ¢å  requests æ—¶ï¼Œå®ƒç¡®ä¿æœ€æ—©åˆ°è¾¾çš„ requests ä¼˜å…ˆè¢«æœåŠ¡ï¼Œè€Œæœ€æ™š/æœ€è¿‘çš„ requests åˆ™ä¼˜å…ˆè¢«æŠ¢å 

LLM services face a unique challenge: the input prompts for an LLM can vary significantly in length, and the resulting output lengths are not known a priori, contingent on both the input prompt and the model. As the number of requests and their outputs grow, vLLM can run out of the GPUâ€™s physical blocks to store the newly generated KV cache. There are two classic questions that vLLM needs to answer in this context: (1) Which blocks should it evict? (2) How to recover evicted blocks if needed again? Typically, eviction policies use heuristics to predict which block will be accessed furthest in the future and evict that block. Since in our case we know that all blocks of a sequence are accessed together, we implement an all-or-nothing eviction policy, i.e., either evict all or none of the blocks of a sequence. Furthermore, multiple sequences within one request (e.g., beam candidates in one beam search request) are gang-scheduled as a sequence group . The sequences within one sequence group are always preempted or rescheduled together due to potential memory sharing across those sequences. To answer the second question of how to recover an evicted block, we consider two techniques: 
> LLM æœåŠ¡çš„ä¸€ä¸ªç‹¬ç‰¹æŒ‘æˆ˜æ˜¯ï¼šLLM çš„è¾“å…¥ prompt çš„é•¿åº¦ä¹‹é—´çš„å·®å¼‚ä¼šå¾ˆå¤§ï¼Œä»¥åŠè¾“å‡ºçš„é•¿åº¦ä¹Ÿä¸æ˜¯é¢„å…ˆå¯çŸ¥çš„ï¼Œè€Œæ˜¯å–å†³äºè¾“å…¥ prompt å’Œæ¨¡å‹
> éšç€ requests çš„æ•°é‡å’Œå®ƒä»¬çš„è¾“å‡ºé•¿åº¦å¢é•¿ï¼ŒvLLM å¯èƒ½ä¼šè€—å°½ GPU çš„ç‰©ç†å—ï¼Œä»¥è‡³äºæ— æ³•å­˜å‚¨æ–°ç”Ÿæˆçš„ KV cacheï¼Œåœ¨è¯¥èƒŒæ™¯ä¸‹ï¼ŒvLLM éœ€è¦å›ç­”ä¸¤ä¸ªç»å…¸é—®é¢˜ï¼š
> 1. åº”è¯¥æ·˜æ±°å“ªäº›å—ï¼Ÿ
> 2. å¦‚æœéœ€è¦å†æ¬¡ä½¿ç”¨è¿™äº›å—ï¼Œå¦‚ä½•æ¢å¤å®ƒä»¬ï¼Ÿ
> ä¸€èˆ¬åœ°ï¼Œæ·˜æ±°ç­–ç•¥ä½¿ç”¨å¯å‘å¼ç®—æ³•é¢„æµ‹å“ªä¸ªå—å°†åœ¨æœªæ¥æœ€ä¸å®¹æ˜“è¢«è®¿é—®
> å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜çš„å›ç­”ï¼š
> åœ¨å¤„ç†ä¸€ä¸ªåºåˆ—æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è®¿é—®åºåˆ—ä¸­çš„æ‰€æœ‰ KV å—ï¼Œå› æ­¤æˆ‘ä»¬å®ç° all-or-nothing æ·˜æ±°ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯è¦ä¹ˆä¸æ·˜æ±°ï¼Œè¦ä¹ˆæ·˜æ±°ä¸€ä¸ªåºåˆ—æ‰€æœ‰çš„ KV å—
> å¦å¤–ï¼Œå•ä¸ª request å¯¹åº”å¤šä¸ªåºåˆ— (ä¾‹å¦‚ beam search request çš„å¤šä¸ª beam candidates) åˆ™ä½œä¸ºåºåˆ—ç»„å…±åŒè¢«è°ƒåº¦
> å› ä¸ºä¸€ä¸ªåºåˆ—å†…çš„åºåˆ—å¯èƒ½å­˜åœ¨å†…å­˜å…±äº«ï¼Œå› æ­¤ä¸€ä¸ªç»„å†…çš„åºåˆ—æ€»æ˜¯è¢«ä¸€èµ·æŠ¢å æˆ–é‡æ–°è°ƒåº¦
> å¯¹äºç¬¬äºŒä¸ªé—®é¢˜çš„å›ç­”ï¼Œæˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹ä¸¤ä¸ªæŠ€æœ¯ï¼š

**Swapping.** This is the classic technique used by most virtual memory implementations which copy the evicted pages to a swap space on the disk. In our case, we copy evicted blocks to the CPU memory. As shown in Fig. 4, besides the GPU block allocator, vLLM includes a CPU block allocator to manage the physical blocks swapped to CPU RAM. When vLLM exhausts free physical blocks for new tokens, it selects a set of sequences to evict and transfer their KV cache to the CPU. Once it preempts a sequence and evicts its blocks, vLLM stops accepting new requests until all preempted sequences are completed. Once a request completes, its blocks are freed from memory, and the blocks of a preempted sequence are brought back in to continue the processing of that sequence. Note that with this design, the number of blocks swapped to the CPU RAM never exceeds the number of total physical blocks in the GPU RAM, so the swap space on the CPU RAM is bounded by the GPU memory allocated for the KV cache. 
> äº¤æ¢
> äº¤æ¢æ˜¯è™šæ‹Ÿå†…å­˜å®ç°ä¸­çš„ç»å…¸æŠ€æœ¯ï¼Œå®ƒå°†è¢«æ·˜æ±°çš„é¡µäº¤æ¢åˆ°ç£ç›˜ä¸Šçš„äº¤æ¢åŒº
> vLLM å°†æ·˜æ±°çš„å—å†™åˆ° CPU å†…å­˜ï¼Œå¦‚ Figure 4 æ‰€ç¤ºï¼ŒvLLM æœ‰ GPU block allocator å’Œ CPU block allocatorï¼ŒCPU block allocator è´Ÿè´£ç®¡ç†äº¤æ¢åˆ° CPU RAM çš„ç‰©ç†å—ï¼Œå½“ vLLM è€—å°½å¯ä»¥ç”¨äºæ–° token çš„ç‰©ç†å—åï¼Œå®ƒé€‰æ‹©ä¸€ç»„åºåˆ—ï¼Œå°†å…¶ KV cache å—äº¤æ¢åˆ° CPU RAM
> å½“ vLLM æŠ¢å äº†ä¸€ä¸ªåºåˆ—ï¼Œå¹¶å°†å…¶ KV cache å—å…¨éƒ¨æ·˜æ±°åï¼ŒvLLM å°†åœæ­¢æ¥å—æ–°çš„è¯·æ±‚ï¼Œç›´åˆ°æ‰€æœ‰è¢«æŠ¢å çš„åºåˆ—å®Œæˆ (é˜²æ­¢é¥¥é¥¿)
> æŠ¢å è¯·æ±‚å®Œæˆåï¼Œå®ƒçš„ KV å—å°±ä»å†…å­˜ä¸­è¢«é‡Šæ”¾ï¼Œè¢«å®ƒæŠ¢å çš„åºåˆ—çš„å—ä¼šè¢«äº¤æ¢å›æ¥ï¼ŒvLLM ç»§ç»­å¤„ç†è¯¥åºåˆ—
> åœ¨è¯¥è®¾è®¡ä¸‹ï¼Œäº¤æ¢åˆ° CPU RAM çš„ KV å—çš„æ•°é‡å°†æ°¸è¿œä¸ä¼šè¶…è¿‡ GPU RAM ä¸­çš„æ€»ç‰©ç†å—æ•°é‡ï¼Œå› æ­¤ CPU RAM çš„äº¤æ¢ç©ºé—´çš„ä½¿ç”¨ä¸Šé™å°±æ˜¯ GPU å†…å­˜ä¸º KV cache åˆ†é…å¤§å°çš„ä¸Šé™ (æœ€åæƒ…å†µä¸‹ï¼Œè¢«äº¤æ¢å‡ºçš„åºåˆ—å æ®äº† GPU RAM çš„å…¨éƒ¨ç‰©ç†å—ï¼Œæ•…æ­¤æ—¶ CPU RAM ä¸­äº¤æ¢åŒºçš„å¤§å°å°±ç­‰äº GPU å†…å­˜ä¸­ä¸º KV cache å—åˆ†é…çš„æ€»ç©ºé—´å¤§å°)

**Recomputation.** In this case, we simply recompute the KV cache when the preempted sequences are rescheduled. Note that re-computation latency can be significantly lower than the original latency, as the tokens generated at decoding can be concatenated with the original user prompt as a new promptâ€”their KV cache at all positions can be generated in one prompt phase iteration. 
> é‡è®¡ç®—
> å¦ä¸€ç§é€‰æ‹©æ˜¯å½“è¢«æŠ¢å çš„åºåˆ—è¢«é‡æ–°è°ƒåº¦æ—¶ï¼Œç›´æ¥é‡æ–°è®¡ç®—å®ƒçš„ KV cache
> æ³¨æ„é‡æ–°è®¡ç®— KV cache çš„å»¶è¿Ÿä¼šæ˜¾è‘—ä½äºåŸæ¥çš„å»¶è¿Ÿï¼Œå› ä¸ºåœ¨è§£ç é˜¶æ®µç”Ÿæˆçš„ tokens å¯ä»¥å’Œç”¨æˆ· prompt æ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºæ–°çš„ promptï¼Œå› æ­¤åœ¨å•ä¸ª prompt é˜¶æ®µå°±ç”Ÿæˆäº†ä¹‹å‰æ‰€æœ‰ tokens çš„ KV cache

The performances of swapping and re-computation depend on the bandwidth between CPU RAM and GPU memory and the computation power of the GPU. We examine the speeds of swapping and re-computation in $\S7.3$ . 
> äº¤æ¢ç­–ç•¥å’Œé‡è®¡ç®—ç­–ç•¥çš„æ€§èƒ½å–å†³äº CPU RAM å’Œ GPU DRAM ä¹‹é—´çš„å¸¦å®½å’Œ GPU çš„è®¡ç®—èƒ½åŠ›

## 4.6 Distributed Execution 
Many LLMs have parameter sizes exceeding the capacity of a single GPU [5 , 9]. Therefore, it is necessary to partition them across distributed GPUs and execute them in a model parallel fashion [28 , 63]. This calls for a memory manager capable of handling distributed memory. vLLM is effective in distributed settings by supporting the widely used Megatron-LM style tensor model parallelism strategy on Transformers [47]. This strategy adheres to an SPMD (Single Program Multiple Data) execution schedule, wherein the linear layers are partitioned to perform block-wise matrix multiplication, and the the GPUs constantly synchronize intermediate results via an allreduce operation. Specifically, the attention operator is split on the attention head dimension, each SPMD process takes care of a subset of attention heads in multi-head attention. 
> è®¸å¤š LLM çš„å‚æ•°å¤§å°è¶…è¿‡äº†å•ä¸ª GPU çš„ DRAM å¤§å°ï¼Œå› æ­¤éœ€è¦å°†è¿™äº›å‚æ•°åˆ’åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œå¹¶é‡‡ç”¨æ¨¡å‹å¹¶è¡Œçš„æ–¹å¼è¿›è¡Œè®­ç»ƒå’Œæ¨ç†
> vLLM æ”¯æŒ Megatron-LM é£æ ¼çš„ tensor æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œå› æ­¤åœ¨åˆ†å¸ƒå¼æ‰§è¡Œçš„æƒ…å†µä¸‹åŒæ ·é«˜æ•ˆ
> è¯¥ç­–ç•¥éµå¾ªå•ç¨‹åºå¤šæ•°æ®çš„æ‰§è¡Œè°ƒåº¦ï¼Œå…¶ä¸­çº¿æ€§å±‚è¢«åˆ’åˆ†ä»¥æ‰§è¡Œåˆ†å—çš„çŸ©é˜µä¹˜æ³•ï¼Œå¹¶ä¸” GPUs æŒç»­åœ°é€šè¿‡ allreduce æ“ä½œè¿›è¡ŒåŒæ­¥
> ç‰¹åˆ«åœ°ï¼Œattention ç®—å­åœ¨ attention head ç»´åº¦ä¸Šè¢«åˆ’åˆ†ï¼Œæ¯ä¸ª SPMD è¿›ç¨‹å¤„ç†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­çš„ä¸€éƒ¨åˆ†å¤´

We observe that even with model parallel execution, each model shard still processes the same set of input tokens, thus requiring the KV Cache for the same positions. Therefore, vLLM features a single KV cache manager within the centralized scheduler, as in Fig. 4. Different GPU workers share the manager, as well as the mapping from logical blocks to physical blocks. This common mapping allows GPU workers to execute the model with the physical blocks provided by the scheduler for each input request. Although each GPU worker has the same physical block IDs, a worker only stores a portion of the KV cache for its corresponding attention heads. 
> åœ¨æ¨¡å‹å¹¶è¡Œæ‰§è¡Œä¸‹ï¼Œæ¯ä¸ªæ¨¡å‹ç¢ç‰‡ä»ç„¶è¦å¤„ç†ç›¸åŒçš„ä¸€ç»„è¾“å…¥ tokensï¼Œå› æ­¤éœ€è¦ç›¸åŒä½ç½®çš„ KV cache
> å› æ­¤ï¼ŒvLLM åœ¨ä¸­å¿ƒåŒ–çš„è°ƒåº¦å™¨ä¸­ä»…éœ€è¦ä¸€ä¸ª KV cache ç®¡ç†å™¨ï¼Œå¦‚ Figure4 æ‰€ç¤ºï¼Œä¸åŒçš„ GPU worker å…±äº«è¯¥ç®¡ç†å™¨ï¼Œä»¥åŠå…±äº«é€»è¾‘å—åˆ°ç‰©ç†å—çš„æ˜ å°„
> å¤„ç†ä¸€ä¸ªè¾“å…¥ request çš„ KV cache æ—¶ï¼Œè™½ç„¶æ¯ä¸ª GPU worker éƒ½å…·æœ‰ç›¸åŒçš„ç‰©ç†å— IDsï¼Œä½†ä¸€ä¸ª GPU worker (çš„ç‰©ç†å—) ä»…å­˜å‚¨å¯¹åº”çš„ attention heads çš„ä¸€éƒ¨åˆ† KV cache

In each step, the scheduler first prepares the message with input token IDs for each request in the batch, as well as the block table for each request. Next, the scheduler broadcasts this control message to the GPU workers. Then, the GPU workers start to execute the model with the input token IDs. In the attention layers, the GPU workers read the KV cache according to the block table in the control message. During execution, the GPU workers synchronize the intermediate results with the all-reduce communication primitive without the coordination of the scheduler, as in [47]. In the end, the GPU workers send the sampled tokens of this iteration back to the scheduler. In summary, GPU workers do not need to synchronize on memory management as they only need to receive all the memory management information at the beginning of each decoding iteration along with the step inputs. 
> åœ¨æ‰§è¡Œçš„æ¯ä¸€æ­¥ï¼Œè°ƒåº¦å™¨é¦–å…ˆæ ¹æ® batch ä¸­æ¯ä¸ª request çš„è¾“å…¥ tokens ç¡®å®š token IDsï¼Œå¹¶ä¸”ä¸ºæ¯ä¸ª request å‡†å¤‡ block tableï¼›ç„¶åå°†å¸¦æœ‰ token IDs å’Œ table ä¿¡æ¯çš„æ§åˆ¶æ¶ˆæ¯å¹¿æ’­åˆ° GPU workers
> ä¹‹åï¼ŒGPU workers æ ¹æ®æ”¶åˆ°çš„ token IDs å¼€å§‹æ‰§è¡Œæ¨¡å‹
> åœ¨ attention å±‚ï¼ŒGPU worker æ ¹æ®æ§åˆ¶æ¶ˆæ¯ä¸­çš„ block table è¯»å– KV cache
> åœ¨æ‰§è¡Œæ—¶ï¼ŒGPU workers ä¹‹é—´é€šè¿‡ all-reduce é€šè®¯åŸè¯­åŒæ­¥ä¸­é—´ç»“æœï¼Œä¸éœ€è¦è°ƒåº¦å™¨çš„ååŠ©
> æœ€åï¼ŒGPU workers å°†è¯¥æ¬¡è¿­ä»£é‡‡æ ·å¾—åˆ°çš„ tokens è¿”å›ç»™è°ƒåº¦å™¨
> æ€»ä¹‹ï¼ŒGPU workers ä¸éœ€è¦åœ¨ memory ç®¡ç†ä¸ŠåŒæ­¥ï¼Œå› ä¸ºå®ƒä»¬ä»…éœ€è¦åœ¨æ¯æ¬¡è§£ç è¿­ä»£çš„å¼€å§‹æ¥å—æ‰€æœ‰çš„ memory ç®¡ç†ä¿¡æ¯ (ä»¥åŠè¯¥æ­¥çš„è¾“å…¥)

# 5 Implementation
vLLM is an end-to-end serving system with a FastAPI [15] frontend and a GPU-based inference engine. The frontend extends the OpenAI API [34] interface, allowing users to customize sampling parameters for each request, such as the maximum sequence length and the beam width $k$ . The vLLM engine is written in 8.5K lines of Python and 2K lines of C++/CUDA code. We develop control-related components including the scheduler and the block manager in Python while developing custom CUDA kernels for key operations such as Paged Attention. For the model executor, we implement popular LLMs such as GPT [5], OPT [62], and LLaMA [52] using PyTorch [39] and Transformers [58]. We use NCCL [32] for tensor communication across the distributed GPU workers. 
> vLLM æ˜¯ç«¯åˆ°ç«¯çš„æœåŠ¡ç³»ç»Ÿï¼Œå…·æœ‰ FastAPI å‰ç«¯å’ŒåŸºäº GPU çš„æ¨ç†å¼•æ“
> å‰ç«¯æ‹“å±•äº† OpenAI API æ¥å£ï¼Œå…è®¸ç”¨æˆ·ä¸ºæ¯ä¸ª request è‡ªå®šä¹‰é‡‡æ ·å‚æ•°ï¼Œä¾‹å¦‚æœ€å¤§åºåˆ—é•¿åº¦å’ŒæŸå®½åº¦ $k$
> vLLM å¼•æ“ä¸º Python + C++/CUDAï¼Œå’Œæ§åˆ¶ç›¸å…³çš„ç»„ä»¶ï¼ŒåŒ…æ‹¬è°ƒåº¦å™¨å’Œå—ç®¡ç†å™¨éƒ½ç”¨ Python å¼€å‘ï¼Œå¯¹äºå…³é”®è¿ç®—ä¾‹å¦‚ PagedAttention åˆ™å®ç°ä¸º CUDA
>  kernel
>  å¸¸è§çš„ LLM ä¾‹å¦‚ GPTã€OPTã€LLaMA ç­‰ä½¿ç”¨ PyTorch å’Œ Transformers å®ç°
>  GPU workers ä¹‹é—´çš„ tensor é€šè®¯ä½¿ç”¨ NCCL å®ç°

## 5.1 Kernel-level Optimization 
Since Paged Attention introduces memory access patterns that are not efficiently supported by existing systems, we develop several GPU kernels for optimizing it. (1) *Fused reshape and block write.* In every Transformer layer, the new KV cache are split into blocks, reshaped to a memory layout optimized for block read, then saved at positions specified by the block table. To minimize kernel launch overheads, we fuse them into a single kernel. (2) *Fusing block read and attention.* We adapt the attention kernel in Faster Transformer [31] to read KV cache according to the block table and perform attention operations on the fly. To ensure coalesced memory access, we assign a GPU warp to read each block. Moreover, we add support for variable sequence lengths within a request batch. (3) *Fused block copy.* Block copy operations, issued by the copy-on-write mechanism, may operate on discontinuous blocks. This can lead to numerous invocations of small data movements if we use the `cudaMemcpyAsync` API. To mitigate the overhead, we implement a kernel that batches the copy operations for different blocks into a single kernel launch. 
> PagedAttention çš„å†…å­˜è®¿é—®æ¨¡å¼ç”±å¤šä¸ª GPU kernel ä¼˜åŒ–ï¼ŒåŒ…æ‹¬
> (1) èåˆçš„ reshape å’Œ block write kernel
> åœ¨æ¯ä¸ª Transformer å±‚ä¸­ï¼Œæ–°çš„ KV cache ä¼šè¢«åˆ’åˆ†ä¸ºå—ï¼Œå¹¶ reshape åˆ°é€‚åˆ blockwise è¯»å–çš„å†…å­˜å¸ƒå±€ï¼Œç„¶åå­˜å‚¨/å†™åˆ° block table æŒ‡å®šçš„ä½ç½®
> è¿™ä¸¤ä¸ªæ“ä½œè¢«èåˆä¸ºä¸€ä¸ª kernel ä»¥å‡å°‘ kernel launch å¼€é”€
> (2) èåˆçš„ block read å’Œ attention kernel
> æˆ‘ä»¬é‡‡ç”¨ Faster Transformer ä¸­çš„ attention kernel æ¥æ ¹æ® block table è¯»å– KV cacheï¼Œå¹¶åŒæ—¶æ‰§è¡Œ attention æ“ä½œ
> ä¸ºäº†ä¿è¯åˆå¹¶çš„å†…å­˜è®¿é—®ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ª block çš„è¯»å–åˆ†é…å•ä¸ª GPU warpï¼Œå¦å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºåºåˆ—é•¿åº¦ä¸ä¸€çš„ request batch æ·»åŠ äº†æ”¯æŒ
> (3) èåˆçš„ block copy kernel
> block copy æ“ä½œç”±å†™æ—¶æ‹·è´æœºåˆ¶å‘èµ·ï¼Œè¯¥æ“ä½œå¯èƒ½å¯¹éè¿ç»­çš„ block æ‰§è¡Œï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ `cudaMemcpyAsync` APIï¼Œè¿™ä¼šå¯¼è‡´è°ƒç”¨è®¸å¤šæ¬¡å°æ•°æ®ç§»åŠ¨
> ä¸ºäº†ç¼“è§£è¯¥å¼€é”€ï¼Œæˆ‘ä»¬å®ç°å°†ä¸åŒçš„ block çš„æ‹·è´æ“ä½œè¿›è¡Œæ‰¹å¤„ç†çš„ kernel

## 5.2 Supporting Various Decoding Algorithms 
vLLM implements various decoding algorithms using three key methods: `fork` , `append` , and `free` . The `fork` method creates a new sequence from an existing one. The `append` method appends a new token to the sequence. Finally, the `free` method deletes the sequence. For instance, in parallel sampling, vLLM creates multiple output sequences from the single input sequence using the `fork` method. It then adds new tokens to these sequences in every iteration with `append` , and deletes sequences that meet a stopping condition using `free` . The same strategy is also applied in beam search and prefix sharing by vLLM. We believe future decoding algorithms can also be supported by combining these methods. 
> vLLM ä½¿ç”¨ä¸‰ä¸ªå…³é”®æ–¹æ³•ï¼š`fork/append/free` å®ç°å¤šç§è§£ç ç®—æ³•
> `fork` æ–¹æ³•ä»ç°æœ‰åºåˆ—åˆ›å»ºæ–°åºåˆ—
> `append` æ–¹æ³•å°†æ–° token æ·»åŠ åˆ°åºåˆ—ä¸Š
> `free` æ–¹æ³•åˆ é™¤åºåˆ—
> ä¾‹å¦‚ï¼Œåœ¨å¹¶è¡Œé‡‡æ ·æ—¶ï¼ŒvLLM é¦–å…ˆç”¨ `fork` ä»å•ä¸ªè¾“å…¥åºåˆ—åˆ›å»ºå¤šä¸ªè¾“å‡ºåºåˆ—ï¼Œç„¶ååœ¨æ¯ä¸ªè¿­ä»£ä½¿ç”¨ `append` å°†æ–°çš„ tokens å„è‡ªæ·»åŠ åˆ°è¿™äº›è¾“å‡ºåºåˆ—ä¸Šï¼Œæœ€åä½¿ç”¨ `free` åˆ é™¤æ»¡è¶³åœæ­¢æ¡ä»¶çš„åºåˆ—
> beam search å’Œ prefix sharing ä¹Ÿé‡‡ç”¨åŒæ ·ç­–ç•¥

# 6 Evaluation 
In this section, we evaluate the performance of vLLM under a variety of workloads. 

## 6.1 Experimental Setup 
**Model and server configurations.** We use OPT [62] models with 13B, 66B, and 175B parameters and LLaMA [52] with 13B parameters for our evaluation. 13B and 66B are popular sizes for LLMs as shown in an LLM leader board [38], while 175B is the size of the famous GPT-3 [5] model. For all of our experiments, we use A2 instances with NVIDIA A100 GPUs on Google Cloud Platform. The detailed model sizes and server configurations are shown in Table 1. 
> Model and server configurations
> æ¨¡å‹ä½¿ç”¨ OPT 13/66/175B å’Œ LLaMA 13B

![[vLLM-Table 1.png]]

**Workloads.** We synthesize workloads based on ShareGPT [51] and Alpaca [50] datasets, which contain input and output texts of real LLM services. The ShareGPT dataset is a collection of user-shared conversations with ChatGPT [35]. The Alpaca dataset is an instruction dataset generated by GPT3.5 with self-instruct [57]. We tokenize the datasets and use their input and output lengths to synthesize client requests. As shown in Fig. 11, the ShareGPT dataset has $8.4\times$ longer input prompts and $5.8\times$ longer outputs on average than the Alpaca dataset, with higher variance. Since these datasets do not include timestamps, we generate request arrival times using Poisson distribution with different request rates. 
> Workloads
> workload åŸºäº ShareGPT å’Œ Alpaca æ•°æ®é›†è¿›è¡Œåˆæˆï¼Œè¿™äº›æ•°æ®é›†åŒ…å«äº†çœŸå® LLM æœåŠ¡çš„è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬ï¼Œå…¶ä¸­ShareGPT æ•°æ®é›†æ˜¯ä¸€ç»„ç”¨æˆ·å’Œ ChatGPT çš„å¯¹è¯ï¼ŒAlpaca æ•°æ®é›†æ˜¯ç”± GPT3.5 åœ¨ self-instruct ä¸‹ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®é›†
> æˆ‘ä»¬å°†è¿™äº›æ•°æ®é›† tokenizeï¼Œç„¶åä½¿ç”¨å®ƒä»¬çš„è¾“å…¥å’Œè¾“å‡ºé•¿åº¦æ¥åˆæˆ requests
> å¦‚ Figure 11ï¼Œå¯ä»¥çœ‹åˆ° ShareGPT çš„è¾“å…¥å’Œè¾“å‡ºé•¿åº¦éƒ½é•¿äº Alpacaï¼ŒåŒæ—¶æ–¹å·®æ›´å¤§
> å› ä¸ºæ•°æ®é›†ä¸åŒ…å«æ—¶é—´æˆ³ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒè¯·æ±‚ç‡çš„ Possion åˆ†å¸ƒç”Ÿæˆ request åˆ°è¾¾æ—¶é—´

![[vLLM-Figure11.png]]

**Baseline 1: Faster Transformer.** Faster Transformer [31] is a distributed inference engine highly optimized for latency. 

As Faster Transformer does not have its own scheduler, we implement a custom scheduler with a dynamic batching mechanism similar to the existing serving systems such as Triton [30]. Specifically, we set a maximum batch size $B$ as large as possible for each experiment, according to the GPU memory capacity. The scheduler takes up to $B$ number of earliest arrived requests and sends the batch to Faster Transformer for processing. 

> Baseline 1: Faster Transformer
> Faster Transformer æ˜¯åˆ†å¸ƒå¼çš„æ¨ç†å¼•æ“
> Faster Transformer æ²¡æœ‰è‡ªå·±çš„è°ƒåº¦å™¨ï¼Œæˆ‘ä»¬ä¸ºå…¶å®ç°äº†å¸¦æœ‰åŠ¨æ€ batching æœºåˆ¶çš„è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼Œç±»ä¼¼äºç°å­˜çš„æœåŠ¡ç³»ç»Ÿï¼Œä¾‹å¦‚ Triton
> æ¯æ¬¡è¯•éªŒçš„æœ€å¤§ batch size $B$ éƒ½è®¾å®šä¸ºè¶Šå¤§è¶Šå¥½ï¼Œè°ƒåº¦å™¨æœ€å¤šæ¥å— $B$ ä¸ªæœ€æ—©åˆ°è¾¾çš„ requestsï¼Œç„¶åå°†å…¶ä½œä¸º batch å‘é€ç»™ Faster Transformer

**Baseline 2: Orca.** Orca [60] is a state-of-the-art LLM serving system optimized for throughput. Since Orca is not publicly available for use, we implement our own version of Orca. We assume Orca uses the buddy allocation algorithm to determine the memory address to store KV cache. We implement three versions of Orca based on how much it over-reserves the space for request outputs: 

- **Orca (Oracle).** We assume the system has the knowledge of the lengths of the outputs that will be actually generated for the requests. This shows the upper-bound performance of Orca, which is infeasible to achieve in practice.
- **Orca (Pow2).** We assume the system over-reserves the space for outputs by at most $2\times$  . For example, if the true output length is 25, it reserves 32 positions for outputs.
- **Orca (Max).** We assume the system always reserves the space up to the maximum sequence length of the model, i.e., 2048 tokens. 

> Baseline 2: Orca
> Orca ä¸º SOTA çš„ LLM æœåŠ¡ç³»ç»Ÿ
> æˆ‘ä»¬å®ç°äº†è‡ªå·±çš„ Orcaï¼Œå…¶ä¸­å‡å®šäº† Orca ä½¿ç”¨ buddy åˆ†é…ç®—æ³•æ¥å†³å®šå­˜å‚¨ KV cache çš„å†…å­˜åœ°å€
> åŸºäº Orca æ˜¯å¦‚ä½•ä¸º request çš„è¾“å‡ºé¢„ç•™å†…å­˜ç©ºé—´çš„ï¼Œæˆ‘ä»¬å®ç°äº†ä¸‰ä¸ªç‰ˆæœ¬çš„ Orcaï¼ŒåŒ…æ‹¬ï¼š
> - Orca (Orcale)ï¼Œå‡è®¾ç³»ç»ŸçŸ¥é“è¾“å‡ºåºåˆ—çš„é•¿åº¦ï¼Œè¯¥ç‰ˆæœ¬æ˜¯ Orca çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨å®é™…ä¸­ä¸ä¼šè¾¾åˆ°
> - Orca (Pow2)ï¼Œå‡è®¾ç³»ç»Ÿä¸ºè¾“å‡ºåºåˆ—é¢„ç•™çš„ç©ºé—´ä¸ºå¤§äºè¾“å‡ºåºåˆ—é•¿åº¦çš„æœ€å°çš„2çš„å¹‚æ¬¡ï¼Œä¾‹å¦‚ä¸ºé•¿åº¦ä¸º 25 çš„è¾“å‡ºåºåˆ—é¢„ç•™ 32 ä¸ªä½ç½®
> - Orca (Max)ï¼Œå‡è®¾ç³»ç»Ÿé¢„ç•™çš„ç©ºé—´æ€»æ˜¯ä¿æŒæ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œä¾‹å¦‚ 2048

**Key metrics.** We focus on serving throughput. Specifically, using the workloads with different request rates, we measure normalized latency of the systems, the mean of every requestâ€™s end-to-end latency divided by its output length, as in Orca [60]. A high-throughput serving system should retain low normalized latency against high request rates. For most experiments, we evaluate the systems with 1-hour traces. As an exception, we use 15-minute traces for the OPT-175B model due to the cost limit. 
> Key metrics
> æˆ‘ä»¬èšç„¦äºååé‡
> å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒè¯·æ±‚ç‡ä¸‹çš„å·¥ä½œè´Ÿè½½åº¦é‡ç³»ç»Ÿçš„è§„èŒƒåŒ–å»¶è¿Ÿï¼Œå³æ¯ä¸ªè¯·æ±‚çš„ç«¯åˆ°ç«¯å»¶è¿Ÿé™¤ä»¥å®ƒçš„è¾“å‡ºé•¿åº¦çš„å¹³å‡å€¼
> é«˜ååé‡çš„æœåŠ¡ç³»ç»Ÿåº”è¯¥åœ¨é«˜çš„è¯·æ±‚ç‡ä¸‹ä¿æŒä½çš„è§„èŒƒåŒ–å»¶è¿Ÿ
> å¤§å¤šæ•°è¯•éªŒä½¿ç”¨ä¸€å°æ—¶çš„è·Ÿè¸ªæ•°æ®è¯„ä¼°ï¼ŒOPT-175B ä½¿ç”¨15åˆ†é’Ÿçš„è·Ÿè¸ªæ•°æ®

## 6.2 Basic Sampling
We evaluate the performance of vLLM with basic sampling (one sample per request) on three models and two datasets. The first row of Fig. 12 shows the results on the ShareGPT dataset. The curves illustrate that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes. This can be attributed to the fact that when the request rate surpasses the capacity of the serving system, the queue length continues to grow infinitely and so does the latency of the requests. 
> æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ¨¡å‹å’Œä¸¤ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº† vLLM çš„åŸºç¡€é‡‡æ · (æ¯ä¸ª request ä»…é‡‡æ ·ä¸€ä¸ªæ ·æœ¬)ï¼Œå¦‚ Figure 12 æ‰€ç¤º
> éšç€è¯·æ±‚ç‡é€æ¸å¢å¤§ï¼Œå»¶è¿Ÿä¸€å¼€å§‹é€æ¸æå‡ï¼Œç„¶åçªç„¶çŒ›å¢ï¼Œå…¶åŸå› åœ¨äºå½“è¯·æ±‚ç‡è¶…è¿‡äº†æœåŠ¡ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œæ’é˜Ÿç­‰å¾…å¤„ç†çš„è¯·æ±‚æ•°é‡å°†æ— é™åˆ¶å¢å¤§ï¼Œæ•…å»¶è¿Ÿä¹Ÿå°†æ— é™åˆ¶å¢å¤§

![[vLLM-Figure12.png]]

On the ShareGPT dataset, vLLM can sustain $1.7\times-2.7\times$ higher request rates compared to Orca (Oracle) and $2.7\times-8\times$ compared to Orca (Max), while maintaining similar latencies. This is because vLLMâ€™s Paged Attention can efficiently manage the memory usage and thus enable batching more requests than Orca. For example, as shown in Fig. 13a, for OPT-13B vLLM processes $2.2\times$ more requests at the same time than Orca (Oracle) and $4.3\times$ more requests than Orca (Max). Compared to Faster Transformer, vLLM can sustain upto $22\times$ higher request rates, as Faster Transformer does not utilize a fine-grained scheduling mechanism and inefficiently manages the memory like Orca (Max). 
> åœ¨ ShareGPT æ•°æ®é›†ä¸Šï¼ŒvLLM ç›¸è¾ƒäº Orca (Orcale) å¯ä»¥ç»´æŒ 1.7x-2.7x å€æ›´é«˜çš„è¯·æ±‚ç‡ï¼Œç›¸è¾ƒäº Orca (Max) å¯ä»¥ç»´æŒ 2.7x-8x å€æ›´é«˜çš„è¯·æ±‚ç‡ï¼ŒåŒæ—¶å»¶è¿Ÿæ¥è¿‘
> åŸå› æ˜¯ vLLM çš„ PagedAttention å¯ä»¥é«˜æ•ˆç®¡ç†å†…å­˜ä½¿ç”¨ï¼Œæ•…å¯ä»¥æ‰¹å¤„ç†æ›´å¤šçš„è¯·æ±‚

The second row of Fig. 12 and Fig. 13b shows the results on the Alpaca dataset, which follows a similar trend to the ShareGPT dataset. One exception is Fig. 12 (f), where vLLMâ€™s advantage over Orca (Oracle) and Orca (Pow2) is less pronounced. This is because the model and server configuration for OPT-175B (Table 1) allows for large GPU memory space available to store KV cache, while the Alpaca dataset has short sequences. In this setup, Orca (Oracle) and Orca (Pow2) can also batch a large number of requests despite the inefficiencies in their memory management. As a result, the performance of the systems becomes compute-bound rather than memory-bound. 
> Alpaca ä¸Šçš„ç»“æœå’Œ ShareGPT ä¸Šçš„ç»“æœç±»ä¼¼
> åœ¨ OPT-175B æ—¶ vLLM çš„ä¼˜åŠ¿ç›¸å¯¹ä¸é«˜ï¼ŒåŸå› åœ¨äº OPT-175B çš„æ¨¡å‹å’ŒæœåŠ¡é…ç½® (Table 1) å…è®¸ä½¿ç”¨æ›´å¤§çš„ GPU æ˜¾å­˜ç©ºé—´å­˜å‚¨ KV cacheï¼ŒåŒæ—¶ Alpaca çš„åºåˆ—ä¸»è¦æ˜¯çŸ­åºåˆ—ï¼Œå› æ­¤ Orca (Oracle), Orca (Pow2) å³ä¾¿å†…å­˜ç®¡ç†ä½æ•ˆï¼Œä¹Ÿå¯ä»¥æ‰¹å¤„ç†å¤§é‡è¯·æ±‚ï¼Œå› æ­¤ç³»ç»Ÿçš„æ€§èƒ½æ›´å€¾å‘äº compute-bound è€Œä¸æ˜¯ memory-bound

![[vLLM-Figure13.png]]

## 6.3 Parallel Sampling and Beam Search 
We evaluate the effectiveness of memory sharing in PagedAttention with two popular sampling methods: parallel sampling and beam search. In parallel sampling, all parallel sequences in a request can share the KV cache for the prompt. As shown in the first row of Fig. 14, with a larger number of sequences to sample, vLLM brings more improvement over the Orca baselines. Similarly, the second row of Fig. 14 shows the results for beam search with different beam widths. Since beam search allows for more sharing, vLLM demonstrates even greater performance benefits. The improvement of vLLM over Orca (Oracle) on OPT-13B and the Alpaca dataset goes from $1.3\times$ in basic sampling to $2.3\times$ in beam search with a width of 6. 
> æˆ‘ä»¬ä½¿ç”¨å¹¶è¡Œé‡‡æ ·å’ŒæŸæœç´¢è¯„ä¼° PagedAttention çš„å†…å­˜å…±äº«çš„æœ‰æ•ˆæ€§
> å¹¶è¡Œé‡‡æ ·ä¸­ï¼Œrequest çš„æ‰€æœ‰å¹¶è¡Œåºåˆ—å…±äº« prompt çš„ KV cacheï¼Œå¦‚ Figure 14æ‰€ç¤ºï¼Œå½“å¹¶è¡Œé‡‡æ ·çš„æ•°é‡è¶Šå¤šï¼ŒvLLM ç›¸è¾ƒäº Orca çš„ä¼˜åŠ¿å°±è¶Šå¤§
> æŸæœç´¢çš„ç»“æœä¹Ÿç±»ä¼¼ï¼Œå½“ beam å®½åº¦è¶Šå¤§ï¼ŒvLLM ä¼˜åŠ¿è¶Šå¤§ï¼Œå¹¶ä¸”ç”±äº beam search å¯ä»¥æœ‰æ›´å¤šçš„å…±äº«æœºä¼šï¼ŒvLLM çš„ä¼˜åŠ¿ä¹Ÿæ›´åŠ æ˜¾è‘—

![[vLLM-Figure14.png]]

Fig. 15 plots the amount of memory saving, computed by the number of blocks we saved by sharing divided by the number of total blocks without sharing. We show $6.1\%-9.8\%$ memory saving on parallel sampling and $37.6\%\textrm{-}55.2\%$ on beam search. In the same experiments with the ShareGPT dataset, we saw $16.2\%\textrm{-}30.5\%$ memory saving on parallel sampling and $44.3\%\textrm{-}66.3\%$ on beam search. 
> vLLM å¯¹å†…å­˜èŠ‚çº¦çš„æ¯”ä¾‹å¦‚ Figure 15 æ‰€ç¤ºï¼Œæ¯”ä¾‹é€šè¿‡å°†å…±äº«çš„å—çš„æ•°é‡é™¤ä»¥æ²¡æœ‰å…±äº«çš„å—çš„æ•°é‡å¾—åˆ°
> Alpaca æ•°æ®é›†ä¸Šï¼Œåœ¨å¹¶è¡Œé‡‡æ ·æ—¶ï¼Œå†…å­˜çš„èŠ‚çº¦ç¨‹åº¦è¾¾åˆ° 6.1%-9.8%ï¼Œbeam search æ—¶ï¼Œå†…å­˜çš„èŠ‚çº¦ç¨‹åº¦è¾¾åˆ° 37.6%-55.2%

![[vLLM-Figure15.png]]

## 6.4 Shared prefix 
We explore the effectiveness of vLLM for the case a prefix is shared among different input prompts, as illustrated in Fig. 10. For the model, we use LLaMA-13B [52], which is multilingual. For the workload, we use the WMT16 [4] Englishto-German translation dataset and synthesize two prefixes that include an instruction and a few translation examples. The first prefix includes a single example (i.e., one-shot) while the other prefix includes 5 examples (i.e., few-shot). As shown in Fig. 16 (a), vLLM achieves $1.67\times$ higher throughput than Orca (Oracle) when the one-shot prefix is shared. Furthermore, when more examples are shared (Fig. 16 (b)), vLLM achieves $3.58\times$ higher throughput than Orca (Oracle). 
> æˆ‘ä»¬ä½¿ç”¨ LLaMA-13B æ¢ç©¶ vLLM å¯¹äºä¸åŒè¾“å…¥ prompt å…±äº«å‰ç¼€çš„æ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ WMT16 English-German ç¿»è¯‘æ•°æ®é›†ï¼Œä¸ºæ•°æ®åˆæˆäº†ä¸¤ä¸ªå‰ç¼€ï¼Œæ¯ä¸ªå‰ç¼€åŒ…æ‹¬ä¸€ä¸ªæŒ‡ä»¤å’Œä¸€éƒ¨åˆ†ç¿»è¯‘ç¤ºä¾‹æ ·æœ¬ï¼Œä½œä¸º workload
> ç¬¬ä¸€ä¸ªå‰ç¼€æ˜¯ one-shotï¼Œä»…åŒ…å«å•ä¸ªç¤ºä¾‹æ ·æœ¬ï¼Œç¬¬äºŒä¸ªå‰ç¼€ä¸º few-shotï¼ŒåŒ…å«5ä¸ªç¤ºä¾‹æ ·æœ¬
> ç»“æœè§ Figure 16ï¼Œå¯ä»¥çœ‹åˆ°å‰ç¼€è¶Šé•¿ï¼Œæ•ˆæœè¶Šæ˜æ˜¾

![[vLLM-Figure16.png]]

## 6.5 Chatbot 
A chatbot [8 , 19 , 35] is one of the most important applications of LLMs. To implement a chatbot, we let the model generate a response by concatenating the chatting history and the last user query into a prompt. We synthesize the chatting history and user query using the ShareGPT dataset. Due to the limited context length of the OPT-13B model, we cut the prompt to the last 1024 tokens and let the model generate at most 1024 tokens. We do not store the KV cache between different conversation rounds as doing this would occupy the space for other requests between the conversation rounds. 
> è¦å®ç° chatbotï¼Œæˆ‘ä»¬è¦è®©æ¨¡å‹å°†èŠå¤©å†å²å’Œç”¨æˆ·æŸ¥è¯¢æ‹¼æ¥ä¸º prompt
> æˆ‘ä»¬ä½¿ç”¨ ShareGPT æ•°æ®é›†åˆæˆèŠå¤©å†å²å’Œç”¨æˆ·æŸ¥è¯¢
> OPT-13B çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æœ‰é™ï¼Œæ•…æˆ‘ä»¬å°† prompt é•¿åº¦è®¾å®šä¸ºæœ€åçš„ 1024 tokensï¼Œå¹¶ä¸”è®©æ¨¡å‹æœ€å¤šç”Ÿæˆ 1024 tokens
> æˆ‘ä»¬ä¸ä¿å­˜å¯¹è¯è½®æ¬¡ä¹‹é—´çš„ KV cacheï¼Œé˜²æ­¢åœ¨å¯¹è¯è½®æ¬¡ä¹‹é—´å ç”¨å…¶ä»–è¯·æ±‚çš„ç©ºé—´

Fig. 17 shows that vLLM can sustain $2\times$ higher request rates compared to the three Orca baselines. Since the ShareGPT dataset contains many long conversations, the input prompts for most requests have 1024 tokens. Due to the buddy allocation algorithm, the Orca baselines reserve the space for 1024 tokens for the request outputs, regardless of how they predict the output lengths. For this reason, the three Orca baselines behave similarly. In contrast, vLLM can effectively handle the long prompts, as Paged Attention resolves the problem of memory fragmentation and reservation. 
> vLLM ç›¸è¾ƒäº Orca å¯ä»¥ç»´æŒ 2x ä»¥ä¸Šçš„è¯·æ±‚ç‡
> ShareGPT åŒ…å«è®¸å¤šé•¿å¯¹è¯ï¼Œå› æ­¤è®¸å¤šè¯·æ±‚ prompt éƒ½è¾¾åˆ° 1024 tokens
> Orca çš„ buddy åˆ†é…ç®—æ³•æ€»æ˜¯ä¸ºè¯·æ±‚è¾“å‡ºé¢„ç•™ 1024 tokens çš„ç©ºé—´ï¼Œæ— è®ºå®é™…è¾“å‡ºå¤šé•¿ï¼Œå› æ­¤ä¸‰ç§ Orca å®ç°çš„è¡¨ç°éƒ½ç±»ä¼¼
> è€Œ vLLM è§£å†³äº†å†…å­˜ç¢ç‰‡å’Œé¢„ç•™çš„é—®é¢˜ï¼Œæ•…å¯ä»¥é«˜æ•ˆå¤„ç†é•¿çš„ prompt

![[vLLM-Figure17.png]]

# 7 Ablation Studies 
In this section, we study various aspects of vLLM and evaluate the design choices we make with ablation experiments. 

## 7.1 Kernel Microbenchmark 
The dynamic block mapping in Paged Attention affects the performance of the GPU operations involving the stored KV cache, i.e., block read/writes and attention. Compared to the existing systems, our GPU kernels (Â§5) involve extra overheads of accessing the block table, executing extra branches, and handling variable sequence lengths. As shown in Fig. 18a, this leads to $20{-}26\%$ higher attention kernel latency, compared to the highly-optimized Faster Transformer implementation. We believe the overhead is small as it only affects the attention operator but not the other operators in the model, such as Linear. Despite the overhead, Paged Attention makes vLLM significantly outperform Faster Transformer in end-to-end performance (Â§6). 
> PagedAttention çš„åŠ¨æ€ block æ˜ å°„ä¼šå½±å“æ¶‰åŠåˆ° KV cache çš„ GPU æ“ä½œçš„è¡¨ç°ï¼Œå³ block è¯»å†™å’Œ attention è®¡ç®—
> ç›¸è¾ƒäºç°æœ‰ç³»ç»Ÿï¼Œæˆ‘ä»¬çš„ GPU kernel åŒ…å«äº†è®¿é—® block tableã€æ‰§è¡Œé¢å¤–åˆ†æ”¯ã€å¤„ç†å¯å˜åºåˆ—é•¿åº¦çš„é¢å¤–å¼€é”€ï¼Œå¦‚ Figure 18a æ‰€ç¤ºï¼Œè¿™å°†å¯¼è‡´ attention kernel æ¯” Faster Transformer å®ç°å¤šå‡º 20-26% çš„å»¶è¿Ÿ
> ä½† vLLM çš„ç«¯åˆ°ç«¯è¡¨ç°ä»ç„¶æ˜¾è‘—é«˜äº Faster Transformer

![[vLLM-Figure18.png]]

## 7.2 Impact of Block Size 
The choice of block size can have a substantial impact on the performance of vLLM. If the block size is too small, vLLM may not fully utilize the GPUâ€™s parallelism for reading and processing KV cache. If the block size is too large, internal fragmentation increases and the probability of sharing decreases. 
> KV block å¤ªå°æ—¶ï¼ŒvLLM å¯èƒ½æ— æ³•å®Œå…¨åˆ©ç”¨ GPU çš„å¹¶è¡Œæ€§è´¨ä¼˜åŒ– KV cache çš„è¯»å–å’Œå¤„ç†ï¼ŒKV block å¤ªå¤§æ—¶ï¼Œåˆ™ä¼šå¯¼è‡´æ›´å¤§çš„å†…éƒ¨ç¢ç‰‡ï¼Œä¸”èƒ½å…±äº«çš„æ¦‚ç‡é™ä½

In Fig. 18b, we evaluate the performance of vLLM with different block sizes, using the ShareGPT and Alpaca traces with basic sampling under fixed request rates. In the ShareGPT trace, block sizes from 16 to 128 lead to the best performance. In the Alpaca trace, while the block size 16 and 32 work well, larger block sizes significantly degrade the performance since the sequences become shorter than the block sizes. In practice, we find that the block size 16 is large enough to efficiently utilize the GPU and small enough to avoid significant internal fragmentation in most workloads. Accordingly, vLLM sets its default block size as 16. 
> Figure 18b è¯„ä¼°äº† vLLM åœ¨ä¸åŒ block size ä¸‹çš„è¡¨ç° (basic sampling, fixed request rate)
> ShareGPT çš„ block size æœ€å¥½åœ¨ 16-128ï¼ŒAlpaca çš„ block size è¿‡å¤§æ—¶è¡¨ç°æ˜¾è‘—é™ä½ï¼Œå› ä¸º block size è¶…è¿‡äº†åºåˆ—é•¿åº¦
> å®è·µä¸­ï¼Œblock size = 16 çš„æ•ˆæœæœ€ä¼˜ï¼Œå¯ä»¥åœ¨åˆ©ç”¨ GPU å¹¶è¡Œæ€§çš„åŒæ—¶é¿å…å¤§å¤šæ•° workload ä¸­çš„è¿‡å¤§å†…éƒ¨ç¢ç‰‡

## 7.3 Comparing Recomputation and Swapping 
vLLM supports both recomputation and swapping as its recovery mechanisms. To understand the tradeoffs between the two methods, we evaluate their end-to-end performance and micro benchmark their overheads, as presented in Fig. 19. Our results reveal that swapping incurs excessive overhead with small block sizes. This is because small block sizes often result in numerous small data transfers between CPU and GPU, which limits the effective PCIe bandwidth. In contrast, the overhead of re computation remains constant across different block sizes, as re computation does not utilize the KV blocks. Thus, recomputation is more efficient when the block size is small, while swapping is more efficient when the block size is large, though recomputation overhead is never higher than $20\%$ of swappingâ€™s latency. For medium block sizes from 16 to 64, the two methods exhibit comparable end-to-end performance. 
> vLLM æ”¯æŒçš„æŠ¢å æ¢å¤æœºåˆ¶æœ‰é‡è®¡ç®—å’Œäº¤æ¢
> æˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ç§æ–¹æ³•çš„ç«¯åˆ°ç«¯è¡¨ç°ï¼Œå¹¶ä¸”æµ‹è¯•äº†å®ƒä»¬çš„å¼€é”€ï¼Œå¦‚ Figure 19 æ‰€ç¤º
> äº¤æ¢æ–¹æ³•åœ¨ block size è¾ƒå°æ—¶ä¼šæ˜¾è‘—å¼€é”€ï¼Œå› ä¸ºå°çš„ block size å®¹æ˜“å¯¼è‡´ CPU å’Œ GPU ä¹‹é—´æœ‰è¿‡å¤šçš„å°æ•°æ®ä¼ è¾“ï¼Œé™åˆ¶äº†æœ‰æ•ˆ PCIe å¸¦å®½
> é‡è®¡ç®—çš„å¼€é”€éš block size å˜åŒ–åŸºæœ¬ä¸å˜ï¼Œå› ä¸ºé‡è®¡ç®—ä¸æ¶‰åŠæ•°æ®ä¼ è¾“ï¼Œä¸ä¼šä½¿ç”¨ KV blocks
> å› æ­¤ block size è¾ƒå°æ—¶é‡è®¡ç®—è¾ƒé«˜æ•ˆï¼Œblock size è¾ƒå¤§æ—¶äº¤æ¢æ–¹æ³•è¾ƒé«˜æ•ˆï¼Œblock size ä¸ºä¸­ç­‰å¤§å°æ—¶ï¼ŒäºŒè€…çš„ç«¯åˆ°ç«¯è¡¨ç°å¯æ¯”

![[vLLM-Figure19.png]]

# 8 Discussion 
**Applying the virtual memory and paging technique to other GPU workloads.** The idea of virtual memory and paging is effective for managing the KV cache in LLM serving because the workload requires dynamic memory allocation (since the output length is not known a priori) and its performance is bound by the GPU memory capacity. However, this does not generally hold for every GPU workload. For example, in DNN training, the tensor shapes are typically static, and thus memory allocation can be optimized ahead of time. For another example, in serving DNNs that are not LLMs, an increase in memory efficiency may not result in any performance improvement since the performance is primarily compute-bound. In such scenarios, introducing the vLLMâ€™s techniques may rather degrade the performance due to the extra overhead of memory indirection and non-contiguous block memory. However, we would be excited to see vLLMâ€™s techniques being applied to other workloads with similar properties to LLM serving. 
> Applying the virtual memory and paging techinque to other GPU workloads
> è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæœºåˆ¶åœ¨ç®¡ç† KV cache æ—¶é«˜æ•ˆçš„åŸå› åœ¨äº LLM çš„ workload éœ€è¦åŠ¨æ€å†…å­˜åˆ†é… (å› ä¸ºè¾“å‡ºé•¿åº¦ä¸èƒ½æå‰é¢„çŸ¥)ï¼Œæ•…æ€§èƒ½å— GPU æ˜¾å­˜å®¹é‡é™åˆ¶
> å¯¹äºå…¶ä»–çš„ GPU workload è¿™ä¸€ç‚¹ä¸ä¸€å®šæˆç«‹
> ä¾‹å¦‚è®­ç»ƒ DNN æ—¶ï¼Œå¼ é‡çš„å½¢çŠ¶ä¸€èˆ¬æ˜¯é™æ€çš„ï¼Œå› æ­¤å¯ä»¥æå‰ä¼˜åŒ–å†…å­˜åˆ†é…ï¼›åŒæ—¶å¯¹äºä¸æ˜¯ LLM çš„ DNN æ¥è¯´ï¼Œå†…å­˜æ•ˆç‡çš„æå‡ä¸ä¸€å®šä¼šè®©æ€§èƒ½æå‡ï¼Œå› ä¸ºæ€§èƒ½ä¹Ÿå¯èƒ½æ˜¯ compute-bound
> å¯¹äºè¿™æ ·çš„åœºæ™¯ï¼ŒvLLM æŠ€æœ¯å¯èƒ½åè€Œä¼šé™ä½æ€§èƒ½ï¼Œå› ä¸ºé—´æ¥å†…å­˜å’Œä¸è¿ç»­çš„å—å¼å†…å­˜ä¼šå¼•å…¥é¢å¤–å¼€é”€

**LLM-specific optimizations in applying virtual memory and paging.** vLLM re-interprets and augments the idea of virtual memory and paging by leveraging the application specific semantics. One example is vLLMâ€™s all-or-nothing swap-out policy, which exploits the fact that processing a request requires all of its corresponding token states to be stored in GPU memory. Another example is the recomputation method to recover the evicted blocks, which is not feasible in OS. Besides, vLLM mitigates the overhead of memory in direction in paging by fusing the GPU kernels for memory access operations with those for other operations such as attention.
> LLM-specific optimizations in applying virtual memory and paging
> vLLM åœ¨é’ˆå¯¹åº”ç”¨ç¨‹åºçš„è¯­ä¹‰ä¸Šé‡æ–°è§£é‡Šå¹¶å¼ºåŒ–äº†è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µçš„æ€æƒ³
> ä¸€ä¸ªä¾‹å­å°±æ˜¯ vLLM çš„å…¨æœ‰æˆ–å…¨æ— çš„æ¢å‡ºç­–ç•¥ï¼Œè¿™åŸºäºçš„äº‹å®æ˜¯å¤„ç†ä¸€ä¸ªè¯·æ±‚éœ€è¦å®ƒæ‰€æœ‰å¯¹åº”çš„ tokens çš„çŠ¶æ€éƒ½è¢«å­˜å‚¨åœ¨ GPU æ˜¾å­˜ä¸­
> å¦ä¸€ä¸ªä¾‹å­æ˜¯å¯ä»¥ç”¨é‡è®¡ç®—æ¢å¤è¢«é©±é€çš„æ•°æ®å—ï¼Œè¿™åœ¨ OS ä¸­æ˜¯ä¸å¯è¡Œçš„
> æ­¤å¤–ï¼ŒvLLM é€šè¿‡èåˆäº†æ‰§è¡Œå†…å­˜è®¿é—®æ“ä½œçš„ kernel å’Œæ‰§è¡Œå…¶ä»–æ“ä½œä¾‹å¦‚ attention çš„ kernel æ¥ç¼“è§£äº†å†…å­˜é—´æ¥è®¿é—®çš„å¼€é”€

# 9 Related Work 
**General model serving systems.** Model serving has been an active area of research in recent years, with numerous systems proposed to tackle diverse aspects of deep learning model deployment. Clipper [11], TensorFlow Serving [33], Nexus [45], InferLine [10], and Clockwork [20] are some earlier general model serving systems. They study batching, caching, placement, and scheduling for serving single or multiple models. More recently, DVABatch [12] introduces multi-entry multi-exit batching. REEF [21] and Shepherd [61] propose preemption for serving. AlpaServe [28] utilizes model parallelism for statistical multiplexing. However, these general systems fail to take into account the autoregressive property and token state of LLM inference, resulting in missed opportunities for optimization. 
> é€šç”¨æ¨¡å‹æœåŠ¡ç³»ç»Ÿ
> æ¨¡å‹æœåŠ¡è¿‘å¹´æ¥ä¸€ç›´æ˜¯ç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸï¼Œè®¸å¤šç³»ç»Ÿè¢«æå‡ºä»¥è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²çš„å„ç§æ–¹é¢é—®é¢˜
> Clipper [11]ã€TensorFlow Serving [33]ã€Nexus [45]ã€InferLine [10] å’ŒClockwork [20] æ˜¯ä¸€äº›è¾ƒæ—©çš„é€šç”¨æ¨¡å‹æœåŠ¡ç³»ç»Ÿã€‚å®ƒä»¬ç ”ç©¶äº†æ‰¹é‡å¤„ç†ã€ç¼“å­˜ã€éƒ¨ç½²ä½ç½®å’Œè°ƒåº¦ç­‰é—®é¢˜ï¼Œç”¨äºæœåŠ¡å•ä¸ªæˆ–å¤šä¸ªæ¨¡å‹
> æœ€è¿‘ï¼ŒDVABatch [12] å¼•å…¥äº†å¤šå…¥å£å¤šå‡ºå£æ‰¹é‡å¤„ç†ã€‚REEF [21] å’ŒShepherd [61] æå‡ºäº†é¢„è°ƒåº¦æœåŠ¡çš„æ–¹æ³•ã€‚AlpaServe [28] åˆ©ç”¨äº†æ¨¡å‹å¹¶è¡Œæ€§æ¥è¿›è¡Œç»Ÿè®¡å¤ç”¨
> ç„¶è€Œï¼Œè¿™äº›é€šç”¨ç³»ç»Ÿæœªèƒ½è€ƒè™‘åˆ°LLMæ¨ç†ä¸­çš„è‡ªå›å½’ç‰¹æ€§å’ŒtokençŠ¶æ€ï¼Œå¯¼è‡´é”™å¤±äº†ä¼˜åŒ–çš„æœºä¼šã€‚

**Specialized serving systems for transformers.** Due to the significance of the transformer architecture, numerous specialized serving systems for it have been developed. These systems utilize GPU kernel optimization s [1, 29, 31, 56], advanced batching mechanisms [14 , 60], model parallelism [1 , 41 , 60], and parameter sharing [64] for efficient serving. Among them, Orca [60] is most relevant to our approach. 
>  é’ˆå¯¹ transformers çš„æœåŠ¡ç³»ç»Ÿ
>  æœ‰è®¸å¤šä¸“é—¨çš„é’ˆå¯¹ transformer æ¶æ„çš„æœåŠ¡ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿåˆ©ç”¨äº† GPU å†…æ ¸ä¼˜åŒ–[1, 29, 31, 56]ã€é«˜çº§æ‰¹é‡å¤„ç†æœºåˆ¶[14, 60]ã€æ¨¡å‹å¹¶è¡Œæ€§[1, 41, 60]ä»¥åŠå‚æ•°å…±äº«[64]ï¼Œä»¥å®ç°é«˜æ•ˆçš„æœåŠ¡ã€‚å…¶ä¸­ï¼ŒOrca [60] æœ€æ¥è¿‘æˆ‘ä»¬çš„æ–¹æ³•ã€‚

**Comparison to Orca.** The iteration-level scheduling in Orca [60] and Paged Attention in vLLM are complementary techniques: While both systems aim to increase the GPU utilization and hence the throughput of LLM serving, Orca achieves it by scheduling and interleaving the requests so that more requests can be processed in parallel, while vLLM is doing so by increasing memory utilization so that the working sets of more requests fit into memory. By reducing memory fragmentation and enabling sharing, vLLM runs more requests in a batch in parallel and achieves a $2â€“4\times$ speedup compared to Orca. Indeed, the fine-grained scheduling and interleaving of the requests like in Orca makes memory management more challenging, making the techniques proposed in vLLM even more crucial. 
> ä¸ Orca çš„å¯¹æ¯”
> Orca [60]ä¸­çš„è¿­ä»£çº§åˆ«è°ƒåº¦å’Œ vLLM ä¸­çš„åˆ†é¡µæ³¨æ„åŠ›æœºåˆ¶æ˜¯äº’è¡¥çš„æŠ€æœ¯ï¼šè™½ç„¶ä¸¤ä¸ªç³»ç»Ÿéƒ½æ—¨åœ¨æé«˜ GPU åˆ©ç”¨ç‡å’Œ LLM æœåŠ¡çš„ååé‡ï¼Œä½† Orca é€šè¿‡è°ƒåº¦å’Œäº¤é”™è¯·æ±‚ä½¿å¾—æ›´å¤šè¯·æ±‚å¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œè€Œ vLLM åˆ™æ˜¯é€šè¿‡å¢åŠ å†…å­˜åˆ©ç”¨ç‡ä½¿æ›´å¤šè¯·æ±‚çš„å·¥ä½œé›†å¯ä»¥å®¹çº³åœ¨æ˜¾å­˜ä¸­
> é€šè¿‡å‡å°‘å†…å­˜ç¢ç‰‡å¹¶å¯ç”¨å…±äº«ï¼ŒvLLM èƒ½å¤Ÿå¹¶è¡Œè¿è¡Œæ›´å¤šçš„è¯·æ±‚ï¼Œå¹¶ç›¸è¾ƒäº Orca å®ç°äº†2-4å€çš„é€Ÿåº¦æå‡
> å®é™…ä¸Šï¼Œåƒ Orca é‚£æ ·å¯¹è¯·æ±‚è¿›è¡Œç»†ç²’åº¦è°ƒåº¦å’Œäº¤é”™å¤„ç†ä¼šä½¿å†…å­˜ç®¡ç†æ›´åŠ å¤æ‚ï¼Œè¿™ä¹Ÿä½¿å¾— vLLM ä¸­æå‡ºçš„æŠ€æœ¯æ›´ä¸ºå…³é”®

**Memory optimizations.** The widening gap between the compute capability and memory capacity of accelerators has caused memory to become a bottleneck for both training and inference. Swapping [23 , 42 , 55], re computation [7 , 24] and their combination [40] have been utilized to reduce the peak memory of training. Notably, FlexGen [46] studies how to swap weights and token states for LLM inference with limited GPU memory, but it does not target the online serving settings. OLLA [48] optimizes the lifetime and location of tensors to reduce fragmentation, but it does not do finegrained block-level management or online serving. FlashAttention [13] applies tiling and kernel optimization s to reduce the peak memory of attention computation and reduce I/O costs. This paper introduces a new idea of block-level memory management in the context of online serving. 
> å†…å­˜ä¼˜åŒ–
> åŠ é€Ÿè®¾å¤‡çš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜å®¹é‡ä¹‹é—´çš„å·®è·è¶Šæ¥è¶Šå¤§ï¼Œå¯¼è‡´å†…å­˜æˆä¸ºäº†è®­ç»ƒå’Œæ¨ç†çš„ç“¶é¢ˆï¼Œäº¤æ¢[23, 42, 55]ã€é‡è®¡ç®—[7, 24]åŠäºŒè€…çš„ç»„åˆ[40]å·²è¢«ç”¨æ¥å‡å°‘è®­ç»ƒçš„å³°å€¼å†…å­˜éœ€æ±‚ã€‚
> å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFlexGen [46]ç ”ç©¶äº†å¦‚ä½•åœ¨æœ‰é™çš„ GPU å†…å­˜ä¸‹é€šè¿‡äº¤æ¢æƒé‡å’Œ token çŠ¶æ€æ¥è¿›è¡Œ LLM æ¨ç†ï¼Œä½†å®ƒå¹¶ä¸é’ˆå¯¹åœ¨çº¿æœåŠ¡åœºæ™¯ã€‚OLLA [48]ä¼˜åŒ–äº†å¼ é‡çš„ç”Ÿå‘½å‘¨æœŸå’Œä½ç½®ï¼Œä»¥å‡å°‘ç¢ç‰‡åŒ–ï¼Œä½†å¹¶æ²¡æœ‰è¿›è¡Œç»†ç²’åº¦çš„å—çº§ç®¡ç†å’Œåœ¨çº¿æœåŠ¡ã€‚FlashAttention [13]é€šè¿‡åˆ†å—å’Œå†…æ ¸ä¼˜åŒ–æ¥å‡å°‘æ³¨æ„åŠ›è®¡ç®—çš„å³°å€¼å†…å­˜å’Œ I/O æˆæœ¬
> æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºäºå—çº§åˆ«çš„å†…å­˜ç®¡ç†æ€æƒ³ï¼Œé€‚ç”¨äºåœ¨çº¿æœåŠ¡åœºæ™¯ã€‚

# 10 Conclusion 
This paper proposes Paged Attention, a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory, and presents vLLM, a high-throughput LLM serving system with efficient memory management enabled by Paged Attention. Inspired by operating systems, we demonstrate how established techniques, such as virtual memory and copy-on-write, can be adapted to efficiently manage KV cache and handle various decoding algorithms in LLM serving. Our experiments show that vLLM achieves $2â€“4\times$ throughput improvements over the state-of-the-art systems. 
> æœ¬æ–‡æå‡ºäº† PagedAttentionï¼Œè¯¥ç®—æ³•å…è®¸ attention keys å’Œ values è¢«å­˜å‚¨åœ¨ä¸è¿ç»­çš„åˆ†é¡µå†…å­˜
> æœ¬æ–‡å±•ç¤ºäº† vLLMï¼Œä¸€ä¸ªé«˜ååçš„ LLM æœåŠ¡ç³»ç»Ÿï¼Œä½¿ç”¨ PagedAttention è¿›è¡Œé«˜æ•ˆå†…å­˜ç®¡ç†
> æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åº”ç”¨æˆç†Ÿçš„æŠ€æœ¯ï¼Œä¾‹å¦‚è™šæ‹Ÿå†…å­˜å’Œå†™æ—¶æ‹·è´ï¼Œæ¥é«˜æ•ˆç®¡ç† KV cache å¹¶å¤„ç† LLM æœåŠ¡ä¸­çš„å¤šç§è§£ç ç®—æ³•
> è¯•éªŒæ ‡è¯†äº† vLLM ç›¸è¾ƒäº SOTA ç³»ç»Ÿå®ç°äº† 2-4x çš„ååæå‡