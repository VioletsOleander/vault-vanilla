# Abstract
 As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. ( 语言建模从统计语言模型发展至神经语言模型 ) 
 
 Recently, pre-trained language models (PLMs 预训练语言模型 ) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect ( 规模化效应 ) by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level ( 参数规模超过特定级别 ), these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT).
 
 To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters 百亿以上的参数量 ). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. 

we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. ( 预训练、适应微调、能力评估 )
# 1 Introduction
Technically, language modeling (LM 语言建模) is one of the major approaches to advancing language intelligence of machines. In general, LM aims to model the generative likelihood of word sequences, ( 建模词序列的生成式似然 ) so as to predict the probabilities of future (or missing) tokens. ( 预测 token 的概率 ) The research of LM has received extensive attention in the literature, which can be divided into four major development stages:
- Statistical language models (SLM 统计语言模型). 
    SLMs [6–9] are developed based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, ( 基于 Markov 假设构建词预测模型 ) e.g., predicting the next word based on the most recent context. ( 最近的上下文 ) The SLMs with a fixed context length n ( 固定的上下文长度 n ) are also called n-gram language models, e.g., bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval (IR) [10, 11] and natural language processing (NLP) [12–14]. However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated. Thus, specially designed smoothing strategies ( 平滑策略 ) such as backoff estimation [15] and Good–Turing estimation [16] have been introduced to alleviate the data sparsity problem.
- Neural language models (NLM 神经语言模型). 
    NLMs [1, 17, 18] characterize the probability of word sequences by neural networks, e.g., multi-layer perceptron (MLP) and recurrent neural networks (RNNs). As a remarkable contribution, the work in [1] introduced the concept of distributed representation of words (词的分布式表示) and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). ( 条件于分布式词向量构建词预测函数 ) By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for various NLP tasks [2]. Furthermore, word2vec [19, 20] was proposed to build a simplified shallow neural network for learning distributed word representations, ( 构建浅的 NN 学习分布式词表示 ) which were demonstrated to be very effective across a variety of NLP tasks. These studies have initiated the use of language models for representation learning (表示学习) (beyond word sequence modeling), having an important impact on the field of NLP
- Pre-trained language models (PLM). 
    As an early attempt, ELMo [21] was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM (biLSTM) network (instead of learning fixed word representations) and then fine-tuning the biLSTM network according to specific downstream tasks. Furthermore, based on the highly parallelizable Transformer architecture [22] with self-attention mechanisms, BERT [23] was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora. These pre-trained context-aware word representations ( 预训练的上下文感知的词表示 ) are very effective as general-purpose semantic features ( 作为通用目的的语义特征 ), which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the “pre-training and fine-tuning” learning paradigm ( 预训练并微调的范式 ). Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures [24, 25] (e.g., GPT-2 [26] and BART [24]) or improved pre-training strategies [27–29]. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.
- Large language models (LLM). 
    Researchers find that scaling PLM (e.g., scaling model size or data size) often leads to an improved model capacity on downstream tasks (i.e., following the scaling law [30]). A number of studies have explored the performance limit by training an ever larger PLM (e.g., the 175B-parameter GPT-3 and the 540B parameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks) ( 规模化主要体现在模型大小，而模型结构和预训练任务是相似的 ), these large-sized PLMs display different behaviors from smaller PLMs (e.g., 330M-parameter BERT and 1.5B parameter GPT-2) and show surprising abilities (called emergent abilities [31]) in solving a series of complex tasks. For example, GPT-3 can solve few-shot tasks through in-context learning, whereas GPT-2 cannot do well. Thus, the research community coins the term “large language models (LLM)” for these large-sized PLMs [32–35], which attract increasing research attention (See Figure 1). A remarkable application of LLMs is ChatGPT that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. 

As discussed before, language model is not a new technical concept specially for LLMs, but has evolved with the advance of artificial intelligence over the decades. Early language models mainly aim to model and generate text data, while latest language models (e.g., GPT-4) focus on complex task solving. From language modeling to task solving, ( 从语言建模到任务解决 ) it is an important leap in scientific thinking, which is the key to understand the development of language models in the research history. From the perspective of task solving, the four generations of language models have exhibited different levels of model capacities. In Figure 2, we describe the evolution process of language models in terms of the task solving capacity. At first, statistical language models mainly assisted in some specific tasks (e.g., retrieval or speech tasks), in which the predicted or estimated probabilities can enhance the performance of task-specific approaches. Subsequently, neural language models focused on learning task-agnostic representations (e.g., features), aiming to reduce the efforts for human feature engineering. Furthermore, pre-trained language models learned context-aware representations that can be optimized according to downstream tasks. For the latest generation of language model, LLMs are enhanced by exploring the scaling effect on model capacity, which can be considered as general-purpose task solvers. To summarize, in the evolution process, the task scope that can be solved by language models have been greatly extended, and the task performance attained by language models have been significantly enhanced.

In the existing literature, PLMs have been widely discussed and surveyed [36–39], while LLMs are seldom reviewed in a systematic way. To motivate our survey, we first highlight three major differences between LLMs and PLMs. ( PLMs LLMs 之间的三大差异 ) First, LLMs display some surprising emergent abilities that may not be observed in previous smaller PLMs. ( 涌现能力 ) These abilities are key to the performance of language models on complex tasks. Second, LLMs would revolutionize the way that humans develop and use AI algorithms. Unlike small PLMs, the major approach to accessing LLMs is through the prompting interface (e.g., GPT-4 API). Humans have to understand how LLMs work and format their tasks in a way that LLMs can follow. Third, the development of LLMs no longer draws a clear distinction between research and engineering. The training of LLMs requires extensive practical experiences in large-scale data processing and distributed parallel training. To develop capable LLMs, researchers have to solve complicated engineering issues, working with engineers or being engineers.

The research areas of AI are being revolutionized by the rapid progress of LLMs. In the field of NLP, LLMs can serve as a general-purpose language task solver (to some extent), and the research paradigm has been shifting towards the use of LLMs. In the field of IR, traditional search engines are challenged by the new information seeking way through AI chatbots (i.e., ChatGPT), and New Bing3 presents an initial attempt that enhances the search results based on LLMs. In the field of CV, the researchers try to develop ChatGPT-like vision-language models that can better serve multimodal dialogues [42–45], and GPT-4 [46] has supported multimodal input by integrating the visual information.

Despite the progress and impact, the underlying principles of LLMs are still not well explored. Firstly, it is mysterious why emergent abilities occur in LLMs, instead of smaller PLMs. As a more general issue, there lacks a deep, detailed investigation of the key factors that contribute to the superior abilities of LLMs. It is important to study when and how LLMs obtain such abilities [47]. Although there are some meaningful discussions about this problem [31, 47], more principled investigations are needed to uncover the “secrets“ of LLMs. Secondly, it is difficult for the research community to train capable LLMs. Due to the huge demand of computation resources, it is very costly to carry out repetitive, ablating studies for investigating the effect of various strategies for training LLMs. Indeed, LLMs are mainly trained by industry, where many important training details (e.g., data collection and cleaning) are not revealed to the public. Thirdly, it is challenging to align LLMs with human values or preferences. Despite the capacities, LLMs are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches to eliminating the potential risk of the use of LLMs [46].

In order to provide a basic understanding of LLMs, this survey conducts a literature review of the recent advances in LLMs from four major aspects, including pre-training (how to pretrain a capable LLM), adaptation (how to effectively adapt pre-trained LLMs for better use), utilization (how to use LLMs for solving various downstream tasks) and capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings). We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs. For this survey, we also create a GitHub project website by collecting the supporting resources for LLMs, at the link https://github. com/RUCAIBox/LLMSurvey. These papers either discuss PLMs or some specific (or general) aspects of LLMs. Compared with them, we focus on the techniques and methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects of LLMs.

The remainder of this survey is organized as follows: Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization of available resources for developing LLMs in Section 3. Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation, utilization, and capacity evaluation, respectively. Then, Section 8 discusses the practical guide for prompt design, and Section 9 reviews the applications of LLMs in several representative domains. Finally, we conclude the survey in Section 10 by summarizing the major findings and discuss the remaining issues for future work.
# 2 Overview
In this section, we present an overview about the background of LLMs and then summarize the technical evolution of the GPT-series models.
## 2.1 Background for LLMs
Typically, large language models LLMs) refer to Transformer language models that contain hundreds of billions (or more) of parameters, which are trained on massive text data [32], such as GPT-3 [55], PaLM [56], Galactica [35], and LLaMA [57]. LLMs exhibit strong capacities to understand natural language and solve complex tasks (via text generation). To have a quick understanding of how LLMs work, this part introduces the basic background for LLMs, including scaling laws, emergent abilities and key techniques.

**Formulation of Scaling Laws for LLMs.** Currently, LLMs are mainly built upon the Transformer architecture [22], where multi-head attention layers are stacked in a very deep neural network. Existing LLMs adopt similar Transformer architectures and pre-training objectives (e.g., language modeling) as small language models. However, LLMs significantly extend the model size, data size, and total compute (orders of magnification). Extensive research has shown that scaling can largely improve the model capacity of LLMs [26, 55, 56]. ( 扩大规模可以大幅提高模型能力 ) Thus, it is useful to establish a quantitative approach to characterizing the scaling effect. Next, we introduce two representative scaling laws ( 两大代表性的扩展法则 ) for Transformer language models [30, 34].
-  **KM scaling law**. In 2020, Kaplan et al. [30] (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (N), dataset size (D), and the amount of training compute (C), for neural language models. Given a compute budget ( 计算预算 ) c, they empirically presented three basic formulas for the scaling law: ( 经验上，扩展法则的三大基本公式 )

$$
\begin{align}
L(N) = \left(\frac {N_c}{N}\right)^{\alpha_N}, \alpha_N \sim 0.076, N_c\sim 8.8\times 10^{13}\\
L(D) = \left(\frac {D_c}{D}\right)^{\alpha_D}, \alpha_D \sim 0.095, D_c\sim 5.4\times 10^{13}\\
L(C) = \left(\frac {C_c}{C}\right)^{\alpha_C}, \alpha_C \sim 0.050, C_c\sim 3.1\times 10^{13}\\
\end{align}
$$

where $L(·)$ denotes the cross entropy loss in nats, and a follow-up study [58] from OpenAI has shown that the language modeling loss ( 语言建模损失 ) can be decomposed into two parts, namely irreducible loss (the entropy of the true data distribution) ( 不可减少的损失-真实数据分布的熵 ) and reducible loss (an estimate of the KL divergence between the true and model distributions) ( 可减少的损失-模型分布和真实分布之间的估计 KL 散度 ). The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768M to 1.5B non-embedding parameters) and training compute, under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors.

- **Chinchilla scaling law.** As another representative study, Hoffmann et al. [34] (the Google DeepMind team) proposed an alternative form for scaling laws to instruct the computeoptimal training for LLMs. They conducted rigorous experiments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients as below [34]: ( 将模型损失建模为与模型大小 $N$ 和数据规模 $D$ 相关的函数 )

$$
L(N,D) = E+ \frac {A} {N^{\alpha}} + \frac {B}{D^{\beta}}
$$
where $E = 1.69$, $A = 406.4$, $B = 410.7$, $\alpha = 0.34$ and $\beta = 0.28$. By optimizing the loss $L(N, D)$ under the constraint $C \approx 6ND$, they showed that the optimal allocation of compute budget to model size and data size ( 相对于模型大小和数据大小最优的计算资源分配 ) can be derived as follows:

$$
N_{opt}(C) = G\left(\frac C 6\right)^a, D_{opt}(C) = G^{-1}\left(\frac C 6\right)^b\tag{3}
$$

(通过 $C$ 计算 $N$ 和 $D$)
where $a = \frac {\alpha} {\alpha + \beta}$ , $b = \frac {\beta} {\alpha + \beta}$ and $G$ is a scaling coefficient that can be computed by A, B, $\alpha$ and $\beta$. As analyzed in [34], given an increase in compute budget, ( 给定相同的计算预算增长 ) the KM scaling law favors a larger budget allocation in model size than the data size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales, i.e., having similar values for a and b in Equation (3).
(
scaling law 尝试用公式联系模型大小、数据规模和计算预算
)

**Discussion on Scaling Laws.** After introducing the formulations, we continue to discuss scaling law in the following two aspects, to enhance its understanding:

• *Predictable scaling.* ( 可预测的扩展 ) In practice, scaling law can be used to instruct the training of LLMs ( 指导 LLM 的训练 ), and it has been proven feasible to reliably estimate the performance of larger models based on that of smaller models ( 可以基于小模型估计更大模型的性能 ), called predictable scaling [46]. The benefits of predictable scaling for training LLMs are mainly twofold. Firstly, for large models, it is infeasible to rigorously examine various training tricks or variants, ( 难以在大模型中测试各种训练技巧及变体 ) and it would be very helpful if experiences gained from small models could also apply to large models. For instance, small proxy models can be trained to find the optimal schedule of the data mixture for large models [59]. ( 训练小的代理模型用于找到对大模型最优的数据混合调度 ) Secondly, the training of large-scale models takes a long time, often suffering from issues such as training loss spike ( 训练损失尖峰 ), and scaling law can be employed to monitor the training status of LLMs ( 监控大模型的训练状态 ), e.g., identifying abnormal performance at an early time. ( 尽早识别异常的表现 ) Despite that scaling law characterizes a smooth trend of performance increase (or loss decrease) ( scaling law 表示的是性能提升或损失下降的一个平滑的趋势 ), it also indicates that *diminishing returns* might occur as model scaling. ( 随着模型扩展，会出现回报减少 ) An empirical study [58] from the OpenAI team has shown that representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns (i.e., approaching the irreducible loss) [58]. This finding suggests that training large models are promising for improving the performance of downstream tasks. To further explore scaling effect, a potential issue is that the amount of available data for training LLMs is actually limited. With the ever-increasing model scale, the public text data would be soon “exhausted” for LLMs [60]. Thus, it will be meaningful to study how scaling laws apply to a data-constrained regime [61], where data repetition or augmentation might be useful to alleviate data scarcity.
• *Task-level predictability.*( 任务级别的可预测性 ) Existing research of scaling laws are mostly conducted in terms of language modeling loss ( 现存的对于扩展法则的研究主要聚焦于语言建模损失 )(e.g., per-token cross-entropy loss in nats [30]), while in practice we are more concerned about the performance of LLMs on actual tasks. ( 我们关心 LLMs 在实际任务的表现 ) Thus, a basic problem is that how the decrease of language modeling loss translates into the improvement of task performance [58]. ( 语言建模损失的下降如何转换为实际任务性能的提升 ) Intuitively, a model with a smaller language modeling loss tends to yield a better performance on downstream tasks, since language modeling loss can be considered as a general measure of the overall model capacity. ( 语言建模损失可以认为是整体模型能力的通用衡量 ) GPT-4 [46] has reported that some capabilities (e.g., coding ability) can be accurately predicted via scaling law. Despite that, readers should be aware that a direct decrease in language modeling loss does not always indicate an improvement of model performance on downstream tasks. Specially, the phenomenon of inverse scaling ( 逆扩展 ) would occur for some tasks, where task performance surprisingly becomes worse as the language modeling loss decreases [62]. Overall, it is more difficult to explore and characterize task-level scaling laws, since it might be also dependent on task-related information ( 可能依赖于任务相关的信息 )(task metric, task difficulty, etc.). Furthermore, some capacities (e.g., in-context learning [55]) are unpredictable according to the scaling law, which can be observed only when the model size exceeds a certain level (as discussed below).

**Emergent Abilities of LLMs.** In the literature [31], emergent abilities of LLMs are formally defined as “the abilities that are not present in small models but arise in large models”, which is one of the most prominent features that distinguish LLMs from previous PLMs. It further introduces a notable characteristic when emergent abilities occur [31]: performance rises significantly above random when the scale reaches a certain level. ( 当扩展达到特定级别，性能相较于随机有巨大提升 ) By analogy, such an emergent pattern has close connections with the phenomenon of phase transition in physics [31, 63]. ( 和物理学中的相变现象有联系 ) In principle, emergent abilities can be defined in relation to some complex tasks [31, 64], while we are more concerned with general abilities that can be applied to solve a variety of tasks. Here, we briefly introduce three typical emergent abilities ( 三个典型的涌现能力 ) for LLMs and representative models that possess such an ability.
- *In-context learning.* ( 上下文学习 ) The in-context learning (ICL) ability is formally introduced by GPT-3 [55]: assuming that the language model has been provided with a natural language instruction ( 自然语言指令 ) and/or several task demonstrations ( 任务展示 ), it can generate the expected output for the test instances by completing the word sequence of input text ( 通过完成输入文本的补全来生成期待的输出 ), without requiring additional training or gradient update. ( 不需要额外的训练和梯度更新 ) Among the GPTseries models, the 175B GPT-3 model exhibited a strong ICL ability in general, but not the GPT-1 and GPT-2 models. Such an ability also depends on the specific downstream task. ( ICL 也依赖于下游任务 ) For example, the ICL ability can emerge on the arithmetic tasks (e.g., the 3-digit addition and subtraction) for the 13B GPT-3, but 175B GPT-3 even cannot work well on the Persian QA task [31].
- *Instruction following.* ( 指令微调 ) By fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions (called instruction tuning) ( 指令微调: 用自然语言描述格式化的混合多任务数据集进行微调 ), LLMs are shown to perform well on unseen tasks that are also described in the form of instructions [28, 66, 67]. ( 对使用指令形式描述的不可见任务也表现得很好 ) With instruction tuning, LLMs are enabled to follow the task instructions for new tasks without using explicit examples, thus having an improved generalization ability. According to the experiments in [67], instruction-tuned LaMDA-PT [68] started to significantly outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study [69] found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (e.g., MMLU).
- *Step-by-step reasoning.* ( 逐步推理 ) For small language models, it is usually difficult to solve complex tasks that involve multiple reasoning steps, e.g., mathematical word problems. In contrast, with the chain-of-thought (CoT) prompting strategy [33] ( 思维链提示策略 ), LLMs can solve such tasks by utilizing the prompting mechanism that involves intermediate reasoning steps for deriving the final answer. This ability is speculated to be potentially obtained by training on code [33, 47]. An empirical study [33] has shown that CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B. ( 其相对于标准提示词得优势随着模型大小超过100B 变得明显 ) Furthermore, the performance improvement with CoT prompting seems to be also varied for different tasks, e.g., GSM8K > MAWPS > SWAMP for PaLM [33].

**How Emergent Abilities Relate to Scaling Laws.** ( 涌现能力如何与扩展法则相关 ) In existing literature [30, 31, 34], scaling laws and emergent abilities provide two perspectives to understand the advantage of large models over small models. In general, scaling law (often measured by language modeling loss 扩展法则常以语言建模损失衡量，可以预测 ) describes predictable performance relation with the potential effect of diminishing returns, while emergent abilities (often measured by task performance 涌现能力则以任务表现衡量，无法预测 ) are unpredictable but very profitable once such abilities actually emerge. Since the two perspectives reflect different performance trends (continuous improvement v.s. sharp performance leap 连续的提升 vs 猛烈的跳跃 ), they might lead to misaligned findings or observations. There are also extensive debates on the rationality of emergent abilities. A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (e.g., the discontinuous evaluation metrics) [70, 71]: when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear. However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way. For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones. More recently, a study [72] proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. Despite these efforts, more fundamental research (e.g., grokking) about the working mechanism of LLMs is still in need to understand the emergence of certain abilities. The subtle relation between scaling law and emergent abilities can be explained by analogy with the ability acquisition of human. Take the speaking ability as an example. For children, language development (especially infants) can be also considered as a multi-level process where “emergent abilities” occur. Specially, the language ability would relatively stable within a time interval, but qualitative change only occurs when evolving into another ability level (e.g., from speaking simple words to speaking simple sentences). Such a learning process is essentially not smooth and stable (i.e., language ability does not develop at a constant rate over time), though a child actually grows every day. ( scaling law 量变，emergent ability 质变)

**Key Techniques for LLMs.**( LLM 的关键技术) It has been a long way that LLMs evolve into the current state: general and capable learners. In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs. Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.

- *Scaling.* ( 扩展 ) As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity [30, 34]. As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively. Since compute budget is usually limited, scaling laws can be further employed to conduct a more compute-efficient allocation of the compute resources. For example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget [34]. In addition, data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity. ( 更大的模型、数据规模 )
- *Training.*( 训练 ) Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed training algorithms ( 分布式训练算法 ) are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several optimization frameworks ( 优化框架 ) have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed [74] and Megatron-LM [75–77]. Also, optimization tricks ( 优化技巧 ) are also important for training stability and model performance, e.g., restart to overcome training loss spike [56] and mixed precision training [78]. More recently, GPT-4 [46] proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models. ( 分布式训练、以及优化技巧 )
- *Ability eliciting.* ( 能力引导 ) After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general-purpose task solvers. These abilities might not be explicitly exhibited when LLMs perform some specific tasks. As the technical approach, it is useful to design suitable task instructions or specific in-context learning strategies to elicit such abilities. For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps. Furthermore, we can perform instruction tuning on LLMs with task descriptions expressed in natural language, for improving the generalizability of LLMs on unseen tasks. These eliciting techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models. ( 指令微调以及提示词 )
- *Alignment tuning.* ( 对齐微调 ) Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans. It is necessary to align LLMs with human values, e.g., helpful, honest, and harmless. For this purpose, InstructGPT [66] designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback [66, 79]. It incorporates human in the training loop with elaborately designed labeling strategies. ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, e.g., rejecting to answer insulting questions. ( 对齐人类价值观，提高输出质量 )
- *Tools manipulation.*( 工具使用 ) In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best expressed in the form of text (e.g., numerical computation). In addition, their capacities are also limited to the pre-training data, e.g., the inability to capture up-to-date information. ( 不易更新信息 ) To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs [80, 81]. For example, LLMs can utilize the calculator for accurate computation [80] and employ search engines to retrieve unknown information [81]. More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps), which are by analogy with the “eyes and ears” of LLMs. Such a mechanism can broadly expand the scope of capacities for LLMs. ( LLM 使用工具 )
## 2.2 Technical Evolution of GPT-series Models
we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure 4. The basic principle underlying GPT models is to compress the world knowledge into the decoder-only Transformer model by language modeling, such that it can recover (or memorize) the semantics of world knowledge and serve as a general-purpose task solver. ( GPT 模型的基本原则: 将世界知识通过语言建模压缩入模型，模型得以记忆、恢复世界知识的语义，以作为一个通用的任务解决者 ) 

Two key points to the success are (I) training decoder-only Transformer language models that can accurately predict the next word and (II) scaling up the size of language models. ( 两大关键点：训练 decorder-only 语言模型，预测下一个词；扩展规模 )

Overall, the research of OpenAI on LLMs can be roughly divided into the following stages. ( OpenAI 对 LLMs 的研究划分为以下阶段 )
**Early Explorations.** ( 早期探索 ) 
According to one interview with Ilya Sutskever14 (a co-founder and chief scientist of OpenAI), the idea of approaching intelligent systems with language models was already explored in the early days of OpenAI, while it was attempted with recurrent neural networks (RNN) [121]. With the advent of Transformer, OpenAI developed two initial GPT models, namely GPT-1 [122] and GPT-2 [26], which can be considered as the foundation to more powerful models subsequently i.e., GPT-3 and GPT-4.
- *GPT-1.* 
    In 2017, the Transformer model [22] was introduced by Google, and the OpenAI team quickly adapted their language modeling work to this new neural network architecture. They released the first GPT model in 2018, i.e., GPT-1 [122], and coined the abbreviation term GPT as the model name, standing for Generative Pre-Training. GPT-1 was developed based on a generative, decoder-only Transformer architecture, and adopted a hybrid approach of unsupervised pretraining and supervised fine-tuning. GPT- 1 has set up the core architecture for the GPT-series models and established the underlying principle to model natural language text, i.e., predicting the next word. ( GPT-1建立了建模自然语言文本的底层的原则，即预测下一个词 )
- *GPT-2.* 
    Following a similar architecture of GPT-1, GPT-2 [26] increased the parameter scale to 1.5B, which was trained with a large webpage dataset WebText. ( 大型网页数据集 ) As claimed in the paper of GPT-2, it sought to perform tasks via unsupervised language modeling, without explicit fine-tuning using labeled data. To motivate the approach, they introduced a probabilistic form for multi-task solving, i.e., $p(output|input, task)$ (similar approaches have been adopted in [123]), which predicts the output conditioned on the input and task information. ( 多任务解决的概率形式: 条件于任务和输入信息预测输出 ) 
    To model this conditional probability, language text can be naturally employed as a unified way to format input, output and task information. In this way, the process of solving a task can be cast as a word prediction problem for generating the solution text.  ( 将解决任务的过程转换为词预测问题 ) Further, they introduced a more formal claim for this idea: “Since the (task-specific) supervised objective is the same as the unsupervised (language modeling) objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective (for various tasks)” [26]. A basic understanding of this claim is that each (NLP) task can be considered as the word prediction problem based on a subset of the world text. ( 每个 NLP 任务都可以视作基于世界文本的一个子集进行的词预测任务 ) 
    Thus, unsupervised language modeling could be capable in solving various tasks,  if it was trained to have sufficient capacity in recovering the world text. These early discussion in GPT-2’s paper echoed in the interview of Ilya Sutskever by Jensen Huang: “What the neural network learns is some representation of the process that produced the text. This text is actually a projection of the world... the more accurate you are in predicting the next word, the higher the fidelity, the more resolution you get in this process...” ( NN 学习到的是生成文本这一过程的某种表示，文本就是世界的投影，对于下一个词预测得越准确，对于世界知识的掌握就越好 )
    ( GPT-2背后的思想就是用词预测任务训练的模型可以成为通用的、学习到世界知识的任务解决模型，且 NLP 任务都可以看作是词预测任务 )

**Capacity Leap.** ( 能力飞跃 ) 
Although GPT-2 is intended to be an “unsupervised multitask learner”, it overall has an inferior performance compared with supervised fine-tuning stateof-the-art methods. Because it has a relatively small model size, it has been widely fine-tuned in downstream tasks, especially the dialog tasks [124, 125]. Based on GPT-2, GPT-3 demonstrates a key capacity leap by scaling of the (nearly same) generative pre-training architecture.
- *GPT-3.* 
    GPT-3 [55] was released in 2020, which scaled the model parameters to an ever larger size of 175B. In the GPT-3’s paper, it formally introduced the concept of in-context learning (ICL), which utilizes LLMs in a fewshot or zero-shot way. ICL can teach (or instruct) LLMs to understand the tasks in the form of natural language text. ( 上下文学习即用自然语言文本指示模型理解任务 ) 
    With ICL, the pre-training and utilization of LLMs converge to the same language modeling paradigm: pre-training predicts the following text sequence conditioned on the context, while ICL predicts the correct task solution, which can be also formatted as a text sequence, given the task description and demonstrations. ( 此时模型的预训练和使用都指向同一概念：基于上下文，预训练预测下一个文本序列 ) 
    GPT-3 not only demonstrates very excellent performance in a variety of NLP tasks, but also on a number of specially designed tasks that require the abilities of reasoning or domain adaptation. Although the GPT-3’s paper does not explicitly discuss the emergent abilities of LLMs, we can observe large performance leap that might transcend the basic scaling law [30], e.g., larger models have significantly stronger ICL ability (illustrated in the original Figure 1.2 of the GPT-3’s paper [55]). Overall, GPT-3 can be viewed as a remarkable landmark in the journey evolving from PLMs to LLMs. It has empirically proved that scaling the neural networks to a significant size can lead to a huge increase in model capacity. ( GPT-3 经验性地证明了扩展模型规模至极大可以有效提高模型能力 )
    ( GPT-3 的主要思路就是扩大模型规模 )

**Capacity Enhancement**( 能力增强 )
Due to the strong capacities, GPT-3 has been the base model to develop even more capable LLMs for OpenAI. Overall, OpenAI has explored two major approaches to further improving the GPT-3 model, i.e., training on code data and alignment with human preference, which are detailed as follows. ( OpenAI 继续探索了两种方法来进一步提升 GPT-3：在代码数据上训练、和人类偏好对齐 )
- *Training on code data.* A major limitation of the original GPT-3 model (pre-trained on plain text) lies in the lack of the reasoning ability on complex tasks, ( 训练于纯文本导致缺乏对复杂任务的逻辑推理能力 ) e.g., completing the code and solving math problems. To enhance this ability, Codex [105] was introduced by OpenAI in July 2021, which was a GPT model fine-tuned on a large corpus of GitHub code. It demonstrated that Codex can solve very difficult programming problems, and also lead to a significant performance improvement in solving math problems [126]. Further, a contrastive approach [127] to training text and code embedding was reported in January 2022, which was shown to improve a series of related tasks (i.e., linearprobe classification, text search and code search). Actually, the GPT-3.5 models are developed based on a code-based GPT model (i.e., code-davinci-002), which indicates that training on code data is a very useful practice to improve the model capacity of GPT models, especially the reasoning ability. Furthermore, there is also a speculation that training on code data can greatly increase the chain-of-thought prompting abilities of LLMs [47], while it is still worth further investigation with more thorough verification.
  ( 使用代码数据进行训练或微调可以显著提高模型的逻辑推理能力 )
- *Human alignment.* The related research of human alignment can be dated back to the year 2017 (or earlier) for OpenAI: a blog article entitled “learning from human preferences” was posted on the OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans [79] (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12). Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimization (PPO) [128] was published in July 2017, which now has been the foundational RL algorithm for learning from human preferences [66]. Later in January 2020, GPT-2 was finetuned using the aforementioned RL algorithms [79, 128], which leveraged human preferences to improve the capacities of GPT-2 on NLP tasks. In the same year, another work [129] trained a summarization model for optimizing human preferences in a similar way. Based on these prior work, InstructGPT [66] was proposed in January 2022 to improve the GPT-3 model for human alignment, which formally established a three-stage reinforcement learning from human feedback (RLHF) algorithm. Note that it seems that the wording of “instruction tuning” has seldom been used in OpenAI’s paper and documentation, which is substituted by supervised fine-tuning on human demonstrations (i.e., the first step of the RLHF algorithm [66]). In addition to improving the instruction following capacity, the RLHF algorithm is particularly useful to mitigate the issues of generating harm or toxic content for LLMs, which is key to the safe deployment of LLMs in practice. OpenAI describes their approach to alignment research in a technical article [130], which has summarized three promising directions: “training AI systems to use human feedback, to assist human evaluation and to do alignment research”.  ( RLHF 提高了模型的指令遵循能力，同时显著缓解了模型生成有害内容的问题 )

These enhancement techniques lead to the improved GPT-3 models with stronger capacities, which are called GPT-3.5 models by OpenAI (see the discussion about the OpenAI API in Section 3.1). ( GPT-3用了这两个增强技巧，得到了 GPT-3.5 )

**The Milestones of Language Models.** ( 语言模型的里程碑 )
Based on all the exploration efforts, two major milestones have been achieved by OpenAI, namely ChatGPT [131] and GPT-4 [46], which have largely raised the capacity bar of existing AI systems. ( 包括了 ChatGPT 和 GPT-4 )
- *ChatGPT.* In November 2022, OpenAI released the conversation model ChatGPT, based on the GPT models (GPT-3.5 and GPT-4). As the official blog article introduced [131], ChatGPT was trained in a similar way as InstructGPT (called “a sibling model to InstructGPT” in the original post), while specially optimized for dialogue. They reported a difference between the training of ChatGPT and InstructGPT in the data collection setup: human-generated conversations (playing both the roles of user and AI) are combined with the InstructGPT dataset in a dialogue format for training ChatGPT. ( 训练 ChatGPT 的数据集中包含了人类生成的对话，人类扮演了 AI 和人类两角，以及对话形式的 InstructGPT 的数据集 ) ChatGPT exhibited superior capacities in communicating with humans: possessing a vast store of knowledge, skill at reasoning on mathematical problems, tracing the context accurately in multi-turn dialogues, and aligning well with human values for safe use. Later on, the plugin mechanism has been supported in ChatGPT, which further extends the capacities of ChatGPT with existing tools or apps. So far, it seems to be the ever most powerful chatbot in the AI history. The launch of ChatGPT has a significant impact on the AI research in the future, which sheds light on the exploration of human-like AI systems.
- *GPT-4.* As another remarkable progress, GPT-4 [46] was released in March 2023, which extended the text input to multimodal signals. Overall, GPT-4 has stronger capacities in solving complex tasks than GPT-3.5, showing a large performance improvement on many evaluation tasks. A recent study [41] investigated the capacities of GPT-4 by conducting qualitative tests with human-generated problems, spanning a diverse range of difficult tasks, and showed that GPT-4 can achieve more superior performance than prior GPT models such as ChatGPT. Furthermore, GPT-4 responds more safely to malicious or provocative queries, due to a six-month iterative alignment (with an additional safety reward signal in the RLHF training). In the technical report, OpenAI has emphasized how to safely develop GPT-4 and applied a number of intervention strategies to mitigate the possible issues of LLMs, such as hallucinations, privacy and overreliance. For example, they introduced the mechanism called red teaming [132] to reduce the harm or toxic content generation. As another important aspect, GPT- 4 has been developed on a well-established deep learning infrastructure with improved optimization methods. They introduced a new mechanism called *predictable scaling* ( 可预测拓展 ) that can accurately predict the final performance with a small proportion of compute during model training. ( GPT-4在各方面的能力优于 ChatGPT )
- *GPT-4V, GPT-4 turbo, and beyond.* Based on the work done for GPT-4 [46], OpenAI further released GPT-4V in September 2023, which focused on the safe deployment of the vision capabilities of GPT-4. In the GPT-4V’s system card [133], it has extensively discussed the assessment and mitigation of risks related to visually augmented inputs. Specially, GPT-4V exhibited strong vision capacities in various application scenarios, showing the great potential asa powerful multimodal learning system. ( GPT-4V 强化了视觉能力 ) More recently, in November 2023, OpenAI released an upgraded generation of GPT-4 model at DevDay, named GPT-4 Turbo, with a series of technical improvements. GPT-4 Turbo is featured by the improved model capacity (more capable than GPT- 4), the extended knowledge source (up to April 2023), long context window (up to 128k tokens), optimized model performance (cheaper price), and other useful functionality updates (function call, reproducible outputs, etc.). At the same time, Assistants API was launched to ease the rapid development of agent-like assistants. With this API, developers can easily create goal-oriented assistants within their applications, by leveraging specific instruction, extra knowledge and tool use. Furthermore, multimodal capacities (see, hear, and speak) were also enhanced in this new release, supported by GPT-4 Turbo with vision, DALL·E 3, Text-to-speech (TTS), and Listen to voice samples. These improvements have greatly extended the capacity scope and enhanced the task performance of GPT models.

Despite the huge progress, there are still limitations with these superior LLMs, e.g., generating hallucinations with factual errors or potentially risky response within some specific context [46]. More limitations or issues of LLMs will be discussed in Section 7. It poses long-standing research challenges to develop more capable, safer LLMs. ( LLMs 的幻觉问题仍需解决 )

From the perspective of engineering, OpenAI has adopted an iterative deployment strategy [134] to develop the models and products by following a five-stage development and deployment life-cycle, which aims to effectively reduce the potential risks of using the models. In the following, we will dive into the technical details in order to have a specific understanding of how they have been developed. ( OpenAI 采用迭代式部署策略来开发模型 )
# 3 Resources of LLMs
It is by no means an easy job to develop or reproduce LLMs, considering the challenging technical issues and huge demands of computation resources. A feasible way is to learn experiences from existing LLMs and reuse publicly available resources for incremental development or experimental study. In this section, we briefly summarize the publicly available resources for developing LLMs, including model checkpoints (or APIs), corpora and libraries. ( 本节总结公开可用的 LLM 资源，包括模型 checkpoint，语料库和库 )
## 3.1 Publicly Available Model Checkpoints or APIs
Given the huge cost of model pre-training, well-trained model checkpoints are critical to the study and development of LLMs for the research community. Since the parameter scale is a key factor to consider for using LLMs, we categorize these public models into two scale levels (i.e., tens of billions of parameters and hundreds of billions of parameters), which is useful for users to identify the suitable resources according to their resource budget.  ( 模型分为两类，百亿参数级别的和千亿参数级别的 )

In addition, for inference, we can directly employ public APIs to perform our tasks, without running the model locally. Next, we introduce the publicly available model checkpoints and APIs.

**Models with Tens of Billions of Parameters.** Most of the models in this category have a parameter scale ranging from 10B to 20B, except LLaMA [57] and LLaMA2 [99] (containing 70B parameters in the largest version), NLLB [91] (containing 54.5B parameters in the largest version), and Falcon [135] (containing 40B parameters in the largest version). Other models within this range include mT5 [83], PanGu-α [84], T0 [28], GPT-NeoX-20B [87], CodeGen [86], UL2 [89], Flan-T5 [69], and mT0 [94]. Among them, FlanT5 (11B version) can serve as a premier model for research on instruction tuning, since it explores the instruction tuning from three aspects [69]: increasing the number of tasks, scaling the model size, and fine-tuning with chain-ofthought prompting data. ( FlanT5 从三个方面探索了指令微调，包括提高任务数量，扩展模型大小，以及用思维链提示数据微调 ) Besides, CodeGen (11B version), as an autoregressive language model designed for generating code, can be considered as a good candidate for exploring the code generation ability. ( 代码生成任务 ) It also introduces a new benchmark MTPB [86] specially for multi-turn program synthesis, which is composed by 115 expert-generated problems. To solve these problems, it requires LLMs to acquire sufficient programming knowledge (e.g., math, array operations, and algorithms). More recently, CodeGen2 [97] has been released to explore the impact of choices in model architecture, learning algorithms, and data distributions on the model. As another LLM specialized in coding abilities, StarCoder [98] has also achieved excellent results. As for multilingual tasks ( 多语言任务 ), mT0 (13B version) might be a good candidate model, which has been fine-tuned on multilingual tasks with multilingual prompts. Furthermore, PanGu-α [84] shows good performance in Chinese downstream tasks in zero-shot or fewshot settings, which is developed based on the deep learning framework MindSpore [136]. Note that PanGu-α [84] holds multiple versions of models (up to 200B parameters), while the largest public version has 13B parameters. As a popular LLM, LLaMA (65B version) [57], which contains approximately five times as many parameters as other models, has exhibited superior performance in tasks related to instruction following. Compared to LLaMA, LLaMA2 [99] has made more explorations in reinforcement learning from human feedback (RLHF) ( LLaMA2在 RLHF 上做了更多探索 ) and developed a chat-oriented version called LLaMA-chat, which generally outperforms existing open-source models across a range of helpfulness and safety benchmarks. Due to the openness and effectiveness, LLaMA has attracted significant attention from the research community, and many efforts [137–140] have been devoted to fine-tuning or continually pre-training its different model versions for implementing new models or tools. More recently, Falcon [135], as another open-source LLM, has also achieved very excellent performance on open benchmarks. It is featured by a more careful data cleaning process to prepare the pre-training data (with a publicly shared dataset RefinedWeb [141]). Typically, pre-training models at this scale require hundreds or even thousands of GPUs or TPUs. For instance, GPT-NeoX-20B uses 12 supermicro servers, each equipped with 8 NVIDIA A100-SXM4-40GB GPUs, while LLaMA utilizes 2,048 A100-80G GPUs as reported in their original publications. To accurately estimate the computation resources needed, it is suggested to use the metrics measuring the number of involved computations such as FLOPS (i.e., FLoating point number Operations Per Second) [30].

**Models with Hundreds of Billions of Parameters.** For models in this category, only a handful of models have been publicly released. ( 仅有少量千亿参数的模型公开发布 ) For example, OPT [90], OPT-IML [95], BLOOM [78], and BLOOMZ [94] have nearly the same number of parameters as GPT-3 (175B version), while GLM [93] and Galactica [35] have 130B and 120B parameters, respectively. Among them, OPT (175B version), with the instruction-tuned version OPT-IML, has been specially motivated for open sharing, which aims to enable researchers to carry out reproducible research at scale. For research in cross-lingual generalization, ( 跨语言泛化 ) BLOOM (176B version) and BLOOMZ (176B version) can be used as base models, due to the competence in multilingual language modeling tasks. As a bilingual LLM, GLM has also provided a popular small-sized Chinese chat model ChatGLM2-6B (a updated version for ChatGLM-6B), which is featured with many improvements in efficiency and capacity (e.g., quantization, 32K-length context, fast inference rate). Models of this scale typically require thousands of GPUs or TPUs to train. For instance, OPT (175B version) used 992 A100-80GB GPUs, while GLM (130B version) used a cluster of 96 NVIDIA DGX-A100 (8x40G) GPU nodes.

**LLaMA Model Family.** The collection of LLaMA models [57] were introduced by Meta AI in February, 2023, consisting of four sizes (7B, 13B, 30B and 65B). Since released, LLaMA has attracted extensive attention from both research and industry communities. LLaMA models have achieved very excellent performance on various open benchmarks, which have become the most popular open language models thus far. A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining. In particular, instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs. ( 指令调节 LLaMA 是开发自定义模型或特定模型的主流方法，因为计算成本低 ) To effectively adapt LLaMA models in non-English languages, it often needs to extend the original vocabulary (trained mainly on English corpus) or fine-tune it with instructions or data in the target language. ( LLaMA 原始词袋主要为英语 ) Among these extended models, Stanford Alpaca [142] is the first open instruct-following model fine-tuned based on LLaMA (7B). It is trained by 52K instruction-following demonstrations generated via selfinstruct [143] using text-davinci-003. The instruction data, named Alpaca-52K, and training code have been extensively adopted in subsequent work, such as AlpacaLoRA [144] (a reproduction of Stanford Alpaca using LoRA [145]), Koala [146], and BELLE [147]. In addition, Vicuna [138] is another popular LLaMA variant, trained upon user-shared conversations collected from ShareGPT [148]. Due to the excellent performance and availability of the LLaMA model family, many multimodal models incorporate them as the base language models, to achieve strong language understanding and generation abilities. ( 许多多模态模型将 LLaMA 模型作为基础模型 ) Compared with other variants, Vicuna is more preferred in multimodal language models, which have led to the emergence of a variety of popular models, including LLaVA [149], MiniGPT- 4 [150], InstructBLIP [151], and PandaGPT [152]. The release of LLaMA has greatly advanced the research progress of LLMs. To summarize the research work conducted on LLaMA, we present a brief evolutionary graph in Figure 5
![[A Survey of LLMs-Fig5.png]]

**Public API of LLMs.** Instead of directly using the model copies, APIs provide a more convenient way for common users to use LLMs, without the need of running the model locally. As a representative interface for using LLMs, the APIs for the GPT-series models [46, 55, 66, 105] have been widely used for both academia and industry. OpenAI has provided seven major interfaces to the models in GPT-3 series: `ada`, `babbage`, `curie`, `davinci` (the most powerful version in GPT-3 series), `text-ada-001`, `text-babbage-001`, and `text-curie-001`. Among them, the first four interfaces can be further finetuned on the host server of OpenAI. In particular, `babbage`, `curie`, and `davinci` correspond to the GPT-3 (1B), GPT-3 (6.7B), and GPT-3 (175B) models, respectively [55]. In addition, there are also two APIs related to Codex [105], called `code-cushman-001` (a powerful and multilingual version of the Codex (12B) [105]) and `code-davinci-002`. Further, GPT-3.5 series include one base model `code-davinci-002` and three enhanced versions, namely `text-davinci-002`, `text-davinci-003`, and `gpt-3.5-turbo`. As more powerful alternatives, in this year, OpenAI has released the model interfaces for GPT-4 series, including `gpt-4`, `gpt-4-32k`, `gpt-4-1106-preview` (i.e., GPT-4 Turbo) and `gpt-4-vision-preview` (i.e., GPT-4 Turbo with vision, a multimodal model). It is worth noting that OpenAI has been maintaining and upgrading these model interfaces (`gpt-3.5-turbo`, `gpt-4`, `gpt-4-32k`), so the API name will actually point to the latest version. Currently, ChatGPT can be powered by either GPT-3.5 or GPT-4 models. Overall, one select the suitable model interface based on the specific application scenarios and response requirements. The detailed usage can be found on their project websites. ( OpenAI 基于 GPT 系列模型提供了许多 API )
## 3.2 Commonly Used Corpora for Pre-training
![[A Survey of LLMs-Table 2.png]]
In contrast to earlier PLMs, LLMs which consist of a significantly larger number of parameters require a higher volume of training data that covers a broad range of content. For this need, there are increasingly more accessible training datasets that have been released for research. In this section, we will briefly summarize several widely used corpora for training LLMs. Based on their content types, we categorize these corpora into six groups: Books, CommonCrawl, Reddit links, Wikipedia, Code, and others. ( 本节总结几个用于训练 LLMs 的广泛使用的语料库，它们分为6组 )

**Books.** BookCorpus [153] is a commonly used dataset in previous small-scale models (e.g., GPT [122] and GPT-2 [26]), consisting of over 11,000 books covering a wide range of topics and genres (e.g., novels and biographies). Another large-scale book corpus is Project Gutenberg [154], consisting of over 70,000 literary books including novels, essays, poetry, drama, history, science, philosophy, and other types of works in the public domain. It is currently one of the largest open-source book collections, which is used in training of MT-NLG [113] and LLaMA [57]. As for Books1 [55] and Books2 [55] used in GPT-3 [55], they are much larger than BookCorpus but have not been publicly released so far.

**CommonCrawl.** CommonCrawl [163] is one of the largest open-source web crawling databases, containing a petabytescale data volume, which has been widely used as training data for existing LLMs. As the whole dataset is very large, existing studies mainly extract subsets of web pages from it within a specific period. However, due to the widespread existence of noisy and low-quality information in web data, it is necessary to perform data preprocessing before usage. ( 由于噪声的存在以及网页数据的低质量信息的存在，在使用之前进行数据清洗是必要的 ) Based on CommonCrawl, there are four filtered datasets that are commonly used in existing work: C4 [82], CCStories [155], CC-News [27], and RealNews [156]. The Colossal Clean Crawled Corpus (C4) includes five variants, namely en (806G), en. noclean (6T), realnewslike (36G), webtextlike (17G), and multilingual (38T). The en version has been utilized for pre-training T5 [82], LaMDA [68], Gopher [64], and UL2 [89]. The multilingual C4, also called mC4, has been used in mT5 [83]. CC-Stories (31G) is composed of a subset of CommonCrawl data, in which the contents are made in a story-like way. Because the original source of CC-Stories is not available now, we include a reproduction version, CC-Stories-R [164], in Table 2. Moreover, two news corpora extracted from CommonCrawl, i.e., REALNEWS (120G) and CC-News (76G), are also commonly used as the pre-training data.

**Reddit Links.** Reddit is a social media platform that enables users to submit links and text posts, which can be voted on by others through “upvotes” or “downvotes”. Highly upvoted posts are often considered useful, and can be utilized to create high-quality datasets. WebText [26] is a well-known corpus composed of highly upvoted links from Reddit, but it is not publicly available. As a surrogate, there is a readily accessible open-source alternative called OpenWebText [157]. Another corpus extracted from Reddit is PushShift. io [158], a real-time updated dataset that consists of historical data from Reddit since its creation day. Pushshift provides not only monthly data dumps but also useful utility tools to support users in searching, summarizing, and conducting preliminary investigations on the entire dataset. This makes it easy for users to collect and process Reddit data.

**Wikipedia.** typically, the Englishonly filtered versions of Wikipedia are widely used in most LLMs (e.g., GPT-3 [55], LaMDA [68], and LLaMA [57]). Wikipedia is available in multiple languages, so it can be used in multilingual settings.

**Code.** To collect code data, existing work mainly crawls open-source licensed codes from the Internet. Two major sources are public code repositories under open-source licenses (e.g., GitHub) and code-related question-answering platforms (e.g., StackOverflow).

**Others.** 

In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure 6), instead of a single corpus. Therefore, existing studies commonly mix several ready-made datasets (e.g., C4, OpenWebText, and the Pile), and then perform further processing to obtain the pre-training corpus.  ( 现存的研究一般会混合多个数据集，并进行进一步处理 )

To have a quick reference of the data sources used in existing LLMs, we present the pre-training corpora of three representative LLMs: ( 如下展示了三个代表性 LLM 使用的预训练语料库 )
- *GPT-3* (175B) [55] was trained on a mixed dataset of 300B tokens, including CommonCrawl [163], WebText2 [55], Books1 [55], Books2 [55], and Wikipedia [159].
- *PaLM* (540B) [56] uses a pre-training dataset of 780B tokens, which is sourced from social media conversations, filtered webpages, books, Github, multilingual Wikipedia, and news.
 - LLaMA [57] extracts training data from various sources, including CommonCrawl, C4 [82], Github, Wikipedia, books, ArXiv, and StackExchange. The training data size for LLaMA (6B) and LLaMA (13B) is 1.0T tokens, while 1.4T tokens are used for LLaMA (32B) and LLaMA (65B).
## 3.3 Commonly Used Datasets for Fine-tuning
After pre-training, it requires further fine-tuning LLMs to enhance the model capacity, which often involve two major steps, namely instruction tuning (supervised fine-tuning) and alignment tuning. In this section, we mainly focus on discussing the related available datasets for the two kinds of tuning approaches, and more algorithm details can be found in Section 5. ( 微调 LLM 一般包含两个主要步骤：指令微调/有监督微调和对齐微调，本节介绍用于两种微调的相关数据集 )
### 3.3.1 Instruction Tuning Datasets
![[A Survey of LLMs-Table 3.png]]
After pre-training, instruction tuning (a.k.a., supervised finetuning) is an important method to enhance or unlock specific abilities of LLMs (e.g., instruction following). ( 指令微调即有监督微调，它对于 LLM 的指令遵循能力十分重要 )

In this part, we introduce several widely used datasets for instruction tuning, and categorize them into three main types based on the construction method of formatted instruction instances, namely NLP task datasets, daily chat datasets and synthetic datasets. We show their details in Table 3. ( 本节介绍指令微调常用数据集，基于格式化的指令实例的构建方法分为三个主类，即 NLP 任务数据集、日常聊天数据集、合成数据集 )

**NLP Task Datasets.** This kind of datasets are formatted based on collected NLP task datasets (e.g., text classification and summarization) with corresponding natural language task descriptions. In this category, P3 [182] and FLAN [67, 183] are two widely used datasets for instruction tuning. ( 这类数据集基于收集到的 NLP 任务数据集，即文本分类和总结数据集，进行格式化 )
-  *P3* [182] is composed of 170 English NLP datasets and 2,052 English prompt templates, where the input and output of each data example have been formatted with specific prompt templates for composing the training instance
- *FLAN* [67] consists of 62 widely used NLP benchmarks in its original version. Recently, FLAN-v2 [183] is also proposed, which expands FLAN by mixing additional instruction datasets, including Muffin [67], NIV2 [88], T0-SF [28], and CoT [184–186]. Muffin contains 62 tasks from the original FLAN and additional 26 tasks, including conversation and code synthesis tasks. T0-SF is extracted from T0 [28] while ensuring no overlap with Muffin. NIV2 refers to the Natural-Instructions v2 dataset [88], and CoT [184–186] is a combination of nine reasoning tasks with corresponding chain-of-thought prompts and outputs.

**Daily Chat Datasets.** This kind of datasets are constructed based on real user conversations where queries are posed by humans and responses are mainly generated by human labelers or LLMs (e.g., ChatGPT, GPT-4). The conversation types include open-ended generation, question answering, brainstorming, and chatting. In this category, ShareGPT [148], OpenAssistant [173] and Dolly [172] are three commonly used datasets for LLM fine-tuning.
-  *ShareGPT* [148] is collected from a data collection platform where users can upload their conversations with ChatGPT or GPT-4 through the ShareGPT API. Currently, this dataset consists of approximately 90,000 conversations, including real instructions or inquiries from human and responses from ChatGPT.
-  *OpenAssistant* [173] is a multilingual corpus containing 66,497 real-world conversation trees between human and AI assistant. Each conversation tree consists of multiple nodes, and each node represents the information generated by a role in the dialogue. It spans 35 languages and includes 461,292 manually annotated quality ratings of responses.
-  *Dolly* [172] is an English dataset comprising 15,000 human-generated data instances (prompt-response pairs) from Databricks. This dataset covers seven domains outlined in the InstructGPT [66], including brainstorming, classification, closed-book quality assurance, generation, information extraction, open-book quality assurance, and summarization.

**Synthetic Datasets.** This kind of datasets are typically constructed by instructing LLMs, based on pre-defined guidance rules or methods. In this category, Self-Instruct- 52K [143], Alpaca [142] and Baize [175] are three commonly used synthetic datasets for LLMs.
-  Self-Instruct-52K [143] is an instruction dataset generated through the self-instruct [143] method, consisting of 82,000 instances with 52,000 instructions. Concretely, the authors construct 175 seed instances, and then iteratively prompt the LLM [55] to synthesize additional instructions based on randomly selected 8 instructions as reference. Subsequently, the LLM is further instructed to generate instance inputs and their corresponding outputs based on the synthetic instructions, and finally obtain the Self-Instruct- 52K dataset.
-  Alpaca [142] is also a synthetic dataset based on the selfinstruct [143] method. It utilizes the text-davinci-003 model on the 175 seed datasets from Self-Instruct-52K to obtain 52,000 new instructions and corresponding inputs and outputs. Moreover, 60% of the examples are pure instructions without the input part in the final dataset.
-  Baize [175] is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances. To create Baize, a method called “self-chat” [175] is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format.
### 3.3.2 Alignment Datasets
![[A Survey of LLMs-Table 4.png]]
Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences (e.g., helpfulness, honesty, and harmlessness). In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF [170], SHP [177], PKU-SafeRLHF [181], Stack Exchange Preferences [178] and Sandbox Alignment Data [179]. We show their details in Table 4. ( 本节介绍用于对齐训练的数据集，对齐训练帮助 LLM 对齐人类价值观，使 LLM 的回答无害、有帮助、诚实等等 )
-  *HH-RLHF* [170] consists of around 169K instances, and can be divided into two parts that focus on the helpfulness and harmlessness of LLMs, respectively. Each instance is an open-ended conversation between a crowdworker and a chat model, about seeking assistance, advice, or task completion. The chat model provides two responses to each user query, and the more helpful or harmful responses will be chosen as the annotations.
-  *SHP* [177] focuses on the helpfulness of responses. It comprises 385K collective human preferences over responses to questions/instructions across 18 diverse subject areas, spanning topics from cooking to legal advice. Each instance is a Reddit post containing a question or instruction and a pair of top-level comments, one of which is deemed as more preferable by Reddit users and the other one is deemed as less helpful. Different from HH-RLHF [170], the data in SHP consists of naturally occurring and humanwritten responses.
-  *PKU-SafeRLHF* [181] encompasses more than 330K instances of expert comparison data, concentrating on the helpfulness and harmlessness. Each instance in the dataset includes a question and two responses, accompanied by safety labels for each response and two preference annotations between the two responses according to helpfulness and harmlessness. The harmlessness of a response indicates its classification as risk-neutral across all 14 harm categories, while the helpfulness of a response is evaluated based on its effectiveness in addressing the question.
-  *Stack Exchange Preferences* [178] focuses on the helpfulness of answers. It comprises about 10M questions and answers from Stack Overflow. Each instance consists of a question and more than two corresponding answers. Each answer is annotated with a score calculated based on its votes and a label denoting whether it is selected.
-  *Sandbox Alignment Data* [179] is an alignment dataset containing feedback from LLMs rather than human. It comes from a virtual interaction environment called SANDBOX, where the model simulates social interactions with other models and revise responses according to the feedback from other models. The dataset contains 169K instances, and each instance consists of a societal query, several responses, and corresponding ratings from other models.
## 3.4 Library Resource
In this part, we briefly introduce a series of available libraries for developing LLMs. ( 本节介绍一些用于构建 LLM 可用的库 )
- **Transformers** [187] is an open-source Python library for building models using the Transformer architecture, which is developed and maintained by Hugging Face. It has a simple and user-friendly API, making it easy to use and customize various pre-trained models. It is a powerful library with a large and active community of users and developers who regularly update and improve the models and algorithms.
- **DeepSpeed** [74] is a deep learning optimization library (compatible with PyTorch) developed by Microsoft, which has been used to train a number of LLMs, such as MTNLG [113] and BLOOM [78]. It provides the support of various optimization techniques for distributed training, such as memory optimization (ZeRO technique, gradient checkpointing), and pipeline parallelism.
- **Megatron-LM** [75–77] is a deep learning library developed by NVIDIA for training large-scale language models. It also provides rich optimization techniques for distributed training, including model and data parallelism, mixedprecision training, and FlashAttention. These optimization techniques can largely improve the training efficiency and speed, enabling efficient distributed training across GPUs.
- **JAX** [188] is a Python library for high-performance machine learning algorithms developed by Google, allowing users to easily perform computations on arrays with hardware acceleration (e.g., GPU or TPU). It enables efficient computation on various devices and also supports several featured functions, such as automatic differentiation and just-in-time compilation.
- **Colossal-AI** [189] is a deep learning library developed by HPC-AI Tech for training large-scale AI models. It is implemented based on PyTorch and supports a rich collection of parallel training strategies. Furthermore, it can also optimize heterogeneous memory management with methods proposed by PatrickStar [190]. Recently, a ChatGPT-like model called ColossalChat [140] has been publicly released with two versions (7B and 13B), which are developed using Colossal-AI based on LLaMA [57].
- **BMTrain** [191] is an efficient library developed by OpenBMB for training models with large-scale parameters in a distributed manner, which emphasizes code simplicity, low resource, and high availability. BMTrain has already incorporated several common LLMs (e.g., Flan-T5 [69] and GLM [93]) into its ModelCenter, where developers can use these models directly.
- **FastMoE** [192] is a specialized training library for MoE (i.e., mixture-of-experts) models. It is developed based on PyTorch, prioritizing both efficiency and user-friendliness in its design. FastMoE simplifies the process of transferring Transformer models to MoE models and supports both data parallelism and model parallelism during training.
- **vLLM** [193] is a fast, memory efficient, and easyto-use library for LLM inference and serving. To enable fast inference, it is specially optimized with high serving throughput, effective attention memory management using PagedAttention [193], continuous batching, and optimized CUDA kernels. Furthermore, vLLM also supports various decoding algorithms, tensor parallelism and streaming outputs. To ease the integration with other systems, vLLM is friendly to the use of HuggingFace models, and also provide OpenAI-compatible API servers.
- **DeepSpeed-MII** [194] is also a memory efficient Python library developed by DeepSpeed [74]. It aims to democratize LLMs inference by prioritizing high throughput, low latency, and cost-effectiveness. DeepSpeed-MII achieves accelerated text generation inference by leveraging four essential technologies: blocked KV caching, continuous batching, dynamic SplitFuse, and high-performance CUDA Kernels. It currently supports over 13,000 models across three popular model architectures, such as LLaMA [57], Mistral [195], and OPT [90].
- **DeepSpeed-Chat** [196] is a fast, cost-effective, and easy-to-use system framework that enables the integration of the complete RLHF process during model training. It is featured by three major functionalities: (1) it simplifies the training and inference process for ChatGPT-like models, enabling using a simple script to implement multiple training or inference steps; (2) it replicates the training mode of InstructGPT [66] and provides a complete pipeline for three training steps (i.e., SFT, reward model fine-tuning, and RLHF); (3) it integrates the training engine and inference engine of Deepspeed into a unified hybrid engine (Deepspeed HE) for RLHF training, which enables seamless switch between training and inference modes, and leveraging various optimizations from DeepSpeed Inference.

In addition to the above library resources, existing deep learning frameworks (e.g., PyTorch [197], TensorFlow [198], MXNet [199], PaddlePaddle [200], MindSpore [136] and OneFlow [201]) have also provided the support for parallel algorithms, which are commonly used for training largescale models
# 4 Pre-Training
Pre-training establishes the basis of the abilities of LLMs. By pre-training on large-scale corpora, LLMs can acquire essential language understanding and generation skills [55, 56]. ( 在预训练时，LLM 获得语言理解和生成能力 )
In this process, the scale and quality of the pre-training corpus are critical for LLMs to attain powerful capabilities.  ( 用于预训练的语料库的规模和质量很重要 )
Furthermore, to effectively pre-train LLMs, model architectures, acceleration methods, and optimization techniques need to be well designed. In what follows, we first discuss the data collection and processing in Section 4.1, then introduce the commonly used model architectures in Section 4.2, and finally present the training techniques to stably and efficiently optimize LLMs in Section 4.3. ( 本节介绍预训练的数据收集和处理、常用的模型架构、用于稳定和高效优化 LLM 的训练技巧 )
## 4.1 Data Collection and Preparation
Compared with small-scale language models, LLMs have a stronger demand for high-quality data for model pretraining, and their model capacities largely rely on the pretraining corpus and how it has been preprocessed. In this part, we discuss the collection and processing of pre-training data, including data sources, preprocessing methods, and important analysis of how pre-training data affects the performance of LLMs ( LLM 的能力高度依赖于预训练语料库的质量及其处理方式，本节讨论预训练数据的收集和处理，包括数据源、预处理方法，并分析预训练数据如何影响 LLM )
### 4.1.1 Data Source
To develop a capable LLM, it is key to collect a large amount of natural language corpus from various data sources. Existing LLMs mainly leverage a mixture of diverse public textual datasets as the pre-training corpus. Figure 6 shows the distribution of the sources of pre-training data for a number of representative LLMs. ( 现存的 LLM 主要利用多个多样的公共文本数据集的混合作为预训练语料库 )
![[A Survey of LLMs-Fig6.png]]

The source of pre-training corpus can be broadly categorized into two types: general data and specialized data. General data, such as webpages, books, and conversational text, is utilized by most LLMs [55, 56, 90] due to its large, diverse, and accessible nature, which can enhance the language modeling and generalization abilities of LLMs. ( 预训练语料库的源总体分为两类：通用数据和特定数据，通用数据包括网页、书、对话文本，其量大且多样并易于获取，这些数据帮助 LLM 学习语言建模和泛化能力 )
In light of the impressive generalization capabilities exhibited by LLMs, there are also studies that extend their pre-training corpus to more specialized datasets, such as multilingual data, scientific data, and code, endowing LLMs with specific task-solving capabilities [35, 56, 86]. ( 特定数据包括多语言数据、科学数据、代码，它们使 LLM 获得特定的任务解决能力 )
In what follows, we describe these two types of pre-training data sources and their effects on LLMs. For a detailed introduction to the commonly used corpus, one can refer to Section 3.2. ( 本节之后将描述这两类数据对 LLM 的影响 )

**General Text Data.** As we can see in Figure 6, the vast majority of LLMs adopt general-purpose pre-training data, such as webpages, books, and conversational text, which provides rich text sources on a variety of topics. Next, we briefly summarize three important kinds of general data. ( 总结三个重要的通用数据类型 )
-  *Webpages.* Owing to the proliferation of the Internet, various types of data have been created, which enables LLMs to gain diverse linguistic knowledge and enhance their generalization capabilities [26, 82]. For convenient use of these data resources, a large amount of data is crawled from the web in previous work, such as CommonCrawl [163]. However, the crawled web data tends to contain both high-quality text, such as Wikipedia and lowquality text, like spam mail, thus it is important to filter and process webpages for improving the data quality. ( 网页数据需要进行过滤 )
-  *Conversation text.* Conversation data can enhance the conversational competence of LLMs [90] and potentially improve their performance on a range of question-answering tasks [56]. Researchers can utilize subsets of public conversation corpus (e.g., PushShift. io Reddit corpus) [158, 202] or collect conversation data from online social media. Since online conversational data often involves discussions among multiple participants, an effective processing way is to transform a conversation into a tree structure, where the utterance is linked to the one it responds to. In this way, the multi-party conversation tree can be divided into multiple sub-conversations, which can be collected in the pre-training corpus. Furthermore, a potential risk is that the excessive integration of dialogue data into LLMs may result in a side effect [90]: declarative instructions and direct interrogatives are erroneously perceived as the beginning of conversations, thus leading to a decline in the efficacy of the instructions. ( 对话数据的一个潜在风险在于过多的对话数据会使得 LLM 将陈述式的指令和直接的询问认为是一个对话的开始，导致指令的效力降低 )
-  *Books.* Compared to other corpus, books provide an important source of formal long texts, which are potentially beneficial for LLMs to learn linguistic knowledge, model long-term dependency, and generate narrative and coherent texts. To obtain open-source book data, existing studies usually adopt the Books3 and Bookcorpus2 datasets, which are available in the Pile dataset [161]. ( 书籍数据对于模型学习语言学知识、建模长距离依赖、生成记叙文和一致的文本可能有帮助 )

**Specialized Text Data.** Specialized datasets are useful to improve the specific capabilities of LLMs on downstream tasks. Next, we introduce three kinds of specialized data. ( 特定的数据集利于提高 LLM 在特定下游任务的能力，介绍三种特定数据 )
- *Multilingual text.* In addition to the text in the target language, integrating a multilingual corpus can enhance the multilingual abilities of language understanding and generation. For example, BLOOM [78] and PaLM [56] have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora. FLM [102] mixes Chinese and English corpora in nearly equal proportions. These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-the art models that are fine-tuned on the corpus in the target language (s).
- *Scientific text.* The exploration of science by humans has been witnessed by the increasing growth of scientific publications. In order to enhance the understanding of scientific knowledge for LLMs [35, 203], it is useful to incorporate a scientific corpus for model pre-training [35, 203]. By pretraining on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks [204]. To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources. Due to the complex nature of data in scientific fields, such as mathematical symbols and protein sequences, specific tokenization and preprocessing techniques are usually required to transform these different formats of data into a unified form that can be processed by language models. ( 科学文献文本存在一些特殊性，例如数学符号和蛋白质序列，因此需要特定的 token 化方法以及预处理技术 )
-  *Code.* Program synthesis has been widely studied in the research community [105, 205–208], especially the use of PLMs trained on code [165, 209]. However, it remains challenging for these PLMs (e.g., GPT-J [165]) to generate highquality and accurate programs. Recent studies [105, 208] have found that training LLMs on a vast code corpus can lead to a substantial improvement in the quality of the synthesized programs. The generated programs can successfully pass expert-designed unit-test cases [105] or solve competitive programming questions [114]. In general, two types of code corpora are commonly used for pre-training LLMs. The first source is from programming question answering communities like Stack Exchange [210]. The second source is from public software repositories such as GitHub [86, 105, 208], where code data (including comments and docstrings) are collected for utilization. ( 代码于利阿库一般收集于 GitHub 和 Stack Exchange ) Compared to natural language text, code is in the format of a programming language, corresponding to long-range dependencies and accurate execution logic [211]. A recent study [47] also speculates that training on code might be a source of complex reasoning abilities (e.g., chain-of-thought ability [33]). Furthermore, it has been shown that formatting reasoning tasks into code can help LLMs generate more accurate results [211]. ( 一些研究表明代码语料帮助 LLM 建立复杂分析能力，例如思维链，同时有研究发现将推理任务格式化为代码帮助 LLM 得到更准确的答案 )
### 4.1.2 Data Preprocessing
![[A Survey of LLMs-Fig7.png]]
After collecting a large amount of text data, it is essential to preprocess the data for constructing the pre-training corpus, especially removing noisy, redundant, irrelevant, and potentially toxic data [56, 64, 212], which may largely affect the capacity and performance of LLMs. 
> 数据预处理中重要的一步是移除对于 LLM 性能有影响的噪声、冗余、无关和有害数据

To facilitate the data processing, a recent study [213] proposes a useful data processing system for LLMs, named Data-Juicer, which provides over 50 processing operators and tools. In this part, we review the detailed data preprocessing strategies to improve the quality of the collected data [64, 78, 112]. A typical pipeline of preprocessing the pre-training data for LLMs has been illustrated in Figure 7. 
> 本节介绍一些数据预处理策略以提高数据质量

**Quality Filtering.** (质量筛选) To remove low-quality data from the collected corpus, existing work generally adopts two approaches: (1) classifier-based, and (2) heuristic-based. ( 现存的将低质量数据移除的方法分为两种：基于分类器的、基于启发式的 ) The former approach trains a selection classifier based on highquality texts and leverages it to identify and filter out lowquality data. Typically, these methods [55, 56, 112] train a binary classifier with well-curated data (e.g., Wikipedia pages) as positive instances and sample candidate data as negative instances, and predict the score that measures the quality of each data example. ( 基于分类器的方法用高质量的数据作为正例训练二元分类器 ) However, several studies [64, 112] find that a classifier-based approach may result in the unintentional removal of high-quality texts in dialectal, colloquial, and sociolectal languages, which potentially leads to bias in the pre-training corpus and diminishes the corpus diversity. ( 一些研究发现基于分类器的方法可能误删除方言、口语、社会方言中的高质量文本，导致模型偏见，降低语料库多样性 ) As the second approach, several studies, such as BLOOM [78] and Gopher [64], employ heuristicbased approaches to eliminate low-quality texts through a set of well-designed rules, which can be summarized as follows: ( 基于启发式的方法使用一系列规则移除低质量文本 )
- *Language based filtering.* If a LLM would be mainly used in the tasks of certain languages, the text in other languages can be filtered. ( 移除 LLM 可能不会主要使用的语言语料 )
- *Metric based filtering.* Evaluation metrics about the generated texts, e.g., perplexity, can be employed to detect and remove unnatural sentences. ( 评估语料的度量例如 perplexity )
- *Statistic based filtering.* Statistical features of a corpus, e.g., the punctuation distribution, symbol-to-word ratio, and sentence length, can be utilized to measure the text quality and filter the low-quality data. ( 评估语料的统计量 )
- *Keyword based filtering.* Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed. ( 基于关键词 )

**De-duplication.**(去重) Existing work [214] has found that duplicate data in a corpus would reduce the diversity of language models, which may cause the training process to become unstable and thus affect the model performance. 
> 研究发现语料库中重复数据会降低语言模型多样性，导致训练过程不稳定

Therefore, it is necessary to de-duplicate the pre-training corpus. Specially, de-duplication can be performed at different granularities, including sentence-level, document-level, and dataset-level de-duplication. 
> 语料的去重可以在多个粒度执行，包括句子级别、文档级别、数据集级别

First, low-quality sentences that contain repeated words and phrases should be removed, as they may introduce repetitive patterns in language modeling [215]. At the document level, existing studies mostly rely on the overlap ratio of surface features (e.g., words and n-grams overlap) between documents to detect and remove duplicate documents containing similar contents [57, 64, 78, 216]. Furthermore, to avoid the dataset contamination problem, it is also crucial to prevent the overlap between the training and evaluation sets [56], by removing the possible duplicate texts from the training set. It has been shown that the three levels of de-duplication are useful to improve the training of LLMs [56, 217], which should be jointly used in practice.
> 句子级别即句子内重复的词和短语被移除，防止它们在语言建模中引入重复的模式
> 文档级别上，现存研究依赖于文档之间的表面特征 (词/n-gram) 的重叠以探测相似的内容
> 数据集级别上，为了避免污染，需要防止训练集和验证集之间重叠，因此需要移除训练集部分文本
> 三种方法都有益处，需要一同使用

**Privacy Reduction.**(隐私减少) The majority of pre-training text data is obtained from web sources, including user-generated content involving sensitive or personal information, which may increase the risk of privacy breaches [218]. Thus, it is necessary to remove the personally identifiable information (PII) from the pre-training corpus. One direct and effective approach is to employ rule-based methods, such as keyword spotting, to detect and remove PII such as names, addresses, and phone numbers [162]. Furthermore, researchers also find that the vulnerability of LLMs under privacy attacks can be attributed to the presence of duplicate PII data in the pre-training corpus [219]. Therefore, de-duplication can also reduce privacy risks to some extent.
> 移除可以个人身份信息，一种方法是基于规则，例如锁定关键词，包括地址、人名、电话号码
> 研究发现 LLM 对于隐私攻击的脆弱性可以归因于预训练语料库中出现的重复的个人身份信息

**Tokenization.**(词元化) Tokenization is also a crucial step for data preprocessing. It aims to segment raw text into sequences of individual tokens, which are subsequently used as the inputs of LLMs. In traditional NLP research (e.g., sequence labeling with conditional random fields [220]), word-based tokenization is the predominant approach, which is more aligned with human’s language cognition. 
> tokenization 将原始文本化为 token 序列，用作随后 LLM 的输入
> 传统 NLP 研究中常用基于单词的 tokenization

However, wordbased tokenization can yield different segmentation results for the same input in some languages (e.g., Chinese word segmentation), generate a huge word vocabulary containing many low-frequency words, and also suffer from the “outof-vocabulary” issue. 
> 基于词的 tokenization 对于中文的同一文本会有不同的 tokenization 结果，且会产生大量包含了低频率词语的巨大词袋

Thus, several neural network models employ character as the minimum unit to derive the word representation (e.g., a CNN word encoder in ELMo [21]). Recently, subword tokenizers have been widely used in Transformer based language models, typically including BytePair Encoding tokenization, WordPiece tokenization and Unigram tokenization. HuggingFace has maintained an excellent online NLP course on tokenizer with running examples, and we refer to the beginners to this course. Next, we briefly describe the three representative tokenization methods.
> 一些研究使用字符作为最小单位
> subwork tokenizer 最近广泛使用，包括了 BPE，WordPiece，Unigram 
> 下面介绍三种代表性的 tokenization 方法

- *Byte-Pair Encoding (BPE) tokenization.* BPE was originally proposed as a general data compression algorithm in 1994 [221], and then adapted to NLP for tokenization [222]. It starts with a set of basic symbols (e.g., the alphabets and boundary characters), and iteratively combine frequent pairs of two consecutive tokens in the corpus as new tokens (called merge). For each merge, the selection criterion is based on the co-occurrence frequency of two contiguous tokens: the top frequent pair would be selected. The merge process continues until it reaches the predefined size. Further, Byte-level BPE has been used to improve the tokenization quality for multilingual corpus (e.g., the text containing non-ASCII characters) by considering bytes as the basic symbols for merge. Representative language models with this tokenization approach include GPT-2, BART, and LLaMA.
> BPE 从一个基础符号集合开始，例如字母表和边界字符，然后迭代式结合经常出现的符号对作为新的 token，每次迭代合并最经常出现的 token 对，迭代持续直到集合大小达到预定义大小
> 字节级别的 BPE 已经用于提高多语言语料库的 tokenization 质量，它将字节作为用于合并的基本符号

- *WordPiece tokenization.* WordPiece was a Google internal subword tokenization algorithm. It was originally proposed by Google in developing voice search systems [223]. Then, it was used in the neural machine translation system in 2016 [224], and was adopted as the word tokenizer for BERT in 2018 [23]. WordPiece has a very similar idea with BPE by iteratively merging consecutive tokens, whereas taking a slightly different selection criterion for the merge. To conduct the merge, it first trains a language model and employs it to score all possible pairs. Then, at each merge, it selects the pair that leads to the most increase in the likelihood of training data. Since Google has’t released the official implementation of the WordPiece algorithm, HuggingFace gives a more intuitive selection measure in its online NLP course: a pair is scored by dividing the co-occurrence count by the product of the occurrence counts of two tokens in the pair based on training corpus.
> WordPiece 是一个 Google 使用的 subword tokenization 算法，被用作 BERT 的 tokenization 算法，其思想和 BPE 类似，即迭代式合并连续 tokens，但选择标准不同，它会训练一个语言模型，用该模型为所有可能 token pair 打分，之后每次迭代都选择能让训练数据的似然提升最高的 pair
> HuggingFace 有一个直观的实现：一个 pair 的分数是将两个 token 在训练语料库中的出现次数相乘，然后被它们共同出现的次数相除

- *Unigram tokenization.* Unlike BPE and WordPiece, Unigram tokenization [225] starts with a sufficiently large set of possible substrings or subtokens for a corpus, and iteratively removes the tokens in the current vocabulary until the expected vocabulary size is reached. As the selection criterion, it calculates the yielded increase in the likelihood of training corpus by assuming that some token was removed from current vocabulary. This step is conducted based on a trained unigram language model. To estimate the unigram language model, it adopts an expectation–maximization (EM) algorithm: at each iteration, we first find the currently optimal tokenization of words based on the old language model, and then re-estimate the probabilities of unigrams to update the language model. During this procedure, dynamic programming algorithms (i.e., the Viterbi algorithm) are used to efficiently find the optimal decomposition way of a word given the language model. Representative models that adopt this tokenization approach include T5 and mBART.
> 和 BPE 和 WorkPiece 不同，Unigram 从一个足够大的、包含了语料库中可能的 substring 和 subtoken 的集合开始，然后迭代式移除 token 直到词袋达到指定大小
> 它计算移除 token 后训练语料库的似然增长，移除增长最高的 token，该步骤由训练好的 unigram 语言模型进行
> 为了估计 unigram 语言模型，它采用 EM 算法：每个迭代中，我们首先找到基于旧的语言模型最优的 tokenizatin，然后用该 tokenization 重新评估 unigram 的概率，更新语言模型，其中 DP 算法被用于高效地找到给定一个语言模型，最优的 word 分解方法

Although it is expedient to leverage an existing tokenizer (e.g., OPT [90] and GPT-3 [55] utilize the tokenizer of GPT- 2 [26]), using a tokenizer specially designed for the pretraining corpus can be highly beneficial [78], especially for the corpus that consists of diverse domains, languages, and formats.
> 使用为预训练语料库专门设计的 tokenizer 有很大的好处，尤其是对于包含了多个格式、领域、语言的语料库

Therefore, recent LLMs often train the customized tokenizers specially for the pre-training corpus with the SentencePiece library [226], which includes Byte-level BPE and Unigram tokenization.
> 最近的 LLM 使用 SentencePiece 库训练专门针对预训练语料库的自定义 tokenizer，SentencePiece 库包含了字节级别的 BPE 和 Unigram tokenization

A note is that normalization techniques in BPE, such as NFKC [227], may degrade the tokenization performance [34, 64, 78]. When extending existing LLMs (i.e., continual pre-training or instruction tuning), we should be also aware of the potential side effect with customized tokenizers. For example, LLaMA trains the BPE tokenizer based on a pre-training corpus mainly consisting of English texts, and the derived vocabulary might be less capable in processing non-English data, e.g., taking longer inference latency to generate Chinese texts.
> BPE 中的 normalization 技术可能会降低 tokenization 的表现，对现存的 LLM 延伸时，也需要注意自定义 tokenizer 的潜在影响，例如 LLaMA 基于主要是英语的预训练语料库训练了 BPE tokenizer，它的词袋对于处理非英语数据不友好，例如需要更长的推理时间以生成中文文本

**Discussion on Effect of Data Quality.** ( 关于数据质量的探讨 ) For pre-training, the quality of pre-training data is vital to the model capacities of LLMs. Existing work has shown that pre-training on the low-quality corpus, such as noisy, toxic, and duplicate data, would largely hurt the performance of models [64, 214, 216, 219]. Recent studies, such as T5 [82], GLaM [112], and Gopher [64], have investigated the influence of data quality on the LLMs’ capacities. By comparing the performance of models trained on the filtered and unfiltered corpus, they have reached the similar conclusion that pre-training LLMs on cleaned data can improve the model performance. More specifically, the duplication of data may result in “double descent” (referring to the phenomenon of performance initially deteriorating and subsequently improving) [214, 228], or even overwhelm the training process [214]. In addition, it has been shown that duplicate data degrades the ability of LLMs to copy from the context, which might further affect the generalization capacity of LLMs using in-context learning [214]. Therefore, as suggested in [56, 64, 78, 212], it is essential to utilize preprocessing methods like quality filtering, toxic filtering and deduplication to carefully clean the pre-training corpus (as illustrated in Section 4.1.2), to improve stability of the training process and avoid affecting the model performance.
> 研究表明在低质量数据集上预训练会严重损害 LLM 表现
> 另外，数据重复可能导致 double descent (指模型表现最开始下降，然后之后又上升)，甚至压制训练过程
> 另外，有研究发现重复的数据会降低 LLM 从上下文 copy 的能力，进而损害上下文学习
> 因此数据预处理对于稳定训练、提高 LLM 能力是必要的
### 4.1.3 Data Scheduling
![[A Survey of LLMs-Fig8.png]]
After data preprocessing, it is essential to design suitable strategies to schedule these multi-source data for pretraining a capable LLM. Generally, two key aspects should be paid close attention for data scheduling: the proportion of each data source (data mixture), and the order in which each data source is scheduled for training (data curriculum). Next, we discuss the two aspects in detail. An illustration of data scheduling has been presented in Figure 8.
> 数据预处理后，需要设计策略调度这些多源数据用于训练 LLM
> 数据调度需要注意两个关键方面：每个数据源的比例 (数据混合)；数据用于训练的顺序 (数据 curriculum)
> 本节相机讲述这两个方面

**Data Mixture.** Since each kind of data source is closely related to the development of certain capacities for LLMs (referring to the discussions in Section 4.1), it is important to set a suitable distribution to mix these data. 
> 每一类数据都对 LLM 的某一方面能力有重要性，因此需要关注混合数据的分布

The data mixture is generally set in a global level (i.e., the distribution of the entire pre-training data), and can be also locally set to varied proportions at different training stages. 
> 数据混合比例一般全局设置，即所有预训练数据的分布
> 也可以在不同的训练阶段其分布不同，即局部设置

During pre-training, data samples from different sources would be selected according to the mixture proportions: more data will be sampled from a data source with a larger weight. 
> 预训练时，来自于多个数据源的数据样本根据混合比例被选择，显然权重大的源会有更多的样本被选择

Typically, existing LLMs such as LLaMA [57] may employ upsampling or downsampling on the full data of each source to create specific data mixtures as pre-training data. 
> 现存的 LLM 例如 LLaMA 会对每个源的数据进行上采样或下采样，来构造特定的混合数据作为预训练数据

As Figure 6 illustrates, existing LLMs use different data mixtures to construct the pre-training data. As a representative model, the pre-training data of LLaMA [57] mainly consists of webpages (over 80%), alongside 6.5% of code-heavy data from GitHub and StackExchange, 4.5% from books, and 2.5% of scientific data sourced from arXiv, which has become an important reference for training general-purpose LLMs. Furthermore, special data mixtures can be used to facilitate different purposes. For example, Falcon [141] is trained on pure webpages, and CodeGen [86] largely increases the amount of code data. In practice, data mixture is often determined empirically, and we summarize several common strategies for finding an effective data mixture as follows:
> 数据混合分布在实践中常常是经验性地决定，我们总结几个有效寻找数据混合地策略如下
- *Increasing the diversity of data sources.* Recent studies have empirically shown that training on excessive data about a certain domain would degrade the generalization capability of LLMs on other domains [35, 64]. In contrast, increasing the data source heterogeneity (e.g., including diverse data sources) is critical for improving the downstream performance of LLMs [212, 229, 230]. To further examine the effect of different data sources, some studies have conducted ablation experiments by removing each data source one by one, and pre-train LLMs with specially curated datasets [212]. It has been shown that dropping data sources with high heterogeneity (e.g., webpages) impacts LLM’s abilities more severely than dropping sources with low heterogeneity (e.g., academic corpus).
> 提高数据源的多样性
> 最近的研究经验性地发现在关于某一领域的过多数据上训练会降低 LLM 在其他领域的泛化性
> 提高数据源的异质性则有助于提高 LLM 下游的表现
> 一些研究进行消融实验，发现移除具有高异质性 (例如网页) 的数据集对 LLM 的影响要比移除具有低异质性 (例如学术语料) 的数据集更严重
- *Optimizing data mixtures.* In addition to manually setting the data mixtures, several studies have proposed to optimize the data mixtures for improving the model pretraining [59, 231]. Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space [231] or those that provide positive influences on downstream task performance [232]. Further, to reduce the reliance of target tasks, DoReMi [59] first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood between the two models are observed. Finally, the learned domain weights of the proxy model are applied to train a much larger LLM. In a more simple way, one can train several small language models with different data mixtures, and select the data mixture that leads to the most desirable performance. However, an assumption made in this approach is, when trained in a similar way, small models would resemble with large models in model abilities or behaviors, which may not always hold in practice.
> 优化数据混合
> 一些研究提出优化数据混合以提高预训练质量
> 给定目标下游任务，可以多选择在特征空间具有更高相似度的数据为预训练数据，或选择对下游任务有益的数据
> 为了减少对目标任务的依赖，DoReMi 首先用给定的领域权重初始化小的参考模型，然后训练另一个小的代理模型，然后对于两个模型中观察到的在似然上具有最大差异的数据进行上采样
> 我们可以用不同的数据混合训练多个小模型，然后选择效果最好的数据混合训练大的 LLM，但其背后的推断并不总是成立

- *Specializing the targeted abilities.* The model capacities of LLMs heavily rely on data selection and mixture, and one can boost the proportions of specific data sources to enhance certain model abilities [64, 212]. For example, the mathematical reasoning and coding abilities can be specially enhanced by training with more mathematical texts and code data, respectively. Furthermore, experimental results on the LAMBADA dataset [233] show that increasing the proportion of books data can improve the model capacity in capturing long-term dependencies from text, and increasing the proportion of the C4 dataset [82] leads to performance improvement on the C4 validation dataset [64]. Generally, it is important to identify more implicit relations between data sources and model abilities. To enhance specific skills such as mathematics and coding in LLMs, or to develop specialized LLMs, a practical way is to employ a multi-stage training approach, e.g., general and skill-specific data can be scheduled at two consecutive stages. This approach of training LLMs on varying sources or proportions of data across multiple stages is also known as “data curriculum”, which will be introduced below.
> 专攻所需的能力
> LLM 的能力高度依赖于数据选择和混合，可以通过增大特定数据源的比例强化 LLM 在该方面的能力，例如，数学推理和代码能力可以通过训练于更多的数学文本和代码数据强化
> 有研究展示提高数据数据的比例可以提高模型捕获长期文本中依赖的能力
> 因此一般来说判断数据源和模型能力之间的隐式关系是很重要的
> 要提高 LLM 的特定能力，一种实用的方法是采用多阶段训练，即将通用和数据和针对技能的数据调度为两个连续的阶段
> 在不同的阶段用不同的比例的数据以及不同的源的数据训练 LLM 被称为“数据课程”

**Data Curriculum.** After preparing the data mixture, it is important to schedule the order that specific data is presented to LLMs for pre-training. It has been shown that, in some cases, to learn a certain skill, learning in a skillset sequence (e.g., basic skills → target skill) outperforms direct learning from a corpus focused solely on the target skill [234, 235].
> 一些研究表明，要学习一项技能，首先学习基本技能，然后学习目标技能的效果要好于直接在聚焦于目标技能的语料库上学习

Following the idea of curriculum learning [236], data curriculum has been proposed and widely used in model pre-training [234, 235, 237, 238]. It aims to organize different parts of pre-training data for LLMs in a specific order, e.g., starting with easy/general examples and progressively introducing more challenging/specialized ones. More generally, it can broadly refer to the adaptive adjustment of data proportions for different sources during pre-training. Existing work about data curriculum mainly focuses on continual pre-training, such as specialized coding LLMs (e.g., CodeLLaMA [235]) or long context LLMs (e.g., LongLLaMA [238]). However, it still lacks of more detailed report about data curriculum for generalpurpose LLMs (e.g., LLaMA) in the literature. To determine data curriculum, a practical approach is to monitor the development of key abilities of LLMs based on specially constructed evaluation benchmarks, and then adaptively adjust the data mixture during pre-training. Next, we take three common abilities as examples to introduce how the concept of data curriculum applies in continual pre-training.
> 数据课程模仿的就是课程学习的思路，它为 LLM 的预训练的不同阶段组织特定顺序的数据，例如从简单的数据开始，逐渐变难
> 数据课程也可以指对不同的源数据比例在预训练过程中的适应性调整
> 现存的关于数据课程的工作聚焦于继续性的工作，例如继续预训练 LLaMA 为 CodeLLaMA，但缺乏使用数据课程预训练通用目的的 LLM
> 决定数据课程的一个实用方法就是监控 LLM 在特意构建的 benchmark 上的关键能力的演变，然后适应性调节数据混合
> 我们以三个常见的能力为例，介绍在继续预训练中应用数据混合的概念

- *Coding.* To improve the coding ability of LLMs, CodeLLaMA [235] is developed based on LLaMA 2 [99] (2T general tokens → 500B code-heavy tokens), aiming to improve the code generation ability and retain natural language understanding skills. CodeLLaMA also provides a version that is further specialized to a certain programming language, namely CodeLLaMA-Python (2T general tokens → 500B code-heavy tokens → 100B Python-heavy tokens).
> 代码能力
> CodeLLaMA 基于 LLaMA 2开发，数据课程为2T 通用的 tokens 到500B 多代码的 tokens，意在提高代码生成能力的同时报纸自然语言理解能力，CodeLLaMA 还提供了对 python 特别训练的版本，即在数据课程之后又添加了100B 多 python 的 tokens
- *Mathematics.* Llemma [239] is proposed to enhance the mathematical capacities of general-purpose LLMs. It is developed based on CodeLLaMA. Although CodeLLaMA [235] mainly focuses on the coding ability, experiments have shown that it performs better than its base model LLaMA 2 on mathematics benchmarks [239]. Based on CodeLLaMA, Llemma is continually trained on mixtures of scientific papers, web data containing mathematical text and code (2T general tokens → 500B code-heavy tokens → 50∼200B math-heavy tokens). Note that the pre-training data of Llemma also contains 5% general domain data as a form of regularization.
> 数学能力
> Llemma 基于 CodeLLaMA 开发，实验表明 CodeLLaMA 比 LLaMA2在数学 benchmark 上表现更好
> Llemma 继续训练于混合了科学文献、包含了数学文本、代码的网页数据的混合数据集上
> Llemma 中的预训练数据中还包含5%的通用领域数据作为正则化的一种形式

- *Long context.* Long context modeling is an important ability for LLMs, and many studies have explored extending the context windows of LLMs via continually training [235, 238]. With modifications on position embeddings (i.e., position interpolation) of RoPE-based LLMs [57, 99, 240], CodeLLaMA further extends the context window of LLaMA 2 (2.5T tokens with 4K context window → 20B tokens with 16K context window). LongLLaMA [238] also achieves longer context window with the help of external memory and a unique training objective (1T tokens with 2K context window → 10B tokens with 8K context window).
> 长上下文
> CodeLLaMA 对位置编码进行了修改 (即位置插值)，进一步将 LLaMA 2的上下文窗口从 4K 延伸到了 16K，LongLLaMA 在外部记忆以及独特的训练目标的帮助下，将上下文窗口从 2K 提升到了 8K
### 4.1.4 Summary of Data Preparation
In this part, we summarize the general procedure and key points to prepare pre-training data for LLMs, which are detailed in the following three aspects.
> 本节总结为 LLM 准备预训练数据的一般过程和关键点，分为以下三个方面

- *Data collection.* It is suggested to include diverse data sources in the pre-training data. Although Falcon [141] shows that webpages alone can be employed to train powerful LLMs, a more typical approach is to also incorporate diverse high-quality text like code, books, scientific papers, etc. If a LLM is specialized with a certain skill, the proportion of corresponding data source should be increased accordingly. For example, Gopher [64] and Chinchilla [34] are trained with approximately 40% of data from books. PaLM [44] and LaMDA [68] use approximately 50% conversational data.
> 数据收集
> 数据尽可能多样化
> 提高特定数据比例强化 LLM 技能
- *Data cleaning.* After data collection, it is crucial to clean the raw corpus to enhance its quality as possible. First, deduplication is commonly used in existing work [99, 141, 229]. Second, low-quality text, toxic content, and data with privacy concerns should be removed at different granularities (e.g., document, passage or sentence). In practice, both heuristic and classifier-based methods can be employed for quality and toxicity filtering (e.g., CCNet [241], fastText [242], and Data-Juicer [243]). Third, with the cleaned data, one can further unify or specify the format for pretraining data, and perform the tokenization by training the tokenizer on the filtered and deduplicated corpus with libraries like SentencePiece [226].
> 数据清洗
> 清洗原始语料库，强化质量
> 手段包括：去重、在多个粒度 (文档、段落、句子) 去除低质量、有害、隐私数据
> 对于清洗过的数据，可以进一步统一化或指定数据的格式
> 最后用得到的语料库训练 tokenizer 作 tokenization
- *Data scheduling.* With the preprocessed data, the next step is to determine the data mixture and the specific order of data for pre-training LLMs. To determine both settings, a practical way is to first train several small language models with multiple candidate plans and then select a good plan among them [59]. Overall, it is more difficult to find a suitable data curriculum. In practice, one can monitor the performance of intermediate model checkpoints on specific evaluation benchmarks, and dynamically tune the data mixture and distribution during pre-training. In this process, it is also useful to explore the potential relations between data sources and model abilities to instruct the design of data curriculum.
> 数据调度
> 需要决定数据的混合比列以及用作训练的顺序
> 一种实用的方法是首先用多个备选计划训练多个小语言模型，然后从中选出一个好的
## 4.2 Architecture
![[A Survey of LLMs-Table 5.png]]
In this section, we review the architecture design of LLMs, i.e., mainstream architecture, pre-training objective, and detailed configuration. Table 5 presents the model cards of several representative LLMs with public details.
> 本节回顾 LLMs 的架构设计，即 LLMs 的主流架构、预训练目标、细节的配置
### 4.2.1 Typical Architectures
![[A Survey of LLMs-Fig9.png]]
Due to the excellent parallelizability and capacity, the Transformer architecture [22] has become the de facto backbone to develop various LLMs, making it possible to scale language models to hundreds or thousands of billions of parameters. In general, the mainstream architectures of existing LLMs can be roughly categorized into three major types, namely encoder-decoder, causal decoder, and prefix decoder, as shown in Figure 9.
> Transformer 的并行性和能力使得它成为开发各种 LLM 的主流标准骨干，使得语言模型可以扩展到百亿以及千亿参数，现存的 LLMs 的主流架构可以分为三个主要类别，即编码器-解码器，因果解码器和前缀解码器

**Encoder-decoder Architecture.** The vanilla Transformer model is built on the encoder-decoder architecture [22], which consists of two stacks of Transformer blocks as the encoder and decoder, respectively. The encoder adopts stacked multi-head self-attention layers to encode the input sequence for generating its latent representations, while the decoder performs cross-attention on these representations and autoregressively generates the target sequence. Encoder-decoder PLMs (e.g., T5 [82] and BART [24]) have shown effectiveness on a variety of NLP tasks. So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, e.g., Flan-T5 [69]. We leave a detailed discussion about the architecture selection in Section 4.2.6.
> 传统 Transformer 建立于编码器-解码器架构，包括两个 Transformer block 栈分别作为 encoder 和 decoder
> encoder 采用堆叠的多头自注意力层编码输入序列，生成隐藏表示
> decoder 进行交叉初一里，自回归生成目标序列
> 仅有少量的 LLMs 基于 encoder-decoder 架构

**Causal Decoder Architecture.** The causal decoder architecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT-series models [26, 55, 122] are developed based on the causal-decoder architecture. In particular, GPT-3 [55] has successfully demonstrated the effectiveness of this architecture, also showing an amazing in-context learning capability of LLMs. Interestingly, GPT-1 [122] and GPT- 2 [26] do not exhibit such superior abilities as those in GPT-3, and it seems that scaling plays an important role in increasing the model capacity of this model architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as OPT [90], BLOOM [78], and Gopher [64]. Note that both the causal decoder and prefix decoder discussed next belong to decoder-only architectures. When mentioning “decoder only architecture”, it mainly refers to the causal decoder architecture in existing literature, unless specified.
> 因果解码器架构包含一个单向的 attention mask，保证每个输入 token 仅注意到之前的 token 和它自己，decoder 以相同的方式处理输入和输出 token
> GPT 系列为该架构的代表模型
> 因果解码器架构被广泛采用，当提到 decoder only 架构时，一般指因果解码器架构，当然前缀解码器架构实质上也属于 decoder only 架构

**Prefix Decoder Architecture.** The prefix decoder architecture (a.k.a., non-causal decoder [244]) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens [245] and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Instead of pre-training from scratch, a practical suggestion is to continually train causal decoders and then convert them into prefix decoders for accelerating convergence [29], e.g., U-PaLM [118] is derived from PaLM [56]. Existing representative LLMs based on prefix decoders include GLM- 130B [93] and U-PaLM [118].
> 前缀解码器架构即非因果解码器，它修改了因果解码器的 masking 机制，使得它可以对前缀 tokens 执行双向 attention，仅对生成的 tokens 执行单向 attention
> 在该架构下，前缀伽玛琪可以双向编码前缀序列，然后自回归预测输出 token，它的 encoder-decoder 的差异在于编码和解码共享相同的参数
> 一个实用的技巧是继续训练因果解码器为前缀解码器，以加快收敛
> GLM-130B 采用的就是前缀解码器

**Mixture-of-Experts.** For the above three types of architectures, we can further extend them via the mixture-of-experts (MoE) scaling, in which a subset of neural network weights for each input are sparsely activated, e.g., Switch Transformer [25] and GLaM [112]. The major merit is that MoE is a flexible way to scale up the model parameter while maintaining a constant computational cost [25]. It has been shown that substantial performance improvement can be observed by increasing either the number of experts or the total parameter size [246]. Despite the merits, training large MoE models may suffer from instability issues due to the complex, hard-switching nature of the routing operation. To enhance the training stability of MoE-based language models, techniques such as selectively using high-precision tensors in the routing module or initializing the model with a smaller range have been introduced [25]. More recently, there is widespread speculation that GPT-4 has been developed based on the MoE architecture, but without official verification.
> 混合专家模型
> 对于以上三种架构，我们可以通过 MoE 对它们进行拓展
> MoE 中，对于每一个输入，NN 权重的一个子集会被稀疏地激活
> MoE 的主要优势在于它是一个保持计算开销不变的同时扩展模型参数的一个玲化的方式，研究发现提高专家的数量或者总的参数大小都可以极大提高性能
> 但训练大型的 MoE 模型可能由于路由操作的困难性而遭受不稳定性问题
> 可以用选择性地在路由模块中使用高精度 tensors 以及用更小的范围初始化模型来强化基于 MoE 的语言模型的训练稳定性
> 有人推测 GPT-4基于 MoE 架构开发

**Emergent Architectures.** The conventional Transformer architectures typically suffer from quadratic computational complexity. Because of this, efficiency has become an important issue when training and making inference with long inputs. To improve efficiency, some studies aim to devise new architectures for language modeling, including parameterized state space models (e.g., S4 [247], GSS [248], and H3 [249]), long convolutions like Hyena [250], and Transformer-like architectures that incorporate recursive update mechanisms (e.g., RWKV [251] and RetNet [252]). The key merits of these new architectures are twofold. First, these models can generate outputs recursively like RNNs, meaning that they only need to refer to the single previous state during decoding. It makes the decoding process more efficient as it eliminates the need to revisit all previous states as in conventional Transformers. Second, these models have the capacity to encode an entire sentence in parallel like Transformers. This contrasts with conventional RNNs which has to encode sentences on a token-by-token basis. Thus, they can benefit from the parallelism of GPUs with techniques such as Parallel Scan [253, 254], FFT [250, 251], and Chunkwise Recurrent [252]. These techniques enable models with these new architectures to be trained in a highly parallel and efficient manner.
> 传统的 Transformer 架构容易遭受二次计算复杂度的问题
> 一些研究致力于设计新的架构用于语言建模，提高效率，包括了参数化的状态空间模型、长卷积、类 Transformer 架构但包含了回归式更新机制的模型
> 这些新架构的优势有两点：首先，它们回归式生成输出，如同 RNNs，因此在解码时仅需要注意单个的前一个状态，不需要回顾前面所有的状态；其次，这些模型可以并行地编码整个句子，如同传统 Transformer，这与传统 RNN 不同，传统 RNN 需要 token-by-token 编码整个句子，因此可以获益于 GPU 的并行性质
### 4.2.2 Detailed Configuration
![[A Survey of LLMs-Table6.png]]
Since the launch of Transformer [22], various improvements have been proposed to enhance its training stability, performance, and computational efficiency. In this part, we will discuss the corresponding configurations for four major parts of the Transformer, including normalization, position embeddings, activation functions, and attention and bias. To make this survey more self-contained, we present the detailed formulations for these configurations in Table 6.
> 本节讨论 Transformer 的四个主要部分对应的配置，包括 normalization，位置编码，激活函数以及 attention 和偏置

**Normalization Methods.** Training instability is a challenging issue for pre-training LLMs. To alleviate this issue, normalization is a widely adopted strategy to stabilize the training of neural networks. In the vanilla Transformer [22], LayerNorm [256] is employed. Recently, several advanced normalization techniques have been proposed as alternatives to LayerNorm, e.g., RMSNorm, and DeepNorm.
> 为了解决 LLM 预训练的训练不稳定性问题，normalization 被广泛使用
> 传统 Transformer 中使用的是 LayerNorm，最近提出了一些替代的方案
-  *LayerNorm.* In the early research, BatchNorm [265] is a commonly used normalization method. However, it is difficult to deal with sequence data of variable lengths and small-batch data. Thus, LayerNorm [256] is introduced to conduct layerwise normalization. Specifically, the mean and variance over all activations per layer are calculated to recenter and re-scale the activations.
> 早期研究中 batchnorm 使用较多，但 batchnorm 对于变长的序列数据以及小 batch 的数据不好处理
> layernorm 被提出用于进行层级别的规范化，即每个 layer 的所有激活的方差和均值会被计算，然后用于重中心化和重缩放所有的激活
-  *RMSNorm.* To improve the training speed of LayerNorm (LN), RMSNorm [257] is proposed by re-scaling the activations with only the root mean square (RMS) of the summed activations, instead of the mean and variance. Related research has demonstrated its superiority in training speed and performance on Transformer [266]. Representative models that adopt RMSNorm include Gopher [64] and Chinchilla [34].
> RMSNorm 为了提高 LN 的训练速度，提出仅使用激活的和的均方根进行重缩放，而非均值和方差
> 相关研究表明它的 Transformer 的训练速度和性能都更好
-  *DeepNorm.* DeepNorm is proposed by Microsoft [258] to stabilize the training of deep Transformers. With DeepNorm as residual connections, Transformers can be scaled up to 1,000 layers [258], which has shown the advantages of stability and good performance. It has been adopted by GLM-130B [93].
> DeepNorm 的目的在于稳定深度 Transformers 的训练，Transformer 以 DeepNorm 作为残差连接时，它可以扩展到1000层以上
> GLM-130B 采用了 DeepNorm

**Normalization Position.** In addition to the normalization method, normalization position also plays a crucial role in the LLMs. There are generally three choices for the normalization position, i.e., post-LN, pre-LN, and sandwich-LN.
> 有三种规范化位置的选择：post-LN、pre-LN 和 sandwich-LN

-  Post-LN. Post-LN is used in the vanilla Transformer [22], which is placed between residual blocks. However, existing work has found that the training of Transformers with post-LN tends to be instable due to the large gradients near the output layer [267]. Thus, post-LN is rarely employed in existing LLMs except combined with other strategies (e.g., combining post-LN with pre-LN in GLM- 130B [93]).
> 传统 Transformer 使用的是 Post-LN，即 layernorm 层位于两个残差块之间
> 现存的工作发现使用 post-LN 训练会趋于不稳定，因为在接近输出层时会有更大的梯度
> 现存 LLMs 中较少有采用 post-LN 的，除非与其他技巧结合

- Pre-LN. Different from post-LN, pre-LN [268] is applied before each sub-layer, and an additional LN is placed before the final prediction. Compared with post-LN, the Transformers with pre-LN are more stable in training. However, it performs worse than the variants with post-LN [269]. Despite the decreasing performance, most LLMs still adopt pre-LN due to the training stability. However, one exception is that pre-LN has been found unstable in GLM when training models more than 100B parameters [93].
> pre-LN 在每个 sub-layer 之前应用，此外在最后的预测之前还有一次额外的 LN
> 使用 pre-LN 的 Transformer 在训练时更加稳定，但它的效果比 post-LN 的变体更差，但大多数 LLMs 为了训练稳定性仍然采用 pre-LN
> 一个例外是在 GLM 训练超过千参数模型时，发现 pre-LN 也变得不稳定

- Sandwich-LN. Based on pre-LN, Sandwich-LN [255] adds extra LN before the residual connections to avoid the value explosion issues in Transformer layer outputs. However, it has been found that Sandwich-LN sometimes fails to stabilize the training of LLMs and may lead to the collapse of training [93].
> Sandwich-LN 基于 pre-LN，额外再残差连接之前添加了 LN 层，避免 Transformer 层输出的数值爆炸问题
> 但有人发现 Sandwich-LN 优势也难以稳定 LLMs 的训练，导致训练崩溃

**Activation Functions.** To obtain good performance, activation functions also need to be properly set in feed-forward networks. In existing LLMs, GeLU activations [270] are widely used. Specially, in the latest LLMs (e.g., PaLM and LaMDA), variants of GLU activation [262, 271] have also been utilized, especially the SwiGLU and GeGLU variants, which often achieve better performance in practice [266]. However, compared with GeLU, they require extra parameters (about 50%) in the feed-forward networks [272].
> 关于 FFN 层的激活函数
> 现存的 LLMs 广泛使用 GeLU 激活，最新的 LLMs 中也有使用了 GLU 激活的变体，在实践中的效果常常更好，但相较于 GeLU，它们需要更多的参数

**Position Embeddings.** Since the self-attention modules in Transformer are permutation equivariant, position embeddings (PE) are employed to inject absolute or relative position information for modeling sequences.
> 由于 Transformer 中的自注意力模块是排列不变的，因此 Transformer 采用了位置编码来向序列中注入绝对的或相对的位置信息

- *Absolute position embedding.* In the vanilla Transformer [22], absolute position embeddings are employed. At the bottoms of the encoder and the decoder, the absolute positional embeddings are added to the input embeddings. There are two variants of absolute position embeddings proposed in the vanilla Transformer [22], i.e., sinusoidal and learned position embeddings, where the latter is commonly used in existing pre-trained language models.
> 绝对位置嵌入
> 传统 Transformer 中采用绝对位置编码，在 encoder 和 decoder 的底部为输入嵌入加上了绝对位置编码
> 传统 Transformer 位置编码有使用 sin 函数的和学习到的两种变体，现存 LLM 常用学习到的位置编码

- *Relative position embedding.* Unlike absolute position embeddings, relative positional embeddings are generated according to the offsets between keys and queries [273]. A popular variant of relative PE was introduced in Transformer-XL [274, 275]. The calculation of attention scores between keys and queries has been modified to introduce learnable embeddings corresponding to relative positions. T5 [82] further simplified relative positional embeddings, which was subsequently adopted by Gopher [64]. Specifically, it adds learnable scalars to the attention scores, where the scalars are calculated based on the distances between the positions of the query and the key. Compared with the absolute PE, Transformers with relative position embedding can generalize to sequences longer than those sequences for training, i.e., extrapolation [264].
> 相对位置嵌入
> 相对位置嵌入根据 keys 和 queries 之间的 offset 生成
> Transformer-XL 中，keys 和 queries 之间的 attention 分数计算被进行了修改，以引入了对应于相对位置的可以学习的嵌入
> T5对相对位置嵌入进行了进一步简化，它为 attention score 添加了可学习的常量，该常量基于 query 和 key 之间的位置的距离进行计算
> 相较于绝对 PE，使用相对 PE 的 Transformers 可以泛化到比训练序列更长的序列，即外推 (extrapolation)
- *Rotary Position Embedding.* Rotary position embedding (RoPE) [263] sets specific rotatory matrices based on the absolute position of each key or query. The scores between keys and queries can be computed with relative position information (Table 6). RoPE combines each consecutive pair of elements in query and key vectors as a dimension, so there are $d/2$ dimensions for an original d-length embedding. For each dimension $i\in\{1,\dots, d/2\}$ the pair of involved elements will rotate based on the rotation angle $t\cdot \theta_i$, where t denotes the position index and $\theta_i$ is the basis in the dimension. Following sinusoidal position embeddings [22], RoPE defines the basis $\theta_i$ as an exponentiation of the *base b* (set to 10000 by default):
$$
\Theta = \{\theta_i = b^{-2(i-1)/d} | i\in\{1,2,\dots,d/2\}\}\tag{4}
$$
    Furthermore, a recent study [276] defines the distance required to rotate one cycle (2 $\pi$) for each dimension as wavelength:
$$
\lambda_i = 2\pi b^{2(i-1)/d} = 2\pi/\theta_i\tag{5}
$$
    Due to the excellent performance and the long-term decay property, RoPE is widely adopted in the latest LLMs, e.g., PaLM [56] and LLaMA [57]. Based on RoPE, xPos [277] further improves the translation invariance and length extrapolation of Transformer. At each dimension of the rotation angle vector, xPos adds a special exponential decay that is smaller when the basis is larger. It can alleviate the unstable phenomenon during training as the distance increases.
> 旋转位置嵌入基于每个 key 或 query 的绝对位置设定特定的旋转矩阵，key 和 query 之间的分数可以通过相对位置信息进行计算
> 旋转位置嵌入将 query 和 key 向量中的各个连续的元素对结合为一个维度，因此相对于原先 d 维度的嵌入，它的维度仅有 d/2 ，其中每一维度的元素对将基于旋转角度 $t\cdot \theta_i$ 旋转，其中 $t$ 是位置索引，$\theta_i$ 为该维度的基
> RoPE 被最新的 LLMs 广泛使用
> xPos 添加了一个特殊的指数衰减项，当基更大，它就更小，它可以减缓距离增大时在训练中导致的不稳定现象

-  *ALiBi.* ALiBi [264] is proposed to improve the extrapolation of Transformer. Similar to relative position embedding, it biases attention scores with a penalty based on the distances between keys and queries. Different from the relative positional embedding methods like T5 [82], the penalty scores in ALiBi are pre-defined without any trainable parameters. Empirical results in [264] have shown that ALiBi has a better extrapolation performance on sequences that are longer than those for training than several popular position embedding methods such as sinusoidal PE [22], RoPE [263], and T5 bias [82]. In addition, it has been shown that ALiBi can also improve training stability in BLOOM [78].
> ALiBi 意图提高 Transformer 的外推能力，和相关的 PE 类似，它对 attention score 进行偏置，使用了一个基于 key 和 query 之间距离的惩罚
> 和相关 PE 不同的是，它的惩罚分数是预定义的，没有可训练的参数，经验性结果说明了 ALiBi 相较于其他 PE，对于比训练文本长的序列有更好的外推能力

**Attention.** Attention mechanism is a critical component of Transformer. It allows the tokens across the sequence to interact with each other and compute the representations of the input and output sequence.
> Attention 技术允许序列内的 tokens 相互交互，计算输入和输出序列的表示

- *Full attention.* In the vanilla Transformer [22], the attention mechanism is conducted in a pairwise way, considering the relations between all token pairs in a sequence. It adopts scaled dot-product attention, in which the hidden states are mapped into queries, keys, and values. Additionally, Transformer uses multi-head attention instead of single attention, projecting the queries, keys, and values with different projections in different heads. The concatenation of the output of each head is taken as the final output.
> 经典 Tranformer 中，attention 成对地计算，考虑了序列内的所有 token 对
> 它采用了放缩的点积 attention，其中，隐藏状态被映射为 query，key 和 value
> Transformer 还用了多头注意力，在不同的头中对 query，key 和 value 有不同的映射，每个头的输出被拼接为最终输出

- *Sparse attention.* A crucial challenge of full attention is the quadratic computational complexity, which becomes a burden when dealing with long sequences. Therefore, various efficient Transformer variants are proposed to reduce the computational complexity of the attention mechanism [278, 279]. For instance, locally banded sparse attention (i.e., Factorized Attention [280] has been adopted in GPT- 3 [55]. Instead of the whole sequence, each query can only attend to a subset of tokens based on the positions.
> GPT-3采用了局部的稀疏注意力，即分解注意力，其中每个 query 只能基于其位置 attend to 一个 tokens 子集

- *Multi-query/grouped-query attention.* Multi-query attention refers to the attention variant where different heads share the same linear transformation matrices on the keys and values [281]. It achieves higher inference speed with only a minor sacrifice in model quality. Representative models with multi-query attention include PaLM [56] and StarCoder [98]. To make a trade-off between multi-query attention and multi-head attention, grouped-query attention (GQA) [282] has been explored. In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices. Specially, GQA has been adopted and empirically tested in the recently released LLaMA 2 model [99].
> 多 query 注意力指不同的头共享对于 key 和 value 的线性变化矩阵，它在少量牺牲模型质量的情况下达到了更快的推理速度
> grouped-query 注意力是多 query 和多 head 注意力之间的权衡，其中 heads 被分为不同的 group，相同 group 的 head 共享相同的变换矩阵

- *FlashAttention.* Different from most existing approximate attention methods that trade-off model quality to improve the computing efficiency, FlashAttention [283] proposes to optimize the speed and memory consumption of attention modules on GPUs from an IO-aware perspective. There exist different levels of memory on modern GPUs, e.g., SRAM with a fast IO and HBM with a relatively slow IO. FlashAttention organizes the input into blocks and introduces necessary recomputation, both to make better use of the fast memory SRAM. Implemented as a fused kernel in CUDA, FlashAttention has been integrated into PyTorch [197], DeepSpeed [74], and Megatron-LM [75]. The updated version FlashAttention-2 [284] further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2× speedup when compared to the original FlashAttention.
> FlashAttention 从 IO 角度提出优化 GPUs 中 attention 模块的速度和存储消耗，FlashAttention 将输入组织为块，引入必要的重计算，以更好利用 SRAM
> FlashAttention2 进一步优化了 GPU thread block 和 warp 的工作划分，相较于第一版的速度提升了一倍

- *PagedAttention.* It has been observed when LLM are deployed on servers, GPU memory is largely occupied by cached attention key and value tensors (called KV cache). The major reason is that the input lengths are often varied, leading to fragmentation and over-reservation issues. Inspired by the classic paging technique in operating systems, PagedAttention has been proposed to improve the memory efficiency and throughput of deployed LLMs [285]. In detail, PagedAttention partitions each sequence into subsequences, and the corresponding KV caches of these subsequences are allocated into non-contiguous physical blocks. The paging technique increases the GPU utilization and enables efficient memory sharing in parallel sampling.
> LLM 部署在服务器上时，GPU 存储会大量被 cached attention key 和 value tensor 占据，即被 KV cache 占据，原因是输入长度常常会变化，因此导致 fragmentation 和 over-reservation 的问题
> PagedAttention 被提出以提高部署的 LLMs 的存储效率和吞吐，它将序列划分为子序列，子序列对应的 KV cache 被分配在物理不连续的块中
> 分页机制提高了 GPU 利用率，使得并行采样中的存储共享更加高效

To put all these discussions together, we summarize the suggestions from existing literature for detailed configuration. For stronger generalization and training stability, it is suggested to choose the pre RMSNorm for layer normalization, and SwiGLU or GeGLU as the activation function. In addition, LN may not be used immediately after embedding layers, which is likely to incur performance degradation. As for position embeddings, RoPE or ALiBi is a better choice since it performs better on long sequences.
> 小结
> 为了更强的泛化性能和更好的训练稳定性，建议使用 pre RMSNorm 进行 layernorm，使用 SwiGLU 或 GeGLU 作为激活函数
> 另外 LN 可以不在嵌入层之后立即使用，以避免性能下降
> RoPE 和 ALiBi 是更好的位置嵌入的选择，它们对于长序列表现更好
### 4.2.3 Pre-training Tasks
Pre-training plays a key role that encodes general knowledge from large-scale corpus into the massive model parameters. For training LLMs, there are two commonly used pretraining tasks, namely language modeling and denoising autoencoding.
> LLMs 有两种常用的预训练任务，即语言建模和去噪自动编码

**Language Modeling.** The language modeling task (LM) is the most commonly used objective to pre-train decoder-only LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of tokens $\mathbf x = \{x_1,\dots, x_n\}$, the LM task aims to autoregressively predict the target tokens $x_i$ based on the preceding tokens $x_i$ in a sequence. A general training objective is to maximize the following likelihood:
$$
\mathcal L_{LM}(\mathbf x) = \sum_{i=1}^n \log P(x_i | \mathbf x_{<i})\tag{6}
$$
> 语言建模任务是最常用的用于预训练 decoder-only LLMs 的目标，例如 GPT-3
> 语言建模的目标是根据给定 token 序列自回归预测目标 token，通用的训练目标是最大化似然的和

Since most language tasks can be cast as the prediction problem based on the input, these decoder-only LLMs might be potentially advantageous to implicitly learn how to accomplish these tasks in a unified LM way. Some studies have also revealed that decoder-only LLMs can be naturally transferred to certain tasks by autoregressively predicting the next tokens [26, 55], without fine-tuning. An important variant of LM is the prefix language modeling task, which is designed for pre-training models with the prefix decoder architecture. The tokens within a randomly selected prefix would not be used in computing the loss of prefix language modeling. With the same amount of tokens seen during pretraining, prefix language modeling performs slightly worse than language modeling, since fewer tokens in the sequence are involved for model pre-training [29].
> 由于大多数语言任务可以被转化为基于输入的预测问题，因此这些 decoder-only 的 LLMs 可能潜在地对于隐式地用一种联合的语言建模的方式学习如何完成这些任务
> 一些研究揭示了 decoder-only 的 LLM 可以自然地通过自回归预测下一个 token 以迁移到特定的任务，而且是在没有进行微调的情况下
> 语言建模的一个重要变体就是前缀语言建模任务，该任务为预训练前缀 decoder 架构而设计，在该任务中，在一个随机选取的前缀序列中的 token 将不会被用于计算损失
> 因此，在预训练时见到的 token 数量相同的情况下，前缀语言建模任务预训练的模型的性能会略微弱于常规语言建模预训练的模型，因为在模型预训练的过程中序列中有更少的 token 进行参与

**Denoising Autoencoding.** In addition to conventional LM, the denoising autoencoding task (DAE) has also been widely used to pre-train language models [24, 82]. The inputs $\mathbf x_{\backslash \tilde{x}}$ for DAE task are corrupted text with randomly replaced spans. Then, the language models are trained to recover the replaced tokens $\tilde {\mathbf x}$.
> 去噪自动编码任务也被广泛应用于预训练语言模型，DAE 任务的输入 $\mathbf x_{\backslash {\tilde x}}$ 是被污染的文本，其中的一些片段被随机替换，语言模型的训练目标是恢复这些被替换的 tokens $\tilde {\mathbf x}$

Formally, the training objective of DAE is denoted as follows:
$$
\mathcal L_{DAE}(\mathbf x) = \log P(\tilde {\mathbf x}|\mathbf x_{\backslash \tilde x})\tag{7}
$$
> DAE 的训练目标同样是极大似然

However, the DAE task seems to be more complicated in implementation than LM task. As a result, it has not been widely used to pre-train large language models. Existing LLMs that take DAE as pre-training objectives include T5 [82] and GLM-130B [93]. These models are mainly trained to recover the replaced spans in an autoregressive way.
> DAE 任务相较于 LM 任务在实现上更为复杂，使用 DAE 作为预训练目标的模型主要的训练任务是以自回归的方式恢复被替换的片段

**Mixture-of-Denoisers.** Mixture-of-Denoisers (MoD) [89], also known as UL2 loss, was introduced as a unified objective for pre-training language models. MoD regards both LM and DAE objectives as different types of denoising tasks, namely S-denoiser (LM), R-denoiser (DAE, short span and low corruption), and X-denoiser (DAE, long span or high corruption). Among the three denoising tasks, S-denoiser is similar to the conventional LM objective (Equation (6)), while R-denoiser and X-denoiser are similar to DAE objectives (Equation (7)) but differ from each other in the lengths of spans and ratio of corrupted text. For input sentences started with different special tokens (i.e., $\{[R], [S], [X]\}$), the model will be optimized using the corresponding denoisers. MoD has been applied in the latest PaLM 2 model [120].
> 混合的去噪器，或者说 UL2损失，也被用作预训练语言模型的一个联合的目标
> MoD 将 LM 和 DAE 都视作去噪任务的不同类型，对于以不同的特殊 tokens 起始的输入句子，模型就将使用对应的去噪方式对其进行优化
### 4.2.4 Long Context Modeling
In real applications, there is an increasing demand for long context modeling capacities of LLMs, such as PDF processing and story writing [286]. Many closed-source LLMs provide professional support for long text processing. For instance, OpenAI releases GPT-4 Turbo with a 128K context window, and Anthropic releases Claude 2.1 with a 200K context window. To enhance the long context modeling abilities, there are generally two feasible directions, namely scaling position embeddings and adapting context window. Next, we introduce the two parts in detail.
> 实际中存在对于长上下文建模的需求，对于强化长上下文建模，目前有两个可行的方向，即放缩位置编码以及调节上下文窗口，本节对其进行介绍

**Scaling Position Embeddings.** Transformer-based LLMs can learn effective position embeddings within the maximum training length. Thus, when adapting LLMs to language tasks beyond the maximum training length, it is necessary to scale to larger position indices. Some specific position embeddings have been shown to possess a certain degree of ability to generalize to text beyond the training length, which is formally termed extrapolation capability, including T5 bias [82], ALiBi [264], xPos [277] and even NoPE [287]. However, as one of the mainstream position embedding methods, RoPE exhibits limited extrapolation ability in empirical studies [240]. In the following, we discuss several methods that can scale RoPE to longer texts.
> 基于 Transformer 的 LLM 可以在最大的训练长度下学习高效的位置编码，因此当要调节 LLM 至超过了最大训练长度的语言任务时，就有必要扩展至更大的位置索引
> 一些特定的位置编码有一定能力可以泛化到可以超过训练长度的文本，这个能力被称为外推能力
> 而作为主流的位置编码，RoPE 的外推能>力在经验学习中有限
> 我们将讨论几个方法将 RoPE 扩展到更长的文本

- *Direct model fine-tuning.* To adapt LLMs to a long context window, a straightforward approach is to directly finetune the models on long texts with the desired length. The context extension can be scheduled with increased lengths in a multi-stage approach (e.g., 2K → 8K → 32K). To conduct effective extension, it needs specially prepared long texts for training. Specially, some recent study has shown that the quality is more important than the lengths of training text in long context models [288]. However, a recent study has highlighted that the fine-tuning approach tends to be inherently slow when adapting LLMs for long texts [240].
> 直接的模型微调
> 即直接用更长的文本微调模型，可以多阶段调度逐渐增大上下文长度
> 这需要特别准备的长文本语料，另外，一些研究表明训练长上下文模型时，语料质量比文本长度更重要
> 最近的研究发现微调使得模型对长文本适应的速度是较慢的

- *Position interpolation.* This method downscales the position indices within the original context window, to avoid out-of-distribution rotation angles during pre-training [240, 289]. To be more specific, this approach multiplies all position indices by a coefficient $L/L'(L<L')$, where $L$ and $L'$ represent the original and target context window length, respectively. Experimental results [240] have shown that this method can extend the context window effectively and efficiently, compared to the above approach of direct model fine-tuning. However, it is worth noting that this technique may have an adverse impact on the model’s performance when handling shorter texts[240, 290].
> 位置插值方法
> 该方法对原始的上下文窗口的位置索引进行下采样，以避免预训练时的分布外旋转角，具体地说，该法对所有位置索引乘上系数 $L/L' (L<L')$，其中 $L, L'$ 表示原始和目标窗口长度
> 实验结果表明该方法相较于直接微调，可以有效拓展上下文窗口，但可能对于 LLM 处理小文本的表现有反作用

- *Position truncation.* To mitigate the challenges posed by out-of-distribution rotation angles, another practical approach is to truncate longer relative positions to satisfy the requirement of the maximum training length. Specifically, ReRoPE and LeakyReRoPE [291] introduce a pre-defined window length, which is smaller than the maximum training length. Position indices within this pre-defined window are retained, while those indices beyond the window are either truncated to the pre-defined window length or interpolated to align with the maximum training length. This strategy can reserve local position relationships and enhance the extrapolation capacity. However, this approach needs to compute the attention matrices twice, accommodating additional computational budget.
> 位置截断
> 同样为了缓解分布外旋转角的问题，该方法对更长的相对位置进行截断以满足最大训练长度的要求，具体地说，它们引入预定义的窗口长度，其小于最大训练长度，在该窗口内的位置索引保持，之外的索引则被截断到预定义的窗口长度，或者被插值以和最大训练长度对齐
> 该策略保持了局部位置关系，强化了外推能力，但需要计算两次 attention 矩阵，有额外计算开销

- *Base modification.* LLMs are usually trained with a preset maximum training length, e.g., 4096 in Llama 2 [99]. However, wavelengths in certain dimensions of RoPE may exceed the training length for longer text [276], so that language models have not undergone sufficient training (i.e., a complete rotation cycle) on these dimensions. Thus, when we adapt LLMs to longer texts, the rotation angles for certain dimensions would be never seen in the training phase [292]. Given a fixed rotation angle $t\cdot \theta_i$, a smaller basis $\theta_i$ allows for a greater distance $t$, i.e., enabling the modeling of longer texts [235, 276, 288]. According to the formula $\theta_i = b^{-2 (i-1)/d}$ in Equation 4, decreasing the basis can be achieved by increasing the value of the base. In addition, decreasing the base can also help re-scale the wavelengths of all dimensions below the training length, while it often needs continual pre-training to adapt the LLMs to long context windows [292]. A recent study [292] has empirically compared these two base modification methods, and shown that decreasing the base demonstrates a better extrapolation capacity beyond the training length, while increasing the base performs better within the training length.
> 基数修改
> LLM 一般以预定义的最大训练长度训练，如4096，但 RoPE 特定维度的波长可能超过对于更长文本的训练长度，因此模型对于这些维度并没进行有效训练
> 为了将 LLM 适应至更长文本，对于特定维度的旋转角需要在训练过程中不可见
> 给定固定的旋转角 $t\cdot \theta_i$，更小的基 $\theta_i$ 允许更大的距离 $t$，即可以建模更长的文本，要降低 $\theta_i$，可以通过增大基数 $b$ 实现
> 但是，更小的基数可以帮助重放缩低于训练长度的所有维度的波长，但一般需要继续预训练使得 LLM 适应更长的上下文窗口
> 最近的研究表明，降低基数会对于超过训练长度更好的插值表现，增大基础在训练长度内表现更好

- *Basis truncation.* Similar to the base modification, the truncation of the basis also concentrates on dealing with the singular dimensions with wavelengths exceeding the training length [293]. According to the definition $\lambda_i = 2\pi/\theta_i$ in Equation 5, the dimension with a large wavelength $\lambda_i$ has a small basis $\theta_i$ accordingly. Based on this observation, this approach first defines a basis range $[a, c]$. Given the basis range, the value of basis is modified according to the following ways: (1) when $\theta_i \ge c$, the value is retained, (2) when $\theta_i \le a$, the value is set to zero, and (3) when $a<\theta_i<c$, the value is truncated to a fixed small value. Via basis truncation, the out-of-distribution rotation angles can be avoided at larger position indices. However, this approach does not perform very well at long context tasks [293].
> 基截断
> 同样为了解决单个维度的波长超过训练长度的问题
> 某个维度的波长 $\lambda_i$ 越大，其基 $\theta_i$ 越小，该方法构建定义基的范围 $[a, c]$，给定范围，对应修改基的值，若 $\theta_i$ 过高，保持，过低，变为0，在范围内，则截断为固定的小值
> 该方法可以避免在更大的位置索引上出现分布外旋转角，但在长上下文任务上表现不好

**Adapting Context Window.** Since Transformer-based LLMs have limited context windows, they can not directly integrate or utilize the entire information of the long sequences exceeding the context window. To alleviate the limitation, several methods adapting LLMs to long context have been proposed, as discussed below.
> 基于 Transformer 的 LLM 限制了上下文窗口，故难以直接利用超过窗口长度的序列的完整信息，一些方法被提出以使 LLM 适应长上下文

- *Parallel context window.* Inspired by fusion-indecoder [294], parallel context window methods [295, 296] adopt a divide-and-conquer strategy to process input text. Specially, it divides the input text into multiple segments, each independently encoded with shared position embeddings. In the generation stage, the attention masks are modified to make that subsequent tokens can access to previous tokens in each segment. Nevertheless, this method cannot distinguish the order of different segments, constraining the model capacity on certain tasks.
> 并行上下文窗口
> 采用分治思想处理输入文本，将输入文本划分为块，各个块内部用共享的位置嵌入编码
> 在生成阶段，mask 被修改，使得块内的随后的 token 可以访问每个块前面部分的 tokens
> 该方法不能区分不同 segment 的顺序，模型的能力限制于特定任务

- *$\Lambda$ -shaped context window.* Some prior work has revealed that LLMs tend to allocate greater attention weights to the starting and nearest tokens among all previous tokens [297, 298], so called the “lost in the middle” phenomenon [299]. Based on this observation, LM-Infinite [300] and StreamingLLM [298] propose to employ a “$\Lambda$ -shaped” attention mask, which selectively preserves the initial tokens and the nearest tokens that each query can attend to and then discards any tokens beyond this scope. Experiments demonstrate that this method can facilitate extra-long text generation with a fixed memory [298]. However, it may struggle to model the long-range dependency in prompts, since it cannot effectively utilize the information from the discarded tokens [298].
> $\Lambda$ 形状上下文窗口
> 一些工作表明 LLM 趋向于为开始的和最近的 token 分配更大的 attention 权重，称为“lost in the middle” 现象
> $\Lambda$ -shaped attention mask 选择性保持每个 query 可以注意的起始 tokens 和最近 tokens，丢弃其他 tokens，实验表明该方法可以促进在固定记忆的情况下的更长文本的生成
> 但不好处理 prompts 中的长距离依赖，因为无法利用丢弃的 tokens 中的信息

- *External memory.* It has been shown that a relatively small subset of tokens can effectively capture the majority of attention patterns in a Transformer [301], i.e., the topk attention keys can well approximate the original full attention. Therefore, a number of studies propose to store the past keys in external memory and utilize a k-NN search method to retrieve the k most relevant tokens for generation [238, 301, 302]. For a decoder model, it typically employs one certain layer to access these top-k external tokens, while still adopts the normal context window in the rest layers [238, 302].
> 外部记忆
> 研究表明相对较小的一个 tokens 子集就可以高效捕获大部分的 attention pattern，换句话说就是 topk attention keys 可以近似完全的 attention
> 一些研究提出将过去的 keys 存于外部记忆，利用 k-NN 检索 k 个最相关的 token 用于生成
> 在 decoder 模型中，该方法直接实现为一层，访问外部的 topk tokens，而在其余层仍然采用正常的上下文窗口

In addition to the studies based on vanilla Transformer, there are a surge of Transformer variants with efficient attentions and other efficient architectures, aiming to alleviate high computational cost for modeling long texts. These studies have been extensively discussed in Section 4.2.1 and Section 4.2.2. Furthermore, context compression and prompting techniques (e.g., iterative reasoning [303]) have also been proven to be a viable strategy for handling long text tasks [303–306], without the need of model adaption.
> 上下文压缩和迭代式推理的 prompt 技巧被证明是在不需要模型适应的情况下，帮助处理长文本任务的有效策略
### 4.2.5 Decoding Strategy
After the LLMs have been pre-trained, it is essential to employ a specific decoding strategy to generate the appropriate output from the LLMs.
> LLM 预训练好了之后，需要采用特定的解码策略让 LLM 生成合适的输出

**Background.** We start the discussion with the prevalent decoder-only architecture, and introduce the auto-regressive decoding mechanism. 
> 我们介绍 decoder-only 架构中的自回归 decoding 机制

Since such LLMs are pre-trained based on the language modeling task (Equation 6), a basic decoding method is greedy search that predicts the most likely token at each step based on the previously generated tokens, formally modeled as:
$$
x_i = \arg\max_x P(x|\mathbf x_{<i})\tag{8}
$$
> decoder-only 的 LLM 预训练于语言建模任务，因此一个基本的 decoding 方法就是贪心搜索，在每一步，根据之前生成的 tokens，预测下一个最可能的 token

where $x_i$ is the token with the highest probability at ith step of generation conditioned on the context $x<i$. For instance in Figure 10, when predicting the next token of the sentence “I am sleepy. I start a pot of”, greedy search selects the token “coffee” which has the highest probability at the current step. 

Greedy search can achieve satisfactory results in text generation tasks (e.g., machine translation and text summarization), in which the output is highly dependent on the input [307]. However, in terms of openended generation tasks (e.g., story generation and dialog), greedy search sometimes tends to generate awkward and repetitive sentences [308].
> 贪心搜索可以在文本生成任务，例如机器翻译和文本总结中达到满意的结果，这些任务中，输出高度依赖于输入
> 但对于开放式的任务，例如故事生成和对话，贪心搜索有时趋向于生成古怪和重复的句子

As another alternative decoding strategy, samplingbased methods are proposed to randomly select the next token based on the probability distribution to enhance the randomness and diversity during generation:
> 基于采样的方法
> 该方法基于概率分布随机选取下一个 token，强化生成的随机性和多样性，也就是不是贪心选取了

$$
x_i\sim P(x|\mathbf x_{<i})\tag{9}
$$
For the example in Figure 10, sampling-based methods will sample the word “coffee” with higher probability while also retaining the possibilities of selecting the rest words, “water”, “tea”, “rice”, etc.
![[A Survey of LLMs-Fig10.png]]

Not limited to the decoder-only architecture, these two decoding methods can be generally applied to encoderdecoder models and prefix decoder models in a similar way.
> 这两种解码方法也可以应用于 encoder-decoder 模型和 prefix decoder 模型

**Improvement for Greedy Search.** Selecting the token with the highest probability at each step may result in overlooking a sentence with a higher overall probability but a lower local estimation. Next, we introduce several improvement strategies to alleviate this issue.
> 对贪心搜索的改进
> 在每一步选择最高概率的 token 可能导致忽视了总体有更高概率，但局部估计较低的句子，我们介绍几个策略缓解这个问题

- *Beam search.* Beam search [309] retains the sentences with the n (beam size) highest probabilities at each step during the decoding process, and finally selects the generated response with the top probability. Typically, the beam size is configured within the range of 3 to 6. However, opting for a larger beam size might result in a decline in performance [310].
> beam search
> 在每一步保留 n 个最高概率的 token，最后选择具有最高概率的 response
> beam size 一般在 $[3,6]$
> beam size 过大可能导致性能下降

- *Length penalty.* Since beam search favours shorter sentences, imposing length penalty (a.k.a., length normalization) is a commonly used technique [311] to overcome this issue, which normalizes the sentence probability according to the sentence length (divided by an exponential power $\alpha$ of the length).
> length penalty
> beam search 偏好更短的句子，解决该问题的一个常用方法是施加长度惩罚/长度规范化，它根据句子长度规范化句子概率，即除去长度的一个指数幂

Besides, some researchers [312] propose to penalize the generation of previously generated tokens or n-grams to alleviate the issue of repetitive generation. In addition, diverse beam search [313] can be leveraged to produce a set of diverse outputs based on the same input.
> 有人提出惩罚先前生成的 tokens 或 n-grams，以缓解重复生成的问题
> 另外，diverse beam search 可以用于基于相同输入生成 diverse 的输出

**Improvement for Random Sampling.** Sampling-based methods sample the token over the whole vocabulary, which may select wrong or irrelevant tokens (e.g., “happy” and “Boh” in Figure 10) based on the context. To improve the generation quality, several strategies have been proposed for mitigating or preventing the selection of words with exceedingly low probabilities.
> 对随机采样的改进
> 基于采样的方法在整个词袋上采样，可能采样到错误的 token，一些策略用于缓解或避免选择到概率过于低的 token

- *Temperature sampling.* To modulate the randomness of sampling, a practical method is to adjust the temperature coefficient of the softmax function for computing the probability of the j-th token over the vocabulary:
> 温度采样
> 为了调节采样的随机性，一个方法是调节 softmax 在词袋上计算第 $j$ 个 token 的概率的 temperature 系数

$$
P(x_j|\mathbf x_{<i}) = \frac {\exp(l_j/t)}{\sum_{j'}\exp(l_{j'}/t)}\tag{10}
$$
where $l_{j'}$ is the logits of each word and $t$ is the temperature coefficient. Reducing the temperature t increases the chance of selecting words with high probabilities while decreases the chances of selecting words with low probabilities. When t is set to 1, it becomes the default random sampling; when t is approaching 0, it is equivalent to greedy search. In addition, when t goes to infinity, it degenerates to uniform sampling.
> 其中 $l_{j'}$ 是每个词的对数几率 (即模型对于 token 的原始分数值)，$t$ 是温度系数
> 减少 $t$ 会提高选中高概率的词的概率，降低选中低概率的词的概率，当 $t=1$，等价于原来的随机采样，当 $t=0$，等价于贪心搜索，当 $t$ 趋近于无穷，等价于均匀采样
> $t$ 越大，token 的原始分数之间的差异越小，概率分布越加趋向于均匀
> $t$ 越小，token 的原始分数之间的差异越大，概率分布越加趋向于确定

- *Top-k sampling.* Different from temperature sampling, top-k sampling directly truncates the tokens with lower probability and only samples from the tokens with the top k highest probabilities [314]. For example in Figure 10, top- 5 sampling will sample from the words “coffee”, “water”, “tea”, “rice”, and “chai” from their re-scaled probabilities.
> Top-k 采样
> 和温度采样不同，top-k 采样直接将具有低概率的 tokens 截去，仅从前 k 个具有最高概率的 tokens 中采样

- *Top-p sampling.* Since top-k sampling does not consider the overall possibility distribution, a constant value of k may be not be suitable for different contexts. Therefore, top-p sampling (a.k.a., nucleus sampling) is proposed by sampling from the smallest set having a cumulative probability above (or equal to) p [308]. In practice, the smallest set can be constructed by gradually adding tokens from the vocabulary sorted in descending order of generative probability, until their cumulative value exceeds p.
> Top-p 采样
> top-k 采样并未考虑完整的概率分布，对于不同的上下文，k 保持常值可能不合适
> top-p 采样 (核采样) 从一个累计概率大于等于 p 的最小集合中采样，实践中，该最小集合通过逐渐从由概率排序的 tokens 中一个个加入构建

Recently, researchers have also explored other sampling strategies for LLMs. For instance, $\eta$ -sampling [315] further improves top-p sampling by introducing a dynamic threshold based on the probability distribution. Furthermore, contrastive search [316] and typical sampling [317] can be utilized to improve the generation coherence during decoding. Since it has been found that large models tend to assign higher probability to important tokens compared to small models, contrastive decoding [318] utilizes a larger LM (e.g., OPT- 13B) and a smaller LM (e.g., OPT-125M) to measure their log-likelihood differences. Subsequently, tokens are sampled based on the delta value of the probability distribution, thereby amplifying the impact of important tokens. Based on this contrastive idea, DoLa [319] further extends this approach to contrasting the logits across different layers of a single LLM, as higher layers tend to assign more weight to important tokens.
> 其他采样方法
> $\eta$ 采样引入基于概率分布的动态 threshold，进一步改进 top-p 采样
> 对比搜索和典型采样被用于提高 decoding 的生成一致性
> 有人发现相较于小模型，大模型更趋向于为重要的 token 分配更高的概率，故 contrastive decoding 利用了更大的模型和更小的模型来衡量它们的对数似然差异，随后，token 将基于概率分布的 delta 值被采样，因此该方法放大了重要 token 的影响
> 基于 contrastive 思想，DoLa 拓展了该法，将单个 LLM 中的不同层之间的 logits 进行对比，因为更高的层趋向于为重要的 tokens 分配更多的权重

>Memory Wall
>When generating a new token, the most timeconsuming steps revolve around data transfer and weight computation. A main issue is the significant amount of time overwhelmed by data transfer, often referred to as the memory wall issue.
>生成新 token 最耗时的步骤和数据传输和权重计算有关
>一个主要问题就是数据传输需要大量时间，该问题称为 memory wall 问题
>
>To address this issue, researchers formally quantify data transfer from GPU memory to GPU caches using the number of bytes in I/O, and they assess weight computation by measuring the number of FLOPs [320]. 
>Specifically, let $b$, $s$, $n$, $d$, and $h$ denote the batch size, sequence length, number of attention heads, hidden size of each head, and overall hidden size ($h = n\cdot d$), respectively. 
>During the layerwise multi-head self-attention calculation in causal decoder, the I/O bytes and FLOPs at each decoding step can be expressed as $8bsn + 4bsnd + 4bnd$ and $8bsnd$, respectively [320].
>在因果 decoder 的逐层多头注意力计算中，每一步 decoding 的 I/O bytes 是 $8bsn + 4bsnd + 4bnd$，每一步的 FLOPS 是 $8bsnd$
>
>*Arithmetic intensity* is further defined as the ratio of FLOPs to I/O bytes:

$$
\text{intensity} = \frac {\text{FLOPs}}{\text{I/O bytes}} = \frac {2}{1 + \frac 2 d + \frac 1 s}\tag{11}
$$

>Let’s consider LLaMA 13B ($d = 128$) with a sequence length of 1024 ($s = 1024$) as an example. The calculated arithmetic intensity is $1.97$. However, the A100 80G GPU can perform $312$ TFLOPs and transfer $2$ TB of data in one second, i.e., its ideal arithmetic intensity is $156$. This indicates that the bottleneck in attention calculation lies in the process of data transfer (i.e., excessive I/O loading).
> Attention 计算的算数密度过低，导致瓶颈在 IO 上

**Decoding Efficiency Issues.** In this part, we briefly analyze the decoding efficiency issues of LLMs. Overall, the decoding process of LLMs can be divided into two stages for overhead analysis: (1) the prefill stage, which computes the hidden states of the input sequence, and (2) the incremental decoding stage, which generates a token and updates hidden states in an auto-regressive manner [321]. As shown in the above memory wall box, the arithmetic intensity of the incremental decoding stage is only 1.97, which is far from the expected value of 156 (calculated according to the standard configuration of A100 80GB GPU). In contrast, the arithmetic intensity of the prefill stage achieves 113.78 for LLaMA-13B. Consequently, existing work mainly investigates how to enhance the efficiency of the incremental decoding algorithm, which can be categorized into two main approaches:
> 本部分介绍 LLMs 的解码效率问题
> 开销分析中，LLM 的 decoding 过程可以分为两阶段：prefill 阶段，该阶段计算输入序列的隐藏状态；incremental decoding 阶段，该阶段自回归生成 token，并更新隐藏层状态
> incremental decoding 阶段的算数密度仅有1.97，而 prefill 阶段的算数密度可以达到113.78
> 现存的工作主要研究如何优化 incremental decoding 阶段，可以分为两类

- *Reducing data transfer* mainly focuses on optimizing GPU memory access, thereby increasing the arithmetic intensity. As introduced in Section 4.2.2, KV cache can avoid redundant computation of previous tokens and PagedAttention allocates KV caches into continuous blocks to reduce memory fragmentation. Furthermore, Flash-Decoding [322] speeds up attention computation by loading the keys and values in parallel, especially effective for long text generation. As another alternative approach, multi-query and grouped-query attention can reduce the GPU memory bandwidth overhead by sharing KV parameters (loading fewer weights).
> 减少数据传输
> 聚焦于优化 GPU 存储访问，提高算数密度
> KV cache 可以避免对之前的 tokens 的冗余计算，PageAttention 将 KV cache 分配至连续的块以减少 memory fragmentation
> Flash-Decoding 通过将 keys 和 values 并行地装载，加速了 attention 计算，尤其是对于长文本生成
> Flash-Decoding 的一个替代方法是 multi-query 和 grouped-query attention，可以通过共享 KV 参数，因此装载更少的权重，进而减少 GPU 存储带宽开销

- *Decoding strategy optimization* aims to improve the sequential nature of the auto-regressive generation manner in different ways. As a representative study, speculative decoding [323, 324] first leverages a compact but efficient model (e.g., a n-gram model or a small PLM) to generate short segments and then utilizes the LLM to verify and correct these drafts. It can lead to a notable $2\times$ to $3\times$ speedup without compromising the generation quality. Researchers further suggest several variants to improve the efficiency of this approach, such as a learning-based method to combine several small models [325] and a stage-wise acceleration which employs a more smaller LM to accelerate the small LM first [326]. In addition, token-level early-exit techniques have been proposed enabling the generation of a token at lower Transformer layers, rather than passing through all the layers [327]. It can attain greater speedup, but at the cost of sacrificing generation quality.
> 解码策略优化
> 意图优化自回归生成的顺序本质
> 代表性的工作是 speculative decoding，它利用了一个精简且高效的模型生成短的 segment，然后再用 LLM 验证和修改它
> 该方法可以有2-3倍的加速，且不牺牲生成质量
> 进一步的研究提高了该方法的效率，例如基于学习结合多个小模型、分阶段加速 (利用更小的模型加速小模型)
> token-level early-exit 技术让 token 在较低的 Transformer 层就可以生成，不需要经过所有层，该方法加速效果良好，但损害质量

**Practical Settings.** In practice, existing libraries (e.g., Transformers [187]) and public APIs of LLMs (e.g., OpenAI) have supported various decoding strategies to serve different scenarios of text generation. Next, we present the decoding settings of several representative LLMs:
> 现存的库和 API 支持多种 decoding 方式，这里展示代表性 LLMs 的 decoding 设定

• T5 [82] utilizes greedy search as the default setting and applies beam search (beam size of 4) with a length penalty of 0.6 for translation and summarization tasks. ( 默认为贪心 )
• GPT-3 [55] employs beam search with a beam size of 4 and a length penalty of 0.6 for all generation tasks. ( beam search )
• Alpaca [142] utilizes sampling-based strategies with top-k (k = 50), top-p (p = 0.9), and temperature of 0.7 for open-ended generation. ( top-k, top-p, temperature 采样 )
• LLaMA [57] applies diverse decoding strategies tailored to specific tasks. For instance, it employs the greedy search for question answering tasks while utilizes a sampling strategy with the temperature settings of 0.1 (pass@1) and 0.8 (pass@100) for code generation. 
• OpenAI API supports several basic decoding strategies, including greedy search (by setting temperature to 0), beam search (with the setting best_of), temperature sampling (with the setting temperature), nucleus sampling (with the setting top_p). It also introduce parameters presence_penalty and frequency_penalty to control the repetition degree of generation. According to the OpenAI’s document, their APIs would produce different outputs even if the input and the hyper-parameters are the same. Setting temperature to 0 can yield more deterministic outputs, albeit with a slight chance of variability.
### 4.2.6 Summary and Discussion
The choice of architecture and pre-training tasks may incur different inductive biases for LLMs, which would lead to different model capacities. In this part, we discuss one open issue about the architecture choice for LLMs.
> 架构和预训练任务的选择会为 LLM 引入归纳偏置
> 本部分讨论 LLLM 的架构选择

**Architecture Choice.** In earlier literature of pre-trained language models, there are lots of discussions on the effects of different architectures [29, 89]. However, most LLMs are developed based on the causal decoder architecture, and there still lacks a theoretical analysis on its advantage over the other alternatives. Next, we briefly summarize existing discussions on this issue.
> 对于目前广泛使用的因果 decoder 架构的优势仍缺乏理论分析，此处对关于该问题的讨论进行总结
- By pre-training with the LM objective, it seems that causal decoder architecture can achieve a superior zeroshot and few-shot generalization capacity. Existing research has shown that without multi-task fine-tuning, the causal decoder has better zero-shot performance than other architectures [29]. The success of GPT-3 [55] has demonstrates that the large causal decoder model can be a good fewshot learner. In addition, instruction tuning and alignment tuning discussed in Section 5 have been proven to further enhance the capability of large causal decoder models [66, 67, 69].
> 现存研究发现 causal decoder 比其他架构有更好的 zero-shot 表现
> GPT-3说明了 large causal decoder 的 fewshot 表现也很好

- Scaling law has been widely observed in causal decoders. By scaling the model size, the dataset size, and the total computation, the performance of causal decoders can be substantially improved [30, 55]. Thus, it has become an important strategy to increase the model capacity of the causal decoder via scaling. However, more detailed investigation on encoder-decoder models is still lacking, and more efforts are needed to investigate the performance of encoder-decoder models at a large scale.
> causal decoder 架构中 scaling law 更明显

More research efforts about the discussions on architectures and pre-training objectives are in need to analyze how the choices of the architecture and pre-training tasks affect the capacity of LLMs, especially for encoder-decoder architectures. Despite the effectiveness of decoder-only architecture, it is also suggested to make more diverse exploration on architecture design. Besides the major architecture, the detailed configuration of LLM is also worth attention, which has been discussed in Section 4.2.2.

>Why does Predicting the Next Word Works?
>
>The essence of decoder-only architecture is to accurately predict the next word for reconstructing the pre-training data. Till now, there has been no formal study that theoretically demonstrates its advantage over other architectures. An interesting explanation was from Ilya Sutskever during the interview held by Jensen Huanga. The original transcript from the interview was copied belowb:
> decoder-only 架构的本质是通过重构预训练数据以正确预测 next token
>
>Say you read a detective novel. It’s like complicated plot, a storyline, different characters, lots of events, mysteries like clues, it’s unclear.
>
>Then, let’s say that at the last page of the book, the detective has gathered all the clues, gathered all the people and saying, "okay, I’m going to reveal the identity of whoever committed the crime and that person’s name is". Predict that word. 
>...
>Now, there are many different words. But predicting those words better and better, the understanding of the text keeps on increasing. GPT-4 predicts the next word better.
>在重复 predict next word 的过程中，模型对于文本的理解能力在不断增长
## 4.3 Model Training
In this part, we review the important settings, techniques, or tricks for training LLMs.
> 本节介绍 LLM 训练的技巧和设定
### 4.3.1 Optimization Setting
For parameter optimization of LLMs, we present the commonly used settings for batch training, learning rate, optimizer, and training stability.
> 关于参数优化，我们展示关于 batch training、lr、optimizer、training stability 的常用的参数设定

**Batch Training.** For language model pre-training, existing work generally sets the batch size to a large number (e.g., 2,048 examples or 4M tokens) to improve the training stability and throughput. For LLMs such as GPT-3 and PaLM, they have introduced a new strategy that dynamically increases the batch size during training, ultimately reaching a million scale. Specifically, the batch size of GPT-3 is gradually increasing from 32K to 3.2M tokens. Empirical results have demonstrated that the dynamic schedule of batch size can effectively stabilize the training process of LLMs [56].
> 现存工作通常 batch size 较大，如2048样本或4M tokens，以提高训练稳定性和 throughput
> GPT-3在训练中动态增大 batch size，逐渐从32K 到3.2 M tokens
> Empirical results 说明动态调度 batch size 可以高效稳定 LLM 的训练

**Learning Rate.** Existing LLMs usually adopt a similar learning rate schedule with the warm-up and decay strategies during pre-training. Specifically, in the initial 0.1% to 0.5% of the training steps, a linear warm-up schedule is employed for gradually increasing the learning rate to the maximum value that ranges from approximately $5 \times 10^{-5}$ to $1 \times 10^{-4}$ (e.g., $6\times 10^{-5}$  for GPT-3). Then, a cosine decay strategy is adopted in the subsequent steps, gradually reducing the learning rate to approximately 10% of its maximum value, until the convergence of the training loss.
> 现存 LLM 采用类似的 lr 调度，即预训练时 warm-up 然后 decay
> 在开始的百分之0.1到0.5个训练步中，采用线性 warm-up 调度，提高 lr 到最大，然后 cosine decay，降低 lr 至最高的百分之10，直到训练收敛

**Optimizer.** The Adam optimizer [328] and AdamW optimizer [329] are widely utilized for training LLMs (e.g., GPT- 3), which are based on adaptive estimates of lower-order moments for first-order gradient-based optimization. Commonly, its hyper-parameters are set as follows: $\beta_1 = 0.9, \beta_2 = 0.95$ and $\epsilon = 10^{-8}$. Meanwhile, the Adafactor optimizer [330] has also been utilized in training LLMs (e.g., PaLM and T5), which is a variant of the Adam optimizer specially designed for conserving GPU memory during training. The hyper-parameters of the Adafactor optimizer are set as:  $\beta_1 = 0.9$ and $\beta_2 = 1.0 - k^{-0.8}$, where k denotes the number of training steps.
> Adam/AdamW 广泛被使用

**Stabilizing the Training.** During the pre-training of LLMs, it often suffers from the training instability issue, which may cause the model collapse. To address this issue, weight decay and gradient clipping have been widely utilized, where existing studies [55, 78, 90, 93, 113] commonly set the threshold of gradient clipping to 1.0 and weight decay rate to 0.1. However, with the scaling of LLMs, the training loss spike is also more likely to occur, leading to unstable training. To mitigate this problem, PaLM [56] and OPT [90] use a simple strategy that restarts the training process from an earlier checkpoint before the occurrence of the spike and skips over the data that may have caused the problem. Further, GLM [93] finds that the abnormal gradients of the embedding layer usually lead to spikes, and proposes to shrink the embedding layer gradients to alleviate it.
> LLM 预训练过程中的稳定性问题可能会导致模型崩溃
> weight decay 和 gradient clipping 被用于解决稳定性问题
> LLM 预训练过程中也会出现 training loss spike，使得训练不稳定
> 简单的技巧是从之前的 checkpoint 重新开始训练，并跳过可能导致 spike 的数据
> GLM 发现 embedding layer 的异常的梯度经常导致 spike，因此提出 shrink embedding layer 的梯度
### 4.3.2 Scalable Training Techniques
As the model and data sizes increase, it has become challenging to efficiently train LLMs under a limited computational resource. Especially, two primary technical issues are required to be resolved, i.e., increasing training throughput and loading larger models into GPU memory. In this part, we review several widely used approaches in existing work to address the above two challenges, namely 3D parallelism [75, 331, 332], ZeRO [333], and mixed precision training [334], and also give general suggestions about how to utilize them for training.
> 在数据和模型大小扩展的情况下训练 LLM 需要解决两个问题：提高训练 throughput 和将更大的模型 load 进 GPU memory
> 本节介绍解决上述两个问题的方法，包括3D parallelism，ZeRO 和混合精度训练

**3D Parallelism.** 3D parallelism is actually a combination of three commonly used parallel training techniques, namely data parallelism, pipeline parallelism [331, 332], and tensor parallelism [75]. We next introduce the three parallel training techniques.
> 3D parallelism 即3个常用的 parallel 训练技巧的结合，包括 data parallelism, pipeline parallelism, tensor parallelism

- *Data parallelism.* Data parallelism is one of the most fundamental approaches to improving the training throughput. It replicates the model parameters and optimizer states across multiple GPUs and then distributes the whole training corpus into these GPUs. In this way, each GPU only needs to process the assigned data for it, and performs the forward and backward propagation to obtain the gradients. The computed gradients on different GPUs will be further aggregated to obtain the gradients of the entire batch for updating the models in all GPUs. In this way, as the calculations of gradients are independently performed on different GPUs, the data parallelism mechanism is highly scalable, enabling the way that increases the number of GPUs to improve training throughput. Furthermore, this technique is simple in implementation, and most of existing popular deep learning libraries have already implemented data parallelism, such as TensorFlow and PyTorch.
> 数据并行 (划分数据集)
> 提高训练 throughput 最基础的方法
> 它复制模型参数和 optimizer 状态到各个 GPU，然后划分整个 corpus 给各个 GPU，每个 GPU 处理分配给它的数据，执行前向和反向传播获得梯度，在不同 GPU 上获得的梯度被聚合，以获得整个 batch 的梯度，然后用该梯度更新所有 GPU 上的模型
> 数据并行方法高度可扩展，增大 GPU 数量即可增大训练 throughput，且该技术易于实现

- *Pipeline parallelism.* Pipeline parallelism aims to distribute the different layers of a LLM into multiple GPUs. Especially, in the case of a Transformer model, pipeline parallelism loads consecutive layers onto the same GPU, to reduce the cost of transmitting the computed hidden states or gradients between GPUs. However, a naive implementation of pipeline parallelism may result in a lower GPU utilization rate as each GPU has to wait for the previous one to complete the computation, leading to the unnecessary cost of bubbles overhead [331]. To reduce these bubbles in pipeline parallelism, GPipe [331] and PipeDream [332] propose the techniques of padding multiple batches of data and asynchronous gradient update to improve the pipeline efficiency.
> 流水线并行 (划分模型)
> pipeline 并行目标于将 LLM 的不同层分配给多个 GPUs
> 对于 Transformer 模型，pipeline 并行在相同 GPU 上 load 连续的层，以减少在 GPUs 之间传输隐藏状态或梯度的开销，但 pipeline 并行的 naive 实现会导致低的 GPU 利用率，因为 GPU 之间计算的状态存在相互依赖，这种相互等待的状态称为 bubbles 开销
> GPipe 和 PipeDream 意在减少 bubbles，对多个 batch 的数据 padding，并进行异步的梯度更新

- *Tensor parallelism.* Tensor parallelism is also a commonly used technique that aims to decompose the LLM for multi-GPU loading. Unlike pipeline parallelism, tensor parallelism focuses on decomposing the tensors (the parameter matrices) of LLMs. For a matrix multiplication operation Y = XA in the LLM, the parameter matrix A can be split into two submatrices, $A_1$ and $A_2$, by column, which can be expressed as $Y = [XA_1, XA_2]$. By placing matrices A1 and A2 on different GPUs, the matrix multiplication operation would be invoked at two GPUs in parallel, and the final result can be obtained by combining the outputs from the two GPUs through across-GPU communication. Currently, tensor parallelism has been supported in several open-source libraries, e.g., Megatron-LM [75], and can be extended to higher-dimensional tensors. Also, Colossal-AI has implemented tensor parallelism for higher-dimensional tensors [335–337] and proposed sequence parallelism [338] especially for sequence data, which can further decompose the attention operation of the Transformer model.
> 张量并行 (划分参数)
> 意在分解 LLM 为多 GPU loading，它将 tensor，即参数矩阵，进行分解，对于 LLM 计算中的矩阵乘 $Y=XA$，参数矩阵 $A$ 可以按列被分解为两个子矩阵 $A_1, A_2$，$Y = X[A_1, A_2]$
> $A_1, A_2$ 会被放在不同的 GPU，因此矩阵乘法会由两个 GPU 同时执行，最后结果再通过 GPU 通信结合


**ZeRO.** ZeRO [333] technique, proposed by the DeepSpeed [74] library, focuses on the issue of memory redundancy in data parallelism. As mentioned before, data parallelism requires each GPU to store the same copy of a LLM, including model parameters, model gradients, and optimizer parameters. Whereas, not all of the above data is necessary to be retained on each GPU, which would cause a memory redundancy problem. To resolve it, the ZeRO technique aims to retain only a fraction of data on each GPU, while the rest data can be retrieved from other GPUs when required. Specifically, ZeRO provides three solutions, depending on how the three parts of the data are stored, namely optimizer state partitioning, gradient partitioning, and parameter partitioning. Empirical results indicate that the first two solutions do not increase the communication overhead, and the third solution increases about 50% communication overhead but saves memory proportional to the number of GPUs. PyTorch has implemented a similar technique as ZeRO, called FSDP [339].
> ZeRO 意在解决 data parallelism 中的 memory redundancy 的问题
> data parallelism 中，每个 GPU 都存储 LLM 的一个 copy，包括了 model 参数，model 梯度和 optimizer 参数，ZeRO 意在仅在各个 GPU 保持一部分数据，需要的其余数据从其他 GPU 取得
> ZeRO 提供了3个 solution，不同点在于模型参数、模型梯度和优化器参数这三部分数据是如何存储的，3个 solution 分别是 optimizer 状态划分、梯度划分、参数划分

**Mixed Precision Training.** In previous PLMs (e.g., BERT [23]), 32-bit floating-point numbers, also known as FP32, have been predominantly used for pre-training. In recent years, to pre-train extremely large language models,
some studies [334] have started to utilize 16-bit floatingpoint numbers (FP16), which reduces memory usage and communication overhead. Additionally, as popular NVIDIA GPUs (e.g., A100) have twice the amount of FP16 computation units as FP32, the computational efficiency of FP16 can be further improved. However, existing work has found that FP16 may lead to the loss of computational accuracy [64, 78], which affects the final model performance. To alleviate it, an alternative called Brain Floating Point (BF16) has been used for training, which allocates more exponent bits and fewer significant bits than FP16. For pre-training, BF16 generally performs better than FP16 on representation accuracy [78].
> 近几年为了训练更大的 LLM，一些研究尝试使用 FP16以减少 memory usage 和通讯开销
> 现存工作发现 FP16会导致计算准确率下降，影响 model performance
> 一个替代是 BF16，它分配更多的指数位，更少的有效位，在预训练时，BF16相较于 FP16在 representation 准确率上表现更好
 
**Overall Training Suggestion.** In practice, the above training techniques, especially 3D parallelism, are often jointly used to improve the training throughput and large model loading. For instance, researchers have incorporated 8-way data parallelism, 4-way tensor parallelism, and 12-way pipeline parallelism, enabling the training of BLOOM [78] on 384 A100 GPUs.
> 实际中上述方法会结合使用

Currently, open-source libraries like DeepSpeed [74], Colossal-AI [189], and Alpa [340] can well support the three parallel training methods. To reduce the memory redundancy, ZeRO, FSDP, and activation recomputation techniques [77, 341] can be also employed for training LLMs, which have already been integrated into DeepSpeed, PyTorch, and Megatron-LM. In addition, the mixed precision training technique such as BF16 can be also leveraged to improve the training efficiency and reduce GPU memory usage, while it requires necessary support on hardware (e.g., A100 GPU). 
Because training large models is a time-intensive process, it would be useful to forecast the model performance and detect abnormal issues at an early stage. For this purpose, GPT-4 [46] has recently introduced a new mechanism called predictable scaling built on a deep learning stack, enabling the performance prediction of large models with a much smaller model, which might be quite useful for developing LLMs. In practice, one can further leverage the supporting training techniques of mainstream deep learning frameworks. For instance, PyTorch supports the data parallel training algorithm FSDP [339] (i.e., fully sharded data parallel), which allows for partial offloading of training computations to CPUs if desired.
# 5 Adaptation of LLMs
After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences. Further, we will also discuss efficient tuning and quantization for model adaptation in resource-limited settings. In what follows, we will introduce the four parts in detail.
> 本节介绍 adapting LLMs 的两大方法：instruction tuning 和 alignment tuning
> instruction tuning 的意图是强化 LLM 的能力，alignment tuning 意图将 LLM 的行为和人类价值观对齐
> 本节还介绍在 resource-limited 的设定下对模型进行高效的 tuning 和 quantization

## 5.1 Instruction Tuning
In essence, instruction tuning is the approach to fine-tuning pre-trained LLMs on a collection of formatted instances in the form of natural language [67], which is highly related to supervised fine-tuning [66] and multi-task prompted training [28]. 
> instruction tuning 即以自然语言的形式在 formatted instances 集合上微调 LLM，这和有监督微调和多任务 prompted 训练高度相关

In order to perform instruction tuning, we first need to collect or construct instruction-formatted instances. Then, we employ these formatted instances to fine-tune LLMs in a supervised learning way (e.g., training with the sequence-to-sequence loss). After instruction tuning, LLMs can demonstrate superior abilities to generalize to unseen tasks [28, 67, 69], even in a multilingual setting [94].
> 要 instruction tuning，首先需要收集 instruction 格式的实例，然后以有监督的方式训练 LLM (损失是 sequence-to-sequence loss)
> instruction tuning 之后，LLM 可以更好泛化到未见的任务，甚至在是多语言设定下

A recent survey [342] presents a systematic overview of the research on instruction tuning. In comparison to that, we mainly focus on the effect of instruction tuning on LLMs and provide detailed guidelines or strategies for instance collection and tuning. In addition, we also discuss the use of instruction tuning for satisfying the real needs of users, which has been widely applied in existing LLMs, e.g., InstructGPT [66] and GPT-4 [46].
> 我们聚焦 instruction tuning 对 LLM 的影响
> 并提供如何收集 instance 和如何 tuning 的指导

### 5.1.1 Formatted Instance Construction
Generally, an instruction-formatted instance consists of a task description (called an instruction), an optional input, the corresponding output, and a small number of demonstrations (optional). As important public resources, existing studies have released a large number of labeled data formatted in natural language (see the list of available resources in Table 3) as introduced in Section 3.3.1. Next, we introduce three major methods for constructing formatted instances (see an illustration in Figure 11) and then discuss several key factors for instance construction.
> 一个指令格式的 instance 由任务描述 (即 instruction)，一个可选的输入，以及对应的输出构成，同时可以有可选的一些演示
> 我们将介绍3个构建 formatted instances 的方法

**Formatting NLP Task Datasets.** Before instruction tuning was proposed, several early studies [168, 343, 344] collected the instances from a diverse range of traditional NLP tasks (e.g., text summarization, text classification, and translation) to create supervised multi-task training datasets. As a major source of instruction tuning instances, it is convenient to format these multi-task training datasets with natural language task descriptions. 
> 格式化 NLP 任务数据集
> 对传统 NLP 任务数据集 (文本总结/分类/翻译) 进行格式化，为其添加自然语言任务描述

Specifically, recent work [28, 66, 67, 88] augments the labeled datasets with human-written task descriptions, which instructs LLMs to understand the tasks by explaining the task goal.
> 最近的工作用人写的任务描述来格式化有标签的数据集，指导 LLM 通过解释任务目标来理解任务

![[A Survey of LLMs-Fig11.png]]
For example, in Figure 11 (a), a task description “Please answer this question” is added for each example in the question-answering task. 
> 例如为问题回答任务中的每个样本添加 “Please answer this question”的任务描述

After instruction tuning, LLMs can generalize well to other unseen tasks by following their task descriptions [28, 67, 69]. In particular, it has been shown that instructions are the crucial factor in task generalization ability for LLMs [67]: by fine-tuning the model on labeled datasets with the task descriptions removed, it results in a dramatic drop in model performance. 
> 指令调节后，LLM 通过遵循它们的任务描述，可以泛化到未见的任务
> 指令对于 LLM 的任务泛化能力起到关键作用，如果在有标签数据集上移除任务描述进行微调，性能会大量降低

To better generate labeled instances for instruction tuning, a crowd-sourcing platform, PromptSource [167] has been proposed to effectively create, share, and verify the task descriptions for different datasets. To enrich the training instances, several studies [28, 168, 345] also try to invert the input-output pairs of existing instances with specially designed task descriptions for instruction tuning. For instance, given a question-answer pair, we can create a new instance by predicting the answer-conditioned question (e.g., “Please generate a question based on the answer:”).
> 一些研究尝试颠倒 input-output 对，同时设计对应的任务描述，以丰富训练实例
> 例如：“Please generate a question based on the answer”

**Formatting Daily Chat Data.** Despite that a large number of training instances have been formatted with instructions, they mainly come from public NLP datasets, either lacking instruction diversity or mismatching with real human needs [66]. To overcome this issue, InstructGPT [66] proposes to take the queries that real users have submitted to the OpenAI API as the task descriptions. Additionally, to enrich the task diversity, human labelers are also asked to compose the instructions for real-life tasks, including openended generation, open question answering, brainstorming, and chatting. Then, they let another group of labelers directly answer these instructions as the output. Finally, they pair one instruction (i.e., the collected user query) and the expected output (i.e., the human-written answer) as a training instance. Note that InstructGPT also employs these real-world tasks formatted in natural language for alignment tuning (discussed in Section 5.2). Further, GPT-4 [46] has designed potentially high-risk instructions and guided the model to reject these instructions through supervised fine-tuning for safety concerns. Considering the absence of high-quality public chat data, several studies have also collected users’ chat requests as input data, and then utilized ChatGPT or GPT-4 to generate responses as output data. A notable example of such a dataset is the conversational data from ShareGPT [148]. Additionally, Dolly [172] and OpenAssistant [173] have further released their conversation data, which has been carefully labeled by human annotators to attain a high level of quality.
> 格式化日常聊天数据
> 对 NLP 数据集进行格式化会缺乏 instruction 的多样性，也会与真实人类需求不匹配
> InstructGPT 提出将真实用户提交给 OpenAI API 的 query 作为任务描述，同时让 human labeler 为 real-life 任务构建指令，然后让另一群 labeler 直接回答这些指令作为 output
> InstructGPT 将收集到的 user query 和 human written answer 作为指令和输出，结合为一个实例
> GPT-4设计了潜在高风险的指令，并引导模型拒绝这些指令
> 一些研究收集用户 chat requests 和 GPT-4 generated response 作为数据，例如 ShareGPT

**Formatting Synthetic Data**. To reduce the burden of human annotation or manual collection, several semi-automated approaches [143] have been proposed for constructing instances by feeding existing instances into LLMs to synthesize diverse task descriptions and instances. As illustrated in Figure 11 (c), the Self-Instruct method only needs 175 instances as the initial task pool. Then, they randomly select a few instances from the pool as demonstrations and prompt a LLM to generate new instructions and corresponding input-output pairs. After the quality and diversity filtering, newly generated instances would be added into the task pool. Hence, the synthetic method is an effective and economical way to generate large-scale instruction data for LLMs. 
> 格式化合成数据
> 半自动的构建实例的方法，通过将现存实例 feed into LLM 让它合成多样的任务描述和实例
> 该 Self-Instruct 方法仅需要175实例作为初始任务池，然后随机选择一些 instance from the pool 作为展示，并 prompt LLM 生成新的指令和对应的 input-output 对，经过过滤后，将它们加入 task pool

However, the instances generated by the Self-Instruct method might be simplistic or lack the diversity. To improve the quality of synthetic int ructions, WizardLM [346] introduces Evol-Instruct by proposing in-depth and in-breadth evolving to enrich the complexity and diversity of the instances. Furthermore, Self-Align [347] establishes multiple human-aligned principles to filter the synthesized instances. It then employs these instances to train a LLM in order to yield more aligned instances. To enhance the quality of the instance output, researchers directly adopt humanwritten texts as the output and synthesize corresponding instructions using ICL examples [348].
> Self-Instruct 方法生成的实例可能过于简单，缺乏 diversity
> Evol-Instruct 意在提高 Self-Instruct 生成数据的质量
> Self-Align 用 human-aligned 的 principle 过滤合成的实例，再用这些实例训练 LLM 以得到更 aligned 的实例
> 也有研究直接以 humanwritten 文本作为 output，然后用 ICL 实例让 LLM 合成对应指令

**Key Factors for Instance Construction.** The quality of instruction instances has an important impact on the performance of the model. Here, we discuss some essential factors for instance construction.
> 本部分讨论 instance 构建的一些关键因素

- *Scaling the instructions.* It has been widely shown that scaling the number of tasks can largely enhance the generalization ability of LLMs [28, 67, 88]. With the increasing of the task number, the model performance initially shows a continuous growth pattern, while the gain becomes negligible when it reaches a certain level [69, 88].  A plausible speculation is that a certain number of representative tasks can provide relatively sufficient knowledge and adding more tasks may not bring additional gains [69]. Also, it is beneficial to enhance the diversity of the task descriptions in several aspects, such as length, structure, and creativity [28]. 
> 扩展任务的数量可以大幅强化 LLM 的泛化能力，随着任务数量增长，模型性能最初会连续增长，在达到特定 level 之后增长趋零
> 一个推测是特定数量的 representative 任务可以提供相对充足的知识，过多则不会有额外提升
> 在多个方面提升任务描述的 diversity 如长度、结构、创造力也有帮助

As for the number of instances per task, it has been found that a small number of instances can usually saturate the generalization performance of the model to perform a specific task [67, 69]. Specially, several recent work [349, 350] has explored the effect of fine-tuning with a small amount of high-quality instruction data (e.g., one or a few thousand instances), showing very promising results on the evaluation tasks. In contrast, another line of studies continue to explore the scaling effect of instruction data [351, 352]. For example, Orca [351] scales up the synthesized instances to 5 million with step-by-step explanations, and it achieves superior performance across a wide range of tasks compared to the methods tuned with instruction data.
> 关于每个任务的 instance 的数量
> 对于每个 task，少量的 instances 通常会饱和模型执行特定 task 的泛化能力，一些工作探究了使用少量的高质量 instruction data 微调模型，发现效果很好
> 另一些工作探究 instruction data 的扩展效果，Orca 将合成 instances 的数量提升到5 million，在大范围的任务上效果很好

- *Formatting design.* As an important factor, the design of natural language format also highly impacts the generalization performance of LLMs [88]. Typically, we can add task descriptions and optional demonstrations to the inputoutput pairs of existing datasets, where the task description is the most key part for LLMs to understand the task [88]. Further, it can lead to substantial improvements by using an appropriate number of exemplars as demonstrations [69], which also alleviates the model sensitivity to instruction engineering [67, 69]. However, incorporating other components (e.g., things to avoid, reasons, and suggestions) into instructions may have a negligible or even adverse effect on the performance of LLMs [88, 166]. Recently, to elicit the step-by-step reasoning ability of LLMs, some work [69] proposes to include chain-of-thought (CoT) examples for some reasoning datasets, such as arithmetic reasoning. It has been shown that fine-tuning LLMs with both CoT and non-CoT examples can lead to a good performance across various reasoning tasks, including those that require multihop reasoning ability (e.g., commonsense question answering and arithmetic reasoning) as well as those without the need for such a reasoning way (e.g., sentiment analysis and extractive question answering) [69, 95].
>格式化设计
>任务描述是 LLM 理解任务最重要的部分，且使用一定数量的 demonstrations 可以大幅提高表现，同时也缓解了模型对 instruction engineering 的敏感性
>但在指令中包含其他的一些信息可能有反作用
>最近又研究提出为 reasoning 数据集 include CoT examples，例如算数分析，发现使用 CoT 或 non-CoT 样本微调都可以 lead to LLM 在多个 reasoning 任务中有好的表现，包括在需要多步推理的任务，例如常识问题回答和算数分析，以及在不需要多步推理的任务，例如情感分析和提取式问题回答

To summarize, diversity and quality of instructions seem to be more important than the number of instances [349] since the well-performing InstructGPT [66] and LLaMA-2- Chat [99] utilize fewer but more diverse instructions (or instances) than the Flan-series LLMs [67, 69]. However, a large amount of training data may compensate for the absence of high-quality data [351]. Further, it is more useful to invite labelers to compose human-need tasks than using dataset-specific tasks. However, it still lacks general guidelines to annotate human-need instances, making the task composition somehow heuristic. To reduce human efforts, we can either reuse existing formatted datasets (Table 3) or automatically construct the instructions using existing LLMs [143]. We conduct a preliminary experiment to show the effectiveness of different construction methods in Section 5.1.4.
> 小结
> 重要性：指令的多样性和质量 > 指令的数量，InstructGPT 和 LLaMA-2都使用更少但更多样的指令实例
> 但大量的训练数据可以弥补高质量数据的缺乏
> 让 labeler 构建 human-need 的任务比使用 dataset 构建的任务数据更有效

### 5.1.2 Instruction Tuning Strategies
Unlike pre-training, instruction tuning is often more efficient since only a moderate number of instances are used for training. Since instruction tuning can be considered as a supervised training process, its optimization is different from pre-training in several aspects [69], such as the training objective (i.e., sequence-to-sequence loss) and optimization configuration (e.g., smaller batch size and learning rate), which require special attention in practice. In addition to these optimization configurations, there are also four important aspects to consider for instruction tuning:
> Instruction tuning 视为有监督训练过程，其优化过程和 pre-training 有不同，包括了训练目标 (sequence-to-sequence loss、优化配置 (smaller batch size and lr)
> 除此以外，还有4个需要考虑的方面

**Balancing the Data Distribution.** Since instruction tuning involves a mixture of different tasks, it is important to balance the proportion of different tasks during finetuning.  A widely used method is the examples-proportional mixing strategy [82], i.e., combining all the datasets and sampling each instance equally from the mixed datasets.  Furthermore, increasing the sampling ratio of high-quality collections (e.g., FLAN [67] and P3 [167]) can generally lead to performance improvement according to recent findings [69, 95]. 
> 平衡数据分布
> instruction tuning 包含了多个不同任务，需要平衡其分布
> 一个方法是结合所有数据，然后从混合的数据集中均匀采样
> 提高高质量数据的采样率 --> 性能更好

Further, it is common to set a maximum cap to control the maximum number of examples that a dataset can contain during instruction tuning [82], which is set to prevent larger datasets from overwhelming the entire distribution [82, 95]. In practice, the maximum cap is typically set to several thousands or tens of thousands according to different datasets [67, 69]. Recently, it has been empirically found that existing instruction datasets (Table 3) mainly focus on enhancing LLMs’ capabilities in certain aspects, and a single dataset alone cannot lead to a comprehensive enhancement in model capacity [353]. Therefore, it is often suggested to use a mixture of existing instruction datasets to achieve a balanced improvement in different capacities, including NLP task data (e.g., FLAN v2 [292]), chat data (e.g., ShareGPT [148]), and synthetic data (e.g., GPT4-Alpaca [354]).
> 最好设置上限值，控制 instruction tuning 时的数据集大小，防止破坏 pre-trained 的完整分布
> 一般设定为几千或几万
> 单个数据集无法综合提高 LLM 能力，建议使用现存指令数据集的混合，例如混合 NLP task、chat、synthetic 数据

**Combining Instruction Tuning and Pre-Training.** To make the tuning process more effective and stable, OPT-IML [95] incorporates pre-training data during instruction tuning, which can be regarded as regularization for model tuning. Further, instead of using a separate two-stage process (pretraining then instruction tuning), some studies attempt to train a model from scratch with a mixture of pre-training data (i.e., plain texts) and instruction tuning data (i.e., formatted datasets) using multi-task learning [82]. Specifically, GLM-130B [93] and Galactica [35] integrate instruction formatted datasets as a small proportion of the pre-training corpora to pre-train LLMs, which potentially achieves the advantages of pre-training and instruction tuning at the same time.
> 结合指令微调和预训练
> 可以视为模型微调的正则化
> 一些研究还尝试用多任务学习，用预训练数据和质量微调数据的混合从头训练模型

**Multi-stage Instruction Tuning.** For instruction tuning, there are two kinds of important instruction data, namely task-formatted instructions and daily chat instructions. Generally, the former has a significantly larger volume than the latter. It is important to balance the training with the two kinds of instruction data. In addition to carefully mixing different instruction data, we can also adopt a multi-stage instruction tuning strategy [352], where LLMs are first finetuned with large-scale task-formatted instructions and subsequently fine-tuned on daily chat ones. To avoid the capacity forgetting issue, it is also useful to add an amount of taskformatted instructions at the second stage. Actually, such a multi-stage tuning strategy can be also applied to other settings for instruction tuning. For example, we can schedule different fine-tuning stages with progressively increased levels on difficulty and complexity, and gradually improve the capacities of LLMs to follow complex instructions.
> 多阶段指令微调
> instruction tuning 有两类数据：task-formatted 和 daily chat 指令，前者数量远多于后者
> 二者需要在训练时平衡
> 可以采用多阶段微调，现在 task-formatted 上微调，然后在 daily chat 上
> 为了避免遗忘也可以在第二阶段添加一定数量 task-formatted
> 多阶段除了用于平衡不同类的数据，也可以用于调度，例如分阶段逐渐提高质量的难度和复杂度，以逐渐提高 LLM 遵循复杂指令的能力

**Other Practical Tricks.** In practice, there are also several useful strategies and tricks that are helpful to improve the fine-tuning performance of LLMs. We list several representative ones as follows:
> 再介绍一些其他帮助提高 LLM 微调表现的技巧

- **Efficient training for multi-turn chat data.** Given a multiturn chat example (the conversation between a user and chatbot), a straightforward fine-tuning way is to split it into multiple context-response pairs for training: a LLM is finetuned to generate the response based on the corresponding context for all splits (i.e., at each utterance from the user). In such a fine-tuning way, it is apparent that there exist overlapping utterances in the split examples from a conversation. To save the training cost, Vicuna [138] has adopted an efficient way that feeds the whole conversation into the LLM, but relies on a loss mask that only computes the loss on the responses of the chatbot for training. It can significantly reduce the compute costs derived from the overlapped utterances.
> 针对 multi-turn chat 数据的高效训练
> 简单方式即分成多个 context-response 对
> 但该方式中存在重叠的 utterances
> Vicuna 将完整对话 feed into LLM，并使用 loss mask，仅计算 chatbot 的 response 的 loss 用于训练，降低了训练开销

- **Establishing self-identification for LLM.** To deploy LLMs for real-world applications, it is necessary to establish its identity and make LLMs aware of these identity information, such as name, developer and affiliation. A practical way is to create identity-related instructions for fine-tuning the LLM. It is also feasible to prefix the input with the selfidentification prompt, e.g., “The following is a conversation between a human and an AI assistant called CHATBOTNAME, developed by DEVELOPER.”, where CHATBOTNAME and DEVELOPER refer to the name and developer of the chatbot, respectively.
> 构建 LLM 的自我认同
> 需要让 LLM 认识到自己的身份信息，包括名称、开发者、从属
> 一个方式就是创建身份相关的指令，用它微调
> 也可以用 self identification prompt prefix 输入

In addition to the above practical strategies and tricks, existing work has also used other tricks, e.g., concatenating multiple examples into a single sequence to approach the max length [355].

### 5.1.3 The Effect of Instruction Tuning
In this part, we discuss the effect of instruction tuning on LLMs in three major aspects.
> 本节从3个方面讨论 instruction tuning 对于 LLM 的影响

**Performance Improvement.** Despite being tuned on a moderate number of instances, instruction tuning has become an important way to improve or unlock the abilities of LLMs [69]. Recent studies have experimented with language models in multiple scales (ranging from 77M to 540B), showing that the models of different scales can all benefit from instruction tuning [69, 345], yielding improved performance as the parameter scale increases [94]. Further, smaller models with instruction tuning can even perform better than larger models without fine-tuning [28, 69]. Besides the model scale, instruction tuning demonstrates consistent improvements in various model architectures, pre-training objectives, and model adaptation methods [69]. In practice, instruction tuning offers a general approach to enhancing the abilities of existing language models [69] (including small-sized PLMs). Also, it is much less costly than pretraining, since the amount of instruction data required by LLMs is significantly smaller than pre-training data.
> 表现提升
> 不同大小的模型都会从 instruction tuning 中获益，且参数规模增大，性能提升增大
> instruction tuned 的小模型效果甚至优于没有 instruction tuned 的大模型
> 另外，模型的架构、预训练目标、适应方法相同的情况下，instruction tuning 都可以提高模型表现
> 因此 instruction tuning 是一个提升语言模型能力的通用方法，且它的开销比预训练小

**Task Generalization.** Instruction tuning encourages the model to understand natural language instructions for task completion. It endows LLMs with the ability (often considered as an emergent ability) to follow human instructions [31] to perform specific tasks without demonstrations, even on unseen tasks [69]. A large number of studies have confirmed the effectiveness of instruction tuning to achieve superior performance on both seen and unseen tasks [95, 345]. Also, instruction tuning has been shown to be useful in alleviating several weaknesses of LLMs (e.g., repetitive generation or complementing the input without accomplishing a certain task) [66, 69], leading to a superior capacity to solve real-world tasks for LLMs. Furthermore, LLMs trained with instruction tuning can generalize to related tasks across languages. For example, BLOOMZ-P3 [94] is fine-tuned based on BLOOM [78] using English-only task collection P3 [167]. Interestingly, BLOOMZ-P3 can achieve a more than 50% improvement in multilingual sentence completion tasks compared to BLOOM, which shows that instruction tuning can help LLMs acquire general task skills from English-only datasets and transfer such skills into other languages [94]. In addition, it has been found that using English-only instructions can produce satisfactory results on multilingual tasks [94], which helps reduce the effort of instruction engineering for a specific language.
> 任务泛化
> Instruction tuning 鼓励模型理解自然语言指令以完成任务，让模型对于 unseen 任务在没有 demonstration 的情况下执行特定的任务
> Instruction tuning 还可以缓解 LLM 的弱点，例如重复生成或不完成任务仅填补输入，使得 LLM 适合解决实际任务
> Instruction tuning 还让模型可以泛化到跨语言的相关任务，甚至使用 English-only 的指令就可以在多语言任务上产生满意的结果

**Domain Specialization.** Existing LLMs have showcased superior capabilities in traditional NLP tasks (e.g., generation and reasoning) and daily questions. However, they may still lack domain knowledge to accomplish specific tasks, such as medicine, law, and finance (See Section 8 for a detailed discussion of LLMs in different applications). Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts. For instance, researchers propose to fine-tune Flan-PaLM [69] using medical datasets to create Med-PaLM [356], a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians. Furthermore, a recent study [357] fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks. There are also several open-sourced medical models instructiontuned based on LLaMA [57], such as BenTsao [358]. Also, researchers explore instruction tuning on law [359], finance [360], and arithmetic computation [361].
> 领域专门化
> 现存的 LLM 在传统 NLP 任务 (生成和推理) 以及日常问答表现良好，但缺乏完成特定任务的能力，例如医学、法律、经济
> Instruction tuning 可以用于调节 LLM 为领域特定的专家

### 5.1.4 Empirical Analysis for Instruction Tuning
![[A Survey of LLMs-Table 9.png]]
Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks. In this section, we will explore the effect of different types of instructions in fine-tuning LLMs (i.e., LLaMA (7B) and LLaMA (13B)), as well as examine the usefulness of several instruction improvement strategies.
> 本节探究不同类型的指令在微调 LLM 时的效果，同时检验一些指令 improvement 策略的有效性

**Instruction Datasets.** According to the discussion in Section 5.1.1, we mainly consider three common kinds of instructions as follows:
- *Task-specific instructions.* For the first type of instructions, we adopt the most commonly-used multi-task instruction dataset, FLAN-T5 [69], which contains 1,836 tasks and over 15M instructions by combining four data mixtures from prior work.
- *Daily chat instructions.* This type of instructions are conversations posed by users about daily life, which are more closely related to real-life scenarios. We adopt the ShareGPT instruciton set, consisting of 63K real-user instructions. It has been used as the core instructions for Vicuna.
- *Synthetic instructions.* In addition to reusing existing instructions, we can also automatically synthesize massive instructions using LLMs. We adopt the popular synthetic instruction dataset Self-Instruct-52K [143], consisting of 52K instructions paired with about 82K instance inputs and outputs. These generated instructions have a similar data distribution as the human-written seed tasks (e.g., grammar checking, brainstorming).

As the original FLAN-T5 dataset is very large (i.e., over 15M), we randomly sample 80,000 instructions from it for conducting a fair comparison with other instruction datasets (i.e., ShareGPT and Self-Instruct-52K) at a similar scale. In our experiments, we test on each individual instruction set to explore their own effects and also examine their combinatorial effects on model performance.
> 我们考虑三种类型的指令
> 针对任务的指令
> 日常聊天的指令
> 合成的指令，这些生成的指令和 human-written 的 seed 任务的数据分布是相似的
> 我们测试了各个指令集单独的影响以及它们结合起来的影响

**Improvement Strategies.** Although real-world instructions from human users are more suitable for fine-tuning LLMs, it is difficult to collect them at a large scale. As alternatives to human-generated instructions, most existing research mainly adopts synthetic instructions generated by LLMs. However, there are some potential problems with synthetic instructions, such as poor topic diversity and uneven instruction difficulty (either too simple or too difficult). Thus, it is necessary to improve the quality of the synthetic instructions. Next, we summarize four major improvement strategies widely used in existing work as follows:
> 合成指令存在问题，例如低的主题多样性和不平稳的指令难度
> 我们总结四个提高合成指令质量的策略

- *Enhancing the instruction complexity.* As discussed in existing work [346], enhancing the complexity of instructions can improve the model capacity of LLMs in following complex instructions, e.g., including more task demands or requiring more reasoning steps. To validate this strategy, we follow WizardLM [346] by gradually increasing the complexity levels, e.g., adding constraints, increasing reasoning steps, and complicating the input. We leverage the publicly released WizardLM-70K instructions [346] as the complexity-enhanced instruction dataset, which has been generated via the above enhancement approach based on the Self-Instruct-52K dataset [346].
> 增强指令复杂度
> 增强指令复杂度可以提高模型遵循复杂指令的能力
> 包括 include 更多任务要求，要求更多 reasoning 步骤数
> 我们逐步提高指令的复杂度等级：增加限制、提高 reasoning 步骤数、复杂化输入
- *Increasing the topic diversity.* In addition to the complexity, improving the topic diversity of the instruction dataset can help elicit different abilities of LLMs on diverse tasks in real world [347]. However, it is difficult to directly control the self-instruct process for generating diverse instructions. Following YuLan-Chat [352], we employ ChatGPT to rewrite the instructions from Self-Instruct-52K dataset for adapting them into 293 topics via specific prompts. Finally, we obtain 70K instructions as the diversity-increased dataset.
> 提高主题多样性
> 提高主题多样性可以引出模型在多样任务上的能力
> 我们用 ChatGPT 重写生成的指令，用 prompt 将它们写为293个主题

- *Scaling the instruction number.* In addition to the above aspects, the number of instructions is also an important factor that may affect the model performance. Specially, using more instructions can extend the task knowledge and improve the ability of instruction following for LLMs [69]. To examine this strategy, we sample new instructions from the synthesized instruction set released from the MOSS project [362], as they are also synthesized using the same self-instruct method [143]. We mix them with the SelfInstruct-52K dataset to compose a larger one containing 220K instructions.
> 扩展指令数量

- *Balancing the instruction difficulty.* As the synthetic instructions tend to contain too easy or too hard ones, it is likely to result in training instability or even overfitting for LLMs. To explore the potential effects, we leverage the perplexity score of LLMs to estimate the difficulty of instructions and remove too easy or too hard instructions. To generate the same scale of instructions for fair comparison, we adopt a LLaMA (7B) model to compute the perplexity for the 220K instructions from the large instruction dataset, and then keep 70K instructions of moderate perplexity scores as the difficulty-balanced dataset.
> 平衡指令难度
> 合成指令趋向于过于简单或过于难，容易导致 LLM 得训练不稳定性甚至过拟合
> 我们用 LLM 计算的 perplexity score 评估指令的难度，并移除过于简单或过难的指令，保持中等 perplexity score 的指令作为平衡了难度的数据集

**Experimental Setup.** To conduct the experiments on the effect of instruction data, we leverage these new instruction datasets for tuning LLaMA, a popular LLM backbone that has been widely used for instruction-tuning. We use the code from YuLan-Chat [352] for our experiments, and train LLaMA 7B and 13B on a server of 8 A800-80G GPUs. All the hyper-parameters settings remain the same as Stanford Alpaca. 
> 调节 LLaMA7B 和 13B，超参数和 Standford Alpaca 相同

To better evaluate the instruction following ability of fine-tuned models, we consider two settings, namely Chat setting and QA setting. 
> 采用 Chat setting 和 QA setting 评估

The chat setting mainly utilizes user instructions and queries from daily chat, whereas the QA setting mainly employs question answering examples from existing NLP datasets. The evaluation on the chat setting is conducted based on the AlpacaFarm evaluation set [363]. Instead of using a full pairwise comparison, we select the LLaMA 7B and 13B models fine-tuned on SelfInstruct-52K as the reference baselines, and then compare them with other fine-tuned LLaMA 7B and 13B models using different instructions, respectively. Since our focus is to examine the usefulness of different strategies to generate the instructions, the model fine-tuned on Self-Instruct-52K can serve as a good reference. Following AlpacaFarm [363], for each comparison, we employ ChatGPT to automatically annotate which response from two compared models each time is the best for the user query, and report the win rate (%) as the evaluation metric. 
> 对于 Chat setting
> 使用 SelfInstruct-52K 微调的 LLaMA 7B/13B 作为 baseline
> 采用 ChatGPT 自动标记两个比较的模型的回答哪个更适合回答 user query，报告 win rate 作为评估度量

For the QA setting, we select two benchmarks, MMLU [364] and BBH [365], and evaluate the accuracy based on their default settings by using heuristic rules to parse the answers from these LLMs.
For both instruction tuning and evaluation, we adopt the following prompt: `“The following is a conversation between a human and an AI assistant. The AI assistant gives helpful, detailed, and polite answers to the user’s questions.\n [|Human|]:{input}\n[|AI|]:”`. To reproduce our results, we release the code and data at the link: https://github. com/RUCAIBox/LLMSurvey/tree/main/Experiments.
> 对于 QA setting
> 使用 MMLU 和 BBH 作为 benchmark

**Results and Analysis.** The results using different instruction datasets based on 7B and 13B LLaMA are in Table 9. Next, we summarize and analyze our findings in detail.
> 结果分析
- *Task-formatted instructions are more proper for the QA setting, but may not be useful for the chat setting.* By comparing the performance of instruction tuning using FLAN-T5 with that of ShareGPT and Self-Instruct-52K, we can observe that FLAN-T5 mostly achieves a better performance on QA benchmarks while underperforms ShareGPT on the chat setting. The reason is that FLAN-T5 is composed of a mixture of instructions and examples from existing NLP tasks, e.g., translation and reading comprehension. As a result, LLaMA fine-tuned with FLAN-T5 performs better on QA tasks, but poorly on user queries. In contrast, ShareGPT consists of real-world human-ChatGPT conversations, which is able to better elicit LLaMA to follow user instructions in daily life, while may not be suitable for accomplishing the QA tasks.
> Task-formatted 的指令更适合 QA，在 Chat 中不有用

- *A mixture of different kinds of instructions are helpful to improve the comprehensive abilities of LLMs.* After mixing the three kinds of instructions for fine-tuning, we can see that the derived LLaMA variant (with FLAN-T5, ShareGPT and Self-Instruct-52K) performs well in both task settings. In MMLU, the performance of LLaMA (7B) can surpass the ones using individual instruction set by a large margin, i.e., 43.69 vs. 38.58 (FLAN-T5). It shows that mixing multiple sources of instruction datasets is helpful to improve the performance of instruction-tuned LLMs, which scales the instruction number as well as increases the diversity.
> 不同类型的指令混合有助于提高 LLM 的综合能力

- *Enhancing the complexity and diversity of instructions leads to an improved model performance.* By increasing the complexity and diversity of the Self-Instruct-52K dataset respectively, the chat and QA performance of LLaMA can be consistently improved, e.g., from 37.52 to 39.73 in MMLU for LLaMA (7B). It demonstrates that both strategies are useful to improve the instruction following ability of LLMs. Further, we can see that improving the complexity yields a larger performance improvement on QA tasks. The reason is that the QA tasks mostly consist of difficult questions for evaluating LLMs, which can be better solved by LLMs that have learned complex instructions at the fine-tuning stage.
> 提高指令的复杂度和多样性有助于提高 LLM 的表现，且而提高复杂度有助于提高 QA 任务表现

- *Simply increasing the number of instructions may not be that useful, and balancing the difficulty is not always helpful.* As the results shown in Table 9, balancing the difficulty and increasing the number of fine-tuning instructions are not very helpful in our experiments. Especially for scaling the instruction number, it even hurts the performance, e.g., a decrease from 29.81 to 26.63 in BBH for LLaMA (7B). It shows that simply scaling the number of synthesized instructions without quality control may not be effective to improve the performance. Furthermore, fine-tuning with the instructions of moderate difficulty also performs well in the chat setting, while slightly decreasing the performance in the QA setting. A possible reason is that we filter complex and hard instructions with large perplexity scores, hurting the model performance in answering complex questions.
> 简单提高指令数量不一定有用，平衡难度不总是有用
> 简单提高指令数量且不进行质量控制甚至可能损害表现
> 使用中等难度的指令对于 Chat setting 有效果，但对于 QA setting 起到反效果，可能的原因是高 perplexity score 的复杂指令被过滤了

- *A larger model scale leads to a better instruction following performance.* By comparing the performance of LLaMA (7B) and LLaMA (13B) models fine-tuned with the same set of instruction data, we can see that LLaMA (13B) mostly achieves a better performance. It indicates that scaling the model size is helpful for improving the instruction following capability. Besides, we can see that the QA performance has been improved a lot, e.g., from 38.11 to 47.49 in MMLU. It is likely because that the larger models generally have better knowledge utilization and reasoning capability [33, 55], which can accurately answer more complex questions.
 >更大规模的模型有更好的指令遵守表现


> Instruction Tuning Suggestions
>
>To conduct instruction tuning on LLMs, one can prepare the computational resources according to the basic statistics about the required number of GPUs and tuning time in Table 8. After setting up the development environment, we recommend beginners to follow the code of Alpaca repository [137] for instruction tuning. Subsequently, one should select the base model and construct the instruction datasets as we discuss in this section. When computational resources for training are constrained, users can utilize LoRA for parameterefficient tuning (see Section 5.3). As for inference, users can further use quantization methods to deploy LLMs on fewer or smaller GPUs (see Section 5.4).

## 5.2 Alignment Tuning
This part first presents the background of alignment with its definition and criteria, then focuses on the collection of human feedback data for aligning LLMs, and finally discusses the key technique of reinforcement learning from human feedback (RLHF) for alignment tuning.
> 本节首先展示对齐的背景及其定义和标准
> 然后介绍收集用于对齐的 human feedback 数据
> 最后讨论 RLHF 
### 5.2.1 Background and Criteria for Alignment
**Background.** LLMs have shown remarkable capabilities in a wide range of NLP tasks [55, 56, 67, 90]. However, these models may sometimes exhibit unintended behaviors, e.g., fabricating false information, pursuing inaccurate objectives, and producing harmful, misleading, and biased expressions [66, 366]. For LLMs, the language modeling objective pre-trains the model parameters by word prediction while lacking the consideration of human values or preferences. To avert these unexpected behaviors, human alignment has been proposed to make LLMs act in line with human expectations [66, 367]. However, unlike the original pre-training and adaptation tuning (e.g., instruction tuning), such an alignment requires considering very different criteria (e.g., helpfulness, honesty, and harmlessness). It has been shown that alignment might harm the general abilities of LLMs to some extent, which is called alignment tax in related literature [368].
> LLM 的语言建模预训练目标是缺乏对人类价值和偏好的考虑的
> alignment 可能会损害 LLM 的一般能力，这被称为 alignment tax

**Alignment Criteria.** Recently, there is increasing attention on developing multifarious criteria to regulate the behaviors of LLMs. Here, we take three representative alignment criteria (i.e., helpful, honest, and harmless) as examples for discussion, which have been widely adopted in existing literature [66, 368]. In addition, there are other alignment criteria for LLMs from different perspectives including behavior, intent, incentive, and inner aspects [366], which are essentially similar (or at least with similar alignment techniques) to the above three criteria. It is also feasible to modify the three criteria according to specific needs, e.g., substituting honesty with correctness [116]. Next, we give brief explanations about the three representative alignment criteria:
> 我们讨论三个有代表性的 alignment 标准：有帮助、无害、诚实
> 对于 LLM 还有其他许多 alignment criteria，这些对齐标准和上面三个标准实质上是类似的

- *Helpfulness.* To be helpful, the LLM should demonstrate a clear attempt to assist users in solving their tasks or answering questions in a concise and efficient manner as possible. At a higher level, when further clarification is needed, the LLM should demonstrate the capability of eliciting additional relevant information through pertinent inquiries and exhibit suitable levels of sensitivity, perceptiveness, and prudence [368]. Realizing the alignment of helpful behavior is challenging for LLMs since it is difficult to precisely define and measure the intention of users [366].
> 有帮助性
> LLM 应该表现出尝试解决用户问题，并且可以给出进一步的解释，能够通过询问引出进一步的信息
- *Honesty.* At a basic level, a LLM aligned to be honest should present accurate content to users instead of fabricating information. Additionally, it is crucial for the LLM to convey appropriate degrees of uncertainty in its output, in order to avoid any form of deception or misrepresentation of information. This requires the model to know about its capabilities and levels of knowledge (e.g., “know unknowns”). According to the discussion in [368], honesty is a more objective criterion compared to helpfulness and harmlessness, hence honesty alignment could potentially be developed with less reliance on human efforts.
> 诚实
> LLM 应该给用户展示真实的内容而非伪造，且需要在其输出表达一定的不确定性，避免欺骗用户
> 这要求模型知道自己的能力和知识水平
> 相较于 Helpfulness 和 Harmlessness，Honesty 是一个更客观的标准

- *Harmlessness.* To be harmless, it requires that the language produced by the model should not be offensive or discriminatory. To the best of its abilities, the model should be capable of detecting covert endeavors aimed at soliciting requests for malicious purposes. Ideally, when the model was induced to conduct a dangerous action (e.g., committing a crime), the LLM should politely refuse. Nonetheless, what behaviors are deemed harmful and to what extent vary amongst individuals or societies [368] highly depend on who is using the LLM, the type of the posed question, and the context (e.g., time) at which the LLM is being used.
> 无害
> 这要求模型生成的语言需要不冒犯和不歧视，且 LLM 需要检测出尝试请求有害答案的问题，并礼貌拒绝

As we can see, these criteria are quite subjective, and are developed based on human cognition. Thus, it is difficult to directly formulate them as optimization objectives for LLMs. In existing work, there are many ways to fulfill these criteria when aligning LLMs. A promising technique is red teaming [369], which involves using manual or automated means to probe LLMs in an adversarial way to generate harmful outputs and then updates LLMs to prevent such outputs.
> 这三个对齐标准较为主观，是基于人类认知建立的，因此不容易直接将它们公式化为一个 LLM 的优化目标
> 现存的工作中，red teaming 使用人工或自动化的方式以对抗的方式 probe LLM to 生成 harmful 的输出，然后更新 LLM 防止这类输出
### 5.2.2 Collecting Human Feedback
During the pre-training stage, LLMs are trained using the language modeling objective on a large-scale corpus. However, it cannot take into account the subjective and qualitative evaluations of LLM outputs by humans (called human feedback in this survey). High-quality human feedback is extremely important for aligning LLMs with human preferences and values. In this part, we discuss how to select a team of human labelers for feedback data collection.
> 高质量的人类反馈对于对齐 LLM 极其重要
> 本部分讨论如何选择 a team of human labelers 用于收集反馈数据

**Human Labeler Selection.** In existing work, the dominant method for generating human feedback data is human annotation [66, 116, 367]. 
This highlights the critical role of selecting appropriate human labelers. To provide highquality feedback, human labelers are supposed to have a qualified level of education and excellent proficiency in English. For example, Sparrow [116] requires human labelers to be UK-based native English speakers who have obtained at least an undergraduate-level educational qualification. Even then, several studies [367] have found that there still exists a mismatch between the intentions of researchers and human labelers, which may lead to low-quality human feedback and cause LLMs to produce unexpected output. 
To address this issue, InstructGPT [66] further conducts a screening process to filter labelers by assessing the agreement between human labelers and researchers. Specifically, researchers first label a small amount of data and then measure the agreement between themselves and human labelers. The labelers with the highest agreement will be selected to proceed with the subsequent annotation work. In some other work [370], “super raters” are used to ensure the high quality of human feedback. Researchers evaluate the performance of human labelers and select a group of well-performing human labelers (e.g., high agreement) as super raters. The super raters will be given priority to collaborate with the researchers in the subsequent study. When human labelers annotate the output of LLMs, it is helpful to specify detailed instructions and provide instant guidance for human labelers, which can further regulate the annotation of labelers.
> 现存工作中，主流的生成 human feedback 的方法就是人工标注
> 人类标注者应该熟悉英语且受过教育，避免低质量的 human feedback

**Human Feedback Collection.** In existing work, there are mainly three kinds of approaches to collecting feedback and preference data from human labelers.
> 目前有三种收集人类反馈的方法
- *Ranking-based approach.* In early work [367], human labelers often evaluate model-generated outputs in a coarsegrained manner (i.e., only selecting the best) without taking into account more fine-grained alignment criteria. Nonetheless, different labelers may hold diverse opinions on the selection of the best candidate output, and this method disregards the unselected samples, which may lead to inaccurate or incomplete human feedback. To address this issue, subsequent studies [116] introduce the Elo rating system to derive the preference ranking by comparing candidate outputs. The ranking of outputs serves as the training signal that guides the model to prefer certain outputs over others, thus inducing outputs that are more reliable and safer.
> 基于排序
> 早期工作中，human labeler 为 LLM 的输出粗粒度评估 (仅选出最好的)，不考虑细粒度的对齐标准，这会导致未被选择的样本被丢弃
> 后续研究引入 Elo rating 系统，通过比较 candidate outputs 得到偏好排序，人类标记者的偏好将引导 LLM 的训练
- *Question-based approach.* Further, human labelers can provide more detailed feedback by answering certain questions designed by researchers [81], covering the alignment criteria as well as additional constraints for LLMs. Specially, in WebGPT [81], to assist the model in filtering and utilizing relevant information from retrieved documents, human labelers are required to answer questions with multiple options about whether the retrieved documents are useful for answering the given input.
> 基于问题
> 人类标记者可以通过回答研究者设计的特定问题提供更细节的反馈
> 例如 WebGPT 中，人类标记者会回答关于 retrieved documents 对于给定的输入是否有用的多选题
- *Rule-based approach.* Many studies also develop rulebased methods to provide more detailed human feedback. As a typical case, Sparrow [116] not only selects the response that labelers consider the best but also uses a series of rules to test whether model-generated responses meet the alignment criteria of being helpful, correct, and harmless. In this way, two kinds of human feedback data can be obtained: (1) the response preference feedback is obtained by comparing the quality of model-generated output in pairs, and (2) the rule violation feedback is obtained by collecting the assessment from human labelers (i.e., a score indicating to what extent the generated output has violated the rules). Furthermore, GPT-4 [46] utilizes a set of zero-shot classifiers (based on GPT-4 itself) as rule-based reward models, which can automatically determine whether the model-generated outputs violate a set of human-written rules.
> 基于规则
> Sparrow 还设定了一系列规则测试模型生成的回答是否满足对齐标准，因此存在了两类的人类反馈数据: 对 response 的偏好反馈和对 rule 的 violation 反馈
> GPT-4使用了一系列 zero-shot 分类器作为基于规则的奖励模型，自动决定模型生成的输出是否 violate 一系列 human-written 的规则

In the following, we focus on a well-known technique, reinforcement learning from human feedback (RLHF), which has been widely used in the recent powerful LLMs such as ChatGPT. As discussed below, the alignment criteria introduced in Section 5.2.1 can be fulfilled by learning from human feedback on the responses of LLMs to users’ queries.
### 5.2.3 Reinforcement Learning from Human Feedback
To align LLMs with human values, reinforcement learning from human feedback (RLHF) [79, 367] has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria (e.g., helpfulness, honesty, and harmlessness). RLHF employs reinforcement learning (RL) algorithms (e.g., Proximal Policy Optimization (PPO) [128]) to adapt LLMs to human feedback by learning a reward model. Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT [66].
> RLHF 采用 PPO 强化学习算法将 LLM 根据人类反馈适应
> 该方法将人类包含进训练过程，以开发良好对齐的 LLM

**RLHF System.** The RLHF system mainly comprises three key components: a pre-trained LM to be aligned, a reward model learning from human feedback, and a RL algorithm training the LM. Specifically, the pre-trained LM is typically a generative model that is initialized with existing pretrained LM parameters. For example, OpenAI uses 175B GPT-3 for its first popular RLHF model, InstructGPT [66], and DeepMind uses the 280 billion parameter model Gopher [64] for its GopherCite model [370]. Further, the reward model (RM) provides (learned) guidance signals that reflect human preferences for the text generated by the LM, usually in the form of a scalar value. The reward model can take on two forms: a fine-tuned LM or a LM trained de novo using human preference data. Existing work typically employs reward models having a parameter scale different from that of the aligned LM [66, 370]. For example, OpenAI uses 6B GPT-3 and DeepMind uses 7B Gopher as the reward model, respectively. Finally, to optimize the pre-trained LM using the signal from the reward model, a specific RL algorithm is designed for large-scale model tuning. Specifically, Proximal Policy Optimization (PPO) [128] is a widely used RL algorithm for alignment in existing work [66, 116, 370].
> RLHF 系统有三个成分：一个需要对齐的预训练的 LLM，一个从人类反馈学习的奖励模型，一个训练 LLM 的 RL 算法
> 奖励模型提供一个学习到的引导信号，反映 LLM 生成的文本的人类偏好，通常是一个标量，奖励模型可以有两种形式：一个微调过的语言模型和使用人类偏好数据从头训练的语言模型
> 现存工作使用的奖励模型一般是一个参数规模和要对齐的语言模型不同的语言模型
> 最后，为了使用来自奖励模型的信号优化预训练的语言模型，会使用一个特定的 RL 算法，广泛使用的是 PPO

**Key Steps for RLHF.** Figure 12 illustrates the overall threestep process of RLHF [66] as introduced below.
> 接下来介绍 RLHF 的完整三个步骤 
![[A Survey of LLMs-Fig12.png]]

- *Supervised fine-tuning.* To make the LM initially perform desired behaviors, it usually needs to collect a supervised dataset containing input prompts (instruction) and desired outputs for fine-tuning the LM. These prompts and outputs can be written by human labelers for some specific tasks while ensuring the diversity of tasks. For example, InstructGPT [66] asks human labelers to compose prompts (e.g., “List five ideas for how to regain enthusiasm for my career”) and desired outputs for several generative tasks such as open QA, brainstorming, chatting, and rewriting. Note that the first step is optional in specific settings or scenarios.
> 有监督微调
> 首先收集包含了 input prompts (instruction) 和对应 outputs 的有监督数据集，进行有监督微调
> 可以由人类标记者针对特定任务写一些 prompts 和 outputs，以保持任务的多样性
> 这一步是可选的

- *Reward model training.* The second step is to train the RM using human feedback data. Specifically, we employ the LM to generate a certain number of output texts using sampled prompts (from either the supervised dataset or the human-generated prompt) as input. We then invite human labelers to annotate the preference for these pairs. The annotation process can be conducted in multiple forms, and a common approach is to annotate by ranking the generated candidate texts, which can reduce the inconsistency among annotators. Then, the RM is trained to predict the human-preferred output. In InstructGPT, labelers rank model-generated outputs from best to worst, and the RM (i.e., 6B GPT-3) is trained to predict the ranking. Note that, in recent work [371], the annotation of preference on response pairs has been conducted by an AI agent (usually an aligned LLM) instead of humans, which is called “reinforcement learning from AI feedback (RLAIF)”. LLMs trained with typical RLHF algorithms tend to generate harmless responses with less helpfulness, which is called evasion problem [371]. To guarantee both the harmlessness and helpfulness, RLAIF generates the AI feedback based on pre-set alignment principles in instructions [371, 372], which can also reduce the efforts of human annotation.
> 奖励模型训练
> 该步骤即使用人类反馈数据训练奖励模型，具体地说，我们先使用采样的 prompts feed into LM，让 LM 生成特定数量的输出，然后让 human 为这些输出标记偏好，例如对这些 candidate  text 排序
> 然后奖励模型被训练以生成 human-prefered 的输出
> InstructGPT 中，人类标记着为模型生成的输出排序，从最好到最差，然后奖励模型 (6B 的 GPT-3) 被训练预测该排序
> 最近的工作中，对于输出的偏好标记可以被一个 AI 智能体 (通常是一个对齐的 LLM) 完成，而不需要人类，这被称为 RLAIF
> 由 RLHF 算法训练的 LLMs 倾向于生成更无害和但比较没有帮助的回答，即逃避问题，为了同时保证无害性和有帮助性，RLAIF 会基于先设定好的指令对齐规则以生成 AI 反馈

- *RL fine-tuning.* At this step, aligning (i.e., fine-tuning) the LM is formalized as an RL problem. In this setting, the pre-trained LM acts as the policy that takes as input a prompt and returns an output text, the action space of it is the vocabulary, the state is the currently generated token sequence, and the reward is provided by the RM. To avoid eviating significantly from the initial (before tuning) LM, a penalty term is commonly incorporated into the reward function. For example, InstructGPT optimizes the LM against the RM using the PPO algorithm. For each input prompt, InstructGPT calculates the KL divergence between the generated results from the current LM and the initial LM as the penalty. It is noted that the second and final steps can be iterated in multiple turns for better aligning LLMs. Due to the instability of the RL algorithm, recent work [373] replaces the RL tuning with another supervised fine-tuning by reusing the best ranked samples with higher rewards.
> RL 微调
> 对于 LM 的对齐被构造为一个 RL 问题，在这一步中，预训练好的 LM 的 policy 是接受输入 prompt ，产生一个输出文本，动作空间是词袋，状态是当前生成的 token 序列，奖励则由奖励模型提供
> 为了避免和原来的预训练模型偏移太多，奖励函数中通常会包含一个惩罚项
> InstructGPT 使用 PPO 算法优化 LM，对于每个输入 prompt ，InstructGPT 会计算当前 LM 的生成的结果和最初的 LM 生成的结果的 KL 散度，作为惩罚项

**Practical Strategies for RLHF.** Although RLHF is promising to effectively improve the alignment of LLMs with humans, it is practically challenging for researchers to successfully implement it. In this part, we focus on discussing several useful strategies and tricks for improving the effectiveness and efficiency of RLHF. Concretely, we focus on the effective training of reward models, efficient and effective RL training, respectively.
>我们在这部分讨论提高 RLHF 的效率的几个方法，我们会分别讨论奖励模型的高效训练和 RL 训练的高效实行

- *Effective reward model training.* Despite that InstructGPT used a small reward model (6B GPT model), increasing work [99] has shown it is often more effective to use a large reward model (e.g., equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs.  In LLaMa 2 [99], pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge. Whereas, it is common to encounter the overfitting problem when training large-scale reward models. As a simple yet effective solution, existing work [374, 375] has introduced the LM loss on the preferred response of the input prompt from the human-annotated alignment dataset as a regularizer, which alleviates the overfitting of the reward model on the binary classification task. In addition, as there are multiple criteria for alignment (e.g., helpfulness and honesty), it is often difficult to train a single reward model that can satisfy all the alignment criteria. Therefore, it is useful to train multiple reward models that focus on different alignment criteria [99], and compute the final reward based on the produced ones from them via special combination strategies (e.g., mean pooling and weighted sum). Such a way enables more flexible rules or standards on multiple criteria, e.g., relaxing the requirement on helpfulness while posing more strict limits on harmfulness.
> 训练有效的奖励模型
> 许多工作表明使用更大的奖励模型（比原来的模型还大）会更加有效，因为大的奖励模型一般在决定 LLM 生成的输出的结果质量方面表现更好
 > LLaMA 2使用一个预训练的模型 checkpoint 初始化奖励模型，他们认为通过共享相同的预训练知识，可以减少 LM 和 RM 之间的信息不匹配
 > 但训练大的奖励模型容易遭遇过拟合问题，现存的工作使用 LM 在 preferred respones 上的 loss 作为正则项，以减轻奖励模型在二元分类任务中的过拟合
 > 另外，由于存在多个对齐标准，仅训练一个奖励模型是不够的，需要训练多个奖励模型，分别注重于不同的对齐标准，并通过特殊的结合技巧 (mean pooling and weight sum)，计算最终的奖励，在结合的过程中，我们可以灵活设定各个对齐标准的相对重要性

- *Effective RL training.* As the RL training process tends to be unstable and hyper-parameter sensitive, it is suggested that the language model should be well supervised finetuned before RL training, so as to reaching a good model capacity. A commonly-used way is to fine-tune the LLM on its best outputs of the prompts (referred to as rejection sampling or best-of-N) from the alignment dataset until convergence before RL. Given a prompt, the LLM would first produce N outputs via the sampling algorithm, and then the best candidate from the model will be selected by the reward model for learning. After fine-tuning the LLM on the best samples until convergence, the RL process will be performed to further improve the performance. LLaMA 2 [99] has successively trained five versions of RLHF models, where the LLM has been progressively improved with the improvement of the reward models. In this way, the collected prompts and annotations of human preference data can better reflect the issues of the current model checkpoint, thus making special tuning to address these issues. In addition, LLaMA 2 also adds samples from prior iterations into the subsequent ones, to alleviate the possible capacity regression issue during iterative optimization.
> 进行有效的 RL 训练
> RL 训练过程倾向于不稳定，且对超参数敏感，因此 LLM 需要在 RL 训练之前首先被有监督微调好
> 一个常用的方式就是用 LLM 最好的输出结果 (rejection sampling or best of-N) 对其进行微调，prompt 来自于 alignment 数据集，在 RL 训练之前首先微调到收敛：给定一个 prompt，LLM 会首先生成 N 个输出，然后其中最好的一个会被奖励模型选出来，用于微调
> 在微调之后，进行 RL 进一步提高表现
> LLaMA 2训练了5个版本的 RLHF 模型，其中 LLM 随着奖励模型的提升而进步，LLaMA 2还将先前迭代中的部分样本加入到后续迭代中，以减轻迭代时优化时的 capacity regression 问题

- *Efficient RL training.* As the RL training requires iterate the inference process of both the LLM and reward models, it would greatly increase the total memory and computation cost, especially for larger reward models and LLMs. As a practical trick, we can deploy the reward model on a separate server, and invoke the corresponding API to work with the LLM on its own server. In addition, as RLHF requires the LLM to generate multiple candidate outputs, instead of calling the sample decoding procedure for multiple times, it is more efficient to utilize the beam search decoding algorithm. It only needs to perform onepass decoding for response generation, meanwhile such a strategy can also enhance the diversity of the generated candidate responses.
> 进行高效的 RL 训练
> RL 训练需要迭代 LLM 和奖励模型的推理过程，会提高存储和计算开销
> 一个 trick 是在一个分离的 server 上部署奖励模型，调用对应的 API 和本地 server 的 LLM 协同
> 另外，因为 RLHF 需要 LLM 生成多个候选的输出，我们可以采用 group beam search，只需要 one pass，而不是调用多次 decoding 过程，并且该策略也可以提高生成的候选 response 的多样性

**Process-Supervised RLHF.** In existing literature of RLHF [376], the supervision signals for RL training can be generally classified into two distinct categories: outcomesupervision signals and process-supervision signals. 
The outcome-supervised RLHF employs a quantitative score to assess the quality of the whole text generated by LLMs. 
In contrast, process-supervised RLHF offers an evaluation of each individual component (e.g., sentence, word, or reasoning step) within the generated content, which can provide fine-grained supervision signals to guide the training, helping LLMs refine the undesired generation contents [376, 377].
> RLHF 中，RL 训练的监督 signal 可以分为两类：outcome supervision signal 和 process supervision signal
> outcome-supervised RLHF 采用量化的分数评估 LLM 生成的全文的指令
> process-supervised RLHF 对生成的内容的各个部分单独评估 (例如 sentence, word, reasoning step)，提供了更加细粒度的监督信号

OpenAI has proposed a fine-grained annotation dataset named PRM800k [377] consisting of 12K process-annotated mathematical problems (i.e., MATH dataset [378]) and 75K solutions generated by LLMs of these problems, where each reasoning step of mathematical problems is labeled as positive, negative or neutral in PRM800k. This fine-grained dataset has been utilized in existing work [377, 379] to train the process-supervised reward models (PRM), and the probability from the prediction of each label can be considered as the supervision signals during RLHF procedure. 
> PRM800k 数据集中，包括了 process-annotated 的数学问题以及 LLM 对这些问题生成的答案，其中每一步推理过程都被标记为: positive/negative/neutral
> 现存有工作使用了该细粒度的数据集，训练 process-supervised 奖励模型 (PRM)，之后，奖励模型对于每个 label (positive/negative/neutral) 的预测概率就可以作为 RLHF 过程中的有监督信号

To effectively leverage processsupervision signals from PRMs, existing work [376] has utilized expert iteration [380, 381], an effective RL algorithm to improve the base policy via learning from expert policy. Typically, expert iteration contains two main stages: policy improvement and distillation [376]. In the policy improvement stage, expert policy processes the systematic search procedure to produce the samples. PRMs provide process-supervision signals to guide expert policy in the search procedure and enhance the quality of samples. Subsequently, during the distillation stage, the samples generated by expert policy in the first stage are utilized to improve the base policy through supervised fine-tuning. In addition to expert iteration, PRMs can also be utilized to re-rank the candidates of the final answers generated by LLMs [377] or to select better intermediate reasoning steps during step by step reasoning [379, 382].
> 现存有工作利用 expert iteration 算法，通过学习 expert policy，提高 base policy，以更好地利用 process-supervision 信号
> expert iteration 包含两个阶段：policy improvement 和 distillation
> policy improvement 阶段中，expert policy 进行系统的搜索以生成样本，奖励模型提供过程监督信号，引导 expert policy 的搜索，之后，在 distillation 阶段，由 expert policy 在第一阶段生成的样本会被用于通过 supervised fine-tuning 提高 base policy
> 除了用于 expert iteration 强化学习过程中，奖励模型还可以被用于重新排序 LLM 生成的最终答案候选，或者在一步一步的分析中选择出更好的步骤
### 5.2.4 Alignment without RLHF
Although RLHF has achieved great success in aligning the behaviors of LLMs with human values and preferences, it also suffers from notable limitations. First, RLHF needs to train multiple LMs including the model being aligned, the reward model, and the reference model at the same time, which is tedious in algorithmic procedure and memoryconsuming in practice. Besides, the commonly-used PPO algorithm in RLHF is rather complex and often sensitive to hyper-parameters. As an alternative, increasing studies explore to directly optimize LLMs to adhere to human preferences, using supervised fine-tuning without reinforcement learning [349].
> RLHF 存在限制
> RLHF 需要训练多个 LM ，包括了奖励模型和推理模型，实际中非常耗存
> RLHF 中常用的 PPO 算法较为复杂，对超参数敏感
> 作为替代，有人研究直接使用有监督微调，无强化学习，来对齐 LLM

**Overview.** The basic idea of non-RL alignment approaches is to directly fine-tune LLMs with supervised learning on high-quality alignment dataset. It basically assumes that response feedback or golden rules to avert unsafe behaviors have been injected or included in the specially curated alignment dataset, so that LLMs can directly learn aligned behaviors from these demonstration data via suitable fine-tuning strategies. Thus, to implement this approach, two key issues are the construction of alignment dataset and the design of fine-tuning loss. For the first issue, the alignment dataset can be automatically constructed by an aligned LLMs according to human-written safety principles [347] or refining existing examples using edits operations [383]. In addition, we can also reuse existing reward models to select highrated responses from existing human feedback data [373]. For the second issue, non-RL alignment approaches mainly fine-tune LLMs in a supervised learning way (the same as the original instruction tuning loss) on a high-quality alignment dataset, meanwhile auxiliary learning objectives can be used to enhance the alignment performance, e.g., ranking responses or contrasting instruction-response pairs.
> non-RL 对齐的基本思想就是直接用在 alignment 数据集上用有监督学习微调 LLM
> 该方法假设 response feedback 或避免危险行为的引导规则已经包含或被注入在了 alignment 数据集中，因此 LLM 可以直接从数据展示中学习到 aligned 的行为
> 该方法的执行有两个关键问题：对齐数据集的构建和微调损失的设计
> 对于第一个问题：alignment 数据集可以让一个已经 aligned 的 LLM 根据 human-written 的安全规则自动创建；或者编辑优化现存的数据样本，或者可以复用现存的奖励模型，从现存的 human feedback 数据中选择出高分的部分
> 对于第二个问题，non-RL alignment 方法主要用有监督学习微调 LLM，即和指令微调损失相同，但同时可以使用辅助的学习目标来强化对齐表现，例如对 responses 进行排序或比较 instruction-response 对

**Alignment Data Collection.** The construction of alignment data is important to effectively align the behaviors of LLMs with human preferences. To collect high-quality alignment data, some work tries to reuse existing reward models to select high-rated responses, and others explore to leverage powerful LLMs (e.g., ChatGPT) or build a simulated environment to generate synthetic alignment examples. Next, we will discuss these three lines of research.
> 对齐数据收集
> 一些工作尝试使用现存的奖励模型，选择 high-rated responses
> 一些工作使用强大的 LLMs，如 ChatGPT 构建模拟的环境以生成 alignment 样本
> 我们讨论对齐数据收集的三类方法

- *Reward model based approaches.* The reward model in RLHF has been trained to measure the alignment degree on the responses of LLMs. It is straightforward to leverage existing reward models to select high-quality responses as alignment data for subsequent fine-tuning. Based on this idea, RAFT [373] adopts reward models trained on human preference data to rank the responses of LLMs and collect those with higher rewards for supervised fine-tuning. In addition, the reward model can be also used to score model responses and assign them to different quality groups. Quark [384] sorts the responses of LLMs into different quantiles based on the reward scores. Each quantile is attached with a special reward token to represent the reward level of the quantile. Conditioned on the highest-reward tokens, LLMs are subsequently prompted to generate high-quality responses. Given an initial answer and the corresponding human feedback, ILF [385] first adopts LLMs to generate refined answers, then utilizes the reward model to select the answer that best matches the feedback for further training. As valuable resources for aligning LLMs, several reward models have been released, including DeBERTa base/large/xxlarge from OpenAssistant, Moss-7B from Fudan, and Flan-T5-xl from Stanford.
> 基于 RM 的方法
> 思想：RLHF 中，RM 被训练来衡量 LLM 的 response 的 alignment degree，因此可以用 RM 选择高质量的 response 作为 alignment 数据，用于后续微调
> 例如，RAFT 使用 RM 对 LLMs 的回答排序，收集高分的回答用于有监督微调
> 另外，RM 也可以用于给更细粒度地对 LLMs 的回答打分，将它们分到不同的 quality group，例如 Quark 将 LLMs 的 responses 分为不同的 quantile，每个 quantile 附加一个特殊的 reward token，表示 quantile 的 reward level
> 另外，RM 还可以用于选出对应最为匹配对应答案的 human feedback
- *LLM based generative approaches.* Reward models help to select aligned data from model responses. However, training reward models itself necessitates substantial highquality human-labeled data, which is typically expensive and in short supply. In addition, although existing reward models can be reused, they might not be able to accurately capture the nonalignment behaviors in another separately trained LLM. Therefore, some work explores leveraging powerful LLMs to automatically generate human-aligned data. As a representative work, constitutional AI [371] proposes that human supervision comes from a set of principles (i.e., natural language instructions) governing AI behaviors. Based on these principles, LLMs will critique their own harmful responses and revise them repeatedly into finally aligned responses. Similarly, Self-Align [347] first adopts self-instruct [143] to generate instructions focusing on covering diverse topics. Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several incontext exemplars), to generate helpful, ethical, and reliable responses as alignment data. To mitigate the limit that the original SFT method can only learn from positive responses, FIGA [386] develops an improved supervised alignment approach, where both negative (the original output of low quality) and positive (the refined output by LLMs) responses are leveraged in a contrastive way, to enable LLMs to deeply understand what fine-grained revisions actually lead to good response.
> 基于 LLM 的生成式方法
> 训练 RM 本身需要大量的高质量人类标记数据，并且 RM 可能捕获另一个分别训练的 LLM 的 nonalignment 行为，因此一些工作探究让 LLM 自己生成 aligned 的回答
> constitutional AI 提出使用一系列 principles 作为 human supervision (例如自然语言指令)，来管理 LLM 的行为，基于这些 principle，LLM 可以批评它们自己的回答，并且反复修改到 aligned 的回答
> 类似地，Self-Align 首先采用 self-instruct 生成聚焦于覆盖多样主题的指令，然后再用多个 human-written principles prompt 模型，最后让模型生成 aligned 的回答
> 除此以外，还可以同时给 LLM 提供正例和负例，让 LLM 理解并修正自身行为
- *LLM based interactive approaches.* Most existing approaches train LLMs in isolation, where LLMs are not present in actual environments to improve themselves through external feedback signals. As a comparison, humans learn social norms and values from interactions with others in social environments [387]. To mimic such a learning approach, Stable Alignment [179] builds a simulated interaction environment consisting of a number of LLM agents, where AI agents keep interacting with and each other, receiving feedback on improvement. Once a central agent receives an instruction, it produces a response and shares it with nearby agents. These critic agents generate feedback comprising ratings about the response and revision suggestions. Then the central agent would revise the original response following these suggestions. Such an alignment approach can be also extended to real-world environment with humans.
> 基于 LLM 的交互式方法
> 大多数现存方法单独训练 LLM，LLM 并未在真实的环境中通过外部反馈信号提高自己，这和人类是不一样的
> 为了模仿人类的学习行为，Stable Alignment 建立由多个 LLM agent 构成的模拟交互环境，AI agent 在其中相互交互，回收反馈，当中心的 agent 收到一个指令，它会生成一个 response，并将其与临近的 agent 共享
> 批判的 agent 会生成反馈，包括了对 response 的 rating 和 revision 建议，然后中心 agent 会修改相应修改 response

**Supervised Alignment Tuning.** After obtaining alignment data, it is also key to design suitable fine-tuning strategies for direct alignment. A straightforward approach is to optimize LLMs using the conventional sequence-to-sequence objective based on the alignment data. In addition to the conventional optimization objective, several studies further explore auxiliary losses that enhance the learning from the alignment data.
> 需要设计 fine-tuning 策略来引导 alignment
> 一个直接的方法是使用常规的 sequence-to-sequence 目标
> 一些方法探究了辅助损失

- *Primary training objective.* Since the alignment data typically consists of an input instruction and an output response, the primary training loss is still the traditional crossentropy loss for sequence-to-sequence learning. Based on this loss, many studies propose a number of improvement variants for enhancing the supervised alignment tuning. For example, CoH [388] constructs the training data by prepending “A helpful answer:” and “An unhelpful answer:” to the annotated good and bad responses, respectively, and only compute losses for those response tokens with special masking. Quark [384] sorts model responses into different quantiles with varying alignment quality, it prepends a special reward token to each model response to represent the reward level of the response. Further, to enable the preference modeling via the maximum likelihood objective, DPO [389] first reparameterizes the response rewards using the policy model (i.e., the language model being optimized), and then the original reward modelling objective can be reformulated only based on the policy model. In this way, DPO removes the explicit reward modeling step, and optimizing the new learning objective only involving the policy model is equivalent to optimizing the rewards. Furthermore, FIGA [386] designs a fine-grained contrastive loss that aims to encourage desirable tokens, penalize undesirable ones, and disregard trivial tokens.
> 主训练目标
> alignment 数据一般由一个输入指令和一个输出回应组成，主要的训练目标仍然是 seq-to-seq 的交叉熵损失
> 基于该损失，许多研究提出变体，例如 CoH 对训练数据 prepend 提示，并仅为由特殊 mask 的 response token 计算损失，例如 Quark 将 responses 分 quantile，并 prepend reward token 到各个 quantile group 
> 此外，为了可以通过极大似然目标进行 preference modeling，DPO 用 policy model (即需要被优化的模型) 重参数化 response reward，然后仅基于 policy model 重构原始的 reward modeling 目标，DPO 借此移除了显式的 reward 建模步骤，并且此时优化仅包含了 policy model 的新学习目标就等价于优化 rewards
> FIGA 设计对比损失，鼓励需要的 tokens，惩罚不需要的 tokens，忽视 trivial tokens

- *Auxiliary optimization objectives.* Besides the primary cross-entropy loss, several studies propose auxiliary training loss to enhance the learning from the alignment data. First, since the responses of each instruction can be scored by the reward model, the ranking loss can be used to train the model to preserve the ranking order of these responses. For example, RRHF [390] samples responses from multiple sources, including model-generated responses, such as those derived from the model itself, ChatGPT, and GPT-4, as well as human-written responses, spanning both highquality and low-quality instances. To align with the scores from reward models, it further optimizes the ranking loss by encouraging the model to have a higher conditional log probability for the response with a higher ranking. SLiCHF [391] proposes to assess the similarity between model outputs and human preference via the distance in the latent space, and introduces specific calibration and regularization loss to calibrate the candidate sequences based on humanpreference data. Second, to enhance the relatedness between the response and the instruction, some work adopts contrastive learning to push up the probability of correct instruction-response pairs while pushing down incorrect instruction-response pairs. Specifically, for an output response, the proposed approach in [392] contrasts the target instruction to the other irrelevant instructions. By doing so, it can enable the model to learn the right correlation between instructions and responses.
> 辅助训练目标
> 首先，因为对于每个 instruction 的 responses 可以被 reward 模型打分，ranking loss 就可以被用于训练模型，让模型保持这些 responses 的 ranking order
> 例如 RRHF 从多个源对 responses 进行采样，然后针对 ranking loss 优化，鼓励模型对于高 ranking 的 response 有更高的条件对数似然
> SLICHF 提出通过隐藏空间中的距离评估模型 outputs 和 humane 偏好的相似性，并引入特定的校正和正则损失来基于 human preference 数据校正序列
> 其次，为了提高 response 和 instruction 之间的相关性，一些工作采用对比学习，将正确的 instruction-response 对的概率推高，并拉低不正确的 instruction-response 对的概率，让模型学习 instruction 和 response 之间的正确关联
### 5.2.5 Remarks on SFT and RLHF
As discussed in Section 5.1, instruction tuning is the process of training pre-trained language models with formatted demonstration data (instructions paired with desired outputs). At early exploration, instruction data was mainly collected from NLP tasks [67], while it has been now extended to more diverse supervision data that pairs input and output texts (e.g., the utterances of open-ended dialogues). Training with such paired texts is also called supervised finetuning (SFT) in the context of LLMs [66]. In this part, we mainly use the abbreviation SFT for discussion but not instruction tuning, due to the simplicity and popularity.
> 本节开始用有监督微调 (SFT) 代指指令微调，有监督微调是指令微调的主要方式

Since SFT and RLHF are two major adaptation tuning methods for LLMs, it is important to understand the connections and difference between them. Next, we make some discussions on this issue.
> 我们将讨论 SFT 和 RLHF 之间的关联和差异

**Overall Comparison with RL Formulation.** Following the discussion in Section 5.2.3 (the part related to RL training), the text generation problem can be formulated as a decisionmaking process based on RL. Taking a prompt as input, the task of a LLM is to generate a text completion that appropriately responds to the prompt. This task would be completed step by step. At each step, an agent (i.e., LLM) will perform an action (i.e., generating a token) according to the policy (i.e., the generative probability distribution of LLM) conditioned on the current state (currently generated token sequence and other available context information). It is expected that a high-quality output text would be produced by the LLM, which can earn a large reward score based on the entire response. Overall, RLHF and SFT can be considered as two different training approaches to optimizing the above decision making process for LLMs. Specially, RLHF firstly learns the reward model, and then employs it to improve the LLM with RL training (e.g., PPO). As a comparison, SFT adopts a teacher-forcing approach, which directly optimizes the likelihood of a demonstration output. Such a token-level training way essentially does behavior cloning (a special algorithm of imitation learning [393]): it utilizes the expert’s action (i.e., the target token at each step) as the supervision label and directly learns to imitate the demonstrations from experts without specifying a reward model as in typical RL algorithms. To learn the desired policies, SFT adopts a “local” optimization way (i.e., tokenlevel loss) based on demonstration data, while RLHF takes a “global” optimization way (i.e., text-level loss) by involving human preference. More theoretical analysis about imitation learning and reinforcement learning can be referred to the related RL literature [393, 394].
> 文本生成问题可以构建为一个基于 RL 的决策问题
> 给定 prompt，在每一步，agent (LLM) 根据 policy (generative probability distribution) 以及当前的状态 (当前的 token 序列和其他的上下文信息) 执行一个动作 (generate token)，LLM 需要生成基于整个 response 可以获得尽可能高的 reward score 的输出
> RLHF 和 SFT 可以被认为优化上述决策制定过程的两个不同训练方法
> RLHF 首先学习 reward 模型，然后用 reward 模型在 RL 训练下优化 LLM
> SFT 直接优化似然，这种 token-level 的训练方式实质上是行为克隆，即使用每一步的目标 token 作为 supervision label，然后直接学习模仿
> SFT 采用的是“局部“的优化方式，即 token-level loss，而 RLHF 采用的是“全局“优化方式，即 text-level loss

**Pros and Cons of SFT.** SFT has been shown to be an effective approach to boosting the performance of LLMs on various benchmarks [67, 69, 137, 138], which can largely enhance the task generalization ability and flexibly endow specific functions (e.g., establishing the chatbot’s identity). More discussions about the usefulness of SFT can be found in Section 5.1.3. It has been widely recognized that SFT mainly unlocks the abilities but not inject new abilities into LLMs. Thus, it might become problematic when one tries to stimulate the non-endogenous abilities of LLMs via SFT. As a concrete scenario, it would potentially advocate the hallucination behaviors when demonstration data is beyond the knowledge or ability scope of LLMs, e.g., training a LLM to answer questions about its unknown facts. An interesting viewpoint from John Schulman’s talk on RLHF [395] is that distilling superior models to train less capable models (e.g., prompting GPT-4 to generate the response as fine-tuning data) might increase the possibilities of generating the hallucinated texts, thus likely affecting the factual accuracy of LLMs. Furthermore, as a behavior cloning method, SFT aims to imitate the behaviors (without explorations) of the experts who construct the demonstration data. However, there often exist variations among different annotators on the writing styles, quality, and preferences of demonstration data, which tends to affect the learning performance of SFT. Thus, high-quality instruction data (but not the quantity) is the primary factor for effective training of LLMs during the SFT stage [99].
> 一个共识是 SFT 主要是解锁 LLM 的能力，而不是向 LLM 注入能力
> 因此通过 SFT 让 LLM 学习非内在的能力或许不行，当展示的数据超过的 LLMs 的能力范围，SFT 可能还会助长 LLM 的幻觉
> John Schulman 认为蒸馏更强大的模型以训练更弱的模型可能会提高生成幻觉数据的概率
> 另外，作为一个 behavior cloning 方法，SFT 意在模仿构建的 demonstration 数据的专家的行为 (没有探索) ，但 annotators 有不同的 writing style, quality 和对 demonstration 数据的 preference，这些也会影响 SFT 的学习效果
> 因此 instruction 数据的质量而非数量是 SFT 阶段训练 LLM 的主因

**Pros and Cons of RLHF.** RLHF was early explored in the literature of deep RL [79], then borrowed to improve the capacity of language models (e.g., summarization [129]), and subsequently adopted as the fundamental technique to develop InstructGPT [66]. Recently, increasing evidence [99, 371] has demonstrated the effectiveness of RLHF in mitigating the harmful responses and enhancing the model capacity. Specially, LLaMA 2 has demonstrated that RLHF can improve both the helpfulness and harmlessness scores [99], and attributed this to a better human-LLM synergy for data annotation. They explain this reason in two major aspects as follows. First, since human annotators mainly provide preference annotations for RLHF, it can largely alleviate the discrepancies of annotators as that in SFT. Secondly, preference annotation is much easier than writing the demonstration data, and annotators can even judge the quality of more superior generations than those they create, making it possible to explore a broader state space beyond what can be demonstrated by human annotators. Another key point is that RLHF essentially encourages LLMs to learn correct policies by contrasting the self-generated responses (discriminating between good and bad responses). It no longer forces the model to imitate external demonstration data, and thus can mitigate the hallucination issues with SFT as discussed above. Actually, RLHF has been demonstrated to be an important approach to reduce the hallucination behaviors in GPT-4 [46]. However, RLHF inherits the drawbacks of classic RL algorithms, e.g., sample inefficiency and training instability. When adapted to LLMs, RLHF further relies on a strong SFT model as initial model checkpoint for efficiently achieving good performance. In addition, human annotators are involved in a complex iterative optimization process, in which a number of important details (e.g., the prompt selection, the schedule of reward model training and PPO training, and the settings of hyper-parameters) have important impact on the whole model performance.
> 许多研究展示了 RLHF 在减少有害的 response 和提高模型能力方面的效果
> LLaMA 2展示了 RLHF 可以同时提高 helpfulness 和 harmlessness scores，他们从两方面解释了原因，其一是 RLHF 中，人类标记着主要提供的是偏好标记，因此大幅缓解了 SFT 中标记者之间的差异，其二是 preference 标记相较于 demonstration 数据更好写，且标记者可以评判比他们自己写的内容还要好的 LLM 生成的内容，因此状态空间会更大
> 另一个关键点是 RLHF 实质上通过让 LLM 比较自己生成的 responses 以鼓励 LLM 学习正确的 policies，而不强制模型模仿外部的 demonstration 数据，因此可以缓解 SFT 的幻觉问题
> GPT-4中，RLHF 被展示为减少幻觉的重要方法
> 但 RLHF 同样继承了 RL 的缺点，包括了低效采样和不稳定训练，且 RLHF 用于 LLM 时，它需要依赖 strong SFT 模型作为最初的 model checkpoint 以高效达到好的表现

Overall, SFT is particularly useful to increase the model capacity of pre-trained model checkpoints right after pretraining, while RLHF is promising to further improve the model capacity of SFT models. However, RLHF has been difficult to implement, and far from well explored (according to public literature), and more improvements (e.g., efficient and reliable annotation [371] and simplified optimization [389]) are still needed for further research.
> 总之，SFT 对于在预训练后提高模型能力特别有用，RLHF 则用于进一步提高 SFT 模型的能力

## 5.3 Parameter-Efficient Model Adaptation
In the above, we have discussed the approaches of instruction tuning and alignment tuning to adapt LLMs according to specific goals. Since LLMs consist of a huge amount of model parameters, it would be costly to perform the fullparameter tuning. In this section, we will discuss how to conduct efficient tuning on LLMs. We first review several representative parameter-efficient fine-tuning methods for Transformer language models, and then summarize existing work on parameter-efficient fine-tuned LLMs.
> 我们讨论了指令微调和对齐微调的方法
> 但进行全参数微调不现实
> 本节讨论进行高效微调的方式
### 5.3.1 Parameter-Efficient Fine-Tuning Methods
![[A Survey of LLMs-Fig13.png]]
In existing literature, parameter-efficient fine-tuning [145, 396, 397] has been an important topic that aims to reduce the number of trainable parameters while retaining a good performance as possible. In what follows, we briefly review four parameter-efficient fine-tuning methods for Transformer language models, including adapter tuning, prefix tuning, prompt tuning and LoRA. The illustration of these four methods are shown in Figure 13.
> 参数高效的微调方法意在减少可训练参数的同时尽可能保持一个好的表现
> 本节将回顾4个参数高效的微调方法，包括适应期调节、前缀调节、prompt 调节和 LoRA

**Adapter Tuning.** Adapter tuning incorporates small neural network modules (called adapter) into the Transformer models [398]. To implement the adapter module, a bottleneck architecture has been proposed in [398, 399], which first compresses the original feature vector into a smaller dimension (followed by a nonlinear transformation) and then recovers it to the original dimension. The adapter modules would be integrated into each Transformer layer, typically using a serial insertion after each of the two core parts (i.e., attention layer and feed-forward layer) of a Transformer layer. Alternatively, parallel adapters [400] can be also used in Transformer layers, where it places two adapter modules in parallel with the attention layer and feed-forward layer accordingly. During fine-tuning, the adapter modules would be optimized according to the specific task goals, while the parameters of the original language model are frozen in this process. In this way, we can effectively reduce the number of trainable parameters during fine-tuning.
> 适应器调节
> Adapter tuning 将小的神经网络模块 (adapter) 放入 Transformer 模型中
> bottleneck 架构首先将原始特征向量压缩为小维度，然后进行非线性变换，然后将其恢复至原始维度
> adapter 模块会被集成入各个 Transformer 层，一般就是串行地插入在 attention 曾和 feed-forward 层之后，也有将两个 adapter 模块和 attention 层和 feed-forward 层并行的方法
> 在微调的时候，adapter 模块相应地被优化，而 LLM 的原始参数则 freeze，因此有效减少了微调时可训练参数的数量

**Prefix Tuning.** Prefix tuning [396] prepends a sequence of prefixes, which are a set of trainable continuous vectors, to each Transformer layer in language models. These prefix vectors are task-specific, which can be considered as virtual token embeddings. To optimize the prefix vectors, a reparameterization trick [396] has been proposed by learning a MLP function that maps a smaller matrix to the parameter matrix of prefixes, instead of directly optimizing the prefixes. It has been shown that this trick is useful for stable training. After optimization, the mapping function would be discarded, and only the derived prefix vectors are kept to enhance task-specific performance. Since only the prefix parameters would be trained, it can lead to a parameterefficient model optimization. Similar to prefix tuning, ptuning v2 [401] incorporates layer-wise prompt vectors into the Transformer architecture specially for natural language understanding, which also utilizes multi-task learning for jointly optimizing shared prompts. It has been shown to be useful in improving the model performance of different parameter scales on natural language understanding tasks.
> 前缀微调
> 前缀微调为 sequence prepend 前缀，即一系列可训练的连续向量
> 这些前缀向量是针对任务的，可以视它们为虚拟 token 嵌入
> 可以用重参数化技巧，学习一个将更小的矩阵映射到前缀的参数矩阵的 MLP，而不是直接优化前缀，该技巧有利于稳定训练
> 在优化之后，映射函数会被丢弃，仅保留前缀向量，用于提高针对任务的表现
> 该方法仅训练了前缀相关的参数
> 类似的 ptuning v2使用了逐层的 prompt 向量，对于模型在自然语言理解上有帮助

**Prompt Tuning.** Different from prefix tuning, prompt tuning [397, 402] mainly focuses on incorporating trainable prompt vectors at the input layer. Based on the discrete prompting methods [404, 405], it augments the input text by including a group of soft prompt tokens (either in a free form [402] or a prefix form [397]), and then takes the prompt-augmented input to solve specific downstream tasks. In implementation, task-specific prompt embeddings are combined with the input text embeddings, which are subsequently fed into language models. P-tuning [402] has proposed a free form to combine the context, prompt and target tokens, which can be applied to the architectures for both natural language understanding and generation. They further learn the representations of soft prompt tokens by a bidirectional LSTM. Another representative approach [397] named prompt tuning directly prepends prefix prompts to the input. During training, only the prompt embeddings would be learned according to task-specific supervisions. Since this method only includes a small number of trainable parameters at the input layer, it has been found that the performance highly relies on the model capacity of the underlying language models [397].
> 提示微调
> prompt tuning 在输入层添加可以训练的 prompt 向量
> 该方法通过一组 soft prompt tokens 增强输入文本，然后使用 prompt-augmented 输入解决下游任务
> 实现中，针对任务的 prompt embeddings 会和输入文本 embeddings 结合作为输入
> P-tuning 通过双向 LSTM 学习 soft prompt 的表示
> 该方法仅在输入层包含少量的可训练参数，因此表现高度依赖于模型能力

**Low-Rank Adaptation (LoRA).** LoRA [145] imposes the low-rank constraint for approximating the update matrix at each dense layer, so as to reduce the trainable parameters for adapting to downstream tasks. Consider the case of optimizing a parameter matrix $W$. The update process can be written in a general form as: $W \leftarrow W + \Delta W$. The basic idea of LoRA is to freeze the original matrix $W\in R^{m\times n}$ while approximating the parameter update $\Delta W$ by lowrank decomposition matrices, i.e., $\Delta W = A\cdot B^T$, where $A\in R^{m\times k}$ and $B\in R^{m\times k}$ are the trainable parameters for task adaptation and $k\ll \min (m, n)$ is the reduced rank. The major merit of LoRA is that it can largely save the memory and storage usage (e.g., VRAM). Further, one can only keep a single large model copy, while maintaining a number of task-specific low-rank decomposition matrices for adapting to different downstream tasks. Further, several studies have also discussed how to set the rank in a more principled approach, e.g., importance score based allocation [406] and search-free optimal rank selection [407].
> LoRA 为每一个 dense 层的近似更新矩阵施加低秩限制，以减少适应下游任务时可训练参数的数量
> LoRA 的基本思想是固定原始矩阵 $W\in R^{m\times n}$，同时用低秩分解矩阵近似参数更新 $\Delta W$，即 $W = A\cdot B^T$
> LoRA 的主要优势是它大幅节约了存储空间，并且针对任务的 LoRA 块可以分别保存
> 一些研究进一步讨论了如何设定 rank，例如基于 importance score 的 allocation 和不需搜索的 optimal rank selection

Besides the above methods, there is extensive research on efficient tuning of Transformer language models. However, a more comprehensive discussion of efficient tuning is beyond the scope of this article, which can be found in the related papers on this topic [400, 408].
### 5.3.2 Parameter-Efficient Fine-Tuning on LLMs
With the rising of LLMs, efficient tuning has attracted increasing research attention for developing a more lightweight adaptation approach in downstream tasks.

In particular, LoRA [145] has been widely applied to open-source LLMs (e.g., LLaMA and BLOOM) for parameter-efficient fine-tuning. Among these research attempts, LLaMA and its variants have gained much attention for parameter-efficient tuning. For example, AlpacaLoRA [144] has been trained using LoRA as a lightweight tuned version of Alpaca [142] (a fine-tuned 7B LLaMA model with 52K human demonstrations of instruction following). There are extensive explorations of Alpaca-LoRA ranging in different languages or model sizes, which can be found in the collection page33. A recent study LLaMAAdapter [409] inserts learnable prompt vectors into each Transformer layer, in which zero-initialized attention has been proposed to improve the training by mitigating the influence of under-fitted prompt vectors. They also extend this approach to a multi-modal setting, e.g., visual question answering.
> 开源 LLMs 广泛使用 LoRA 进行参数高效的微调
> 例如 AlpacaLoRA 就是 Alpaca 使用 LoRA 的轻量微调的版本
> LLaMAAdapter 将可学习的 prompt 向量插入到各个 Transformer 层，并使用 zero-initialized attention ，通过降低 under-fitted prompt 向量的影响来帮助训练

Further, an empirical study [399] has been conducted to examine the effect of different tuning methods on language models. They compare four efficient tuning methods including serial adapter tuning [398], parallel adapter tuning [400, 410], and LoRA [145], on three open-source LLMs, namely GPT-J (6B), BLOOM (7.1B) and LLaMA (7B), for evaluation. Based on the experimental results on six math reasoning datasets, they show that these efficient-tuning methods under-perform the reference baseline GPT-3.5 on difficult tasks, while achieving a comparable performance on simple tasks. Overall, LoRA performs relatively well among these comparison methods, using significantly fewer trainable parameters.
> 有研究比较了不同的微调方法，包括并行的 adapter tuning、LoRA、串行的 adapter tuning
> 发现三个结果在简单任务上取得了可比的结果
> 总体上 LoRA 表现较好，因为它使用了非常少的参数

As an important resource, the library PEFT [411] (standing for parameter-efficient fine-tuning) has been released on GitHub. It has included several widely used efficient tuning methods, including LoRA [145]/AdaLoRA [406], prefixtuning [396, 401], P-Tuning [402], and prompt-tuning [397]. Further, it supports a number of language models such as GPT-2 and LLaMA, and also covers several representative vision Transformer models (e.g., ViT and Swin Transformer).
> PEFT (Parameter Efficient Fine-Tuning) 库包括了几个常用的高效微调方法

As discussed in Section 5.3.1, there have been a large number of efficient tuning methods proposed in the existing literature. However, most of these approaches are tested on small-sized pre-trained language models, instead of the LLMs. So far, there still lacks a thorough investigation on the effect of different efficient tuning methods on large-sized language models at different settings or tasks.
> 目前对于微调方法的测试仅限于小规模的模型，仍缺乏对于大规模 LLM 的影响测试
## 5.4 Memory-Efficient Model Adaptation
Due to the huge number of model parameters, LLMs take a significant memory footprint for inference, making it very costly to be deployed in real-world applications. In this section, we discuss how to reduce the memory footprint of LLMs via a popular model compression approach (i.e., model quantization), so that large-sized LLMs can be used in resource-limited settings, which also likely reduces the inference latency.
> LLM 要部署需要大量 memory
> 本节讨论减少 LLM memory footprint 的方法，即模型压缩方法 (model quantization)，该方法在资源限制的情况下使用 LLM，以减少推理延迟
### 5.4.1 Background for Quantization
In this part, we present a general introduction of quantization techniques for neural networks.
> 本节为 NN 的量化做一个常规的介绍

In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers [412], especially the 8-bit integer quantization (i.e., INT8 quantization). For neural network models, there are typically two kinds of data to be quantized, namely weights (model parameters) and activations (hidden activations), which are originally represented in floating-point numbers. To illustrate the essential idea of model quantization, we introduce a simple yet popular quantization function: $x_q = R(x/S)- Z$, which transforms a floating number $x$ into a quantized value $x_q$. In this function, $S$ and $Z$ denote the scaling factor (involving two parameters $\alpha$ and $\beta$ that determine the clipping range) and zero-point factor (determining symmetric or asymmetric quantization), respectively, and $R (\cdot)$ denotes the rounding operation that maps a scaled floating value to an approximate integer.
> NN 压缩中，量化通常指将浮点数映射到整数的过程，尤其是8-bit 整数量化
> 对于 NN 模型，一般需要量化的数据有两种：权重、激活
> 量化函数写为 $x_q = R (x/S) - Z$，它将浮点数 $x$ 转化为一个量化的值 $x_q$，函数中 $S, Z$ 表示 scaling factor，它包括两个参数 $\alpha , \beta$ 表示 clipping 范围，以及一个 zero-point factor 用于决定对称还是不对称量化
> $R (\cdot)$ 表示将一个 scaled 浮点值映射到近似整数的 rounding operation

As the reverse process, dequantization recovers the original value from the quantized value accordingly: $\tilde x = S\cdot ( x_q + Z)$. The quantization error is calculated as the numerical difference between the original value $x$ and the recovered value $\tilde x$. The range parameters $\alpha$ and $\beta$ have a large impact on the quantization performance, which often need to be calibrated according to real data distributions, in either a static (offline) or dynamic way (runtime).
> 去量化从量化值恢复原始值，根据 $\tilde x = S\cdot (x_q + Z)$
> 量化误差根据 $x, \tilde x$ 之间的数值差异计算
> 范围参数 $\alpha, \beta$ 对于量化表现有很大影响，它们常常需要根据真实数据分布被校准

For more details, we refer to the readers to the excellent survey [412] about quantization methods on neural networks.
### 5.4.2 Quantization Methods for LLMs
There are generally two major model quantization approaches, namely quantization-aware training (QAT) (requiring additional full model retraining) and post-training quantization (PTQ) (requires no model retraining). Compared with small-sized language models, two major differences need to be considered when designing or selecting quantization methods for LLMs. Firstly, LLMs consist of a huge number of parameters, and thus PTQ methods are more preferred due to a much lower computational cost than QAT methods. Secondly, LLMs exhibit very different activation patterns (i.e., large outlier features), and it becomes more difficult to quantize LLMs, especially hidden activations. Next, we will briefly review several representative PTQ methods for LLMs.
> 有两个主要的模型量化方法：quantization-aware training (QAT) (需要额外的完全模型重训练) 以及 post-training quantization (PTQ) (不需要模型重训练)
> 对于参数量大的 LLM，一般为了计算高效，会考虑使用 PTQ，另外 LLM 展现出非常不同的激活模式 (即会有 outlier feature)，因此量化 LLM，尤其是隐藏层激活较难
> 我们介绍几个用于 LLM 的 PTQ

**Post-Training Quantization (PTQ).** We first introduce the PTQ methods for LLMs.
- *Mixed-precision decomposition.* As observed in [413], extreme large values occur in hidden activations (called the emergence of outliers) when the model size reaches 6.7B parameters or above. Interestingly, these outliers are mainly distributed in some specific feature dimensions at Transformer layers. Based on this finding, a vector-wise quantization approach, called LLM.int8 (), has been proposed in [413], which separates the feature dimensions with outliers and the rest dimensions in matrix multiplication. Then, the calculations for the two parts are performed with 16- bit floating numbers and 8-bit integers, respectively, so as to recover these outliers in a high precision.
> 混合精度分解
> 有研究发现当模型大小达到 67 亿参数及以上时，会出现极其大的隐藏层特征，这些 outliers 一般主要分布在一些特定的特征维度
> LLM. int8 () 是一个向量级的量化方法，它用矩阵乘法将 outlier 的维度和其余的维度分开，然后对于这两个部分分别用16为浮点数和8位整数进行计算，目的是在高精度下恢复这些 outliers

- *Fine-grained quantization.* For Transformer models, weights and activations are usually represented in the form of tensors. A straightforward approach is to use coarse-grained quantization parameters for the whole tensor (i.e., per-tensor quantization) [414]. However, it usually leads to inaccurate reconstruction results. Thus, finegrained methods are proposed to reduce the quantization error. ZeroQuant [415] adopts a token-wise quantization approach with dynamic calibration for compressing activations. Whereas for weights (easier to be quantized), it uses a group-wise quantization. In practice, a group size of 128 [415, 416] is commonly used for model quantization.
> 细粒度的量化
> Transformer 模型的权重和参数一般以 tensor 形式表示，直接的量化方法是粗粒度的量化 (per-tensor quantization)，但容易导致错误的重构结果
> 细粒度的量化用于减少量化误差
> ZeroQuant 对于激活使用 token-wise 量化，对于权重使用 group-wise 量化

- *Balancing the quantization difficulty.* Considering that weights are easier to be quantized than activations, SmoothQuant [414] proposes to migrate the difficulty from activations to weights. Specially, they incorporate a scaling transformation to balance the difficulty between weights and activations in a linear layer: $\mathbf Y = (\mathbf X \text{diag} (\mathbf s)^{-1})(\text{diag} (\mathbf s)\mathbf W)$). By introducing an mathematically equivalent transformation, this formula controls the quantization difficulty through the scaling factor s. To set $s$, it incorporates a migration strength parameter $\alpha$ to balance the difficulties, where each entry $s_j = \max (\mathbf x_j)^{\alpha}/ \max (w_j)^{1-\alpha }$ is determined by the migration strength.
> 平衡量化难度
> 考虑到权重相对于激活更容易量化，SmmothQuant 提出将困难从激活迁移到权重，它在线性层进行 sacling 变化，平衡权重和激活的困难
> 该方法通过 scaling factor $s$ 控制量化难度，$s$ 则通过 migration strength 参数 $\alpha$ 设置

- *Layerwise quantization.* This approach finds optimal quantized weights that minimize a layerwise reconstruction loss: $\arg \min_{\widehat W}||WX - \widehat W X||_2^2$. To efficiently optimize this objective, GPTQ [417] improves the original optimal brain quantization (OBQ) [418] method by fixing the quantization order of weights for all rows. Further, with specially designed methods (i.e., lazy batch-updates and Cholesky reformulation), GPTQ is feasible to quantize very large models (e.g., 175B OPT) in 3 or 4 bit precision. More recently, AWQ [416] further simplifies the optimization form by incorporating activation-aware scaling for weights, which resembles the idea of SmoothQuant [414]: weights corresponding to outlier activations are more important to be precisely quantized. It does not directly optimize the reconstruction loss, but instead performs simple hyper-parameter search to achieve the minimal loss on calibration data.
> 层级量化
> 该方法认为最有的量化权重可以最小化重构损失 $\arg \min_{\widehat W}||WX-\widehat WX||_2^2$ ，
> GPTQ 固定权重的所有行的量化顺序，可以用3/4-bit 量化非常大的模型
> AWQ 采用 activation-aware 的权重 scaling，即对应离群值的激活的权重会更加重要，因此更加精细地量化

These strategies in the above methods can be jointly used to improve the quantization performance. In order to achieve high-efficiency implementation, quantization methods also rely on hardware- or system-level support (e.g., efficient GPU kernels or hardware-friendly group partition).

**Other Quantization Methods.** In the above, we mainly focus on PTQ methods, and next introduce two recent studies that explore efficient fine-tuning methods or QAT methods for quanitizing LLMs.
> 接着介绍两个 QAT 方法

- *Efficient fine-tuning enhanced quantization.* For post training quantization, direct low-bit quantization (e.g., INT4 quantization) often results in large performance degradation. To overcome this challenge, QLoRA [419] incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning. It combines the merits of LoRA (See Section 5.3.1) and quantization methods. The experiment results show that 4-bit quantized models can achieve the full 16-bit fine-tuning performance by QLoRA.
> 高效微调增强的量化
> 直接用 low-bit 进行 QAT 一般会大幅降低能力
> QLoRA 将小的可调节的 adapter (16 bit) 加入需要量化的模型，结合了 LoRA 和量化方法，实验表示 4-bit quantized 的模型可以达到 full 16-bit fine-tuning 的表现

- *Quantization-aware training (QAT) for LLMs.* A recent study [420] explores the effect of QAT methods by applying a data-free distillation method to compress the weights, activations as well as key-value cache. By conducting extensive experiments based on LLaMA, they show promising results with 4-bit quantization on both weights and keyvalue cache, but not on 4-bit activation quantization, which still needs more exploration.
> 针对 LLM 的 QAT
> 有研究探索了使用无数据蒸馏方法压缩权重、激活、kv cache 的 QAT 方法
### 5.4.3 Empirical Analysis and Findings
Quantization has currently become a common technique to reduce the memory footprint and latency of LLMs in deployment. In particular, it is important to understand what level of precision (e.g., INT8 or INT4) can be applied to quantize different parts of LLMs (e.g., weights or activations), while retaining a high accuracy. In this part, we first summarize the major findings about the quantization of LLMs in existing literature, and then present some empirical analysis with quantization experiments.
> quantization 已经是减少 LLM 部署的 memory footprint 和 latency 的常用方法
> 并且对于 LLM 的不同部分 (权重或激活) 可以采用不同精度的量化
> 本节首先总结关于 LLMs 当前使用的主要量化方法，然后进行一些经验性分析

**Important Findings from Existing Work.** Recently, a very comprehensive evaluation [421] has been conducted about the impact of multiple factors (e.g., model size and sensitivity) on the post-training quantization methods. Another study [422] examines the scaling law of k-bit quantization in inference performance. In addition to the overall performance, the study [423] specifically focuses on the potential impact of quantification on emergent capabilities, as well as the levels of performance that can be achieved across various levels of bit precision. Also, prior work (e.g., LLM.int8 () [424], GPTQ [417], QLoRA [419], and GLM [93]) has also extensively examined the performance of quantization methods in various settings. Next, we summarize several important findings from these studies, which will be useful for those who may not want to delve into the technical details of quantization methods.
> 有研究探究了模型大小或敏感度等因素对于 PTQ 方法的影响
> 其他研究探究了 k-bit 量化在推理性能方面的 scaling law，并且还探究了量化对于涌现能力的潜在影响，以及不同 bit 级别的精度可以达到的性能 level
> 我们总结这些研究中的重要发现

- *INT8 weight quantization can often yield very good results on LLMs, while the performance of lower precision weight quantization depends on specific methods [414, 416, 417, 421].* In most cases, INT8 weight quantization can be effectively applied to reduce the memory footprint without performance degradation. While for INT4 (or INT3) weight quantization, existing methods rely on specific strategies to reduce the performance degradation, e.g., layerwise method [415, 417], activation-aware scaling [416] and low-rank adapter tuning [419]. Interestingly, LLMs seem to be less sensitive to low-bit weight quantization than small-sized language models [421]. In practice, with the same memory cost, it is suggested to use a larger language model with a lower quantization precision rather than a smaller language model with a higher quantization precision. For example, a 4-bit 60GB LLM is demonstrated to have better performance than a 8-bit 30GB LLM [422]. Moreover, focusing on emergent capabilities, the study [423] finds that in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization. This result suggests that INT4 quantization exhibits a favorable trade-off in terms of both total bits and performance of emergent abilities.
> INT8权重量化在 LLMs 一般效果较好，而更低精度的量化的性能则取决于具体方法
> 多数情况下使用 INT8不会导致性能下降，INT4/3则依赖具体方法，例如 layerwise method/activation-aware scaling/low-rank adapter tuning
> LLMs 相对于小模型，对于 low-bit 权重量化更加不敏感，因此一般在 memory footprint 相同的情况下，建议采用大模型 + low-bit 而不是小模型 + high-bit，例如 4-bit 60GB > 8-bit 30GB
> 针对涌现能力，研究发现上下文学习、逐步推理、指令服从的能力在 4-bit 权重量化下不会被影响，因此 INT4 量化在在 total bits 和 emergent ability performance 之间达到了好的 trade-off

- *Activations are more difficult to be quantized than weights [413, 414, 421].* It has been found that large outliers would occur for Transformer language models having a size of 6.7B or above [413]. This issue has been one of the most fundamental difficulties to quantize LLMs. To overcome this issue, various methods, e.g., mixed-precision decomposition [413], fine-grained quantization [413, 425] and difficulty migration [414], can be applied to alleviate the influence of outlier values. Since large outliers mainly exist in the activations of LLMs, small language models are more resistant to activation quantization [421, 423]. In practice, high-quality INT8 activation quantization is still a difficult task, though several methods can attain satisfying results. Further, lower precision activation quantization has still not been successfully explored, even for QAT methods [420].
> 激活相较于权重更加难以量化
> 研究发现大于 6.7B 的语言模型会有 large outlier，该问题是量化 LLM 最重要的困难之一
> 缓解该问题的方法有：混精分解、细粒度量化、难度迁移
> 相较于大模型，小模型对于激活量化更不敏感，因为不容易有 large outlier
> 目前激活量化在 INT8和低精度都尚待进一步探索

- *Efficient fine-tuning enhanced quantization is a good option to enhance the performance of quantized LLMs [145, 419].* The benefits of efficient fune-tuning methods in quantization can be twofold. Firstly, it can directly compensate the performance degradation suffered from low-bit quantization [421, 423], by increasing the fitting capacity by updating high precision adapters. Secondly, it is flexible to support task-specific or goal-specific fine-tuning of LLMs in a lightweight way [419], e.g., instruction tuning or chatoriented tuning, by only tuning the small adapters. Overall, it makes a good trade-off between the effectiveness and training cost, which provides a promising approach to enhancing the performance of quantized LLMs.
> 提高量化 LLM 的表现的一个好方法是高效微调强化的量化
> 高效微调对于量化有两点好处：
> 首先，可以通过更新高精度的 adapter 提高模型拟合能力，弥补 low-bit 量化的 performance degration
> 其次，(和第一点语义类似)，微调提高模型针对任务的能力，弥补量化的 performance degration
> 量化 + 高效微调达到了 effectiveness 和 training cost 之间的 trade-off

![[A Survey of LLMs-Table 10.png]]
**Empirical Analysis on Quantization Experiments.** To further help readers understand the impact of quantization on LLMs, we also conduct a group of experiments to investigate the inference performance of quantized models here. Specifically, we focus on the fine-tuned LLaMA models (i.e., 7B and 13B) using popular SFT datasets, including FLANv2 [69], Alpaca-52K [137] and ShareGPT [148]. For evaluation, we utilize the same tasks in Table 9, and follow the quantization settings in the study [423] examining the performance of quantized language models at three precision levels: 4-bit, 8-bit and 16-bit. The results are summarized in Table 10. As can be observed from Table 10, the results obtained with 8-bit and 4-bit weight quantization are close to the performance of 16-bit models while significantly reducing memory consumption. In practice, it is recommended to first examine the performance of 4-bit weight quantization for LLMs if reducing memory usage is a critical consideration for deployment.
> 我们探究了 4/8/16-bit 量化对于模型表现的影响
> 8/4-bit 的表现和16-bit 是十分接近的，因此实际中，如果 LLM memory usage 是重要优化对象，推荐首先进行4-bit 量化尝试
### 5.4.4 Open-source Libraries and Quantized LLMs
In this part, we briefly introduce the available open-source quantization libraries and quantized LLMs.
> 本部分介绍可用的 LLM 量化库

**Quantization Libraries.** Next, we introduce three major quantization libraries for LLMs, including:
> 介绍三大主流的量化库

- *Bitsandbytes* is developed based on the methods introduced in the papers of LLM.int8 () [413] and 8-bit optimizers [426]. It focuses on the quantization of both activations and weights for LLMs, including the support on 8-bit and 4-bit (NF4, FP4) matrix multiplication for efficient inference, as well as an 8-bit optimizer for efficient training.

- *GPTQ-for-LLaMA* is developed specially for quantizing LLaMA models. It enables 4-bit quantization of LLaMA models of varied sizes based on the GPTQ algorithm [417] models of varied sizes based on the GPTQ algorithm [417]. Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website.

- *AutoGPTQ* is a quantization package developed based on the GPTQ algorithm [417], which supports INT4 quantization for LLMs. It includes a number of quantized models in the library, and supports LoRA by integrating with HuggingFace PEFT library.

- *llama. cpp* makes it feasible to run quantized LLaMA models on a MacBook device. It supports INT4, INT5 and INT8 quantization, which is developed in efficient C/C++ implementation. It also supports a number of LLaMA based models, such as Alpaca and Vicuna.

**Quantized LLMs.** Compared with original models, quantized language models take a smaller memory footprint, and likely have a faster inference speed [93, 413, 427]. Recently, a number of quantized model copies of several publicly available language models have been released on HuggingFace, including BLOOM, GPT-J, and ChatGLM. 
In particular, GPTQ [417] has been widely used to quantize generative language models, leading to various quantized variants for LLaMA and OPT. Further, it has been also applied to quantize instruction-tuned models, such as Vicuna and WizardLM. Due to the large number of quantized LLMs, we do not directly incorporate the corresponding links of these models. The readers can easily find them by searching on HuggingFace.
> GPTQ 被广泛用于量化大模型，许多量化的大模型可以在 HuggingFace 找到
# 6 Utilization
After pre-training or adaptation tuning, a major approach to using LLMs is to design suitable prompting strategies for solving various tasks. In existing literature, task-specific prompts can be effectively learned through manual creation and automatic optimization. A representative prompting method is in-context learning [50, 55], which formulates the task description and/or demonstrations in the form of natural language text. In addition, chain-of-thought prompting [33] can be employed to enhance in-context learning by involving a series of intermediate reasoning steps in prompts. Furthermore, planning [439] is proposed for solving complex tasks, which first breaks them down into smaller sub-tasks and then generates a plan of action to solve these sub-tasks one by one. We summarize representative work for these prompting approaches in Table 11. Next, we will elaborate on the details of the four techniques.
> 使用 LLM 的主要方式是设计针对任务的 prompt 策略
> 一个代表性的 prompting 方法就是 in-context learning，它使用自然语言构建任务描述和对应的示例
> 另外，chain-of-thought 通过展示一系列中间推理步骤增强 ICL 
> 有人提出用 planning 解决复杂任务，planning 将复杂任务首先分解为小的子任务，然后生成 a plan of action 一个一个解决这些子任务
> 本章详细讲述这些 prompt 技巧
## 6.1 Prompting
As discussed in previous work [36], prompting is the major approach to utilizing LLMs for solving various tasks. Since the quality of prompts will largely influence the performance of LLMs in specific tasks, there have been a series of studies proposed to generate suitable task prompts through manual creation or automatic optimization, which will be introduced in this section. 
> 本节介绍关于如何人工或自动生成合适的任务 prompt 的方法
### 6.1.1 Prompt Creation
The process of manually creating a suitable prompt is also called prompt engineering [452, 453]. A well-designed prompt is very helpful to elicit the abilities of LLMs for accomplishing specific tasks. In this part, we will first introduce the key components of prompts and discuss several principles for prompt design. Then, we evaluate ChatGPT with different prompts to show the results on several representative tasks. We are aware that there have been several existing papers [453, 454] and websites [455–457] that present the suggestions and guidelines to design good prompts. As a comparison, we mainly aim to discuss the key factors (ingredients and principles) that are useful for prompt creation, and provide experimental results and analysis on popular tasks as the reference to the beginners.
> 人工构建 prompt 即 prompt engineering
> 我们讨论对于 prompt 构建有益的关键因素，并进行实验性分析

**Key Ingredients.** Typically, there are four key ingredients that depict the functionality of a prompt for eliciting the abilities of LLMs to complete the tasks, including task description, input data, contextual information, and prompt style. To have an intuitive understanding of our discussion, we also present three prompt examples for question answering, meta-review generation, and text-to-SQL in Table 13.
> prompt 构建有4个关键因素：任务描述、输入数据、上下文信息、prompt 风格

- *Task description.*  task description is typically a specific instruction that LLMs are expected to follow. In general, one should clearly describe the task goal in natural language. For the tasks with special input or output format, detailed clarifications are often needed, and one can further utilize keywords to highlight the special settings for better guiding LLMs in task completion.

- *Input data.* In common cases, it is straightforward to describe input data (e.g., an instance to be responded by LLMs) in natural language. For special input data, such as knowledge graph and table, it is necessary to apply an appropriate and convenient way to make them readable for LLMs. For structured data, linearization is commonly used to transform the original records (e.g., knowledge triples) into sequences [458] due to the simplicity. Further, the programming language (e.g., executable code) has also been utilized to formulate the structured data, which can also support using external tools (e.g., program executor) to produce the precise results [459, 460].
> 对于结构化的数据，常用的 transformation 方式是 linearization，例如将 knowledge triples 转化为 sequences，也可以用程序进行转化

- *Contextual information.* In addition to the task description and input data, contextual or background information is also essential for specific tasks. For example, retrieved documents are highly useful for open-domain question answering as supporting evidence. Both the quality of the retrieved documents and their relevance to the question have an impact on the generated answers [461]. Thus, it needs to include such information in a proper prompt pattern or expression format. Furthermore, in-context task exemplars are also helpful for eliciting LLMs to accomplish a complex task, which can better depict the task goal, the special output formats, and the mapping relation between input and output.
> retrieved documents 对于 open-domain 问答十分有用，document 可以作为支持的证据
> 可以在 prompt 中提供对于 retrieved document 的格式和质量要求

- *Prompt style.* For different LLMs, it is important to design a suitable prompt style for eliciting their abilities to solve specific tasks. Overall, one should express the prompt as a clear question or detailed instruction that can be well understood and answered. In some cases, it is also useful to add the prefix or suffix to better guide LLMs. For example, using the prefix “Let us think step by step” can help elicit LLMs perform step-by-step reasoning, and using the prefix “You are an expert on this task (or in this domain)” can boost the performance of LLMs in some specific tasks. Further, for chat-based LLMs (e.g., ChatGPT), instead of directly feeding a long or complex task prompt, it is suggested to decompose it into multiple prompts for the sub-tasks and then feed them into LLMs via a multi-turn conversation [448].
> prompt: clear question or detailed instruction
> 可以添加前缀后缀：Let us think step by step/You are an expert on this task (or in this domain)
> 划分任务为多个子任务

**Design Principles.** Based on the key ingredients of prompts, we summarize several critical design principles that can help create more effective prompts for solving various tasks.
> 介绍 prompt 的设计准则

- *Expressing the task goal clearly.* Task descriptions should not be ambiguous or unclear, which likely lead to inaccurate or inappropriate responses. This highlights the need for clear and unambiguous directives when utilizing these models [66]. A clear and detailed description should contain various elements to explain a task, including task objective, input/output data (e.g., “Given a long document, I want you to generate a concise summary.”), and the response constraints (e.g., “the length of the summary cannot exceed 50.”). By providing a well-clarified task description, LLMs can more effectively understand the target task and generate the desired output.
> 清楚表示任务目标
> 任务描述包括任务目标、输入/输出数据、response 约束

- *Decomposing into easy, detailed sub-tasks.* To solve complex tasks, it is important to decompose the difficult task into several more easier, detailed sub-tasks for helping LLMs accomplish the goal step by step, which is closely related to the planning technique in Section 6.4. For example, following the suggestion [454], we can explicitly list the subtasks in the form of multiple numbered items (e.g., “Braid a coherent narrative by performing the following tasks: 1. ...; 2. ...; 3. ...”). By decomposing a target task into sub-tasks, LLMs can focus on solving easier sub-tasks and finally achieve more accurate results for complex tasks.
> 分治

- *Providing few-shot demonstrations.* As discussed in Section 6.2, LLMs can benefit from in-context learning for solving complex tasks, where the prompts contain a small number of task examples of the desired input-output pairs, i.e., few-shot demonstrations. Few-shot demonstrations can help LLMs learn the semantic mapping between input and output without parameter tuning. In practice, it is suggested that one should generate a few high-quality demonstrations for the target task, which would highly benefit the final task performance.
> 提供 few-shot demonstrations

- *Utilizing model-friendly format.* Since LLMs are pretrained on specially constructed datasets, there are some prompt formats that can make LLMs better understand the instruction. For example, as the OpenAI documentation suggests, we can use ### or """ as a stop symbol to separate the instruction and context, which can be better understood by LLMs. As a general guideline, most existing LLMs perform a task better in English, thus it is useful to employ English instructions to solve difficult tasks based on machine translation.
> LLM 预训练于特殊构建的数据集，故使用一些 prompt 格式可以让 LLM 更好理解指令，例如用 ### or """" 作为 stop symbol，分离指令和上下文
> 另外，多数 LLM 在英语下表现更好，故可以考虑使用英文指令

**Useful Tips.** In addition to the design principles, we also present a collection of useful prompt tips based on existing work or our empirical experiences in Table 12. Note that these tips are suggested in a general manner, it does not indicate that they are the best prompts for the corresponding tasks. This part will be continuously updated with more guidelines or tips. We welcome readers to contribute to this collection of prompt tips. We present the detailed procedure to contribute to the prompt tips, at the link: 
https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts.
> 除去 prompt 的设计准则，本部分基于现有经验和工作还提供一些小贴士，总结于 Table 12 ( 见原文 )

**Empirical Analysis.** We further conduct empirical studies to present the impact of prompts on task performance. To conduct the experiments, we select a variety of tasks that span language generation, knowledge utilization, complex reasoning, structure data generation, and information retrieval. For each task, we manually write a prompt that follows general guidelines introduced above. Note that the tested prompts may not be the optimal for these tasks, since they mainly aim to help readers understand how to write an effective prompt for solving different tasks. Also, we add a simplified prompt as the comparison for most tasks. Following the experimental settings in Section 7.4, we examine the 3-shot performance of ChatGPT on complex reasoning tasks (Colored Objects and GSM8k), and zeroshot performance on other tasks. We report the experimental results in Table 17, where we also include the supervised performance in existing papers as reference.
> 我们进一步经验性研究 prompt 对于任务表现的影响
> 研究的任务包括语言生成、知识利用、复杂推理、结构数据生成、信息检索
> 对于每个任务，我们对比遵循 guideline 写的 prompt 和简化的 prompt 的表现
> 结果见 Table 17

- *Carefully designed prompts can boost the zero-shot or fewshot performance of ChatGPT.* By comparing the results of using different prompts on the same task, we can see that using the carefully designed prompts can achieve better performance than the simpler ones. In the carefully designed prompts, we provide a more clearly expressed task description (e.g., WMT and WikiFact), or use a model-friendly format (e.g., GSM8k and OBQA). For example, for WikiFact task, the prompt with a more detailed task description leads to a performance increase from 29.25 to 31.21.
> 认真设计的 prompt 可以提高 ChatGPT 的零样本或少样本表现

- *More complex tasks can benefit more from careful prompt engineering on ChatGPT.* In the WikiFact and Colored Objects tasks, the designed prompts have greatly improved the performance of ChatGPT, i.e., from 23.61 to 28.47 on WikiFact and from 53.20 to 66.75 on Colored Objects. It indicates the necessity of prompt engineering for LLMs to perform well on complex tasks, since these tasks typically have specific output formats or require background knowledge. Our example prompts provide more detailed task description (e.g., output format and task goal), which can help ChatGPT better understand the complex task requirement for fulfilling it.
> prompt engineering 对于更加复杂的任务帮助更大

- *For mathematical reasoning tasks, it is more effective to design specific prompts based on the format of programming language.* For GSM8k, the designed prompt employs codeformatted few-shot demonstrations to convert this mathematical reasoning task into code generation task, which can leverage the strong code synthesis ability of ChatGPT for solving mathematical problems. Further, with the help of an external program executor, we are able to obtain more precise results instead of using LLMs for arithmetic operation. As we can see, the performance is boosted from 78.47 to 79.30 on GSM8k, indicating the usefulness of programming language in mathematical reasoning tasks.
> 对于数学推理任务，基于变成语言的格式设计 prompt 会更加有效

- *In knowledge utilization and complex reasoning tasks, ChatGPT with proper prompts achieves comparable performance or even outperforms the supervised baselines methods.* In knowledge utilization and complex reasoning tasks, ChatGPT with proper zero-shot or few-shot prompts can achieve comparable performance or even outperform the supervised methods, e.g., 31.21 (ChatGPT) v.s. 34.20 (supervised baseline) on WikiFact. Despite that, ChatGPT still performs worse than supervised baseline models on some specific tasks (e.g., ARC and WikiFact), since these supervised models have been specially optimized with task-specific data.
> 对于知识利用和复杂推理问题，ChatGPT 可以在 proper prompt 下达到有监督的 baseline 方法的性能

- *Through suitable prompt engineering, LLMs can handle some non-traditional NLP tasks.* With the help of specific prompts, ChatGPT can also accomplish non-traditional NLP tasks, i.e., the general recommendation and conversational recommendation. A key point is that these tasks can be well expressed or described in natural language. However, the performance of ChatGPT is still far from the referenced performance in these tasks, as LLMs cannot directly fit these tasks, which require specific domain knowledge and task adaptation [357, 462].
> LLM 可以在合适 prompt engineering 的帮助下处理非传统的 NLP 任务
### 6.1.2 Prompt Optimization
Although manually creating task prompts is more intuitive, it is time consuming and, more importantly, models are highly sensitive to the crafted prompts—improper prompts will lead to low task performance (as shown in Table 17). Therefore, a large body of studies propose automatic optimization approaches for discrete prompts and continuous prompts to achieve the optimal performance [396, 405]. In this part, we will detail these studies from two perspectives, i.e., discrete prompts and continuous prompts.
> 本节从两个方面：离散 prompts 和连续 prompts 介绍 prompt 的自动优化方法

**Discrete Prompt Optimization.** Discrete prompt is typically composed of a sequence of natural language tokens. Despite that the form is simple and flexible, optimizing prompts in discrete space is a challenging problem due to the combinatorial huge search space. To automatically search effective prompts for downstream tasks, existing studies propose a wide spectrum of discrete prompt approaches, which are detailed as follows.
> 离散的 prompt 由自然语言 token 序列构成
> 在离散空间优化 prompts 较难，因为搜索空间是组合式的，很大

- *Gradient-based approaches.* This kind of approaches aims to optimize the prompt search process by maximizing the output likelihood via gradient update [405, 464–466]. As a representative work, Auto-Prompt [405] proposes a gradient-guided method to greedily searches the optimal token for each position of the prompt, leveraging the gradient approximated by the change in the log-likelihood when replacing a prompt token with another candidate token from vocabulary. However, such a search process can be extremely expensive since it needs to evaluate each candidate token for each position of the prompt, leading to a number of additional forward passes. Therefore, improved gradient method [464] has been proposed by transforming discrete tokens into continuous embeddings and computing the gradient on continuous space during optimization.
> 基于梯度的方法
> 通过梯度更新来最大化输出似然以优化 prompt 搜索空间
> Auto-Prompt 使用梯度引导的方法，在 prompt 的每个位置贪心搜索最优的 token，其梯度是通过首先从词袋中选择 token 替代 prompt token 然后计算对数似然变化而近似的
> 但该搜索过程非常昂贵，因为每个位置都需要评估每个候选 token
> 为此有方法提出将离散的 token 转换为连续的 embedding，然后基于连续空间计算梯度

- *RL-based approaches.* Since discrete prompts are difficult to be learned through gradient back-propagation, a number of studies propose to formulate the discrete prompt optimization as a reinforcement learning (RL) problem and leverage RL algorithms for optimization [467, 468]. For example, RLPrompt [467] trains a policy network to generate desired prompts with multiple reward functions. In this approach, several effective reward stabilization strategies are also proposed to enhance the RL training efficiency. Compared to previous work that requires sufficient data for training, TEMPERA [468] proposes to directly generate prompts at test time by utilizing a pre-trained RL agent to sequentially edit different parts of an manually-written initial prompt.
> 基于 RL 的方法
> 通过梯度反向传播难以学习离散的 prompt，为此有人提出将离散 prompt 优化构建为 RL 问题
> RLPrompt 使用多个 reward 函数训练 policy 网络以生成需要的 prompt，其中还利用了 reward 稳定策略提高训练效率
> TEMPERA 提出在测试阶段使用预训练好的 RL agent 以顺序编辑 manually-written 的 initial prompt 的不同部分

- *Edit-based approaches.* For the above methods, gradientbased and RL-based tuning can be extremely computationally demanding for ever larger models, and may not be feasible for API-based model calls (e.g., ChatGPT). Therefore, another line of work aims to directly edit existing prompts based on the task performance. Specifically, GPS [469] borrows an idea from the genetic algorithm and proposes a genetic prompt search method that utilizes a language model (i.e., T5) to edit prompts by taking the cloze task form. In addition to model based edit methods, human-defined operations can be also employed for prompt editing [470], including delete, swap, paraphrase, and addition. Based on these operations, they iteratively edit the prompts and greedily search for the best prompt guided by the model performance on a small pool of examples.
> 基于编辑的方法
> 上述方法都需要大量计算，并且对于仅提供 API 的模型不可用
> 有人提出基于 task performance 编辑现存的 prompt
> GPS 使用遗传 prompt 搜索方法，利用语言模型执行完型填空，来编辑 prompt
> 除了用模型，还可以用人工，即迭代式编辑 prompt，贪心搜索能让模型在 a small pool of examples 上的达到最好表现的 prompt

- *LLM-based approaches.* Due to the exceptional capacities of LLMs, an increasing number of studies directly leverage LLMs as prompt generator [471–473]. Specifically, APE [471] utilizes an LLM to generate initial prompts, then selects the best prompt with the highest accuracy, and finally improves the best candidate through an iterative Monte Carlo search method. Similarly, APO [472] instructs the LLM to generate text feedback on how to refine an old prompt into new improved prompts. However, their search in the prompt space might be inefficient without fully considering the whole refinement trace of previous prompts, thus potentially leading to sub-optimal results. Therefore, another study [473] incorporates the previous prompts with their scores to instruct LLMs for progressively generating better new prompts. However, these approaches still struggle in exploring the vast space of effective prompts. Inspired by human-like trial-and-error, prompt optimization is further formulated as a strategic planning problem [474] and uses Monte Carlo tree search to navigate the vast prompt space.
> 基于 LLM 的方法
> 一些研究直接只用 LLM 作为 prompt generator
> APE 让 LLM 生成初始 prompt，然后选择最高 accuracy 的作为 best prompt
> APO 指导 LLM 如何 refine 旧的 prompt 为新的 prompt，但它的 refinement trace 不会被完全考虑，因此在 prompt 空间的搜索可能是低效的，因此有方法将先前 prompt 的分数也含入
> 但这些方法依然难以探索完全广大的 propmt 空间
> 有人还采用了试错的方法，将 prompt 优化构建为一个策略规划问题，使用蒙特卡洛树来搜索 prompt 空间

**Continuous Prompt Optimization.** Different from discrete prompts, continuous prompts consist of a set of continuous embeddings, which can be directly optimized through the gradient update based on the loss of downstream tasks. Note that continuous prompt optimization has been mainly studied in PLMs, but draws limited attention in era of LLMs due to their massive magnitudes of parameters. We include the discussion of this part for content completeness. In prior work, most studies typically rely on supervised learning to train continuous prompts based on task data. Furthermore, in data-scarce scenarios, transfer learning methods can be employed to alleviate the lack of labeled data on target tasks. These two approaches are detailed below.
> 连续 Prompt 优化
> 连续的 prompt 由一系列连续的 embedding 构成，因此可以直接通过基于下游任务损失的梯度进行更新
> 多数研究依赖于有监督学习来训练基于任务数据的连续 prompt

- *Prompt learning with sufficient data.* In this approach, most existing methods regard continuous prompts as trainable model parameters and then leverage supervised learning to optimize the continuous prompts by minimizing the cross-entropy loss based on sufficient downstream task data [396, 397, 401, 475]. As discussed in Section 5.3.1, prefix tuning [396] prepends a sequence of prefixes (i.e., a set of trainable continuous vectors) to each Transformer layer in language models, while prompt tuning [397] only incorporates trainable prompt vectors at the input layer. By fixing the large-scale parameters of LLMs and only tuning continuous prompt vector, this kind of approaches can be extremely parameter-efficient (Section 5.3). However, these approaches are typically independent of the inputs, lacking sufficient consideration of input semantics. Therefore, the authors in [475] propose context tuning, where the continuous prompts are derived based on the input text and learned through the downstream task losses.
> 具有足够数据的 prompt 学习
> 这类方法将连续的 prompt 视作可以训练的模型参数，并通过最小化基于足够下游数据的交叉熵损失来优化连续的 prompt
> prefix tuning 将 prefix 序列 (一系列可以训练的连续向量) prepend 到语言模型的每一个 Transformer 层，而 prompt tuning 不同的地方在于它仅仅在输入层包含了可训练的 prompt 向量
> 该方法固定了模型参数，仅仅调节连续的 prompt 向量，但这类方法独立于输入，缺乏对于输入语义的考虑
> context tuning 中，连续的 prompt 则基于输入文本得到，并且通过下游任务损失学习

- *Prompt transferring with scarce data.* Supervised learning approaches demand in sufficient training data to learn optimal continuous prompts, which may not work well in data-scarce domains and tasks. To address this problem, SPoT [476] proposes a prompt-based transfer learning approach, which first learns a single continuous prompt for several representative source tasks and then uses this prompt to initialize the prompt for a target task. However, this approach leverages the same prompt for solving all instances of the target task. For a single task, even a well-learned prompt may not be suitable for all the data instances from a large population. To address this issue, an improved method [477] designs an adaptive attention mechanism during the prompt transfer process to derive the target prompts, considering both task- and instance-level information. The prompt transfer paradigm can leverage the knowledge of data-sufficient source tasks encoded in source prompts for solving data-scarce target tasks.
> 缺乏数据时的 prompt 迁移
> 有监督学习方法需要足够的训练数据以学习最优的连续 prompt，但缺乏数据时就不够有效
> 为了解决该问题，SPoT 提出基于 prompt 的迁移学习方法，首先为多个代表性的源任务学习单个连续的 prompt，然后使用该 prompt 初始化针对特定任务的 prompt ，但该方法对于所有的任务都使用一个初始化
> 另有方法在 prompt 迁移过程中设计了适应性 attention 机制以得到目标 prompt，由此考虑了任务和实例级别的信息
> prompt 迁移方法利用了数据充足的原任务以解决数据稀缺的目标任务
## 6.2 In-Context Learning
As a special prompting form, in-context learning (ICL) is first proposed along with GPT-3 [55], which has become a typical approach to utilizing LLMs.
> 上下文学习是一种特殊的 prompt 形式
### 6.2.1 ICL Formulation
As stated in [55], ICL uses a formatted natural language prompt, consisting of the task description and/or a few task examples as demonstrations. Figure 14 presents an illustration of ICL. First, starting with a task description, a few examples are selected from the task dataset as demonstrations. Then, they are combined in a specific order to form natural language prompts with specially designed templates. Finally, the test instance is appended to the demonstration as the input for LLMs to generate the output. Based on task demonstrations, LLMs can recognize and perform a new task without explicit gradient update.
> ICL 使用格式化 prompt，它由任务描述和少量的任务示例构成

Formally, let $D_k = \{f(x_1,y_1),\dots,f(x_k,y_k))\}$ represent a set of demonstrations with k examples, where $f(x_k,y_k)$  is the prompt function that transforms the k-th task example into natural language prompts. Given the task description $I$, demonstration $D_k$, and a new input query $x_{k+1}$, the prediction of the output $\hat y_{k+1}$ generated from LLMs can be formulated as follows:

$$
\text{LLM}(I,f(x_1,y_1),\dots, f(x_k,y_k),f(x_{k+1},\_))\rightarrow \hat y_{k+1}
$$
where the actual answer $y_{k+1}$  is left as a blank to be predicted by the LLM. Since the performance of ICL heavily relies on demonstrations, it is important to properly design them in the prompts. 

According to the construction process in Equation (12), we focus on three major aspects of formatting demonstrations in the prompts, including how to select examples that make up demonstrations, format each example into the prompt with the function $f(\cdot)$, and arrange demonstrations in a reasonable order.
> 我们考虑构造 prompt 中的展示部分的三个重要方面，包括如何选择构成展示的样本、将各个样本通过函数 $f(\cdot)$ 格式化为 prompt、以合理的顺序安排展示

A comprehensive review of ICL has been presented in the survey paper [50], and we suggest the readers referring to it for a more general, detailed discussion on this topic. Compared with this survey, we specially focus on the discussion of applying ICL to LLMs in two major aspects, i.e., demonstration design and the underlying mechanism of ICL. Also, ICL has a close connection with instruction tuning (discussed in Section 5.1) in that both utilize natural language to format the task or instances. However, instruction tuning needs to fine-tune LLMs for adaptation, while ICL only prompts LLMs for utilization. Furthermore, instruction tuning can enhance the ICL ability of LLMs to perform target tasks, especially in the zero-shot setting (only using task descriptions) [69].
> 我们主要讨论 ICL 中的 demonstration 设计和 ICL 的底层机制
> ICL 和指令微调也有联系，指令微调也使用自然语言将任务格式化，但指令微调会调节 LLM 的参数，而 ICL 仅提供 prompt
> 指令微调可以强化 ICL 的能力，尤其是在 zero-shot 下，即仅有任务描述的情况下
### 6.2.2 Demonstration Design
Several studies have shown that the effectiveness of ICL is highly affected by the design of demonstrations [432, 478, 479] Following the discussion in Section 6.2.1, we will introduce the demonstration design of ICL from three major aspects, i.e., demonstration selection, format, and order.
> 一些研究展示了 ICL 的有效性高度受 demonstartion 的设计的影响
> 我们从 demonstration 选择、格式、顺序三点展开讨论

**Demonstration Selection.** The performance of ICL tends to have a large variance with different demonstration examples [428], so it is important to select a subset of examples that can effectively leverage the ICL capability of LLMs. There are two main demonstration selection approaches, namely heuristic and LLM-based approaches:

- *Heuristic approaches.* Due to their simplicity and low costs, existing work widely adopts heuristic methods to select demonstrations. Several studies employ a k-NN based retriever to select examples that are semantically relevant to the query [428, 480]. However, they perform the selection individually for each example, rather than evaluating the example set as a whole. To resolve this issue, diversitybased selection strategies are proposed to choose the most representative set of examples for specific tasks [481, 482]. Furthermore, in [483], both relevance and diversity are taken into consideration when selecting demonstrations.

- *LLM-based approaches.* Another line of work selects demonstrations by making use of LLMs. For example, LLMs can be utilized to directly measure the informativeness of each example according to the performance gain after adding the example [484]. In addition, EPR [429] proposes a two-stage retrieval approach that first recalls similar examples with an unsupervised method (e.g., BM25) and then ranks them using a dense retriever (trained with positive and negative examples labeled by LLMs). As an alternative approach, the task of demonstration selection can be formulated into a RL problem, where LLMs serve as the reward function to provide feedback for training the policy model [485]. Since LLMs perform well for text annotation [486], some recent studies employ LLM itself as the demonstration generator without human intervention [487].
> 启发式选择方法：
> 一些工作使用基于 k-NN 的 retriever 选取和 query 语义上相关的 example，但它们针对单个 example 进行选取
> 基于多样性的选取策略针对特定任务选取最具代表性的 example 集合
> 基于 LLM 的方法：
> 直接利用 LLM ，衡量加入的 example 之后的 performance gain
> EPR 提出两阶段的检索方法，首先用无监督方法回忆相似的 example，然后将用训练于 LLM 标记的正例和负例样本上的 dense retriever 对这些 examples 进行排序
> 有人将 demonstration 选取构建为 RL 问题，其中 LLM 作为奖励函数，为训练策略模型提供反馈
> 一些工作让 LLM 自己生成 demonstration

To summarize, as discussed in [488], the selected demonstration examples in ICL should contain sufficient information about the task to solve as well as be relevant to the test query, for the above two selection approaches.
> 小结：demonstration 应该和 query 相关，并且包含需要解决的任务的信息

**Demonstration Format.** After selecting task examples, the next step is to integrate and format them into a natural language prompt for LLMs. A straightforward method is to instantiate a pre-defined template with the corresponding input-output pairs [36]. To construct more informative templates, recent studies consider adding task descriptions [69] or enhancing the reasoning capability of LLMs with chain-of-thought prompts [33]. For instance, in [166], the authors collect a large-scale dataset with task descriptions written by humans. After tuning with this dataset, the performance on seen tasks can be boosted, and LLMs can also generalize to unseen tasks to some extent. To reduce the annotation costs, a semi-automated approach has been proposed in [143] by employing a seed set consisting of human-written task descriptions to guide LLMs to generate task descriptions for new tasks. Since it is costly to manually annotate demonstration formats for different tasks, some work also studies how to automatically generate high-quality ones. As two representative methods, Auto-CoT [434] leverages LLMs with the zero-shot prompt “Let’s think step by step” for generating intermediate reasoning steps, while least-to most prompting [439] first queries LLMs to perform problem decomposition and then utilizes LLMs to sequentially solve sub-problems based on the intermediate answers to previously solved ones.
> Demonstration 格式
> 格式化 demonstration 最简单的方法就是使用一个预定义的模板
> 有研究收集了带有人类编写的任务描述的大规模数据集，使用该数据集调节之后，模型在可见任务上的表现可以被增强，且可以在一定程度上泛化到未见的任务
> 为了减少标记开销，有人提出半自动化的方法，使用一个包含了 human-written 任务描述的种子集合，引导 LLM 为新的任务生成任务描述

**Demonstration Order.** LLMs are shown to sometimes suffer from the recency bias, i.e., they are prone to repeat answers that are near the end of demonstrations [479]. Thus, it is important to arrange demonstrations (i.e., task examples) in a reasonable order. Early work proposes several heuristic methods to quickly find a good order. For example, demonstrations can be directly organized according to their similarity to the query in the embedding space [428]: the more similar, the closer to the end. In addition, global and local entropy metrics can be used to score different demonstration orders [432]. To integrate more task information, some recent studies propose to minimize the code length required to compress and transmit task labels, which is inspired by information theory [489]. However, these methods need additional labeled data as the validation set to evaluate the performance of specific demonstration orders. To eliminate this need, the authors in [432] propose to sample the validation data from the LLM itself.
> Demonstration 顺序
> 有人发现 LLM 容易遭受 recency bias，即它们容易重复 demonstration 的临近结尾处的答案，因此需要以合理的顺序排布 demonstration
> 早期的工作提出几个启发式方法，以快速找到好得到顺序，例如按照 demonstration 在嵌入空间相对于 query 的相似度安排顺序，相似度越高越接近结尾
> 另外，有研究使用全局和局部的熵度量来 score 不同的 demonstration 顺序
> 为了整合更多任务信息，一些研究提出最小化压缩和传输任务标签所需要的代码长度
> 这些方法都需要额外的有标记的数据作为验证集，以评估特定 demonstration 顺序的表现，为此，有研究提出让 LLM 自己采样验证集数据
### 6.2.3 Underlying Mechanism
After pre-training, LLMs can exhibit intriguing ICL capability without being updated. In what follows, we discuss two key questions about the ICL ability of LLMs, i.e., “how does pre-training affect the ICL ability” and “how do LLMs perform ICL during inference”.
> 在预训练之后，LLM 就可以在不更新权重的情况下展现出 ICL 能力
> 我们讨论 LLM 的 ICL 能力的两个关键问题：
> 预训练如何影响 ICL 能力
> LLM 如何在推理时执行 ICL

**How Pre-Training Affects ICL?** ICL is first proposed in GPT-3 [55], and it has been shown that the ICL ability becomes more significant with a larger model size. Further, some studies reveal that small-scale PLMs can also demonstrate a strong ICL ability by continual pre-training [490] or fine-tuning [491] on specially designed training tasks, which typically involve additional task examples in the input during the training process. It suggests that the design of training tasks is an important influence factor on the ICL capability of LLMs. Besides training tasks, recent studies have also investigated the relationship between ICL and pre-training corpora [488, 492]. For example, ICL can be theoretically explained as the product of pre-training on documents that exhibit long-range coherence [488]. Further, another study [492] theoretically analyzes that when scaling parameters and data, LLMs based on next-word prediction can emerge the ability of ICL by learning from the compositional structure (e.g., how words and phrases are combined to form larger linguistic units like sentences) present in language data.
> 预训练如何影响 ICL
> 研究发现随着模型规模增大，ICL 能力会变得显著
> 一些研究发现小规模的预训练语言模型也可以通过在特殊设计的训练任务上继续预训练或者微调而展现出强的 ICL 能力
> 这表明训练任务的设计是影响 ICL 能力的重要因素
> 此外，研究发现 ICL 可以理论上被解释为在具有长距离一致性的文档上预训练的结果，也有研究理论上分析了随着参数和数据的 scaling，LLM 可以通过从语言数据中的组合结构 (单词和词组是如何结合起来形成更大的语言单元的) 学习而将下一个词的预测涌现为 ICL

**How LLMs Perform ICL?** At the inference stage, researchers focus on analyzing how the ICL capability operates based on given demonstrations since no explicit learning or updating is involved. According to the discussion in [493], there are two main ways for LLMs to utilize demonstrations: task recognition and task learning.
> LLM 如何进行 ICL
> LLM 利用 demonstartion 主要有两种方式：任务识别和任务学习

- *Task recognition.* In the first way, LLMs recognize the task from demonstrations and utilize the prior knowledge obtained from pre-training to solve new test tasks. A Probably Approximately Correct (PAC) framework [494] has been proposed to assess the learnability of ICL. It assumes that there exists a latent variable representing the task in the pretraining data, and LLMs have been shown to be capable of capturing this variable from demonstrations, enabling them to recognize the task in ICL. Also, the interpretation of ICL as task recognition is supported by several empirical studies [478, 495]. For example, it has been observed that replacing the inputs or labels of demonstrations with random ones sampled from the input or label space does not seriously hurt the performance of LLMs, indicating that LLMs mainly recognize the target task from demonstrations instead of learning from them [478, 493]. Similarly, LLMs can exhibit decent performance even if the prompt template is irrelevant or misleading [495].
> 任务识别
> LLM 从示例中识别任务，然后使用从预训练中获得的先验知识解决新的测试任务
> 概率近似正确框架 (PAC) 被提出用于评估 ICL 的可学习性，它假设在预训练数据中存在能表示任务的隐变量，而 LLM 经过训练后，有能力从示例中捕获这些隐变量，这使得它们可以识别 ICL 中的任务
> 一些经验性的学习也支持了将 ICL 解释为任务识别，例如，有人观察到将示例中的输入或标签替换为从输出或标签空间随机采样的样本并不会严重损害 LLM 的表现，这表明了 LLM 主要从示例中识别出目标任务，而不是从中学习
> 类似地，LLM 甚至可以在 prompt 模板是不相关的或误导的情况下展示出较好的表现

- *Task learning.* In the second way, LLMs learn new tasks unseen in the pre-training stage only through demonstrations. Specially, task learning is analyzed mainly from the perspective of gradient descent and considered as implicit fine-tuning [65, 496]. Then, ICL can be explained as follows: by means of forward computation, LLMs generate metagradients with respect to demonstrations and implicitly perform gradient descent via the attention mechanism. Experiments also show that certain attention heads in LLMs are capable of performing task-agnostic atomic operations (e.g., copying and prefix matching), which are closely related to the ICL ability [497]. Furthermore, some studies abstract ICL as an algorithm learning process [498]. For example, the authors in [498] find that LLMs essentially encode implicit models through their parameters during pre-training. With the examples provided in ICL, LLMs can implement learning algorithms such as gradient descent or directly compute the closed-form solution to update these models during forward computation. Under this explanation framework, it has been shown that LLMs can effectively learn simple linear functions and even some complex functions like decision trees with ICL [498].
> 任务学习
> LLM 从示例中学习在预训练阶段没有见过的任务
> 任务学习一般从梯度下降的角度被分析，并且被任务是一种隐式的微调，由此，这样解释 ICL：在前向计算中，LLM 生成相对于示例的元梯度，并且隐式地通过 attention 机制执行梯度下降
> 实验也展示了 LLM 中特定的 attention head 可以执行任务不可知的原子操作，例如拷贝和前缀匹配，这和 ICL 能力是紧密相关的
> 进一步，一些研究将 ICL 抽象为一个算法学习过程，例如有作者发现 LLM 在预训练时实质上通过它们的参数编码隐式模型，借助 ICL 中提供的示例，LLM 在前向传播中执行例如梯度下降的学习算法或直接计算闭式解以更新这些隐式模型
> 在这个解释框架下，研究发现 LLM 可以通过 ICL 学习简单的线性函数甚至向决策树这样的复杂函数

As discussed in a recent study [493], LLMs exhibit the abilities of both task recognition and task learning in ICL, but the two abilities seem to be possessed with different model scales. As shown in the experiments [493], the ability of task recognition is easier to obtain, and even a small LM with only 350M parameters can exhibit this ability, while task learning can only emerge for LLMs with at least 66B parameters. Another study [499] also supports this finding with specially designed experiments. They set up the tasks with flipped and semantically unrelated labels in the experiment, which require task learning when performing ICL. The results suggest that small LMs tend to disregard the labels and mainly depend on their prior knowledge to accomplish the task, while LLMs have the ability to surpass their prior knowledge and acquire new knowledge from demonstrations, resulting in better outcomes. Furthermore, to improve the task learning ability, Meta-In-Context Learning [500] proposes to include multiple related tasks instead of just a single one in the prompt. In addition, Symbol Tuning [501] fine-tunes LLMs on demonstrations with semantically unrelated labels (e.g., foo/bar instead of positive/negative for sentiment analysis), forcing LLMs to learn the task from demonstrations instead of relying on prior knowledge.
> LLM 在 ICL 展示了任务识别和任务学习两种能力，但这两种能力似乎由不同规模的模型拥有
> [493]展示了任务识别能力更易于获得，并且即便仅有 350M 的小模型也会展示这一能力，但是任务学习能力似乎需要至少 66B 以上的模型
> [499]设计实验支持这一发现，它们发现小模型倾向于忽视这些标签，主要依赖于它们的先验知识来完成任务，而大模型则会从示例中获取新的知识
> Me
> 为了进一步提高模型的任务学习能力，[500]提出将多个相关的任务放入 prompt，而不是仅涉及一个任务，[501]使用带有语义上不相关标签的示例微调 LLM，强迫 LLM 从示例中学习任务，而不是依赖于先验知识

## 6.3 Chain-of-Thought Prompting
![[A Survey of LLMs-Fig14.png]]

Chain-of-Thought (CoT) prompting [33, 502] is an improved prompting strategy to boost the performance of LLMs on complex reasoning tasks, such as arithmetic reasoning [503], commonsense reasoning [504], and symbolic reasoning [33]. Instead of simply constructing the prompts with inputoutput pairs like ICL, CoT prompting further incorporates intermediate reasoning steps, which serve as the bridge between inputs and outputs. Figure 14 presents an illustration of CoT. In the following part, we will first elaborate on the basic CoT prompting approach and its improved strategies, then discuss when and why CoT prompting works.
> ICL 使用输入-输出对构造 prompt，CoT 比 ICL 更进一步，在 prompt 中包含了中间分析步骤，连接了输入和输出
> 本节介绍基本 CoT 提示方法和其策略，然后讨论 CoT 有效的原因

### 6.3.1 Basic CoT Prompting Approach
CoT prompting is first proposed as an extension of ICL [33], which augments each demonstration ⟨input, output⟩ as ⟨input, CoT, output⟩. A CoT is a series of intermediate reasoning steps for connecting the input and output. With these augmented demonstrations, LLMs can follow them to generate CoTs and the answer for a new input. However, unlike ⟨input, output⟩ pairs in ICL, CoTs are difficult to obtain and usually require human annotation. Fortunately, it has been found that LLMs can be triggered to generate CoTs through simple instructions like “Let’s think step by step.” [505], making CoT prompting easy to use. There are also alternative magic prompts that can elicit the ability of CoT reasoning and further improve the performance of LLMs, such as “Take a deep breath and work on this problem step-by-step.” [473].
> CoT 最先作为 ICL 的拓展被提出
> CoT 相较于 ICL 的输入-输出对更难以获得，一般需要人类标记
> LLM 可以通过简单的指令例如“Let's think step by step”来被引发 CoT，或者是 “Take a deep breath and work on this problem step-by-step”

As illustrated in Figure 15, the generation process of CoT follows a chain structure in the basic CoT prompting approach, where LLMs generate CoTs step by step. Typically, CoT takes the format of natural language text. However, textual CoTs may not work well on complex tasks that require rigorous logic for reasoning. Considering this, some work uses code [506, 507] due to its structured and precise nature. Furthermore, the authors in [508] propose to dynamically select text or code as the format of CoTs to combine their advantages.
> CoT 的结构是链式的，LLM 一步一步生成 CoT，CoT 一般是文本形式
> 一些工作使用代码提高对于需要逻辑和推理的复杂任务的表现
> 有人提出动态选择代码或文本作为 CoT 的格式

### 6.3.2 Improved CoT Prompting Strategies
![[A Survey of LLMs-Fig15.png]]

Despite the performance improvement in complex reasoning tasks, CoT prompting still suffers from problems like incorrect reasoning and instability. In this part, we first introduce how to design better CoT prompts and enhanced CoT generation strategies, and then introduce the extension of the basic chain structure of CoT. Figure 15 illustrates the evolution of representative CoT prompting strategies.
> CoT 仍然存在错误推理和不稳定的问题
> 本节介绍 CoT 的策略

**Better Prompt Design.** Since CoT prompting relies on prompts to elicit the reasoning capabilities of LLMs, the design of prompts is critical to its performance. As a direct approach, it is shown that using diverse CoTs (i.e., multiple reasoning paths for each problem) can effectively enhance the performance [437]. Another intuitive idea is that prompts with more complex reasoning paths are more likely to elicit the reasoning ability of LLMs [433], which can result in higher accuracy in generating correct answers. However, all these approaches rely on annotated CoT datasets, which limits their use in practice. To overcome this limitation, magic instructions such as “Let’s think step by step” can be used to automatically construct CoTs by prompting LLMs [434].
> 使用多样的 CoT，即对于一个问题使用多个推理路径，可以有效提高表现
> 使用更复杂的推理路径可以提高表现
> 但它们都依赖于有标记的 CoT 数据集
> 一些指令例如 "magic instructions" 可以用于指导 LLM 自动构造 CoT

**Enhanced CoT Generation.** Since LLMs are prone to producing incorrect reasoning steps and exhibiting instability in the generation process, there are a number of studies [436, 509] to improve the generation of CoT. In this part, we will introduce two typical approaches to enhancing the generation of CoT: sampling- and verification-based methods.
> LLM 容易生成错误的推导步骤，为此本节介绍提高 CoT 生成的方法：基于采样的以及基于验证的方法

- *Sampling-based methods.* LLMs are known to suffer from instability during inference, which can lead to unfaithfulness in the generated reasoning steps. To address this issue, some work proposes to sample multiple reasoning paths instead of using greedy decoding. As a representative solution, self-consistency [436] first generates several reasoning paths and then takes an ensemble over the corresponding answers, selecting the most consistent one through majority voting. However, such a method can still lead to wrong answers when most of the reasoning paths are misled. Considering this, the authors in [433] only vote on the k most complex reasoning paths based on their observation that reasoning paths with higher complexity (e.g., more reasoning steps) usually have better performance. Furthermore, MCR [510] proposes referring to the steps from other reasoning paths when generating the next step, and performs reasoning across multiple reasoning paths to generate the final answer.
> 基于采样
> 有工作提出采样多个推理路径，而不使用贪心解码
> self-consistency 先生成数个推理路径，然后将对应的答案集成 (多数投票，选择最一致的)，但多数路径都错误时，该方法也会错误
> [433]观察到具有更高复杂度的推理路径一般表现更好，因此仅在最复杂的 k 个路径中投票
> [510]提出生成下一步时参考其他推理路径，并跨多个推理路径进行推理以生成最终的答案

- *Verification-based methods.* The sequential nature of reasoning steps in CoTs can lead to the accumulation of errors in the generated CoTs when certain steps are incorrect. To mitigate this problem, recent studies propose to verify the correctness of generated reasoning steps with either trained verifiers or LLMs themselves. For example, DIVERSE [509] trains solution-level and step-level verifiers respectively to examine the reasoning steps at different granularities. Another approach [511] utilizes LLMs to verify the correctness of reasoning steps through step-by-step self-verification with a specially designed reasoning format. In addition, several studies propose backward reasoning for verification: it first deduces the necessary question conditions [512, 513] or variables [514] from the model’s predictions, and then compares them with the original ones.
> 基于验证
> 推理步骤是顺序的，可能一步错步步错，导致错误累积
> 研究提出验证推理步骤的正确性，使用训练的 verifier 或 LLM 本身
> [509]训练 solution-level 和 step-level 的 verifier，在不同粒度验证推理步骤
> [511]让 LLM 自己验证自己的每一步
> 研究提出向后推理来验证：首先从模型预测反向推导必要的问题条件或变量，然后和原来的比较

**Reasoning Structure Extension.** Despite the generality, the chain reasoning structure of basic CoT prompting limits its effectiveness in solving complex tasks, which require exploration like foresight and backtracking during inference. Therefore, many studies have been devoted to extending the reasoning structure by designing more intricate thought processes, e.g., tree- and graph-structured reasoning.
> 复杂任务在推理时需要 foresight 和 backtracking，因此研究意在设计更复杂的思考过程，例如树或图结构的推理，拓展推理结构

- *Tree-structured reasoning.* This approach (exemplified by Tree of Thoughts (ToT) [451, 515]) formulates the reasoning process in a hierarchical tree structure, where intermediate thoughts are nodes. In this way, it enables LLMs to explore multiple reasoning paths in parallel and further supports the operation of lookahead and backtracking to facilitate more comprehensive decisions. In addition, TouT [516] takes the uncertainty of intermediate thoughts into account for thought evaluation based on Monte Carlo Dropout.
> 树结构
> 以层次树结构进行推理，中间思考是节点
- *Graph-structured reasoning.* Although the tree structure facilitates parallel reasoning, it also imposes restrictions on the reasoning process. With more complex topological structures, graphs offer greater flexibility in reasoning, enabling the characterization of more intricate relationships and interactions. For instance, Graph of Thoughts (GoT) [517, 518] conceptualizes the reasoning process as an arbitrary graph, where vertices denote intermediate thoughts and edges denote the interdependence between these thoughts. Compared with ToT, it can further utilize thoughts from other reasoning paths when generating new thoughts. However, such an approach requires a large number of interactions with LLMs, making the thought exploration process highly inefficient. To reduce potentially meaningless thought exploration, XoT [519] further proposes to guide the search of thoughts with pre-trained policy and value networks.
> 图结构
> 节点表示中间思考，边表示依赖
> XoT 提出用预训练的策略和价值网络引导思考的搜索
### 6.3.3 Further Discussion on CoT Prompting
In this part, we present discussions regarding two fundamental questions related to CoT prompting, i.e., “when does CoT prompting work for LLMs” and “why can LLMs perform CoT reasoning”.

*When CoT Prompting Works For LLMs*? Since CoT reasoning is an emergent ability [31], it only has a positive effect on sufficiently large models (typically containing 10B or more parameters [33]) but not on small models. Moreover, since CoT prompting augments the standard prompting with intermediate reasoning steps, it is mainly effective for the tasks that require step-by-step reasoning [33], e.g., arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Whereas, for other tasks that do not rely on complex reasoning, CoT prompting might lead to worse performance than standard prompting [438], e.g., MNLIm/mm, SST-2, and QQP from GLUE [260]. Interestingly, it seems that the performance gain brought by CoT prompting could be significant only when standard prompting yields poor results [33].
> CoT 什么时候对于 LLM 有用
> CoT 推理是涌现能力，它仅对足够大的模型有正面效果
> CoT 对于需要步步推理的任务有益
> 对于不依赖于推理的任务，CoT 可能导致比标准 prompt 效果更差
> 似乎只有在标准 prompt 效果很差时，CoT 才有比较好的效果

Why LLMs Can Perform CoT Reasoning? As the second question, we discuss the underlying mechanism of CoT prompting in the following two aspects.

- *The source of CoT reasoning ability.* Regarding the source of CoT reasoning capability, it is widely hypothesized that it can be attributed to training on code since models trained on it show a strong reasoning ability [47, 520, 521]. Intuitively, code data is well organized with algorithmic logic and programming flow, which may be useful to improve the reasoning performance of LLMs. However, this hypothesis still lacks publicly reported evidence of ablation experiments (with and without training on code). In addition, instruction tuning seems not to be the key reason for obtaining the CoT reasoning ability, since it has been empirically shown that instruction tuning on non-CoT data does not improve the performance on held-out CoT reasoning benchmarks [69].
> CoT 能力的来源可以归功于训练于代码，有假设认为是因为训练于代码数据的模型有强的逻辑推理能力
> 代码数据按照算法逻辑和编程流组织
> 但该假设缺乏消融实验的证据
> 另外，指令微调也不是 CoT 的关键，因为有实验表明在非 CoT 数据上的指令微调不会提高 CoT 表现

- *The effect of CoT prompting components.* The major distinction between CoT prompting and standard prompting is the incorporation of reasoning paths prior to the final answer. Thus, some researchers investigate the effects of different components in the reasoning paths. Specifically, a recent study identifies three key components in CoT prompting, namely symbols (e.g., numerical quantities in arithmetic reasoning), patterns (e.g., equations in arithmetic reasoning), and text (i.e., the rest of tokens that are not symbols or patterns) [522]. It is shown that the latter two parts (i.e., patterns and text) are essential to the model performance, and removing either one would lead to a significant performance drop. However, the correctness of symbols and patterns does not seem critical. Further, there exists a symbiotic relationship between text and patterns: the text helps LLMs to generate useful patterns, and patterns aid LLMs to understand tasks and generate texts that help solve them [522].
> 一些研究者研究了推理路径中不同成分的效果，确定了三个关键成分：符号 (数学量)、模式 (等式)、文本 (不是符号和模式的其他 token)，其中模式和文本较为关键，移除任意一个性能都会显著降低
> 符号和模式的正确性则不关键
> 另外，文本和模式之间存在共生的关系，文本帮助 LLM 生成模式，模式帮助 LLM 理解任务，生成文本

In summary, CoT prompting provides a general and flexible approach to eliciting the reasoning ability of LLMs. There are also some preliminary attempts to extend this technique to solve multimodal [523] and multilingual tasks [524].
## 6.4 Planning for Complex Task Solving
Prompting with ICL and CoT is a conceptually simple yet general approach to solving various tasks. However, this approach struggles with complex tasks like mathematical reasoning [525] and multi-hop question answering [526]. As an enhanced approach, prompt-based planning has been proposed to break down complex tasks into smaller subtasks and generate a plan of actions to accomplish the task.
> ICL 和 CoT 难以解决数学推理和多跳问答
> 基于 prompt 的规划将复杂问题分解为子问题，并生成动作计划以完成任务
### 6.4.1 The Overall Framework
![[A Survey of LLMs-Fig16.png]]

In this part, we first formulate the general planning paradigm of LLMs for solving complex tasks, which is illustrated in Figure 16.

In this paradigm, there are typically three components: task planner, plan executor, and environment. Specifically, task planner, which is played by LLMs, aims to generate the whole plan to solve a target task. The plan can be presented in various forms, e.g., an action sequence in the form of natural language [439] or an executable program written in programming language [443]. The LLM-based task planner can be enhanced with the memory mechanism for plan storage and retrieval, which is helpful for long-horizon tasks. Then, plan executor is responsible for executing the actions in the plan. It can be implemented by models like LLMs for textual tasks [441] or by tools like code interpreters for coding tasks [450]. Furthermore, environment refers to where the plan executor carries out the actions, which can be set differently according to specific tasks, e.g., the LLM itself [527] or an external virtual world like Minecraft [528]. It provides feedback about the execution result of the action to the task planner, either in the form of natural language [450] or from other multimodal signals [446].
> planning 范式中，一般有三个成分：任务规划者、任务执行者、环境
> LLM 作为任务规划者，生成解决任务的计划，可以是动作序列或者是程序
> 任务执行者执行动作
> 环境指执行者执行动作的环境，可以是 LLM 自己，或者外部环境例如 Minecraft，环境将动作的反馈提供给规划者，反馈可以是文本也可以是多模态信号

For solving a complex task, the task planner first needs to clearly understand the task goal and generate a reasonable plan based on the reasoning of LLMs (See Section 6.4.2). Then, the plan executor acts according to the plan in the environment, and the environment will produce feedback for the task planner (See Section 6.4.3). The task planner can further incorporate the feedback obtained from the environment to refine its initial plan and iteratively perform the above process to get better results as the task solution (See Section 6.4.4).
> 规划者先理解任务目标，生成 plan，执行者根据 plan 执行，环境提供反馈
> 规划者接受反馈，改善初始 plan
> 该过程迭代进行
### 6.4.2 Plan Generation
Plan generation focuses on directly generating action sequences by prompting LLMs. Based on the format of the generated plans, existing work can be divided into two groups: text-based and code-based approaches.
> plan 生成就是通过 prompt LLM 生成动作序列

*Text-based Approaches.* It is straightforward for LLMs to generate plans in the form of natural language. In this approach, LLMs are prompted to generate a sequence of actions for the plan executor to perform and solve the complex task. For example, Plan-and-Solve [441] adds explicit instructions like “devise a plan” to directly prompt the LLM for planning in a zero-shot manner, while Self-planning [529] and DECOMP [440] add demonstrations in the prompt to guide the LLM to devise a plan through ICL. Following this way, some work further considers incorporating extra tools or models when planning. For example, ToolFormer [80] first annotates a pre-training corpus with potential API calls using LLMs, and then fine-tunes LLMs on it, so that LLMs can learn when and how to call APIs and incorporate the results returned by APIs during generation. HuggingGPT [444] introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution.
> 基于文本：生成文本 plan
> ToolFormer 标注了包含 API 调用的训练语料库，在其上微调 LLM，以让 LLM 学习如何在生成时调用 API，并接受结果
> HuggingGPT 将 LLM 作为 controller ，基于模型描述，使用对应的 HuggingFace 的模型

*Code-based Approaches.* Although text-based approaches sound intuitive, they cannot guarantee faithful execution of the plan, which may lead to failure even when the plan is sound. To address this issue, code-based approaches have been proposed to generate more verifiable plans in the form of executable code in programming languages, e.g., Python or PDDL. In this way, LLMs are first prompted to generate the program and then utilize a deterministic solver to execute it. For example, Faithful CoT [442] and PAL [443] decompose a reasoning task into two stages: at the first stage, the LLM generates a plan conditioned on the query; at the second stage, a deterministic solver executes the plan to derive the final answer. Furthermore, code-based approaches can be applied to embodied agents in a similar way. For example, PROGPROMPT [530] and LLM+P [531] first utilize LLMs to generate plans in the form of python functions or PDDL files, and then leverage a virtual agent or classical planner to solve the problem according to the code-based plans.
> 基于代码：生成代码
> 例如生成 Python，代码的执行是确定性的，不像文本
> [442-443]分解了推理任务，第一阶段，LLM condition on query 生成 plan，第二阶段，按照 plan 确定性执行
> 基于代码的方法可以用于具身智能体，先让 LLM 以 python 函数形式生成 plan，然后执行
### 6.4.3 Feedback Acquisition
After executing the generated plan, the environment would produce the feedback signal to the LLM-based task planner, which can be used to refine its initial plan for better results. In existing work, there are typically two sources of feedback from the environment, depending on their relationship with the LLM-based task planner: internal (i.e., the LLM itself) and external (e.g., tools or virtual worlds) feedback.

*Internal Feedback.* The LLM itself can be utilized as a feedback provider. One straightforward way is to directly evaluate the quality of the generated plans through prompting. For example, RAP [447] evaluate the likelihood that each candidate plan can lead to task success, while Tree of Thoughts [527] proposes to vote across plans by making comparisons between them. Further, LLMs can provide feedback based on the intermediate results from the plan executor. For example, Reflexion [450] utilizes LLMs to transform sparse result signals (e.g., success or failure) into concrete text-based feedback (e.g., “You should recommend comedies that the user mentions in the query instead of horror movies”) and stores this feedback in long-term memory for future planning.
> LLM 自己提供反馈
> 例如通过 prompt 让 LLM 直接评估生成的 plan 的质量
> [447]评估每个候选 plan 可以导向任务成功的概率
> [527]在 plan 之间比较投票
> 或者基于执行者的中间结果给出反馈

*External Feedback.* In addition to LLMs, external objects can also provide feedback signals. For example, tools like code interpreters are widely used in programming tasks to provide real-time error messages [450], models like stable diffusion [532] can be used in multimodal tasks to provide visual perception [446], and virtual worlds like Minecraft can provide immersive experiences [528]. Besides, some work (e.g., Generative Agents [533]) explores multi-agent collaboration in simulated environments, where each agent receives feedback not only from interaction with the environment but also from communication with other agents.
> 外部反馈
> 例如代码解释器，提供实时错误信息
> stable diffusion 提供视觉信息
> 以及 Minecraft
> 有人探究模拟环境中的多智能体合作，每个智能体从与其他智能体的交流以及环境中接受反馈
### 6.4.4 Plan Refinement
With access to feedback from the environment, the task planner can accordingly refine its current plan and iteratively go through the “planning – execution – refinement” loop for better results. In this part, we summarizes three major refinement approaches in existing work.

*Reasoning.* The feedback data from the environment may not be directly suitable to be utilized by LLMs for plan refinement, e.g., containing irrelevant information or taking a non-language form. To solve this, some work adds the explicit reasoning process to extract critical information from feedback [448, 449]. For example, React [449] prompts LLMs with demonstrations to generate reasoning traces over feedback. It has been widely used in autonomous agent projects, such as AutoGPT [534], which can automatically reason over the observed feedback to revise the initial plan for solving various user requests. However, these approaches typically fix the order of reasoning and planning. To support flexible switching between the two processes for better performance, ChatCoT [448] further unifies the tool-augmented reasoning process into a multi-turn conversation between the LLM-based task planner and the tool-based environment.
> 添加对反馈信息进行显式分析的过程，以从中提取关键信息
> [449]让 LLM 生成对于反馈 reasoning trace，被广泛用于自动智能体项目

*Backtracking.* Early methods mainly consider planning forward actions while maintaining the existing plan, thus likely leading to local optimal plans based on a short-term evaluation. To solve this, Tree of Thoughts [527] allows backtracking with search algorithms like breadth-first and depth-first search to make global planning. It refines the plan step by step by backtracking to the last state in the initial plan and choosing the next unexplored action. Furthermore, some studies [446, 535] utilize feedback signals to revise the entire plan. For example, DEPS [535] selects a better plan according to feedback signals, while TIP [446] adds feedback signals to prompts for the LLM-based planner to revise each step in the initial plan.
> 早期方法考虑维护现存计划的同时规划动作，可能导致局部最优
> [527]在规划时使用深度优先或者广度优先的搜索算法回溯
> [446, 535]使用反馈修改整个 plan

*Memorization.* In order to handle long-horizon tasks, it has become a key approach to aid plan refinement with longterm memory in addition to utilizing the short-term memory of LLMs through ICL. For example, Reflexion [450] stores the feedback from self-reflection into the memory, so previous feedback can be retrieved for plan refinement. Generative Agents [533] designs the memory stream mechanism as the core component of agents for action planning and reflection. Further, the skill library mechanism [445, 528] is proposed to store successful plans in the library, which can be reused and synthesized as complex plans for novel tasks. To implement the long-term memory mechanism, tools like vector databases (e.g., milvus [536]) can be used to encode plans or feedbacks into high-dimensional vectors for efficient storage and retrieval at a large scale. MemoryBank [537] further proposes the memory updating mechanism to allow memory forgetting and strengthening following the Ebbinghaus Forgetting Curve theory
> 为了处理时间跨度任务
> Reflexion 将自我反馈存储到 memory，因此可以检索之前的反馈用于 plan refinement
> Generative Agents 设计 memory stream 机制
> 可以用向量数据库将 plan 或反馈编码为高维向量，便于大规模存储和检索
> MemoryBank 提出 memory 更新，允许遗忘和强化记忆，遵循艾宾浩斯曲线
# 7 Capacity and Evaluation
To examine the effectiveness and superiority of LLMs, a surge of tasks and benchmarks have been proposed for conducting empirical ability evaluation and analysis. In this section, we first introduce three types of basic ability evaluation of LLMs for language generation and understanding, then present several advanced ability evaluations with more complicated settings or goals, and finally discuss existing benchmarks, evaluation approaches, and empirical analysis.
> 介绍三类基本评估
> 介绍几个高级评估
> 讨论现存 benchmarks、评估方法、经验分析
## 7.1 Basic Ability
In this part, we mainly focus on three basic types of ability evaluation for LLMs, i.e., language generation, knowledge utilization, and complex reasoning. It is noted that we do not intend to have complete coverage of all the related tasks, but instead only focus on the most widely discussed or studied tasks for LLMs. Next, we introduce these tasks in detail.
> 三类基本评估：语言生成、知识利用、复杂分析
### 7.1.1 Language Generation
According to the task definition, existing tasks about language generation can be roughly categorized into language modeling, conditional text generation, and code synthesis tasks. Note that code synthesis is not a typical NLP task, we include it for discussion because it can be directly solved by a number of LLMs (trained on code data) in a similar generation approach as natural language text.
> 语言生成包括：语言建模、条件文本生成、代码合成

*Language Modeling.* As the most fundamental ability of LLMs, language modeling aims to predict the next token based on the previous tokens [1], which mainly focuses on the capacity of basic language understanding and generation. For evaluating such an ability, typical language modeling datasets that existing work uses include Penn Treebank [538], WikiText-103 [539], and the Pile [161], where the metric of perplexity is commonly used for evaluating the model performance under the zero-shot setting. Empirical studies [55, 93] show that LLMs bring substantial performance gains over the previous state-of-the-art methods on these evaluation datasets. To better test the modeling capacity of long-range dependencies in text, the LAMBADA dataset [233] has been introduced, where LLMs are required to predict the last word of sentences based on a paragraph of context. Then, the accuracy and perplexity of the predicted last words are employed to evaluate LLMs. As shown in existing work, the performance on the language modeling tasks typically follows the scaling law [30], which means that scaling language models would improve the accuracy and reduce the perplexity.
> 语言建模：预测下一个 token，聚焦于基础语言理解能力和生成能力
> 数据集：Penn Treebank、WikiText-103、Pile
> 零样本设定下常用 metric：困惑度 perplexity
> LLM 相较于之前的 sota 在这些 evaluation 数据集上涨幅明显
> 建模长距离依赖的数据集：LAMBADA，LLM 基于段落预测句子的下一个词，然后基于该预测的正确率和 perplexity 评估 LLM
> 语言建模的 performance 遵循 scaling law，即 scaling 语言模型会提高准确率，降低困惑度

*Conditional Text Generation.* As an important topic in language generation, conditional text generation [48] focuses on generating texts satisfying specific task demands based on the given conditions, typically including machine translation [624], text summarization [548], and question answering [557]. To measure the quality of the generated text, automatic metrics (e.g., Accuracy, BLEU [625] and ROUGE [626]) and human ratings have been typically used for evaluating the performance. Due to the powerful language generation capabilities, LLMs have achieved remarkable performance on existing datasets and benchmarks. For instance, GPT-4 exhibits comparable performance as commercial translation products, even for the translation task of languages that are with significant linguistic distance [627]. On news summarization tasks (i.e., CNN/DM and XSUM), LLMs also demonstrate comparable performance with human freelance writers [628]. Despite the rapid progress on model capacity, there are increasing concerns on the feasibility of existing automatic metrics to faithfully assess the performance of LLMs in conditional text generation tasks [628–630]. As the alternatives to automatic metrics, recent studies also propose to incorporate LLMs as generation evaluators to examine the quality of the generated content [138, 631, 632]. Moreover, researchers also explore more challenging language generation tasks for LLMs, such as structured data generation [458] and long text generation [46, 633, 634].
> 条件文本生成：在给定条件下，生成满足任务要求的文本
> 包括了机器翻译、文本总结、问答
> 度量：Accuracy、BLEU、ROUGE
> LLM 在这些任务上表现很好，即便对于具有显著语言距离的目标语言，也有良好的翻译质量
> 有人担心现存的自动度量不能良好反应 LLM 在条件文本生成上的能力
> 有人提出让 LLM 本身作为评估者
> 有研究探索了更挑战的语言生成任务，如：结构数据生成、长文本生成

*Code Synthesis.* In addition to generating high-quality natural language text, existing LLMs also show strong abilities to generate formal language, especially computer programs (i.e., code) that satisfy specific conditions, called code synthesis [635]. Unlike natural language generation, as the generated code can be directly checked by execution with corresponding compilers or interpreters, existing work mostly evaluates the quality of the generated code from LLMs by calculating the pass rate against the test cases, i.e., pass@k. Recently, several code benchmarks focusing on functional correctness are proposed to assess the code synthesis abilities of LLMs, such as APPS [378], HumanEval [105], and MBPP [208]. Typically, they consist of diverse programming problems, with text specification and test cases for correctness checking. To improve such an ability, it is key to fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks [86]. In addition, existing work has proposed new strategies to generate code, e.g., sampling multiple candidate solutions [208] and planning-guided decoding [636], which can be considered as the imitation of bug-fixing and code-planning processes by programmers. Impressively, LLMs have recently shown competitive performance with humans by achieving a ranking of the top 28% among users on the programming contest platform Codeforces [114]. 
> 代码合成
> 代码可以直接由编译器或解释器检查，现存的工作一般计算 LLM 生成的代码相对于测试用例的通过率，例如 pass@k (给定 LLM 生成的 k 个程序，如果没有程序通过，则记为0，否则记为1)
> 最近一些 code benchmark 聚焦于函数正确性，例如 APPS、HumanEVal、MBPP
> 在代码数据上微调可以提高代码合成能力
> 有研究提出生成代码的新策略：采样多个候选解以及规划引导的解码，这可以被视作对于程序员修改 bug 和代码规划行为的模仿

Further, GitHub Copilot has been released to assist programming in coding IDEs (e.g., Visual Stuodio and JetBrain IDEs), which can support a variety of languages including Python, JavaScript, and Java. A viewpoint article entitled “The End of Programming” [637] in Communications of the ACM has discussed the impact of AI programming in the field of computer science, emphasizing an important shift towards the highly adaptive LLM as a new atomic unit of computation. 

**Major Issues.** Although LLMs have achieved splendid performance in generating human-like text, they are susceptible to suffering from two major issues in language generation as discussed below.
> 主要问题

- *Unreliable generation evaluation.* With the advancement of language generation ability of LLMs, existing studies find that the generated texts from LLMs have reached a comparable quality to the reference texts on a variety of text generation tasks. However, due to the intrinsic weakness of existing evaluation benchmarks, there exists pronounced inconsistency between human evaluation and automatic reference-based metrics [628–630, 638]. For example, in OpenDialKG [551], ChatGPT underperforms a fine-tuned GPT-2 on BLEU and ROUGE-L metrics, while earning more favor from human judgment [638]. Furthermore, existing work argues that even human evaluation may not be robust enough [628, 629, 639, 640]. In some cases, it is difficult to achieve a high level of consensus among human annotators [629], and there is also a large gap between the annotation quality of crowdworkers and experts [639, 640]. Thus, how to conduct reliable evaluation for language generation tasks in the era of LLMs has become a fundamental yet challenging research topic. Recently, increasing research work proposes to leverage LLMs to improve the evaluation quality of the generated texts. Specially, LLMs can be used to improve the evaluation quality of existing metrics. For example, Para-Ref [641] augments various automatic metrics by leveraging LLMs to paraphrase existing reference into semantically equivalent references with diverse expressions. Further, LLMs are widely employed as the evaluators of text generation in a reference-free manner, including evaluating a single prediction [631, 632, 642] or comparing several candidates [138, 643–645]. Nevertheless, LLMs may expose bias (e.g., order bias or preference for LLM-generated texts over human-written texts) as language generation evaluators, demonstrating disparities when compared to human evaluation [632, 646, 647].
> 不可靠的生成评估
> LLM 在评估 benchmark 上优秀，但是自动度量和人类评估存在差异
> 有工作也认为人类评估不够健壮，有时难以在人类标记者之间达到共识，标记者之间的标记质量差异也很大
> 有人考虑用 LLM 提高对生成文本的评估质量，即利用 LLM 将现存的参考重述为语义相等但表示多样的参考来增强多种自动度量
> LLM 也可以直接作为评估者，但 LLM 也存在偏差，例如偏差于 LLM 生成的文本而不是人类写的文本

> [! Unreliable Generation Evaluation]
> LLMs have been capable of generating texts with a comparable quality to human-written texts, which however might be underestimated by automatic reference-based metrics. As an alternative evaluation approach, LLMs can serve as language generation evaluators to evaluate a single text, compare multiple candidates, and improve existing metrics. However, this evaluation approach still needs more inspections and examinations in real-world tasks.

- *Underperforming specialized generation.* Although LLMs have learned general language patterns to generate coherent text, their proficiency in generation might be constrained when dealing with a specialized domain or task. For instance, a language model that has been trained on general web articles may face challenges when generating a medical report which involves many medical jargon and methods. Intuitively, domain knowledge should be critical for model specialization. However, it is not easy to inject such specialized knowledge into LLMs. As discussed in recent analyses [47, 648], when LLMs are trained to exhibit some specific ability that allows them to excel in some areas, they might struggle in others. Such an issue is related to catastrophic forgetting [649, 650] in training neural networks, which refers to the conflict phenomenon of integrating new and old knowledge. Similar cases also occur in human alignment of LLMs, where “alignment tax” [66] (e.g., a potential loss in the in-context learning ability) has to be paid for aligning to human values and needs. Moreover, due to the limitations of sequence modeling architecture, LLMs still face challenges in the understanding and generation of structured data. Consequently, they often fall behind task-specific models on complex structured data tasks, such as knowledge-base question answering and semantic parsing [458, 651]. Therefore, it is important to develop effective model specialization methods that can flexibly adapt LLMs to various task scenarios, meanwhile retaining the original abilities as possible.
> 对于特定生成表现不好
> LLM 在处理特定领域/任务时效果不一定好，例如生成医学报告
> 领域知识被用于模型专业化，但需要考虑如何注入知识
> LLM 展现特定能力的同时会在其他领域较弱，相关的问题是灾难性遗忘
> 类似的问题在对齐时也会出现，称为 alignment tax
> 受序列建模架构的限制，LLM 在生成和理解结构化数据时仍有挑战，因此一般在复杂的结构化数据任务上比针对任务的模型弱，例如知识库问答和语义分析
> 需要法阵模型专业化方法，在将模型适应到任务情景的同时，尽量保持原有能力

> [!Underperforming Specialized Generation]
> LLMs may fall short in mastering generation tasks that require domain-specific knowledge or generating structured data. It is non-trivial to inject specialized knowledge into LLMs, meanwhile maintaining the original abilities of LLMs.

### 7.1.2 Knowledge Utilization
Knowledge utilization is an important ability of intelligent systems to accomplish knowledge-intensive tasks (e.g., commonsense question answering and fact completion) based on supporting factual evidence. Concretely, it requires LLMs to properly utilize the rich factual knowledge from the pretraining corpus or retrieve external data when necessary. In particular, question answering (QA) and knowledge completion have been two commonly used tasks for evaluating this ability. According to the test tasks (question answering or knowledge completion) and evaluation settings (with or without external resources), we categorize existing knowledge utilization tasks into three types, namely closed-book QA, open-book QA, and knowledge completion.
> 知识利用：对于基于提供的事实证据，完成知识密集任务（常识问答、事实补全）来说很重要
> 它需要 LLM 利用预训练语料库或检索的数据中的事实性知识
> 两个常用的评估任务：QA、知识补全
> 现存的知识利用任务分为三类：闭卷 QA、开卷 QA、知识补全

**Closed-Book QA.** Closed-book QA tasks [652] test the acquired factual knowledge of LLMs from the pre-training corpus, where LLMs should answer the question only based on the given context without using external resources. For evaluating this ability, there are several datasets that can be leveraged, including Natural Questions [554], Web Questions [557], and TriviaQA [558], where the accuracy metric is widely adopted. Empirical results have revealed that LLMs can perform well in this setting and even match the performance of state-of-the-art open-domain QA systems [56]. Also, the performance of LLMs on closed-book QA tasks shows a scaling law pattern in terms of both model size and data size: scaling the parameters and training tokens can increase the capacity of LLMs and help them learn (or memorize) more knowledge from the pre-training data [56]. Further, under a similar parameter scale, LLMs with more pre-training data relevant to the evaluated tasks would achieve better performance [81]. Also, the closed-book QA setting provides a testbed for probing the accuracy of the factual knowledge encoded by LLMs. However, as shown in existing work [55], LLMs might perform less well on QA tasks relying on fine-grained knowledge, even when it exists in the pre-training data.
> 闭卷 QA
> 不使用外部资源
> 数据集：Natural Questions, Web Questions, TriviaQA
> metric: accuracy
> LLM 在该任务上可以匹配 sota，并且在该任务上展示了相对于模型大小和数据大小的 scaling law，即增大规模，提升能力
> 该任务提供了探究 LLM 编码了多少事实性知识的一种方式，现存工作发现 LLM 在依赖于细粒度知识上的任务表现不好，即便该知识存在于预训练数据中

**Open-Book QA.** Unlike closed-book QA, in open-book QA tasks, LLMs can extract useful evidence from the external knowledge base or document collections, and then answer the question based on the extracted evidence [653–656]. Typical open-book QA datasets (e.g., Natural Questions [554], OpenBookQA [566], and SQuAD [569]) have overlap with closed-book QA datasets, but they incorporate external data sources, e.g., Wikipedia. The metrics of accuracy and F1 score are widely used in open-book QA tasks for evaluation. To select relevant knowledge from external resources, LLMs are often paired with a text retriever (or even a search engine), which is trained independently or jointly with LLMs [81, 653, 657]. Also, previous work [658–660] has indicated that retrievers can assist LLMs in verifying and rectifying the reasoning path. In evaluation, existing studies mainly focus on testing how LLMs utilize the extracted knowledge to answer the question and show that the retrieved evidence can largely improve the accuracy of the generated answers, even enabling a smaller LLM to outperform 10× larger ones [653, 657]. Further, open-book QA tasks can be also employed to evaluate the recency of knowledge information. Pre-training or retrieving from outdated knowledge resources may cause LLMs to generate incorrect answers for time-sensitive questions [653].
> 开卷 QA
> 可以从外部文档提取知识
> 数据集：Natural Questions、OpenBookQA、SQuAD，它们包含了外部数据源例如 Wikipedia
> metric: accuracy, F1 score
> LLM 一般会配备一个检索器（甚至搜索引擎），检索器可以和 LLM 分离训练，也可以一起训练
> 研究表明检索器可以帮助 LLM 验证和修正推理路径，并且利用检索的证据进行问答会大幅提高答案的准确率，甚至小模型的表现可以超过10倍大的模型
> 开卷 QA 也可以用于检验知识信息的新近度，对于时间敏感的问题，从过时的数据上预训练或检索就会导致答案错误

**Knowledge Completion.** In knowledge completion tasks, LLMs might be (to some extent) considered as a knowledge base [576], which can be leveraged to complete or predict the missing parts of knowledge units (e.g., knowledge triples). Such tasks can probe and evaluate how much and what kind of knowledge LLMs have learned from the pre-training data. Existing knowledge completion tasks can be roughly divided into knowledge graph completion tasks (e.g., FB15k- 237 [572] and WN18RR [574]) and fact completion tasks (e.g., WikiFact [571]), which aim to complete the triples from a knowledge graph and incomplete sentences about specific facts, respectively. Empirical studies have revealed that it is difficult for existing LLMs to accomplish knowledge completion tasks related to specific relation types [520]. As shown in the evaluation results on WikiFact, LLMs perform well on several frequent relations that occur in the pre-training data (e.g., currency and author), while not well on rare ones (e.g., discoverer_or_inventor and place_of_birth). Interestingly, under the same evaluation settings (e.g., in-context learning), InstructGPT (i.e., text-davinci-002) outperforms GPT-3 in all subsets of WikiFact.
> 知识补全
> LLM 需要补全知识单元 (knowledge triples) 的缺失部分
> 该任务探究 LLM 从预训练数据学了多少知识
> 任务包括：图补全任务（从知识图中补全三元组），事实补全任务（补全关于特定事实的句子）
> LLM 对于与训练数据中经常出现的关系（新进性和作者）表现好，而少出现的 （发现者、发明者、生日）表现不好

**Major Issues.** Although LLMs have achieved key progress in capturing and utilizing knowledge information, they suffer from two major issues as discussed below.

![[A Survey of LLMs-Fig17.png]]

- Hallucination. In generating factual texts, a challenging issue is hallucination generations [638, 661], where the generated information is either in conflict with the existing source (intrinsic hallucination) or cannot be verified by the available source (extrinsic hallucination), which are illustrated by two examples in Figure 17. Hallucination widely occurs in existing LLMs, even the most superior LLMs such as GPT-4 [46]. Furthermore, existing work shows that LLMs encounter difficulties in recognizing the hallucinated content in text [602], even the powerful ChatGPT. Additionally, beyond language tasks, a recent study has shown that large vision-language models (LVLM) also face challenges with hallucination, i.e., generating objects that are not present in the accompanying images [662]. In essence, LLMs seem to “unconsciously” utilize the knowledge in task solving, which still lack an ability to accurately control the use of internal or external knowledge. Hallucinations would mislead LLMs to generate undesired outputs and mostly degrade the performance, leading to potential risks when deploying LLMs in real-world applications. To alleviate this problem, alignment tuning strategies (as discussed in Section 5.2) have been widely utilized in existing work [66], which rely on tuning LLMs on high-quality data or using human feedback. Moreover, the integration of external tools for the provision of credible information sources can help alleviate the hallucination issue [81, 602, 659]. Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations [663, 664]. For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheckGPT [664] detects hallucination by measuring information inconsistency within sampled outputs. For the evaluation of the hallucination problem, a set of hallucination detection tasks have been proposed, e.g., TruthfulQA [556] for detecting human falsehood mimicked by models. More recently, HaluEval [602] creates a large-scale LLM-generated and human-annotated hallucinated samples to evaluate the ability of language models to recognize hallucination in both task-specific and general scenarios.
> 幻觉
> 和现存知识冲突（内在幻觉）；不能被验证（外在幻觉）
> LLM 难以识别幻觉内容
> 大语言视觉模型也有幻觉：生成对应图片中没有的对象
> LLM 似乎在解决任务时“无意识”地利用知识
> 缓解：对齐微调策略，即使用人类反馈，在高质量数据上调节 LLM；外部工具提供可信的信息
> 有工作利用 LLM 地不确定性估计来识别幻觉，因为幻觉事实在不同的采样出的输出中不具有一致性
> TruthfulQA、HauluEval：幻觉检测任务

> [!Hallucination]
>LLMs are prone to generate untruthful information that either conflicts with the existing source or cannot be verified by the available source. Even the most powerful LLMs such as ChatGPT face great challenges in migrating the hallucinations of the generated texts. This issue can be partially alleviated by special approaches such as alignment tuning and tool utilization.

- *Knowledge recency.* As another major challenge, LLMs would encounter difficulties when solving tasks that require the latest knowledge beyond the training data. To tackle this issue, a straightforward approach is to regularly update LLMs with new data. However, it is very costly to fine-tune LLMs, and also likely to cause the catastrophic forgetting issue when incrementally training LLMs. Therefore, it is necessary to develop efficient and effective approaches that can integrate new knowledge into existing LLMs, making them up-to-date. Existing studies have explored how to utilize the external knowledge source (e.g., search engine) to complement LLMs, which can be either jointly optimized with LLMs [653] or used as a plug-and-play module [659]. For instance, ChatGPT utilizes a retrieval plugin to access up-to-date information sources [665]. By incorporating the extracted relevant information into the context [666–668], LLMs can acquire new factual knowledge and perform better on relevant tasks. However, such an approach seems to be still at a superficial level. In addition, existing studies also explore editing parameters of language models to update intrinsic knowledge [669–671]. Nevertheless, previous work [672] has shown that several parameter editing methods perform not well on LLMs, though they can improve the performance of small language models. Therefore, it is still difficult to directly amend intrinsic knowledge or inject specific knowledge into LLMs, which remains an open research problem [672]. Recently, a useful framework EasyEdit [673] has been released to facilitate the research of knowledge editing for LLMs.
> 知识新近性
> 简单方法：规律性用新数据更新 LLM，但贵，且灾难性遗忘
> 可以利用外部知识源，使用和 LLM 一同优化的搜索引擎搜索，或者使用即插即用的搜索模块
> 有工作探究编辑语言模型的参数以更新内在知识，但对于大模型效果不好，相关的框架是 EasyEdit

> [!Knowledge Recency]
> The parametric knowledge of LLMs is hard to be updated in a timely manner. Augmenting LLMs with external knowledge sources is a practical approach to tackling the issue. However, how to effectively update knowledge within LLMs remains an open research problem.

### 7.1.3 Complex Reasoning
Complex reasoning refers to the ability of understanding and utilizing supporting evidence or logic to derive conclusions or make decisions [51, 52]. According to the type of involved logic and evidence in the reasoning process, we consider dividing existing evaluation tasks into three major categories, namely knowledge reasoning, symbolic reasoning, and mathematical reasoning.
> 复杂分析：理解、利用支持证据或逻辑进行推理和决策
> 现存的评估任务：知识推理、符号推理、数学推理

**Knowledge Reasoning.** The knowledge reasoning tasks rely on logical relations and evidence about factual knowledge to answer the given question. Existing work mainly uses specific datasets to evaluate the reasoning capacity of the corresponding type of knowledge, e.g., CSQA [504]/StrategyQA [185] for commonsense knowledge reasoning and ScienceQA [565] for science knowledge reasoning. In addition to the accuracy of the predicted results, existing work [565] has also evaluated the quality of the generated reasoning process, via automatic metrics (e.g., BLEU) or human evaluation. Typically, these tasks require LLMs to perform step-by-step reasoning based on factual knowledge, until reaching the answer to the given question. To elicit the step-by-step reasoning ability, chain-of-thought (CoT) prompting strategy [33] has been proposed for enhancing the complex reasoning capacity of LLMs. As discussed in Section 6.3, CoT involves the intermediate reasoning steps, which can be manually created [33] or automatically generated [674], into the prompts to guide LLMs to perform multi-step reasoning. Such a way largely improves the reasoning performance of LLMs, leading to new state-of-the-art results on several complex knowledge reasoning tasks [33, 56, 526]. Further, after reformulating knowledge reasoning tasks into code generation tasks, researchers have found that the performance of LLMs can be further improved [211], especially with the LLMs pretrained on code. However, due to the complexity of knowledge reasoning tasks, the performance of current LLMs still lags behind human results on tasks such as commonsense reasoning [33, 56, 675]. As a common type of mistakes, LLMs might generate inaccurate intermediate steps, leading to a wrong final result. To address this issue, existing work has proposed special decoding or ensemble strategies to improve the accuracy of the whole reasoning chain [436, 437].
> 知识推理
> 数据集：CSQA/StrategyQA for 常识知识推理，ScienceQA for 科学知识推理
> metric：答案的准确率、推理过程的质量 （BLEU 或人类评估）
> 使用思维链 prompt 可以提高表现
> 将知识推理任务重构为代码生成任务可以提高表现
> LLM 在常识推理上的表现仍然落后于人类
> 常见的错误：生成错的中间步骤，导致答案错；缓解方法：集成策略或特殊的解码策略

**Symbolic Reasoning.** The symbolic reasoning tasks mainly focus on manipulating the symbols in a formal rule setting to fulfill some specific goal [51], where the operations and rules may have never been seen by LLMs during pretraining. Existing work [33, 439, 505] commonly evaluates LLMs on the task of last letter concatenation and coin flip, where the evaluation examples require the same reasoning steps as the in-context examples (called in-domain test) or more steps (called out-of-domain test). For an example of the out-of-domain test, LLMs could only see the examples with two words in context, but it requires LLMs to concatenate the last letters of three or more words. Typically, the accuracy of the generated symbols is adopted to evaluate the performance of LLMs on these tasks. Thus, LLMs need to understand the semantic relations among the symbolic operations and their composition in complex scenarios. However, under the out-of-domain setting, as LLMs have not seen the complex compositions of symbolic operations and rules (e.g., twice number of operations in context examples), it is hard for LLMs to capture their accurate meanings. To solve this issue, existing studies incorporate scratchpad [591, 676] and tutor [677] strategies to help LLMs better manipulate symbolic operations, for generating longer and more complex reasoning processes. Another line of research work utilizes the formal programming language to represent the symbolic operations and rules, which requires LLMs to generate code and perform the reasoning process by executing it with external interpreters. Such a way can decompose the complex reasoning process into code synthesis and program execution for LLMs and interpreters, respectively, leading to a simplified reasoning process with yet more accurate results [443].
> 符号推理
> 评估 LLM 在给定规则下操作符号达到目标的能力，任务包括：最后字符拼接、硬币翻转
> 在域测试：测试例子的推理步骤和上下文示例相同
> 离域测试：测试例子的推理步骤和上下文示例不同，例如，LLM 在示例中最多见到两个单词，但要求补全三个或者更多单词的最后字符
> 度量：生成符号的准确率

**Mathematical Reasoning.** The mathematical reasoning tasks need to comprehensively utilize mathematical knowledge, logic, and computation for solving problems or generating proof statements. Existing mathematical reasoning tasks can be mainly categorized into math problem solving and automated theorem proving. For math problem solving tasks, SVAMP [592], GSM8k [184] and MATH [364] datasets are commonly used for evaluation, where LLMs need to generate accurate concrete numbers or equations to answer the mathematical problem. As these tasks also require multi-step reasoning, the CoT prompting strategy has been widely adopted for LLMs to improve the reasoning performance [33]. As another practical strategy, continually pre-training LLMs on large-scale mathematical corpora can largely boost their performance on mathematical reasoning tasks [35, 203, 678]. Further, since math problems in different languages share the same mathematical logic, researchers also propose a multilingual math word problem benchmark [524] to evaluate the multilingual mathematical reasoning capacity of LLMs. As another challenging task, automated theorem proving (ATP) [598, 600, 679] requires the reasoning model to strictly follow the reasoning logic and mathematical skills. To evaluate the performance on this task, PISA [599] and miniF2F [600] are two typical ATP datasets with the proof success rate as the evaluation metric. As a typical approach, existing work on ATP utilizes LLMs to aid the search for proofs using an interactive theorem prover (ITP), such as Lean, Metamath, and Isabelle [680– 682]. A major limitation of ATP research is the lack of related corpora in formal language. To tackle it, several studies utilize LLMs to convert informal statements into formal proofs for augmenting new data [683] or generate drafts and proof sketches to reduce the search space of the proofs [684].
> 数学推理
> 数学推理任务可以分类为解决数学问题和自动定理证明
> 解决数学问题相关数据集：SVAMP、GSM8k、MATH
> 常用的提高推理能力的技巧是 CoT，另一个就是在数学语料库中持续地预训练 LLM，这可以大幅提高数学推理任务的表现
> 自动定理证明相关数据集：PISA、miniF2F，其中度量是证明成功率
> 自动定理证明研究的主要限制是缺乏形式语言的相关语料库，一些工作使用 LLM 将非正式的语句转化为形式化的证明以增添新数据

**Major Issues.** In spite of the advancements, LLMs still have several limitations in solving complex reasoning tasks.
> LLM 在解决复杂分析问题中还存在以下限制

- *Reasoning inconsistency.* With improved reasoning strategies (e.g., CoT prompting), LLMs can solve some complex reasoning tasks, by performing step-by-step reasoning based on the supporting logic and evidence. Despite the effectiveness, the reasoning inconsistency issue often occurs in the decomposed reasoning process. Concretely, LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process [33, 442], leading to inconsistency between the derived answer and the reasoning process. To alleviate this problem, existing work has proposed to guide the whole generation process of LLMs via external tools or models [437, 451, 636], to re-check the reasoning process and final answer for correcting the potential errors [685–687] or fine-tune LLMs with process-based feedback [688, 689]. For instance, Tree of Thoughts (ToT) [451] empowers LLMs to engage in the decision-making process by concurrently exploring and self-evaluating various reasoning paths. To refine the reasoning processes, Self-Refine [685] elicits feedback from LLMs on self-generated solutions, enabling the iterative refinement of solutions based on the feedback. Moreover, several studies improve the consistency in the reasoning chain of LLMs through the integration of process based supervision during training [688, 689]. As a promising solution, recent approaches reformulate the complex reasoning tasks into code generation tasks, where the strict execution of the generated code ensures the consistency between the reasoning process and the outcome. Also, it has been revealed that there might exist inconsistency between tasks with similar inputs, where small changes in the task description may cause the model to produce different results [49, 592]. To mitigate this problem, selfconsistency [436] adopts the ensemble of multiple reasoning paths to enhance the decoding process of LLMs.
> 分析不一致性
> 分析不一致性的问题主要出现在分解的分析过程中，也就是说，LLM 会根据一个无效的分析路径生成正确的答案，或者在正确的分析过程之后生成错误的答案，因此分析过程和推导出的答案是不一致的
> 为了缓解该问题，现存的工作提出通过外在的工具或模型来引导 LLM 的整个生成过程，以 re-check 其分析过程和最终答案，以修正潜在的错误；或者通过基于过程的反馈微调 LLM
> 例如，Tree of Throughts 通过让 LLM 并行地探索和自我评估多个分析路径
> Self-Refine 让 LLM 为自己生成的 solution 生成反馈，使得 LLM 可以基于反馈迭代式生成 solution
> 一些研究通过在训练时结合基于过程的监督来提高 LLM 分析的一致性

> [!Reasoning inconsistency]
>LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process, leading to inconsistency between the derived answer and the reasoning process. The issue can be alleviated by fine-tuning LLMs with process-level feedback, using an ensemble of diverse reasoning paths, and refining the reasoning process with self-reflection or external feedback.

- *Numerical computation.* For complex reasoning tasks, LLMs still face difficulties in the involved numerical computation, especially for the symbols that are seldom encountered during pre-training, such as arithmetic with large numbers [49, 677, 690]. To tackle this issue, a direct way is to tune LLMs on synthesized arithmetic problems [361, 691]. Also, a surge of studies improve the numerical computation performance by tracing intermediate calculation steps in training and inference stages [361, 676, 692], e.g., scratchpad tracing. In addition, existing work [80] has also incorporated external tools (e.g., calculator), especially for handling arithmetic operations. More recently, ChatGPT has provided a plugin mechanism to use external tools [665]. In this way, LLMs need to learn how to properly manipulate the tools. For this purpose, researchers have augmented the examples using tools (even the LLM itself) for tuning the LLM [80, 693], or devised instructions and exemplars for in-context learning [443]. In addition to the aid of external tools, recent studies find that tokenizing digits into individual tokens (e.g., LLaMA and Galactica tokenizers) is a useful approach to enhancing the inherent arithmetic ability of LLMs [361, 690]. One possible explanation is that subword tokenization techniques can result in inconsistent sequences when tokenizing numbers. For instance, with a subword tokenizer the integer 7481 may be tokenized as 7 481, while 74815 may be tokenized as 748 15 (the same numerical substrings with different splits) [361]. As a comparison, digit-based tokenization for numbers can avoid such an inconsistency, thus likely improving the numerical computation ability of LLMs.
> 数值计算
> LLM 难以处理带有数值计算的问题，尤其是面对在预训练时较少碰到的符号，例如和大数值的算术
> 为解决这一问题，一种直接的方法是对在合成算术问题上对 LLM 进行微调
> 此外，许多研究通过追踪训练和推理阶段的中间计算步骤来提升数值计算性能，例如草稿纸追踪 scratchpad tracing
> 此外，现有的工作还结合了外部工具（如计算器），特别是用于处理算术运算，最近，ChatGPT提供了一种插件机制来使用外部工具，通过这种方式，LLMs需要学会如何正确操作这些工具，为此，研究人员通过使用工具（甚至LLM本身）增强了示例以微调LLM，或者设计了指导和示例以便于上下文学习
> 除了借助外部工具外，最近的研究发现，将数字分解成单个 token（例如，LLaMA和Galactica的 tokenizer）是增强LLMs固有算术能力的一种有用方法，一种可能的解释是，subword tokenization技术在 tokenize 数字时可能导致不一致的序列，例如，使用子词标记器，整数7481可能被标记为7 481，而74815可能被标记为748 15（相同的数字子串有不同的分割），相比之下，基于数字的标记可以避免这种不一致性，因此可能提高LLMs的数值计算能力

> [!Numerical Computation]
> LLMs face difficulties in numerical computation, especially for the symbols that are seldom encountered during pre-training. In addition to using mathematical tools, tokenizing digits into individual tokens is also an effective design choice for improving the arithmetic ability of LLMs.

## 7.2 Advanced Ability
In addition to the above basic evaluation tasks, LLMs also exhibit some superior abilities that require special considerations for evaluation. In this part, we discuss several representative advanced abilities and the corresponding evaluation approaches, including human alignment, interaction with the external environment, and tool manipulation. Next, we discuss these advanced abilities in detail.
> 以上介绍了基本的评估任务
> 接下来介绍关于几个代表性的高级能力和对应的评估方法，包括了 human alignment、和外部环境交互、工具使用
### 7.2.1 Human Alignment
It is desired that LLMs could well conform to human values and needs, i.e., human alignment, which is a key ability for the broad use of LLMs in real-world applications.
> human alignment: LLM 符合人类价值观

To evaluate this ability, existing studies consider multiple criteria for human alignment, such as helpfulness, honesty, and safety [46, 170, 368]. For helpfulness and honesty, adversarial question answering tasks (e.g., TruthfulQA [556]) can be utilized to examine LLM’s ability in detecting possible falsehood in the text [46, 81]. Furthermore, harmlessness can be also evaluated by several existing benchmarks, e.g., CrowS-Pairs [603] and Winogender [604]. Despite the automatic evaluation with the above datasets, human evaluation is still a more direct way to effectively test the human alignment ability of LLMs. OpenAI invites many experts in domains related to AI risks to evaluate and improve the behaviors of GPT-4 when encountering risky contents [46]. In addition, for other aspects of human alignment (e.g., truthfulness), several studies propose to use specific instructions and devise annotation rules to guide the annotation process [81]. Empirical studies have revealed that these strategies can greatly improve the human alignment ability of LLMs [170]. For instance, after alignment tuning on data collected through interactions with experts, the incorrect behavior rate of GPT-4 can be largely reduced when it deals with sensitive or disallowed prompts. In addition, highquality pre-training data can reduce the effort required for alignment [46]. For instance, Galactica is potentially more harmless due to the less biased contents in the scientific corpus [35].
> 为了评估这种能力，现有的研究考虑了多项人类对齐的标准，如有用性、诚实性和安全性
> 对于有用性和诚实性，可以利用对抗性问答任务（例如 TruthfulQA）来检验 LLMs 在文本中识别可能虚假信息的能力
> 无害性：CrowS-Pairs 和 Winogender
> 尽管使用上述数据集进行了自动评估，但人类评估仍然是有效测试 LLMs 的人类对齐能力的更为直接的方式
> OpenAI 邀请了许多 AI 风险相关领域的专家来评估并改进 GPT-4在遇到危险内容时的行为
> 此外，对于人类对齐的其他方面（例如真实性），一些研究提出使用特定指令和制定标注规则来引导标注过程，经验研究表明，这些策略可以大大提高 LLMs 的人类对齐能力，例如，在通过与专家互动收集的数据上进行对齐微调后，GPT-4在处理敏感或禁止的提示时，其不正确行为的发生率可以大大降低
> 另外，高质量的预训练数据可以减少对齐所需的精力，例如，由于科学语料库中的内容偏见较少，Galactica 潜在上更加无害
### 7.2.2 Interaction with External Environment
In addition to standard evaluation tasks, LLMs have the ability to receive feedback from the external environment and perform actions according to the behavior instruction, e.g., generating action plans in natural language to manipulate agents [694, 695]. Such an ability is also emergent in LLMs that can generate detailed and highly realistic action plans, while smaller models (e.g., GPT-2) tend to generate shorter or meaningless plans [694].
> LLM 可以用于生成自然语言形式的 action plan 以操纵智能体
> 生成详细且高度实际的 action plan 也是 LLM 的涌现能力之一，小模型更倾向于生成更短的或无意义的 plan

To test this ability, several embodied AI environments and benchmarks can be used for evaluation, described as follows. VirtualHome [606] builds a 3D simulator for household tasks such as cleaning and cooking, in which the agent can execute natural language actions generated by LLMs. ALFRED [608] includes more challenging tasks that require LLMs to accomplish compositional targets. BEHAVIOR [607] focuses on everyday chores in simulation environments and requires LLMs to generate complex solutions, e.g., changing the internal status of objects. Apart from restricted environments such as household tasks, a line of research work investigates the proficiency of LLMbased agents to explore open-world environments, such as Minecraft and the Internet [696, 697]. Voyager [697] introduces an automatic curriculum module that enables LLMs to continuously acquire new skills based on feedback from the environment. GITM [696] focuses on solving various challenges in Minecraft based on LLM, through task decomposition, planning, and invocation of interfaces. Based on the generated action plans or task completions, existing work either adopts the regular metrics (e.g., executability and correctness of the generated action plans) [694] in the benchmark or directly conducts real-world experiments and measures the success rate [698], to evaluate such ability. It has been shown that LLMs are capable in interacting with the external environment and generating accurate action plans [699]. Recently, several improvement methods have been proposed to enhance the interaction ability of LLMs, e.g., designing code-like prompts [530] and providing realworld grounding [698].
> 该能力可以使用 embodied AI 环境和基准进行评估，包括：VirtualHome、BEHAVIOR、Voyager、GITM
> 对于模型生成的 action plan 和 task completion，可以采用常规的度量（action plan 的可执行性和正确性）或者直接在现实世界实验中衡量成功率
> LLM 是可以与外界环境交互并且生成正确 action plan 的

In addition, recent work also explores multi-agent collaboration based on LLMs in simulated environments [533, 700, 701]. These studies simulate human social behaviors by instantiating multiple LLM-based agents with observations, planning, and memories in a sandbox environment. In controlled evaluation, the abilities of generative agents to search, plan, and think are evaluated by humans in an interview-like manner. Further, they also conduct descriptive measurements on multiple agents within a simulated environment to examine emergent social behaviors.
> 在虚拟环境中的基于 LLM 的多 agent 合作: 在沙箱环境中，通过 observation、planning、memories 实例化基于 LLM 的 agent，模拟人类社交活动
### 7.2.3 Tool Manipulation
When solving complex problems, LLMs can turn to external tools if they determine it is necessary. By encapsulating available tools with API calls, existing work has involved a variety of external tools, e.g., search engine [81], calculator [80], and compiler [443], to enhance the performance of LLMs on several specific tasks. Recently, OpenAI has supported the use of plugins in ChatGPT [665], which can equip LLMs with broader capacities beyond language modeling. For example, the web browser plugin enables ChatGPT to access fresh information. Further, incorporating thirdparty plugins is particularly key for creating a prosperous ecosystem of applications based on LLMs.
> LLM 调用工具：将工具封装为 API 调用
> 工具包括：搜索引擎、计算器、编译器

To examine the ability of tool manipulation, existing work mostly adopts complex reasoning tasks for evaluation, such as mathematical problem solving (e.g., GSM8k [184] and SVAMP [592]) or knowledge question answering (e.g., TruthfulQA [556]), where the successful utilization of tools is very important for enhancing the required skills that LLMs are incapable in (e.g., numerical calculation). In this way, the evaluated performance on these tasks can reflect the ability of LLMs in tool manipulation. To teach LLMs to utilize tools, existing studies add exemplars using tools in context to elicit LLMs [443], or fine-tune LLMs on simulated data about tool utilization [80, 693]. It has been found that with the help of tools, LLMs become more capable of handling the issues that they are not good at, e.g., equation calculation and answering timely questions [80, 448]. However, as the number of available tools increases, the limited context length of LLMs may pose challenges in describing and demonstrating extensive tool APIs. To address this issue, existing work retrieves the usage of relevant tools, or encoding tool information as tokens within the embedding space [702–704] In addition to existing tools developed by humans, LLMs possess the capability to make their own tools for specific tasks autonomously [705]. This enables the models to independently explore and manipulate these self-created tools, thereby expanding their potential for autonomous exploration in solving a wide range of real-world tasks.
> 评估工具利用能力：采用复杂的任务来评估，例如数学问题求解（GSM8k、SVAMP）或知识问答（TruthfulQA），要回答对其中的问题，需要正确应用工具
> 教 LLM 使用工具：在上下文中使用工具、使用关于工具利用的数据微调 LLM、检索工具的相关用法、在嵌入空间编码将工具信息编码为 token
> LLM 可以为特定任务自己制定工具

*Summary.* The above three abilities are of great value to the practical performance of LLMs: conforming to human values and preferences (human alignment), acting properly in real-world scenarios (interaction with the external environment), and expanding the ability scope (tool manipulation). In addition to the above three advanced abilities, LLMs might also show other abilities that are specially related to some tasks (e.g., data annotation [486]) or learning mechanisms (e.g., self-improvement [706]). It will be an open direction to discover, measure and evaluate these newly emerging abilities, so as to better utilize and improve LLMs.
## 7.3 Benchmarks and Evaluation Approaches
In the above, we have discussed the basic and advanced abilities of LLMs. Next, we will introduce existing evaluation benchmarks and approaches [733, 734].
### 7.3.1 Comprehensive Evaluation Benchmarks
Recently, several comprehensive benchmarks [70, 364, 520] have been released for the evaluation of LLMs. In this part, we introduce several widely used benchmarks, i.e., MMLU, BIG-bench, HELM, and a series of human exam benchmarks.

- *MMLU* [364] is a versatile benchmark for large-scale evaluation of multi-task knowledge understanding, covering a wide range of knowledge domains from mathematics and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced. As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark [35, 56, 57, 69], which shows the scaling law in model size. More recently, GPT-4 achieves a remarkable record (86.4% in 5- shot setting) in MMLU, which is significantly better than the previous state-of-the-art models [46].
> MMLU:
> 用于评估大规模的多任务知识理解，它包括许多知识领域，从数学和计算机科学到人类学和社会科学
> 任务的难度从基础到高级
> 该 Benchmark 下，LLM 比小模型好很多

- *BIG-bench* [70] is a collaborative benchmark intended to probe existing LLMs from various aspects. It comprises 204 tasks that encompass a broad range of topics, including linguistics, childhood development, mathematics, commonsense reasoning, biology, physics, social bias, software development, and so on. By scaling the model size, LLMs can even outperform the average human performance under the few-shot setting on 65% of tasks in BIG-bench [56]. Considering the high evaluation cost of the entire benchmark, a lightweight benchmark BIG-bench-Lite has been proposed, which contains 24 small yet diverse and challenging tasks from BIG-bench. Additionally, the BIG-bench hard (BBH) benchmark [365] has been proposed to concentrate on investigating the currently unsolvable tasks of LLMs by selecting the challenging tasks in which LLMs exhibit inferior performance compared to humans. Since BBH becomes more difficult, small models mostly achieve performance close to random. As a comparison, CoT prompting can elicit the abilities of LLMs to perform step-by-step reasoning for enhancing the performance, even exceeding the average human performance in BBH.
> Big-bench:
> 包括204个任务，包含的主题有语言学、儿童培养、数学、常识推理、生物学、物理、社会偏见、软件开发
> 其中65%的任务中，LLM 表现比平均人类表现好
> Big-bench-lite 的评估开销更低，它包含24个 Big-bench 中的小且多样的任务
> Big-bench-hard 聚焦于当前 LLM 不易解决的问题（表现比人类差的任务）

- *HELM* [520] is a comprehensive benchmark that currently implements a core set of 16 scenarios and 7 categories of metrics. It is built on top of many prior studies, conducting a holistic evaluation of language models. As shown in the experimental results of HELM, instruction tuning can consistently boost the performance of LLMs in terms of accuracy, robustness, and fairness. Further, for reasoning tasks, the LLMs that have been pre-trained on the code corpus show superior performance.
> HEML:
> 综合性的度量，基于许多先验研究构建
> HELM 上的结果表示指令微调可以持续强化 LLM 的能力（accuracy, robustness, fairness）

- *Human-level test benchmarks* aim to evaluate the comprehensive ability of LLMs with questions designed for testing humans, such as AGIEval [708], MMCU [709], M3KE [710], C-Eval [711] and Xiezhi [712]. These benchmarks encompass a wide range of domains, difficulty levels, and languages to provide a comprehensive evaluation of LLMs’ general capabilities. Compared to publicly available models, models offering API services (e.g., GPT-4, ChatGPT, Claude) demonstrate superior performance compared to publicly available models on these evaluation benchmarks. As the bestperforming model in evaluations, GPT-4 surpasses average human performance in AGIEval [708]. However, it still lags behind the top human performance on these challenging benchmarks. Hence, there remains ample room for further enhancements in the overall abilities of LLMs, particularly for publicly accessible models.
> Human-level test benchmark
> 包括 AGIEval、MMCU、M3KE、C-Eval、Xiezhi
> 仅提供 API 的模型在这些任务上标下比开源模型好

The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs. Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA [735] for multilingual knowledge utilization and MGSM [524] for multilingual mathematical reasoning. To conduct the evaluation, one can select suitable benchmarks according to specific goals. In addition, there are also several open-source evaluation frameworks for researchers to evaluate LLMs on existing benchmarks or extend new tasks for customized evaluations, such as Language Model Evaluation Harness [736] and OpenAI Evals [46]. Further, some researchers also construct continuously updated leaderboards by aggregating representative benchmarks, to compare the performance of existing LLMs, such as Open LLM Leaderboard [707]. The above benchmarks and leaderboards provide important references to demonstrate the basic and advanced abilities of LLMs. We will give more deep discussions on pros and cons on evaluation approaches in Section 7.3.2.
> TyDiEQ: 评估多语言知识利用
> MGSM: 多语言数学推理
> 开源的评估框架：Language Model Evaluation Harness, OpanAI Evals
> 持续更新的排行榜：Open LLM Leaderboard
### 7.3.2 Evaluation Approaches
After introducing existing benchmarks, in this part, we will review existing evaluation approaches for assessing the performance of LLMs. To organize our discussion, we categorize LLMs into three different types: base LLMs (pretrained model checkpoints), fine-tuned LLMs (instruction or alignment fine-tuned model checkpoints), and specialized LLMs (adapted model checkpoints for some specific task or domain). Here, we keep both fine-tuned LLMs and specialized LLMs, to distinguish the different purposes of LLMs: general or specific task solvers. To evaluate the three types of LLMs, we can test the LLM’s performance related to different abilities (e.g., basic or advanced abilities as discussed in Section 7.1 and 7.2). In general, there are three main approaches to evaluating LLMs, namely benchmarkbased approach [364], human-based approach [727], and model-based approach [729]. Table 15 shows an illustration of the relationship among LLM type, evaluation approach, and tested abilities. Next, we will discuss the evaluation approaches for different types of LLMs.
> 我们将 LLM 分为三个不同类型：
> base LLM (pretrained model checkpoints)
> fine-tuned LLM (instruction or alighment fine-tuned model checkpoints)
> specialized LLM (adapted model checkpoints for some specific task or domain)
> 评估 LLM 的三个主要方式：
> benchmark-based
> human-based
> model-based

**Evaluation of Base LLMs.** Base LLMs refer to the model checkpoints obtained right after pre-training. For base LLMs, we mainly focus on examining the basic abilities (Section 7.1), such as complex reasoning and knowledge utilization. Since most of these basic abilities can be assessed with well-defined tasks, benchmark-based approaches have been widely used to evaluate base LLMs. Next, we will introduce common evaluation benchmarks and evaluation procedures for base LLMs.
> base LLM: 主要评估其基础能力，例如复杂推理和知识利用，一般用 benchmark-based 方法

- *Common benchmarks.* To evaluate base LLMs, typical benchmarks are designed in the form of close-ended problems like multiple-choice questions. These commonly used benchmarks can be mainly divided into two categories: knowledge-oriented and reasoning-oriented benchmarks. Knowledge-oriented benchmarks (e.g., MMLU [364] and CEval [711]) aim to evaluate the capacity of world knowledge, while reasoning-oriented benchmarks (e.g., GSM8K [643], BBH [365], and MATH [364]) focus on evaluating the capability of solving complex reasoning tasks. Further, some recently proposed benchmarks (e.g., OpenCompass [713]) combine these two types for a comprehensive comparison.
> Common benchmark: 一般形式是多选
> 分为两类：知识导向、推理导向
> 知识导向：MMLU、CEval
> 推理导向：GSM8K、BBH、MATH
> OpenCompass 结合了这两个类型

- *Benchmark based evaluation procedure.* To perform the benchmark evaluation, each problem will first be formatted into a prompt for LLMs to generate the result text. Then, the generated result text will be parsed with human-written rules to get the predicted answer. Finally, the performance of LLMs can be automatically calculated using standard metrics like accuracy by comparing the predicted answer with the ground-truth one. The evaluation approach can be conducted in either the few-shot or zero-shot setting, which might lead to different evaluation results or rankings. Since base LLMs have not been instruction fine-tuned (with relatively weak task generalization ability), the few-shot setting is often more suitable for evaluation. For some complex reasoning tasks, CoT prompts also need to be used to fully exhibit the capacity during evaluation. Another note is that this evaluation approach can also be applied to assess the abilities of fine-tuned LLMs. Actually, several leaderboards (e.g., Open LLM Leaderboard [707]) are built upon this approach, evaluating both base and fine-tuned LLMs.
> 评估流程：
> 一般采用封闭式问题，将问题格式化为 prompt，LLM 生成结果，结果根据 human-written rule 被解析，得到答案
> 因为 base LLM 没有被指令微调过，few-shot 设定一般比较适合评估
> 对于 fine-tuned LLM 的评估也是用这一方法

**Evaluation of Fine-tuned LLMs.** Fine-tuned LLMs in this part refer to the model checkpoints obtained after instruction tuning or alignment tuning based on pre-trained model weights. Typically, fine-tuned LLMs will be tested on various abilities (e.g., knowledge utilization and human alignment), and thus it is common that they are assessed with multiple evaluation approaches. In addition to benchmark-based evaluation, human-based and modelbased approaches have also been widely used to evaluate the advanced abilities of fine-tuned LLMs. Next, we will introduce the two evaluation methods.
> 对于 fine-tuned LLM，还会有 human-based, model-based 的方法来评估模型能力

- *Human-based evaluation.* Unlike automatic evaluation for basic abilities, human evaluation typically considers more factors or abilities in real-world use, such as human alignment and tool manipulation. In this evaluation approach, test tasks are usually in the form of openended questions, and human evaluators are invited to make judgments on the quality of answers generated by LLMs. Typically, there are two main types of scoring methods for human evaluators: pairwise comparison and single answer grading. In pairwise comparison, given the same question, humans are assigned two answers from different models to determine which one is better, while in singleanswer grading, they only need to score a single answer at a time. For example, HELM [520] employs humans to perform single-answer grading on summarization and disinformation tasks, while Chatbot Arena [727] constructs a crowdsourcing platform that allows users to engage in conversations with two anonymous chat LLMs and report pairwise comparison results.
> human-based
> 此时会考虑更多现实世界使用时的能力了，例如 human alignment 和 tool manipulation
> 评估中一般采用开放式问题，human evaluator 需要对 LLM 的答案评估质量
> human evaluator 用两个打分方法：pairwise comparision, single answer grading
> HELM 采用 single-answer grading
> Chatbot Arena 让用户进行 pairwise comparision

- *Model-based evaluation.* Since human-based evaluation is both expensive and time-consuming, some work has proposed leveraging powerful closed-source LLMs such as ChatGPT and GPT-4 as a surrogate for human evaluators [727, 729]. For example, AlpacaEval [729] collects a set of instructions and utilizes a capable LLM (e.g., GPT-4) as the judge to perform pair-wise comparisons against the reference outputs. Furthermore, MT-bench [727] collects a set of multi-turn questions for evaluation and improves the reliability of LLM-based evaluators through methods like ICL and CoT. Compared with human evaluators, LLMs such as ChatGPT and GPT-4 can achieve high agreement with humans, in both small-scale handcrafted and large-scale crowdsourced evaluation tasks. Despite this, these closedsource LLMs are limited in access and have the potential risk of data leakage. To address this, recent work [727] has explored fine-tuning open-source LLMs (e.g., Vicuna [138]) as model evaluators using scoring data from human evaluators, which has narrowed the gap with powerful closedsource LLMs (e.g., GPT-4).
> model-based
> 用闭源的 LLM 替代 human evaluator
> AlpacaEval 收集一系列指令，利用 GPT-4评估 pairwise comparision against 参考答案
> MT-bench 收集一系列多轮问答
> GPT 可以和人类评估者有较高的一致性，但这些闭源模型可能存在数据泄露问题

**Evaluation of Specialized LLMs.** Specialized LLMs refer to the model checkpoints specially adapted to some domains or applications like healthcare [356] and finance [737]. As special task solvers, specialized LLMs will be tested not only on general abilities (e.g., basic ability like complex reasoning and advanced ability like human alignment), but also on specific abilities related to their designated domains or applications. For this purpose, one often needs to construct specific benchmarks tailored for the target domains or applications. Then, these domain-specific benchmarks can be combined with general benchmarks to conduct both comprehensive and targeted evaluation for specialized LLMs. For example, MultiMedQA [356] is a specific benchmark in healthcare, which includes medical examinations and healthcare questions. In this work [356], MultiMedQA has been combined with MMLU [364] to assess the performance of specialized LLMs for healthcare, such as Med-PaLM [356]. Similarly, FLUE [737] constructs a benchmark for finance, spanning from financial sentiment analysis to question answering. It has been used collaboratively with BBH [365] to evaluate finical LLMs like BloombergGPT [360].
> specialized LLM 还会在特定的领域或应用上被测试，因此需要构建特定的 domain-specific benchmark
> 医疗领域：MultiMedQA
> 经济领域：FLUE

**Pros and Cons of Different Evaluation Approaches.** In the above, we have discussed different evaluation approaches to assess the abilities of LLMs. Next, we simply analyze the pros and cons of each evaluation approach.

- *Benchmark-based approach.* This evaluation approach can leverage existing benchmarks for assessing the performance of LLMs. The tasks involved in these benchmarks often contain sufficient test samples to measure the core abilities (e.g., reasoning). The whole evaluation procedure can be (almost) automatic, and it is convenient to carry out test experiments for various base LLMs, especially useful for monitoring the performance of model checkpoints during pre-training. However, LLMs are often sensitive to the evaluation settings, including the question prompts, zero-shot or few-shot tests, and the answer parsing methods. Thus, one should take possible influencing factors into consideration when conducting the evaluation experiments. The evaluation results should be noted with the adopted evaluation settings. Another issue is the data contamination [56, 738], i.e., the test data itself or relevant content has been contained in the pre-training corpora. This phenomenon has become increasingly severe since more and more open data has been collected for developing LLMs.
> benchmark-based
> 优势：测试样例充足，可以评估模型核心能力（如推理），可以自动评估
> 劣势：LLM 对 evaluation setting 敏感，这些设定包括了 question prompt、是 zero-shot 还是 few-shot、answer parsing 方法，因此评估结果应该附上相应的评估设定，另一个问题是数据污染，即测试数据出现在了预训练语料库中

- *Human-based approach.* Human evaluation offers several advantages when assessing the capabilities of LLMs to solve real-world tasks. One of the key benefits is its ability to directly reflect the actual abilities of LLMs. Based on feedback and experiences from real users, human evaluation provides a more direct measure of LLMs’ performance in real-world scenarios. Further, it can conduct more flexible and diverse evaluation tasks based on human evaluators. For instance, users can submit various queries and test the abilities of LLMs according to their own task cognition. It allows for a deep understanding of the strengths and weaknesses of LLMs across different types of tasks and contexts. However, human evaluation also has inherent limitations that could potentially affect its accuracy and consistency. Factors such as personalized tastes and varying education levels among evaluators can introduce biases or even inconsistencies in the evaluation process. In some cases, users’ judgments are likely to be subjective, which may not reflect the true capabilities of the LLMs. Moreover, conducting robust and reliable human evaluations often requires a large number of evaluators, which can be very expensive and time-consuming. In addition, human evaluation is often not reproducible, making it infeasible to extend existing evaluation results or track the progress of LLMs.
> human-based
> 优势：直接反应 LLM 能力，并且可以进行更多样和灵活的评估
> 劣势：一致性问题、准确率问题、不可复制性问题

- *Model-based approach.* As a surrogate for human-based approaches, model-based approaches serve to diminish the reliance on human involvement, and enable more efficient and scalable evaluation. In addition, LLMs can provide meaningful explanations for the assigned rating scores, thereby enhancing the interpretability of evaluations. Despite their scalability and explanability, model-based approaches have been found to suffer from several issues, including position, verbosity, and self-enhancement bias [727]. Specially, position bias (i.e., the order to present the responses) refers to the fact that LLMs tend to assign high scores for the answers at specific positions over others, verbosity bias means that LLMs favor verbose answers even if they are short in quality compared with shorter answers, and self-enhancement bias indicates that LLMs often overrate in their own generations. In addition, since LLMs have limited capacities in solving complex reasoning problems, they cannot serve as qualified evaluators for some difficult tasks (e.g., mathematical reasoning). These limitations can be mitigated to some extent by specific prompt engineering and fine-tuning strategies [727].
> model-based
> 优势：为评估结果提供有意义的解释
> 劣势：position, verbosity, self-enhancement 偏置，其中 position bias 指 LLM 趋向于选择特定位置的答案，verbosity bias 指 LLM 趋向于更 verbose 的答案，self-enhancement 指 LLM 会过高评估自己生成的答案；LLM 不能评估过于复杂的任务例如数学推理

To summarize, our categorization (Table 15) of existing work on LLM evaluation is mainly based on two major dimensions, namely evaluation methodology and model type, which are further extended with the test abilities. There are some recent work [733, 734] that also has discussed the categorization or taxonomies of existing work for LLM evaluation.
## 7.4 Empirical Evaluation
The above evaluation benchmarks and approaches are mainly employed to evaluate the overall abilities of LLMs. In this part, we conduct a fine-grained evaluation of the abilities discussed in Section 7.1 and Section 7.2. For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the corresponding performance of LLMs.
### 7.4.1 Experimental Settings
In this part, we introduce the experimental settings for our evaluation.

**Evaluation Models.** To conduct the evaluation, we consider representative LLMs from open-source models to closedsource API-accessing models as follows:

- *Open-source models.* Existing open-source models can be categorized into base models and instruction-tuned models. Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning. In our evaluation, we select four representative base models including LLaMA (7B) [57], LLaMA 2 (7B) [99], Pythia (7B and 12B) [96], and Falcon (7B) [747]46. Instruction-tuned models are those fine-tuned using instructions (i.e., task datasets, daily chat, or synthetic instructions). In our experiments, we select four representative instruction-tuned models including Vicuna (7B and 13B) [138], Alpaca (7B) [137], and ChatGLM (6B) [93]. In addition, we also include LLaMA 2-Chat (7B) [99] for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B).
> 开源模型
> 分类为 base model 和 instruction-tuned model (使用任务数据集、日常对话、合成指令微调的 LLM)
> 代表性 base model: LLaMA-7B, LLaMA2-7B, Pythia-7B, 12B, Falcon-7B
> 代表性 instruction-tuned model: Vicuna-7B, 13B, Alpaca-7B, ChatGLM-6B

- Closed-source models. In addition to the open-source models, there are also closed-source models that can only be accessed via APIs, which have gained much attention from both developers and researchers. Here, we select four representative closed-source models including text-davinci-002/003 (short as Davinci002/003), ChatGPT, Claude, and Claude 2, where the first three models are developed by OpenAI and the other two are developed by Anthropic. 

**Tasks and Datasets.** Next, we set up the evaluation tasks and datasets for the abilities discussed in Section 7.1 and Section 7.2. We mainly evaluate the zero-shot performance of LLMs on these datasets. For more complex tasks that are hard to be solved in the zero-shot manner (e.g., mathematical reasoning and tool manipulation), we mainly report the 3-shot performance, considering the context length limit of open-source models.

- *Language generation.* As discussed before, for language generation, we consider evaluating three kinds of tasks, i.e., language modeling, conditional text generation, and code synthesis. Specially, we select four commonly-used datasets, namely LAMBADA [233] (language modeling), WMT’22 [545] (machine translation), XSum [549] (text summarization), and HumanEval [105] (code synthesis) for evaluation. In WMT’22, we construct a new evaluation set by selecting 1000 examples for each language pair from the original large-scale test set to examine the average performance of LLMs in machine translation. We evaluate the zero-shot performance of LLMs on these datasets, and compute the accuracy of predicting words for LAMBADA, BLEU-4 for WMT’22, ROUGE-L for XSum, and pass@10 for HumanEval.
> 语言生成任务：
> 语言建模、代码合成、条件文本生成
> 数据集包括：LAMBADA, WMT'22, XSum, HumanEval

- *Knowledge utilization.* To evaluate the ability of knowledge utilization, we select four question answering datasets (i.e., TriviaQA [558], Natural Questions [554], Web Questions [557], and ARC [555]), and a fact extraction dataset, WikiFact [571]. We also report the zero-shot performance of LLMs on these datasets, and compute accuracy for ARC and exact match for other datasets.
> 知识利用任务：
> 使用问答数据集、事实提取数据集

- *Complex reasoning.* For complex reasoning, we evaluate the comparison models on OpenbookQA [566], HellaSwag [582], and SocialIQA [581] for knowledge reasoning; Colored Objects [70] and Penguins in the Table [70] for symbolic reasoning; GSM8k [184] and MATH [364] for mathematical reasoning. We compute the accuracy for OpenbookQA, HellaSwag, and SocialIQA; solve rate for Colored Objects and Penguins in the Table; and accuracy for GSM8k and MATH. For knowledge reasoning tasks, we evaluate the zero-shot performance, since they are all QA tasks that can be solved in a zero-shot setting. For complex symbolic reasoning and mathematical reasoning tasks, we leverage 3-shot in-context exemplars to better elicit LLMs to accomplish them. Following existing work [33, 443], we also utilize the chain-of-thought prompting strategy for better solving the mathematical reasoning tasks.
> 复杂分析：
> 包括知识推理、符号推理、数学推理

- *Human alignment.* For human alignment, we select TruthfulQA [556] to measure whether a LLM is truthful in generating answers to questions, CrowS-Pairs [603] and WinoGender [604] to assess the stereotypes in LLMs, RealToxityPrompts [605] to evaluate the extent to which LLMs generate toxic language, and HaluEval [602] to test the ability of LLMs to recognize hallucination. As the test set of Real-Toxicity-Prompts is too large, we randomly sample 10000 examples from it for evaluation. We follow LLaMA [57] to report the zero-shot performance, and compute the accuracy of identifying a claim as true for TruthfulQA, accuracy of recognizing biased sentences (high perplexity) for CrowS-Pairs, coreference resolution accuracy (he/she/they) for WinoGender, toxicity score for RealToxityPrompts, and average accuracy of recognizing hallucinations for HaluEval. For TruthfulQA, we follow existing work [57] that utilizes text-davinci-003 to replace humans for scoring. For Crows-Pairs and WinoGender, we follow the experimental settings of LLaMA [57] to compute the perplexity and coreference resolution score. For RealToxityPrompts, we utilize the Perspective-API47 for toxicity evaluation.
> human alignment

- Interaction with environment. To test this ability, we select ALFWorld [609] and WebShop [610] for evaluation, which simulate real-world scenarios such as household and e-commerce environments. We follow the setting of ReAct [449] that evaluate the 1-shot and 2-shot performance of LLMs on WebShop and ALFWorld respectively, and com pute success rate for ALFWorld and average score/success rate for WebShop. Further, we also follow ReAct [449] to reduce the length of the input prompt and utilize line break as the EOS token.
> 环境交互

- Tool manipulation. For tool manipulation, we consider two kinds of tools including search engine and model interfaces. Therefore, we adopt two tool manipulation benchmarks, i.e., HotpotQA [579] and Gorilla [617]. HotpotQA requires LLMs to use search engine to retrieve documents from the web, and Gorilla to invoke model APIs from three hubs of TorchHub, TensorHub and HuggingFace. We compute exact match for HotpotQA and accuracy for Gorilla. For HotpotQA, we follow ReAct [449] to report the 3-shot performance. For Gorilla, we follow the code released by its paper [617], and evaluate the zero-shot performance.
> 工具利用
> 包括搜索引擎和模型接口
> benchmark：HotpotQA、Gorilla

**Implementation Details.** For each task and dataset, we evaluate the compared LLMs using the same prompts and results parsing method provided by existing work (i.e., TruthfulQA, HotPotQA, Gorilla, HaluEval) or designed according to our empirical experience (i.e., TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT’22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt). Specifically, all the experiments about closed-source models are based on invoking their official APIs, while for opensource models, we utilize their publicly available code and model parameters, and perform the inference on 8 A800- 80G GPUs. For TriviaQA, OpenbookQA, HellaSwag, and SocialIQA, we experiment on the development set since the test set is not publicly released. While for other datasets, we experiment on the test set. To reproduce our experiments, we also publicly release our experimental code and data in https://github.com/RUCAIBox/LLMSurvey/tree/ main/Experiments.
> 评估时，每个 LLM 使用相同的 prompt 和结果解析方法
### 7.4.2 Results Analysis and Findings
We report the experimental results in Table 16, and analyze the results in the following.

**Analysis of Closed-Source Models.** We summarize our analysis and findings of the four closed-source models (i.e., ChatGPT, Claude, Davinci003 and Davinci002) as follows:

- These five closed-source models achieve promising results as general-purpose task solvers, in which ChatGPT mostly performs the best. ChatGPT, Claude, Claude 2, Davinci003 and Davinci002 perform well in most of tasks, including complex tasks (e.g., GSM8k), which have shown great potential to be general-purpose task solvers. Among them, ChatGPT exhibits a more superior model capacity on the evaluation tasks, winning the most across all tasks. In some evaluation tasks, the performance gap between ChatGPT and other closed-source models is very large, especially for complex tasks e.g., 78.47 (ChatGPT) v.s. 49.96 (Davinci002) on GSM8k, and 79.88 (ChatGPT) v.s. 51.22 (Claude) on HumanEval.
> 通用任务中，闭源模型表现好，ChatGPT 最强

- Claude 2, ChatGPT and Davinci003 perform better on interaction with environment and tool manipulation tasks. On the two evaluation tasks, Claude 2, ChatGPT and Davinci003, perform better than other models by a large margin, e.g., 36.40 (Claude 2) v.s. 26.00 (Davinci002) on HotpotQA, 44.53 (ChatGPT) v.s. 7.74 (Claude) on Gorilla-TF, and 72.58 (Davinci003) v.s. 22.04 (Claude) on Gorilla-TH. A possible reason is that these three models have been specially optimized towards these advanced abilities, e.g., supporting the use of external plugins.
> 环境交互和工具使用中，Claude，ChatGPT，Davinci003较好，可能这些模型被针对优化过

- All the comparison models perform not well on very difficult reasoning tasks. On MATH and HotpotQA, all models (including ChatGPT) perform not well. The two tasks are very difficult to solve, requiring accurate understanding of complex mathematical knowledge and performing multihop reasoning across documents, respectively. Further, these models also have a relatively weak performance on machine translation task (WMT). A possible reason is that WMT also contains many evaluation examples in minor languages, which might not be well covered in the pre-training data of these LLMs.
> 所有模型在非常难的分析任务上表现不好

**Analysis of Open-Source Models.** Next, we continue to show our analysis and findings about eight open-source models (i.e., LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM, LLaMA 2, LLaMA, Pythia and Falcon) as follows:

- Instruction-tuned models mostly perform better than the base models. Among all the compared open-source methods, the instruction-tuned models (i.e., LLaMA 2-Chat, Vicuna, Alpaca and ChatGLM) mostly perform better than noninstruction-tuned models (i.e., LLaMA 2, LLaMA, Pythia and Falcon). It indicates that instruction tuning is generally capable of improving the few-shot or zero-shot ability of LLMs in solving various tasks. However, after instruction tuning, Vicuna (7B) and Alpaca (7B) suffer from performance degradations on LAMBADA, a language modeling task. The reason may be that the instruction data mainly focuses on enabling LLMs to follow human instructions, which is not always useful for the general language generation task.
> Instruction-tuned 模型表现比 base 模型更佳，但可能会导致语言生成任务能力下降

- These small-sized open-source models perform not well on mathematical reasoning, interaction with environment, and tool manipulation tasks. On the tasks of mathematical reasoning, interaction with environment and tool manipulation, all these evaluated open-source models perform not well, including instruction-tuned ones. A possible reason is that the instruction data for fine-tuning these models is not specifically designed for these tasks. In addition, these closedsource models may have limited model capacities due to small model sizes.
> 小模型处理不了高级任务

- The top-performing model varies on different human alignment tasks. For different human alignment tasks, we can see that these models achieve inconsistent performance rankings. For example, LLaMA 2-Chat (7B) performs the best among the compared open-source models on TruthfulQA, while Vicuna (13B) performs the best on CrowS-Pairs. A possible reason is that these tasks are designed with specific purposes for evaluating different aspects of human alignment, and these models exhibit varied performance on different tasks, even for the variants of the same model (e.g., Pythia (7B) and Pythia (12B)). More experiments and analysis on human alignment evaluation are needed to reveal more detailed findings.
> 不同 human alignment 任务上 top 模型不同

- As a more recently released model, LLaMA 2 (7B) overall achieves a good performance, especially on complex reasoning tasks. For complex reasoning tasks, LLaMA 2 (7B) mostly performs better than other base models, e.g., 43.95 (LLaMA 2 (7B)) v.s. 29.80 (Falcon (7B)) in C-Objects. For other tasks (e.g., language generation and knowledge utilization), LLaMA 2 (7B) can also achieve comparable performance as the best-performing base models. It has used more data for pre-training (i.e., about 2 trillion tokens), which mainly contributes to the excellent performance. Furthermore, it also conducts a more robust data cleaning process.
> LLaMA2表现很好，LLaMA2使用了更多与训练数据 (2 trillion tokens)，数据清理也更 robust

- Scaling the open-source modes can improve the performance consistently. By comparing the performance of Vicuna (7B) and Vicuna (13B), Pythia (7B) and Pythia (13B), we can see that the models with larger scales mostly perform better than smaller ones on these evaluation tasks, indicating the effectiveness of scaling up the model size. Across different tasks, scaling model is more beneficial for more complex tasks (e.g., symbolic and mathematical reasoning), where the larger models mostly outperform smaller ones in a large margin.
> Scaling open-source 模型可以稳定提高表现

The readers should be note that these findings about open-source language models are limited to the model sizes. We will continually update this part by including the results of larger versions of these models, and also call for the support of computational resources for more experiments.
# 8 Application
In this section, we briefly review the recent progress on the applications of LLMs in two aspects, namely the impact to research community and representative domains. Figure 18 shows a content organization of this section48.

![[A Survey of LLM-Fig18.png]]

## 8.1 LLM for Research Community
As LLMs have revolutionized the way how we develop AI algorithms, it poses significant impact on the research community. In this part, we briefly review the advances that led by LLMs for several representative research directions.
### 8.1.1 LLM for Classic NLP Tasks
As pre-trained language models (e.g., BERT) have originated in the field of NLP, the technical advances of language models has an important impact on the research of NLP. In this part, we discuss the application of LLMs on five kinds of classic NLP tasks, including word-level, sentence-level, sequence tagging, relation extraction, and text generation tasks, which had been the foundation of many existing NLP systems and applications. Note that we do not intend to comprehensively cover all NLP tasks, but instead try to analyze the impact of LLMs for fundamental NLP research through the basic tasks. We also omit the discussion of several tasks (e.g., language modeling) that have been discussed early in this survey.
> 我们讨论 LLM 在5个经典 NLP 任务中的应用，包括 word-level, sentence-level, sequence tagging, relation extraction, text generation

**Word/Sentence-level Tasks.** As long-standing NLP tasks, word-level (e.g., word clustering [748] and sense disambiguation [749]) and sentence-level tasks (sentence matching [750] and sentiment classification [751]) have been widely studied in the literature and applied in real-world platforms. To solve these tasks, the key is to accurately understand the semantic information about the words or sentences. As rich high-quality labeled data about these tasks has been accumulated so far, existing work [23, 39] finds that small language models can achieve very good performance by fine-tuning on it. Recent studies [55, 752] have also tested the performance of LLMs on these tasks, showing that LLMs can also perform well via in-context learning (with very few examples). Whereas, as small models can be specially optimized on these tasks to learn the specific task requirement and domain knowledge, full-data fine-tuned small models can mostly outperform LLMs using in-context learning on several classic tasks [753, 754], e.g., semantic matching and sentiment analysis.
> word-level: word clustering, sense disambiguation 
> sentence-level: sentence matching, sentiment classification
> 这些任务小模型在微调后效果就很好了，full-data fine-tuned 的小模型在经典任务上，例如情感匹配和分析，可以优于使用 ICL 的 LLM

**Sequence Tagging.** The sequence tagging tasks, e.g., named entity recognition (NER) [755] and part-of-speech (POS) tagging [756], are also fundamental tasks. Typically, such tasks require assigning each token in the input sequence a proper semantic category label, e.g., the classic B-I-O (Beginning, Inside and Outside) tagging scheme for NER tasks. In the era of deep learning, early efforts [757, 758] mainly integrate the learned sequence representations (e.g., using CNN, LSTM, and BERT) into the classic conditional random field model (CRF), which performs the tagging task based on structural prediction. Recently, researchers have tested the performance of LLMs in sequence tagging tasks, but observed that LLMs still face challenges in solving them using in-context learning [753], especially for special categories with ambiguous or rare names, e.g., the “MISC” (miscellaneous entity) and “ORG” (organization) classes. A possible reason is that LLMs may misunderstand the meanings of these classes in the human-annotated dataset, making it difficult to accurately understand their semantics according to the instruction and limited examples in the context.
> Sequence Tagging 任务包括 named entity recognition 和 part-of-speech tagging
> 这类任务需要对输入序列的每个 token 赋予一个语义类别标签，例如 Begining, Inside, Outside
> 早期是用 CNN/LSTM/BERT 将序列表示放入条件随机场模型进行 tagging
> LLM 使用上下文学习解决该任务表现一般，尤其是对于一些特殊的类别，一个可能的原因时 LLM 没有理解这些标签的意思

**Information Extraction.** The information extraction task focuses on automatically extracting useful structured information from unstructured text data, such as relation extraction [759] and event extraction [760], which is also a crucial task relating to many NLP applications. Typically, previous studies formulate this task as a text classification task or a sequential labeling task. As information extraction often needs to accurately understand and process complex semantic relations (multiple relations within one sentence), incontext learning with LLMs typically underperform stateof-the-art full-data fine-tuning methods [761, 762]. Whereas, it is shown that enabling collaboration between LLMs and small models can further boost the performance of specific tasks [762, 763]. In addition, a recent study [425] also reveals that LLMs can achieve competitive zero-shot performance for information extraction with a two-stage workflow, making this approach attractive in future applications.
> Information Extraction 即从 unstructured 数据中提取 structured 信息，例如关系/事件提取
> 之前的工作将该任务构建为文本分类或序列标签任务
> 信息提取仅需要理解并处理语义关系即可，LLM 的效果比不上 sota，但 LLM 使用 two-stage workflow 可以有较好的零样本表现

**Text Generation.** Text generation tasks, e.g., machine translation [624] and automatic summarization [548], are longstanding NLP tasks that have been widely studied, and there have been a number of deployed products and systems based on fine-tuned small models [311, 764]. Since the pre-training of LLMs is established on text prediction, they exhibit strong language generation abilities as commercial products [627] and humans [628], with the help of proper prompts [765, 766]. Additionally, LLMs are flexible to effectively handle special requirement in real-world application scenarios, e.g., document-level translation [767], and also enable natural language interaction with users to further improve the generation quality [768]. Despite the above success, recent work also reveals that LLMs are hard to well address the generation tasks about low-resource languages and domains, e.g., Marathi-to-English translation [769], due to their unbalanced training data across different languages.
> Text Generation 例如翻译，自动总结
> LLM 的使用体验最佳，还可以与用户交互，进一步改善生成质量
> LLM 不易于处理 low-resource 的语言和领域，例如 Marathi-to-English 翻译，因为它们的训练数据在语言上是不平衡的

**Summary.** Based on the above discussion, we summarize the suggestions, and future direction about the use of LLMs in classic NLP tasks as follows:

- Suggestions: LLMs and small models have their own merits in different aspects: LLMs are can provide unified solutions to various NLP tasks and achieve competitive performance (especially in the zero/few-shot setting), while small models are economical to develop and can be specially tuned according to target tasks, which can achieve good performance with sufficient high-quality labeled data [753, 754, 770, 771]. In applications, one can make suitable choices based on the actual needs, comprehensively considering flexibility, data availability, training compute, and efficiency.

- Future direction: Despite the excellent general capacities, LLMs still cannot effectively process the NLP tasks in low-resource domains, e.g., minor language translation. To tackle such tasks, it needs to develop effective approaches to injecting necessary task information or domainspecific knowledge into LLMs, either through fine-tuning or prompting. In addition, it is still challenging for LLMs to handle complex semantic relations in classic NLP tasks (e.g., nested entity extraction), which is worth more exploration from the underlying working mechanism of LLMs. It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks [772]. Another promising direction is to conduct human-machine collaborative research (e.g., conversational translation [768]) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses.
> low-resource 领域下 LLM 表现不好，故需要探究向 LLM 高效注入知识和任务信息的方式
> LLM 不擅长处理复杂语义关系，例如嵌套实体提取
### 8.1.2 LLM for Information Retrieval
The goal of information retrieval (IR) systems is to assist users in discovering ideal information resources (typically documents) and mitigating the information overload issue. Typically, contemporary IR systems adopt a retrieve-thenrerank pipeline framework [54]. Within this framework, the retriever initially retrieves relevant information from a large-scale corpus, and the reranker subsequently performs multi-stage ranking procedure to acquire the most relevant information [773]. Since the advent of LLMs has significant impact on the way of information access, we discuss how it advances the development of IR from two main aspects, namely LLMs as IR models and LLM-enhanced IR models.
> 目前的信息检索系统采用的是先检索然后排序的框架，检索模型先从大规模的语料库中检索相关的信息，然后排序模型进行多阶段排序，得到最相关的信息
> LLM 对于信息检索的影响：LLM as IR 模型，LLM-enhanced IR 模型

**LLMs as IR Models.** Existing IR models can be overall categorized into sparse models (relying on term-based lexical similarity) and dense models (relying on embedding based semantic similarity) [740]. Specially, dense models are mainly implemented by fine-tuned PLMs (e.g., BERT). Compared to PLMs, LLMs have more strong model capacities in capturing text semantics, thus having the potential to improve existing dense IR models. However, due to the high overhead of LLMs, the majority of studies concentrate on employing LLMs as rerankers, aiming to refine the ranking of retrieved candidates. To achieve this, recent efforts often formulate special instructions that enable LLMs to perform reranking on a small set of provided candidate documents. Typically, such an approach does not necessitate model training, and achieve promising results compared with well-trained reranking methods [774, 775]. Specially, the LLM-based reranking approach can be implemented in different ways by zero-shot or few-shot instruction, including pointwise (estimating the relevance scores for querydocument pairs) [776], pairwise (determining the relevance order of two documents) [775], or listwise ranking (sorting a subset of candidate documents) [777]. The essence of these methods lies in the special design of instructions for text reranking, such as sliding window strategy for document lists [774, 778], setwise selection prompting [779], fine-grained relevance labels incorporation [780], and pairwise comparison prompting [775]. In addition, recent efforts employ LLMs to generate intermediate texts (e.g., URLs) as retrieval results using few-shot demonstrations [781]. To further enhance the model performance, LLMs can be specially fine-tuned as backbones for reranking [782, 783] or retrieval (including dense retrieval [54] and model-based retrieval [784, 785]), similar to the fine-tuning process for traditional PLM-based IR models [782]. However, fine-tuning LLMs as IR models entails considerable expenses given the huge parameter scale of LLMs.
> LLM 作为 IR 模型
> 现存的 IR 模型可以分为：稀疏模型（依赖基于 term 的词相似度进行检索），稠密模型（依赖基于语义相似度的嵌入进行检索）
> 稠密模型主要通过 fine-tuned PLM（例如 BERT）实现
> 而 LLM 捕获文本语义的能力要强于 PLM，但 LLM 开销高，因此主要的研究都是将 LLM 作为 reranker，以 refine 检索候选的排序
> reranking 并不必须模型训练，只需给出指令就有较好的效果，可以让 LLM 估计一个 pair 的相似度，或者直接排序一个列表
> 也可以微调 LLM 为检索模型或者排序模型，但开销大

**LLM-Enhanced IR Models.** As another major research direction, LLMs can be employed to improve existing IR models (e.g., small models). A common challenge faced by existing IR models is the lack of relevant judgment annotation [786, 787]. To tackle this problem, LLMs can be instructed to annotate positive or negative documents for a given query [788], or to generate corresponding queries based on a set of documents in the corpus by referring to a few demonstrations [789, 790]. In addition to training data augmentation, LLM has the potential to improve existing IR models by refining the search-oriented informativeness of both queries and documents. In IR systems, the input queries may be constrained by a user’s cognitive and cultural competency, making it challenging to accurately express the real intent, and irrelevant content present in documents can also impact the relevance evaluation with the query. As a solution, LLM can be utilized to rewrite the query for enhancing the understanding of the query intent and incorporating additional knowledge into the query through well-designed instructions. The rewritten query can take the form of an improved version of the original query [791], a document in the corpus that related to the query [792], or an expansion of the query that concatenated with a pseudo generated document [793]. In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension [794].
> 使用 LLM 帮助 IR 模型
> IR 模型的一个常见问题是缺乏相关的判断标注，故可以用 LLM 来在给定 query 下为文档标注 positive 或 negative，LLM 也可以基于文档生成 query
> LLM 也可以用于 refine query 和 document

**Remaining Issues.** In this part, we further discuss several important issues to apply LLMs to improve IR systems. First, though LLMs are capable of being as general-purpose task solvers, they are not directly well suited for existing IR systems: they require high overhead for inference [774, 782], have limitations in modeling long texts or document lists [778], and need special adaptation (e.g., instruction tuning) to perform the text ranking task [795]. Therefore, more systematic approaches to adapt LLMs for modern IR systems should be investigated, to leverage their benefits and meanwhile overcome these limitations. Secondly, the advent of LLMs sheds lights on the development of new information seeking ways (e.g., New Bing). It is meaningful to explore how to reshape the architecture and paradigm of IR by integrating the LLMs’ capacities and the merits of existing IR systems [796]. Thirdly, existing work mainly focuses on text retrieval tasks, lacking a comprehensive consideration of multimodal information sources. As will be discussed in Section 8.1.4, multimodal large language models [797] are also widely studied, making it feasible to develop more powerful multimedia retrieval systems.
> LLM 推理开销对于 IR 系统太大，并需要适应（例如指令微调）才可执行文本排序任务

### 8.1.3 LLM for Recommender Systems
Unlike IR systems that analyze user search queries to retrieve relevant documents, recommender systems (RS) aim to capture the underlying user preference and provide appropriate information resources to users [798–801]. Typically, existing studies train a recommendation model (either classic or deep learning model) by fitting it over the user’s logged data (e.g., click data) [745, 802]. However, these models often suffer from a series of technical issues, e.g., cold-start recommendation, domain transfer, and poor explainability. Recently, LLMs have demonstrated the potential to alleviate these issues of recommendation models [357, 803, 804], due to the strong capacities of domain generalization and language generation. In this part, we briefly review the recent progress of LLMs in recommender systems, from the following three aspects, namely LLMs as recommendation models, LLM-enhanced recommendation models, and LLMs as recommendation simulators.
> 现存的推荐系统是在用户数据上训练拟合的，存在的问题是冷启动、领域迁移、低可解释性
> LLM 具有较强的领域泛化和语言生成能力，在三种方面应用于推荐系统：LLM as，LLM-enhanced，LLM as recommendation simulators

**LLMs as Recommendation Models.** With specific methods or mechanisms, LLMs can be adapted to serve as recommendation models. Existing work along this line can be generally divided into two main categories. First, some methods prompt LLMs for completing the recommendation task in a zero-shot paradigm (i.e., without parameter tuning) [805, 806]. A series of prompt engineering methods like recency-focused and in-context learning are introduced to improve recommendation performance as well as alleviate the potential model biases [807, 808]. Second, another category of studies aim to specialize LLMs for personalized recommendation through instruction tuning [357, 809]. Specially, high-quality instruction data is key to adapt LLMs to the recommendation tasks, which can be constructed based on user-item interactions with heuristic templates. To further improve the instruction diversity, InstructRec [357] employs self-instruct technique to simulate large amounts of potential user instructions in various scenarios like product search and personalized recommendations. In addition to representing each item by its text description, there is also growing attention on extending LLM’s vocabulary with semantic identifiers in recommender systems [810, 811], to incorporate collaborative semantics into LLMs.
> LLM 作为推荐模型
> 两类：1. prompt LLM 来完成推荐任务 2. 通过指令微调，让 LLM 进行个性化推荐，指令数据基于 user-item 互动数据来构建
> InstructRec 模仿用户行为，自己生成数据
> 也可以用推荐系统中的语义标识符扩张 LLM 的词袋

**LLM-enhanced Recommendation Models.** In addition to instructing LLMs to directly provide recommendations, researchers also propose leveraging the universal knowledge encoded in LLMs to improve traditional recommender systems. Existing approaches in this line can be divided into three main categories. The first category employs LLMs to infer users’ potential intention from their historical interaction data. Furthermore, traditional recommendation/search models employ the inferred intentions to improve the retrieval of relevant items [812, 813]. Additionally, several studies explore the use of LLMs as feature encoders. They employ LLMs to encode the side information of items and users (e.g., item’s descriptions and user’s reviews), thus deriving more informative representations of users and items. These representations are then fed into traditional recommender systems as augmented input [814, 815]. As another alternative approach, several studies [816, 817] adopt a distillation-like way to transfer LLM’s capacities (e.g., semantic encoding) to improve traditional recommenders (i.e., small models). Specially, they align the hidden states of LLMs and traditional recommendation models via joint training. After training, since only the enhanced small model will be deployed online, it can avoid the huge overhead of LLMs in online service.
> LLM-enhanced
> 三类：1. 使用 LLM 从用户的历史交互数据推导用户的潜在意图，然后推荐/搜索模型使用该意图来提高自己对相关物品的检索 2. 使用 LLM 作为特征编码器，编码 item 描述和用户评论，得到 user 和 item 更 informative 的表示，这些表示作为增强的输入 fed into 传统推荐系统 3. 类似蒸馏，将 LLM 的 hidden state 通过共同训练与传统推荐系统对齐，提高传统推荐系统小模型的能力

**LLM as Recommendation Simulator.** Inspired by the recent success of autonomous AI agents [818], LLMs have been also utilized to develop recommendation simulators [819, 820] (exemplified by RecAgent [819]), showing great potential to simulate user real behaviors in recommender systems [819, 821, 822]. Specifically, to make personalized simulation, an agent will be equipped with a profiling module that encompasses relevant identity information. Then, a memory module is introduced to store agents’ past interaction experiences. During the process of simulation, agents are further prompted to conduct self-reflection based on their past experiences, to capture their underlying user preference. Most of existing recommendation simulators are conducted in a user-oriented way, without explicitly modeling the items in the interaction process. To address this, AgentCF [821] models both users and items as agents, and further facilitates collaborative reflections to simulate useritem interactions, so as to capturing the two-sided relations between users and items.
> LLM 模拟在推荐系统中模拟真实用户的行为
> RecAgent：agent 会带有 profiling 模块（包含了相关用户信息），以及 memory 模块（存储 agent 的交互经历），模拟时，还会 prompt 让 agnet 思考自己的用户偏好
> AgentCF：将用户和 item 都建模为 agent，模拟用户-item 交互，捕获二者的关系

**Remaining Issues.** Despite these efforts, there are still several challenges to address when applying LLMs in recommender systems. First, existing studies have shown that LLM-based recommendation models in zero/few-shot settings tend to perform worse than traditional ID-based recommenders [806, 807]. This indicates that LLMs might lack an understanding of personalized user behaviors and domain-specific collaborative semantics. Although instruction tuning alleviates this issue to some extent [357, 809], it can’t fully reduce the semantic gap between LLMs and recommender systems, and also suffers from high tuning costs. Furthermore, recommender systems prioritize minimizing inference latency to enhance users’ experience in low-resourced environments (e.g., phones), which poses a challenge to LLMs’ inference speed as well as memory overhead. Therefore, it is important to explore improvement techniques, such as efficient tuning and quantization methods, to deploy LLMs efficiently and effectively in real-world recommender systems. In addition, existing LLMs have limited capacities in long context modeling, make it difficult to process the huge amount of user-item interaction data. Improved context length extension and context information utilization approaches should be developed to improve the modeling capacities of LLMs in long interaction sequences.
> 基于 LLM 的模型弱于传统的基于 ID 的模型，说明 LLM 和推荐系统之间存在语义差异
> 推荐系统的推理延迟需要在 low-resourced 环境下保持极低，LLM 不适合
> 现存 LLM 弱于处理长上下文建模，因此难以处理大规模用户交互数据
### 8.1.4 Multimodal Large Language Model
In existing literature [823, 824], multimodal models mainly refer to the models that can process and integrate information of various modalities (e.g., text, image, and audio) from input, and further produce corresponding output in certain modalities. In this part, we mainly focus on the multimodal extension of LLMs by enabling the information modeling of non-textual modalities, especially the vision modality, called multimodal large language models (MLLMs) [797]. To start our discussion, we specify the input to be text-image pairs and the output to be text responses. Similar discussions can be made for other modalities, e.g., language-audio models [825], which is beyond our scope here. 

In essence, MLLMs are developed by adapting the information from other modalities to the text modality, so as to leverage the excellent model capacities of LLMs that are learned based on world text. Typically, a MLLM comprises an image encoder for image encoding and a LLM for text generation, associated by a connection module that aligns vision and language representations. During generation, the image is first split into patches, and then transformed into patch embeddings by the image encoder and the connection module, to derive a visual representation that can be understood by the LLM. Subsequently, the patch embeddings and text embeddings are concatenated, and fed into the MLLM, allowing the language model to generate the response autoregressively. In the following, we will discuss the training, evaluation, and key points to develop capable MLLMs.
> 本质上，多模态 LLM 是将其他模态的信息适应至文本模态
> 一般地，MLLM 包括一个图像编码器编码图像、一个 LLM 编码文本，通过连接模块对齐视觉和文本表示
> 生成时，图像被分为 patch，然后转换为 patch embedding，和文本 embedding 拼接，然后 fed into MLLM

**Training Process.** The training process of the MLLM includes two major stages: vision-language alignment pretraining and visual instruction tuning.
> MLLM 的训练包括两个主要阶段：视觉语言对齐预训练、视觉指令微调

- Vision-language alignment pre-training. To develop MLLMs, existing work mostly initializes the vision encoder and the LLM with pre-trained models [149, 150, 826]. These models retain excellent vision and language capacities, but span different semantic spaces. Thus, the goal of visionlanguage alignment pre-training (i.e., the first-stage training) is to align the vision encoder and the LLM through end-toend training on large-scale image-text pairs [827, 828]. However, directly tuning these two models on image-text pairs may cause the degradation of the original representation capacities. To improve the alignment performance, it is crucial to design effective training strategies and select appropriate pre-training data [829, 830]. Existing work mainly employs the following strategies for cross-modality alignment: (1) if the number of image-text pairs is not sufficiently large (e.g., less than 1M), it is often suggested to only update the connection module [831]; (2) if the training data includes high-quality text corpora [832] or image-text pairs with fine-grained annotations [833], fine-tuning the LLM can be conducted to boost the performance; (3) if the number of image-text pairs is very large (e.g., about 1B), fine-tuning the vision encoder is also plausible [829, 830], but the benefit remains further verification.
> 现存的工作都使用已有的模型初始化视觉编码器和 LLM，它们的语义空间是不同的，因此视觉语言对齐预训练的目的是通过端到端的在 image-text pair 上的预训练对齐二者
> 但直接 tuning 会导致原始的表示能力下降，现存的策略有：1. 如果 image-text pair 的数量不多，一般仅更新连接模块 2. 如果文本质量高，可以微调 LLM 3. 如果 image-text pair 数量多，可以考虑微调视觉模型

- Visual instruction tuning. After vision-language pretraining, the second-stage training, i.e., visual instruction tuning, aims to improve the instruction-following and tasksolving abilities of MLLMs. Generally, the input of visual instruction tuning consists of an image and a task description, and the task is to generate a corresponding text output. To boost the performance, high-quality visual instruction data is key to eliciting and enhancing the abilities of MLLMs. Therefore, most studies are dedicated to constructing various visual instruction datasets. As the basic approaches, early studies construct visual instructions by distilling from GPT-4 [149] or reformulating vision-language task datasets [151]. To enhance the quality of instruction data, recent work further proposes improved strategies by increasing the instruction diversity [834], incorporating finegrained information (e.g., coordinate of objects) into the instruction [833], or synthesizing complex visual reasoning instructions [835].
> 视觉指令微调：输入是图片和任务描述，任务一般是生成对应文本输出
> 其关键是高质量的视觉指令数据，因此数据集工作很多，
> 早期工作有从 GPT-4蒸馏，或者重构现存的视觉-语言任务数据集，
> 最近工作提出提高指令多样性，在指令中包含了细粒度的信息

Evaluation of MLLM. After introducing the approaches to developing MLLMs, we further discuss how to effectively assess the multimodal capabilities of MLLMs from the following three aspects.

- Evaluation perspectives. The evaluation tasks for MLLMs can be categorized into two main types: perception and cognition tasks. Specifically, perception tasks aim to assess the model’s abilities in understanding the basic semantics of the image content, while cognition tasks evaluate models with more complex tasks that require reasoning based on perception results. The perception ability is typically evaluated through classification tasks about attributes of image (e.g., topic and style) and object (e.g., existence and color) or OCRrelated tasks, based on existing datasets or new datasets derived from existing images with annotations by humans or LLMs [836–839]. A notable perception issue is hallucination [840], where the model’s responses contain inconsistent content with the image. Among existing studies about hallucination in MLLMs [834, 841, 842], object hallucination [843] has received much research attention. To conduct a stable, robust evaluation of object hallucination, POPE [844] proposes a polling-based object probing approach for converting object recognition into a series of binary questions, and the results indicate that current MLLMs often struggle with object hallucination. Cognition tasks, on the other hand, require MLLMs to perform reasoning based on image perception. A common reasoning task is visual question answering (VQA), where models answer questions about images that demand reasoning about spatial relationships [845], general knowledge [846], or scene text [847]. To fully explore the capabilities of MLLMs, HallusionBench [848] collects 200 sophisticated visual dependent or supplement questions, on which even the most advanced MLLMs like LLaVA-1.5 [831] and GPT-4V [133] fail to achieve good performance.
> MLLM 的评估角度有：perception 和 cogintion
> perception 任务：评估模型理解基本图像语义的能力
> cognition 任务：评估模型处理需要基于 perception 结果进行分析的任务的能力
> perception 能力评估：关于图像属性的分类任务，或 OCR 相关任务，perception 能力也存在幻觉，即模型对于图像的反应不一致
> cognition 能力评估：VQA，问题可以关于空间关系、通用知识、风景文本，相关 benchmark：HallusionBench

- Evaluation paradigms. The responses of MLLMs can be evaluated either in a closed-ended or an open-ended manner. Traditional multimodal tasks often rely on a closedended evaluation framework, where the assessment is based on the exact match between the model’s response and the ground-truth answer. Examples include the VQA score [849] for visual question answering tasks and the CIDEr [850] score for captioning tasks. However, MLLMs generate responses in an open-ended way, which may contain the correct answer but not exactly match the ground-truth perfectly. This discrepancy can lead to the underestimation of the model’s performance in previous evaluation paradigms. To address this issue, recent approaches have incorporated humans or LLMs as evaluators [829]. For instance, MMBench [838] employs ChatGPT to align the model responses with the most relevant option in a set of multiple-choice questions. Similarly, LLaVA [851] utilizes GPT-4 for evaluating MLLMs’ output, where GPT-4 takes the generated image captions and object bounding boxes as visual inputs for assessment. Such open-ended evaluation methods can improve assessment accuracy while incurring higher costs due to the involvement of humans or LLMs.
> 评估范式：
> 主要使用 close-ended 评估框架，即需要模型回答和 ground-truth 完全匹配
> 例如：VQA score、CIDEr (for 图像标注任务)
> 但 MLLM 生成的是 open-ended 的答案，为此，可以用人或 LLM 作为评估者，例如 MMBench 用 ChatGPT 将答案对齐到最相近的选项；LLaVA 直接使用 GPT-4评估

- Evaluation benchmarks. To facilitate a more thorough evaluation of MLLMs, various benchmarks have been developed. Part of them collect existing vision-language tasks for comprehensive evaluation. For instance, LVLM-eHub [852] aggregates 47 existing text-related visual tasks to assess six distinct capabilities of MLLMs, and Reform-Eval [853] takes this a step further by standardizing questions from existing benchmarks into a uniform format and discusses how the backbone models influence MLLMs’ performance. In addition to incorporating existing tasks, several work also derives new questions annotated by humans or with the help of LLMs. MME [839] creates a dataset by pairing images from public sources with manually-collected text instructions for perception and cognition evaluations. MMBench [838] transforms these instructions into multiplechoice questions and introduces CircularEval to ensure evaluation consistency. SEED-Bench [854] further considers temporal understanding tasks and enlarges the evaluation scale to 19K multiple-choice questions with the assistance of LLMs. MM-Vet [855] presents more complex tasks to assess the integrated multimodal capabilities of MLLMs. It starts by defining six essential multimodal abilities and then creates intricate questions by combining multiple abilities. In summary, the above benchmarks collectively contribute to the comprehensive evaluation and improved development of MLLMs.
> 评估 benchmark：
> LVLM-eHub：收集了47个现存的文本相关视觉任务
> Reform-Eval：将问题统一格式
> MMBench：将 MME 的数据集转化为多选题
> SEED-Bench：添加了时序理解任务

**Key Points for Improving MLLMs.** To develop capable MLLMs, we continue to discuss three key points to improve the model capacities, from the perspectives of instruction data, training strategy, and safety and alignment.

- *Visual instruction data.*  Extensive work [831, 856] has empirically found that both quantity and quality of visual instructions have an important impact on model performance of MLLMs. One basic way to construct visual instructions is to leverage the exceptional capability of LLMs to synthesize instructions based on text descriptions of images [851]. To further enhance the quality of instructions, one can construct fine-grained visual instructions with the help of human annotation [833, 857] or synthesize more complex data through carefully-designed prompts [835]. Despite the effectiveness of the above LLM-based approaches, one primary question emerges as to whether a LLM (i.e., text generation model without training on any images) possesses the ability to generate sufficiently good visual instructions solely based on verbalized visual information (e.g., captions and coordinates). Specially, existing work has also revealed that visual instructions generated by LLMs sometimes contain misinterpretations about the visual information, e.g., object hallucination [844]. Therefore, it is crucial to design effective verification methods to control the quality of instruction data generated by LLMs [835]. Furthermore, it still needs more investigation about what makes good visual instructions and how visual instructions elicit specific multimodal abilities in MLLMs.
> 视觉指令数据：
> 一种构造视觉指令的方式就是让 LLM 基于图片的文本描述合成指令，但疑问是仅训练于文本上的 LLM 是否仅基于图片描述生成好的视觉指令
> 现存工作揭示了 LLM 生成的视觉指令会包含误解释，例如物体幻觉

- *Model training.* Different from LLMs, MLLMs are not trained from scratch, but instead developed based on pretrained language and vision models. Existing work employs a typical two-stage approach for training MLLMs, i.e., vision-language alignment pre-training and visual instruction tuning. In essence, existing MLLMs aim to (1) preserve the inherent capabilities and parametric knowledge of LLMs as possible, and meanwhile (2) effectively adapt to multimodal tasks by leveraging the pre-trained LLMs and visual encoders. To achieve the above two goals, two typical training strategies are often employed for visual instruction tuning, either only optimizing the connection module [151] or fine-tuning both the connector module and LLM component [851]. As we can see, the former can reserve the original capacities of LLMs but likely have a weak an adaptation performance, while the latter can fully adapt to multimodal tasks but suffer from the loss of original capacities of LLMs. More efforts should be made to investigate how to effectively balance the two aspects, so as to achieving improved multimodal capacities. In addition, existing MLLMs are still overly dependent on the capacities of LLMs, which pose the limits on many multimodal tasks (e.g., space positioning). It will be meaningful to explore improved training approaches of language models, so that multimodal information can be also utilized in this process.
> 视觉指令微调的训练策略：仅仅优化连接模块、微调连接模块和 LLM 模块
> 前者可以保持 LLM 的原有能力，但适应表现较弱
> 后者可以完全适应到多模态任务，但会损失 LLM 的原有能力
> 现有的 MLLM 仍然过度依赖于 LLM 的能力，因此限制了许多多模态任务（例如空间定位）

- *Safety and alignment.* Safety and alignment has been widely discussed in LLMs, which aim to regulate the behaviors of models by technical approaches [66]. This topic is also important to MLLMs. Even a highly advanced MLLM (e.g., GPT-4V [133]) can be susceptible to safety issues. For example, GPT-4V might occasionally exhibit factual inaccuracies and baseless inferences about images. In some cases, it may even generate harmful content targeting specific individuals or groups [133]. Furthermore, open-sourced MLLMs are also prone to generate hallucinated response [844] and can be easily manipulated to produce harmful content [858]. To address the aforementioned issues, some studies collect specialized visual instructions to mitigate the problem of hallucination [834]. Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way [859]. Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality [860]. Despite these efforts, existing alignment techniques for MLLMs mainly concentrate on several specific aspects (e.g., hallucination), lacking a comprehensive consideration of alignment criteria. More efforts should be made to promote the research of safety and alignment for MLLMs.
> 解决对齐问题的方法有：收集特别的视觉指令来缓解幻觉、训练一个修正模型，在生成后修正 MLLM 生成的回答、使用 RLHF 对齐
### 8.1.5 KG-Enhanced LLM
Despite the excellent capacities, LLMs often suffer from challenges on knowledge-intensive tasks, such as the potential to generate hallucinated content [602] and the lack of domain-specific knowledge [861]. As a promising solution, knowledge graphs (KGs), which store enormous knowledge in the triple format, i.e., ⟨ head entity, relation, tail entity ⟩, can be utilized to enhance the task performance of LLMs by providing precise and necessary knowledge. Generally, knowledge enhanced approaches can be expanded into other forms of structured data (e.g., tables and databases) [862], while we limit our discussion to the integration of KG for improving LLMs, which are detailed in two aspects, namely retrieval-augmented LLM and synergy-augmented LLM.
> 知识图谱以三元组的形式存储知识，可以用于提高 LLM 在知识密集任务的表现，我们聚焦两个方面：检索增强的 LLM 和协同增强的 LLM

**Retrieval-Augmented LLM.** Due to the huge amount of fact records in a KG, existing work typically adopts a retrieval model to first obtain a relatively small subgraph from KG, and then leverages it to enhance LLMs by enriching the relevant knowledge. Before the advent of LLMs, the retrieved subgraphs are often supplemented into training data, injecting knowledge information into PLMs via parameter learning [863–865]. In contrast, to leverage the retrieved knowledge, LLMs mainly incorporate it as part of the prompt, without parameter update. To implement this approach, there are two main technical problems, i.e., how to retrieve relevant knowledge from KGs and how to make better use of the structured data by LLMs. For the first issue (i.e., retrieving relevant knowledge), a typical approach is to train a small language model (e.g., RoBERTa) to identify question-related fact triples [866]. To further improve the retrieval performance, several studies also propose an iterative reading-then-reasoning framework, enabling the LLM to interact with the KG multiple times and acquire the required knowledge in a more accurate way [458]. For the second issue (i.e., utilizing retrieved knowledge), a straightforward approach is to serialize the retrieved subgraph and craft specific prompts to include it as the input of LLMs [471, 651]. However, due to the loss of structured information in knowledge serialization, LLMs cannot fully capture the structural semantics conveyed by original KGs. To address this issue, several model-based approaches train a specialized language model (e.g., T5) to transform the subgraph into the natural language text [867]. To guarantee the transformation accuracy, it relies on sufficient training pairs (often unsupervised constructed) [868] and excellent model capability [869].
> 现存的工作采用一个检索模型首先从 KG 获得一个小子图，然后使用子图帮助 LLM
> LLM 之前，子图会被补充到 PLM 的训练数据中，LLM 则将其作为 prompt，没有参数更新
> 检索增强涉及到两个问题：如何从 KG 中检索、如何让 LLM 利用结构化的数据
> 对于检索：典型方法是训练一个语言模型，例如 RoBERTa 识别问题相关的事实三元组，一些研究还提出迭代的读-分析框架，让 LLM 和 KG 多次交互获取知识
> 对于数据利用：一种方法即序列化子图，然后构造 prompt，但序列化时结构信息会丢失，为此，一些工作训练特定的语言模型，例如 T5，将子图转化为自然语言文本

**Synergy-Augmented LLM.** To solve complex tasks (e.g., multi-hop question answering [656]), it often requires LLMs to query a KG multiple times, following a systematic solution plan. We call such a multi-turn interaction approach to enhancing LLM synergy-augmented LLM. To better synergize the LLM and KG in a complementary manner, recent studies propose to decompose the complex task into multiple subgoals and iteratively solve each one by leveraging the necessary knowledge from KG [458, 870, 871]. In this process, the LLM can be regarded as an autonomous agent (detailed in Section 8.1.6), which automatically generates the plan and executes it through interaction with the KG environment [870]. Specially, the mainstream approaches typically start by enumerating the candidates using the available knowledge information at the current step, and then retrieve the most appropriate candidates for the next step according to the question [870, 871]. By iterating the above two steps, LLMs can gradually collect relevant evidence [870, 871], and finally approach the correct solution. Despite the effectiveness, enumeration of the candidates over the KG would lead to a vast search space [872]. To address it, StructGPT [458] proposes a more efficient way to access knowledge information using the specialized interfaces for KGs. Specifically, it carefully designs the specialized interfaces according to the common data operations on KG (e.g., relation extraction and triple extraction), to ensure efficient and accurate data extraction. In this way, LLMs can be instructed to better manipulate and process the structural information of KGs, thus achieving improved task performance.
> 解决复杂问题时，LLM 需要通过一个系统的解决计划查询 KG 多次，这被称为协同增强
> 为了协同 LLM 和 KG，研究提出将复杂任务分解为多个子目标，迭代式求解，每次仅利用 KG 中必要的知识，其中 LLM 视为一个自动智能体，自动生成计划并通过与 KG 交互然后执行
> 主流方法是在当前步列举出候选，然后根据问题检索最恰当的

**Future Directions.** Besides the above approaches, there are several promising directions for KG-enhanced LLM remaining underexplored. First, due to the variety of structured data, it is still difficult for LLMs to directly leverage various kinds of knowledge sources, e.g., domain-specific KGs. Therefore, it is essential to explore the unified way to manipulate and utilize different knowledge sources by LLMs. As a potential solution, it is promising to develop effective approaches to help LLMs comprehend and make use of the access interfaces provided by specific knowledge sources to acquire precise knowledge [458], while more efforts should be made to investigate how to adapt to the data variety in a cost-effective way. Second, with the evolution of real-world information, the knowledge stored in LLMs may become outdated or incorrect. It is necessary to explore how to synchronize the updated knowledge into LLMs through a cost-effective manner [873, 874]. Third, it is promising to investigate the use of factual information from KG to align LLMs in generating more faithful content [875, 876], which can help reduce the hallucination of LLMs.

In addition to exploring KG-enhanced LLMs, it is also meaningful to leverage LLMs to improve the tasks on the KG side (i.e., LLM4KG) [861, 877]. A typical example is that LLMs can help supplement or construct the KG. We omit the discussion of this part, since it is beyond our scope.

### 8.1.6 LLM-based Agent
The research on agents in AI aims to develop entities that can perceive the environment, make decisions, and take actions to achieve specific goals [878]. However, traditional agents are often limited to heuristic rules or specific environments, which constrain their generalization to open-domain scenarios [879]. Given that LLMs possess excellent capacities in solving complex tasks, they have rapidly emerged as promising solutions for serving as the core computation unit of agents [818]. In this part, we will first introduce the framework for LLM-based agents and then discuss their applications.
> 智能体：观察环境、决策、采取行动

**Overall Framework.** Next, we first detail the key components of an LLM-based agent and then present the typical workflow.

- *Components.* Typically, there are three main components in an LLM-based agent: memory, planning50, and execution. Specifically, the memory component aims to store the information perceived from the environment and can be utilized to support decision-making. In particular, LLMbased agents usually maintain information in both shortterm memory and long-term memory with the operations of reading and writing. Short-term memory usually refers to the internal context window of LLMs (i.e., input), where LLMs can read and write through actions like reasoning [880]. While long-term memory can be mapped to the external storage like vector databases [537], where LLMs can read through retrieval and write with reflection [686]. Specially, profiles are usually implemented with long-term memory, which is an important feature for an agent that specifies its role and function [818]. The planning component is responsible for generating the action plan based on the information from the memory component. In data format, the plan usually takes the form of text-based instructions [441] or code-based programs [443]. To generate it, LLM-based agents will first propose several candidates and then select a more suitable one among them [436]. The initial plan can be further refined with execution feedback from the environment [528]. The execution component is in charge of carrying out the plan from the planning component, which can be fulfilled by the internal LLM [441] or external tools [880].
> 基于 LLM 的智能体的三大成分：记忆、规划、执行
> 记忆
> 记忆用于存储从环境中观察到的信息，以支持决策，一般会同时维护短期记忆和长期记忆
> 短期记忆一般指上下文窗口，
> 长期记忆一般指外部存储例如向量数据库，LLM 通过检索读取
> profile 一般就存储于长期记忆，以制定 agent 的角色和功能
> 规划
> 规划负责基于记忆生成动作计划，一般的形式就是基于文本的指令，或者基于代码的程序，LLM 一般会提出一些候选，然后从中选择
> 且最初的计划可以通过从环境中的执行反馈进一步 refine
> 执行
> 执行成分可以是 LLM 也可以是外部工具

- *Workflow.* With the three components mentioned above, a typical workflow of an LLM-based agent is as follows. First, it receives information from the environment and writes it into short-term memory. Then, the agent processes the newly received information in the short-term memory. Such a process can be enhanced with information retrieved from long-term memory. Subsequently, the planning component utilizes the processed information from short-term memory to generate the next plan. Finally, the execution component carries out the plan generated from the planning component, which can be further assisted with external tools. By repeating the aforementioned process, the LLM-based agent can autonomously adjust its behavior in response to feedback from the environment and ultimately achieve its goal. Once LLM-based agents receive user requests or are assigned goals, they follow the above workflow to accomplish tasks through multi-turn interactions with the environment.
> 工作流：从环境接受信息，写入短期记忆，agent 处理（可以在长期记忆检索信息的帮助下），生成计划，根据计划执行

To summarize, in an LLM-based agent, the LLM serves as the core computation unit and is equipped with components including memory, planning, and execution. These components are integrated in a systematic way under the control of the LLM during interactions with the environment. For more details, the readers might refer to the comprehensive survey for LLM-based AI agents [818].

**Applications.** Recently, LLM-based agents have shown great potential in autonomously solving complex tasks, making it feasible to rapidly develop capable applications for specific domains or tasks. In this section, we will discuss the applications in single-agent and multi-agent scenarios.

- *Single-agent based applications.* Applications based on a single-agent mode mainly aim to develop capable task solvers that can autonomously complete user requests. A large number of single-agent projects have been developed, which focus on general-purpose task solving. As a representative project, AutoGPT [534] empowers LLMs with long/short-term memory management and external tools like search engines. In order to autonomously address a user request, AutoGPT understands the request with knowledge from its memory and actions like reasoning, decomposes it into a detailed plan, executes the plan step-bystep with the assistance of tools, and refines the rest plan based on feedback from the environment. Such an iterative process continues until the user request is successfully resolved. Other similar projects include GPT-Engineer [881] and XAgent [882]. In addition, there is also some work that aims to develop autonomous agents for specific domains, such as WebGPT [81] for the web-browsing environment, ProgPrompt [530] for the real-life environment, and Voyager [697] for the Minecraft environment.

- *Multi-agent based applications.* Different from singleagent systems where agents work independently, multiagent systems work in collaboration to unleash collective intelligence. Typically, multiple agents can be instantiated from the same or different LLMs, each with their respective roles and functions. According to the coordinating strategies among these agents, multi-agent systems can be divided into two categories: cooperation-based and competitionbased. In the cooperation-based mode, to share information and seek collaborative actions among agents, various communication protocols have been proposed, including freeform dialogue [883], structured document [884], and data embedding [885]. Based on the communication protocol, agents can be effectively organized for downstream applications, such as software engineering [884], user behavior analysis [819, 821], and society simulation [533]. In the competition-based mode, debate serves as one of the popular communication protocols to foster divergent thinking and elicit valuable external feedback among agents. Such a way is beneficial for domains that demand precise decisionmaking and accurate responses, such as mathematical reasoning [886] and evaluation [732].
> 多个智能体各自的 role 和 function 不同
> 多智能体系统可以分为：基于合作的、基于竞争的（例如辩论）

**Remaining Issues.** Despite the huge success, there are still several issues that limit the development and applications of LLM-based agents. First, with the explosive growth of the model scale, the efficiency of LLM-based agents, including both the time and memory overhead, becomes an important issue for large-scale deployment, especially for multi-agent systems with numerous instances of LLMs. Second, with the scaling of the number of LLM-based agents, more effective and efficient communication protocols and architectures are required to support the increased complexity of coordination among agents. Furthermore, building capable agents poses technical challenges for the capacities of LLMs like instruction following and long text modeling. Since existing LLMs are not specially optimized for instantiating agents, most public-sourced LLMs like LLaMA cannot effectively facilitate the development of agents. Therefore, it is crucial to develop capable, specialized models to serve as the core computation unit of agents.
> 问题：部署开销过大、智能体之间的协调复杂

### 8.1.7 LLM for Evaluation
While human evaluation can generally offer reliable quality assessment, it is also often hindered by high annotation costs, significant time requirements, and annotation inconsistencies [887]. In contrast, automatic evaluation can be employed as a scalable alternative to human evaluation. Traditional automatic evaluations have relied on referencebased metrics (e.g., BLEU and ROUGE). Recently, with the emergence of LLMs as general task solvers highlights their potential as automatic evaluators [647, 727], making it promising to conduct LLM based evaluation. In the following part, we will introduce the recent progress on LLM for evaluation, including evaluation formats, methods, metaevaluation, and the remaining issues.

**Evaluation Formats.** Depending on the type of evaluation outcome, the evaluation format can be categorized into score-based evaluation and language-based evaluation. Scorebased evaluation employs measurable metrics to assign quality scores (e.g., ratings or rankings) for evaluated texts. A prevalent way is to conduct pairwise comparison, where LLMs are used to determine the partial order relation of candidate texts following specific guidelines [354, 647, 727], which greatly simplifies the evaluation task. However, it may face the inefficiency issue when scaling up the number of candidates [727]. When high-quality reference texts are available during evaluation, LLMs can be instructed to score texts under the guidance provided by references [716, 727, 728]. On the other hand, language-based evaluation focuses on generating critiques and suggestions, offering qualitative explanation beyond simple quantitative scoring [371, 888– 890]. It is particularly useful for gathering language feedback signals for human alignment tuning [371, 888]. Furthermore, it can evolve into a multi-turn interaction framework, where LLM-based evaluators provide natural language feedback to existing solutions from task solvers [891]. This framework evaluates the ability of LLMs to leverage language feedback for refining self-generated solutions.
> 评估格式：基于分数评估、基于语言评估
> 基于分数评估
> 使用 metrics 来赋分（或排序），例如让 LLM 进行 pairwise 比较，决定其偏序关系
> 但要排序的候选太多开销太大
> 如果存在高质量的参考文本，可以让 LLM 根据参考文本打分
> 基于语言评估
> LLM 生成批评和建议，提供性质上的分析解释

**Evaluation Methods.** A common method for LLM-based evaluation involves prompting LLMs with specific instructions. To further improve the quality of LLM-based evaluation, recent work proposes to prompt LLMs with varied contexts to generate diverse evaluation feedback. These contexts vary in aspects such as the candidate order [647, 727], evaluation perspectives [892, 893] (e.g., relevance, clarity, originality), and evaluation explanation [647]. The generated multiple evaluation feedbacks are then aggregated to produce a final evaluation result, which makes the evaluation process less prone to biases from individual feedback and allows for a more thorough evaluation by covering a wider range of evaluation aspects. To further improve the quality of the single-model evaluation, recent studies also develop multi-agent collaboration frameworks [893– 895] or fine-tune LLMs as specified evaluators [371, 888– 890, 896]. In a multi-model collaboration mode, different LLMs evaluate the candidates by engaging in discussions to align preferences and reach a consensus [894, 895]. This method helps reduce the potential biases in individual models through the consensus reached by multiple agents. Another approach to improving single-model evaluation is to specialize LLMs as scores or critics through finetuning [371, 888–890, 896]. This process involves creating datasets annotated with preferences and feedback from humans or proficient LLMs. These datasets are then used to train evaluation-oriented models, enabling them to generate pairwise preference or language feedback. The specialized LLM evaluators demonstrate competitive performance with fewer parameters [889, 890, 896].
> 评估方法：
> 为 LLM 提供不同的上下文以让 LLM 生成更多样的反馈，然后聚合它们产生最终评估结果
> 也可以多个模型配合
> 也可以训练 evaluation-oriented 模型

**Meta-Evaluation.** To effectively assess the quality of LLM-based evaluators, meta-evaluation benchmarks have been introduced, for gauging the agreement with human preferences and the fairness of the evaluations made by LLMs [647, 727, 893, 897, 898]. As a representative benchmark, MT-Bench [727] evaluates the agreement between LLMs and human judgments, demonstrating that GPT-4 aligns closely with human preferences in no-tie comparisons on 80 multi-turn questions. In addition, to address potential biases arising from subjective human evaluations, LLMBar [897] manually designs outputs that are objectively worse but superficially appealing, which could mislead evaluators. The evaluation results reveal that even the most advanced LLMs still fall short of human-level evaluation in the challenging setting.
> meta-evaluation: 评估 LLM 的 evaluation 和人类偏好以及公平性的对齐程度

**Remaining Issues.** As discussed in Section 7.1.1, recent studies demonstrate that LLM-based evaluators expose multiple types of bias, such as order bias, self-preference bias, and length bias [647, 727]. Although some biases can be mitigated through methods like multi-path ensemble or multi-agent collaboration, they remain inherent to LLMbased evaluators. Consequently, addressing these biases intrinsically within the models continues to be an a challenging issue. In addition, recent work has revealed that LLMs may be incapable of understanding the self-generated content, exhibiting a weaker understanding capacity compared to their generation capabilities [899]. Even the most advanced LLMs still struggle identifying their reasoning or factual errors without external feedback [900, 901]. Consequently, current LLM-based evaluators might not be adequate for evaluating top-tier LLMs or complex tasks. This underscores the importance of improvement approaches for LLM-based evaluators, especially for evaluating capable LLMs and complex tasks demanding sophisticated reasoning, planning, and domain-specific knowledge.
> 基于 LLM 的评估者有多种偏好：order biases, self-preference biases, length biases
> 研究发现 LLM 的理解能力远弱于生成能力，即便最先进的 LLM 也不能在没有外部提示的情况下识别其推理或事实性错误

## 8.2 LLM for Specific Domains
In this part, we discuss the applications of LLMs on several representative domains, including healthcare, education, law, finance, and scientific research assistance.

**Healthcare** is a vital application field closely related to human life. Ever since the advent of ChatGPT, a number of studies have applied ChatGPT or other LLMs to the medical domain. It has been shown that LLMs are capable of handling a variety of healthcare tasks, e.g., biology information extraction [763], medical advice consultation [902], mental health analysis [903], and report simplification [904]. As the major technical approach, researchers typically design specific prompts or instructions to guide LLMs to perform a wide range of medical tasks. To further harness the power of LLMs in the healthcare domain, researchers propose to develop healthcare-related LLMs [356, 905, 906]. Specifically, the Med-PaLM models [356, 905] achieves expert-level performance on the United States Medical Licensing Examination (USMLE), and earns greater approval from physicians in answering consumer’s medical questions. However, LLMs may fabricate medical misinformation [904, 907], e.g., misinterpreting medical terms and suggesting advice inconsistent with medical guidelines. In addition, it would also raise privacy concerns to upload the health information of patients [763] into a commercial server that support the LLM.

**Education** is also an important application domain where LLMs potentially exert significant influence. Existing work has found that LLMs can achieve student-level performance on standardized tests [46] in a variety of subjects of mathematics (e.g., physics, computer science) on both multiplechoice and free-response problems. In addition, empirical studies have shown that LLMs can serve as writing or reading assistant for education [908, 909]. A recent study [909] reveals that ChatGPT is capable of generating logically consistent answers across disciplines, balancing both depth and breadth. Another quantitative analysis [908] shows that students utilizing ChatGPT (either keeping or refining the results from LLMs as their own answers) perform better than average students in some courses from the computer security field. Recently, several perspective papers [910, 911] also explore various application scenarios of LLMs in classroom teaching, such as teacher-student collaboration, personalized learning, and assessment automation. However, the application of LLMs in education may lead to a series of practical issues, e.g., plagiarism, potential bias in AIgenerated content, overreliance on LLMs, and inequitable access for non-English speaking individuals [912].

**Law** is a specialized domain that is built on professional domain knowledge. Recently, a number of studies have applied LLMs to solve various legal tasks, e.g., legal document analysis [913], legal judgment prediction [914], and legal document writing [915]. A recent study [916] has found that LLMs exhibit powerful abilities of legal interpretation and reasoning. Moreover, the latest GPT-4 model achieves a top 10% score in a simulated bar exam compared with human test-takers [46]. To further improve the performance of LLMs in the law domain, specially designed legal prompt engineering are employed to yield advanced performance in long legal document comprehension and complex legal reasoning [917, 918]. To summarize the progress, LLMs can act as helpful assistants to legal profession. Despite the progress, the use of LLMs in law raises concerns about legal challenges, including copyright issues [919], personal information leakage [920], or bias and discrimination [921].

**Finance** is an important field where LLMs have promising application prospects. LLMs have been employed on various finance related tasks, such as numerical claim detection [922], financial sentiment analysis [923], financial named entity recognition [924], and financial reasoning [925]. Despite the competitive zero-shot performance exhibited by general-purpose LLMs in the finance tasks, they still underperform domain-specific PLMs containing million-scale parameters [922]. To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs (e.g., BloombergGPT [360], XuanYuan 2.0 [926], and FinGPT [927]). BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks [360]. Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets [360]. Therefore, it needs more strict reviewing and monitoring on the use of LLMs in the financial field.

**Scientific research** is another promising field that LLMs can empower the development progress. Prior research demonstrates the effectiveness of LLMs in handling knowledge-intensive scientific tasks (e.g., PubMedQA [928], BioASQ [929]), especially for LLMs that are trained on scientific-related corpora [35, 203, 930]. Given the excellent general abilities and broad scientific knowledge, LLMs hold significant potential as helpful assistants across various stages of the scientific research pipeline [931]. First, during the literature survey stage, LLMs can help conduct a comprehensive overview of the progress in a specific research field [932, 933]. Second, during the research idea generation stage, LLMs demonstrate the ability to generate intriguing scientific hypotheses [934]. Third, during the data analysis stage, LLMs can be employed to conduct automatic approaches to analyzing the data characteristics, including data exploration, visualization, and deriving analytical conclusions [935, 936]. Fourth, during the paper writing stage, researchers can also benefit from the assistance of LLMs in scientific writing [937, 938], in which LLMs can offer valuable support for scientific writing through diverse means, such as summarizing the existing content and polishing the writing [939]. In addition, LLMs can aid in the automated paper review process, encompassing tasks such as error detection, checklist verification, and candidate ranking [940]. Despite these advances, there is much room for improving the capacities of LLMs to serve as helpful, trustworthy scientific assistants, to both increase the quality of the generated scientific content and reduce the harmful hallucinations.

**Summary.** In addition to the aforementioned work, the applications of LLMs have been also discussed in several other domains. For instance, in the psychologic domain, some recent work has studied the human-like characteristics of LLMs, such as self-awareness, theory of mind (ToM), and affective computing [941, 942]. In particular, an empirical evaluation of ToM conducted on two classic false-belief tasks speculates that LLMs may have ToM-like abilities since the model in the GPT-3.5 series achieves comparable performance with nine-year-old children in ToM task [941]. In addition, another line of work has investigated applying LLMs into the software development domain, e.g., code suggestion [943], code summarization [944], and automated program repair [945]. To summarize, to assist humans by LLMs in real-world tasks has become a significant area of research. However, it also presents challenges. Ensuring the accuracy of LLM-generated content, addressing biases, and maintaining user privacy and data security are crucial considerations when applying LLMs to real-world scenarios.
# 9 Conclusion and future directions
In this survey, we have reviewed the recent progress of large language models (LLMs), and introduced the key concepts, findings, and techniques for understanding and utilizing LLMs. We focus on the large-sized models (i.e., having a size larger than 10B) while excluding the contents of early pretrained language models (e.g., BERT and GPT-2) that have been well covered in the existing literature. In particular, our survey has discussed four important aspects of LLMs, i.e., pre-training, adaptation, utilization, and evaluation. For each aspect, we highlight the techniques or findings that are key to the success of LLMs. Furthermore, we also summarize the available resources for developing LLMs and discuss important implementation guidelines for reproducing LLMs. This survey tries to cover the most recent literature about LLMs and provides a good reference resource on this topic for both researchers and engineers.
> survey 讨论了 LLM 的四个重要方面：预训练、适应、使用、评估

Next, we summarize the discussions of this survey, and introduce the challenges and future directions for LLMs, in the following aspects.

**Basics and Principles.** Instead of training on specific task goals, LLMs learn from unsupervised pre-training on largescale text data. This is quite different from previous multitask learning approaches, which aim to extend the training tasks as possible to achieve sufficient generalization. Thus, it is essential to reveal the basic principles or elements that establish the foundation of the abilities of LLMs. Although the basic idea of language models is intuitive, it is still challenging to formally explain why LLMs trained by simple language modeling objectives (e.g., next token prediction) can become capable of solving various real-world tasks. To investigate this problem, a promising approach is to study the capacity learning (or selection) mechanism based on unsupervised pre-training, since the model capacity of LLMs strongly depends on pre-training data. In addition, scaling plays an important role in improving the capacity of LLMs [31, 55, 64], and it is very useful to conduct more theoretical analysis about how the behaviors of large models relate to those of small models, e.g., what behaviors of large models can be inferred from small models and what can’t be predicted indeed. Another research direction is to explore more deep analysis on model generalization for LLMs, since increasing concerns have been raised about whether LLMs can generalize beyond the knowledge encoded by pre-training data. Furthermore, data contamination has become a severe issue for fairly assessing the performance of LLMs [738], and thus setting appropriate evaluation protocol will be the basis to investigate and analyze the model capacity of LLMs.
> LLM 的预训练任务简单，目的是得到充足的泛化性

**Model Architecture.** Due to the scalability and effectiveness, Transformer has become the de facto architecture for building LLMs. Various strategies have been proposed to improve the performance of this architecture, such as neural network configuration and scalable parallel training (see discussions in Section 4.2.2). However, Transformer still suffers from high training costs and slow inference rates. More efforts [251, 252] are still in need to develop improved model architectures for large-scale pre-training. Specially, system-level or hardware-level optimization (e.g., FlashAttention [284]) is worth more exploration to improve the efficiency of Transformer architectures. In addition, as an important basic capacity, existing LLMs typically maintain a long context window. For example, the most recent GPT-4 Turbo enables a long context of 128K tokens, and Claude 2.1 also supports the input up to 200K tokens. Although many efforts have been made to enhance the long context modeling ability of LLMs [264, 291], the resulting models still can’t well process the information in the context window [299]. To address this issue, specific architecture adaptations or algorithms might be needed to enhance the modeling and utilization of long context information. Another worrying concern is that existing work mostly focuses on training LLMs with decoder-only Transformers. Despite the effectiveness, it severely limits the more wide, diverse explorations on alternative model architectures.

**Model Training.** For pre-training, it is essential to establish a data-centric infrastructure and training procedure for LLM optimization, which can effectively support a systematic process of data collection, data cleaning, data mixture, and data curriculum. Furthermore, it also calls for more flexible mechanisms of hardware support or resource schedule, so as to better organize and utilize the resources in a computing cluster. In practice, it is very challenging to pre-train capable LLMs, due to the huge compute consumption and the sensitivity to data quality and training tricks [78, 93]. Thus, it becomes particularly important to develop systemic, economical pre-training approaches for optimizing LLMs, e.g., predictable scaling [46] and proxy model training [59]. More training recipes or principles should be investigated and shared to reduce the potential risk of degradation or failure in large-scale model optimization. Although increasingly more model checkpoints and cleaned datasets have been released, there still lacks reproducible work on pre-training data preparation (e.g., detailed cleaning strategies) and data scheduling (e.g., data mixture and curriculum). Since it is very costly to pre-train a LLM from scratch, it is important to design suitable mechanisms for continually pre-training or fine-tuning the LLM based on publicly available model checkpoints (e.g., LLaMA [57] and Flan-T5 [69]). For this purpose, a number of technical issues have to be resolved, e.g., catastrophic forgetting and task specialization. Furthermore, it is also useful to develop effective tuning strategies that effectively inject or edit specific knowledge [672], e.g., correcting the outdated facts.
> 缺乏系统的训练框架
> 需要更好的预训练技巧，例如可预测的 scaling 以及代理模型训练等
> 仍缺乏可复现的数据准备过程，包括细节的数据清理策略、数据调度（数据混合和数据课程）
> 需要可以持续预训练或微调 LLM 的机制

**Model Utilization.** Based on the natural language interface, prompting has become the prominent approach for using LLMs to solving various tasks. By combining task descriptions and demonstration examples into prompts, incontext learning (ICL) endows LLMs with the ability to perform well on new tasks, even outperforming full-data fine-tuned models in some cases. To enhance the ability of complex reasoning, advanced prompting techniques have been proposed, exemplified by the chain-of-thought (CoT) strategy, which includes the intermediate reasoning steps into prompts. Furthermore, planning is a promising approach for solving complex tasks, which iteratively invokes LLMs by leveraging tool use capacities. Despite these efforts, several basic problems related to prompting are still under-explored: why a good prompt can elicit the correct answer but a bad prompt cannot, how to reveal the working principles of advanced prompting methods (e.g., ICL and CoT) and further improve these existing approaches, and how to efficiently find the effective prompts for LLMs on specific tasks. Furthermore, from a practical perspective, it has become a fundamental challenge to reduce the inference cost of LLMs, especially in large-scale deployment. Another popular research direction is retrieval-augmented generation, where retrieved contexts from supporting sources are included into prompts for task solving. It has been shown that retrieval augmentation can extend the knowledge boundary and improve the question answering capacity [461], but may suffer from the effectiveness of long context utilization by LLMs [299].

**Safety and Alignment.** Despite the capacities, LLMs are faced with great safety challenges in practical use. As a fundamental issue of probabilistic modeling nature, LLMs exhibit a tendency to generate hallucinations [638], referring to texts that seem plausible but may be factually incorrect [46]. What is worse, LLMs might be elicited by intentional instructions to produce harmful, biased, or toxic texts for malicious systems, leading to the potential risks of misuse [55, 66]. To have a detailed discussion of the safety issues of LLMs (e.g., privacy, overreliance, disinformation, and influence operations), the readers can refer to the GPT-3/4 technical reports [46, 55]. As the major technical approach to averting these issues, alignment methods (e.g., RLHF) [66, 116] have been widely used by leveraging human feedback for developing well-aligned LLMs. However, RLHF heavily relies on high-quality human feedback data from professional labelers, which is costly and timeconsuming to recruit qualified human annotators. Therefore, it is necessary to improve the RLHF framework for reducing the efforts of human labelers and seek a more efficient annotation approach with guaranteed data quality, e.g., LLMs can be employed to assist the labeling work. Furthermore, it is also suggested to develop simplified optimization algorithms for alignment [386, 389], to reduce the training difficulty and unstability of RLHF. As another practical approach, red teaming [132, 369] has been adopted for improving the model safety of LLMs, which utilizes the collected adversarial prompts to refine the LLMs (i.e., avoiding the attacks from red teaming). In addition, privacy concerns are also important to consider when fine-tuning LLMs with domain-specific data, and thus federated based learning [946] can be useful in privacy-restricted scenarios.

**Application and Ecosystem.** As LLMs have shown strong capacities in solving various tasks, they can be applied in a broad range of real-world applications (i.e., following task-specific natural language instructions). As a remarkable progress, ChatGPT has potentially changed the way how humans access information, which has been additionally integrated in the release of New Bing. Generally, in the near future, it can be foreseen that LLMs would have a significant impact on information-seeking techniques, including both search engines and recommender systems. Furthermore, LLMs make it possible to develop more intelligent systems (e.g., autonomous AI agents) to tackle various complex tasks in real-world scenarios. Specially, Assistants API has been launched by OpenAI (featured by instructions, knowledge and tool use), enabling rapid development of agent-like assistants within the applications. This wave of technical innovation would lead to an ecosystem of LLMempowered applications (e.g., OpenAI’s GPT Store), which has a close connection with human life. Lastly, the rise of LLMs sheds light on the exploration of artificial general intelligence (AGI). It is promising to develop more smart AI systems than ever. However, in this development process, AI safety should be one of the primary concerns, i.e., making AI lead to good for humanity but not bad [40].
