# Abstract
In this work, we propose a novel method named Automated Process-Supervised Verifier (AUTOPSV) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. AUTOPSV begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. 
>  本文提出自动过程监督的验证器 (AUTOPSV) ，AUTOPSV 通过自动标注推理步骤来强化 LLM 的推理能力
>  AUTOPSV 先根据最终答案的正确与否，训练一个验证模型，然后让该验证模型可以生成自动的过程标记
>  验证模型为每一个推理步赋予置信度分数，置信度分数表示了从当前推理步道道最终正确答案的概率

We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process, enabling error detection even in scenarios where ground truth answers are unavailable. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. 
>  我们通过检测推理步之间的置信度分数的相对变化来自动标记推理过程，例如当一个推理步的置信度分数突然下降时，通常表明该推理步骤存在错误，这种方法在缺乏正确答案的情况下也能进行错误检测，这缓解了人工标注的需求

We experimentally validate that the step-level confidence changes learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps. We demonstrate that the verification model, when trained on process annotations generated by AUTOPSV, exhibits improved performance in selecting correct answers from multiple LLM-generated outputs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of AUTOPSV is available at https://github.com/rookie-joe/AutoPSV.
>  我们实验验证了基于最终答案的正确性训练的验证模型所学习到的 step-level 置信度变化可以有效的识别推理步中的错误
>  我们证实了当验证模型使用 AUTOPSV 生成的过程标注训练时，在从多个 LLM 生成的输出中选择正确答案方面的表现显著提高

# 1 Introduction
Large language models (LLMs) have shown impressive performance on various reasoning tasks [1-4]. Prior effects primarily focus on specific promoting techniques, such as few-shot prompting with intermediate steps and augmented demonstrations [5-8]. While these methods have shown promise, their effectiveness is often task-specific, and designing prompts can be labor-intensive, leading to inconsistent results [9, 10]. Another approach to improve reasoning in LLMs is through instruction tuning or knowledge distillation [11-14]. These methods typically involve fine-tuning LLMs and require a large set of examples annotated with chain-of-thoughts (CoT; [5]). However, these approaches can be resource-intensive and may not always produce reliable results.
>  提高 LLM 推理能力的方法有: prompt 、instruction tuning 和 knowledge distillation

To address these challenges, verification techniques have emerged as a promising solution [15, 16]. Verification models are trained to evaluate and potentially correct the reasoning process generated by LLMs. This approach aims to mitigate the risk of relying solely on the top-1 result, which may not always be reliable [17, 18]. By reranking candidate responses, verification models can ensure higher accuracy and consistency in LLM outputs, and provide valuable feedback for improving LLMs [19, 20] further.
>  验证技术训练一个验证模型，来评估并纠正 LLM 的推理过程
>  验证方法的目标是缓解仅依赖于 top-1 result 的风险
>  验证模型会对 candidate response 重新排序，确保 LLM 输出具有更高的正确性和一致性
>  验证模型也可以为 LLM 提供反馈，以进一步提升 LLM

Verification models generally fall into two training paradigms: outcome supervision and process supervision. In outcome supervision, the training annotations rely on the correctness of the final answer [21, 22], while in process supervision, the annotations are based on evaluations of each reasoning step [23, 19]. However, process supervision is demanding in terms of annotations. Typically, it relies on either expensive and highly skilled human evaluators [23, 16] or model-induced process annotations [18, 17] to estimate the future correctness of the current reasoning step using Monte Carlo tree search [24, 25]. In contrast, outcome supervision only requires annotations for the output, making it more economical in terms of annotation effort but less effective. That being said when answers involve multiple reasoning paths, all aforementioned model-induced methods require numerous samples to ensure accurate estimations.
>  验证模型通常归于两类训练范式: 结果监督和过程江都
>  结果监督中，训练标记为最终答案的正确性；过程监督中，标记为每一个推理步的评估

In this paper, we introduce Automated Process-Supervised Verifier (AUTOPSV), a novel approach that synergistically combines the strengths of both process supervision and output supervision. Our method begins by training an outcome-supervised verification model using outcome supervision annotations. This model then assigns confidence scores to each intermediate reasoning step, estimating their likelihood of contributing to a correct final answer. A distinguishing feature of AUTOPSV is its ability to automatically generate process annotations through relative step-level confidence change analysis, significantly reducing annotation effort while maintaining supervision quality without requiring ground truth answers. These automatically generated process annotations subsequently serve as training data for developing an enhanced verification model that leverages both process-level and outcome-level supervision signals. 
>  本文提出 AUTOPSV 来结合过程监督和结果监督的能力
>  AUTOPSV 先训练一个结果监督的验证模型，然后让该验证模型为每个中间对立步骤赋予置信分数，置信分数评估了中间步对最终正确答案的贡献
>  AUTOPSV 可以通过相对的 step-level 置信度改变来自动生成过程标记，这些自动生成的过程标记会作为进一步强化验证模型的训练标签

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-13/93111879-9d76-4736-8153-16ea774e753d/03cb0c9ad8d886f284421a46a3d3ef1830938baf98e2a65f0ba8ea7abb298293.jpg)  

Figure 1: An overview of AUTOPSV. It utilizes an outcome-supervised verifier to automatically generate process annotations for each reasoning step by detecting its own confidence variations, without relying on ground truth annotations. AUTOPSV efficiently produces annotations serving as process supervision during the LLM training, which sidesteps costly annotations.

The complete framework of AUTOPSV is illustrated in Figure 1. We conduct extensive experiments across five datasets, including mathematical reasoning benchmarks and commonsense reasoning tasks. The results demonstrate that our method effectively improves the reasoning capability of the model with our highly efficient labeling scheme for process supervision.
>  实验结果表明了 AUTOPSV 可以提高模型的推理能力

Our contribution is summarized as follows:

- We introduce AUTOPSV to automate the labeling of process data to enhance LLMs' reasoning capabilities. By combining the strengths of output and process supervision, AUTOPSV effectively identifies variations in model confidence to annotate the correctness of intermediate reasoning steps, enabling efficient automatic labeling for process supervision.
- Comprehensive experiments demonstrate that AUTOPSV significantly improves the performance and scalability of verification models in mathematical and commonsense reasoning tasks. This approach greatly reduces the need for manual intervention and extensive computational resources, making it a valuable tool for enhancing LLM capabilities.
- AUTOPSV's versatility is evident in its applicability to both labeled and unlabeled dataset settings after completing the training process. This flexibility and generalizability highlight the method's potential for widespread adoption in various LLM applications.

>  本文的贡献包括:
>  - AUTOPSV，通过识别置信度的变化，标记中间推理步骤的正确性，实现了自动标记 LLM 的推理过程
>  - 相关试验，验证了 AUTOPSV 可以在数学和常识推理任务上提升验证模型的性能和可拓展性
>  - AUTOPSV 在完成训练后，既可以用于有标签的数据集，也可以用于无标签的数据集

# 2 Related Works
**Improving Reasoning Abilities of LLMs** To enhance the reasoning capabilities of LLMs, prior research primarily focuses on specific prompting techniques [26]. Existing efforts include few-shot prompting with intermediate steps augmented demonstrations [5-7, 27] or zero-shot prompting with specific instructions [28, 29]. Although these methods have shown promising results, their effectiveness is often constrained by their task-specific nature and the labour-intensive process of designing prompts, leading to inconsistent outcomes across different tasks [9, 10]. Another strategy to facilitate reasoning involves instruction tuning or knowledge distillation, which elicits reasoning paths from LLMs without explicit prompting [11-13, 30]. These approaches typically involve resource-intensive fine-tuning over LLMs and require a large set of examples annotated with chain-of-thoughts (CoT). Unlike methods that directly modify parameters or prompts, AUTOPSV focuses on training an additional verification model to select the desired output from the original model's output. This approach is further discussed in the context of process supervision in the following paragraph.

**From Outcome to Process Supervision** Recent efforts have focused on enhancing the reasoning capabilities of LLMs through the use of verifiers to select the best answer from multiple candidates. There are two main types of verifiers: the Outcome-Supervised Verifier (OSV) and the Process-Supervised Verifier (PSV). The OSV is supervised with a signal based on the final answer [21, 22], while the PSV is with detailed feedback which requires evaluating individual reasoning steps [15, 19, 16, 23]. Despite the time-consuming annotation cost, the PSV offers several advantages that make it preferable to the OSV. It can provide fine-grained feedback by pinpointing the location of errors, which is valuable for reinforcement learning and automatic correction [16, 20]. To alleviate the extensive human annotation, recent approaches [17, 18] propose a machine annotation framework using Monte Carlo Tree Search [24, 25]. This process demands a lot of computing resources, potentially imposing a limitation on the usage. AUTOPSV is more efficient, as it utilizes an outcome-supervised verification model to assign confidence to each reasoning step and calculate relative step-level confidence changes, eliminating the need for additional sampling or manual labeling.

# 3 AUTOPSV
In this section, we introduce the main problem that this paper focuses on in Section 3.1. We then discuss the motivation behind why we believe it is necessary to train a verification model in Section 3.2. Finally, we describe how we accomplish the transition from outcome supervision to process supervision during the training of the verification model in Section 3.3.

## 3.1 Problem Setting
**Objective** Our research addresses the challenge of response selection from multiple candidates generated by a Large Language Model (LLM). Specifically, given an LLM acting as a response generator, we seek to develop an effective method for identifying the correct response among multiple generated solutions. Our primary goal is to maximize the probability of selecting an accurate solution from the available candidates.
>  目标: 给定 LLM，我们需要识别 LLM 生成的多个 solutions 中正确的那一个，因此主要目标是最大化从多个 candidates 中选择一个正确的解的概率

**Notation** We define  $q$  as the input question presented to the model, and  $S_{i}^{(1:t)}$  as the sequence of intermediate reasoning steps up to step  $t$  for the  $i$ -th solution to question  $q$ . The binary correctness label for the  $i$ -th solution is denoted as  $q_{t}$ , and  $Q$  represents our training question dataset. This formalization enables us to systematically approach the response selection problem while maintaining mathematical rigor in our methodology.

## 3.2 Motivation
Response selection methods can be broadly divided into two categories: models specifically fine-tuned for the selection task and those that employ various prompting strategies.
>  response selection 方法可以大致分为两类: 对 selection task 直接微调模型、使用各种 prompting 策略的方法

In our exploration, we initially investigate whether existing open-source LLMs could serve as effective selection agents to evaluate model outputs and choose the correct response without fine-tuning. We choose Mixelral-Instruct [31] as the response generator, and its response results are listed Table 1. Our objective focuses on identifying the correct response from five candidate solutions.
>  我们首先调查了现存的开源 LLM 在不微调的情况下是否可以作为 selection agent 来评估模型输出
>  我们使用了 Mixeral-Instruct 作为 response generator

Table 1: Performance of Mixtral-Instruct on GSM8K. All results are reported in accuracy  $(\%)$  

<table><tr><td>Response Generator</td><td>Model Size (Parameters)</td><td>Pass@1 (%)</td><td>Pass@5 (%)</td><td>Self-Consistency (%)</td></tr><tr><td>Mixtral-Instruct [31]</td><td>8 x 7B (MOE)</td><td>62.55</td><td>82.31</td><td>69.06</td></tr></table>

To establish robust conclusions, we evaluate selector models ranging from 7B to over 70B parameters, applying various prompting strategies. The results, tested on the GSM8K test set [21], are presented in Table 2. Notably, even models exceeding 70 billion parameters demonstrate suboptimal selection performance when relying solely on prompting without fine-tuning.
>  我们评估了 7B - 70B 参数的 selector 模型，并采用了各种 prompting 策略
>  结果来看，即便是 70B 参数的模型在没有微调，仅采用 prompting 时的 selection 表现也是次优的

Table 2: Comparison of different selection methods across various model sizes for selecting a response from candidate responses generated by Mixtral-Instruct. All results are reported in accuracy  $(\%)$  

<table><tr><td rowspan="2">Selector</td><td rowspan="2">Model Size</td><td colspan="5">Prompt Strategy</td></tr><tr><td>Pairwise</td><td>Classification</td><td>Classification + CoT</td><td>Scoring</td><td>Scoring + CoT</td></tr><tr><td>Mistral-Instruct [32]</td><td>7B</td><td>60.73</td><td>61.18</td><td>64.82</td><td>61.49</td><td>69.75</td></tr><tr><td>Mistral-Instruct [31]</td><td>8×7B</td><td>58.83</td><td>59.14</td><td>67.40</td><td>61.79</td><td>65.58</td></tr><tr><td>Llama2-chat [33]</td><td>70B</td><td>59.28</td><td>62.70</td><td>66.79</td><td>59.74</td><td>62.93</td></tr><tr><td>Qwen [34]</td><td>72B</td><td>59.14</td><td>66.64</td><td>69.52</td><td>61.86</td><td>65.88</td></tr></table>

Based on these findings, our research focuses on training dedicated verification models and enhancing their response selection capabilities.
>  基于这些发现，我们的研究聚焦于训练专用的验证模型，并强化它们的 selection 能力

## 3.3 Training Methodology
AUTOPSV integrates outcome supervision with process supervision to create an effective training methodology. Below, we detail each component of our approach.

**Outcome-Supervision** We begin by training an outcome-supervised verification (OSV) model, denoted as  $f_{\theta}(\cdot)$ , where  $\theta$  represents the optimized parameters. The training utilizes Mean Squared Error (MSE) loss for each solution step:

$$
\mathcal{L}(S_i^{(1:t)},y_i;q) = \left(f_{\theta}(S_i^{(1:t)};q) -y_i\right)^2 \tag{1}
$$

The complete objective function across all training questions  $\mathcal{Q}$  is:

$$
\mathcal{L}_{\mathrm{total}}(\mathcal{Q}) = \frac{1}{|\mathcal{Q}|}\sum_{q\in \mathcal{Q}}\frac{1}{n}\sum_{i = 1}^{n}\sum_{t = 1}^{m_i}\left(f_{\theta}(S_i^{(1:t)};q) -y_i\right)^2 \tag{2}
$$

Where  $n$  represents **solutions per question** and  $m_i$  denotes **steps in the  $i$ -th solution.**

>  我们先训练一个结果监督的验证模型，记作 $f_\theta(\cdot)$
>  每个 solution step 的均方误差如 Eq 1
>  整个训练数据集 $\mathcal Q$ 上的目标函数如 Eq 2

The OSV output approximates the expected correctness probability, as formalized in the following theorem:
>  结果监督的验证模型的输出近似的期望的正确概率

**Theorem 1** For a model trained with outcome supervision,  $f_{\theta}$ , characterized by optimally tuned parameters  $\theta$ , the assigned score for the sequence  $S^{(1:t)}$  is an estimation of the likelihood of ultimately deriving a correct answer, denoted by  $\hat{a}$ , based on the progression observed in  $S^{(1:t)}$  and the pertinent question  $q$ . This is mathematically represented as:

$$
f_{ {\theta}}(S^{(1:t)};q)\approx p(\hat{a} |S^{(1:t)},q)
$$

The proof follows from optimizing the MSE loss in equation (2), with details in [22].

>  **Theorem 1**
>  使用结果监督训练的模型 $f_\theta$ 为序列 $S^{(1:t)}$ 预测的分数是 (基于 $S^{(1:t)}$ 和 $q$ ) 对该序列最终得到正确答案 ($\hat a$) 的概率的估计

>  推导
>  从统计学角度看，最小化 MSE 时，模型学习到的结果就是给定输入 $X$ 的条件下目标 $Y$ 的条件期望 $E[Y\mid X]$ ，例如在特定输入 (房屋面积、位置) 下，房子的平均预期价格
>  而在原文的背景下，目标 $y_i$ 为二元变量，因此它的条件期望就是它等于 1 的概率值，因此模型 $f_\theta$ 就预测了给定 $q, S^{(1:t)}$ 下，$y_i = 1$ 的概率值，即该序列得到的最终答案 $\hat a$ 正确的概率

**Process-Supervision** We compute the relative confidence change between steps:

$$
\Delta_{conf}^{t} = \frac{f_{\pmb{\theta}}(S^{(1:t + 1)};q) -f_{\pmb{\theta}}(S^{(1:t)};q)}{f_{\pmb{\theta}}(S^{(1:t)};q)} \tag{3}
$$

$\Delta_{conf}^{t}$  represents the relative variation in the model's confidence score from step  $t$  to step  $t + 1$ . A negative value of  $\Delta_{conf}^{t}$  signifies a reduced confidence in achieving a correct answer after incorporating information from the  $(t + 1)$ -th step. 

>  **Process Supervision**
>  我们按照 Eq 5 计算推理步之间，例如 $t$ 到 $t+1$ 步的相对置信度变化 $\Delta_{conf}^t$
>  $\Delta_{conf}^t$ 为负值表示 $t+1$ 步的推理减少了达到正确答案的置信度

We denote the process label for the  $t$ -th step as  $y_{i}^{t}$  and  $\theta$  as the variation threshold.

For process labeling, we employ the "first error location" strategy [15] with threshold  $\theta$ :
- If  $\Delta_{conf}^{t} > \theta$ :  $y_{i}^{t} = 1$  
- Otherwise:  $y_{i}^{t} = 0$  and  $\forall t' > t$ ,  $y_{i}^{t'} = 0$

>  我们采用 “第一错误位置” 策略来进行过程标记:
>  - 如果相对置信度变化 $\Delta_{conf}^t > \theta$，则这一步被标记为 $1$，即 $y_i^t = 1$，意味着这一步是有价值的，可以增大导向正确答案的概率
>  - 否则，将这一步标记为 $0$，并且一旦某个步骤被标记为 $0$，该步骤后的所有步骤都标记为 $0$
>  第一错误位置策略假设一旦模型犯了错误 (导致置信度下降)，那么后续步骤无论如何生成，都无法纠正这个错误位置，因此就将这个步骤的所有后续步骤都视作错误的

The process-supervision loss function is:

$$
\mathcal{L}_{proc}(S_i^{(1:t)},y_i^t;q) = \left(f_{\pmb{\theta}}(S_i^{(1:t)};q) -y_i^t\right)^2 \tag{4}
$$

>  有了过程监督标记后，我们就可以将原来损失函数中的标记替换为对应的过程监督标记，得到 Eq 4 的过程监督损失函数
>  该损失函数让模型生成的预测分数去匹配自动生成的过程标签

>  感觉存在进一步理论优化的空间，直接优化？

# 4 Preliminary Findings
In this section, we present our findings aiming to validate two key aspects: In Section 4.1, we present a comprehensive analysis of the OSV model, i.e., to validate that the initially trained OSV model is effective and robust. In Section 4.2, we further introduce a self-designed benchmark for process errors and calculate  $\Delta_{conf}^{t}$  to detect these errors, i.e., to demonstrate the effectiveness and reliability of relative step-level confidence change in the proposed method. 
>  我们通过试验验证两个方面:
>  1. 分析结果监督验证模型，以验证最初训练得到的结果监督验证模型是健壮且高效的
>  2. 引入存在过程错误的 benchmark，并通过计算 $\Delta_{conf}^t$ 来检测这些错误，以展示 step-level 置信度变化的有效性和可靠性

The validation of these two components serves as a foundation for automatic process labeling via AUTOPSV, as described in Section 3.3.

## 4.1 Experiment on Outcome-Supervised Verifier Performance
In this section, we validate the effectiveness and scalability of the OSV model. Initially, we fine-tune a pretrained language model using ground truth data from the GSM8K dataset. Then, we use this fine-tuned model to generate multiple response samples for the GSM8K training prompts. We label these samples based on the correctness of their final answers. After this, we train an OSV model using the method described in Eq. (1).
>  本节验证结果监督验证模型的有效性
>  我们使用 GSM8K 数据集对语言模型进行微调，然后用微调后的语言模为 GSM8K 的 training prompts 生成多个 response samples
>  我们基于最终答案的正确性对这些 samples 进行标记，在标记后，我们利用 Eq 1 描述的方法 (给定部分序列，预测其结果正确的概率) 训练结果监督验证模型

To evaluate the OSV, we measure its ability to select a sample with the correct final answer from samples generated by various LLMs, denoted as the Response Generator. Specifically, our task involves selecting the correct candidate from five responses.  We assess the effectiveness of outcome supervision on two models: Phi2 (OSV (Phi)) [35] and Mistral-7B (OSV (Mistral)) [32].
>  我们度量 OSV 模型从 Response Generator 生成的多个答案中选择出正确的样本的能力
>  我们的任务为从 5 个 responses 中选择出正确的 1 个
>  我们评估了两个 OSV 模型的有效性

To explore the scalability of this outcome-supervised verifier effect, we choose Response Generators of varying scales, ranging from 7B to 72B parameters, i.e., Mistral-7B-Instruct (Mistral-Instruct) [32], Mixtral  $8\times 7\mathrm{B}$  (Mixtral-Instruct) [31] and Qwen-72B-Chat (Qwen) [34]. This allows us to check the OSV's generalized ranking capability across different LLM scales.
 >  为了探究 OSV 效果的可拓展性，我们选择了不同 scale 的 Response Generator，从 7B 到 70B (有点莫名，应该选择不同 scale 的 OSV 模型才比较合理吧)，以探索 OSV 在不同的 LLM 规模下的泛用排序能力 (非常牵强感觉)

Table 3: Performance of OSV models across different configurations.  

<table><tr><td>Response Generator</td><td>Pass@1</td><td>Pass@5</td><td>SC</td><td>OSV (Mistral)</td><td>OSV (Phi)</td></tr><tr><td>Mistral-Instruct</td><td>42.08</td><td>69.90</td><td>50.03</td><td>60.72</td><td>52.61</td></tr><tr><td>Mixtral-Instruct</td><td>62.55</td><td>82.31</td><td>69.06</td><td>74.07</td><td>69.37</td></tr><tr><td>Qwen</td><td>77.03</td><td>91.13</td><td>81.27</td><td>85.00</td><td>84.19</td></tr></table>

The results demonstrate the effectiveness and scalability of the OSV model in selecting the correct response among multiple responses generated by different generators. Specifically, the OSV models, trained using either Mistral or Phi, consistently outperform the self-consistency (SC) baseline across all generator configurations. The results validate the effectiveness of the OSV model in enhancing model selection strategies, particularly when applied to larger and more accurate LLM generators.
>  结果见 Table 3，可以看到不论 Response Generator 是什么模型，OSV 模型的表现都由于 self-consistency 模型，并且在当 Response Generator 的规模更大时，OSV 的优势更大一点

We further analyze the performance discrepancy between the two OSV models:

**Performance Analysis of Different OSVs** The performance disparity among the verifiers can be attributed primarily to variations in model sizes and the quality of their training data. It's important to note that the OSV model is continuously trained from the GSM8K fine-tuned model with the addition of a value head. This means that the training data for each OSV model is generated from its corresponding fine-tuned base model. Specifically, the training data for OSV (Mistral) is generated from the fine-tuned Mistral model, while the training data for OSV (Phi) is generated from the fine-tuned Phi model.

Table 4: Model sizes and training data accuracy for training OSVs.  

<table><tr><td rowspan="2">Verifier</td><td rowspan="2">Size</td><td colspan="2">Training Data</td></tr><tr><td>Quality (acc.%)</td><td>Quantity (per question)</td></tr><tr><td>OSV (Mistral)</td><td>7B</td><td>0.9914</td><td>100</td></tr><tr><td>OSV (Phi)</td><td>2.7B</td><td>0.9905</td><td>100</td></tr></table>

Table 4 presents a consolidated view of the model sizes along with the precision metrics of their outcome supervision training data.

>  我们进一步分析了两个 OSV 模型的性能差异，我们认为二者的性能差异主要归结于模型大小的不同和训练数据质量的不同
>  要注意 OSV 模型的训练数据 (responses) 是由它对应的微调过的 base 模型生成的，即 Mistral OSV 的数据由 fine-tuned Mistral 生成，Phi OSV 由 fine-tuned Phi 生成

It is worth noting that while both models are trained on the same quantity of data per question (100 samples), the quality of this data differs due to the capabilities of their respective base models. The Mistral model, being larger and potentially more capable, generates higher quality training data for its OSV, which in turn leads to better performance. In our content, we select the OSV (Mistral) model as the OSV model among other experiment settings due to its superior performance, as demonstrated Table 3.

## 4.2 Detecting Calculation Error During Math Reasoning
In this section, we verify the effectiveness and reliability of our method AUTOPSV. Specifically, We calculate  $\Delta_{conf}^t$  to identify inaccuracies in the process, as outlined in Eq. (3).
>  本节验证 $\Delta_{conf}^t$ 在识别过程错误的有效性和可靠性

In Section 4.2.1, we introduce the concept of math calculation error and establish a preliminary benchmark. In Section 4.2.2, we assess the performance of calculating  $\Delta_{conf}^t$  to detect calculation errors against our established benchmark.

### 4.2.1 Math Calculation Error
We outline a method for identifying instances of math calculation error, which we define as occurrences where the numerical values on either side of an equals sign within a mathematical expression do not align. This misalignment indicates a breakdown in logical reasoning, categorizing the instance as a calculation error in the context of mathematical problem-solving. This process establishes a benchmark for math calculation error detection with more details in Appendix E.
>  我们使用 `=` 两边的值没有在计算上对齐来表示 math calculation error
>  我们基于这个思路构建了一个 math calculation error detection benchmark

**Math Calculation Error Detection** To identify calculation errors in mathematical reasoning, we monitor the relative step-level confidence changes between the intermediate steps as defined in Eq. (3). if  $\Delta_{conf}^t\leq \theta$  , we view the step as "incorrect". We also provide a detection example in Figure 2 for better understanding.
>  我们通过监督 $\Delta_{conf}^t$ 的值来辨识 calculation error，即如果 $\Delta_{conf}^t \lt \theta$，我们就称这一步为 “incorrect”

### 4.2.2 Quantitative Results
We introduce three metrics for a thorough evaluation of math calculation error detection: Precision (Prec.), which calculates the proportion of samples with correct final answers but exhibiting hallucinatory errors during the reasoning process; Recall, which determines the proportion of samples with math calculation errors that the OSV model successfully identifies through step-level confidence changes; F1-score, which gauges the verifier's overall efficacy. 
>  我们为 math calculation error detection 引入了三个指标: 
>  - Precision: 表示最终答案正确但是在推理过程中出现幻觉的比例 (应该是 OSV 识别出的样本当中·，真正有错误的样本所占的比例)
>  - Recall: 表示 OSV 模型通过 $\Delta_{conf}^t$ 的变化成功识别出的具有错误的样本的比例
>  - F1 score: 衡量 OSV 的整体有效性

In Table 5, we explore how different threshold  $(\theta)$  values affect the precision, recall, and F1-score for math calculation error detection.

Table 5: Process Calculation Error Detection Performance with Varying Threshold  $(\theta)$  Values.  

<center><table><tr><td rowspan="2">Metric</td><td colspan="5">Threshold (θ) Value</td></tr><tr><td>0.5</td><td>0.6</td><td>0.7</td><td>0.8</td><td>0.9</td></tr><tr><td>Prec.</td><td>0.85</td><td>0.88</td><td>0.91</td><td>0.93</td><td>0.94</td></tr><tr><td>Recall</td><td>0.90</td><td>0.89</td><td>0.86</td><td>0.83</td><td>0.80</td></tr><tr><td>F1-Score</td><td>0.88</td><td>0.89</td><td>0.88</td><td>0.88</td><td>0.86</td></tr></table></center>

The results in Table 5 demonstrate that our method using step-level confidence change effectively detects calculation errors across threshold values from -0.5 to -0.9. As the threshold becomes more negative (stricter for labeling errors), the precision increases, indicating higher precision in identifying true errors. However, the recall decreases, meaning fewer actual errors are caught. Importantly, the F1-score, balancing precision and recall, remains relatively stable across thresholds. This demonstrates that our method strikes a good balance between detecting real errors and minimizing incorrect flagging of valid calculations. Overall, our detection method is effective and robust, performing well over a range of thresholds without significantly compromising overall detection quality.
>  Table 5 展示了使用 step-level confidence change 探查错误的有效性
>  随着阈值增大，precision 增大，recall 减小，在不同的阈值下，recall 都维持相对稳定

We note that setting  $\theta = -0.5$  in our detection methods helps maintain a balance between precision and recall, which can ensure a balanced distribution of labeled "incorrect' and "correct' responses.

**Validation and Foundation for AUTOPSV** Our empirical validation of the OSV model encompasses two key aspects: its efficacy in response selection (Section 4.1) and its capability in detecting calculation errors (Section 4.2). These experimental results provide a robust foundation for automating process annotations using AUTOPSV. 
>  我们对 OSV 的经验性验证包含了两个关键点:
>  - OSV 可以进行有效的 response selection (这是它的训练目的)
>  - OSV 可以进行有效的 process error detection (检测 calculation error)
>  这两个试验结果支持了 AUTOPSV 进行过程标记的合理性

Furthermore, Theorem 1 establishes the theoretical framework for utilizing OSV to estimate the probability of reaching correct final answers from any given intermediate reasoning step. 
>  此外，Theorem 1 也表明了，训练 OSV 的方式在理论上可以达到对从任意中间推理步骤对正确最终答案是否正确的概率估计

This convergence of theoretical guarantees and empirical evidence provides the methodological groundwork for applying AUTOPSV to generate process-supervised training data in our subsequent experiments.

# 5 Experiment
In this section, we first introduce the experimental setup in a subsection, which includes the response generator LLMs and evaluation settings in Section 5.1. We then present the main result of our process supervision-enhanced verification model on both mathematical and commonsense reasoning benchmarks, as described in Section 5.2.
>  本节介绍过程监督强化的验证模型在数学和常识推理任务上的有效性

## 5.1 Experimental Setup
**Models:** We selected three instruction-tuned LLMs of varying sizes, ranging from 7 billion to over 70 billion parameters, to serve as the response generator. Specifically, we used Mistral-Instruct-7B (Mistral-Instruct), Mixtral-8x7B-Instruct-v0.1 (Mixtral-Instruct), and Qwen-72B-Chat (Qwen).
>  **Models**
>  Response Generators 的大小从 7B 到 70B 都有

**Datasets:** Our evaluation encompasses five benchmarks across two reasoning domains: For mathematical reasoning, we include GSM8K [21], a dataset containing math word problems requiring multi-step reasoning, and MATH [36], composed of high school-level competition problems covering a range of math subjects. For commonsense reasoning, we use HellaSwag [37], a dataset for physically situated commonsense reasoning, Winogrande [38], fill-in-the-blank problems requiring commonsense pronoun resolution and ANLI [39], a dataset for natural language understanding and reasoning.
>  **Datasets**
>  包括了 5 个数据集，数学推理使用 GSM8K, MATH；常识推理使用 HellaSwag, Winogrande, ANLI

**Evaluation:** For evaluation, we follow the methodology outlined in [40] to ensure consistency across benchmarks. Our protocol involves:

(i) Our object is to select the correct answer from five candidate responses. (ii) For OSV models, we use the final value assigned to the whole solution for evaluation, while for PSV models, we utilize the product of step-level scores as the aggregation function. 
(iii) To obtain more reliable pass@k results, we implement the estimation method described in [41]. This involves generating n samples per task (where  ${n} > {k}$  ) and assessing the number of correct samples that pass unit tests. We then calculate the unbiased estimator for pass@k. For self-consistency (SelfCons.) and verifier results, we randomly select  ${k}$  out of n samples and perform separate calculations. All results are reported with an accuracy of  $\pm 0.1$  at a  $95\%$  confidence level.

>  **Evaluation**
>  - 我们的目标是从 5 个候选 responses 选择出正确的一个
>  - OSV 模型使用对整个 response 的 final value 作为标准，PSV 则使用 step-level scores 的乘积作为标准
>  - 我们为每个任务生成 n 个样本，然后评估通过单元测试的正确样本数，然后计算 `pass@k` 的无偏估计值

Additional details regarding generation hyperparameters and different aggregation functions are provided in Appendix F.2.

## 5.2 Enhanced LLMs Reasoning via Process Supervision
To evaluate the efficacy and scalability of our proposed approach (detailed in Section 5.2), we conducted comprehensive experiments across mathematics and commonsense reasoning tasks using five diverse datasets.

Our experimental framework employs an autonomous process-supervision data annotation method where we calculate  $\Delta_{conf}^t$  based on model confidence from OSV, using a threshold of  $\theta = -0.5$  . This annotated data is then utilized to continuously fine-tune the OSV model, resulting in our enhanced  $\mathrm{OSV} + \mathrm{PSV}$  model.
>  我们使用 OSV 给出的 $\Delta_{conf}^t$ 作为过程监督标签，阈值定位 $\theta = -0.5$，标记后的过程数据会用于微调 OSV 模型，记作 OSV + PSV 模型

We evaluate three distinct approaches: (i) Self-Consistency (Self-Cons.): Our baseline approach (ii) Outcome-supervised verifier (OSV): Our initial verification model (iii) Process-supervised enhanced verifier OSV + PSV  : Our proposed enhancement. For each approach, we assess Pass@5 performance, which represents the upper limit of achievable performance on these benchmarks.

**Mathematics Reasoning:** As shown in Table 6, the process-supervised enhanced verifier demonstrates superior performance over the outcome-supervised verifier and Self-Consistency models for all evaluated response generators on GSM8K. For the MATH benchmark, the process-supervised enhanced verifier outperforms the other two approaches for Mistral-Instruct and Mixtral-Instruct, but it is slightly less effective than the Self-Consistency model when applied to Qwen-72b.

Table 6: Results on mathematics benchmarks.  

<table><tr><td rowspan="2">Response Generator</td><td colspan="4">GSM8K</td><td colspan="4">MATH</td></tr><tr><td>Pass@5</td><td>Self-Cons.</td><td>OSV</td><td>OSV + PSV</td><td>Pass@5</td><td>Self-Cons.</td><td>OSV</td><td>OSV + PSV</td></tr><tr><td>Mistral-Instruct</td><td>69.90</td><td>50.03</td><td>61.18</td><td>61.41</td><td>7.7</td><td>1.64</td><td>5.10</td><td>5.30</td></tr><tr><td>Mistral-Instruct</td><td>82.30</td><td>69.06</td><td>74.91</td><td>76.04</td><td>22.80</td><td>10.66</td><td>15.2</td><td>16.92</td></tr><tr><td>Qwen</td><td>91.13</td><td>81.27</td><td>84.91</td><td>85.15</td><td>56.10</td><td>40.10</td><td>38.94</td><td>39.36</td></tr></table>

**Commonsense Reasoning:** According to Table 7,  $\mathrm{OSV + PSV}$  again leads to the best results among the three methods for each response generator tested on HellaSwag. For Winogrande, Mistral-Instruct paired with  $\mathrm{OSV + PSV}$  achieves the highest performance, whereas, for Mixtral-Instruct and Qwen72b, the original OSV without process supervision has a marginal advantage. When looking at the results of the ANLI benchmark,  $\mathrm{OSV + PSV}$  is the highest-performing method for Mistral-Instruct and Mixtral-Instruct. Despite this, for Qwen-72b, the OSV model alone falls slightly behind the integrated  $\mathrm{OSV + PSV}$

Table 7: Results on commonsense reasoning benchmarks.  

<table><tr><td rowspan="2">Response Generator</td><td colspan="4">HellaSwag</td><td colspan="4">Winogrande</td><td colspan="4">ANLI</td></tr><tr><td>Pass@5</td><td>Self-Cons.</td><td>OSV</td><td>OSV + PSV</td><td>Pass@5</td><td>Self-Cons.</td><td>OSV</td><td>OSV + PSV</td><td>Pass@5</td><td>Self-Cons.</td><td>OSV</td><td>OSV + PSV</td></tr><tr><td>Mistral-Instruct</td><td>76.84</td><td>40.30</td><td>73.81</td><td>74.45</td><td>91.16</td><td>28.04</td><td>79.16</td><td>79.98</td><td>73.4</td><td>45.6</td><td>59.8</td><td>59.3</td></tr><tr><td>Mistral-Instruct</td><td>84.05</td><td>73.67</td><td>82.83</td><td>83.62</td><td>79.16</td><td>68.75</td><td>73.40</td><td>73.88</td><td>68.4</td><td>59.0</td><td>62.9</td><td>64.0</td></tr><tr><td>Qwen-72b</td><td>95.28</td><td>85.44</td><td>93.08</td><td>93.99</td><td>88.63</td><td>72.21</td><td>80.34</td><td>79.32</td><td>82.4</td><td>63.8</td><td>69.1</td><td>71.4</td></tr></table>

**Conclusion:** Our experimental results indicate that the process-supervised enhanced verifier (OSV  $+\mathrm{PSV})$  consistently outperforms or matches the baseline models across most mathematical and commonsense reasoning tasks. By leveraging automatic process annotations, our approach enhances the model's capacity to verify reasoning processes, resulting in improved accuracy and robustness across a wide range of benchmarks and response generators.
>  结论: OSV + PSV 最优

# 6 Analysis
In Section 6.1, we compare our process annotation method, AUTOPSV with two other model-induced annotation methods to showcase the effectiveness and efficiency of our proposed approach. In Section 6.2, we validate the data quality constructed via AUTOPSV as described in Section 5.2.
>  Section 6.1 将 AUTOPSV 和另外两个模型引导的标记方法进行比较
>  Section 6.2 验证通过 AUTOPSV 构造的数据质量

## 6.1 Advantages of AutoPSV in Labeled and Unlabeled Settings
Aside from the labeling method defined by Eq. (3) in AUTOPSV, another labeling strategy is the Monte Carlo Tree sampling estimation (MCTS), as described in [17, 18]. To better demonstrate the effect of our method, we make a comparison with this approach and conduct experiments on mathematical benchmarks across both labeled and unlabeled settings (i.e., whether the ground-truth answers to the questions are provided).
>  AUTOPSV 通过置信度变化来进行过程标记，而基于 MCTS 的方法通过多次采样和探索来标记每个决策步骤的潜在价值

**Performance in Labeled Settings** We first evaluate the performance of AutoPSV in labeled settings. We follow the experimental settings described in [17, 18] to ensure a fair comparison. More implementation details are provided in Appendix F.3. Table 8 presents a comparison of process labeling methods across different response generators on the GSM8K and MATH datasets.
>  我们先在有标签环境下 (数据集中的每个问题都有正确的已知答案) 评估效果
>  Table 8 展示了在 GSM8K 和 MATH 两个数据集上，先用 MCTS 和 AUTOPSV 两种方法生成过程标注后，再用这些标注微调模型，模型的性能表现
>  评估指标包括了 `pass@5` (衡量生成的 5 个答案中至少一个是正确的概率) 和 `self-cons` (模型自我检查和生成正确答案的概率)

Table 8: Comparison of process labeling methods' performance across different response generators on GSM8K and MATH datasets. The table evaluates the Pass@5  Self-Consistency (Self-Cons.), and response selection performance of models fine-tuned using process annotations labeled by MCTS and AUTOPSV.  

<table><tr><td>Response Generator</td><td>Pass@5</td><td>Self-Cons.</td><td>GSM8K Process (MCTS)</td><td>Process (AUTOPSV)</td><td>Pass@5</td><td>Self-Cons.</td><td>MATH Process (MCTS)</td><td>Process (AUTOPSV)</td></tr><tr><td>Mistral-Instruct</td><td>69.90</td><td>50.03</td><td>54.13</td><td>53.32</td><td>7.7</td><td>1.64</td><td>3.3</td><td>3.24</td></tr><tr><td>Mixtral-Instruct</td><td>82.30</td><td>69.06</td><td>72.36</td><td>72.12</td><td>22.80</td><td>10.66</td><td>12.18</td><td>12.54</td></tr><tr><td>Qwen-72b</td><td>91.13</td><td>81.27</td><td>82.17</td><td>82.83</td><td>56.10</td><td>40.10</td><td>36.88</td><td>37.10</td></tr></table>

The experimental results shown in Table 8 suggest that our proposed method for process labeling, which relies on detecting changes in model confidence, performs competitively with the MCTS method from [18, 17]. In some cases, our method even outperforms the MCTS method, especially on the more challenging MATH benchmark.

A key advantage of AutoPSV is its computational efficiency. As shown in Table 9, our method requires significantly fewer tokens for process labeling compared to MCTS-based methods.

Table 9: Comparison of annotation costs between MCTS and AUTOPSV for process labeling on the GSM8K and MATH datasets. Annotation cost represents the processed tokens into a model when generating process annotations, encompassing both input and output tokens.  

<table><tr><td>Dataset</td><td>#Questions</td><td>#Steps(Avg.)</td><td>#Steps(Overall)</td><td>#Token(Avg.)</td><td>#Tokens(Overall)</td><td>Annotation Cost Process (MCTS)</td><td>Process (AUTOPSV)</td></tr><tr><td>GSM8K</td><td>7,473</td><td>4.47</td><td>334,358</td><td>126</td><td>9,379,258</td><td>2,808</td><td>127</td></tr><tr><td>MATH</td><td>7,498</td><td>16.00</td><td>1,200,177</td><td>272</td><td>1,621,515,894</td><td>21,626</td><td>273</td></tr></table>

This efficiency stems from AutoPSV's ability to generate process annotations without requiring multiple samples for each reasoning step, making it particularly suitable for large-scale applications or scenarios with limited computational resources.

>  在生成过程标记时，AUTOPSV 也比 MCTS 快，因为模型处理的 token 数量更少 (AUTOPSV 只需要通过训练好的验证模型，一步一步评估置信度变化来进行标注，不需要在每个推理步骤生成样本)

**Performance in Unlabeled Settings** To further demonstrate the flexibility of AutoPSV, we evaluate its performance in unlabeled settings. We generated an additional dataset of 7,000 unlabeled math problems using the Evol-Instruct method from WizardLM [42]. These problems, created by LLMs without accompanying gold solutions, represent a challenging scenario for traditional supervision methods. We then conducted an experiment incorporating these unlabeled questions alongside the GSM8K dataset. Table 10 compares various methods across different response generators. In this context,  $\mathrm{OSV + PSV}$  (GSM8K) refers to the original AutoPSV setting, while  $\mathrm{OSV + PSV}$  (GSM8K+WizardLM) includes process annotations sourced from both GSM8K and WizardLM unlabeled questions. Notably, both MCTS and OSV-only training cannot leverage these unlabeled data, highlighting another key advantage of AutoPSV.
>  我们进一步评估无标签设定下的性能，在没有结果标签的情况下，AUTOPSV 仍然可以利用训练好的验证模型，通过置信度变化来进行过程标记，这意味着即便我们不知道最终答案是否正确，我们也可以识别过程中的显著错误

Table 10: Performance enhancement of our proposed AutoPSV method in unlabeled settings, where both MCTS and OSV-only training are unable to utilize unlabeled data.  

<table><tr><td>Response Generator</td><td>Pass@5</td><td>Self-Cons.</td><td>OSV (GSM8K)</td><td>MCTS (GSM8K)</td><td>OSV+PSV (GSM8K)</td><td>OSV+PSV (GSM8K+WizardLM)</td></tr><tr><td>Mistral-Instruct</td><td>69.90</td><td>50.03</td><td>61.18</td><td>60.82</td><td>61.41</td><td>63.11</td></tr><tr><td>Mixtral-Instruct</td><td>82.30</td><td>69.06</td><td>74.91</td><td>75.10</td><td>76.04</td><td>78.15</td></tr><tr><td>Qwen</td><td>91.13</td><td>81.27</td><td>84.91</td><td>84.85</td><td>85.15</td><td>86.77</td></tr></table>

The results demonstrate that the addition of unlabeled data leads to noticeable improvements across all response generators. For instance, the performance of Mistral-Instruct improves from 61.18 (OSV) to 63.11 (OSV+PSV with GSM8K+WizardLM). These results further underscore the value of the AutoPSV approach, particularly its ability to effectively utilize unlabeled data for enhanced performance.

In summary, AutoPSV offers several key advantages over MCTS-based methods:

**Consistent Performance with Computational Efficiency:** AutoPSV demonstrates robust, consistent improvements across different response generators and datasets. Its ability to efficiently generate process annotations—without the need for extensive sampling like MCTS—makes it particularly well-suited for large-scale applications and environments with limited computational resources.

**Leveraging Unlabeled Data for Enhanced Performance:** Unlike MCTS, which relies on ground truth labels, AutoPSV can effectively utilize unlabeled data. This capability not only enhances model performance in real-world settings but also offers scalability in scenarios where labeled data is scarce. The flexibility to harness unlabeled data ensures that AutoPSV can drive significant improvements even in data-constrained situations.

>  AUTOPSV 的优势: 验证模型训练好之后，没有结果标签也可以生成过程标签；生成的过程标签质量有保证 (根据之前的试验结果)；生成过程标签时不需要额外的计算开销 (处理的 tokens 数量)

## 6.2 Outcome-Supervised Verification vs. Process-Supervised Verification
We apply the OSV to relabel the process-supervised training data as in Section 5.2. We then retrain a new model using this relabeled data. This experiment highlights the performance gap between outcome-supervised and process-supervised training.

Table 11: Experimental results showing the performance of OSV models across different configurations tested on GSM8K test sets.  

<table><tr><td>Response Generator</td><td>Pass@1</td><td>Pass@5</td><td>SC</td><td>OSV</td><td>PSV</td></tr><tr><td>Mistral-Instruct</td><td>42.08</td><td>69.90</td><td>50.03</td><td>60.72</td><td>59.14</td></tr><tr><td>Mixtral-Instruct</td><td>62.55</td><td>82.31</td><td>69.06</td><td>74.07</td><td>71.39</td></tr><tr><td>Qwen-72b</td><td>77.03</td><td>91.13</td><td>81.27</td><td>85.00</td><td>83.70</td></tr></table>

The experimental results in Table 11 reveal that retraining the model with process supervision from AUTOPSV still yields better performance than self-consistency across three different response generators. We also noticed a small performance gap between the PSV and OSV. It is worth noting that our PSV was trained using data from the OSV. The small performance gap between the PSV and OSV models demonstrates that the relabeled process-supervised training method successfully inherits information from the outcome-supervised model without requiring ground truth annotations. 

>  我们评估了仅仅使用 PSV (而不是 OSV + PSV) 标注的过程数据 (不使用结果数据) 来微调得到的模型表现
>  结果表明仅仅使用 PSV 仍然可以得到比较好的表现，因为 PSV 继承了 OSV 中的 ground-truth 标记信息

This ablation study further provides quality assurance for automatic process labeling via AUTOPSV. Moreover, OSV training is limited to labeled datasets, while AUTOPSV demonstrates superior performance by utilizing both labeled and unlabeled data as shown in Table 10. This comparison further highlights the versatility and effectiveness of AUTOPSV in real-world scenarios where ground truth annotations may be scarce or unavailable.

# 7 Conclusion
In conclusion, we propose a novel method for automatic process labeling in LLMs by detecting relative changes in model confidence. 
>  本文提出了通过检查模型置信度的相对改变来进行自动过程标记的方法

Our experimental results demonstrate that AUTOPSV significantly enhances the precision and scalability of the verifier models in various reasoning tasks, ranging from mathematical to commonsense reasoning. AUTOPSV therefore has the potential to considerably enhance existing LLMs' performance while drastically reducing the need for intensive computation and manual intervention. 

For future work, we aim to utilize the automatically constructed PSV to supervise the generator using step-wise proximal policy optimization, to enhance the accuracy of the generator's output during greedy decoding without the need for subsequent reranking. This avenue of research could lead to even more advancements in the capabilities of LLMs and their application in reasoning tasks. The limitations and broader impact of the paper are discussed in Appendix A and B.

# A Limitations
AUTOPSV is a promising solution for enhancing the reasoning capabilities of LLMs. However, it is important to acknowledge several potential limitations of the method. 

Firstly, while AUTOPSV aims to reduce the need for manual intervention, there is still a risk of inaccurate annotations. The relative step-level confidence change used to produce process annotations is an estimation and may not always accurately represent the actual correctness of a reasoning step. This could compromise the quality of the annotations and, in turn, the effectiveness of the method. 
>  AUTOPSV 可能生成不正确的标记

Secondly, the success of AUTOPSV is heavily dependent on the performance of the verifier. If the model is not accurate enough in its step-level scores, the quality of the process annotations generated by AUTOPSV could be compromised. 
>  AUTOPSV 依赖于验证器模型的性能

Thirdly, AUTOPSV is specifically designed to improve the reasoning capabilities of LLMs. Therefore, its applicability may be limited to tasks that involve complex multi-step reasoning. It is unclear how well the method will scale or generalize to other tasks or domains that do not involve intensive reasoning. This is an important consideration for future research and development of the method.

# B Broader Impact
**Positive Societal Impacts** The proposed AUTOPSV has the potential to bring about several positive societal impacts. By enhancing the reasoning capabilities of LLMs, AUTOPSV can lead to more accurate and reliable information, which in turn can support better decision-making in various sectors, including healthcare, education, and finance. Moreover, AUTOPSV combines the strengths of output supervision and process supervision to automatically annotate reasoning steps, significantly reducing the time, effort, and cost associated with manual annotation. This makes the process of training LLMs more efficient and accessible. Additionally, the process supervision data generated by AUTOPSV can improve the performance and scalability of verification models, allowing for the development of more complex and sophisticated LLMs capable of handling a wider range of tasks and applications.

**Negative Societal Impacts** However, AUTOPSV also presents several potential negative societal impacts. The automation of the annotation process could lead to job displacement for individuals currently employed in this role. There is also a risk that AUTOPSV and the enhanced LLMs could be misused, for instance, to spread misinformation or manipulate public opinion. The increased reliance on LLMs for decision-making could potentially result in a decrease in critical thinking and problem-solving skills among individuals. Furthermore, the use of LLMs in various sectors could lead to privacy and security issues, as these models often require large amounts of data for training.

# C More Related Work
**Learning From Feedback** Improving LLMs through learning from feedback has become a prevalent strategy, notably through reinforcement learning from human feedback, which seeks to align LLMs with human values by refining their outputs based on feedback [43-45]. However, this method faces challenges such as high costs due to manual labor and a lack of real-time feedback capabilities [46, 47]. An alternative strategy involves using self-correcting LLMs, which rely on automated feedback to iteratively adapt and understand the consequences of their actions without heavy reliance on human intervention. This feedback can be derived from outside sources such as other models [48, 49], tools [50, 51], knowledge bases [52, 53], evaluation metrics [54, 55] or generation logits [56].

**External feedback** leverages external perspectives to identify errors and verify factual accuracy, offering insights that may not be recognized by the LLM alone. Conversely, feedback can also be internally generated, where the LLM evaluates and refines its output iteratively until a desired quality is achieved [57-60]. This self-improvement mechanism is particularly valuable in scenarios where external feedback is scarce or restricted [61, 62]. However, recent effort [63] suggests that LLMs struggle to independently identify and correct errors through self-generated prompts.

# D Vanilla Evaluation Methods Description
Classification: For this method, the evaluator is presented with multiple answers for a given question and is required to choose the best answer among them. The selection is made based on the evaluator's judgment of which answer most accurately addresses the question or provides the most relevant information.

Given the following question: [question]', and these five answers:

```
1. [answer] 
2. [answer] 
3. [answer] 
4. [answer] 
5. [answer] Which answer is the best? Please provide the number of the best answer. You should strictly follow the output format requirements and not output any other content. Example: Answer [number] is better. Let's Begin!
```

Classification  $+$  COT: In Classification  $+$  COT, the evaluator must not only identify the best answer but also analyze and compare all provided answers before making their decision. This method

requires a deeper examination of the content and context of each answer to determine its quality and relevance to the question.

Given the following question: '[question]', and these five answers:

```
1. [answer]  
2. [answer]  
3. [answer]  
4. [answer]  
5. [answer]  Which answer is the best? Please analyze and compare the provided answers and then identify the number of the best answer. You should strictly follow the output format requirements and not output any other content.  Example: Comparison and Analysis: [analysis]. Best answer: [number]. Let's Begin!
```

Scoring: In the Scoring method, the evaluator assigns a numerical score to each answer based on its quality or relevance to the given question. The scores typically range from 1 to 10, with 10 representing the highest quality or most relevant answer.

Given the following question: '[question]', please score these five answers on a scale from 1 to 10, where 10 is the best:

```
1. [answer]  
2. [answer]  
3. [answer]  
4. [answer]  
5. [answer]  Please provide a score for each answer. You should strictly follow the output format requirements and not output any other content.  Example: Answer i: [score]. Let's Begin!
```

Scoring + COT: Similar to Scoring, Scoring + COT also involves assigning numerical scores to each answer. However, in Scoring_cot, the evaluator is required to provide an analysis of each answer before assigning a score. This analysis informs the scoring process and ensures a more informed evaluation of each answer.

Given the following question: '[question]', please score these five answers on a scale from 1 to 10, where 10 is the best:

```
1. [answer]  
2. [answer]  
3. [answer]  
4. [answer]  
5. [answer]  Please analyze each answer, and then provide a score for each answer. You should strictly follow the output format requirements and not output any other content.  Example: Answer i: analysis: [analysis]. score: [score]. Let's Begin!
```

Example: Answer i: analysis: [analysis]. score: [score]. Let's Begin!

Pairwise Comparison: In this method, the evaluator is presented with pairs of answers for a given question and is tasked with determining which answer is better in each pair. The evaluator compares the content or quality of each answer and selects the one they deem superior. The Pairwise Comparison method differs from other evaluation methods in that it evaluates two candidate answers at a time and chooses the winner to proceed to the next comparison with the next candidate answer. For a set of n candidates, this method conducts n-1 pairwise comparisons. To mitigate the potential order preference bias exhibited by LLMs, we adopt a method similar to [64] which shuffles the positions of two answers during prompting. This ensures a fair evaluation process by eliminating any bias toward the position of the answers.

Given the following question: '[question]', compare each pair of answers and decide which one is better: Compare 1. [answer1] with 2. [answer2] For the comparison, indicate the better answer with its number. You should strictly follow the output format requirements and not output any other content. Example: Answer i is better. Let's Begin!

By employing these different evaluation methods, we aim to comprehensively assess the quality and relevance of the answers generated by our models for various questions. Each method offers a unique perspective and contributes to a more thorough evaluation process.

# E Math Calculation Error Benchmark
**Methodology** We utilize the LlaMA2-chat model (LlaMA) for mathematical reasoning step generation. Using regular expressions, we extract computational steps and evaluate expressions to the left of the "=" sign using Python's eval function to verify their correctness against right-hand side results. We term this process "Math Calculation Error Detection" and present the results in Table 12.

Table 12: Performance of the LlaMA model on GSM8K training data, evaluated through few-shot cot prompting strategy.  

<table><tr><td>Model</td><td>Pass@5 (%)</td><td>Self-Consistency (%)</td><td>Math Calculation Error Detection (%)</td></tr><tr><td>LlaMA</td><td>0.4791</td><td>0.2881</td><td>0.1824</td></tr></table>

**Data Processing** Non-computational expressions (e.g.,  $\mathrm{"x + 1 + 2 = 4"}$  ) that cannot be evaluated due to unsolvability or incorrect formatting are excluded from the ground truth data. This ensures our ground truth accurately reflects only computational errors during reasoning. While DeepMind [15] employs human labelers for "trace errors" annotation, our automated approach using Python's eval function provides a scalable alternative.

**Error Detection** Example To demonstrate our calculation of  $\Delta_{conf}^t$  for math calculation error identification, we present an example in Figure 2. The red highlights indicate calculation errors in the reasoning process, detected through significant decreases in model confidence.

# F Experiment Details
