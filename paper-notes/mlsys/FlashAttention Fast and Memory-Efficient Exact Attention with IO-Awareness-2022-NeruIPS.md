# Abstract 
Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. 
>Transformersåœ¨é•¿åºåˆ—ä¸Šè¿è¡Œç¼“æ…¢ä¸”å ç”¨å¤§é‡å†…å­˜ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ä¸åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å…³ç³»

Approximate attention methods have attempted to address this problem by trading oï¬€ model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms $I O-$ aware â€”accounting for reads and writes between levels of GPU memory. 
> è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•è¯•å›¾é€šè¿‡ç‰ºç‰²æ¨¡å‹è´¨é‡æ¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä½†é€šå¸¸æ— æ³•å®ç°å®é™…é€Ÿåº¦æå‡ (wall-clock speedup)ï¼Œæˆ‘ä»¬è®¤ä¸ºä¸€ä¸ªç¼ºå¤±çš„åŸåˆ™æ˜¯ä½¿æ³¨æ„åŠ›ç®—æ³•å…·æœ‰ IO æ„è¯†â€”â€”è€ƒè™‘ GPU å†…å­˜å±‚æ¬¡ä¹‹é—´çš„è¯»å†™æ“ä½œ

We propose FlashAttention , an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention , showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. 
>æˆ‘ä»¬æå‡º FlashAttentionï¼Œè¿™æ˜¯ä¸€ç§ IO æ„è¯†çš„ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•ï¼Œå®ƒä½¿ç”¨ tiling å‡å°‘ GPU HBM å’Œ GPU ç‰‡ä¸Š SRAM ä¹‹é—´çš„å†…å­˜è¯»å†™æ¬¡æ•°
>æˆ‘ä»¬åˆ†æäº† FlashAttention çš„ IO å¤æ‚åº¦ï¼Œè¡¨æ˜å®ƒæ¯”æ ‡å‡†æ³¨æ„åŠ›éœ€è¦æ›´å°‘çš„ HBM è®¿é—®ï¼Œå¹¶ä¸”å…¶ IO å¤æ‚åº¦å¯¹äºä¸€ç³»åˆ— SRAM å¤§å°éƒ½æ˜¯æœ€ä¼˜çš„

We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.
>æˆ‘ä»¬è¿˜å°† FlashAttention æ‹“å±•åˆ°å—ç¨€ç–æ³¨æ„åŠ›ï¼Œå¾—åˆ°äº†ä¸€ç§æ¯”ä»»ä½•ç°æœ‰è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•éƒ½å¿«çš„è¿‘ä¼¼æ³¨æ„åŠ›ç®—æ³•

 FlashAttention trains Transformers faster than existing baselines: $15\%$ end-to-end wall-clock speedup on BERT-large (seq. length compared to the MLPerf 1.1 training speed record, $3\times$ speedup on GPT-2 (seq. length 1K), and 2.4 Ã— speedup on long-range arena (seq. length 1K-4K). 
>FlashAttention è®­ç»ƒ Transformer çš„é€Ÿåº¦æ¯”ç°æœ‰åŸºçº¿æ›´å¿«ï¼š
>ä¸ MLPerf 1.1 è®­ç»ƒé€Ÿåº¦è®°å½•ç›¸æ¯”ï¼Œåœ¨ BERT-large (åºåˆ—é•¿åº¦ 512) ä¸Šå®ç°äº† 15%çš„ç«¯åˆ°ç«¯å®é™…é€Ÿåº¦æå‡ (end-to-end wall-clock speedup)ï¼›
>GPT-2 (åºåˆ—é•¿åº¦ 1K) çš„é€Ÿåº¦æå‡äº† 3 å€ï¼Œlong-range arena (åºåˆ—é•¿åº¦ 1K-4K) çš„é€Ÿåº¦æå‡äº† 2.4 å€

FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, $61.4\%$ accuracy) and Path-256 (seq. length 64K, $63.1\%$ accuracy). 
>FlashAttention å’Œå—ç¨€ç– FlashAttention ä½¿ Transformer èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„æ¨¡å‹ (GPT-2 ä¸Šçš„å›°æƒ‘åº¦æé«˜äº† 0.7ï¼Œé•¿æ–‡æ¡£åˆ†ç±»æé«˜äº† 6.4 ä¸ªç™¾åˆ†ç‚¹) å’Œå…¨æ–°çš„èƒ½åŠ›ï¼šé¦–æ¬¡å®ç°åœ¨ Path-X æŒ‘æˆ˜ (åºåˆ—é•¿åº¦ 16Kï¼Œå‡†ç¡®ç‡ 61.4%) å’Œ Path-256 (åºåˆ—é•¿åº¦ 64Kï¼Œå‡†ç¡®ç‡ 63.1%) ä¸Šå®ç°æ¯”éšæœºçŒœæµ‹æ›´å¥½çš„æ€§èƒ½çš„ Transformer

# 1 Introduction 
![[FlashAttention-Fig1.png]]

Transformer models [ 82 ] have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger [ 5 ] and deeper [ 83 ], but equipping them with longer context remains difficult [ 80 ], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. 
> Transformer æ¨¡å‹[82]å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ç±»ç­‰åº”ç”¨ä¸­æœ€å¹¿æ³›ä½¿ç”¨çš„æ¶æ„ï¼ŒTransformer å·²ç»å˜å¾—æ›´å¤§[5]å’Œæ›´æ·±[83]ï¼Œä½†æ˜¯è®©å®ƒä»¬å…·æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ (longer context) ä»ç„¶å¾ˆå›°éš¾[80]ï¼Œå› ä¸ºå®ƒä»¬æ ¸å¿ƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—åœ¨åºåˆ—é•¿åº¦ä¸Šå…·æœ‰äºŒæ¬¡æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦

An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences. Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [ 51 , 74 ] to low-rank approximation [ 12 , 50 , 84 ], and their combinations [ 3 , 9 , 92 ]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). 
>ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯ï¼Œä½¿æ³¨æ„åŠ›æ›´å¿«ã€æ›´èŠ‚çœå†…å­˜æ˜¯å¦å¯ä»¥å¸®åŠ©Transformeræ¨¡å‹è§£å†³é•¿åºåˆ—çš„è¿è¡Œæ—¶é—´å’Œå†…å­˜æŒ‘æˆ˜
>è®¸å¤šè¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•æ—¨åœ¨å‡å°‘æ³¨æ„åŠ›çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œè¿™äº›æ–¹æ³•åŒ…æ‹¬ç¨€ç–è¿‘ä¼¼[51,74]åˆ°ä½ç§©è¿‘ä¼¼[12,50,84]ï¼Œä»¥åŠå®ƒä»¬çš„ç»„åˆ[3,9,92]ï¼Œå°½ç®¡è¿™äº›æ–¹æ³•å°†è®¡ç®—éœ€æ±‚å‡å°‘åˆ°çº¿æ€§æˆ–æ¥è¿‘çº¿æ€§çš„åºåˆ—é•¿åº¦ï¼Œä½†å…¶ä¸­è®¸å¤šå¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸æ¯”çš„å®æ—¶é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”æ²¡æœ‰å¾—åˆ°å¹¿æ³›é‡‡ç”¨
>ä¸€ä¸ªä¸»è¦åŸå› æ˜¯å®ƒä»¬ä¸“æ³¨äºFLOPå‡å°‘(è¿™å¯èƒ½ä¸å®æ—¶é€Ÿåº¦æ— å…³)ï¼Œå¹¶ä¸”å€¾å‘äºå¿½ç•¥æ¥è‡ªå†…å­˜è®¿é—®(IO)çš„å¼€é”€

In this paper, we argue that a missing principle is making attention algorithms $I O$ -aware [ 1 ]â€”that is, carefully accounting for reads and writes to diï¬€erent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1 left). 
>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºä¸€ä¸ªè¢«å¿½è§†åŸåˆ™æ˜¯ä½¿æ³¨æ„åŠ›ç®—æ³•IOæ„ŸçŸ¥[1]â€”â€”å³ä»”ç»†è€ƒè™‘å¯¹ä¸åŒçº§åˆ«å¿«é€Ÿå’Œæ…¢é€Ÿå†…å­˜çš„è¯»å†™(ä¾‹å¦‚ï¼Œåœ¨å¿«é€Ÿçš„GPUç‰‡ä¸ŠSRAMå’Œç›¸å¯¹è¾ƒæ…¢çš„GPUé«˜å¸¦å®½å†…å­˜[45]ä¹‹é—´ï¼Œå¦‚Figure 1 leftæ‰€ç¤º)

On modern GPUs, compute speed has out-paced memory speed [ 61 , 62 , 63 ], and most operations in Transformers are bottlenecked by memory accesses [ 43 ]. IO-aware algorithms have been critical for similar memory-bound operations, when reading and writing data can account for a large portion of the runtimeâ€”such as database joins [ 71 ], image processing [ 70 ], numerical linear algebra [ 4 ], and more [ 40 , 85 ]. However, common Python interfaces to deep learning such as PyTorch and Tensorï¬‚ow do not allow fine-grained control of memory access. 
> åœ¨ç°ä»£ GPU ä¸Šï¼Œè®¡ç®—é€Ÿåº¦å·²ç»è¶…è¿‡äº†å†…å­˜é€Ÿåº¦ï¼ŒTransformer ä¸­çš„å¤§å¤šæ•°è¿ç®—éƒ½å—åˆ°å†…å­˜è®¿é—®çš„é™åˆ¶
> å½“è¯»å†™æ•°æ®å¯ä»¥å æ®å¤§éƒ¨åˆ†çš„è¿è¡Œæ—¶é—´æ—¶â€”â€”ä¾‹å¦‚æ•°æ®åº“è¿æ¥ (database joins)[71]ã€å›¾åƒå¤„ç† (image processing)[70]ã€æ•°å€¼çº¿æ€§ä»£æ•°[4]ç­‰[40,85]ï¼ŒIO æ„ŸçŸ¥ç®—æ³•å¯¹äºç±»ä¼¼çš„å—å†…å­˜é™åˆ¶çš„è¿ç®—å°±æ˜¯è‡³å…³é‡è¦çš„ï¼Œç„¶è€Œï¼Œåƒ PyTorch å’Œ Tensorflow è¿™æ ·çš„å¸¸è§ Python æ·±åº¦å­¦ä¹ æ¥å£ä¸å…è®¸å¯¹å†…å­˜è®¿é—®è¿›è¡Œç»†ç²’åº¦æ§åˆ¶

We propose FlashAttention , a new attention algorithm that computes exact attention with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM. This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass.  
>æˆ‘ä»¬æå‡ºäº† FlashAttentionï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œå®ƒä½¿ç”¨æ›´å°‘çš„å†…å­˜è®¿é—®è¿›è¡Œç²¾ç¡®çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯é¿å…è¯»å†™æ³¨æ„åŠ›çŸ©é˜µåˆ° HBM ä¸­ï¼Œè¿™éœ€è¦ï¼š
>1. åœ¨ä¸è®¿é—®æ•´ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è®¡ç®—softmaxå½’çº¦
>2. ä¸ä¸ºåå‘ä¼ æ’­å­˜å‚¨å¤§å‹ä¸­é—´æ³¨æ„åŠ›çŸ©é˜µ

We apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling ). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.
>æˆ‘ä»¬åº”ç”¨ä¸¤ç§æˆç†Ÿçš„æŠ€æœ¯æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼š
>1. æˆ‘ä»¬é‡æ–°ç»„ç»‡æ³¨æ„åŠ›è®¡ç®—ï¼Œå°†è¾“å…¥åˆ†æˆå—ï¼Œå¹¶åœ¨è¾“å…¥å—ä¸Šè¿›è¡Œå¤šæ¬¡ä¼ é€’ (make several passes over input blocks)ï¼Œä»è€Œé€æ­¥æ‰§è¡Œ softmax å½’çº¦ (è¯¥æŠ€æœ¯ä¹Ÿç§°ä¸ºå¹³é“º tiling)
>2. æˆ‘ä»¬å­˜å‚¨å‰å‘ä¼ æ’­ä¸­çš„ softmax å½’ä¸€åŒ–å› å­ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­ä¸­å¿«é€Ÿåœ¨ç‰‡ä¸Šé‡æ–°è®¡ç®—æ³¨æ„åŠ›ï¼Œè¿™æ¯”ä» HBM è¯»å–ä¸­é—´æ³¨æ„åŠ›çŸ©é˜µçš„æ ‡å‡†æ–¹æ³•æ›´å¿«

We implement FlashAttention in CUDA to achieve fine-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6x on GPT-2 [ 67 ], Figure 1 right) and uses less memory â€”linear in sequence lengthâ€”than standard attention, thanks to the massively reduced amount of HBM access.
>æˆ‘ä»¬åœ¨ CUDA ä¸­å®ç°äº† FlashAttentionï¼Œä»¥å®ç°å¯¹å†…å­˜è®¿é—®çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œå¹¶å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œèåˆåˆ°ä¸€ä¸ª GPU å†…æ ¸ä¸­ï¼Œå³ä½¿ç”±äºé‡å¤è®¡ç®— (recomputation) è€Œå¢åŠ äº† FLOPsï¼Œæˆ‘ä»¬çš„ç®—æ³•ç›¸è¾ƒäºæ ‡å‡†æ³¨æ„åŠ›åœ¨è¿è¡Œé€Ÿåº¦ä¸Šæ›´å¿« (åœ¨ GPT-2[67]ä¸Šé«˜è¾¾ 7.6 å€ï¼Œå¦‚ Figure 1 right æ‰€ç¤º) å¹¶ä¸”ä½¿ç”¨æ›´å°‘çš„å†…å­˜â€”â€”çº¿æ€§äºåºåˆ—é•¿åº¦ (linear in sqeuence length)ï¼Œè¿™è¦å½’åŠŸäºå¤§å¤§å‡å°‘çš„ HBM è®¿é—®é‡

We analyze the IO complexity [ 1 ] of FlashAttention , proving that it requires $O(N^{2}d^{2}M^{-1})$ HBM accesses where ğ‘‘ is the head dimension and ğ‘€ is the size of SRAM, as compared to $\Omega(N d+N^{2})$ of standard attention. For typical values of $d$ and $M$ , FlashAttention requires many times fewer HBM accesses compared to standard attention (up to $9\times$ fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes. 
>æˆ‘ä»¬åˆ†æäº† FlashAttention çš„ IO å¤æ‚åº¦[1]ï¼Œè¯æ˜å®ƒéœ€è¦ $O(N^2d^2M^{-1})$ HBM è®¿é—®ï¼Œå…¶ä¸­ $d$ æ˜¯å¤´ç»´åº¦ï¼Œ$M$ æ˜¯ SRAM çš„å¤§å°ï¼Œè€Œæ ‡å‡†æ³¨æ„åŠ›åˆ™éœ€è¦ $\Omega(Nd+N^2)$
>å¯¹äºå…¸å‹çš„ $d$ å’Œ $M$ å€¼ï¼ŒFlashAttention éœ€è¦æ¯”æ ‡å‡†æ³¨æ„åŠ›å°‘å¾—å¤šçš„ HBM è®¿é—® (é«˜è¾¾ 9 å€ï¼Œå¦‚ Figure 2 æ‰€ç¤º)
>æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¸‹ç•Œï¼Œè¡¨æ˜æ²¡æœ‰ç²¾ç¡®çš„æ³¨æ„åŠ›ç®—æ³•å¯ä»¥åœ¨æ‰€æœ‰ SRAM å¤§å°ä¸Šæ¸è¿‘åœ°æ”¹è¿› HBM è®¿é—®æ¬¡æ•°

We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 Ã— faster than even FlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. 
>æˆ‘ä»¬è¿˜å±•ç¤ºäº† FlashAttention é€šè¿‡å…‹æœæ½œåœ¨çš„è¿‘ä¼¼æ³¨æ„åŠ›ç®—æ³•çš„å†…å­˜è®¿é—®å¼€é”€çš„é—®é¢˜ï¼Œå¯ä»¥ä½œä¸ºå®ç°å®ƒä»¬çš„æœ‰ç”¨åŸè¯­ (primitive)
>ä½œä¸ºä¸€ä¸ªæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬å®ç°äº†å—ç¨€ç– FlashAttentionï¼Œè¿™æ˜¯ä¸€ç§ç¨€ç–æ³¨æ„åŠ›ç®—æ³•ï¼Œæ¯” FlashAttention å¿« 2-4 å€ï¼Œå¯æ‰©å±•åˆ° 64k çš„åºåˆ—é•¿åº¦ï¼Œæˆ‘ä»¬è¯æ˜äº†å—ç¨€ç– FlashAttention çš„ IO å¤æ‚åº¦æ¯” FlashAttention å¥½ä¸€ä¸ªä¸ç¨€ç–æ¯”æˆæ¯”ä¾‹çš„å› ç´ 

We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive. 
>æˆ‘ä»¬åœ¨ç¬¬ 5 èŠ‚ä¸­è®¨è®ºäº†å¯¹å…¶ä»–è¿ç®— (å¤š GPU ä¸Šçš„æ³¨æ„åŠ›ã€æ ¸å›å½’ã€å—ç¨€ç–çŸ©é˜µä¹˜æ³•) çš„è¿›ä¸€æ­¥æ‰©å±•

We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. 
>æˆ‘ä»¬ç»éªŒä¸Šåœ°éªŒè¯äº† FlashAttention åŠ é€Ÿäº†æ¨¡å‹è®­ç»ƒå¹¶é€šè¿‡å»ºæ¨¡æ›´é•¿çš„ä¸Šä¸‹æ–‡æé«˜äº†æ¨¡å‹è´¨é‡ï¼Œæˆ‘ä»¬è¿˜å¯¹FlashAttentionå’Œå—ç¨€ç–FlashAttentionçš„è¿è¡Œæ—¶é—´å’Œå†…å­˜å ç”¨(memory footprint)ä¸ä»¥å‰çš„æ³¨æ„åŠ›å®ç°è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œæ¯”è¾ƒ

- Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) $15\%$ faster than the training speed record in MLPerf 1.1 [ 58 ], GPT2 (seq. length 1K) $3\times$ faster than baseline implementations from HuggingFace [ 87 ] and Megatron-LM [ 77 ], and long-range arena (seq. length 1K-4K) 2.4 Ã— faster than baselines. 
>æ›´å¿«çš„æ¨¡å‹è®­ç»ƒ
>FlashAttention åœ¨å®æ—¶æ—¶é—´ä¸­å¯ä»¥æ›´å¿«è®­ç»ƒ Transformer æ¨¡å‹
>æˆ‘ä»¬è®­ç»ƒ BERT-large (åºåˆ—é•¿åº¦ 512) æ¯”åœ¨ MLPerf 1.1[58]ä¸­çš„è®­ç»ƒé€Ÿåº¦è®°å½•å¿« 15%ï¼Œè®­ç»ƒ GPT2 (åºåˆ—é•¿åº¦ 1K) æ¯”åœ¨ HuggingFace[87]å’Œ Megatron-LM[77]ä¸­çš„åŸºçº¿å®ç°å¿« 3 å€ï¼Œé•¿è·ç¦»ç«æŠ€åœº (åºåˆ—é•¿åº¦ 1K-4K) æ¯”åŸºçº¿å¿« 2.4 å€

- Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification [13]. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X [ 80 ] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the first model that can achieve better-than-chance performance on Path-256. 
>æ›´é«˜è´¨é‡çš„æ¨¡å‹
>FlashAttention å°† Transformer æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—ï¼Œè¿™æé«˜äº†å®ƒä»¬çš„è´¨é‡å¹¶å¯ç”¨äº†æ–°åŠŸèƒ½ (enables new capabilities)
>æˆ‘ä»¬åœ¨ GPT-2ä¸Šè§‚å¯Ÿåˆ°å›°æƒ‘åº¦æé«˜äº†0.7ï¼Œåœ¨é•¿æ–‡æ¡£åˆ†ç±»[13]ä¸Šé€šè¿‡å»ºæ¨¡æ›´é•¿çš„åºåˆ—æé«˜äº†6.4ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡ï¼ŒFlashAttention å®ç°äº†ç¬¬ä¸€ä¸ªä»…é€šè¿‡ä½¿ç”¨æ›´é•¿çš„åºåˆ—é•¿åº¦ (16K)ï¼Œå°±èƒ½åœ¨ Path-X[80]æŒ‘æˆ˜ä¸­è¾¾åˆ°æ¯”å¶ç„¶æ›´å¥½çš„æ€§èƒ½ (better-than-chance) çš„ Transformerï¼Œå—ç¨€ç– FlashAttention ä½¿ Transformer èƒ½å¤Ÿæ‰©å±•åˆ°æ›´é•¿çš„åºåˆ— (64K)ï¼Œä»è€Œå®ç°äº†ç¬¬ä¸€ä¸ªå¯ä»¥åœ¨ Path-256ä¸Šè¾¾åˆ°æ¯”å¶ç„¶æ›´å¥½çš„æ€§èƒ½çš„æ¨¡å‹

- Benchmarking Attention. FlashAttention is up to $3\times$ faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of. 
>åŸºå‡†æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—
>FlashAttention åœ¨ä» 128 åˆ° 2K çš„å¸¸è§åºåˆ—é•¿åº¦ä¸Šæ¯”æ ‡å‡†æ³¨æ„åŠ›å®ç°å¿« 3 å€ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ° 64K
>åœ¨åºåˆ—é•¿åº¦å°äº 512 æ—¶ï¼ŒFlashAttention åœ¨é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡æ–¹é¢éƒ½æ¯”ä»»ä½•ç°æœ‰çš„æ³¨æ„åŠ›æ–¹æ³•æ›´å¿«ï¼Œè€Œå¯¹äºè¶…è¿‡ 1K çš„åºåˆ—é•¿åº¦ï¼Œä¸€äº›è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³• (ä¾‹å¦‚ï¼ŒLinformer) å¼€å§‹å˜å¾—æ›´å¿«ï¼Œå¦ä¸€æ–¹é¢ï¼Œå—ç¨€ç– FlashAttention æ¯”æˆ‘ä»¬æ‰€çŸ¥é“çš„æ‰€æœ‰ç°æœ‰è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•éƒ½å¿«

# 2 Background 
We provide some background on the performance characteristics of common deep learning operations on modern hardware (GPUs). We also describe the standard implementation of attention. 
## 2.1 Hardware Performance 
We focus here on GPUs. Performance on other hardware accelerators are similar [46, 48]. 
>æˆ‘ä»¬åœ¨æœ¬å°èŠ‚å…³æ³¨ GPUï¼Œå…¶ä»–ç¡¬ä»¶åŠ é€Ÿå™¨ (hardware accelerators) çš„æ€§èƒ½ä¹Ÿç±»ä¼¼[46, 48]

**GPU Memory Hierarchy.** The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of memory of diï¬€erent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [ 44 , 45 ]. The on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute has gotten faster relative to memory speed [ 61 , 62 , 63 ], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. 
>**GPU Memory Hierarchy**
>GPU å†…å­˜å±‚æ¬¡ç»“æ„ (Figure 1 left) åŒ…æ‹¬ä¸åŒå¤§å°å’Œé€Ÿåº¦çš„å¤šç§å†…å­˜ï¼Œè¾ƒå°çš„å†…å­˜é€Ÿåº¦æ›´å¿«ï¼Œä¾‹å¦‚ï¼ŒA100 GPU æ‹¥æœ‰40-80GB çš„é«˜å¸¦å®½å†…å­˜ (HBM)ï¼Œå¸¦å®½ä¸º1.5-2.0TB/sï¼Œæ¯ä¸ª108ä¸ªæµå¼å¤šå¤„ç†å™¨å„æœ‰192KB çš„ç‰‡ä¸Š SRAMï¼Œå¸¦å®½ä¼°è®¡çº¦ä¸º19TB/s[44, 45]
>ç‰‡ä¸Š SRAM æ¯” HBM å¿«ä¸€ä¸ªæ•°é‡çº§ (an order of magnitude)ï¼Œä½†å¤§å°å°å¾ˆå¤šä¸ªæ•°é‡çº§ï¼Œéšç€è®¡ç®—ç›¸å¯¹äºå†…å­˜é€Ÿåº¦å˜å¾—æ›´å¿«[61, 62, 63]ï¼Œè¿ç®— (operations) è¶Šæ¥è¶Šå—åˆ°å†…å­˜ (HBM) è®¿é—®çš„é™åˆ¶ï¼Œå› æ­¤ï¼Œåˆ©ç”¨å¿«é€Ÿ SRAM å˜å¾—æ›´åŠ é‡è¦

**Execution Model.** GPUs have a massive number of threads to execute an operation (called a kernel). Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM. 
>**Execution Model**
>GPUæœ‰å¤§é‡çš„çº¿ç¨‹æ¥æ‰§è¡Œä¸€ä¸ªè¿ç®—(ç§°ä¸ºå†…æ ¸)ï¼Œæ¯ä¸ªå†…æ ¸ä»HBMåŠ è½½è¾“å…¥åˆ°å¯„å­˜å™¨å’ŒSRAMï¼Œè®¡ç®—ï¼Œç„¶åå°†è¾“å‡ºå†™å›HBM

**Performance characteristics.** Depending on the balance of computation and memory accesses, op- erations can be classified as either compute-bound or memory-bound. This is commonly measured by the arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.
>**Performance characteristics**
>æ ¹æ®è®¡ç®—å’Œå†…å­˜è®¿é—®çš„å¹³è¡¡ï¼Œè¿ç®—å¯ä»¥è¢«åˆ†ç±»ä¸ºè®¡ç®—å—é™(compute-bound)æˆ–å†…å­˜å—é™(memory-bound)ï¼Œè¿™é€šå¸¸é€šè¿‡ç®—æœ¯å¯†åº¦æ¥è¡¡é‡ï¼Œå³æ¯ä¸ªå­—èŠ‚çš„å†…å­˜è®¿é—®çš„ç®—æœ¯æ“ä½œæ•°

 1. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner dimension, and convolution with large number of channels.
 >è®¡ç®—å—é™ (compute-buond)ï¼šæ“ä½œæ‰€éœ€æ—¶é—´ç”±ç®—æœ¯æ“ä½œçš„æ•°é‡å†³å®šï¼Œè€Œè®¿é—® HBM çš„æ—¶é—´è¦å°å¾—å¤šï¼Œå…¸å‹çš„ä¾‹å­åŒ…æ‹¬å…·æœ‰å¤§çš„å†…ç»´åº¦çš„çŸ©é˜µä¹˜æ³•å’Œå…·æœ‰å¤§é‡é€šé“çš„å·ç§¯

 2. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most other operations: elementwise (e.g., activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm). Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations [ 53 , 65 , 75 ]. 
 >å†…å­˜å—é™ (memory-bound)ï¼šæ“ä½œæ‰€éœ€æ—¶é—´ç”±å†…å­˜è®¿é—®æ¬¡æ•°å†³å®šï¼Œè€Œè®¡ç®—æ‰€èŠ±è´¹çš„æ—¶é—´è¦å°å¾—å¤šï¼Œä¾‹å­åŒ…æ‹¬å¤§å¤šæ•°å…¶ä»–æ“ä½œï¼šé€å…ƒç´ æ“ä½œ (ä¾‹å¦‚ï¼Œæ¿€æ´»ï¼Œdropout) å’Œå½’çº¦æ“ä½œ (ä¾‹å¦‚ï¼Œæ±‚å’Œï¼Œsoftmaxï¼Œæ‰¹é‡å½’ä¸€åŒ–ï¼Œå±‚å½’ä¸€åŒ–)

**Kernel fusion.** The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations [53, 65, 75].
>**Kernel funsion**
>åŠ é€Ÿå†…å­˜å—é™æ“ä½œçš„æœ€å¸¸è§æ–¹æ³•æ˜¯å†…æ ¸èåˆï¼šå¦‚æœæœ‰å¤šä¸ªè¿ç®—åº”ç”¨äºåŒä¸€è¾“å…¥ï¼Œåˆ™è¾“å…¥å¯ä»¥ä» HBM åŠ è½½ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªè¿ç®—éƒ½åŠ è½½ä¸€æ¬¡
>ç¼–è¯‘å™¨å¯ä»¥è‡ªåŠ¨èåˆè®¸å¤šé€å…ƒç´ æ“ä½œ[53, 65, 75]ï¼Œç„¶è€Œï¼Œåœ¨æ¨¡å‹è®­ç»ƒçš„èƒŒæ™¯ä¸‹ï¼Œä¸­é—´å€¼ä»éœ€è¦å†™å› HBM ä»¥ä¿å­˜ç”¨äºåå‘ä¼ é€’ (backward pass)ï¼Œé™ä½äº†ç®€å•å†…æ ¸èåˆçš„æœ‰æ•ˆæ€§ (effectiveness of naive kernel fuse)

## 2.2 Standard Attention Implementation 
Given input sequences $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ , where $N$ is the sequence length and $d$ is the head dimension, we want to compute the attention output $\mathbf{O}\in\mathbb{R}^{N\times d}$ : 

$$
\mathbf{S}=\mathbf{Q}\mathbf{K}^{\top}\in\mathbb{R}^{N\times N},\quad\mathbf{P}=\mathrm{softmax}(\mathbf{S})\in\mathbb{R}^{N\times N},\quad\mathbf{O}=\mathbf{P}\mathbf{V}\in\mathbb{R}^{N\times d},
$$ 
where softmax is applied row-wise. 
>ç»™å®šè¾“å…¥åºåˆ— $Q,K,V \in \mathbb R^{N\times d}$ï¼Œå…¶ä¸­ $N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯å¤´ç»´åº¦ï¼Œæˆ‘ä»¬æƒ³è¦è®¡ç®—æ³¨æ„åŠ›è¾“å‡º $O\in \mathbb R^{N\times d}$ï¼Œ
>å…¶ä¸­softmaxæ˜¯é€è¡Œåº”ç”¨çš„(applied row-wise)

Sta attention impleme materi he matrices $\mathbf{S}$ and $\mathbf{P}$ to HBM, which takes $O(N^{2})$ memory. Often $N\gg d$ (e.g., for GPT2, $N=1024$ = and $d=64$ ). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time. 
> æ ‡å‡†æ³¨æ„åŠ›å®ç°å°†çŸ©é˜µ $S$ å’Œ $P$ å­˜å‚¨ (materialize) åˆ° HBM ä¸­ï¼Œè¿™éœ€è¦ $O(N^2)$ çš„å†…å­˜ï¼Œé€šå¸¸æ¥è¯´ $N\gg d$ (ä¾‹å¦‚ï¼Œå¯¹äº GPT2ï¼Œ$N=1024$ï¼Œ$d=64$)
> æˆ‘ä»¬åœ¨ Algorithm 0 ä¸­æè¿°äº†æ ‡å‡†æ³¨æ„åŠ›å®ç°ï¼Œå¯¹äºæ ‡å‡†çš„æ³¨æ„åŠ›å®ç°ï¼Œç”±äºä¸€äº›æˆ–å…¨éƒ¨æ“ä½œæ˜¯å†…å­˜å—é™çš„(ä¾‹å¦‚ï¼Œsoftmax)ï¼Œå¤§é‡çš„å†…å­˜è®¿é—®ä¼šå¯¼è‡´æ…¢çš„å®é™…è¿è¡Œæ—¶é—´

This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to $\mathbf{P}$ . As a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax [77]. 
>è€Œè¿™ä¸ªé—®é¢˜è¿˜ä¼šè¢«åº”ç”¨äºæ³¨æ„åŠ›çŸ©é˜µçš„å…¶ä»–é€å…ƒç´ æ“ä½œè€ŒåŠ å‰§ï¼Œä¾‹å¦‚åº”ç”¨äº $S$ çš„æ©ç (masking)æˆ–åº”ç”¨äº $P$ çš„dropoutï¼Œå› æ­¤ï¼Œå·²ç»æœ‰è®¸å¤šå·¥ä½œå°è¯•èåˆå‡ ä¸ªé€å…ƒç´ æ“ä½œï¼Œä¾‹å¦‚å°†æ©ç ä¸softmaxèåˆ[77]

In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length $N$ . We also compare the number of FLOPs and number of HBM accesses of standard attention and of our method ( FlashAttention ). 
>åœ¨ç¬¬3.2èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ ‡å‡†æ³¨æ„åŠ›å®ç°æ‰§è¡Œçš„ HBM è®¿é—®æ˜¯åºåˆ—é•¿åº¦ $N$ çš„äºŒæ¬¡æ–¹ï¼Œæˆ‘ä»¬è¿˜æ¯”è¾ƒäº†æ ‡å‡†æ³¨æ„åŠ›å’Œæˆ‘ä»¬çš„æ–¹æ³• (FlashAttention) çš„ FLOPs æ•°é‡å’Œ HBM è®¿é—®æ•°é‡

**Algorithm 0** Standard Attention Implementation
**Require:** Matrices $Q,K,V \in \mathbb R^{N\times d}$ in HBM
  1: Load $Q, K$ by blocks from HBM, computes $S = QK^T$, writes $S$ to HBM.
  2: Read $S$ from HBM, compute $P = \text{softmax}(S)$, write $P$ to HBM.
  3: Load $P$ and $V$ by blocks from HBM, compute $O = PV$, write $O$ to HBM.
  4: Return $O$.

# 3 FlashAttention: Algorithm, Analysis, and Extensions 
We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is both memory efficient and faster in wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses compared to standard attention. We further show that FlashAttention can serve as a useful primitive by extending it to handle block-sparse attention. 
>æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨ä½¿ç”¨æ›´å°‘çš„é«˜å¸¦å®½å­˜å‚¨å™¨(HBM)è¯»å†™æ¬¡æ•°å’Œä¸å­˜å‚¨ç”¨äºåå‘ä¼ æ’­çš„å¤§å‹ä¸­é—´çŸ©é˜µçš„æƒ…å†µä¸‹è¿›è¡Œç²¾ç¡®çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¢èŠ‚çœå†…å­˜åˆåœ¨å®é™…æ—¶é—´ä¸Šæ›´å¿«çš„æ³¨æ„åŠ›ç®—æ³•
>æˆ‘ä»¬åˆ†æäº†å…¶I/Oå¤æ‚æ€§ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸æ¯”éœ€è¦æ›´å°‘çš„HBMè®¿é—®ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶æ‰©å±•åˆ°å¤„ç†å—ç¨€ç–æ³¨æ„åŠ›ï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº†FlashAttentionå¯ä»¥ä½œä¸ºä¸€ä¸ªæœ‰ç”¨çš„åŸè¯­

We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward. 
>æœ¬èŠ‚å†…å®¹ä¸“æ³¨äºå‰å‘ä¼ æ’­ï¼Œä»¥ä¾¿äºè§£é‡Šï¼›Appendix BåŒ…å«äº†åå‘ä¼ æ’­çš„è¯¦ç»†ä¿¡æ¯

## 3.1 An Efficient Attention Algorithm With Tiling and Recomputation 
Given the inputs $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ in HBM, we aim to compute the attention output $\mathbf{O}\in\mathbb{R}^{N\times d}$ and write it to HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in $N$ ). 
>ç»™å®šåœ¨HBMä¸­çš„è¾“å…¥ $Q,K,V \in R^{NÃ—d}$ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ³¨æ„åŠ›è¾“å‡º $O \in \mathbb R^{N\times d}$ å¹¶å°†å…¶å†™å…¥HBMï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡å°‘ HBM è®¿é—®æ¬¡æ•°(ä½¿å…¶åœ¨ $N$ çš„æ¬¡æ–¹ä¸‹ä¸ºæ¬¡çº¿æ€§ sub-quadratic in $N$)

We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs $\mathbf{Q},\mathbf{K},\mathbf{V}$ into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end. 
>æˆ‘ä»¬åº”ç”¨ä¸¤ç§æˆç†Ÿçš„æŠ€æœ¯(å¹³é“º tilingã€é‡è®¡ç®— recomputation)æ¥å…‹æœåœ¨æ¬¡çº¿æ€§HBMè®¿é—®ä¸­è®¡ç®—ç²¾ç¡®æ³¨æ„åŠ›çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œè§ Algorithm 1
>å…¶ä¸»è¦æ€æƒ³æ˜¯æˆ‘ä»¬å°†è¾“å…¥ $Q, K, V$ åˆ†æˆå—(split into blocks)ï¼Œå°†å®ƒä»¬ä»æ…¢é€ŸHBMåŠ è½½åˆ°å¿«é€ŸSRAMä¸­ï¼Œç„¶åè®¡ç®—ä¸è¿™äº›å—ç›¸å…³çš„æ³¨æ„åŠ›è¾“å‡ºï¼Œå†åœ¨å°†å®ƒä»¬ç›¸åŠ ä¹‹å‰å°†æ¯ä¸ªå—çš„è¾“å‡ºä¹˜ä»¥æ­£ç¡®çš„å½’ä¸€åŒ–å› å­ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†æ­£ç¡®çš„ç»“æœ

**Tiling.** We compute attention by blocks. Softmax couples columns of $\mathbf{K}$ , so we decompose the large softmax with scaling [51, 60, 66]. 
>æˆ‘ä»¬æŒ‰å—è®¡ç®—æ³¨æ„åŠ›
>Softmaxå°† $K$ çš„åˆ—è€¦åˆåœ¨ä¸€èµ·ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç¼©æ”¾åˆ†è§£äº†å¤§çš„Softmax

For numerical stability, the softmax of vector $x\in\mathbb{R}^{B}$ is computed as: 
>ä¸ºäº†æ•°å€¼ç¨³å®šæ€§(numerical stability)ï¼Œå‘é‡ $x \in \mathbb R^{B}$ çš„Softmaxè®¡ç®—ä¸ºï¼š

$$
m(x):=\max_i x_i,f(x):=[e^{x_1-m(x)}\dots e^{x_B - m(x)}], \mathscr l(x):=\sum_i f(x)_i, \text{softmax}(x):=\frac {f(x)}{\mathscr l(x)}
$$

( è®¡ç®—å‡†ç¡®æ€§ï¼š
$$\text{softmax}(x) = \frac {f(x)}{\mathscr l(x)} = \frac {\frac 1 {e^{m(x)}}[e^{x_1}\dots e^{x_B}]}{\frac 1 {e^{m(x)}}\sum_i e^{x_i}} = \frac {[e^{x_1}\dots e^{x_B}]}{\sum_i e^{x_i}}$$
æ•°å€¼ç¨³å®šæ€§ï¼š
$f(x)_i = e^{x_i- m(x)}$ï¼Œå› ä¸º$x_i - m(x) \le 0$ï¼Œæ•…æ»¡è¶³$0\le f(x)_i \le e^{0} = 1$
$\mathscr l(x) = \sum_i f(x)_i$ï¼Œå› ä¸º $0\le f(x)_i \le 1$ï¼Œæ•…æ»¡è¶³ $0\le \mathscr l(x) \le B$ )

For vectors $x^{(1)},x^{(2)}\in\mathbb{R}^{B}$ , we can decompose the softmax of the concatenated $x=\left[x^{(1)}\;x^{(2)}\right]\in\mathbb{R}^{2B}$ as: 
>å¯¹äºå‘é‡ $x^{(1)}, x^{(2)} \in \mathbb R^B$ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‹¼æ¥çš„ $x = [x^{(1)}, x^{(2)}]\in \mathbb R^{2B}$ çš„softmaxåˆ†è§£ä¸ºï¼š

$$
\begin{align}
m(x) &= m([x^{(1)}\ x^{(2)}]) = \max(m(x^{(1)}), m(x^{(2)})),\\
f(x) &= [e^{m(x^{(1)})- m(x)}f(x^{(1)})\quad e^{m(x^{(2)})- m(x)}f(x^{(2)})],\\
\\
\mathscr l(x)&= \mathscr l([x^{(1)}\ x^{(2)}]) = e^{m(x^{(1)})-m(x)}\mathscr l(x^{(1)}) + e^{m(x^{(2)})-m(x)}\mathscr l(x^{(2)}),\\
\text{softmax}(x)&=\frac {f(x)}{\mathscr l(x)}
\end{align}
$$

( è®¡ç®—å‡†ç¡®æ€§ï¼š
$$
\begin{align}
f(x) &= [e^{m(x^{(1)})- m(x)}f(x^{(1)}), e^{m(x^{(2)})- m(x)}f(x^{(2)})]\\
&=e^{-m(x)}[e^{x^{(1)}_1},\dots ,e^{x^{(1)}_B}, e^{x^{(2)}_1},\dots  ,e^{x^{(2)}_B}]\\
&=e^{-m(x)}[e^{x_1},\dots, e^{x_{2B}}]
\\
\\
\mathscr l(x) &=e^{m(x^{(1)})-m(x)}\mathscr l(x^{(1)}) + e^{m(x^{(2)})-m(x)}\mathscr l(x^{(2)})\\
&=e^{-m{(x)}}(\sum_i e^{x^{(1)}_i} + \sum_i e^{x^{(2)}_i})\\
&=e^{-m(x)}\sum_i e^{x_i}
\\
\\
\text{softmax}(x)&=\frac {f(x)}{\mathscr l(x)}=\frac {[e^{x_1},\dots,e^{x_{2B}}]}{\sum_i e^{x_i}}
\end{align}
$$
æ•°å€¼ç¨³å®šæ€§ï¼š
$f(x) = [e^{m(x^{(1)})- m(x)}f(x^{(1)})\ e^{m(x^{(2)})- m(x)}f(x^{(2)})]$ï¼Œå› ä¸º$m(x^{(j)})- m(x)\le 0(j = 1, 2)$ï¼Œæ•…$0 \le e^{m(x^{(j)})-m(x)}\le 1(j=1,2)$ï¼Œåˆ$0 \le f(x^{(j)})_i \le 1(j=1,2)$ï¼Œæ•…æ»¡è¶³$0 \le f(x)_i \le 1$
$\mathscr l(x) =  \mathscr l([x^{(1)}\ x^{(2)}]) = e^{-m(x)}\sum_i e^{x_i} = \sum_i e^{x_i-m(x)}$ï¼Œå› ä¸º$x_i - m(x)\le 0(i=1,\dots,2B)$ï¼Œæ•…$0\le e^{x_i-m(x)} \le 1$ï¼Œæ•…æ»¡è¶³$0 \le \mathscr l(x) \le 2B$ )

Therefore if we keep track of some extra statistics $(m(x),\ell(x))$ , we can compute softmax one block at a time. We thus split the inputs $\mathbf{Q},\mathbf{K},\mathbf{V}$ into blocks (Algorithm 1 line 3), compute the softmax values along with extra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12). 
>å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è·Ÿè¸ªä¸€äº›é¢å¤–çš„ç»Ÿè®¡æ•°æ® $(m(x), \mathscr l(x))$ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ¬¡è®¡ç®—ä¸€ä¸ªå—çš„ softmax
>å› æ­¤ï¼Œæˆ‘ä»¬å°†è¾“å…¥ $Q,K,V$ åˆ†æˆå—(Algorithm 1 line 3)ï¼Œè®¡ç®—softmaxå€¼ä»¥åŠé¢å¤–çš„ç»Ÿè®¡æ•°æ®(Algorithm 1 line 10)ï¼Œå¹¶ç»„åˆç»“æœ(Algorithm 1 line 12)

![[FlashAttention-Fig2.png]]

**Recomputation.** One of our goals is to not store $O(N^{2})$ intermediate values for the backward pass. The backward pass typically requires the matrices ${\bf S},{\bf P}\in\mathbb{R}^{N\times N}$ to compute the gradients with respect to $\mathbf{Q},\mathbf{K},\mathbf{V}$ . However, by storing the output O and the softmax normalization statistics $(m,\ell)$ , we can recompute the attention matrix S and $\mathbf{P}$ easily in the backward pass from blocks of $\mathbf{Q},\mathbf{K},\mathbf{V}$ in SRAM. This can be seen as a form of selective gradient checkpointing [ 10 , 34 ]. While gradient checkpointing has been suggested to reduce the maximum amount of memory required [ 66 ], all implementations (that we know oï¬€) have to trade speed for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses (Fig. 2). The full backward pass description is in Appendix B. 
>æˆ‘ä»¬çš„ç›®æ ‡ä¹‹ä¸€æ˜¯ä¸éœ€è¦å­˜å‚¨ $O(N^2)$ çš„ä¸­é—´å€¼æ¥ç”¨äºåå‘ä¼ æ’­
>åå‘ä¼ æ’­é€šå¸¸éœ€è¦çŸ©é˜µ $S,P \in \mathbb R^{N\times N}$ æ¥è®¡ç®—ç›¸å¯¹äº $Q, K, V$ çš„æ¢¯åº¦ï¼Œç„¶è€Œï¼Œé€šè¿‡å­˜å‚¨è¾“å‡º $O$ å’Œ softmax å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ® $(m,\mathscr l)$ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åå‘ä¼ æ’­ä¸­ä» SRAM ä¸­çš„ $Q,K,V$ å—è½»æ¾é‡æ–°è®¡ç®—æ³¨æ„åŠ›çŸ©é˜µ $S$ å’Œ $P$
>è¿™å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§é€‰æ‹©æ€§çš„æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè™½ç„¶æ¢¯åº¦æ£€æŸ¥ç‚¹å·²è¢«å»ºè®®ç”¨äºå‡å°‘æ‰€éœ€çš„æœ€å¤§å†…å­˜é‡ï¼Œä½†å…¶æ‰€æœ‰å®ç° (æˆ‘ä»¬çŸ¥é“çš„) éƒ½å¿…é¡»åœ¨é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´è¿›è¡Œæƒè¡¡
>ç›¸æ¯”ä¹‹ä¸‹ï¼Œå³ä½¿æœ‰äº†æ›´å¤šçš„ FLOPsï¼Œæˆ‘ä»¬çš„é‡æ–°è®¡ç®—ç”±äºå‡å°‘äº† HBM è®¿é—® (Figure 2) è€ŒåŠ é€Ÿäº†åå‘ä¼ æ’­ï¼Œå®Œæ•´çš„åå‘ä¼ æ’­æè¿°åœ¨ Appendix B ä¸­

**Implementation details: Kernel fusion.** Tiling enables us to implement our algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout in Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM. 
>å¹³é“ºä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸€ä¸ª CUDA å†…æ ¸ä¸­å®ç°æˆ‘ä»¬çš„ç®—æ³•ï¼ŒåŒ…æ‹¬äº†ä» HBM åŠ è½½è¾“å…¥ï¼Œæ‰§è¡Œæ‰€æœ‰è®¡ç®—æ­¥éª¤ (çŸ©é˜µä¹˜æ³•ã€softmaxã€å¯é€‰çš„æ©è”½å’Œ dropoutã€çŸ©é˜µä¹˜æ³•)ï¼Œç„¶åå°†ç»“æœå†™å› HBM (æ©è”½å’Œ dropout è§ Appendix B)ï¼Œè¿™é¿å…äº†åå¤ä» HBM è¯»å–å’Œå†™å…¥è¾“å…¥å’Œè¾“å‡º

We show FlashAttention â€™s correctness, runtime, and memory requirement (proof in Appendix C). 
>æˆ‘ä»¬å°†å±•ç¤ºFlashAttentionçš„æ­£ç¡®æ€§ã€è¿è¡Œæ—¶é—´ä»¥åŠå†…å­˜éœ€æ±‚(è¯æ˜è§ Appendix C)

**Theorem 1.** Algorithm 1 returns $\mathbf{O}=\mathrm{softmax}(\mathbf{Q}\mathbf{K}^{\top})\mathbf{V}$ with $O(N^{2}d)$ FLOPs and requires $O(N)$ additional memory beyond inputs and output. 
> å®šç†1
> ç®—æ³•1è¿”å› $\mathbf O = \text{softmax}(\mathbf Q \mathbf K^T)\mathbf V$ï¼ŒFLOPs ä¸º $O (N^2 d)$ï¼Œè¾“å…¥è¾“å‡ºä»¥å¤–çš„å†…å­˜éœ€æ±‚ä¸º $O (N)$

![[FlashAttention-Algorithm1.png]]

**Algorithm 1** FlashAttention
**Require:** Matrices $Q,K,V\in \mathbb R^{N\times d}$ in HBM, on-chip SRAM of size $M$.
  1: Set block sizes $B_c = \lceil {\frac M {4d}} \rceil$ï¼Œ$B_r = \min(\lceil {\frac M {4d}} \rceil,d)$.
  2: Initialize $O = (0)_{N\times d}\in \mathbb R^{N\times d}, \mathscr l = (0)_N \in \mathbb R^N, m = (-\infty)_N \in \mathbb R^N$ in HBM.
  3: Divide $Q$ into $T_r = \lceil \frac N {B_r} \rceil$ blocks $Q_1, \dots ,Q_{T_r}$ of size $B_r \times d$ each, and divide $K, V$ into $T_c = \lceil \frac N{B_c} \rceil$ blocks $K_1, \dots, K_{T_c}$ and $V_1, \dots, V_{T_c}$ of size $B_c\times d$ each.
> åˆ’åˆ† $Q, K, V$ï¼Œåˆ’åˆ†æ—¶ä¿æŒåµŒå…¥ç»´åº¦ $d$ ä¸å˜ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $Q$ åˆ’åˆ†å•ä½ä¸º $B_r \times d$ï¼Œ$K, V$ åˆ’åˆ†å•ä½ä¸º $B_c\times d$
> å¾—åˆ° $T_r$ ä¸ª $Q$ å—ï¼Œå¾—åˆ° $T_c$ ä¸ª $K, V$ å—

  4: Divide $O$ into $T_r$ blocks $O_i, \dots, O_{T_r}$ of size $B_r \times d$ each, divide $\mathscr l$ into $T_r$ blocks $\mathscr l_i,\dots, \mathscr l_{T_r}$ of size $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
> åˆ’åˆ† $O$ï¼Œåˆ’åˆ†æ—¶ä¿æŒåµŒå…¥ç»´åº¦ $d$ ä¸å˜ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $O$ åˆ’åˆ†å•ä½ä¸º $B_r \times d$
> å¾—åˆ° $T_r$ ä¸ª $O$ å—ï¼Œåˆå§‹å€¼ä¸ºå…¨é›¶
> åˆ’åˆ† $\mathscr l$ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $\mathscr l$ çš„åˆ’åˆ†å•ä½ä¸º $B_r$
> å¾—åˆ° $T_r$ ä¸ª $\mathscr l$ å—ï¼Œåˆå§‹å€¼ä¸ºå…¨é›¶
> åˆ’åˆ† $m$ï¼Œä»åºåˆ—é•¿åº¦çš„ç»´åº¦åˆ’åˆ†
> $m$ çš„åˆ’åˆ†å•ä½ä¸º $B_r$
> å¾—åˆ° $T_r$ ä¸ª $m$ å—ï¼Œåˆå§‹å€¼ä¸ºå…¨è´Ÿæ— ç©·

  5: **for** $1\le j \le T_c$ **do**
  6:     Load $K_j, V_j$ from HBM to on-chip SRAM.
> å¤–å±‚å¾ªç¯ï¼šè£…è½½ $K, V$ å—åˆ° SRAM
> $K, V$ å—å æ®ç©ºé—´ $2dB_c= 2d\lceil \frac M {4d} \rceil$
> å› ä¸º $\lceil \frac M {4d} \rceil \ge \frac M {4d}$ï¼Œæ•… $2d\lceil \frac M {4d} \rceil \ge \frac M 2$

  7:     **for** $1\le i \le T_r$ **do**
  8:         Load $Q_i, O_i, \mathscr l_i, m_i$ from HBM to on-chip SRAM.
  9:         On chip, computes $S_{ij} = Q_iK^T_j \in \mathbb R^{B_r\times B_c}$.
 10:        On chip, compute $\tilde m_{ij} = \text{rowmax}(S_{ij}) \in \mathbb R^{B_r}$ï¼Œ$\tilde P_{ij} = \exp(S_{ij}-\tilde m_{ij})\in \mathbb R^{B_r\times B_c}$(pointwise)ï¼Œ$\tilde {\mathscr l}_{ij} = \text{rowsum}(\tilde P_{ij}) \in \mathbb R^{B_r}$.
 11:         On chip, compute $m_i^{new} = \max(m_i, \tilde m_{ij})\in \mathbb R^{B_r}, \mathscr l_{i}^{new} = e^{m_i - m_i^{new}}\mathscr l_i + e^{\tilde m_{ij} - m_i^{new}}\tilde {\mathscr l}_{ij} \in \mathbb R^{B_r}$.
 12:        Write $O_i \leftarrow \text{diag}(\mathscr l_i^{new})^{-1}(\text{diag}(\mathscr l_i)e^{m_i - m_i^{new}}O_i+e^{\tilde m_{ij}- m_i^{new}}\tilde P_{ij} V_j)$ to HBM.
 13:       Write $\mathscr l_i \leftarrow \mathscr l_i^{new}, m_i \leftarrow m_i^{new}$ to HBM.
> å†…å±‚å¾ªç¯ï¼šè£…è½½ $Q, O,\mathscr l, m$ å—åˆ° SRAM
> $Q, O$ å—å æ®ç©ºé—´ $2dB_r = 2d\min (\lceil \frac M {4d} \rceil, d)$ï¼Œ$\mathscr l, m$ å—å æ®ç©ºé—´ $2B_r = 2\min (\lceil \frac {M}{4d} \rceil, d)$
> 
> åœ¨ç‰‡ä¸Šè®¡ç®— $S$ å—ï¼š$S = QK^T \in \mathbb R^{B_r\times B_c}$ (score æ˜¯ final çš„)
> æŒ‰è¡Œå–æœ€å¤§å€¼: $\tilde m = \text{rowmax}(S) \in \mathbb R^{B_r}$ï¼Œ
> æŒ‰è¡Œè§„èŒƒåŒ– $S$: $S = S - \tilde m \in \mathbb R^{B_r\times B_c}$ï¼Œ
> å–æŒ‡æ•°: $\tilde P = \exp (S-\tilde m) \in \mathbb R^{B_r \times B_c}$ ï¼Œ($\exp (S-\tilde m) = \frac {\exp (S)}{\exp (\tilde m)}$ï¼Œ$\exp (S)$ æ˜¯ final çš„)
> æŒ‰è¡Œæ±‚å’Œ: $\mathscr {\tilde l} = \text{rowsum}(\tilde P) \in \mathbb R^{B_r}$
>  
> è®¡ç®— $m^{new} = \max (m, \tilde m) \in \mathbb R^{B_r}$ï¼Œå³æ›´æ–°è®°å½•çš„æ¯è¡Œæœ€å¤§å€¼ï¼›
> è®¡ç®— $e^{m - m^{new}}\mathscr l\in \mathbb R^{B_r}$ ï¼Œå³ç”¨æ›´æ–°çš„æœ€å¤§å€¼é‡æ”¾ç¼©ç›®å‰ä¸ºæ­¢ç´¯åŠ çš„å„è¡ŒæŒ‡æ•°å’Œï¼Œ
> è®¡ç®— $e^{\tilde m - m^{new}}\mathscr {\tilde l}\in \mathbb R^{B_r}$ ï¼Œå³ç”¨æ›´æ–°çš„æœ€å¤§å€¼é‡æ”¾ç¼©å½“å‰ $S$ å—çš„å„è¡ŒæŒ‡æ•°å’Œï¼Œ
> è®¡ç®— $\mathscr l^{new} = e^{m-m^{new}}\mathscr l + e^{\tilde m - m^{new}}\mathscr {\tilde l} \in \mathbb R^{B_r}$ï¼Œå³ç´¯åŠ /æ›´æ–°ç›®å‰ä¸ºæ­¢çš„å„è¡ŒæŒ‡æ•°å’Œï¼›
> 
> è®¡ç®— $\text{diag}(\mathscr l) e^{m - m^{new}}O$ï¼Œå¯ä»¥è§†ä¸ºï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œå…ˆä¹˜ä¸Šç›®å‰ä¸ºæ­¢çš„å„è¡ŒæŒ‡æ•°å’Œï¼Œæ¢å¤ç›®å‰ä¸ºæ­¢æ³¨æ„åˆ°çš„æ ·æœ¬çš„æŒ‡æ•°åˆ†æ•°ï¼Œç„¶åç”¨æ›´æ–°çš„æœ€å¤§å€¼é‡æ”¾ç¼©ç›®å‰ä¸ºæ­¢æ³¨æ„åˆ°çš„æ ·æœ¬çš„æŒ‡æ•°åˆ†æ•°ï¼Œæ³¨æ„å¯¹äºæ¯ä¸€è¡Œï¼Œç›®å‰ä¸ºæ­¢æ³¨æ„åˆ°çš„æ ·æœ¬æ•°é‡éšç€å¤–å±‚å¾ªç¯å¢é•¿ï¼›
> è®¡ç®— $e^{\tilde m_{ij} - m_i^{new}}\tilde P_{ij}V_j$ï¼Œå¯ä»¥è§†ä¸ºï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œç”¨æ›´æ–°çš„æœ€å¤§å€¼é‡æ”¾ç¼©å½“å‰å—æ³¨æ„åˆ°çš„æ ·æœ¬çš„æŒ‡æ•°åˆ†æ•°ï¼Œç„¶åæŒ‰ç…§æŒ‡æ•°åˆ†æ•°å¯¹æ³¨æ„åˆ°çš„æ ·æœ¬åŠ æƒæ±‚å’Œï¼›
> è®¡ç®— $\text{diag}(\mathscr l) e^{m - m^{new}}O + e^{\tilde m_{ij} - m_i^{new}}\tilde P_{ij}V_j$ï¼Œå¯ä»¥è§†ä¸ºï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œè¡¥å……æ³¨æ„åˆ°çš„ï¼ˆå½“å‰å—ï¼‰æ ·æœ¬çš„åŠ æƒå’Œï¼›
> è®¡ç®— $\text{diag}(\mathscr l_i^{new})^{-1}(\text{diag}(\mathscr l) e^{m - m^{new}}O + e^{\tilde m_{ij} - m_i^{new}}\tilde P_{ij}V_j)$ï¼Œå¯ä»¥è§†ä¸ºï¼šå¯¹äºæ¯ä¸€è¡Œï¼Œè§„èŒƒåŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆå³é™¤ä»¥å„è¡Œçš„æ”¾ç¼©æŒ‡æ•°åˆ†æ•°å’Œï¼‰ï¼›
>
> å°† $\mathscr l^{new}, m^{new}$ å†™å› HBMï¼Œå³æ›´æ–° $\mathscr l, m$

 14:     **end for**
 15: **end for**
 16: Return $O$.

> å‰å‘ä¼ æ’­çš„å›¾ç¤ºè§[[#Figure Illustration for FlashAttention Forward Algorithm|é™„å½•]]

## 3.2 Analysis: IO Complexity of FlashAttention
We analyze the IO complexity of FlashAttention , showing significant reduction in HBM accesses compared to standard attention. We also provide a lower bound, proving that no exact attention algorithm can asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C. 
>æˆ‘ä»¬åˆ†æäº†FlashAttentionçš„I/Oå¤æ‚æ€§ï¼Œä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸æ¯”ï¼Œå…¶HBMè®¿é—®æ˜¾è‘—å‡å°‘ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªä¸‹ç•Œï¼Œè¯æ˜æ²¡æœ‰ä»»ä½•ç²¾ç¡®çš„æ³¨æ„åŠ›ç®—æ³•å¯ä»¥åœ¨æ‰€æœ‰SRAMå¤§å°ä¸Šæ¸è¿‘åœ°æ”¹å–„HBMè®¿é—®ï¼Œè¯æ˜åœ¨ Appendix Cä¸­

**Theorem 2** Let $N$ be the sequence length, $d$ be the head dimension, and $M$ be size of SRAM with $d\le M \le Nd$. Standard attention (Algorithm 0) requires $\Theta(Nd + N^2)$ HBM accesses, while FlashAttention (Algorithm 1) requires $\Theta(N^2d^2M^{-1})$ HBM accesses.
> å®šç†2
> åºåˆ—é•¿åº¦ $N$ï¼Œå¤´ç»´åº¦ $d$ï¼ŒSRAM å¤§å° $M$ï¼Œæ»¡è¶³ $d\le M \le Nd$
> æ ‡å‡†çš„ attention ç®—æ³•éœ€è¦ $\Theta (Nd + N^2)$ HBM è®¿é—®
> FlashAttention ç®—æ³•éœ€è¦ $\Theta (N^2d^2M^{-1})$ HBM è®¿é—®

For typical values of $d$ (64-128) and $M$ (round 100KB), $d^{2}$ is many times smaller than $M$ , and thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and lower memory footprint, which we validate in Section 4.3. 
>å¯¹äºå…¸å‹çš„ğ‘‘å€¼(64-128)å’Œğ‘€å€¼(å¤§çº¦100KB)ï¼Œ$d^2$ æ¯”ğ‘€å°å¾ˆå¤šå€ï¼Œå› æ­¤FlashAttentionéœ€è¦çš„HBMè®¿é—®æ¬¡æ•°æ¯”æ ‡å‡†å®ç°å°‘å¾ˆå¤šå€ï¼Œè¿™å¸¦æ¥äº†æ›´å¿«çš„æ‰§è¡Œå’Œæ›´ä½çš„å†…å­˜å ç”¨ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬4.3èŠ‚ä¸­éªŒè¯è¿™ä¸€ç‚¹

The main idea of the proof is that given the SRAM size of $M$ , we can load blocks of $\mathbf{K},\mathbf{V}$ of size $\Theta(M)$ each (Algorithm 1 line 6). For each block of $\mathbf{K}$ and $\mathbf{V}$ , we iterate over all blocks of $\mathbf{Q}$ (Algorithm 1 line 8) to compute the intermediate values, resulting in $\Theta(N d M^{-1})$ passes over $\mathbf{Q}$ . Each pass loads Î˜ ( ğ‘ğ‘‘ ) elements, which amounts to $\Theta(N^{2}d^{2}M^{-1})$ HBM accesses.
>è¯æ˜çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œç»™å®šSRAMå¤§å°ğ‘€ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½å¤§å°å„ä¸º $\Theta(M)$ çš„ $K,V$ å—(Algorithm 1 line6)ï¼Œå¯¹äºæ¯ä¸ª $K$ å’Œ $V$ å—ï¼Œæˆ‘ä»¬éå†æ‰€æœ‰ $Q$ å—(Algorithm 1 line 8)ä»¥è®¡ç®—ä¸­é—´å€¼ï¼Œè¿™å°†ä¼šæ€»å…±å¯¹ $Q$ è¿›è¡Œ $\Theta(NdM^{-1}$) æ¬¡éå†ï¼Œæ¯æ¬¡éå†åŠ è½½ $\Theta(Nd)$ ä¸ªå…ƒç´ ï¼Œè¿™ç›¸å½“äº $\Theta(N^2d^2M^{-1})$ æ¬¡HBMè®¿é—®
>(æ ‡å‡†æ³¨æ„åŠ›ç®—æ³•ï¼šæ²¡æœ‰è€ƒè™‘ SRAMï¼Œç›´æ¥è¯»å†™ HBMï¼Œ$K,Q,V$ çš„è¯»å–æ¬¡æ•°ä¸º $\Theta (Nd)$ï¼Œ$S$ çš„è¯»å–æ¬¡æ•°ä¸º $\Theta (N^2)$ï¼Œæ•…æ€»è¯»å–æ¬¡æ•°ä¸º $\Theta(Nd + N^2)$)

We similarly prove that the backward pass of standard attention requires $\Theta(N d+N^{2})$ HBM accesses while the backward pass of FlashAttention requires $\Theta(N^{2}d^{2}M^{-1})$ HBM accesses (Appendix B). 
>ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜æ ‡å‡†æ³¨æ„åŠ›çš„åå‘ä¼ æ’­éœ€è¦ $\Theta(Nd + N^2)$ æ¬¡HBMè®¿é—®ï¼Œè€ŒFlashAttentionçš„åå‘ä¼ æ’­éœ€è¦ $\Theta(N^2d^2M^{-1})$ æ¬¡HBMè®¿é—®(Appendix B)

We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all values of $M$ (the SRAM size) when computing exact attention. 
>æˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªä¸‹ç•Œï¼šåœ¨è®¡ç®—ç²¾ç¡®æ³¨æ„åŠ›æ—¶ï¼Œå¯¹äºæ‰€æœ‰ $M$ å€¼(SRAMå¤§å°)ï¼Œä¸èƒ½æ¸è¿‘åœ°æ”¹å–„HBMè®¿é—®æ¬¡æ•°

**Proposition 3.** 
Let $N$ be the sequence length, ğ‘‘ be the head dimension, and ğ‘€ be size of SRAM with $d\leq M\leq N d$ . There does not exist an algorithm to compute exact attention with $o(N^{2}d^{2}M^{-1})$ HBM accesses for all ğ‘€ in the range $[d,N d]$ . 
> å‘½é¢˜3ï¼š
> åºåˆ—é•¿åº¦ $N$ï¼Œå¤´ç»´åº¦ $d$ï¼ŒSRAM å¤§å° $M$ï¼Œæ»¡è¶³ $d\le M \le Nd$ï¼Œå¯¹äºåœ¨ $[d, Nd]$ èŒƒå›´å†…çš„ $M$ ï¼Œä¸å­˜åœ¨å¯ä»¥ä»¥ $o (N^2 d^2 M^{-1})$ HBM è®¿é—®è®¡ç®—ç²¾ç¡®æ³¨æ„åŠ›çš„ç®—æ³•

The proof relies on the fact that for $M=\Theta(N d)$ , any algorithm must perform $\Omega(N^{2}d^{2}M^{-1})\,=\Omega(N d)$ HBM accesses. This type of lower bound over a subrange of $M$ is common in the streaming algorithms literature [88]. We leave proving parameterized complexity [27] lower bounds in terms of $M$ as exciting future work. 
> è¯æ˜åŸºäºä¸€ä¸ªäº‹å®ï¼šå¯¹äº $M = \Theta (Nd)$ï¼Œä»»æ„ç®—æ³•å¿…é¡»æ‰§è¡Œ $\Omega (N^2 d^2 M^{-1}) = \Omega (Nd)$ æ¬¡ HBM è®¿é—® 
>å‚æ•°åŒ–çš„å¤æ‚æ€§ä¸‹ç•Œåˆ†æç•™å¾…ä¹‹åçš„å·¥ä½œ

![[FlashAttention-Fig2.png]]

We validate that the number of HBM accesses is the main determining factor of attention run-time. In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much faster runtime. In Fig. 2 (middle), we vary the block size $B_{c}$ of FlashAttention , which results in diï¬€erent amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations). Moreover, larger block size will not fit into the small SRAM size. 
> æˆ‘ä»¬å°†éªŒè¯ HBM çš„è®¿é—®æ¬¡æ•°å°†æ˜¯ attention è¿è¡Œæ—¶é—´çš„ä¸»è¦å†³å®šå› ç´ 
> Fig2 leftä¸­ï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒFlashAttention å¯¹æ¯”äºæ ‡å‡† attention è®¡ç®—æœ‰æ›´å¤šçš„ FLOP æ•°é‡ (åå‘ä¼ æ’­ä¸­çš„é‡è®¡ç®—)ï¼Œä½†å› ä¸ºå…¶å°‘å¾—å¤šçš„ HBM è®¿é—®æ¬¡æ•°ï¼Œå…¶è¿è¡Œæ—¶é—´å¤§å¤§å‡å°‘
> Fig2 middle å±•ç¤ºäº† HBM è®¿é—®æ¬¡æ•°å’Œ $K, V$ å—å¤§å° $B_c$ çš„å…³ç³»ï¼Œå¯ä»¥çœ‹åˆ°å—è¶Šå¤§ï¼ŒHBM è®¿é—®æ¬¡æ•°è¶Šå°‘ï¼Œå‰å‘ä¼ æ’­æ—¶é—´è¶ŠçŸ­ï¼Œå—è¶³å¤Ÿå¤§æ—¶ (è¶…è¿‡256)ï¼Œè¿è¡Œæ—¶é—´çš„ç“¶é¢ˆç”±å…¶ä»–å› ç´ åˆ¶çº¦ (å¦‚ç®—æ•°è¿ç®—)ï¼Œä¸å†éšç€å—å¤§å°å¢å¤§è€Œå‡å°‘ï¼Œå½“ç„¶ï¼Œå—è¿‡å¤§ SRAM ä¹Ÿæ”¾ä¸ä¸‹

## 3.3 Extension: Block-Sparse FlashAttention 
We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention , whose IO complexity is smaller than FlashAttention by a factor proportional to the sparsity. 
> æˆ‘ä»¬å°† FlashAttention æ‹“å±•ä¸ºè¿‘ä¼¼ attention è®¡ç®—ï¼Œå³ block-sparse FlashAttentionï¼Œå…¶ IO å¤æ‚åº¦æ¯” FlashAttention å°ä¸€ä¸ªæ­£æ¯”äºç¨€ç–åº¦çš„å› å­

Given inputs $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ and a mask matrix $\tilde{\mathbf{M}}\in\{0,1\}^{N\times N}$ , we want to compute: 

$$
\mathbf{S}=\mathbf{Q}\mathbf{K}^{\top}\in\mathbb{R}^{N\times N},\quad\mathbf{P}=\mathrm{softmax}(\mathbf{S}\odot\mathbb{1}_{\tilde{\mathbf{M}}})\in\mathbb{R}^{N\times N},\quad\mathbf{O}=\mathbf{P}\mathbf{V}\in\mathbb{R}^{N\times d},
$$ 
where $(\mathbf{S}\odot\mathbb{1}_{\tilde{\mathbf{M}}})_{k l}=\mathbf{S}_{k l}$ if $\tilde{\mathbf{M}}_{k l}=1$ and $-\infty$ if $\mathbf{M}_{k l}=0$ . 
> ç¨€ç– Attention è®¡ç®—æ¯”æ ‡å‡† Attention è®¡ç®—å¤šäº†æ©ç çŸ©é˜µ $\tilde {\mathbf M}\in \{0, 1\}^{N\times N}$ï¼Œæ©ç çŸ©é˜µå’Œåˆ†æ•°çŸ©é˜µ $\mathbf S$ é€å…ƒç´ è¿ç®—ï¼Œæ©ç ä¸º0å°±å°†åˆ†æ•°è®¾ä¸ºè´Ÿæ— ç©·ï¼Œå¦åˆ™ä¸å˜

We require $\tilde{\textbf{M}}$ to have block form: for some block sizes $B_{r},B_{c}$ , for all $k,l$ , $\tilde{\mathbf{M}}_{kl}=\mathbf{M}_{i j}$ with $i=\lfloor k/B_{r}\rfloor,j=\lfloor l/B_{c}\rfloor$ for some $\mathbf{M}\in\{0,1\}^{N/B_{r}\times N/B_{c}}$ . 
> æˆ‘ä»¬è¦æ±‚ $\tilde {\mathbf M}$ å…·æœ‰å—å½¢å¼ï¼Œå—å¤§å°ä¸º $B_r \times B_c$
> $\tilde {\mathbf M}$ å‹ç¼©ä¹‹åçš„çŸ©é˜µä¸º $\mathbf M \in \{0, 1\}^{N/ B_r\times N/ B_c}$
> $\tilde {\mathbf M}$ çš„ä¸€ä¸ª $B_r\times B_c$ å—å†…çš„å…ƒç´ éƒ½æ˜¯ç›¸åŒçš„ï¼Œæ˜ å°„åˆ°å‹ç¼©çŸ©é˜µ $\mathbf M$ ä¸­çš„ä¸€ä¸ªå…ƒç´ 
> å…·ä½“åœ°è¯´ï¼Œå°±æ˜¯æ»¡è¶³å¯¹äºæ‰€æœ‰çš„ $k, l$ï¼Œ$\tilde {\mathbf M}_{kl} = \mathbf M_{ij}$ï¼Œå…¶ä¸­ $i = \lfloor k / B_r \rfloor$ ( $k$ åœ¨ç¬¬å‡ ä¸ªå—è¡Œ)ï¼Œ$j = \lfloor l / B_c \rfloor$ ( $j$ åœ¨ç¬¬å‡ ä¸ªå—åˆ—)

Given a predefined block sparsity mask $\mathbf{M}\in\{0,1\}^{N/B_{r}\times N/B_{c}}$ we can easily adapt Algorithm 1 to only compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. 
> ç§° $\tilde {\mathbf M}$ å‹ç¼©å¾—åˆ°çš„ $\mathbf M$ ä¸ºå—ç¨€ç–æ©ç ï¼Œç»™å®šå—ç¨€ç–æ©ç ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•è°ƒæ•´ç®—æ³•1ï¼Œä½¿å…¶ä»…è®¡ç®—éé›¶å—çš„ attention çŸ©é˜µï¼Œå®é™…ç®—æ³•å’Œç®—æ³•1ä¸€è‡´ï¼Œå·®åˆ«ä»…åœ¨è·³è¿‡é›¶å—

We also analyze the IO complexity of block-sparse FlashAttention . 

**Proposition 4.** 
Let ğ‘ be the sequence length, ğ‘‘ be the head dimension, and ğ‘€ be size of SRAM with $d\,\leq\,M\,\leq\,N d$ . Block-sparse FlashAttention (Algorithm 5) requires $\Theta(N d+N^{2}d^{2}M^{-1}s)$ HBM accesses where ğ‘  is the fraction of nonzero blocks in the block-sparsity mask. 
> å‘½é¢˜4ï¼š
> åºåˆ—é•¿åº¦ $N$ï¼Œ$d$ ä¸ºå¤´ç»´åº¦ï¼Œ$M$ ä¸º SRAM å¤§å°ï¼Œæ»¡è¶³ $d\le M \le Nd$
> å—ç¨€ç– FlashAttention (ç®—æ³•5) éœ€è¦ $\Theta (Nd + N^2d^2 M s)$ æ¬¡ HBM è®¿é—®ï¼Œå…¶ä¸­ $s$ ä¸ºå—ç¨€ç–æ©ç ä¸­çš„éé›¶å—æ¯”ä¾‹

We see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the IO complexity. For large sequence lengths $N$ , is often set to $N^{-1/2}$ [11] or $N^{-1}\log N$ [3 ,17 ,92], resulting in $\Theta(N{\sqrt{N}})$  or $\Theta(N\log N)$ IO complexity. For downstream experiments, we use the fixed butterfly sparsity pattern [17], which has been shown to be able to approximate arbitrary sparsity [16]. 
> å¯ä»¥çœ‹åˆ° IO å¤æ‚åº¦éšç€å—ç¨€ç–ç¨‹åº¦è€Œä¸‹é™ï¼Œå¯¹äºå¤§çš„ $N$ï¼Œç¨€ç–åº¦ä¸€èˆ¬è®¾ä¸º $N^{-1/2}$ æˆ– $N^{-1}\log N$ï¼Œå¯¹åº”çš„ IO å¤æ‚åº¦é™ä¸º $\Theta (N\sqrt N)$ æˆ– $\Theta (N\log N)$

In Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves $2.8\times$ speedup, while performing on par with standard attention (Section 4). 
> Fig2 (right) ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå—ç¨€ç– FlashAttention çš„è¿è¡Œæ—¶é—´éšç€ç¨€ç–åº¦æé«˜è€Œæˆæ¯”ä¾‹ä¸‹é™

# 4 Experiments 
We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims about training time and model accuracy, and report attention runtime and memory benchmarks.
> æˆ‘ä»¬éªŒè¯ FlashAttention çš„è®­ç»ƒæ—¶é—´å’Œæ¨¡å‹å‡†ç¡®åº¦

- **Training Speed.** FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by $15\%$ , and speeds up GPT-2 up to 3Ã— over HuggingFace [87] and $1.8\times$ over Megatron over standard Transformers. FlashAttention speeds up the long-range arena (LRA) benchmark 2.4Ã— . 
> è®­ç»ƒé€Ÿåº¦ï¼š
> FlashAttention è®­ BERT ä¸Šæ¯” MLPerf 1.1 å¿«15%ï¼Œè®­ GPT-2 æ¯” HuggingFace å¿«3å€ï¼Œè®­æ ‡å‡† Transformer æ¯” Megatron å¿« 1.8 å€
> FlashAttention æ¯” LRA benchmark å¿«2.4å€

- **Quality.** FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAttention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-document classification tasks. Finally, FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K).
> è´¨é‡ï¼š
> FlashAttention å°† Transformer æ‰©å±•åˆ°æ›´é•¿åºåˆ—ï¼Œæ•…è´¨é‡æ›´é«˜
> FlashAttention è®­ GPT-2 çš„çª—å£ä¸º 4K é•¿åº¦ï¼Œæ¯” Megatron è®­ 1K é•¿åº¦çš„é€Ÿåº¦è¿˜å¿«ï¼Œå›°æƒ‘åº¦ä¹Ÿæ›´é«˜ï¼Œé•¿æ–‡æ¡£åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡ä¹Ÿæ›´é«˜
> FlashAttention è®­ç»ƒå‡ºç¬¬ä¸€ä¸ªåœ¨ Path-X ä»»åŠ¡è¡¨ç°æ¯”éšæœºå¥½çš„ Transformer (åºåˆ—é•¿åº¦16K)ï¼Œå—ç¨€ç– FlashAttention è®­ç»ƒå‡ºç¬¬ä¸€ä¸ªåœ¨ Path-256 ä»»åŠ¡è¡¨ç°æ¯”éšæœºå¥½çš„ Transformer (åºåˆ—é•¿åº¦64K)

- **Benchmarking Attention.** We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3x faster than standard attention for common seq. lengths (up to 2K). We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines. 
> FlashAttention çš„å†…å­˜å ç”¨å’Œåºåˆ—é•¿åº¦æˆçº¿æ€§å…³ç³»ï¼Œåœ¨å¸¸è§„åºåˆ—é•¿åº¦ä¸Šä¸‰å€å¿«äºæ ‡å‡† attention
> å—ç¨€ç– FlashAttention çš„è¿è¡Œæ—¶é—´å’Œåºåˆ—é•¿åº¦æˆçº¿æ€§å…³ç³»ï¼Œå¿«äºæ‰€æœ‰çš„ç°å­˜è¿‘ä¼¼ attention ç®—æ³•

Additional experiment details are in Appendix E. 

## 4.1 Faster Models with FlashAttention 
**BERT.** FlashAttention yields the fastest single-node BERT training speed that we know of. We train a BERT-large [22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the implementation from Nvidia that set the training speed record for MLPerf 1.1 [58]. Our implementation is $15\%$ faster. 
> BERT è®­ç»ƒæ—¶é—´æ¯” MLPerf å¿«15%

![[FlashAttention-Table1.png]]


**GPT-2.** FlashAttention yields faster training times for GPT-2 [67] on the large OpenWebtext dataset [32] than the widely used HuggingFace [87] and Megatron-LM [77] implementations. Table 2 shows up to 3Ã— end-to-end speedup compared to Huggingface and 1.7Ã— speedup compared to Megatron-LM. FlashAttention achieves the same perplexity as the other two implementations, as we do not change the model definition. Appendix E includes plots of the validation perplexity throughout training, confirming that FlashAttention is as numerically stable as the baselines and produces the same training/validation curves. 
> GPT-2åœ¨ OpenWebtext æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶é—´æ¯” HuggingFace å’Œ Megatron-LM å¿«ï¼Œä¸”å›°æƒ‘åº¦ä¸€æ ·
> Appendix E æä¾›äº†è®­ç»ƒæ—¶çš„éªŒè¯ perplexity æ›²çº¿ï¼ŒFlashAttention çš„æ•°å€¼ç¨³å®šæ€§å’Œ baseline ä¸€è‡´ï¼Œè®­ç»ƒå’ŒéªŒè¯æ›²çº¿ä¹Ÿä¸€è‡´

![[FlashAttention-Table2.png]]

**Long-range Arena.** We compare vanilla Transformer (with either standard implementation or FlashAt- tention ) on the long-range arena (LRA [ 80 ]) benchmark. We measure accuracy, throughput, and training time of all models. Each task has a diï¬€erent sequence length varying between 1024 and 4096. We follow the implementation and experimental setting in Tay et al. [80] and Xiong et al. [90] . Table 3 shows that FlashAt- tention achieves up $2.4\times$ speed-up compared to standard attention. Block-sparse FlashAttention is faster than all of the approximate attention methods that we have tested. 
> LRA benchmark ä¸Šï¼ŒFlashAttention æ¯”æ ‡å‡† Attention å¿«2.4å€ï¼Œå—ç¨€ç– FlashAttention æ¯”æ‰€æœ‰è¿‘ä¼¼ attention æ–¹æ³•éƒ½å¿«

![[FlashAttention-Table3.png]]

## 4.2 Better Models with Longer Sequences 
**Language Modeling with Long Context.** The runtime and memory-efficiency of FlashAttention allow us to increase the context length of GPT-2 by 4x while still running faster than the optimized implementation from Megatron-LM. Table 4 shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity. 
> è®­ç»ƒ GPT-2 æ—¶ï¼ŒFlashAttention å°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°åŸæ¥4å€ï¼Œä»ç„¶æ¯” Megatron-LM å¿«30%ï¼Œä¸” perplexity æ›´é«˜

![[FlashAttention-Table4.png]]

**Long Document Classification.** Training Transformers with longer sequences with FlashAttention improves performance on the MIMIC-III [47] and ECtHR [6 , 7] datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the  European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights that were allegedly violaged. Both of these datasets contain very long text documents; the average number of tokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and longest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence length of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]). 
> ä½¿ç”¨æ›´é•¿çš„åºåˆ—è®­ç»ƒ Transformer æé«˜äº† MIMIC-III å’Œ EctHR ä¸Šçš„åˆ†ç±»è¡¨ç°ï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†éƒ½åŒ…å«éå¸¸é•¿çš„æ–‡æœ¬ï¼Œå¹³å‡ token æ•°é‡åˆ†åˆ«æ˜¯ 2,395 å’Œ 2,197ï¼Œæœ€é•¿ token æ•°é‡åˆ†åˆ«æ˜¯ 14,562 å’Œ 49,392

Table 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language. 

![[FlashAttention-Table5.png]]

**Path-X and Path-256.** The Path-X and Path-256 benchmarks are challenging tasks from the long-range arena benchmark designed to test long context. The task is to classify whether two points in a black and white 128 $\times$ 128 (or 256 $\times$ 256) image have a path connecting them, and the images are fed to the transformer one pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved random performance [80]. There has been a search for alternative architectures that can model such long context [37]. We present here the first result of Transformer models being able to solve Path-X and Path-256 (Table 6). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse FlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy on Path-256. 
> é¢„è®­ç»ƒäº Path-64ï¼Œç„¶åé€šè¿‡ç©ºé—´æ’å€¼ä½ç½®åµŒå…¥è¿ç§»åˆ° Path-X

## 4.3 Benchmarking Attention 
We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and a padding mask. We compare against reference implementations for exact attention, approximate attention, and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines and full details. 

![[FlashAttention-Fig3.png]]

**Runtime.** Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAttention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAttention runs significantly faster than exact attention baselines, up to $3\times$ faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with sequence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that we know of, across all sequence lengths. 
> å„ä¸ª attention ç®—æ³•çš„è¿è¡Œæ—¶é—´å’Œåºåˆ—é•¿åº¦çš„å…³ç³»å›¾è§ Fig3 (left)ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œè¿è¡Œæ—¶é—´éšç€åºåˆ—é•¿åº¦å¢å¤§è€ŒäºŒæ¬¡å¢åŠ 
> åœ¨æ‰€æœ‰çš„å‡†ç¡® attention ç®—æ³•ä¸­ï¼ŒFlashAttention æœ€å¿«ï¼Œå¹¶ä¸”åœ¨åºåˆ—é•¿åº¦çŸ­æ—¶æ¯”ä¸€äº›è¿‘ä¼¼ attention ç®—æ³•è¿˜å¿«
> block-sparse FlashAttention æ¯”æ‰€æœ‰çš„ attention ç®—æ³•éƒ½å¿«

**Memory Footprint.** Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to $20\times$ more memory efficient than exact attention baselines, and is more memory-efficient than the approximate attention baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still  $2\times$ more efficient than Linformer. 
> å„ä¸ª attention ç®—æ³•çš„å†…å­˜ä½¿ç”¨å’Œåºåˆ—é•¿åº¦çš„å…³ç³»è§ Fig3 (right)ï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒFlashAttention å’Œ block-sparse FlashAttention æœ‰ç›¸åŒçš„å†…å­˜å ç”¨ï¼Œå ç”¨å¤§å°éšç€åºåˆ—é•¿åº¦çº¿æ€§å¢åŠ ï¼Œå…¶å†…å­˜æ•ˆç‡æ¯”è¿‘ä¼¼ attention ç®—æ³•å’Œå‡†ç¡® attention ç®—æ³•éƒ½é«˜
> é™¤äº† Linformerï¼Œæ‰€æœ‰å…¶ä»–ç®—æ³•éƒ½åœ¨åºåˆ—é•¿åº¦è¶…è¿‡ 64K åå†…å­˜æº¢å‡º

# 5 Limitations and Future Directions 
We discuss limitations of our approach and future directions. Related work is given in Appendix A. 

**Compiling to CUDA.** Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation. This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires significant engineering eï¬€ort. Implementations may also not be transferrable across GPU architectures. These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDAâ€”similar to eï¬€orts such as Halide in image processing [70]. 

**IO-Aware Deep Learning.** We believe that the IO-aware approach can extend beyond attention. Attention is the most memory-intensive computation in Transformers, but every layer in a deep network touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss these potential extensions in Appendix D. 
> attention æ˜¯ Transformer ä¸­æœ€ä¸ºå†…å­˜å¯†é›†çš„è®¡ç®—

**Multi-GPU IO-Aware Methods.** Our IO-aware implementation of attention is optimal within constants for computing attention on a single GPU. However, the attention computation may be parallelizable across multiple GPUs [72]. Using multiple GPUs adds an additional layer to IO analysisâ€”accounting for data transfer between GPUs. We hope our work inspires future work in this direction. 
> ä½¿ç”¨å¤š GPU å®ç°è¿˜éœ€è¦é¢å¤–è€ƒè™‘ GPU ä¹‹é—´çš„æ•°æ®ä¼ è¾“ï¼Œæ•…æ·»åŠ äº†é¢å¤–çš„ä¸€å±‚ IO åˆ†æ

# A Related Work 
**IO-Aware Runtime Optimization.** The broad concept of optimizing for reading and writing to fast/slow memory has a long history in computer science and has been known by many names. We draw the most direct connection to the literature of analyzing I/O complexity in this work [1], but concepts of memory hierarchies are fundamental and has appeared in many forms, from the working set model [21], to data locality [86], to the Rooï¬‚ine model of arithmetic intensity [85], to analyses of scalability [59], to standard textbook treatments of computer architecture [40]. We hope that this work encourages the community to adopt these ideas in more parts of the deep learning stack. 

**Efficient ML Models with Structured Matrices.** Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more efficient set of matrices. These matrices are called structured matrices , which have subquadratic ( $o(n^{2})$ for dimension $n\times n$ ) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like [78], low-displacement rank [49], quasi-separable [25]). The butterï¬‚y pattern we use for our block-sparse attention is motivated by the fact that butterï¬‚y matrices [15 , 64] and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters [16 , 20]. However, even though structured matrices are efficient in theory, they have not seen wide adoption since it is hard to translate their efficiency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery [41]. Extensions of butterï¬‚y matrices [17 , 18] aimed to make butterï¬‚y matrices more hardware-friendly. 
> çŸ©é˜µä¹˜æ˜¯å¤§å¤šæ•°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ ¸å¿ƒè®¡ç®—ç“¶é¢ˆï¼Œä¸ºäº†å‡å°‘è®¡ç®—å¤æ‚æ€§ï¼Œè®¸å¤šæ–¹æ³•ç ”ç©¶äº†æ›´é«˜æ•ˆçš„ä¸€ç³»åˆ—çŸ©é˜µï¼Œè¿™äº›çŸ©é˜µè¢«ç§°ä¸ºç»“æ„åŒ–çŸ©é˜µï¼Œå®ƒä»¬å…·æœ‰æ¬¡äºŒæ¬¡çš„å‚æ•°å’Œè¿è¡Œæ—¶é—´
> æœ€å¸¸è§çš„ç»“æ„åŒ–çŸ©é˜µå°±æ˜¯ä½ç§©å’Œç¨€ç–çŸ©é˜µï¼Œä»¥åŠåœ¨ä¿¡å·å¤„ç†ä¸­å¸¸è§çš„å¿«é€Ÿå˜æ¢çŸ©é˜µ
> æˆ‘ä»¬åœ¨å—ç¨€ç– attention ä¸­ä½¿ç”¨çš„è´è¶æ¨¡å¼æ¥æºäºè´è¶çŸ©é˜µï¼Œå…¶ä¹˜ç§¯å¯ä»¥ä»¥å‡ ä¹æœ€ä¼˜çš„è¿è¡Œæ—¶é—´å’Œå‚æ•°æ•°é‡è¡¨ç¤ºä»»æ„ç»“æ„åŒ–çš„çŸ©é˜µ
> ç»“æ„åŒ–çŸ©é˜µåœ¨ç†è®ºä¸­æ˜¯é«˜æ•ˆçš„ï¼Œä½†å¹¶æœªå¹¿æ³›åœ¨å®é™…ä¸­ä½¿ç”¨ï¼Œå› ä¸ºä¸å®¹æ˜“å°†å®ƒä»¬çš„ç†è®ºæ•ˆç‡è½¬åŒ–ä¸ºå®é™…çš„é€Ÿåº¦æå‡ï¼Œç¨ å¯†çš„çŸ©é˜µä¹˜å®é™…ä¸Šå·²ç»æœ‰é«˜åº¦ä¼˜åŒ–çš„å®ç°

**Sparse Training.** Our block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23 , 38 , 39 , 55 , 76]. For model training, the lottery tickets hypothesis [28 , 29 , 30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterï¬‚y pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks. 
> å—ç¨€ç– FlashAttention å¯ä»¥è§†ä½œè®©ç¨€ç–æ¨¡å‹è®­ç»ƒæ›´é«˜æ•ˆçš„æ–¹æ³•
> ç¨€ç–æ–¹æ³•é€šè¿‡ç¨€ç–åŒ–æƒé‡çŸ©é˜µæ¥å‹ç¼©æ¨¡å‹ï¼Œæé«˜æ¨ç†æ•ˆç‡
> å¯¹äºæ¨¡å‹è®­ç»ƒï¼Œlottery tickets å‡è®¾è¡¨æ˜ï¼šä»æ›´å¤§çš„å¯†åº¦ç½‘ç»œä¸­è¡ç”Ÿçš„ä¸€ç»„å°çš„è‡ªç½‘ç»œå¯ä»¥å’ŒåŸç½‘ç»œçš„è¡¨ç°ç›¸è¿‘
> å—ç¨€ç– FlashAttention å¯ä»¥è§†ä¸ºå›ºå®šçš„ lottery ticketï¼šå°†ç¨€ç–æ¨¡å¼åœ¨è®­ç»ƒæ—¶å›ºå®šä¸ºè´è¶æ¨¡å¼ï¼Œå‘ç°æ¨¡å‹çš„è¡¨ç°å’Œå¯†åº¦ FlashAttention åœ¨ LRA ä»»åŠ¡ä¸Šç›¸è¿‘

**Efficient Transformer.** Transformer-based models have become the most widely-used architecture in natural language processing [22] and computer vision [24 , 91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12 , 54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3 ], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73 ]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52 , 57 , 79 , 89]. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details. 
> Transformer çš„æ—¶é—´å’Œå†…å­˜éƒ½éšç€åºåˆ—é•¿åº¦è€ŒäºŒæ¬¡å¢é•¿
> å¯¹åº”çš„è§£å†³æ–¹æ³•æœ‰ï¼šä½¿ç”¨å“ˆå¸Œè¿‘ä¼¼ï¼ˆå³ç¨€ç–ï¼‰çš„ Reformer, Smyrf ï¼›ä½¿ç”¨ä½ç§©è¿‘ä¼¼çš„ Performerï¼›Longformer, BIgBird ç­‰å°†ç¨€ç–å’Œä½ç§©è¿‘ä¼¼ç»“åˆ
> è¿˜æœ‰çš„æ–¹æ³•å‹ç¼©åºåˆ—ç»´åº¦ï¼Œä½¿å¾—ä¸€æ¬¡ attend å¤šä¸ª token
> ä¹Ÿå¯ä»¥ attend ä¹‹å‰åºåˆ—çš„çŠ¶æ€æ¥å¸®åŠ©å¢é•¿ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚ Transformer-XL

There are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31 , 36 , 37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (efficient training), RNNs (efficient inference), and continuous models (robust to change in sampling rates). LambdaNetworks [2], AFT [93] and FLASH [42] are other attempts at replacing attention in the context of image classification and language modeling. 

# B Algorithm Details 
We first derive the forward and backward passes of attention and show that they can be computed in a memory-efficient manner (requiring extra memory linear instead of quadratic in the sequence length). Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed. We describe the FlashAttention algorithm to implement both the forward and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller memory footprint. 
> æˆ‘ä»¬é¦–å…ˆæ¨å¯¼ attention è®¡ç®—çš„å‰å‘å’Œåå‘è¿‡ç¨‹ï¼Œç„¶åè¡¨æ˜å®ƒä»¬å¯ä»¥ä»¥å†…å­˜é«˜æ•ˆçš„å½¢å¼è®¡ç®—ï¼ˆå¯¹é¢å¤–å†…å­˜çš„éœ€æ±‚çº¿æ€§äºåºåˆ—é•¿åº¦ï¼Œè€Œä¸æ˜¯äºŒæ¬¡ï¼‰ï¼Œè™½ç„¶å‡å°‘äº†å¯¹é¢å¤–å†…å­˜çš„éœ€æ±‚ï¼Œä½†æœ´ç´ ç®—æ³•ä»ç„¶éœ€è¦äºŒæ¬¡çš„ HBM è®¿é—®
> æˆ‘ä»¬æ¥ç€ä»‹ç» FlashAttention ç®—æ³•åœ¨ GPUs ä¸Šçš„æ­£å‘å’Œåå‘ä¼ æ’­çš„å®ç°ï¼Œä»¥å‡å°‘ HBM è®¿é—®ï¼Œè¿™æ ·æˆ‘ä»¬å°±åŒæ—¶å…·æœ‰äº†æ›´å°‘çš„å†…å­˜éœ€æ±‚å’Œæ›´å°‘çš„è¿è¡Œæ—¶é—´

## B.1 Memory-efficient forward pass 
The main challenge in making attention memory-efficient is the softmax that couples the columns of $\mathbf{K}$ (and columns of $\mathbf{V}$ ). Our approach is to compute the softmax normalization constant separately to decouple the columns. This technique [60] has been used in the literature [51 , 66] to show that attention computation does not need quadratic extra memory (though the number of HBM accesses is still quadratic, resulting in slow run-time). 
> è¦è®© attention å†…å­˜é«˜æ•ˆï¼Œä¸»è¦çš„æŒ‘æˆ˜å°±æ˜¯ softmax è®¡ç®—ï¼Œå®ƒç»‘å®šäº† $\mathbf K, \mathbf V$ çš„åˆ—
> æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåˆ†åˆ«è®¡ç®— softmax è§„èŒƒåŒ–å¸¸æ•°ï¼Œä»¥è§£è€¦è¿™äº›åˆ—ï¼Œè¯¥æŠ€æœ¯è¢«ç”¨äº [51, 66]ï¼Œå±•ç¤ºäº† attention å¹¶ä¸éœ€è¦äºŒæ¬¡çš„é¢å¤–å†…å­˜ï¼ˆå½“ç„¶ HBM è®¿é—®çš„æ¬¡æ•°ä»ç„¶æ˜¯äºŒæ¬¡çš„ï¼Œæ•…è®¡ç®—æ—¶é—´å°šæœªä¼˜åŒ–ï¼‰

For simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B.3 contains all the steps. 

Recall that given input sequences $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ , we want to compute the attention output $\mathbf{O}\in\mathbb{R}^{N\times d}$ : 

$$
\mathbf{S}=\mathbf{Q}\mathbf{K}^{\top}\in\mathbb{R}^{N\times N},\quad\mathbf{P}=\mathrm{softmax}(\mathbf{S})\in\mathbb{R}^{N\times N},\quad\mathbf{O}=\mathbf{P}\mathbf{V}\in\mathbb{R}^{N\times d}.
$$ 
We have that $S_{i j}=q_{i}^{T}{k}_{j}$ where $q_i$ and $k_{j}$ are the $i$ -th and $j$ -th columns of $\mathbf{Q}$ and $\mathbf{K}$ respectively. 
> $q_i$ æ˜¯ $\mathbf Q$ çš„ç¬¬ $i$ è¡Œå‘é‡ï¼ˆå†™ä¸ºåˆ—å‘é‡ï¼‰ï¼Œ$k_j$ æ˜¯ $\mathbf K$ çš„ç¬¬ $j$ è¡Œå‘é‡ï¼ˆå†™ä¸ºåˆ—å‘é‡ï¼‰

Define the normalization constants of softmax: 
> softmax æ¯ä¸€è¡Œçš„è§„èŒƒåŒ–å› å­å®šä¹‰ä¸º $L_i = \sum_j e^{S_{ij}} = \sum_j  e^{q_i^Tk_j}$

$$
L_{i}=\sum_{j}e^{q_{i}^{T}k_{j}}.\tag{1}
$$ 
Let $v_j$ be the $j$ -th columns of $\mathbf{V}$ , then the $i$ -th columns of the output is
> è¾“å‡º $\mathbf O$ çš„ç¬¬ $i$ è¡Œ $o_i = P_{i:}\mathbf V$ï¼Œä»¤ $v_j$ ä¸º $\mathbf V$ çš„ç¬¬ $j$ è¡Œå‘é‡
> æ•… $o_i = P_{i:}\mathbf V = \sum_{j} P_{ij}v_j$
> å°† $P_{ij}$ æ˜¾å¼å†™ä¸º $\frac {e^{S_{ij}}}{L_i} = \frac {e^{q_i^Tk_j}}{L_i}$ å°±å¾—åˆ°äº† (2)

$$
o_{i}=P_{i:}{\bf V}=\sum_{j}P_{i j}v_{j}=\sum_{j}\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}v_{j}.\tag{2}
$$ 
We see that once $L_{i}$ is computed, we can compute $o_{i}$ without extra memory by repeatedly summing $\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}v_{j}$ . Therefore the forward pass can be computed with $O(n)$ extra memory: 

1. Compute $L_{i}$ for all $i$ according to Eq. (1), which takes $O(n)$ extra memory.
2. Compute $o_{i}$ for all $i$ according to Eq. (2), which takes $O(d)$ extra memory. 

> å½“æ¯ä¸€è¡Œçš„è§„èŒƒåŒ–å¸¸æ•° $L_i$ å¾—åˆ°åï¼Œè¾“å‡º $\mathbf O$ çš„ç¬¬ $i$ è¡Œ $o_i$ çš„è®¡ç®—å°±æ˜¯åå¤æ±‚å’Œ $\frac {e^{q_i^Tk_j}}{L_i}v_j$
> ä¸ºæ­¤ï¼Œå¯ä»¥ä»¥ $O (n)$ çš„é¢å¤–å†…å­˜éœ€æ±‚è®¡ç®—å‰å‘ä¼ æ’­ï¼š
> 1. å¯¹äºæ‰€æœ‰çš„ $i$ï¼Œæ ¹æ® (1) è®¡ç®— $L_i$ï¼Œéœ€è¦ $O (n)$ é¢å¤–å†…å­˜
> 2. å¯¹äºæ‰€æœ‰çš„ $o_i$ï¼Œæ ¹æ® (2) è®¡ç®—ç»“æœï¼Œéœ€è¦ $O (d)$ é¢å¤–å†…å­˜

> åœ¨å¸¸è§„çš„ attention è®¡ç®—ä¸­ï¼Œä¸­é—´ç»“æœåˆ†æ•°çŸ©é˜µ $\mathbf S\in \mathbb R^{N\times N}$ æ˜¯ç›´æ¥é€šè¿‡çŸ©é˜µä¹˜æ³•è®¡ç®—å¾—åˆ°çš„ï¼Œå­˜å‚¨è¿™ä¸ªä¸­é—´ç»“æœéœ€è¦ $O (n^2)$ çš„é¢å¤–å†…å­˜
> è¿™é‡Œä»‹ç»çš„æ–¹æ³•ä¸ç›´æ¥è®¡ç®—å‡ºå®Œæ•´çš„åˆ†æ•°çŸ©é˜µ $\mathbf S$ï¼Œè€Œæ˜¯ç”¨å¾ªç¯æ¥ä¸€è¡Œä¸€è¡Œåœ°è®¡ç®—è€ƒè™‘ç®—æ³•çš„ç¬¬ä¸€æ­¥ï¼š
> 1. for all $i$ï¼Œcompute $L_i$ ç­‰ä»·äº for all $i$ï¼Œcompute $\mathbf S_{i:}$ï¼Œç„¶åè®¡ç®— $L_i = \sum_je ^{S_{ij}}$
> ç®—æ³•çš„ç¬¬ä¸€æ­¥è®¡ç®—æ¯ä¸€è¡Œçš„è§„èŒƒåŒ–å¸¸æ•°ï¼Œå› ä¸ºæ¯æ¬¡è®¡ç®—ä»…å…³å¿ƒ $\mathbf S$ çš„ä¸€è¡Œï¼Œè®¡ç®—æ—¶éœ€æ±‚çš„å†…å­˜æ˜¯ $O (d)$ï¼Œæœ€åå­˜å‚¨å…¨éƒ¨çš„ $L_i$ éœ€æ±‚çš„å†…å­˜æ˜¯ $O (n)$ï¼Œæ³¨æ„è®¡ç®—å¾—åˆ°çš„ $\mathbf S$ çš„ç»“æœæ²¡æœ‰å­˜å‚¨ä¸‹æ¥ï¼Œå®ƒéœ€è¦åœ¨ç¬¬äºŒæ­¥å†é‡æ–°è®¡ç®—
> è€ƒè™‘ç®—æ³•çš„ç¬¬äºŒæ­¥ï¼š
> 2. for all $o_i$, copmute $o_i$
> è¿™é‡ŒåŒæ ·æ˜¯é€è¡Œåœ°è®¡ç®— $o_i$ï¼Œæ­¤æ—¶è¿˜éœ€è¦é‡æ–°è®¡ç®— $\mathbf S_{i:}$ ï¼Œå–æŒ‡æ•°åï¼Œé™¤ä»¥ç¬¬ä¸€æ­¥è®¡ç®—å¾—åˆ°çš„ $L_i$ è§„èŒƒåŒ–å¾—åˆ°æƒé‡ï¼Œç„¶åå¯¹ $\mathbf V$ çš„æ‰€æœ‰è¡ŒåŠ æƒæ±‚å’Œï¼Œè®¡ç®—æƒé‡éœ€è¦çš„é¢å¤–å†…å­˜æ˜¯ $O (d)$

## B.2 Memory-efficient backward pass 
We derive the backward pass of attention and show that it can also be computed with linear memory. Rabe and Staats [66] suggests that the backward pass can be done without quadratic extra memory by applying gradient checkpointing to the memory-efficient forward pass. We instead derive the backward pass explicitly and show how it can be computed in a memory-efficient manner. 
> æˆ‘ä»¬æ¨å¯¼ attention è®¡ç®—çš„åå‘è¿‡ç¨‹ï¼Œè¡¨é¢å®ƒåŒæ ·å¯ä»¥ä»¥çº¿æ€§å†…å­˜çš„æ–¹å¼è®¡ç®—
> [66]ä¸­çš„å†…å­˜é«˜æ•ˆçš„åå‘ä¼ æ’­æ˜¯é€šè¿‡ä¸ºå†…å­˜é«˜æ•ˆçš„å‰å‘ä¼ æ’­åº”ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
> æˆ‘ä»¬ç›´æ¥æ˜¾å¼ä¸ºåå‘ä¼ æ’­è¿›è¡Œæ¨å¯¼

Suppose that there is a scalar loss function $\phi$ , and let the output gradient be $\mathbf{d}\mathbf{O}\in\mathbb{R}^{n\times d}$ (where $\mathbf{dO}$ denotes $\frac{\partial\phi}{\partial{\bf O}}$ ). We want to compute the input gradients $\mathbf{dQ},\mathbf{K},\mathbf{dV}\,\in\,\mathbb{R}^{n\times d}$ (where $\bf {dQ} , \bf {dK} , \bf {dV}$ denote $\frac{\partial\phi}{\partial\mathbf{Q}},\frac{\partial\phi}{\partial\mathbf{K}},\frac{\partial\phi}{\partial\mathbf{V}}$ respectively). 
> è®°æ ‡é‡æŸå¤±å‡½æ•°ä¸º $\phi$ï¼Œè®°è¾“å‡ºæ¢¯åº¦ä¸º $\bf {dO} = \frac {\partial \phi}{\partial \bf O}$
> æˆ‘ä»¬éœ€è¦è®¡ç®—æ¢¯åº¦ $\bf {dQ} = \frac {\partial \phi}{\partial \bf Q},\bf {dK} = \frac {\partial \phi}{\partial \bf K},\bf {dV} = \frac {\partial \phi}{\partial \bf V}$

The gradient $\mathbf{dV}$ is easy to see. Applying reverse-mode autodiff by hand (aka the chain rule), we obtain (in matrix notation) $\bf {dV} = \bf P^T \bf {dO}$, and so:
> å…³äº $\mathbf {dV} = \mathbf P^T \mathbf {dO}$ çš„æ¨å¯¼è§[[#Deduction for chain rule of matrix multiplication|é™„å½•]]

$$
dv_{j}=\sum_{i}P_{i j}d o_{i}=\sum_{i}\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}d o_{i}.\tag{3}
$$ 
> $dv_j = \sum_{i}P_{ij}do_i$ æ¥æºäºï¼š

$$
\begin{align}
dV_{j:} &= (\mathbf P^T \mathbf {dO})_{j:}\\
&= [(P^T)_{j:} dO_{: 1}, \cdots, (P^T)_{j:} dO_{: d}]\\
&= (P^T)_{j:}[dO_{: 1}, \cdots, dO_{:d}]\\
&=(P^T)_{j:}\mathbf {dO}\\
&=(P_{:j})^T\mathbf {dO}\\
&=\sum_{i}P_{ij}dO_{i:}\\
\end{align}
$$

Since we already computed $L_{i}$ , $dv_{j}$ can be computed without extra memory by repeated summing. 
> ç”± (3) å¯çŸ¥ï¼Œ$\mathbf {dV}$ ä¸­çš„è¡Œ $dv_j$ å’Œ $P$ çš„ç¬¬ $j$ åˆ— $P_{:j}$ å’Œ $\mathbf {dO}$ çš„æ‰€æœ‰è¡Œ $do_i$ æœ‰å…³
> è®¡ç®—æ—¶ï¼Œæˆ‘ä»¬é€è¡Œè®¡ç®— $\mathbf {dV}$ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡è®¡ç®—ä¸€ä¸ª $dv_j$ï¼Œ
> è¿™éœ€è¦æˆ‘ä»¬å®æ—¶è®¡ç®— $P$ çš„ç¬¬ $j$ åˆ— $P_{:j}$ï¼Œå…¶ä¸­ $P_{ij} = \frac {e^{q_i^T k_j}}{L_i}$ï¼Œ
> ç„¶åæ ¹æ® $\sum_i P_{ij}do_i$ å¯¹ $\mathbf {dO}$ çš„æ‰€æœ‰è¡Œè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ° $dv_j$

The gradients $\mathbf {dQ}$ and $\mathbf{dK}$ are a little more complicated. We go through the gradients $\mathbf{dP}$ and $\mathbf {dS}$ first. From Eq. (2), we have that $\mathbf{d}\mathbf{P}=\mathbf{d}\mathbf{O}\mathbf{V}^{T}$ , and so: 
> å…³äº $\mathbf {dP} = \mathbf {dOV}^T$ çš„æ¨å¯¼è§[[#Deduction for chain rule of matrix multiplication|é™„å½•]]

$$
\begin{array}{r}{d P_{i j}=d o_{i}^{T}v_{j}.}\end{array}
$$ 
> $dP_{ij} = do_i^T v_j$ æ¥æºäºï¼š

$$
\begin{align}
dP_{ij} &= dO_{i:}V^T_{:j}\\
&=dO_{i:}V_{j:}
\end{align}
$$

Recall that $P_{i:}=\mathrm{softmax}(S_{i:})$ . Using the fact that the Jacobian of $y=\mathrm{softmax}(x)$ is $\mathrm{diag}(y)-y y^{T}$ , we have that 
> å…³äº $y = \text{softmax}(x)$ çš„ Jacobian çš„æ¨å¯¼è§[[#Deduction for Jacobian of softmax|é™„å½•]]

$$
\begin{array}{r}{d S_{i\colon}=(\mathrm{diag}(P_{i\colon})-P_{i\colon}P_{i\colon}^{T})d P_{i\colon}=P_{i\colon}\circ d P_{i\colon}-(P_{i\colon}^{T}d P_{i\colon})P_{i\colon},}\end{array}
$$ 
where ${\circ}$ denotes pointwise multiplication. 
> å…³äº $dS_{i:}$ çš„æ¨å¯¼è§[[#Deduction for $dS_{i }$|é™„å½•]]

Define 

$$
D_{i}=P_{i:}^{T}d P_{i:}=\sum_{j}\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}d o_{i}^{T}v_{j}=d o_{i}^{T}\sum_{j}\frac{e^{q_{i}^{\top}k_{j}}}{L_{i}}v_{j}=d o_{i}^{T}o_{i},\tag{4}
$$

then

$$
d S_{i:}=P_{i:}\circ d P_{i:}-D_{i}P_{i:}.
$$ 
Hence

$$
d S_{i j}=P_{i j}d P_{i j}-D_{i}P_{i j}=P_{i j}(d P_{i j}-D_{i}).
$$

Now we can get the gradients $\bf {dQ}$ and $\mathbf{dK}$ . Recall that $S_{i j}=q_{i}^{T}k_{j}$ , so 

$$
d q_{i}=\sum_{j}d S_{i j}k_{j}=\sum_{j}P_{i j}(d P_{i j}-D_{i})k_{j}=\sum_{j}\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}(d o_{i}^{T}v_{j}-D_{i})k_{j}.\tag{5}
$$ 

> $\frac {\partial \phi}{\partial q_i} = \sum_k\sum_l \frac {\partial \phi}{S_{kl}}\frac {\partial S_{kl}}{\partial q_i} = \sum_l \frac {\partial \phi} {\partial S_{il}} \frac {\partial S_{il}}{\partial q_i} = \sum_{j} dS_{ij}\frac {\partial S_{ij}}{\partial q_i} = \sum_j dS_{ij}k_j$

Similarly, 

$$
d k_{j}=\sum_{i}d S_{i j}q_{i}=\sum_{i}P_{i j}(d P_{i j}-D_{i})q_{i}=\sum_{i}\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}(d o_{i}^{T}v_{j}-D_{i})q_{i}.\tag{6}
$$ 

> $\frac {\partial \phi}{\partial k_j} = \sum_{k}\sum_{l}\frac {\partial \phi}{\partial S_{kl}}\frac {\partial S_{kl}}{\partial k_j} = \sum_{k}\frac {\partial \phi}{\partial S_{kj}}\frac {\partial S_{kj}}{\partial k_j} = \sum_{i}dS_{ij}\frac {\partial S_{ij}}{\partial k_j} = \sum_i dS_{ij}q_i$

Therefore the backward pass can also be computed with $O(n)$ extra memory: 

1. Compute $dv_{j}$ for all $j$ according to Eq. (3), which takes $O(d)$ extra memory. 
2. Compute $D_{i}$ for all $i$ according to Eq. (4), which takes $O(n)$ extra memory. 
3. Compute $d q_{i}$ for all $i$ according to Eq. (5), which takes $O(d)$ extra memory. 
4. Compute $d k_{j}$ for all $j$ according to Eq. (6), which takes $O(d)$ extra memory. 

> ä½¿ç”¨ $O (n)$ çš„é¢å¤–å†…å­˜çš„æ–¹å‘ä¼ æ’­ï¼š
> 1. è®¡ç®— $\mathbf {dV}$ï¼šæ ¹æ® eq (3) è®¡ç®— $dv_j$ (ä¹Ÿå°±æ˜¯é€è¡Œè®¡ç®— $\mathbf {dV}$ )ï¼Œå…¶ä¸­éœ€è¦çš„é¢å¤–å†…å­˜æ¥è‡ªäº $do_j$ ï¼Œå³ $O (d)$
> 2. æ ¹æ® eq (4) è®¡ç®—å…¨éƒ¨çš„ $D_i$ï¼Œä¸€å…± $n$ ä¸ªï¼Œæ•…éœ€è¦ $O (n)$ çš„é¢å¤–å†…å­˜
> 3. è®¡ç®— $\mathbf {dQ}$ å’Œ $\mathbf {dK}$ï¼šæ ¹æ® (5), (6) è®¡ç®— $dq_i, dk_j$ ( åŒæ ·æ˜¯é€è¡Œè®¡ç®— $\mathbf {dQ}, \mathbf {dK}$ )ï¼Œå…¶ä¸­éœ€è¦çš„é¢å¤–å†…å­˜éƒ½æ¥è‡ªäº $do_i$ï¼Œå³ $O (d)$

## B.3 FlashAttention : Forward Pass 
We describe the full details of FlashAttention forward pass. Given input sequences $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ , we want to compute the attention output $\mathbf{O}\in\mathbb{R}^{N\times d}$ :

$$
\begin{align}
\mathbf S &= \tau \mathbf {QK}^T \in \mathbb R^{N\times N},\\
\mathbf S^{\text{masked}} &= \text{MASK}(\mathbf S)\in \mathbb R^{N\times N},\\
\mathbf P &= \text{softmax}(\mathbf S^{\text{masked}})\in \mathbb R^{N\times N},\\
\mathbf P^{\text{dropped}} &= \text{dropout}(\mathbf P, p_{\text{drop}}),\\
\mathbf O &= \mathbf P^{\text{dropped}}\mathbf V \in\mathbb R^{N\times d},
\end{align}
$$

where $\tau\in\mathbb{R}$ is some softmax scaling (typically $\textstyle{\frac{1}{\sqrt{d}}}$ ), mask is some masking function that sets some entries of the input to $-\infty$ and keep other entries the same (e.g., key padding mask when sequences in the batch donâ€™t have the same lengths and are padded), and dropout $(x,p)$ applies dropout to $x$ elementwise (i.e., output $\scriptstyle{\frac{x}{1-p}}$ âˆ’ with probability $1-p$ and output $0$ with probability $p$ for each lement $x$ ). 

The full algorithm is in Algorithm 2. We save the output $\mathbf O$ , the softmax statistics $\ell$ and $m$ , and the pseudo-random number generator state $\mathcal{R}$ for the backward pass. 
> å‰å‘ä¼ æ’­ä¸­ï¼Œéœ€è¦ä¿å­˜ $\mathbf O, \ell, m$ ä»¥åŠéšæœºæ•°ç”ŸæˆçŠ¶æ€ $\mathcal R$ ç”¨äºåå‘ä¼ æ’­

![[FlashAttention-Algorithm2.png]]

**Algorithm 2** FlashAttention Forward Pass
**Require**: Matrices $\mathbf {Q, K, V} \in \mathbb R^{N\times d}$ in HBM, on-chip SRAM of size $\bf M$, softmax scaling constant $\tau \in \mathbb R$, masking function $\text{MASK}$, dropout probability $p_{\text{drop}}$.
 1: Initialize the pseudo-random number generator state $\mathcal R$ and save to HBM.
 2: Set block sizes $B_c = \lceil \frac {\bf M}{4d} \rceil, B_r = (\lceil \frac {\bf M}{4d}\rceil, d)$.
 3: Initialize $\mathbf O = (0)_{N\times d} \in \mathbb R^{N\times d}$, $\ell = (0)_N \in \mathbb R^N$, $m = (-\infty)_N \in \mathbb R^N$ in HBM.
 4: Divide $\mathbf Q$ into $T_r = \lceil \frac {N}{B_r} \rceil$ blocks $\mathbf Q_1, \dots, \mathbf Q_{T_r}$ of size $B_r\times d$ each, and divide $\mathbf K, \mathbf V$ into $T_c = \lceil \frac {N}{B_c} \rceil$ blocks $\mathbf K_q, \dots, \mathbf K_{T_c}$ and $\mathbf V_1, \dots, \mathbf V_{T_c}$, of size $B_c \times d$ each.
 5: Divide $\mathbf O$ into $T_r$ blocks $\mathbf O_i, \dots, \mathbf O_{T_r}$ of size $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_i, \dots, \ell_{T_r}$ of size $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
 6: **for** $1\le j \le T_c$ **do**
 7:    Load $\mathbf K_j, \mathbf V_j$ from HBM to on-chip SRAM.
 8:    **for** $1\le i \le T_r$ **do**
 9:    Load $\mathbf Q_i, \mathbf O_i , \ell_i, m_i$ from HBM to on-chip SRAM.
10:    On chip, computes $\mathbf S_{ij} = \tau \mathbf Q_i \mathbf K_{j}^T\in \mathbb R^{B_r \times B_c}$.
11:    On chip, computes $\mathbf S_{ij}^{\text{masked}}  = \text{MASK}(\mathbf S_{ij})$.
> ç›¸è¾ƒäº Algorithm 1ï¼Œå¤šå‡ºæ¥ä¸€æ­¥ scale å’Œ mask

12:    On chip, computes $\tilde m_{ij} = \text{rowmax}(\mathbf S_{ij}^{\text{masked}}) \in \mathbb R^{B_r}$, $\tilde {\mathbf P}_{ij}=\exp (\mathbf S_{ij}^{\text{masked}} - \tilde m_{ij})\in \mathbb R^{B_r \times B_c}$ (pointwise), $\tilde \ell_{ij} = \text{rowsum}(\tilde {\mathbf P}_{ij} )\in \mathbb R^{B_r}$.
13:    On chip, compute $m_i^{\text{new}} = \max (m_i, \tilde m_{ij}) \in \mathbb R^{B_r}$, $\ell_i^{\text{new}} = e^{m_i - m_i^{new}} \ell_i  + e^{\tilde m_{ij} - m_i^{\text{new}}} \tilde \ell_{ij} \in \mathbb R^{B_r}$.

14:    On chip, compute $\tilde {\mathbf P}_{ij}^{\text{dropped}} = \text{dropout}(\tilde {\mathbf P}_{ij}, p_{\text{drop}})$.
> ç›¸è¾ƒäº Algorithm 1ï¼Œå¤šå‡ºæ¥ä¸€æ­¥ dropout

15:    Write $\mathbf O_i \leftarrow \text{diag}(\ell_i^{\text{new}})^{-1}(\text{diag}(\ell_i)e^{m_i-m_i^{\text{new}}}\mathbf O_i + e^{\tilde m_{ij} - m_i^{\text{new}}}\tilde {\mathbf P}_{ij}^{\text{dropped}}\mathbf V_j)$ to HBM.
16:    Write $\ell_i \leftarrow \ell_i^{\text{new}}, m_i \leftarrow m_i^{\text{new}}$ to HBM.
17:    **end for**
18:  **end for**
19: Return $\mathbf O,\ell, m, \mathcal R$.

## B.4 FlashAttention : Backward Pass 
We des ull details of FlashAttetion backward pass. Given input sequences $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ output $\mathbf{O}\in\mathbb{R}^{N\times d}$ , and the output gradient $\bf dO$ , we want to compute the input gradients $\mathbf {dQ}$ , $\mathbf {dK}$ , $\mathbf{d}\mathbf{V}\in\mathbb{R}^{N\times d}$ . 
> æœ¬èŠ‚è®¨è®º FlashAttention çš„åå‘ä¼ æ’­ç®—æ³•ï¼Œ
> ç®—æ³•çš„è¾“å…¥åŒ…æ‹¬è¾“å…¥åºåˆ—çš„ $\mathbf {Q, K, V}\in \mathbb R^{N\times d}$ã€è¾“å‡ºåºåˆ— $\bf O \in \mathbb R^{N\times d}$ã€è¾“å‡ºåºåˆ—çš„æ¢¯åº¦ $\bf dO$
> ç®—æ³•éœ€è¦è®¡ç®—è¾“å…¥åºåˆ—çš„æ¢¯åº¦ $\mathbf {dQ, dK, dV}\in \mathbb R^{N\times d}$

We first describe the standard attention backward pass in Algorithm 3 for completeness. 

![[FlashAttention-Algorithm3.png]]


**Algorithm 3** Standard Attention Backward Pass
**Requrie**: Matrices $\mathbf {Q, K, V, dO}\in \mathbb R^{N\times d}, \mathbf P \in \mathbb R^{N\times N}$ in HBM.
> æ ‡å‡†çš„ attention åå‘ä¼ æ’­åœ¨ HBM ä¸­å­˜å‚¨äº†å®Œæ•´çš„â€œæƒé‡çŸ©é˜µâ€ $\mathbf P \in \mathbb R^{N\times N}$

1: Load $\bf P, dO$ by blocks from HBM, compute $\mathbf {dV = P^T dO} \in \mathbb R^{N\times d}$, write $\bf dV$ to HBM.
> æ ¹æ® $\bf P$ å’Œ $\bf dO$ è®¡ç®— $\bf dV$

2: Load $\bf O, V$ by blocks from HBM, compute $\mathbf {dP = dO V^T} \in \mathbb R^{N\times N}$, write $\bf dP$ to HBM.
3: Read $\bf P, dP$ from HBM, compute $\mathbf {dS}\in \mathbb R^{N\times N}$ where $dS_{ij} = P_{ij}(dP_{ij} - \sum_l P_{il}dP_{il})$, write $\bf dS$ to HBM.
> æ ¹æ® $\mathbf {OV}$ è®¡ç®— $\bf dP$ 
> æ ¹æ® $\bf dP$ è®¡ç®— $\bf dS$

4: Load $\bf dS$ and $\bf K$ by blocks from HBM, compute $\bf dQ = \bf dSK$, write $\bf dQ$ to HBM.
5: Load $\bf dS$ and $\bf Q$ by blocks from HBM, compute $\bf dK = \bf dS^TQ$, write $\bf dK$ to HBM.
> æ ¹æ® $\bf dS$ å’Œ $\bf K$ è®¡ç®— $\bf dQ$
> æ ¹æ® $\bf dS$ å’Œ $\bf Q$ è®¡ç®— $\bf dK$

We now make two observations about FlashAttention backward pass: 

1. We do not need to store the dropout mask of size $O(N^{2})$ from the forward pass. Instead, we can save the pseudo-random number generator states from the forward pass and re-generate the dropout mask in the backward pass. This allows us to only use $O(N)$ extra memory. 

2. When computing the softmax gradient, we use Eq. (4) to compute $D_{i}=P_{i:}^{\top}d P_{i}$ : without reducing over $P_{i}$ and $d P_{i}$ of size $N$ (they might not fit into SRAM). Instead we can rewrite $D_{i}=d o_{i}^{\top}o_{i}$ and compute the dot product between vectors of size $d$ . 

> FlashAttention çš„æ–¹å‘ä¼ æ’­è¿‡ç¨‹ï¼š
> 1. ä¸ä¼šæ˜¾å¼å­˜å‚¨å¤§å°ä¸º $O (N^2)$ çš„å‰å‘ä¼ æ’­ä¸­è®¡ç®—å¾—åˆ°çš„ dropout maskï¼Œè€Œæ˜¯ä¿å­˜ç”Ÿæˆ dropout mask çš„éšæœºæ•°ç§å­ï¼Œç”¨å®ƒåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é‡æ–°ç”Ÿæˆdropout maskï¼Œè¿™å…è®¸æˆ‘ä»¬ä»…ä½¿ç”¨ $O (N)$ çš„é¢å¤–å†…å­˜
> 2. è®¡ç®— softmax æ¢¯åº¦æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ® eq (4) è®¡ç®— $D_i$ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨ $D_i = P_{i:} ^\top dP_i$ æ¥è®¡ç®—(è¿™æ˜¯ä¸¤ä¸ªå¤§å°ä¸º $N$ çš„å‘é‡ï¼Œå¯èƒ½æ”¾ä¸è¿› SRAM)ï¼Œè€Œæ˜¯ä½¿ç”¨ $D_i = do_i^\top o_i$ (è¿™æ˜¯ä¸¤ä¸ªå¤§å°ä¸º $d$ çš„å‘é‡)

The full FlashAttention backward pass algorithm is in Algorithm 4. Conceptually it is just a block version of the derivation in Appendix B.2. 
> å®Œæ•´çš„ FlashAttention åå‘ä¼ æ’­ç®—æ³•å¦‚ä¸‹
> å…¶æœ¬è´¨ä¸Šä¹Ÿå°±æ˜¯æ ¹æ® Appendix B.2 ä¸­çš„æ¨å¯¼çš„ tiled block version

![[FlashAttention-Algorithm4.png]]

**Algorithm 4** FlashAttention Backward Pass
**Require**: Matrices $\mathbf {Q, K, V, O, dO} \in \mathbb R^{N\times d}$ in HBM, vectors $\ell , m \in \mathbb R^N$ in HBM, on-chip SRAM of size $M$, softmax scaling constant $\tau \in \mathbb R$, masking function $\text{MASK}$, dropout probability $p_{\text{drop}}$, pseudo-random number generator state $\mathcal R$ from the forward pass.
 1: Set the pseudo-random number generator state to $\mathcal R$.
 2: Set block sizes $B_c = \lceil \frac {M}{4d} \rceil, B_r = \min(\lceil \frac M {4d}\rceil, d)$.
 3: Divide $\bf Q$ into $T_r = \lceil \frac N {B_r} \rceil$ blocks $\bf Q_1, \dots, Q_{T_r}$ of size $B_r \times d$ each, and divide $\bf K, V$ into $T_c = \lceil \frac N {B_c} \rceil$ blocks $\bf K_1, \dots, K_{T_c}$ and $\bf V_1, \dots, V_{T_c}$ of size $B_c \times d$ each.
 4: Divide $\bf O$ into $T_r$ blocks $\bf O_1, \dots, O_{T_r}$ of size $B_r \times d$ each, divide $\bf dO$ into $T_r$ blocks $\bf dO_i, \dots, dO_{T_r}$ of size $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_1, \dots, \ell_{T_r}$ of size $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
 5: Initialize $\mathbf {dQ} = (0)_{N\times d}$ in HBM and divide it into $T_r$ blocks $\bf dQ_1, \dots, dQ_{T_r}$ of size $B_r \times d$ each. Initialize $\mathbf {dK} = (0)_{N\times d}, \mathbf {dV} = (0)_{N\times d}$ in HBM and divide it into $T_c$ blocks of size $B_c\times d$ each.
> è®¾å®šéšæœºæ•°ç”Ÿæˆå™¨ã€åˆ†å— $\mathbf {K, Q, V, O,dO}, \ell, m$ã€åˆå§‹åŒ– $\mathbf {dK, dQ,dV}$ å¹¶åˆ†å—

 6: **for** $1\le j \le T_c$ **do**
 7:   Load $\mathbf {K_j, V_j}$ from HBM to on-chip SRAM.
 8:   Initialize $\mathbf {\tilde {dK}}_j = (0)_{B_c\times d}, \mathbf {\tilde {dV}}_j = (0)_{B_c\times d}$ on SRAM.
> å¤–å±‚å¾ªç¯è¿­ä»£ $\mathbf {K, V, dK, dV}$ å—

 9:   **for*** $1 \le i \le T_r$ **do**
10:     Load $\mathbf {Q_i, O_i, dO_i, dQ_i}, \ell_i, m_i$ from HBM to on-chip SRAM.
> å†…å±‚å¾ªç¯è¿­ä»£ $\mathbf {Q, O, dO, dQ}, \ell, m$ å—

11:     On chip, compute $\mathbf {S}_{ij} = \tau \mathbf Q_i \mathbf K_j^{\top} \in \mathbb R^{B_r \times B_c}$.
12:     On chip, compute $\mathbf {S}_{ij}^{\text{masked}} = \text{MASK}({\mathbf S_{ij}})$.
13:     On chip, compute $\mathbf {P}_{ij} = \text{diag}(l_i)^{-1}\exp (\mathbf S_{ij}^{\text{masked}} - m_i)\in\mathbb R^{B_r \times B_c}$.
>  åœ¨ç‰‡ä¸Šæ ¹æ® $\mathbf {K_j, V_j, Q_i, O_i}$ é‡æ–°è®¡ç®—å¾—åˆ°æƒé‡çŸ©é˜µ $\mathbf P$ çš„å— $\mathbf P_{ij}$

14:     On chip, compute dropout mask $\mathbf Z_{ij} \in \mathbb R^{B_r \times B_c}$ where each entry has value $\frac {1}{1 - p_{\text{drop}}}$ with probability $1 - p_{\text{drop}}$ and value $0$ with probability $p_{\text{drop}}$.
15:     On chip, compute $\mathbf P_{ij}^{\text{dropped}} = \mathbf P_{ij} \circ \mathbf Z_{ij}$ (pointwise multiply).
> åœ¨ç‰‡ä¸Šæ ¹æ®éšæœºæ•°ç§å­è®¡ç®— dropout mask $\mathbf Z_{ij}\in \mathbb R^{B_r\times B_c}$ å¹¶è¿›è¡Œ dropout
 
16:     On chip, compute $\mathbf {\tilde {dV}}_j \leftarrow \mathbf {\tilde {dV}}_j + (\mathbf P_{ij}^{\text{dropped}})^\top \mathbf {dO}_i  \in\mathbb R^{B_c \times d}$.
> æ›´æ–° $\mathbf {dV}_j$ï¼šæœ¬è´¨æ˜¯ $\mathbf {dV} = \mathbf P^\top \mathbf {dO}$ åˆ†å—å½¢å¼ï¼Œå›¾è§£è§[[#$ mathbf {dV}$|é™„å½•]]

17:     On chip, compute $\mathbf {dP}_{ij}^{\text{dropped}} = \mathbf {dO}_{i}\mathbf V_j^{\top}\in \mathbb R^{B_r \times B_c}$.
18:     On chip, compute $\mathbf {dP}_{ij}= \mathbf {dP}_{ij}^{\text{dropped}} \circ \mathbf Z_{ij}$ (pointwise multiplly).
> è®¡ç®— $\mathbf {dP}_{ij}^{\text{dropped}}$ ï¼šæœ¬è´¨æ˜¯ $\mathbf {dP} = \mathbf {dOV}^\top$ çš„åˆ†å—å½¢å¼ï¼Œå›¾è§£è§[[#$ mathbf {dP}$|é™„å½•]]
> è®¡ç®— $\mathbf {dP}_{ij}$ï¼š å› ä¸º $\frac {\partial \phi} {\partial P_{ij}} = \frac {\partial \phi}{\partial P_{ij}^{\text{dropped}}}\frac {\partial P_{ij}^{\text{dropped}}}{\partial P_{ij}} = dP_{ij}^{\text{dropped}}\cdot \frac {\partial P_{ij}\cdot Z_{ij}}{\partial P_{ij}} = dP_{ij}^{\text{dropped}}\cdot Z_{ij}$ï¼Œ
> æ•… $\mathbf {dP} = \mathbf {dP}^{\text{dropped}} \circ \mathbf {Z}$

19:     On chip, compute $D_i = \text{rowsum}(\mathbf {dO}_i \circ \mathbf O_i)\in \mathbb R^{B_r}$.
20:    On chip, compute $\mathbf {dS}_{ij} = \mathbf P_{ij} \circ (\mathbf {dP}_{ij}-D_i) \in \mathbb R^{B_r \times B_c}$.
> æ ¹æ® $\mathbf {dO}$ å’Œ $\mathbf O$ è®¡ç®—å½“å‰ $B_r$ è¡Œçš„ $D_i$
> æ ¹æ® $\mathbf {dP}_{ij}$ å’Œ $D_i$ è®¡ç®— $\mathbf {dS}_{ij}$  ($d S_{i j}=P_{i j}(d P_{i j}-D_{i})$)

21:     Write $\mathbf {dQ}_i \leftarrow \mathbf {dQ}_i + \tau\mathbf {dS}_{ij}\mathbf K_j\in \mathbb R^{B_r \times d}$ to HBM.
> æ›´æ–° $\mathbf {dQ}_i$ï¼š$\mathbf {dQ}_i$ æ¯æ¬¡å¤–å±‚å¾ªç¯æ›´æ–°ä¸€æ¬¡ï¼Œå› æ­¤ $\mathbf {dQ}_i$ å—åœ¨å†…å±‚å¾ªç¯ä¸€ç›´ä¿å­˜åœ¨ç‰‡ä¸Šä¸ç°å®ï¼Œæ•…éœ€è¦å†™å› HBMï¼Œå›¾è§£è§[[#$ mathbf {dQ}$|é™„å½•]]

22:     On chip, compute $\tilde {\mathbf {dK}}_j  \leftarrow \tilde {\mathbf {dK}_j} + \tau \mathbf {dS}_{ij}^\top\mathbf Q_i\in\mathbb R^{B_c \times d}$.
> æ›´æ–° $\mathbf {dK}_j$ï¼šæœ¬è´¨æ˜¯ $\mathbf {dK} = \mathbf {dS}^\top \mathbf {Q}$ çš„åˆ†å—å½¢å¼ï¼Œå›¾è§£è§[[#$ mathbf {dK}$|é™„å½•]]

23:   **end for**
24:   Write $\mathbf {dK}_j \leftarrow \mathbf {\tilde {dK}}, \mathbf {dV}_j \leftarrow  \mathbf {\tilde {dV}}$ to HBM.
> å†…å±‚å¾ªç¯ç»“æŸåï¼Œå¯ä»¥å¾—åˆ°è®¡ç®—å¥½çš„ $\mathbf {dK, dV}$ å—

25: **end for**
26: Return $\mathbf {dQ, dK, dV}$.
> å¤–å±‚å¾ªç¯ç»“æŸåï¼Œ$\mathbf {dQ}$ æ‰èƒ½å®Œæ•´ç®—å®Œ

We see that similar to the forward pass, the backward pass performs $O(N^{2})$ FLOPs and only requires $O(N)$ extra memory beyond inputs, output, output gradient, and input gradients. We analyze the IO-complexity of the backward pass, similar to the forward pass (Theorem 2). 
> åå‘ä¼ æ’­éœ€è¦ $O (N^2)$ çš„ FLOPsï¼Œä½†ä»…éœ€è¦ $O (N)$ çš„é¢å¤–å†…å­˜ (é™¤å»è¾“å…¥ã€è¾“å‡ºã€è¾“å‡ºæ¢¯åº¦ã€è¾“å…¥æ¢¯åº¦æ‰€å ç”¨çš„å†…å­˜)

**Theorem 5.** Let ğ‘ be the sequence length, ğ‘‘ be the head dimension, and ğ‘€ be size of SRAM with $d\leq M\leq N d$ . Standard attention (Algorithm 0) backward pass requires $\Theta(N d+N^{2})$ HBM accesses, while FlashAttention backward pass (Algorithm 4) requires $\Theta(N^{2}d^{2}M^{-1})$ HBM accesses. 
> å®šç†ï¼š
> $N$ ä¸ºåºåˆ—é•¿åº¦ï¼Œ$d$ ä¸ºå¤´ç»´åº¦ï¼Œ$M$ ä¸º SRAM å¤§å°ï¼Œæ»¡è¶³ $d\le M \le Nd$
> æ ‡å‡† attention åå‘ä¼ æ’­éœ€è¦ $\Theta (Nd + N^2)$ HBM è®¿é—®
> FlashAttention åå‘ä¼ æ’­éœ€è¦ $\Theta (N^2d^2 M^{-1})$ HBM è®¿é—®

The proof is in Appendix C. 

## B.5 Comparison with Rabe and Staats
We describe here some similarities and diï¬€erences between our FlashAttention algorithm and the algorithm of Rabe and Staats [66]. 

Conceptually, both FlashAttention and Rabe and Staats [66] operate on blocks of the attention matrix using the well-established technique of tiling (or softmax scaling) [ 51 , 60 ]. To reduce the memory footprint, both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward pass. 
>åœ¨æ¦‚å¿µä¸Šï¼ŒFlashAttention å’Œ Rabe ä¸ Staats[66]éƒ½ä½¿ç”¨ tiling (æˆ– softmax scaling) æŠ€æœ¯å¯¹æ³¨æ„åŠ›çŸ©é˜µè¿›è¡Œåˆ†å—çš„è¿ç®—ï¼›ä¸ºäº†å‡å°‘å†…å­˜å ç”¨ï¼Œä¸¤ç§æ–¹æ³•éƒ½åœ¨å‰å‘ä¼ æ’­ä¸­é¿å…å­˜å‚¨å¤§çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå¹¶åœ¨åå‘ä¼ æ’­ä¸­é‡æ–°è®¡ç®—å®ƒ

The first major diï¬€erence is that Rabe and Staats [66] focuses on the reducing the total memory footprint (maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses (the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the primary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount of memory required (e.g., if an operation incurs $A$ memory accesses, then its total memory requirement is at most $A$ ). As a result, FlashAttention is faster than standard attention (2-4x) while Rabe and Staats [66] is around the same speed or slightly slower than standard attention. In terms of total memory required, both methods offer substantial memory saving. 
> ç¬¬ä¸€ä¸ªä¸»è¦å·®å¼‚ï¼š
> [66]å…³æ³¨å‡å°‘æ€»çš„æ˜¾å­˜ä½¿ç”¨é‡ (GPU æ˜¾å­˜çš„æœ€å¤§éœ€æ±‚é‡)ï¼Œè€Œ FlashAttention å…³æ³¨å‡å°‘æ˜¾å­˜è®¿é—®æ•° (æ˜¾å­˜è¯»å†™çš„æ•°é‡)
> å†…å­˜è®¿é—®æ¬¡æ•°æ˜¯è¿è¡Œæ—¶é—´çš„ä¸»è¦å†³å®šå› ç´ ï¼Œæ•…å‡å°‘è®¿é—®æ¬¡æ•°å¯ä»¥æœ‰æ•ˆå‡å°‘è¿è¡Œæ—¶é—´ï¼Œè€Œå‡å°‘è®¿é—®æ¬¡æ•°çš„åŒæ—¶ä¹Ÿå¿…è¦åœ°å‡å°‘äº†æ‰€éœ€è¦çš„æ€»å†…å­˜ (ä¾‹å¦‚ä¸€ä¸ªè¿ç®—è®¿é—®å†…å­˜ $A$ æ¬¡ï¼Œåˆ™å…¶æœ€å¤šçš„å†…å­˜éœ€æ±‚é‡å°±æ˜¯ $A$)

The second difference between the two methods is the way information is summarized from each block to pass to the next block. Rabe and Staats [66] summarizes each block with its temporary output along with the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the blocks are combined using the statistics to produce the final output. FlashAttention instead incrementally updates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed (instead of $K$ copies for $K$ blocks). This means that FlashAttention has smaller total memory requirement compared to Rabe and Staats [66]. 
> ç¬¬äºŒä¸ªå·®å¼‚ï¼š
> ä¿¡æ¯ä»æ¯ä¸ª block æ€»ç»“å¹¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå—çš„æ–¹å¼ä¸åŒ
> [66]ä¸­æ¯ä¸ªå—éƒ½ä¼šç»™å‡ºè‡ªå·±æš‚æ—¶çš„è¾“å‡ºä»¥åŠ softmax è§„èŒƒåŒ–ç»Ÿè®¡é‡ï¼Œåœ¨å‰å‘ä¼ æ’­æœ€åç»“åˆæ‰€æœ‰å—çš„æš‚æ—¶è¾“å‡ºï¼Œç„¶åç”¨ç»Ÿè®¡é‡è®¡ç®—å‡ºæœ€åç»“æœ
> FlashAttention åœ¨å¤„ç†æ¯ä¸ªå—ä¹‹åé€’å¢åœ°æ›´æ–°è¾“å‡º (Algorithm1 line 12)ï¼Œå› æ­¤ä»…éœ€è¦ä¸€ä¸ªè¾“å‡ºæ‹·è´ (è€Œä¸æ˜¯ $K$ ä¸ªå—æœ‰ $K$ ä¸ªæ‹·è´)ï¼Œ å› æ­¤ FlashAttention æœ‰æ›´å°‘çš„æ€»å†…å­˜éœ€æ±‚

The final major diï¬€erence is the way the backward pass is computed. Rabe and Staats [66] uses gradient checkpointing to recompute the attention matrix and the temporary output of each block. FlashAttention instead simplifies the backward pass analytically (Appendices B.2 and B.4). It only recomputes the attention matrix and does not recompute the temporary output of each block. This reduces the memory requirement for the backward pass and yields speedup. 
> ç¬¬ä¸‰ä¸ªä¸»è¦å·®å¼‚ï¼šåå‘ä¼ æ’­
> [66]ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹é‡æ–°è®¡ç®— attention å’Œæ¯ä¸ªå—çš„æš‚æ—¶è¾“å‡º
> FlashAttention è§£æä¸Šåœ°ç®€åŒ–äº†åå‘ä¼ æ’­ï¼Œä»…é‡æ–°è®¡ç®— attention çŸ©é˜µï¼Œä¸éœ€è¦é‡æ–°è®¡ç®—æ¯ä¸ªå—çš„æš‚æ—¶è¾“å‡ºï¼Œè¿™å‡å°‘äº†åå‘ä¼ æ’­çš„å†…å­˜éœ€æ±‚

# C Proofs 
*Proof of Theorem 1.* 
We first count the number of FLOPs and extra memory required. 

The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we compute $\mathbf{Q}_{i}\mathbf{K}_{j}^{\top}\in\mathbb{R}^{B_{r}\times B_{c}}$ for $\mathbf{Q}_{i}\in\mathbb{R}^{B_{r}\times d}$ and $\mathbf{K}_{j}\in\mathbb{R}^{B_{c}\times d}$ , which takes $O(B_{r}B_{c}d)$ FLOPs. We also compute (Algorithm 1 line 12) $\tilde{\mathbf{P}}_{i j}\mathbf{V}_{j}\in\mathbb{R}^{B_{r}\times d}$ for $\tilde{\mathbf{P}}_{i j}\in\mathbb{R}^{B_{r}\times B_{c}}$ and $\mathbf{V}_{j}\in\mathbb{R}^{B_{c}\times d}$ , which takes $O(B_{r}B_{c}d)$ FLOPs. We execute the inner loops $\begin{array}{r}{T_{c}T_{r}=\left\lceil\frac{N}{B_{c}}\right\rceil\left\lceil\frac{N}{B_{r}}\right\rceil}\end{array}$ times. 
Therefore the total number of FLOPs is 

$$
O\left(\frac{N^{2}}{B_{c}B_{r}}B_{r}B_{c}d\right)=O(N^{2}d).
$$ 
> FLOPs:
> Algorithm 1 çš„å†…å±‚å¾ªç¯ä¸­ï¼Œline 9 è®¡ç®—äº† $\mathbf Q_i \mathbf K_j^\top \in \mathbb R^{B_r \times B_c}$ï¼Œå…¶ä¸­ $\mathbf Q_i \in \mathbb R^{B_r\times d}, \mathbf K_j \in \mathbb R^{B_c \times d}$ï¼Œè¯¥çŸ©é˜µä¹˜æ³•çš„ FLOPs æ˜¯ $O (B_r B_c d)$
> Algorithm 1 çš„å†…å±‚å¾ªç¯ä¸­ï¼Œline 12 è®¡ç®—äº† $\tilde {\mathbf P}_{ij}\mathbf V_j \in \mathbb R^{B_r \times d}$ï¼Œå…¶ä¸­ $\tilde {\mathbf P}_{ij} \in \mathbb R^{B_r \times B_c}, \mathbf V_j \in \mathbb R^{B_c \times d}$ï¼Œè¯¥çŸ©é˜µä¹˜æ³•çš„ FLOPs æ˜¯ $O (B_rB_cd)$
>
> å†…å±‚å¾ªç¯ä¸€å…±è¢«æ‰§è¡Œäº† $T_c T_r = \lceil \frac N {B_c} \rceil \lceil \frac N {B_r} \rceil$ æ¬¡
> æ•…æ€» FLOPs å³å®ƒä»¬ç›¸ä¹˜ï¼Œå¦‚ä¸Šæ‰€ç¤ºï¼Œå¾—åˆ° $O (N^2 d)$

In terms of extra memory required, we see that we need $O(N)$ memory to store the statistics $(\ell,m)$ . 
> ç»Ÿè®¡é‡ $\ell, m$ éƒ½ä¸º $N$ ç»´å‘é‡ï¼Œå‰å‘ä¼ æ’­ä¸­ï¼Œéœ€è¦é¢å¤–çš„ $O (N)$ å†…å­˜æ¥å­˜å‚¨è¿™ä¸¤ä¸ªç»Ÿè®¡é‡

We now prove the algorithmâ€™s correctness by induction on $j$ for $0\,\leq\,j\,\leq\,T_{c}$ . Let $\mathbf{K}_{:j}\,\in\,\mathbb{R}^{j B_{c}\times d}$ be the first $j B_{c}$ rows of $\mathbf K$ , and similarly $\mathbf{V}_{:j}\in\mathbb{R}^{j B_{c}\times d}$ the the first $j B_{c}$ rows of $\mathbf{V}$ . Let $\mathbf{S}_{:,:j}=\mathbf{Q}\mathbf{K}_{:j}^{\top}\in\mathbb{R}^{N\times j B_{c}}$ , and $\mathbf{P}_{:,:j}=\mathrm{softmax}(\mathbf{S}_{:,:j})\in\mathbb{R}^{N\times j B_{c}}$ (softmax applied row-wise). Let $m^{(j)},\ell^{(j)},\mathbf{O}^{(j)}$ be the values of $m,\ell,\mathbf{O}$ in HBM after the $j$ -th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of $m,\ell,\mathbf{O}$ are updated after each iteration of the outer loop.) 
> æ¥ä¸‹æ¥é€šè¿‡å¯¹ $0\le j \le T_c$  ($j$ æ˜¯å¤–å±‚å¾ªç¯æ¬¡æ•°)å½’çº³æ¥è¯æ˜ç®—æ³•çš„æ­£ç¡®æ€§
> $\mathbf K, \mathbf V$ çš„å‰ $jB_c$ è¡Œè®°ä¸º $\mathbf K_{: j}, \mathbf V_{:j}  \in \mathbb R^{jB_c \times d}$ï¼Œ$\mathbf S_{:, : j} = \mathbf Q\mathbf K_{: j}^\top \in \mathbb R^{N\times jB_c}$, $\mathbf P_{:,: j} = \text{softmax}(\mathbf S_{:,: j}) \in \mathbb R^{N\times jB_c}$ï¼Œè®° $m^{(j)}, \ell^{(j)}, \mathbf O^{(j)}$ ä¸ºç¬¬ $j$ æ¬¡å¤–å±‚å¾ªç¯ä¹‹å $m, \ell, \mathbf O$ åœ¨ HBM ä¸­çš„å€¼ (å®ƒä»¬çš„å€¼åœ¨æ¯ä¸€æ¬¡å¤–å±‚å¾ªç¯æ›´æ–°ä¸€æ¬¡)

We want to show that after the $j$ -th iteration of the outer loop, we have computed in HBM: 
> è¦è¯æ˜çš„æ˜¯åœ¨ç¬¬ $j$ è½®å¤–å±‚å¾ªç¯ä¹‹åï¼ŒHBM ä¸­çš„è®¡ç®—ç»“æœä¸ºï¼š

$$
\begin{align}
m^{(j)} &= \text{rowmax}(\mathbf S_{:,:j})\in \mathbb R^N\\
\ell^{(j)} &= \text{rowsum}(\exp(\mathbf S_{:,:j}-m^{(j)})\in \mathbb R^N\\
\mathbf O^{(j)}&= \mathbf P_{:,:j}\mathbf V_{:,j} \in \mathbb R^{N\times d}
\end{align}
$$

Based on our initialization (Algorithm 1 line 2), this claim is true for $j=0$ (i.e., before the any iteration of the outer loop is executed). Suppose that the claim holds for some $j=0,\dots,T_{c}-1$ . We want to show that the claim also holds for $j+1$ . 

Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10) on the $(j+1)$ -th iteration of the outer loop, we update $m^{(j+1)}=\operatorname*{max}(m^{(j)},\tilde{m})$  where $\tilde{m}\in\mathbb{R}^{N}$ is the row-max of $\mathbf{S}_{:,j:j+1}$ , the slice of $\mathbf S$ from column $j B_{c}$ to column $(j+1)B_{c}-1$ . This implies that 
> åœ¨ç¬¬ $j+1$ æ¬¡å¤–å±‚å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬åœ¨ Algorithm 1 line 11 æŒ‰ç…§ $m^{(j+1)} = \max (m^{(j)}, \tilde m)$ æ›´æ–°ç»Ÿè®¡é‡ $m$ï¼Œ
> å…¶ä¸­ $\tilde m \in \mathbb R^N$ å°±æ˜¯åˆ‡ç‰‡ $\mathbf S_{:, j:j+1}$ (ä»ç¬¬ $jB_c$ åˆ—åˆ°ç¬¬ $(j+1) B_c - 1$ åˆ—) çš„ rowmaxï¼Œè€Œ $m^{(j)}$ åˆ™æ˜¯ $\mathbf S_{:, :j}$ (ä»ç¬¬ $1$ åˆ—åˆ°ç¬¬ $jB_c - 1$ åˆ—)çš„ rowmaxï¼Œæ˜¾ç„¶ï¼Œ$m^{(j+1)}$ å°±æ˜¯ $\mathbf S_{:,:j+1}$ (ä»ç¬¬ $1$ åˆ—åˆ°ç¬¬ $(j+1) B_c - 1$ åˆ—) çš„ rowmax

$$
\begin{array}{r}{m^{(j+1)}=\operatorname{rowmax}(\mathbf{S}_{:,:j+1})\in\mathbb{R}^{N}.}\end{array}
$$ 
Similarly, we update 

$$
\ell^{(j+1)}=e^{m^{(j)}-m^{(j+1)}}\ell^{(j)}+e^{\tilde{m}-m^{(j+1)}}\tilde{\ell},
$$ 
where $\begin{array}{r}{\tilde{\ell}=\mathrm{rowsum}(\exp(\mathbf{S}_{:,j:j+1}-\tilde{m}))\in\mathbb{R}^{N}}\end{array}$ . By the same algebraic manipulation in Section 3.1, we obtain: 

$$
\ell^{(j+1)}=\mathrm{rowsum}(\exp(\mathbf{S}_{:,:j+1}-m^{(j+1)}))\in\mathbb{R}^{N}.
$$
> ç±»ä¼¼åœ°ï¼Œ$\mathcal \ell$ çš„æ›´æ–°å…¬å¼ $\ell^{(j+1)}=e^{m^{(j)}-m^{(j+1)}}\ell^{(j)}+e^{\tilde{m}-m^{(j+1)}}\tilde{\ell}$ ä¸­ï¼Œ$\ell^{(j)}$ ä¸º $\mathbf S_{:, :j}$ çš„æ”¾ç¼©åæŒ‡æ•° rowsumï¼Œ$\tilde \ell$ ä¸º $\mathbf S_{:, j:j+1}$ çš„æ”¾ç¼©åæŒ‡æ•° rowsumï¼Œæ›´æ–°å…¬å¼ä½¿ç”¨æœ€æ–°çš„ $m$ é‡æ”¾ç¼© $\ell^{(j)}$ å’Œ $\tilde \ell$ï¼Œç„¶åç›¸åŠ ï¼Œå¾—åˆ° $\mathbf S_{:, :j+1}$ çš„æ”¾ç¼©åæŒ‡æ•° rowsum

Let $\mathbf{V}_{j:j+1}$ be the slice of $\mathbf{V}$ from column $j B_{c}$ to column $(j+1)B_{c}-1$ , we also update: 

$$\begin{align*} 
\mathbf{O}^{(j+1)} &= \mathrm{diag}(\ell^{(j+1)})^{-1} \left( \mathrm{diag}(\ell^{(j)}) e^{m^{(j)} - m^{(j+1)}} \mathbf{O}^{(j)} + e^{\tilde{m} - m^{(j+1)}} \exp(\mathbf{S}_{j:j+1} - \tilde{m}) \mathbf{V}_{j:j+1} \right) \\ 
&= \mathrm{diag}(\ell^{(j+1)})^{-1} \left( \mathrm{diag}(\ell^{(j)}) e^{m^{(j)} - m^{(j+1)}} \mathbf{P}_{:,:j} \mathbf{V}_{:j} + e^{-m^{(j+1)}} \exp(\mathbf{S}_{j:j+1}) \mathbf{V}_{j:j+1} \right) \\
&= \mathrm{diag}(\ell^{(j+1)})^{-1} \left( \mathrm{diag}(\ell^{(j)}) e^{m^{(j)} - m^{(j+1)}} \mathrm{diag}(\ell^{(j)})^{-1}\exp(\mathbf{S}_{:,:j} - m^{(j)}) \mathbf{V}_{:j} + e^{-m^{(j+1)}} \exp(\mathbf{S}_{j:j+1}) \right) \\ 
&= \mathrm{diag}(\ell^{(j+1)})^{-1} \left( e^{-m^{(j+1)}} \exp(\mathbf{S}_{:,:j}) \mathbf{V}_{:j} + e^{-m^{(j+1)}} \exp(\mathbf{S}_{j:j+1}) \mathbf{V}_{j:j+1} \right) \\ 
&= \mathrm{diag}(\ell^{(j+1)})^{-1} \left( \exp(\mathbf{S}_{:,:j} - m^{(j+1)}) \mathbf{V}_{:j} + \exp(\mathbf{S}_{j:j+1} - m^{(j+1)}) \mathbf{V}_{j:j+1} \right) \\ 
&= \mathrm{diag}(\ell^{(j+1)})^{-1} \left( \exp \left( \left[ \mathbf{S}_{:,:j} \quad \mathbf{S}_{j:j+1} \right] - m^{(j+1)} \right) \right) \begin{bmatrix}\mathbf V_{:j}\\\mathbf{V}_{j:j+1}\end{bmatrix}\\ 
&= \mathrm{softmax}(\mathbf{S}_{:,:j+1}) \mathbf{V}_{:j+1}. \end{align*}$$

>  $\mathbf O$ åœ¨å¤–å±‚å¾ªç¯ä¹‹é—´çš„æ›´æ–°å…¬å¼å¦‚ä¸Šï¼Œå®ƒåšçš„äº‹æƒ…åŒ…æ‹¬ï¼š
>  è°ƒèŠ‚ $\mathbf V_{:, j}$ åŠ æƒæ±‚å’Œçš„æƒé‡ (æƒé‡çš„ç›¸å¯¹å¤§å°æ²¡æœ‰æ”¹å˜ï¼Œåªæ˜¯éšç€ $\ell$ çš„æ›´æ–°è€Œæ›´æ–°äº†å½’ä¸€åŒ–å¸¸æ•°)
>  è°ƒèŠ‚ $\mathbf V_{:, j: j+1}$ åŠ æƒæ±‚å’Œçš„æƒé‡ (åŒæ ·ï¼Œç›¸å¯¹å¤§å°ä¸å˜ï¼Œåªæ˜¯æ›´æ–°äº†å½’ä¸€åŒ–å¸¸æ•°)
>  å¯¹ $\mathbf V_{:, j}$ å’Œ $\mathbf V_{:, j:j+1}$ åŠ æƒæ±‚å’Œï¼Œç„¶åç›¸åŠ ï¼Œå¾—åˆ°æ›´æ–°çš„ $\mathbf O$

We then see that the claim is also true for $j+1$ . By duction, the claim is true for all $j=0,\dots,T_{c}$ . When $j=T_{c}$ , we conclude that the final value of $\mathbf O$ in HBM is $\text{softmax}(\mathbf S)\mathbf V = \text{softmax }(\mathbf{Q}\mathbf{K}^{\top})\mathbf{V}$ . 

*Proof of Theorem 2.* 
We first analyze the IO complexity of standard attention implementation. The inputs $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$ reside in HBM, and the at the end of the algorithm the output $\mathbf{O}\in\mathbb{R}^{N\times d}$ is written to HBM. 
> é¦–å…ˆåˆ†ææ ‡å‡† attention å®ç°çš„ IO å¤æ‚åº¦
> è¾“å…¥ $\mathbf {Q, K, V}\in \mathbb R^{N\times d}$ å­˜å‚¨äº HBM ä¸­ï¼Œè¾“å‡º $\mathbf O \in \mathbb R^{N\times d}$ éœ€è¦å†™å› HBM

In the first step of computing the matrix multiply $\begin{array}{r}{\mathbf{S}=\mathbf{Q}\mathbf{K}^{\top}}\end{array}$ , the inputs $\mathbf{Q},\mathbf{K}$ are read from HBM and the output $\mathbf{S}\in\mathbb{R}^{N\times N}$ is written to HBM (Algorithm 0 line 1). This incurs $\Theta(N d+N^{2})$ HBM accesses. 
> ç¬¬ä¸€æ­¥ (Algorithm 0 line 1) è®¡ç®— $\mathbf S = \mathbf Q \mathbf K^\top$ï¼Œè¾“å…¥ $\mathbf {Q, K}\in \mathbb R^{N\times d}$ éœ€è¦ä» HBM è¯»å–ï¼Œè¾“å‡º $\mathbf S\in \mathbb R^{N\times N}$ éœ€è¦å†™å› HBMï¼Œä¸€å…±éœ€è¦ $\Theta (Nd + N^2)$ HBM è®¿é—®

In the second step of computing $\mathbf{P}=\mathrm{softmax}(\mathbf{S})$ , the input $\mathbf S$ is read from HBM and the output $\mathbf{P}$ is written to HBM (Algorithm 0 lin 2). This incurs $\Theta(N^{2})$ HBM accesses. 
> ç¬¬äºŒæ­¥ (Algorithm 0 line 2) è®¡ç®— $\mathbf P = \text{softmax}(\mathbf S)$ï¼Œè¾“å…¥ $\mathbf S \in \mathbb R^{N\times N}$ ä» HBM è¯»å–ï¼Œè¾“å‡º $\mathbf P\in \mathbb R^{N\times N}$ éœ€è¦å†™å› HBMï¼Œä¸€å…±éœ€è¦ $\Theta (N^2)$ HBM è®¿é—®

In the last step of computing $\mathbf{O}=\mathbf{P}\mathbf{V}$ , the inputs $\mathbf{P},\mathbf{V}$ are read from global memory and the output $\mathbf{O}$ is written to HBM (Algorithm 0 line 3). This incurs $\Theta(N d+N^{2})$ HBM accesses. 
> ç¬¬ä¸‰æ­¥ (Algorithm 0 line 3) è®¡ç®— $\mathbf O = \mathbf {PV}$ï¼Œè¾“å…¥ $\mathbf P \in \mathbb R^{N\times N}, \mathbf V \in \mathbb R^{N\times d}$ ä» HBM è¯»å–ï¼Œè¾“å‡º $\mathbf O\in \mathbb R^{N\times N}$ éœ€è¦å†™å› HBMï¼Œä¸€å…±éœ€è¦ $\Theta (Nd + N^2)$ HBM è®¿é—®

Overall, standard attention implementation requires $\Theta(N d+N^{2})$ global memory accesses. 
> æ•…æ ‡å‡†ç®—æ³•éœ€è¦ $\Theta (Nd + N^2)$ HBM è®¿é—®

We now analyze the IO complexity of streaming attention. 

Following Algorithm 1, we see that each element of $\mathbf{K}$ and $\mathbf{V}$ is loaded from HBM once (Algorithm 1 line 6). We make $T_{c}$ passes over $\mathbf{Q}$ and $\mathbf{O}$ , each pass loading all of $\mathbf{Q}$ and all of $\mathbf{O}$ to HBM (Algorithm 1 line 8). Therefore the number of HBM accesses is $\Theta\left(N d+N d T_{c}\right)=\Theta(N d T_{c})$ . 
> Algorithm 1ä¸­ï¼Œ$\mathbf {K, V}$ ä¸­çš„æ¯ä¸ªå…ƒç´ ä»…ä» HBM ä¸­è£…è½½åˆ° SRAM ä¸­ä¸€æ¬¡
> Algorithm 1ä¸­ï¼Œæˆ‘ä»¬å¯¹ $\mathbf Q, \mathbf O$ è¿›è¡Œäº† $T_c$ æ¬¡éå† ($T_c$ æ¬¡å¤–å±‚å¾ªç¯)ï¼Œæ¯æ¬¡éå†éƒ½ä¼šé™†ç»­å°† $\mathbf {Q, O}$ çš„å…¨éƒ¨å…ƒç´ ä» HBM ä¸­è£…è½½åˆ° SRAM ä¸­ä¸€æ¬¡
> å› æ­¤ HBM è®¿é—®æ¬¡æ•°æ˜¯ $\Theta (Nd + NdT_c) = \Theta (Nd T_c)$

We derive the conditions on the block sizes $B_{c}$ and $B_{r}$ . We need the blocks $\mathbf{K}_{j}$ and $\mathbf{V}_{j}$ of size $B_{c}\times d$ to fit into on-chip memory, which translates to: 

$$
B_{c}d=O(M)\Leftrightarrow B_{c}=O\left({\frac{M}{d}}\right).
$$ 
Similarly, we need the blocks $\mathbf{Q}_{i},\mathbf{0}_{i}$ of size $B_{r}\times d$ to fit into on-chip memory, which translates to: 

$$
B_{r}d=O(M)\Leftrightarrow B_{r}=O\left({\frac{M}{d}}\right).
$$ 
Finally, we need the block $\mathbf{S}_{i j}$ of size $B_{r}\times B_{c}$ to fit into on-chip memory, which translates to: 

$$
B_{r}B_{c}={{O}}(M).
$$

> $B_c, B_r$ éœ€è¦æ»¡è¶³ $B_c = O (\frac M d), B_r = O (\frac M d), B_rB_c = O (M)$

We therefore set: 

$$
B_{c}=\Theta\left(\frac{M}{d}\right),\qquad B_{r}=\Theta\left(\operatorname*{min}\left(\frac{M}{d},\frac{M}{B_{c}}\right)\right)=\Theta\left(\operatorname*{min}\left(\frac{M}{d},d\right)\right).
$$ 
We then have: 

$$
T_{c}=\frac{N}{B_{c}}=\Theta\left(\frac{N d}{M}\right).
$$ 
As a result, the number of HBM accesses is: 

$$
\Theta\left(N d T_{c}\right)=\Theta\left(\frac{N^{2}d^{2}}{M}\right).
$$ 
*Proof of Proposition 3.* For contradiction, suppose that there exists an algorithm that computes exact attention where the number for HBM access for all $M\in[d,N d]$ is 
> åè¯æ³•ï¼Œå‡å®šå­˜åœ¨å¤æ‚åº¦å¦‚ä¸‹çš„ç²¾ç¡® attention ç®—æ³•

$$
o\left(\frac{N^{2}d^{2}}{M}\right).
$$ 
In the regime of $M=\Theta(N d)$ , this results in the number of HBM accesses: 

$$
o\left(\frac{N^{2}d^{2}}{N d}\right)=o(N d).
$$ 
However, the input to attention (matrices $\mathbf{Q},\mathbf{K},\mathbf{V}$ ) and the output $\mathbf{O}$ have size $N d$ and they start out being in HBM, so if the algorithm computes exact attention it must incur at least $\Omega(N d)$ HBM accesses. This is a contradiction.
> attention çš„è¾“å…¥å’Œè¾“å‡ºçš„å¤§å°éƒ½ä¸º $Nd$ï¼Œä» HBM è¯»å–è¾“å…¥çš„è®¿é—®æ¬¡æ•°å°±è‡³å°‘ä¸º $\Omega (Nd)$ï¼Œæ•…çŸ›ç›¾

*Proof of Theorem 5.* The IO complexity of the attention backward is very similar to the IO complexity of the attention forward (Theorem 2). Here we provide a sketch of the proof. 

We first analyze the IO complexity of standard attention backward pass. The inputs $\mathbf{Q},\mathbf{K},\mathbf{V},\mathbf{d}\mathbf{O}\in\mathbb{R}^{N\times d}$ reside in HBM, and the at the end of the algorithm the outputs $\mathbf{dQ},\mathbf{dK},\mathbf{dV}\in\mathbb{R}^{N\times d}$ are written to HBM. 
> é¦–å…ˆåˆ†ææ ‡å‡† attention å®ç°çš„ IO å¤æ‚åº¦
> è¾“å…¥ $\mathbf {Q, K, V, dO}\in \mathbb R^{N\times d}$ å­˜å‚¨äº HBM ä¸­ï¼Œè¾“å‡º $\mathbf {dQ, dK, dV} \in \mathbb R^{N\times d}$ éœ€è¦å†™å› HBM

At each step of the standard attention backward pass, one needs to load inputs of size ğ‘ğ‘‘ or $N^{2}$ from HBM, and needs to write the outputs of size $N^{2}$ or $N d$ to HBM. This incurs $\Theta(N d+N^{2})$ HBM accesses.
> æ ‡å‡†ç®—æ³•ä¸­çš„æ¯ä¸€æ­¥éƒ½éœ€è¦ä» HBM ä¸­è£…è½½å¤§å°ä¸º $Nd$ æˆ– $N^2$ çš„è¾“å…¥ï¼Œå¹¶ä¸”éœ€è¦å°†å¤§å°ä¸º $Nd$ æˆ– $N^2$ çš„è¾“å‡ºå†™å› HBMï¼Œå› æ­¤è®¿é—®æ¬¡æ•°ä¸º $\Theta (Nd + N^2)$

We now analyze the IO complexity of FlashAttention backward pass. 

Similar to Theorem 2, we see that each element of $\mathbf{K}$ and $\mathbf{V}$ is loaded from HBM once. Each element of $\mathbf {dK}$ and $\mathbf {dV}$ is only written to HBM once. We make $T_{c}$ passes over $\mathbf{Q},\mathbf{O},\mathbf{dO}$ , each pass loading all of $\mathbf {Q , O , dO}$ to HBM. We also make $T_{c}$ passes over $\mathbf{d}\mathbf{Q}$ , each pass reading/writing all of $\mathbf {dQ}$ from/to HBM. Therefore the number of HBM accesses is $\Theta\left(N d+N d T_{c}\right)=\Theta(N d T_{c})$ . 
> $\mathbf {K, V}$ çš„æ¯ä¸ªå…ƒç´ ä»…è£…è½½ä¸€æ¬¡ï¼Œ$\mathbf {dK, dV}$ çš„æ¯ä¸ªå…ƒç´ ä»…å†™å›ä¸€æ¬¡
> ç®—æ³•å¯¹ $\mathbf {Q, O, dO}$ éå†äº† $T_c$ æ¬¡ï¼Œæ¯æ¬¡éå†è£…è½½å…¨éƒ¨çš„ $\mathbf {Q, O ,dO}$
> ç®—æ³•å¯¹ $\mathbf {dQ}$ éå†äº† $T_c$ æ¬¡ï¼Œæ¯æ¬¡éå†è¯»å†™å…¨éƒ¨çš„ $\mathbf {dQ}$
> æ•… HBM è®¿é—®æ¬¡æ•°ä¸º $\Theta (Nd + NdT_c) = \Theta (NdT_c)$

As in the proof of Theorem 2, the constraints on the block sizes are that: 

$$
B_{c}=\Theta\left(\frac{M}{d}\right),\qquad B_{r}=\Theta\left(\operatorname*{min}\left(\frac{M}{d},d\right)\right).
$$ 
We then have: 

$$
T_{c}=\frac{N}{B_{c}}=\Theta\left(\frac{N d}{M}\right).
$$ 
As a result, the number of HBM accesses is: 

$$
\Theta\left(N d T_{c}\right)=\Theta\left(\frac{N^{2}d^{2}}{M}\right).
$$ 
# D Extension Details 
## D.1 Block-sparse FlashAttention 
We describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical to Algorithm 2, except that we skip zero blocks. 
> å—ç¨€ç– FlashAttention çš„ç®—æ³•å’Œæ™®é€šå‰å‘ç®—æ³•å”¯ä¸€çš„åŒºåˆ«å°±æ˜¯è·³è¿‡äº†é›¶å—

![[FlashAttention-Algorithm 5.png]]

**Algorithm 5** Block-Sparse FlashAttention Forward Pass
**Require**: Matrices $\mathbf {Q, K, V} \in \mathbb R^{N\times d}$ in HBM, on-chip SRAM of size $\bf M$, softmax scaling constant $\tau \in \mathbb R$, masking function $\text{MASK}$, dropout probability $p_{\text{drop}}$, block sizes $B_c = \lceil \frac {M}{4d} \rceil, B_r = \min (\lceil \frac {M}{4d}\rceil, d)$, block sparsity mask $M \in \{0, 1\}^{N/B_r \times N/B_c}$
 1: Initialize the pseudo-random number generator state $\mathcal R$ and save to HBM.
 2: Initialize $\mathbf O = (0)_{N\times d} \in \mathbb R^{N\times d}$, $\ell = (0)_N \in \mathbb R^N$, $m = (-\infty)_N \in \mathbb R^N$ in HBM.
 3: Divide $\mathbf Q$ into $T_r = \lceil \frac {N}{B_r} \rceil$ blocks $\mathbf Q_1, \dots, \mathbf Q_{T_r}$ of size $B_r\times d$ each, and divide $\mathbf K, \mathbf V$ into $T_c = \lceil \frac {N}{B_c} \rceil$ blocks $\mathbf K_q, \dots, \mathbf K_{T_c}$ and $\mathbf V_1, \dots, \mathbf V_{T_c}$, of size $B_c \times d$ each.
 4: Divide $\mathbf O$ into $T_r$ blocks $\mathbf O_i, \dots, \mathbf O_{T_r}$ of size $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_i, \dots, \ell_{T_r}$ of size $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
 5: **for** $1\le j \le T_c$ **do**
 6:    Load $\mathbf K_j, \mathbf V_j$ from HBM to on-chip SRAM.
 7:    **for** $1\le i \le T_r$ **do**
 8:      **if** $M_{ij} \ne 0$ **then**
 9:        Load $\mathbf Q_i, \mathbf O_i , \ell_i, m_i$ from HBM to on-chip SRAM.
10:       On chip, computes $\mathbf S_{ij} = \tau \mathbf Q_i \mathbf K_{j}^T\in \mathbb R^{B_r \times B_c}$.
11:        On chip, computes $\mathbf S_{ij}^{\text{masked}}  = \text{MASK}(\mathbf S_{ij})$.
12:        On chip, computes $\tilde m_{ij} = \text{rowmax}(\mathbf S_{ij}^{\text{masked}}) \in \mathbb R^{B_r}$, $\tilde {\mathbf P}_{ij}=\exp (\mathbf S_{ij}^{\text{masked}} - \tilde m_{ij})\in \mathbb R^{B_r \times B_c}$ (pointwise), $\tilde \ell_{ij} = \text{rowsum}(\tilde {\mathbf P}_{ij} )\in \mathbb R^{B_r}$.
13:        On chip, compute $m_i^{\text{new}} = \max (m_i, \tilde m_{ij}) \in \mathbb R^{B_r}$, $\ell_i^{\text{new}} = e^{m_i - m_i^{new}} \ell_i  + e^{\tilde m_{ij} - m_i^{\text{new}}} \tilde \ell_{ij} \in \mathbb R^{B_r}$.
14:        On chip, compute $\tilde {\mathbf P}_{ij}^{\text{dropped}} = \text{dropout}(\tilde {\mathbf P}_{ij}, p_{\text{drop}})$.
15:        Write $\mathbf O_i \leftarrow \text{diag}(\ell_i^{\text{new}})^{-1}(\text{diag}(\ell_i)e^{m_i-m_i^{\text{new}}}\mathbf O_i + e^{\tilde m_{ij} - m_i^{\text{new}}}\tilde {\mathbf P}_{ij}^{\text{dropped}}\mathbf V_j)$ to HBM.
16:        Write $\ell_i \leftarrow \ell_i^{\text{new}}, m_i \leftarrow m_i^{\text{new}}$ to HBM.
17:      **end if**
18:    **end for**
19:  **end for**
20: Return $\mathbf O,\ell, m, \mathcal R$.

We prove the IO-complexity of block-sparse FlashAttention . 

*Proof of Proposition 4.* The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice that we only need to load blocks corresponding to nonzero blocks. As a result, the number of HBM accesses are scaled by $s$ , the fraction of nonzero blocks in the block-sparsity mask. However, for small values of $s$ , we would still need to write the result $\mathbf{O}\in\mathbb{R}^{N\times d}$ . Therefore the number of HBM accesses is 
> ç±»ä¼¼å®šç†äºŒçš„è¯æ˜ï¼Œå®é™…çš„ HBM è®¿é—®åº”è¯¥ä¸º $\Theta (Nd + N^2d^2 T_cs)$ï¼Œç„¶åå°† $T_c$ ä»£å…¥å³å¯å¾—åˆ° $\Theta (Nd + \frac {N^2d^2}{M}s)$
> è¯æ˜å®šç†äºŒæ—¶ï¼Œå°† $\Theta (Nd + N^2 d^2 T_c)$ åŒ–ç®€ä¸ºäº† $\Theta (N^2d^2T_c)$ï¼Œå› ä¸º $T_c$ æ˜¾ç„¶å¤§äº $1$ï¼Œæ•… $N^2d^2 T_c$ çš„æ•°é‡çº§æ˜¾ç„¶é«˜äº $Nd$ï¼Œè€Œ block-sparse ä¸‹è¯¥é¡¹ä¼šå— $s$ æ”¾ç¼©ï¼Œå¦‚æœ $s$ å¾ˆå°ï¼Œåˆ™ $Nd$ ä¸å¯å¿½è§†ï¼Œæ•…æ­¤æ—¶å‰é¢çš„ $Nd$ é¡¹é€‰æ‹©ä¸åŒ–ç®€è€Œä¿ç•™

$$
\Theta\left(N d+\frac{N^{2}d^{2}}{M}s\right)\,.
$$ 
## D.2 Potential Extensions 
We discuss here a few potential extensions of the IO-aware approach to speed up deep learning training. 

**Multi-GPU Attention.** Large language models are trained on hundreds or thousands of GPUs, and one typically splits the attention computation between 4-8 GPUs on the same node [77]. This introduces another level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other GPUs. For very long sequences, the diï¬€erent GPUs on the same node can cooperate to compute attention by taking into account the asymmetry of diï¬€erent levels of memory hierarchy. 
> å¤š GPU ä¸Šçš„ attention è®¡ç®—å¼•å…¥äº†æ–°çš„å†…å­˜å±‚æ¬¡ï¼šå…¶ä»– GPU çš„ HBM å’Œ SRAM

**Sparse MLP layers.** Typical dense MLP layers are compute-bound and not memory-bound. To improve their efficiency, MLP layers with sparse weight matrices can be used [17]. However, many sparse MLP layers are instead memory-bound, and their speedup is often not proportional to the sparsity. We believe that an IO-aware implementation can alleviate this issue and realize the benefits of sparsity. We are excited about future work in this direction, to reduce the computational requirement of large models and improve their wall-block runtime. 
> dense MLP å±‚ä¸€èˆ¬æ˜¯ compute-bound è€Œä¸æ˜¯ memory-bound
> sparse MLP å±‚åˆ™å­˜åœ¨ memory-boundï¼Œå­˜åœ¨ IO-aware ä¼˜åŒ–çš„å¯èƒ½æ€§

**Kernel machine learning.** Our approach in FlashAttention relies on the fact that the $N\times N$ attention matrix is a function of a low-rank matrix $\mathbf {QK}^{\top}$ (of rank $d\ll N$  ). As a result, we can repeatedly load the inputs $\mathbf{Q},\mathbf{K}$ and recompute the block of the attention matrix that we need, significantly reducing HBM access. As similar scenario happens in kernel machine learning: each element $K_{i j}$ of the $N\times N$ kernel matrix $\mathbf{K}$ is a function of two vectors of size $d\ll N$ , as it measures the similarity between two datapoints $x_{i}$ and $x_{j}$ . The KeOps library [8 , 26] is a successful example of how reducing memory reads/writes can speed up kernel operations. We hope that this will motivate kernel methods that focus more on reducing IOs instead of just FLOPs. 
> $N\times N$ çš„ attention çŸ©é˜µå®é™…æ˜¯ç”±ä½ç§©çŸ©é˜µ $\mathbf {QK}^{\top}$ è®¡ç®—å¾—åˆ° ($d\ll N$)ï¼ŒFlashAttention åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨ $N$ ç»´ tileï¼Œåå¤è£…è½½ $\mathbf {Q, K}$ å—ï¼Œé‡è®¡ç®—æ‰€éœ€çš„ attention çŸ©é˜µå—
> kernel ML ä¸­ï¼Œ$N\times N$ çš„ $\mathbf K$ çŸ©é˜µä¸­ $K_{ij}$ æ˜¯å…³äºä¸¤ä¸ªå¤§å°ä¸º $d\ll N$ çš„å‘é‡çš„å‡½æ•°ï¼Œæ˜¯ç›¸ä¼¼çš„

# E Full Experimental Results 
## E.1 BERT 
We train BERT-large following the training procedure and hyperparameters of the reference MLPerf 1.1 implementation. In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448, trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language modeling) reaches the target $72.0\%$ , and the wall-clock run-time is measured. We train with FP16 precision using Apex AMP (with O2 optimization level). 

We compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1 (Table 1). We use the same train / validation data split provided by MLPerf 1.1 reference implementation. In particular, we evaluate on the same 10000 validation examples as the baseline from Nvidia. We train the model on 8 $\times$ A100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we average the results of 10 runs. 
> æ¨¡å‹ï¼šBERT-large
> è¶…å‚æ•°ï¼šå’Œ MLPerf 1.1 ä¸€è‡´
> ç²¾åº¦ï¼šFP16 Apex AMP

## E.2 GPT-2 
We use the standard implementations of GPT-2 [67] from Huggingface transformers library and from Nvidiaâ€™s Megatron-LM repo. We follow the training recipe of the Megatron-LM repo. 

We use an eï¬€ective batch size of 512, and use gradient accumulation to fit into available GPU memory. We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and weight decay of 0.1. All models are trained with the same hyperparameters for 400K steps. We run all implementations with mixed-precision training (PyTorch AMP). 

We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select $0.5\%$ of the dataset as the validation set, with the rest being used as training set. This random selection of validation set is done once, and all models are evaluated on the same validation set. 

We train the model on 8 $\times$ A100-40GB GPUs, and we measure the wall-clock training time. Training GPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table 2). 
> æ¨¡å‹ï¼šGPT-2 small/medium
> ç²¾åº¦ï¼šPyTorch AMP
> æ•°æ®é›†ï¼šOpenwebtext

In Fig. 4, we plot of the validation perplexity throughout training of GPT-2 small/medium, using either HuggingFace implementation or our FlashAttention implementation. We see that FlashAttention behaves the same as the baseline implementation and the validation perplexity curves of the two implementations almost lie on top of each other. 
> FlashAttention çš„éªŒè¯é›† perplexity æ›²çº¿å’Œ HuggingFace çš„å®ç°å®Œå…¨é‡åˆ

![[FlashAttention-Fig4.png]]

**Long Document Classification.** For MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al. [13]. 
## E.3 LRA details 
We follow the hyperparameters from the Long-range arena paper [ 80 ], the Long-range arena repo ( https: //github.com/google-research/long-range-arena ), and the NystrÃ¶mformer reproduction [ 90 ]. To be generous to the baseline methods, if we are unable to reproduce the performance of any baseline for any of the five tasks, we report the better performance from Tay et al. [80] or Xiong et al. [90] for that baseline on that task. 

After hyperparameter tuning, almost all of the attention methods achieve similar accuracy on all of the five LRA tasks. We run all methods with mixed-precision training, except for Performer (not stable with mixed precision) and Local Attention (implementation does not support FP16). To calculate the overall wallclock-time speedup, we take the geometric mean of the wallclock-time speedup of each of the five tasks. 

**Path-X** For Path-X and Path-256, we follow the hyperparameters from the PathFinder-32 experiments from the long-range arena paper [80]. For both, we first pretrain a model on Path-64. We take the checkpoint after 200 epochs, upsample its positional embedding (we duplicate the positional embeddings gridwise in space), and fine-tune it on the downstream task for 200 epochs with one epoch of linear warmup, and cosine decay of the learning rate. For Path-X, we take the best performing checkpoint (according to val accuracy), and additionally fine-tune it for 200 epochs with the same warmup and learning rate (this adds roughly 4 points of accuracy to FlashAttention for Path-X, but the model starts overfitting afterwards). 

## E.4 Comparison with Apex FMHA 
We compare our method/implementation with Apex FMHA ( https://github.com/NVIDIA/apex/tree/master/apex/contrib/csrc/fmha ). 

![[FlashAttention-Table7.png]]

When we started this project, Apex FMHA was the fastest implementation of attention (that we knew of), tailored for short sequences of length at most 512. In fact, almost all MLPerf submissions for BERT training benchmark running on Nvidia GPUs use FMHA for their model code, as of MLPerf 1.1 [58]. Since FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation $\text{dropout}(\text{softmax}(\text{MASK}(\mathbf {QK}^{\top})))\mathbf V$ into one CUDA kernel. In the forward pass, it stores the attention matrix $\text{softmax} (\text{MASK} (\mathbf {QK}))$ to HBM to be used in gradient computation. As a result, it does not oï¬€er substantial memory saving (though for shorter sequences memory footprint is often not a primary concern). 
> Apex FMHA ä»…æ”¯æŒ 64 ç»´ head dimensionï¼Œä»…è¿è¡Œäº A100 GPU
> FMHA å°†è®¡ç®— $\text{dropout}(\text{softmax}(\text{MASK}(\mathbf {QK}^{\top})))\mathbf V$ èåˆåˆ°ä¸€ä¸ª CUDA kernelï¼Œä½†åœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œå®ƒå°† attention çŸ©é˜µ $\text{softmax} (\text{MASK} (\mathbf {QK}))$ å­˜å‚¨åˆ° HBM ä»¥åœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨ï¼Œæ•…å¹¶æ²¡æœ‰èŠ‚çœå†…å­˜

We use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputation) to deal with long sequences and to save memory as mentioned in Section 3. As a result, we can support much longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and broader GPU types (all Turing and Ampere GPUs at the time of writing). 
> æˆ‘ä»¬ç”¨ tiling å’Œ recomputation æ‹“å±•äº† FMHAï¼Œä½¿å…¶å¯ä»¥å¤„ç†æ›´é•¿åºåˆ—ã€æ”¯æŒæ›´å¤š GPU ç±»å‹

In Table 7, we compare the performance of FlashAttention and Apex FMHA for short sequences (as FMHA only supports sequence length at most 512). Generally FlashAttention is slightly faster than FMHA in the forward pass and slightly slower than FMHA in the backward pass. This is because we do not store the attention matrix in the forward pass and recompute it in the backward pass. Compared to FMHA, the overall runtime of FlashAttention is about 4% slower for sequence length 128, 8% faster for sequence length 256, and 5% faster for sequence length 512. 
> FlashAttention åœ¨çŸ­åºåˆ—é•¿åº¦ä¸‹åå‘ä¼ æ’­æ¯” FMHA ç•¥æ…¢ï¼ŒåŸå› æ˜¯ FlashAttentionæ²¡æœ‰å­˜å‚¨å‰å‘ä¼ æ’­ä¸­çš„ attention çŸ©é˜µè€Œæ˜¯é‡è®¡ç®—

## E.5 Speedup On Different Hardware and Configurations 
Speedup varies between diï¬€erent types of GPU types and generations depending on HBM bandwidth and SRAM size. In this section, we profile FlashAttention speedup on diï¬€erent GPUs and configurations. 

**A100** Figure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention heads, across diï¬€erent sequence lengths. We generally see 2-4 Ã— speedup, and we see more speedup when using dropout and masking due to kernel fusion. 

**A100, Head Dimension 128** Speedup also changes when we increase the head dimension. Each block requires more memory, so we need to use smaller block sizes to fit into SRAM. Figure 6 shows speedup with head dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overallâ€”but we can still see significant speedup (up to $3\times$ ) with a causal mask, where half the blocks are masked out. 
> head dimension å¢å¤§åï¼Œæ¯ä¸ªå—çš„å¤§å°éœ€è¦ç›¸åº”å˜å°
> causal mask ä¸‹ï¼Œspeedup å¹…åº¦æ›´å¤§

**RTX 3090** Figure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention heads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5 $\times$ ), since the memory bandwidth on an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s). 

**T4** Figure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block sizes smaller in FlashAttention . As a result, we observe less speedup on T4, which matches the IO complexity analysis in Section 3.2. T4 GPUs are commonly used for inference, so we also report speedup on the forward pass only. 

## E.6 Full Benchmarking Results 
We report the full benchmarking results and experimental details on A100. 

**Baselines** We compare against reference implementations for exact attention from PyTorch/HuggingFace and Megatron, approximate attention, and sparse attention. For approximate attention, we compare against reference implementations of Reformer [ 51 ], Local Attention [ 68 ], Linformer Attention [ 84 ], Smyrf [ 19 ], and LongShortFormer (LSFormer) [ 94 ]. For sparse attention, we compare against reference implementations of Block-Sparse Attention form OpenAI [ 11 ], Longformer[ 3 ], and BigBird Attention [ 92 ]. For the approximate and sparse attention, we use a compression ratio of $1/8$ , or a compressed sequence length of 256, whichever is smaller. 

**Setup** We measure runtime and memory usage of the attention computation with 8 heads of dimension 64, and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM. We vary sequence length in our experiments. We compute attention on random vectors for $\mathbf{Q}$ , $\mathbf{K}$ , and $\mathbf{V}$ (we do not measure the projection from the hidden layer). For dropout, we use dropout 0.1; for masking, we use a padding mask with uniformly-random mask lengths between the total sequence length and the total sequence length minus 20. To measure runtime, we take the average of 100 measurements of the attention call. We only measure memory footprint once, since it does not vary between runs. 
> mask ç”¨ padding maskï¼Œå…¶é•¿åº¦é€šè¿‡åœ¨ \[åºåˆ—é•¿åº¦-20, åºåˆ—é•¿åº¦\] ä¸­å‡åŒ€éšæœºé‡‡æ ·å¾—åˆ°

We report timing results on the forward pass, backward pass, and combined forward $^+$ backward pass. We measure each method with and without dropout, masking, or bothâ€”except for Block Sparse, Longformer, and BigBird. These methods did not successfully run the backward pass with masking due to a bug in external libraries, so we measured them without masking to be generous. We use FP16 for all measurements, except for Local Attention, whose implementation only supports FP32. 

For each baseline, we increase sequence length until it runs out of memory on the GPU, except for the following exceptions: The Megatron implementation does not support sequence lengths longer than 2048. Block-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not support sequence lengths longer than 8092. 

We measure memory usage on the combined forward $^+$ backward pass, without dropout or masking. 

**Results** Table 8 summarizes all the experimental configurations and contains pointers to the results tables. 

# Appendix
## Figure Illustration for FlashAttention Forward Algorithm
### $\mathbf S$
å†…å±‚å¾ªç¯ + å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig8.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig9.png]]

### $\mathbf O$
å†…å±‚å¾ªç¯ + å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig10.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig11.png]]

å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig12.png]]

### Generalization for forward pass
å†…å±‚å¾ªç¯ + å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig13.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig14.png]]

å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig15.png]]

## Deductions
### Deduction for chain rule of matrix multiplication
è€ƒè™‘çŸ©é˜µä¹˜æ³• $\mathbf {LR} = \mathbf Y$ï¼Œå…¶ä¸­ $\mathbf L \in \mathbb R^{m\times k}, \mathbf R \in \mathbb R^{k\times n}, \mathbf Y \in \mathbb R^{m\times n}$

æœ‰ $\mathbf Y$ ç›¸å¯¹äºæŸä¸ªæ ‡é‡å‡½æ•° $\phi$ çš„å¯¼æ•°ï¼Œè®°ä½œ $\frac {\partial \phi}{\partial \mathbf Y} = \mathbf {dY}$

#### $\mathbf {dL}$
è€ƒè™‘ ${\mathbf {dL}}$ ä¸­çš„ç¬¬ $ij$ ä¸ªå…ƒç´ ï¼š

$$
\begin{align}
\frac {\partial \phi}{\partial L_{ij}} &= \sum_{k=1}^m\sum_{l=1}^n\frac {\partial \phi}{\partial Y_{kl}}\frac {\partial Y_{kl}}{\partial L_{ij}}\\
&=Tr\left[\left(\frac {\partial \phi}{\partial \mathbf Y}\right)^T\frac {\partial \mathbf Y}{\partial L_{ij}}\right]\\
&=Tr\left[(\mathbf {dY})^T\frac {\partial \mathbf Y}{\partial L_{ij}}\right]
\end{align}
$$

å…¶ä¸­ï¼š

$$
\begin{align}
\frac {\partial \mathbf Y}{\partial L_{ij}}&= \frac {\partial\begin{bmatrix}
Y_{11} & \cdots & Y_{1n}\\
\vdots & \ddots & \vdots \\
Y_{m1} & \cdots & Y_{mn}
\end{bmatrix}}{\partial L_{ij}}\\
&=\frac {\partial\begin{bmatrix}
\sum_{t=1}^k L_{1t}R_{t1} & \cdots & \sum_{t=1}^k L_{1t}R_{tn}\\
\vdots & \ddots & \vdots \\
\sum_{t=1}^k L_{mt}R_{t1} & \cdots & \sum_{t=1}^k L_{mt}R_{tn}\\
\end{bmatrix}}{\partial L_{ij}}\\
&=\frac {\partial\begin{bmatrix}
0& \cdots & 0\\
\vdots &  & \vdots \\
\sum_{t=1}^k L_{it}R_{t1} &\cdots & \sum_{t=1}^kL_{it}R_{tn}\\
\vdots & & \vdots\\
0 & \cdots & 0\\
\end{bmatrix}}{\partial L_{ij}}\\
&=\frac {\partial\begin{bmatrix}
0& \cdots & 0\\
\vdots &  & \vdots \\
 L_{ij}R_{j1} &\cdots & L_{ij}R_{jn}\\
\vdots & & \vdots\\
0 & \cdots & 0\\
\end{bmatrix}}{\partial L_{ij}}\\
&= {\begin{bmatrix}
0& \cdots & 0\\
\vdots &  & \vdots \\
 R_{j1} &\cdots & R_{jn}\\
\vdots & & \vdots\\
0 & \cdots & 0\\
\end{bmatrix}}\\
&=\begin{bmatrix}
\mathbf 0_n^T\\
\vdots\\
R_{j:}\\
\vdots\\
\mathbf 0_n^T
\end{bmatrix}
\end{align}
$$

æ³¨æ„ $\frac  {\partial \mathbf Y}{\partial L_{ij}} \in \mathbb R^{m\times n}$ çš„ç¬¬ $i$ è¡Œæ˜¯ $R_{j:}$ï¼Œä¹Ÿå°±æ˜¯ $\mathbf R$ çš„ç¬¬ $j$ è¡Œï¼Œå…¶ä»–æ‰€æœ‰çš„è¡Œéƒ½æ˜¯ $\mathbf 0_n^T$

å› æ­¤ï¼š

$$
\begin{align}
\frac {\partial \phi}{\partial L_{ij}} 
&=Tr\left[(\mathbf {dY})^T\frac {\partial \mathbf Y}{\partial L_{ij}}\right]\\
&=Tr\left[\mathbf {d}\mathbf {Y}^T\frac {\partial \mathbf Y}{\partial L_{ij}}\right]\\
&=Tr\left[\mathbf {dY}^T\begin{bmatrix}
\mathbf 0_n^T\\
\vdots \\
R_{j:}\\
\vdots\\
\mathbf 0_n^T
\end{bmatrix}\right]\\
&=Tr\left[ [(dY^T)_{:1}, \cdots, (dY^T)_{:m}]\begin{bmatrix}
\mathbf 0_n^T\\
\vdots \\
R_{j:}\\
\vdots\\
\mathbf 0_n^T
\end{bmatrix}\right]\\
&=Tr\left[ [(dY_{1:})^T, \cdots, (dY_{m:})^T]\begin{bmatrix}
\mathbf 0_n^T\\
\vdots \\
R_{j:}\\
\vdots\\
\mathbf 0_n^T
\end{bmatrix}\right]\\
&=Tr[R_{j1}(dY_{i:})^T, R_{j2}(dY_{i:})^T,\cdots,R_{jn}(dY_{i:}^T)]\\
&=R_{j1}dY_{i1} + R_{j2}dY_{i2} + \cdots + R_{jn}dY_{in}\\
&=\sum_{t=1}^n dY_{it}R_{jt}\\
&=\sum_{t=1}^n dY_{it}(R^T)_{tj}\\
&=\langle dY_{i:}, R^T_{:j}\rangle
\end{align}
$$

å› æ­¤ $dL_{ij} = \langle dY_{i:}, R_{: j}^T \rangle$ï¼Œæ•…æ˜¾ç„¶ 

$$\mathbf {dL} = \mathbf {dYR}^T$$

#### $\mathbf {dR}$
å°† $\mathbf {LR} = \mathbf Y$ å·¦å³åŒæ—¶è½¬ç½®ï¼Œå¾—åˆ° $\mathbf {R}^T \mathbf {L}^T = \mathbf {Y}^T$

å› æ­¤å®¹æ˜“çŸ¥é“ï¼š

$$
\begin{align}
\mathbf {dR}^T &= \mathbf {dY}^T (\mathbf L^T)^T\\
&=\mathbf {dY}^T \mathbf L\\
\mathbf {dR}&=\mathbf L^T \mathbf {dY}
\end{align}
$$

### Deduction for Jacobian of softmax
è€ƒè™‘ $\pmb y = \text{softmax}(\pmb x)$ï¼Œå…¶ä¸­ $\pmb y, \pmb x \in \mathbb R^{n\times 1}$ ï¼Œæ»¡è¶³

$$
y_i = \frac {\exp(x_i)} {\sum_{i=1}^n\exp{(x_i)}},i=1,\cdots, n
$$

ä¸ºäº†ä¹¦å†™æ–¹ä¾¿ï¼Œè®° $\sum_{i=1}^n \exp (x_i) = L$

$\pmb x$ ç›¸å¯¹äº $\pmb y$ çš„ Jacobian å†™ä½œï¼š

$$
\begin{align}
J = \begin{bmatrix}
\frac {\partial y_1}{\partial x_1}& \cdots & \frac {\partial y_n}{\partial x_1}\\
\vdots & \ddots & \vdots \\
\frac {\partial y_1}{\partial x_1}& \cdots & \frac {\partial y_n}{\partial x_1}
\end{bmatrix}
\end{align}
$$

å…¶ä¸­ï¼Œå¯¹è§’çº¿å…ƒç´ æ»¡è¶³ï¼š

$$
\begin{align}
\frac {\partial y_i}{\partial x_i}&=\frac {\partial y_i}{\partial \exp(x_i)}\frac {\partial \exp(x_i)}{\partial x_i}\\
&=\frac {\partial \frac {t}{t+c}}{\partial t}\cdot \exp(x_i)\\
&=\frac {c}{(t+c)^2}\cdot \exp(x_i)\\
&=\frac {L-\exp(x_i)}{L^2}\cdot \exp(x_i)\\
&=\frac {(L-\exp(x_i))\exp(x_i)}{L^2}\cdot \\
&=\frac {L-\exp(x_i)}{L}\cdot \frac {\exp(x_i)}{L}\\
&=(1-y_i)y_i\\
&=y_i - y_i^2
\end{align}$$

éå¯¹è§’çº¿å…ƒç´ æ»¡è¶³ï¼š

$$
\begin{align}
\frac {\partial y_j}{\partial x_i}&=\frac {\partial y_j}{\partial \exp(x_i)}\frac {\partial \exp(x_i)}{\partial x_i}\\
&=\frac {\partial \frac {\exp(x_j)}{L}}{\partial \exp(x_i)}\cdot \exp(x_i)\\
&=\frac {\partial \frac {1}{L}}{\partial \exp(x_i)}\cdot \exp(x_j)\cdot\exp(x_i)\\
&=\frac {\partial \frac {1}{t+c}}{\partial t}\cdot \exp(x_j)\cdot\exp(x_i)\\
&=\frac {-1}{(t+c)^2}\cdot \exp(x_j)\cdot\exp(x_i)\\
&=\frac {-\exp(x_i)\exp(x_j)}{L^2}\\
&=-\frac {\exp(x_j)}{L}\frac {\exp(x_i)}{L}\\
&=-y_iy_j
\end{align}
$$

è€ƒè™‘ $\text{diag}(\pmb y) - \pmb y\pmb y^T$ ï¼š

$$
\begin{align}
\text{diag}(\pmb y) - \pmb y\pmb y^T&=\begin{bmatrix}
y_1 - y_1^2 & \cdots & -y_1y_n\\
\vdots & \ddots & \vdots \\
-y_ny_1 & \cdots & y_n - y_n^2
\end{bmatrix}
\end{align}
$$

æ•…æ˜¾ç„¶æˆ‘ä»¬æœ‰ï¼š

$$
J = \text{diag}(\pmb y) - \pmb y \pmb y^T
$$

### Deduction for $\mathbf {dS}_{i:}$
è€ƒè™‘ $\mathbf {dS}$ ä¸­çš„ç¬¬ $ij$ ä¸ªå…ƒç´ ï¼š

$$
\begin{align}
\frac {\partial \phi}{\partial S_{ij}} &= \sum_{k=1}^n\sum_{l=1}^n \frac {\partial \phi}{\partial P_{kl}} \frac {\partial P_{kl}}{\partial S_{ij}}\\
&=\sum_{l=1}^n\frac {\partial \phi}{\partial P_{il}}\frac {\partial P_{il}}{\partial S_{ij}}\\
&=\sum_{l=1}^ndP_{il}\frac {\partial P_{il}}{\partial S_{ij}}\\
&=\langle dP_{i:}, \frac {\partial P_{i:}}{\partial S_{ij}}\rangle
\end{align}
$$

è€ƒè™‘ $\mathbf {dS}$ çš„ç¬¬ $i$ è¡Œï¼š

$$
\begin{align}
\frac {\partial \phi}{\partial S_{i:}} &=\left[\frac {\partial \phi}{\partial S_{i1}}, \dots, \frac {\partial \phi}{\partial S_{in}}\right]^T\\
&=\left[\langle dP_{i:}, \frac {\partial P_{i:}}{\partial S_{i1}}\rangle, \dots, \langle dP_{i:}, \frac {\partial P_{i:}}{\partial S_{in}}\rangle\right]^T\\
&=\left[\frac {\partial P_{i:}}{\partial S_{i1}}, \dots, \frac {\partial P_{i:}}{\partial S_{in}}\right]^TdP_{i:}\\
&=\begin{bmatrix}
\frac {\partial P_{i1}}{\partial S_{i1}} & \cdots & \frac {\partial P_{in}}{\partial S_{i1}}\\
\vdots & \ddots & \vdots\\
\frac {\partial P_{i1}}{\partial S_{in}} & \cdots &\frac {\partial P_{in}}{\partial S_{in}}
\end{bmatrix}dP_{i:}\\
&=J\cdot dP_{i:}\\
&=(\text{diag}(P_{i:}) - P_{i:}P_{i:}^T)dP_{i:}\\
&=\text{diag}(P_{i:})dP_{i:} - P_{i:}P_{i:}^TdP_{i:}\\
&=P_{i:}\circ dP_{i:} - (P_{i:}^TdP_{i:})P_{i:}
\end{align}
$$

## Figure Illustration for FlashAttention Backward Algorithm
### $\mathbf {dV}$
å¤–å±‚ + å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig1.png]]

å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig2.png]]

### $\mathbf {dP}$
å¤–å±‚ + å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig3.png]]
### $\mathbf {dQ}$
å¤–å±‚ + å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig4.png]]

å¤–å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig5.png]]

### $\mathbf {dK}$
å¤–å±‚ + å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig6.png]]
å†…å±‚å¾ªç¯ï¼š

![[FlashAttention-App-Fig7.png]]

### Generalization for backward pass
#### $\mathbf {dV}$
å¤–å±‚ + å†…å±‚ï¼š

![[FlashAttention-App-Fig16.png]]

å†…å±‚ï¼š

![[FlashAttention-App-Fig17.png]]

å¤–å±‚ï¼š

![[FlashAttention-App-Fig18.png]]

#### $\mathbf {dQ}$
å†…å±‚ + å¤–å±‚ï¼š

![[FlashAttention-App-Fig19.png]]

å†…å±‚ï¼š

![[FlashAttention-App-Fig20.png]]

å¤–å±‚ï¼š

![[FlashAttention-App-Fig21.png]]

#### $\mathbf {dK}$
å¤–å±‚ + å†…å±‚ï¼š

![[FlashAttention-App-Fig22.png]]

å†…å±‚ï¼š

![[FlashAttention-App-Fig23.png]]

å¤–å±‚ï¼š

![[FlashAttention-App-Fig24.png]]

