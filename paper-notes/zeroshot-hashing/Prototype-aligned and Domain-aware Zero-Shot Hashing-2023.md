[原型对齐和域感知的零样本哈希](<file:///D:\Learning\paper\2023-09-计算机工程-原型对齐和域感知的零样本哈希-1.pdf>)
# 0 概述
# 1 相关工作
## 1.1 传统哈希
传统哈希中，训练集的类别包含了查询样本的类别，不存在属于训练集未见过的类别的查询样本
传统哈希方法分为监督方法和无监督方法

## 1.2 零样本哈希
零样本哈希中，测试样本\查询样本属于的类别可能在训练类别之外，即目标域的类别和源域的类别并不一致

零样本哈希专注于提升模型对未见类样本的泛化能力

作者认为现有的零样本哈希方法通常忽视了强偏差问题，模型不能有效地区分不同域的样本，导致容易把已见类的样本错误地当作未见类
# 2 提出的方法
分为哈希学习和图像检索两阶段

哈希学习包括了原型学习、哈希码学习、哈希函数学习三部分，
首先通过原型学习生成类原型$Y$，
然后类原型$Y$和由类别标签构造的相似度矩阵$S$分别用于原型对齐和成对相似度保持，生成哈希码矩阵$B$和哈希码的实值替换$A$，
最后将生成的哈希码作为监督信息学习数据特征到海明空间的投影$W$

图像检索阶段，
使用映射矩阵$W$分别将检索集样本和查询样本投影到海明空间得到哈希码$D$和$q$，计算它们的哈希距离，
然后通过域感知策略，计算检索集样本哈希码$D$与训练集样本哈希码$B$的相似程度判断检索集样本是否属于源域(是否是见过的类别)，
如果是，则调大它与目标域(未见过的类别)样本的海明距离，以避免检索结果中混入视觉相似但是不同域的样本

## 2.1 符号定义
所有样本划分为两个互不相交的集合：源域$Y_s$和目标域$Y_u$($Y_s \cap Y_u = \emptyset$)
训练集$D_{tr}$来自源域，$D_{tr} \subset Y_s$
测试集$D_{te}$来自目标域，$D_{te}\subset Y_u$
检索集$D_{re}$来自源域和目标域，$D_{re} \subset \{Y_s\cup Y_u\}$

训练集$D_{tr}$的视觉特征矩阵$X \in R^{n\times d}$
训练集$D_{tr}$的类别标签矩阵$L\in \{0,1\}^{n\times c}$
训练集$D_{tr}$的哈希码矩阵$B\in \{-1,1\}^{n\times r}$
其中，$c$是可见类个数，$n$是训练样本个数，$d$是特征维度，$r$是哈希码位数

零样本哈希旨在利用源域的训练数据学习哈希函数$F(\cdot)$，将训练集之外的样本(Out-of-Sample)映射到海明空间$$b = F(x_{out-of-sample})\tag{1}$$

## 2.2 原型学习
预先生成一组哈希码作为各个可见类在海明空间的典型表示，它们彼此之间应该尽可能远离，从而减少不同类的哈希码在海明空间中的碰撞概率
$$\min_{Y}\|YY^T-rC\|_F^2,\ s.t.\ Y\in\{-1,1\}^{c\times r}\tag{2}$$
其中，$Y$是要学习的类原型矩阵，每一行$Y_i$为相应的类原型的哈希码
$C\in\{-1,1\}^{c\times c}$是类相关性矩阵，主对角线元素为$1$，其余元素为$-1$

公式(2)可以等价转换为
$$\min_Y \sum_{i=1}^c\|Y_i\bar Y_i^T+r1^T\|_F^2,\ s.t.\ Y\in\{-1,1\}^{c\times r}\tag{3}$$
其中$Y_i$是类原型矩阵$Y$的第$i$行，$\bar Y_i$是$Y$去除第$i$行$Y_i$之后的矩阵

证明：
容易知道$Y = \begin{bmatrix}Y_1\\\vdots \\Y_n\end{bmatrix}$，$Y^T = [Y_1^T,\cdots,Y_n^T]$，$C = 2I_c - 11^T$
则$$\begin{align} &\|YY^T-rC\|_F^2\\
=&\|\begin{bmatrix}
Y_1Y_1^T - r&\cdots&Y_1Y_n^T+r\\
\vdots & \ddots & \vdots \\
Y_nY_1^T+r & \cdots & Y_nY_n^T-r 
\end{bmatrix}\|_F^2\\
=&\sum_{i=1}^c\|[Y_iY_1^T+r,\cdots,Y_iY_i^T-r,\cdots,Y_iY_n^T+r]\|_F^2\\
\end{align}$$因为对于任意$Y_i$，有$Y_iY_i^T = r$
故
$$\begin{align} &\|YY^T-rC\|_F^2\\
=&\sum_{i=1}^c\|[Y_iY_1^T+r,\cdots,Y_iY_i^T-r,\cdots,Y_iY_n^T+r]\|_F^2\\
=&\sum_{i=1}^c\|[Y_iY_1^T+r,\cdots,0,\cdots,Y_iY_n^T+r]\|_F^2\\
=&\sum_{i=1}^c\|[Y_iY_1^T+r,\cdots,Y_iY_n^T+r]\|_F^2\\
\end{align}$$
而
$$\begin{align}
&\sum_{i=1}^c\|Y_i\bar Y_i^T+r1^T\|_F^2\\
=&\sum_{i=1}^c\|[Y_iY_1^T,\cdots,Y_iY_n^T] + r1^T\|_F^2\\
=&\sum_{i=1}^c\|[Y_iY_1^T+r,\cdots,Y_iY_n^T+r]\|_F^2\\
\end{align}$$
因此
$$\|YY^T-rC\|_F^2 = \sum_{i=1}^c\|Y_i\bar Y_i^T+r1^T\|_F^2$$

将公式(3)的求解分解为$c$个子问题，并按类更新
$$\sum_{i=1}^c\min_{Y_i}\|Y_i\bar Y_i^T + r1^T\|_1,\ s.t.\ Y_i\in\{1,-1\}^{1\times r}\tag{4}$$
每次只更新一个类原型$Y_i$而固定其他类原型，可求得第$i$个类原型的闭式解为
$$Y_i =sgn(-1^T\bar Y_i)\tag{5} $$

证明：
$$\begin{align}
&\min_{Y_i}\|Y_i\bar Y_i^T + r1^T\|_1\\
=&\min_{Y_{i}}\sum_{i\ne j}|Y_iY_j^T+ r|\\
=&\min_{Y_{i}}\sum_{i\ne j} (Y_iY_j^T+r)\\
=&\min_{Y_{i}}\sum_{i\ne j} Y_iY_j^T\\
=&\min_{Y_{i}}Y_i(\sum_{j\ne i} Y_j^T)\\
\end{align}$$

类原型矩阵$Y$的初始化使用Hadamard矩阵，Hadamard矩阵是满秩的正交矩阵，并且矩阵元素取值为$-1$或$1$，其任意两个行向量/列向量内积为$0$
$$HH^T = mI\tag{6}$$
其中$m$为Hadamard矩阵的秩

> Hadamard矩阵是方阵
## 2.3 哈希码学习
### 2.3.1 一致性保持
为了学习语义一致的哈希码，现有的哈希方法通常将哈希码内积近似为ground-truth标签定义的成对相似度矩阵，并由如下成对相似度保持损失实现：
$$\min_B\|BB^T-rS\|_F^2,\ s.t.\ B\in\{-1,1\}^{n\times r}\tag{7}$$
其中，$S = 2LL^T- 11^T\in \{-1,1\}^{n\times n}$表示样本的成对相似度矩阵，取值为$-1$表示两个样本不相似，取值为$1$表示相似
作者认为成对相似度矩阵中的负样本数远多于正样本，如$S$中值为$-1$的元素远多于值为$1$的元素，因此负样本对损失函数的贡献也大于正样本，这会导致生成的哈希码保存更多样本间的不相似关系，产生次优的结果

为了进一步学习语义一致的哈希码，作者设计了原型对齐策略，将类原型作为各个类在海明空间中的典型表示，通过缩小样本哈希码和相应类原型之间的$L_2$距离，实现哈希码与类原型对齐
由于不同类别的原型彼此远离，经过对齐后的哈希码能够保持语义一致性，使相似样本的哈希码更加相似，不相似样本之间相互远离
$$\min_B\|B-LY\|_F^2,\ s.t\ B\in\{-1,1\}^{n\times r}\tag{8}$$

### 2.3.2 联合目标函数
结合公式(7)(8)，得到联合目标函数
$$\min_B\|BB^T-rS\|_F^2+\|B-LY\|_F^2,\ s.t.\ B\in\{-1,1\}^{n\times r}\tag{9}$$
式子中并没有给前后两项添加平衡参数，因为它们的取值有较大差异，通过添加平衡参数很难细致地约束前后两项的作用，因此在后续的优化机制中会使用投票机制

### 2.3.3 高效非对称离散优化
由于哈希码的离散约束，公式(9)是一个对称二进制矩阵分解问题，无法直接求解，因此引入一个实值的辅助变量$A$替换一个哈希码矩阵，将目标函数从四次优化问题简化为二次优化问题，同时为$A$添加正交和平衡约束，使哈希码保持位不相关和位平衡

形式上，目标函数转变为如下形式：
$$\begin{align}
&\min_{B,A}\|BA^T-rS\|_F^2+\|B-LY\|_F^2,\\&s.t.\ B\in\{-1,1\}^{n\times r},A^TA = nI,A^T1 = 0
\end{align}\tag{10}$$
其中，$A^TA= nI$是正交约束，$A^T1 = 0$是平衡约束

通过一个两步迭代优化算法求解公式(10)，首先通过标准正态分布初始化辅助变量$A$和哈希码矩阵$B$，然后通过如下步骤选择性更新$B,A$直到收敛
(一) 求解$B$
固定$A$，将公式(10)转换为如下形式
$$\max_Btr((rSA+LY)B^T),\ s.t.\ B\in\{-1,1\}^{n\times r}\tag{11}$$
推导
固定$A$时，公式(10)可以写为
$$\begin{align}
&\min_{B,A}\|BA^T-rS\|_F^2+\|B-LY\|_F^2\\
=&\min_{B}\|BA^T-rS\|_F^2+\|B-LY\|_F^2\\
=&\min_B tr((BA^T-rS)(BA^T-rS)^T) + tr((B-LY)(B-LY)^T)\\
=&\min_B tr((BA^T-rS)(AB^T-rS))+tr((B-LY)(B^T-Y^TL^T))\\
=&\min_Btr(BA^TAB^T-rBA^TS-rSAB^T+r^2SS)+\\
&\quad\quad tr(BB^T-BY^TL^T-LYB^T+LYY^TL^T)\\
=&\min_Btr(BA^TAB^T)-rtr(BA^TS)-rtr(SAB^T)+r^2tr(SS)+\\
&\quad\quad tr(BB^T)-tr(BY^TL^T)-tr(LYB^T)+tr(LYY^TL^T)\\
=&\min_B ntr(BB^T)-2rtr(BA^TS)+r^2tr(SS)+\\
&\quad\quad tr(BB^T)-2tr(BY^TL^T)+tr(LYY^TL^T)\\
=&\min_B ntr(BB^T)-2rtr(BA^TS)+tr(BB^T)-2tr(BY^TL^T)\\
=&\min_B (n-1)tr(BB^T)-2rtr(SAB^T)-2tr(LYB^T)\\
\end{align}$$
而由于$B\in\{-1,1\}^{n\times r}$，$tr(BB^T) = \|B\|_F^2 = nr$，故
$$\begin{align}
&\min_{B,A}\|BA^T-rS\|_F^2+\|B-LY\|_F^2\\
=&\min_B (n-1)tr(BB^T)-2rtr(SAB^T)-2tr(LYB^T)\\
=&\min_B (n-1)nr-2(rtr(SAB^T+tr(LYB^T)))\\
=&\max_Brtr(SAB^T)+tr(LYB^T)\\
=&\max_B tr(rSAB^T+LYB^T)\\
=&\max_B tr((rSA+LY)B^T)
\end{align}$$

公式(11)的最优解是
$$B = sgn(rSA+LY)\tag{12}$$
推导
$$\begin{align}
&\max_Btr((rSA+LY)B^T)\\
=&\max_B\langle(rSA+LY)^T,B^T\rangle\\
=&\max_B\langle(rSA+LY),B\rangle
\end{align}$$
由于$S$有$n^2$个元素，容易导致第一项在符号化过程中被忽略，为此，采用投票机制，先分别符号化前后两项，再选择各位上票数多的作为结果
$$B = sgn(sgn(rSA)+sgn(LY))\tag{13}$$

(二) 求解$A$
固定$B$，公式(10)可以改写为
$$\max_A tr(rSBA^T),\ s.t.\ A^TA=nI,A^T1 = 0\tag{14}$$

推导
固定$B$时，公式(10)可以写为
$$\begin{align}
&\min_{B,A}\|BA^T-rS\|_F^2+\|B-LY\|_F^2\\
=&\min_A\|BA^T-rS\|_F^2+\|B-LY\|_F^2\\
=&\min_A\|BA^T-rS\|_F^2\\
=&\min_Atr((BA^T-rS)(BA^T-rS)^T)\\
=&\min_Atr((BA^T-rS)(AB^T-rS))\\
=&\min_Atr(BA^TAB^T-rBA^TS-rSAB^T+r^2SS)\\
=&\min_Antr(BB^T)-2r(SAB^T)+r^2tr(SS)\\
=&\max_Atr(SAB^T)\\
=&\max_A tr(SBA^T)
\end{align}$$

如果$AB^T$是对称矩阵，即满足$AB^T = BA^T$，则
$$\begin{align}
&\max_{A}tr(SAB^T)\\
=&\max_{A}tr(SBA^T)
\end{align}$$

为了求解公式(14)，首先定义$J = I - \frac 1 n11^T$，$T = rSB$，然后，对矩阵$T^TJT$进行奇异值分解，得到对角线元素分别是其正特征值的对角矩阵$\Omega$，正特征值对应的特征向量组成的矩阵$Q$，以及零特征值对应的特征向量组成的矩阵$\hat Q$
$$T^TJT = [Q\hat Q]\begin{bmatrix}\Omega&0\\0&0
\end{bmatrix}[Q\hat Q]^T\tag{15}$$

参考[[Discrete Graph Hashing-2014-NeurIPS]]，可以得到公式(14)的最优解为
$$A = \sqrt n[P\bar P][Q\bar Q]^T\tag{16}$$
其中$P = JTQ\Omega^{-1}$，$\bar P$是一个随机的正交矩阵，$\bar Q$则是对$\hat Q$做施密特正交化后得到的正交矩阵

## 2.4 哈希函数学习
学习得到训练集的哈希码后，需要再学习哈希函数将训练样本外的实例映射为哈希码，首先，为了学习数据中的非线性结构信息，使用径向基函数(Radial Basis Function/RBF)将数据的视觉特征进行核化
$$\Phi(x)=\left[\exp(\frac {\|x-g_1\|_2^2}{-2w^2},\cdots,\exp(\frac {\|x-g_k\|_2^2}{-2w^2})\right]\tag{17}$$
其中，$[g_1;\cdots;g_k]$是从训练样本中随机抽取的$k$个锚点，$w$是核宽，$\Phi(\cdot)$是核函数，$\exp(\cdot)$是指数函数

随后，将训练数据的哈希码$B$作为监督信息，采用线性回归模型学习核化特征到哈希码的线性映射，目标函数如下
$$\min_W\|B-\Phi(X)W\|_F^2+\tau\|W\|_F^2\tag{18}$$
其中，$\tau$是正则化系数
令上式关于$W$的导数为零，可以得到其最优解为
$$W = (\Phi(X)^T\Phi(X)+\tau I)^{-1}\Phi(X)^TB\tag{19}$$

对于训练样本之外的实例$x$，可以使用哈希函数$F$将其投影到海明空间中
$$b = F(x) = sgn(\Phi(X)W)\tag{20}$$

## 2.5 域感知策略
由于属于不同域的样本在视觉上也可能具有相似性，模型容易把目标域中的未见实例识别为源域中的已见类，这称为强偏差问题

可以设计一个粗略的分类方法，它不需要确定样本的具体类别，而是区分样本属于已见类还是未见类，随后，在查询排序之前先将检索集中源域和目标域的样本分离，并且只将判定为目标域的样本返回到检索列表中，从而可以提升目标域的检索精度

为此，作者设计了一个域感知策略，首先计算各已见类的哈希中心，然后通过比较检索集各样本与各已见类的哈希中心的海明距离来判定其属于目标域还是源域
作者使用每个类所有样本的均值哈希码作为该类的哈希中心
$$\bar B_i = sgn(mean(B_{[i]}))\tag{21}$$
其中，$B_{[i]}$是第$i$个类样本的哈希码矩阵，$\bar B_i$是第$i$个类别的哈希中心，$mean(\cdot)$是均值函数

然后，计算检索样本哈希码与各已见类哈希中心的海明距离，若最小的海明距离不大于(小于等于)预定义的阈值$E$，则认为这个检索样本属于已见类，其不应该出现在对目标域查询样本的检索列表中，因此在后续排序时，将它与所有查询样本的海明距离设置为比最大海明距离$r$还大的数值，例如$r+1$，
将阈值$E$设置为哈希码长度的倍数
$$E=r/\sigma\tag{22}$$
其中，$r$是哈希码长度，$\sigma$是一个大于1的调节参数

> 24/2/3 
> Q: 作者假设了所有的查询样本(query)都是未见类/目标域的，但实际模型部署的时候，又怎么能事先就确定了query的类别呢？
> 24/2/3
> A: 也可以通过域感知对query进行分类，
> 但是作者没有这么做，作者引入了query一定是属于不可见类的先验知识，
> 但实际情况下，query不一定属于不可见类，因此这种方法会导致属于可见类的query检索精度为0
> 作者的这个方法相当于在一开始就只取了检索集的一个子集

该方法的目的在于，对于一个事先知道了是目标域的query，它可能会和源域的样本视觉上相似，因此其检索结果会包含源域样本，但它们显然是不同类的，因此事先将数据库中判断属于源域的样本都排除
如果将源域query错分为来自目标域，会导致检索不到同类样本，检索精度为零
如果将目标域query错分为来自源域，没有提到有相关措施，因此没有影响
如果将源域query正确分为来自源域，没有提到有相关措施，因此没有影响
如果将目标域query正确分为来自目标域，会提高检索精度，因为完全排除了源域
该方法是只针对目标域query的检索的，但可能影响源域query的检索

作者的数据库/检索集内样本的类别也不是事先知道的，而是通过分类标注的

如果无法确定query样本的类别，需要分类操作
阈值越高，越容易将样本分类为可见类，检索集样本可见类样本数量增加，影响不一定
阈值越低，越容易将样本分类为不可见类，检索集样本不可见类样本数量增加，影响不一定

如果确定query样本一定是不可见类，
则阈值越高，越容易将样本分类为可见类，检索集样本可见类样本数量增加，影响不一定
阈值越低，越容易将样本分类为不可见类，检索集样本不可见类样本数量增加，影响不一定

如果确定query样本一定是不可见类，且与一些可见类样本视觉相似，
则阈值越高，越容易将样本分类为可见类，检索集样本可见类样本数量增加，影响不一定
阈值越低，越容易将样本分类为不可见类，检索集样本不可见类样本数量增加，影响不一定

如果检索集样本的类别是确定的(不太现实)，query样本的类别不确定
阈值越高，越容易将样本分类为可见类，该方法的影响越小
阈值越低，越容易将样本分类为不可见类，可见类样本的检索精度会降低，不可见类样本的检索精度会提高
## 2.6 计算复杂度分析

# 3 实验与分析
## 3.1 数据集与划分
## 3.2 评估标准与对比方法
## 3.3 检索精度分析
## 3.4 训练效率分析
## 3.5 可视化
## 3.6 参数敏感性分析
$\tau$是哈希函数学习中的正则化参数， $\sigma$是调控域感知阈值的特殊参数

$\tau$太小，容易对训练集过拟合，$\tau$太大，容易欠拟合

## 3.7 消融实验
## 3.8 收敛性分析
# 4 结束语
