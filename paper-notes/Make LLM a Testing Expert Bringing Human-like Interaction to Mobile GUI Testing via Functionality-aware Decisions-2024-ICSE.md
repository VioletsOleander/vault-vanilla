# Abstract
Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. 
>  自动化 GUI 测试是确保应用质量的重要一环

Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. 
>  基于学习的技术可以生成类似人类的交互，广泛用于自动化 GUI 测试中，但仍存在局限性，例如测试覆盖率低、泛化能力不足、过度依赖训练数据

Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose `GPTDroid`, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. 
>  我们将移动 GUI 测试问题形式化为问答任务，进而提出 `GPTDroid`，它将 GUI 页面信息传递给 LLM，让 LLM 和移动应用对话，进而生成测试脚本，然后执行该测试脚本，将应用的反馈持续传递给 LLM，整个过程迭代执行

Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. 
>  在此框架内，我们还引入了一种功能感知的记忆提示机制，使 LLM 在整个过程保留测试知识，并且执行长期的基于功能的推理，以指导探索

We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by $32\%$ in activity coverage, and detects $31\%$ more bugs at a faster rate. Moreover, `GPTDroid` identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed. 
>  我们在 Google Play 的 93 个 app 上评估了该框架，结果表明，其活动覆盖率比最优 baseline 高出 32%，并且以更快速度检测出了 31% 更多的 bugs
>  GPTDoird 还发现了 Google Play 的 53 个新 bug，其中 35 个已经被确认并被修复

# 1 Introduction
In recent years, mobile apps have become an indispensable part of our daily life, with millions of apps available for download from app stores like the Google Play Store [4] and Apple App Store [3]. With the rise of app importance in our daily life, it has become increasingly critical for app developers to ensure that their apps are of high quality and perform as expected for users. 

To avoid time-consuming and labor-extensive manual testing, automated GUI (Graphical User Interface) testing is widely used for quality assurance of mobile apps [43, 44, 47, 77, 78, 83], i.e., dynamically exploring mobile apps by executing different actions such as scrolling and clicking to verify the app functionality. 
>  自动 GUI 测试被广泛用于移动应用的测试中，即通过执行不同的动作例如划动、点击以动态地探索移动应用，检验应用的功能

Unfortunately, existing GUI testing tools such as probability-based or model-based ones [31, 56, 63] suffer from low testing coverage, meaning that they may miss important bugs and issues. This is because of the complex and dynamic nature of modern mobile apps [18, 22, 31, 52, 55, 56], which can have hundreds or even thousands of different screens, each with its own unique set of interactions and possible user actions and logic. In addition, test inputs generated by these methods are significantly different from real users’ interaction traces [53], resulting in low testing coverage. 
>  现存的 GUI 测试工具例如基于概率或基于模型的工具的测试覆盖率低，故会错失重要的 bugs
>  这是因为移动应用的交互存在复杂性
>  并且，这些方法生成的测试输入和真实用户的交互轨迹显著不同，进而导致低测试覆盖率

To address these limitations, there has been a growing interest in using deep learning (DL) [34, 79] and reinforcement learning (RL) [50, 54] techniques for automated mobile GUI testing. By learning from human testers’ behavior, these methods aim to generate humanlike actions and interactions that can be used to test the app’s GUI more thoroughly and effectively. These approaches are based on the idea that the more closely the actions performed by the testing algorithm mimic those of a human user, the more comprehensive and effective the testing will be. 
>  使用 DL 和 RL 的方法学习人类测试者的行为以执行自动化 GUI 测试，这些方法的目标是生成类似人类的动作和交互，其思想是算法模拟人类用户动作得越接近，测试就越全面和有效

Nevertheless, there are still some limitations with these DL or RL-based GUI testing methods. First, learning algorithms require large amounts of data which is difficult to collect from real-world users’ interactions. Second, learning algorithms are designed to learn and predict from training data, so they may not generalize well to new, unseen situations, as apps are constantly evolving and updating. Third, mobile apps can be non-deterministic, meaning that the outcome of an action may not be the same every time it is performed (e.g., clicking the “delete” button from a list with the last content would produce an empty list for which the delete button no longer works) which specifically makes it difficult for RL algorithms to learn and make accurate predictions.
>  这些方法的限制在于 1. 训练需要从真实用户交互中收集大量数据 2. 算法的泛化性可能不强，难以泛化到新的未见过的情况，因为 apps 会更新 3. 移动 apps 可能是非确定的，即每次执行相同动作，结果不一定相同 (例如对空列表执行 “删除”)

Therefore, Large Language Models (LLMs) [17, 58, 68, 84] such as GPT-3/4 have emerged as a powerful tool for natural language understanding and question answering. Recent advances in LLM have triggered various studies examining the use of these models for software development tasks[30, 37, 73] The ChatGPT [58] (Chat Generative Pre-trained Transformer) from OpenAI, has billions of parameters and is trained on a vast dataset comprising test scripts and bug reports. Its exceptional performance across diverse domains and topics demonstrates the LLM’s ability to comprehend human knowledge and interact with humans as a knowledgeable expert. Inspired by ChatGPT, we formulate the GUI testing problem as a questions & answering (Q&A) task, i.e., asking the LLM to play the role as a human tester to test the target app. 
>  ChatGPT 的测试数据包含了测试脚本和 bug 报告
>  我们将 GUI 测试问题构造为问答任务，即请求 LLM 扮演人类测试者的角色以测试目标 app

We propose `GPTDroid` for automated GUI testing, which asks LLM to chat with mobile apps by passing the GUI page information to LLM to elicit testing scripts and execute them to keep passing the app feedback to LLM, iterating the whole process. 

To convert the visual information of the app GUI into the corresponding natural language description, we first extract the semantic information of the app and GUI page by decompiling the target app and view hierarchy files, and design linguistic patterns to encode the information as the prompt of LLM. We then utilize few-shot learning by providing demonstrations with the output template to facilitate the LLM generating desired executive commands to execute the app. 
>  为了将 app GUI 的视觉信息转化为对应的自然语言描述，我们反编译目标 app 和其视图层次文件，并设计语言模式将这些信息编码为 LLM 的 prompt
>  我们使用 few-shot learning，为 LLM 展示带有输出模板的演示样例，让 LLM 生成所需的执行命令以运行 app

Nevertheless, there are two main challenges during the interactive Q&A GUI testing process. The first is the local dilemma. Different from the LLM-based program repair or unit test generation which mainly targets a determined piece of software, `GPTDroid` formulates the GUI testing as a multi-turn task and the LLM faces varying GUI pages, i.e., interacting between LLM and mobile app to explore various pages of the app. During the interaction process, it is hard for the LLM to clearly and accurately remember the historical explorations, especially those that happened long before. Because of this, the LLM might only rely on the recent interactive information to make the decision, while omitting the global viewpoints, which can make the exploration fall into a local dilemma and hinder it from achieving higher coverage. The second is the low-level dilemma. Being fed with the descriptive GUI information, the LLM can easily focus more on the low-level semantics as the widgets or activities, yet less on the high-level semantics as the functionalities which is achieved with sequences of operations with the widgets/activities. However, the functional aspect of the mobile app is of high interest to testers and users, and has long been an obstacle to existing techniques. 
>  然而，在交互式问答 GUI 测试流程中，存在两个主要挑战
>  第一个是局部困境，与主要针对软件的特定部分的基于 LLM 的程序修复或单元测试生成不同，`GPTDroid` 将 GUI 测试构建为一个多轮任务，LLM 需要面对不断变化的 GUI 页面，即 LLM 需要和应用不断交互，探索应用的不同页面
>  在交互过程中，LLM 难以清晰且准确地记忆历史探索，尤其是那些发生在很早之前的探索，因此，LLM 可能仅仅依赖最近的交互信息来做决策，忽略了全局视角
>  这使得探索陷入了局部困境，阻碍了测试达到更高的覆盖率
>  第二个是低级困境，在接收到描述性 GUI 信息后，LLM 可以关注到更多低级的语义信息，例如组件或活动，但会较少关注高级语义，例如通过一系列对组件的操作或活动达成的功能
>  而应用的功能方面对于用户和测试人员都非常重要，并且长期依赖，在测试方面探索功能都是现有技术的障碍

To overcome these challenges, within `GPTDroid`, we develop a functionality-aware memory mechanism. It builds a testing sequence memorizer to record all the interactive testing information in terms of the explored activities and widgets. It also queries the LLM about the function-level progress of the testing during the iterative process, e.g., which function is under test, to enable the LLM to conduct the explicit reasoning by itself. And the information is then encoded into a functionality-aware memory prompt and fed into the LLM to enable the LLM in deciding the meaningful operation sequence to explore the app’s functionality and conduct global exploration to cover unexplored areas. 
>  为了克服这些挑战，在 `GPTDroid` 中，我们开发了功能感知的记忆机制
>  该机制构建一个测试序列记忆器，以记录所有交互式测试信息，包括已探索的活动和组件
>  在迭代过程中，该机制向 LLM 查询测试在功能级别的进展，例如哪个功能正在被测试，以便让 LLM 能够自行进行明确的推理
>  这些信息会被编码为功能感知的记忆提示词，这些提示词帮助 LLM 决定有意义的操作序列来探索应用的功能，并执行全局探索以覆盖未探索的区域

![[pics/GPTDroid-FIg1.png]]

One example chat log can be seen in Figure 1. LLM can understand the app GUI, and provide detailed actions to navigate the app (e.g., A1-A5 at Figure 1). To compensate for its wrong prediction (A2 at Figure 1), the real-time feedback by `GPTDroid` guides it to regenerate the input until triggering a valid page transition. It remains clear testing logic even after a long testing trace to make complex reasoning of actions (A3, A4 at Figure 1), and it can prioritize to test important functions earlier (e.g., A5 at Figure 1). A more detailed analysis of `GPTDroid` ’s capability is in Section 5. 
>  一个聊天记录的示例见 Fig1
>  LLM 通过 prompt 理解程序 GUI，然后提供详细的导航操作步骤
>  LLM 出现了错误预测后，`GPTDroid` 的实时反馈将引导其重新生成输入，直到触发一个有效的页面转换
>  `GPTDroid` 在长时间的测试跟踪后仍然可以保持清晰的测试逻辑，进行复杂的动作推理，并且可以优先测试重要的功能

To evaluate the effectiveness of `GPTDroid`, we carry out an experiment on 93 popular Android apps in Google Play with 143 bugs. Compared with 10 common-used and state-of-the-art baselines, `GPTDroid` can achieve more than $32\%$ boost in activity coverage and $20\%$ boost in code coverage than the best baseline, resulting in $75\%$ activity coverage and $66\%$ code coverage. As `GPTDroid` can cover more activities, the method can detect $31\%$ more bugs with a faster speed than the best baseline. 
>  为了评估 `GPTDroid` 的有效性，我们在 93 个具有 143 个 bug 的 Android 应用上执行了试验，和 10 个 baseline 比较
>  `GPTDroid` 的活动覆盖率比 sota 高出了 32%，代码覆盖率高出了 20%，其活动覆盖率和代码覆盖率分别是 75% 和 66%

Apart from the accuracy of our `GPTDroid`, we also evaluate the usefulness of our `GPTDroid` by detecting unseen crash bugs in real-world apps from Google Play. Among 223 apps, we obtain 53 crash bugs with 35 of them being confirmed and fixed by developers, while the remaining are still pending. 
>  除了测试 `GPTDrodi` 的正确性外，我们还评估了 `GPTDroid` 的有效性，我们用 `GPTDroid` 检测真实应用中的未见过的 bugs
>  在 223 个应用中，我们检测出了 53 个 crash bugs，其中的 35 个已经被开发者确认并修复，其余的仍在等待处理

To reveal reasons behind the promising performance of our approach, we further investigate the experiment results qualitatively and summarize 4 findings including function-aware exploration through long meaningful testing trace, function-aware prioritization, valid text input and compound action. 
>  为了揭示我们的方法表现优异的背后原因，我们进一步对实验结果进行了定量分析，并总结了 4 个发现，包括: 基于长而有意义的测试轨迹的功能感知探索、功能感知优先级排序、有效文本输入、复合动作

The contributions of this paper are as follows: 

- Vision. The first work to formulate the automatic GUI testing problem to an interactive question & answering task to let the LLM conduct the whole app testing by understanding the GUI semantic information and automatically inferring possible operation steps. 
- Technique. A function-aware automatic GUI testing approach `GPTDroid` which designs the function-aware memory mechanism to enable the LLM to focus more on the global and functional viewpoints of the mobile app. 
- Evaluation. Effectiveness and usefulness evaluation of the `GPTDroid` in the real-world apps with practical bugs detected (Section 3 and 4). 
- Insight. Detailed qualitative analysis revealing the reasons why LLM can generate human-like and functionality-aware actions for app testing (Section 5). 

>  本文的贡献包括:
>  - 视角: 首次将自动 GUI 测试问题构建为一个交互式问答任务，让 LLM 通过理解 GUI 语义信息并自动推断可能的操作步骤来完成整个应用测试过程
>  - 技术: 提出一种功能感知的自动 GUI 测试方法 `GPTDroid` ，`GPTDroid` 基于功能感知的记忆机制使得 LLM 能够关注移动应用的全局和功能视角
>  - 评估: 评估了 `GPTDroid` 的有效性和实用性
>  - 观点: 详细定性分析并揭示了为什么 LLM 可以生成类似人类且功能感知的应用测试操作

# 2 Approach

![[pics/GPTDroid-Fig2.png]]

We model the GUI testing as a Question & Answering (Q&A) problem, i.e., asking the LLM to play a role as a human tester, and enabling the interactions between the LLM and the app under testing. To realize this, we propose `GPTDroid`, as demonstrated in Figure 2, with nested loops. 
>  我们将 GUI 测试建模为问答问题，即让 LLM 扮演人类测试者的决策，在测试中，由 LLM 和应用交互
>  为了实现这一点，我们提出了 `GPTDroid` ，其结构如 Fig 2 所示，`GPTDroid` 的工作流程是一个嵌套循环

In the outer loop, it extracts the GUI context information of the current GUI page, encodes them into prompt questions for LLM, decodes LLM’s feedback answer into actionable operation scripts to execute the app, and iterates the whole process. Specifically, in each iteration of the testing, `GPTDroid` first obtains the view hierarchy file of the mobile app and extracts the GUI context including the app information, information of the current GUI page, and details of each widget in the page. We then design linguistic patterns for generating the GUI prompt as input of LLM. We utilize the idea of few-shot learning to enable the LLM’s output to conform with our expected standards which can be directly executed in the app, by providing the demonstrations as reference. 
>  在外层循环中，`GPTDroid` 提取当前 GUI 页面的 GUI 上下文信息，将其编码为 LLM 的提示词，输入给 LLM，接收 LLM 的输出，将其解码为可以用于执行应用的动作脚本，整个过程如此迭代
>  具体地说，在测试的每个迭代中，`GPTDroid` 首先获取移动应用的视图层次文件，并从中提取 GUI 上下文，包括应用信息、当前 GUI 页面信息、页面中各个组件的细节
>  这些信息会经由设计的 GUI prompt 模式转化为 prompt，作为 LLM 的输入，我们利用 few-shot learning 的思想，为 LLM 提供了演示示例作为参考，以使得 LLM 的输出复合预期标准

In the inner loop, it builds a testing sequence memorizer to record all the detailed interactive testing information, e.g., the explored activities and widgets. During the process, the memorizer also stores the functionality-level progress of testing, e.g., which function is under test, which is derived by querying the LLM and is to enable LLM to conduct the explicit reasoning by itself.
>  在内层循环中，`GPTDroid` 构建了一个测试序列记忆器，以记录所有的细节交互测试信息，例如探索过的活动和组件，测试序列记忆器还存储测试在功能级别上的进展，例如哪个函数正在被测试，这一进展情况直接通过询问 LLM 得到

We also design linguistic patterns to encode the information into the functionality-aware memory prompt, to equip LLM with the capability of retaining knowledge of the whole testing and conducting the long-term reasoning. The prompt in both the outer loop and inner loop would together input into the LLM for querying the next operation. 

## 2.1 GUI Context Extraction 
Despite its excellence on various tasks, the performance of LLM can be significantly influenced by the quality of its input, i.e., whether the input can precisely describe what to ask [16, 35, 89]. 

In the scenario of this interactive mobile GUI testing, we need to accurately depict the GUI page currently under test, as well as its contained widgets information from a more micro perspective, and the app information from a more macro perspective. 

This Section describes which information will be extracted, and Section 2.2 will describe how we organize the information into the style that LLM can better understand. GUI context relates to the information of the app, the GUI page currently tested, and all the widgets on the page. 

![[pics/GPTDroid-Table1.png]]

The app information is extracted from the AndroidMaincast.xml file, while the other two types of information are extracted from the view hierarchy file, which can be obtained by UIAutomator [67]. Table 1 presents the summarized view of them. 
>  app 信息从 `AndroidMaincast.xml` 文件中提取，当前测试的 GUI 页面信息和页面中的组件信息从视图层次文件中提取，视图层次文件通过 `UIAutomator` 得到

**App information** provides the macro-level semantics of the app under testing, which facilitates the LLM to gain a general perspective about the functions of the app. The extracted information includes the name of the app and the name of all its activities. 
>  app 信息提供了被测试 app 的宏观语义，app 信息包括了 app 名称和其所有活动的名称

**Page GUI information** provides the semantics of the current page under testing during the interactive process, which facilitates the LLM to capture the current snapshot. We extract the activity name of the page, all the widgets represented by the “text” field or “resource-id” field (the first non-empty one in order), and the widget position of the page. For the position, inspired by the screen reader [61, 69, 86], we first obtain the coordinates of each widget in order from top to bottom and from left to right, and the widgets whose ordinate is below the middle of the page is marked as lower, and the rest is marked as upper. 
>  页面 GUI 信息提供了当前交互页面的语义信息，页面 GUI 信息包括页面活动名称、页面的所有组件的名称或 id、页面的组件位置
>  我们按照从上到下，从左到右的顺序获取组件坐标，并将纵坐标低于页面中间的组件标记为 lower，其余标记为 upper

**Widget information** denotes the micro-level semantics of the GUI page, i.e., the inherent meaning of all its widgets, which facilitates the LLM in providing actionable operational steps related to these widgets. The extracted information includes “text”, “hint-text”, and “resource-id” field (the first non-empty one in order), “class” field, and “clickable” field. To avoid the empty textual fields of a widget, we also extract the information from nearby widgets to provide a more thorough perspective, which includes the “text” of parent node widgets and sibling node widgets. 
>  组件信息提供了 GUI 页面的微观语义，即每个组件的意义
>  组件信息包括组件的名称或 id、组件的文本和提示文本、组件的类、组件是否可点击，以及组件的父组件和邻居组件的文本

## 2.2 GUI Prompting and Executive Command Generation 
With the extracted information, we design linguistic patterns to generate prompts for inputting into the LLM. We first conduct preprocessing for the information, to facilitate the follow-up design. We tokenize attributes by the underscore and Camel Case [6] considering the naming convention in app development. 
>  提取出的信息将基于我们设计的语言模式生成 LLM 的输入 prompt
>  我们先对信息预处理，我们将属性通过下划线或 Camel Case 进行分词 (tokenize)

### 2.2.1 Linguistic Patterns of GUI Prompt. 
To design the patterns, each of the five annotators is asked to write the prompt sentence following regular prompt template [14, 16, 26], and questions the LLM for generating the operation steps. We then check to what extent the recommended operation is reasonable considering the whole testing process. With the prompt sentences, the five annotators then conduct card sorting [60] and discussion to derive the linguistic patterns. 

![[pics/GPTDroid-Table2.png]]

As shown in Table 2, this process comes out with 6 linguistic patterns corresponding with the three sub-types of information in Table 1 and two operation & feedback patterns. 
>  一共有 6 个语言模式，分别对应于 Table 1 中三个信息类型、两类操作模式、一个反馈模式

**Pattern related to GUI context (Table 2-Id 1,2,3)** 
We design three patterns to describe the overview of the GUI page currently under testing, respectively corresponding to the app information, page GUI information, and widget information in Table 1. 

**Pattern related to operation & feedback question (Table 2-Id 4,5,6)** 
We also design patterns to describe operation and feedback questions. For the operational questions, we ask the LLM what operation is required. We also provide the output template in the prompt to enable the LLM to generate a desired executive command for testing the app, and details are in Section 2.2.3. Note that, we separate the patterns for querying general action (e.g., click) and text input (e.g., input certain text) with pattern-Id 4 and 5 respectively, to enable the generated commands can better match the widgets in the GUI page. For the feedback question, after deciding the previous operation is not applicable, we inform the LLM that there is no such widget on the current page, and let it re-try. 

### 2.2.2 Prompt Generation Rules. 
Since the designed patterns describe information from different points of view, we combine the patterns from different viewpoints and generate the prompt rules as shown in Table 2. We design three kinds of prompts respectively for starting the test, routine inquiry, and getting feedback in case of error occurred. Note that, due to the robustness of the LLM, the prompt sentence doesn’t need to follow the grammar completely. 

**Test prompt** is the most commonly used prompt for informing the LLM of the current status and query for the next operation. Specifically, we tell the LLM the GUI context, i.e., the information about the current GUI page and detailed widget information; then ask the LLM which operation is required. 
>  Test Prompt 用于告知 LLM 当前 GUI 的上下文信息，然后请求 LLM 下一个动作是什么

**Feedback prompt** is used for informing the LLM error occurred and re-try for querying the next operation. Specifically, we first tell LLM its generation operation cannot correspond to the widget on the page; re-provide it the detailed widget information of the page and let the LLM recommend the operation again. 
>  Feedback prompt 用于告知 LLM 发生的错误，并重新请求下一个动作

Besides the above two kinds of prompts, we additionally design **start prompt** to start the testing of the app and only used it once. Different from the test prompt, it provides the LLM with the app information including all activities for a global overview. 
>  Start Prompt 仅被使用一次，用于启动测试流程，该 prompt 为 LLM 提供 app 信息和其所有活动

### 2.2.3 Executive Command Generation. 
After inputting the generated prompt, LLM will output the natural language sentence of operation, e.g., input 3500 for price, input salary in title widget and personal in category widget, then click submit which depicts the example output for the second image in Figure 1. 

Considering that a testing operation can be expressed in different ways and with different words, it is challenging to map natural language testing operations to the app for execution. Therefore, we utilize the idea of in-context learning to provide LLM with the output template, including available operations and operation primitives, which can be mapped directly to the instructions for executing the app. 
>  为了格式化 LLM 输出的操作流程，我们提供了输出模板，模板中包含了可用的操作和操作原语，操作原语可以直接映射到用于执行 app 的指令

**Available operations.** We identify five commonly used standard operations for mobile applications, including click, double-click, long press, scroll and input. Although there are other customizations, such as custom gestures, they are not common, and we leave them for future work. 
>  移动应用常用的五个标准操作是: 点击、双击、长按、滑动、输入，其他的操作例如自定义手势不考虑

**Output templates.** We design different operation primitives for the above-mentioned operations to represent widgets and executive commands. The above five operations can be divided into two main categories, i.e., action (the first four operations) and input. For the action category, we formulate it as `<Operation> [click / double-click / long press / scroll] + <Widget name>`, e.g., `Operation: “Click”. Widget: “ADD INCOME”`. For the input category, the GUI page usually involves text field widgets for entering specific values, and the follow-up operations (usually for submitting the text input). We formulated it as `<Widget name> + <Input content>`, e.g., `Widget: “Price”, Input: "3500"` , followed by `<Operation> + <Widget name>`, e.g., `Operation: “Click”. Widget: “Submit”` . Table 4 provides an example answer from LLM for these two categories. 
>  我们为上述五个标准操作设计操作原语
>  五个标准动作中前四个属于动作操作，这类操作的原语模板为 `<Operation> [click/doubl-click/long press/scroll] + <Widget name>`
>  输入操作的原语模板为 `<Widget name> + <Input content>`  + `<Operation> + <Widget name>`

![[pics/GPTDroid-Table4.png]]

## 2.3 Functionality-aware Memory Prompting 
With the context extraction, GUI prompting and executive command generation in the previous two sections, `GPTDroid` can already conduct the automated GUI testing. 
>  基于上述的上下文提取、GUI prompting, LLM 已经可以执行自动化测试

This Section proposes the function-aware memory prompting, which further improves the capability of the approach in retaining the knowledge during the iterative testing process and understanding the functional aspects of the mobile app, so as to generate the function-aware operations to guide the operation from global viewpoints. 

To achieve this, we build a testing sequence memorizer (Section 2.3.2) to record all detailed testing information. We also query the LLM about the function-level progress of the testing (Section 2.3.1) in each iterative testing step, and store it into the memorizer. We then design linguistic patterns to encode the information into the functionality-aware memory prompt (Section 2.3.3), and query the LLM together with the prompt in the previous section. 

### 2.3.1 Functionality-level Progress. 
Due to the importance of functionalities for app users, we hope our automated GUI testing can conduct the exploration from the viewpoints of the functionalities. 

Hence we need to make LLM understand what function is currently being tested derived from the explored activities and widgets. Specifically, we design a prompt to ask LLM what functionality is currently being tested and whether it has been completed in Section 2.3.3. And the LLM would provide us with the functionality-level progress as `<Function name> + <Status>`, in which ‘YES’ denotes it has completed testing the specific functionality. 
>  我们想让 LLM 理解目前探索的活动和组件涉及到哪个功能
>  我们设计 prompt 询问 LLM 当前测试的功能是什么，且测试是否完成，LLM 回答模板为 `<Function name> + <Status>`

To facilitate the LLM to make reasonable functionality-level decisions, we also provide the LLM with the list of functionalities of the app in the prompt. This information is first extracted from the app description file and the activity names to serve as the initial seed, and will continuously let the LLM output the refined information during the iterative testing process. 
>  我们在 prompt 中为 LLM 提供 app 的功能列表
>  该信息从 app 描述文件和活动名称中提取，并且会让 LLM 自己在测试过程中不断完善

### 2.3.2 Testing Sequence Memorizer. 
We design a testing sequence memorizer to keep the record of testing, including the set of tested functions (in Section 2.3.1), the testing path of activity, the set of tested activities with page visits number, the set of tested widgets of the current page with widgets visits number. 
>  测试序列记忆器用于记录测试流程，包括了测试过的功能、活动的测试路径、测试过的活动+页面访问次数、当前页面测试过的组件+组件访问次数

Specifically, during the iteration, when an operation is conducted, we can obtain the status of the function (`<Function name> + <Status>`), the operation of the widget (`<Operation>+<Widget name>`), and the test path of activity ( `<Activity1>+<Operation>+<Activity2>`). Then the operation memorizer is updated accordingly. 

In detail, the visit number of the widget is updated by finding the same widget in the operation memorizer with the “text” field and “resource-id” field of the widget. The visit number of activity is updated by finding the same activity in the memorizer with the “ActivityName” field. 

>  测试序列记忆器根据测试过程中 LLM 的输出相应地更新

### 2.3.3 Functionality-aware Memory Prompt.
Based on the information in the testing sequence memorizer, we further construct a functionality-aware memory prompt to facilitate the LLM to keep an eye on the functionality during recommending the next operation. It consists of three parts, including the explored functionalities, the covered activities, and the recently tested operations, and it would refer to the testing sequence memorizer in each iteration to fetch the latest information. We follow the same procedure in Section 2.2 to derive these prompt patterns. The details and examples of the prompts are shown in Table 3. 
>  功能感知的记忆 prompt 基于测试序列记忆器构建，prompt 包括了: 探索过的功能、覆盖过的活动、最近测试的操作

![[pics/GPTDroid-Table3.png]]

**Pattern related to explored functionalities (Table 3-Id 1).** This describes which functions have been explored, the number of explorations, and whether it is finished in testing the function. 

Since the GUI prompt in the previous Section only demonstrates the widgets and activities in the current GUI page, it could not provide the high-level viewpoints of the functionality which is accomplished by a sequence of operations on the widgets/activities. Therefore, this prompt can remind the LLM about the functional aspects of the app, and facilitate the LLM in deciding the meaningful operation sequence to explore the app’s functionality. 

Specifically, `GPTDroid` extracts the tested functions from the testing sequence memorizer, including the visit times of function pages, and the testing status of the functions. 

**Pattern related to covered activities. (Table 3-Id 2)** This describes the sequence of covered activities during the testing process. Since the basic component of the mobile app is the activity which is recorded in the AndroidManifest.xml file, this prompt aims at providing the activity viewpoints of testing history to enable the LLM better capture the tested functionalities and to cover more unexplored areas. Specifically, we merge the adjacent same activity and activity sequence, and update the number of visits accordingly. 
>  移动应用的基本组件是活动，且其活动都记录在 `AndroidManifest.xml` 文件中

**Pattern related to recently tested operations. (Table 3-Id 3)** This denotes the latest test page with the detailed visiting status of all its contained widgets, as well as the operations leaving the page. This information is the first-hand and fine-grained testing recording, which can facilitate the LLM in capturing the latest status of the testing progress, and make informed decisions about exploring a certain functionality. 

Specifically, `GPTDroid` extracts the widgets tested on each GUI page and their visit times from the testing sequence memorizer. For the current test page, we will select the operation pages of the lastest $k$ steps and the corresponding operations. For each page, we provide LLM with the activity name of the current page and the number of visits to each widget when the page is accessed. We set $k$ as 5 based on the empirical experience. 

**Pattern related to functionality inquiry: (Table 3-Id 4)** We ask the LLM what function is currently tested, and also provide the output template in the prompt to enable the LLM to generate the function name and its status, with details in Section 2.3.1. 

**Prompt Generation Rules.** We combine the above three patterns for providing LLM with functionality aspects of testing information from different viewpoints, and generate the prompt rules as shown in Table 3. We provide examples of how the GUI context prompts and memory prompts work to enable app testing. 

## 2.4 Implementation 
`GPTDroid` is implemented as a fully automated GUI app testing tool, which uses or extends the following tools: VirtualBox [9] and the Python library pyvbox [7] for running and controlling the Androidx86 OS, Android UIAutomator [67] for extracting the view hierarchy file, and Android Debug Bridge (ADB) [1] for interacting with the app under test (Section 2.1). 

For the LLM (Section 2.2), we use the pre-trained ChatGPT model which was released on the OpenAI website2. The basic model of ChatGPT is the gpt-3.5-turbo model which is extremely powerful and good at answering questions. 

# 3 Effectiveness Evaluation 
In order to verify the performance of `GPTDroid`, we evaluate it by investigating the activity and code coverage (RQ1), as well as the number of detected bugs (RQ2). We also present the ablation study of each module in `GPTDroid` (RQ3). 
>  我们评估 `GPTDroid` 的活动和代码覆盖率、检测的 bug 数量，并进行消融试验

Note that, this Section utilizes the previously-detected bugs in the app’s repositories to demonstrate the effectiveness of `GPTDroid`, and the next Section will evaluate the usefulness of `GPTDroid` in detecting new bugs. 

## 3.1 Experimental Setup 
The experimental dataset comes from two sources. The first is from the apps in the Themis benchmark [63], which contains 20 opensource apps with 34 bugs in GitHub. Considering the small number of apps in the benchmark, we collect a second dataset following similar procedures as the benchmark. 

In detail, we crawl the 50 most popular apps of each category from Google Play [4], and we keep the ones with at least one update after May. 2022, resulting in 407 apps in 12 Google Play categories. 

Then, we use 10 commonly used and state-of-the-art automated GUI testing tools (details are in Section 3.2) to run these apps, in turn, to ensure that they work properly. We then filter out the unusable apps by the following criteria: (1) They would constantly crash on the emulator. (2) One or more tools can not run on them. (3) The registration and login functions cannot be skipped with scripts [21, 38, 42, 63, 87]. (4) They do not have issue records or pull requests on GitHub. 

There are 73 apps (with 109 bugs) remaining for this effectiveness evaluation. Note that, same as the benchmark, all bugs are crash bugs. Specifically, for each app, we select the version in which the bugs are confirmed by developers (merged GitHub pull requests) as our experimental data, following the practice of the benchmark. The details of all 93 experimental apps $(20+73)$ and related bugs are shown in Table 5. 
>  所有的 bugs 都是 crash bugs

Table 5: Dataset of effectiveness evaluation. 
<html><body><table><tr><td>Statistics</td><td>#Activities</td><td>#Bugs</td><td>#Download</td><td>#Update</td></tr><tr><td>Min</td><td>7</td><td>1</td><td>50K+</td><td>05/22</td></tr><tr><td>Max</td><td>33</td><td>9</td><td>100M+</td><td>05/23</td></tr><tr><td>Median</td><td>17</td><td>4</td><td>5M+</td><td></td></tr><tr><td>Average</td><td>15</td><td>1.5</td><td>10M+</td><td></td></tr><tr><td>All</td><td>1398</td><td>143</td><td></td><td></td></tr></table></body></html> 

Note that, there are 101 apps that are filtered out for effectiveness evaluation, yet can successfully run with our proposed approach. We apply them to the manual prompt generation in Section 2.2.1. And this ensures that there is no overlap between the apps in approach design and evaluation. 

We employ activity coverage, code coverage and the number of detected bugs, which are widely used metrics for evaluating GUI testing [11, 27, 34, 36, 70, 72]. We also present the number of covered activities and widgets which are also commonly-used metrics in Table 6. 

We treat the activities defined in the AndroidManifest.xml file of an Android app as the whole set of activities [2, 50, 62]. During the testing process, we collect the unique activity name and widget ID of the GUI page with which the operation interacts, and treat them as the activity number and widget number. 

## 3.2 Baselines 
To demonstrate the advantage of `GPTDroid`, we compare it with 10 common-used and state-of-the-art automated testing techniques. 

We roughly divide them into random-/rule-based, model-based, and learning-based methods, to facilitate understanding. For random/rule-based methods, we use Monkey [20] and Droidbot [33], TimeMachine [21]. For model-based methods, we use WCTester[83, 88], Stoat [62], Ape [25], Fastbot [13], ComboDroid [70]. For learningbased methods, we use Humanoid [34] and Q-testing [50]. 

For a more thorough comparison, we additionally include 5 other baselines, in which the originally proposed techniques aim at enhancing the automated GUI testing, and can be utilized by integrating with the above-mentioned automated testing tools. Specifically, QTypist [37] is used to generate valid text input to enhance the coverage of automated testing tools. Toller [71] is a tool consisting of infrastructure enhancements to the Android operating system. Vet [71] is used to identify exploration tarpits by recognizing their patterns in the UI traces, so as to optimize the exploration sequences. We use the experimental setup as their original paper to derive the following 5 baselines, i.e, QTypist is integrated with Droidbot and Ape (Droidbot ${+Q T,}$ Ape $+\mathrm{QT}$ ), Toller is integrated with Stoat $\mathrm{(Stoat+TO)}$ ), Vet is integrated with WCTester and Ape (WCTester $+\mathrm{VE}$ , Ape+VE). 

We deploy the baselines and our approach on a 64-bit Ubuntu 18.04 machine (64 cores, AMD CPU) and evaluate them on Google Android 7.1 emulators. Each emulator is configured with 2GB RAM, 1GB SDCard, 1GB internal storage, and X86 ABI image. Different types of external files (including PNGs / MP3s / PDFs / TXTs / DOCXs) are stored on the SDCard to facilitate file access from apps. 

Following common practice [25, 33], we registered separate accounts for each bug that requires login and wrote the login scripts, and during testing reset the account data before each run to avoid possible interference. In order to ensure fair and reasonable use of resources, we set up the running time of each tool in one app to 60 minutes, which is widely used in other GUI testing studies [22, 25, 33, 63]. We run each tool three times and obtain the highest performance to mitigate potential bias. 

## 3.3 Results and Analysis 
### 3.3.1 Performance of Coverage (RQ1). 

Table 6: Performance of activity coverage (RQ1). 
<html><body><table><tr><td rowspan="2">Metric</td><td rowspan="2">MK</td><td colspan="3">Random-/rule-based</td><td rowspan="2"></td><td colspan="7">Model-based</td><td colspan="4">Learning-based</td></tr><tr><td>DB</td><td>DB+OT</td><td>TM</td><td>WC</td><td>WC+VE</td><td>ST</td><td>ST+TO</td><td>AP</td><td>AP+QT</td><td>AP+VE</td><td>FB</td><td>CD HM</td><td>1-O</td><td>`GPTDroid`</td></tr><tr><td>#Widgets</td><td>691</td><td>1707</td><td>2522</td><td>3811</td><td>1745</td><td>2465</td><td>2830</td><td>3087</td><td>2964</td><td>3496</td><td>3406</td><td>2944</td><td>3210</td><td>3022</td><td>2261</td><td>5243</td></tr><tr><td>#Activities</td><td>266</td><td>461</td><td>573</td><td>811</td><td>545</td><td>573</td><td>615</td><td>671</td><td>741</td><td>853</td><td>811</td><td>755</td><td>783</td><td>657</td><td>685</td><td>1049</td></tr><tr><td>Avg.activity coverage</td><td>0.25</td><td>0.33</td><td>0.41</td><td>0.56</td><td>0.39</td><td>0.41</td><td>0.44</td><td>0.48</td><td>0.53</td><td>0.57</td><td>0.56</td><td>0.54</td><td>0.55</td><td>0.47</td><td>0.49</td><td>0.75</td></tr><tr><td>Avg.code coverage</td><td>0.17</td><td>0.28</td><td>0.36</td><td>0.53</td><td>0.31</td><td>0.33</td><td>0.40</td><td>0.44</td><td>0.45</td><td>0.55</td><td>0.52</td><td>0.47</td><td>0.48</td><td>0.43</td><td>0.41</td><td>0.66</td></tr></table></body></html>

Table 6 shows **the number of covered widgets, number of covered activities, and average activity coverage** of `GPTDroid` and the baselines. 

We can see that `GPTDroid` covers far more widgets and activities than the baselines, and the average activity coverage achieves $75\%$ and average code coverage achieves $66\%$ across the 93 apps. It is $32\%$ (0.75 vs. 0.57) activity coverage higher even compared with the best baseline (Ape with QTypist). Meanwhile, on the Themis benchmark and Google Play datasets, it was $28\%$ (0.69 vs. 0.54) and $33\%$ (0.77 vs. 0.58) higher than the best baseline. 

This indicates the effectiveness of `GPTDroid` in covering more activities and codes, thus bringing higher confidence to the app quality and potentially uncovering more bugs. Section 5 will further analyze why `GPTDroid` performs well. 

>  `GPTDroid` 的覆盖率显著更高

![](https://cdn-mineru.openxlab.org.cn/extract/release/47fc30a6-8a2f-461c-b63f-f3b79037ea35/bd0ee018b8e6b444125820769a8fe262a52f036aae2294536314343214f656e0.jpg) 

Figure 3: Activity coverage with varying time (RQ1). 

Figure 3 additionally demonstrates the average activity coverage with varying times. We can see that, at every time point, `GPTDroid` achieves higher activity coverage than the baselines, and it achieves high coverage within about 24 minutes. This again indicates the effectiveness and efficiency of `GPTDroid` in covering more activities with less time, which is valuable considering the testing budget. 

Among the baselines, the model-based and learning-based approaches have relatively higher performance. Yet the model-based approaches can’t capture the GUI semantic information and the exploration could not well understand the inherent business logic of the app. Learning-based approaches only use little context information for guiding the exploration, and don’t have the mechanism for enabling the model considering the app’s functionalities. 

We further analyze the potential reasons for the uncovered cases. First, some widgets or inputs do not have meaningful “text” or “resource-id”, which hinders the approach of effectively understanding the GUI page. Second, some app requires specific operations, e.g., database connection, long press and drag widgets to a fixed location, which is difficult if not impossible to be automatically achieved. 

### 3.3.2 Performance of Bug Detection (RQ2). 

![](https://cdn-mineru.openxlab.org.cn/extract/release/47fc30a6-8a2f-461c-b63f-f3b79037ea35/d223b6ea8f7ea8689b7aeaf9a3a349e40fc6087ab4e945dbcc0fbc1ab797ed43.jpg) 
Figure 4: Bug detection with varying time (RQ2). 

Figure 4 shows the overall number of detected bugs of `GPTDroid` and baselines with varying times. `GPTDroid` detects 95 bugs for the 93 apps, $31\%$ (95 vs. 66) higher than the best baseline (Stoat with Toller). We also compare the similarities and differences of the bugs between Stoat with Toller and our approach, and the results show that all bugs detected by Stoat with Toller are also detected by `GPTDroid`. This indicates the effectiveness of `GPTDroid` in detecting bugs and helps to ensure app quality. 

>  `GPTDroid` 检测到的 (crash) bug 数量显著更多

We can also see that, in every time point, `GPTDroid` detects more bugs than the baselines, and reaches the highest value in about 27 minutes, saving $35\%$ (17 vs. 26) of the testing time compared with the best baseline (also with more detected bugs). This again proves the effectiveness and efficiency of `GPTDroid`, which is valuable for saving more time for follow-up bug fixing. We will conduct a further discussion about the reason behind the superior performance in Section 3.3.3. 


### 3.3.3 Ablation Study (RQ3). 

Table 7: Contribution of different modules (RQ3) 
<html><body><table><tr><td>Module</td><td>Activitycoverage</td><td>Codecoverage</td></tr><tr><td>`GPTDroid`</td><td>0.75</td><td>0.66</td></tr><tr><td>w/oGUIContext</td><td>0.17</td><td>0.14</td></tr><tr><td>w/oFunctionMemory</td><td>0.34</td><td>0.28</td></tr></table></body></html> 

**Contribution of Modules.** Table 7 shows the performance of `GPTDroid` and its 2 variants. In detail, for `GPTDroid` $w/o$ GUI Context (Sec 2.1), we replace the GUI context information with the raw view hierarchy file and extracted the widgets name. For `GPTDroid` w/o Function Memory (Sec 2.3), we remove the functionality-aware memory prompting. 

>  GUI Context 和 Function Memory 都非常重要

We can see that `GPTDroid` ’s activity and code coverage are much higher than all other variants, indicating the necessity of the designed modules and the advantage of our approach. Compared with `GPTDroid`, `GPTDroid` w/o GUI Context results in the largest performance decline, i.e., $77\%$ drop (0.17 vs. 0.75) in activity coverage. This further indicates that the GUI context extraction can help LLM understand the structure and semantic information of GUI pages and make reasonable judgments. 

`GPTDroid` w/o Function Memory also undergoes a big performance decrease, i.e., $55\%$ (0.34 vs. 0.75) in activity coverage. This implies our proposed functionality-aware memory prompt can help retain the knowledge during the testing process and gain global viewpoints to reach the uncovered areas. 


![](https://cdn-mineru.openxlab.org.cn/extract/release/47fc30a6-8a2f-461c-b63f-f3b79037ea35/f63b5c51fffb7b14bf2a5f6dfee7c0ffbc871d52605c57933a377f043c392a6e.jpg) 
Figure 5: Contribution of different sub-modules (RQ3). 

**Contribution of Sub-modules.** Figure 5 further demonstrates the performance of `GPTDroid` and its 8 variants. We remove the part of prompt when querying LLM, i.e., the first four variants respectively remove pattern 1, 2, 3, 5, 6 of Table 2, and the last three variants respectively remove pattern 1, 2, 3 of Table 3. 

The experimental results demonstrate that removing any of the sub-modules would result in a noticeable performance decline, indicating the necessity and effectiveness of the designed submodules. 

Removing the explored functionalities (`GPTDroid` w/o Explored Function) has the greatest impact on the performance, reducing the activity coverage by $51\%$ (0.37 vs. 0.75). This indicates by explicitly querying the LLM about the functionality aspects of the testing progress, the approach can be more aware of what functionality is under test and effectively plan the exploration path to cover more functionalities. 

>  每个 prompt 模块都是有用的

![](https://cdn-mineru.openxlab.org.cn/extract/release/47fc30a6-8a2f-461c-b63f-f3b79037ea35/b73b7f8647d878d493d1c71ace79dc7e87e181b537844d418b4fe9cc3ab5e3c1.jpg) 

Figure 6: Different number of tested operations (RQ3). 

**Influence of Different Number of Recent Tested Operations.** Figure 6 demonstrates the performance under the different number of latest tested operations. We can see that the activity and code coverage increase with the more tested pages in the latest tested operations, i.e., 5 steps of tested pages and operations. And after that, the performance would gradually decrease even increasing the number of tested pages. It also further verified that the 5 steps selected by our pilot study are effective and valuable. 

# 4 Usefulness Evaluation 
## 4.1 Experimental Setup 
This Section further evaluates the usefulness of `GPTDroid` in detecting new crash bugs. We employ a similar experimental setup to the previous section. To make it brief, we only compare the best baselines, i.e., TimeMachine, Stoat with Toller and Humanoid, the best one from each type of method following their bug detection performance as shown in Figure 4. Note that, for coverage, the top three are different sets of baselines, yet this Section is concerned about their capability in detecting bugs, so we use their bug detection performance as the criteria. 

We begin with the most popular and recently updated 317 apps from 12 categories as in the previous Section. Then we reuse the four criteria in Section 3.1 for filtering out the unusable apps. Differently, we loosen criteria 4, which only requires the app to have ways for bug reporting, since the issue records or pull requests are not mandatory in this Section. This results in 223 apps for usefulness evaluation. Note that, this Section aims at evaluating whether `GPTDroid` can detect new bugs in the apps, thus the overlap between the apps of this Section and the previous Section is allowed. 

We use the same hardware and software configurations as the previous evaluation Section. When the crash bugs are detected, we report them to the development team through online issue reports or emails. 

## 4.2 Results and Analysis 

Table 8: Confirmed or fixed bugs 
<html><body><table><tr><td>PI</td><td>APP Name</td><td>Category</td><td>Download</td><td>Status</td><td>TM|</td><td>ST+TO|HM</td><td></td></tr><tr><td>1 2 3 4 5 6 7 8 9 10 11</td><td>PerfectPia MusicPlayer NoxSecu INSTA Degoo Proxy Secure Revolut Thunder ApowerMir</td><td>Music Music Tool Finance Tool Tool Tool Finance Tool</td><td>50M+ 50M+ 10M+ 10M+ 10M+ 10M+ 10M+ 10M+</td><td>confirmed confirmed fixed fixed fixed confirmed fixed fixed confirmed</td><td></td><td></td><td></td></tr></table></body></html> 

For the 223 apps, `GPTDroid` detects 135 bugs involving 115 apps, of which 53 bugs involving 41 apps are newly-detected bugs. Furthermore, only 9 of these new bugs were detected by three baselines. We submit these 53 bugs to developers, and 35 of them have been fixed/- confirmed so far (20 fixed and 15 confirmed), while the remaining are still pending (none of them is rejected). This further indicates the effectiveness of our proposed `GPTDroid` in bug detection. Due to space limit, Table 8 presents these fixed/confirmed bugs, and the full lists can be found on our website1. 

We further analyze the details of these bugs, and 17 of them involve **multiple text inputs or compound operations.** Besides, we also observe that there are 11 bugs with **more than 20 operations** before triggering the bug, counting from the MainActivity page, which indicates the ability of `GPTDroid` in testing deeper features. 

Furthermore, we find at least 28 bugs related to the main business logic of the app. This further demonstrates the capabilities of `GPTDroid`, and we provide analysis in Section 5. 

# 5 Insights from Experiment Results 
This Section summarizes 4 kinds of capabilities of `GPTDroid` including high-level (i.e., long meaningful test trace, test case prioritization) and low-level ones (i.e., valid text input, compound actions), to unveil the mystery of why `GPTDroid` outperforms existing method. 

![](https://cdn-mineru.openxlab.org.cn/extract/release/47fc30a6-8a2f-461c-b63f-f3b79037ea35/e3448b93b9b8e10a89c81a50c605174eea27c1c96ccc6c8df8f6073f31056126.jpg) 

Figure 7: Examples of our insights from experiments. 

**Functionality-aware exploration through the long meaningful testing trace.**
`GPTDroid` can automatically generate the test cases with a long sequence of operations which together accomplish a business logic of the app, and this is quite important for covering the app features and ensuring its quality. As shown in Figure 7 (a), in SmartMeter app [8], to test a commonly-used app feature “delete equipment”, the automated tool first needs to click “Find Devices” on the device page, then select a device (Bluetooth is turned on and there are candidate devices) and click “Add Devices” for adding it in the device page, input the related information and click “Start” to start the device, then turn off this device in the device page, long press it and click “Delete” from the pop-up menu. Only with this long sequence of operations that touches the “deleting equipment” feature, a crash can be revealed. Our functionality-aware memory prompt can enable the LLM to capture the long-term dependencies among GUI pages to conduct the functionality-guided exploration. 

**Function-aware prioritization.** We also observe that `GPTDroid` usually prioritizes testing the “important” functions, which is valuable for reaching a higher activity coverage and covering more key activities with relatively less time. As shown in Figure 7 (b), on the Main page of the Moni app [5], the baseline tools tend to first click the “Setting” button following the exploration order from upper to lower, which leads the testing easily trapped into the setting page cycle. `GPTDroid` chooses to first click the “AddIncome” button to explore the add income functionality which is the key feature of the app. This is facilitated by the semantic understanding of the GUI page and the functionality-aware memory designed in `GPTDroid`. 

**Valid text inputs.** `GPTDroid` can automatically fill in valid text content to the input widget which is essentially the key for passing the page as seen in Figure 7 (c). Similar to prior work [37], our model can generate semantic text input (e.g., income, date, etc) accordingly. Besides single text input, it can also successfully fill in multiple input widgets at the same time which are correlated to each other like the departure and arrival cities and dates in the flight booking app. We have designed a prompt, especially for querying the text input and utilize the few-shot learning by providing demonstrations with the output template to facilitate the LLM generating desired executive commands, which can be accurately mapped to the GUI widgets of text inputs to enable it to execute automatically. 

**Compound actions.** `GPTDroid` can conduct complex compound operations guided by the LLM. As shown in (Figure 7 (c), to add the “Cable crunch” information, it first inputs the text, selects the date, sets the “SETS” and “REPS” by clicking the upper or lower button, then click the submit button in the lower right corner. Thanks to our designed executive command generation method with few-shot learning and output template, `GPTDroid` can accurately map the LLM’s output into the actions related to GUI widgets. 

# 6 Related  Work
**Automated GUI testing.** To ensure the quality of mobile apps, many researchers study the automatic generation of large-scale test scripts to test apps [74]. Monkey [20] is the popular randombased automated GUI testing tool, which emits pseudo-random streams of UI events and some system events. However, the randombased testing strategy cannot formulate a reasonable testing path according to the characteristics of the app, resulting in low test coverage. To improve the test coverage, researchers propose modelbased [21, 25, 44, 47, 62, 70, 77, 78, 83] automated GUI testing methods, design corresponding models through the research and analysis of large-scale apps. Although model-based automated GUI testing tools can improve test coverage, the coverage is still low because it does not consider the semantic information of the app’s GUI and Page. Researchers further proposed human-like testing strategies and designed learning-based [34, 50] automated GUI testing methods. Although the learning-based approach can improve the test coverage by learning the interactive processes or using the idea of reinforcement learning. However, it is still unable to better understand the semantic information of the page and plan the path according to the actual situation of the app. We aim at proposing a more effective approach to generate human-like actions for thoroughly and more effectively testing the app, accomplishing it with LLM. There are also studies that tried to find bugs in similar apps to move beyond crashes [39–41, 65], yet it cannot reveal the crashes automatically as this work. Techniques related to test migration [12, 64] can generate meaningful operation sequences borrowed from the source app, yet it is quite demanding to require the test cases of an app, and by comparison, `GPTDroid` can generate more meaningful test traces from scratch. 

**LLM for Software Engineering.** Considering the powerful performance of LLM, researchers have successfully leveraged it to solve various tasks in the field of software engineering [10, 29, 45, 46, 48, 49, 51, 57, 59, 66, 75, 80–82, 85]. Supported by code naturalness [28], researchers applied the LLMs to code writing in different programming languages [15, 23, 24, 76]. In testing, LLMFuzz [19] used LLMs to generate input programs for fuzzing Deep Learning libraries. Xia et al. [73] applied LLM for automatic program repair to improve the accuracy of the generated repair patches. Lemieux et al. [32] leveraged LLM in escaping the coverage plateaus in test generation. Kang et al. [30] explored the LLM-based bug reproduction. A similar work QTypist [37] leveraged the LLM to generate the text inputs for passing a GUI page in order to improve the testing coverage of mobile testing. Different from its sole focus on text input generation to boost existing GUI testing tools, `GPTDroid` is a complete GUI testing tool in asking LLM to propose different actions to interact with the target app including clicking buttons, filling in text and even more complicated compound actions. It makes this work more generalized for wild mobile app testing. 

# 7 Conclusion 
Automated GUI testing has made much progress, yet still suffers from low activity coverage and may miss critical bugs. 

This paper aims at generating human-like actions to facilitate app testing more thoroughly and effectively. Inspired by ChatGPT, we formulate the GUI testing problem as a Q&A task and propose `GPTDroid`. It extracts the GUI context and functionality-aware memory, encodes them into prompt questions to ask the LLM, decodes the LLM’s feedback answer into actionable operations to execute the app, and iterates the whole process. 

Results on 93 popular apps demonstrate that `GPTDroid` can achieve $75\%$ activity coverage, with $32\%$ higher than the best baseline, and can detect $31\%$ more bugs with faster speed than the best baseline. `GPTDroid` also detects 53 new bugs on Google Play with 35 of them being confirmed/fixed. In the future, we plan to explore more advanced prompt engineering design to better exploit the power of LLM, and may also fine-tune open-source LLM for this specific tasks for better performance. 

