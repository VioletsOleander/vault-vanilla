# Abstract
We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. 
>  The Google File System 是针对大型分布式数据密集应用的可拓展分布式文件系统，该文件系统运行在便宜的商业机硬件上，可以对大量的客户端提供高的总体性能

While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. 
>  The Google File System 的设计由对于 Google 应用的工作负载的观察以及技术环境所驱动，二者都反映了和早期文件系统假设的重大偏移
>  因此，我们需要重新审视传统选择，并探索不同的设计点

The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. 
>  The Google File System 满足了 Google 服务的存储需求
>  目前最大的集群通过数千个机器上的数千个磁盘提供了几百 T 的存储空间，该系统可以被上百个客户端并发访问

In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. 

# 1 Introduction
We have designed and implemented the Google File System (GFS) to meet the rapidly growing demands of Google’s data processing needs. GFS shares many of the same goals as previous distributed file systems such as performance, scalability, reliability, and availability. However, its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system design assumptions. We have reexamined traditional choices and explored radically different points in the design space. 
>  GFS 和先前的分布式文件系统一样，其目标包括了：性能、可拓展性、可靠性、可用性，但其设计由对于 Google 应用的工作负载的观察以及技术环境所驱动，二者都反映了和早期文件系统假设的重大偏移
>  因此，我们需要重新审视传统选择，并探索不同的设计点

First, component failures are the norm rather than the exception. The file system consists of hundreds or even thousands of storage machines built from inexpensive commodity parts and is accessed by a comparable number of client machines. The quantity and quality of the components virtually guarantee that some are not functional at any given time and some will not recover from their current failures. We have seen problems caused by application bugs, operating system bugs, human errors, and the failures of disks, memory, connectors, networking, and power supplies. Therefore, constant monitoring, error detection, fault tolerance, and automatic recovery must be integral to the system. 
>  首先，组件故障是非常常见的，而不是属于异常
>  文件系统包括了上百到上千个普通商用机器，并且由数量可比的客户端访问，这使得几乎在任意给定时刻都会存在出现故障的组件，并且其中一些组件无法从其当前故障中恢复
>  我们已经经历了由应用程序 bug, OS bug, 人为错误, 磁盘故障, 内存, 连接器, 网络, 供电导致的故障，因此，系统必须具有持续监控、错误检测、容错、自动恢复能力

Second, files are huge by traditional standards. Multi-GB files are common. Each file typically contains many application objects such as web documents. When we are regularly working with fast growing data sets of many TBs comprising billions of objects, it is unwieldy to manage billions of approximately KB-sized files even when the file system could support it. As a result, design assumptions and parameters such as I/O operation and block sizes have to be revisited. 
>  其次，文件本身非常大，多个 GB 的文件很常见
>  每个文件一般包含多个应用程序对象，例如网页文档，随着对于包含数十亿个对象的几 TB 的数据集的处理变得频繁，管理数十亿个大约 KB 大小的文件是不明智地
>  因此，需要重新审视例如 IO 操作和块大小等设计假设和参数

Third, most files are mutated by appending new data rather than overwriting existing data. Random writes within a file are practically non-existent. Once written, the files are only read, and often only sequentially. A variety of data share these characteristics. Some may constitute large repositories that data analysis programs scan through. Some may be data streams continuously generated by running applications. Some may be archival data. Some may be intermediate results produced on one machine and processed on another, whether simultaneously or later in time. Given this access pattern on huge files, appending becomes the focus of performance optimization and atomicity guarantees, while caching data blocks in the client loses its appeal. 
>  第三，大多数文件的修改是附加新数据而不是覆盖写现存的数据
>  文件内的随机写入实际上不存在的，当文件被写入后，文件就是只读状态，并且是顺序读取
>  许多的数据类型都具有这一特性，例如数据分析系统频繁扫描的大规模存储库、运行中的程序持续生成的数据流、归档数据、在一个机器上生成并在另一个机器上处理的数据
>  考虑到这一访问模式，性能优化和原子化保证的关键就是附加操作，而不是将数据块缓存在客户端中

Fourth, co-designing the applications and the file system API benefits the overall system by increasing our flexibility. For example, we have relaxed GFS’s consistency model to vastly simplify the file system without imposing an onerous burden on the applications. We have also introduced an atomic append operation so that multiple clients can append concurrently to a file without extra synchronization between them. These will be discussed in more details later in the paper. 
>  第四，协同设计文件系统 API 和应用程序可以提高灵活性，使整个系统获益
>  例如，我们松弛了 GFS 的一致性模型，在没有对应用程序施加繁重负担的情况下简化了文件系统；我们也引入了原子附加操作，使得多个客户端可以并发地对一个文件执行附加操作，而不需要客户端之间的额外同步

Multiple GFS clusters are currently deployed for different purposes. The largest ones have over 1000 storage nodes, over 300 TB of disk storage, and are heavily accessed by hundreds of clients on distinct machines on a continuous basis. 
>  目前部署的最大 GFS 集群包含了上千个存储节点，提供了 300 TB 的磁盘存储，被数百个远端机器持续访问

# 2. Design Overview
## 2.1 Assumptions 
In designing a file system for our needs, we have been guided by assumptions that offer both challenges and opportunities. We alluded to some key observations earlier and now lay out our assumptions in more details. 
- The system is built from many inexpensive commodity components that often fail. It must constantly monitor itself and detect, tolerate, and recover promptly from component failures on a routine basis. 
- The system stores a modest number of large files. We expect a few million files, each typically 100 MB or larger in size. Multi-GB files are the common case and should be managed efficiently. Small files must be supported, but we need not optimize for them. 
- The workloads primarily consist of two kinds of reads: large streaming reads and small random reads. In large streaming reads, individual operations typically read hundreds of KBs, more commonly 1 MB or more. Successive operations from the same client often read through a contiguous region of a file. A small random read typically reads a few KBs at some arbitrary offset. Performance-conscious applications often batch and sort their small reads to advance steadily through the file rather than go back and forth. 
- The workloads also have many large, sequential writes that append data to files. Typical operation sizes are similar to those for reads. Once written, files are seldom modified again. Small writes at arbitrary positions in a file are supported but do not have to be efficient. 
- The system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file. Our files are often used as producer-consumer queues or for many-way merging. Hundreds of producers, running one per machine, will concurrently append to a file. Atomicity with minimal synchronization overhead is essential. The file may be read later, or a consumer may be reading through the file simultaneously. 
- High sustained bandwidth is more important than low latency. Most of our target applications place a premium on processing data in bulk at a high rate, while few have stringent response time requirements for an individual read or write. 

>  设计文件系统时，我们的假设包括了：
>  - 系统基于多个廉价的商品组件之上而构建，这些组件经常故障，系统必须持续监控自己，并且执行定期的故障检测、容错和恢复
>  - 系统存储一定数量的大文件, 预计会有数百万个文件，每个文件大小通常为 100 MB 或更大。数 GB 的文件也很常见，需要被高效管理。需要支持小文件，但不必针对其优化
>  - 工作负载主要包括两类读操作：大范围流式读取和小范围随机读取。大范围流式读取中，单次读操作一般读取几百 KB，1 MB 或更多，来自于同一客户端的后续的读操作一般会读完一个文件的一个连续区域。小范围随机读取一般在某个任意位置读几 KB，注重性能的应用程序通常会将小范围随机读取批处理并排序，以稳步推进文件的读取，而不是来回反复读取
>  - 工作负载包括了许多大范围、顺序的写入操作，将数据附加到文件中。典型的操作大小和读取操作类似。写入之后，文件就很少再被修改，在文件中任意位置的小范围写入是支持的，但不高效
>  - 系统必须高效实现良定义的语义，支持多个客户端并发地向同个文件附加。我们的文件通常会用于生产者-消费者队列或用于多路合并，例如，数百个机器上运行的数百个生产者进程会并发地对一个文件附加内容，因此必须要有最小化同步开销的原子化操作，消费者进程可能在之后读该文件，也可以正在同时读取该文件
>  - 持续的高带宽比低延迟更重要。我们的大多数应用的主要目标时以高速率处理大批量数据，很少有应用对于单次的读写有严格的响应时间要求

## 2.2 Interface 
GFS provides a familiar file system interface, though it does not implement a standard API such as POSIX. Files are organized hierarchically in directories and identified by pathnames. We support the usual operations to create, delete, open, close, read, and write files. 
>  GFS 提供的文件系统接口和常规的类似，文件按照目录层次化地组织，并且由路径名标识
>  GFS 支持常规的文件操作，包括创建、删除、打开、关闭、读写

Moreover, GFS has snapshot and record append operations. Snapshot creates a copy of a file or a directory tree at low cost. Record append allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual client’s append. It is useful for implementing multi-way merge results and producer-consumer queues that many clients can simultaneously append to without additional locking. We have found these types of files to be invaluable in building large distributed applications. Snapshot and record append are discussed further in Sections 3.4 and 3.3 respectively. 
>  GFS 还有快照和记录附加操作
>  快照操作以低成本为一个文件或一个目录树创建拷贝，记录附加操作允许多个客户端并发地为一个文件附加数据，同时保持每个客户端的附加操作的原子性，这一操作对于实现多路合并和生产者-消费者队列 (多个客户端需要同时附加数据且不需要额外的锁机制) 非常有用

## 2.3 Architecture 
A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients, as shown in Figure 1. Each of these is typically a commodity Linux machine running a user-level server process. It is easy to run both a chunkserver and a client on the same machine, as long as machine resources permit and the lower reliability caused by running possibly flaky application code is acceptable. 
>  一个 GFS 集群由一个 master 和多个 chunkserver 组成，该集群会被多个客户端访问，每个客户端一般都是一个运行一个用户级别服务进程的商用 Linux 机器
>  可以在同一个机器上同时运行 chunkserver 和 client，只要机器资源允许，并且可以接受较低的可靠性 (因为用户代码不一定是可靠的，它可能导致机器故障，进而导致 chunkserver 故障)

![[pics/GFS-Fig1.png]]

Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64 bit chunk handle assigned by the master at the time of chunk creation. Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. For reliability, each chunk is replicated on multiple chunkservers. By default, we store three replicas, though users can designate different replication levels for different regions of the file namespace. 
>  文件会被分为固定大小的块，每个块通过一个不可变的全局 64 位块句柄标识，该句柄由 master 在块创建时赋予
>  chunkserver 将文件块以 Linux 文件的形式存储在本地磁盘上，读写数据块时，需要指定 chunk handle 和读取字节范围
>  为了保障可靠性，每个 chunk 都会被拷贝到多个 chunkserver 上，默认情况下存储三个拷贝，用户可以文件命名空间的不同区域指定不同的拷贝级别

The master maintains all file system metadata. This includes the namespace, access control information, the mapping from files to chunks, and the current locations of chunks. It also controls system-wide activities such as chunk lease management, garbage collection of orphaned chunks, and chunk migration between chunkservers. The master periodically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state. 
>  master 维护所有的文件系统元数据
>  文件系统元数据包括了命名空间、访问控制信息、从文件到块的映射、块的当前位置
>  master 还控制系统范围的活动，包括块租约管理，孤立块的垃圾回收、块在 chunkserver 之间的的迁移
>  master 通过心跳消息定期和每个 chunkserver 通信，向其发送指令并收集其状态

GFS client code linked into each application implements the file system API and communicates with the master and chunkservers to read or write data on behalf of the application. Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers. 
>  GFS 客户端代码链接到每个应用程序中，客户端代码实现了文件系统 API (用户通过 API 和 GFS 交互，进行文件创建等工作) ，客户端代码与 master 和 chunkservers 通信，代表应用程序 (向文件系统) 读写数据
>  客户端与 master 通信以实现元数据的操作，但所有承载数据的通信都直接与 chunkserver 进行

We do not provide the POSIX API and therefore need not hook into the Linux vnode layer. 
>  我们不提供 POSIX API，因此不需要与 Linux vnode 层进行集成，也就是说，GFS 的客户端直接通过 GFS API 和 GFS 文件系统本身进行交互

> [!info] POSIX API
> POSIX API 是指 Portable Operating System Interface 标准所定义的一系列应用程序接口，它是一套操作系统交互的标准，使得应用程序可以在不同的 POSIX 兼容的操作系统上移植
> POSIX API 包括了系统调用和库函数，涵盖了文件操作、进程管理、线程管理、网络通信、信号处理 (指和程序异常相关的 signal ) 等各个方面
> Linux 是 POSIX 标准的一个实现，因此 Linux 的许多系统调用和库函数都遵循 POSIX 规范

> [!info] vnode
> Linux 系统中，vnode 是虚拟文件系统 (VFS) 中的一部分，用于抽象不同文件系统之间的操作，它隐藏了不同文件系统的实现细节，为上层提供统一接口
> POSIX API 通常通过 vnode 层和底层文件系统交互，即应用程序通过 POSIX API 进行文件操作时，POSIX API 会调用 vnode 的接口，vnode 和底层文件系统交互
> 如果一个系统不提供 POSIX API (例如 GFS)，则它也不需要和 vnode 层进行集成，因为 vnode 层主要是为了支持 POSIX API 而存在的

Neither the client nor the chunkserver caches file data. Client caches offer little benefit because most applications stream through huge files or have working sets too large to be cached. Not having them simplifies the client and the overall system by eliminating cache coherence issues. (Clients do cache metadata, however.) Chunkservers need not cache file data because chunks are stored as local files and so Linux’s buffer cache already keeps frequently accessed data in memory. 
>  客户端和 chunkserver 都不会缓存文件数据
>  客户端缓存的优势很小，因为大多数应用要么会流式处理大文件，要么其工作集太大而无法缓存，不使用缓存简化了客户端和整个系统，不必要考虑缓存一致性的问题 (但客户端会缓存元数据)
>  chunkserver 不缓存的原因是 chunks 以本地文件的形式存储，故 Linux 的缓冲区缓存已经将频繁访问的数据保留在内存中

## 2.4 Single Master 
Having a single master vastly simplifies our design and enables the master to make sophisticated chunk placement and replication decisions using global knowledge. However, we must minimize its involvement in reads and writes so that it does not become a bottleneck. Clients never read and write file data through the master. Instead, a client asks the master which chunkservers it should contact. It caches this information for a limited time and interacts with the chunkservers directly for many subsequent operations. 
>  单个 master 简化了设计，并且使得 master 可以使用全局数据执行复杂的 chunk 放置和复制决策
>  但是，我们必须最小化 master 在读写中的参与，防止它成为瓶颈。客户端永远不会通过 master 读写数据，客户端会询问 master 它应该访问哪些 chunkserver，然后直接与 chunkserver 交互，执行后续的操作

Let us explain the interactions for a simple read with reference to Figure 1. First, using the fixed chunk size, the client translates the file name and byte offset specified by the application into a chunk index within the file. Then, it sends the master a request containing the file name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key. The client then sends a request to one of the replicas, most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires or the file is reopened. In fact, the client typically asks for multiple chunks in the same request and the master can also include the information for chunks immediately following those requested. This extra information sidesteps several future client-master interactions at practically no extra cost. 
>  考虑一个读操作的交互过程
>  首先，客户端根据固定的块大小，将应用程序所指定的文件名和字节偏移量转化为文件中的特定块的索引
>  然后，客户端向 master 发送一个请求，该请求包含了文件名和块索引，master 的回复中会包含对应的块句柄和其副本的位置，客户端使用文件名和块索引作为 key，缓存这一信息 (客户端会缓存元信息) 
>  然后客户端向其中一个副本发送请求，一般是最近的一个，该请求指定了块句柄和该块内的字节范围 (注意到 master 仅考虑块级别的信息，块内的字节范围由客户端自己处理)
>  之后，对于同一个块的进一步读取就不再需要客户端和 master 交互，客户端直接向 chunkservers 发送请求，直到其缓存的信息过期或者要读取的文件被重新打开过
>  实践中，客户端会在同一个请求中请求多个块，而 master 也可以在回复中额外附加上紧接着所请求的块之后的部分块的信息，这样的额外信息几乎不增加额外成本，但可以省下未来的多次 client-master 交互

## 2.5 Chunk Size 
Chunk size is one of the key design parameters. We have chosen 64 MB, which is much larger than typical file system block sizes. Each chunk replica is stored as a plain Linux file on a chunkserver and is extended only as needed. Lazy space allocation avoids wasting space due to internal fragmentation, perhaps the greatest objection against such a large chunk size. 
>  Chunk size 设定为 64 MB，这比典型的文件系统块大小 (几 KB - 几 MB) 大许多
>  每个块拷贝都在 chunkserver 存储为一个普通的 Linux 文件，仅在需要时被拓展 (意思是在创建时文件不会被分配完整 64 MB 空间，而是随着数据的实际写入而拓展)
>  这样的延迟空间分配机制可以避免由于内部碎片导致的空间浪费

A large chunk size offers several important advantages. First, it reduces clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information. The reduction is especially significant for our workloads because applications mostly read and write large files sequentially. Even for small random reads, the client can comfortably cache all the chunk location information for a multi-TB working set. Second, since on a large chunk, a client is more likely to perform many operations on a given chunk, it can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time. Third, it reduces the size of the metadata stored on the master. This allows us to keep the metadata in memory, which in turn brings other advantages that we will discuss in Section 2.6.1. 
>  大的块大小提供了几个重要优势：
>  第一，它减少了 client 和 master 交互的需要，因为在同一个块内的读写仅需要 client 向 master 发起一次初始请求，获取块位置信息即可
>  对于我们的工作负载来说，这一减少很显著，因为应用程序主要都是顺序地读写大文件
>  即便对于小的随机读取，client 也可以利用这一机会轻松缓存多个 TB 工作集上的所有块信息 (因为 chunk size 大了，总的 chunk 数量就少了)
>  第二，由于在一个大块中，client 更有可能在同一块内执行给定的多个操作，故可以长时间保持和 chunkserver 的 TCP 连接以减少网络开销
>  第三，它也减少了 master 所需要存储的元数据量，这使得元数据可以存储在内存中

On the other hand, a large chunk size, even with lazy space allocation, has its disadvantages. A small file consists of a small number of chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients are accessing the same file. In practice, hot spots have not been a major issue because our applications mostly read large multi-chunk files sequentially. 
>  另一方面，大的 chunk size 也有劣势，一个小文件可能只由少量的块组成，可能只有一个。如果有许多 client 同时访问该文件，存储这些少量的块的 chunkserver 就可能成为热点
>  在实践中，热点并不是一个主要问题，因为我们的应用主要读取大型的多块文件

However, hot spots did develop when GFS was first used by a batch-queue system: an executable was written to GFS as a single-chunk file and then started on hundreds of machines at the same time. The few chunkservers storing this executable were overloaded by hundreds of simultaneous requests. We fixed this problem by storing such executables with a higher replication factor and by making the batch-queue system stagger application start times. A potential long-term solution is to allow clients to read data from other clients in such situations. 
>  但是，当 GFS 首次被批处理队列系统使用时，确实出现了热点问题: 一个可执行文件被写入 GFS 的单个块，然后被数百个机器同时启动。此时，存储该块的几个 chunkservers 会因为上百个同时请求和过载
>  为了解决该问题，我们提高了这类可执行文件的复制因子，并且让批处理队列系统错开应用启动时间
>  一个潜在的长期解决方案是让 client 在该情况下从其他 client 读取数据

## 2.6 Metadata 
The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas. All metadata is kept in the master’s memory. The first two types (namespaces and file-to-chunk mapping) are also kept persistent by logging mutations to an operation log stored on the master’s local disk and replicated on remote machines. Using a log allows us to update the master state simply, reliably, and without risking inconsistencies in the event of a master crash. The master does not store chunk location information persistently. Instead, it asks each chunkserver about its chunks at master startup and whenever a chunkserver joins the cluster. 
>  master 存储了三类主要的元数据：文件和 chunk 命名空间、从文件到 chunks 的映射、每个 chunk 的副本的位置
>  所有元数据都存储在 master 的**内存**中
>  前两类元数据 (命名空间、映射) 还会持久化存储，master 会将修改记录写入操作日志中，操作日志储存在 master 的本地磁盘中，并且在远程机器上也有副本
>  使用操作日志使得我们可以简单可靠地更新 master 状态，并且在 master 崩溃时不必担心数据不一致的问题
>  master 不会持久化存储 chunk 位置，也就是说 chunk 位置信息是动态的，master 会在启动时以及有新的 chunkserver 加入集群时向每个 chunkserver 询问 chunk 位置信息

### 2.6.1 In-Memory Data Structures 
Since metadata is stored in memory, master operations are fast. Furthermore, it is easy and efficient for the master to periodically scan through its entire state in the background. This periodic scanning is used to implement chunk garbage collection, re-replication in the presence of chunkserver failures, and chunk migration to balance load and disk space usage across chunkservers. Sections 4.3 and 4.4 will discuss these activities further. 
>  因为元数据储存在内存中，master 的操作是快速的，并且，master 可以高效地定期在后台扫描整个状态
>  这种周期性扫描用于实现块垃圾回收、在 chunkserver 故障时的重新复制、以及为了平衡各个 chunkserver 的负载和磁盘空间的块迁移

One potential concern for this memory-only approach is that the number of chunks and hence the capacity of the whole system is limited by how much memory the master has. This is not a serious limitation in practice. The master maintains less than 64 bytes of metadata for each 64 MB chunk. Most chunks are full because most files contain many chunks, only the last of which may be partially filled. Similarly, the file namespace data typically requires less then 64 bytes per file because it stores file names compactly using prefix compression. 
>  这种仅基于内存的方法的一个潜在问题就是块的数量 (整个系统的容量) 会受限于 master 的内存大小，实践中这不是一个严重的限制
>  master 为每个 64 MB 的块维护少于 64 字节的元数据，而大多数的块都是满的，因为大多数文件会包含许多块，只有其包含的最后一个块是部分填充的
>  类似地，文件命名空间数据通常没文件需要不到 64 字节的数据，因为 master 使用前缀压缩来紧凑存储文件名

If necessary to support even larger file systems, the cost of adding extra memory to the master is a small price to pay for the simplicity, reliability, performance, and flexibility we gain by storing the metadata in memory. 
>  如果需要支持更大的文件系统，为 master 添加额外内存的相对开销也是很小的

### 2.6.2 Chunk Locations 
The master does not keep a persistent record of which chunkservers have a replica of a given chunk. It simply polls chunkservers for that information at startup. The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunkserver status with regular HeartBeat messages. 
>  master 不会保存关于哪个 chunkserver 具有给定块副本的持久记录 (也就是不会将块的具体位置信息写入磁盘)
>  master 会在启动时向 chunkservers 轮询这些信息，并且在之后，master 会通过定期的心跳消息对 chunkserver 状态进行监控，更新所有块的位置信息

We initially attempted to keep chunk location information persistently at the master, but we decided that it was much simpler to request the data from chunkservers at startup, and periodically thereafter. This eliminated the problem of keeping the master and chunkservers in sync as chunkservers join and leave the cluster, change names, fail, restart, and so on. In a cluster with hundreds of servers, these events happen all too often. 
>  我们最初尝试将块的位置信息持久保存在 master 中，但发现在启动时向 chunkservers 请求数据 (之后周期性地请求) 会更加简单，这避免了在 chunkserver 加入和离开集群、改名、故障、重启时的信息同步开销，注意在具有上百个机器的集群中，这样的事的发生是常态

Another way to understand this design decision is to realize that a chunkserver has the final word over what chunks it does or does not have on its own disks. There is no point in trying to maintain a consistent view of this information on the master because errors on a chunkserver may cause chunks to vanish spontaneously (e.g., a disk may go bad and be disabled) or an operator may rename a chunkserver. 
>  并且，每个 chunkserver 对其自身磁盘上的 chunk 有最终决定权，试图在 master 上保持这种一致性是没有意义的，因为 chunkserver 上的错误可能导致 chunks 自发消失 (例如磁盘损坏或被禁用)，或者操作员可能重新命名 chunkserver

### 2.6.3 Operation Log 
The operation log contains a historical record of critical metadata changes. It is central to GFS. Not only is it the only persistent record of metadata, but it also serves as a logical time line that defines the order of concurrent operations. Files and chunks, as well as their versions (see Section 4.5), are all uniquely and eternally identified by the logical times at which they were created. 
>  操作日志包含了关键元数据修改的历史记录，操作日志不仅是元数据的唯一持久记录，还充当定义并发操作顺序的逻辑时间线
>  文件、块及其版本都是通过创建时的逻辑时间来唯一且永久地标识的

Since the operation log is critical, we must store it reliably and not make changes visible to clients until metadata changes are made persistent. Otherwise, we effectively lose the whole file system or recent client operations even if the chunks themselves survive. Therefore, we replicate it on multiple remote machines and respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. The master batches several log records together before flushing thereby reducing the impact of flushing and replication on overall system throughput. 
>  我们需要可靠地存储操作日志，并且在元数据修改被持久性写入之前不将更改对 client 可见，否则，即便文件块本身存在，我们也可能丢失其信息或最近的客户端操作信息
>  我们在多台远程机器上对其进行复制，并且仅在本地和远端的日志都刷新到磁盘后，才响应客户端操作
>  master 会在刷新之前将多个日志记录一起批处理，以减少刷新和复制对整体系统吞吐量的影响

The master recovers its file system state by replaying the operation log. To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size so that it can recover by loading the latest checkpoint from local disk and replaying only the limited number of log records after that. The checkpoint is in a compact B-tree like form that can be directly mapped into memory and used for namespace lookup without extra parsing. This further speeds up recovery and improves availability. 
>  master 通过重放操作日志来恢复其文件系统状态
>  为了最小化启动时间，我们必须保持日志较小，每当日志增长超过一定大小时，master 会对其状态进行检查点记录，这样它可以通过仅加载最后的检查点，并重放有限数量的记录来进行恢复
>  检查点以紧凑的 B-tree 形式存在，可以直接被映射到内存，并用于命名空间查找而无需额外解析，这进一步加快了恢复速度

Because building a checkpoint can take a while, the master’s internal state is structured in such a way that a new checkpoint can be created without delaying incoming mutations. The master switches to a new log file and creates the new checkpoint in a separate thread. The new checkpoint includes all mutations before the switch. It can be created in a minute or so for a cluster with a few million files. When completed, it is written to disk both locally and remotely. 
>  由于构建检查点需要一定时间，master 的内部状态结构被设计为可以在不延迟传入更新的情况下创建新的检查点 (checkpointing 时仍然可以更新内存中的日志)
>  master 会切换到一个新的日志文件，然后在单独的线程中创建新的检查点，这个新的检查点包含了 master 切换之前的所有变化，对于有数百万个文件的集群，这个检查点可以在一分钟内完成创建，完成后，它会被写入本地和远程的磁盘

Recovery needs only the latest complete checkpoint and subsequent log files. Older checkpoints and log files can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not affect correctness because the recovery code detects and skips incomplete checkpoints. 
>  恢复只需要最新的完整检查点和后续的日志文件，较久的检查点和日志文件可以自由删除
>  checkpointing 过程中的故障不会影响正确性，因为恢复代码会检测并且跳过不完整的检查点

## 2.7 Consistency Model 
GFS has a relaxed consistency model that supports our highly distributed applications well but remains relatively simple and efficient to implement. We now discuss GFS’s guarantees and what they mean to applications. We also highlight how GFS maintains these guarantees but leave the details to other parts of the paper. 
>  GFS 具有一个松弛的一致性模型，非常适合我们高度分布的应用程序，且容易简单高效地实现

### 2.7.1 Guarantees by GFS 
File namespace mutations (e.g., file creation) are atomic. They are handled exclusively by the master: namespace locking guarantees atomicity and correctness (Section 4.1); the master’s operation log defines a global total order of these operations (Section 2.6.3). 
>  文件命名空间的变更 (例如文件创建) 的修改是原子性的，这些变更都由 master 处理
>  master 通过命名空间锁定保证原子性和正确性，master 的操作日志定义了这些操作的全局全序关系

![[pics/GFS-Table1.png]]

The state of a file region after a data mutation depends on the type of mutation, whether it succeeds or fails, and whether there are concurrent mutations. Table 1 summarizes the result. 
>  文件区域在数据变更后的状态取决于变更的类型、变更是否成功、是否有并发变更，结果总结于 Table 1

A file region is consistent if all clients will always see the same data, regardless of which replicas they read from. A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety. 
>  如果所有 clients 无论从哪个副本读取，总是能看到相同的数据，则该文件区域是一致的
>  在文件数据变更后，如果区域保持一致，并且 clients 可以看到变更写入的所有内容，则该文件区域是被定义的 (defined 基于 consistent，比 consistent 更强)

When a mutation succeeds without interference from concurrent writers, the affected region is defined (and by implication consistent): all clients will always see what the mutation has written. Concurrent successful mutations leave the region undefined but consistent: all clients see the same data, but it may not reflect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations. 
>  当一个变更在没有并发写入者的干扰的情况下成功时 (即只有一个写入者变更该文件)，受影响的区域是被定义的 (故也是一致的)：所有的 clients 都会看到该变更写入的内容
>  并发的成功变更会让区域未定义但一致：所有的 clients 会看到相同的数据，但该数据可能不能反映出任何一次变更所写入的信息，它通常有多个变更的混合片段构成

A failed mutation makes the region inconsistent (hence also undefined): different clients may see different data at different times. 
>  一个失败的变更会让区域不一致 (进而也是未定义的)，不同的 clients 会在不同的时间看到不同的数据

We describe below how our applications can distinguish defined regions from undefined regions. The applications do not need to further distinguish between different kinds of undefined regions. 
>  GFS 会区分未定义的区域和定义的区域，但不会进一步区分不同类型的未定义的区域

Data mutations may be writes or record appends. A write causes data to be written at an application-specified file offset. A record append causes data (the “record”) to be appended atomically at least once even in the presence of concurrent mutations, but at an offset of GFS’s choosing (Section 3.3). (In contrast, a “regular” append is merely a write at an offset that the client believes to be the current end of file.) The offset is returned to the client and marks the beginning of a defined region that contains the record. In addition, GFS may insert padding or record duplicates in between. They occupy regions considered to be inconsistent and are typically dwarfed by the amount of user data. 
>  GFS 中的数据变更可以是写入或者是记录追加
>  写入操作会在应用程序指定的文件偏移处写入数据
>  记录追加操作会使得数据 (“记录”) 在多个并发变更的情况下至少被原子化地追加一次，但偏移处由 GFS 选择 (相比之下，常规的追加仅仅是在 client 认为的当前文件末尾处的写入操作)。GFS 选择的偏移量会被返回给 client，并且该偏移量标记了一个包含了追加记录的被定义的区域的开始。此外，GFS 可能在记录之间插入填充数据或重复记录，这些数据占据的区域通常认为是不一致的，并且通常远小于数据量

After a sequence of successful mutations, the mutated file region is guaranteed to be defined and contain the data written by the last mutation. GFS achieves this by (a) applying mutations to a chunk in the same order on all its replicas (Section 3.1), and (b) using chunk version numbers to detect any replica that has become stale because it has missed mutations while its chunkserver was down (Section 4.5). Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity. 
>  GFS 保证，在一系列成功的变更后，被变更的文件区是被定义的，并且包含最后一次变更写入的数据
>  GFS 通过以下两种方式实现这一点：
>  (a) 在一个块的所有副本上以相同的顺序应用所有的变更
>  (b) 使用块版本号来检测是否存在由于 chunkserver 故障而过期的块副本
>  过期的块副本将不会参加变更，master 也不会在 client 请求块数据时提供过期副本的位置，它们会尽早被垃圾回收

Since clients cache chunk locations, they may read from a stale replica before that information is refreshed. This window is limited by the cache entry’s timeout and the next open of the file, which purges from the cache all chunk information for that file. Moreover, as most of our files are append-only, a stale replica usually returns a premature end of chunk rather than outdated data. When a reader retries and contacts the master, it will immediately get current chunk locations. 
>  因为 client 会缓存块位置，因此在它们的信息刷新之前，它们可能会从过期的块副本中读取数据
>  这一窗口期由 client 的缓存项的超时时间和文件的下一次打开操作所限制 (文件的打开操作会清除该文件的所有块信息缓存)
>  此外，由于我们大多数文件是仅追加的，一个过期的副本通常返回的是该块的更早的结束位置，而不是过时的数据 (因为追加操作不会修改之前的数据)
>  当读取者重试并联系 master 时，它就会立即得到当前的块位置

Long after a successful mutation, component failures can of course still corrupt or destroy data. GFS identifies failed chunkservers by regular handshakes between master and all chunkservers and detects data corruption by checksumming (Section 5.2). Once a problem surfaces, the data is restored from valid replicas as soon as possible (Section 4.3). A chunk is lost irreversibly only if all its replicas are lost before GFS can react, typically within minutes. Even in this case, it becomes unavailable, not corrupted: applications receive clear errors rather than corrupt data. 
>  在成功的变更之后，组件故障仍然可能污染或者销毁数据
>  GFS 通过 master 和所有 chunkserver 之间的定期握手来识别故障的块服务器，并且通过校验和检测数据污染
>  一旦发现问题，数据会尽可能块地从有效的副本中恢复，只有在 GFS 能够做出反应之前 (一般在几分钟内)，所有的副本都丢失了，一个块的数据才会不可逆转地丢失。即使在这种情况下，数据也只是变得不可用，而不是被污染：应用程序会收到明确的错误信息，而不是损坏的数据

### 2.7.2 Implications for Applications 
GFS applications can accommodate the relaxed consistency model with a few simple techniques already needed for other purposes: relying on appends rather than overwrites, checkpointing, and writing self-validating, self-identifying records. 
>  GFS 应用程序可以通过几种已经为了其他目的而需要的简单技术来适应 GFS 的松弛的一致模型：依赖追加而不是覆盖写、记录检查点、写入自我验证和自我标识的记录

Practically all our applications mutate files by appending rather than overwriting. In one typical use, a writer generates a file from beginning to end. It atomically renames the file to a permanent name after writing all the data, or periodically checkpoints how much has been successfully written. Checkpoints may also include application-level checksums. Readers verify and process only the file region up to the last checkpoint, which is known to be in the defined state. Regardless of consistency and concurrency issues, this approach has served us well. Appending is far more efficient and more resilient to application failures than random writes. Checkpointing allows writers to restart incrementally and keeps readers from processing successfully written file data that is still incomplete from the application’s perspective. 
>  我们几乎所有的应用程序都通过追加而不是覆盖写来变更文件
>  在一个典型的使用场景中，一个写入者需要从头到尾生成一个文件。它可以在写完所有数据后，原子化地将文件重命名一个永久名称，或者周期性地记录已经成功写入的内容。检查点还可以包括应用级别的校验和。读取者只验证并处理到上一个检查点为止的文件区域，该区域的状态是定义的。不论是并发性问题还是一致性问题，这种方法一直都较为有效。
>  追加相较于随机写，会远远地更加高效并且更能容忍应用故障
>  检查点机制允许写入者增量式地重启，并且可以防止读取者处理那些从应用角度来看仍然不完整的成功写入的数据

In the other typical use, many writers concurrently append to a file for merged results or as a producer-consumer queue. Record append’s append-at-least-once semantics preserves each writer’s output. Readers deal with the occasional padding and duplicates as follows. Each record prepared by the writer contains extra information like checksums so that its validity can be verified. A reader can identify and discard extra padding and record fragments using the checksums. If it cannot tolerate the occasional duplicates (e.g., if they would trigger non-idempotent operations), it can filter them out using unique identifiers in the records, which are often needed anyway to name corresponding application entities such as web documents. These functionalities for record I/O (except duplicate removal) are in library code shared by our applications and applicable to other file interface implementations at Google. With that, the same sequence of records, plus rare duplicates, is always delivered to the record reader. 
>  另一个典型的使用场景中，许多写入者会并发地向一个文件追加内容，例如要合并结果，或者是一个生产者-消费者队列模型 (多个生产者向队列中写入内容)
>  记录追加操作的 “至少一次追加” 语义会保留每个写入者的输出，读取者会按照以下方式处理偶尔出现的填充和重复项：使用校验和来识别和丢弃多余的记录片段 (回忆起写入者准备的每个记录都包含了额外的信息，例如校验和，用于验证信息的有效性)，如果读取者不能容忍偶尔的重复记录 (例如重复记录会出发非等幂操作)，它可以用记录中的唯一标识符过滤掉重复的记录 (记录中的唯一标识符实践中也常常需要被用来命名相应的应用程序实体，例如网页文档)
>  这些用于记录 IO (这里记录是名词) 的功能都在我们应用程序共享的库代码中，并且适用于 Google 中的其他文件接口实现
>  因此，对于重复记录的处理会由读取程序自行调库完成，数据传输时，会直接向读取者传递完整的记录序列

# 3 System Interactions
We designed the system to minimize the master’s involvement in all operations. With that background, we now describe how the client, master, and chunkservers interact to implement data mutations, atomic record append, and snapshot. 
>  在系统设计时，我们意图最小化 master 对于所有操作的参与

## 3.1 Leases and Mutation Order 
A mutation is an operation that changes the contents or metadata of a chunk such as a write or an append operation. Each mutation is performed at all the chunk’s replicas.  
>  变更是指修改了一个块的内容或者元数据的操作，例如写入或追加操作，每个变更都会在该块的所有副本上执行

We use leases to maintain a consistent mutation order across replicas. The master grants a chunk lease to one of the replicas, which we call the primary. The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations. Thus, the global mutation order is defined first by the lease grant order chosen by the master, and within a lease by the serial numbers assigned by the primary.
>  我们使用租约来维护副本之间的一致变更顺序
>  master 将一个块的租约授予它的一个副本，我们称其为 primary, primary 为块的所有变更操作选择一个串行的顺序，所有的副本都遵循这一顺序应用变更
>  因此，对于一个块的内容，其全局变更顺序首先由 master 决定的租约授予顺序定义，然后在租约中进一步被 primary 分配的变更应用顺序定义

The lease mechanism is designed to minimize management overhead at the master. A lease has an initial timeout of 60 seconds. However, as long as the chunk is being mutated, the primary can request and typically receive extensions from the master indefinitely. These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunkservers. The master may sometimes try to revoke a lease before it expires (e.g., when the master wants to disable mutations on a file that is being renamed). Even if the master loses communication with a primary, it can safely grant a new lease to another replica after the old lease expires. 
>  租约机制旨在最小化 master 的管理开销
>  租约的最初超时时间为 60 秒，但只要块正在被变更，primary 可以无限地向 master 请求并且收到租约时间的延长
>  这些延长请求和允许会附带在 master 和所有 chunkservers 之间定期交换的 HeartBeat 消息中
>  有时，master 可能会在租约到期之前尝试撤销它 (例如，当 master 想要禁止对于正在重命名的文件的修改时)
>  即便 master 和 primary 失去通信，它也可以在就租约到期后安全地将新租约授予给新副本

![[pics/GFS-Fig2.png]]

In Figure 2, we illustrate this process by following the control flow of a write through these numbered steps. 
1. The client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses (not shown). 
2. The master replies with the identity of the primary and the locations of the other (secondary) replicas. The client caches this data for future mutations. It needs to contact the master again only when the primary  becomes unreachable or replies that it no longer holds a lease.  
3. The client pushes the data to all the replicas. A client can do so in any order. Each chunkserver will store the data in an internal LRU buffer cache until the data is used or aged out. By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the network topology regardless of which chunkserver is the primary. Section 3.2 discusses this further. 
4. Once all the replicas have acknowledged receiving the data, the client sends a write request to the primary. The request identifies the data pushed earlier to all of the replicas. The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization. It applies the mutation to its own local state in serial number order. 
5. The primary forwards the write request to all secondary replicas. Each secondary replica applies mutations in the same serial number order assigned by the primary. 
6. The secondaries all reply to the primary indicating that they have completed the operation. 
7. The primary replies to the client. Any errors encountered at any of the replicas are reported to the client. In case of errors, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. (If it had failed at the primary, it would not have been assigned a serial number and forwarded.) The client request is considered to have failed, and the modified region is left in an inconsistent state. Our client code handles such errors by retrying the failed mutation. It will make a few attempts at steps (3) through (7) before falling back to a retry from the beginning of the write. 

>  Figure 2 展示了写入操作的控制流和数据流，其步骤为：
>  1. client 请求 master 哪个 chunkserver 持有当前块的租约，以及当前块的其他副本的位置。如果此时没有 chunkserver 持有租约，master 将选择一个副本，为其授予租约
>  2. master 回复 client primary 的身份和其他 (次级) 副本的位置，client 缓存这部分数据。只有当 primary 变得不可达或者它回复它不再持有租约时，client 才会再次联系 master
>  3. client 将数据 (要写入的数据) 推送到所有副本，推送的顺序可以任意。每个 chunkserver 将该数据存储在一个内部 LRU 缓冲区缓存中，直到该数据被使用或者老化。通过将数据流从控制流中解耦，我们可以根据网络拓扑安排昂贵的数据流，而不考虑哪个 chunkserver 是 primary
>  4. 一旦所有副本都确认收到数据，client 向 primary 发送一个写请求，该请求标识之前推送到所有副本的数据。primary 为它收到的所有变更 (可能来自于不同的 clients) 分配连续的序列号，它按照序列号顺序将变更应用到自身本地状态上
>  5. primary 将写请求转发给所有次级副本，每个次级副本按照相同的序列号应用变更
>  6. 所有次级副本向 primary 回复，表明它们已完成操作
>  7. primary 回复 client，任何副本遇到的错误都会报告给 client。如果发生了错误，写入操作可能在 primary 和次级副本的任意子集上成功完成 (如果在 primary 失败，写入操作不会被分配序列号并且不会被转发)，此时 client 请求被视作失败，被修改的区域将处于不一致状态。我们的 client 代码通过重试失败的变更来处理这样的错误，在回退到重新开始之前，它会在步骤 (3)-(7) 之间尝试几次

If a write by the application is large or straddles a chunk boundary, GFS client code breaks it down into multiple write operations. They all follow the control flow described above but may be interleaved with and overwritten by concurrent operations from other clients. Therefore, the shared file region may end up containing fragments from different clients, although the replicas will be identical because the individual operations are completed successfully in the same order on all replicas. This leaves the file region in consistent but undefined state as noted in Section 2.7. 
>  如果应用程序的写入操作较大或跨越了块边界，GFS client 代码会将其拆分为多个写入操作，这些操作都遵循上述控制流，但可能会与其他 client 的并发操作交错并被覆盖
>  因此，共享的文件区域最终可能包含来自不同客户端的片段，尽管由于各个操作在所有副本上都以相同顺序成功完成，副本将保持一致，但这会使得文件区域处于一致但未定义状态

## 3.2 Data Flow 
We decouple the flow of data from the flow of control to use the network efficiently. While control flows from the client to the primary and then to all secondaries, data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. Our goals are to fully utilize each machine’s network bandwidth, avoid network bottlenecks and high-latency links, and minimize the latency to push through all the data. 
>  我们解耦数据流和控制流，以高效利用网络
>  控制流主要从 client 流向 primary 然后流向次级副本，数据流则以管道方式线性地被推送到 chunkserver 链中的各个 chunkserver
>  我们的目标是充分利用每个机器的网络带宽，避免网络瓶颈和高延迟链路，并尽量减少推送所有数据的延迟

To fully utilize each machine’s network bandwidth, the data is pushed linearly along a chain of chunkservers rather than distributed in some other topology (e.g., tree). Thus, each machine’s full outbound bandwidth is used to transfer the data as fast as possible rather than divided among multiple recipients. 
>  为了充分利用每台机器的网络带宽，数据是线性地推送到一系列 chunkservers 中，而不是按照其他拓扑结构分布
>  因此，每台机器的全部 outbound 带宽都会被用来尽可能快地传输数据，而不是分散给多个接收者

To avoid network bottlenecks and high-latency links (e.g., inter-switch links are often both) as much as possible, each machine forwards the data to the “closest” machine in the network topology that has not received it. Suppose the client is pushing data to chunkservers S1 through S4. It sends the data to the closest chunkserver, say S1. S1 forwards it to the closest chunkserver S2 through S4 closest to S1, say S2. Similarly, S2 forwards it to S3 or S4, whichever is closer to S2, and so on. Our network topology is simple enough that “distances” can be accurately estimated from IP addresses. 
>  为了尽可能避免网络瓶颈和高延迟链路 (例如，交换机之间的链路通常就会是高延迟的瓶颈链路)，每台机器都将数据转发到网络拓扑中 “最近的” 尚未接受该数据的机器
>  假设 client 正在向 chunkserver S1 到 chunkserver S4 推送数据，它会先将数据发送到最近的 chunkserver，例如 S1。S1 再将其转发到最近的 chunkserver，例如 S2。S2 再将其转发给最近的 chunkserver，以此类推
>  我们的网络拓扑足够简单，chunkserver 之间的 "距离" 可以通过 IP 地址准确估计

Finally, we minimize latency by pipelining the data transfer over TCP connections. Once a chunkserver receives some data, it starts forwarding immediately. Pipelining is especially helpful to us because we use a switched network with full-duplex links. Sending the data immediately does not reduce the receive rate. Without network congestion, the ideal elapsed time for transferring $B$ bytes to $R$ replicas is $B/T+R L$ where $T$ is the network throughput and $L$ is latency to transfer bytes between two machines. Our network links are typically 100 Mbps $(T)$ , and $L$ is far below 1 ms. Therefore, 1 MB can ideally be distributed in about 80 ms. 
>  最后，我们通过在 TCP 连接上执行数据传输流水线来最小化延迟，即当一个 chunkserver 收到一些数据时，它会立即开始转发
>  流水线化带来了很大帮助，因为我们使用的是具有全双工链路的交换网络，chunkserver 立即发送数据不会降低它的接受速率
>  在网络没有拥塞的情况下，将 B 字节传输给 R 个副本的理想时间是 $B/T + RL$，其中 $T$ 是网络吞吐量，$L$ 是在两台机器之间传输字节的延迟
>  我们的网络链路通常是 100Mbps ($T$ 是 100/8 MB/s)，$L$ 远低于 1ms，因此，理想状态下，1MB 的数据可以在大约 80ms 内分发完毕 ($\frac {1}{100/8}=0.08$ s)

## 3.3 Atomic Record Appends 
GFS provides an atomic append operation called record append. In a traditional write, the client specifies the offset at which data is to be written. Concurrent writes to the same region are not serializable: the region may end up containing data fragments from multiple clients. In a record append, however, the client specifies only the data. GFS appends it to the file at least once atomically (i.e., as one continuous sequence of bytes) at an offset of GFS’s choosing and returns that offset to the client. This is similar to writing to a file opened in `O_APPEND` mode in Unix without the race conditions when multiple writers do so concurrently. 
>  GFS 提供了称为记录追加的原子追加操作
>  在传统的写入中，client 指定数据要被写入的偏移量，对于相同区域的并发写入是不可序列化的，该区域最后会包含来自多个 clients 的数据片段
>  在记录追加操作中，client 仅指定要追加的数据，GFS 将保证将这些数据原子化地 (即作为一个连续的字节序列) 追加到文件中至少一次，具体的偏移量将由 GFS 选定，该偏移量会被返回给 client
>  这类似于在 Unix 中对以 `O_APPEND` 打开的文件进行写入，同时多个写入者同时 写入时不会有竞争条件

Record append is heavily used by our distributed applications in which many clients on different machines append to the same file concurrently. Clients would need additional complicated and expensive synchronization, for example through a distributed lock manager, if they do so with traditional writes. In our workloads, such files often serve as multiple-producer/single-consumer queues or contain merged results from many different clients. 
>  记录追加在我们的分布式应用程序中被广泛使用，在我们的应用程序中，许多在不同机器上的 clients 会并发地向一个文件中追加内容
>  如果使用传统的写入方式，clients 将需要额外的复杂且昂贵的同步机制，例如通过分布式锁管理器
>  在我们的工作负载中，这样会被并发追加的文件通常用作多生产者/单消费者队列或包含来自不同 clients 的合并结果

Record append is a kind of mutation and follows the control flow in Section 3.1 with only a little extra logic at the primary. The client pushes the data to all replicas of the last chunk of the file Then, it sends its request to the primary. The primary checks to see if appending the record to the current chunk would cause the chunk to exceed the maximum size (64 MB). If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client indicating that the operation should be retried on the next chunk. (Record append is restricted to be at most one-fourth of the maximum chunk size to keep worstcase fragmentation at an acceptable level.) If the record fits within the maximum size, which is the common case, the primary appends the data to its replica, tells the secondaries to write the data at the exact offset where it has, and finally replies success to the client. 
>  记录追加操作属于变更，遵循 3.1 节的控制流，同时在 primary 处有一点额外逻辑：client 将要追加的数据推送到块的所有副本，然后向 client 发送追加写请求，primary 会额外检查将记录追加到当前块是否会导致块超过最大大小，如果是，它会将当前块填充到最大大小，然后告诉次级副本做同样的事 (填充)，然后回复 client，表示操作应该在下一个块重试 (为了保持在最坏情况下的内部碎片在可接受的水平，记录追加操作被限制在最多为最大段大小的四分之一)
>  如果记录追加不会导致块超过最大大小，就是常规情况，primary 将数据追加到其副本，并告诉次级副本在同样的偏移处写入，然后回复客户端成功

If a record append fails at any replica, the client retries the operation. As a result, replicas of the same chunk may contain different data possibly including duplicates of the same record in whole or in part. GFS does not guarantee that all replicas are bytewise identical. It only guarantees that the data is written at least once as an atomic unit. This property follows readily from the simple observation that for the operation to report success, the data must have been written at the same offset on all replicas of some chunk. Furthermore, after this, all replicas are at least as long as the end of record and therefore any future record will be assigned a higher offset or a different chunk even if a different replica later becomes the primary. In terms of our consistency guarantees, the regions in which successful record append operations have written their data are defined (hence consistent), whereas intervening regions are inconsistent (hence undefined). Our applications can deal with inconsistent regions as we discussed in Section 2.7.2. 
>  如果记录追加操作在任意副本失败，client 会重试该操作，因此，同一个块的副本可能会包含不同的数据，可能包括重复的或者部分的相同记录
>  GFS 不保证所有副本在字节上相同，它仅保证数据会至少被原子写入一次，因此，为了能向 client 报告成功写入，数据将至少在所有副本的相同偏移处写入一次，并且，写入之后，所有副本的长度至少是写入该记录一次后的长度，因此即便之后不同的副本成为 primary，未来会写入的记录都将被分配更高的偏移或者其他的块 (primary 决定追加的偏移量)
>  就我们的一致性保证而言，成功被追加数据后的区域是定义的 (因此是一致的)，而中间区域是不一致的 (因此是未定义的)，我们的应用程序可以处理这些不一致的区域

## 3.4 Snapshot 
The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations. Our users use it to quickly create branch copies of huge data sets (and often copies of those copies, recursively), or to checkpoint the current state before experimenting with changes that can later be committed or rolled back easily. 
>  快照操作会几乎瞬时地拷贝文件或目录树 ("源")，同时会最小对于正在进行的变更带来的中断
>  我们的用户用快照操作快速创建大型数据集的分支副本 (并且通常递归地利用副本创建副本)，或者用快照操作对当前状态记录检查点，便于执行实验性修改后回滚这些更改

Like AFS [5], we use standard copy-on-write techniques to implement snapshots. When the master receives a snapshot request, it first revokes any outstanding leases on the chunks in the files it is about to snapshot. This ensures that any subsequent writes to these chunks will require an interaction with the master to find the lease holder. This will give the master an opportunity to create a new copy of the chunk first. 
>  我们用标准的写时拷贝技术实现快照操作
>  当 master 收到快照请求，它先撤销快照涉及的文件块的任何租约，这确保任何后续对这些块的写入操作将需要重新和 master 交互以找到租约持有者，这会为 master 提供先创建块的新副本的机会

After the leases have been revoked or have expired, the master logs the operation to disk. It then applies this log record to its in-memory state by duplicating the metadata for the source file or directory tree. The newly created snapshot files point to the same chunks as the source files. 
>  租约被撤销或过期后，master 将操作日志记录到磁盘，master 会在复制了其内存中的源树的元数据后对其应用操作日志，新创建的快照文件实际上指向与源文件相同的块

The first time a client wants to write to a chunk C after the snapshot operation, it sends a request to the master to find the current lease holder. The master notices that the reference count for chunk C is greater than one. It defers replying to the client request and instead picks a new chunk handle C’. It then asks each chunkserver that has a current replica of C to create a new chunk called C’. By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network (our disks are about three times as fast as our 100 Mb Ethernet links). From this point, request handling is no different from that for any chunk: the master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally, not knowing that it has just been created from an existing chunk. 
>  首次有 client 想要在快照操作后对 chunk C 进行写入时，client 会向 master 发送请求，以找到当前的租约持有者，master 注意到 chunk C 的引用计数大于 1，会推迟对 client 请求的回复，然后选择一个新的 chunk 句柄 C'
>  master 向每个拥有 C 副本的 chunkserver 请求创建一个名为 C' 的新块，新块和源块处于同一 chunkserver，这确保数据可以本地拷贝，而不是通过网络传输 (我们的磁盘速度是 100Mb 以太网链路的三倍)
>  之后，master 对 client 的请求处理方式就照常，master 将新块 C' 的一个副本授予租约，然后回复 client, client 可以正常写入该块

# 4 Master Operation
The master executes all namespace operations. In addition, it manages chunk replicas throughout the system: it makes placement decisions, creates new chunks and hence replicas, and coordinates various system-wide activities to keep chunks fully replicated, to balance load across all the chunkservers, and to reclaim unused storage. We now discuss each of these topics. 
>  master 执行所有的命名空间操作
>  此外，master 还管理整个系统的块副本，它需要做 placement 决策、创建新的块和其副本、协调各种系统级活动以保持块完整复制和平衡 chunkservers 之间的负载、回收未使用的存储

## 4.1 Namespace Management and Locking 
Many master operations can take a long time: for example, a snapshot operation has to revoke chunkserver leases on all chunks covered by the snapshot. We do not want to delay other master operations while they are running. Therefore, we allow multiple operations to be active and use locks over regions of the namespace to ensure proper serialization. 
>  许多 master 操作会需要很长时间，例如，快照操作必须撤销快照涉及的所有块的租约
>  我们不希望这些需要长时间执行的操作延迟其他 master 操作，因此，我们允许多个操作同时进行，并使用命名空间区域上的锁来确保正确的顺序

Unlike many traditional file systems, GFS does not have a per-directory data structure that lists all the files in that directory. Nor does it support aliases for the same file or directory (i.e, hard or symbolic links in Unix terms). GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With prefix compression, this table can be efficiently represented in memory. Each node in the namespace tree (either an absolute file name or an absolute directory name) has an associated read-write lock. Each master operation acquires a set of locks before it runs. 
>  和许多传统文件系统不同，GFS 没有那种列出目录下所有文件的 per-directory 数据结构，它也不支持同一文件或目录的别名 (即 Unix 中的硬链接或符号链接)
>  GFS 逻辑上将其命名空间表示为一个映射表，它将完整的路径名映射到元数据，通过前缀压缩，该表可以被有效的存放在内存中
>  命名空间树中的每个节点 (无论是绝对文件名还是绝对目录名) 都有一个相关的读写锁，每个 master 操作都会在执行之前获取一组锁

Typically, if it involves /d1/d2/.../dn/leaf, it will acquire read-locks on the directory names /d1, /d1/d2, ..., /d1/d2/.../dn, and either a read lock or a write lock on the full pathname /d1/d2/.../dn/leaf. Note that leaf may be a file or directory depending on the operation. 
>  例如，如果操作涉及 `/d1/d2/.../dn/leaf` ，它将获取名为 `/d1, /d1/d2, ..., /d1/d2/.../d2` 的目录的读锁，以及获取完整路径 `/d1/d2/.../dn/leaf` 的写锁或读锁
>  注意，根据操作的类型，`leaf` 可以是文件也可以是目录

We now illustrate how this locking mechanism can prevent a file /home/user/foo from being created while /home/user is being snapshotted to /save/user. 
>  我们举例说明这个锁机制如何避免在 `/home/user` 正在被快照到 `/save/user` 时文件 `/home/user/foo` 被创建

The snapshot operation acquires read locks on /home and /save, and write locks on /home/user and /save/user. The file creation acquires read locks on /home and /home/user, and a write lock on /home/user/foo. The two operations will be serialized properly because they try to obtain conflicting locks on /home/user. File creation does not require a write lock on the parent directory because there is no “directory”, or inode-like, data structure to be protected from modification. The read lock on the name is sufficient to protect the parent directory from deletion. 
>  快照操作获取 `/home, /save` 的读锁，以及 `/home/user, /save/user` 的写锁
>  文件创建操作获取 `/home, /home/user` 的读锁，以及 `/home/user/foo` 的写锁
>  这两个操作会被正确地串行化，因为它们都尝试获取 `/home/user` 的锁
>  文件创作操作不需要获取父目录的写锁，因为 GFS 不存在需要防止不被修改的 "目录" 或类似 inode 的数据结构，对目录的读锁就足以防止父目录不被删除

One nice property of this locking scheme is that it allows concurrent mutations in the same directory. For example, multiple file creations can be executed concurrently in the same directory: each acquires a read lock on the directory name and a write lock on the file name. The read lock on the directory name suffices to prevent the directory from being deleted, renamed, or snapshotted. The write locks on file names serialize attempts to create a file with the same name twice. 
>  这个锁方案的一个优势是它允许同一个目录中并发地进行变更
>  例如，同个目录内可以并发执行多个文件创建操作，每个操作对目录名称获取一个读锁，对文件名获取一个写锁
>  对目录名的读锁足以防止目录被删除、重命名、被快照，对文件名的写锁可以正确串行化多次尝试创建同一名称的文件的操作

Since the namespace can have many nodes, read-write lock objects are allocated lazily and deleted once they are not in use. Also, locks are acquired in a consistent total order to prevent deadlock: they are first ordered by level in the namespace tree and lexicographically within the same level. 
>  由于命名空间可以包含多个节点，读写锁对象会按需分配，并在不再使用后删除
>  为了防止死锁，锁的获取遵循一致的全序关系：首先按照命名空间树中的层级顺序排序，同一层级内按照字典序排序

## 4.2 Replica Placement 
A GFS cluster is highly distributed at more levels than one. It typically has hundreds of chunkservers spread across many machine racks. These chunkservers in turn may be accessed from hundreds of clients from the same or different racks. Communication between two machines on different racks may cross one or more network switches. Additionally, bandwidth into or out of a rack may be less than the aggregate bandwidth of all the machines within the rack. Multi-level distribution presents a unique challenge to distribute data for scalability, reliability, and availability. 
>  GFS 集群在多个层级上都是高度分布的，它一般有上百个 chunkservers，分布在多个机架上
>  这些 chunkservers 可以被上百个相同或不同机架上的 clients 访问
>  不同机架上的两台机器的通信可能经过一个或多个网络交换机，此外，进入或离开一个机架的总带宽可能小于机架上所有机器的带宽总和

The chunk replica placement policy serves two purposes: maximize data reliability and availability, and maximize network bandwidth utilization. For both, it is not enough to spread replicas across machines, which only guards against disk or machine failures and fully utilizes each machine’s network bandwidth. We must also spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or offline (for example, due to failure of a shared resource like a network switch or power circuit). It also means that traffic, especially reads, for a chunk can exploit the aggregate bandwidth of multiple racks. On the other hand, write traffic has to flow through multiple racks, a tradeoff we make willingly. 
>  分块副本放置的策略的目的有两个：最大化数据可靠性和可用性、最大化网络带宽利用
>  为此，仅仅将副本放在不同的机器上是不够的，这只能防止机器或磁盘故障，并只能充分利用每台机器的网路带宽
>  我们还需要将块副本分布在不同机架上，确保同一个块的副本在整个机架受损或离线 (例如，由于机架的共享资源例如网络交换机或电源电路故障) 时也可用。这也意外着对于某个块的流量 (特别是读) 可以利用多个机架的聚合带宽。但另一方面，写入流量需要流经多个机架

## 4.3 Creation, Re-replication, Rebalancing 
Chunk replicas are created for three reasons: chunk creation, re-replication, and rebalancing. 
>  块副本在三种情况下会被创建：块被创建时、块被重新拷贝时、块的副本分布需要重新平衡时

When the master creates a chunk, it chooses where to place the initially empty replicas. It considers several factors. (1) We want to place new replicas on chunkservers with below-average disk space utilization. Over time this will equalize disk utilization across chunkservers. (2) We want to limit the number of “recent” creations on each chunkserver. Although creation itself is cheap, it reliably predicts imminent heavy write traffic because chunks are created when demanded by writes, and in our append-once-read-many workload they typically become practically read-only once they have been completely written. (3) As discussed above, we want to spread replicas of a chunk across racks. 
>  当 master 创建块时，它需要选择在哪里放置最初为空的副本，考虑的因素包括：
>  1. 希望将新的副本放在磁盘空间利用率低于平均水平的 chunkserver，随着时间的推移，这会使得各个 chunkserver 的磁盘利用率趋于均衡
>  2. 希望限制每个 chunkserver 上的 “近期” 创建数量，虽然创建操作本身 cheap，但它预示了即将出现的大量写入流量，因为块是在有写入需求时才创建的，并且在我们的 “一次写入，多次读取” 的工作负载中，一旦数据完全写入，它们通常会变得几乎只读
>  3. 希望将一个块的副本分布在不同机架上

The master re-replicates a chunk as soon as the number of available replicas falls below a user-specified goal. This could happen for various reasons: a chunkserver becomes unavailable, it reports that its replica may be corrupted, one of its disks is disabled because of errors, or the replication goal is increased. 
>  master 会在可用副本数量低于用户指定的目标时尽可能快地重拷贝一个快
>  “可用副本数量低于用户指定的目标” 的发生有多种原因：一个 chunkserver 变得不可用、chunkserver 报告其副本可能损坏、chunkserver 的某个磁盘由于错误被禁用、用户增大了目标数量

Each chunk that needs to be re-replicated is prioritized based on several factors. One is how far it is from its replication goal. For example, we give higher priority to a chunk that has lost two replicas than to a chunk that has lost only one. In addition, we prefer to first re-replicate chunks for live files as opposed to chunks that belong to recently deleted files (see Section 4.4). Finally, to minimize the impact of failures on running applications, we boost the priority of any chunk that is blocking client progress. 
>  每个需要被重拷贝的块都会根据多个因素进行优先级排序，其中一个因素是和目标数量差异程度
>  此外，我们更倾向于优先重新拷贝有活动文件的块，而不是属于最近被删除文件的块
>  为了最小化故障对运行程序的影响，我们会增大任何阻塞了 client 进度的块的优先级

The master picks the highest priority chunk and “clones” it by instructing some chunkserver to copy the chunk data directly from an existing valid replica. The new replica is placed with goals similar to those for creation: equalizing disk space utilization, limiting active clone operations on any single chunkserver, and spreading replicas across racks. To keep cloning traffic from overwhelming client traffic, the master limits the numbers of active clone operations both for the cluster and for each chunkserver. Additionally, each chunkserver limits the amount of bandwidth it spends on each clone operation by throttling its read requests to the source chunkserver. 
>  master 挑选最高优先级的块，指示某个 chunkserver 直接从现有副本拷贝其数据以复制块数据
>  新副本的防止目标和块创建时类似：均衡磁盘空间利用率、限制任何单个 chunkserver 上的活跃 clone 操作数量、在机架之间分散副本
>  为了避免 cloning 流量压倒 client 流量，master 会限制集群和每个 chunkserver 的活跃 clone 操作数量
>  此外，每个 chunkserver 会通过限制它对源 chunkserver 的读取请求数量来限制它花在 clone 操作使用的带宽量

Finally, the master rebalances replicas periodically: it examines the current replica distribution and moves replicas for better disk space and load balancing. Also through this process, the master gradually fills up a new chunkserver rather than instantly swamps it with new chunks and the heavy write traffic that comes with them. The placement criteria for the new replica are similar to those discussed above. In addition, the master must also choose which existing replica to remove. In general, it prefers to remove those on chunkservers with below-average free space so as to equalize disk space usage. 
>  master 周期性重新平衡副本：它检查当前副本分布，并移动副本，以实现更好的磁盘空间和负载均衡
>  通过这一过程，master 会逐渐填满一个新的 chunkserver，而不是立即向它添加大量块，导致它承受随后的大量写入流量
>  新副本的放置标准和上述讨论的类似
>  此外，master 必须也选择要移除哪个现存的副本，通常，它偏好移除磁盘剩余空间低于平均水平的 chunkserver，以均衡磁盘空间使用

## 4.4 Garbage Collection 
After a file is deleted, GFS does not immediately reclaim the available physical storage. It does so only lazily during regular garbage collection at both the file and chunk levels. We find that this approach makes the system much simpler and more reliable. 
>  一个文件被删除后，GFS 不会立刻收回其物理空间，它仅在执行文件和块级别的常规垃圾回收操作时才真正删除文件
>  这种方法使得系统更加简单可靠

### 4.4.1 Mechanism 
When a file is deleted by the application, the master logs the deletion immediately just like other changes. However instead of reclaiming resources immediately, the file is just renamed to a hidden name that includes the deletion timestamp. During the master’s regular scan of the file system namespace, it removes any such hidden files if they have existed for more than three days (the interval is configurable). Until then, the file can still be read under the new, special name and can be undeleted by renaming it back to normal. When the hidden file is removed from the namespace, its in-memory metadata is erased. This effectively severs its links to all its chunks. 
>  当应用删除一个文件时，master 会立刻记录这一删除操作 (其他更改也一样，会被立刻记录)
>  master 不会立刻收回资源，该文件会被重命名为一个包含删除时间戳的隐藏名称，在 master 对常规文件系统命名空间的扫描过程中，如果发现隐藏文件的存活时间超过三天，就删除它
>  在这个时间点之前，文件仍然可以使用新的特殊名称读取，并且可以将其重命名为正常名称以恢复它
>  当隐藏文件从命名空间被删除后，它内存中的元数据也会被删除，这有效切断了它与所有块的链接

In a similar regular scan of the chunk namespace, the master identifies orphaned chunks (i.e., those not reachable from any file) and erases the metadata for those chunks. In a HeartBeat message regularly exchanged with the master, each chunkserver reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata. The chunkserver is free to delete its replicas of such chunks. 
>  在对块命名空间的常规扫描中，master 会识别无主的块 (不能从任意文件访问的块)，并删除这些块的元数据
>  chunkservers 在和 master 定期交换的 HeartBeat 消息中会汇报它目前具有哪些块，master 会回复其中哪些块的 ID 已经不在 master 的元数据中了 (被删了)，chunkserver 进而删除自己存储的对应块副本

### 4.4.2 Discussion 
Although distributed garbage collection is a hard problem that demands complicated solutions in the context of programming languages, it is quite simple in our case. We can easily identify all references to chunks: they are in the file-to-chunk mappings maintained exclusively by the master. We can also easily identify all the chunk replicas: they are Linux files under designated directories on each chunkserver. Any such replica not known to the master is “garbage.” 
>  GFS 中，master 通过它维护的 file-to-chunk 映射确定各个 chunks 的所有引用，因此可以识别确定所有的 chunk 副本 (chunk 副本本质是每个 chunkserver 的指定目录下的 Linux 文件)，如果存在 master 未知的 chunk 副本，就会视为垃圾

The garbage collection approach to storage reclamation offers several advantages over eager deletion. First, it is simple and reliable in a large-scale distributed system where component failures are common. Chunk creation may succeed on some chunkservers but not others, leaving replicas that the master does not know exist. Replica deletion messages may be lost, and the master has to remember to resend them across failures, both its own and the chunkserver’s. Garbage collection provides a uniform and dependable way to clean up any replicas not known to be useful. Second, it merges storage reclamation into the regular background activities of the master, such as the regular scans of namespaces and handshakes with chunkservers. Thus, it is done in batches and the cost is amortized. Moreover, it is done only when the master is relatively free. The master can respond more promptly to client requests that demand timely attention. Third, the delay in reclaiming storage provides a safety net against accidental, irreversible deletion. 
>  垃圾回收方法相较于主动删除方法，在存储回收方面提供了几大优势：
>  1. 在组件故障很常见的大规模分布式系统中，垃圾回收方法简单且可靠。chunk 创建操作可能在某些 chunkserver 失败，这会导致这些副本将对于 master 不可知。副本删除消息可能丢失，master 需要记住在故障后 (无论是它自己的还是 chunkserver 的重新发送它们)。
>  2. 垃圾回收方法将存储回收和 master 常规的后台活动合并，例如定期扫描命名空间和与 chunkservers 的 handshakes。因此，垃圾回收是按批量执行的，并且成本会被摊销。此外，只有在 master 相对空闲时才会执行此操作，master 可以更及时相应需要立即关注的 client 请求
>  3. 存储回收的延迟可以一定程度上避免意外的、不可逆的删除

In our experience, the main disadvantage is that the delay sometimes hinders user effort to fine tune usage when storage is tight. Applications that repeatedly create and delete temporary files may not be able to reuse the storage right away. We address these issues by expediting storage reclamation if a deleted file is explicitly deleted again. 
>  垃圾回收的延迟的主要劣势是会妨碍用户在存储空间紧张时对使用方式进行微调，那些反复创建并删除临时文件的应用可能无法立即重用存储空间
>  为了解决这个问题，当一个被删除的文件又被显式删除时，我们会加快存储回收。
 
We also allow users to apply different replication and reclamation policies to different parts of the namespace. For example, users can specify that all the chunks in the files within some directory tree are to be stored without replication, and any deleted files are immediately and irrevocably removed from the file system state. 
>  我们还允许用户为命名空间的不同部分应用不同的拷贝和回收策略
>  例如，用户可以指定某个目录树下的文件的 chunks 不需要拷贝，并且任何被删除的文件都会立即并且不可撤销地从文件系统状态中删除

## 4.5 Stale Replica Detection 
Chunk replicas may become stale if a chunkserver fails and misses mutations to the chunk while it is down. For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas. 
>  如果 chunkserver 故障，故错过了对 chunk 的变更，其 chunk 副本会过期
>  master 对于每个 chunk 维护一个 chunk 版本号，以区分最新的和过期的版本

Whenever the master grants a new lease on a chunk, it increases the chunk version number and informs the up-to-date replicas. The master and these replicas all record the new version number in their persistent state. This occurs before any client is notified and therefore before it can start writing to the chunk. If another replica is currently unavailable, its chunk version number will not be advanced. The master will detect that this chunkserver has a stale replica when the chunkserver restarts and reports its set of chunks and their associated version numbers. If the master sees a version number greater than the one in its records, the master assumes that it failed when granting the lease and so takes the higher version to be up-to-date. 
>  当 master 为一个 chunk 授予新的租约时，它会增加其版本号，并且通知所有最新的副本。master 和这些副本都会在其持久化状态 (磁盘存储) 中记录新的版本号，如果有副本不可用，则其 chunk 版本号不会更新
>  这一过程早于任何 client 被通知之前，因此也早于 client 开始向 chunk 的写入操作
>  当故障的 chunkserver 重启并向 master 报告其 chunk 副本的版本号时，master 会检测到 chunkserver 是否有过期的副本，如果 master 发现 chunkserver 有版本号高于自己记录的版本号，它会认为在授予租约的时候自身发生了故障，进而会将较高的版本号视为最新版本

The master removes stale replicas in its regular garbage collection. Before that, it effectively considers a stale replica not to exist at all when it replies to client requests for chunk information. As another safeguard, the master includes the chunk version number when it informs clients which chunkserver holds a lease on a chunk or when it instructs a chunkserver to read the chunk from another chunkserver in a cloning operation. The client or the chunkserver verifies the version number when it performs the operation so that it is always accessing up-to-date data. 
>  master 在其常规垃圾回收时会移除过期的副本，在此之前，master 在回应 client 关于 chunk 信息的请求时，会直接认为过期的副本不存在 (故不会报告任何过期副本的信息)
>  并且，master 在回复 client 哪个 chunkservers 持有对应 chunk 的租约时、或者在指示 chunkserver 在克隆操作中从另一个 chunkserver 读取 chunk 时，会附带上 chunk 的版本号，则 client 和 chunkserver 就可以借由版本号判断它访问的是否是最新数据

# 5 Fault Tolerance and Diagnosis
One of our greatest challenges in designing the system is dealing with frequent component failures. The quality and quantity of components together make these problems more the norm than the exception: we cannot completely trust the machines, nor can we completely trust the disks. 
>  GFS 设计时最大的挑战在于处理频繁的组件故障
>  集群的组件数量过大，且质量一般，故组件故障的情况很常见，我们无法完全信任机器、硬盘

Component failures can result in an unavailable system or, worse, corrupted data. We discuss how we meet these challenges and the tools we have built into the system to diagnose problems when they inevitably occur. 

## 5.1 High Availability 
Among hundreds of servers in a GFS cluster, some are bound to be unavailable at any given time. We keep the overall system highly available with two simple yet effective strategies: fast recovery and replication. 
>  GFS 保持高度可用性的策略有：快速恢复和复制

### 5.1.1 Fast Recovery 
Both the master and the chunkserver are designed to restore their state and start in seconds no matter how they terminated. In fact, we do not distinguish between normal and abnormal termination; servers are routinely shut down just by killing the process. Clients and other servers experience a minor hiccup as they time out on their outstanding requests, reconnect to the restarted server, and retry. Section 6.2.2 reports observed startup times. 
>  master 和 chunkserver 都被设计为无论是如何终止的，都可以在几秒内恢复其状态并启动
>  我们并不区分正常和异常的终止，服务器通常只是通过杀死进程 (服务进程) 来关闭，clients 和其他 servers 会在它们的请求超时未完成后，略微等待，然后重新连接到重启后的服务器，并重试操作

### 5.1.2 Chunk Replication 
As discussed earlier, each chunk is replicated on multiple chunkservers on different racks. Users can specify different replication levels for different parts of the file namespace. The default is three. The master clones existing replicas as needed to keep each chunk fully replicated as chunkservers go offline or detect corrupted replicas through checksum verification (see Section 5.2). 
>  每个 chunk 都会在不同机架上的多个 chunkservers 中具有副本，用户可以为文件命名空间的不同部分指定不同的拷贝级别
>  master 会在 chunkserver 下线或者检查到损坏的副本 (通过校验和验证) 时，按需克隆现存的副本

Although replication has served us well, we are exploring other forms of cross-server redundancy such as parity or erasure codes for our increasing read-only storage requirements. We expect that it is challenging but manageable to implement these more complicated redundancy schemes in our very loosely coupled system because our traffic is dominated by appends and reads rather than small random writes. 
>  尽管复制方法对我们一直很有效，但我们正在探索其他形式的跨服务器冗余方案，例如奇偶校验或纠删码，以满足日益增长的只读存储需求。
>  我们预计，在我们的非常松散耦合的系统中实现这些更为复杂的冗余方案具有挑战性但可以管理，因为我们的流量主要由追加操作和读取操作组成，而不是小范围的随机写入操作。

### 5.1.3 Master Replication 
The master state is replicated for reliability. Its operation log and checkpoints are replicated on multiple machines. A mutation to the state is considered committed only after its log record has been flushed to disk locally and on all master replicas.
>  master 状态也会拷贝，其操作日志和检查点会拷贝到多个机器上，对于其状态的变更只有在更新到全部的拷贝后才视作完全提交
 
For simplicity, one master process remains in charge of all mutations as well as background activities such as garbage collection that change the system internally. When it fails, it can restart almost instantly. If its machine or disk fails, monitoring infrastructure outside GFS starts a new master process elsewhere with the replicated operation log. Clients use only the canonical name of the master (e.g. gfs-test), which is a DNS alias that can be changed if the master is relocated to another machine. 
>  为了简单起见，全部的变更以及后台活动例如垃圾收集这样内部改变系统状态的活动仍然由单个 master 进程管理
>  当该进程故障，它可以几乎瞬时重启。如果其机器或磁盘故障，GFS 外的监控程序会在具有操作日志拷贝的机器上启动一个新的 master 进程
>  clients 是通过 master 的名称与其通信的，其名称只是一个 DNS 别名，如果 master 在新的机器上启动，这也会相应改变
 
Moreover, “shadow” masters provide read-only access to the file system even when the primary master is down. They are shadows, not mirrors, in that they may lag the primary slightly, typically fractions of a second. They enhance read availability for files that are not being actively mutated or applications that do not mind getting slightly stale results. In fact, since file content is read from chunkservers, applications do not observe stale file content. What could be stale within short windows is file metadata, like directory contents or access control information. 
>  此外，shadow masters 会在主 master 故障时提供对文件系统的只读访问，shadow 不是镜像，因此可能会比主 master 稍微滞后，通常是几秒钟
>  shadow masters 增强了对于哪些不被活跃变更的文件或不在意读取略微过时的信息的应用程序的可读性
>  实际上，由于文件内容是从 chunkserver 读取的，应用程序不会观察到过期的文件内容，在短时间内过期的可能是文件元数据，例如目录内容或访问控制信息

To keep itself informed, a shadow master reads a replica of the growing operation log and applies the same sequence of changes to its data structures exactly as the primary does. Like the primary, it polls chunkservers at startup (and infrequently thereafter) to locate chunk replicas and exchanges frequent handshake messages with them to monitor their status. It depends on the primary master only for replica location updates resulting from the primary’s decisions to create and delete replicas. 
>  shadow master 通过读取 master 的操作日志，并将其应用到自己的数据结构中，以更新自身信息
>  shadow master 和 primary master 一样，在启动时会轮询 chunkservers，以定位 chunk 副本的位置，并且也会频繁与 chunkservers 交换握手消息，以监控其状态
>  shadow master 仅依赖于 primary master 来获取由于 primary master 决定创建或删除副本而导致的副本位置更新

## 5.2 Data Integrity 
Each chunkserver uses checksumming to detect corruption of stored data. Given that a GFS cluster often has thousands of disks on hundreds of machines, it regularly experiences disk failures that cause data corruption or loss on both the read and write paths. (See Section 7 for one cause.) We can recover from corruption using other chunk replicas, but it would be impractical to detect corruption by comparing replicas across chunkservers. Moreover, divergent replicas may be legal: the semantics of GFS mutations, in particular atomic record append as discussed earlier, does not guarantee identical replicas. Therefore, each chunkserver must independently verify the integrity of its own copy by maintaining checksums. 
>  每个 chunkserver 都使用校验和来检查数据损坏
>  GFS 集群经常出现磁盘故障，导致数据损坏或丢失，通过比较不同 chunkservers 中的副本来检查数据损坏较不现实，并且即便副本存在不同，也不一定是数据损坏的原因 (GFS 变更语义、原子追加操作并不保证所有副本完全相同)
>  因此，每个 chunkserver 必须独立地通过校验和验证它自己的拷贝的完整性

A chunk is broken up into 64 KB blocks. Each has a corresponding 32 bit checksum. Like other metadata, checksums are kept in memory and stored persistently with logging, separate from user data. 
>  chunk 被拆分成 64 KB 的块，每个块都有相应的 32 bit 校验和
>  校验和存储保留在内存中，并且会随着日志持久化保留

For reads, the chunkserver verifies the checksum of data blocks that overlap the read range before returning any data to the requester, whether a client or another chunkserver. Therefore chunkservers will not propagate corruptions to other machines. If a block does not match the recorded checksum, the chunkserver returns an error to the requestor and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica. 
>  在读时，在回复之前，chunkserver 要验证覆盖了要读的范围的部分的数据块的校验和，避免传播损坏的数据
>  如果发现校验和检验不过，chunkserver 返回错误给请求者，并向 master 报告，请求者向其他 chunkserver 请求，master 从其他 chunkserver 克隆一份给该 server，并且指示该 server 删除损坏的数据

Checksumming has little effect on read performance for several reasons. Since most of our reads span at least a few blocks, we need to read and checksum only a relatively small amount of extra data for verification. GFS client code further reduces this overhead by trying to align reads at checksum block boundaries. Moreover, checksum lookups and comparison on the chunkserver are done without any I/O, and checksum calculation can often be overlapped with I/Os. 
>  计算校验和对于读性能影响不大，因为 GFS 面临的多数读操作会涉及多个块，chunkserver 只需要为额外的 (不超过一个块) 少量数据额外计算校验和
>  GFS client 会尝试尽量将读取对齐到校验和边界，这进一步降低了开销
>  此外，在 chunkserver 上的校验和查找和比较工作不需要 IO，且校验和计算可以和 IO 重叠

Checksum computation is heavily optimized for writes that append to the end of a chunk (as opposed to writes that overwrite existing data) because they are dominant in our workloads. We just incrementally update the checksum for the last partial checksum block, and compute new checksums for any brand new checksum blocks filled by the append. Even if the last partial checksum block is already corrupted and we fail to detect it now, the new checksum value will not match the stored data, and the corruption will be detected as usual when the block is next read. 
>  校验和计算针对追加写入进行了优化，我们只需增量更新最后一个 partial block 的校验和，并且为追加操作填满的块计算新的校验和

In contrast, if a write overwrites an existing range of the chunk, we must read and verify the first and last blocks of the range being overwritten, then perform the write, and finally compute and record the new checksums. If we do not verify the first and last blocks before overwriting them partially, the new checksums may hide corruption that exists in the regions not being overwritten. 

During idle periods, chunkservers can scan and verify the contents of inactive chunks. This allows us to detect corruption in chunks that are rarely read. Once the corruption is detected, the master can create a new uncorrupted replica and delete the corrupted replica. This prevents an inactive but corrupted chunk replica from fooling the master into thinking that it has enough valid replicas of a chunk. 
>  在空闲时段，ChunkServer 可以扫描并验证不活跃 Chunk 的内容。这使得我们能够检测到很少被读取的 Chunk 中的损坏情况。一旦发现损坏，主服务器可以创建一个新的未损坏的副本并删除损坏的副本。这样可以防止一个不活跃但已损坏的 Chunk 副本欺骗主服务器，使其误以为该 Chunk 已有足够的有效副本。

## 5.3 Diagnostic Tools 
Extensive and detailed diagnostic logging has helped immeasurably in problem isolation, debugging, and performance analysis, while incurring only a minimal cost. 

Without logs, it is hard to understand transient, non-repeatable interactions between machines. GFS servers generate diagnostic logs that record many significant events (such as chunkservers going up and down) and all RPC requests and replies. These diagnostic logs can be freely deleted without affecting the correctness of the system. However, we try to keep these logs around as far as space permits. 
>  如果没有日志，就很难理解机器之间短暂且不可重复的交互。
>  GFS 服务器会生成诊断日志，记录许多重要事件（例如 ChunkServer 的上线和下线）以及所有的 RPC 请求和响应。这些诊断日志可以自由删除而不影响系统的正确性。然而，只要空间允许，我们尽量保留这些日志。

The RPC logs include the exact requests and responses sent on the wire, except for the file data being read or written. By matching requests with replies and collating RPC records on different machines, we can reconstruct the entire interaction history to diagnose a problem. The logs also serve as traces for load testing and performance analysis. 
>  RPC 日志包含了在通信线路上发送的精确请求和响应，但不包括正在读取或写入的文件数据。通过将请求与回复匹配，并在不同机器上整合 RPC 记录，我们可以重建整个交互历史以诊断问题。这些日志还可用于负载测试和性能分析。

The performance impact of logging is minimal (and far outweighed by the benefits) because these logs are written sequentially and asynchronously. The most recent events are also kept in memory and available for continuous online monitoring. 
>  日志记录对性能的影响微乎其微（且远远被其带来的好处所抵消），因为这些日志是按顺序异步写入的。最近发生的事件也会保留在内存中，可供持续的在线监控使用。

# 6 Measurements
In this section we present a few micro-benchmarks to illustrate the bottlenecks inherent in the GFS architecture and implementation, and also some numbers from real clusters in use at Google. 

## 6.1 Micro-benchmarks 
We measured performance on a GFS cluster consisting of one master, two master replicas, 16 chunkservers, and 16 clients. Note that this configuration was set up for ease of testing. Typical clusters have hundreds of chunkservers and hundreds of clients. 
>  我们在一个 GFS 集群上测量了性能，该集群包含一个 master、两个 master replicas、16 个块服务器（chunkservers）和 16 个客户端。请注意，这个配置是为了便于测试而设置的。典型的集群会有数百个块服务器和数百个客户端。

All the machines are configured with dual 1.4 GHz PIII processors, 2 GB of memory, two 80 GB 5400 rpm disks, and a 100 Mbps full-duplex Ethernet connection to an HP 2524 switch. All 19 GFS server machines are connected to one switch, and all 16 client machines to the other. The two switches are connected with a 1 Gbps link. 
>  所有机器都配备了双核 1.4 GHz 的 PIII 处理器、2GB 内存、两个 80GB 转速为 5400rpm 的硬盘，以及通过 HP 2524 交换机连接的 100Mbps 全双工以太网连接。
>  所有 19 台 GFS 服务器机器连接到一个交换机，而所有 16 台客户端机器连接到另一个交换机。这两个交换机之间通过一条 1Gbps 链路相连。

### 6.1.1 Reads 
$N$ clients read simultaneously from the file system. Each client reads a randomly selected 4 MB region from a 320 GB file set. This is repeated 256 times so that each client ends up reading 1 GB of data. The chunkservers taken together have only 32 GB of memory, so we expect at most a $10\%$ hit rate in the Linux buffer cache. Our results should be close to cold cache results. 
>  有 $N$ 个客户端同时从文件系统中读取数据。每个客户端随机从一个 320 GB 的文件集合选择一个 4 MB 的区域中进行读取。这个过程重复 256 次，使得每个客户端最终读取了 1 GB 的数据。
>  所有 ChunkServer 合计只有 32 GB 的内存，因此我们预计 Linux 缓冲区缓存的命中率最多为 $10\%$。我们的结果应该接近冷缓存的结果。

![[pics/GFS-Fig3.png]]

Figure 3(a) shows the aggregate read rate for $N$ clients and its theoretical limit. The limit peaks at an aggregate of 125 MB/s when the 1 Gbps link between the two switches is saturated, or 12.5 MB/s per client when its 100 Mbps network interface gets saturated, whichever applies. 
> 图 3 (a) 展示了 $N$ 个客户端的聚合读取速率及其理论极限。当两个交换机之间的 1 Gbps 链路达到饱和时，该极限在 125 MB/s 处达到峰值；或者当每个客户端的 100 Mbps 网络接口饱和时，每客户端为 12.5 MB/s，以两者中较低者为准。

The observed read rate is $10~\mathrm{MB/s}$ , or $80\%$ of the per-client limit, when just one client is reading. The aggregate read rate reaches 94 MB/s, about $75\%$ of the 125 MB/s link limit, for 16 readers, or 6 MB/s per client. The efficiency drops from $80\%$ to $75\%$ because as the number of readers increases, so does the probability that multiple readers simultaneously read from the same chunkserver. 
> 当只有一个客户端进行读取时，观察到的读取速率为 $10~\mathrm{MB/s}$，即每客户端限制的 $80\%$。对于 16 个读者，聚合读取速率达到 94 MB/s，约为 125 MB/s 链路限制的 $75\%$，即每个客户端 6 MB/s。效率从 $80\%$ 下降到 $75\%$ 的原因是，随着读者数量的增加，多个读者同时从同一个 ChunkServer 读取数据的概率也随之增加。

### 6.1.2 Writes 
$N$ clients write simultaneously to $N$ distinct files. Each client writes 1 GB of data to a new file in a series of 1 MB writes. The aggregate write rate and its theoretical limit are shown in Figure 3(b). The limit plateaus at 67 MB/s because we need to write each byte to 3 of the 16 chunkservers, each with a 12.5 MB/s input connection. 
>  $N$ 个客户端同时向 $N$ 个不同的文件写入数据。每个客户端以一系列 1 MB 的写操作向新文件写入 1 GB 的数据。 图 3(b) 显示了聚合写入速率及其理论极限值。该极限在 67 MB/s 处趋于平稳，因为我们需要将每个字节写入到 16 个分片服务器中的 3 个，而每个分片服务器的输入连接速率为 12.5 MB/s。

The write rate for one client is 6.3 MB/s, about half of the limit. The main culprit for this is our network stack. It does not interact very well with the pipelining scheme we use for pushing data to chunk replicas. Delays in propagating data from one replica to another reduce the overall write rate. 
>  单个客户端的写入速率为 6.3 MB/s，约为极限值的一半。主要原因是我们的网络栈与用于将数据推送到分片副本的流水线方案不兼容。从一个副本传播数据到另一个副本的延迟降低了整体写入速率。

Aggregate write rate reaches 35 MB/s for 16 clients (or 2.2 MB/s per client), about half the theoretical limit. As in the case of reads, it becomes more likely that multiple clients write concurrently to the same chunkserver as the number of clients increases. Moreover, collision is more likely for 16 writers than for 16 readers because each write involves three different replicas. 
>  对于 16 个客户端，聚合写入速率达到了 35 MB/s（即每个客户端 2.2 MB/s），约为理论极限值的一半。与读取的情况类似，随着客户端数量的增加，多个客户端更有可能并发地向同一个分片服务器写入数据。
>  此外，由于每个写入涉及三个不同的副本，因此对于 16 个写入者来说，冲突的可能性比 16 个读取者更高。

Writes are slower than we would like. In practice this has not been a major problem because even though it increases the latencies as seen by individual clients, it does not significantly affect the aggregate write bandwidth delivered by the system to a large number of clients. 
>  写入速度比我们预期的要慢。但在实际应用中，这并不是一个主要问题，因为尽管它增加了单个客户端的延迟，但并未显著影响系统向大量客户端提供的聚合写入带宽。

### 6.1.3 Record Appends 
Figure 3(c) shows record append performance. $N$ clients append simultaneously to a single file. Performance is limited by the network bandwidth of the chunkservers that store the last chunk of the file, independent of the number of clients. It starts at 6.0 MB/s for one client and drops to 4.8 MB/s for 16 clients, mostly due to congestion and variances in network transfer rates seen by different clients. 
>  图 3 (c) 展示了记录追加性能。$N$ 个客户端同时向一个文件追加数据。性能受存储文件最后一个块的 chunkserver 的网络带宽限制，与客户端的数量无关。
>  对于一个客户端时性能为 6.0 MB/s，而当有 16 个客户端时下降到 4.8 MB/s，这主要是由于拥塞以及不同客户端观察到的网络传输速率的变化。

Our applications tend to produce multiple such files concurrently. In other words, $N$ clients append to $M$ shared files simultaneously where both $N$ and $M$ are in the dozens or hundreds. Therefore, the chunkserver network congestion in our experiment is not a significant issue in practice because a client can make progress on writing one file while the chunkservers for another file are busy. 
>  我们的应用程序倾向于并发生成多个这样的文件。换句话说，$N$ 个客户端同时向 $M$ 个共享文件追加数据，其中 $N$ 和 $M$ 的数量都在几十到几百之间。因此，在我们的实验中，chunkserver 的网络拥塞在实际应用中并不是一个重要问题，因为一个客户端可以在写入一个文件时，而另一个文件的 chunkserver 正忙于处理其他任务。

## 6.2 Real World Clusters 
We now examine two clusters in use within Google that are representative of several others like them. Cluster A is used regularly for research and development by over a hundred engineers. A typical task is initiated by a human user and runs up to several hours. It reads through a few MBs to a few TBs of data, transforms or analyzes the data, and writes the results back to the cluster. Cluster B is primarily used for production data processing. The tasks last much longer and continuously generate and process multi-TB data sets with only occasional human intervention. In both cases, a single “task” consists of many processes on many machines reading and writing many files simultaneously. 
>  我们现在研究谷歌内部正在使用的两个典型的集群，它们代表了许多类似的集群。集群 A 经常被一百多名工程师用于研发工作。一个典型的任务由人类用户发起，并运行数小时。它会读取几 MB 到几 TB 的数据，对数据进行转换或分析，并将结果写回集群。集群 B 主要用于生产数据处理。任务持续时间更长，并且会连续生成和处理多 TB 的数据集，只有偶尔需要人工干预。
>  在这两种情况下，一个“任务”由许多机器上的多个进程同时读写多个文件组成

### 6.2.1 Storage 
![[pics/GFS-Table2.png]]

As shown by the first five entries in the table, both clusters have hundreds of chunkservers, support many TBs of disk space, and are fairly but not completely full. “Used space” includes all chunk replicas. Virtually all files are replicated three times. Therefore, the clusters store 18 TB and 52 TB of file data respectively. 
>  如表格中前五个条目所示，这两个集群都拥有数百个 ChunkServer，支持数 TB 的磁盘空间，并且已经相当但并未完全填满。“已用空间”包括所有副本的 Chunk。几乎所有文件都是三副本存储。因此，这两个集群分别存储了 18TB 和 52TB 的文件数据。

The two clusters have similar numbers of files, though B has a larger proportion of dead files, namely files which were deleted or replaced by a new version but whose storage have not yet been reclaimed. It also has more chunks because its files tend to be larger. 
>  这两个集群的文件数量大致相同，不过 B 集群中有更高比例的“死文件”，即已被删除或被新版本替换但其存储空间尚未被回收的文件。此外，B 集群中的 Chunk 数量更多，因为其文件的平均大小较大。

### 6.2.2 Metadata 
The chunkservers in aggregate store tens of GBs of metadata, mostly the checksums for 64 KB blocks of user data. The only other metadata kept at the chunkservers is the chunk version number discussed in Section 4.5. 
>  总体而言，chunkserver 存储了数十 GB 的元数据，主要是用户数据 64KB 块的校验和。chunkserver 上唯一保存的其他元数据是第 4.5 节中提到的块版本号。

The metadata kept at the master is much smaller, only tens of MBs, or about 100 bytes per file on average. This agrees with our assumption that the size of the master’s memory does not limit the system’s capacity in practice. Most of the per-file metadata is the file names stored in a prefix-compressed form. Other metadata includes file ownership and permissions, mapping from files to chunks, and each chunk’s current version. In addition, for each chunk we store the current replica locations and a reference count for implementing copy-on-write. 
>  主服务器存储的元数据要少得多，只有几十 MB，平均每个文件大约 100 字节。这与我们的假设一致，即主服务器内存的大小实际上并不会限制系统的容量。大多数文件级别的元数据是以前缀压缩的形式存储的文件名。其他元数据包括文件的所有权和权限、文件到块的映射关系以及每个块的当前版本号。此外，对于每个块，我们还存储当前副本的位置信息以及用于写时复制的引用计数。

Each individual server, both chunkservers and the master, has only 50 to 100 MB of metadata. Therefore recovery is fast: it takes only a few seconds to read this metadata from disk before the server is able to answer queries. However, the master is somewhat hobbled for a period – typically 30 to 60 seconds – until it has fetched chunk location information from all chunkservers. 
>  每个单独的服务器（无论是数据块服务器还是主服务器）只有 50 到 100MB 的元数据。因此恢复速度很快：服务器只需从磁盘读取这些元数据几秒钟即可开始响应查询。然而，在主服务器从所有数据块服务器获取块位置信息之前，通常需要 30 到 60 秒的时间，这段时间内主服务器的功能会受到一定限制。

### 6.2.3 Read and Write Rates 

![[pics/GFS-Table3.png]]

Table 3 shows read and write rates for various time periods. Both clusters had been up for about one week when these measurements were taken. (The clusters had been restarted recently to upgrade to a new version of GFS.) 
>  表 3 显示了不同时间段的读取和写入速率。这些测量数据是在两个集群运行大约一周后采集的。（这两个集群最近已重启以升级到 GFS 的新版本。）

The average write rate was less than 30 MB/s since the restart. When we took these measurements, B was in the middle of a burst of write activity generating about 100 MB/s of data, which produced a 300 MB/s network load because writes are propagated to three replicas. 
>  自重启以来，平均写入速率低于 30 MB/s。在我们进行这些测量时，B 集群正处于一次写入活动的爆发期，生成了约 100 MB/s 的数据，由于写入操作需要传播到三个副本，这导致了 300 MB/s 的网络负载。

The read rates were much higher than the write rates. The total workload consists of more reads than writes as we have assumed. Both clusters were in the middle of heavy read activity. In particular, A had been sustaining a read rate of 580 MB/s for the preceding week. Its network configuration can support 750 MB/s, so it was using its resources efficiently. Cluster B can support peak read rates of 1300 MB/s, but its applications were using just 380 MB/s. 
>  读取速率远高于写入速率。正如我们所假设的那样，总工作负载中读取量多于写入量。两个集群都处于大量读取活动之中。特别是，A 集群在过去的一周内一直保持 580 MB/s 的读取速率。其网络配置可以支持高达 750 MB/s 的速率，因此它使用资源非常高效。B 集群的峰值读取速率可达 1300 MB/s，但其应用程序仅使用了 380 MB/s。

### 6.2.4 Master Load 
Table 3 also shows that the rate of operations sent to the master was around 200 to 500 operations per second. The master can easily keep up with this rate, and therefore is not a bottleneck for these workloads. 
>  表 3 还显示，发送到主服务器的操作速率约为每秒 200 到 500 次操作。主服务器可以轻松跟上这个速度，因此对于这些工作负载来说，它不是瓶颈。

In an earlier version of GFS, the master was occasionally a bottleneck for some workloads. It spent most of its time sequentially scanning through large directories (which contained hundreds of thousands of files) looking for particular files. We have since changed the master data structures to allow efficient binary searches through the namespace. It can now easily support many thousands of file accesses per second. If necessary, we could speed it up further by placing name lookup caches in front of the namespace data structures. 
>  在 GFS 的一个早期版本中，主服务器偶尔会成为某些工作负载的瓶颈。它大部分时间都在顺序扫描包含数十万文件的大目录，以查找特定的文件。我们已经对主服务器的数据结构进行了更改，使其能够高效地通过命名空间进行二分搜索。现在它可以轻松支持每秒数千次文件访问。如果有必要，我们可以通过在命名空间数据结构前放置名称查找缓存来进一步提高其速度。

### 6.2.5 Recovery Time 
After a chunkserver fails, some chunks will become under replicated and must be cloned to restore their replication levels. The time it takes to restore all such chunks depends on the amount of resources. In one experiment, we killed a single chunkserver in cluster B. The chunkserver had about 15,000 chunks containing 600 GB of data. To limit the impact on running applications and provide leeway for scheduling decisions, our default parameters limit this cluster to 91 concurrent clonings ( $40\%$ of the number of chunkservers) where each clone operation is allowed to consume at most 6.25 MB/s (50 Mbps). All chunks were restored in 23.2 minutes, at an effective replication rate of $440~\mathrm{MB/s}$ . 
>  在某个块服务器故障后，一些块会变成欠复制状态，并且必须进行克隆以恢复其复制级别。
>  恢复所有此类块所需的时间取决于可用资源的数量。在一项实验中，我们在集群 B 中关闭了一个块服务器。该块服务器大约有 15,000 个块，包含 600 GB 的数据。为了限制对正在运行的应用程序的影响并为调度决策提供灵活性，我们的默认参数将该集群的并发克隆操作限制为最多 91 次（占块服务器数量的 40%），并且每个克隆操作最多可以消耗 6.25 MB/s（50 Mbps）。所有块在 23.2 分钟内被恢复，有效复制速率为 440 MB/s。

In another experiment, we killed two chunkservers each with roughly 16,000 chunks and 660 GB of data. This double failure reduced 266 chunks to having a single replica. These 266 chunks were cloned at a higher priority, and were all restored to at least 2x replication within 2 minutes, thus putting the cluster in a state where it could tolerate another chunkserver failure without data loss. 
>  在另一项实验中，我们关闭了两个块服务器，每个块服务器大约有 16,000 个块和 660 GB 的数据。这两个故障导致 266 个块只剩下单副本。这 266 个块以更高的优先级进行克隆，并在 2 分钟内至少恢复到 2 倍复制，从而将集群置于一种状态，使其能够承受另一个块服务器故障而不会造成数据丢失。

## 6.3 Workload Breakdown 
In this section, we present a detailed breakdown of the workloads on two GFS clusters comparable but not identical to those in Section 6.2. Cluster X is for research and development while cluster Y is for production data processing. 
>  在本节中，我们将详细介绍两个与第 6.2 节中的集群相似但不完全相同的 GFS 集群的工作负载。集群 X 用于研发，而集群 Y 用于生产数据处理。

### 6.3.1 Methodology and Caveats 
These results include only client originated requests so that they reflect the workload generated by our applications for the file system as a whole. They do not include interserver requests to carry out client requests or internal background activities, such as forwarded writes or rebalancing. 
>  这些结果仅包括客户端发起的请求，以便反映我们的应用程序为整个文件系统生成的工作负载。它们不包括用于执行客户端请求的服务器间请求或内部后台活动，例如转发写入或再平衡操作。

Statistics on I/O operations are based on information heuristically reconstructed from actual RPC requests logged by GFS servers. For example, GFS client code may break a read into multiple RPCs to increase parallelism, from which we infer the original read. Since our access patterns are highly stylized, we expect any error to be in the noise. Explicit logging by applications might have provided slightly more accurate data, but it is logistically impossible to recompile and restart thousands of running clients to do so and cumbersome to collect the results from as many machines. 
>  I/O 操作的统计数据是基于从 GFS 服务器记录的实际 RPC 请求中启发式重建的信息。例如，GFS 客户端代码可能会将一次读取操作拆分为多个 RPC，以提高并行性，而我们则推断出原始的读取操作。由于我们的访问模式非常固定，因此我们认为任何误差都可能在噪声范围内。应用程序的显式日志记录可能会提供稍微更准确的数据，但重新编译和重启成千上万台运行中的客户端在物流上是不可能的，并且从这么多机器收集结果也十分繁琐。

One should be careful not to overly generalize from our workload. Since Google completely controls both GFS and its applications, the applications tend to be tuned for GFS, and conversely GFS is designed for these applications. Such mutual influence may also exist between general applications and file systems, but the effect is likely more pronounced in our case. 
>  需要注意的是，不要过度泛化我们的工作负载。由于谷歌完全控制着 GFS 及其应用程序，这些应用程序往往针对 GFS 进行了优化，反之亦然，GFS 也是为这些应用程序设计的。这种相互影响也可能存在于通用应用程序和文件系统之间，但在我们的案例中，这种影响可能更为显著。

### 6.3.2 Chunkserver Workload 
Table 4 shows the distribution of operations by size. Read sizes exhibit a bimodal distribution. The small reads (under 64 KB) come from seek-intensive clients that look up small pieces of data within huge files. The large reads (over 512 KB) come from long sequential reads through entire files. 
>  表 4 展示了按操作大小分布的情况。读取大小呈现双峰分布。小的读取（小于 64 KB）来自密集查找的客户端，它们在大文件中查找小数据片段。大的读取（超过 512 KB）来自整个文件的长顺序读取。

A significant number of reads return no data at all in cluster Y. Our applications, especially those in the production systems, often use files as producer-consumer queues. Producers append concurrently to a file while a consumer reads the end of file. Occasionally, no data is returned when the consumer outpaces the producers. Cluster X shows this less often because it is usually used for short-lived data analysis tasks rather than long-lived distributed applications. 
>  在集群 Y 中，相当数量的读取根本不返回任何数据。我们的应用程序，特别是生产系统中的应用程序，经常将文件用作生产者-消费者队列。生产者并发地向文件追加数据，而消费者从文件末尾读取数据。偶尔，当消费者的速度快于生产者时，不会返回任何数据。由于集群 X 通常用于短期数据分析任务而不是长期分布式应用程序，因此这种情况出现得较少。

Write sizes also exhibit a bimodal distribution. The large writes (over 256 KB) typically result from significant buffering within the writers. Writers that buffer less data, checkpoint or synchronize more often, or simply generate less data account for the smaller writes (under 64 KB). 
>  写入大小也呈现双峰分布。大的写入（超过256 KB）通常是由写入器内部的大量缓冲引起的。缓冲较少数据、更频繁地检查点或同步，或者生成较少数据的写入器则产生较小的写入（小于64 KB）。

As for record appends, cluster Y sees a much higher percentage of large record appends than cluster X does because our production systems, which use cluster Y, are more aggressively tuned for GFS. 
>  至于记录追加，集群Y看到的大记录追加比例远高于集群X，因为我们的生产系统对GFS进行了更积极的调优。

Table 5 shows the total amount of data transferred in operations of various sizes. For all kinds of operations, the larger operations (over 256 KB) generally account for most of the bytes transferred. Small reads (under 64 KB) do transfer a small but significant portion of the read data because of the random seek workload. 
>  表5显示了各种大小的操作中传输的总数据量。对于所有类型的操作，较大的操作（超过256 KB）通常占传输字节的绝大部分。由于随机查找的工作负载，小的读取（小于64 KB）确实会传输一小部分但显著的读取数据。

### 6.3.3 Appends versus Writes 
Record appends are heavily used especially in our production systems. For cluster X, the ratio of writes to record appends is 108:1 by bytes transferred and 8:1 by operation counts. For cluster Y, used by the production systems, the ratios are 3.7:1 and 2.5:1 respectively. Moreover, these ratios suggest that for both clusters record appends tend to be larger than writes. For cluster X, however, the overall usage of record append during the measured period is fairly low and so the results are likely skewed by one or two applications with particular buffer size choices. 
>  记录追加在我们的生产系统中被大量使用。对于集群 X，按传输字节数计算，写入与记录追加的比例为 108:1；按操作次数计算，比例为 8:1。对于由生产系统使用的集群 Y，相应的比例分别为 3.7:1 和 2.5:1。此外，这些比例表明，在两个集群中，记录追加的大小通常大于写入操作。然而，对于集群 X，在测量期间记录追加的整体使用量相对较低，因此结果可能受到一两个具有特定缓冲区大小选择的应用程序的影响。

As expected, our data mutation workload is dominated by appending rather than overwriting. We measured the amount of data overwritten on primary replicas. This approximates the case where a client deliberately overwrites previous written data rather than appends new data. For cluster X, overwriting accounts for under $0.0001\%$ of bytes mutated and under $0.0003\%$ of mutation operations. For cluster Y, the ratios are both $0.05\%$ . Although this is minute, it is still higher than we expected. It turns out that most of these overwrites came from client retries due to errors or timeouts. They are not part of the workload per se but a consequence of the retry mechanism. 
>  正如预期的那样，我们的数据变更工作负载主要以追加为主，而不是覆盖写入。我们测量了在主副本上被覆盖的数据量。这大致相当于客户端有意覆盖之前写入的数据，而不是追加新数据的情况。对于集群 X，覆盖写入占被变更字节的不到 0.0001%，占变更操作的不到 0.0003%。对于集群 Y，这两个比例均为 0.05%。尽管这个比例非常小，但仍然高于我们的预期。事实证明，大多数覆盖写入是由于客户端因错误或超时而重试导致的。它们并不是工作负载的一部分，而是重试机制的结果。

### 6.3.4 Master Workload 
Table 6 shows the breakdown by type of requests to the master. Most requests ask for chunk locations (FindLocation) for reads and lease holder information (FindLeaseLocker) for data mutations. 
>  表6展示了主节点请求类型的分解。大多数请求是为读取操作查询块位置（FindLocation）以及为数据变更操作获取租约持有者信息（FindLeaseLocker）。  

Clusters X and Y see significantly different numbers of Delete requests because cluster Y stores production data sets that are regularly regenerated and replaced with newer versions. Some of this difference is further hidden in the difference in Open requests because an old version of a file may be implicitly deleted by being opened for write from scratch (mode “w” in Unix open terminology). 
>  集群X和Y的Delete请求数量存在显著差异，因为集群Y存储的是定期重新生成并替换为新版本的生产数据集。这种差异部分隐藏在Open请求的数量差异中，因为在Unix打开模式下，以“w”模式打开一个文件从头开始写入可能会隐式删除旧版本的文件。  

FindMatchingFiles is a pattern matching request that supports “ls” and similar file system operations. Unlike other requests for the master, it may process a large part of the namespace and so may be expensive. Cluster Y sees it much more often because automated data processing tasks tend to examine parts of the file system to understand global application state. In contrast, cluster X’s applications are under more explicit user control and usually know the names of all needed files in advance. 
>  FindMatchingFiles是一种支持“ls”等类似文件系统操作的模式匹配请求。与其他主节点请求不同，它可能处理命名空间的大部分内容，因此可能较为昂贵。集群Y中该请求出现得更频繁，因为自动化数据处理任务倾向于检查文件系统的某些部分以了解全局应用程序状态。相比之下，集群X的应用程序受到更多用户直接控制，并且通常提前知道所有需要的文件名称。

## 7 Experiences
In the process of building and deploying GFS, we have experienced a variety of issues, some operational and some technical. 

Initially, GFS was conceived as the backend file system for our production systems. Over time, the usage evolved to include research and development tasks. It started with little support for things like permissions and quotas but now includes rudimentary forms of these. While production systems are well disciplined and controlled, users sometimes are not. More infrastructure is required to keep users from interfering with one another. 

Some of our biggest problems were disk and Linux related. Many of our disks claimed to the Linux driver that they supported a range of IDE protocol versions but in fact responded reliably only to the more recent ones. Since the protocol versions are very similar, these drives mostly worked, but occasionally the mismatches would cause the drive and the kernel to disagree about the drive’s state. This would corrupt data silently due to problems in the kernel. This problem motivated our use of checksums to detect data corruption, while concurrently we modified the kernel to handle these protocol mismatches. 

Earlier we had some problems with Linux 2.2 kernels due to the cost of fsync(). Its cost is proportional to the size of the file rather than the size of the modified portion. This was a problem for our large operation logs especially before we implemented checkpointing. We worked around this for a time by using synchronous writes and eventually migrated to Linux 2.4. 

Another Linux problem was a single reader-writer lock which any thread in an address space must hold when it pages in from disk (reader lock) or modifies the address space in an mmap() call (writer lock). We saw transient timeouts in our system under light load and looked hard for resource bottlenecks or sporadic hardware failures. Eventually, we found that this single lock blocked the primary network thread from mapping new data into memory while the disk threads were paging in previously mapped data. Since we are mainly limited by the network interface rather than by memory copy bandwidth, we worked around this by replacing mmap() with pread() at the cost of an extra copy. 

Despite occasional problems, the availability of Linux code has helped us time and again to explore and understand system behavior. When appropriate, we improve the kernel and share the changes with the open source community. 

# 8 Related Work
Like other large distributed file systems such as AFS [5], GFS provides a location independent namespace which enables data to be moved transparently for load balance or fault tolerance. Unlike AFS, GFS spreads a file’s data across storage servers in a way more akin to xFS [1] and Swift [3] in order to deliver aggregate performance and increased fault tolerance. 
>  类似 AFS，GFS 提供了位置无关的命名空间，允许数据可以透明地 (命名空间上看不出来) 被移动以实现负载均衡和容错
>  GFS 将数据分布在存储服务器上的方式更类似 xFS 和 Swift

As disks are relatively cheap and replication is simpler than more sophisticated RAID [9] approaches, GFS currently uses only replication for redundancy and so consumes more raw storage than xFS or Swift. 
>  GFS 仅使用复制以保障冗余性，不使用复杂的 RAID 方法

In contrast to systems like AFS, xFS, Frangipani [12], and Intermezzo [6], GFS does not provide any caching below the file system interface. Our target workloads have little reuse within a single application run because they either stream through a large data set or randomly seek within it and read small amounts of data each time. 
>  GFS 在文件系统接口下不提供缓存，因为目标工作负载的复用情况少，目标程序一般流式读取大规模数据，或者随机在大规模数据中搜索并读取少量数据

Some distributed file systems like Frangipani, xFS, Minnesota’s GFS[11] and GPFS [10] remove the centralized server and rely on distributed algorithms for consistency and management. We opt for the centralized approach in order to simplify the design, increase its reliability, and gain flexibility. In particular, a centralized master makes it much easier to implement sophisticated chunk placement and replication policies since the master already has most of the relevant information and controls how it changes. We address fault tolerance by keeping the master state small and fully replicated on other machines. Scalability and high availability (for reads) are currently provided by our shadow master mechanism. Updates to the master state are made persistent by appending to a write-ahead log. Therefore we could adapt a primary-copy scheme like the one in Harp [7] to provide high availability with stronger consistency guarantees than our current scheme. 
>  一些分布式文件系统移除了中心服务器，依赖分布式算法实现一致性和管理
>  GFS 为了简化设计，提高灵活性，选择了 master-slave 方案，这使得实现 chunk placement 和 replication policies 更加简单，因为 master 知道全部的相关信息，并且具有控制权
>  我们通过保持 master 状态小，并且在其他机器上具有拷贝，以确保容错
>  对于读操作的可拓展性和高可用性通过 shadow master 机制实现 (多个 master 服务读操作)

We are addressing a problem similar to Lustre [8] in terms of delivering aggregate performance to a large number of clients. However, we have simplified the problem significantly by focusing on the needs of our applications rather than building a POSIX-compliant file system. Additionally, GFS assumes large number of unreliable components and so fault tolerance is central to our design. 
>  GFS 假设了集群是由大量不可靠成分组成的，故其设计核心是容错

GFS most closely resembles the NASD architecture [4]. While the NASD architecture is based on network-attached disk drives, GFS uses commodity machines as chunkservers, as done in the NASD prototype. Unlike the NASD work, our chunkservers use lazily allocated fixed-size chunks rather than variable-length objects. Additionally, GFS implements features such as rebalancing, replication, and recovery that are required in a production environment. 
>  GFS 的 chunkservers 使用延迟分配的固定大小 chunks，而不是变长的对象
>  GFS 针对生成环境实现了负载均衡、复制、恢复等机制

Unlike Minnesota’s GFS and NASD, we do not seek to alter the model of the storage device. We focus on addressing day-to-day data processing needs for complicated distributed systems with existing commodity components. 

The producer-consumer queues enabled by atomic record appends address a similar problem as the distributed queues in River [2]. While River uses memory-based queues distributed across machines and careful data flow control, GFS uses a persistent file that can be appended to concurrently by many producers. The River model supports m-to-n distributed queues but lacks the fault tolerance that comes with persistent storage, while GFS only supports m-to-1 queues efficiently. Multiple consumers can read the same file, but they must coordinate to partition the incoming load. 
>  River 使用基于内存的分布式生产者-消费者队列，GFS 使用持久文件，具有更高容错，但 GFS 仅高效支持 m-to-1 队列，即多个生产者，单个消费者 (被追加的文件)
>  GFS 中，多个消费者可以读取相同文件，但需要互相协调以划分 incoming load

# 9 Conclusions
The Google File System demonstrates the qualities essential for supporting large-scale data processing workloads on commodity hardware. While some design decisions are specific to our unique setting, many may apply to data processing tasks of a similar magnitude and cost consciousness. 
>  GFS 支持了在商用硬件 (不太行的硬件) 上处理大规模工作负载

We started by reexamining traditional file system assumptions in light of our current and anticipated application workloads and technological environment. Our observations have led to radically different points in the design space. We treat component failures as the norm rather than the exception, optimize for huge files that are mostly appended to (perhaps concurrently) and then read (usually sequentially), and both extend and relax the standard file system interface to improve the overall system. 
>  在设计时，我们认为组件故障是常规情况，而不是偶尔的异常，我们根据工作负载特性，针对对大型文件的并发追加操作和顺序读取进行优化

Our system provides fault tolerance by constant monitoring, replicating crucial data, and fast and automatic recovery. Chunk replication allows us to tolerate chunkserver failures. The frequency of these failures motivated a novel online repair mechanism that regularly and transparently repairs the damage and compensates for lost replicas as soon as possible. Additionally, we use checksumming to detect data corruption at the disk or IDE subsystem level, which becomes all too common given the number of disks in the system. 
>  GFS 通过常规的监控、复制关键数据、快速和原子化的恢复提供容错
>  chunk 拷贝用于容忍 chunkserver 故障，因为 chunkserver 故障频繁，GFS 采用在线的修复机制，定期地且透明地 (不可见地) 修复 replica 的丢失
>  GFS 使用校验和检查磁盘的数据损坏 (数据损坏也是常见的情况)

Our design delivers high aggregate throughput to many concurrent readers and writers performing a variety of tasks. We achieve this by separating file system control, which passes through the master, from data transfer, which passes directly between chunkservers and clients. Master involvement in common operations is minimized by a large chunk size and by chunk leases, which delegates authority to primary replicas in data mutations. This makes possible a simple, centralized master that does not become a bottleneck. We believe that improvements in our networking stack will lift the current limitation on the write throughput seen by an individual client. 
>  GFS 对于多个并发读者和多个执行不同任务的写者具有高的聚合吞吐
>  这是通过分离文件系统控制和数据传输实现的，控制经过 master，数据传输直接在 chunkserver 和 client 之间进行，通过大的 chunk size 和 chunk 租约机制，使得 master 的参与最小化，其中 chunk 租约机制将数据变更的权限委托给 chunk 的 primary 副本
>  这使得中心化的 master 不会成为瓶颈

GFS has successfully met our storage needs and is widely used within Google as the storage platform for research and development as well as production data processing. It is an important tool that enables us to continue to innovate and attack problems on the scale of the entire web. 
