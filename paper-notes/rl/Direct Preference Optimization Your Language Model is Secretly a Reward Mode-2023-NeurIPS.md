# Abstract
While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. 
>  LLM 的预训练是完全无监督的，故精细地控制它们的行为很难

Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). 
>  现存的方法: RLHF 收集人类对模型回应的打出的质量标签，然后用这些标签对模型进行微调，以对其人类偏好

However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. 
>  RLHF 先拟合一个反映人类偏好的奖励模型，然后使用 RL 最大化该奖励模型给出的奖励来微调 LLM，这样的过程复杂且不稳定

In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. 
>  本文提出另一种对 RLHF 中的奖励模型的参数化形式，在该形式下，最优策略具有闭式解，可以通过简单的分类损失求解标准 RLHF 问题

The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning.
>  整个算法称为直接偏好优化，该算法下，对 LLM 进行 RLHF 微调时，不需要执行过多的超参数调节，**不需要对 LLM 进行采样**

Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.
>  使用 DPO 微调 LLM 的在控制生成的情感方面效果超过 PPO-based RLHF，在总结指令和单论对话方面的能力也匹配 PPO-based RLHF

# 1 Introduction
Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities [11, 7, 42, 8]. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to understand common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. 
>  LLM 在大量语料上预训练，为了强化或者让 LLM 偏置到特定的能力，需要对模型进行微调

Similarly, we might want our language model to be aware of a common misconception believed by $50\%$ of people, but we certainly do not want the model to claim this misconception to be true in $50\%$ of queries about it! In other words, selecting the model's desired responses and behavior from its very wide knowledge and abilities is crucial to building AI systems that are safe, performant, and controllable [28]. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL),

While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.
>  现存方法用 RL 让 LLM 对其人类偏好
>  我们认为现存方法使用的基于 RL 的目标可以通过简单的二元交叉熵损失**直接优化**

At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [12, 2]). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.
>  现存的方法使用人类偏好数据集对齐 LLM，LLM 对人类的偏好学习阶段在预训练之后
>  最直接的偏好学习方法是有监督微调，但最成功的一类方法是 RLHF
>  RLHF 先用偏好数据集拟合一个奖励模型，然后用 RL 优化 LLM 策略，以生成获取最高奖励的回应，RLHF 确保 LLM 在微调时不会过度偏移原模型
>  RLHF 的流程比 SFT 复杂，它包括训练多个语言模型、从 LLM 从采样，需要很多计算开销

In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. 
>  我们将在偏好学习阶段直接优化 LLM，不需要隐式的奖励建模或强化学习
>  直接偏好优化算法优化和现存 RLHF 算法相同的目标: 带有 KL 散度约束的奖励最大化目标，但训练和实现都更简单

Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. 
>  DPO 的更新步会增加偏好的回应和不被偏好的回应的相对对数概率，但 DPO 为每个样本引入了动态权重，避免了在使用简单的概率比目标时的模型退化问题

Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [5]) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data.
>  DPO 依赖于一个理论上的偏好模型来度量给定的奖励函数和经验上偏好的数据的对齐程度
>  但现存的方法的流程是: 使用偏好模型定义偏好损失来训练奖励模型，使用奖励模型训练策略；DPO 则通过变量变化，直接将偏好损失定义为关于策略的函数，进而直接优化偏好损失即可得到策略
>  给定一个人类对模型回应的偏好数据集，DPO 可以直接使用简单的二元交叉熵目标优化策略，优化得到的策略是相对于偏好数据的一个隐式奖励函数最优的

> [!info] Bradley-Terry model
>  在 RLHF 中，B-T 模型用于将人类标注的偏好比较转化为可学习的损失函数
>  RLHF 中的奖励模型接收输入 $(\mathbf x, \mathbf y)$，给出标量分数 $r_\theta(\mathbf x, \mathbf y)$，表示回应 $\mathbf y$ 相对于 $\mathbf x$ 的质量
> 
>  B-T 模型假设人类偏好回应 $\mathbf y_i$ 而非 $\mathbf y_j$ 的概率与它们各自的奖励分数 (或称为 “实力” 的指数函数之比有关)
> 
>   $$ P(\mathbf y_i\text{ is preferred over }\mathbf y_j) = \frac{\exp(r_\theta(\mathbf x, \mathbf y_i))}{\exp(r_\theta(\mathbf x, \mathbf y_i) + \exp(r_\theta(\mathbf x, \mathbf y_j))} $$
> 
>  B-T 模型的核心思想就是每一个被比较的实体都有一个正值的实力参数 $\pi_i$，实力越大，这个实体就越受偏好，当两个实体进行比较时，实体 $i$ 战胜实体 $j$ 的概率就等于 $P(i\text{ beats }j ) = \frac {\pi_i}{\pi_j + \pi_i}$
> 
> 在训练奖励模型时，如果人类标注员偏好 $\mathbf y_i$ 而非 $\mathbf y_j$，奖励模型就要最大化 $P(\mathbf y_i \text{ is preferrred over } \mathbf y_j)$，因此关于参数 $\theta$ 的损失函数就定义为 
> 
> $$\mathbb E_{\mathcal D}[-\log P(\mathbf y_i\text{ is preferred over }\mathbf y_j)]$$
> 
> 其中 $\mathbb E_{\mathcal D}$ 表示对整个偏好数据集求期望

Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.
>  DPO 是从偏好数据训练 LLM 的 RL-free 算法

![](https://cdn-mineru.openxlab.org.cn/extract/57987395-f3ea-45d4-83af-37e1c411b1ba/61a9a84e4a285ff2779460bdde71fdc423ea46c65c2c587e783d6106be067871.jpg) 

Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.

# 2 Related Work
Self-supervised language models of increasing scale learn to complete some tasks zero-shot [33] or with few-shot prompts [6, 27, 11]. However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions [25, 38, 13, 41]. This 'instruction-tuning' procedure enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability [13]. 
>  在具有指令 + 人类写的回应的数据集上进行 instruction-tuning 可以提高 LLM 对下游任务的能力

Despite the success of instruction tuning, relative human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation [20], summarization [40, 51], story-telling [51], and instruction-following [28, 34]. These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model [5], then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE [47], proximal policy optimization (PPO; [39]), or variants [34].
>  instruction-tuning 需要人类写回应，直接判断人类对模型回应偏好会更简单
>  故风向又转向了用人类偏好数据集做微调: 先优化一个匹配偏好数据集的奖励模型 (奖励对偏好数据集的 “匹配程度” 由 B-T 模型给出)，然后用 RL 算法训练 LLM

A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness [2], using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives [55, 29, 48] and another body of work on general methods for learning from human preferences [12, 21]. 

Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.

Outside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; [50, 14]). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a von Neumann winner, a policy whose expected win rate against any other policy is at least $50\%$ [14]. However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs [49]. Similarly, preference-based RL (PbRL) learns from binary preferences generated by an unknown 'scoring' function rather than rewards [9, 37]. Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it [16, 9, 12, 36, 21]. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.
>  从偏好中学习策略已经在 RL 中被研究

# 3 Preliminaries
We review the RLHF pipeline in Ziegler et al. (and later [40, 1, 28]). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.
>  RLHF 过程通常包含三个阶段:
>  1. SFT
>  2. 偏好采样和奖励学习
>  3. RL 优化

**SFT:** RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model $\pi^{\mathrm{SFT}}$
>  SFT: 先用下流任务相关数据 SFT 模型

**Reward Modelling Phase:** In the second phase the SFT model is prompted with prompts $x$ to produce pairs of answers $(y_{1},y_{2})\sim \pi^{\mathrm{SFT}}(y\mid x)$ . These are then presented to human labelers who express preferences for one answer, denoted as $y_{w}\succ y_{l}\mid x$ where $y_{w}$ and $y_{l}$ denotes the preferred and dispreferred completion amongst $(y_{1},y_{2})$ respectively. 
>  Reward Modelling Phase:
>  给模型输入 prompt，得到多个模型输出，让人类标记员标记对输出的偏好

The preferences are assumed to be generated by some latent reward model $r^*\left(y,x\right)$ , which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular choice (although more general Plackett-Luce ranking models [32, 23] are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution $p^*$ can be written as:

$$
p^* (y_1\succ y_2\mid x) = \frac{\exp(r^*(x,y_1))}{\exp(r^*(x,y_1)) + \exp(r^*(x,y_2))}. \tag{1}
$$

Assuming access to a static dataset of comparisons $\mathcal{D} = \{x^{(i)},y_w^{(i)},y_l^{(i)}\}_{i = 1}^N$ sampled from $p^*$ , we can parametrize a reward model $r_\phi (x,y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:

$$
\mathcal{L}_R(r_\phi ,\mathcal{D}) = -\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}\big[\log \sigma (r_\phi (x,y_w) -r_\phi (x,y_l))\big] \tag{2}
$$

where $\sigma$ is the logistic function. 
>  假设人类标记员表示的偏好是和某个隐式的奖励模型 $r^*(y, x)$ 匹配的，通过 B-T 方法将人类偏好标记和奖励模型联系起来
>  在偏好数据集上通过极大似然训练奖励模型的参数，训练的负对数损失形式如 Eq 2，其中 $\sigma$ 表示为 logistic/sigmoid 函数，$\sigma(x) = \frac {1}{1 + e^{-x}}$

In the context of LMs, the network $r_\phi (x,y)$ is often initialized from the SFT model $\pi^{\mathrm{SFT}}(y\mid x)$ with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value [51]. To ensure a reward function with lower variance, prior works normalize the rewards, such that $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi (x,y)\right] = 0$ for all $x$
>  通常奖励模型会直接初始化为 SFT 过后的模型，再加上一层线性层，以生成标量奖励
>  一些工作为了确保奖励函数的方差低，对奖励函数施加了规范化约束，记作 $\mathbb E_{x, y\sim \mathcal D}[r_\phi(x, y)] = 0 \text{ for all } x$

**RL Fine-Tuning Phase:** During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works [17, 18], the optimization is formulated as

$$
\max_{\pi_{\theta}}\mathbb{E}_{x\sim \mathcal{D},y\sim \pi_{\theta}(y|x)}\big[r_{\phi}(x,y)\big] -\beta \mathbb{D}_{\mathrm{KL}}\big[\pi_{\theta}(y\mid x)\mid \pi_{\mathrm{ref}}(y\mid x)\big], \tag{3}
$$

where $\beta$ is a parameter controlling the deviation from the base reference policy $\pi_{\mathrm{ref}}$ , namely the initial SFT model $\pi^{\mathrm{SFT}}$ . 
>  RL Fine-Tuning Phase:
>  使用学习到的奖励函数寻找最优策略，同时施加不让模型分布偏移太远的 KL 散度正则化项

In practice, the language model policy $\pi_{\theta}$ is also initialized to $\pi^{\mathrm{SFT}}$ . The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. 
>  正则化项非常关键，它保持了生成的多样性，避免模式崩溃到单个高奖励答案

Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach [51, 40, 1, 28] has been to construct the reward function $r(x,y) = \pi_{\phi}(x,y) -\beta (\log \pi_{\theta}(y|x) -\log \pi_{\mathrm{ref}}(y|x))$ , and maximize using PPO [39].

# 4 Direct Preference Optimization
Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop. 
>  我们期望直接用偏好数据优化策略，而不是先学习奖励函数再通过 RL 优化
>  我们以特定的形式参数化奖励模型，使得其最优策略存在封闭形式，不需要 RL 就可以得到

As we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies. This change-of-variables approach avoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the (implicit) reward.
>  我们的关键思路是利用奖励函数到最优策略的解析形式映射，将定义于奖励函数上的损失函数转换到定义于策略上的损失函数
>  这种变量变换方法避免了拟合一个单独的、显式的奖励模型，且仍然可以在现有的人类偏好模型 (例如 B-T 模型) 下进行优化
>  本质上，在我们的思路下，策略网络同时表示语言模型和 (隐式) 奖励模型

**Deriving the DPO objective.** We start with the same RL objective as prior work, Eq. 3, under a general reward function $r$ . Following prior work [31, 30, 19, 15], it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq. 3 takes the form:

$$
\pi_r(y\mid x) = \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp \left(\frac{1}{\beta} r(x,y)\right), \tag{4}
$$

where $\begin{array}{r}Z(x) = \sum_{y}\pi_{\mathrm{ref}}(y\mid x)\exp \left(\frac{1}{\beta} r(x,y)\right) \end{array}$ is the partition function. See Appendix A.1 for a complete derivation. 

>  Eq 3 的目标函数下，最优策略 $\pi$ 的封闭形式如上

Even if we use the MLE estimate $r_{\phi}$ of the ground-truth reward function $r^*$ , it is still expensive to estimate the partition function $Z(x)$ [19, 15], which makes this representation hard to utilize in practice. 
>  计算策略的闭式解的难点在于配分函数的计算，对于所有可能的 $y$ 求解是不现实的

However, we can rearrange Eq. 4 to express the reward function in terms of its corresponding optimal policy $\pi_{T}$ , the reference policy $\pi_{\mathrm{ref}}$ , and the unknown partition function $Z(\cdot)$ . Specifically, we first take the logarithm of both sides of Eq. 4 and then with some algebra we obtain:

$$
r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)} +\beta \log Z(x). \tag{5}
$$

>  对 Eq 4 的简单变化可以让我们得到对奖励函数 $r(x, y)$ 的另一种表现形式

We can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\pi^*$ . Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., $p^* (y_1 \succ y_2 \mid x) = \sigma (r^* (x,y_1) -r^* (x,y_2))$ . Substituting the reparameterization in Eq. 5 for $r^* (x,y)$ into the preference model Eq. 1, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\pi^*$ and reference policy $\pi_{\mathrm{ref}}$ . Thus, the optimal RLHF policy $\pi^*$ under the Bradley-Terry model satisfies the preference model:

$$
p^* (y_1\succ y_2\mid x) = \frac{1}{1 + \exp\left(\beta\log\frac{\pi^*(y_2|x)}{\pi_{\mathrm{ref}}(y_2|x)} -\beta\log\frac{\pi^*(y_1|x)}{\pi_{\mathrm{ref}}(y_1|x)}\right)} \tag{6}
$$

The derivation is in Appendix A.2. 

>  我们用 Eq 5 的形式代入 B-T 模型，就得到了偏好概率相对于最优策略和参考策略的形式，即 Eq 6

While Eq. 6 uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models [32, 23], shown in Appendix A.3.

Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\pi_{\theta}$ . Analogous to the reward modeling approach (i.e. Eq. 2), our policy objective becomes:

$$
\begin{align}
\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) &=-\mathbb E_{(x,y_w,y_l)\sim \mathcal D}\left[\log P(y_w \succ y_l\mid x)\right]\\
&=-\mathbb E_{(x,y_w,y_l)\sim \mathcal D}\left[\log \frac {1}{1 + \exp\left(\beta \log\frac {\pi^*(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)}-\beta \log\frac {\pi^*(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)}\right)}\right]\\
&=-\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)} -\beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}\right)\right]. \tag{7}
\end{align}
$$

>  使用最优策略、优化目标策略表示偏好概率后，进而可以使用它们表示优化目标 (优化目标就是最大化偏好概率，或者说最小化其负对数似然)
>  故 DPO 的优化目标 $\mathcal L(\pi_\theta;\pi_{\text{ref}})$ 形式如上所示，它直接和目标策略 $\pi_\theta$ 相关

This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\pi_{\theta}$ . Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution [4]. In Section 5, we further discuss theoretical properties of DPO in relation to other works.
>  虽然没有直接拟合奖励模型，但我们实际上隐式地拟合了一个奖励模型，在定义上，这个奖励模型的最优策略就是我们的策略 $\pi_\theta$
>  虽然做了一些重参数化，但我们实际上还是在拟合 B-T 模型基于数据集定义的偏好概率，进而继承了一些理论保证，例如在对偏好数据分布的一定假设下，符合一致性
>  (在统计学上，一致估计器是指能够随着统计数据量的增加收敛到被估计值的真实值的估计器)

What does the DPO update do? For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\mathcal{L}_{\mathrm{DPO}}$ . The gradient with respect to the parameters $\theta$ can be written as:

$$
\begin{align}
&\nabla_\theta \mathcal L_{\text{DPO}}(\pi_\theta, \pi_{\text{ref}})\\
=&\nabla_\theta - \mathbb E_{(x,y_w, y_l)\sim \mathcal D}\left[\log \sigma \left(\hat r_\theta(x,y_w)  - \hat r_\theta(x,y_l) \right)\right]\\
=& - \mathbb E_{(x,y_w, y_l)\sim \mathcal D}\left[\nabla_\theta\log \sigma \left(\hat r_\theta(x,y_w)  - \hat r_\theta(x,y_l) \right)\right]\\
=& - \mathbb E_{(x,y_w, y_l)\sim \mathcal D}\left[ \frac 1{\sigma(\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l))}\nabla_\theta\sigma \left(\hat r_\theta(x,y_w)  - \hat r_\theta(x,y_l) \right)\right]\\
=& - \mathbb E_{(x,y_w, y_l)\sim \mathcal D}\left[(1-\sigma(\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l))\left[\nabla_\theta \hat r_\theta(x,y_w) - \nabla_\theta \hat r_\theta(x,y_l)\right] \right]\\
=& - \mathbb E_{(x,y_w, y_l)\sim \mathcal D}\left[\sigma(\hat r_\theta(x,y_l) - \hat r_\theta(x,y_w)\left[\nabla_\theta \beta\log \pi(y_w\mid x) - \nabla_\theta \beta\log \pi(y_l\mid x)\right] \right]\\
=&{-\beta \mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}\bigg[\underbrace{\sigma(\hat{r}_{\theta}(x,y_l) -\hat{r}_{\theta}(x,y_w))}_{\mathrm{higher~weight~when~reward~estimate~is~wrong}}\bigg[\underbrace{\nabla_{\theta}\log\pi(y_w\mid x)}_{\mathrm{increase~likelihood~of~}y_w} -\underbrace{\nabla_{\theta}\log\pi(y_l\mid x)}_{\mathrm{decrease~likelihood~of~}y_l}\bigg]\bigg],} 
\end{align}
$$

where $\begin{array}{r}\hat{r}_{\theta}(x,y) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{ref}}(y|x)} \end{array}$ is the reward implicitly defined by the language model $\pi_{\theta}$ and reference model $\pi_{\mathrm{ref}}$ (more in Section 5). 

>  DPO 损失相对于参数 $\theta$ 的梯度的形式如上
>  其中 $\hat r_\theta = \beta \log \frac {\pi_\theta(y\mid x)}{\pi_{\text{ref}}(y\mid x)}$ 是目标策略 $\pi_\theta$ 和参考模型 $\pi_{\text{ref}}$ 隐式定义的奖励函数 (参照 Eq 5 ，实际上还是差了一个关于配分函数 $Z(x)$ 的常数)

Intuitively, the gradient of the loss function $\mathcal{L}_{\mathrm{DPO}}$ increases the likelihood of the preferred completions $y_{w}$ and decreases the likelihood of dispreferred completions $y_{l}$ . Importantly, the examples are weighed by how much higher the implicit reward model $\hat{r}_{\theta}$ rates the dispreferred completions, scaled by $\beta$ , i.e., how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. 
>  直观上看，$\mathcal L_{DPO}$ 的梯度会让参数朝着增加 $y_w$ 的对数似然、减少 $y_l$ 的对数似然的方向前进，同时每个样本对 $(y_w, y_l)$ 对梯度的贡献权重由隐式的奖励模型 $\hat r_\theta$ 对 $y_l$ 相对于 $y_w$ 的偏好程度 (乘上 $\beta$) 决定
>  乘上 $\beta$ 可以这样直观理解: $\beta$ 越大，KL 约束越强，说明模型的惰性越大，也就是 $\beta$ 控制了单位奖励可以引起的策略偏移
>  从系数 $\sigma(\hat r_\theta(x, y_l) - \hat r_\theta(x, y_w))$ 来看，如果 $\sigma$ 内的值是负数 (表明当前模型已经更偏好 $y_w$，这是我们想看到的)，则 $\beta$ 会放大一个负值，进而导致权重 $\sigma(...)$ 更小，让更新更保守，而对于哪些错误的排序，$\beta$ 才是放大权重的作用
>  因此，更大的 $\beta$ 只有在奖励信号和期望情况有显著差异时才强化更新，在没有那么大差异的情况下，会削弱更新的幅度，让模型更加保守，更倾向于保持和参考策略的接近

Our experiments suggest the importance of this weighting, as a naive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table 3).

**DPO outline.** The general DPO pipeline is as follows: 1) Sample completions $y_{1},y_{2}\sim \pi_{\mathrm{ref}}(\cdot \mid x)$ for every prompt $x$ , label with human preferences to construct the offline dataset of preferences $\mathcal{D} = \{x^{(i)},y_w^{(i)},y_l^{(i)}\} _i^{N}$ and 2) optimize the language model $\pi_{\theta}$ to minimize $\mathcal{L}_{\mathrm{DPO}}$ for the given $\pi_{\mathrm{ref}}$ and $\mathcal{D}$ and desired $\beta$ . 
>  DPO 的一般流程为:
>  1. 为 prompt $x$ 采样回应 $y_1, y_2 \sim \pi_{ref}(\cdot\mid x)$，让人类标记员标记偏好，如此构建偏好数据集 $\mathcal D = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_i^N$
>  2. 优化 $\pi_\theta$，优化目标即最小化 $\mathcal L_{DPO}$

In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\pi^{\mathrm{SFT}}$ , we initialize $\pi_{\mathrm{ref}} = \pi^{\mathrm{SFT}}$ whenever available. However, when $\pi^{\mathrm{SFT}}$ is not available, we initialize $\pi_{\mathrm{ref}}$ by maximizing likelihood of preferred completions $(x,y_w)$ , that is, $\pi_{\mathrm{ref}} = \arg \max_{\pi}\mathbb{E}_{x,y_{w}\sim \mathcal{D}}[\log \pi (y_{w}\mid x)]$ . This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\pi_{\mathrm{ref}}$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix B.
>  实践中，如果有 SFT 模型，就将 $\pi_{ref}$ 初始化为 SFT 模型
>  如果没有 SFT 模型，就直接在偏好数据集 $(x, y_w)$ 上微调基础模型，将得到的模型作为 $\pi_{ref}$ 
>  (Q: 话说这样直接微调是不是效果应该也 OK，不用费力从 RL 的角度在偏好数据集上优化？
>  A: 或许这样直接优化没办法利用到 $y_w, y_l$ 之间的相对信息)

# 5 Theoretical Analysis of DPO
In this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO [39]).

## 5.1 Your Language Model Is Secretly a Reward Model
DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. 5 is equivalent to a Bradley-Terry model with a reward parameterization $r^* (x,y) = \beta \log \frac{\pi_\theta^* (y|x)}{\pi_{\mathrm{ref}}(y|x)}$ and we optimize our parametric model $\pi_{\theta}$ , equivalently to the reward model optimization in Eq. 2 under the change of variables. 
>  前文中，我们已经证明了 DPO 的优化等价于在 Eq 2 中优化一个参数化形式为 $r^*(x, y) = \beta \log \frac {\pi_\theta^*(y\mid x)}{\pi_{\text{ref}}(y\mid x)}$ 的奖励函数

In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions.
>  本节将证明这种参数化形式不会约束学习到的奖励模型，并且可以精确恢复到最优策略

**Definition 1.** We say that two reward functions $r(x,y)$ and $r'(x,y)$ are equivalent iff $r(x,y) -r'(x,y) = f(x)$ for some function $f$ .
>  定义: 两个奖励函数 $r(x, y)$，$r'(x, y)$ 当且仅当 $r(x, y) - r'(x, y) = f(x)$ 时，它们等价

>  如果两个奖励函数对于 $x, y$ 给出的奖励之差和 $y$ 无关，只和 $x$ 有关，说明对于相同的 $x$，不同的 $y_1, y_2$，在两个奖励函数下的相对差异: $r(x, y_1) - r(x, y_2)$, $r'(x, y_1) - r'(x, y_2)$ 相等的，故我们称它们 “等价” 是恰当的
>  证明: $r(x, y_1) - r(x, y_2) = r'(x, y_1) + f(x) - r'(x, y_2) - f(x) = r'(x, y_1)- r'(x, y_2)$

It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. 
>  这个我们定义的 “等价关系” 就是 equivalent relation，它将所有的奖励函数划分为了不同的等价类

We can state the following two lemmas:

**Lemma 1.** Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.
>  引理 1
>  在 Plackett-Luce, 尤其是 Bradley-Terry 偏好框架下，来自相同等价类的奖励函数将引出相同的偏好分布

**Lemma 2.** Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.
>  引理 2
>  两个来自相同等价类的奖励函数将引出约束 RL 问题下的相同最优策略

The proofs are straightforward and we defer them to Appendix A.5. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models [32]. 
>  第一个引理实际上就是 P-L 模型的欠规范化性质
>  如果一个统计模型的不同参数组合可以产生完全相同的观测数据分布，我们就说这个模型是欠规范化的，或者说参数是不可识别的
>  第一个引理就说明了不同的奖励函数可以导出 P-L 模型下相同的偏好分布，故我们无法确定不同项目 (回应) 的绝对价值，只能确定它们之间的相对比例

Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. 2 [4]. 
>  因为这种欠规范形式，我们通常需要施加额外的可识别性约束，以让根据 Eq 2 获得的 MLE 估计达到一定保证

>  可识别性约束即能够让只有唯一一组参数能够生成观测数据的约束，对于 P-T 模型，常见的约束是:
>  - 将其中一个项目的强度设为 1
>  - 或者将所有项目的强度之和设为 1
>  - 或者将所有参数的对数之和设为 1
>  如果没有约束，MLE 可能得到无限多组解，施加约束后，我们可以确保得到唯一解

The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class.
>  而引理二说明了来自相同等价类的奖励函数最终都导出相同的最优策略，因为我们实际只关心策略，故我们只要获取任意一个最优策略对应的奖励函数等价类中的奖励函数即可

We prove the following Theorem in Appendix A.6:

**Theorem 1.** Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization $r(x,y) = \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)}$ for some model $\pi (y\mid x)$ and a given reference model $\pi_{ref}(y\mid x)$ .
>  Theorem 1
>  在 $\pi_{\text{ref}}(y\mid x) > 0 \text{ for all } x, y$ 和 $\beta > 0$ 的假设下，任意的奖励函数等价类都可以用参数化形式 $r(x, y) = \beta \log \frac {\pi(y\mid x)}{\pi_{\text{ref}}(y\mid x)}$ 表示，其中 $\pi(y\mid x)$ 是奖励函数等价类对应的 KL 约束 RL 问题下的最优策略

**Proof Sketch.** Consider any reward function $r(x,y)$ , which induces a corresponding optimal model $\pi_r(y\mid x)$ , specified by Eq. 4. We will show that a reward function from the equivalence class of $r$ can be represented using the reparameterization given above. We define the projection $f$ as

$$
f(r;\pi_{\mathrm{ref}},\beta)(x,y) = r(x,y) -\beta \log \sum_y\pi_{\mathrm{ref}}(y\mid x)\exp \left(\frac{1}{\beta} r(x,y)\right) \tag{8}
$$

The operator $f$ simply normalizes the reward function with the logarithm of the partition function of $\pi_r$ . Since the added normalization term is only a function of the prefix $x$ , $f(r;\pi_{\mathrm{ref}},\beta)(x,y)$ is a reward function in the equivalence class of $r(x,y)$ . Finally, replacing $r$ with the RHS of Eq. 5 (which holds for any reward function), we have $f(r;\pi_{\mathrm{ref}},\beta)(x,y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\mathrm{ref}}(y|x)}$ . That is, the projection $f$ produces a member of the equivalence class of $r$ with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.

We can alternatively view Theorem 1 as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:

$$
\sum_{y}\underbrace{\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)}_{= \pi (y|x),\mathrm{using~Thm.~1~reparam.}} = 1, \tag{9}
$$

i.e., $\pi (y\mid x)$ is a valid distribution (probabilities are positive and sum to 1). However, following Eq. 4, we can see that Eq. 9 is the partition function of the optimal policy induced by the reward function $r(x,y)$ . 

>  Theorem 1 为每个奖励函数等价类选择出的最简形式的奖励函数实际上就是满足了 Eq 9 的约束 (即 $Z(x) = 1$) 的奖励函数
>  也就是使得 $\pi(y\mid x) = \pi_{\text{ref}}(y\mid x) \exp\left(\frac 1 \beta r(x, y)\right)$ 不需要规范化，本身就是一个合法的分布的奖励函数

The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. 4 analytically tractable for all prompts $x$ .
>  DPO 的关键思想是对奖励函数的参数化形式施加了约束 (满足目前为止一直讨论的重参数化形式)
>  尽管施加了约束，但并没有限制表达能力，任何通用的 RL 算法 (给定奖励函数) 可以学习到的最优策略 (在带和参考策略的 KL 约束的 RL 优化目标下) 仍然可以在 DPO 框架内表示

>  DPO 的关键思想是根据目标函数的形式，解析式写出了奖励函数和最优策略的关系，那么面对奖励函数的优化就可以直接转移到面对最优策略的优化

## 5.2 Instability of Actor-Critic Algorithms
We can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. 
>  DPO 的框架还可以用于诊断用于 RLHF 中的标准 actor-critic 算法 (例如 PPO) 中的不稳定性

We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section 3. We can draw connections to the control as inference framework [22] for the constrained RL problem outlined in 3.  We assume a parameterized model $\pi_{\theta}(y\mid x)$ and minimize $\mathbb{D}_{\mathrm{KL}}[\pi_{\theta}(y|x)\mid \pi^{*}(y\mid x)]$ where $\pi^{*}$ is the optimal policy from Eq. 7 induced by the reward function $r_{\phi}(y,x)$ . 
>  我们考虑标准 RLHF 流程中的 RL 微调步骤
>  在该步骤中，按照标准的 RLHF 流程，我们已经训练好了奖励函数，假设我们有参数化的模型 $\pi_\theta(y\mid x)$，则该步骤的目标是最小化 $\mathbb D_{\mathrm{KL}}[\pi_\theta(y\mid x) \mid \pi^*(y\mid x)]$，其中 $\pi^*$ 表示奖励函数 $r_\phi(y, x)$ 下的最优策略

With some algebra this leads to the optimization objective:

$$
\max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y|x)}\left[\underbrace{r_{\phi}(x,y) -\beta\log\sum_{y}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x,y)\right)}_{f(r_{\phi},\pi_{\mathrm{ref}},\beta)} -\underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}}_{\mathrm{KL}}\right] \tag{10}
$$

>  我们可以将该步骤下的优化目标重写为以上形式，推导如下

$$
\begin{align}
&\min_{\pi_\theta} \mathrm D_{\mathrm{KL}}(\pi_\theta(y\mid x)\| \pi^*(y\mid x))\\
=&\min_{\pi_\theta} \mathbb E_{\pi_\theta}\left[\log\frac {\pi_\theta(y\mid x)}{\pi^*(y\mid x)}\right]\\
=&\min_{\pi_\theta} \mathbb E_{\pi_\theta}\left[ \log \frac {\pi_\theta(y\mid x)}{\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp \left(\frac{1}{\beta} r_\phi(x,y)\right)}\right]\\
=&\min_{\pi_\theta} \mathbb E_{\pi_\theta}\left[ \log \pi_\theta (y\mid x) + \log Z(x) - \log \pi_{\text{ref}}(y\mid x) - \frac 1 \beta r_\phi(x,y)\right]\\
=&\min_{\pi_\theta} \mathbb E_{\pi_\theta}\left[ \log \frac {\pi_\theta(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \log Z(x)  - \frac 1 \beta r_\phi(x,y)\right]\\
=&\max_{\pi_\theta}\mathbb E_{\pi_\theta}\left[\frac 1 \beta r_\phi(x,y) - \log Z(x) - \log \frac {\pi_\theta(y\mid x)}{\pi_{\text{ref}}(y\mid x)}\right]\\
=&\max_{\pi_\theta}\mathbb E_{\pi_\theta}\left[r_\phi(x,y) - \beta\log Z(x) - \beta\log \frac {\pi_\theta(y\mid x)}{\pi_{\text{ref}}(y\mid x)}\right]
\end{align}
$$

This is the same objective optimized in prior works [51, 40, 1, 28] using the DPO-equivalent reward for the reward class of $r_{\phi}$ . 
>  Eq 10 实际上就是早期一些工作中使用的优化目标

In this setting, we can interpret the normalization term in $f(r_{\phi},\pi_{\mathrm{ref}},\beta)$ as the soft value function of the reference policy $\pi_{\mathrm{ref}}$ . 
>  在这种设定下，我们可以将 $f(r_\phi, \pi_{\text{ref}}, \beta)$ 中的归一化项解释为参考策略 $\pi_{\text{ref}}$ 的软价值函数

>  不考虑之后的步骤，则 $\sum_y \pi_{\text{ref}}(y\mid x)r_\phi(x,y) = \mathbb E_{y\sim \pi_{\text{ref}}(\cdot \mid x)}[r_\phi(x,y)]$ 就可以视作策略 $\pi_{\text{ref}}$ 的 “硬” (状态) 价值函数，则
 
$$
\begin{align}
&\beta \log \sum_{y}\pi_{\text{ref}}(y\mid x) \exp\left(\frac 1 \beta r_\phi(x, y)\right)\\
=&\beta \log \mathbb E_{y\sim \pi_{\text{ref}}(\cdot\mid x)}\left[\exp\left(\frac 1 \beta r_\phi(x,y)\right)\right]\\
\ge&\beta \mathbb E_{y\sim \pi_{\text{ref}}(\cdot\mid x)}\left[\log\exp\left(\frac 1 \beta r_\phi(x,y)\right)\right]\\
=&\beta \mathbb E_{y\sim \pi_{\text{ref}}(\cdot\mid x)}\left[\frac 1 \beta r_\phi(x,y)\right]\\
=& \mathbb E_{y\sim \pi_{\text{ref}}(\cdot\mid x)}\left[ r_\phi(x,y)\right]\\
\end{align}
$$

>  故这个 “软” 价值函数实际上是 “硬” 价值函数的一个上界，如果 Jensen's inequality 的等号成立，则二者相等
>  而 Jensen's inequality 的等号成立要求 $\pi_{\text{ref}}$ 为一个确定性分布，即熵为 0，因此 “软” 价值函数实际上是在 “硬” 价值函数的基础上加上了对策略熵的衡量
>  在能够保持相近的动作选择分布的情况下，策略熵越大，软价值会越高

While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. 
>  这一项不会影响最优策略，但会影响策略梯度的方差
>  我们可以用学习到的价值函数作为规范化项，这在 actor-critic 方法中很常见，但学习需要额外的开销
>  先前有工作使用单步的 MC 估计作为规范化项

In contrast the DPO reparameterization yields a reward function that does not require any baselines.
>  相较于这些工作，DPO 直接优化策略，这个归一化项隐式地被吸收到了优化过程中 (可以看到 $\nabla_{\theta} \mathcal L_{\text{DPO}}$ 的 $\hat r_\theta$ 都是已经减去了 $Z(x)$ 的形式，我们不需要计算/估计 $Z(x)$，且 $Z(x)$ 本身也难以估计)，因此不需要任何的显式基线函数

# 6 Experiments
In this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. 
>  本节经验性评估 DPO 直接从偏好中训练策略的效果，包括
>  - 在文本生成的背景下，相较于常见的偏好学习算法，DPO 在最大化奖励和最小化 KL 散度中的 tradeoff 效率如何
>  - 在更大的模型和更难的 RLHF 任务下，评估 DPO 的表现

We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of $N$ sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix C.
>  我们发现在几乎不调节超参数的情况下，DPO 可以优于 baseline

**Tasks.** Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences $\mathcal{D} = \{x^{(i)},y_w^{(i)},y_l^{(i)}\}_{i = 1}^N$ . 

In controlled sentiment generation, $x$ is a prefix of a movie review from the IMDb dataset [24], and the policy must generate $y$ with positive sentiment. In order to perform a controlled evaluation, for this experiment we generate preference pairs over generations using a pre-trained sentiment classifier, where $p(\text{positive} | x, y_w) > p(\text{positive} | x, y_l)$ . For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App C.1). 

In summarization, $x$ is a forum post from Reddit; the policy must generate a summary $y$ of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset [43] along with human preferences gathered by Stiennon et al.. We use an SFT model fine-tuned on human-written forum post summaries with the TRLX [44] framework for RLHF. The human preference dataset was gathered by Stiennon et al. on samples from a different, but similarly-trained, SFT model. Finally, in single-turn dialogue, $x$ is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset [1], containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.

**Evaluation.** Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have access to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their win rate against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics [10], we conduct a human study to justify our usage of GPT-4 for evaluation in Sec. 6.4. We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement.

**Methods.** In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with GPT-I [45] in the summarization task and 2-shot prompting with Pythia-2.8B [3] in the dialogue task. In addition, we evaluate the SFT model as well as Preferred-FT, which is a model fine-tuned with supervised learning on the chosen completion $y_{w}$ from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is Unlikelihood [46], which simply optimizes the policy to maximize the probability assigned to $y_{w}$ and minimize the probability assigned to $y_{l}$ ; we use an optional coefficient $\alpha \in [0,1]$ on the 'unlikelihood' term. We also consider PPO [39] using a reward function learned from the preference data and PPO-GT, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of the-shelf version [44] as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running 'normal' PPO with learned rewards). Finally, we consider the Best of $N$ baseline, sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every query at test time.

## 6.1 How well can DPO optimize the RLHF objective?
The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. 

![](https://cdn-mineru.openxlab.org.cn/extract/57987395-f3ea-45d4-83af-37e1c411b1ba/a380355e97cafe42cf74b4c9fd566c0e815a1d811888a38d51af4f114b48e43c.jpg) 

Figure 2: Left. The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected reward for all KL values, demonstrating the quality of the optimization. Right. TL; DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO's best-case performance on summarization, while being more robust to changes in the sampling temperature.

Figure 2 shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target $\mathrm{KL} \in \{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$ , $\alpha \in \{0.05,0.1,0.5,1\}$ for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level $\mathrm{KL}^3$ with the reference policy $\mathrm{KL}(\pi \parallel \pi_{\mathrm{ref}})$ . 

We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, even when PPO can access ground truth rewards (PPO-GT).

## 6.2 Can DPO scale to real preference datasets?
Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, automatic evaluation metrics such as ROUGE can be poorly correlated with human preferences [40], and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure 2 (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model<sup>4</sup>. We find that DPO has a win rate of approximately $61\%$ at a temperature of 0.0, exceeding the performance of PPO at $57\%$ at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that we did not meaningfully tune DPO's $\beta$ hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section 6.4, where DPO samples at temperature 0.25 were preferred $58\%$ times over PPO samples at temperature 0.

On single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset [1] with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of $N$ baseline platreas at 128 completions for this task; see Appendix Figure 4) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset from a well-known source, but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure 3 shows that DPO converges to its best performance relatively quickly.

## 6.3 Generalization to a new input distribution
To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset [26], using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table 1. We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words "forum post" with "news article". For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.

<center><table><tr><td rowspan="2">Alg.</td><td colspan="2">Win rate vs. ground truth</td></tr><tr><td>Temp 0</td><td>Temp 0.25</td></tr><tr><td>DPO</td><td>0.36</td><td>0.31</td></tr><tr><td>PPO</td><td>0.26</td><td>0.23</td></tr></table></center>

Table 1: GPT-4 win rates vs. ground truth summaries for out-of-distribution CNN/DailyMail input articles.

We conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The GPT-4 (S) (simple) prompt simply asks for which summary better-summarizes the important information in the post. The GPT-4 (C) (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the GPT-4 (S) prompt. See Appendix C.2 for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the GPT-4 (C) prompt generally provides with rates more representative of humans; we therefore use this prompt for the main results in Section 6.2. For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix D.3.

<center><table><tr><td>DPO</td><td>SFT</td><td>PPO-1</td><td></td></tr><tr><td>N respondents</td><td>272</td><td>122</td><td>199</td></tr><tr><td>GPT-4 (S) win %</td><td>47</td><td>27</td><td>13</td></tr><tr><td>GPT-4 (C) win %</td><td>54</td><td>32</td><td>12</td></tr><tr><td>Human win %</td><td>58</td><td>43</td><td>17</td></tr><tr><td>GPT-4 (S)-H agree</td><td>70</td><td>77</td><td>86</td></tr><tr><td>GPT-4 (C)-H agree</td><td>67</td><td>79</td><td>85</td></tr><tr><td>H-H agree</td><td>65</td><td>-</td><td>87</td></tr></table></center>

Table 2: Comparing human and GPT-4 win rates and per-judgment agreement on TL;DR summarization samples. Humans agree with GPT-4 about as much as they agree with each other. Each experiment compares a summary from the stated method with a summary from PPO with temperature 0.

# 7 Discussion
Learning from preferences is a powerful, scalable framework for training capable, aligned language models. 

We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences directly, with a simple cross-entropy loss, without reinforcement learning or loss of generality. 
>  DPO 是一个不使用 RL ，从偏好中学习策略的训练范式
>  DPO 利用了策略和奖励函数之间的关系，直接推导出了满足人类偏好的等效交叉熵目标

With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.
>  DPO 大幅降低了从人类偏好中微调语言模型的成本

![](https://cdn-mineru.openxlab.org.cn/extract/57987395-f3ea-45d4-83af-37e1c411b1ba/a0ee481f2c4a1f28d480c57850227b64591c87976571271eade7bc7c8d01e33c.jpg) 

Figure 3: Left. Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method that improves over chosen summaries in the Anthropic-HH test set. Right. Win rates for different sampling temperatures over the course of training. DPO's improvement over the dataset labels is fairly stable over the course of training for different sampling temperatures.

**Limitations & Future Work.** Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure 3-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.

# A Mathematical Derivations
## A.1 Deriving the Optimum of the KL-Constrained Reward Maximization Objective
In this appendix, we will derive Eq. 4. Analogously to Eq. 3, we optimize the following objective:

$$
\max_{\pi}\mathbb{E}_{x\sim \mathcal{D},y\sim \pi}\big[r(x,y)\big] -\beta \mathbb{D}_{\mathrm{KL}}\big[\pi (y|x)\big||\pi_{\mathrm{ref}}(y|x)\big] \tag{11}
$$

under any reward function $r(x,y)$ , reference model $\pi_{\mathrm{ref}}$ and a general non-parametric policy class. 

>  Eq 3 定义的优化目标形式如上

We now have:

$$
\begin{align}
& \max_{\pi} \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi}\left[r (x, y)\right] -\beta \mathbb{D}_{\mathrm{KL}}\left[\pi (y|x)\,\|\,\pi_{\mathrm{ref}}(y|x)\right] \\
&  = \max_{\pi} \mathbb{E}_{x\sim \mathcal{D}} \mathbb{E}_{y\sim \pi (y|x)}\left[r (x, y) -\beta \log \frac{\pi (y|x)}{\pi_{\mathrm{ref}}(y|x)}\right] \\
&  = \min_{\pi} \mathbb{E}_{x\sim \mathcal{D}} \mathbb{E}_{y\sim \pi (y|x)}\left[\log \frac{\pi (y|x)}{\pi_{\mathrm{ref}}(y|x)} -\frac{1}{\beta} r (x, y)\right] \\
&  = \min_\pi \mathbb E_{x\sim \mathcal D}\mathbb E_{y\sim \pi(y\mid x)}\left[\log \frac {\pi(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \log Z(x) - \log Z(x) - \frac 1 \beta r(x,y)\right]\\
&  = \min_\pi \mathbb E_{x\sim \mathcal D}\mathbb E_{y\sim \pi(y\mid x)}\left[\log \frac {\pi(y\mid x)}{\frac 1 {Z(x)}\pi_{\text{ref}}(y\mid x)} - \log Z(x) - \frac 1 \beta r(x,y)\right]\\
&  = \min_{\pi} \mathbb{E}_{x\sim \mathcal{D}} \mathbb{E}_{y\sim \pi (y|x)}\left[\log \frac{\pi (y|x)}{\frac{1}{Z (x)}\pi_{\mathrm{ref}}(y|x)\exp\left (\frac{1}{\beta}r (x, y)\right)} -\log Z (x)\right]
\end{align}
\tag{12}
$$

where we have partition function:

$$
Z(x) = \sum_{y}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r(x,y)\right).
$$

Note that the partition function is a function of only $x$ and the reference policy $\pi_{\mathrm{ref}}$ , but does not depend on the policy $\pi$ . We can now define

$$
\pi^{*}(y|x) = \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r(x,y)\right),
$$

which is a valid probability distribution as $\pi^{*}(y|x)\geq 0$ for all $y$ and $\textstyle \sum_{y}\pi^{*}(y|x) = 1$ . 

>  Eq 3/Eq 11 可以按照 Eq 12 等价变化，其中函数 $Z(x)$ 可以是任意函数
>  实际中函数 $Z(x)$ 被定义为配分函数，形式如上

>  根据 $Z(x)$ 的如上形式，可以知道 $Z(x)$ 实际上是分布

$$
\pi^*(y\mid x) = \frac 1 {Z(x)}\pi_{\text{ref}}(y\mid x)\exp\left(\frac 1 \beta r(x,y)\right)
$$

>  的配分函数，也就是对应了未归一化分布 $\pi_{\text{ref}}(y\mid x)\exp\left(\frac 1 \beta r (x, y)\right)$ 的归一化因子

>  因此 Eq 12 的推导实际上是将原目标重写为了

$$
\min_\pi\mathbb E_{x\sim \mathcal D}\mathbb E_{y\sim \pi(y\mid x)}\left[\log \frac {\pi(y\mid x)}{\pi^*(y\mid x)}-\log Z(x)\right]
$$

Since $Z(x)$ is not a function of $y$ , we can then re-organize the final objective in Eq 12 as:

$$
\begin{array}{r}\min_{\pi}\mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{E}_{y\sim \pi (y|x)}\left[\log \frac{\pi(y|x)}{\pi^{*}(y|x)}\right] -\log Z(x)\right] = \\ \min_{\pi}\mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{D}_{\mathrm{KL}}(\pi (y|x)\mid \pi^{*}(y|x)) -\log Z(x)\right] \end{array} \tag{13}
$$

>  $Z(x)$ 和 $y$ 无关，故 Eq 12 可以按照上面简化，其中利用了 KL 散度的定义 $D_{\text{KL}}(p\| q) = \mathbb E_p[\log \frac p q]$

Now, since $Z(x)$ does not depend on $\pi$ , the minimum is achieved by the policy that minimizes the first KL term. Gibbs' inequality tells us that the KL-divergence is minimized at 0 if and only if the two distributions are identical. Hence we have the optimal solution:

$$
\pi (y|x) = \pi^{*}(y|x) = \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r(x,y)\right) \tag{15}
$$

for all $x\in D$ . This completes the derivation.

>  $Z(x)$ 和优化目标 $\pi$ 无关，故实际的优化目标就是第一项 KL 散度项
>  KL 散度项最小当且仅当 $\pi(y\mid x) = \pi^*(y\mid x)$，因此优化目标 $\pi$ 的最优解就是 $\pi^*$

## A.2 Deriving the DPO Objective Under the Bradley-Terry Model
It is straightforward to derive the DPO objective under the Bradley-Terry preference model as we have

$$
p^* (y_1\succ y_2|x) = \frac{\exp(r^*(x,y_1))}{\exp(r^*(x,y_1)) + \exp(r^*(x,y_2))} \tag{16}
$$

>  B-T 偏好模型下，$y_1$ 相对于 $y_2$ 被偏好的概率的形式定义如上

In Section 4 we showed that we can express the (unavailable) ground-truth reward through its corresponding optimal policy:

$$
r^* (x,y) = \beta \log \frac{\pi^* (y|x)}{\pi_{\mathrm{ref}} (y|x)} + \beta \log Z(x) \tag{17}
$$

Substituting Eq. 17 into Eq. 16 we obtain:

$$
\begin{align}
p^{*}(y_{1}\succ y_{2}|x) &= \frac{\exp\left (\beta\log\frac{\pi^{*}(y_{1}|x)}{\pi_{\mathrm{ref}}(y_{1}|x)} + \beta\log Z (x)\right)}{\exp\left (\beta\log\frac{\pi^{*}(y_{1}|x)}{\pi_{\mathrm{ref}}(y_{1}|x)} + \beta\log Z (x)\right) + \exp\left (\beta\log\frac{\pi^{*}(y_{2}|x)}{\pi_{\mathrm{ref}}(y_{2}|x)} + \beta\log Z (x)\right)} \\
&  = \frac{1}{1 + \exp\left (\beta\log\frac{\pi^{*}(y_{2}|x)}{\pi_{\mathrm{ref}}(y_{2}|x)} -\beta\log\frac{\pi^{*}(y_{1}|x)}{\pi_{\mathrm{ref}}(y_{1}|x)}\right)} \\
&  = \sigma \left (\beta \log \frac{\pi^{*}(y_{1}|x)}{\pi_{\mathrm{ref}}(y_{1}|x)} -\beta \log \frac{\pi^{*}(y_{2}|x)}{\pi_{\mathrm{ref}}(y_{2}|x)}\right).
\end{align}
$$

The last line is the per-instance loss in Equation 7.

>  我们将奖励模型以最优策略、参考策略、配分函数相关的形式写出，并代入 Eq 17，可以得到偏好概率的另一种表示形式
>  显然，在该表示形式下，偏好概率和最优策略、参考策略相关，和奖励函数无关，也就是我们将偏好概率直接写为了和最优策略相关的形式

## A.3 Deriving the DPO Objective Under the Plackett-Luce Model
The Plackett-Luce model [32, 23] is a generalization of the Bradley-Terry model over rankings (rather than just pairwise comparisons). Similar to the Bradley-Terry model, it stipulates that when presented with a set of possible choices, people prefer a choice with probability proportional to the value of some latent reward function for that choice. 
>  P-L 模型是对 B-T 模型的推广，和 B-T 模型一样，该模型也规定了当面对一组可能的选择时，人们倾向于选择某个选项的概率和其该选项潜在的价值成正比

> [!info] Plackett-Luce
> P-L 模型是 B-T 模型的推广，B-T 模型处理成对比较，P-L 模型将其拓展到 3 个以及 3 个以上，如果只处理成对比较，P-L 模型就是 B-T 模型
> 
> P-L 模型是用于分析和预测一组项目之间的排名或偏好的统计模型
> P-L 模型基于 Luce 的选择公理，该公理指出，在选择集中，选择一个项目而非另一个项目的几率，独立于其他任何项目存在与否
> 
> P-L 模型按照顺序和各个项目的强度参数对项目进行排名
> 1. 第一次选择: 某个项目被排在首位的概率是其强度除以所有项目的强度和
> 2. 第二次选择: 某个项目排在第二的概率是其强度除以除了第一个项目的剩余项目的强度和 (也就是忽略第一个项目，该项目排在剩余项目的首位的概率)
> 3. 这个过程逐渐进行，直到所有项目完成排名

In our context, when presented with a prompt $x$ and a set of $K$ answers $y_{1},\ldots ,y_{K}$ a user would output a permutation $\tau :[K]\to [K]$ , giving their ranking of the answers. The Plackett-Luce model stipulates that

$$
p^* (\tau |y_1,\ldots ,y_K,x) = \prod_{k = 1}^{K}\frac{\exp(r^*(x,y_{\tau(k)}))}{\sum_{j = k}^{K}\exp(r^*(x,y_{\tau(j)}))} \tag{18}
$$

Notice that when $K = 2$ , Equation 18 reduces to the Bradley-Terry model. 

>  P-L 模型中，一个特定的排名的概率通过 Eq 18 计算

However, for the general Plackett-Luce model, we can still utilize the results of Eq. 5 and substitute the reward function parameterized by its optimal policy. Similarly to Appendix A.2, the normalization constant $Z(x)$ cancels out and we're left with:

$$
p^* (\tau |y_1,\ldots ,y_K,x) = \prod_{k = 1}^{K}\frac{\exp\left(\beta\log\frac{\pi^*(y_\tau(k)|x)}{\pi_{\mathrm{ref}}(y_\tau(k)|x)}\right)}{\sum_{j = k}^{K}\exp\left(\beta\log\frac{\pi^*(y_\tau(j)|x)}{\pi_{\mathrm{ref}}(y_\tau(j)|x)}\right)} \tag{19}
$$

>  我们奖励函数表示 “强度”，代入 Eq 18，就得到了根据奖励函数对 $K$ 个回应进行某个特定排名的概率
>  我们继而用和最优策略相关的形式替代奖励函数的原形式，就得到了 Eq 19

Similarly to the approach of Section 4, if we have access to a dataset $\begin{array}{rl}{\mathcal{D}}&{=}\end{array}$ $\{\tau^{(i)},y_{1}^{(i)},\dots,y_{K}^{(i)},x^{(i)}\}_{i = 1}^{N}$ of prompts and user-specified rankings, we can use a parameterized model and optimize this objective with maximum-likelihood.:

$$
\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta},\pi_{\mathrm{ref}}) = -\mathbb{E}_{\tau ,y_{1},\ldots ,y_{K},x\sim \mathcal{D}}\left[\log \prod_{k = 1}^{K}\frac{\exp\left(\beta\log\frac{\pi_{\theta}(y_{\tau(k)}|x)}{\pi_{\mathrm{ref}}(y_{\tau(k)}|x)}\right)}{\sum_{j = k}^{K}\exp\left(\beta\log\frac{\pi_{\theta}(y_{\tau(j)}|x)}{\pi_{\mathrm{ref}}(y_{\tau(j)}|x)}\right)}\right] \tag{20}
$$

>  训练目标是要最大化标记员给出的排序的似然，因此根据 Eq 19 构造出的直接和最优策略相关的损失函数就是 Eq 20

## A.4 Deriving the Gradient of the DPO Objective
In this section we derive the gradient of the DPO objective:

$$
\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) = -\nabla_{\theta}\mathbb{E}_{(x,y_{w},y_{l})\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)} -\beta \log \frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}\right)\right] \tag{21}
$$

We can rewrite the RHS of Equation 21 as

$$
\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) = -\mathbb{E}_{(x,y_{w},y_{l})\sim \mathcal{D}}\left[\frac{\sigma^{\prime}(u)}{\sigma(u)}\nabla_{\theta}(u)\right], \tag{22}
$$

where $\begin{array}{r}u = \beta \log \frac{\pi_{\theta}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)} -\beta \log \frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)} \end{array}$

Using the properties of sigmoid function $\sigma^{\prime}(x) = \sigma (x)(1 -\sigma (x))$ and $\sigma (-x) = 1 -\sigma (x)$ , we obtain the final gradient

$$
\begin{array}{r l} & {\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) =}\\ & {-\mathbb{E}_{(x,y_{w},y_{l})\sim \mathcal{D}}\left[\beta \sigma \left(\beta \log \frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)} -\beta \log \frac{\pi_{\theta}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right)\left[\nabla_{\theta}\log \pi (y_{w}\mid x) -\nabla_{\theta}\log \pi (y_{l}\mid x)\right]\right],} \end{array}
$$

After using the reward substitution of $\begin{array}{r}{\hat{r}_{\theta}(x,y) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{ref}}(y|x)}} \end{array}$ we obtain the final form of the gradient from Section 4.

## A.5 Proof of Lemma 1 and 2
In this section, we will prove the two lemmas from Section 5.

**Lemma 1 Restated.** Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution.
>  Lemma: 在 P-L 偏好框架下，来自相同等价类的两个奖励函数将导出相同的偏好分布

**Proof.** We say that two reward functions $r(x,y)$ and $r^{\prime}(x,y)$ are from the same equivalence class if $r^{\prime}(x,y) = r(x,y) + f(x)$ for some function $f$ . We consider the general Plackett-Luce (with the Bradley-Terry model a special case for $K = 2$ ) and denote the probability distribution over rankings induced by a particular reward function $r(x,y)$ as $p_{r}$ . 
>  Proof
>  Definition 1 中声明了，如果两个奖励函数满足 $r'(x, y) = r(x, y) + f(x)$，则二者来自同一个等价类
>  我们记由奖励函数 $r(x, y)$ 导出的 (在排序上的) 偏好分布为 $p_r$

For any prompt $x$ , answers $y_{1},\ldots ,y_{K}$ and ranking $\tau$ we have:

$$
\begin{array}{r l} & {p_{r^{\prime}}(\tau |y_{1},\ldots ,y_{K},x) = \prod_{k = 1}^{K}\frac{\exp(r^{\prime}(x,y_{\tau(k)}))}{\sum_{j = k}^{K}\exp(r^{\prime}(x,y_{\tau(j)}))}}\\ & {\qquad = \prod_{k = 1}^{K}\frac{\exp(r(x,y_{\tau(k)}) + f(x))}{\sum_{j = k}^{K}\exp(r(x,y_{\tau(j)}) + f(x))}}\\ & {\qquad = \prod_{k = 1}^{K}\frac{\exp(f(x))\exp(r(x,y_{\tau(k)}))}{\exp(f(x))\sum_{j = k}^{K}\exp(r(x,y_{\tau(j)}))}}\\ & {\qquad = \prod_{k = 1}^{K}\frac{\exp(r(x,y_{\tau(k)}))}{\sum_{j = k}^{K}\exp(r(x,y_{\tau(j)}))}}\\ & {\qquad = p_{r}(\tau |y_{1},\ldots ,y_{K},x),} \end{array}
$$

which completes the proof.

>  我们进而直接将奖励函数代入 P-L 模型的概率函数形式中，很容易可以发现两个奖励函数之间相差的 $f(x)$ 项可以在上下被消掉，因此证明了来自同一等价类的两个奖励函数在 P-L 框架下定义的是相同的偏好分布

**Lemma 2 Restated.** Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.
>  Lemma 2
>  来自相同等价类的两个奖励函数在约束 RL 问题下导出相同的最优策略

**Proof.** Let us consider two reward functions from the same class, such that $r'(x,y) = r(x,y) + f(x)$ and, let us denote as $\pi_r$ and $\pi_{r'}$ the corresponding optimal policies. By Eq. 4, for all $x,y$ we have

$$
\begin{align*}
& \pi_{r^{\prime}}(y|x) = \frac{1}{\sum_{y}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r^{\prime}(x,y)\right)}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r^{\prime}(x,y)\right) \\
& \quad = \frac{1}{\sum_{y}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}(r(x,y) + f(x))\right)}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} (r(x,y) + f(x))\right) \\
& \quad = \frac{1}{\exp\left(\frac{1}{\beta}f(x)\right)\sum_{y}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r(x,y)\right)\exp \left(\frac{1}{\beta} f(x)\right) \\
& \quad = \frac{1}{\sum_{y}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r(x,y)\right) \\
& \quad = \pi_{r}(y|x),
\end{align*}
$$

which completes the proof. 

>  Proof.
>  假设由两个来自相同等价类的奖励函数 $r'(x, y), r(x, y)$，记它们各自的最优策略为 $\pi_r, \pi_{r'}$
>  我们根据最优策略和奖励函数相关的形式，用奖励函数和参考策略表示最优策略，同样可以发现 $f(x)$ 可以被轻松消掉

## A.6 Proof of Theorem 1
In this section, we will expand on the results of Theorem 1.

**Theorem 1 Restated.** Assume, we have a reference model, such that $\pi_{ref}(y|x) > 0$ for all pairs of prompts $x$ and answers $y$ and a parameter $\beta >0$ . All reward equivalence classes, as defined in Section 5 can be represented with the reparameterization $\begin{array}{r}r(x,y) = \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} \end{array}$ for some model $\pi (y|x)$
>  Theorem 1
>  假设有参考模型满足 $\pi_{\text{ref}}(y\mid x) > 0\ \text{for all } x, y$，有参数 $\beta > 0$
>  则所有的奖励函数等价类，都可以用参数化形式 $r(x, y) = \beta \log \frac {\pi(y\mid x)}{\pi_{\text{ref}}(y\mid x)}$ 表示，其中 $\pi(y\mid x)$ 是某个模型

**Proof.** Consider any reward function $r(x,y)$ , which induces an optimal model $\pi_r(y|x)$ under the KL-constrained RL problem, with solution given by 4. Following Eq. 5, when we log-linearize both sides we obtain:

$$
r(x,y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\mathrm{ref}}(y|x)} +\beta \log Z(x)
$$
 
where $\begin{array}{r}Z(x) = \sum_{y}\pi_{\mathrm{ref}}(y|x)\exp \left(\frac{1}{\beta} r(x,y)\right) \end{array}$ (notice that $Z(x)$ also depends on the reward function $r$ . 

>  根据 Eq 5，我们知道在 KL 约束 RL 问题下，任意一个奖励函数 $r(x, y)$ 都可以写成和它的最优策略 $\pi_r(y\mid x)$ 相关的形式，如上所示
>  其中 $Z(x)$ 是配分函数，它和 $x$ 以及 $r$ 相关
 
 Using the operator $r^{\prime}(x,y) = f(r,\pi_{\mathrm{ref}},\beta)(x,y) = r(x,y) -\beta \log Z(x)$ , we see that this new reward function is within the equivalence class of $r$ and, we have:
 
$$
r^{\prime}(x,y) = \beta \log \frac{\pi_{r}(y|x)}{\pi_{\mathrm{ref}}(y|x)}
$$

which completes the proof.

>  我们定义一个新的奖励函数 $r'(x, y) = r(x, y) - \beta \log Z(x)$，容易知道 $r'(x, y)$ 和 $r(x, y)$ 属于同一个等价类 (故导出相同的最优策略)，$r'(x, y)$ 的形式如上所示
>  因此，我们可以用 $r'(x, y)$ 的这种更简单的形式表示策略 $\pi_r(y\mid x)$ 对应的奖励函数等价类，证明完毕

We can further expand on these results. We can see that if $r$ and $r^\prime$ are two reward functions in the same class, then

$$
f(r,\pi_{\mathrm{ref}},\beta)(x,y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\mathrm{ref}}(y|x)} = \beta \log \frac{\pi_r'(y|x)}{\pi_{\mathrm{ref}}(y|x)} = f(r',\pi_{\mathrm{ref}},\beta)(x,y)
$$

where the second equality follows from Lemma 2. 

We have proven that the operator $f$ maps all reward functions from a particular equivalence class to the same reward function. 

>  我们可以用算子 $f$ 表示将任意的奖励函数映射到和它相同等价类中，形式最简单的那个奖励函数的映射，那么以上的等式就成立

Next, we show that for every equivalence class of reward functions, the reward function that has the reparameterization outlined in Theorem 1 is unique.
>  我们接着证明在任意的奖励函数等价类中，形式为 Theorem 1 所描述的奖励函数 (形式最简单的那一个奖励函数) 是唯一的

**Proposition 1.** Assume, we have a reference model, such that $\pi_{ref}(y|x) > 0$ for all pairs of prompts $x$ and answers $y$ and a parameter $\beta >0$ .Then every equivalence class of reward functions, as defined in Section 5, has a unique reward function $r(x,y)$ , which can be reparameterized as $\begin{array}{r}r(x,y) = \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} \end{array}$ for some model $\pi (y|x)$
>  假设有参考模型满足 $\pi_{\text{ref}}(y\mid x) > 0\ \text{for all } x, y$，有参数 $\beta > 0$
>  则每个奖励函数等价类中都有会有一个唯一的奖励函数 $r(x, y)$，其参数化形式为 $r(x, y) = \beta \log \frac {\pi(y\mid x)}{\pi_{\text{ref}}(y\mid x)}$，其中 $\pi(y\mid x)$ 是某个模型

**Proof.** We will proceed using proof by contradiction. Assume we have two reward functions from the same class, such that $r^{\prime}(x,y) = r(x,y) + f(x)$ . Moreover, assume that $\begin{array}{r}r^{\prime}(x,y) = \beta \log \frac{\pi^{\prime}(y|x)}{\pi_{\mathrm{ref}}(y|x)} \end{array}$ for some model $\pi^{\prime}(y|x)$ and $\begin{array}{r}r(x,y) = \beta \log \frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)} \end{array}$ for some model $\pi (y|x)$ , such that $\pi \neq \pi^{\prime}$ . We then have

$$
r^{\prime}(x,y) = r(x,y) + f(x) = \beta \log \frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)} +f(x) = \beta \log \frac{\pi(y|x)\exp(\frac{1}{\beta}f(x))}{\pi_{\mathrm{ref}}(y|x)} = \beta \log \frac{\pi^{\prime}(y|x)}{\pi_{\mathrm{ref}}(y|x)}
$$

for all prompts $x$ and completions $y$ . Then we must have $\begin{array}{r}\pi (y|x)\exp (\frac{1}{\beta} f(x)) = \pi '(y|x) \end{array}$ . Since these are distributions, summing over $y$ on both sides, we obtain that $\begin{array}{r}\exp (\frac{1}{\beta} f(x)) = 1 \end{array}$ and since $\beta >0$ we must have $f(x) = 0$ for all $x$ . Therefore $r(x,y) = r'(x,y)$ . This completes the proof.

>  Proof
>  使用反证法证明，假设有两个来自同一等价类的奖励函数 $r'(x, y) , r(x, y)$，并假设它们对应的模型是不同的
>  则根据假设，可以推导出 $\pi(y\mid x) \exp(\frac 1 \beta f(x)) = \pi'(y\mid x)$，两边对 $y$ 求和，就可以得到 $\exp(\frac 1 \beta f(x)) = 1$，因为 $\beta > 0$，故 $f(x) = 0 \text{ for all } x$ 
>  因此，$\pi(y\mid x) = \pi'(y\mid x)$，故每个等价类中的最简参数化形式是唯一的

We have now shown that every reward class has a unique reward function that can be represented as outlined in Theorem 1, which is given by $f(r, \pi_{\mathrm{ref}}, \beta)$ for any reward function in that class.

# B DPO Implementation Details and Hyperparameters
DPO is relatively straightforward to implement; PyTorch code for the DPO loss is provided below:

```python
import torch.nn.functional as F
def dpo_loss(pi_logps, ref_logps, yw_idx, yl_idx, beta): 
    """
    pi_logps: policy logprobs, shape (B,) 
    ref_logps: reference model logprobs, shape (B,) 
    yw_idx: preferred completion indices in [0, B-1], shape (T,)
    yl_idx: dispreferred completion indices in [0, B-1], shape (T,)
    beta: temperature controlling strength of KL penalty
    
    Each pair of (yw_idx[i], yl_idx[i]) represents the indices of a single preference pair. 
    """
    pi_yw_logps, pi_yl_logps = pi_logps[yw_idx], pi_logps[yl_idx]     
    ref_yw_logps, ref_yl_logps = ref_logps[yw_idx], ref_logps[yl_idx]
    
    pi_logratios = pi_yw_logps -pi_yl_logps 
    ref_logratios = ref_yw_logps -ref_yl_logps
    
    losses = -F.logsigmoid(beta * (pi_logratios -ref_logratios))
    rewards = beta * (pi_logps -ref_logps).detach()
    return losses, rewards
```

Unless noted otherwise, we use a $\beta = 0.1$ , batch size of 64 and the RMSprop optimizer with a learning rate of 1e-6 by default. We linearly warm up the learning rate from 0 to 1e-6 over 150 steps. For TL;DR summarization, we use $\beta = 0.5$ , while rest of the parameters remain the same.

# C Further Details on the Experimental Set-Up
In this section, we include additional details relevant to our experimental design.

## C.1 IMDB Sentiment Experiment and Baseline Details
The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained sentiment classifier siebert/sentiment-roberta-large-english as a ground-truth reward model and gpt2-large as a base model. We use these larger models as we found the default ones to generate low-quality text and rewards to be somewhat inaccurate. We first use supervised fine-tuning on a subset of the IMDB data for 1 epoch. We then use this model to sample 4 completions for 25000 prefixes and create 6 preference pairs for each prefix using the ground-truth reward model. The RLHF reward model is initialized from the gpt2-large model and trained for 3 epochs on the preference datasets, and we take the checkpoint with the highest validation set accuracy. The "TRL" run uses the hyper-parameters in the TRL library. Our implementation uses larger batch samples of 1024 per PPO step.

## C.2 GPT-4 prompts for computing summarization and dialogue win rates
A key component of our experimental setup is GPT-4 win rate judgments. In this section, we include the prompts used to generate win rates for the summarization and dialogue experiments. We use gpt-4-0314 for all our experiments. The order of summaries or responses are randomly chosen for every evaluation.

## C.3 Unlikelihood baseline
While we include the unlikelihood baseline [46] (simply maximizing $\log p(y_w|x)$ , the log probability of the preferred response, while minimizing $\log p(y_t|x)$ , the log probability of the dispreferred response) in our sentiment experiments, we do not include it as a baseline in either the summarization or dialogue experiment because it produces generally meaningless responses, which we believe is a result of unconstrained likelihood minimization.

# D Additional Empirical Results
## D.1 Performance of Best of $N$ baseline for Various $N$
We find that the Best of $N$ baseline is a strong (although computationally expensive, requiring sampling many times) baseline in our experiments. We include an evaluation of the Best of $N$ baseline for various $N$ for the Anthropic-HH dialogue and TL;DR summarization; the results are shown in Figure 4.

![](https://cdn-mineru.openxlab.org.cn/extract/57987395-f3ea-45d4-83af-37e1c411b1ba/422028df0a2e6377569646d7eef6a0e2d3fa00e5f3038ef79a0cb3167b5db6e4.jpg) 
Figure 4: Best of $N$ baseline for $N = \{1,4,16,64,128\}$ . Performance plateaus after roughly 64-128 samples.

## D.2 Sample Responses and GPT-4 Judgments
In this section, we present examples of comparisons between DPO and the baseline (PPO temp 0. for summarization, and the ground truth chosen response for dialogue). See Tables 4-6 for summarization examples, and Tables 7-10 for dialogue examples.

## D.3 Human study details
In order to validate the usage of GPT-4 for computing win rates, our human study collects human preference data for several matchups in the TL;DR summarization setting. We select three different algorithmic matchups, evaluating DPO (temp. 0.25), SFT (temp. 0.25), and PPO (temp 1.0) compared to the reference algorithm PPO (temp 0. ). By selecting matchups for three unique algorithms as well as algorithms with a wide range of win rates vs the reference, we capture the similarity of human and GPT-4 win rates across the response quality spectrum. We sample 150 random comparisons of DPO vs PPO-0 and 100 random comparisons PPO-1 vs PPO-0, assigning two humans to each comparison, producing 275 judgments for DPO-PPO and 200 judgments for PPO-PPO. We sample 125 SFT comparisons, assigning a single human to each. We ignore judgments that humans labeled as ties (which amount to only about $1\%$ of judgments), and measure the raw agreement percentage between human A and human B (for comparisons where we have two human annotators, i.e., not SFT) as well as between each human and GPT-4.